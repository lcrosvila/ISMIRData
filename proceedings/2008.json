[
    {
        "title": "Characterisation of Harmony With Inductive Logic Programming.",
        "author": [
            "Amelie Anglade",
            "Simon Dixon"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416550",
        "url": "https://doi.org/10.5281/zenodo.1416550",
        "ee": "https://zenodo.org/records/1416550/files/AngladeD08.pdf",
        "abstract": "We present an approach for the automatic characterisation of the harmony of song sets making use of relational induction of logical rules. We analyse manually annotated chord data available in RDF and interlinked with web identi\ufb01ers for chords which themselves give access to the root, bass, com- ponent intervals of the chords. We pre-process these data to obtain high-level information such as chord category, de- gree and intervals between chords before passing them to an Inductive Logic Programming software which extracts the harmony rules underlying them. This framework is tested over the Beatles songs and the Real Book songs. It gen- erates a total over several experiments of 12,450 harmony rules characterising and differentiating the Real Book (jazz) songs and the Beatles\u2019 (pop) music. Encouragingly, a pre- liminary analysis of the most common rules reveals a list of well-known pop and jazz patterns that could be completed by a more in depth analysis of the other rules. 1 INTRODUCTION The explosion of the size of personal and commercial mu- sic collections has left both content providers and customers with a common dif\ufb01culty: organising their huge musical li- braries in such a way that each song can be easily retrieved, recommended and included in a playlist with similar songs. Most of the metadata provided are hand-annotated by ex- perts such as in AllMusicGuide 1 or the result of a commu- nity effort such as in MusicBrainz 2 . Because classifying large amounts of data is expensive and/or time-consuming, people are gaining interest in the automatic characterisation of songs or groups of songs. Songs can be characterised by their rhythm, harmony, structure, instrumentation, etc. In this article we present the \ufb01rst step towards a framework able to automatically induce rules characterising songs by various musical phenomena. For this study we are interested in the automatic extraction of harmony patterns. Many of the existing approaches for pattern extraction and recognition make use of statistical descriptions [14]. 1 http://www.allmusic.com/ 2 http://musicbrainz.org/ However some of the attempts to automatically build de- scriptors of music explore the idea of using logical rules [6]. Such rules could be expressed as follows: (X1 \u2227X2...Xn) \u21d2Musical Phenomenon (1) where the Xi are structured descriptions of local musical content and Musical Phenomenon is the high level prop- erty we are interested in. The logical framework offers sev- eral advantages over the statistical framework, for example temporal relations between local musical events can easily be expressed. Moreover, logical rules are human-readable. Thus, automatically extracted musical patterns expressed in logical formulae can be transmitted as they are to musicolo- gists who can in turn analyse them. To induce such logical rules we can either use statistical",
        "zenodo_id": 1416550,
        "dblp_key": "conf/ismir/AngladeD08"
    },
    {
        "title": "Uncovering Affinity of Artists to Multiple Genres from Social Behaviour Data.",
        "author": [
            "Claudio Baccigalupo",
            "Enric Plaza",
            "Justin Donaldson"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417823",
        "url": "https://doi.org/10.5281/zenodo.1417823",
        "ee": "https://zenodo.org/records/1417823/files/BaccigalupoPD08.pdf",
        "abstract": "In organisation schemes, musical artists are commonly iden- ti\ufb01ed with a unique \u2018genre\u2019 label attached, even when they have af\ufb01nity to multiple genres. To uncover this hidden cul- tural awareness about multi-genre af\ufb01nity, we present a new model based on the analysis of the way in which a commu- nity of users organise artists and genres in playlists. Our work is based on a novel dataset that we have elabo- rated identifying the co-occurrences of artists in the playlists shared by the members of a popular Web-based community, and that is made publicly available. The analysis de\ufb01nes an automatic social-based method to uncover relationships between artists and genres, and introduces a series of novel concepts that characterises artists and genres in a richer way than a unique \u2018genre\u2019 label would do. 1 INTRODUCTION Musical genres have typically been used as a \u2018high level\u2019 classi\ufb01cation of songs and artists. However, many musical artists cannot be described with a single genre label. Artists can change style throughout their career, and perform songs that do not entirely fall into single categories such as \u2018Rock\u2019, \u2018R&B\u2019 or \u2018Jazz\u2019. In general terms, artists can be said to have a certain degree of af\ufb01nity to each speci\ufb01c genre. In this paper we propose to model relationships from artists to genres as fuzzy sets, where the membership degree indicates the af\ufb01nity of an artist to a genre. This is in con- trast to conventional labelling approaches, in which artists either belong or do not belong to a genre, and allows for a sentence like \u201cMadonna is Pop and R&B, not Jazz\u201d to be rephrased as \u201cMadonna belongs with a membership degree of 0.8 to Pop genre, with 0.6 to R&B and with 0.1 to Jazz\u201d. To uncover the af\ufb01nity of different artists to different gen- res, we follow a social approach: we exploit information about how people aggregate and organise artists and genres in playlists. Playlists are the predominant form of casual This research is partially supported by the MID-CBR (TIN2006- 15140-C03-01) project, by the Generalitat de Catalunya under the grant 2005-SGR-00093, and by a MyStrands scholarship. music organisation. When playlists are collected in aggre- gate over a large sampling of the music listening populace, they re\ufb02ect current cultural sensibilities for the association of popular music. Our assumption about large repositories of playlists is that when two artists or genres co-occur to- gether and closely in many playlists, they have some form of shared cultural af\ufb01nity. For example, if we observe that a large number of songs by an artist x co-occur often and closely with songs whose artists are classi\ufb01ed in the \u2018Jazz\u2019 genre, then we argue that x has a certain af\ufb01nity with Jazz, independently from the original \u2018genre\u2019 label attached to x. On the basis of this assumption, we build a model of multi-genre af\ufb01nity for artists, grounded on data about the usage of music objects by real people. We suggest this kind of analysis can provide results which are complementary to those provided by other content-based or social techniques, which we discuss in the following subsection. In this paper, we \ufb01rst discuss different approaches to- wards musical genre analysis, and provide arguments for the consideration of a community-based approach. Next, we report on the analysis we performed on a novel dataset, which lists the co-occurrences of 4,000 artists in a large col- lection of playlists, to uncover associations between artists and genres. Finally, we depict possible applications, such as af\ufb01nity-based generation of \u2018smooth sequences\u2019 of songs.",
        "zenodo_id": 1417823,
        "dblp_key": "conf/ismir/BaccigalupoPD08"
    },
    {
        "title": "Combining Feature Kernels for Semantic Music Retrieval.",
        "author": [
            "Luke Barrington",
            "Mehrdad Yazdani",
            "Douglas Turnbull",
            "Gert R. G. Lanckriet"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415070",
        "url": "https://doi.org/10.5281/zenodo.1415070",
        "ee": "https://zenodo.org/records/1415070/files/BarringtonYTL08.pdf",
        "abstract": "We apply a new machine learning tool, kernel combination, to the task of semantic music retrieval. We use 4 different types of acoustic content and social context feature sets to describe a large music corpus and derive 4 individual kernel matrices from these feature sets. Each kernel is used to train a support vector machine (SVM) classi\ufb01er for each seman- tic tag (e.g., \u2018aggressive\u2019, \u2018classic rock\u2019, \u2018distorted electric guitar\u2019) in a large tag vocabulary. We examine the individual performance of each feature kernel and then show how to learn an optimal linear combination of these kernels using convex optimization. We \ufb01nd that the retrieval performance of the SVMs trained using the combined kernel is superior to SVMs trained using the best individual kernel for a large number of tags. In addition, the weights placed on indi- vidual kernels in the linear combination re\ufb02ect the relative importance of each feature set when predicting a tag. 1 INTRODUCTION A signi\ufb01cant amount of music information retrieval (MIR) research has focused on signal processing techniques that extract features from audio content. Often, these feature sets are designed to re\ufb02ect different aspects of music such as timbre, harmony, melody and rhythm [15, 9]. In addition, researchers use data from online sources that places music in a social context [6, 16]. While individual sets of audio content and social context features have been shown to be useful for various MIR tasks (e.g., classi\ufb01cation, similarity, recommendation), it is unclear how to combine multiple feature sets for these tasks. This paper presents a principled approach to combining multiple feature sets by applying a useful machine learning technique: kernel combination [7]. A kernel function for each feature set measures the similarity between pairs of songs. Kernel functions can be designed to accommodate heterogeneous feature representations such as vectors [8], sets of vectors [4], distributions [10], or graphs [1]. We use the kernel function to compute a kernel matrix that encodes the similarity between all pairs of songs. From multiple feature sets, we compute multiple kernel matrices for a set of songs. We \ufb01nd the optimal linear combination of the kernel matrices using quadratically-constrained quadratic- programming (i.e., convex optimization). The result is a single kernel matrix that is used by a support vector machine (SVM) to produce a decision boundary (i.e., a classi\ufb01er). In this paper, we use kernel combination to integrate music information from two audio content and two social context feature sets. For each semantic tag (e.g., \u201caggressive\u201d, \u201cclas- sic rock\u201d, \u201cdistorted electric guitar\u201d) in our vocabulary, we learn the linear combination of the four kernel matrices that is optimal for SVM classi\ufb01cation and rank-order songs based on their relevance to the tag. The weights that are used to produce the combined kernel re\ufb02ect the importance of each feature set when producing an optimal classi\ufb01er. In the following subsection, we discuss related work on music feature representations, kernel methods in MIR re- search, and the origins of kernel combination. We continue in Section 2 with an overview of kernel methods and the ker- nel combination algorithm. Section 3 introduces a number of features that capture information about musical content and context and describes how to build song-similarity kernels from these features. Section 4 describes our experiments using individual and combined heterogenous feature kernels for semantic music retrieval. Section 5 concludes with a discussion of our \ufb01ndings.",
        "zenodo_id": 1415070,
        "dblp_key": "conf/ismir/BarringtonYTL08"
    },
    {
        "title": "Structured Polyphonic Patterns.",
        "author": [
            "Mathieu Bergeron",
            "Darrell Conklin"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415638",
        "url": "https://doi.org/10.5281/zenodo.1415638",
        "ee": "https://zenodo.org/records/1415638/files/BergeronC08.pdf",
        "abstract": "This paper presents a new approach to polyphonic music retrieval, based on a structured pattern representation. Poly- phonic patterns are formed by joining and layering pattern components into sequences and simultaneities. Pattern com- ponents are conjunctions of features which encode event properties or relations with other events. Relations between events that overlap in time but are not simultaneous are sup- ported, enabling patterns to express many of the temporal relations encountered in polyphonic music. The approach also provides a mechanism for de\ufb01ning new features. It is illustrated and evaluated by querying for three musicolog- ical patterns in a corpus of 185 chorale harmonizations by J.S.Bach. 1 INTRODUCTION",
        "zenodo_id": 1415638,
        "dblp_key": "conf/ismir/BergeronC08"
    },
    {
        "title": "Enhanced Bleedthrough Correction for Early Music Documents with Recto-Verso Registration.",
        "author": [
            "John Ashley Burgoyne",
            "Johanna Devaney",
            "Laurent Pugin",
            "Ichiro Fujinaga"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417235",
        "url": "https://doi.org/10.5281/zenodo.1417235",
        "ee": "https://zenodo.org/records/1417235/files/BurgoyneDPF08.pdf",
        "abstract": "Ink bleedthrough is common problem in early music doc- uments. Even when such bleedthrough does not pose prob- lems for human perception, it can inhibit the performance of optical music recognition (OMR). One way to reduce the amount of bleedthrough is to take into account what is printed on the reverse of the page. In order to do so, the reverse of the page must be registered to match the front of the page on a pixel-by-pixel basis. This paper describes our approach to registering scanned early music scores as well as our modi\ufb01cations to two robust binarization approaches to take into account bleedthrough and the information avail- able from the registration process. We determined that al- though the information from registration itself often makes little difference in recognition performance, other modi\ufb01ca- tions to binarization algorithms for correcting bleedthrough can yield dramatic increases in OMR results. 1 MOTIVATION AND BACKGROUND",
        "zenodo_id": 1417235,
        "dblp_key": "conf/ismir/BurgoyneDPF08"
    },
    {
        "title": "Using the SDIF Sound Description Interchange Format for Audio Features.",
        "author": [
            "Juan Jos\u00e9 Burred",
            "Carmine-Emanuele Cella",
            "Geoffroy Peeters",
            "Axel R\u00f6bel",
            "Diemo Schwarz"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415762",
        "url": "https://doi.org/10.5281/zenodo.1415762",
        "ee": "https://zenodo.org/records/1415762/files/BurredCPRS08.pdf",
        "abstract": "We present a set of extensions to the Sound Description In- terchange Format (SDIF) for the purpose of storage and/or transmission of general audio descriptors. The aim is to al- low portability and interoperability between the feature ex- traction module of an audio information retrieval application and the remaining modules, such as training, classi\ufb01cation or clustering. A set of techniques addressing the needs of short-time features and temporal modeling over longer win- dows are proposed, together with the mechanisms that allow further extensions or adaptations by the user. The paper is completed by an overview of the general aspects of SDIF and its practical use by means of a set of existing program- ming interfaces for, among others, C, C++ and Matlab. 1 INTRODUCTION The Sound Description Interchange Format (SDIF) [1, 2] is an established standard for the precise, well-de\ufb01ned and extensible storage and interchange of a variety of sound de- scriptions including representations of the signal for anal- ysis/synthesis like spectral, sinusoidal, time-domain, or higher-level models, audio descriptors like loudness or fun- damental frequency, markers, labels, and statistical models. SDIF consists of a basic data format framework based on time-tagged frames containing matrices of binary numbers or text, and an extensible set of standard type declarations corresponding to different sound descriptions. The SDIF standard was created in 1998 in collabora- tion between Ircam\u2013Centre Pompidou in Paris, France, CN- MAT at the University of Berkeley, USA, and the Music Technology Group (MTG) of the Universitat Pompeu Fabra, Barcelona, Spain. It arose out of the need to be able to store and exchange sound representation data between dif- ferent analysis/synthesis programs, research teams, and in- stitutions, and to enable anyone to interpret the information in the \ufb01les correctly, needing as little external information as possible. With the previously widely used headerless ASCII formats, this goal was seriously compromised, as well as precision and space ef\ufb01ciency lost. In the present contribution, we propose a set of exten- sions to the SDIF description types and conventions and best practices for the application of storing and/or transmitting a wide range of audio descriptors, e.g. as generated by the fea- ture 1 extraction stage of an audio pattern recognition sys- 1 We will use the terms feature and descriptor interchangeably. tem. In the Audio or Music Information Retrieval \ufb01elds, this is of interest to assure interoperability between feature ex- traction and further processing modules, such as feature se- lection, training, clustering or classi\ufb01cation, possibly being developed independently from each other, or even by differ- ent institutions. Such a need for standardization can arise in the context of multi-partner evaluation, such as in the Music Information Retrieval Evaluation eXchange (MIREX) [3]. SDIF is an open format, unhampered by licensing or in- tellectual property restrictions, made by the research com- munity for research and creation. There is a wide range of software available for its use: Libraries in C, C++, Java, Matlab, Perl, and Python, command-line and graph- ical tools, and plugins for audio-processing systems such as Max/MSP and PureData. After an introduction to the basics of SDIF (section 2), and a general overview of common different types of audio descriptors (section 3), we will propose several format con- ventions for storing general descriptor information (section 4). The standard\u2019s type declarations are completely exten- sible and modi\ufb01able by the user (section 5), which makes the current proposal adaptable to special application needs. Finally, in section 7, we will present a selection of helpful tools distributed with the SDIF library, programming inter- faces for reading and writing SDIF \ufb01les, a number of appli- cations and online resources. 2 OVERVIEW OF THE SDIF SPECIFICATION SDIF is a binary meta-format based on streams, frames and matrices. The matrix content representation can be chosen to be text, integer, or \ufb02oating point in different bit-widths. Several matrices are grouped into a frame, which has a pre- cise 64 bit \ufb02oating point time tag in seconds relative to the start of the \ufb01le. Frames are always stored in increasing tem- poral order and are assigned to a stream. Streams separate different entities of the same type, such as different sound \ufb01les, channels, or different analyses or descriptors, thus al- lowing to unite a database of several source sound \ufb01les, plus various stages of analysis and descriptor data, in one single SDIF \ufb01le. 2 For example, one could choose to store two channels of audio, their STFT representation, fundamental frequency and sinusoidal partial representation, markers, la- bels, and harmonicity audio descriptors in one \ufb01le. 2 This was formerly achieved by separate \ufb01les with different \ufb01le exten- sions, grouped in the same directory, with the risk of loss of single \ufb01les, and a certain implicitness of their relationships. 427 ISMIR 2008 \u2013 Session 4a \u2013 Data Exchange, Archiving and Evaluation Frame and matrix types are identi\ufb01ed by 4-byte ASCII signatures, inspired by the IFF format. Each frame starts with a header containing the frame signature, the number of matrices it contains, an integer identifying the stream it belongs to, and the time tag. Each matrix\u2019 header contains its signature, its content data type, and its number of rows and columns. Matrices are padded to 8 byte boundaries to achieve 64 bit word alignment for fast access using memory mapped \ufb01les. Special header-frames can store global textual meta-data in so-called Name\u2013Value Tables (NVTs), together with re- lationships of different streams (see section 6). However, global information that is needed to interpret the data, for instance the sampling rate, is stored in information matri- ces, making SDIF \ufb01les streamable. There are a number of standard description types that specify the columns of matrices, and which matrices can occur in which frame type. Examples include 1FQ0 to store fundamental frequencies and 1TRC to store partial tracks resulting from sinusoidal analysis. By convention, the sig- natures of standard frame and matrix signatures start with a version number, mostly 1. It is always possible to declare new experimental description types, or to extend existing types by new matrices for a frame, or new columns for a ma- trix. New experimental types have signatures starting with X. SDIF is designed such that programs can always ignore additional data they don\u2019t know how to interpret. 3 OVERVIEW OF AUDIO DESCRIPTORS Audio descriptors (also named audio features or audio at- tributes) are numerical values describing the contents of an audio signal according to various points of view, e.g. temporal, spectral or perceptual characteristics. They are used nowadays in many areas such as for the development of query-by-example (search-by-similarity), automatic in- dexing, segmentation or identi\ufb01cation applications. These features are extracted from the audio signal using signal processing algorithms. Several taxonomies can be used to classify them, for example according to the type of con- tent they can be applied to (single musical notes, percus- sion, sound effects, generic audio, etc.), the underlying sig- nal model they rely on (source-\ufb01lter, sinusoidal harmonic models, etc.), their abstractness, i.e. what the audio feature represents (spectral envelope, temporal envelope, etc.), their steadiness or variability, i.e., the fact that they represent a value extracted from the signal at a given time, or a param- eter from a model of the signal behavior along time (mean, standard deviation, derivative or Markov model of a param- eter), or the time extent of the description provided by them: some descriptors apply to only part of the object (e.g., de- scription of the attack of the sound) whereas others apply to the whole signal (e.g., loudness). On the computer side, an audio feature is represented by a numerical value or a vector of numerical values when several audio features can be linked together (such as the coef\ufb01cients of the MFCC, or various de\ufb01nitions of the spectral centroid). Each of such numerical values describes a speci\ufb01c prop- erty of the signal around a speci\ufb01c time (the time of the segment corresponding to the analysis window). Most au- dio features are \ufb01rst computed on a small-time scale (often called short-term analysis) which for most of them corre- spond to the length of the analysis window used for FFT analysis (usually 40 ms to 80 ms). Other features are com- puted on a longer-time scale due to their semantics (the log- attack-time, the roughness or the \ufb02uctuation strength are as- sociated to a whole musical note) or due to computational requirements (the estimation of a vibrato or tremolo at a fre- quency of 6 Hz necessitates a window larger than 80 ms). In this case the numerical value needs to be associated ex- actly to the segment used to compute it. Finally, it has be- come frequent to model the behavior of an audio feature over time. Using long-term analysis (often with window lengths of 500 ms or larger), it is common to compute the mean, variance or derivative values of the feature over the sliding window. This type of description is often named temporal modeling, and the corresponding large frames are sometimes called texture windows or macroframes. In this case, the segment over which the temporal model is com- puted does not need to have a speci\ufb01c semantic (e.g., \u201cevery 200 ms\u201d has no meaning by itself), although it can have (for example for beat-synchronous analysis). In the proposed SDIF extensions, we address these spe- ci\ufb01c requirements concerning de\ufb01nition of different time horizons, multidimensionality and addition of semantic in- formation. It is out of the scope of this article to present detailed mathematical de\ufb01nitions of the descriptors. A large set of them can be found in [4]. 4 EXTENDED TYPES FOR GENERALIZED AUDIO DESCRIPTORS A de\ufb01ning aspect of SDIF is its \ufb02exibility regarding data types. Using the data declaration syntax introduced later in section 5, the user is able to de\ufb01ne or rede\ufb01ne the frame/matrix grouping rules and to add or modify type dec- larations at will. Throughout the paper, we will present dif- ferent possibilities for different applications in the context of descriptor extraction. At the same time, however, we are proposing a ready-to-use set of type declarations designed according to what we consider appropriate to most audio content analysis and retrieval applications. The proposed type declarations, together with related documentation and several example SDIF \ufb01les can be accessed online 3 . In both short-time and temporal modeling cases, the val- ues of a given descriptor at a given time are stored in a sepa- rate matrix. For example, spectral centroid values are stored in matrices of signature 1SCN and for zero-crossing values the matrix signature is 1ZCR. In analogy with the standard matrix types (e.g., with sinusoidal modeling), the number of rows of the matrix must correspond to the dimensionality of the descriptor, i.e., with the number of frequency bands (such as with spectral \ufb02atness) or with the number of coef- \ufb01cients (such as with MFCCs or autocorrelation features). 3 http://sdif.sourceforge.net/descriptor-types 428 ISMIR 2008 \u2013 Session 4a \u2013 Data Exchange, Archiving and Evaluation The columns are intended to store the values for different de\ufb01nitions or implementations of the same descriptor. For example, the \ufb01rst column of the spectral centroid could con- tain the values for a centroid de\ufb01ned on linear frequency and amplitude scales, and the second the values using logarith- mic scalings. We will thus use dimensions to denote rows and variations to denote columns. Both the number of rows and of columns are not \ufb01xed beforehand by the type declaration. In most cases, the num- ber of rows will depend on a user-de\ufb01ned feature extraction parameter, such as number of frequency bands or cepstral coef\ufb01cients. It is also most likely that the user will just store a single variation for each descriptor. In some training ex- periments, however, it can be interesting to store different variations at the same time and let an automatic feature se- lection algorithm select the most informative one. Even if not strictly necessary for a clustering or statisti- cal training algorithm, the feature extraction program should store additional information alongside the descriptor val- ues, such as bandwidths, thresholds or labels describing the contents of the columns. This data will be included in in- formation matrices, whose signatures will be equal to the descriptor matrix signatures, with the \ufb01rst character 1 re- placed by I. Entries on information matrices are always stored column-wise. The next subsections will detail how to store such matrices. To accommodate the different descriptor time-spans mentioned above (short-, mid-, long-term and temporal modeling) we propose two different schemes for SDIF stor- age. In fact, we simplify such categorization by only con- sidering, on the one hand, the descriptors themselves (which are directly based on the signal) and, on the other hand, temporal modelings (which are based on descriptors), which will respectively be addressed in the next two subsections. We deliberately avoid a categorical distinction between the length of the time spans, since the only difference from the SDIF point of view is the de\ufb01nition of the time bound- aries. Also, we will avoid the terminology \u201clow-level\u201d or \u201chigh-level\u201d feature because of the lack of consensus about its meaning. By convention, an SDIF \ufb01le containing only audio descriptors should end with the double extension .descr.sdif.",
        "zenodo_id": 1415762,
        "dblp_key": "conf/ismir/BurredCPRS08"
    },
    {
        "title": "Detection of Pitched/Unpitched Sound using Pitch Strength Clustering.",
        "author": [
            "Arturo Camacho"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1418169",
        "url": "https://doi.org/10.5281/zenodo.1418169",
        "ee": "https://zenodo.org/records/1418169/files/Camacho08.pdf",
        "abstract": "A method for detecting pitched/unpitched sound is presented. The method tracks the pitch strength trace of the signal, determining clusters of pitch and unpitched sound. The criterion used to determine the clusters is the local maximization of the distance between the centroids. The method makes no assumption about the data except that the pitched and unpitched clusters have different centroids. This allows the method to dispense with free parameters. The method is shown to be more reliable than using fixed thresholds when the SNR is unknown.",
        "zenodo_id": 1418169,
        "dblp_key": "conf/ismir/Camacho08"
    },
    {
        "title": "Extending Content-Based Recommendation: The Case of Indian Classical Music.",
        "author": [
            "Parag Chordia",
            "Mark Godfrey",
            "Alex Rae"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415132",
        "url": "https://doi.org/10.5281/zenodo.1415132",
        "ee": "https://zenodo.org/records/1415132/files/ChordiaGR08.pdf",
        "abstract": "We describe a series of experiments that attempt to create a content-based similarity model suitable for making rec- ommendations about North Indian classical music (NICM). We introduce a dataset (nicm2008) consisting of 897 tracks of NICM along with substantial ground-truth annotations, including artist, predominant instrument, tonic pitch, raag, and parent scale (thaat). Using a timbre-based similarity model derived from short-time MFCCs we \ufb01nd that artist R-precision is 32.69% and that the predominant instrument is correctly classi\ufb01ed 90.30% of the time. Consistent with previous work, we \ufb01nd that certain tracks (\u201chubs\u201d) appear falsely similar to many other tracks. We \ufb01nd that this prob- lem can be attenuated by model homogenization. We also introduce the use of pitch-class distribution (PCD) features to measure melodic similarity. Its effectiveness is evaluated by raag R-precision (16.97%), thaat classi\ufb01cation accuracy (75.83%), and comparison to reference similarity metrics. We propose that a hybrid timbral-melodic similarity model may be effective for Indian classical music recommenda- tion. Further, this work suggests that \u201chubs\u201d are a general features of such similarity modeling that may be partially alleviated by model homogenization 1 INTRODUCTION AND MOTIVATION North Indian classical music (NICM) has for the past forty years become an increasingly international phenomenon. A great number of people have had some exposure but other- wise know little about the tradition. This presents an oppor- tunity for a music discovery system that can suggest music based either on known artists or on simple descriptive terms. However, metadata for Indian classical music is often miss- ing or inaccurate, and user-tagging of Indian classical mu- sic tracks is uncommon. These problems suggest the use of a content-based recommender. In addition to the goal of exploring models that can be used for content-based rec- ommendation, we hope to explore whether issues observed with the standard timbre-based content-based recommenda- tion (CBR) models, such as the prevalence of many false hits due to a few tracks (\u201chubs\u201d), are artifacts of the data or appear more generally when novel music is considered. To date, published work on CBR [1, 2, 12] has focused on a few datasets that consist solely of a small slice of Western pop- ular music. Finally we hope to show how properties of the musical genre can be exploited to improve CBR. Because NICM is signi\ufb01cantly less polyphonic than most Western music, it relatively easy for us to extract melodic informa- tion which can be used in a similarity model. 2 BACKGROUND NICM is one of the oldest continuous musical traditions in the world and it is an active contemporary performance practice. Since the 1960\u2019s, due to the emigration of Indians and the popularity of artists such as Ravi Shankar and Zakir Hussain, it has become widely known to international audi- ences. The repertoire of Indian classical music is extensive, consisting of dozens of styles and hundreds of signi\ufb01cant performers. The most prevalent instruments, such as sitar, sarod and tabla, are timbrally quite different from popular Western instruments. NICM is an oral tradition and record- ings therefore represent the primary materials. The performance of Indian classical music typically in- volves a soloist, either a vocalist or instrumentalist, accom- panied by a tanpura (drone) throughout and a tabla (percus- sion) in rhythmic sections. Most presentations begin with an extended ametric melodic improvisation (alap) and build in intensity throughout the performance. After this section, several compositions are usually presented with tabla ac- companiment. Here too, the majority of the music is impro- vised. All NICM is based on raag, a melodic concept that de\ufb01nes the melodic parameters of a given piece. There are hundreds of raags, of which approximately 150 are widely performed. Raags are typically de\ufb01ned by a scale and a set of interrelated phrases. Although highly structured, they al- low the performer tremendous scope for improvisation and elaboration. It is this development that forms the core of most NICM performances. Another important, though quite distinct, performance tradition is solo tabla, in which the tabla player becomes the soloist. Here timbral and rhythmic patterns form the core material, and the melodic accompaniment is used pri- marily as a time-keeper. 571 ISMIR 2008 \u2013 Session 5a \u2013 Content-Based Retrieval, Categorization and Similarity 2 3 DATABASE The database was assembled from the author\u2019s personal mu- sic collection. Recordings encompass both commercial and non-commercial sources. Many of the non-commercial record- ings are live concerts that are distributed informally amongst NICM listeners. A substantial number of the most histori- cally important recordings are of this type. The recordings span a range from the early 20th century to present with the vast majority of recordings being from the second half of the century. Low \ufb01delity is common due to the quality of the initial recording or the number of intermediate ana- log copies. Common degradations include hiss and crackle, overloading, missing high and/or low frequency content, ar- tifacts from excessive noise reduction processing, and wow due to tape speed \ufb02uctuation. The balance of the accompa- nying drone and tabla varies widely, in some cases barely audible, in other cases overwhelming. The database consist of 897 tracks. Only the \ufb01rst \ufb01ve minutes of each track was used, leading to a total size of ap- proximately seventy hours. This was done to reduce compu- tation time since many of the tracks were over thirty minutes long. A total of 141 artists are contained in the database as well as 14 different instruments. The instruments include sitar, sarod, tabla, shenai, \ufb02ute, violin, and pakhawaj, with the \ufb01rst three being the most common. There are 171 differ- ent raags. The distribution of tracks amongst the raags was uneven and 71 raags are represented by only one recording in the database. All the features used in this study, including MFCCs and pitch-tracks, along with ground-truth annotation will be made available at paragchordia.com/data/nicm08/.",
        "zenodo_id": 1415132,
        "dblp_key": "conf/ismir/ChordiaGR08"
    },
    {
        "title": "Evaluating and Visualizing Effectiveness of Style Emulation in Musical Accompaniment.",
        "author": [
            "Ching-Hua Chuan",
            "Elaine Chew"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417095",
        "url": "https://doi.org/10.5281/zenodo.1417095",
        "ee": "https://zenodo.org/records/1417095/files/ChuanC08.pdf",
        "abstract": "We propose general quantitative methods for evaluating and visualizing the results of machine-generated style-speci\ufb01c accompaniment. The evaluation of automated accompani- ment systems, and the degree to which they emulate a style, has been based primarily on subjective opinion. To quan- tify style similarity between machine-generated and orig- inal accompaniments, we propose two types of measures: one based on transformations in the neo-Riemannian chord space, and another based on the distribution of melody-chord intervals. The \ufb01rst set of experiments demonstrate the meth- ods on an automatic style-speci\ufb01c accompaniment (ASSA) system. They test the effect of training data choice on style emulation effectiveness, and challenge the assumption that more data is better. The second set of experiments compare the output of the ASSA system with those of a rule-based system, and random chord generator. While the examples focus primarily on machine emulation of Pop/Rock accom- paniment, the methods generalize to music of other genres. 1 INTRODUCTION Automatic generation of music in a speci\ufb01c style has been a focal topic in computational music research since the early days of computing. Many researchers have designed sys- tems that emulate music styles in compositions. The evalu- ation of the musical output, the degree to which it achieves its goal of emulating a particular style remains a challenge. Evaluation is often in the form of subjective opinion. It is our goal to \ufb01ll this gap in quantitative evaluation of style emulation in automatic accompaniment systems. In [1], Meyer states that \u201cstyle is a replication of pattern- ing, ..., that results from a series of choices made within some set of constraints.\u201d Accompaniment can be considered the outcome of a series of choices over possible chords un- der certain contextual constraints, such as melody, phrase, and key. For example, in four-part harmonization, the com- position must follow the counterpointand voice-leading rules in music theory. In contrast, modeling of the accompani- ment style in Pop/Rock tends to be vague and dif\ufb01cult be- cause, according to Stephenson [2], \u201cin rock, ..., melody is allowed to interact with harmony more freely,\u201d and the pro- cess becomes more complex because, as Everett states [3], \u201cmany different tonal systems are now practiced by the same artist, on the same album.\u201d In this paper we propose methods to visualize, measure, and quantify the quality of an accompaniment generated with the goal of emulating an original accompaniment, the \u201cground truth\u201d. These measures are designed to evaluate automatic style-speci\ufb01c accompaniment (ASSA) systems. We exam- ine the style as captured by the musical relations between melody and harmony, and between adjacent chords. We de- velop six metrics based on these two types of measures. Using these quantitative methods, we design experiments to explore training set selection strategies that further the ASSA system\u2019s style emulation goals. For machine learning tasks, it is often the case that more training data guarantee better results. For ASSA goals, more training songs may not necessarily improve the output quality if the training set is not consistent with the desired style. The \ufb01rst set of experi- ment explores the factors impacting style emulation success. A second set of experiments compare the degree of style- speci\ufb01city between the best ASSA systems, a rule-based harmonization system, and a random chord generator. We conduct the experiments on \ufb01ve well known Pop/Rock al- bums by Green Day, Keane, and Radiohead, providing de- tailed statistics and case studies for the resulting accompani- ments. The \ufb01ndings we report generalize to genres outside of Rock music, and to some extent to simulation of style in music in general. The paper is organized as follows: Section 2 describes related automatic accompaniment systems, with and with- out style requirements, and their evaluations. It concludes with a brief description of an ASSA system, which forms the basis of much of our evaluations. We present the eval- uation methods in Section 3. Intra-system comparisons are shown in Sections 4, and inter-system results in 5, followed by the conclusions. 57 ISMIR 2008 \u2013 Session 1a \u2013 Harmony 2 RELATED WORK Baroque style four-part harmonization has been a popular accompaniment generation application since the earliest days of computing. Recent examples include [4, 5]. Many rules govern these Chorale-type harmonizations, which can be ap- plied in their synthesis and evaluation. Sixteenth century compositions in the style of Palestrina have also be emulated using Markov models [6]. Such compositions are similarly and strictly circumscribed by a host of rules, which can be used in the evaluation of their correctness [7]. Temperley and Sleator proposed a set of preference rules to harmoniz- ing melodies in the Western classical style [8]; these rules are implemented in their Harmonic Analyzer [9]. In the popular realm, the i-Ring system [10] generates an accompaniment for any eight-measure melody. The ac- companiment is based on state transition probabilities cal- culated from a training set of 150 songs. For evaluation, 10 participants were asked to rate the accompaniment as Good, Medium, or Bad. More recently, MySong [11] uses Hidden Markov Mod- els, training on 298 popular songs in genres including Pop, Rock, R&B, Jazz, and Country Music. Users can choose between two style-related modes: \u2018Jazz,\u2019 which uses less common triads, or \u2018Happy,\u2019 which selects more major-mode chord transitions. 26 sample accompaniments by MySong were evaluated subjectively by 30 volunteer musicians. While the output of the above systems \ufb01t the melody, be- cause the systems learn from general training sets, the style captured by the output lacks speci\ufb01city. Thus, these sys- tems do not address the emulation of speci\ufb01c accompani- ment styles, as embodied in a distinctive song or a partic- ular band\u2019s output. Evaluations take the form of subjective opinion, and lack an objective or quantitative component. In [12], Chuan & Chew proposed an ASSA system that can generate style-speci\ufb01c accompaniment to a melody given only a few examples. The system consists of a chord tone determination and a chord progression module. The chord tone determination module applies machine learning to de- termine which notes in a melody are likely chord tones based on the speci\ufb01ed style. The system then constructs possi- ble chord progressions using neo-Riemannian transforms, representing them in a tree structure, and selects the high- est probability path based on the learned probabilities in a Markov chain. This system was rated informally by subjec- tively judgement via a Turing test. 3 QUANTIFYING ACCOMPANIMENT DISTANCE This section \ufb01rst presents ways to quantify and visualize dis- tance between two chords, and metrics to evaluate distance between two different accompaniments to the same melody. The examples are generated by the ASSA system of [12].",
        "zenodo_id": 1417095,
        "dblp_key": "conf/ismir/ChuanC08"
    },
    {
        "title": "Connecting the Dots: Music Metadata Generation, Schemas and Applications.",
        "author": [
            "Nik Corthaut",
            "Sten Govaerts",
            "Katrien Verbert",
            "Erik Duval"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415244",
        "url": "https://doi.org/10.5281/zenodo.1415244",
        "ee": "https://zenodo.org/records/1415244/files/CorthautGVD08.pdf",
        "abstract": "With the ever-increasing amount of digitized music becoming available, metadata is a key driver for different music related application domains. A service that combines different metadata sources should be aware of the existence of different schemas to store and exchange music metadata. The user of a metadata provider could benefit from knowledge about the metadata needs for different music application domains. In this paper, we present how we can compare the expressiveness and richness of a metadata schema for an application. To cope with different levels of granularity in metadata fields we defined clusters of semantically related metadata fields. Similarly, application domains were defined to tackle the fine-grained functionality space in music applications. Next is shown to what extent music application domains and metadata schemas make use of the metadata field clusters. Finally, we link the metadata schemas with the application domains. A decision table is presented that assists the user of a metadata provider in choosing the right metadata schema for his application.",
        "zenodo_id": 1415244,
        "dblp_key": "conf/ismir/CorthautGVD08"
    },
    {
        "title": "Development of a Music Organizer for Children.",
        "author": [
            "Sally Jo Cunningham",
            "Edmond Zhang"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1418329",
        "url": "https://doi.org/10.5281/zenodo.1418329",
        "ee": "https://zenodo.org/records/1418329/files/CunninghamZ08.pdf",
        "abstract": "Software development for children is challenging; children have their own needs, which often are not met by \u2018grown up\u2019 software. We focus on software for playing songs and managing a music collection\u2014tasks that children take great interest in, but for which they have few or inappropriate tools. We address this situation with the design of a new music management system, created with children as design partners: the Kids Music Box.",
        "zenodo_id": 1418329,
        "dblp_key": "conf/ismir/CunninghamZ08"
    },
    {
        "title": "Perceptually-Based Evaluation of the Errors Usually Made When Automatically Transcribing Music.",
        "author": [
            "Adrien Daniel",
            "Valentin Emiya",
            "Bertrand David 0002"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417155",
        "url": "https://doi.org/10.5281/zenodo.1417155",
        "ee": "https://zenodo.org/records/1417155/files/DanielED08.pdf",
        "abstract": "This paper investigates the perceptual importance of typi- cal errors occurring when transcribing polyphonic music ex- cerpts into a symbolic form. The case of the automatic tran- scription of piano music is taken as the target application and two subjective tests are designed. The main test aims at understanding how human subjects rank typical transcrip- tion errors such as note insertion, deletion or replacement, note doubling, incorrect note onset or duration, and so forth. The Bradley-Terry-Luce (BTL) analysis framework is used and the results show that pitch errors are more clearly per- ceived than incorrect loudness estimations or temporal devi- ations from the original recording. A second test presents a \ufb01rst attempt to include this information in more perceptually motivated measures for evaluating transcription systems. 1 INTRODUCTION In the benchmarking of Information Retrieval systems, per- formance is often evaluated by counting and classifying er- rors. Classically the ratio of relevant items that are re- turned out of the full set of original ones, referred to as re- call, measures the completeness of the system performance whereas the proportion of relevant items that are retrieved, or precision, indicates the correctness of the answer. The F-measure, combining precision and recall, offers a single score to assess the performance. When music processing systems are involved, the question arises as to how to com- plement such a quantitative assessment by incorporating a certain amount of perceptually motivated criteria or weights. This paper investigates the perceptual importance of typi- cal errors occurring when transcribing polyphonic music ex- cerpts into a symbolic form, e.g. converting a piece recorded in a PCM (.wav) format into a MIDI \ufb01le. This particular The authors thank all the subjects involved in the perceptive test for their participation. They also thank M. Castellengo, A. de Cheveign\u00e9, D. Pressnitzer and J. Benenson for their useful remarks, and M. Marolt, N. Bertin and P. Leveau for sharing their programs. The research leading to this paper was supported by Institut TELECOM under the Automatic Tran- scription of Music: Advanced Processing and Implementation - TAMTAM project and by the French GIP ANR under contract ANR-06-JCJC-0027- 01, D\u00e9composition en \u00c9l\u00e9ments Sonores et Applications Musicales - DE- SAM. Music Information Retrieval (MIR) task and its related sub- tasks (onset detection, multipitch estimation and tracking) have received a lot of attention [9] from the MIR commu- nity since the early works of Moorer [14] in the mid 70s. The approaches used to accomplish the goal are very di- verse [4, 5, 14, 15, 16] and the evaluation of the performance for such systems is almost as varied. Some papers [4, 14] fo- cus on a couple of sound examples, to probe typical errors such as octave errors, or deviations from ground truth such as duration differences, and so forth. However, the most widely used criteria for assessing automatic transcription are quantitative, even if the evaluation framework is not always similar (frame-based [15], note-based [16] or both [1]). In the practical context of piano music for instance, the evaluation task is often handled by generating the PCM for- mat piece from an original MIDI \ufb01le which makes it pos- sible to compare the input (ground truth) and output MIDI \ufb01les. For that particular case, in this study, a perception test has been designed for subjectively rating a list of typi- cal transcription errors (note insertions, deletions, incorrect onsets or duration...). The test is based on pairwise compar- isons of sounds holding such targeted errors. The results are then analyzed by means of the Bradley-Terry-Luce (BTL) method [3]. In a second step, the question emerged of \ufb01nding a way to take into account the perceptual ranking of the discomfort levels we obtained. Another test was designed to subjec- tively compare transcriptions resulting from different sys- tems. It aimed at deriving more perceptually relevant met- rics from the preceding BTL results by synthetically com- bining their main \ufb01ndings, and at checking their compliance with the test results. We worked in two directions: percep- tually weighting typical errors, countable by comparing the input and output MIDI \ufb01les, and adaptating similarity met- rics [17]. 2 THE EVALUATION MEASURES The commonly-used F-measure is de\ufb01ned by: f \u225c2 rp r + p = #TP #TP + 1 2#FN + 1 2#FP (1) 550 ISMIR 2008 \u2013 Session 4c \u2013 Automatic Music Analysis and Transcription where r denotes the recall, p the precision, #TP the number of true positives (TP), #FN the number of false negatives (FN) and #FP the number of false positives (FP). f is equiv- alent to the quantity a, that is referred to as either accuracy or score [5], since f = 2 1 a +1. The F-measure is useful to ob- tain the error rate for individually counted errors, but does not consider aspects like sequentiality, chords, harmonic or tonal relationships, etc. Another evaluation approach comes from the problem of \ufb01nding the similarity between two (musical) sequences. At the moment, these methods are commonly used to search for similar melodies in large databases, rather than in the \ufb01eld of the evaluation of transcriptions. Let us assume that one must compare two sequences of symbols, A and B. The Levenshtein\u2019s distance, or edit dis- tance [11], is a metric that counts the minimal number of operations necessary to transform A to B. The possible op- erations on symbols are: deletion from A, insertion into B, or replacement of a symbol in A by another one in B. Mongeau and Sankoff [13] proposed adapting this dis- tance to the case of monophonic musical sequences, in or- der to de\ufb01ne a similarity metric between two melodies. The two sequences of notes are ordered according to the onset of each note. Each note is characterized by its pitch and du- ration, which are used to compute the cost of the following possible operations: insertion, deletion, replacement, with costs depending on tonal criteria, fragmentation and consol- idation of several notes with the same pitch. These oper- ations re\ufb02ect typical mistakes in transcriptions. The min- imum distance between the sets of notes is then estimated using the edit distance framework. This melodic edit distance being applicable only to mo- nophonic sequences, an extension to the polyphonic case has been recently proposed [8]. In order to represent the polyphonic nature of musical pieces, quotiented sequences are used. So far, this representation has only been applied to chord sequences, which constitute a restricted class of musical pieces: the notes within a chord must have the same onset and duration. Another way to compute the similarity between two mu- sical sequences [17] consists in considering each set of notes as points in a multidimensional space, e.g. the pitch/time domain. The algorithm is based on two choices. First, each point must be assigned a weight, e.g. the note duration. Sec- ond, a distance between a point in the \ufb01rst set and a point in the second one is de\ufb01ned, e.g. the euclidian distance in the time/pitch space. Then, the overall distance can be computed with the Earth Movers Distance (EMD) or the Proportional Transportation Distance (PTD). It is related to the minimum amount of work necessary to transform one set of weighted points to the other using the previously- de\ufb01ned distance, making it possible to transfer the weight of a source note towards several targets. In all of these methods, the setting of the parameters is a crucial point. Indeed, the weighting between the time and the pitch dimensions, for instance, depends on music per- ception. The tests presented in this paper aim at assessing the trends of the perceptive impact of typical errors and the distribution of their related weights. 3 EXPERIMENTAL SETUP",
        "zenodo_id": 1417155,
        "dblp_key": "conf/ismir/DanielED08"
    },
    {
        "title": "High-Level Audio Features: Distributed Extraction and Similarity Search.",
        "author": [
            "Fran\u00e7ois Deli\u00e8ge",
            "Bee Yong Chua",
            "Torben Bach Pedersen"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416514",
        "url": "https://doi.org/10.5281/zenodo.1416514",
        "ee": "https://zenodo.org/records/1416514/files/DeliegeCP08.pdf",
        "abstract": "Today, automatic extraction of high-level audio features suf- fers from two main scalability issues. First, the extraction algorithms are very demanding in terms of memory and com- putation resources. Second, copyright laws prevent the au- dio \ufb01les to be shared among computers, limiting the use of existing distributed computation frameworks and reducing the transparency of the methods evaluation process. The iSound Music Warehouse (iSoundMW), presented in this paper, is a framework to collect and query high-level au- dio features. It performs the feature extraction in a two-step process that allows distributed computations while respect- ing copyright laws. Using public computers, the extraction can be performed on large scale music collections. How- ever, to be truly valuable, data management tools to search among the extracted features are needed. The iSoundMW enables similarity search among the collected high-level fea- tures and demonstrates its \ufb02exibility and ef\ufb01ciency by us- ing a weighted combination of high-level features and con- straints while showing good search performance results. 1 INTRODUCTION Due to the proliferation of music on the Internet, many web portals proposing music recommendations have appeared. As of today, the recommendations they offer remain very limited: manual tagging has proved to be time consuming and often results in incompleteness, inaccuracy and incon- sistency; automatic tagging systems based on web scanning or relying on millions of users are troubled, e.g., by mind- less tag copying practices, thus blowing bag tags. Automatic extraction of music information is a very active topic ad- dressed by the Music Information Retrieval (MIR) research community. Each year, the Music Information Retrieval Evaluation eXchange (MIREX) gives to researchers an op- portunity to evaluate and compare new music extraction meth- ods [9]. However, the MIREX evaluation process has proved to be resource consuming and slow despites attempts to ad- dress these scalability issues [4, 11]. So far, concerns with copyright issues have refrained the community to distribute the extraction among public computers as most algorithms require the audio material to be available in order to per- form the feature extraction 1 . The features are therefore ex- tracted from a relatively small music collection that narrows their generality and usefulness. Additionally, the feature ex- traction, being run by private computers on a private music collection, limits the transparency of the evaluation process. These limitations call for the development of a system able to extract meaningful, high-level audio features over large music collections. Such a system faces data management challenges. Noteworthily, the impressive amount of infor- mation generated requires an adapted search infrastructure to become truly valuable. Our intention is to create a system able to cater for dif- ferent types of features. Present literature mainly focuses on features that have either absolute or relative values, thus mo- tivating the handling of both kinds of features. In this paper, the exact selection of the features is actually not as impor- tant as it is to demonstrate how extraction can be handled on public computers and enabling researchers to compare results obtained by using different algorithms and features. The contributions of this paper are two-fold. First, we propose a framework for collecting high-level audio features (that were recently proposed by [5, 6, 7, 13]) over a large music collection of 41,446 songs. This is done by outsourc- ing the data extraction to remote client in a two-step feature extraction process: (1) dividing the audio information into short term segments of equal length and distributing them to various clients; and (2) sending the segment-based fea- tures gathered during step one to various clients to compute high-level features for the whole piece of music. Second, we propose a \ufb02exible and ef\ufb01cient similarity search approach, which uses a weighted combination of high-level features, to enable high-level queries (such as \ufb01nding songs with a similar happy mood, or \ufb01nding songs with a similar fast tempo). Additionally, to support the practical bene\ufb01ts of these contributions, we propose a short scenario illustrating the feature extraction and search abilities of the iSoundMW. For the general public, the iSoundMW music offers rec- ommendation without suffering from a \u201ccold start\u201d, i.e., new artists avoid the penalties of not being well known, and new listeners obtain good music recommendation before being pro\ufb01led. For researchers, the iSoundMW (1) offers \ufb02exi- 1 https://mail.lis.uiuc.edu/pipermail/evalfest/2008-May/000765.html 565 ISMIR 2008 \u2013 Session 5a \u2013 Content-Based Retrieval, Categorization and Similarity 2 ble search abilities; (2) enables visual comparison of both segment-based and aggregated high-level features; (3) pro- vides a framework for large scale computations of features; and (4) gives good search performances. The remainder of the paper is organized as follows. Re- lated work is presented in Section 2. Section 3 offers an overview of the system, explains the process of collecting the high-level audio features, describes how similarity search with weighted coef\ufb01cients is performed, and how the search can be further optimized by using range searches. Section 4 illustrates the similarity search on a concrete example. Sec- tion 5 concludes and presents future system improvements and research directions. 2 RELATED WORK Research on distributed computing has received a lot of at- tention in diverse research communities. The Berkeley Open Infrastructure for Network Computing framework (BOINC) is a well-known middleware system in which the general public volunteers processing and storage resources to com- puting projects [1, 2] such as SETI@home [3]. However, in its current state, BOINC does not address copyright issues, does not feature \ufb02exible similarity search, and does not en- able multiple steps processes, i.e., acquired results serve as input for other tasks. Closer to the MIR community, the On- demand Metadata Extraction Network system (OMEN) [15] distributes the feature extraction among trusted nodes rather than public computers. Furthermore, OMEN does not store the computed results, and, like BOINC, does not allow sim- ilarity search to be performed on the extracted features. Audio similarity search is often supported by creating indexes [10]. Existing indexing techniques can be applied to index high dimensional musical feature representations. However, as a consequence of the subjective nature of mu- sical perception, the triangular inequality property of the metric space is typically not preserved for similarity mea- sures [14, 16]. Work on indexes for non-metric space is presented in the literature [12, 17]. Although the similarity function is non-metric, it remains con\ufb01ned in a pair of lower and upper bounds speci\ufb01cally constructed. Therefore, us- ing these indexes would impose restrictions on the similarity values that would limit the \ufb02exibility of the iSoundMW. 3 SYSTEM DESCRIPTION In this section, we present an overview of the system fol- lowed by a more detailed description of a two-step process for extracting high-level features. Later, we describe how \ufb02exible similarity searches are performed and how they are further optimized using user-speci\ufb01ed constraints. \u0013\u000b\u0007\u0004\u0011\u0002\u0007\b\u000e\u0017\u0011\u0014\u0002 \u0012\u000f\u0005\u0004\u000f\b :\f\r\u0002\u0013\u000e\u000f\u000e\u0005\b\u0017 \u0007\f\u000f\u0005\u0002\u0002 \u0002\u000f\u001b\f\u0017\u001c\u000e\u0006\u0004\f\u000f \u0007\b\u0005\u001c\b\u000f\u0006\u0002 \u0002\u000f\u001b\f\u0017\u001c\u000e\u0006\u0004\f\u000f",
        "zenodo_id": 1416514,
        "dblp_key": "conf/ismir/DeliegeCP08"
    },
    {
        "title": "Online Activities for Music Information and Acoustics Education and Psychoacoustic Data Collection.",
        "author": [
            "Travis M. Doll",
            "Raymond Migneco",
            "Youngmoo E. Kim"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1418341",
        "url": "https://doi.org/10.5281/zenodo.1418341",
        "ee": "https://zenodo.org/records/1418341/files/DollMK08.pdf",
        "abstract": "Online collaborative activities provide a powerful plat- form for the collection of psychoacoustic data on the per- ception of audio and music from a very large numbers of subjects. Furthermore, these activities can be designed to simultaneously educate users about aspects of music infor- mation and acoustics, particularly for younger students in grades K-12. We have created prototype interactive activ- ities illustrating aspects of two different sound and acous- tics concepts: musical instrument timbre and the cocktail party problem (sound source isolation within mixtures) that also provide a method of collecting perceptual data related to these problems with a range of parameter variation that is dif\ufb01cult to achieve for large subject populations using tra- ditional psychoacoustic evaluation. We present preliminary data from a pilot study where middle school students were engaged with the two activities to demonstrate the potential bene\ufb01ts as an education and data collection platform. 1 INTRODUCTION Recently, a number of web-based games have been devel- oped for the purpose of large-scale data labeling [1]. Simi- lar activities can be used to collect psychoacoustic data from a large number of users, which is dif\ufb01cult to obtain using traditional evaluations. We present two such activities that explore the perception of audio (instrument timbre and the cocktail party problem), with the additional aim of educat- ing users, particularly K-12 students, about aspects of music and acoustics. These web-based interfaces are designed as game activities with minimal complexity so they can be eas- ily used by students without previous training. To maintain widespread accessibility, the activities require only internet access through a web browser and run independently of ex- ternal applications. We believe this platform will enable us to collect a very large number of samples exploring myriad parameter variations to better de\ufb01ne perceptual boundaries and quantitative models of perceived features. 2 BACKGROUND Relatively little research has been conducted on human per- formance in the identi\ufb01cation of musical instruments after timbral modi\ufb01cations. Saldanha and Corso demonstrated that the highest performance is achieved when the test tone consists of the initial transient and a short steady-state seg- ment and the lowest performance occurs using test tones consisting of only the steady-state component and the end- ing transient [2]. Iverson examined the dynamic attributes of timbre by evaluating the effects of the transient in sound similarity tests. While results of instrument identi\ufb01cation experiments depended on the presence of the initial transient in the sound, the results of similarity tests using tones with and without the initial transient suggest that the initial tran- sient is not required to imply similarity. This research sug- gests that similarity judgments may be attributed to acoustic properties of an instrument\u2019s sound other than the transient component [3]. Martin demonstrated that humans can iden- tify an instrument\u2019s family with greater accuracy than the instrument itself [4]. Other studies on instrument identi\ufb01- cation suggest that musically inclined test subjects perform better than non-musically inclined subjects and in particular, subjects with orchestra experience perform better than those without [5]. The performance of automatic speaker and speech recog- nition algorithms often use human performance as a bench- mark, but it is dif\ufb01cult to obtain a large human subject popu- lation for comparisons. Atal [6] provides a summary of hu- man speaker recognition evaluations before 1976 in which no more than 20 human listeners are employed for each evaluation. Stifelman [7] performed a speech recognition evaluation that simulated the environment of the cocktail party problem, testing the listening comprehension and tar- get monitoring ability of 3 pilot and 12 test subjects. Lipp- mann [8] provides a summary of several human vs. machine speech recognition comparisons, all of which distinctly show humans outperforming machines with a limited number of test subjects. In the area of speaker recognition, Schmidt- Nielsen [9] conducted an evaluation in which 65 human lis- teners were tested with speech data from the 1998 NIST automatic speaker recognition evaluations. The experiment was administered in the same manner as the automated sys- 445 ISMIR 2008 \u2013 Session 4a \u2013 Data Exchange, Archiving and Evaluation tems to create a direct comparison, and the results show that humans perform at the level of the best automated systems and exceed the performance of typical algorithms. 3 DEVELOPED ACTIVITIES",
        "zenodo_id": 1418341,
        "dblp_key": "conf/ismir/DollMK08"
    },
    {
        "title": "Accessing Music Collections Via Representative Cluster Prototypes in a Hierarchical Organization Scheme.",
        "author": [
            "Markus Dopler",
            "Markus Schedl",
            "Tim Pohle",
            "Peter Knees"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417305",
        "url": "https://doi.org/10.5281/zenodo.1417305",
        "ee": "https://zenodo.org/records/1417305/files/DoplerSPK08.pdf",
        "abstract": "This paper addresses the issue of automatically organizing a possibly large music collection for intuitive access. We present an approach to cluster tracks in a hierarchical man- ner and to automatically \ufb01nd representative pieces of music for each cluster on each hierarchy level. To this end, audio signal-based features are complemented with features de- rived via Web content mining in a novel way. Automatic hierarchical clustering is performed using a variant of the Self-Organizing Map, which we further modi\ufb01ed in order to create playlists containing similar tracks. The proposed approaches for playlist generation on a hier- archically structured music collection and \ufb01nding prototyp- ical tracks for each cluster are then integrated into the Trav- eller\u2019s Sound Player, a mobile audio player application that organizes music in a playlist such that the distances between consecutive tracks are minimal. We extended this player to deal with the hierarchical nature of the playlists generated by the proposed structuring approach. As for evaluation, we \ufb01rst assess the quality of the clustering method using the measure of entropy on a genre-annotated test set. Second, the goodness of the method to \ufb01nd proto- typical tracks for each cluster is investigated in a user study. 1 INTRODUCTION AND MOTIVATION Increased storage capabilities, high-speed Internet access, and novel techniques for music compression have tremen- dously increased the size of private as well as commercial music collections over the past few years. Due to the large number of audio tracks in private repositories, it is virtually impossible for users to keep track of every piece. Hence, elaborate methods for organizing large music collections be- come increasingly important. The enormous economic suc- cess of high-capacity mobile music players, such as Ap- ple\u2019s \u201ciPod\u201d, necessitates intelligent methods for automated structuring of music collections. The paper at hand presents a possible solution to this prob- lem. The approach proposed here employs an unsupervised hierarchical clustering algorithm, more precisely the Grow- ing Hierarchical Self-Organizing Map (GHSOM) [3], on audio features extracted from the music collection. Each cluster of the resulting hierarchical organization is then la- beled automatically with a typical track from the cluster to facilitate exploration. To this end, we elaborated an origi- nal approach that not only relies on audio features, but in- corporates Web-based information on the prototypicality of artists to determine a representative track for each cluster. The main motivation for this is the bad quality of straightfor- ward methods to \ufb01nd a representative for a set of data items. Indeed, choosing for example the music track whose audio feature representation is closest to the average feature vector of the cluster may give a mathematically sound prototype for the cluster. However, in preliminary experiments that em- ployed this selection approach, we encountered many rep- resentatives that were either songs by barely known artists (even though the cluster contained very well known artists) or barely known tracks by popular artists. In any case, using such tracks as representative for a cluster seems counterin- tuitive to guide the user in exploring his music collection. The main contributions of this work are the novel approach to determine cluster prototypes, based not only on the au- dio features, but also on Web mining techniques, in order to omit \u201caverage\u201d cluster representatives that are unknown to most users. In addition, we reimplemented the GHSOM al- gorithm and modi\ufb01ed it in order to generate playlists. Fur- thermore, a novel version of the Traveller\u2019s Sound Player application, which uses the cluster prototypes to facilitate navigating the collection, is proposed. The approaches to generate playlists and \ufb01nding cluster prototypes are further evaluated thoroughly. The remainder is structured as follows. Section 2 brie\ufb02y discusses related work. In Section 3, the acquisition of fea- ture data to describe artists and pieces of music is elabo- rated. Our approaches to hierarchically cluster a given mu- sic collection and creating playlists from these clusters can be found in Section 4, whereas \ufb01nding representative pro- totypes for each cluster is addressed in Section 5. The ex- periments conducted to evaluate the proposed approaches are detailed in Section 6. Finally, Section 7 gives some re- marks on the integration of the presented approaches in the Traveller\u2019s Sound Player application, and in Section 8, con- clusions are drawn. 179 ISMIR 2008 \u2013 Session 2a \u2013 Music Recommendation and Organization 2 RELATED WORK This paper was mainly motivated by the work done in [9], where Pohle et al. employ different heuristics to solve a Traveling Salesman Problem on a track-level audio simi- larity matrix. The resulting tour represents a playlist with minimized distances between consecutive pieces of music. The playlist generated for a complete music collection is then made accessible via a special music player. In [8], this approach is modi\ufb01ed in that the audio similarity matrix is adapted according to artist similarity estimations, which are derived from TF \u00d7IDF features computed on artist-related Web pages. In contrast to [9, 8], we do not model a Trav- eling Salesman Problem to generate playlists. Instead we follow a recursive approach that builds upon the GHSOM algorithm to hierarchically break down the tracks of a mu- sic collection until every track is represented by a sole map unit. The playlist is then generated by successively visiting each such unit. As we build our playlist generation approach upon a variant of the GHSOM, a hierarchical version of the Self-Organizing Map, this work is certainly also related to [3, 4], where the GHSOM is presented. 3 FEATURE EXTRACTION As we combine features extracted from the audio signal with cultural features from the Web in order to determine pro- totypical pieces of music for each cluster, we perform the following two feature extraction stages.",
        "zenodo_id": 1417305,
        "dblp_key": "conf/ismir/DoplerSPK08"
    },
    {
        "title": "A Study on Feature Selection and Classification Techniques for Automatic Genre Classification of Traditional Malay Music.",
        "author": [
            "Shyamala Doraisamy",
            "Shahram Golzari",
            "Noris Mohd. Norowi",
            "Md Nasir Sulaiman",
            "Nur Izura Udzir"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415124",
        "url": "https://doi.org/10.5281/zenodo.1415124",
        "ee": "https://zenodo.org/records/1415124/files/DoraisamyGNSU08.pdf",
        "abstract": "Machine learning techniques for automated musical genre classification is currently widely studied. With large collections of digital musical files, one approach to classification is to classify by musical genres such as pop, rock and classical in Western music.  Beat, pitch and temporal related features are extracted from audio signals and various machine learning algorithms are applied for classification.  Features that resulted in better classification accuracies for Traditional Malay Music (TMM), in comparison to western music, in a previous study were beat related features. However, only the J48 classifier was used and in this study we perform a more comprehensive investigation on improving the classification of TMM.  In addition, feature selection was performed for dimensionality reduction.  Classification accuracies using classifiers of varying paradigms on a dataset comprising ten TMM genres were obtained.  Results identify potentially useful classifiers and show the impact of adding a feature selection phase for TMM genre classification.",
        "zenodo_id": 1415124,
        "dblp_key": "conf/ismir/DoraisamyGNSU08"
    },
    {
        "title": "Audio Cover Song Identification: MIREX 2006-2007 Results and Analyses.",
        "author": [
            "J. Stephen Downie",
            "Mert Bay",
            "Andreas F. Ehmann",
            "M. Cameron Jones"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417133",
        "url": "https://doi.org/10.5281/zenodo.1417133",
        "ee": "https://zenodo.org/records/1417133/files/DownieBEJ08.pdf",
        "abstract": "This paper presents analyses of the 2006 and 2007 results of the Music Information Retrieval Evaluation eXchange (MIREX) Audio Cover Song Identification (ACS) tasks. The Music Information Retrieval Evaluation eXchange (MIREX) is a community-based endeavor to scientifically evaluate music information retrieval (MIR) algorithms and techniques. The ACS task was created to motivate MIR researchers to expand their notions of similarity beyond acoustic similarity to include the important idea that musical works retain their identity notwithstanding variations in style, genre, orchestration, rhythm or melodic ornamentation, etc. A series of statistical analyses were performed that indicate significant improvements in this domain have been made over the course of 2006-2007. Post-hoc analyses reveal distinct differences between individual systems and the effects of certain classes of queries on performance. This paper discusses some of the techniques that show promise in this research domain",
        "zenodo_id": 1417133,
        "dblp_key": "conf/ismir/DownieBEJ08"
    },
    {
        "title": "Collective Annotation of Music from Multiple Semantic Categories.",
        "author": [
            "Zhiyao Duan",
            "Lie Lu",
            "Changshui Zhang"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416694",
        "url": "https://doi.org/10.5281/zenodo.1416694",
        "ee": "https://zenodo.org/records/1416694/files/DuanLZ08.pdf",
        "abstract": "Music semantic annotation aims to automatically annotate a music signal with a set of semantic labels (words or tags). Existing methods on music semantic annotation usually take it as a multi-label binary classi\ufb01cation problem, and model each semantic label individually while ignoring their rela- tionships. However, there are usually strong correlations between some labels. Intuitively, investigating this corre- lation can be helpful to improve the overall annotation per- formance. In this paper, we report our attempts to collective music semantic annotation, which not only builds a model for each semantic label, but also builds models for the pairs of labels that have signi\ufb01cant correlations. Two methods are exploited in this paper, one based on a generative model (Gaussian Mixture Model), and another based on a discrim- inative model (Conditional Random Field). Experiments show slight but consistent improvement in terms of preci- sion and recall, compared with the individual-label model- ing methods. 1 INTRODUCTION Semantic annotation of music signals have become an im- portant direction in music information retrieval. With music annotation approaches, a music signal is associated with a set of semantic labels (text, words), which is a more com- pact and ef\ufb01cient representation than the raw audio or low level features. It can also potentially facilitate a number of music applications, such as music retrieval and recommen- dation, since it is more natural for a user to describe a song by semantic words, and it is more \ufb02exible to measure music similarities with vectors of semantic labels. Several methods have been proposed for automatic mu- sic semantic annotation, which basically can be classi\ufb01ed into two categories: non-parametric and parametric. Non- parametric methods model the text-audio relations implic- This wok was performed when the \ufb01rst author was a visiting student in Microsoft Research Asia. itly. For example, Slaney [10] created separate hierarchi- cal models in the acoustic and text spaces and then linked the two spaces for annotation and retrieval. Cano and Kop- penberger [2] proposed an approach to predict the semantic words based on nearest neighbor classi\ufb01cation. On the other hand, parametric methods explicitly model the text-audio re- lations. For instance, Whitman et al. [14, 13] trained a one- versus-all discriminative model (a regularized least-square classi\ufb01er or a support vector machine) for each word, based on which the audio frames were classi\ufb01ed. Turnbull et al. [11] built a generative model for each semantic word, and calculated a multinomial distribution over the word vocabu- lary for each song. Eck et al. [3] used AdaBoost to predict the strength of the occurrence of each social tag (a word) on a large audio data set. The methods abovementioned achieve good results by modeling the text-audio relations, however, they share two drawbacks. First, there lacks of a taxonomy to organize the semantic labels. Music has a number of important as- pects affecting music perception and music similarity, such as genre, instrumentation, emotion, and tempo, etc. The classi\ufb01cation or detection of these aspects, were also inves- tigated in many previous methods [12, 4, 8, 9]. Semantic labels used for music annotation can be also naturally di- vided into groups corresponding to these aspects. However, although most of the annotation methods use a rather large semantic vocabulary that covers almost all the aspects, the words are not structurally organized. One consequence is that they cannot make sure that a song is annotated from all the aspects. For example, Turnbull et al. [11] calculated the posterior probability for each word in the vocabulary given a song, and selected the A (a constant) words with the largest posterior probability to annotate the song. Thus, suppose for some songs, the posterior probabilities of some words describing genre are larger than those of all the words de- scribing instrumentation, this may cause the words describ- ing instrumentation being absent in the annotation. Second, in the previous methods, semantic labels are mod- eled individually, that is, the methods only build text-audio 237 ISMIR 2008 \u2013 Session 2c \u2013 Knowledge Representation, Tags, Metadata relations, but ignore the text-text relations between two la- bels. However, some labels do have strong correlations, and this information can be investigated to improve annotation schemes. For example, \u201chard rock\u201d and \u201celectronic guitar\u201d tend to co-occur in the annotations of a song, while \u201chappy\u201d and \u201cminor key\u201d, \u201cfast tempo\u201d and \u201cgloomy\u201d tend rarely to co-occur. Using the text-text relation information, strong evidence for the occurrence of \u201chard rock\u201d may help to pre- dict the presence of \u201celectronic guitar\u201d. On the other hand, con\ufb02icts in the annotated labels such as the co-occurrence of \u201cfast tempo\u201d and \u201cgloomy\u201d, which may happen using the individually annotation methods, could be mostly avoided by employing the text-text correlation. To address the two issues above, this paper divides the semantic vocabulary into a number of categories, and pro- poses two collective annotation methods by exploiting the correlations within label pairs. Speci\ufb01cally, 50 web-parsed semantic labels are used to form the vocabulary, and are divided into 10 categories, each of which describes an as- pect of music attributes, including genre, instrumentation, texture, vocal, arousal, affectivity, tempo, rhythm, tonality and production process. We also pose the restriction that the obtained annotation should contain labels from all the categories. In order to estimate the text-text relations, the normalized mutual information (NormMI) between the la- bels in the vocabulary is calculated. The label pairs whose NormMI values are larger than a threshold are selected to be modeled. Two methods are then exploited to integrate correlation modeling: one is a generative method, in which each selected label pair is modeled by a Gaussian Mixture Model (GMM); the other is a discriminative method, which is based on Conditional Random Field (CRF). The rest of the paper is organized as follows: Section 2 describes the semantic vocabulary and the selection process of important word pairs. Section 3 describes audio feature extraction. The two proposed annotation methods are pre- sented in Section 4. Section 5 presents the experimental re- sults, and Section 6 concludes this paper. 2 SEMANTIC VOCABULARY A vocabulary lists all the labels that can be used for semantic annotation. Currently there is not a standard vocabulary, and most researchers build their own vocabularies. Cano and Koppenberger [2] used the taxonomy provided by Word- Net 1 . Whitman and Rifkin [14] extracted about 700 words from web documents associated with artists. Turnbull et al. [11] extracted 135 musically relevant words spanning six semantic categories from song reviews. These vocabular- ies are usually large enough to cover all the aspects of mu- sic. However, due to the large vocabulary, it is usually hard to avoid preference over words when acquiring the ground 1 http://wordnet.princeton.edu/ Category Words Num Genre Blues, Country, Electronica, Folk, 1-2 Funk, Gospel, HardRock, Jazz, Pop, Punk, Rap, R&b, Rock-roll, SoftRock Instrument Acoustic guitar, Acoustic piano, 1-5 Bass, Drum, Electric guitar, Electric piano, Harmonica, Horn, Organ, Percussion, Sax, String Texture Acoustic, Electric, Synthetic 1-2 Vocal Group, Male, Female, No 1-2 Affective Positive, Neutral, Negative 1 Arousal Strong, Middle, Weak 1 Rhythm Strong, Middle, Weak 1 Tempo Fast, Moderato, Slow 1 Tonality Major, Mixed, Minor 1 Production Studio, Live 1 Table 1. The vocabulary contains 50 quantized labels span- ning 10 semantic categories. Each song is annotated using labels from all the categories with a number limitation. truth annotations. In this paper, we build a simpli\ufb01ed (but still general) vo- cabulary from a list of web-parsed musically relevant words. 50 commonly used labels are manually selected and quan- tized, covering 10 semantic categories (aspects) to describe characteristics of music signals. Table 1 lists the vocabulary. Using this vocabulary, each song will be annotated by la- bels from each category with label number limitations. This solves the problem that the annotations of a music signal are missing in some music aspects when the vocabulary is not organized as categories. It is noticed that multiple labels can be selected from the categories of genre, instrument, texture and vocal, while the labels within the other categories are exclusive with each other. The same as existing methods, each label can be viewed as a binary variable in modeling and annotation. As men- tioned previously, some labels have strong relations: posi- tive or negative correlations. For example, the labels within some categories (e.g. Rhythm and Tempo) have negative correlations and are exclusive with each other. Moreover, some labels from different categories may have positive or negative correlations. For example, \u201cGenre.HardRock\u201d and \u201cArousal.Strong\u201d tend to co-occur, while \u201cTonality.Major\u201d and \u201cAffective.Negative\u201d tend rarely to co-occur. The normalized mutual information (NormMI) is used to measure the correlations of each label pair (X, Y ) as NormMI(X, Y ) = I(X, Y ) min{H(X), H(Y )} (1) 238 ISMIR 2008 \u2013 Session 2c \u2013 Knowledge Representation, Tags, Metadata Word pair NormMI (Production.Live, Production.Studio)",
        "zenodo_id": 1416694,
        "dblp_key": "conf/ismir/DuanLZ08"
    },
    {
        "title": "Computational Temporal Aesthetics; Relation Between Surprise, Salience and Aesthetics in Music and Audio Signals.",
        "author": [
            "Shlomo Dubnov"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.4285466",
        "url": "https://doi.org/10.5281/zenodo.4285466",
        "ee": null,
        "abstract": "While computational aesthetic evaluation has been applied to images and visual output, it is not as widely employed for generative music systems. Computational aesthetic evaluation is not to be confounded with numerical evaluation of the system's output; such a notion is in danger of offering a reduced and impoverished interpretation of the aesthetic experience, which is innately dialogical, between the creator or the user, the sociological context, and the creative process or product. This paper reviews common computational aesthetic measures that have been used for musical applications, whilst arguing for a pragmatist perspective and a framework foregrounding the primacy of intentionality and agency in inducing aesthetic responses.",
        "zenodo_id": 4285466,
        "dblp_key": "conf/ismir/Dubnov08"
    },
    {
        "title": "Machine Annotation of Sets of Traditional Irish Dance Tunes.",
        "author": [
            "Bryan Duggan",
            "Brendan O&apos;Shea",
            "Mikel Gainza",
            "Padraig Cunningham"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415106",
        "url": "https://doi.org/10.5281/zenodo.1415106",
        "ee": "https://zenodo.org/records/1415106/files/DugganOGC08.pdf",
        "abstract": "A set in traditional Irish music is a sequence of two or more dance tunes in the same time signature, where each tune is repeated an arbitrary number of times. A turn in a set represents the point at which either a tune repeats or a new tune is introduced. Tunes in sets are played in a segue (without a pause) and so detecting the turn is a significant challenge. This paper presents the MATS algorithm, a novel algorithm for identifying turns in sets of traditional Irish music. MATS works on digitised audio files of monophonic flute and tin-whistle music. Previous work on machine annotation of traditional music is summarised and experimental results validating the MATS algorithm are presented.",
        "zenodo_id": 1415106,
        "dblp_key": "conf/ismir/DugganOGC08"
    },
    {
        "title": "Vocal Segment Classification in Popular Music.",
        "author": [
            "Ling Feng",
            "Andreas Brinch Nielsen",
            "Lars Kai Hansen"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416932",
        "url": "https://doi.org/10.5281/zenodo.1416932",
        "ee": "https://zenodo.org/records/1416932/files/FengNH08.pdf",
        "abstract": "This paper explores the vocal and non-vocal music classi\ufb01- cation problem within popular songs. A newly built labeled database covering 147 popular songs is announced. It is de- signed for classifying signals from 1sec time windows. Fea- tures are selected for this particular task, in order to capture both the temporal correlations and the dependencies among the feature dimensions. We systematically study the per- formance of a set of classi\ufb01ers, including linear regression, generalized linear model, Gaussian mixture model, reduced kernel orthonormalized partial least squares and K-means on cross-validated training and test setup. The database is divided in two different ways: with/without artist overlap between training and test sets, so as to study the so called \u2018artist effect\u2019. The performance and results are analyzed in depth: from error rates to sample-to-sample error correla- tion. A voting scheme is proposed to enhance the perfor- mance under certain conditions. 1 INTRODUCTION The wide availability of digital music has increased the in- terest in music information retrieval, and in particular in features of music and of music meta-data, that could be used for better indexing and search. High-level musical fea- tures aimed at better indexing comprise, e.g., music instru- ment detection and separation [13], automatic transcription of music [8], melody detection [2], musical genre classi\ufb01ca- tion [10], sound source separation [18], singer recognition [16], and vocal detection [4]. While the latter obviously is of interest for music indexing, it has shown to be a surpris- ingly hard problem. In this paper we will pursue two ob- jectives in relation to vocal/non-vocal music classi\ufb01cation. We will investigate a multi-classi\ufb01er system, and we will publish a new labeled database that can hopefully stimulate further research in the area. While almost all musical genres are represented in digital forms, naturally popular music is most widely distributed, and in this paper we focus solely on popular music. It is not clear that the classi\ufb01cation problem can be generalized between genres, but this is a problem we will investigate in later work. Singing voice segmentation research started less than a decade ago. Berenzweig and Ellis attempted to locate the vocal line from music using a multi-layer perceptron speech model, trained to discriminate 54 phone classes, as the \ufb01rst step for lyric recognition [4]. However, even though singing and speech share certain similarities, the singing process in- volves the rapid acoustic variation, which makes it statisti- cally different from normal speech. Such differences may lie in the phonetic and timing modi\ufb01cation to follow the tune of the background music, and the usage of words or phrases in lyrics and their sequences. Their work was in- spired by [15] and [19], where the task was to distinguish speech and music signals within the \u201cmusic-speech\u201d corpus: 240 15s extracts collected \u2018at random\u2019 from the radio. A set of features have been designed speci\ufb01cally for speech/music discrimination, and they are capable of measuring the con- ceptually distinct properties of both classes. Lyrics recognition can be one of a variety of uses for vo- cal segmentation. By matching the word transcriptions, it is applicable to search for different versions of the same song. Moreover, accurate singing detection could be po- tential for online lyrics display by automatically aligning the singing pieces with the known lyrics available on the Internet. Singer recognition of music recordings has later received more attention, and has become one of the pop- ular research topics within MIR. In early work of singer recognition, techniques were borrowed from speaker recog- nition. A Gaussian Mixture Model (GMM) was applied based on Mel-frequency Cepstral Coef\ufb01cients (MFCC) to detect singer identity [20]. As brie\ufb02y introduced, singing voices are different from the conventional speech in terms of time-frequency features; and vocal and non-vocal fea- tures have differences w.r.t. spectral distribution. Hence the performance of a singer recognition system has been investigated using the unsegmented music piece, the vocal segments, and the non-vocal ones in [5]. 15% improve- ment has been achieved by only using the vocal segments, compared to the baseline of the system trained on the un- segmented music signals; and the performance became 23% worse when only non-vocal segments were used. It demon- strated that the vocal segments are the primary source for recognizing singers. Later, work on automatic singer recog- nition took vocal segmentation as the \ufb01rst step to enhance 121 ISMIR 2008 \u2013 Session 1c \u2013 Timbre the system performance, e.g. [16]. Loosely speaking, vocal segmentation has two forms. One is to deal with a continuous music stream, and the locations of the singing voice have to be detected as well as classi- \ufb01ed, one example is [4]. The second one is to pre-segment the signals into windows, and the task is only to classify these segments into two classes. Our work follows the sec- ond line, in order to build models based on our in-house Pop music database. A detailed description of the database will be presented in section 4. The voice is only segmented in the time domain, instead of the frequency domain, mean- ing the resulting vocal segments will still be a mixture of singing voices and instrumental background. Here we will cast the vocal segments detection in its simplest form, i.e. as a binary classi\ufb01cation problem: one class represents signals with singing voices (with or without background music); the other purely instrumental segments, which we call accom- paniment. In this paper we study this problem from a different an- gle. Several classi\ufb01ers are invoked, and individual perfor- mance (errors and error rates) is inspected. To enhance per- formance, we study the possibility of sample-to-sample cross- classi\ufb01er voting, where the outputs of several classi\ufb01ers are merged to give a single prediction. The paper is organized as follows. Section 2 explains the selection of features. Clas- si\ufb01cation frameworks are covered by section 3. With the purpose of announcing the Pop music database, we intro- duce the database design in section 4. In section 5, the ex- periments are described in depth, and the performance char- acteristics are presented. At last, section 6 concludes the current work. 2 ACOUSTIC FEATURES",
        "zenodo_id": 1416932,
        "dblp_key": "conf/ismir/FengNH08"
    },
    {
        "title": "Support for MIR Prototyping and Real-Time Applications in the ChucK Programming Language.",
        "author": [
            "Rebecca Fiebrink",
            "Ge Wang 0002",
            "Perry R. Cook"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415268",
        "url": "https://doi.org/10.5281/zenodo.1415268",
        "ee": "https://zenodo.org/records/1415268/files/FiebrinkWC08.pdf",
        "abstract": "In this paper, we discuss our recent additions of audio analysis and machine learning infrastructure to the ChucK music programming language, wherein we provide a complementary system prototyping framework for MIR researchers and lower the barriers to applying many MIR algorithms in live music performance. The new language capabilities preserve ChucK\u2019s breadth of control\u2014from high-level control using building block components to sample-level manipulation\u2014and on-the-fly re- programmability, allowing the programmer to experiment with new features, signal processing techniques, and learning algorithms with ease and flexibility. Furthermore, our additions integrate tightly with ChucK\u2019s synthesis system, allowing the programmer to apply the results of analysis and learning to drive real-time music creation and interaction within a single framework. In this paper, we motivate and describe our recent additions to the language, outline a ChucK-based approach to rapid MIR prototyping, present three case studies in which we have applied ChucK to audio analysis and MIR tasks, and introduce our new toolkit to facilitate experimentation with analysis and learning in the language.",
        "zenodo_id": 1415268,
        "dblp_key": "conf/ismir/FiebrinkWC08"
    },
    {
        "title": "Social Playlists and Bottleneck Measurements: Exploiting Musician Social Graphs Using Content-Based Dissimilarity and Pairwise Maximum Flow Values.",
        "author": [
            "Benjamin Fields",
            "Christophe Rhodes",
            "Michael A. Casey",
            "Kurt Jacobson"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417939",
        "url": "https://doi.org/10.5281/zenodo.1417939",
        "ee": "https://zenodo.org/records/1417939/files/FieldsRCJ08.pdf",
        "abstract": "We have sampled the artist social network of Myspace and to it applied the pairwise relational connectivity measure Minimum cut/Maximum \ufb02ow. These values are then com- pared to a pairwise acoustic Earth Mover\u2019s Distance mea- sure and the relationship is discussed. Further, a means of constructing playlists using the maximum \ufb02ow value to ex- ploit both the social and acoustic distances is realized. 1 INTRODUCTION As freely\u2013available audio content continues to become more accessible, listeners require more sophisticated tools to aid them in the discovery and organization of new music that they \ufb01nd enjoyable. This need, along with the recent advent of Internet based social networks and the steady progress of signal based Music Information Retrieval have created an opportunity to exploit both social relationships and acoustic similarity in recommender systems. Motivated by this, we examine the Myspace artist net- work. Though there are a number of music oriented social networking websites, Myspace 1 has become the de facto standard for web-based music artist promotion. Although exact \ufb01gures are not made public, recent estimates suggest there are well over 7 million artist pages 2 on Myspace. For the purpose of this paper, artist and artist page are used interchangeably to refer to the collection of media and so- cial relationships found at a speci\ufb01c Myspace page residing in Myspace\u2019s artist subnetwork, where this subnetwork is de\ufb01ned as those Myspace user pages containing the audio player application. The Myspace social network, like most social networks, is based upon relational links between friends designating some kind of association. Further, a Myspace user has a subset of between 8 and 40 top friends. While all friends 1 http://myspace.com/ 2 http://scottelkin.com/archive/2007/05/11/ MySpace-Statistics.aspx reports as of April 2007 \u223c25 mil- lion songs, our estimates approximate 3.5 songs/artist, giving \u223c7 million artists are mutually con\ufb01rmed, individual users unilaterally select top friends. Additionally, pages by artists will usually con- tain streaming and downloadable media of some kind either audio, video or both. Social networks of this sort present a way for nearly any- one to distribute their own media and as a direct result, there is an ever larger amount of available music from an ever increasing array of artists. Given this environment of con- tent, how can we best use all of the available information to discover new music? Can both social metadata and content based comparisons be exploited to improve navigation? To work towards answers to these and related questions, we explore the relationship between the connectivity of pairs of artists on the Myspace top friends network and a measure of acoustic dissimilarity of these artists. We begin this paper by brie\ufb02y reviewing graph theoretic network \ufb02ow analysis and previous work in related topics including musician networks, content-based artist similarity. We go on to explain our methodology including our network sampling method in Section 3.1 and our connectivity analy- sis techniques in Section 3.2. These connectivity measures are then compared to acoustic artist similarity for the struc- tured network in Section 4 and they are used to construct a social playlist in Section 5. We \ufb01nish with a discussion of the results and what these results may mean for future work in this space. 2 BACKGROUND This work uses a combination of complex network theory, network \ufb02ow analysis and signal-based music analysis. Both disciplines apply intuitively to Music Information Retrieval; however, the two have only recently been applied simulta- neously to a single data set [9].",
        "zenodo_id": 1417939,
        "dblp_key": "conf/ismir/FieldsRCJ08"
    },
    {
        "title": "Quantifying Metrical Ambiguity.",
        "author": [
            "Patrick Flanagan"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417117",
        "url": "https://doi.org/10.5281/zenodo.1417117",
        "ee": "https://zenodo.org/records/1417117/files/Flanagan08.pdf",
        "abstract": "This paper explores how data generated by meter induc- tion models may be recycled to quantify metrical ambigu- ity, which is calculated by measuring the dispersion of met- rical induction strengths across a population of possible me- ters. A measure of dispersion commonly used in economics to measure income inequality, the Gini coef\ufb01cient, is intro- duced for this purpose. The value of this metric as a rhyth- mic descriptor is explored by quantifying the ambiguity of several common clave patterns and comparing the results to other metrics of rhythmic complexity and syncopation. 1 INTRODUCTION This study initially grew out of an interest in modeling lis- tener responses of a tapping study in which listeners tapped to the beat of sections of Steve Reich\u2019s Piano Phase [17]. For some sections, tapping rate and phase varied widely among listeners, and for others they displayed relative con- sistency. Accordingly, the goal of the study was to model metrical ambiguity as the degree of variety of listener re- sponses to a given rhythm. This task, however, would be complicated by a lack of simpli\ufb01ed experimental data; the experiments of [17] in- volved pitched music, which added a layer of complexity I wanted to avoid. I therefore turned my attention to a corol- lary kind of metrical ambiguity, which is the amenability of a rhythm to gestalt \ufb02ip, in which a listener reevaluates the location of the beat. This amenability has been convinc- ingly advanced as one reason for the pull of African and African-derived rhythms on listeners [7]. Data on metrical gestalt \ufb02ips is no easier to come by, but orientating the study around the proverbial average listener instead of groups of listeners allowed it to interface with the voluminous litera- ture on syncopation. While metrical ambiguity is not equiv- alent to syncopation, the results of this study suggest that it roughly correlates to syncopation and offers an interesting rhythmic descriptor in its own right that could be used for theoretical analysis, genre classi\ufb01cation, and the production of metrically ambiguous music. To describe the ambiguity of the metrical scene, the am- biguity model recycles data that existing models of meter and beat induction often discard, the data about the induc- tion strengths of all potential meters. The central idea of the model is that a more even distribution of induction strengths across a group of potential meters will cause greater am- biguity in the perception of meter. The model of ambigu- ity, therefore, requires a model of meter induction that, for a given rhythm, produces numerical values for an array of potential meters, with each meter de\ufb01ned by its period and phase relation to some arbitrary point in the rhythm. For the sake of simplicity, this study uses a hybrid model based on older models of meter induction that operate on quantized symbolic data, but the theory of ambiguity presented here is extensible to any model, including those that work with audio, that produces such an array. In this paper the term \u201cmeter\u201d refers to an isochronous, tactus-level pulse, characterized by a period and phase ex- pressed in discrete integer multiples of some smallest unit of musical time. The more common term for this isochronous pulse is, of course, \u201cthe beat.\u201d \u201cMeter,\u201d however, is the preferable term because this paper uses the conventionally understood downbeat of musical examples as a reference point for the tactus-level pulse. 2 METER INDUCTION MODEL",
        "zenodo_id": 1417117,
        "dblp_key": "conf/ismir/Flanagan08"
    },
    {
        "title": "Playlist Generation using Start and End Songs.",
        "author": [
            "Arthur Flexer",
            "Dominik Schnitzer",
            "Martin Gasser",
            "Gerhard Widmer"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1418273",
        "url": "https://doi.org/10.5281/zenodo.1418273",
        "ee": "https://zenodo.org/records/1418273/files/FlexerSGW08.pdf",
        "abstract": "A new algorithm for automatic generation of playlists with an inherent sequential order is presented. Based on a start and end song it creates a smooth transition allowing users to discover new songs in a music collection. The approach is based on audio similarity and does not require any kind of meta data. It is evaluated using both objective genre labels and subjective listening tests. Our approach allows users of the website of a public radio station to create their own digital \u201cmixtapes\u201d online. 1 INTRODUCTION This work is concerned with the creation of playlists with an inherent sequential order. Such a playlist consists of a start and an end song, both chosen by a user. The songs in between should form a smooth transition, with songs at the beginning sounding similar to the start song, songs at the end similar to the end song and songs in the middle similar to both start and end songs. Our approach is based solely on audio analysis and does not require any kind of meta- data. It could therefore easily replace or at least support manual creation of playlists. It allows to explore audio col- lections by simply choosing two songs and a desired length for the playlist. It also enables ef\ufb01cient discovery of new music if applied to collections of yet unknown songs by au- tomatically creating a smooth transition between only two supporting songs. Most existing approaches to playlist generation rely on the usage of one seed song or a group of seed songs. The playlist then consists of songs which are somehow simi- lar to this seed. Some authors use different kinds of audio similarity to create the playlists [7, 10]. Others work with some kind of metadata [11, 14, 16]. Seed based creation of playlists has the problem of producing too uniform lists of songs if applied to large data bases with lots of similar mu- sic. If a data base does not contain enough similar music to a seed song there is the danger of \u201cplaylist drift\u201d towards music that sounds very different. Few authors report about generating playlists with an in- herent sequential order. Most approaches are solely based on metadata and not audio similarity. Case Based Reason- ing has been applied to create new playlists with inherent temporal structure based on patterns of subsequences of a collection of existing playlists [4]. Creation of playlists sat- isfying user constraints based on rich metadata has also been reported [3]. These constraints may also concern the tem- poral order of the playlists (e.g. rising tempo, change of genre). This constraint based approach has been extended [2] to include notions of audio similarity as yet another con- straint (e.g. timbre continuity through a playlist). Related approaches have been formulated based on simulated an- nealing [12] and linear programming [1]. Travelling Sales- man algorithms applied to audio similarity have been used to generate a sequential order of all songs in a data base [15]. Since all songs have to be part of the playlist, this is a quite different kind of organising principle for the playlist. Direct interaction with a two dimensional mapping of mu- sic spaces based on audio similarity also allows creation of playlists with inherent sequential structure [9]. Computa- tions are based on the lower dimensional representations and not directly on the audio models of the songs themselves. A related approach also using a two dimensional display which is enriched with various kinds of meta data has also been presented [5]. Contrary to the work reviewed above, our approach is (i) based on audio similarity, (ii) requires very little user in- teraction and (iii) results in playlists with smooth temporal transitions potentially including songs previously unknown to the users. Our playlist generation algorithm has been de- veloped for the internet portal of an Austrian radio station to allow creation of digital \u201cmixtapes\u201d online. 2 DATA This work is part of a project aiming at providing novel ways of accessing the music of an Austrian music portal. The FM4 Soundpark is an internet platform 1 of the Aus- 1 http://fm4.orf.at/soundpark 173 ISMIR 2008 \u2013 Session 2a \u2013 Music Recommendation and Organization HiHo Regg Funk Elec Pop Rock No. 226 60 56 918 158 1148 % 9 2 2 36 6 45 Table 1. Number of songs and percentages across genres in our data base. Genres are Hip Hop, Reggae, Funk, Elec- tronic, Pop and Rock. trian public radio station FM4. This internet platform allows artists to present their music free of any cost in the WWW. All interested parties can download this music free of any charge. At the moment this music collection contains about 10000 songs but it is only organised alphabetically and in a coarse genre taxonomy. The artists themselves choose which of the six genre labels \u201cHip Hop, Reggae, Funk, Elec- tronic, Pop and Rock\u201d best describe their music. We use a development data base of 2566 songs for our experiments. Number of songs and percentages across genres are given in Tab. 1. The distribution of genres is quite unbalanced with \u201cElectronic\u201d and \u201cRock\u201d together taking up 81%. This is representative of the full data base. From the 22050Hz mono audio signals two minutes from the center of each song are used for further analysis. We di- vide the raw audio data into non-overlapping frames of short duration and use Mel Frequency Cepstrum Coef\ufb01cients (MFCC) to represent the spectrum of each frame. MFCCs are a perceptually meaningful and spectrally smoothed rep- resentation of audio signals. MFCCs are now a standard technique for computation of spectral similarity in music analysis (see e.g. [6]). The frame size for computation of MFCCs for our experiments was 46.4ms (1024 samples). We used the \ufb01rst 20 MFCCs for all our experiments. 3 METHODS Our playlist generation algorithm consists of two basic parts: (i) computation of similarities between songs, (ii) computa- tion of the actual playlists based on these similarities. Please note that the actual generation of playlists does not rely on a speci\ufb01c similarity function and could therefore also be done using different approaches towards computation of similar- ity.",
        "zenodo_id": 1418273,
        "dblp_key": "conf/ismir/FlexerSGW08"
    },
    {
        "title": "Automatic Mapping of Scanned Sheet Music to Audio Recordings.",
        "author": [
            "Christian Fremerey",
            "Meinard M\u00fcller",
            "Frank Kurth",
            "Michael Clausen"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416034",
        "url": "https://doi.org/10.5281/zenodo.1416034",
        "ee": "https://zenodo.org/records/1416034/files/FremereyMKC08.pdf",
        "abstract": "Signi\ufb01cant digitization efforts have resulted in large multi- modal music collections comprising visual (scanned sheet music) as well as acoustic material (audio recordings). In this paper, we present a novel procedure for mapping scanned pages of sheet music to a given collection of au- dio recordings by identifying musically corresponding au- dio clips. To this end, both the scanned images as well as the audio recordings are \ufb01rst transformed into a common feature representation using optical music recognition (OMR) and",
        "zenodo_id": 1416034,
        "dblp_key": "conf/ismir/FremereyMKC08"
    },
    {
        "title": "Hyperlinking Lyrics: A Method for Creating Hyperlinks Between Phrases in Song Lyrics.",
        "author": [
            "Hiromasa Fujihara",
            "Masataka Goto",
            "Jun Ogata"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1418251",
        "url": "https://doi.org/10.5281/zenodo.1418251",
        "ee": "https://zenodo.org/records/1418251/files/FujiharaGO08.pdf",
        "abstract": "We describe a novel method for creating a hyperlink from a phrase in the lyrics of a song to the same phrase in the lyrics of another song. This method can be applied to various ap- plications, such as song clustering based on the meaning of the lyrics and a music playback interface that will enable a user to browse and discover songs on the basis of lyrics. Given a song database consisting of songs with their text lyrics and songs without their text lyrics, our method \ufb01rst extracts appropriate keywords (phrases) from the text lyrics without using audio signals. It then \ufb01nds these keywords in audio signals by estimating the keywords\u2019 start and end times. Although the performance obtained in our experi- ments has room for improvement, the potential of this new approach is shown. 1 INTRODUCTION The goal of this study is to enable a Music Web where songs are hyperlinked to each other on the basis of their lyrics (\ufb01gure 1). Just as some hypertext phrases on the web are hyperlinked, so some phrases of lyrics (which we call hyperlyrics) on the Music Web can be hyperlinked. Such a hyperlinked structure of the Music Web can be used as a basis for various applications. For example, we can clus- ter songs based on the meanings of their lyrics by analyzing the hyperlinked structure or can show relevant information during music playback by analyzing the hyperlinked songs. We can also provide a new active music listening interface [1] where a user can browse and discover songs by clicking a hyperlinked phrase in the lyrics of a song to jump to the same phrase in the lyrics of another song. Although we can think of many possible ways to hyperlink musical pieces on the Music Web, focusing on song lyrics is natural because the lyrics are one of the most important elements of songs and often convey their essential messages. Most approaches for analyzing inter-song relationship have been based on musical similarity between songs, and various music interfaces based on such song-level similarity have been proposed [2, 3, 4, 5, 6]. the methods of music browsing for intra-song navigation have also been studied, such as music browsing based on the structure [7, 8] and the lyrics [9, 10, 11]. However, hyperlinking lyrics \u2014 i.e., a combination of inter-song and intra-song navigations based on lyrics phrases \u2014 has not been proposed. In this paper, we propose a method for hyperlinking iden- tical keywords (phrases) that appear in the lyrics of different songs. Hyperlinking lyrics enables us to bene\ufb01t from vari- ous studies dealing with sung lyrics in musical audio signals. For example, by using methods for automatic synchroniza- tion of lyrics with musical audio signals[9, 10], we can \ufb01rst \ufb01nd a keyword pair in the text lyrics for two different songs Music Web Hyperlink Hyperlyrics Songs are hyperlinked to each other on the basis of their lyrics. Figure 1. Hyperlinking lyrics. on Music Web and then locate (i.e., estimate the start and end times of) the keyword in the audio signals of each song. However, it is still dif\ufb01cult to achieve accurate automatic lyrics recogni- tion that enables us to \ufb01nd a keyword pair in sung lyrics that are recognized in polyphonic musical audio signals, de- spite the achievements made by studies on automatic lyrics recognition for musical audio signals [12, 13, 14]. While studies have also been done on analyzing text lyrics without using audio signals [15, 16], we cannot use their results to hyperlink lyrics. Figure 2 shows the strategy our method uses to hyperlink lyrics. We assume that a user has a song database that is a set of audio \ufb01les (e.g., MP3 \ufb01les) and that we can pre- pare text \ufb01les of the lyrics for some of the songs. Note that we do not require text lyrics for all the songs \u2014 i.e., a song database consists of songs with their text lyrics and songs without them. We therefore apply two different strate- gies for hyperlinking: one for hyperlinking from a phrase in other text lyrics to the same phrase in the text lyrics, and one for hyperlinking from a phrase in the text lyrics to the same phrase in the sung lyrics in a song (polyphonic music sig- nals) without its text lyrics. Here, \u201ctext lyrics\u201d means a text document containing the lyrics of a song, and \u201csung lyrics\u201d means audio signals containing the lyrics sung by a singer in a polyphonic sound mixture. Although hyperlinking from text lyrics to text lyrics is relatively easy with the support of LyricSynchronizer [10, 1], hyperlinking from text lyrics to sung lyrics is more dif\ufb01cult. 2 OUR METHOD FOR HYPERLINKING LYRICS Our method hyperlinks phrases that appear in the lyrics of different songs. In other words, if different songs share 281 ISMIR 2008 \u2013 Session 2d \u2013 Social and Music Networks Keywords extraction Hyperlink from text lyrics to text lyrics Hyperlink from text lyrics to sung lyrics Songs with text lyrics Songs without text lyrics Each keyword is hyperlinked in the text/sung lyrics Song database easier than like a dream take me higher make me cry fall in love you don\u2019t need to Figure 2. Two different strategies for hyperlinking: hyper- link from text lyrics to text lyrics and hyperlink from text lyircs to sung lyrics. the same phrase (what we call a keyword1 ) in their lyrics, a section (temporal region) corresponding to the keyword in one song is hyperlinked with a section corresponding to the same keyword in another song. This method should deal with polyphonic music mixtures containing sounds of various instruments as well as singing voices. If a song database consists of songs with text lyrics and songs without text lyrics, this approach creates two different types of bidi- rectional hyperlinks: a hyperlink between songs with text lyrics and a hyperlink from a song with text lyrics to a song without them. The former hyperlink can be created by ex- tracting potential keywords from all the text lyrics and \ufb01nd- ing sections corresponding to them (i.e., temporally locat- ing them) in audio signals with the help of their lyrics. This estimation can be done using an automatic lyrics synchro- nization method described in [10]. For the latter hyperlink, our method looks for sections including voices that sing the keywords by using a keyword spotting technique for poly- phonic music mixtures.",
        "zenodo_id": 1418251,
        "dblp_key": "conf/ismir/FujiharaGO08"
    },
    {
        "title": "Using XQuery on MusicXML Databases for Musicological Analysis.",
        "author": [
            "Joachim Ganseman",
            "Paul Scheunders",
            "Wim D&apos;haes"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1414836",
        "url": "https://doi.org/10.5281/zenodo.1414836",
        "ee": "https://zenodo.org/records/1414836/files/GansemanSD08.pdf",
        "abstract": "MusicXML is a fairly recent XML-based \ufb01le format for mu- sic scores, now supported by many score and audio editing software applications. Several online score library projects exist or are emerging, some of them using MusicXML as main format. When storing a large set of XML-encoded scores in an XML database, XQuery can be used to re- trieve information from this database. We present some small practical examples of such large scale analysis, using the Wikifonia lead sheet database and the eXist XQuery en- gine. This shows the feasibility of automated musicological analysis on digital score libraries using the latest software tools. Bottom line: it\u2019s easy. 1 INTRODUCTION MusicXML is an open \ufb01le format developed by Recordare LLC [1]. Its development started around the year 2000 [2]. Some design decisions on the format are well explained in [3]. In 2002 a paper was presented on the XML conference [4] explaining how XQuery can be used to search for struc- tures in a MusicXML document, but since then it has been very quiet on the front. In a 2007 poster, Viglianti [5] points out the possibilities of using XQuery for musicological anal- ysis. Unfortunately, his text contains some errors - the most important one being the remark that XQuery would not sup- port arrays. XQuery de\ufb01nitely does support arrays, it is even proved to be a Turing complete language [6]. There- fore has the same expressive power as languages like C++ or Java, and any computable function can be performed us- ing XQuery. In the meantime, XQuery 1.0 has made it to a W3C rec- ommendation [7]. Galax [8] strives to be a reference im- plementation. eXist [9], Sedna [10], and Oracle\u2019s Berkeley DB XML [11] are native XML database management sys- tems incorporating their own XQuery engines. All of those are open source and free. Both standards and software have thus matured. Digital score libraries already exist, emanating from re- search (e.g. KernScores [12], based on HumDrum [13]) or the open source community (e.g. Mutopia [14], using Lily- pond [15]). The Wikifonia project [16] uses MusicXML. It is a wiki-style collaborative environment for publishing lead sheets - i.e. reduced scores where arrangement details have been removed and only melody, lyrics and chord progres- sion information is available. Wikifonia is at the moment of writing still relatively small. We worked with a database downloaded on April 1st 2008, which contained just over 200 songs. To do automated musicological analysis, the HumDrum toolkit [13] is probably the most widely used today. It is very powerful, but it has its drawbacks. It works with in- put and output in its own plain text (ASCII) \ufb01le format. Formatting output, converting to and from other formats, dynamically generating queries etc., require the creation of dedicated software or advanced scripts. To use it effectively, HumDrum requires a good deal of knowledge of UNIX style scripting, which is even more dif\ufb01cult to use on Windows platforms. On the other hand, XML formats are so generic that a huge set of software tools already exist to manipu- late XML data in almost any way imaginable. Knowing that XQuery is Turing complete, we can state that it is theoret- ically possible to translate the whole HumDrum toolkit to XQuery. There is not much software around that is capable of doing pattern analysis on MusicXML \ufb01les: MelodicMatch [17] is at the moment of writing probably the most complete one. THoTH [18] also has some limited selection and ex- traction functionality based on the contents of a MusicXML \ufb01le. Both software packages are closed source. Neverthe- less, this kind of functionality is very desirable in music score software. For example, in regular PDF \ufb01les, we are 433 ISMIR 2008 \u2013 Session 4a \u2013 Data Exchange, Archiving and Evaluation not impressed any more by functionality that allows us to search for a text pattern, highlighting all occurrences in the text itself. But in score editors, searching for a music pattern and highlighting the results in the score, is a functionality that is only noticeable by its absence. In this paper we give some practical examples that illus- trate how XQuery can be used to easily and quickly search large collections of MusicXML scores for interesting data, and return the results formatted in any way desired. We will point out some interesting directions for future projects. By showing that only a few proven, stable, off-the-shelf software components are needed for advanced score analy- sis, we think that advanced search and information retrieval functionality can be incorporated in music score software very quickly. 2 USED TECHNOLOGIES It is not our goal to provide an in-depth tutorial on Mu- sicXML [1], XPath [19] or XQuery [7]. Numerous excellent books or online tutorials can be found on those subjects, and the websites of the respective projects contain valuable in- formation. Nevertheless, to understand the examples in this paper, a small introduction is given in the next paragraphs. MusicXML can encode scores in 2 ways: part-wise or time-wise. In a part-wise score, the \ufb01le is roughly struc- tured as follows: a score contains a number of parts, a part contains a number of measures, and a measure contains a number of notes. For a time-wise score, measure and part are switched in the hierarchy. An XSLT \ufb01le (eXtensible Stylesheet Language Transformation) can be used to convert between the 2 formats, with a notable exception: multimet- ric music - where different parts have different time signa- tures - can only be properly encoded in part-wise scores. In practice, most MusicXML \ufb01les today are part-wise encoded, and for the rest of this paper, we will assume part-wise \ufb01le structure. Example \ufb01les of MusicXML documents can be found on Recordare\u2019s website [1]. XPath is a language for retrieving information from an XML document. It is relatively easy to understand, and is used by XQuery to traverse XML documents. A complete reference of the latest W3C recommendation, version 2.0, can be found online [19]. Next to the document traversal functionality, XPath contains a wide range of operators for union and intersection, comparison, arithmetic, etc. A list of about 100 built-in functions completes the XPath language. Important to note is that an XPath query preserves document order. XQuery extends XPath, making it more suitable for larger queries than XPath is. The most important extension is the so-called FLWOR (pronounce \u2019\ufb02ower\u2019) construct - acronym for \u2019for, let, where, order by, return\u2019 - describing the typ- ical form of an XQuery query. This is roughly analogous to the \u2019select, from, where\u2019 structure used in SQL (Struc- tured Query Language). Results of XQuery FLWORs can be returned in any desired XML format, including XHTML, allowing immediate inclusion in a dynamic webpage. A sec- ond extension is the possibility to de\ufb01ne functions yourself, functions which may be recursive, making XQuery Turing complete. Last but not least, (: comments look like this :). eXist [9] is an open source XML database management system, written in Java. It implements XPath 2.0, XQuery",
        "zenodo_id": 1414836,
        "dblp_key": "conf/ismir/GansemanSD08"
    },
    {
        "title": "Towards Structural Alignment of Folk Songs.",
        "author": [
            "J\u00f6rg Garbers",
            "Frans Wiering"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415838",
        "url": "https://doi.org/10.5281/zenodo.1415838",
        "ee": "https://zenodo.org/records/1415838/files/GarbersW08.pdf",
        "abstract": "We describe an alignment-based similarity framework for folk song variation research. The framework makes use of phrase and meter information encoded in Hum- drum scores. Local similarity measures are used to compute match scores, which are combined with gap scores to form increasingly larger alignments and higher-level similarity values. We discuss the effects of some similarity measures on the alignment of four groups of melodies that are variants of each other. 1 INTRODUCTION In the process of oral transmission folk songs are re- shaped in many different variants. Given a collection of tunes, recorded in a particular region or environ- ment, folk song researchers try to reconstruct the ge- netic relation between folk songs. For this they study historical and musical relations of tunes to other tunes and to already established folk song prototypes. It has often been claimed that their work could ben- e\ufb01t from support by music information retrieval (MIR) similarity and alignment methods and systems. In prac- tice however it turns out that existing systems do not work well enough out of the box [5]. Therefore the re- search context must be analyzed and existing methods must be adapted and non-trivially combined to deliver satisfying results.",
        "zenodo_id": 1415838,
        "dblp_key": "conf/ismir/GarbersW08"
    },
    {
        "title": "Streamcatcher: Integrated Visualization of Music Clips and Online Audio Streams.",
        "author": [
            "Martin Gasser",
            "Arthur Flexer",
            "Gerhard Widmer"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416362",
        "url": "https://doi.org/10.5281/zenodo.1416362",
        "ee": "https://zenodo.org/records/1416362/files/GasserFW08.pdf",
        "abstract": "We propose a content-based approach to explorative visu- alization of online audio streams (e.g., web radio streams). The visualization space is de\ufb01ned by prototypical instances of musical concepts taken from personal music collections. Our system shows the relation of prototypes to each other and generates an animated visualization that places repre- sentations of audio streams in the vicinity of their most simi- lar prototypes. Both computation of music similarity and vi- sualization are formulated for online real time performance. A software implementation of these ideas is presented and evaluated. 1 INTRODUCTION A massive amount of music is already available on the web in the form of internet radio streams. At the time of writ- ing this paper, the \u201cradio-locator\u201d website 1 lists more than 2500 entries in its directory of free internet radio stations, a number that is likely to increase rapidly in the future. Be- cause it is infeasible for users to keep an overview of avail- able radio streams, software that recommends possibly in- teresting radio streams would be desirable. When talking about music, humans usually use exam- ples of similar music to describe new and unknown songs or artists. For example, one might characterize Nirvana as a combination of the Pixies\u2019 and Sonic Youth\u2019s guitar sound, the Beatles\u2019 sense for catchy melody lines, and the heritage of American folk-rock music in the style of Neil Young. So, instead of resorting to prede\ufb01ned taxonomies for music cat- egorization (e.g., by Genre), an alternative is to present a \u201cquery by example\u201d based user interface directly to the user. To support the user in \ufb01nding new music he or she might like in audio streams, we propose a simple interactive visu- alization approach that incorporates the user\u2019s musical vo- cabulary into the de\ufb01nition of semantic spaces for music similarity judgement. The user de\ufb01nes her own semantic 1 http://www.radio-locator.com/ space by supplying, coloring and labeling an arbitrary num- ber of reference songs from her own music collection. In this way, she can de\ufb01ne her personal musical concepts via examples she is familiar with. In our application, songs from personal music collections are imported in an incremental, user-feedback based man- ner: (1) A new sound clip (concept prototype) is loaded into the system and it is compared to already loaded clips with a content-based music similarity measure, (2) the soft- ware shows the user what it thinks the new music is similar to and puts a preliminary label on it, (3) the user re\ufb01nes the software suggestion by correcting the label, (4) go back to (1). When the user decides that the concepts he or she is interested in are suf\ufb01ciently well represented, unknown music from internet radio streams can be compared to the sound prototypes (using the same similarity metric), and a visualization is generated by means of a multidimensional scaling [4] technique. The visualization also provides di- rect interaction facilities that allow the playback of streams and clips to interactively explore the audio streams audio- visually. Additionally to mapping acoustic similarity to proximity information in the 2D visualization, color is used to encode the user\u2019s view of what the music sounds like. Colors can be assigned to songs upon loading them into the system by the user. This idea is motivated by the neurologically based phe- nomenon of synesthesia [1, 14], in which stimulation of one sense automatically triggers experiences in another sense. 2 RELATED WORK Berenzweig et al. [3] construct an Anchor Space by training N classi\ufb01ers to prototypical instances of concepts, classify- ing unknown music, and mapping posterior probabilities to the individual dimensions of vectors in an N-dimensional space. Distances in this space have been shown to better re\ufb02ect the user\u2019s notion of music similarity. Lidy and Rauber [7] calculated features derived from long- term observations of (terrestrial) radio stations and used a SOM to generate a 2D visualization of the types of music 205 ISMIR 2008 \u2013 Session 2b \u2013 Music Recognition and Visualization Personal Music Collection Stream Stream Stream Distance calculation User Interface + Visualization + Playback Prototype selection Feature Extraction Feature Extraction Figure 1. System components played in different radio stations. However, they do not per- form online analysis of radio streams. Tzanetakis [13] extracted features from audio streams in real time and displayed genre membership of the streams in a real time visualization. By reducing dimensionality of the feature space with a Principal Component Analysis, he also mapped audio signals from high-dimensional timbre- similarity space into a three-dimensional visualization space. To the authors\u2019 knowledge, this was the \ufb01rst published im- plementation of online/real time audio signal classi\ufb01cation and similarity analysis. Lamere [6] uses a technique based on acoustic similarity and multidimensional scaling to visualize and explore music collections in 3D space. This work is related to our work in that it also shows the disparity between acoustic and high- level similarity. L\u00a8ubbers [9] derives prototypical songs by hierarchically clustering music collections, and proposes a multi-modal user interface for interactively exploring the cluster centers\u2019 neighborhood. Recently, some work in the \ufb01eld of Human-Computer In- teraction has been published (see also [1, 14]) that gives rise to the presumption that organizing music by color is quite intuitive. We also used this idea in our application to iden- could be a genre, a particular kind of instrumentation, a cer- tain guitar sound, and so on. 3 SYSTEM OVERVIEW To evaluate our approach, we implemented an application prototype that performs online (a) feature extraction, (b) similarity calculation, and (c) visualization. Figure 1 sketches the main components of the application. The feature extraction subsystems are responsible for ex- tracting feature frames from of\ufb02ine clips and online streams of audio data. The distribution of these feature frames is then modeled as a single Gaussian with full covariance [10] per clip/stream. Central to our framework is the similarity calculation com- ponent that calculates distance matrices holding clip-to-clip and stream-to-clip similarities by calculating the symmetric Kullback-Leibler (KL) divergence [5] between each pair of Gaussians. Distances between clips are then projected to a 2D visu- alization space with a Multidimensional Scaling [4, 2] algo- rithm, whereas streams are linearly embedded into a local subspace spanned by the 3 nearest neighbors of a stream model in feature space.",
        "zenodo_id": 1416362,
        "dblp_key": "conf/ismir/GasserFW08"
    },
    {
        "title": "Hubs and Homogeneity: Improving Content-Based Music Modeling.",
        "author": [
            "Mark Godfrey",
            "Parag Chordia"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415154",
        "url": "https://doi.org/10.5281/zenodo.1415154",
        "ee": "https://zenodo.org/records/1415154/files/GodfreyC08.pdf",
        "abstract": "We explore the origins of hubs in timbre-based song mod- eling in the context of content-based music recommenda- tion and propose several remedies. Speci\ufb01cally, we \ufb01nd that a process of model homogenization, in which certain components of a mixture model are systematically removed, improves performance as measured against several ground- truth similarity metrics. Extending the work of Aucouturier, we introduce several new methods of homogenization. On a subset of the uspop data set, model homogenization im- proves artist R-precision by a maximum of 3.5% and agree- ment to user collection co-occurrence data by 7.4%. We also explore differences in the effectiveness of the various homogenization methods for hub reduction. Further, we ex- tend the modeling of frame-based MFCC features by using a kernel density estimation approach to non-parametric mod- eling. We \ufb01nd that such an approach signi\ufb01cantly reduces the number of hubs (by 2.6% of the dataset) while improv- ing agreement to ground-truth by 5% and slightly improving artist R-precision as compared with the standard parametric model. 1 INTRODUCTION Content-based music similarity is a promising but under- developed approach to automatic music recommendation. To date, most work in this area has been focused on cal- culating similarity through comparison of song-level statis- tical models. However, such systems have thus far yielded only limited results [2], regardless of modeling method. It is thought that this may be connected to the existence of \u201chubs\u201d, songs that are found to be inaccurately similar to a large number of songs in a database. The origin of these hubs has been conjectured, yet no clear strategy for combat- ing them has been established. We conjecture that some songs are modeled particularly poorly, in effect leaving them far from other songs in the database and thus unlikely to be recommended. These songs, which we call \u201canti-hubs\u201d, are shown to be identi\ufb01able from certain properties of their models. In this paper, we propose a method to systematically reduce the incidence of anti-hubs. Another modeling approach suggested by the goal of hub re- duction was also explored. 2 METHODOLOGY",
        "zenodo_id": 1415154,
        "dblp_key": "conf/ismir/GodfreyC08"
    },
    {
        "title": "Tonal Pitch Step Distance: a Similarity Measure for Chord Progressions.",
        "author": [
            "W. Bas de Haas",
            "Remco C. Veltkamp",
            "Frans Wiering"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1418069",
        "url": "https://doi.org/10.5281/zenodo.1418069",
        "ee": "https://zenodo.org/records/1418069/files/HaasVW08.pdf",
        "abstract": "The computational analysis of musical harmony has received a lot of attention the last decades. Although it is widely rec- ognized that extracting symbolic chord labels from music yields useful abstractions, and the number of chord label- ing algorithms for symbolic and audio data is steadily grow- ing, surprisingly little effort has been put into comparing sequences of chord labels. This study presents and tests a new distance function that measures the difference between chord progressions. The presented distance function is based on Lerdahl\u2019s Tonal Pitch Space [10]. It compares the harmonic changes of two sequences of chord labels over time. This distance, named the Tonal Pitch Step Distance (TPSD), is shown to be effec- tive for retrieving similar jazz standards found in the Real Book [3]. The TPSD matches the human intuitions about harmonic similarity which is demonstrated on a set of blues variations. 1 INTRODUCTION Among musicians and music researchers, harmony is con- sidered a fundamental aspect of western tonal music. For centuries, analysis of harmony has aided composers and per- formers in understanding the tonal structure of music. The chord structure of a piece alone reveals modulations, tonal ambiguities, tension and release patterns, and song structure [11]. Not surprisingly, the modeling of tonality and compu- tational harmonic analysis have become important areas of interest in music research. Such models can play an impor- tant role in content based music information retrieval (MIR). There are obvious bene\ufb01ts in retrieval methods based on har- monic similarity. Melodies are often accompanied by a sim- ilar or identical chord progression, or songs may belong to a class with a speci\ufb01c harmonic structure, e.g. blues or rhythm changes, but also cover songs or variations over standard basses in baroque instrumental music could be identi\ufb01ed by their harmony. In this article we present a method for matching two se- quences of symbolic chord labels that is based on Lerdahl\u2019s Tonal Pitch Space (TPS). Lerdahl [10] developed a formal music-theoretic model that correlates well with data from psychological experiments and uni\ufb01es the treatment of pit- ches, chords and keys within a single model. Our proposed method uses this model to create step functions that repre- sent the change of harmonic distance in TPS over time. Two step functions can be ef\ufb01ciently compared using an algo- rithm designed by Aloupis et al. [2]. Therefore the proposed measure is named the Tonal Pitch Step Distance (TPSD). Contribution: We introduce a new distance function in the domain of polyphonic music that measures the differ- ence between chord progressions. It is key invariant, inde- pendent of the sequences\u2019 lengths, allows for partial match- ing, can be computed ef\ufb01ciently, is based on a cognitive model of tonality, and matches human intuitions about har- monic similarity. We illustrate the soundness of the distance measure by applying it on a set of blues variations. The ef\ufb01- cacy for retrieval purposes is demonstrated in an experiment on 388 Real Book [3] songs. 2 RELATED WORK Theoretical models of tonality have a long tradition in mu- sic theory and music research. The \ufb01rst geometric represen- tations of tonality date back at least two centuries. Some authors have investigated the formal mathematical proper- ties of harmonic structures [14], but of particular interest for the current research are the models grounded in data from psychological experiments (see for reviews, [7] [8]). A notable model is Chew\u2019s [5] spiral array. The Spiral Ar- ray is founded on music-theoretical principles (the Rieman- nian Tonnetz) and, as the name suggests, places pitches, chords and keys on a spiral. Chords are represented as three- dimensional shapes within the spiral. Despite the fact that distances between pitches are incompatible with empirical \ufb01ndings [9] [10], Chew\u2019s model has yielded some useful and theoretically interesting algorithms. Another important model is Lerdahl\u2019s TPS [10], which correlates reasonably well with Krumhansl\u2019s empirical data [9] and matches music- theoretical intuitions. TPS serves as a basis of the distance function here presented and will be explained in the next section. A related problem that has gained a lot of attention as well is the problem of automatic chord labeling. Chord la- beling is the task of \ufb01nding the right segmentation and la- 51 ISMIR 2008 \u2013 Session 1a \u2013 Harmony (a) octave (root) level: 0 (0) (b) \ufb01fths level: 0 7 (0) (c) triadic (chord) level: 0 4 7 (0) (d) diatonic level: 0 2 4 5 7 9 11 (0) (e) chromatic level: 0 1 2 3 4 5 6 7 8 9 10 11 (0) Table 1. The basic space of the tonic chord in the key of C Major (C = 0, C# = 1, . . . , B = 11), from Lerdahl [10]. bels for a musical piece. A chord label consists of a chord root, triadic quality, inversion, and extensions (additional chord notes). Nowadays, several algorithms can correctly segment and label approximately 80 percent of a symbolic dataset (see for reviews [20] [16]). Within the audio domain, hidden Markov Models are frequently used for chord label assignment [17] [18]. It is widely accepted that chord in- formation from symbolic or audio data yields a relevant and musicological valid abstraction that can aid in discerning the structure of a piece and making similarity judgments. Both research areas previously touched upon are impor- tant matters when it comes to MIR. Although the value of chord descriptions is generally recognized, and various mod- els about the cognition of tonality are available, surprisingly little research has focused on how similar chord sequences relate to each other. Attempts include string matching [12] as a measure of similarity, and analyzing a chord sequences on the basis of rewrite rules [15] [19]. Mauch [13] analyzed the frequencies of chord classes, chord progression patterns within the Real Book data [3] that is used in this study as well. Still, we argue that similarity of chord sequences is underexposed within the MIR \ufb01eld, which is also evident from the fact that there is no MIREX track for chord pro- gression similarity. 3 TONAL PITCH SPACE The TPS is a model of tonality that \ufb01ts human intuitions and is supported by empirical data from psychology [9] 1 . The TPS model can be used to calculate the distances between all possible chords and to predict corresponding tension and release patterns. Although the TPS can be used for de\ufb01ning relationships between chords in different keys, it is more suitable for calculating distances within local harmonies [4]. Therefore our here presented distance measure only utilizes the parts of TPS needed for calculating the chordal distances within a given key (this is motivated in section 4). The basis of the TPS model is the basic space (see Ta- ble 1) which comprises \ufb01ve hierarchical levels consisting of pitch class subsets ordered from stable to unstable. Pitch classes are categories that contain all pitches one or more octaves apart. The \ufb01rst and most stable level (a) is the root level, containing only the root of the analyzed chord. The 1 The TPS is an elaborate model; due to space limitation we have to refer to [10], chapter 2, pages 47 to 59, for a more detailed explanation and additional examples. next level (b) adds the \ufb01fth of the chord. The third level (c) is the triadic level containing all pitch classes of the chord. The fourth (d) level is the diatonic level consisting of all pitch classes of the diatonic scale of the current key. The last and least stable level (e) is the chromatic level contain- ing all pitch classes. The shape of the basic space of C major strongly resembles Krumhansl and Kessler\u2019s [9] C major- key pro\ufb01le. For every chord change, the levels (a-c) must be adapted properly and for a change of key, level d must be adapted. The basic space is hierarchical: if a pitch class is present at a certain level, it is also present at subsequent levels. The basic spaces of chords can be used to calculate dis- tances between these chords. First, the basic space is set to match the key of the piece (level d). Then the levels (a-c) can be adapted to match the chords to be compared. The distance between two chords is calculated by applying the Chord distance rule. Some examples of calculation are given in Tables 2 and 3. The proposed distance measure uses a Chord distance rule that is slightly different from the Chord distance rule de\ufb01ned in TPS [10] and is de\ufb01ned as follows: CHORD DISTANCE RULE: d(x, y) = j + k, where d(x, y) is the distance between chord x and chord y. j is the minimal number of appli- cations of the Circle-of-\ufb01fths rule in one direc- tion needed to shift x into y. k is the number of non-common pitch classes in the levels (a-d) within the basic spaces of x and y together di- vided by two 2 . A pitch class is non-common if it is present in x or y but not in both chords. CIRCLE-OF-FIFTHS RULE: move the levels (a-c) four steps to the right or four steps to the left (modulo 7) on level d 3 . When plotted geometrically, the distances exhibit a reg- ular pattern combining the diatonic circle of \ufb01fths horizon- tally and the common tone circle vertically (see Figure 1). If the chordal space is extended, it forms a toroidal structure. Because we are interested in the metrical properties of the chord distance rule it is good to observe that it has a maxi- mum of 13 (see Table 4) 4 . This maximum can be obtained, for instance, by calculating the distance between a C major chord and an E chord containing all notes of the chromatic scale. 2 This calculation of k deviates from the calculation described in [10]. Lerdahl de\ufb01ned k as the number of non-common pitch classes in the levels (a-d) within the basic space of y compared to those in the basic space of x. This de\ufb01nition has the undesirable side effect of making the distance func- tion non-symmetrical. Our proposed calculation preserves the symmetry of the distance function and yields equal or similar scores. 3 If the chord root is non-diatonic j receives the maximum penalty of 3. 4 Chords with a TPS score of 13 are not musically realistic, but it is useful from a computational point of view to observe that the TPS, and hence the TPSD, has a maximum score. 52 ISMIR 2008 \u2013 Session 1a \u2013 Harmony 0 7 0 2 7 0 2 4 5 7 11 0 2 4 5 7 9 11 0 1 2 3 4 5 6 7 8 9 10 11 Table 2. The distance between the C and G7 in the context of a C major key. The bold numbers in the basic space are speci\ufb01c for the C major chord and the underlined numbers are speci\ufb01c for the G7 chord. The only common pitch class is G (7). All other 9 pitch classes in the levels (a-c) are non- common, therefore k = 9",
        "zenodo_id": 1418069,
        "dblp_key": "conf/ismir/HaasVW08"
    },
    {
        "title": "Melody Expectation Method Based on GTTM and TPS.",
        "author": [
            "Masatoshi Hamanaka",
            "Keiji Hirata 0001",
            "Satoshi Tojo"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416822",
        "url": "https://doi.org/10.5281/zenodo.1416822",
        "ee": "https://zenodo.org/records/1416822/files/HamanakaHT08.pdf",
        "abstract": "A method that predicts the next notes is described for assisting musical novices to play improvisations. Melody prediction is one of the most difficult problems in musical information retrieval because composers and players may or may not create melodies that conform to our expectation. The development of a melody expectation method is thus important for building a system that supports musical novices because melody expectation is one of the most basic skills for a musician. Unlike most previous prediction methods, which use statistical learning, our method evaluates the appropriateness of each candidate note from the view point of musical theory. In particular, it uses the concept of melody stability based on the generative theory of tonal music (GTTM) and the tonal pitch space (TPS) to evaluate the appropriateness of the melody. It can thus predict the candidate next notes not only from the surface structure of the melody but also from the deeper structure of the melody acquired by GTTM and TPS analysis. Experimental results showed that the method can evaluate the appropriateness of the melody sufficiently well.",
        "zenodo_id": 1416822,
        "dblp_key": "conf/ismir/HamanakaHT08"
    },
    {
        "title": "A New Music Database Describing Deviation Information of Performance Expressions.",
        "author": [
            "Mitsuyo Hashida",
            "Toshie Matsui",
            "Haruhiro Katayose"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416418",
        "url": "https://doi.org/10.5281/zenodo.1416418",
        "ee": "https://zenodo.org/records/1416418/files/HashidaMK08.pdf",
        "abstract": "We introduce the CrestMuse Performance Expression Data- base (CrestMusePEDB), a music database that describes mu- sic performance expression and is available for academic re- search. While music databases are being provided as MIR technologies continue to progress, few databases deal with performance expression. We constructed a music expres- sion database, CrestMusePEDB. It may be utilized in the research \ufb01elds of music informatics, music perception and cognition, and musicology. It will contain music expression information on virtuosis\u2019 expressive performances, includ- ing those of 3 to 10 players at a time, on about 100 pieces of classical Western music. The latest version of the database, CrestMusePEDB Ver. 2.0, is available. The paper gives an overview of CrestMusePEDB. 1 INTRODUCTION Constructing music databases is one of the most important themes in the \ufb01eld of music studies. The importance of mu- sic databases has been recognized through the progress of music information retrieval technologies and benchmarks. Since the year 2000, some large-scale music databases have been created, and they have had a strong impact on the global research area [1, 2, 3]. Corpora of scores, audio recordings, and information on books of composers and musicians have been collected and used in the analysis of music styles, structures, and per- formance expressions. Meta-text information, such as the names of composers and musicians, has been attached to large-scale digital databases and been used in the analysis of music styles, structures, and performance expressions from the viewpoint of social \ufb01ltering. In spite of there being many active music database projects, few projects have dealt with music performance expression, such as dynamics, tempo, and the progression of pitch. The performance expression plays an important role in formulating impressions of music. Providing a performance expression database, especially de- scribing deviation information from neutral expression, can be used as a research in MIR \ufb01elds. In musicological analysis, some researchers construct a database of the transition data of pitch and loudness and then use the database through statistical processing. Widmer et al. analyzed deviation of tempi and dynamics of each beat from Horowitz\u2019s piano performances [4]. Sapp et al., work- ing on the Mazurka Project [5], collected as many record- ings of Chopin mazurka performances as possible in order to analyze deviations of tempo and dynamics by each beat in a similar manner to Widmer. Their researches are expected to provide the fundamental knowledge in the quantitative analysis of music style and performance expression. How- ever, their archives are focused on the speci\ufb01ed composer and player but ignored all other types of music. And they are not enough to be covered with the description of the de- viation of duration and loudness of each note from score. Toyoda et al. have provided a performance expression devi- ation database which is aimed to describe those of individ- ual note [6]. But their data format is limited to performances recorded by MIDI signals not audio signals. However, most of the analysis of classical Western music expression is de- termined by the researchers\u2019 acute musical sense; even less research has quantitatively analyzed music performance ex- pression based on data [7, 8, 9, 10, 11]. One of the reasons for this absence of data is that this qualitative research has been based on the \ufb01eld methods of music performance edu- cation up until now. MIDI \ufb01les of performances are proli\ufb01c on the Internet; however, very few of these are actually created by profes- sional musicians, but rather by students and amateurs. Also, searching with the precise details on the performances is too dif\ufb01cult, although the information on each piece (e.g., the title, composer, genre, and the composition of the musical instruments) is rich. A database, which describes outlines of changes in tempo, dynamics, the delicate control of each note, the deviation re- garding starting time and duration to existing virtuoso per- formances in the form of acoustic signals as time-series data, can be used for new studies in the \ufb01elds of music informat- ics, musicology and music education. To that end, we con- structed a performance expression database, \u2018CrestMuse- PEDB,\u2019 that covers classical Western music, especially pi- ano pieces performed by famous professionals. 489 ISMIR 2008 \u2013 Session 4b \u2013 Musical Expression and Meaning 2 CONSIDERATIONS IN THE DATABASE DESIGN",
        "zenodo_id": 1416418,
        "dblp_key": "conf/ismir/HashidaMK08"
    },
    {
        "title": "Content-Based Musical Similarity Computation using the Hierarchical Dirichlet Process.",
        "author": [
            "Matthew D. Hoffman",
            "David M. Blei",
            "Perry R. Cook"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417223",
        "url": "https://doi.org/10.5281/zenodo.1417223",
        "ee": "https://zenodo.org/records/1417223/files/HoffmanBC08.pdf",
        "abstract": "We develop a method for discovering the latent structure in MFCC feature data using the Hierarchical Dirichlet Process (HDP). Based on this structure, we compute timbral simi- larity between recorded songs. The HDP is a nonparametric Bayesian model. Like the Gaussian Mixture Model (GMM), it represents each song as a mixture of some number of multivariate Gaussian distributions However, the number of mixture components is not \ufb01xed in the HDP, but is deter- mined as part of the posterior inference process. Moreover, in the HDP the same set of Gaussians is used to model all songs, with only the mixture weights varying from song to song. We compute the similarity of songs based on these weights, which is faster than previous approaches that com- pare single Gaussian distributions directly. Experimental re- sults on a genre-based retrieval task illustrate that our HDP- based method is both faster and produces better retrieval quality than such previous approaches. 1 INTRODUCTION We develop a new method for estimating the timbral sim- ilarity between recorded songs. Our technique is based on the hierarchical Dirichlet process, a \ufb02exible Bayesian model for uncovering latent structure in high-dimensional data. One approach to computing the timbral similarity of two songs is to train a single Gaussian or a Gaussian Mixture Model (GMM) on the Mel-Frequency Cepstral Coef\ufb01cient (MFCC) feature vectors for each song and compute (for the single Gaussian) or approximate (for the GMM) the Kullback-Leibler (K-L) divergence between the two models [1]. The basic single Gaussian approach with full covariance matrix (\u201cG1\u201d [2]) has been successful, forming the core of the top-ranked entries to the MIREX similarity evaluation task two years running [3, 4]. Although MFCC data are not normally distributed within songs, using a richer model such as the GMM to more ac- curately represent their true distribution provides little or no improvement in numerous studies [2, 5, 1]. This sug- gests that a \u201cglass ceiling\u201d has been reached for this type of representation. Moreover, the computational cost of the Monte Carlo estimation procedure involved in comparing two GMMs is orders of magnitude more than that incurred by computing the K-L divergence between two single Gaus- sians exactly. This is a very signi\ufb01cant issue if we want to compute similarity matrices for large sets of songs, since the number of comparisons between models that must be done grows quadratically with the number of songs. Another approach [6] produced results statistically indis- tinguishable from the other top algorithms in MIREX 2007 by using a mid-level semantic feature representation to com- pute similarity. Using painstakingly human-labeled data, Barrington et al. trained GMMs to estimate the posterior likelihood that a song was best characterized by each of 146 words. These models then produced a vector for each test song de\ufb01ning a multinomial distribution over the 146 se- mantic concepts. To compute the dissimilarity of two songs, the K-L divergence between these multinomial distributions for the songs was computed. The success of this method suggests that alternative sta- tistical representations of songs are worth exploring. Rather than take a supervised approach requiring expensive hand- labeled data, we make use of the Hierarchical Dirichlet Pro- cess (HDP), which automatically discovers latent structure within and across groups of data (songs, in our case). This latent structure generates a compact alternative representa- tion of each song, and the model provides a natural and ef- \ufb01cient way of comparing songs using K-L divergence. 2 HDP-BASED SIMILARITY USING LATENT FEATURES The hierarchical Dirichlet process (HDP) is an extension of the Dirichlet process (DP), a nonparametric Bayesian model of mixtures of an unknown number of simple densities. We \ufb01rst outline the DP and then describe how we model songs with an HDP.",
        "zenodo_id": 1417223,
        "dblp_key": "conf/ismir/HoffmanBC08"
    },
    {
        "title": "Beat Tracking using Group Delay Based Onset Detection.",
        "author": [
            "Andre Holzapfel",
            "Yannis Stylianou"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415516",
        "url": "https://doi.org/10.5281/zenodo.1415516",
        "ee": "https://zenodo.org/records/1415516/files/HolzapfelS08.pdf",
        "abstract": "This paper introduces a novel approach to estimate onsets in musical signals based on the phase spectrum and specif- ically using the average of the group delay function. A frame-by-frame analysis of a music signal provides the evo- lution of group delay over time, referred to as phase slope function. Onsets are then detected simply by locating the positive zero-crossings of the phase slope function. The pro- posed approach is compared to an amplitude-based onset de- tection approach in the framework of a state-of-the-art sys- tem for beat tracking. On a data set of music with less per- cussive content, the beat tracking accuracy achieved by the system is improved by 82% when the suggested phase-based onset detection approach is used instead of the amplitude- based approach, while on a set of music with stronger per- cussive characteristics both onset detection approaches pro- vide comparable results of accuracy. 1 INTRODUCTION The task of estimating the times at which a human would tap his foot to a musical sound is known as beat tracking [1]. All state-of-the-art approaches ([1, 2, 3, 4]) for this task \ufb01rst conduct an onset detection. The output of the onset de- tection is a signal with a lower time resolution than the input signal, which has peaks at the time instances where a musi- cal instrument in the input started playing a note. Usually, this onset signal is derived from the amplitude of the signal, as in [1, 2, 3]. Only in [4], phase information is consid- ered, by computing the phase deviation between neighbor- ing analysis frames. Several approaches of computing on- sets from musical signals are compared in [5], resulting in the conclusion that in general the moments of big positive change in the amplitude spectrum of the signal provide the most preferable estimators for the onsets. This is because using information contained in the complex spectrum or in the phase changes might lead to similar onset estimations, but using only the amplitude spectrum is preferred for com- putational reasons [5]. A similar conclusion can be drawn from the results of the MIREX 2007 Audio Onset Detection contest 1 , where most systems use only the amplitude infor- 1 http://www.music-ir.org/mirex/2007/index.php/Audio Onset Detection mation. Considering the results on complex music mixtures, which are the signals of interest in beat tracking, the usage of phase deviation by some systems does not improve the onset estimation accuracy [6]. As can be seen in the results depicted in [7] (Table II), the state-of-the-art approaches for beat tracking decrease sig- ni\ufb01cantly in accuracy, when applied to folk music. These music signals contain weaker percussive content than music of rock or disco styles. This problem is of particular impor- tance when dealing with traditional dances as well, as they are often played using string or wind instruments only [8]. Based on the results obtained in [7], it is necessary to im- prove beat tracking on music with little percussive content. While a decrease in the case of jazz and classical music can partly be attributed to the complex rhythmic structure, rhyth- mic structure of folk music is simpler, and thus the decrease in this forms of music may be attributed solely to the prob- lem of detecting onsets. In [9], the negative derivative of the unwrapped phase, i.e. the group delay, is used to determine instants of signi\ufb01cant excitation in speech signals. This approach has been fur- ther developed and used for the detection of clicks of ma- rine mammals in [10]. There, it has been shown that pulses can be reliably detected by using group delay, even when the pulses have been \ufb01ltered by a minimum phase system. Moti- vated by these works, we suggest the use of the group delay function for onset estimation in musical signals, and then use this estimation as input to a beat tracker based on the state-of-the-art system presented in [2]. The goal of this pa- per is to provide an improved beat tracking performance on musical signals with simple rhythmic structure and little or no percussive content. The proposed approach to consider phase information is novel in Music Information Retrieval, as previous approaches computed a time derivative of phase [5], while the suggested approach makes use of group de- lay, which is a derivative of phase over frequency. The group delay function is computed in frame-by-frame analy- sis and its average is computed for each frame. This results in time-domain signal referred to as phase slope function [10]. Onsets are then simply estimated by detecting the pos- itive zero-crossings of the phase slope function. Therefore, the suggested approach does not require the use of time- dependent (adaptive) energy-based thresholds as the ampli- 653 ISMIR 2008 \u2013 Session 5c \u2013 Rhythm and Meter tude and phase based approaches for detecting the onsets [5]. In Section 2, the characteristics of the group delay function for minimum phase signals are shortly reviewed to support our motivation. In Section 3 we present a method to com- pute an onset signal for music based on the phase slope func- tion and how this information has been incorporated into the state-of-the-art system suggested by Klapuri et al. [2] for beat tracking. Section 4 shows results of the proposed approach on arti\ufb01cial and music signals comparing the sug- gested phase-based approach with a widely used amplitude- based approach. Section 5 concludes the paper and dis- cusses future work. 2 BASIS FOR THE PROPOSED METHOD Consider a delayed unit sample sequence x[n] = \u03b4[n \u2212n0] and its Fourier Transform X(\u03c9) = e\u2212j\u03c9n0. The group delay is de\ufb01ned as: \u03c4(\u03c9) = \u2212d\u03c6(\u03c9) d\u03c9 (1) so the group delay for the delayed unit sample sequence is \u03c4(\u03c9) = n0 \u2200\u03c9, since the phase spectrum of the signal is \u03c6(\u03c9) = \u2212\u03c9n0. The average over \u03c9 of \u03c4(\u03c9) provides n0 which corresponds to the negative of the slope of the phase spectrum for this speci\ufb01c signal and to the delay of the unit sample sequence. An example of a delayed unit sample se- quence with n0 = 200 samples as well as the associated group delay function are depicted in Fig.1(a) and (b), re- spectively. In the above example the Fourier Transform has been computed considering the center of analysis window to be at n = 0. When the window center is moved to the right (closer to the instant n = n0), the slope of the phase spec- trum is increased (the average of the group delay function is decreased) re\ufb02ecting the distance between the center of the analysis window and the position of the impulse. When the center of the analysis window is at n = n0 then the slope is zero. Continuing moving the analysis window to the right the slope will start to increase (while the average of the group delay will decreased). In this way, the slope of the phase spectrum is a function of n. Note that the lo- cation of the zero-crossing of this function will provide the position of the non-zero value of the unit sample sequence independently of the amplitude value of the impulse. In general, the average value of the group delay is de- termined by the distance between the center of the anal- ysis window and the delay of the unit sample sequence, even when it has been \ufb01ltered by a minimum phase system [9]. The group delay function will still provide information about this delay value as well information about the poles of the minimum phase system. In Fig. 1(c),(d) the output of the minimum phase signal and the associated group delay are depicted. The slope function will have a similar behav- ior to this described earlier for the unit sample sequence. Figure 1. (a) A delayed by 200 samples unit sample se- quence. (b) The group delay function of the signal in (a). (c) A minimum phase signal with an oscillation at \u03c0/4. (d) The group delay function of the signal in (c). In Figure 2, the phase slope of a periodic sequence of mini- mum phase signals is depicted. The dash-dotted line depicts a phase slope resulting from a window shorter than the pe- riod of the signal, the dashed line results from an analysis using a longer window. The phase slope values have been assigned to the middle of the analysis window, the analysis step size was set to one sample. It can be seen that even in the case of low signal amplitude, the positive zero crossing coincides in each case with the beginning of the minimum phase signal. As a musical instrument could be considered as a causal and stable \ufb01lter, that is driven by a minimum phase excitation, it can be assumed that an onset estimation using the positive zero crossings of the phase slope is a valid approach. To avoid the problems of unwrapping the phase spectrum of the signal for the computation of group delay, the slope of the phase function can be computed as [11]: \u03c4(\u03c9) = XR(\u03c9)YR(\u03c9) + XI(\u03c9)YI(\u03c9) |X(\u03c9)|2 (2) where X(\u03c9) = XR(\u03c9) + jXI(\u03c9) Y (\u03c9) = YR(\u03c9) + jYI(\u03c9) are the Fourier Transforms of x[n] and nx[n], respectively. The phase slope is then computed as the negative of the av- erage of the group delay function. 3 METHOD FOR BEAT TRACKING",
        "zenodo_id": 1415516,
        "dblp_key": "conf/ismir/HolzapfelS08"
    },
    {
        "title": "The 2007 MIREX Audio Mood Classification Task: Lessons Learned.",
        "author": [
            "Xiao Hu 0001",
            "J. Stephen Downie",
            "Cyril Laurier",
            "Mert Bay",
            "Andreas F. Ehmann"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416380",
        "url": "https://doi.org/10.5281/zenodo.1416380",
        "ee": "https://zenodo.org/records/1416380/files/HuDLBE08.pdf",
        "abstract": "Recent music information retrieval (MIR) research pays increasing attention to music classification based on moods expressed by music pieces. The first Audio Mood Classification (AMC) evaluation task was held in the 2007 running of the Music Information Retrieval Evaluation eXchange (MIREX). This paper describes important issues in setting up the task, including dataset construction and ground-truth labeling, and analyzes human assessments on the audio dataset, as well as system performances from various angles. Interesting findings include system performance differences with regard to mood clusters and the levels of agreement amongst human judgments regarding mood labeling. Based on these analyses, we summarize experiences learned from the first community scale evaluation of the AMC task and propose recommendations for future AMC and similar evaluation tasks.",
        "zenodo_id": 1416380,
        "dblp_key": "conf/ismir/HuDLBE08"
    },
    {
        "title": "Music, Movies and Meaning: Communication in Film-Makers&apos; Search for Pre-Existing Music, and the Implications for Music Information Retrieval.",
        "author": [
            "Charlie Inskip",
            "Andy MacFarlane",
            "Pauline Rafferty"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1418101",
        "url": "https://doi.org/10.5281/zenodo.1418101",
        "ee": "https://zenodo.org/records/1418101/files/InskipMR08.pdf",
        "abstract": "While the use of music to accompany moving images is widespread, the information behaviour, communicative practice and decision making by creative professionals within this area of the music industry is an under- researched area. This investigation discusses the use of music in films and advertising focusing on communication and meaning of the music and introduces a reflexive communication model. The model is discussed in relation to interviews with a sample of music professionals who search for and use music for their work. Key factors in this process include stakeholders, briefs, product knowledge and relevance. Searching by both content and context is important, although the final decision when matching music to picture is partly intuitive and determined by a range of stakeholders.",
        "zenodo_id": 1418101,
        "dblp_key": "conf/ismir/InskipMR08"
    },
    {
        "title": "Instrument Equalizer for Query-by-Example Retrieval: Improving Sound Source Separation Based on Integrated Harmonic and Inharmonic Models.",
        "author": [
            "Katsutoshi Itoyama",
            "Masataka Goto",
            "Kazunori Komatani",
            "Tetsuya Ogata",
            "Hiroshi G. Okuno"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417407",
        "url": "https://doi.org/10.5281/zenodo.1417407",
        "ee": "https://zenodo.org/records/1417407/files/ItoyamaGKOO08.pdf",
        "abstract": "This paper describes a music remixing interface, called In- strument Equalizer, that allows users to control the volume of each instrument part within existing audio recordings in real time. Although query-by-example retrieval systems need a user to prepare favorite examples (songs) in general, our interface gives a user to generate examples from exist- ing ones by cutting or boosting some instrument/vocal parts, resulting in a variety of retrieved results. To change the vol- ume, all instrument parts are separated from the input sound mixture using the corresponding standard MIDI \ufb01le. For the separation, we used an integrated tone (timbre) model consisting of harmonic and inharmonic models that are ini- tialized with template sounds recorded from a MIDI sound generator. The remaining but critical problem here is to deal with various performance styles and instrument bodies that are not given in the template sounds. To solve this problem, we train probabilistic distributions of timbre features by us- ing various sounds. By adding a new constraint of maxi- mizing the likelihood of timbre features extracted from each tone model, we succeeded in estimating model parameters that better express actual timbre. 1 INTRODUCTION One of promising approaches of music information retrieval is the query-by-example (QBE) retrieval [1, 2, 3, 4, 5, 6, 7] where a user can receive the list of musical pieces ranked by their similarity to a musical piece (example) that the user gives as a query. Although this approach is powerful and useful, a user has to prepare or \ufb01nd favorite examples and sometimes feels dif\ufb01culty to control/change the retrieved pieces after seeing them because the user has to \ufb01nd another appropriate example to get better results. For example, if a user feels that vocal or drum sounds are too strong in the retrieved pieces, the user has to \ufb01nd another piece that has weaker vocal or drum sounds while keeping the basic mood and timbre of the piece. It is sometimes very dif\ufb01cult to \ufb01nd such a piece within a music collection. We therefore propose yet another way of preparing an ex- ample for the QBE retrieval by using a music remixing inter- face. The interface enables a user to boost or cut the volume of each instrument part of an existing musical piece. With this interface, a user can easily give an alternative query with a different mixing balance to obtain re\ufb01ned results of the QBE retrieval. The issue in the above example of \ufb01nding another piece with weaker vocal or drum sounds can thus be resolved. Note that existing graphic equalizers or tone controls on the market cannot control each individual instru- ment part in this way: they can adjust only frequency char- acteristics (e.g., boost or cut for bass and treble). Although remixing stereo audio signals [8] had reported previously, it had tackled to control only harmonic instrument sounds. Our goal is to control all instrument sounds including both harmonic and inharmonic ones. This paper describes our music remixing interface, called Instrument Equalizer, in which a user can listen to and remix a musical piece in real time. It has sliders corresponding to different musical instruments and enables a user to ma- nipulate the volume of each instrument part in polyphonic audio signals. Since this interface is independent of the suc- ceeding QBE system, any QBE system can be used. In our current implementation, it leverages the standard MIDI \ufb01le (SMF) corresponding to the audio signal of a musical piece to separate sound sources. We can assume that it is relatively easy to obtain such SMFs from the web, etc. (especially for classical music). Of course, given a SMF, it is quite easy to control the volume of instrument parts during the SMF play- back, and readers might think that we can use it as a query. Its sound quality, however, is not good in general and users would lose their drive to use the QBE retrieval. Moreover, we believe it is important to start from an existing favorite musical piece of high quality and then re\ufb01ne the retrieved results. 2 INSTRUMENT EQUALIZER The Instrument Equalizer enables a user to remix existing polyphonic musical signals. The screenshot of its interface is shown in Figure 1 and the overall system is shown in Fig- ure 2. It has two features for remixing audio mixtures as follows: 133 ISMIR 2008 \u2013 Session 1c \u2013 Timbre Figure 1. Screenshot of main window. Figure 2. Instrument Equalizing System.",
        "zenodo_id": 1417407,
        "dblp_key": "conf/ismir/ItoyamaGKOO08"
    },
    {
        "title": "A Robust Musical Audio Search Method Based on Diagonal Dynamic Programming Matching of Self-Similarity Matrices.",
        "author": [
            "Tomonori Izumitani",
            "Kunio Kashino"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417399",
        "url": "https://doi.org/10.5281/zenodo.1417399",
        "ee": "https://zenodo.org/records/1417399/files/IzumitaniK08.pdf",
        "abstract": "We propose a new musical audio search method based on audio signal matching that can cope with key and tempo variations. The method employs the self-similarity matrix of an audio signal to represent a key-invariant structure of musical audio. And, we use dynamic programming (DP) matching of self-similarity matrices to deal with time vari- ations. However, conventional DP-based sequence match- ing methods cannot be directly applied for self-similarity matrices because they cannot treat gaps independently of other time frames. We resolve this problem by introduc- ing \u201cmatched element indices,\u201d which re\ufb02ect the history of matching, to a DP-based sequence matching method. We performed experiments using musical audio signals. The results indicate that the proposed method improves the de- tection accuracy in comparison to that that obtained by two conventional methods, namely, DP matching with chroma- based vector rotations and a simple matching of self-similarity feature vectors. 1 INTRODUCTION Musical audio search, that is, identifying the title and corre- sponding locations in an audio database from a short musi- cal audio excerpt of a query, has become important with the rapid expansion of digital content utilization. A technology known as audio \ufb01ngerprinting has been widely used to search for identical audio signals to queries. Most audio \ufb01ngerprinting systems can be applied to audio signals with noises or other signal distortions. However, it is also demanded to cope with other kinds of variations caused by different players, keys, tempi, arrangements, and so on. We focus on a musical audio search task where audio sig- nals include key and tempo variations. For this task, many approaches have been reported. An early work using the chroma-based features was reported by Nagano et al., who proposed a feature representation named the polyphonic bi- nary feature vector (PBFV) [1]. Their method deals with key variations by rotating query vector elements. They used beat detection and dynamic programming for tempo varia- tions. M\u00a8uller et al. also used chroma-based features for au- dio matching with different timbres played by various play- ers [2]. For similar purposes, cover song detection methods have been proposed. Typically, they detect the same title as the query in the database by comparing music tracks. Ellis et al. proposed a method based on chroma-based features and a beat tracking technique [3]. Tsai et al. adopted melody ex- traction and a dynamic programming (DP) based matching method, known as dynamic time warping (DTW) [4]. For the audio cover song identi\ufb01cation task of MIREX1 , sev- eral methods have been proposed and evaluated using the same test data. In the above mentioned methods, chroma-based features with element rotation and key adjustment are often used to detect transposed musical pieces. The former needs twelve matching processes and the latter need schemes for accu- rate key adjustment, for example, when key changes are in- cluded in a piece. For tempo variations, DP-based matching or time adjustment by beat tracking is commonly used. In contrast, here we deal with key variation in terms of features invariant to it. We use a self-similarity matrix that has elements de\ufb01ned by spectral (or other) similarities be- tween two different time points in an audio signal. The self-similarity matrix has been used to represents the global structure of a musical piece. Foote used it for visu- alization [5] and audio segmentation [6]. Marolt used it to detect repeated parts within a musical piece for a musical audio retrieval system [7]. A self-similarity matrix also represents local structures, such as frame-wise relationships. And, it is insensitive to key variations because the relationship between two time points within a musical audio signal tends to be kept even when the music is played in a different key. In this work, we constructed a musical audio search system using this prop- erty. In our earlier work, self-similarity was applied for mu- sical audio search with key variations using simple vector matching [8]. However, the detection accuracy degrades when tempo variations are signi\ufb01cant. As a solution to this problem, we utilize a DP-based match- ing scheme. However, conventional DP methods for se- quential data are not applicable because columns and rows of self-similarity matrices, which include gaps, have to be matched simultaneously in each step of the DP calculation. 1 http://www.music-ir.org/mirex/2007/index.php/Main Page 609 ISMIR 2008 \u2013 Session 5b \u2013 Feature Representation We propose a method for resolving this problem. The key idea is the introduction of \u201cmatched element indices,\u201d which re\ufb02ect the history of the matching process. This reduces the problem to one that conventional DP-based sequence match- ing schemes can handle. 2 METHODS",
        "zenodo_id": 1417399,
        "dblp_key": "conf/ismir/IzumitaniK08"
    },
    {
        "title": "Using Audio Analysis and Network Structure to Identify Communities in On-Line Social Networks of Artists.",
        "author": [
            "Kurt Jacobson",
            "Mark B. Sandler",
            "Benjamin Fields"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417867",
        "url": "https://doi.org/10.5281/zenodo.1417867",
        "ee": "https://zenodo.org/records/1417867/files/JacobsonSF08.pdf",
        "abstract": "Community detection methods from complex network the- ory are applied to a subset of the Myspace artist network to identify groups of similar artists. Methods based on the greedy optimization of modularity and random walks are used. In a second iteration, inter-artist audio-based simi- larity scores are used as input to enhance these community detection methods. The resulting community structures are evaluated using a collection of artist-assigned genre tags. Evidence suggesting the Myspace artist network structure is closely related to musical genre is presented and a Semantic Web service for accessing this structure is described. 1 INTRODUCTION The dramatic increase in popularity of online social net- working has led hundreds of millions of individuals to pub- lish personal information on the Web. Music artists are no exception. Myspace 1 has become the de-facto standard for web-based music artist promotion. Although exact \ufb01g- ures are not made public, recent blogosphere chatter sug- gests there are well over 7 million artist pages 2 on Mys- pace. Myspace artist pages typically include some stream- ing audio and a list of \u201cfriends\u201d specifying social connec- tions. This combination of media and a user-speci\ufb01ed social network provides a unique data set that is unprecedented in both scope and scale. However, the Myspace network is the result of hundreds of millions individuals interacting in a virtually unregulated fashion. Can this crowd-sourced tangle of social network- ing ties provide insights into the dynamics of popular mu- sic? Does the structure of the Myspace artist network have any relevance to music-related studies such as music recom- mendation or musicology? In an effort to answer these questions, we identify com- munities of artists based on the Myspace network topology and attempt to relate these community structures to musical 1 http://myspace.com 2 http://scottelkin.com/archive/2007/05/11/ Myspace-Statistics.aspx \u223c25 million songs, \u223c3.5 songs/artist, \u223c7 million artists genre. To this end, we examine a sample of the Myspace so- cial network of artists. First we review some previous work on the topics of artist networks, audio-based music anal- ysis, and complex network community identi\ufb01cation. We then describe our methodology including our network sam- pling method in Section 3.1 and our community detection approaches in Section 3.2. In Section 3.3 we describe the concept of genre entropy - a metric for evaluating the rele- vance of these community structures to music. Finally, we include a discussion of the results, suggestions for future work, and describe a Semantic Web service that can be used to access some of the data in a structured format. 2 BACKGROUND",
        "zenodo_id": 1417867,
        "dblp_key": "conf/ismir/JacobsonSF08"
    },
    {
        "title": "MoodSwings: A Collaborative Game for Music Mood Label Collection.",
        "author": [
            "Youngmoo E. Kim",
            "Erik M. Schmidt",
            "Lloyd Emelle"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416586",
        "url": "https://doi.org/10.5281/zenodo.1416586",
        "ee": "https://zenodo.org/records/1416586/files/KimSE08.pdf",
        "abstract": "There are many problems in the \ufb01eld of music infor- mation retrieval that are not only dif\ufb01cult for machines to solve, but that do not have well-de\ufb01ned answers. In labeling and detecting emotions within music, this lack of speci\ufb01city makes it dif\ufb01cult to train systems that rely on quanti\ufb01ed la- bels for supervised machine learning. The collection of such \u201cground truth\u201d data for these subjectively perceived features necessarily requires human subjects. Traditional methods of data collection, such as the hiring of subjects, can be \ufb02awed, since labeling tasks are time-consuming, tedious, and expensive. Recently, there have been many initiatives to use customized online games to harness so-called \u201cHuman Computation\u201d for the collection of label data, and several such games have been proposed to collect labels spanning an excerpt of music. We present a new game, MoodSwings (http://schubert.ece.drexel.edu/moodswings), which differs in that it records dynamic (per-second) labels of players\u2019 mood ratings of music, in keeping with the unique time- varying quality of musical mood. As in prior collaborative game approaches, players are partnered to verify each oth- ers\u2019 results, and the game is designed to maximize consensus- building between users. We present preliminary results from an initial set of game play data. 1 INTRODUCTION The detection and labeling of the emotional content (mood) of music is one of many music information retrieval prob- lems without a clear \u201cground truth\u201d answer. The lack of eas- ily obtained ground truth for these kinds of problems further complicates the development of automated solutions, since classi\ufb01cation methods often employ a supervised learning approach relying on such ground truth labels. The collec- tion of this data on subjectively perceived features, such as musical mood, necessarily requires human subjects. But tra- ditional methods of data collection, such as the hiring of subjects, have their share of dif\ufb01culties since labeling tasks can be time-consuming, tedious, error-prone and expensive. Recently, a signi\ufb01cant amount of attention has been placed on the use of collaborative online games to collect such ground truth labels for dif\ufb01cult problems, harnessing so- called \u201cHuman Computation\u201d. For example, von Ahn et al. have created several such games for image labeling: the ESP Game, Peekaboom [1], and Phetch. More recently, several such games have been been proposed for the collection of music data, such as MajorMiner [2], Listen Game [3], and TagATune [4]. These implementations have primarily fo- cused on the collection of descriptive labels for a relatively short audio clip. We present a new game, MoodSwings, designed to ex- plore the unique time-varying nature of musical mood. Of course, one of the joys of music is that the mood of a piece may change over time, gradually or suddenly. According to Huron [5], this combination of anticipation and surprise may be at the core of our enjoyment of music. Thus, our game is targeted at collecting dynamic (per-second) labels of users\u2019 mood ratings, which are collected in real-time as a player hears the music using the two-dimensional grid of emotional components: valence and arousal. As in other collaborative games, players are partnered in order to verify each others\u2019 results, providing a strong incentive for pro- ducing high-quality labels that others can agree upon. Ac- cordingly, game scoring is designed to maximize consensus- building between partners. In this paper, we present data from an initial pilot phase of the game and demonstrate the utility of this approach for the collection of high-quality, dy- namic labels of musical affect. 2 BACKGROUND Models of affect and the categorization and labeling of spe- ci\ufb01c emotions has received signi\ufb01cant attention from a vari- ety of research areas including psychology, physiology, neu- roscience, as well as musicology. With the advent of digital music and very large music collections, recent work has fo- cused on the problem of automatic music mood detection. Next, we brie\ufb02y summarize some of the related work.",
        "zenodo_id": 1416586,
        "dblp_key": "conf/ismir/KimSE08"
    },
    {
        "title": "A Framework for Automated Schenkerian Analysis.",
        "author": [
            "Phillip B. Kirlin",
            "Paul E. Utgoff"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415892",
        "url": "https://doi.org/10.5281/zenodo.1415892",
        "ee": "https://zenodo.org/records/1415892/files/KirlinU08.pdf",
        "abstract": "In Schenkerian analysis, one seeks to \ufb01nd structural de- pendences among the notes of a composition and organize these dependences into a coherent hierarchy that illustrates the function of every note. This type of analysis reveals multiple levels of structure in a composition by construct- ing a series of simpli\ufb01cations of a piece showing various elaborations and prolongations. We present a framework for solving this problem, called IVI, that uses a state-space search formalism. IVI includes multiple interacting compo- nents, including modules for various preliminary analyses (harmonic, melodic, rhythmic, and cadential), identifying and performing reductions, and locating pieces of the Ur- satz. We describe a number of the algorithms by which IVI forms, stores, and updates its hierarchy of notes, along with details of the Ursatz-\ufb01nding algorithm. We illustrate IVI\u2019s functionality on an excerpt from a Schubert piano composi- tion, and also discuss the issues of subproblem interactions and the multiple parsings problem. 1 SCHENKERIAN ANALYSIS A number of types of music analysis are concerned with \u201cla- beling\u201d individual objects in a musical score. In harmonic analysis, labels are assigned to chords and notes correspond- ing to harmonic function; in contrapuntal voice segregation, labels are assigned to notes indicating voice assignment. A rhythmic analysis may assign different levels of metrical im- portance to notes. These styles of analysis are similar in that they often describe musical components in isolation, or only in relation to their immediate neighbors on the musical sur- face. Structural analysis, on the other hand, emphasizes dis- covering relationships among notes and chords in a com- position, rather than studying individual tones in a vacuum. The word \u201cstructure\u201d here refers to \u201cthe complete fabric of the composition as established by melody, counterpoint, and harmony in combination\u201d [1]. Schenkerian analysis is the most well-developed type of structural analysis. This type of analysis examines the \u201cin- terrelationships among melody, counterpoint, and harmony\u201d [1] in a hierarchical manner. Schenker\u2019s theory of music al- lows one to determine which notes in a passage of music are more structurally signi\ufb01cant than others. It is important not to confuse \u201cstructural signi\ufb01cance\u201d with \u201cmusical im- portance;\u201d a musically important note (e.g., crucial for artic- ulating correctly in a performance) can be a very insigni\ufb01- cant tone from a structural standpoint. Judgments regarding structural importance result from \ufb01nding dependences be- tween notes or sets of notes: if a note X derives its musical function or meaning from the presence of another note Y , then X is dependent on Y and Y is deemed more structural than X. The process of completing a Schenkerian analysis pro- ceeds in a recursive manner. Starting from the musical score of a composition, one may locate any number of structural dependences. After no more can be found, an abstracted score is produced by rearranging or removing the less struc- tural notes. The new abstracted score will reveal new de- pendences: when their subservient neighbors are moved or eliminated, various structurally important notes in the origi- nal score will be deemed less signi\ufb01cant to their new neigh- bors. This iterative process illustrates Schenker\u2019s conception that tonal compositions consist of a \u201ccontinuum of interre- lated structural levels,\u201d [1] where each structural level of the music represents that composition at a different level of ab- level of the piece is the background level. At the other end of the spectrum is the foreground level: an analysis at this level usually still contains most of the notes of the score and most closely represents the musical surface. Between these levels is the middleground level. While Schenker\u2019s own analyses usually only contain these three levels, there can be many levels in between the surface level music and the ultimate",
        "zenodo_id": 1415892,
        "dblp_key": "conf/ismir/KirlinU08"
    },
    {
        "title": "Oh Oh Oh Whoah! Towards Automatic Topic Detection In Song Lyrics.",
        "author": [
            "Florian Kleedorfer",
            "Peter Knees",
            "Tim Pohle"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416154",
        "url": "https://doi.org/10.5281/zenodo.1416154",
        "ee": "https://zenodo.org/records/1416154/files/KleedorferKP08.pdf",
        "abstract": "We present an algorithm that allows for indexing music by topic. The application scenario is an information retrieval system into which any song with known lyrics can be in- serted and indexed so as to make a music collection browse- able by topic. We use text mining techniques for creating a vector space model of our lyrics collection and non-negative matrix factorization (NMF) to identify topic clusters which are then labeled manually. We include a discussion of the decisions regarding the parametrization of the applied meth- ods. The suitability of our approach is assessed by measur- ing the agreement of test subjects who provide the labels for the topic clusters. 1 INTRODUCTION The past few years have seen a tremendous amount of scien- ti\ufb01c work in the \ufb01eld of music information retrieval. Much, if not most of this work was concentrated on the acoustic properties of music, while the semantic content conveyed by the words of the songs was mostly ignored. There has been some work on song lyrics, but the \ufb01eld has not gained momentum comparable to work on acoustic features of mu- sic. In much of the older work lyrics were used to side or contrast with acoustic-based techniques in genre classi\ufb01ca- tion of songs [8] or artists [7]. In newer work, lyrics have become sources for metadata generation [9] and, probably inspired by the evolution of Web 2.0, lyrics were found use- ful as a basis for keyword generation for songs, a technique that may ultimately lead to automatic tagging [12]. In our view, any music browsing or recommendation sys- tem is incomplete if it does not incorporate the dimension of the songs\u2019 semantic content. It is therefore our goal to cre- ate elements of such a system based on the analysis of song lyrics. In the work at hand, we present the building blocks of a system that allows for searching a collection of lyrics by selecting from a set of topics. We describe the formal details of the procedure which we propose for making a collection of songs browseable by topic. Centrally to our algorithm, we apply NMF for clustering the lyrics documents as pro- posed in [13]. The resulting clusters are labeled manually so that they can be used as the dimensions of a topic space in which the documents are arranged. The remainder of this paper is organized as follows: Sec- tion 2 describes the dataset we work with, in Section 3 we explain the details of our algorithm. Section 4 gives an ac- count of the quality assessment we performed. 2 THE DATA The music archive we work with is a subset of the collec- tion marketed through Verisign Austria\u2019s 1 content down- load platform. This subset comprises approximately 60.000 of the most popular audio tracks 2 by some 6.000 artists. The tracks are af\ufb01liated with one or more of 31 genres. The most important genres are \u201cPop\u201d (10817 songs), \u201cAlter- native\u201d (9975 songs), \u201cHip-Hop\u201d (4849), \u201cRock\u201d (4632), \u201cCountry\u201d (3936) and \u201cDance/Electronic\u201d (3454). The lyrics to these songs were extracted from the inter- net using the method presented in [5], which worked for roughly two thirds of the songs. The reason for this rather low rate of success is that the method we employed is not suitable for some classes of tracks. Such classes comprise of course the purely instrumental songs, but also works that bear a composite artist attribute, e.g., \u201cMint Condition/feat. Charlie Wilson of the Gap Band\u201d or older and only locally known music. After removal of duplicates, which are also present in our dataset, we get a corpus of 33863 lyrics. The lyrics are in 15 different languages, though the vast majority is in English. We found about 300 Spanish songs, roughly 30 in Italian, French, Gaelic and Latin; all other languages are even less frequent. 3 ALGORITHM The method we propose requires the lyrics collection to be transformed into a vector space model in which each doc- ument is represented as a vector de\ufb01ning its af\ufb01liation to a set of topics. This transformation encompasses the follow- ing steps: 1 http://www.verisign.at/ 2 i.e., the most frequently accessed tracks within a certain time span 287 ISMIR 2008 \u2013 Session 2d \u2013 Social and Music Networks",
        "zenodo_id": 1416154,
        "dblp_key": "conf/ismir/KleedorferKP08"
    },
    {
        "title": "The PerlHumdrum and PerlLilypond Toolkits for Symbolic Music Information Retrieval.",
        "author": [
            "Ian Knopke"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416894",
        "url": "https://doi.org/10.5281/zenodo.1416894",
        "ee": "https://zenodo.org/records/1416894/files/Knopke08.pdf",
        "abstract": "PerlHumdrum is an alternative toolkit for working with large numbers of Humdrum scores. While based on the original Humdrum toolkit, it is a completely new, self-contained im- plementation that can serve as a replacement, and may be a better choice for some computing systems. PerlHumdrum is fully object-oriented, is designed to easily facilitate analy- sis and processing of multiple humdrum \ufb01les, and to answer common musicological questions across entire sets, collec- tions of music, or even the entire output of single or multiple composers. Several extended capabilities that are not avail- able in the original toolkit are also provided, such as transla- tion of MIDI scores to Humdrum, provisions for construct- ing graphs, a graphical user interface for non-programmers, and the ability to generate complete scores or partial musical examples as standard musical notation using PerlLilypond. These tools are intended primarily for use by music theo- rists, computational musicologists, and Music Information Retrieval (MIR) researchers. 1 INTRODUCTION Humdrum [5, 3, 4] is a computer-based system pioneered by David Huron for manipulating and querying symbolic representations of music. Unlike notation-based systems such as GUIDO or MusicXML, Humdrum is primarily in- tended as a set of analytical tools to aid music theorists, mu- sicologists, acousticians, cognitive scientists, and MIR re- searchers, among others. Also, Humdrum data \ufb01les are dif- ferentiated from stored-performance formats such as MIDI, in that almost all have been encoded by hand from musical scores; more than 40,000 Humdrum \ufb01les have been encoded to date, the majority of which are freely available online [1, 6]. The original motivation for this research was the desire to use the Humdrum resources in a series of music analysis projects, coupled with a certain frustration with the lack of functionality of some of the original tools which were reliant on older Unix environments and were dif\ufb01cult to make work under current versions of Linux. Other Humdrum tools, such as those for entering MIDI information, are associated with hardware that currently unavailable in most operating systems. Also, many of the problems that the project ex- plored required writing additional programs that worked di- rectly with the Humdrum data, such as data collection and extraction across large numbers of scores, and it was sim- ply easier to write them fresh in Perl than to try to accom- plish the same thing based on one of the older Awk tools. Over time a collection of alternate tools, extensions, and re- placements began to take shape, and at some point it simply made sense to collect everything under a single code base that shared methods for common tasks in a object oriented framework. In the process, it became possible to rethink some aspects of the original Humdrum Toolkit, and intro- duce some new possibilities that are useful when working on these sorts of problems. Additionally, from working with various musicologists, it became clear that a system was needed for automatically producing musical excerpts in common musical notation, as a method for displaying search results. This led to the cre- ation of the PerlLilypond programs, that provide a scriptable environment for generating notation. The two systems are designed to complement one another. The author refers to the two systems as PerlHumdrum and PerlLilypond. This is primarily to differentiate their names the original programs. However, within the Perl envi- ronment, the module names Humdrum and Lilypond are used. This is partially because of traditional naming con- ventions in Perl, but also because the author doesn\u2019t see the necessity of repeatedly in\ufb02icting an extra syllable on devel- opers and users during tasks such as multiple object cre- ation. 2 OVERVIEW OF HUMDRUM The overall Humdrum system is comprised of two main parts: a set of \ufb01le-based storage formats for time-aligned symbolic data, and the accompanying collection of UNIX-based tools for manipulating and querying this data. The PerlHumdrum system discussed in this paper is designed to simplify oper- ations on the former, while providing an alternative to the latter. Humdrum data \ufb01les consist of symbols representing mu- sical or other symbolic information, arranged in time-aligned columns known as spines. As an example, the \ufb01rst measures 147 ISMIR 2008 \u2013 Session 1d \u2013 MIR Platforms of a Bach chorale and the representative Humdrum excerpt are shown in Figure 1 and Table 1 respectively.",
        "zenodo_id": 1416894,
        "dblp_key": "conf/ismir/Knopke08"
    },
    {
        "title": "Social Tags and Music Information Retrieval.",
        "author": [
            "Paul Lamere",
            "Elias Pampalk"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.5776687",
        "url": "https://doi.org/10.5281/zenodo.5776687",
        "ee": null,
        "abstract": "The Proceedings of the 22nd International Society for Music Information Retrieval Conference, Online, Nov 7-12, 2021 is also archived athttp://archives.ismir.net/ismir2021/2021_Proceedings_ISMIR.pdf and available throughhttps://ismir.net/conferences/ismir2021.html",
        "zenodo_id": 5776687,
        "dblp_key": "conf/ismir/LamereP08"
    },
    {
        "title": "Multi-Feature Modeling of Pulse Clarity: Design, Validation and Optimization.",
        "author": [
            "Olivier Lartillot",
            "Tuomas Eerola",
            "Petri Toiviainen",
            "Jos\u00e9 Fornari"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415514",
        "url": "https://doi.org/10.5281/zenodo.1415514",
        "ee": "https://zenodo.org/records/1415514/files/LartillotETF08.pdf",
        "abstract": "Pulse clarity is considered as a high-level musical dimen- sion that conveys how easily in a given musical piece, or a particular moment during that piece, listeners can perceive the underlying rhythmic or metrical pulsation. The objective of this study is to establish a composite model explaining pulse clarity judgments from the analysis of audio record- ings. A dozen of descriptors have been designed, some of them dedicated to low-level characterizations of the onset detection curve, whereas the major part concentrates on de- scriptions of the periodicities developed throughout the tem- poral evolution of music. A high number of variants have been derived from the systematic exploration of alternative",
        "zenodo_id": 1415514,
        "dblp_key": "conf/ismir/LartillotETF08"
    },
    {
        "title": "Segmentation-Based Lyrics-Audio Alignment using Dynamic Programming.",
        "author": [
            "Kyogu Lee",
            "Markus Cremer"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416934",
        "url": "https://doi.org/10.5281/zenodo.1416934",
        "ee": "https://zenodo.org/records/1416934/files/LeeC08.pdf",
        "abstract": "In this paper, we present a system for automatic alignment of textual lyrics with musical audio. Given an input audio signal, structural segmentation is \ufb01rst performed and simi- lar segments are assigned a label by computing the distance between the segment pairs. Using the results of segmenta- tion and hand-labeled paragraphs in lyrics as a pair of input strings, we apply a dynamic programming (DP) algorithm to \ufb01nd the best alignment path between the two strings, achiev- ing segment-to-paragraph synchronization. We demonstrate that the proposed algorithm performs well for various kinds of musical audio. 1 INTRODUCTION As the market for portable media players increases rapidly, more and more manufacturers and multimedia content providers are searching for outstanding features that will set their of- fering apart from the rest. At the same time the rush to- wards online distribution of digital assets has stripped most content of rich metadata such as album artwork and lyrics. While artwork is gaining a lot of traction with the introduc- tion of higher resolution color displays in devices, only few high end devices offer the ability to display lyrics in a read- able form along with music. As devices tend to get smaller and smaller with advancing technology, there is not much hope for displays to become large enough for the user to comfortably follow song lyrics without some form of syn- chronized presentation. Some devices are already capable of displaying lyrics synchronized to the playback of music using manually inserted time stamps into the lyrics \ufb01le. However, this approach does not scale well for large col- lections, so an automated approach to align lyrics with mu- sic is the strongly preferable approach. It is notable that word-by-word level synchronization for most applications is not necessary. A paragraph or line-by-line synchronization, whatever is more appropriate for the device display resolu- tion, will be suf\ufb01cient. As this promises to be a fairly solv- able problem, to which a scalable automated solution could be provided, it has recently attracted a number of researchers in the music information retrieval (MIR) community. Wang et al., for example, have proposed a hierarchical approach for automatic alignment of acoustic musical sig- nals with textual lyrics [11, 9]. They decompose the prob- lem into two separate tasks \u2014 a higher-level section-based alignment followed by lower-level per-line alignment. To this end, they \ufb01rst process audio to obtain high-level struc- tural information such as Measure, Chorus and Singing Voice Section. In parallel, textural lyrics are analyzed and each section is labeled with one of the pre-de\ufb01ned section types. In this text processing stage, they also compute approximate durations of each section and line. However, their algorithm is limited by strong assumptions about the song structure as well as the \ufb01xed rhythmic structure. A different approach has been taken by Chen et al. who have presented an automatic lyrics-audio synchronization system using the low-level acoustic features only [3]. Their algorithm has two main components: 1) vocal/non-vocal de- tector and 2) alignment of the audio signal with its lyrics at multiple levels using acoustic models. Given a musical audio signal, a vocal/non-vocal classi\ufb01er detects candidates for the singing voice sections. In parallel, they construct the grammar net from the lyrics and force an alignment uti- lizing the previously obtained acoustic model units with a maximum likelihood linear regression technique. They have tested their algorithm on a small set of Chinese song seg- ments and achieve a boundary accuracy of 81.5% at the phrase level. Fujihara et al. tackle the lyrics-audio alignment chal- lenge by solving three sub-problems in series [8]: i.e., 1) separation of singing voice, 2) singing voice detection, and 3) alignment of segregated vocal signals with lyrics using a Viterbi-based matching technique. At the \ufb01nal alignment stage, they \ufb01rst build a language model from the lyrics using only vowel phonemes and short pauses between word, sen- tence or phrase boundaries. They also employ an adaptation of a phone model to the speci\ufb01c singer of the input audio signal to improve performance. Using 10 Japanese popular songs as test bed, they have achieved over 90% accuracy for eight songs. The ultimate goal of the lyrics-audio alignment systems described so far is to automate karaoke-style synchroniza- tion at a line or word level. Although a word-level or even 395 ISMIR 2008 \u2013 Session 3c \u2013 OMR, Alignment and Annotation syllable-level synchronization may appear to be ideal, it is extremely challenging to achieve and usually involves solv- ing other dif\ufb01cult problems such as singing voice separation or constructing proper speech models. In this paper, we deviate from this goal and propose a solution to a simpler, more basic problem: we aim to align song lyrics to the corresponding audio signal at a segment- to-paragraph level. Paragraph-level alignment may not be suf\ufb01cient for karaoke applications, but as stated initially we believe this will help users follow the lyrics as they lis- ten to music by providing them with referential points over time. Furthermore, as reported by Wang et al. [11, 9], section- or segment-level alignment provides an initial solu- tion that makes lower-level alignment such as line- or word- level alignment easier and more robust. This publication is organized as follows. We describe our proposed method for automatic lyrics-audio alignment in detail in the next Section. Thereafter, in Section 3, we present experimental results with several real examples. Fi- nally, in Section 4, we draw conclusions from the previously presented results, and give an outlook on future work in this domain. 2 METHOD As mentioned in Section 1, our system for lyrics-audio align- ment strives to achieve paragraph-to-segmentlevel synchro- nization. The motivations for this are as follows. First, we \ufb01nd that the segment structure in musical audio corresponds approximately to the paragraphs in lyrics. For example, a verse and/or chorus section is found in both audio and lyrics for most popular music, even when investigating one without knowledge of the other. Therefore, if we can divide an entire song into the appropriate segments, we can search for the corresponding paragraphs in lyrics. Second, paragraph-to-segment level alignment is far eas- ier and more robust than word-level or syllable-level align- ment because the latter usually depends on other complex algorithms such as singing voice separation and/or speech recognition, which are very challenging problems by them- selves. On the other hand, structural music segmentation has achieved fairly good performance with relatively simple and straightforward techniques, as previously discussed by many other researchers [7, 5, 1, 10]. Therefore, the \ufb01rst stage in our system consists of a seg- mentation of musical audio, and is described in the next paragraph.",
        "zenodo_id": 1416934,
        "dblp_key": "conf/ismir/LeeC08"
    },
    {
        "title": "Fast Index Based Filters for Music Retrieval.",
        "author": [
            "Kjell Lemstr\u00f6m",
            "Niko Mikkil\u00e4",
            "Veli M\u00e4kinen"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416774",
        "url": "https://doi.org/10.5281/zenodo.1416774",
        "ee": "https://zenodo.org/records/1416774/files/LemstromMM08.pdf",
        "abstract": "We consider two content-based music retrieval problems where the music is modeled as sets of points in the Eu- clidean plane, formed by the (on-set time, pitch) pairs. We introduce fast \ufb01ltering methods based on indexing the un- derlying database. The \ufb01lters run in a sublinear time in the length of the database, and they are lossless if a quadratic space may be used. By taking into account the application, the search space can be narrowed down, obtaining practi- cally lossless \ufb01lters using linear size index structures. For the checking phase, which dominates the overall running time, we exploit previously designed algorithms suitable for local checking. In our experiments on a music database, our best \ufb01lter-based methods performed several orders of a magnitude faster than previous solutions. 1 INTRODUCTION In this paper we are interested in content-based music re- trieval (CBMR) of symbolically encoded music. Such set- ting enables searching for excerpts of music, or query pat- terns, that constitute only a subset of instruments appearing in the full orchestration of a musical work. Instances of the setting include the well-known query-by-humming applica- tion, but our framework can also be used for more complex applications where both the query pattern searched for and the music database to be searched may be polyphonic. The design of a suitable CBMR algorithm is always a compromise between robustness and ef\ufb01ciency. Moreover, as robustness means high precision and recall, the similar- ity/distance measure used by the algorithm should not be too permissive to detect false matches (giving low precision) and not too restrictive to omit true matches (giving low re- call). In this paper, we concentrate on a modeling of music that we believe is robust in this sense, and at the same time provides computationally feasible retrieval performance. As symbolically encoded monophonic music can easily be represented as a linear string, in literature several solu- tions for monophonic CBMR problems are based on an ap- propriate method from the string matching framework (see e.g. [4, 6]). Polyphony, however, imposes a true challenge, especially when no voicing information is available or the occurrence is allowed to be distributed across the voices. In some cases it may suf\ufb01ce to use some heuristic, as for an ex- ample the SKYLINE algorithm [8], to achieve a monophonic reduction out of the polyphonic work. This, however, does not often provide musically meaningful results. In order to be able to deal with polyphonic music, geometric-based modeling has been suggested [1, 7, 9, 10]. Most of these provide also another useful feature, i.e., ex- tra intervening elements in the musical work, such as grace notes, that do not appear in the query pattern can be ig- nored in the matching process. The downside is that the ge- ometric online algorithms [2, 5, 9, 10] are not computation- ally as ef\ufb01cient as their counterparts in the string matching framework. Moreover, the known of\ufb02ine (indexing) meth- ods [1, 7] compromise on crucial matters. These downsides are not surprising: the methods look at all the subsequences and the number of them is exponential in the length of the database. Thus, a total index would also require exponential space. We deal with symbolically encoded, polyphonic music for which we use the pitch-against-time representation of note-on information, as suggested in [10] (see Figs. 1 and 2). The musical works in a database are concatenated in a sin- gle geometrically represented \ufb01le, denoted by T. In a typical case the query pattern P to be searched for is often mono- phonic and much shorter than the database T to be searched. If P and T are readily not given in the lexicographic order, the sets can be sorted in |P| log |P| and |T| log |T| times, respectively. The problems of interest are the following: (P1) Find translations of P = p1, p2, . . . , pm such that each point in P match with a point in T = t1, t2, . . . , tn (pi, tj \u2208R2 for 1 \u2264i, j \u2264m, n). (P2) Find translations of P that give a partial match of the points in P with the points in T. Notice that the partial matches of interest in P2 need to be de\ufb01ned properly, e.g. one can use a threshold k to limit the minimum size of partial matches of interest. Ukkonen et al. [9] presented algorithms PI and PII solv- ing problems P1 and P2 in worst case times O(mn) and O(mn log m), respectively. Their algorithms require O(m) space. Noteworthy, the algorithm solving P1 has an O(n) expected time complexity. Clifford et al. [2] showed that problem P2 is 3SUM-hard, i.e., it is unlikely that an exact solution could run faster than in quadratic time O(mn), and give an approximation algorithm, called MSM, for P2, that runs in time O(n log n). In this paper we introduce index-based \ufb01ltering algo- rithms for the problems presented above. Our contribution 677 ISMIR 2008 \u2013 Session 5d \u2013 MIR Methods Figure 1. A musical excerpt. is twofold. Firstly, our methods outperform its competitors; in particular, the algorithms are output sensitive, i.e., the running time depends more on the output than on the in- put. This is achieved by exploiting a simple indexing struc- ture that is not a total index. Secondly, we show how to keep the index of a practical, linear size. The index enables fast \ufb01ltering; best results are obtained with \ufb01lters running in O(f(m) log n + s) time. The found s (s \u2264n) candidate positions are subsequently checked using Ukkonen et al\u2019s PI and PII algorithms. Thus, executing checking take time O(sm) and O(sm log m), in the worst case, respectively. 2 INDEX BASED FILTERS We will denote by P + f a translation of P by vector f, i.e., vector f is added to each component of P separately: P + f = p1 + f, p2 + f, . . . , pm + f. Problem P1 can then be expressed as the search for a subset I of T such that P + f = I for some f. Please note that a translation corresponds to two musically distinct phenomena: a vertical move corresponds to transposition while a horisontal move corresponds to aligning the pattern time-wise (see Fig. 2). The idea used in [9, 10] is to work on trans-set vectors. Let p \u2208P be a point in the query pattern. A translation vector f is a trans-set vector, if there is a point t \u2208T, such that p + f = t. Without loss of generality, let us assume all the points both in the pattern and database to be unique. So, the number of trans-set vectors is O(n2) in the worst case. For the indexing purposes we consider translation vec- tors that appear within the pattern and the database. We call translation vector f intra-pattern vector, if there are two points p and p\u2032, p, p\u2032 \u2208P, such that p + f = p\u2032. The intra- database vector is de\ufb01ned in the obvious way. The number of intra-pattern and intra-database vectors are O(m2) and O(n2), respectively. A nice property of Ukkonen et al\u2019s PI and PII algorithms is that they are capable of starting the matching process anywhere in the database. Should there be a total of s occurrences of the pattern within the database and an oracle telling where they are, we could check the occurrences in O(sm) and O(sm log m) time, in the worst case, by executing locally PI and PII, respectively. 2 3 4 pitch time P T Figure 2. Pointset T represents Fig. 1 in the geometric rep- resentation. P corresponds to the \ufb01rst 2.5 bars of the melody line with a delayed 5th point. The depicted vectors form the translation f giving a partial match of P in T. We will exploit this property by \ufb01rst running a \ufb01lter whose output is subsequently checked by PI or PII. If a quadratic size for the index structure is allowed, we have a lossless \ufb01lter: all intra-database vectors are stored in a bal- anced search tree in which translations can be retrieved in O(log n) time; Let C(f) be the list of starting positions i of vector f = tj\u2212ti, for some j, in the database, then the list is stored in the leaf of a binary search tree so that a search from root with key f leads this leaf. Let us denote by |C(f)| the number of elements in the list. Since the points are readily in the lexicographic order, building such a structure takes a linear time in the number of elements to be stored. However, for large databases, a quadratic space is infea- sible. To avoid that, we store only a subset of the intra- database vectors. In CBMR, an occurrence is a compact subpart of the database typically not including too many in- tervening elements. Now we make a full use of this locality and that the points are readily sorted: for each point i in the database, 1 \u2264i \u2264n \u22121, we store intra-database vectors to points i + 1, . . . , i + c + 1 (c = min(c, n \u2212i \u22121)), where c is a constant, independent of n and m. Constant c sets the \u2018reach\u2019 for the vectors. Thus, the index becomes of linear, O(n) size. Naturally such \ufb01lters are no more totally loss- less, but by choosing a large c and by careful thinking in the \ufb01ltering algorithms, losses are truly minimal.",
        "zenodo_id": 1416774,
        "dblp_key": "conf/ismir/LemstromMM08"
    },
    {
        "title": "On Rhythmic Pattern Extraction in Bossa Nova Music.",
        "author": [
            "Ernesto Trajano de Lima",
            "Geber L. Ramalho"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417959",
        "url": "https://doi.org/10.5281/zenodo.1417959",
        "ee": "https://zenodo.org/records/1417959/files/LimaR08.pdf",
        "abstract": "The analysis of expressive performance, an important re- search topic in Computer Music, is almost exclusively de- voted to the study of Western Classical piano music. Instru- ments like the acoustic guitar and styles like Bossa Nova and Samba have been little studied, despite their harmonic and rhythmic richness. This paper describes some experimen- tal results obtained with the extraction of rhythmic patterns from the guitar accompaniment of Bossa Nova songs. The songs, played by two different performers and recorded with the help of a MIDI guitar, were represented as strings and processed by FlExPat, a string matching algorithm. The re- sults obtained were then compared to a previously acquired catalogue of \u201cgood\u201d patterns. 1 INTRODUCTION It is common sense that playing music in the exact way it is written in the score results in a mechanical and uninteresting succession of sounds. To make written music interesting, the musician is required to make variations on low level mu- sical parameters, such as: Local tempo (accelerandi, ritar- dandi, rubato); dynamics; notes articulation (staccati, liga- tures, etc.); micro-silences between the notes, etc. [12]. Several researchers stress the importance, as well as the the dif\ufb01culties, of studying this phenomenon, also known as expressive performance [14]. These researches, in gen- eral, focus on building relationships between different mu- sical elements (such as harmony and melody or even the low level parameters previously mentioned), and expressive performance itself. These relationships can be described in many ways, from different points of view or levels of ab- straction, and including various musical parameters. Exam- ples of such relationships are rules like \u201clengthen a note if it is followed by a longer note and if it is in a metrically weak position\u201d or \u201cstress a note by playing it louder if it is preceded by an upward melodic leap larger that a perfect fourth\u201d [13]. With some exceptions [3, 6], the role of rhythm in ex- pressive performance has not been thoroughly studied so far, despite its intuitive importance. Moreover, the research is almost exclusively devoted to the Western Classical Mu- sic composed for the piano. We are interested in studying the M\u00b4usica Popular Brasileira (Brazilian Popular Music)\u2014 MPB, represented by artists like Jo\u02dcao Gilberto, Tom Jobim, Caetano Veloso, Gilberto Gil, etc. We are particularly inter- ested in the guitar accompaniment, that is, in how the guitar player accompanies the singer or solo instrument. This paper presents an experiment that focus on the dis- covery of rhythmic patterns in Bossa Nova music. For this, two different performers played several songs on a MIDI guitar, which were processed in the form of strings. These strings were then processed using FlExPat, a pattern match- ing algorithm [8], and the results were then compared to a catalogue of patterns that re\ufb02ects the rhythmic patterns used by Jo\u02dcao Gilberto, the \u201cinventor\u201d of the Bossa Nova style. The remainder of the paper is organized as follows: In Section 2, we discuss what motivated us to try such an ex- periment. In Section 3, we describe how the data was ac- quired and the representation we used. In Section 4, we present the experiment itself and discuss the results we ob- tained. Finally, in Section 5, we present some conclusions and future directions for this work. 2 MOTIVATION Apart from obvious differences (instrument, genre/style and player\u2019s role), there is a much more fundamental difference between research focusing on Western Classical Music and research that deals with MPB: While in the Western Classi- cal Music case there is some sort of \u201cof\ufb01cial\u201d notated ver- sion of musical pieces (the score, namely), in MPB there is no such thing. What may be available is the chord grid (chord sequence that should be played), and, in some rare cases, the score of the melody. Even when the chord grid is available, the musician is usually allowed to change the har- mony and play something different from what was initially notated. Considering that, it becomes clear that the guitar player has a major role in the accompaniment\u2019s construc- tion. This importance becomes even clearer when the rhythm is taken into consideration, because, although the musi- cian may have the chord sequence notated (i.e., informa- tion about the harmony may be somehow speci\ufb01ed), there is no indication whatsoever of the rhythm the musician should 641 ISMIR 2008 \u2013 Session 5c \u2013 Rhythm and Meter play. It is entirely up to him/her to decide about the rhythm. Some studies pointed out, however, that the guitar ac- companiment in styles like Bossa Nova and Samba is built by the concatenation of certain rhythmical groups or pat- terns [4, 9]. There are, however, several aspects of the accompaniment construction that are only known by prac- titioners of these styles. Moreover, the knowledge about the accompaniment construction is mainly subjective. Due to this lack of formalized knowledge, there are many open questions such as: \u2022 Are there rhythmic patterns that are preferred by a cer- tain performer or required for a certain musical style? In which situations and in which frequency do they show up? \u2022 Are there variations of these patterns? Is it possible to group these variations in meaningful way? Which variations (timing, dynamics, etc.) are acceptable within a pattern? \u2022 Is it really the case that everything is a pattern, i.e., are there parts that are not recurrent? \u2022 Is it possible to justify the choice of a pattern in terms of other musical features (melody, harmony, tempo, musical structure, style, etc.)? \u2022 Is it possible to build a dictionary of patterns for a given player? Does this dictionary changes when the style changes (Bossa Nova and Samba, for instance)? Do different players have different dictionaries? \u2022 Is it possible to build a grammar or a set of rules that is able to describe formally how the patterns are chained and/or the rhythmical transformations done by a given performer? If so, what are the relations between grammars from performers p1 and p2? More general questions could also be posed: To which extent patterns used in Bossa Nova music are different from patterns used in Samba? How different is Samba today (in terms of patterns and pattern usage) from Samba in the 1920\u2019s or 1930\u2019s? Is Bossa Nova today still played as it was in the 1950\u2019s, when it was created? How can we describe those differences, if any? 3 DATA ACQUISITION AND REPRESENTATION For the experiment, two different players, referred to from now on as G1 and G2, were invited to record the accompa- niment of some Bossa Nova songs on a MIDI guitar 1 . G1 1 The equipments we used in the recordings were the following ones: An acoustical guitar with an RMC Poly-Drive II pick-up in- stalled (http://www.rmcpickup.com/polydriveii.html) that was connected to a Roland GR-33 guitar synthesizer responsible for the pitch to MIDI conversion (http://www.roland.com/products/ en/GR-33/index.html). performed the following songs: Bim Bom, O Barquinho, In- sensatez (How Insensitive), Garota de Ipanema (Girl from Ipanema), S\u00b4o Danc\u00b8o Samba, and Wave. From G2, we recorded A Felicidade, Chega de Saudade, Corcovado, De- sa\ufb01nado, Eu Sei Que Vou Te Amar, Samba de uma Nota S\u00b4o, Garota de Ipanema, S\u00b4o Danc\u00b8o Samba, Insensatez, Tarde em Itapo\u02dca, and Wave. In the total, we collected 16 recordings (ca. 30 minutes of music). It was requested for the perform- ers to play the songs according to a provided notation (the chord grid as notated by Chediak [1]). The acquired data was, however, not ready for usage. Probably due to technological restrictions, the resulting MIDI \ufb01les were noisy and it was necessary to clean the col- lected songs before using them. Noisy \ufb01les contained notes that were not played by the guitarist and these notes could be grouped into two basic types: Very short notes (usually high pitched) and notes with very low volume (loudness). There was yet a second type of problem, namely events that were somehow misplaced by the recording equipment (usu- ally a semitone up or down the actually played note). The \ufb01rst type of noise was removed automatically, but the sec- ond one required manual correction, which was done with the help of the recording\u2019s audio \ufb01les 2 . After this step, the data was beat tracked at the eighth note level using Beat- Root [3], an interactive system that outputs the MIDI \ufb01le beat tracked. As we are interested in the discovery of rhythmic pat- terns, an essential information is the moment when a note is played or its onset. It would be very hard, however, to use this information only in a meaningful way. We should, then, associate some other information with onsets in or- der to better represent the songs. Pitches may come to the reader\u2019s mind as the most relevant information that could be used with onsets, but they are not intrinsically linked to the rhythm. If we, however, observe how sound is produced by the guitar player, we may link each onset to the \ufb01nger that generated it. So, the \ufb01nger used to pluck the string and pro- duce sound may prove a much more interesting abstraction than, for instance, the pitches. But, this abstraction was not readily available in the \ufb01les we collected 3 . So, we had to develop an algorithm to auto- matically determine the right hand \ufb01ngering [11]. Roughly speaking, we \ufb01rst introduced the concept of hand position, that is, the \ufb01ngers\u2019 position regarding the string or strings they are about to pluck. Then, we created a hand position set that contains all relevant hand positions. Considering that the \ufb01ngering is formed by transitions between consecu- tive hand positions, we assigned a cost to each transition 4 . 2 We recorded, at the same time, MIDI and audio information. 3 Note that the MIDI \ufb01les we had at hand contained no information at all about the \ufb01ngering. But, we collected them in a way that each string was recorded separately on its own MIDI channel. 4 This cost represents, in fact, the amount of effort required to change from hand position HPi to hand position HPi+1. 642 ISMIR 2008 \u2013 Session 5c \u2013 Rhythm and Meter Each possible hand position in the hand position set can now be represented as a node in a graph, whereas the costs as edges weights. The \ufb01ngering problem can then be reduced to the discovery a path that minimizes the overall cost, and our algorithm simply tries to \ufb01nd an optimum path in this graph 5 . Our algorithm outputs the songs as depicted in Figure 1. Here, letters T, F, M and R represent, respectively, the thumb, fore, middle and ring \ufb01ngers, crosses (+) represent the beats, and pipes (|) represent the measure bars. Each beat was equally divided by four, so each letter, cross or mi- nus (\u2013) represents the duration of a 32nd. Except for the last line, that represents exclusively the beats, each of the remaining lines represents one guitar string, ordered from higher to lower (i.e., \ufb01rst line represents the high E string, second line the B string, and so on until the low E string). |----------------|-------- |R---R-----R-----|R---R--- |M---M-----M-----|M---M--- |F---F-----F-----|F---F--- [...] |----------------|-------- |T-------T-------|T------- |+---+---+---+---|+---+--- Figure 1. Right hand \ufb01ngering for song Insensatez, played by G2 This representation, however, can be viewed as a poly- phonic one (each guitar string being one \u201cvoice\u201d). Poly- phonic pattern matching, however, is a very dif\ufb01cult task [5] and we would like to avoid these dif\ufb01culties, at least at our initial steps and experiments. So, we further reduced this initial representation to a one-dimensional string with mini- mum loss of information 6 . This simpli\ufb01ed string is formed by the alphabet \u03a3 = {b, B, p, P, l, a, A, s, S, \u2212, +, |}. The meaning of each symbol is the following: \u2022 Uppercase letters stand for events that occur on-beat, while lowercase letters for off-beat events; \u2022 Letter b stands for \u201cbass\u201d, i.e., events played with the thumb only, and letter p stands for \u201cchord\u201d (sic), i.e., events that are played with some combinations of two or more of \ufb01ngers F, M and R 7 ; \u2022 Letter l also stands for \u201cchord\u201d, but a chord whose du- ration goes beyond the measure it was played (i.e., we 5 Note that our algorithm follows a similar approach used by Sayeg, as described in [10]. 6 Actually, with the alphabet we used we just can not recover the string where the note was played, what was not relevant for the experiments we made. We could easily avoid this information loss introducing new symbols in the alphabet. 7 The terms \u201cbaixo\u201d and \u201cpuxada\u201d may explain more clearly the origin of letters b and p! make a difference between a chord that is completely within a single measure and a chord that starts in one measure and ends in the next one); \u2022 Letter a stands for \u201call\u201d, i.e., b and p played together, and letter s stands for \u201csingle note\u201d, i.e., events that are played with only one of \ufb01ngers F, M and R; and \u2022 Symbols +, \u2212, and | have the same meaning stated before. It is important to note that this kind of reduction is also done by musicians themselves: They usually describe the rhythmic patterns as sequences of \u201cbaixos\u201d or basses (events played with the thumb only) and \u201cpuxadas\u201d or chords (events played with some combinations of two or more \ufb01ngers). Figure 2 depicts part of the \ufb01ngering for song Insensatez. Above the thick black line is the \ufb01nger- ing as outputted by the \ufb01ngering algorithm. Under it is the resulting simpli\ufb01ed string. Figure 2. Fingering and one-dimensional string for song Insensatez, played by G1 4 EXPERIMENT Considering that patterns are the building blocks used to cre- ate the accompaniment, \ufb01nding rhythmic patterns in Bossa Nova songs is a basic step towards understanding how the accompaniment is built by musicians. We have, however, one initial problem: In order to \ufb01nd patterns automatically, we need to use an existing algorithm or, case it fails to \ufb01nd patterns, we need to develop one that is able to \ufb01nd them. The big issue behind this problem is the following: How can we assess the results? How can we say that one algorithm performs better or worse than another one? How can we say that an algorithm is unsuitable for \ufb01nding rhythmic patterns? There is one particularity in the Bossa Nova domain: Jo\u02dcao Gilberto, the \u201cinventor\u201d of the style, is considered a model, and, as such, the musicians try to imitate him, play- ing the patterns he plays 8 . So, it is perfectly reasonable to assume that an algorithm that \ufb01nds, in any Bossa Nova data set, the patterns used by Jo\u02dcao Gilberto has a minimum acceptable performance level. 8 A musicologist could ask, then: If someone does not play like the inventor is he/she playing Bossa Nova at all? 643 ISMIR 2008 \u2013 Session 5c \u2013 Rhythm and Meter But, what are the patterns Jo\u02dcao Gilberto plays? In the literature we were able to \ufb01nd several transcriptions of pat- terns played by Jo\u02dcao Gilberto [4, 9]. So, we have built a catalogue containing 21 different patterns (labeled P1, P2, etc.), all manually transcribed by musicologists from Jo\u02dcao Gilberto\u2019s recordings. Examples of patterns in this cata- logue are showed in Figure 3. \u000e\u0010 \u000e\u0010 Chord Bass r s",
        "zenodo_id": 1417959,
        "dblp_key": "conf/ismir/LimaR08"
    },
    {
        "title": "Learning Musical Instruments from Mixtures of Audio with Weak Labels.",
        "author": [
            "David Little 0001",
            "Bryan Pardo"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417815",
        "url": "https://doi.org/10.5281/zenodo.1417815",
        "ee": "https://zenodo.org/records/1417815/files/LittleP08.pdf",
        "abstract": "We are interested in developing a system that learns to rec- ognize individual sound sources in an auditory scene where multiple sources may be occurring simultaneously. We fo- cus here on sound source recognition in music audio mix- tures. Many researchers have made progress by using iso- lated training examples or very strongly labeled training data. We consider an alternative approach: the learner is presented with a variety of weaky-labeled mixtures. Positive exam- ples include the target instrument at some point in a mix- ture of sounds, and negative examples are mixtures that do not contain the target. We show that it not only possible to learn from weakly-labeled mixtures of instruments, but that it works signi\ufb01cantly better (78% correct labeling compared to 55%) than learning from isolated examples when the task is identi\ufb01cation of an instrument in novel mixtures. 1 INTRODUCTION We are interested in developing a system that can learn to recognize individual sound objects in an auditory scene where multiple sound sources may be occurring simultaneously. A system able to identify what particular sound sources are present in a mixture would enable automatic tagging of au- dio recordings with meta-data. A system capable of model- ing sources in audio mixtures, without having to \ufb01rst learn the sounds in isolation, would be useful to researchers work- ing on separating audio mixtures into their component sources. In a typical supervised learning paradigm, isolated in- stances of a particular class of data are presented to a learner. Learning in this case is mostly limited to a stage of the de- sign process. With a more ecologically realistic set of re- quirements, where learning can occur in an environemnt af- ter deployment, this isolation of training instances can not be guaranteed. While similar problems have already been stud- ied in the \ufb01eld of computer vision [1], auditory data presents its own unique problems because mixtures of sound can re- sult in a composite of multiple sources at a given time. In this paper, we focus on identi\ufb01cation of musical instru- ments in a musical mixture. Almost all systems that learn to identify musical instruments require isolated examples of the target instrument at some stage in the training process (see [9] for a review). This could mean, isolated notes [12], or more recently, solo phrases from an instrument [11]. A number of these systems have been evaluated using poly- phonic audio [5, 11, 15], i.e. audio with multiple simulta- neous notes, but this work is still limited by the requirement that instruments be learned in isolation. We are aware of two systems that learn solely from poly- phonic audio. However, they differ signi\ufb01cantly from the task we consider here. In [6] the system is trained on poly- phonic audio, but each unique combination of instruments must be learned individually. The authors admit this ap- proach is only realistic when learning is done before deploy- ment. Given n instruments, this results in 2n possible com- binations that must be individually learned, signi\ufb01cantly in- creasing the number of required training examples. This ap- proach also leaves open the question of how to recognize individual instruments when presented in novel audio con- texts. In addition, the training data requires labels indicating all instruments currently playing for every two second seg- ment of audio, a fairly intensive labeling task. This is true even if a score is available (since it must be correctly aligned with a particular audio recording). Kitahara et al [10] learn from polyphonic data, avoiding the combinatorial problems of [6]. However, they require the user to input a musical score for the recording that ex- actly labels every note with pitch, onset time and offset time, i.e. perfect segmentation of each instrument into individual notes is assumed. What we would really like is a system that can learn from weakly labeled mixtures. We call a label weak if only the presence or absence of the target sound object is indicated for some N second length of audio. A weakly labeled pos- itive example will contain the target object at some point in the example, but a signi\ufb01cant portion of the time may not contain audio from the target. A weakly labeled neg- ative example does not contain the target sound object at any point in the example. In this scenario a useful posi- tive training example for the \u201csaxophone\u201d class would be 127 ISMIR 2008 \u2013 Session 1c \u2013 Timbre an acoustic recording of a saxophone and bass duo playing at a gallery opening. A negative example would be some other audio clip that contains no saxophone in the record- ing. Training could be accomplished on-line by recording a few audio clips that contain the desired sound. Our pro- posed input and output have important differences from that of previous systems that learn from polyphonic audio [6, 10] (weakly vs. strongly labeled input and multi-class vs. a presence/absence label as output). As a \ufb01rst step towards our objective, in this paper we evaluate a system that provides presence/absence labels for a single instrument over short time intervals (~2 seconds). We do not require labeling all instruments in the mixtures at short intervals, nor do we require detailed scores. Instead, the system learns from weakly labeled examples that include distractor sounds. Our instrument classes are learned independently of the",
        "zenodo_id": 1417815,
        "dblp_key": "conf/ismir/LittleP08"
    },
    {
        "title": "Clustering Music Recordings by Their Keys.",
        "author": [
            "Yuxiang Liu",
            "Ye Wang 0007",
            "Arun Shenoy",
            "Wei-Ho Tsai",
            "Lianhong Cai"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417375",
        "url": "https://doi.org/10.5281/zenodo.1417375",
        "ee": "https://zenodo.org/records/1417375/files/LiuWSTC08.pdf",
        "abstract": "Music key, a high level feature of musical audio, is an effective tool for structural analysis of musical works. This paper presents a novel unsupervised approach for clustering music recordings by their keys. Based on chroma-based features extracted from acoustic signals, an inter-recording distance metric which characterizes diversity of pitch distribution together with harmonic center of music pieces, is introduced to measure dissimilarities among musical features. Then, recordings are divided into categories via unsupervised clustering, where the best number of clusters can be determined automatically by minimizing estimated Rand Index. Any existing technique for key detection can then be employed to identify key assignment for each cluster. Empirical evaluation on a dataset of 91 pop songs illustrates an average cluster purity of 57.3% and a Rand Index of close to 50%, thus highlighting the possibility of integration with existing key identification techniques to improve accuracy, based on strong cross-correlation data available from this framework for input dataset.",
        "zenodo_id": 1417375,
        "dblp_key": "conf/ismir/LiuWSTC08"
    },
    {
        "title": "Towards Quantitative Measures of Evaluating Song Segmentation.",
        "author": [
            "Hanna M. Lukashevich"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417623",
        "url": "https://doi.org/10.5281/zenodo.1417623",
        "ee": "https://zenodo.org/records/1417623/files/Lukashevich08.pdf",
        "abstract": "Automatic music structure analysis or song segmentation has immediate applications in the \ufb01eld of music information retrieval. Among these applications is active music naviga- tion, automatic generation of audio summaries, automatic music analysis, etc. One of the important aspects of a song segmentation task is its evaluation. Commonly, that implies comparing the automatically estimated segmentation with a ground-truth, annotated by human experts. The automatic evaluation of segmentation algorithms provides the quanti- tative measure that re\ufb02ects how well the estimated segmen- tation matches the annotated ground-truth. In this paper we present a novel evaluation measure based on information- theoretic conditional entropy. The principal advantage of the proposed approach lies in the applied normalization, which enables the comparison of the automatic evaluation results, obtained for songs with a different amount of states. We dis- cuss and compare the evaluation scores commonly used for evaluating song segmentation at present. We provide sev- eral examples illustrating the behavior of different evalua- tion measures and weigh the bene\ufb01ts of the presented metric against the others. 1 INTRODUCTION Automatic analysis of digital audio content has become an important research \ufb01eld in the last years. The rapid growth of music structure analysis also poses a question of suf\ufb01cient evaluation of the proposed algorithms. Commonly the auto- matically estimated segmentation is compared to a ground- truth, provided by human experts. Music structure annota- tion is a challenging task even for the human experts. Its results might strongly vary depending on a particular appli- cation or even on the cultural background of the experts. For example, for popular western music the distinguishable and repeated parts could be \u201cintro\u201d, \u201cverse\u201d, \u201cchorus\u201d, \u201cbridge\u201d and \u201coutro\u201d. There are two principal approaches to the music struc- ture analysis, namely sequence and state representation [1]. In the present work we refer only to the state representa- tion, i.e. we consider the music audio signal as a sequence of states. Each state is characterized by a consequent time region with a similar acoustical content and is assigned to a distinct label. Thus if the similar acoustical content appears once again in the music piece it is assigned to the same la- bel. Especially for popular western music, the semantically distinguishable and repeated parts like \u201cchorus\u201d or \u201cverse\u201d generally have constant acoustical characteristics. Evaluating song segmentation algorithms is not a trivial task. Possible solutions for the case of state representation have been already proposed by [2], [3] and [4]. According to the knowledge of the author, up to now there is no com- monly established standard way of performing a segmenta- tion evaluation. One of the key challenges is the unknown number of possible states which might vary depending on the song. Having just one state of interest (e.g. \u201cchorus\u201d) [5] allows the use of precision, recall and F-measure, origi- nally proposed in [6]. An ideal evaluation measure for song segmentation should possess the following properties: \u2022 to provide the possibility to compare the results ob- tained by different authors andor for different algo- rithms; \u2022 to be easily and intuitively understandable; \u2022 to be insensitive to some particular properties varying for different songs, such as the number of states in the ground-truth segmentation. It is not generally required that the estimated labels them- selves should match the annotated ones, i.e. an additional stage of mapping the estimated labels into annotated ones is required. The architecture of the song segmentation system could imply that a given kind of mismatch between anno- tated and estimated segmentations is not considered to be a failure. For instance in [4] several annotated sequences are allowed to be mapped to a unique estimated one. Thus the evaluation measure should be able to treat this case cor- rectly. Representing a song as a sequence of possibly repeated states is in fact a classical clustering procedure. The same challenges appear while evaluating Speaker Clustering or Image Segmentation algorithms. As such similar evalua- tion measures can be used. Solomonoff et al. introduced the purity concept for Speaker Clustering evaluation [8], which was later extended by Ajmera et al. [9]. Another measure 375 ISMIR 2008 \u2013 Session 3b \u2013 Computational Musicology was originally proposed by Huang and Dom [10] for eval- uating Image Segmentation. Abdallah et al. [2] adapted it to the song segmentation task. The detailed explanation and the discussion of these evaluation measures are given in the next section. In this paper we introduce a novel evaluation measure based on the information-theoretic conditional entropy. Gen- erally, the number and the distribution of the states is differ- ent for each song in the test set. As such the evaluation scores, obtained for different songs are not directly compa- rable. The proposed evaluation measure is designed to over- come this challenge. Obviously, the more states there are in the song, the more \u201cdif\ufb01cult\u201d it is to get the true segmenta- tion by chance, or randomly. If the song has just two states of equal duration, even a \u201crandom\u201d segmentation with two states can lead to 50% of matching between the annotated and the estimated segmentations. This percentage decreases while the number of the states is increased . The absolute value of conditional entropy itself is strongly in\ufb02uenced by the number and by the distribution of the states in the an- notated and the estimated segmentations. Therefore an ad- ditional normalization is required. We propose using two values of normalized conditional entropies as two indepen- dent scores corresponding to the over-segmentation (errors caused by false fragmentation) and to the under-segmentation (errors caused by false merge of the segments). 2 PREVIOUS WORK",
        "zenodo_id": 1417623,
        "dblp_key": "conf/ismir/Lukashevich08"
    },
    {
        "title": "A Comparison of Signal Based Music Recommendation to Genre Labels, Collaborative Filtering, Musicological Analysis, Human Recommendation and Random Baseline.",
        "author": [
            "Terence Magno",
            "Carl Sable"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1418139",
        "url": "https://doi.org/10.5281/zenodo.1418139",
        "ee": "https://zenodo.org/records/1418139/files/MagnoS08.pdf",
        "abstract": "The emergence of the Internet as today\u2019s primary medium of music distribution has brought about demands for fast and reliable ways to organize, access, and discover music online. To date, many applications designed to perform such tasks have risen to popularity; each relies on a spe- ci\ufb01c form of music metadata to help consumers discover songs and artists that appeal to their tastes. Very few of these applications, however, analyze the signal waveforms of songs directly. This low-level representation can provide dimensions of information that are inaccessible by metadata alone. To address this issue, we have implemented signal- based measures of musical similarity that have been opti- mized based on their correlations with human judgments. Furthermore, multiple recommendation engines relying on these measures have been implemented. These systems rec- ommend songs to volunteers based on other songs they \ufb01nd appealing. Blind experiments have been conducted in which volunteers rate the systems\u2019 recommendations along with recommendations of leading online music discovery tools (Allmusic which uses genre labels, Pandora which uses mu- sicological analysis, and Last.fm which uses collaborative \ufb01ltering), random baseline recommendations, and personal recommendations by the \ufb01rst author. This paper shows that the signal-based engines perform about as well as popular, commercial, state-of-the-art systems. 1 INTRODUCTION The nature of online music distribution today is character- ized by massive catalogs of music unbounded by physical constraints. As pointed out in [1], current technology has offered music listeners \u201cmassive, unprecedented choice in terms of what they could hear\u201d. The number of songs avail- able on-line is in the billions, and many millions of users are continuing to \ufb02ock from traditional means of obtaining music (e.g., CD stores) to online alternatives [11]. With such a vast amount of music available on the Inter- net, end users need tools for conveniently discovering mu- sic previously unknown to them (whether recently released or decades old). In the context of electronic music distribu- tion, it is the goal of today\u2019s online discovery tools to au- tomatically recommend music to human listeners. This is no simple task; a program must have an automated way of computing whether or not one song is, in some sense, simi- lar to some other set of songs (i.e., to songs that are already liked by the user to whom the program is recommending new music). In accordance with this goal, we have designed and implemented three systems that use signal-based music similarity measures to recommend songs to users. In this paper, we \ufb01rst discuss existing methods of au- tomatic music recommendation, including a discussion of commercial, state-of-the-art systems that use them, in Sec- tion 2. Next, we discuss techniques for automatically com- puting signal-based music similarity, including a description of our own similarity measures, in Section 3; the optimiza- tion of the measures is discussed in Section 4. In Section 5, we discuss how these similarity measures have been used to design and implement three automatic, signal based mu- sic recommendation engines. Section 6 describes experi- ments in which volunteers have rated the recommendations of these systems, along with those of the popular systems described in Section 2, a baseline system, and human rec- ommendations. We evaluate the results of these experiments in Section 7. We then state some general conclusions in Sec- tion 8. 2 STRATEGIES FOR AUTOMATIC MUSIC RECOMMENDATION Three possible strategies of automatic music recommenda- tion involve expert opinions, collaborative \ufb01ltering, and mu- sicological analysis. Recommendation by expert opinion of- ten relies on the application of genre labels to songs and artists. The wide variety of music genre labels has arisen through a multifaceted interplay of cultures, artists, music journalists, and market forces to make up the complex hi- erarchies that are in use today [16]. Currently, the largest database of music that is organized by genre is Allmusic 1 , 1 http://www.allmusic.com/ 161 ISMIR 2008 \u2013 Session 2a \u2013 Music Recommendation and Organization where professional editors compose brief descriptions of pop- ular musical artists, often including a list of similar artists [6]. In the context of automatic music recommendation, re- cent research has effectively pointed out signi\ufb01cant de\ufb01cien- cies of the traditional genre labeling methodology. For one, as discussed in [16], there is no general agreement in the music community as to what kind of music item genre clas- si\ufb01cation should be consistently applied: a single song, an album, or an artist. Second, as discussed in [14], there is no a general agreement on a single taxonomy between the most widely-used music databases on the Internet. Lastly, it is noted in [16] that the criteria for de\ufb01ning music genres have, for countless years, been inconsistent; some labels are geographically de\ufb01ned, some are de\ufb01ned by a precise set of musical techniques, while others arise from the lexical whims of in\ufb02uential music journalists. Given these inconsistencies, musicological analysis aims to determine music similarity in a way that transcends con- ventional genre labels, focusing primarily on music theo- retic description of the vocal and instrumental qualities of songs. This technique was spearheaded by the Music Genome Project (MGP) in 2000, whose research culminated in the music discovery website/tool Pandora 2 [10]. The automatic recommendation algorithm behind Pandora involves com- parisons of very particular descriptions of songs. The de- scription process involves analysis of songs by a team of professional music analysts, each song being represented by about 150 \u201cgenes,\u201d where each gene describes a musicolog- ical quality of the song. Perhaps the most apparent draw- back of musicological analysis \u2014 especially in the context of Pandora \u2014 is that while the recommendation process is automated, the description aspect is not. It is this aspect that contributes to the relatively slow rate at which new content is added to the Pandora database. Also designed within the context of online music discov- ery, collaborative \ufb01ltering works according to the principle that if songs or artists you like occur commonly in other users\u2019 playlists, then you will probably also like the other songs or artists that occur in those playlists. According to [8], \u201cif your collection and somebody else\u2019s are 80% alike, it\u2019s a safe bet you would like the other 20%\u201d. One of the most popular on-line recommendation engine to use collab- orative \ufb01ltering is Last.fm 3 , which boasts 15 million active users and 350 million songs played every month [12]. One problem with collaborative \ufb01ltering systems is that they tend to highlight popular, mainstream artists. As noted in [8], Last.fm \u201crarely surprises you: It delivers conventional wis- dom on hyperdrive, and it always seems to go for the most obvious, common-sense picks.\u201d In other words, collabora- tive \ufb01ltering is not helpful for discovering lesser known mu- sic which a user might highly appreciate. The past several years have seen considerable progress in 2 http://www.pandora.com/ 3 http://www.last.fm/ the development of mathematical methods to quantify mu- sical characteristics of song waveforms based on the con- tent of their frequency spectra. In particular, these methods have enabled the extraction of features of a song\u2019s waveform that are correlated with the song\u2019s pitch, rhythmic, and tim- bral content. Timbre can be said to be the most important of these three elements when subjectively assessing musical similarity between a pair of songs; indeed, it may even be said that the global timbral similarity between two pieces of music is a reasonable \u2014 and often suf\ufb01cient \u2014 estimate of their overall musical similarity [4]. These research efforts have also gone on to evaluate and test several different timbre-based music similarity measures applied to a number of signal-based music information re- trieval tasks, including supervised and unsupervised classi- \ufb01cation of entire music databases and the segmentation and summarization of individual songs [16]. Following the lead of these efforts, we have applied signal-based measures of music similarity to the task of automatic recommendation of music. An automatic recommendation engine built on a signal-based music similarity measure would possess the advantages that current online music discovery tools merely trade off. It would boast the ability to describe and compare pieces of music based purely on their musical qualities, and would also facilitate the rapid addition of new content to a music database that does not require human intervention. 3 COMPUTING MUSIC SIMILARITY Our signal based recommendation engines rely on the abil- ity to automatically compute the similarity of two songs. First, the relevant information about each song \u2014 features \u2014 is computationally derived from its waveform data. Sec- ond, a compact representation of the song is obtained by modeling the distribution of its feature data using mixture and clustering algorithms. Third, a metric for comparing mixture models of songs is used to estimate the similarity between the feature distributions of two different songs. In effect, the timbral similarity between the two songs is math- ematically computed. As a whole, this music similarity measure framework allows a user to present a song query to the signal-based recommendation engine and receive a set of song recom- mendations (i.e., similar songs) drawn from a target music database. The similarities of the recommended songs to the query song are determined via signal processing alone, with- out human intervention. In this section, a general overview is given of the three similarity measures examined in this pa- per. Our implementations of these measures are based partly on those proposed in [9, 15, 17]. The music feature dataset extracted by the measures\u2019 anal- ysis front-ends are the Mel-frequency cepstral coef\ufb01cients (MFCC\u2019s). These perceptually-motivated features capture the \u201cspectral shape\u201d \u2014 and effectively, the timbral quality 162 ISMIR 2008 \u2013 Session 2a \u2013 Music Recommendation and Organization \u2014 of a music signal within a small frame of the waveform [18, 6]. In the literature, the MFCC feature set has already shown effective performance for various audio classi\ufb01cation experiments [6, 18, 13, 3].",
        "zenodo_id": 1418139,
        "dblp_key": "conf/ismir/MagnoS08"
    },
    {
        "title": "Armonique: Experiments in Content-Based Similarity Retrieval using Power-Law Melodic and Timbre Metrics.",
        "author": [
            "Bill Z. Manaris",
            "Dwight Krehbiel",
            "Patrick Roos",
            "Thomas Zalonis"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416778",
        "url": "https://doi.org/10.5281/zenodo.1416778",
        "ee": "https://zenodo.org/records/1416778/files/ManarisKRZ08.pdf",
        "abstract": "This paper presents results from an on-going MIR study utilizing hundreds of melodic and timbre features based on power laws for content-based similarity retrieval. These metrics are incorporated into a music search engine prototype, called Armonique. This prototype is used with a corpus of 9153 songs encoded in both MIDI and MP3 to identify pieces similar to and dissimilar from selected songs.  The MIDI format is used to extract various power- law features measuring proportions of music-theoretic and other attributes, such as pitch, duration, melodic intervals, and chords. The MP3 format is used to extract power-law features measuring proportions within FFT power spectra related to timbre. Several assessment experiments have been conducted to evaluate the effectiveness of the similarity model.  The results suggest that power-law metrics are very promising for content-based music querying and retrieval, as they seem to correlate with aspects of human emotion and aesthetics.",
        "zenodo_id": 1416778,
        "dblp_key": "conf/ismir/ManarisKRZ08"
    },
    {
        "title": "Multiple-Instance Learning for Music Information Retrieval.",
        "author": [
            "Michael I. Mandel",
            "Daniel P. W. Ellis"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1418299",
        "url": "https://doi.org/10.5281/zenodo.1418299",
        "ee": "https://zenodo.org/records/1418299/files/MandelE08.pdf",
        "abstract": "Multiple-instance learning algorithms train classi\ufb01ers from lightly supervised data, i.e. labeled collections of items, rather than labeled items. We compare the multiple-instance learners mi-SVM and MILES on the task of classifying 10- second song clips. These classi\ufb01ers are trained on tags at the track, album, and artist levels, or granularities, that have been derived from tags at the clip granularity, allowing us to test the effectiveness of the learners at recovering the clip labeling in the training set and predicting the clip labeling for a held-out test set. We \ufb01nd that mi-SVM is better than a control at the recovery task on training clips, with an average classi\ufb01cation accuracy as high as 87% over 43 tags; on test clips, it is comparable to the control with an average classi\ufb01- cation accuracy of up to 68%. MILES performed adequately on the recovery task, but poorly on the test clips. 1 INTRODUCTION There are many high quality sources of metadata about mu- sical material such as Last.fm, the All Music Guide, Pan- dora.com, etc. Typically, however, each source provides metadata only at certain granularities, i.e. describes the mu- sic only at certain scales. For example, the All Music Guide provides metadata about many artists and albums, but few tracks. Similarly, Last.fm users have described a large pro- portion of artists, a smaller proportion of albums, and an even smaller proportion of tracks. Furthermore, there are no pub- licly accessible, large-scale sources of metadata describing parts of tracks known as clips, here taken to be 10-second excerpts. This paper describes the use of clip-level classi\ufb01ers to re\ufb01ne descriptions from one granularity to \ufb01ner granular- ities, e.g. using audio classi\ufb01ers trained on descriptions of artists to infer descriptions of albums, tracks, or clips. Many descriptions of music apply at multiple granulari- ties, like rap, or saxophone, although certain descriptions are valid only at speci\ufb01c granularities like seen live or albums I own. Descriptions valid at one granularity, however, might only apply to certain elements at a \ufb01ner granularity. For example, at the artist level, the Beatles could very reasonably be tagged psychedelic. This tag would certainly apply to an album like Sgt. Pepper\u2019s Lonely Hearts Club Band but would not apply to one like Meet the Beatles. Similarly, the John Coltrane track \u201cGiant Steps\u201d could very reasonably be tagged saxophone. While valid for most clips in the track, it most notably is not valid during the piano solo. This paper describes systems capable of deriving, from feature similarity and a list of psychedelic artists or saxophone tracks, the clips for which these tags are most appropriate. By considering tags one at a time, as either being present or absent from a clip, we pose the automatic tagging (autotag- ging) problem as clip classi\ufb01cation. In this framework, the task of metadata re\ufb01nement is known as multiple instance learning (MIL) [6]. In MIL, classi\ufb01ers are trained on labels that are only applied to collections of instances, known as bags. Positive bags contain one or more positive instances, while negative bags contain no positive instances. Labeled bags provide less information to the learner than labeled in- stances, but are still effective at training classi\ufb01ers. For the purposes of this paper, clips are the instances to be classi\ufb01ed, and artists, albums, and tracks, in turn, are the bags. There are two problems addressed in the multiple-instances learning (MIL) literature, the classi\ufb01cation of bags and the classi\ufb01cation of instances. As we are interested in re\ufb01ning musical metadata from bags to the instances within them, we only concern ourselves with multiple-instance learners that are capable of classifying instances. A related problem that we also examine is the training of a general instance-level classi\ufb01er from bag-level labels. This task is slightly different in that the instances to be labeled are not in the training bags, and are unseen at training time. To evaluate the applicability of MIL to music, we use the data from our MajorMiner game [10]. The game has collected approximately 12,000 clip-level descriptions of ap- proximately 2,200 clips from many different tracks, albums, and artists. The most popular descriptions have been applied to hundreds of clips, and there are 43 tags that have been applied to at least 35 clips. Previous authors have generally used datasets that were labeled at the bag level, making it dif\ufb01cult to evaluate instance-level classi\ufb01cation. Sometimes a subset of the data was laboriously annotated to allow the evaluation of instance-level classi\ufb01cation. In the MajorMiner dataset, however, tags are applied directly to clips, making it 577 ISMIR 2008 \u2013 Session 5a \u2013 Content-Based Retrieval, Categorization and Similarity 2 possible to test instance-level classi\ufb01cation in both the train- ing set and a separate test set. By deriving bag tags from clip tags in the training set, we can directly test the ability of multiple instance learners to recover metadata at the in- stance level from the bag level. This derivation adheres to the MIL formulation, labeling a given bag positive as a positive example of a tag if any of its clips have been labeled with that tag. In addition, a held-out test set allows us to evaluate the generalization of these classi\ufb01ers to instances outside the training set.",
        "zenodo_id": 1418299,
        "dblp_key": "conf/ismir/MandelE08"
    },
    {
        "title": "On the Use of Sparce Time Relative Auditory Codes for Music.",
        "author": [
            "Pierre-Antoine Manzagol",
            "Thierry Bertin-Mahieux",
            "Douglas Eck"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415034",
        "url": "https://doi.org/10.5281/zenodo.1415034",
        "ee": "https://zenodo.org/records/1415034/files/ManzagolBE08.pdf",
        "abstract": "Many if not most audio features used in MIR research are inspired by work done in speech recognition and are varia- tions on the spectrogram. Recently, much attention has been given to new representations of audio that are sparse and time-relative. These representations are ef\ufb01cient and able to avoid the time-frequency trade-off of a spectrogram. Yet lit- tle work with music streams has been conducted and these features remain mostly unused in the MIR community. In this paper we further explore the use of these features for musical signals. In particular, we investigate their use on realistic music examples (i.e. released commercial music) and their use as input features for supervised learning. Fur- thermore, we identify three speci\ufb01c issues related to these features which will need to be further addressed in order to obtain the full bene\ufb01t for MIR applications. 1 INTRODUCTION The majority of the features used in audio-related MIR re- search are based on Fourier analysis, which suffers from two weaknesses. The \ufb01rst is the trade-off in precision between time and frequency. The second, common to all block based representations, is a sensitivity to arbitrary alignment of the blocks with the musical events. Sparse coding assumes a signal can be represented at a given point in time by a rather small number of basis func- tions taken from an overcomplete dictionary [9]. Recent work [2, 10, 13, 14] applies these ideas to audio streams. When set in the time domain, the result is a spikegram, an ef\ufb01cient representation of the signal that avoids both of the spectrogram\u2019s weaknesses. Figure 1 shows a given signal (an ascending and descending C-major scale played on a pi- ano), its spectrogram and its spikegram. As can be seen, a spikegram is composed of a set of spikes, corresponding to the placement of a basis function (kernel) at a precise point in time and with a particular scaling coef\ufb01cient. The spikegram encodes audio very ef\ufb01ciently and with arbitrary resolution along both axes. Figure 1. Top: original signal (an ascending and descending C- major scale played on the piano) and its residual (after encoding the signal to 20 dB SNR using the auditory codes). Middle: spec- trogram. Bottom: spikegram. The horizontal axis represents time with the same resolution as that of the signal. The vertical axis corresponds to the frequencies of the n kernels used (Gammatones in this case). A dot represents a spike, a kernel placed at a speci\ufb01c point in time. The size and color of the dot are representative of the scaling coef\ufb01cient of the kernel. This spikegram contains 13, 000 values, enough to encode the signal to 20dB SNR. The organization of the paper is as follows. Section 2 reviews existing work on sparse coding for audio streams. Section 3 presents the sparse coding algorithm we use. Sec- tion 4 attempts to bring insight into the resulting codes. To this end, we encode the Tzanetakis genre dataset using pre- de\ufb01ned Gammatone kernels. In Section 5 the features are applied in a naive way to the task of genre recognition and are shown to work as well as other commonly used audio features. In section 6 we attempt to learn a set of kernels better suited to music than the general purpose Gamma- tones. The learned kernels are qualitatively different from those learned on other types of sounds. This suggests mu- sic poses speci\ufb01c coding challenges. Finally, in Section 7, 603 ISMIR 2008 \u2013 Session 5b \u2013 Feature Representation we conclude and identify three speci\ufb01c issues with sparse coding for further research. 2 RELATED WORK In this section we present existing work on creating a sparse encoding of audio signals. First, we present work done in the frequency domain, and secondly, methods in the time domain.",
        "zenodo_id": 1415034,
        "dblp_key": "conf/ismir/ManzagolBE08"
    },
    {
        "title": "A Discrete Mixture Model for Chord Labelling.",
        "author": [
            "Matthias Mauch",
            "Simon Dixon"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416982",
        "url": "https://doi.org/10.5281/zenodo.1416982",
        "ee": "https://zenodo.org/records/1416982/files/MauchD08.pdf",
        "abstract": "Chord labels for recorded audio are in high demand both as an end product used by musicologists and hobby musi- cians and as an input feature for music similarity applica- tions. Many past algorithms for chord labelling are based on chromagrams, but distribution of energy in chroma frames is not well understood. Furthermore, non-chord notes com- plicate chord estimation. We present a new approach which uses as a basis a relatively simple chroma model to represent short-time sonorities derived from melody range and bass range chromagrams. A chord is then modelled as a mix- ture of these sonorities, or subchords. We prove the prac- ticability of the model by implementing a hidden Markov model (HMM) for chord labelling, in which we use the dis- crete subchord features as observations. We model gamma- distributed chord durations by duplicate states in the HMM, a technique that had not been applied to chord labelling. We test the algorithm by \ufb01ve-fold cross-validation on a set of 175 hand-labelled songs performed by the Beatles. Accu- racy \ufb01gures compare very well with other state of the art approaches. We include accuracy speci\ufb01ed by chord type as well as a measure of temporal coherence. 1 INTRODUCTION While many of the musics of the world have developed com- plex melodic and rhythmic structures, Western music is the one that is most strongly based on harmony [3]. A large part of harmony can be expressed as chords. Chords can be theoretically de\ufb01ned as sets of simultaneously sounding notes, but in practice, including all sounded pitch classes would lead to inappropriate chord labelling, so non-chord notes are largely excluded from chord analysis. However, the question which of the notes are non-chord notes and which actually constitute a new harmony is a perceptual one, and answers can vary considerably between listeners. This has also been an issue for automatic chord analysers from symbolic data [16]. Flourishing chord exchange web- sites 1 prove the sustained interest in chord labels of ex- isting music. However, good labels are very hard to \ufb01nd, 1 e.g. http://www.chordie.com/ arguably due to the tediousness of the hand-labelling pro- cess as well as the lack of expertise of many enthusiastic authors of transcriptions. While classical performances are generally based on a score or tight harmonic instructions which result in perceived chords, in Jazz and popular mu- sic chords are often used as a kind of recipe, which is then realised by musicians as actually played notes, sometimes rather freely and including a lot of non-chord notes. Our aim is to translate performed pop music audio back to the chord recipe it supposedly has been generated from (lead sheet), thereby imitating human perception of chords. A rich and reliable automatic extraction could serve as a basis for accurate human transcriptions from audio. It could fur- ther inform other music information retrieval applications, e.g. music similarity. The most successful past efforts at chord labelling have been based on an audio feature called the chromagram. A chroma frame, also called pitch class pro\ufb01le (PCP), is a 12-dimensional real vector in which each element represents the energy of one pitch class present in a short segment (frame) of an audio recording. The matrix of the chroma frame columns is hence called chromagram. In 1999, Fujishima [5] introduced the chroma feature to music computing. While being a relatively good representation of some of the harmonic content, it tends to be rather prone to noise in\ufb02icted by transients as well as passing/changing notes. Different models have been proposed to improve es- timation, e.g. by tuning [6] and smoothing using hidden Markov models [2, 11]. All the algorithms mentioned use only a very limited chord vocabulary, consisting of no more than four chord types, in particular excluding silence (no chord) and dominant 7th chords. Also, we are not aware of any attemps to address chord fragmentation issues. We present a novel approach to chord modelling that ad- dresses some of the weaknesses of previous chord recogni- tion algorithms. Inspired by word models in speech process- ing we present a chord mixture model that allows a chord to be composed of many different sonorities over time. We also take account of the particular importance of the bass note by calculating a separate bass chromagram and inte- grating it into the model. Chord fragmentation is reduced using a duration distribution model that better \ufb01ts the actual chord duration distribution. These characteristics approxi- mate theoretic descriptions of chord progressions better than 45 ISMIR 2008 \u2013 Session 1a \u2013 Harmony previous approaches have. The rest of this paper is organised as follows. Section 2 explains the acoustical model we are using. Section 3 de- scribes the chord and chord transition models that constitute the hierarchical hidden Markov model. Section 4 describes how training and testing procedures are implemented. The result section 5 reports accuracy \ufb01gures. Additionally, we introduce a new scoring method. In section 6 we discuss problems and possible future developments. 2 ACOUSTIC MODEL",
        "zenodo_id": 1416982,
        "dblp_key": "conf/ismir/MauchD08"
    },
    {
        "title": "A Music Database and Query System for Recombinant Composition.",
        "author": [
            "James B. Maxwell",
            "Arne Eigenfeldt"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415794",
        "url": "https://doi.org/10.5281/zenodo.1415794",
        "ee": "https://zenodo.org/records/1415794/files/MaxwellE08.pdf",
        "abstract": "We propose a design and implementation for a music information database and query system, the MusicDB, which can be used for Music Information Retrieval (MIR). The MusicDB is implemented as a Java package, and is loaded in MaxMSP using the mxj external. The MusicDB contains a music analysis module, capable of extracting musical information from standard MIDI files, and a search engine. The search engine accepts queries in the form of a simple six-part syntax, and can return a variety of different types of musical information, drawing on the encoded knowledge of musical form stored in the database.",
        "zenodo_id": 1415794,
        "dblp_key": "conf/ismir/MaxwellE08"
    },
    {
        "title": "Rhyme and Style Features for Musical Genre Classification by Song Lyrics.",
        "author": [
            "Rudolf Mayer",
            "Robert Neumayer",
            "Andreas Rauber"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416758",
        "url": "https://doi.org/10.5281/zenodo.1416758",
        "ee": "https://zenodo.org/records/1416758/files/MayerNR08.pdf",
        "abstract": "How individuals perceive music is in\ufb02uenced by many dif- ferent factors. The audible part of a piece of music, its sound, does for sure contribute, but is only one aspect to be taken into account. Cultural information in\ufb02uences how we experience music, as does the songs\u2019 text and its sound. Next to symbolic and audio based music information re- trieval, which focus on the sound of music, song lyrics, may thus be used to improve classi\ufb01cation or similarity ranking of music. Song lyrics exhibit speci\ufb01c properties different from traditional text documents \u2013 many lyrics are for exam- ple composed in rhyming verses, and may have different fre- quencies for certain parts-of-speech when compared to other text documents. Further, lyrics may use \u2018slang\u2019 language or differ greatly in the length and complexity of the language used, which can be measured by some statistical features such as word / verse length, and the amount of repetative text. In this paper, we present a novel set of features devel- oped for textual analysis of song lyrics, and combine them with and compare them to classical bag-of-words indexing approaches. We present results for musical genre classi\ufb01ca- tion on a test collection in order to demonstrate our analysis. 1 INTRODUCTION The prevalent approach in music information retrieval is to analyse music on the symbolic or audio level. Songs are therein represented by low level features computed from the audio waveform or by transcriptions of the music. Addi- tionally, all songs but instrumental tracks can be treated as textual content by means of song lyrics. This information can be exploited by methods of classical text information retrieval to provide an alternative to music processing based on audio alone. On the one hand, lyrics provide the means to identify speci\ufb01c genres such as \u2018love songs\u2019, or \u2019Christ- mas carols\u2019, which are not acoustic genres per se, but, to a large degree, de\ufb01ned by song lyrics [10]. Christmas songs, for instance may appear in a wide range of genres such as \u2018Punk Rock\u2019 or \u2018Pop\u2019 in addition to the classic \u2018Christmas carol\u2019. On the other hand, song lyrics might sometimes be the only available option for extracting features, for exam- ple, when audio \ufb01les are not available in an appropriate for- mat, or only via a stream when bandwith is a limiting factor. Further, processing of audio tracks might be too time con- suming compared to lyrics processing. Song lyrics may differ to a great extent from the docu- ments often dealt with in traditional text retrieval tasks such as searching the web or of\ufb01ce documents. In addition to its plain text content, song lyrics exhibit a certain structure, as they are organised in blocks of choruses and verses. Also, lyrics might feature other speci\ufb01c properties, such as slang language in \u2018Hip-Hop\u2019 or \u2018Rap\u2019 music, or other statistical information such as (average) line lengths, or words per minute. Also special characters, e.g. the number of excla- mation marks used, might be of interest. In this paper, we present a set of features composed of these various textual properties. We then use them for genre classi\ufb01cation, and compare and combine them with features resulting from standard bag-of-words indexing of the song texts. We aim to show the following: a) rhyme, part-of- speech, and simple text statistic features alone can be used for genre classi\ufb01cation, and b) the combination of bag-of- words features and our feature sets is worthwile. The remainder of this paper is structured as follows. We \ufb01rst give an overview of related work on music information retrieval focusing on lyrics processing in Section 2. Then, we give an introduction to lyrics analysis and explain our feature sets in detail in Section 3. In Section 4, we report from experiments performed on a manually compiled col- lection of song lyrics. Finally, we draw conclusions and give an overview of future work in Section 5. 2 RELATED WORK In general, music information retrieval (MIR) is a broad and diverse area, including research on a magnitude of topics such as classic similarity retrieval, genre classi\ufb01cation, visu- alisation of music collections, or user interfaces for access- ing (digital) audio collections. Many of the sub-domains of MIR \u2013 particularly driven by the large-scale use of digital 337 ISMIR 2008 \u2013 Session 3a \u2013 Content-Based Retrieval, Categorization and Similarity 1 audio over the last years \u2013 have been heavily researched. Experiments on content based audio retrieval, i.e. based on signal processing techniques, were reported in [3] as well as [14], focusing on automatic genre classi\ufb01cation. Several feature sets have since been devised to capture the acoustic characteristics of audio material, e.g. [12]. An investigation of the merits for musical genre classi\ufb01- cation, placing emphasis on the usefulness of both the con- cept of genre itself as well as the applicability and impor- tance of genre classi\ufb01cation, is conducted in [8]. A system integrating multi modal data sources, e.g. data in the form of artist or album reviews was presented in [1]. Cultural data is used to organise collections hierarchically on the artist level in [11]. The system describes artists by terms gathered from web search engine results. A promis- ing technique for automatic lyrics alignment from web re- sources is given in [4]. A study focusing solely on the semantic and structural analysis of song lyrics including language identi\ufb01cation of songs based on lyrics is conducted in [6]. Song lyrics can also be feasible input for artist similarity computations as shown in [5]. It is pointed out that similarity retrieval using lyrics, i.e. \ufb01nding similar songs to a given query song, is inferior to acoustic similarity. However, it is also suggested that a combination of lyrics and acoustic similarity could improve results, which motivates future research. Genre classi\ufb01cation based on lyrics data as well as its combination with audio features is presented in [9]. Further, the combination of a range of feature sets for au- dio clustering based on the Self-Organising Map (SOM) is presented in [7]. It is shown that the combination of het- erogeneous features improves clustering quality. Visualisa- tion techniques for multi-modal clustering based on Self- Organising maps are given in [10], demonstrating the poten- tial of lyrics analysis for clustering collections of digital au- dio. Similarity of songs is de\ufb01ned according to both modal- ities to compute quality measures with respect to the differ- ences in distributions across clusterings in order to identify interesting genres and artists. 3 LYRICS FEATURES In this section we present the feature set computed from the song lyrics, namely bag-of-words, rhyme, part-of-speech, and statistical features.",
        "zenodo_id": 1416758,
        "dblp_key": "conf/ismir/MayerNR08"
    },
    {
        "title": "Combining Features Extracted from Audio, Symbolic and Cultural Sources.",
        "author": [
            "Cory McKay",
            "Ichiro Fujinaga"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415576",
        "url": "https://doi.org/10.5281/zenodo.1415576",
        "ee": "https://zenodo.org/records/1415576/files/McKayF08.pdf",
        "abstract": "This paper experimentally investigates the classification utility of combining features extracted from separate au- dio, symbolic and cultural sources of musical information. This was done via a series of genre classification experi- ments performed using all seven possible combinations and subsets of the three corresponding types of features. These experiments were performed using jMIR, a soft- ware suite designed for use both as a toolset for perform- ing MIR research and as a platform for developing and sharing new algorithms. The experimental results indicate that combining fea- ture types can indeed substantively improve classification accuracy. Accuracies of 96.8% and 78.8% were attained respectively on 5 and 10-class genre taxonomies when all three feature types were combined, compared to average respective accuracies of 85.5% and 65.1% when features extracted from only one of the three sources of data were used. It was also found that combining feature types de- creased the seriousness of those misclassifications that were made, on average, particularly when cultural features were included.",
        "zenodo_id": 1415576,
        "dblp_key": "conf/ismir/McKayF08"
    },
    {
        "title": "Creating and Evaluating Multi-Phrase Music Summaries.",
        "author": [
            "Konstantinos A. Meintanis",
            "Frank M. Shipman III"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415510",
        "url": "https://doi.org/10.5281/zenodo.1415510",
        "ee": "https://zenodo.org/records/1415510/files/MeintanisS08.pdf",
        "abstract": "Music summarization involves the process of identifying and presenting melody snippets carrying sufficient information for highlighting and remembering a song. In many commercial applications, the problem of finding those snippets is addressed by having humans select the most salient parts of the song or by extracting a few seconds from the song's introduction. Research in the automatic creation of music summaries has focused mainly on the extraction of one or more highly repetitive phrases to represent the whole song. This paper explores whether the composition of multiple 'characteristic' phrases that are selected to be highly dissimilar to one another will increase the summary's effectiveness. This paper presents three variations of this multi-phrase music summarization approach and a human-centered evaluation comparing these algorithms. Results showed that the resulting multi- phrase summaries performed well in describing the songs. People preferred the multi-phrase summaries over presentations of the introductions of the songs.",
        "zenodo_id": 1415510,
        "dblp_key": "conf/ismir/MeintanisS08"
    },
    {
        "title": "A Music Identification System Based on Chroma Indexing and Statistical Modeling.",
        "author": [
            "Riccardo Miotto",
            "Nicola Orio"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415254",
        "url": "https://doi.org/10.5281/zenodo.1415254",
        "ee": "https://zenodo.org/records/1415254/files/MiottoO08.pdf",
        "abstract": "A methodology is described for the automatic identi\ufb01cation of classical music works. It can be considered an exten- sion of \ufb01ngerprinting techniques because the identi\ufb01cation is carried out also when the query is a different performance of the work stored in the database, possibly played by differ- ent instruments and with background noise. The proposed methodology integrates an already existing approach based on hidden Markov models with an additional component that aims at improving scalability. The general idea is to carry out a clustering of the collection to highlight a limited number of candidates to be used for the HMM-based iden- ti\ufb01cation. Clustering is computed using the chroma features of the music works, hashed in a single value and retrieved using a bag of terms approach. Evaluation results are pro- vided to show the validity of the combined approaches. 1 INTRODUCTION This paper presents a methodology and its Web-based im- plementation for the automatic identi\ufb01cation of music works. The approach is an extension of audio \ufb01ngerprinting tech- niques. As it is well known, an audio \ufb01ngerprint is a unique set of features that allows to identify digital copies of a given audio \ufb01le because it is robust to the presence of noise, dis- tortion, lossy compression and re-sampling. Identi\ufb01cation is carried out by comparing the \ufb01ngerprint of the unknown audio \ufb01le with the \ufb01ngerprints stored in a database, nor- mally using indexing techniques. A comprehensive tutorial on methods and techniques for audio \ufb01ngerprinting can be found in [3]. The main difference between the proposed approach and typical \ufb01ngerprinting techniques is that the goal is to iden- tify any possible recording of a given music work through statistical modeling. This representation is automatically computed from audio recordings, which are stored in a data- base. To this end, the methodology is strictly related to re- search work on cover identi\ufb01cation. Yet, the envisaged ap- plication scenario is the automatic identi\ufb01cation of baroque, classical, and romantic music or of any music genre where a written score is assumed to be the starting point of perfor- mances. In all these cases, the term \u201ccover\u201d is misleading because it assumes the existence of a prototype recording from which all the others derive. For this reason, the more general term music identi\ufb01cation is used in this paper; for simplicity, the term \u201cclassical music\u201d is used to address all the aforementioned repertories. An automatic system able to identify performances of classical music may be helpful in a variety of situations. For instance, most theaters and concert halls routinely record all the rehearsals. This valuable material may totally lack metadata, with the possible exception of the date of the re- hearsals, and the reconstruction of how a particular produc- tion has been created may become a dif\ufb01cult task. Similar considerations apply to the recordings of concerts that have been broadcasted and archived by radio and television com- panies. Most of the times, metadata are uninformative about the content of each single audio track, if not totally missing. From the end user point of view, automatic labeling of clas- sical music can be helpful because many compositions have been recorded in a number of different CDs that, apart from the recordings made by well known artists, may not be listed in online services such as Gracenote [9], in particular for the typical case of compilations containing selected movements of recordings already released. Automatic identi\ufb01cation of classical music can be con- sidered also a viable alternative to \ufb01ngerprinting. In order to effectively identify a recording, \ufb01ngerprinting techniques need to access the original audio \ufb01les. For classical music this means that all the different recordings of a given work should be present, with an increase in computational costs, not mentioning the economical aspects in creating such a large reference collection. The proposed methodology integrates an approach for the identi\ufb01cation of orchestral live music [14] based on hid- den Markov models (HMMs). A HMM is generated for each music work, starting from an audio recording. The approach showed to be effective but not scalable, because the recording has to be compared to all the recordings in the database. The identi\ufb01cation becomes clearly unfeasible when the database contains a large number of music works. To this end, an additional component has been designed with the aim of improving scalability. The idea is to carry out a clustering of the collection to highlight a limited num- ber of candidates to be used for the HMM based identi\ufb01- 301 ISMIR 2008 \u2013 Session 3a \u2013 Content-Based Retrieval, Categorization and Similarity 1 cation. The main requirement of clustering is ef\ufb01ciency, while effectiveness can be modulated by carefully selecting the size of the cluster. 2 CHROMA-BASED DESCRIPTION Chroma features have been extensively used in a number of music retrieval applications. The concept behind chroma is that octaves play a fundamental role in music perception and composition [1]. For instance, the perceived quality \u2013 e.g. major, minor, diminished \u2013 of a given chord depends only marginally on the actual octaves where it spans, while is strictly related by the pitch classes of its notes. Following this assumption, a number of techniques have been proposed based on chroma features, for chord estimation [7, 18], de- tection of harmonic changes [11], and the extraction of re- peating patterns in pop songs [8]. The application of chroma features to an identi\ufb01cation task has been already been pro- posed for classical [15] and for pop [12] music.",
        "zenodo_id": 1415254,
        "dblp_key": "conf/ismir/MiottoO08"
    },
    {
        "title": "Kernel Expansion for Online Preference Tracking.",
        "author": [
            "Yvonne Moh",
            "Joachim M. Buhmann"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415962",
        "url": "https://doi.org/10.5281/zenodo.1415962",
        "ee": "https://zenodo.org/records/1415962/files/MohB08.pdf",
        "abstract": "User preferences of music genres can signi\ufb01cantly changes over time depending on fashions and the personal situation of music consumers. We propose a model to learn user pref- erences and their changes in an adaptive way. Our approach re\ufb01nes a model for user preferences by explicitly consid- ering two plausible constraints of computational costs and limited storage space. The model is required to adapt itself to changing data distributions, and yet be able to compress \u201chistorical\u201d data. We exploit the success of kernel SVM, and we consider an online expansion of the induced space as a preprocessing step to a simple linear online learner that updates with maximal agreement to previously seen data. 1 INTRODUCTION Adaptivity is an indispensable prerequisite for personaliza- tion of acoustic devices like hearing aids. Hearing instru- ments, for instance, are often expected to suppress distract- ing or even annoying acoustic background sounds, whereas enjoyable acoustic scenes should be enhanced. Such acoustic scenes can often be differentiated in a fairly \ufb01ne-grained way, e.g. users might like some speci\ufb01c pop artists, but dislikes other pop entertainers. We currently live in a constantly changing world, and the amount of person- alization that can be predicted and implemented in advance is quite limited. This pre-training of device parameters can- not cover all possible acoustic scenes (databases) to provide user speci\ufb01c settings. Furthermore, new acoustic scenes might evolve (e.g. new music types), and the user might change his preference over time. Hence, we need to look at real-time online adaptive algorithms. Many classi\ufb01cation results are based on batch learning, where the training set with complete or partial labeling is static and given at the training time. State-of-the-art meth- ods for classi\ufb01cation are Support Vector Machines (SVM) [1] which have been successfully employed in genre/artist classi\ufb01cation [2] as well as in active learning [3]. A SVM classi\ufb01er is a kernel method which \ufb01nds an optimal separat- ing hyperplane of the data points x in a high-dimensional projection space \u03a6(x), without having to explicitly expand the data representation in this high-dimensional space. This computational advantage is achieved via the kernel trick, i.e., the scalar product in this high dimensional space as- sumes a functional form de\ufb01ned by the kernel. The large margin discriminatory hyperplane in this high dimensional space accounts for the success of SVM in batch classi\ufb01ers. In our scenario we deal with the user\u2019s preference. Here, batch algorithms are no longer applicable since data are not available in advance, nor can the system select data points from a known database for querying (e.g. in active learn- ing). Furthermore, temporal preference changes can not be modeled via batch learning, since the time information is discarded. Hence we need to consider a sequential online learning paradigm. In sequential online learning [4], the data are processed as a stream, and the algorithm updates the clas- si\ufb01er parameters online. Here, we use two online-learning algorithms: an incremental SVM (LASVM) [5] and online passive-aggressive algorithms (PA) [6]. Both algorithms are discriminative kernel algorithms that learn sequentially. A drawback of kernelized online large margin methods is the tendency to invoke more and more support vectors which in- evitably will violate the constraints on computational costs and space. To maintain the advantages of kernelization without the space/computational violations, we introduce a in\ufb01nite mem- ory extension of kernel machines (speci\ufb01cally to PA) which integrates the advantages of kernelization with the memory space limitations. 2 BACKGROUND In sequential learning, the data is accessible at times t = 1, ..., T, where the horizon T can be arbitrarily large. At each time step t, a new observation xt is presented to the system, which predicts a label ft(xt). After this prediction, the correct label yt is revealed to the system which uses this information to update the classi\ufb01er ft+1. The new classi\ufb01er ft+1 is used for the next time step t + 1. This paper presents an extension of the passive-aggressive algorithm (PA) [6] for online preference learning and it com- pares it with an incremental SVM learning system (LASVM) [5]. Both algorithms learn discriminative classi\ufb01ers in a se- quential manner, i.e., LASVM optimizes a quadratic cost 167 ISMIR 2008 \u2013 Session 2a \u2013 Music Recommendation and Organization function, whereas PA implements gradient descent. Our variation of PA replaces the scalar product in the linear form of PA by a kernel expansion.",
        "zenodo_id": 1415962,
        "dblp_key": "conf/ismir/MohB08"
    },
    {
        "title": "Using Expressive Trends for Identifying Violin P erformers.",
        "author": [
            "Miguel Molina-Solana",
            "Josep Llu\u00eds Arcos",
            "Emilia G\u00f3mez"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417907",
        "url": "https://doi.org/10.5281/zenodo.1417907",
        "ee": "https://zenodo.org/records/1417907/files/Molina-SolanaAG08.pdf",
        "abstract": "This paper presents a new approach for identifying profes- sional performers in commercial recordings. We propose a Trend-based model that, analyzing the way Narmour\u2019s Im- plication-Realization patterns are played, is able to charac- terize performers. Concretely, starting from automatically extracted descriptors provided by state-of-the-art extraction tools, the system performs a mapping to a set of qualita- tive behavior shapes and constructs a collection of frequency distributions for each descriptor. Experiments were con- ducted in a data-set of violin recordings from 23 different performers. Reported results show that our approach is able to achieve high identi\ufb01cation rates. 1 INTRODUCTION Expressive performance analysis and representation is cur- rently a key challenge in the sound and music computing area. Previous research has addressed expressive music per- formance using machine learning techniques. For example, Juslin et al. [6] studied how expressivity could be com- putationally modeled. Ramirez et al. [12] have proposed an approach for identifying saxophone performers by their playing styles. Lopez de Mantaras et al. [9] proposed a case based reasoning approach to deal with expressiveness. Hong [7] investigated expressive timing and dynamics in recorded cello. Dovey [2] analyzed Rachmaninoff\u2019s piano performances using inductive logic programming. Work on automatic piano performer identi\ufb01cation has been done by the group led by Gerhard Widmer. To cite some, in [14] they represent pianists\u2019 performances as strings; in [16] they study how to measure performance aspects applying ma- chine learning techniques; and in [15], Stamatatos and Wid- mer propose a set of simple features for representing stylis- tic characteristics of piano music performers. Sapp [13] work should also be cited as an interesting proposal for representing musical performances by means of scape plots based on tempo and loudness correlation. Goebl [3] is fo- cused on \ufb01nding a \u2018standard performance\u2019 by exploring the consensus among different performers. In this paper, we focus on the task of identifying violin- ists from their playing style using descriptors automatically extracted from commercial audio recordings by means of state-of-the-art feature extraction tools. First, since we con- sider recordings from quite different sources, we assume a high heterogeneity in the recording conditions. Second, as state-of-the-art audio transcription and feature extraction tools are not 100% precise, we assume a partial accuracy in the extraction of audio features. Taking into account these characteristics of the data, our proposal therefore identi\ufb01es violin performers through the following three stage process: (1) using a higher-level ab- straction of the automatic transcription focusing on the mel- odic contour; (2) tagging melodic segments according to the E. Narmour\u2019s Implication-Realization (IR) model [10]; and (3) characterizing the way melodic patterns are played as probabilistic distributions. The rest of the paper is organized as follows: In Section 2 we present the data collection being used. In Section 3, we describe the proposed Trend-Based model and the developed system, including data gathering, representation of record- ings and distance measurement. In Section 4, experiments for the case study are explained and results are presented. The paper concludes with \ufb01nal considerations and pointing out to future work in Section 5. 2 MUSICAL DATA We have chosen to work with Sonatas and Partitas for solo violin from J.S. Bach [8]. Sonatas and Partitas for solo Vio- lin by J.S. Bach is a six work collection (three Sonatas and three Partitas) composed by the German musician. It is a well-known collection that almost every violinist plays dur- ing its artistic life. All of them have been recorded many times by several players. The reason of using this work collection is twofold: 1) we have the opportunity of test- ing our model with existing commercial recordings of the best known violin performers, and 2) we can constrain our research on monophonic music. In our experiments, we have extended the musical collec- tion presented in [11]. We analyzed music recordings from 23 different professional performers. Because these audio \ufb01les were not recorded for our study, we have not interfered at all with players\u2019 style at the performance [5]. The scores 495 ISMIR 2008 \u2013 Session 4b \u2013 Musical Expression and Meaning of the analyzed pieces are not provided to the system. 3 TREND-BASED MODELING Our approach for dealing with the identi\ufb01cation of violin performers is based on the acquisition of trend models that characterize each particular performer to be identi\ufb01ed. Speci- \ufb01cally, a trend model characterizes, for a speci\ufb01c audio de- scriptor, the relationships a given performer is establishing among groups of neighbor musical events. We perform a qualitative analysis of the variations of the audio descrip- tors. Moreover, as we will describe in the next subsection, we analyze these qualitative variations with a local perspec- tive. We will be using two trend models in this paper: energy and duration. The trend model for the energy descriptor re- lates, qualitatively, the energy variation for a given set of consecutive notes, while the trend model for duration indi- cates, also qualitatively, how note durations change for note sequences. Notice that trend models are not trying to char- acterize the audio descriptors with respect to an expected global behavior. Given an input musical recording of a piece, the trend analysis is performed by aggregating the qualitative varia- tions on their small melody segments. Thus, in advance of building trend models, input streams are broken down into segments. As most of automatic melody segmentation ap- proaches, we will perform note grouping according to a hu- man perception model. Our system has been designed in a modular way with the intention of creating an easy extendable framework. We have three different types of modules in the system: 1) the audio feature extraction modules; 2) the trend analysis mod- ules; and 3) the identi\ufb01cation modules. Moreover, the sys- tem may work in two different modes: in a training mode or in a testing mode. Modules from (1) and (2) are used in both modes. Modules from (3) are only used in the testing mode. Figure 1 shows a diagram of the system modules. On top, audio \ufb01les in .wav format as input. At the training stage, the goal of the system is to char- acterize performers by extracting expressive features and constructing trend models. Next, at the identi\ufb01cation stage, the system analyzes the input performance and looks for the most similar previously learnt model. The training process is composed of three main steps: 1) the extraction of audio descriptors and the division of a performance in segments; 2) the tagging of segments according to IR patterns; and 3) the calculus of probabilistic distributions for each IR pattern and descriptor (trend generation).",
        "zenodo_id": 1417907,
        "dblp_key": "conf/ismir/Molina-SolanaAG08"
    },
    {
        "title": "Joint Structure Analysis with Applications to Music Annotation and Synchronization.",
        "author": [
            "Meinard M\u00fcller",
            "Sebastian Ewert"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415236",
        "url": "https://doi.org/10.5281/zenodo.1415236",
        "ee": "https://zenodo.org/records/1415236/files/MullerE08.pdf",
        "abstract": "The general goal of music synchronization is to automati- cally align different versions and interpretations related to a given musical work. In computing such alignments, re- cent approaches assume that the versions to be aligned cor- respond to each other with respect to their overall global structure. However, in real-world scenarios, this assump- tion is often violated. For example, for a popular song there often exist various structurally different album, radio, or ex- tended versions. Or, in classical music, different recordings of the same piece may exhibit omissions of repetitions or signi\ufb01cant differences in parts such as solo cadenzas. In this paper, we introduce a novel approach for automatically de- tecting structural similarities and differences between two given versions of the same piece. The key idea is to perform a single structural analysis for both versions simultaneously instead of performing two separate analyses for each of the two versions. Such a joint structure analysis reveals the re- petitions within and across the two versions. As a further contribution, we show how this information can be used for deriving musically meaningful partial alignments and anno- tations in the presence of structural variations. 1 INTRODUCTION Modern digital music collections contain an increasing number of relevant digital documents for a single musical work comprising various audio recordings, MIDI \ufb01les, or symbolic score representations. In order to coordinate the multiple information sources, various synchronization pro- cedures have been proposed to automatically align musi- cally corresponding events in different versions of a given musical work, see [1, 7, 8, 9, 14, 15] and the references therein. Most of these procedures rely on some variant of dynamic time warping (DTW) and assume a global corre- spondence of the two versions to be aligned. In real-world scenarios, however, different versions of the same piece may exhibit signi\ufb01cant structural variations. For example, in the case of Western classical music, different recordings often The research was funded by the German Research Foundation (DFG) and the Cluster of Excellence on Multimodal Computing and Interaction. exhibit omissions of repetitions (e. g., in sonatas and sym- phonies) or signi\ufb01cant differences in parts such as solo ca- denzas of concertos. Similarly, for a given popular, folk, or art song, there may be various recordings with a different number of stanzas. In particular for popular songs, there may exist structurally different album, radio, or extended versions as well as cover versions. A basic idea to deal with structural differences in the synchronization context is to combine methods from mu- sic structure analysis and music alignment. In a \ufb01rst step, one may partition the two versions to be aligned into musi- cally meaningful segments. Here, one can use methods from automated structure analysis [3, 5, 10, 12, 13] to derive sim- ilarity clusters that represent the repetitive structure of the two versions. In a second step, the two versions can then be compared on the segment level with the objective for match- ing musically corresponding passages. Finally, each pair of matched segments can be synchronized using global align- ment strategies. In theory, this seems to be a straightforward approach. In practise, however, one has to deal with several problems due to the variability of the underlying data. In particular, the automated extraction of the repetitive struc- ture constitutes a delicate task in case the repetitions reveal signi\ufb01cant differences in tempo, dynamics, or instrumen- tation. Flaws in the structural analysis, however, may be aggravated in the subsequent segment-based matching step leading to strongly corrupted synchronization results. The key idea of this paper is to perform a single, joint structure analysis for both versions to be aligned, which pro- vides richer and more consistent structural data than in the case of two separate analyses. The resulting similarity clus- ters not only reveal the repetitions within and across the two versions, but also induce musically meaningful partial align- ments between the two versions. In Sect. 2, we describe our procedure for a joint structure analysis. As a further con- tribution of this paper, we show how the joint structure can be used for deriving a musically meaningful partial align- ment between two audio recordings with structural differ- ences, see Sect. 3. Furthermore, as described in Sect. 4, our procedure can be applied for automatic annotation of a given audio recording by partially available MIDI data. In Sect. 5, we conclude with a discussion of open problems and 389 ISMIR 2008 \u2013 Session 3c \u2013 OMR, Alignment and Annotation 0 100 200 300 0 50 100 150 200 250 300 350 0",
        "zenodo_id": 1415236,
        "dblp_key": "conf/ismir/MullerE08"
    },
    {
        "title": "A Robot Singer with Music Recognition Based on Real-Time Beat Tracking.",
        "author": [
            "Kazumasa Murata",
            "Kazuhiro Nakadai",
            "Kazuyoshi Yoshii",
            "Ryu Takeda",
            "Toyotaka Torii",
            "Hiroshi G. Okuno",
            "Yuji Hasegawa",
            "Hiroshi Tsujino"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415108",
        "url": "https://doi.org/10.5281/zenodo.1415108",
        "ee": "https://zenodo.org/records/1415108/files/MurataNYTTOHT08.pdf",
        "abstract": "A robot that can provide an active and enjoyable user inter- face is one of the most challenging applications for music information processing, because the robot should cope with high-power noises including self voices and motor noises. This paper proposes noise-robust musical beat tracking by using a robot-embedded microphone, and describes its ap- plication to a robot singer with music recognition. The pro- posed beat tracking introduces two key techniques, that is, spectro-temporal pattern matching and echo cancellation. The former realizes robust tempo estimation with a shorter window length, thus, it can quickly adapt to tempo changes. The latter is effective to cancel self periodic noises such as stepping, scatting, and singing. We constructed a robot singer based on the proposed beat tracking for Honda ASIMO. The robot detects a musical beat with its own microphone in a noisy environment. It tries to recognize music based on the detected musical beat. When it successfully recognizes mu- sic, it sings while stepping according to the beat. Otherwise, it performs scatting instead of singing because the lyrics are unavailable. Experimental results showed fast adaptation to tempo changes and high robustness in beat tracking even when stepping, scatting and singing. 1 INTRODUCTION Music information processing draws attention of researchers and industrial people for recent years. Many techniques in music information processing such as music information re- trieval are mainly applied to music user interfaces for cellu- lar phones, PDAs and PCs, and various commercial services have been launched[12]. On the other hand, robots like hu- manoid robots are recently getting popular. They are ex- pected to help us in a daily environment as intelligent phys- ical agents in the future. This means that the robot should not only perform tasks but also make us more enjoyable than PDA or PC based interface. Thus, music is important media for such rich human-robot interaction because music is one of the popular hobbies for humans. This will contribute to MIR society in a sense that robot provides real-world MIR applications. Therefore, we started to apply music informa- tion processing to robots. As a \ufb01rst step, we focused on musical beat tracking because it is a basic function to recog- nize music. However, to be applied to a robot , three issues should be considered for beat tracking as follows:",
        "zenodo_id": 1415108,
        "dblp_key": "conf/ismir/MurataNYTTOHT08"
    },
    {
        "title": "Non-Negative Matrix Division for the Automatic Transcription of Polyphonic Music.",
        "author": [
            "Bernhard Niedermayer"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415040",
        "url": "https://doi.org/10.5281/zenodo.1415040",
        "ee": "https://zenodo.org/records/1415040/files/Niedermayer08.pdf",
        "abstract": "In this paper we present a new method in the style of non-negative matrix factorization for automatic tran- scription of polyphonic music played by a single instru- ment (e.g., a piano). We suggest using a \ufb01xed repos- itory of base vectors corresponding to tone models of single pitches played on a certain instrument. This as- sumption turns the blind factorization into a kind of non-negative matrix division for which an algorithm is presented. The same algorithm can be applied for learning the model dictionary from sample tones as well. This method is biased towards the instrument used during the training phase. But this is admissible in applications like performance analysis of solo music. The proposed approach is tested on a Mozart sonata where a symbolic representation is available as well as the recording on a computer controlled grand piano. 1 INTRODUCTION Transcription of polyphonic music is a di\ufb03cult task even for humans after several years of musical training. In the computational \ufb01eld people have been working on the problem of extracting single note events from com- plex music recordings for more than three decades. In special cases like monophonic music some systems have proven to be successful [1]. But pieces where more than one note is present at a time are much more challenging. Consonant tones in Western music often have frequency relations close to simple integer ratios and therefore cause overlapping harmonics. So when considering a power spectrum the mapping of found energies to cer- tain fundamental frequencies is usually ambiguous. A review of transcription methods is given in [4] and [5], clustering them into three main approaches. The \ufb01rst systems were built on pure bottom-up principles without considering any higher level knowledge. Al- though these algorithms used to \ufb01t very speci\ufb01c cases only, recent works like [6] or [13] show that bottom- up methods have overcome those early restrictions. A second group of transcription methods, like used by [12], is based on blackboard systems. Here low-level information gathered by digital signal processing and frame-wise description of the auditory scene as well as high-level prior knowledge is used to support or discard hypotheses at multiple levels. The third major approach to music transcription is made up of model based algorithms. Similar to black- board systems they also include high-level information as well as low-level signal based features. The di\ufb00er- ence is that prior knowledge is fed into the system by introducing a model of the analyzed data. The signal is then processed in order to estimate the model\u2019s pa- rameters. The results of these methods can only be as good as the assumed model \ufb01ts the actual data. Works like [14] or [2] are examples of this class. During the last years the methods of non-negative matrix factorization (NMF) [10], independent compo- nent analysis (ICA) [9] and sparse coding [13] became of increasing interest in audio analysis. The basic idea is the introduction of hidden, not directly observable atoms. The above cited methods decompose an input matrix into two factors where one is a dictionary de- scribing the individual atoms and the other gives the activation of these components as a function of time. The non-negativity constraint is derived from the areas of application where single observations linearly add up to the whole. For instance in audio analysis it would not make sense to consider notes with negative loudness. Since the only prior knowledge is the maximum num- ber of independent components these algorithms are part of the group of bottom-up methods as described above. In this paper we propose a new method where the ideas of matrix factorization and sparse, independent components are adapted to follow a model based ap- proach. Studies on the human approach to music tran- scription [5] have shown that trained musicians use lots of background information like the style of the piece or the instruments playing. They expect certain tim- bres and therefore would for example never search for distorted guitar tones in a classical piano piece. Apply- ing this principle to the non-negative matrix factoriza- tion, prior knowledge about the dictionary is incorpo- rated. The activation matrix will then remain the only 544 ISMIR 2008 \u2013 Session 4c \u2013 Automatic Music Analysis and Transcription unknown component which needs an operation like a non-negative matrix division in order to be calculated. Section 2 of this paper focuses on the NMF and its shortcomings in the context of transcription of solo mu- sic as a motivation for our method that is then ex- plained in detail. Section 3 describes how the tone models representing a certain instrument can be ex- tracted from sample recordings. The post processing step transforming the activation patterns yielded by the matrix division into discrete note events is described in section 4. Sections on the experimentation results and our conclusions complete the paper. 2 PITCH DECOMPOSITION",
        "zenodo_id": 1415040,
        "dblp_key": "conf/ismir/Niedermayer08"
    },
    {
        "title": "Development of an Automatic Music Selection System Based on Runner&apos;s Step Frequency.",
        "author": [
            "Masahiro Niitsuma",
            "Hiroshi Takaesu",
            "Hazuki Demachi",
            "Masaki Oono",
            "Hiroaki Saito"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415900",
        "url": "https://doi.org/10.5281/zenodo.1415900",
        "ee": "https://zenodo.org/records/1415900/files/NiitsumaTDOS08.pdf",
        "abstract": "This paper presents an automatic music selection system based on runner\u2019s step frequency. Recent development of portable music players like iPod has increased the number of those who listen to music while exercising. However, few systems which connect exercises with music selection have been developed. We propose a system that automati- cally selects music suitable for user\u2019s running exercises. Al- though many parameters can be taken into account, as a \ufb01rst step we focus on runner\u2019s step frequency. This system se- lects music with tempo suitable for runner\u2019s step frequency and when runner\u2019s step frequency changes, it executes an- other music selection. The system consists of three mod- ules: step frequency estimation, music selection, and music playing. In the \ufb01rst module, runner\u2019s step frequency is esti- mated from data derived from an acceleration sensor. In the second module, appropriate music is selected based on the estimated step frequency. In the third module, the selected music is played until runner\u2019s step frequency changes. In the experiment, subjects ran on a running machine at dif- ferent paces listening to the music selected by the proposed system. Experimental results show that the system can es- timate runner\u2019s SPM accurately and on the basis of the es- timated SPM it can select music appropriate for users\u2019 ex- ercises with more than 85.0% accuracy, and makes running exercises more pleasing. 1 INTRODUCTION Recently, people can collect massive volumes of digital mu- sic data easily due to ef\ufb01cient audio compression techniques like MP3 and online music distribution services. Portable music players like iPod have enabled people to listen to mu- sic while doing other things: driving, browsing, or sporting. These circumstances have brought great demand for a sys- tem that selects music data suitable for their listening envi- ronments from among massive volumes of music data. There are some commercial products concerning auto- matic music selection. Portable music players for exercising such as NW-S203F [13] and Nike+iPod Sport Kit [2] have attracted public attention. NW-S203F has an acceleration sensor, with which it can count steps, measure distance trav- eled, and track calories burned. Nike+iPod Sport Kit con- sists of a wireless sensor attached to a shoe and a receiver plugged into a portable music player. The sensor sends in- formation such as pace, distance, and calories burned to the receiver, and the music player can store and display this data. These products function not only as portable music players but also as exercise equipment. However, these func- tions are independent and the information which is obtained while exercising does not in\ufb02uence music selection. There are also some previous studies on automatic mu- sic selection systems using user\u2019s information. MPTrain [9, 10, 11] is designed as a mobile and personal system (hardware and software) that users wear while exercising such as walking, jogging or running. MPTrain\u2019s hardware includes a set of physiological sensors wirelessly connected to a mobile phone carried by the user. MPTrain does not take any action until about 10 seconds before the end of the current song. It then determines whether the user needs to increase, decrease or keep the running pace, by comparing the user\u2019s current heart-rate with the desired heart-rate from the desired workout for that day. Once it has determined the action to take, it searches the user\u2019s digital music library (DML) for the optimal song to play. The context-aware mobile music player PersonalSoundtrack described in [4] works with its owner\u2019s library to select music in real-time based on a taxonomy of attributes and contextual informa- tion derived from an accelerometer connected wirelessly to a laptop carried under the arm. On the basis of user feedback and analysis, a hand-held device is implemented for testing in less constrained mobile scenarios. These systems select music automatically based on user\u2019s listening environments, however, these studies do not in- vestigate in detail how the musical tempo in\ufb02uences user\u2019s pleasingness, nor do an objective evaluation to show how ac- curately their system can estimate runner\u2019s step frequency. In this paper, we propose an automatic music selection system that interacts with user\u2019s running exercises. Many parameters can be taken into account, but as a \ufb01rst step we focus on runner\u2019s step frequency. We conducted a prelim- inary experiment to investigate how actual musical tempo 193 ISMIR 2008 \u2013 Session 2b \u2013 Music Recognition and Visualization in\ufb02uences runner\u2019s pleasingness in the relation with his step frequency. On the basis of the result, we develop a sys- tem that automatically selects music with appropriate tempo based on runner\u2019s step frequency. This paper is structured as follows: in Section 2 we investigate how actual BPM in\ufb02u- ences user\u2019s pleasingness. In Section 3 we propose an au- tomatic music selection system re\ufb02ecting runner\u2019s step fre- quency. In Section 4 we present experiments to evaluate the proposed system. Finally in Section 5 we offer concluding remarks and discuss future work. 2 PRELIMINARY EXPERIMENT In this paper, the developed system which automatically se- lects music by tempo is described. Tempo is de\ufb01ned as speed at which a musical composition is played [12], and represented by BPM (Beat Per Minute). Some experiments have shown that musical tempo has a great effect on people in the areas of performance of track athletes [3] and on gen- eral spatial awareness, arousal and mood [7]. This previous paper [1] studies the in\ufb02uence of the tempo of various pieces of music on SPM (Step Per Minute) of users running while listening to them. It concludes that there is no correlation between tempo and SPM, but there is signi\ufb01cant correlation between tempo and subjective rating. These experiments imply that musical tempo is strongly connected with human perception. In order to investigate how actual BPM in\ufb02uences user\u2019s pleasingness, we conducted a preliminary experiment. In the preliminary experiment, six subjects (university students) ran at constant step frequency (150 SPM) listening to two different kinds of music: one is calm and melodious and has a weak beat (Track1), while the other is rhythmical and has a strong beat (Track2). They were composed for the ex- periment, and BPM of each song was changed at each time :150\u00b15, 10, 35, 40, 45, 70, and 80. Figure 1 indicates the result of the experiment. Corre- lation coef\ufb01cient between the difference between SPM and BPM and evaluated pleasingness of Track1 was -0.589 and that of Track2 was -0.634. This result leads us to the con- clusion that people tend to feel pleased when BPM is near their SPM. 3 AUTOMATIC MUSIC SELECTION SYSTEM",
        "zenodo_id": 1415900,
        "dblp_key": "conf/ismir/NiitsumaTDOS08"
    },
    {
        "title": "N-Gram Chord Profiles for Composer Style Representation.",
        "author": [
            "Mitsunori Ogihara",
            "Tao Li 0001"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415956",
        "url": "https://doi.org/10.5281/zenodo.1415956",
        "ee": "https://zenodo.org/records/1415956/files/OgiharaL08.pdf",
        "abstract": "This paper studies the problem of using weighted N- grams of chord sequences to construct the pro\ufb01le of a com- poser. The N-gram pro\ufb01le of a chord sequence is the col- lection of all N-grams appearing in a sequence where each N-gram is given a weight proportional to its beat count. The N-gram pro\ufb01le of a collection of chord sequences is the sim- ple average of the N-gram pro\ufb01le of all the chord sequences in the collection. Similarity of two composers is measured by the cosine of their respective pro\ufb01les, which has a value in the range [0, 1]. Using the cosine-based similarity, a group of composers is clustered into a hierarchy, which appears to be explicable. Also, the composition style can be identi\ufb01ed using N-gram signatures. 1 INTRODUCTION The chord progression is an important component in music. Musicians and listeners speak of novel and in\ufb02uential chord progressions, typi\ufb01ed in the First Act Prelude of \u201cTristan und Isolde\u201d by Richard Wagner, in \u201cBecause\u201d by The Bea- tles, and in \u201cGiant Steps\u201d by John Coltrane. Among many genres of music the role of chord progressions is the most signi\ufb01cant in Jazz, where performances are improvisational and thus performers often choose to play tunes with interest- ing chord progressions. While many jazz compositions are constructed based on well-known chord progressions, such as 12-bar blues progressions and \u201cI Got Rhythm\u201d by George Gershwin, there are composers, such as Thelonius Monk and Wayne Shorter, whose chord progressions are thought of as unique. The importance of chord progressions in Jazz raises the questions of whether they can be effectively used for music retrieval and whether they can be used to charac- terize composers, which we will address in this paper. An approach to the problem of comparing two chord pro- gressions is sequence alignment, as often used in melody- based music retrieval (see, e.g., [2, 6, 11, 12]). The ba- sis for the sequence-alignment approach is a theory that explains transformation of a chord progression to another (see, e.g., [8, 10]). Such a generative theory offers musi- cal understanding of how two progressions are similar, but has a limitation that not all pairs of progressions are neces- sarily comparable. Also, for comparing multiple progres- sions with each other, generative-theory-based comparisons may be too computation-intensive. This consideration sug- gests the use of N-grams\u2014the patterns consisting of N- consecutive chords that appear in a chord sequence. The use of N-grams is very popular in natural language under- standing (see, e.g., [5]). In music information retrieval, N- grams have been shown effective for melodic contour anal- ysis [3, 4, 9]. Also, the recent work of Mauch et al. [7] use 4-grams to examine the chord sequences of The Beatles. While the chord sequences are an important subject in musicology, we believe that they can be incorporated into a tune retrieval system where the tunes are indexed with meta- data and chord sequence. In such a system, a user provides a chord sequence (either typed or copy-pasted from a se- quence on screen) as input and the system retrieves tunes that contain a part with either exactly the same as (with the possibility of allowing transposition of the key) or similar to the input sequence, where the input chord progression is speci\ufb01ed using an unambiguous notation system (such as the one in [1]). Highly prominent in the Jazz harmony are the 6th, 7th and major 7th notes and the tensions (the 9th, the 11th, and the 13th notes). The former signify the functions that chords possess while the latter add color to triads. Chord progres- sion analysis in terms of triads is likely to enable fundamen- tal understanding of the chord structure, but perhaps deeper understanding can be obtained by examining these non-triad notes. Our work extends [7] by considering functional and additional notes such as the 6th, 7th, and tensions, by creat- ing a pro\ufb01le out of N-gram data, and then by assigning the similarity between two pro\ufb01les.",
        "zenodo_id": 1415956,
        "dblp_key": "conf/ismir/OgiharaL08"
    },
    {
        "title": "A Real-time Equalizer of Harmonic and Percussive Components in Music Signals.",
        "author": [
            "Nobutaka Ono",
            "Kenichi Miyamoto",
            "Hirokazu Kameoka",
            "Shigeki Sagayama"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415044",
        "url": "https://doi.org/10.5281/zenodo.1415044",
        "ee": "https://zenodo.org/records/1415044/files/OnoMKS08.pdf",
        "abstract": "In this paper, we present a real-time equalizer to control a volume balance of harmonic and percussive components in music signals without a priori knowledge of scores or in- cluded instruments. The harmonic and percussive compo- nents of music signals have much different structures in the power spectrogram domain, the former is horizontal, while the latter is vertical. Exploiting the anisotropy, our methods separate input music signals into them based on the MAP es- timation framework. We derive two kind of algorithm based on a I-divergence-based mixing model and a hard mixing model. Although they include iterative update equations, we realized the real-time processing by a sliding analysis technique. The separated harmonic and percussive com- ponents are \ufb01nally remixed in an arbitrary volume balance and played. We show the prototype system implemented on Windows environment. 1 INTRODUCTION A graphic equalizer is one of the most popular tools on an audio player, which allows an user to control the volume balance between frequency bands as its preference by sep- arating an input audio signal by several band-pass \ufb01lters and remixing them with different gains. Recently, based on other kinds of separation, more advanced audio equal- izations have been discussed and developed [1, 2, 3], which increase the variety of modifying audio sounds and enrich functions of audio players. In this paper, focusing on two different components in- cluded in music signals: harmonic and percussive ones, we present a technique to equalize them in real-time without a priori knowledge of the scores or the included instruments. Not only as an extended audio equalizer, the technique should yield the useful pre-processing for various tasks related to music information retrieval from audio signals [4]. It can suppress percussive tones, which often interfere multipitch analysis, while, suppression of harmonic component will fa- cilitate drum detection or rhythm analysis. We have cur- rently applied this technique to automatic chord detection based on emphasized chroma features [5], rhythm pattern extraction and rhythm structure analysis [6], and melody ex- traction. For independently equalizing the harmonic and percus- sive components, it is required to separate them. This kind of separation problem has been widely discussed in the lit- erature. Uhle et al. applied Independent Component Anal- ysis (ICA) to the magnitude spectrogram, and classi\ufb01ed the extracted independent components into a harmonic and a percussive groups based on the several features like percus- siveness, noise-likeness, etc [7]. Helen et al. utilized Non- negative Matrix Factorization (NMF) for decomposing the spectrogram into elementary patterns and classi\ufb01ed them by pre-trained Support Vector Machine (SVM) [8]. Through modeling harmonic and inharmonic tones on spectrogram, Itoyama et al. aimed to an instrument equalizer and pro- posed separation of an audio signal to each track based on the MIDI information synchronized to the input audio signal [1]. The contribution of this paper is to derive a simple and real-time algorithm speci\ufb01cally for the harmonic/percussive separation without any pre-learning or a priori knowledge of score or included instruments of the input audio signals. We present the formulation of the separation in Maximum A Priori (MAP) estimation framework, derive the fast iter- ative solution to it by auxiliary function approach, imple- ment it with sliding update technique for real-time process- ing, and examine the performance by experiments to popu- lar and jazz music songs. 2 FORMULATION OF HARMONIC/PERCUSSIVE SEPARATION",
        "zenodo_id": 1415044,
        "dblp_key": "conf/ismir/OnoMKS08"
    },
    {
        "title": "Hit Song Science Is Not Yet a Science.",
        "author": [
            "Fran\u00e7ois Pachet",
            "Pierre Roy"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417723",
        "url": "https://doi.org/10.5281/zenodo.1417723",
        "ee": "https://zenodo.org/records/1417723/files/PachetR08.pdf",
        "abstract": "We describe a large-scale experiment aiming at validating the hypothesis that the popularity of music titles can be predicted from global acoustic or human features. We use a 32.000 title database with 632 manually-entered labels per title including 3 related to the popularity of the title. Our experiment uses two audio feature sets, as well as the set of all the manually-entered labels but the popularity ones. The experiment shows that some subjective labels may indeed be reasonably well-learned by these techniques, but not popularity. This contradicts recent and sustained claims made in the MIR community and in the media about the existence of 'Hit Song Science'.",
        "zenodo_id": 1417723,
        "dblp_key": "conf/ismir/PachetR08"
    },
    {
        "title": "Music Genre Classification: A Multilinear Approach.",
        "author": [
            "Ioannis Panagakis",
            "Emmanouil Benetos",
            "Constantine Kotropoulos"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1414714",
        "url": "https://doi.org/10.5281/zenodo.1414714",
        "ee": "https://zenodo.org/records/1414714/files/PanagakisBK08.pdf",
        "abstract": "In this paper, music genre classi\ufb01cation is addressed in a multilinear perspective. Inspired by a model of auditory cortical processing, multiscale spectro-temporal modulation features are extracted. Such spectro-temporal modulation features have been successfully used in various content-based audio classi\ufb01cation tasks recently, but not yet in music genre classi\ufb01cation. Each recording is represented by a third-order feature tensor generated by the auditory model. Thus, the ensemble of recordings is represented by a fourth-order data tensor created by stacking the third-order feature tensors associated to the recordings. To handle large data tensors and derive compact feature vectors suitable for classi\ufb01ca- tion, three multilinear subspace techniques are examined, namely the Non-Negative Tensor Factorization (NTF), the High-Order Singular Value Decomposition (HOSVD), and the Multilinear Principal Component Analysis (MPCA). Clas- si\ufb01cation is performed by a Support Vector Machine. Strat- i\ufb01ed cross-validation tests on the GTZAN dataset and the ISMIR 2004 Genre one demonstrate the advantages of NTF and HOSVD versus MPCA. The best accuracies obtained by the proposed multilinear approach is comparable with those achieved by state-of-the-art music genre classi\ufb01cation algo- rithms. 1 INTRODUCTION To manage large music collections, tools able to extract use- ful information about musical pieces directly from audio signals are needed. Such information could include genre, mood, style, and performer [13]. Aucouturier and Pachet [1] indicate that music genre is probably the most popular description of music content. Classifying music recordings into distinguishable genres is an attractive research topic in Music Information Retrieval (MIR) community. Most of the music genre classi\ufb01cation techniques, em- ploy pattern recognition algorithms to classify feature vec- tors, extracted from short-time recording segments into gen- res. In general, the features employed for music genre clas- si\ufb01cation are roughly classi\ufb01ed into three classes: timbral texture features, rhythmic features, and pitch content fea- tures [20]. Commonly used classi\ufb01ers are Support Vector Machines (SVMs), Nearest-Neighbor (NN) classi\ufb01ers, or classi\ufb01ers, which resort to Gaussian Mixture Models, Lin- ear Discriminant Analysis (LDA), etc. Several common au- dio datasets have been used in experiments to make the re- ported classi\ufb01cation accuracies comparable. Notable results on music genre classi\ufb01cation are summarized in Table 1. Reference Dataset Accuracy Bergstra et al. [4] GTZAN 82.50% Li et al. [12] GTZAN 78.50% Lidy et al. [14] GTZAN 76.80% Benetos et al. [3] GTZAN 75.00% Holzapfel et al. [6] GTZAN 74.00% Tzanetakis et al. [20] GTZAN 61.00% Holzapfel et al. [6] ISMIR2004 83.50% Pampalk et al. [19] ISMIR2004 82.30% Lidy et al. [13] ISMIR2004 79.70% Bergstra et al. [4] MIREX2005 82.34% Lidy et al. [14] MIREX2007 75.57% Mandel et al. [16] MIREX2007 75.03% Table 1. Notable classi\ufb01cation accuracies achieved by mu- sic genre classi\ufb01cation approaches. Recently, within MIR community, genre has been crit- icized as being a hopelessly, ambiguous, and inconsistent way to organize and explore music. Consequently users\u2019 needs would be better addressed by abandoning it in favor of more generic music similarity-based approaches [17]. From another point of view, end users are more likely to browse and search by genre than either artist similarity or music similarity by recommendation [11]. Furthermore, Aucou- turier et al. [2] have observed that recent systems, which as- sess audio similarity using timbre-based features, have failed to offer signi\ufb01cant performance gains over early systems and in addition their success rates make them unrealistic for practical use. It is clear that new approaches are needed to make automatic genre classi\ufb01cation systems viable in prac- tice. McKay et al. [17] argues on the importance of con- tinuing research in automatic music genre classi\ufb01cation and 583 ISMIR 2008 \u2013 Session 5a \u2013 Content-Based Retrieval, Categorization and Similarity 2 encourages the MIR community to approach the problem in a inter-disciplinary manner. The novel aspects of this paper are as follows. First, we use bio-inspired multiscale spectro-temporal features for music genre classi\ufb01cation. Motivated by the fact that each sound is characterized by slow spectral and temporal mod- ulations and investigations on the auditory system [22], we use the auditory model proposed in [21] in order to extract features that map a given sound to a high-dimensional repre- sentation of its spectro-temporal modulations. The auditory high-dimensional representation can be viewed as a high- order tensor that is de\ufb01ned on a high-dimensional space. Note that in the \ufb01eld of multilinear algebra, tensors are con- sidered as the multidimensional equivalent of matrices or vectors [8]. In addition, cortical representations are highly redundant [18]. Therefore, it is reasonable to assume that the tensors are con\ufb01ned into a subspace of an intrinsically low dimension. Feature extraction or dimensionality reduction thus aims to transform such a high-dimensional representation into a low-dimensional one, while retaining most of the informa- tion related to the underlying structure of spectro-temporal modulations. Subspace methods are suitable for the afore- mentioned goal. Indeed subspace methods, such as Princi- pal Component Analysis (PCA), Linear Discriminant Anal- ysis (LDA), and Non-Negative Matrix Factorization (NMF) have successfully been used in various pattern recognition problems. The just mentioned methods deal only with vec- torized data. By vectorizing a typical three-dimensional cor- tical representation of 6 scales \u00d7 10 rates \u00d7 128 frequency bands results in a vector having dimension equal to 7680. Many pattern classi\ufb01ers, can not cope with such a high- dimensionality given a small number of training samples. In addition, handling such high-dimensional samples is com- putationally expensive. Therefore, eigen-analysis or Singu- lar Value Decomposition cannot be easily performed. De- spite implementation issues, it is well understood that re- shaping a 3D cortical representation into a vector, breaks the natural structure and correlation in the original data. Thus, in order to preserve natural structure and correlation in the original data, dimensionality reduction operating directly on tensors rather than vectors is desirable. State-of-the-art mul- tilinear dimensionality reduction techniques are employed, e.g. Non-Negative Tensor Factorization (NTF) [3], High- Order Singular Value Decomposition (HOSVD) [9], and Mul- tilinear Principal Component Analysis (MPCA) [15] in or- der to derive compact feature vectors suitable for classi\ufb01ca- tion. Classi\ufb01cation is performed by an SVM. Strati\ufb01ed cross-validation tests on two well-known data- sets, the GTZAN dataset and the ISMIR2004Genre dataset, demonstrate that the effectiveness of the proposed approach is compared with that of state-of-the-art music genre classi- \ufb01cation algorithms on the GTZAN dataset, while its accu- racy exceeds 80% on the ISMIR2004Genre one. In Section 2, the computational auditory model and the cortical representation of sound are described. Basic multi- linear algebra and multilinear subspace analysis techniques are brie\ufb02y introduced in Section 3. The application of mul- tilinear subspace analysis to cortical representations is dis- cussed in this section as well. Datasets and experimental results are presented in Section 4. Conclusions are drawn and future research direction are indicated in Section 5. 2 COMPUTATIONAL AUDITORY MODEL AND CORTICAL REPRESENTATION OF SOUND The computational auditory model, proposed in [21], is in- spired by psychoacoustic and neurophysiological investiga- tions in the early and central stages of the human auditory system. An acoustic signal is analyzed by the human audi- tory model and a four-dimensional representation of sound is obtained, the so-called cortical representation. The model consists of two basic stages. The \ufb01rst stage converts the acoustic signal into a neural representation, the so-called au- ditory spectrogram. This representation is a time-frequency distribution along a tonotopic (logarithmic frequency) axis. At the second stage, the spectral and temporal modulation content of the auditory spectrogram is estimated by mul- tiresolution wavelet analysis. For each frame, the multires- olution wavelet analysis is implemented via a bank of two- dimensional Gabor \ufb01lters, that are selective to spectrotem- poral modulation parameters ranging from slow to fast tem- poral rates (in Hertz) and from narrow to broad spectral scales (in Cycles/Octave). Since, for each frame, the anal- ysis yields a scale-rate-frequency representation, thus the analysis results in a four-dimensional representation of time, frequency, rate, and scale for the entire auditory spectro- gram. Mathematical formulation and details about the audi- tory model and the cortical representation of sound can be found in [18, 21]. Psychophysiological evidence justi\ufb01es the choice of sca- les \u2208{0.25, 0.5, 1, 2, 4, 8} (Cycles / Octave) and posi- tive and negative rates \u2208{\u00b12, \u00b14, \u00b18, \u00b116, \u00b132} (Hz) to represent the spectro-temporal modulation of sound. The cochlear model, employed by the \ufb01rst stage, has 128 \ufb01l- ters with 24 \ufb01lters per octave, covering 5 1 3 octaves along the tonotopic axis. For each sound recording, the extracted four- dimensional cortical representation is averaged along time and the average rate-scale-frequency cortical representation is thus obtained, that is naturally represented by a third- order tensor. Accordingly, the feature tensor D \u2208RI1\u00d7I2\u00d7I3 + , where I1 = Iscales = 6, I2 = Irates = 10, and I3 = Ifrequencies = 128 results. During the analysis, we have used the NSL Matlab toolbox 1 . 1 http:// www.isr.umd.edu/CAAR/pubs.html 584 ISMIR 2008 \u2013 Session 5a \u2013 Content-Based Retrieval, Categorization and Similarity 2 3 MULTILINEAR SUBSPACE ANALYSIS Recently, extensions of linear subspace analysis methods to handle high-order tensors have been proposed. In this sec- tion, three multilinear subspace analysis methods are brie\ufb02y addressed. To begin with, a short introduction in multilinear algebra is given.",
        "zenodo_id": 1414714,
        "dblp_key": "conf/ismir/PanagakisBK08"
    },
    {
        "title": "Music Structure Analysis Using a Probabilistic Fitness Measure and an Integrated Musicological Model.",
        "author": [
            "Jouni Paulus",
            "Anssi Klapuri"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415826",
        "url": "https://doi.org/10.5281/zenodo.1415826",
        "ee": "https://zenodo.org/records/1415826/files/PaulusK08.pdf",
        "abstract": "This paper presents a system for recovering the sectional form of a musical piece: segmentation and labelling of mu- sical parts such as chorus or verse. The system uses three types of acoustic features: mel-frequency cepstral coef\ufb01- cients, chroma, and rhythmogram. An analysed piece is \ufb01rst subdivided into a large amount of potential segments. The distance between each two segments is then calculated and the value is transformed to a probability that the two segments are occurrences of a same musical part. Different features are combined in the probability space and are used to de\ufb01ne a \ufb01tness measure for a candidate structure descrip- tion. Musicological knowledge of the temporal dependen- cies between the parts is integrated into the \ufb01tness measure. A novel search algorithm is presented for \ufb01nding the de- scription that maximises the \ufb01tness measure. The system is evaluated with a data set of 557 manually annotated pop- ular music pieces. The results suggest that integrating the musicological model to the \ufb01tness measure leads to a more reliable labelling of the parts than performing the labelling as a post-processing step. 1 INTRODUCTION Western popular music pieces tend to follow a sectional form where the piece can be thought to be constructed of smaller parts (e.g., verse or chorus) which may be repeated during the piece, often with slight variations. The analysis of mu- sic structure is the process of recovering a description of this kind from audio input. Automatic music structure analysis enables several ap- plications, including a structure-aware music player [4] and music summarisation or thumbnailing [3, 12, 9] (see [8] for a discussion of further applications). A popular aim has been to locate a representative clip of the piece, such as the chorus, but there are also systems which aim to describe the structure of the whole piece, for example [2, 7]. In the proposed method (block diagram illustrated in Fig- ure 1), the audio content is described using three sets of fea- This work was supported by the Academy of Finland, project No. 5213462 (Finnish Centre of Excellence pr ogram 2006 - 2011). INPUT AUDIO BEAT SYNC. FEATURE EXT. SELF-DISTANCE CALCULATION DISTANCE FEATURES MATRICES SEGMENT GENERATION SEGMENT LIST PAIRWISE MATCHING PAIR PROBABILITIES EXPLANATION SEARCH LABELLING STRUCTURE DESCRIPTION MUSICAL KNOWLEDGE Figure 1. Analysis system block diagram. tures: mel-frequency cepstral coef\ufb01cients (MFCCs), chroma, and rhythmogram, to address different perceptual dimen- sions of music. 1 The use of multiple features is motivated by [1], which suggests that changes in timbre and rhythm are important cues for detecting structural boundaries in ad- dition to repetitions. A large set of candidate segmentations of the piece is \ufb01rst constructed. All non-overlapping seg- ment pairs are then compared based on the features and two different distance measures: one based on the average value of a feature over a segment and the other matching the two temporal sequences of features in the segments. Utilising the distance values we calculate the probability of the two segments to be occurrences of same musical part (i.e., re- peats). The values are used as terms in a \ufb01tness measure which is used to rank different candidates for the piece struc- ture description. The found structural description consists of subdivision of the piece into segments and of forming groups of seg- ments that are occurrences of the same musical part. To make the description more informative, the segment groups are automatically named using musical part labels, such as \u201cverse\u201d or \u201cchorus\u201d. The labelling is done by utilising a mu- sicological model, in practice an N-gram model for musical parts estimated from a manually annotated data set. Two dif- ferent strategies for the labelling are evaluated: either per- 1 A similar set of three features have been used earlier in [5], but the features were considered individually instead using a combination of them. 369 ISMIR 2008 \u2013 Session 3b \u2013 Computational Musicology forming it as a post-processing step after the segments and their grouping has been decided, or by integrating it into the \ufb01tness measure for the descriptions. The latter method al- lows assigning the labels already while searching for the de- scription, and enable utilising the potentially useful musical information of the part N-grams in the search. The main original contributions of this paper concern the last two blocks in Figure 1, the pairwise matching of seg- ments, the search algorithm, and musicological model for naming the found parts. The performance of the proposed system is evaluated us- ing a data set of 557 popular music pieces with manually annotated structure information. Different con\ufb01gurations of the overall system are studied in order to determine potential points for improvement. 2 PROPOSED METHOD Different parts of the proposed method are described now in more detail.",
        "zenodo_id": 1415826,
        "dblp_key": "conf/ismir/PaulusK08"
    },
    {
        "title": "A Comparison of Statistical and Rule-Based Models of Melodic Segmentation.",
        "author": [
            "Marcus T. Pearce",
            "Daniel M\u00fcllensiefen",
            "Geraint A. Wiggins"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417115",
        "url": "https://doi.org/10.5281/zenodo.1417115",
        "ee": "https://zenodo.org/records/1417115/files/PearceMW08.pdf",
        "abstract": "We introduce a new model for melodic segmentation based on information-dynamic analysis of melodic structure. The performance of the model is compared to several existing algorithms in predicting the annotated phrase boundaries in a large corpus of folk music. 1 INTRODUCTION The segmentation of melodies into phrases is a fundamen- tal (pre-)processing step for many MIR applications includ- ing melodic feature computation, melody indexing, and re- trieval of melodic excerpts. In fact, the melodic phrase is often considered one of the most important basic units of musical content [16] and many large electronic corpora of music are structured or organised by phrases, for example, the Dictionary of Musical Themes by Barlow and Morgen- stern [2], the Essen Folksong Collection (EFSC) [33] or the RISM collection [28]. At the same time, melodic grouping is thought to be an important part of the perceptual processing of music [11, 14, 27]. It is also fundamental to the phrasing of a melody when sung or played. Melodic segmentation is a task that musicians and musical listeners perform regularly in their everyday musical practice. Several algorithms have been proposed for the auto- mated segmentation of melodies. These algorithms differ in their modelling approach (supervised learning, unsuper- vised learning, music-theoretic rules), and in the type of in- formation they use (global or local). In this paper, we introduce a new statistical model of melodic segmentation and compare its performance to sev- eral existing algorithms on a melody segmentation task. The motivation for this model comparison is two-fold: \ufb01rst, we are interested in the performance differences between dif- ferent types of model; and second, we aim to build a hy- brid model that achieves superior performance by combin- ing boundary predictions from different models. 2 BACKGROUND",
        "zenodo_id": 1417115,
        "dblp_key": "conf/ismir/PearceMW08"
    },
    {
        "title": "MCIpa: A Music Content Information Player and Annotator for Discovering Music.",
        "author": [
            "Geoffroy Peeters",
            "David Fenech",
            "Xavier Rodet"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415796",
        "url": "https://doi.org/10.5281/zenodo.1415796",
        "ee": "https://zenodo.org/records/1415796/files/PeetersFR08.pdf",
        "abstract": "In this paper, we present a new tool for intra-document brows- ing of musical pieces. This tool is a multimedia player which represents the content of a musical piece visually. Each type of musical content (structure, chords, downbeats/ beats, notes, events) is associated which a distinct visual representation. The user sees what he/ she is listening too. He can also browse inside the music according to the vi- sual content. For this, each type of visual object has a ded- icated feedback, either as an audio-feedback or as a play- head feedback. Content information can be extracted auto- matically from audio (using signal processing algorithms) or annotated by hand by the user. This multimedia player can also be used as an annotator tool guided by the content. 1 INTRODUCTION Content description of music based on audio signal anal- ysis has been the subject of many researches over the last few years. Applications such as query over large music col- lections for speci\ufb01c music characteristics (melody, genre, mood, singer type ...) or search-by-similarity (based on melody, chord progression or timbre ...) have now become possible. However, few works address the problem of using content-information to guide the user during its listening, to allow him/ her to have a better understanding of the musi- cal content, to help him/ her to browse inside a track by the content (intra-document browsing) or to help him compar- ing several pieces of music visually. On the other side, the development of music content- extraction algorithms relies for a large part on the availabil- ity of annotated music-audio. These annotations are used to learn concepts such as audio structure, audio chords or to check the performances of developed algorithms (multi- pitch or beat-positions estimation algorithms). Tools that al- low annotating music audio \ufb01les in terms of speci\ufb01c music characteristics (chords, beats, structure) are still missing. In this paper, we present a tool which has been developed in order to allow both user interaction with the music content and annotation of the music content. The paper is organized as follows. In part 2, we give an overview of currently ex- isting music player and audio annotation tools. From this overview we give a set of requirements for our tool. In part 3, we detail the various parts of our tool: its general architec- ture, the various types of content described and explain how the tool is used for annotation. In part 4, we give technical details about the development of our tool. 2 RELATED WORKS AND REQUIREMENTS Computer based media players are numerous (iTunes, Win- dows Media Player, Real Player, Win Amp . . . ). However, currently none of them propose a visualization of the con- tent of the music track in order to allow browsing of the document by its content. On the opposite, there exist sev- eral tools dedicated to the annotation of audio. \u201cAS (Au- dioScuplt) Annotation\u201d [12] is a free software developed by Ircam for Mac-OS-X . The main paradigm of this soft- ware is annotation over the visualization of the spectrogram. Many algorithms are integrated into the software such as transient detection, pitch detection . . . Annotation is done over the spectrogram using temporal markers or by drawing midi notes over a note-scaled spectrogram. \u201cSonic Visu- alizer\u201d [5] is an open source software (Linux/ Mac-OS-X/ Windows) developed by Queen Mary University of London. It is also based on annotation over the visualization of either the waveform or the spectrogram. The various annotations can be super-imposed using a set of visual masks. Content analysis is performed using external plug-ins (Vamp format) so that the user can plug-in its favourite content-analyzer. \u201cCLAM annotator\u201d [1] is an open source software (Linux/ Mac-OS-X/ Windows) developed by IUA-UPF. It is a frame- work for developing graphical interfaces and signal analy- sis algorithms. \u201cMUCOSA\u201d [11] also developed by IUA- UPF is an online system for global (a single description assigned to the whole \ufb01le duration) annotations of music tracks. \u201cWavesurfer\u201d [19] is an open source software (Linux/ Mac-OS-X/ Windows) developed by KTH. It is based on waveform and spectrogram visualization. Some algorithms for energy, pitch or formant estimation are included. An- notation is done by placing markers over the \ufb01le duration. Some plug-ins for music content analysis (such as beat anal- ysis [9]) have been developed. Wavesurfer has functionali- ties for browsing by content (markers) over the \ufb01le duration. \u201cPraat\u201d [3] is an open source software (Linux/ Mac-OS-X/ Windows) developed by IPS. It includes many signal anal- ysis algorithms (pitch, formant) but is mostly dedicated to speech analysis. \u201cAcousmographe\u201d [10] is a Windows XP software developed by the GRM. It is mainly based on anno- tation over spectrogram and includes a wide range of graph- ical tools in order to facilitate annotation reading (speci\ufb01c shape and colour for each annotation). \u201cTranscriber\u201d [6] is an open source software (Linux/ Mac-OS-X/ Windows) developed by the French DGA. It is mostly dedicated to the transcription of speech to text. It has functionalities for browsing by content (text) over the \ufb01le duration. \u201cAudac- 243 ISMIR 2008 \u2013 Session 2c \u2013 Knowledge Representation, Tags, Metadata Figure 1. MCIpa interface: [left] large view of the interface with music track browser (similarity matrix and structure rep- resentation) and music database browser, [right] detailed view of the music track browser (chord progression, drum events, downbeat/ beat positions and multi-pitch). ity\u201d [2] [13] is an open source audio multi-tracks recorder with MIDI integration. Because of the audio/ MIDI syn- chronization it is used for annotation although no speci\ufb01c tools are dedicated to annotation.",
        "zenodo_id": 1415796,
        "dblp_key": "conf/ismir/PeetersFR08"
    },
    {
        "title": "Gamera Versus Aruspix: Two Optical Music Recognition Approaches.",
        "author": [
            "Laurent Pugin",
            "Jason Hockman",
            "John Ashley Burgoyne",
            "Ichiro Fujinaga"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417683",
        "url": "https://doi.org/10.5281/zenodo.1417683",
        "ee": "https://zenodo.org/records/1417683/files/PuginHBF08.pdf",
        "abstract": "Optical music recognition (OMR) applications are pre- dominantly designed for common music notation and as such, are inherently incapable of adapting to specialized notation forms within early music. Two OMR systems, namely Ga- mut (a Gamera application) and Aruspix, have been pro- posed for early music. In this paper, we present a novel comparison of the two systems, which use markedly dif- ferent approaches to solve the same problem, and pay close attention to the performance and learning rates of both appli- cations. In order to obtain a complete comparison of Gamut and Aruspix, we evaluated the core recognition systems and the pitch determination processes separately. With our ex- periments, we were able to highlight the advantages of both approaches as well as causes of problems and possibilities for future improvements. 1 INTRODUCTION Optical music recognition (OMR) enables document images to be encoded into digital symbolic music representations. The encoded music can then be used and processed within a wide range of applications, such as music analysis, editing and music information retrieval. Over the years, multiple approaches have addressed this dif\ufb01cult task, in some cases focusing exclusively on one type of music document, such as keyboard music, orchestral scores, or music manuscripts [1]. Most commercial tools today are non-adaptive, act- ing as black-boxes that do not improve their performance through usage: when a symbol is misread, it continues to be misread in the subsequent pages, even if the user corrects the results by hand on every page. Attempts have been made to remedy this situation by merging multiple OMR systems, yet this remains a challenge [4]. Two research projects have taken an innovative approach to OMR by adopting adaptive strategies, namely Gamut, built based on the Gamera framework, and Aruspix. The main idea behind the adaptive approach in OMR is to en- able the tools to improve themselves through usage by tak- ing bene\ufb01ts from the training data that becomes available as the user corrects recognition errors. Adaptive approaches have been proven to be very promising with historical doc- uments, because the very high variability within the data re- quires the tools to constantly adapt and retarget themselves. This is particularly true for early music prints from the six- teenth and seventeenth centuries, for which Aruspix was de- signed, as they contain an unpredictable variability of font shape, page noise, brightness and contrast [14]. This study is a \ufb01rst attempt at a comparison between Ga- mut and Aruspix. We speci\ufb01cally address the accuracy of the tools and their capability to adapt themselves to a new dataset. This paper is structured as follows. In section 2 we present a brief overview of the two OMR applications in comparison. In section 3 we present the experiments under- taken to enable such a comparison at different stages of the recognition process. Results are presented in section 4, and conclusions and future work are discussed in section 5. 2 INFRASTRUCTURE",
        "zenodo_id": 1417683,
        "dblp_key": "conf/ismir/PuginHBF08"
    },
    {
        "title": "Direct and Inverse Inference in Music Databases: How to Make at Song Funk?",
        "author": [
            "Patrick Rabbat",
            "Fran\u00e7ois Pachet"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416486",
        "url": "https://doi.org/10.5281/zenodo.1416486",
        "ee": "https://zenodo.org/records/1416486/files/RabbatP08.pdf",
        "abstract": "We propose an algorithm for exploiting statistical properties of large-scale metadata databases about music titles to answer musicological queries. We introduce two inference schemes called \u201cdirect\u201d and \u201cinverse\u201d inference, based on an efficient implementation of a kernel regression approach. We describe an evaluation experiment conducted on a large-scale database of fine- grained musical metadata. We use this database to train the direct inference algorithm, test it, and also to identify the optimal parameters of the algorithm. The inverse inference algorithm is based on the direct inference algorithm. We illustrate it with some examples.",
        "zenodo_id": 1416486,
        "dblp_key": "conf/ismir/RabbatP08"
    },
    {
        "title": "Detection of Stream Segments in Symbolic Musical Data.",
        "author": [
            "Dimitrios Rafailidis",
            "Alexandros Nanopoulos",
            "Yannis Manolopoulos",
            "Emilios Cambouropoulos"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416002",
        "url": "https://doi.org/10.5281/zenodo.1416002",
        "ee": "https://zenodo.org/records/1416002/files/RafailidisNMC08.pdf",
        "abstract": "A listener is thought to be able to organise musical notes into groups within musical streams/voices. A stream segment is a relatively short coherent sequence of tones that is separated horizontally from co-sounding streams and, vertically from neighbouring musical sequences. This paper presents a novel algorithm that discovers musical stream segments in symbolic musical data. The proposed algorithm makes use of a single set of fundamental auditory principles for the concurrent horizontal and vertical segregation of a given musical texture into stream segments. The algorithm is tested against a small manually-annotated dataset of musical excerpts, and results are analysed; it is shown that the technique is promising.",
        "zenodo_id": 1416002,
        "dblp_key": "conf/ismir/RafailidisNMC08"
    },
    {
        "title": "A Web of Musical Information.",
        "author": [
            "Yves Raimond",
            "Mark B. Sandler"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1414840",
        "url": "https://doi.org/10.5281/zenodo.1414840",
        "ee": "https://zenodo.org/records/1414840/files/RaimondS08.pdf",
        "abstract": "We describe our recent achievements in interlinking several music-related data sources on the Semantic Web. In partic- ular, we describe interlinked datasets dealing with Creative Commons content, editorial, encyclopedic, geographic and statistical data, along with queries they can answer and tools using their data. We describe our web services, provid- ing an on-demand access to content-based features linked with such data sources and information pertaining to their creation (including processing steps, applied algorithms, in- puts, parameters or associated developers). We also provide a tool allowing such music analysis services to be set up and scripted in a simple way. 1 INTRODUCTION Information management has become a primary concern for multimedia-related technologies, from personal collections management to the construction of large content delivery services. However, the solutions that have emerged still ex- ist in isolation. For example, large online databases (such as Musicbrainz, MyStrands, etc.) and personal music collec- tion management tools such as iTunes or Songbird do not interact with each other, although they actually deal with the same kind of data. The information one of these solutions manages does not bene\ufb01t from the information another may hold. The problem becomes acute when narrowing our view to the exchange of results between music technology re- searchers. If providing access to content-based feature ex- traction through web services is a \ufb01rst step [10, 6], the re- sults they produce, in order for them to be meaningful, must be interlinked with other data sources. Just giving back a set of results from a particular digital audio item is useless unless we know what has been processed, and how. In this paper, we show the bene\ufb01ts of using a set of web standards, often referred to as Semantic Web technolo- gies, to achieve these goals. First, we give an overview of these standards and how they can be used to create a \u2018web of data\u2019\u2014a distributed, domain-independent, web- scale database. We give a brief summary of the Seman- tic Web ontology we described in [12], able to deal with music-related data. We then focus on the music-related in- terlinked datasets we published since, and give examples of the type of queries they can answer and of tools consum- ing their data. We describe our lightweight extension of this music ontology to make content-based features part of this \u2018web of data\u2019. We also describe our web services, providing on-demand access to such resources, as well as the frame- work underlying them. 2 TOWARDS A MUSIC-RELATED WEB OF DATA",
        "zenodo_id": 1414840,
        "dblp_key": "conf/ismir/RaimondS08"
    },
    {
        "title": "Performer Identification in Celtic Violin Recordings.",
        "author": [
            "Rafael Ram\u00edrez 0001",
            "Alfonso P\u00e9rez",
            "Stefan Kersten"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416988",
        "url": "https://doi.org/10.5281/zenodo.1416988",
        "ee": "https://zenodo.org/records/1416988/files/RamirezPK08.pdf",
        "abstract": "We present an approach to the task of identifying perform- ers from their playing styles. We investigate how violinists express and communicate their view of the musical content of Celtic popular pieces and how to use this information in order to automatically identify performers. We study note- level deviations of parameters such as timing and amplitude. Our approach to performer identi\ufb01cation consists of induc- ing an expressive performance model for each of the inter- preters (essentially establishing a performer dependent map- ping of inter-note features to a timing and amplitude expres- sive transformations). We present a successful performer identi\ufb01cation case study. 1 INTRODUCTION Music performance plays a central role in our musical cul- ture today. Concert attendance and recording sales often re\ufb02ect people\u2019s preferences for particular performers. The manipulation of sound properties such as pitch, timing, am- plitude and timbre by different performers is clearly distin- guishable by the listeners. Expressive music performance studies the manipulation of these sound properties in an at- tempt to understand expression in performances. There has been much speculation as to why performances contain ex- pression. Hypothesis include that musical expression com- municates emotions and that it clari\ufb01es musical structure, i.e. the performer shapes the music according to her own intentions In this paper we focus on the task of identifying violin performers from their playing style using high-level descrip- tors extracted from single-instrument audio recordings. The identi\ufb01cation of performers by using the expressive content in their performances raises particularly interesting ques- tions but has nevertheless received relatively little attention in the past. The data used in our investigations are violin audio record- ings of Irish popular music performances. We use sound analysis techniques based on spectral models [15] for ex- tracting high-level symbolic features from the recordings. In particular, for characterizing the performances used in this work, we are interested in inter-note features represent- ing information about the music context in which expres- sive events occur. Once the relevant high-level information is extracted we apply machine learning techniques [9] to automatically discover regularities and expressive patterns for each performer. We use these regularities and patterns in order to identify a particular performer in a given audio recording. The rest of the paper is organized as follows: Section 2 sets the background for the research reported here. Sec- tion 3 describes how we process the audio recordings in or- der to extract inter-note information. Section 4 describes our approach to performance-driven performer identi\ufb01ca- tion. Section 5 describes a case study on identifying per- formers based on their playing style and discusses the re- sults, and \ufb01nally, Section 6 presents some conclusions and indicates some areas of future research. 2 BACKGROUND Understanding and formalizing expressive music performance is an extremely challenging problem which in the past has been studied from different perspectives, e.g. [14], [4], [2]. The main approaches to empirically studying expressive per- formance have been based on statistical analysis (e.g. [12]), mathematical modeling (e.g. [17]), and analysis-by-synthesis (e.g. [3]). In all these approaches, it is a person who is responsible for devising a theory or mathematical model which captures different aspects of musical expressive per- formance. The theory or model is later tested on real per- formance data in order to determine its accuracy. The ma- jority of the research on expressive music performance has focused on the performance of musical material for which notation (i.e. a score) is available, thus providing unam- biguous performance goals. Expressive performance stud- ies have also been very much focused on (classical) piano performance in which pitch and timing measurements are simpli\ufb01ed. Previous research addressing expressive music performance using machine learning techniques has included a number of 483 ISMIR 2008 \u2013 Session 4b \u2013 Musical Expression and Meaning approaches. Lopez de Mantaras et al. [6] report on SaxEx, a performance system capable of generating expressive solo saxophone performances in Jazz. One limitation of their system is that it is incapable of explaining the predictions it makes and it is unable to handle melody alterations, e.g. ornamentations. Ramirez et al. [11] have explored and compared diverse machine learning methods for obtaining expressive music performance models for Jazz saxophone that are capable of both generating expressive performances and explaining the expressive transformations they produce. They propose an expressive performance system based on inductive logic programming which induces a set of \ufb01rst order logic rules that capture expressive transformation both at an inter-note level (e.g. note duration, loudness) and at an intra-note level (e.g. note attack, sustain). Based on the theory generated by the set of rules, they implemented a melody synthesis component which generates expressive monophonic output (MIDI or audio) from inexpressive melody MIDI descrip- tions. With the exception of the work by Lopez de Mantaras et al and Ramirez et al, most of the research in expressive performance using machine learning techniques has focused on classical piano music where often the tempo of the per- formed pieces is not constant. The works focused on classi- cal piano have focused on global tempo and loudness trans- formations while we are interested in note-level tempo and loudness transformations. Nevertheless, the use of expressive performance mod- els, either automatically induced or manually generated, for identifying musicians has received little attention in the past. This is mainly due to two factors: (a) the high complexity of the feature extraction process that is required to character- ize expressive performance, and (b) the question of how to use the information provided by an expressive performance model for the task of performance-based performer iden- ti\ufb01cation. To the best of our knowledge, the only group working on performance-based automatic performer iden- ti\ufb01cation is the group led by Gerhard Widmer. Saunders et al [13] apply string kernels to the problem of recognizing famous pianists from their playing style. The characteris- tics of performers playing the same piece are obtained from changes in beat-level tempo and beat-level loudness. From such characteristics, general performance alphabets can be derived, and pianists\u2019 performances can then be represented as strings. They apply both kernel partial least squares and Support Vector Machines to this data. Stamatatos and Widmer [16] address the problem of iden- tifying the most likely music performer, given a set of per- formances of the same piece by a number of skilled candi- date pianists. They propose a set of very simple features for representing stylistic characteristics of a music performer that relate to a kind of \u2019average\u2019 performance. A database of piano performances of 22 pianists playing two pieces by Frdric Chopin is used. They propose an ensemble of sim- ple classi\ufb01ers derived by both subsampling the training set and subsampling the input features. Experiments show that the proposed features are able to quantify the differences be- tween music performers. 3 MELODIC DESCRIPTION First of all, we perform a spectral analysis of a portion of sound, called analysis frame, whose size is a parameter of the algorithm. This spectral analysis consists of multiplying the audio frame with an appropriate analysis window and performing a Discrete Fourier Transform (DFT) to obtain its spectrum. In this case, we use a frame width of 46 ms, an overlap factor of 50%, and a Keiser-Bessel 25dB win- dow. Then, we compute a set of low-level descriptors for each spectrum: energy and an estimation of the fundamen- tal frequency. From these low-level descriptors we perform a note segmentation procedure. Once the note boundaries are known, the note descriptors are computed from the low- level values. the main low-level descriptors used to charac- terize note-level expressive performance are instantaneous energy and fundamental frequency. Energy computation. The energy descriptor is computed on the spectral domain, using the values of the amplitude spectrum at each analysis frame. In addition, energy is com- puted in different frequency bands as de\ufb01ned in [5], and these values are used by the algorithm for note segmenta- tion. Fundamental frequency estimation. For the estimation of the instantaneous fundamental frequency we use a harmonic matching model derived from the Two-Way Mismatch pro- cedure (TWM) [7]. For each fundamental frequency candi- date, mismatches between the harmonics generated and the measured partials frequencies are averaged over a \ufb01xed sub- set of the available partials. A weighting scheme is used to make the procedure robust to the presence of noise or ab- sence of certain partials in the spectral data. The solution presented in [7] employs two mismatch error calculations. The \ufb01rst one is based on the frequency difference between each partial in the measured sequence and its nearest neigh- bor in the predicted sequence. The second is based on the mismatch between each harmonic in the predicted sequence and its nearest partial neighbor in the measured sequence. This two-way mismatch helps to avoid octave errors by ap- plying a penalty for partials that are present in the measured data but are not predicted, and also for partials whose pres- ence is predicted but which do not actually appear in the measured sequence. The TWM mismatch procedure has also the bene\ufb01t that the effect of any spurious components or partial missing from the measurement can be counteracted by the presence of uncorrupted partials in the same frame. 484 ISMIR 2008 \u2013 Session 4b \u2013 Musical Expression and Meaning Note segmentation is performed using a set of frame de- scriptors, which are energy computation in different frequency bands and fundamental frequency. Energy onsets are \ufb01rst detected following a band-wise algorithm that uses some psycho-acoustical knowledge [5]. In a second step, funda- mental frequency transitions are also detected. Finally, both results are merged to \ufb01nd the note boundaries (onset and offset information). Note descriptors. We compute note descriptors using the note boundaries and the low-level descriptors values. The low-level descriptors associated to a note segment are com- puted by averaging the frame values within this note seg- ment. Pitch histograms have been used to compute the pitch note and the fundamental frequencythat represents each note segment, as found in [8]. This is done to avoid taking into account mistaken frames in the fundamental frequencymean computation. First, frequencyvalues are converted into cents, by the following formula: c = 1200 \u00b7 log ( f fref ) log2 (1) where fref = 8.176 (fref is a the reference frequency of the C0). Then, we de\ufb01ne histograms with bins of 100 cents and hop size of 5 cents and we compute the maximum of the histogram to identify the note pitch. Finally, we com- pute the frequency mean for all the points that belong to the histogram. The MIDI pitch is computed by quantization of this fundamental frequency mean over the frames within the note limits. Musical Analysis. It is widely recognized that humans per- ings under study, we decided to use Narmour\u2019s theory of perception and cognition of melodies [10] to analyze the performances. The Implication/Realization model proposed by Narmour is a theory of perception and cognition of melodies. The the- ory states that a melodic musical line continuously causes listeners to generate expectations of how the melody should continue. The nature of these expectations in an individual are motivated by two types of sources: innate and learned. According to Narmour, on the one hand we are all born with innate information which suggests to us how a particular melody should continue. On the other hand, learned fac- tors are due to exposure to music throughout our lives and familiarity with musical styles and particular melodies. Ac- cording to Narmour, any two consecutively perceived notes constitute a melodic interval, and if this interval is not con- ceived as complete, it is an implicative interval, i.e. an in- terval that implies a subsequent interval with certain char- Figure 1. Prototypical Narmour Structures Figure 2. Narmour analysis of a melody fragment acteristics. That is to say, some notes are more likely than others to follow the implicative interval. Two main prin- ciples recognized by Narmour concern registral direction and intervallic difference. The principle of registral direc- tion states that small intervals imply an interval in the same registral direction (a small upward interval implies another upward interval and analogously for downward intervals), and large intervals imply a change in registral direction (a large upward interval implies a downward interval and anal- ogously for downward intervals). The principle of inter- vallic difference states that a small (\ufb01ve semitones or less) interval implies a similarly-sized interval (plus or minus 2 semitones), and a large interval (seven semitones or more) implies a smaller interval. Based on these two principles, melodic patterns or groups can be identi\ufb01ed that either sat- isfy or violate the implication as predicted by the principles. Such patterns are called structures and are labeled to denote characteristics in terms of registral direction and intervallic difference. Figure 1 shows prototypical Narmour structures. A note in a melody often belongs to more than one struc- ture. Thus, a description of a melody as a sequence of Nar- mour structures consists of a list of overlapping structures. We parse each melody in the training data in order to au- tomatically generate an implication/realization analysis of the pieces. Figure 2 shows the analysis for a fragment of a melody. 4 PERFORMANCE-DRIVEN PERFORMER IDENTIFICATION",
        "zenodo_id": 1416988,
        "dblp_key": "conf/ismir/RamirezPK08"
    },
    {
        "title": "Fast MIR in a Sparse Transform Domain.",
        "author": [
            "Emmanuel Ravelli",
            "Ga\u00ebl Richard",
            "Laurent Daudet"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417002",
        "url": "https://doi.org/10.5281/zenodo.1417002",
        "ee": "https://zenodo.org/records/1417002/files/RavelliRD08.pdf",
        "abstract": "We consider in this paper sparse audio coding as an alter- native to transform audio coding for ef\ufb01cient MIR in the transform domain. We use an existing audio coder based on a sparse representation in a union of MDCT bases, and propose a fast algorithm to compute mid-level representa- tions for beat tracking and chord recognition, respectively an onset detection function and a chromagram. The resulting transform domain system is signi\ufb01cantly faster than a com- parable state-of-the-art system while obtaining close perfor- mance above 8 kbps. 1 INTRODUCTION Music recordings are now widely available in coded format. The reason is that state-of-the-art audio coders such as MP3 [6] or AAC [7] are able to reduce the size of a PCM au- dio signal more than 10 times, while guaranteeing a near- transparent quality. Consequently, such technology allows users to easily exchange and store music on mobile devices and networks. On the other hand, state-of-the-art audio indexing algo- rithms such as beat tracking [8, 2] and chord recognition [1, 10] are designed to process PCM audio signals. Conse- quently, to use them with coded audio, one has to decode to PCM \ufb01rst and then apply the audio indexing algorithm on the PCM signal (Processing in the time domain, see Fig. 1). To save computational cost, which is often required e.g. when using such algorithms with mobile devices or on very large databases, it would be more ef\ufb01cient to design audio indexing algorithms that work directly with the coded data. There are two ways to process coded data depending on which stage of the decoding process we are working on (see Fig. 1). The \ufb01rst way is to use directly the bitstream, this ap- proach is called processing in the compressed domain. The second way is to use the transform representation, this ap- proach is called processing in the transform domain. The \ufb01rst approach is faster as we avoid the cost of decoding the transform representation, however, for certain cases the in- formation available in the bitstream is not suf\ufb01ciently ex- plicit, and it it thus necessary to use the transform domain representation. We consider in this paper two audio indexing applica- tions, beat tracking [8, 2] and chord recognition [1, 10]. Figure 1. Block diagram of a common audio decoder and three possible audio indexing systems. Beat tracking has already been investigated using MP3 au- dio \ufb01les in the transform domain [13] and in the compressed domain [14]. However, no work related to chord recogni- tion using coded data has been found in the literature. This may be due to the limited frequency resolution of the time- frequency analysis used in state-of-the-art transform audio coders such as MP3 [6] and AAC [7]. Due to this limitation of the transform based coders, it is interesting to consider other kinds of audio coders, such as parametric coding (e.g. [3]) or sparse representation based coding (e.g. [12]). We have chosen here to use a new pro- totype audio coder based on a union of MDCT bases [12], which has the advantage to provide a sparse representation which has both precise time and frequency resolution, con- trary to transform based coders. This coder, available for testing in open source 1 , has also the interesting property of \ufb01ne-grain scalability. We show in this paper that this new signal representa- tion approach is not only useful for audio coding, but also for audio indexing. We propose a fast method to calculate, in the transform domain, mid-level representations similar to those used in state-of-the-art systems, namely an onset 1 http://www.emmanuel-ravelli.com/downloads.html 527 ISMIR 2008 \u2013 Session 4c \u2013 Automatic Music Analysis and Transcription detection function and a chromagram. The onset detection function and the chromagram are then used to perform re- spectively beat tracking and chord recognition using same machine learning systems as used in state-of-the-art systems [2, 1]. The remainder of this paper is as follows. In section 2, we describe the signal representation used in [12]. In section 3, the details of the calculation of the mid-level representa- tions are given. In section 4, the machine learning systems we use to perform beat tracking and chord recognition are brie\ufb02y described. Finally, section 5 gives the performance evaluation, section 6 discuss about computation times, and we conclude in section 7. 2 SIGNAL REPRESENTATION IN A UNION OF MDCT BASES In state-of-the-art audio coders such as AAC [7], the Mod- i\ufb01ed Discrete Cosine Transform (MDCT) is used. While such coders allow transparent quality at high bitrates, they give limited performance at low bitrates. In [12], Ravelli et al proposed a generalization of the transform coding ap- proach where the signal is decomposed in a union of MDCT bases with different scales. The results showed that this new approach allows improved performance at low bitrates. We brie\ufb02y present in the following the signal model and the de- composition algorithm.",
        "zenodo_id": 1417002,
        "dblp_key": "conf/ismir/RavelliRD08"
    },
    {
        "title": "Application of the Functional Requirements for Bibliographic Records (FRBR) to Music.",
        "author": [
            "Jenn Riley"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416446",
        "url": "https://doi.org/10.5281/zenodo.1416446",
        "ee": "https://zenodo.org/records/1416446/files/Riley08.pdf",
        "abstract": "This paper describes work applying the Functional Requirements for Bibliographic Records (FRBR) model to music, as the basis for implementing a fully FRBR- compliant music digital library system. A detailed analysis of the FRBR and Functional Requirements for Authority Data (FRAD) entities and attributes is presented. The paper closes with a discussion of the ways in which FRBR is gaining adoption outside of the library environment in which it was born. This work benefits the MIR community by demonstrating a model that can be used in MIR systems for the storage of descriptive information in support of metadata-based searching, and by positioning the Variations system to be a source of robust descriptive information for use by third-party MIR systems.",
        "zenodo_id": 1416446,
        "dblp_key": "conf/ismir/Riley08"
    },
    {
        "title": "A Text Retrieval Approach to Content-Based Audio Hashing.",
        "author": [
            "Matthew Riley",
            "Eric Heinen",
            "Joydeep Ghosh"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417985",
        "url": "https://doi.org/10.5281/zenodo.1417985",
        "ee": "https://zenodo.org/records/1417985/files/RileyHG08.pdf",
        "abstract": "This paper presents a novel approach to robust, content- based retrieval of digital music. We formulate the hash- ing and retrieval problems analogously to that of text re- trieval and leverage established results for this unique ap- plication. Accordingly, songs are represented as a \u201dBag- of-Audio-Words\u201d and similarity calculations follow directly from the well-known Vector Space model [12]. We evaluate our system on a 4000 song data set to demonstrate its prac- tical applicability, and evaluation shows our technique to be robust to a variety of signal distortions. Most interestingly, the system is capable of matching studio recordings to live recordings of the same song with high accuracy. 1 INTRODUCTION Large digital music libraries are becoming commonplace on consumer computer systems, and with their growth our abil- ity to automatically analyze and interpret their content has become increasingly important. The ability to \ufb01nd acousti- cally similar, or even duplicate, songs within a large audio database is a particularly important task with numerous po- tential applications. For example, an automated system sim- ilar to MusicBrainz [11] might organize a user\u2019s music col- lection by properly naming each \ufb01le according to artist and song title. Another application could attempt to retrieve the artist and title of a song given a short clip recorded from a radio broadcast or perhaps even hummed into a microphone. Due to the rich feature set of digital audio, a central task in this process is that of extracting a representative audio \ufb01n- gerprint that describes the acoustic content of each song. We hope to extract from each song a feature vector that is both highly discriminative between different songs and robust to common distortions that may be present in different copies of the same source song. With the multitude of compres- sion formats and signal extraction processes, two copies of the same song can sound perceptually identical while hav- ing very different digital representations. Additionally, it is desirable for the audio \ufb01ngerprint to compress the existing audio information into a much smaller representation, thus enabling ef\ufb01cient retrieval and requiring less storage than that of the initial data set. In this paper, we present a novel hashing methodology that satis\ufb01es these constraints. We show that a technique based on methods for text retrieval performs well for the desired applications, and bene\ufb01ts from established research results in the area. Section 2 reviews existing work related to our application and section 3 details our application of the text retrieval techniques to content-based audio retrieval. Section 4 details our experimental evaluation of the pro- posed algorithm. Section 5 discusses practical implemen- tation considerations and section 6 concludes with \ufb01nal re- marks and suggestions for future work. 2 RELATED WORK The problem of audio \ufb01ngerprinting has been studied widely. In 2002, Haitsma and Kalker proposed a method for ex- tracting audio \ufb01ngerprints that they showed were robust to a variety of signal distortions. In addition, they outlined a database searching algorithm for locating a \ufb01ngerprint most similar to a given target \ufb01ngerprint [7]. One of the draw- backs of their system is the amount of memory required to store an audio \ufb01ngerprint (approx. 100 KBytes for a 5 minute song). In addition, it was unclear whether or not their \ufb01ngerprints could feasibly be used to match a studio recording to a live performance or a cover version (i.e. a performance of the original composition by another artist, possibly rearranged). Existing work on cover song detection was presented for a competition at The Music Information Retrieval Evalua- tion eXchange (MIREX). In 2006, Dan Ellis\u2019 team from Columbia University won the competition by posting ac- curacy of about 60% using a method that computed sim- ilarities between songs by cross-correlating sequences of their so-called Chroma features [5]. Their similarity mea- sure is equally applicable to the problem of matching a stu- dio recording to a live performance. However, the high com- putational complexity of cross-correlating Chroma feature vector sequences does not make sense in an audio retrieval context. We have not found previous research that directly applies text retrieval methods to the task of audio retrieval, but a similar approach has been taken for Object Recognition in images [13]. Further, Casey and Slaney [3] present a system 295 ISMIR 2008 \u2013 Session 3a \u2013 Content-Based Retrieval, Categorization and Similarity 1 \u0002\u0003\u0004\u0005\u0003\u0006\u0007\u0004\b\b",
        "zenodo_id": 1417985,
        "dblp_key": "conf/ismir/RileyHG08"
    },
    {
        "title": "Hybrid Numeric/Rank Similarity Metrics for Musical Performance Analysis.",
        "author": [
            "Craig Sapp"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417561",
        "url": "https://doi.org/10.5281/zenodo.1417561",
        "ee": "https://zenodo.org/records/1417561/files/Sapp08.pdf",
        "abstract": "This paper describes a numerical method for examining similarities among tempo and loudness features extracted from recordings of the same musical work and evaluates its effec- tiveness compared to Pearson correlation. Starting with corre- lation at multiple timescales, other concepts such as a perfor- mance \u201cnoise-\ufb02oor\u201d are used to generate measurements which are more re\ufb01ned than correlation alone. The measurements are evaluated and compared to plain correlation in their ability to identify performances of the same Chopin mazurka played by the same pianist out of a collection of recordings by various pianists. 1 INTRODUCTION As part of the Mazurka Project at the AHRC Centre for the His- tory and Analysis of Recorded Music (CHARM), almost 3,000 recordings of Chopin mazurkas were collected to analyze the stylistic evolution of piano playing over the past 100 years of recording history, which equates to about 60 performances of each mazurka. The earliest collected performance was recorded on wax cylinders in 1902 and the most recent posted as home- made videos on YouTube. Table 1 lists 300 performances of \ufb01ve mazurkas which will be used for evaluation later in this pa- per since they include a substantial number of recordings with extracted tempo and loudness features. Table 1. Collection of musical works used for analysis. For each of the processed recordings, beat timings in the per- formance are determined using the Sonic Visualiser audio edi- tor 1 for markup and manual correction with the assistance of several vamp plugins. 2 Dynamics are then extracted as smoo- thed loudness values sampled at the beat positions.[3] Feature data will eventually be extracted from all collected mazurkas in the above list, but comparisons made in Section 3 are based on 1 http://www.sonicvisualiser.org 2 http://sv.mazurka.org.uk/download the processed performance counts in Table 1. Raw data used for analysis in this paper is available on the web. 3 Figure 1 illustrates extracted performance feature data as a set of curves. Curve 1a plots the beat-level tempo which is calculated from the duration between adjacent beat timings in the recording. For analysis comparisons, the tempo curve is also split into high- and low-frequency components with lin- ear \ufb01ltering. 4 Curve 1b represents smoothed tempo which captures large-scale phrasing architecture in the performance (note there are eight phrases in this example). Curve 1c repre- sents the difference between Curves 1a and 1b which is called here the desmoothed tempo curve, or the residual tempo. This high-frequency tempo component encodes temporal accentu- ation in the music used by the performer to emphasize par- ticular notes or beats. Mazurka performances contain signi\ufb01- cant high-frequency tempo information, since part of the per- formance style depends on a non-uniform tempo throughout the measure\u2014the \ufb01rst beat usually shortened, while the second and/or third beat are lengthened. Curve 1d represents the ex- tracted dynamics curve which is a sampling of the audio loud- ness at each beat location. Other musical features are currently ignored here, yet are important in characterizing a performance. In particular, pi- anists do not always play left- and right-hand notes together, according to aural traditions, although they are written as si- multaneities in the printed score. Articulations such as legato and staccato are also important performance features but are equally dif\ufb01cult to extract reliably from audio data. Nonethe- less, tempo and dynamic features are useful for developing nav- igational tools which allow listeners to focus their attention on speci\ufb01c areas for further analysis. Figure 1. Extracted musical features from a recording of Chopin\u2019s mazurka in B minor, 30/2: a) tempo between beats; b) smoothed tempo; c) residual tempo (c = a \u2212b); and d) beat- level dynamics. 3 http://mazurka.org.uk/info/excel 4 The \ufb01ltering method is available online at http://mazurka.org.uk/software/online/smoother . 501 ISMIR 2008 \u2013 Session 4b \u2013 Musical Expression and Meaning 2 DERIVATIONS AND DEFINITIONS Starting with the underlying comparison method of correlation (called S0 below), a series of intermediate similarity measure- ments (S1, S2, and S3) are used to derive a \ufb01nal measurement technique (S4). Section 3 then compares the effectiveness of S0 and S4 measurements in identifying recordings of the same performer out of a database of recordings of the same mazurka.",
        "zenodo_id": 1417561,
        "dblp_key": "conf/ismir/Sapp08"
    },
    {
        "title": "Timbre and Rhythmic TRAP-TANDEM Features for Music Information Retrieval.",
        "author": [
            "Nicolas Scaringella"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1418113",
        "url": "https://doi.org/10.5281/zenodo.1418113",
        "ee": "https://zenodo.org/records/1418113/files/Scaringella08.pdf",
        "abstract": "The enormous growth of digital music databases has led to a comparable growth in the need for methods that help users organize and access such information. One area in partic- ular that has seen much recent research activity is the use of automated techniques to describe audio content and to allow for its identi\ufb01cation, browsing and retrieval. Con- ventional approaches to music content description rely on features characterizing the shape of the signal spectrum in relatively short-term frames. In the context of Automatic Speech Recognition (ASR), Hermansky [7] described an in- teresting alternative to short-term spectrum features, the TRAP- TANDEM approach which uses long-term band-limited fea- tures trained in a supervised fashion. We adapt this idea to the speci\ufb01c case of music signals and propose a generic system for the description of temporal patterns. The same system with different settings is able to extract features de- scribing either timbre or rhythmic content. The quality of the generated features is demonstrated in a set of music re- trieval experiments and compared to other state-of-the-art models. 1 INTRODUCTION As discussed in [2], most state-of-the-art algorithms dedi- cated to the high-level description of music signals rely on the same basic architecture with different algorithm variants and parameters, i.e. short-term audio features extraction fol- lowed by some supervised or unsupervised machine learn- ing algorithm. The most successful approaches rely on fea- tures describing the shape of short-term spectral frames of audio signal. Spectral shape is indeed known to be corre- lated with the perceived timbre of sounds as con\ufb01rmed by recent experiments in isolated instrument recognition (see notably [8]). As a matter of fact, short-term spectral en- velopes have dominated similar research \ufb01elds for years, like e.g. ASR. These short-term spectral envelopes are typi- cally transformed in accordance with some constraining prop- erties of human hearing such as the nonlinear (critical-band like) frequency resolution (Bark, Mel), the compressive non- linearity between acoustic stimulus and its percept (loga- rithm) or the decreasing sensitivity of hearing at lower fre- quencies (equal-loudness curves). Moreover, these modi\ufb01ed spectral frames are typically projected on spectral basis that decorrelate the feature space (cepstrum). In both speech and music signals, the smallest unit of in- formation, i.e. phonemes on the one hand and notes on the other hand (not only pitched notes but also hits of percussive instruments or whatever that produces sounds), spread on longer time intervals than the usual short-term audio frame. Indeed, typical ASR/MIR (Music Information Retrieval) sys- tems considers slices of audio signals of length 20 to 50 ms (slightly longer when accurate pitch estimates are needed in the lower frequencies). On the contrary, phonemes were demonstrated to spread at least over the interval 200-300ms [26]. As a matter of fact, the minimum discriminable in- ter onset interval (IOI) is estimated to lie within the range 50-100ms (i.e. two sounds separated by less than the min- imum IOI will be perceived as one) so that it is likely that at least 50-100ms of information is needed by a human lis- tener to interpret the incoming sound. Studies on rhythm perception show that the rate of information in music sig- nals is even less. Experiments [16] have indicated that pulse sensation cease to exist outside of the period range of 200- 2000ms which is known as the region of pulse sensation while the most natural foot-tapping period is approximately 500-600ms. Given these observations, it is reasonable to think that it is probably more perceptually relevant to model audio signals with a longer context than the usual 20-50ms spectrum frames. To preserve information related to the short-term dynamics of sounds and to keep suf\ufb01cient time- resolution when detecting e.g. musical note onsets, a trade- off consists in building a model of the sequence of short- term feature vectors over a longer time-scale. This process is sometimes referred to as temporal feature integration [14]. The simplest approach consists in computing simple statis- tics of feature vectors (means and variances) over texture- windows. This has been shown [25] to signi\ufb01cantly improve music genres classi\ufb01cation accuracy when using windows of approximately 1.0 second as opposed to the direct use of short-term frames. However, simple statistics discard dy- namical changes of short-term features while the dynamics of sound and notably the attack time and the \ufb02uctuations 626 ISMIR 2008 \u2013 Session 5b \u2013 Feature Representation of the spectral envelope over time have proved to be of a great importance in the perception of individual instrument notes (see [11]). Meng [14] modeled the evolution of fea- tures over a texture window with an auto-regressive (AR) model and got improved genre classi\ufb01cation results than with simple statistics. McKinney and Breebart [12] com- puted a periodogram for each short-term feature dimension over a frame corresponding to roughly 768ms. Each peri- odogram was then summarized by its power in 4 prede\ufb01ned frequency bands using a \ufb01xed \ufb01lter bank. This approach was pursued by Arenas-Garcia et al. [1] who trained the \ufb01lter-bank in a supervised fashion to optimally suit a partic- ular music organization task. Rauber et al. [20] used crit- ical band energies periodograms with a much longer con- text, i.e. 6 seconds. This longer context was considered to model rhythmic patterns and the range 0-10Hz was consid- ered as higher values are beyond what humans can perceive as rhythm (see again the region of pulse sensation). These approaches to temporal feature integration model the dy- namics of each feature independently. Though Meng [14] describes a general multivariate autoregressive model that does take into account correlations between feature trajec- tories, for the sake of simplicity he experiments in practice with a diagonal multivariate autoregressive model, i.e. an AR model of each feature dimension. Pohle et al. [19] use independent component analysis on short sequences of crit- ical band energies and obtain time-frequency 2D \ufb01lters that are reminiscent of cortical receptive \ufb01elds [4]. Though this approach seems more appropriate to take into account cor- relations between feature trajectories, it is at best of similar quality as short-term features in genre classi\ufb01cation experi- ments. As a matter of fact, the use of long-term features has been investigated more in depth in the context ASR, notably by Hermansky [7]. These features are extracted in 2 steps. Firstly, rather long-term TempoRAL Patterns (TRAPs) of band-limited (1-3 Bark) spectral energies are considered. Though, a context of 200-300ms seems needed for ASR, an even longer time interval of 1 second is considered so that information about slowly varying noise can be removed from the data (i.e. mean/standard deviation normalization). Hermansky [7] argues that, consistently with color separa- tion in vision, it seems likely that the frequency selectiv- ity of the hearing system is used for separating the reliable (high SNR) part of the signals from the unreliable ones, so that it seems reasonable to use independent frequency lo- calized processors. Consequently, each band-limited TRAP is processed individually by a speci\ufb01cally trained system to build as much knowledge as possible into the feature extrac- tion module to minimize the complexity of the subsequent stochastically trained models. The second step, referred to as the TANDEM part, consists in training a system aiming at the combination of frequency-localized evidence into a set of robust features that can be used in conventional HMM- based ASR systems. To our knowledge, there\u2019s been only one application of these TRAP-TANDEM features to music signals in the con- text of drum transcription [17]. In this paper, we further in- vestigate some possible applications of the TRAP-TANDEM approach in the context of music information retrieval. More speci\ufb01cally, we describe in section 2 our own implemen- tation of the TRAP-TANDEM feature extraction module, which slightly differs from the original method. As a matter of fact, we propose two different implementations to focus on two different aspects of music signals, namely timbre and rhythm. In section 3, we evaluate the validity of these fea- tures for music information retrieval in a set of music clus- tering experiments. Section 4 reaches conclusion. 2 MUSICAL TRAP-TANDEM FEATURES The \ufb01rst step of the processing consists in converting the au- dio signal into some time-frequency representation. In prac- tice, we use the typical short-term Fourier transform (STFT) with Hann windowing of the short-term audio frames. The resulting short-term spectra are projected onto the Mel scale to simulate human critical bands of hearing [13]. The per- ceptual relevance of this time-frequency representation is further improved by exploiting masking properties of the human ear (see [22]) and frequency response of the outer and middle ear (see [23]). The loudness in Sone of the spec- tra is \ufb01nally evaluated according to [3]. These short-term spectra will be later used to build tim- bre related TRAPs. Rhythmic TRAPs are based on a slightly different representation. More speci\ufb01cally, each critical band of the time-frequency plane is smoothed with a kernel, which width is taken in the range of the minimum IOI so that rhythmically irrelevant details of the band envelopes will be smoothed while the peaks corresponding to two differ- ent note onsets will not be merged. The \ufb01rst order deriva- tive of each smoothed critical band is then taken to empha- size sudden changes. Critical bands are \ufb01nally combined as suggested by Scheirer [21] who demonstrated that the per- ceived rhythmic content of many types of musical excerpts was mainly contained in 4 to 6 larger critical bands. To reduce further processing, we deploy a note onset de- tection algorithm and we will only compute one TRAP fea- ture vector per onsets instead of using a constant and faster rate of feature extraction. By synchronizing the analysis on detected onsets, an important factor of variability of the data is strongly reduced, i.e. the data is made translation invari- ant, and consequently, we can expect that the task of learn- ing relevant features will be simpli\ufb01ed. The differentiated signals computed for rhythm description are used as a basis to detect note onsets. The signals are \ufb01rst half-wave recti\ufb01ed and peaks are detected by using an adaptive threshold to ac- count for possible loudness changes over the course of the musical piece. Peaks are \ufb01rst combined over the different 627 ISMIR 2008 \u2013 Session 5b \u2013 Feature Representation Figure 1. From audio signal to critical band spectra and onset positions. bands and those closer than the minimum IOI are merged together. Figure 1 illustrates the processing chain that goes from the audio signal to both critical band spectra and onset positions. The data from each critical band and differentiated criti- cal band surrounding each onset is then parameterized with the cosine transform, which has the good property of pro- ducing decorrelated signals. The cosine transform is simple to deploy since it does not need any training phase, plus it has the interesting property that it closely resembles the Principal Component Analysis (PCA) of such critical band signals [7]. The cosine transform also allows for a signi\ufb01- cant reduction of the dimensionality of the input data. The range of modulation frequencies of the cosines is carefully selected. For timbre description, modulations between 4 and up to 100 Hz can be considered. The lower limit of 4 Hz is set in accordance with the smallest perceivable sound unit discussed in section 1. The higher limit is in the range of modulations contributing to perceptual roughness, which is generally thought to be a primary component of musical dis- sonance [24]. As a matter of fact, the percept of roughness is considered to correlate with temporal envelope modula- tions in the range of about 20-150 Hz and maximal at 70 Hz. For rhythm description, lower frequencies are consid- ered. Modulations between 1 and 4 Hz are interesting since they are of the order of typical beat rates, i.e. 60 to 240 Beats Per Minute (BPM). In the original TRAP approach, for each critical band some algorithm, typically a non-linear feedforward multi- layer perceptron (MLP), is trained to estimate posterior prob- abilities of the classes under consideration. In ASR, the targets are phonemes and there exist plenty of annotated datasets. Ideally, we would like to have instrument annota- tions to translate the TRAP idea to the case of music signals. Figure 2. The TRAP-TANDEM feature extraction process- ing chain. Unfortunately, no such dataset exists for real-world poly- phonic music and as a matter of fact, the annotation problem would become even more complex since we\u2019re considering mixture of instruments. For the time being, we left aside the use of critical band MLPs. The TANDEM part of the system is in charge of combin- ing evidence from the different frequency bands into a sin- gle estimate. A MLP is typically trained to combined these band limited features into a set of class posterior probabili- ties. Again, we lack appropriate annotated datasets. As an alternative, we use music genres annotations that are much cheaper to obtain, using e.g. some online music guide such as the AllMusic guide 1 . The merging MLP is fed with the concatenation of all band limited features, which are whitened with PCA to make them decorrelated. The MLP is trained to associate acoustic evidence with 50-dimensional binary vectors for which each dimension corresponds to a particular music genres. Notice that one song may be char- acterised by multiple genres. Once properly trained, the out- puts of the MLP are decorrelated by PCA and can be used as a feature vector describing the different genres in which the acoustic data has been observed. Figure 2 illustrates the processing chain that goes from the critical band or differ- entiated critical band spectra to the \ufb01nal TRAP-TANDEM like features. 3 EVALUATIONS The TRAP-TANDEM features describe the timbre and rhyth- mic context of a note onset. They may need to be aggregated into a song-level model in e.g. song retrieval applications. One application scenario consists in retrieving sets of songs 1 http://www.allmusic.com 628 ISMIR 2008 \u2013 Session 5b \u2013 Feature Representation similar to some query song according to some similarity measure between song-level models. Though listening tests have proved to be a valid evaluation method of such music retrieval systems due to the consistency of judgements ob- served over different listeners [15], they are very demanding in terms of time and human resources. Previous works have shown that evaluations based on genre data correspond to evaluations based on similarity ratings gathered in listening tests [15]. Consequently, we will base our evaluation of the descriptive quality of our features on some genre annotated data. However, since we are more interested in music re- trieval than automatic genre labelling, we will use measures of the ranking quality of the system rather than classi\ufb01cation accuracy.",
        "zenodo_id": 1418113,
        "dblp_key": "conf/ismir/Scaringella08"
    },
    {
        "title": "COCHONUT: Recognizing Complex Chords from MIDI Guitar Sequences.",
        "author": [
            "Ricardo E. P. Scholz",
            "Geber L. Ramalho"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416986",
        "url": "https://doi.org/10.5281/zenodo.1416986",
        "ee": "https://zenodo.org/records/1416986/files/ScholzR08.pdf",
        "abstract": "Chord recognition from symbolic data is a complex task, due to its strong context dependency and the large number of possible combinations of the intervals which the chords are made of, specially when dealing with dissonances, such as 7ths, 9ths, 13ths and suspended chords. None of the current approaches deal with such complexity. Most of them consider only simple chord patterns, in the best cases, including sevenths. In addition, when considering symbolic data captured from a MIDI guitar, we need to deal with non quantized and noisy data, which increases the difficulty of the task. The current symbolic approaches deal only with quantized data, with no automatic technique to reduce noise. This paper proposes a new approach to recognize chords, from symbolic MIDI guitar data, called COCHONUT (Complex Chords Nutting). The system uses contextual harmonic information to solve ambiguous cases, integrated with other techniques, such as decision theory, optimization, pattern matching and rule-based recognition. The results are encouraging and provide strong indications that the use of harmonic contextual information, integrated with other techniques, can actually improve the results currently found in literature.",
        "zenodo_id": 1416986,
        "dblp_key": "conf/ismir/ScholzR08"
    },
    {
        "title": "Survey of Symbolic Data for Music Applications.",
        "author": [
            "Eleanor Selfridge-Field",
            "Craig Stuart Sapp"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.5651429",
        "url": "https://doi.org/10.5281/zenodo.5651429",
        "ee": null,
        "abstract": "Multi-modal dataset for music genre recognition based on six different modalities for the LMD-aligned [1] and SLAC [2] datasets. Further details are provided in [3].\n\nDescriptions of files\n\n\n\t\n\t\t\n\t\t\tLink\n\t\t\tDescription\n\t\t\n\t\n\t\n\t\t\n\t\t\tLMD-aligned_Filelist.arff\n\t\t\tFile list with 1575 music tracks selected from the LMD-aligned dataset [1] with tagtraum genre annotations [4] (only a subset of LMD-aligned is used, which includes only pieces for which all six modalities were accessible, and which includes only well-represented genres)\n\t\t\n\t\t\n\t\t\tLMD-aligned_ExtractedFeatures.tar.gz\n\t\t\tRaw audio signal and model-based features extracted with AMUSE [5]\n\t\t\n\t\t\n\t\t\tLMD-aligned_ProcessedFeatures.tar.gz\n\t\t\tProcessed features: audio signal and model-based features aggregated for 4 s time frames with 2 s step size / all other features (see the table below) with the same values for all time frames\n\t\t\n\t\t\n\t\t\tLMD-aligned_Datasets.tar.gz\n\t\t\tTraining, optimization, and test datasets for 3 splits for the recognition of 5 genres in [3]\n\t\t\n\t\t\n\t\t\tSLAC_Filelist.arff\n\t\t\tFile list with 250 music tracks from the SLAC dataset [2] (genres and sub-genres are provided in the folder structure)\n\t\t\n\t\t\n\t\t\tSLAC_ExtractedFeatures.tar.gz\n\t\t\tRaw audio signal and model-based features extracted with AMUSE [5]\n\t\t\n\t\t\n\t\t\tSLAC_ProcessedFeatures.tar.gz\n\t\t\tProcessed features: audio signal and model-based features aggregated for 4 s time frames with 2 s step size / all other features (see the table below) with the same values for all time frames\n\t\t\n\t\t\n\t\t\tSLAC_Datasets.tar.gz\n\t\t\tTraining, optimization, and test datasets for 3 splits for the recognition of 5 genres and 10 sub-genres in [3]\n\t\t\n\t\n\n\nModalities and feature sub-groups\n\n\n\t\n\t\t\n\t\t\tModality\n\t\t\tSub-group\n\t\t\t\n\t\t\tDimensions in processed\n\n\t\t\tfeatures of LMD-aligned\n\t\t\t\n\t\t\t\n\t\t\tDimensions in processed\n\n\t\t\tfeatures of SLAC\n\t\t\t\n\t\t\n\t\n\t\n\t\t\n\t\t\tAudio signal\n\t\t\tLow-level\n\t\t\t1-524\n\t\t\t1-524\n\t\t\n\t\t\n\t\t\tAudio signal\n\t\t\tSemantic\n\t\t\t525-810\n\t\t\t525-810\n\t\t\n\t\t\n\t\t\tAudio signal\n\t\t\tStructural complexity\n\t\t\t811-908\n\t\t\t811-908\n\t\t\n\t\t\n\t\t\tModel-based\n\t\t\tInstruments\n\t\t\t909-1018\n\t\t\t909-1018\n\t\t\n\t\t\n\t\t\tModel-based\n\t\t\tMoods\n\t\t\t1019-1146\n\t\t\t1019-1146\n\t\t\n\t\t\n\t\t\tModel-based\n\t\t\tVarious\n\t\t\t1147-1402\n\t\t\t1147-1402\n\t\t\n\t\t\n\t\t\tPlaylists\n\t\t\tGenres\n\t\t\t1403-1973\n\t\t\t1403-1973\n\t\t\n\t\t\n\t\t\tPlaylists\n\t\t\tStyles\n\t\t\t1974-1695\n\t\t\t1974-1695\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tPitch\n\t\t\t1696-1757\n\t\t\t1696-1757\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tMelodic\n\t\t\t1758-1781\n\t\t\t1758-1781\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tChords\n\t\t\t1782-1836\n\t\t\t1782-1836\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tRhythm\n\t\t\t1837-1935\n\t\t\t1837-1935\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tTempo\n\t\t\t1936-1963\n\t\t\t1936-1963\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tInstrument presence\n\t\t\t1964-2441\n\t\t\t1964-2441\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tInstruments\n\t\t\t2442-2456\n\t\t\t2442-2456\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tTexture\n\t\t\t2457-2480\n\t\t\t2457-2480\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tDynamics\n\t\t\t2481-2484\n\t\t\t2481-2484\n\t\t\n\t\t\n\t\t\tAlbum covers\n\t\t\tSIFT\n\t\t\t2485-2584\n\t\t\t2485-2584\n\t\t\n\t\t\n\t\t\tLyrics\n\t\t\tjLyrics descriptors\n\t\t\t2585-2603\n\t\t\t2585-2671\n\t\t\n\t\t\n\t\t\tLyrics\n\t\t\tBag-of-Words\n\t\t\t2604-2703\n\t\t\t\n\t\t\n\t\t\n\t\t\tLyrics\n\t\t\tDoc2Vec\n\t\t\t2704-2803\n\t\t\t\n\t\t\n\t\n",
        "zenodo_id": 5651429,
        "dblp_key": "conf/ismir/Selfridge-FieldS08"
    },
    {
        "title": "The Latin Music Database.",
        "author": [
            "Carlos Nascimento Silla Jr.",
            "Alessandro L. Koerich",
            "Celso A. A. Kaestner"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416282",
        "url": "https://doi.org/10.5281/zenodo.1416282",
        "ee": "https://zenodo.org/records/1416282/files/SillaKK08.pdf",
        "abstract": "In this paper we present the Latin Music Database, a novel database of Latin musical recordings which has been de- veloped for automatic music genre classi\ufb01cation, but can also be used in other music information retrieval tasks. The method for assigning genres to the musical recordings is based on human expert perception and therefore capture their tacit knowledge in the genre labeling process. We also present the ethnomusicology of the genres available in the database as it might provide important information for the analysis of the results of any experiment that employs the database. 1 INTRODUCTION The maturation of the music information retrieval (MIR) \ufb01eld has required that researchers move beyond work in- volving simplistic musical databases to more expansive stud- ies that require much larger, more varied and carefully an- notated collections [19]. McKay et al. [19] have suggested a list of desired features for the development of new music databases in order to have music databases that can be em- ployed in different types of research instead of only a few experiments. The suggested list of desired features in new databases can be divided into three major groups: related to the musical recordings, related to the underlying framework to ef\ufb01ciently use the musical recordings and considering the accessibility and distribution of the database. The desired features related to the musical recordings are: (1) The database should contain many different types of music; (2) The database should include many thousands of recordings; (3) The database should include a signi\ufb01cant amount of commercial music; (4) Each recording should be annotated with as diverse a range of metadata \ufb01elds as possi- ble, in order to make the database usable as ground truth for as wide a range of research as possible; (5) Entire record- ings should be accessible, at least indirectly, not just short excerpts; (6) Given that different compression methods can in\ufb02uence extracted feature values, the audio format(s) most commonly used by the public should be adopted in order to re\ufb02ect realistic conditions. The desired features related to the underlaying frame- work are: (7) It should ideally be possible to label segments of recordings as well as recordings as a whole; (8) Anno- tations of subjective \ufb01elds such as genre or mood should include a wide range of candidate categories; allowing ten or so coarse categories is unrealistic; (9) It should be pos- sible to assign multiple independent values to a single \ufb01eld so that, for example, a recording could be classi\ufb01ed as both swing and blues; (10) The ability to construct ontological structures between \ufb01elds could be useful; (11) Metadata should be made available to users in formats that are easy to both manually browse and automatically parse; (12) Automatic tools should be available to validate metadata and generate pro\ufb01les of the database; (13) It should be easy to add new recordings and their metadata. The last desired feature is concerned with the accessi- bility and distribution of the database: (14) Data should be freely and legally distributable to researchers. Another question that often arises is how to assign a mu- sic genre in order to create a musical genre database. In the work of McKay and Fujinaga [18] this issue has been ex- tensively covered. We consider the following de\ufb01nition of a musical genre: \u201ca kind of music, as it is acknowledged by a community for any reason or purpose or criteria\u201d [9]. In this paper we present the Latin Music Database (LMD), a 3.227 musical recordings database from ten different gen- res. This database was \ufb01rstly developed to the task of musi- cal genre classi\ufb01cation, but it can be used by any MIR task since it provides not only the full recordings in the MP3 for- mat but also information about the music title and perform- ing artist. The main motivation behind the development of this database is due to the lack of ground-truth in the area. Most of databases normally contains few musical record- ings, sometimes only excerpts of the full music recording, and also a small number of instances per class. One of the reasons that hinders the development of novel databases is the dif\ufb01cultly in distributing them due to copyright restric- tions of musical recordings. However with the creation of the OmeN (On-demand Metadata Extraction Network) tool [17] researchers now have a tool to overcomes this limita- tion and make databases widely available to the MIR com- munity. In this work an approach to assign musical genres that corroborates with the de\ufb01nition of [9] is used. We also present information concerning how the LMD attends this list of desired features along with a brief ethnomusicology 451 ISMIR 2008 \u2013 Session 4a \u2013 Data Exchange, Archiving and Evaluation description of the musical genres used in the database. This paper is organized as follows: Section 2 presents the related work; Section 3 presents the development process of the LMD; The concluding remarks and a brief discus- sion about future work are presented in the last section; Ap- pendix A presents the ethnomusicology of the genres used in the LMD. 2 RELATED WORK The work of Tzanetakis and Cook [22] have strongly moti- vated the research in automatic music genre classi\ufb01cation. The contributions from this work were three fold: First, it showed the task of an automatic music genre classi\ufb01cation as a pattern recognition problem; Second, it not only pro- posed a feature set for the task but also developed an open source software for feature extraction know as Marsyas; and Third, the database used in the experiments is available. This database contains 1.000 music pieces from 10 musi- cal genres (Blues, Classic, Country, Disco, Hiphop, Jazz, Metal, Pop, Reggae and Rock). In the remainder of this work this database will be as referred the GTZAN database. From this work onwards, different methods were developed based on extracting features from the audio signal [12]. Current work in the \ufb01eld is trying to break through the possible glass ceiling that content-based audio classi\ufb01ca- tion using timbral features has reached [2]. In [3] the au- thors proposed a method for considering context attributes in order to improve classi\ufb01cation accuracy. The experiments were performed with database containing 4.936 songs with 801 boolean attributes grouped in 18 categories, some of which being correlated with some acoustic aspect of the sound (\u201cMain Instrument\u201d, \u201cDynamics\u201d), while others seem to result from a more cultural aspect of the sound (\u201cGenre\u201d, \u201cMood\u201d). This method achieved an improvement of 5% ac- curacy in average when compared to using only features extracted from the music signal. In [16] a framework for augmenting genre classi\ufb01cation accuracy using an ensemble that considers two different views of the same audio signal. The \ufb01rst view is based on the audio signal content, while the second is based on a symbolic form that is obtained us- ing a transcription system. The experiments were performed on three databases: GTZAN, ISMIR Rhythm and ISMIR Genre. The achieved results shown improvements over the previous results on the same databases. Another important aspect that concerns current research is how the evaluation of music genre classi\ufb01cation have been performed. In [10] is veri\ufb01ed that performing classi\ufb01cation without artist \ufb01lters (i.e. using music pieces from the same artist in both training and test sets) not only lower the clas- si\ufb01cation accuracy but may also erode the differences in ac- curacies between different techniques. In [7] is presented a discussion on how to de\ufb01ne ground-truth for the task of music genre classi\ufb01cation. In this work they de\ufb01ne ground- truth as an artefact of an individual response to music, not an artefact of the audio itself; to establish any ground-truth is therefore a cultural study. An approach that comply with this de\ufb01nition is presented by Gouyon et al. [12]. In this work they used the instantly ability that dancers have to rec- ognize different ballroom musical genres (Jive, Quickstep, Tango, Waltz, Viennese Waltz, Cha Cha Cha, Samba and Rumba) to label their database. These approaches are sim- ilar to the one used to de\ufb01ne ground-truth for the LMD, as we will see in section 3.1. Despite the efforts on the development on new techniques for the task of automatic music genre classi\ufb01cation, rela- tively less work have been done concerning the development of music databases that can be used as ground-truth. Besides the GTZAN database, other known databases are: the RWC database [11], the CODAICH database [19], the magnatune database which was used in the MIREX 2004 [8] genre con- test, and the database developed by Homburg et al. [14]. 3 THE DEVELOPMENT OF THE DATABASE We start the development of the LMD 1 aiming to collect at least 3.000 musical recordings from ten Latin musical gen- res. However, there are many issues involved in the devel- opment of a new audio database, as we will see in the fol- lowing.",
        "zenodo_id": 1416282,
        "dblp_key": "conf/ismir/SillaKK08"
    },
    {
        "title": "Speeding Melody Search With Vantage Point Trees.",
        "author": [
            "Michael Skalak",
            "Jinyu Han",
            "Bryan Pardo"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415714",
        "url": "https://doi.org/10.5281/zenodo.1415714",
        "ee": "https://zenodo.org/records/1415714/files/SkalakHP08.pdf",
        "abstract": "Melodic search engines let people find music in online collections by specifying the desired melody. Comparing the query melody to every item in a large database is prohibitively slow. If melodies can be placed in a metric space, search can be sped by comparing the query to a limited number of vantage melodies, rather than the entire database. We describe a simple melody metric that is customizable using a small number of example queries. This metric allows use of a generalized vantage point tree to organize the database. We show on a standard melodic database that the general vantage tree approach achieves superior search results for query-by-humming compared to an existing vantage point tree method. We then show this method can be used as a preprocessor to speed search for non-metric melodic comparison.",
        "zenodo_id": 1415714,
        "dblp_key": "conf/ismir/SkalakHP08"
    },
    {
        "title": "Learning a Metric for Music Similarity.",
        "author": [
            "Malcolm Slaney",
            "Kilian Q. Weinberger",
            "William White"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415554",
        "url": "https://doi.org/10.5281/zenodo.1415554",
        "ee": "https://zenodo.org/records/1415554/files/SlaneyWW08.pdf",
        "abstract": "This paper describe \ufb01ve different principled ways to em- bed songs into a Euclidean metric space. In particular, we learn embeddings so that the pairwise Euclidean distance between two songs re\ufb02ects semantic dissimilarity. This al- lows distance-based analysis, such as for example straight- forward nearest-neighbor classi\ufb01cation, to detect and poten- tially suggest similar songs within a collection. Each of the six approaches (baseline, whitening, LDA, NCA, LMNN and RCA) rotate and scale the raw feature space with a lin- ear transform. We tune the parameters of these models using a song-classi\ufb01cation task with content-based features. 1 INTRODUCTION Measuring the similarity of two musical pieces is dif\ufb01cult. Most importantly, two songs that are similar to two lovers of jazz, might be very different to somebody that does not listen to jazz. It is inherently an ill-posed problem. Still, the task is important. Listeners want to \ufb01nd songs that are related to a song that they like. Music programmers want to \ufb01nd a sequence of songs that minimizes jarring dis- continuities. A system based on measurements from hun- dreds of thousands of users is perhaps the ultimate solution [8], but there is still a need to \ufb01nd new songs, before an item-to-item system has enough data. It is dif\ufb01cult to construct a distance calculation based on arbitrary features. A simple approach places the feature values into a vector and then calculates an Euclidean dis- tance between points. Such a calculation implies two things about the features: their independence and their scale. Most importantly, a Euclidean metric assumes that features are (nearly) orthogonal so the distance along different axis can be summed. A Euclidean metric also assumes that each fea- ture is equally important. Thus a distance of 1 unit in the X direction is perceptually identical to one unit in the Y di- rection. This is unlikely to be true, and this paper describes principled means of \ufb01nding the appropriate weighting. Much work on music similarity and search calculates a feature vector that describes the acoustics of the song, and then computes a distance between these features. In this work we describe six means of assigning weights to the di- mensions and compare their performance. The purpose of this paper is not to determine the best similarity measure\u2014 after all evaluation of a personal decision such as similarity is dif\ufb01cult\u2014but instead to test and compare several quan- titative approaches that MIR practitioners can use to create their own similarity metric. West\u2019s recent paper provides a good overview of the problem and describes successful ap- proaches for music similarity [11]. We hope to improve the performance of future systems by describing techniques for embedding features in a metric space. We measure the performance of our system by testing identi\ufb01cation ability with a k-nearest neighbor (kNN) clas- si\ufb01er. A kNN classi\ufb01er is based on distances between the query point and labeled training examples. If our metric space is \u201cgood\u201d then similar songs will be close together and kNN classi\ufb01cation will produce the right identi\ufb01cation. In our case, we try to identify the album, artist or blog asso- ciated with each song. A kNN classi\ufb01er has several advantages for our task. A kNN classifer is simple to implement, and with large amounts of data they can be shown to give an error rate that is no worse than twice the optimal recognizer [2]. Simple clas- si\ufb01ers have often been shown to produce surprisingly good results [5]. The nearest-neighbor formulation is interesting in our application because we are more interested in \ufb01nd- ing similar songs, than we are in measuring the distance between distant songs or conventional classi\ufb01cation. Thus kNN classi\ufb01cation is a good metric for measuring our ability to place songs into a (linear) similarity space. 2 DATA Our data comes from the top 1000 most-popular mp3 blogs on the Web, as de\ufb01ned by music blog aggregator, The Hype Machine\u2019s \u201cTOP MUSIC BLOGS On The Net\u201d 1 . We an- alyzed each new mp3 track that was posted on these mp3 blogs during the \ufb01rst three weeks of March 2008. In the ongoing quest to discover new music, music blogs provide an engaging and highly useful resource. Their cre- 1 http://hypem.com/toplist 313 ISMIR 2008 \u2013 Session 3a \u2013 Content-Based Retrieval, Categorization and Similarity 1 ators are passionate about music, given that they\u2019re blogging about it all the time. And unlike a traditional playlist or set of recommended tracks, music blogs also provide personal commentary which gives the blog consumer a social context for the music. We\u2019re interested in comparing the similarity across music posted on the same blog to the similarity between different tracks by the same artist or from the same album. One of the dif\ufb01culties in gathering this type of data is the large amount of uncertainty and noise that exists within the metadata that describes mp3s. Our general experience analyzing Web mp3s has been that less than a third of the tracks we encounter have reliable (if any) metadata in their ID3 tags. Therefore the metadata we used for our artists and album calculations is limited to what we were able to parse out of valid ID3 tags or information we could infer from the \ufb01lename, or the HTML used to reference the track. In these 1000 blogs, we found 5689 different songs from 2394 albums and 3366 artists. Just counting blogs for which we found labeled and unique songs, we were left with 586 different blogs in our dataset. After removing IDs for which we did not have enough data (less than 5 instances) we were left with 74 distinct albums, 164 different artists, and 319 blogs. The style of music represented in this collection dif- fers from blog to blog. Many mp3 blogs could be broadly classi\ufb01ed as \u201cIndie\u201d or \u201cIndie Rock\u201d, but music shared on a speci\ufb01c blog is more representative of the blogger\u2019s personal taste than any particular genre. 3 FEATURES We characterized each song using acoustic analysis provided via a public web API provided by The Echo Nest [4]. We send a song to their system, they analyze the acoustics and provide 18 features to characterize global properties of the songs. Although we did not test it, we expect that features from a system such as Marsyas [9] will give similar results. The Echo Nest Analyze API splits the song into seg- ments, each a section of audio with similar acoustic qual- ities. These segments are from 80ms to multiple seconds in length. For each segment they calculate the loudness, attack time and the other measures of the variation in the segment. There are also global properties such as tempo and time sig- nature. The features we used are as follows [4]: \u2022 segmentDurationMean: mean segment duration (sec.). \u2022 segmentDurationVariance: variance of the segment dura- tion (sec.2)\u2014smaller variances indicate more regular seg- ment durations. \u2022 timeLoudnessMaxMean: mean time to the segment maxi- mum, or attack duration (sec.). \u2022 loudnessMaxMean: mean of segments\u2019 maximum loudness (dB). \u2022 loudnessMaxVariance: variance of the segments\u2019 maximum loudness (dB2). Larger variances mean larger dynamic range in the song. \u2022 loudnessBeginMean: average loudness at the start of seg- ments (dB). \u2022 loudnessBeginVariance: variance of the loudness at the start of segments (dB2). Correlated with loudnessMaxVariance \u2022 loudnessDynamicsMean: average of overall dynamic range in the segments (dB). \u2022 loudnessDynamicsVariance: segment dynamic range vari- ance (dB2). Higher variances suggest more dynamics in each segment. \u2022 loudness: overall loudness estimate of the track (dB). \u2022 tempo: overall track tempo estimate (in beat per minute, BPM). Doubling and halving errors are possible. \u2022 tempoCon\ufb01dence: a measure of the con\ufb01dence of the tempo estimate (beween 0 and 1). \u2022 beatVariance: a measure of the regularity of the beat (secs.2). \u2022 tatum: estimated overall tatum duration (in seconds). Tatums are subdivisions of the beat. \u2022 tatumCon\ufb01dence: a measure of the con\ufb01dence of the tatum estimate (beween 0 and 1). \u2022 numTatumsPerBeat: number of tatums per beat \u2022 timeSignature: estimated time signature (number of beats per measure). This is perceptual measures, not what the composer might have written on the score. The description goes as follows: 0=None, 1=Unknown (perhaps too many variations), 2=2/4, 3=3/4 (eg waltz), 4=4/4 (typical of pop music), 5=5/4, 6=6/4. 7=7/4 etc. \u2022 timeSignatureStability: a rough estimate of the stability of the time signature throughout the track 4 ALGORITHMS We create a feature vector by concatenating the individual feature-analysis results (we used the order described in Sec- tion 3, but the order is irrelevant). Let us denote all input features as the matrix f, which is an mxn array of n m- dimensional feature vectors, one vector for each song\u2019s anal- ysis results. Further let fi be the ith feature (column-)vector in f. To measure the distances between different feature vectors, we use learned Mahalanobis metrics [6]. A Mahalanobis (pseudo-)metric is de\ufb01ned as d(fi, fj) = (fi \u2212fj)\u22a4M(fi \u2212fj), (1) where M is any well-de\ufb01ned positive semi-de\ufb01nite matrix. From Eq. (1) it should be clear that the Euclidean distance is a special case of the Mahalanobis metric with M = I, the identity matrix. We considered \ufb01ve different algorithms from the research literature to learn a Mahalanobis matrix to convert the raw features into a well-behaved metric space. Each of the algorithms either learns a positive semi-de\ufb01nite 314 ISMIR 2008 \u2013 Session 3a \u2013 Content-Based Retrieval, Categorization and Similarity 1 matrix M or a matrix A, such that M = A\u22a4A. We can uniquely decompose any positive semi-de\ufb01nite matrix as M = A\u22a4A, for some real-valued matrix A (up to rotation). This reduces Eq. (1) to d(fi, fj) = \u2225A(fi \u2212fj)\u22252, (2) the Euclidean metric after the transformation fi \u2192Afi. One of the approaches\u2014whitening\u2014is unsupervised, i.e. the algorithm does not require any side-information in addi- tion to the pure feature vectors f. The other four use labels to tune the Mahalanobis matrix A so that similar songs are likely to be close to each other in the metric space. In this study we use album, artist and blog labels for each song as a measure of similarity. We evaluate our algorithm by test- ing the performance of a nearest-neighbor classi\ufb01er in these new spaces. The output of our algorithms is F = Af. The learned matrix A is of size m \u00d7 m in this paper, but also can be m\u2032 \u00d7 m, where m\u2032 < m, in which case it reduces the di- mensionality of the output space. The result matrix F has n points arrayed so similar points are close together. We partition the algorithms that we discuss into two groups: al- gorithms based on second-order statistics, and algorithms based on optimization. We will discuss each in turn.",
        "zenodo_id": 1415554,
        "dblp_key": "conf/ismir/SlaneyWW08"
    },
    {
        "title": "The Quest for Musical Genres: Do the Experts and the Wisdom of Crowds Agree?",
        "author": [
            "Mohamed Sordo",
            "\u00d2scar Celma",
            "Martin Blech",
            "Enric Guaus"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415262",
        "url": "https://doi.org/10.5281/zenodo.1415262",
        "ee": "https://zenodo.org/records/1415262/files/SordoCBG08.pdf",
        "abstract": "This paper presents some \ufb01ndings around musical genres. The main goal is to analyse whether there is any agree- ment between a group of experts and a community, when de\ufb01ning a set of genres and their relationships. For this pur- pose, three different experiments are conducted using two datasets: the MP3.com expert taxonomy, and last.fm tags at artist level. The experimental results show a clear agree- ment for some components of the taxonomy (Blues, Hip- Hop), whilst in other cases (e.g. Rock) there is no correla- tions. Interestingly enough, the same results are found in the MIREX2007 results for audio genre classi\ufb01cation task. Therefore, a multi\u2013faceted approach for musical genre using expert based classi\ufb01cations, dynamic associations derived from the wisdom of crowds, and content\u2013based analysis can improve genre classi\ufb01cation, as well as other relevant MIR tasks such as music similarity or music recommendation. 1 INTRODUCTION Music genres are connected to emotional, cultural and social aspects, and all of them in\ufb02uence our music understanding. The combination of these factors produce a personal orga- nization of music which is, somehow, the basis for (human) musical genre classi\ufb01cation. Indeed, musical genres have different meanings for different people, communities, and countries [2]. The use of musical genres has been deeply discussed by the MIR community. A good starting point is the review by McKay [5]. The authors suggested that musical genres are an inconsistent way to organize music. Yet, musical genres remain a very effective way to describe and tag artists. Broadly speaking, there are two complementary approa- ches when de\ufb01ning a set of genre labels: (i) the de\ufb01nition of a controlled vocabulary by a group of experts or musi- cologists, and (ii) the collaborative effort of a community (social tagging). The goal of the former approach is the creation of a list of terms, organised in a hierarchy. A hi- erarchy includes the relationships among the terms; such as hyponymy. The latter method, social tagging, is a less formal bottom\u2013up approach, where the set of terms emerge during the (manual) annotation process. The output of this approach is called folksonomy. The aim of this paper is, then, to study the relationships between these two approaches. Concretely, we want to study whether the controlled vocabulary de\ufb01ned by a group of ex- perts concord with the tag annotations of a large community. Section 2 introduces the pros and cons of expert\u2013based taxonomies and music folksonomies. To compare the simi- larities between both approaches, we gathered data from two different websites: a musical genre taxonomy from MP3.com, and a large dataset of artists\u2019 tags gathered from the last.fm community. Section 3 presents these datasets. The experi- mental results, presented in section 4, are conducted in or- der to analyse the relationships between the genres used in the MP3.com taxonomy, and the genre\u2013tags annotated in the artist dataset from last.fm. Finally, section 5 concludes and summarizes the main \ufb01ndings. 2 MUSICAL GENRES CLASSIFICATION",
        "zenodo_id": 1415262,
        "dblp_key": "conf/ismir/SordoCBG08"
    },
    {
        "title": "Automatic Chord Recognition Based on Probabilistic Integration of Chord Transition and Bass Pitch Estimation.",
        "author": [
            "Kouhei Sumi",
            "Katsutoshi Itoyama",
            "Kazuyoshi Yoshii",
            "Kazunori Komatani",
            "Tetsuya Ogata",
            "Hiroshi G. Okuno"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417221",
        "url": "https://doi.org/10.5281/zenodo.1417221",
        "ee": "https://zenodo.org/records/1417221/files/SumiIYKOO08.pdf",
        "abstract": "This paper presents a method that identi\ufb01es musical chords in polyphonic musical signals. As musical chords mainly represent the harmony of music and are related to other mu- sical elements such as melody and rhythm, the performance of chord recognition should improve if this interrelation- ship is taken into consideration. Nevertheless, this inter- relationship has not been utilized in the literature as far as the authors are aware. In this paper, bass lines are utilized as clues for improving chord recognition because they can be regarded as an element of the melody. A probabilis- tic framework is devised to uniformly integrate bass lines extracted by using bass pitch estimation into a hypothesis- search-based chord recognition. To prune the hypothesis space of the search, the hypothesis reliability is de\ufb01ned as the weighted sum of three reliabilities: the likelihood of Gaussian Mixture Models for the observed features, the joint probability of chord and bass pitch, and the chord transi- tion N-gram probability. Experimental results show that our method recognized the chord sequences of 150 songs in twelve Beatles albums; the average frame-rate accuracy of the results was 73.4%. Keyword: chord recognition, bass line, hypothesis search, probabilistic integration 1 INTRODUCTION In recent years, automatic recognition of musical elements such as melody, harmony, and rhythm (Figure 1) from poly- phonic musical signals has become a subject of great inter- est. The spread of high-capacity portable digital audio play- ers and online music distribution has allowed a diverse user base to store a large number of musical pieces on these play- ers. Information on the content of musical pieces such as their musical structure, mood and genre can be used together with text-based information to make music information re- trieval (MIR) more ef\ufb01cient and effective. Manual annota- tion requires an immense amount of effort, and maintaining a consistent level of quality is not easy. Thus, techniques Melody Harmony Rhythm Chord Beat Bass lines Music Melody Harmony Rhythm Chord Beat Bass lines Music Figure 1. Musical elements for extracting musical elements are essential for obtaining content-based information from musical signals. A key principle in analyzing musical signals is that mu- sical elements are related to each other. Because composers exploit the interrelationship among musical elements, this interrelationship should be considered when analyzing the elements as well. Most studies in the literature have dealt with these elements independently. This paper exploits the interrelationship between chord sequences and bass lines to improve the performance of chord recognition. The chord sequence is regarded as an element of the harmony, while bass lines are regarded as an element of the melody. The chord sequence consists of a chord sym- bol sequence and chord boundary sequence. As the chord sequence may represent the mood of music, it can be used to calculate the similarity in mood between musical pieces. This similarity is important in MIR and music recommen- dation. On the other hand, the bass line represents a melody in the bass register; thus, it leads the chord progression. A recent approach adopted recently by many researchers for automated description of the chord sequence is the use of Hidden Markov Models (HMMs). Several methods have been suggested to explore the analogy between speech recog- nition and chord recognition and to consider the temporal connection of chords [1, 2, 3]. Sheh et al. [1] proposed a method that uses the extended Pitch Class Pro\ufb01le (PCP) [4] as a feature vector. They used an HMM that had one state per chord with a large set of classes (147 chord types). How- ever, they were not able to obtain good enough results for recognition. Bello et al. [2] used chroma features and an HMM; they improved accuracy by incorporating musical 39 ISMIR 2008 \u2013 Session 1a \u2013 Harmony knowledge into the model. Lee et al. [3] built key-speci\ufb01c models for automatic chord transcription. They used a 6- dimensional feature vector, called Tonal Centroid that is based on Tonnetz [5]. Higher accuracies were obtained by limiting the number of chord types that could be recognized. Yoshioka et al. pointed out that chord symbols affect chord boundary recognition and vice versa [6]. They developed a method that concurrently recognizes chord symbols and boundaries by using a hypothesis search that recognizes the chord sequence and key. While previous studies have treated only the features of chords, we focus on the interrelationship among musical el- ements and integrate information about bass lines into chord recognition in a probabilistic framework. The framework enables us to deal with multiple musical elements uniformly and integrate information obtained from statistical analyses of real music. This paper is organized as follows: Section 2 describes our motivation for developing an automatic chord recogni- tion system, the issues in involved in doing so, and our so- lution. Section 3 explains our method in concrete terms. Section 4 reports the experimental results and describes the effectiveness of our method. Our conclusions are discussed in Section 5. 2 CHORD RECOGNITION USING BASS PITCH ESTIMATION",
        "zenodo_id": 1417221,
        "dblp_key": "conf/ismir/SumiIYKOO08"
    },
    {
        "title": "Ternary Semantic Analysis of Social Tags for Personalized Music Recommendation.",
        "author": [
            "Panagiotis Symeonidis",
            "Maria M. Ruxanda",
            "Alexandros Nanopoulos",
            "Yannis Manolopoulos"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416672",
        "url": "https://doi.org/10.5281/zenodo.1416672",
        "ee": "https://zenodo.org/records/1416672/files/SymeonidisRNM08.pdf",
        "abstract": "Social tagging is the process by which many users add metadata in the form of keywords, to annotate information items. In case of music, the annotated items can be songs, artists, albums. Current music recommenders which em- ploy social tagging to improve the music recommenda- tion, fail to always provide appropriate item recommen- dations, because: (i) users may have different interests for a musical item, and (ii) musical items may have multiple facets. In this paper, we propose an approach that tackles the problem of the multimodal use of music. We develop a uni\ufb01ed framework, represented by a 3-order tensor, to model altogether users, tags, and items. Then, we recom- mend musical items according to users multimodal per- ception of music, by performing latent semantic analysis and dimensionality reduction using the Higher Order Sin- gular Value Decomposition technique. We experimentally evaluate the proposed method against two state-of-the-art recommendations algorithms using real Last.fm data. Our results show signi\ufb01cant improvements in terms of effec- tiveness measured through recall/precision. 1 INTRODUCTION Social tagging is the process by which many users add metadata in the form of keywords to annotate and catego- rize information items such as songs, pictures, products. In general, social tagging is associated to the \u201cWeb 2.0\u201d technologies and has already become an important source of information for recommendation. In the music domain, popular web systems such as Last.fm and MyStrands pro- vide possibility for users to tag with free text labels an item of interest - e.g., artist, song, album. Such systems can further exploit these social tags to improve the search mechanisms and the personalized music recommendation. Recent research in the music \ufb01eld has also focused on exploiting the social tags in various ways. For example, a partial solution to the cold-start problem of music rec- ommenders has been proposed in [4]: social tags are used for the automatic generation of new tags, which then can THIS RESEARCH WAS SUPPORTED IN PART BY THE DAN- ISH RESEARCH COUNCIL FOR TECHNOLOGY AND PRODUC- TION SCIENCES PROJECT 26-04-0092 INTELLIGENT SOUND, AND BY GREEK SECRETARIAT FOR RESEARCH AND TECH- NOLOGY \u03a0ABET (05\u03a0AB216) PROJECT. be used to label the untagged music. In [9], the social tags are investigated as a source of semantic metadata for music, which can be used to generate a psychologically- motivated search-space representing musical emotion. However, the social tags carry useful information not only about the musical items they label, but also about the users who tagged. This aspect is not being fully ex- ploited, neither by the music recommenders, neither in the research \ufb01eld. Music is an artistic concept, and the musi- cal items (artists, songs, albums) have a rich and complex view, which is only partially perceived by particular users, depending on their emotional and cultural perspective on music. Social tags are a powerful mechanism that reveal 3-dimensional correlations between users\u2013tags\u2013items. This triplet information can project for each user his perception of a particular musical item. However, the current music recommender systems are commonly using collaborative \ufb01ltering techniques, which traditionally exploit only pairs of 2-dimensional data. Thus, they are not capable of cap- turing well the multimodal use of music. As a simple example, let us consider the social tag- ging system of artists in Last.fm. Assume two users. One is very fond of young female singers and therefore has tagged Christina Aguilera as \u201csexy\u201d and Beyonce as \u201csen- sual\u201d. Another is fond of male singers and has tagged Lenny Kravitz as \u201csexy\u201d and \u201cmale vocalists\u201d. When want- ing to listen to \u201csexy\u201d music, both users are recommended male and female singers, while the \ufb01rst user is expecting female singers and the other prefers the opposite. Recent research has focused on developing recommen- dation algorithms [7, 14], which try to exploit tags given by users on speci\ufb01c items. However, the existing algo- rithms do not consider the 3 dimensions of the problem altogether, and therefore they miss a part of the semantics that is carried by the 3-dimensions. In this paper, we address the problem of music recom- mendation by capturing the multimodal perception of mu- sic by particular users. We perform 3-dimensional analy- sis on the social tags data, attempting to discover the la- tent factors that govern the associations among the triplets user\u2013tag\u2013item. Consequently, the musical items (artists, songs or albums) can be recommended according to the captured associations. That is, given a user and a tag, the purpose is to predict whether and how much the user is likely to label with this tag a speci\ufb01c musical item. Our strategy in dealing with the 3-dimensional social 219 ISMIR 2008 \u2013 Session 2c \u2013 Knowledge Representation, Tags, Metadata tagging data, is to develop a uni\ufb01ed framework to model the three dimensions. Thus, user-tag-item data is rep- resented by a 3-order tensor. Consequently, we have to deal with the data sparsity problem: the three-way data is highly sparse, especially that each user only tags a small number of items. Latent Semantic Indexing (LSI) has been proved useful to address the data sparseness in 2- dimensional data recommender systems, however, it is still an open problem for the 3-dimensional data case. There- fore, we perform 3-mode analysis, using the Higher Order Singular Value Decomposition (HOSVD) technique. The contributions of our approach are as follow: (1) we provide a method to improve music recommendation by capturing users multimodal perception of music; (2) we develop a uni\ufb01ed framework, represented by a 3-order ten- sor, to model the three types of entities that exist in social tagging data; (3) we apply dimensionality reduction in 3- order tensors to reveal the latent semantic associations be- tween users, tags, and items; (4) we perform experimental comparison of the proposed method against two state-of- the-art recommendations algorithms, using Last.fm data; our results show signi\ufb01cant improvements in terms of ef- fectiveness measured through recall/precision. The rest of this paper is organized as follows. Section 2 summarizes the related work, whereas Section 3 brie\ufb02y re- views background techniques employed in our approach. A motivating example and the proposed approach are de- scribed in Section 4. Experimental results are given in Section 5. Finally, Section 6 concludes this paper. 2 RELATED WORK Music recommendation has been addressed in various work. For example, in Logan [11] music recommendation is done based solely on using acoustic-based similarity measure. Other approaches try to bridge the semantic gap and em- ploy hybrid music recommendation methods. Thus, Yoshii et al. [15] model collaborative \ufb01ltering (CF) data and audio- content data together, and unobservable user preferences are statistically estimated. Li et al. [10] employ a proba- bilistic model estimation for CF, where musical items are clustered based on audio-content and user rating, and pre- dictions are made considering the Gaussian distribution of ratings. Celma [2] mines music information from the Web (album releases, MP3 blogs, etc.) and is using it together with user pro\ufb01ling and audio-content descriptions. The above work can be used to improve the music rec- ommendation by addressing the cold-start problem and the bias of CF towards mainstream music. Along the same lines, an innovative use of social tags has been recently proposed in [4]. Eck et al. [4] predict new tags using au- dio features extracted from music and supervised learning. These automatically-generated tags resemble the charac- teristics of those generated by social taggers, and can be used to label new or poorly tagged music. However, current music recommenders fail to always provide good recommendations, because they do not cap- ture well the interest of particular users in musical items that have multiple facets. Recent research work [4] envis- aged that musical items have multiple facets, but it did not address their multimodal perception by particular users. Since social tagging data carry simultaneous information about both the items and the users who tagged them, we propose to use such data as means to improve music rec- ommendation by capturing the multimodal use of music. The characteristics of social tagging systems have been already studied in the literature. Halpin et al. [6] claimed that there are three main entities in any tagging system: users, items, and tags. In contrast to the above ternary re- lation, recommender systems apply collaborative \ufb01ltering on 2-dimensional spaces. For example, the approach of projecting the 3-dimensional space of social tagging into pair relations {user, item}, {user, tag}, {tag, item}, is applied in well-known recommendation algorithms such as Penalty-Reward and FolkRank. The Collaborative Tag Suggestions algorithm [14], also known as Penalty-Reward (PR), uses an authority score for each user, which mea- sures how well each user has tagged in the past. This au- thority score can be computed via an iterative algorithm such as HITS [8]. FolkRank algorithm [7] is inspired by the seminal PageRank [12] algorithm. The key idea of FolkRank is that an item, which is tagged with important tags by important users, becomes important itself (and the same holds for tags and users). However, the above state-of-art algorithms miss a part of the total interaction between the three dimensions of the social tagging space. In contrast, our approach develops a uni\ufb01ed framework to concurrently model the three dimen- sions by employing a 3-order tensor, on which latent se- mantic analysis is performed using HOSVD technique [3]. The HOSVD technique has been successfully applied for computer vision problems. We also use in our approach the work proposed in Wang and Ahuja [13], which present a novel multi-linear algebra-based method to reduce the dimensionality representation of multi-dimensional data. 3 PRELIMINARIES - TENSORS AND HOSVD In the following, we denote tensors by calligraphic up- percase letters (e.g., A, B), matrices by uppercase letters (e.g., A, B), scalars by lowercase letters (e.g., a, b), and vectors by bold lowercase letters (e.g., a, b). SVD and Latent Semantic Indexing The singular value decomposition (SVD) [1] of a ma- trix FI1\u00d7I2 can be written as a product of three matrices, as shown in Equation 1: FI1\u00d7I2 = UI1\u00d7I1 \u00b7 SI1\u00d7I2 \u00b7 V T I2\u00d7I2, (1) where U is the matrix with the left singular vectors of F, V T is the transpose of the matrix V with the right singular vectors of F, and S is the diagonal matrix of (ordered) singular values of F. By preserving only the largest c < min{I1, I2} sin- gular values of S, SVD results to matrix \u02c6F, which is an approximation of F. In Information Retrieval, this tech- nique is used by Latent Semantic Indexing (LSI) [5], to deal with the latent semantic associations of terms in texts 220 ISMIR 2008 \u2013 Session 2c \u2013 Knowledge Representation, Tags, Metadata Figure 1. Visualization of the three unfoldings of a 3- order tensor. and to reveal the major trends in F. The tuning of c is em- pirically determined by the information percentage that is preserved compared to the original matrix [3]. Tensors A tensor is a multi-dimensional matrix. A N-order ten- sor A is denoted as A \u2208RI1...IN , with elements ai1,...,iN . In this paper, for the purposes of our approach, we only use 3-order tensors. HOSVD The high-order singular value decomposition [3] gen- eralizes the SVD computation to multi-dimensional matri- ces. To apply HOSVD on a 3-order tensor A, three matrix unfolding operations are de\ufb01ned as follows [3]: A1 \u2208RI1\u00d7I2I3, A2 \u2208RI2\u00d7I1I3, A3 \u2208RI1I2\u00d7I3 where A1, A2, A3 are called the 1-mode, 2-mode, 3-mode matrix unfoldings of A, respectively. The unfoldings of A in the three modes is illustrated in Figure 1. Example: De\ufb01ne a tensor A \u2208R3\u00d72\u00d73 by a111 = a112 = a211 = \u2212a212 = 1, a213 = a311 = a313 = a121 = a122 = a221 = \u2212a222 = 2, a223 = a321 = a323 = 4, a113 = a312 = a123 = a322 = 0. The ten- sor and its 1-mode matrix unfolding A1 \u2208RI1\u00d7I2I3 are illustrated in Figure 2. Next, we de\ufb01ne the n-mode product of an N-order ten- sor A \u2208RI1\u00d7...\u00d7IN by a matrix U \u2208RJn\u00d7In, which is denoted as A \u00d7n U. The result of the n-mode product is an (I1 \u00d7 I2 \u00d7 . . . \u00d7 In\u22121 \u00d7 Jn \u00d7 In+1 \u00d7 . . . \u00d7 IN)-tensor, the entries of which are de\ufb01ned as follows: (A \u00d7n U)i1i2...in\u22121jnin+1...iN = \u0002 in ai1i2...in\u22121inin+1...iN ujnin (2) Since we focus on 3-order tensors, n \u2208{1, 2, 3}, we use 1-mode, 2-mode, and 3-mode products. 1 2 1 2 2 4 0 2 4 2 4 1 -1 -2 0 0 2 0 1 1 0 2 2 0 1 -1 2 2 -2 4 2 0 2 4 0 4 A1 = Figure 2. Visualization of tensor A \u2208R3\u00d72\u00d73 and its 1-mode matrix unfolding. In terms of n-mode products, SVD on a regular two- dimensional matrix (i.e., 2-order tensor), can be rewritten as follows [3]: F = S \u00d71 U (1) \u00d72 U (2) (3) where U (1) = (u(1) 1 u(1) 2 . . . u(1) I1 ) is a unitary (I1 \u00d7 I1)- matrix, U (2) = (u(2) 1 u(2) 2 . . . u(2) I1 ) is a unitary (I2 \u00d7 I2)- matrix, and S is a (I1 \u00d7 I2)-matrix with the properties of: (i) pseudo-diagonality: S = diag(\u03c31, \u03c32, . . . , \u03c3min{I1,I2}) (ii) ordering: \u03c31 \u2265\u03c32 \u2265. . . \u2265\u03c3min{I1,I2} \u22650. By extending this form of SVD, HOSVD of 3-order tensor A can be written as follows [3]: A = S \u00d71 U (1) \u00d72 U (2) \u00d73 U (3) (4) where U (1), U (2), U (3) contain the orthonormal vectors (called the 1-mode, 2-mode and 3-mode singular vectors, respectively) spanning the column space of the A1, A2, A3 matrix unfoldings. S is the core tensor and has the prop- erty of all orthogonality. 4 THE PROPOSED APPROACH We \ufb01rst provide the outline of our approach, which we name Tensor Reduction, through a motivating example. Next, we analyze the steps of the proposed algorithm.",
        "zenodo_id": 1416672,
        "dblp_key": "conf/ismir/SymeonidisRNM08"
    },
    {
        "title": "Rhythm Complexity Measures: A Comparison of Mathematical Models of Human Perception and Performance.",
        "author": [
            "Eric Thul",
            "Godfried T. Toussaint"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416218",
        "url": "https://doi.org/10.5281/zenodo.1416218",
        "ee": "https://zenodo.org/records/1416218/files/ThulT08.pdf",
        "abstract": "Thirty two measures of rhythm complexity are compared using three widely different rhythm data sets. Twenty-two of these measures have been investigated in a limited con- text in the past, and ten new measures are explored here. Some of these measures are mathematically inspired, some were designed to measure syncopation, some were intended to predict various measures of human performance, some are based on constructs from music theory, such as Press- ing\u2019s cognitive complexity, and others are direct measures of different aspects of human performance, such as perceptual complexity, meter complexity, and performance complex- ity. In each data set the rhythms are ranked either accord- ing to increasing complexity using the judgements of human subjects, or using calculations with the computational mod- els. Spearman rank correlation coef\ufb01cients are computed between all pairs of rhythm rankings. Then phylogenetic trees are used to visualize and cluster the correlation co- ef\ufb01cients. Among the many conclusions evident from the results, there are several observations common to all three data sets that are worthy of note. The syncopation measures form a tight cluster far from other clusters. The human per- formance measures fall in the same cluster as the syncopa- tion measures. The complexity measures based on statisti- cal properties of the inter-onset-interval histograms are poor predictors of syncopation or human performance complex- ity. Finally, this research suggests several open problems. 1 INTRODUCTION Many music researchers consider rhythm to be the most im- portant characteristic of music. Furthermore, one of the main features of rhythm is its complexity. Threfore mea- sures of the complexity of a rhythm constitute key features useful for music pattern recognition and music information retrieval, as well as ethnomusicological analyses of world music [17, 18]. Since the notion of complexity is \ufb02exible, it is not surprising that in the past a variety of different mea- sures of complexity has appeared in the literature. Areas where such measures have been applied range from psychol- ogy, enginneering, computer science and mathematics, to music theory. Given such a wide range of applicable \ufb01elds, different techniques for measuring complexity have been developed. For example, one can analyze a rhythm\u2019s bi- nary sequence representation, ask listeners to rate a rhythm\u2019s complexity, or ask musicians to perform a rhythm. There- fore, in our work, we include measures of information and coding complexity, performance complexity, and cognitive complexity. Furthermore, there are traditional concepts in music such as syncopation [10] which may also be consid- ered as measures of rhythm complexity [7, 8]. With the exception of [7, 8], previous research on rhythm complexity has been limited to determining how good a fea- ture it is for music pattern recognition, or how well it mod- els human judgements of complexity [17, 18]. Moreover, for such studies researchers have used data (families of rhythms) that were generated arti\ufb01cially and randomly with some con- straints. Here, we not only use a large group comprised of 32 measures of complexity that employ a wide variety of measurement techniques, but we also validate these mea- sures against human judgements of perceptual, meter, and performance complexity using three diverse data sets. 2 COMPLEXITY MEASURES One can broadly categorize the complexity measures used in this study into two distinct categories: human perfor- mance measures directly obtained from psychological ex- periments, and measures obtained from mathematical mod- els of rhythm complexity. The human performance mea- sures can be subdivided into three types: perceptual com- plexity, meter complexity, and performance complexity. Per- ceptual complexity is obtained by asking human subjects to judge complexity as they listen to rhythms. Meter complex- ity is obtained by measuring how well the human subjects are able to track the underlying metric beat of a rhythm. It is worth noting that some researchers, for example in mu- sic psychology [4], refer to the metric beat as the pulse. 663 ISMIR 2008 \u2013 Session 5c \u2013 Rhythm and Meter Here we reserve the word pulse for the largest duration in- terval that evenly divides into all the inter-onset onsets (IOI) present in a family of rhythms. This is common terminology in ethnomusicology and music technology. Performance complexity measures pertain to how well the subjects can reproduce (execute, play-back) the rhythms, usually by tap- ping. The mathematical models can be subdivided into two main categories: those that are designed to measure synco- pation, and those that are designed to measure irregularity. The irregularity measures can be divided into statistical and minimum-weight-assignment measures. Due to lack of space, we cannot provide a detailed de- scription of all the complexity measures tested. Thus we list the complexity measures with each corresponding es- sential reference in the literature for further information, along with a label in parentheses pertaining to the phylo- gentic tree labels used in Figures 1, 2, and 3. Measures of syncopation are listed \ufb01rst. The Longuet-Higgins and Lee measure (lhl) [4, 14], along with Smith and Honing\u2019s ver- sion (smith) [19], take advantage of a metric hierarchy of weights [13] to calculate syncopation. A variation of Tous- saint\u2019s metrical complexity (metrical) [21] and Keith\u2019s mea- sure (keith) [10] also use this hierarchy to judge syncopa- tion. The Weighted Note-to-Beat Distance (wnbd, wnbd2, wnbd4, wnbd8) [7] uses the distance from onsets to metric beats to gauge syncopation. Second, we list the measures regarding mathematical ir- regularity. IOI histogram measures for entropy (ioi-g-h, ioi- l-h), standard deviation (ioi-g-sd, ioi-l-sd), and maximum bin height (ioi-g-mm, ioi-l-mm) were used to determine the complexity of both global (full) IOIs [24] and local (relative, adjacent) IOIs [18]. Also, pertaining to entropy calculations are the Coded Element Processing System (ceps) [26], H(k- span) complexity (hk) [25], and the H(run-span) complexity (hrun) [25], which all measure the uncertainty [5] of ob- taining sub-patterns in a rhythm. The directed swap dis- tance (dswap, dswap2, dswap4, dswap8) [1] computes the minimum weight of a linear assignment between onsets of a rhythm and a meter with an onset at every second, fourth, or eigth pulse, and also the average over each meter. Two other measures, Rhythmic Oddity (oddity) [22] and Off-Beatness (off-beatness) [22] take a geometric approach. Third, those measures which do not easily fall into a cat- egory are listed. These include the Lempel-Ziv compres- sion measure (lz) [12], Tanguiane\u2019s [20] complexity mea- sure, which looks at sub-patterns at each metrical beat level, and Pressing\u2019s Cognitive Complexity measure (pressing) de- signed on the basis of music theory principles, which gen- erates rhythmic patterns at each metrical beat, assigning ap- propriate weights to special patterns [16]. Furthermore, Tan- guiane\u2019s measure uses the max (tmmax) and average (tmavg) complexities over different metrical beat levels. In addition, derivatives (tmuavg, tmumax) without the restriction of sub- patterns starting with an onset, were tested. 3 EXPERIMENTAL DATA The measures of complexity in \u00a7 2, were compared using three rhythm data sets. Each data set had been compiled to test human judgements regarding the perceptual, meter, and performance complexities of the rhythms. The \ufb01rst data set shown in Table 1 was synthesized by Povel and Essens in 1985 [15] and then later studied by Shmulevich and Povel in 2000 [17]. The second data set shown in Table 2 was created by Essens in 1995 [2]. The third data set shown in Table 3 was generated by Fitch and Rosenfeld in 2007 [4]. In addition to the rhythms themselves, the results of several human performance complexity measures used in this work are contained in Tables 1, 2, and 3. In the following we de- scribe the methodologies of Povel and Essens [15], Shmule- vich and Povel [17], Essens [2], and Fitch and Rosenfeld [4], used to obtain the human judgements of complexity.",
        "zenodo_id": 1416218,
        "dblp_key": "conf/ismir/ThulT08"
    },
    {
        "title": "Multiple-Feature Fusion Based Onset Detection for Solo Singing Voice.",
        "author": [
            "Chee-Chuan Toh",
            "Bingjun Zhang",
            "Ye Wang 0007"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1414756",
        "url": "https://doi.org/10.5281/zenodo.1414756",
        "ee": "https://zenodo.org/records/1414756/files/TohZW08.pdf",
        "abstract": "Onset detection is a challenging problem in automatic singing transcription.  In this paper, we address singing onset detection with three main contributions. First, we outline the nature of a singing voice and present a new singing onset detection approach based on supervised machine learning.  In this approach, two Gaussian Mixture Models (GMMs) are used to classify audio features of onset frames and non-onset frames.  Second, existing audio features are thoroughly evaluated for this approach to singing onset detection. Third, feature-level and decision-level fusion are employed to fuse different features for a higher level of performance. Evaluated on a recorded singing database, the proposed approach outperforms state-of-the-art onset detection algorithms significantly.",
        "zenodo_id": 1414756,
        "dblp_key": "conf/ismir/TohZW08"
    },
    {
        "title": "Multi-Label Classification of Music into Emotions.",
        "author": [
            "Konstantinos Trohidis",
            "Grigorios Tsoumakas",
            "George Kalliris",
            "Ioannis P. Vlahavas"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1414900",
        "url": "https://doi.org/10.5281/zenodo.1414900",
        "ee": "https://zenodo.org/records/1414900/files/TrohidisTKV08.pdf",
        "abstract": "In this paper, the automated detection of emotion in music is modeled as a multilabel classi\ufb01cation task, where a piece of music may belong to more than one class. Four algorithms are evaluated and compared in this task. Furthermore, the predictive power of several audio features is evaluated using a new multilabel feature selection method. Experiments are conducted on a set of 593 songs with 6 clusters of music emotions based on the Tellegen-Watson-Clark model. Re- sults provide interesting insights into the quality of the dis- cussed algorithms and features. 1 INTRODUCTION Humans, by nature, are emotionally affected by music. Who can argue against the famous quote of the German philoso- pher Friedrich Nietzsche, who said that \u201cwithout music, life would be a mistake\u201d. As music databases grow in size and number, the retrieval of music by emotion is becoming an important task for various applications, such as song selec- tion in mobile devices [13], music recommendation systems [1], TV and radio programs 1 and music therapy. Past approaches towards automated detection of emotions in music modeled the learning problem as a single-label classi\ufb01cation [9, 20], regression [19], or multilabel classi- \ufb01cation [6, 7, 17] task. Music may evoke more than one dif- ferent emotion at the same time. We would like to be able to retrieve a piece of music based on any of the associated (classes of) emotions. Single-label classi\ufb01cation and regres- sion cannot model this multiplicity. Therefore, the focus of this paper is on multilabel classi\ufb01cation methods. A secondary contribution of this paper is a new multil- abel dataset with 72 music features for 593 songs catego- rized into one or more out of 6 classes of emotions. The dataset is released to the public 2 , in order to allow com- parative experiments by other researchers. Publicly avail- able multilabel datasets are rare, hindering the progress of research in this area. 1 http://www.musicovery.com/ 2 http://mlkd.csd.auth.gr/multilabel.html The primary contribution of this paper is twofold: \u2022 A comparative experimental evaluation of four multi- label classi\ufb01cation algorithms on the aforementioned dataset using a variety of evaluation measures. Previ- ous work experimented with just a single algorithm. We attempt to raise the awareness of the MIR commu- nity on some of the recent developments in multilabel classi\ufb01cation and show which of those algorithms per- form better for musical data. \u2022 A new multilabel feature selection method. The pro- posed method is experimentally compared against two other methods of the literature. The results show that it can improve the performance of a multilabel clas- si\ufb01cation algorithm that doesn\u2019t take feature impor- tance into account. The remaining of this paper is structured as follows. Sec- tions 2 and 3 provide background material on multilabel classi\ufb01cation and emotion modeling respectively. Section 4 presents the details of the dataset used in this paper. Section 5 presents experimental results comparing the four multi- label classi\ufb01cation algorithms and Section 6 discusses the new multilabel feature selection method. Section 7 presents related work and \ufb01nally, conclusions and future work are drawn in Section 8. 2 MULTILABEL CLASSIFICATION Traditional single-label classi\ufb01cation is concerned with learn- ing from a set of examples that are associated with a single label \u03bb from a set of disjoint labels L, |L| > 1. In multil- abel classi\ufb01cation, the examples are associated with a set of labels Y \u2286L.",
        "zenodo_id": 1414900,
        "dblp_key": "conf/ismir/TrohidisTKV08"
    },
    {
        "title": "Automatic Identification of Simultaneous Singers in Duet Recordings.",
        "author": [
            "Wei-Ho Tsai",
            "Shih-Jie Liao",
            "Catherine Lai"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416358",
        "url": "https://doi.org/10.5281/zenodo.1416358",
        "ee": "https://zenodo.org/records/1416358/files/TsaiLL08.pdf",
        "abstract": "The problem of identifying singers in music recordings has received considerable attention with the explosive growth of the Internet and digital media. Although a number of studies on automatic singer identification from acoustic features have been reported, most systems to date, however, reliably establish the identity of singers in solo recordings only. The research presented in this paper attempts to automatically identify singers in music recordings that contain overlapping singing voices. Two approaches to overlapping singer identification are proposed and evaluated. Results obtained demonstrate the feasibility of the systems.",
        "zenodo_id": 1416358,
        "dblp_key": "conf/ismir/TsaiLL08"
    },
    {
        "title": "Using Bass-line Features for Content-Based MIR.",
        "author": [
            "Yusuke Tsuchihashi",
            "Tetsuro Kitahara",
            "Haruhiro Katayose"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417895",
        "url": "https://doi.org/10.5281/zenodo.1417895",
        "ee": "https://zenodo.org/records/1417895/files/TsuchihashiKK08.pdf",
        "abstract": "We propose new audio features that can be extracted from bass lines. Most previous studies on content-based music information retrieval (MIR) used low-level features such as the mel-frequencycepstral coef\ufb01cients and spectral centroid. Musical similarity based on these features works well to some extent but has a limit to capture \ufb01ne musical charac- teristics. Because bass lines play important roles in both harmonic and rhythmic aspects and have a different style for each music genre, our bass-line features are expected to improve the similarity measure and classi\ufb01cation accuracy. Furthermore, it is possible to achieve a similarity measure that enhances the bass-line characteristics by weighting the bass-line and other features. Results for applying our fea- tures to automatic genre classi\ufb01cation and music collection visualization showed that our features improved genre clas- si\ufb01cation accuracy and did achieve a similarity measure that enhances bass-line characteristics. 1 INTRODUCTION Content-based music information retrieval (MIR) is ex- pected to be a key technology for developing sophisticated MIR systems. One of the main issues in content-based MIR is the design of music features to be extracted from music data. The effectiveness of various features has been exam- ined by several researchers. For audio data, Tzanetakis et al., for example, used spectral shape features (e.g., the spec- tral centroid, rolloff, \ufb02ux, and mel-frequency cepstral coef\ufb01- cients (MFCCs)), rhythm content features (e.g., the beat his- togram), and pitch content features[1]. Pampalk used fea- tures as zero crossing rates, mel spectral features, and \ufb02uc- tuation patterns (the amplitude modulation of the loudness per frequency band)[2]. Aucouturier et al. used MFCCs with various pre- and post-processing techniques[3]. Beren- zweig et al. also used MFCCs [4]. Some researchers used chroma features, also known as pitch-class pro\ufb01les, in addi- tion to or instead of MFCCs [5, 6]. Most studies handling audio data have used low-level features, as described above. Low-level features such as the MFCCs and spectral centroid can be easily calculated and can capture coarse characteristics of musical content to some extent, but they have a clear limit to their ability to capture \ufb01ne characteristics of musical content; it is not clear, for example, which musical aspects, such as chord voicing and instrumentation, affect the similarity of two vectors of MFCCs. The limit of low-level features was also pointed out by Pampalk[2]. Hence, trying to discover new features beyond such low-level features is a common theme in effort to improve content-based MIR. We focus on bass lines to design new features. Bass parts play an important role for two (rhythm and harmony) of the three basic elements of music. The base lines of each music genre proceed in their own style. Moreover, a method for extracting the pitch of bass lines has been proposed[7]. In this paper, therefore, we propose new audio features that can be extracted from bass lines and describe appli- cations of them to automatic genre classi\ufb01cation and mu- sic collection visualization. Section 2 discusses the char- acteristics of bass parts and the design of audio features to be extracted from bass lines. Section 3 describes an algo- rithm for extracting the features. Section 3 also describes the other conventional low-level features used in the exper- iments. Section 4 examines the effectiveness of the pro- posed features in automatic genre classi\ufb01cation tasks. Sec- tion 5 describes an application of the proposed features to music collection visualization using Music Islands [2]. We generate different views of Music Islands by switching the weights given to the features. The differences are discussed. 2 DESIGNING BASS-LINE FEATURES",
        "zenodo_id": 1417895,
        "dblp_key": "conf/ismir/TsuchihashiKK08"
    },
    {
        "title": "Five Approaches to Collecting Tags for Music.",
        "author": [
            "Douglas Turnbull",
            "Luke Barrington",
            "Gert R. G. Lanckriet"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416804",
        "url": "https://doi.org/10.5281/zenodo.1416804",
        "ee": "https://zenodo.org/records/1416804/files/TurnbullBL08.pdf",
        "abstract": "We compare \ufb01ve approaches to collecting tags for music: conducting a survey, harvesting social tags, deploying anno- tation games, mining web documents, and autotagging audio content. The comparison includes a discussion of both scala- bility (\ufb01nancial cost, human involvement, and computational resources) and quality (the cold start problem & popularity bias, strong vs. weak labeling, vocabulary structure & size, and annotation accuracy). We then describe one state-of- the-art system for each approach. The performance of each system is evaluated using a tag-based music information retrieval task. Using this task, we are able to quantify the effect of popularity bias on each approach by making use of a subset of more popular (short-head) songs and a set of less popular (long-tail) songs. Lastly, we propose a simple hybrid context-content system that combines our individual approaches and produces superior retrieval results. 1 INTRODUCTION Tags are text-based tokens, such as \u201chappy\u201d, \u201cclassic rock\u201d, and \u201cdistorted electric guitar\u201d, that can be used to annotate songs. They represent a rich source of semantic information that is useful for text-based music retrieval (e.g., [19]), as well as recommendation, discovery, and visualization [11]. Tags can be collected from humans using surveys [19, 5], social tagging websites [13], or music annotation games [20, 14, 12]. They can also be generated by text mining web- documents [10, 24] or by autotagging audio content [19, 7, 21]. In Section 2, we introduce key concepts associated with tag collection, and use them to highlight the strengths and weaknesses of each of these \ufb01ve approaches. In Section 3, we describe one implementation of a system for each approach and evaluate its performance on a tag-based music retrieval task. In the \ufb01nal section, we describe a simple hybrid system that combines the output from each of our individual systems. 2 COLLECTING TAGS In this section, we describe \ufb01ve approaches to collecting music tags. Three approaches (surveys, social tags, games) rely on human participation, and as such, are expensive in terms of \ufb01nancial cost and human labor. Two approaches (text mining, autotagging) rely on automatic methods that are computationally intense, but require less direct human involvement. There are a number of key concepts to consider when com- paring these approaches. The cold start problem refers to the fact songs that are not annotated cannot be retrieved. This problem is related to popularity bias in that popular songs (in the short-head) tend to be annotated more thoroughly than unpopular songs (in the long-tail) [11]. This often leads to a situation in which a short-head song is ranked above a long-tail song despite the fact that the long-tail song may be more semantically relevant. We prefer an approach that avoids the cold start problem (e.g., autotagging). If this is not possible, we prefer approaches in which we can explic- itly control which songs are annotated (e.g., survey, games), rather than an approach in which only the more popular songs are annotated (e.g., social tags, web documents). A strong labeling [3] is when a song has been explicitly labeled or not labeled with a tag, depending on whether or not the tag is relevant. This is opposed to a weak labeling in which the absence of a tag from a song does not necessarily indicate that the tag is not relevant. For example, a song may feature drums but is not explicitly labeled with the tag \u201cdrum\u201d. Weak labeling is a problem if we want to design a MIR system with high recall, or if our goal is to collect a training data set for a supervised autotagging system that uses discriminative classi\ufb01ers (e.g., [7, 24]). It is also important to consider the size, structure, and extensibility of the tag vocabulary. In the context of text- based music retrieval, the ideal vocabulary is a large and diverse set of semantic tags, where each tag describes some meaningful attribute or characterization of music. In this paper, we limit our focus to tags that can be used consistently by a large number of individuals when annotating novel songs based on the audio content alone. This does not include tags that are personal (e.g., \u201cseen live\u201d), judgmental (e.g., \u201chorrible\u201d), or represent external knowledge about the song (e.g., geographic origins of an artist). It should be noted that these tags are also useful for retrieval (and recommendation) and merit additional attention from the MIR community. A tag vocabulary can be \ufb01xed or extensible, as well as structured or unstructured. For example, the tag vocabu- lary associated with a survey can be considered \ufb01xed and structured since the set of tags and the grouping of tags into coherent semantic categories (e.g., genres, instruments, emotions, usages) is predetermined by experts using domain knowledge [19, 20]. By contrast, social tagging communities produce a vocabulary that is extensible since any user can 225 ISMIR 2008 \u2013 Session 2c \u2013 Knowledge Representation, Tags, Metadata Approach Strengths Weaknesses custom-tailored vocabulary small, predetermined vocabulary Survey high-quality annotations human-labor intensive strong labeling time consuming approach lacks scalability collective wisdom of crowds create & maintain popular social website Social Tags unlimited vocabulary ad-hoc annotation behavior, weak labeling provides social context sparse/missing in long-tail collective wisdom of crowds \u201cgaming\u201d the system Game entertaining incentives produce high-quality annotations dif\ufb01cult to create viral gaming experience fast paced for rapid data collection listening to short-clips, rather than entire songs Web Documents large, publicly-available corpus of relevant documents noisy annotations due to text-mining no direct human involvement sparse/missing in long-tail provides social context weak labeling not affected by cold-start problem computationally intensive Autotags no direct human involvement limited by training data strong labeling based solely on audio content Table 1. Strengths and weaknesses of tag-based music annotation approaches suggest any free-text token to describe music. This vocabu- lary is also unstructured since tags are not organized in any way. In general, we prefer an extensible vocabulary because a \ufb01xed vocabulary limits text-based retrieval to a small set of predetermined tags. In addition, a structured vocabulary is advantageous since the ontological relationships (e.g., genre hierarchies, families of instruments) between tags encode valuable semantic information that is useful for retrieval. Finally, the accuracy with which tags are applied to songs is perhaps the most important point of comparison. Since there is no ideal ground truth and listeners do not always agree whether (or to what degree) a tag should be applied to a song (i.e., \u2018the subjectivity problem\u2019 [15]), evaluating accu- racy can be tricky. Intuitively, it is preferable to have trained musicologists, rather than untrained non-experts, annotate a music corpus. It is also advantageous to have multiple individuals, rather then a single person, annotate each song. Lastly, individuals who are given incentives to provide good annotations (e.g., a high score in a game) may provide better annotations than unmotivated individuals.",
        "zenodo_id": 1416804,
        "dblp_key": "conf/ismir/TurnbullBL08"
    },
    {
        "title": "A Tunneling-Vantage Indexing Method for Non-Metrics.",
        "author": [
            "Rainer Typke",
            "Agatha Walczak-Typke"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417307",
        "url": "https://doi.org/10.5281/zenodo.1417307",
        "ee": "https://zenodo.org/records/1417307/files/TypkeW08.pdf",
        "abstract": "We consider an instance of the Earth Mover\u2019s Distance (EMD) useful for comparing rhythmical patterns. To make searches for r-near neighbours ef\ufb01cient, we decompose our search space into disjoint metric subspaces, in each of which the EMD reduces to the l1 norm. We then use a combined approach of two methods, one for searching within the sub- spaces, the other for searching between them. For the for- mer, we show how one can use vantage indexing without false positives nor false negatives for solving the exact r- near neighbour problem, and \ufb01nd an optimum number and placement of vantage objects for this result. For searching between subspaces, where the EMD is not a metric, we show how one can guarantee that still no false negatives occur, and the percentage of false positives is reduced as the search ra- dius is increased. 1 INTRODUCTION Searching a database for a melody or rhythm is often equiv- alent to a special version of the nearest neighbour problem: one wants to \ufb01nd items which are similar, but not necessar- ily identical to a given query, and be sure to retrieve all such items up to a certain level of dissimilarity. With a distance measure which adequately captures similarity, this amounts to retrieving all items which lie inside a ball around the query, where the radius of the ball is the dissimilarity thresh- old up to which retrieval results are considered relevant. We will call this search radius r, and items whose distance from a query is at most r will be called r-near neighbours. If the database is large, one needs a data structure which makes it possible to retrieve the r-near neighbours without comparing the query to each point in the database. For high numbers of dimensions, where exact methods such as kd- trees do not offer much improvement over a linear search, a number of approximate methods have been suggested. An- doni and Indyk [2] point to many approximate methods and describe one of the most popular among them, locality sen- sitive hashing (LSH), in detail. With LSH, one applies a hash function to every data point. The hash function must be chosen so that there is a high probability for points which are close to each other to be hashed into the same bin. To search for nearest neighbours, one then applies the hash function 1R. Typke gratefully acknowledges support by the Austrian Research Fund (FWF), Project Number M1027-N15. to the query and searches the bin into which the query was hashed. However, LSH relies on the distance measure being a metric. We describe a method which combines vantage indexing with tunneling, an approach applicable to certain non-metric distance measures. Our speci\ufb01c distance measure is non- metric; however, we can decompose our search space into disjoint metric subspaces. Within the metric subspaces, the non-metric reduces to the l1 norm. We use a new variant of the vantage indexing method which can be used for solving the exact r-near neighbour problem (i. e., without resort- ing to approximation) for the l1 norm in moderately high dimensional spaces (no more than about 9 dimensions). It guarantees to retrieve exactly the items within the ball of in- terest, without any false positives or false negatives, with a time complexity of essentially O(log n) (n=number of data points), but at the cost of using O(n2j\u22121) storage space (j=number of dimensions). It is possible to use less stor- age space, but at a cost: false positives can occur. For certain locality sensitive hash functions, LSH is sim- ilar to vantage indexing. Andoni and Indyk propose hash functions for the l1 and l2 norms which involve calculat- ing the distances to an arbitrary number of randomly chosen vantage objects and then putting all data points into the same bin whose distances to the vantage objects lie in a certain range. We, on the other hand, use exactly as many vantage objects as needed, placed in the space so that we are guar- anteed to retrieve all data points we should retrieve, and no others. Also, we avoid the need to pick a bin size which might not \ufb01t very well with the actual search radius. While our method for vantage indexing is speci\ufb01c to in- stances where the EMD reduces to metric subspaces with regular ball shapes, the second part of our combined method is more generally applicable to prametric spaces that can be decomposed into metric subspaces. By \u201ctunneling\u201d, we can ef\ufb01ciently search between metric subspaces, still without in- troducing false negatives. False positives can occur, but their percentage among the retrieved items shrinks as the search radius grows, making them tolerable. 2 MEASURING MELODIC OR RHYTHMIC SIMILARITY WITH THE EMD",
        "zenodo_id": 1417307,
        "dblp_key": "conf/ismir/TypkeW08"
    },
    {
        "title": "A Manual Annotation Method for Melodic Similarity and the Study of Melody Feature Sets.",
        "author": [
            "Anja Volk",
            "Peter van Kranenburg",
            "J\u00f6rg Garbers",
            "Frans Wiering",
            "Remco C. Veltkamp",
            "Louis P. Grijp"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416428",
        "url": "https://doi.org/10.5281/zenodo.1416428",
        "ee": "https://zenodo.org/records/1416428/files/VolkKGWVG08.pdf",
        "abstract": "This paper describes both a newly developed method for manual annotation for aspects of melodic similarity and its use for evaluating melody features concerning their contri- bution to perceived similarity. The second issue is also ad- dressed with a computational evaluation method. These ap- proaches are applied to a corpus of folk song melodies. We show that classi\ufb01cation of melodies could not be based on single features and that the feature sets from the literature are not suf\ufb01cient to classify melodies into groups of related melodies. The manual annotations enable us to evaluate var- ious models for melodic similarity. 1 INTRODUCTION The long term goal of the WITCHCRAFT-project is to create computational methods that support folk song re- search. 1 This paper takes an essential step towards this goal by investigating the similarity of songs that have been clas- si\ufb01ed by humans into groups of similar melodies. For the computational modeling of melodic similarity numerous features of melody could be taken into account. However, for a speci\ufb01c problem such as classi\ufb01cation only a few features might be suf\ufb01cient. Hence, we need a means to evaluate which features are important. Once a similar- ity measure is designed that uses a single feature or a few features, we also need a means to evaluate that similarity measure. Therefore, we have developed a manual annotation method that gathers expert judgments about the contribution of different musical dimensions to perceived similarity. We use this method to characterize the similarity of selected folk songs from our corpus. The human perception of melodic similarity is a challenging topic in cognition research (see e.g. [1] and [4]). The establishing of the annotation data in this paper is a \ufb01rst step to study the similarity as perceived by humans in the special case of similarity between melo- dies belonging to the same melody group. We evaluate in how far available computational features contribute to the characterization of similarity between these songs. 1 http://www.cs.uu.nl/research/projects/witchcraft Contribution: With these two methods we address the following questions:",
        "zenodo_id": 1416428,
        "dblp_key": "conf/ismir/VolkKGWVG08"
    },
    {
        "title": "Music Information Retrieval in ChucK; Real-Time Prototyping for MIR Systems &amp; Performance.",
        "author": [
            "Ge Wang 0002",
            "Rebecca Fiebrink",
            "Perry R. Cook"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.3727057",
        "url": "https://doi.org/10.5281/zenodo.3727057",
        "ee": null,
        "abstract": "This thesis work conduits research toward the estimation of relevance judgments for the task of Audio Music Similarity in the context of MIREX. It is intended to improve and support the evaluation experiments run for this task from the point of view of efficiency, studying different regression models and methods with the aim of reducing the cost of the annotation process. Therefore, by doing better estimations of relevance judgments and using all the tools at hand (research, literature, technology) the time used by people performing this task can be utilized in others activities.",
        "zenodo_id": 3727057,
        "dblp_key": "conf/ismir/WangFC08"
    },
    {
        "title": "The Liner Notes Digitization Project: Providing Users with Cultural, Historical, and Critical Music Information.",
        "author": [
            "Megan A. Winget"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417933",
        "url": "https://doi.org/10.5281/zenodo.1417933",
        "ee": "https://zenodo.org/records/1417933/files/Winget08.pdf",
        "abstract": "Digitizing cultural information is a complex endeavor. Not only do users expect to have access to primary information like digital music files; it is also becoming more important for digital systems to provide contextual information for the primary artifacts contained within. The Liner Notes Markup Language (LNML) was developed to provide an XML vocabulary for encoding complex contextual documents that include an album\u2019s packaging, informational notes and inserts, liners, and album labels. This paper describes the development of the LNML framework, its major structural elements and functions, and some of the more pressing problems related to usability and purpose. The current LNML model is based on the examination and encoding of fifty albums from the 80s Rock genre. We are currently encoding fifty additional Jazz albums, which will provide data to augment and strengthen the model. Development of the LNML is ongoing, with plans to examine Classical and World Music examples to further augment the model.",
        "zenodo_id": 1417933,
        "dblp_key": "conf/ismir/Winget08"
    },
    {
        "title": "Resolving Overlapping Harmonics for Monaural Musical Sound Separation using Fundamental Frequency and Common Amplitude Modulation.",
        "author": [
            "John Woodruff",
            "Yipeng Li",
            "DeLiang Wang"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1417279",
        "url": "https://doi.org/10.5281/zenodo.1417279",
        "ee": "https://zenodo.org/records/1417279/files/WoodruffLW08.pdf",
        "abstract": "In mixtures of pitched sounds, the problem of overlapping harmonics poses a signi\ufb01cant challenge to monaural musi- cal sound separation systems. In this paper we present a new algorithm for sinusoidal parameter estimation of over- lapping harmonics for pitched instruments. Our algorithm is based on the assumptions that harmonics of the same source have correlated amplitude envelopes and the phase change of harmonics can be accurately predicted from an instru- ment\u2019s pitch. We exploit these two assumptions in a least- squares estimation framework to resolve overlapping har- monics. This new algorithm is incorporated into a separa- tion system and quantitative evaluation shows that the re- sulting system performs signi\ufb01cantly better than an existing monaural music separation system for mixtures of harmonic instruments. 1 INTRODUCTION Musical sound separation attempts to isolate the sound of individual instruments in a polyphonic mixture. In recent years this problem has attracted signi\ufb01cant attention as the demand for automatic analysis, organization, and retrieval of a vast amount of online music data has exploded. A so- lution to this problem allows more ef\ufb01cient audio coding, more accurate content-based analysis, and more sophisti- cated manipulation of musical signals [1]. In this paper, we address the problem of monaural musical sound separation, where multiple harmonic instruments are recorded by a sin- gle microphone or mixed to a single channel. A well known dif\ufb01culty in music separation arises when the harmonics of two or more pitched instruments have fre- quencies that are the same or similar. Since Western music favors the twelve-tone equal temperament scale [2], com- mon musical intervals have pitch relationships very close to simple integer ratios (\u22483/2, 4/3, 5/3, 5/4, etc.). As a conse- quence, a large number of harmonics of a given source may be overlapped with another source in a mixture. When harmonics overlap, the amplitude and phase of in- dividual harmonics become unobservable. To recover an overlapped harmonic, it has been assumed that the ampli- tudes of instrument harmonics decay smoothly as a function of frequency [3]. Based on this assumption, the amplitude of an overlapped harmonic can be estimated from the ampli- tudes of neighboring non-overlapped harmonics of the same source. For example, Virtanen and Klapuri [4] estimated an overlapped harmonic through non-linear interpolation of neighboring harmonics. Every and Szymanski [5] used lin- ear interpolation instead. Recently, Virtanen [1] proposed a system which directly imposes spectral smoothness by modeling the amplitudes of harmonics as a weighted sum of \ufb01xed basis functions having smooth spectral envelopes. However, for real instrument sounds, the spectral smooth- ness assumption is often violated (see Figure 1). Another method of dealing with overlapping harmonics is to use in- strument models that contain the relative amplitudes of har- monics [6]. However, models of this nature have limited success due to the spectral diversity in recordings of differ- ent notes, different playing styles, and even different builds of the same instrument type. Although in general, the absolute value of a harmonic\u2019s amplitude with respect to its neighboring harmonics is dif\ufb01- cult to model, the amplitude envelopes of different harmon- ics of the same source exhibit similar temporal dynamics. This is known as common amplitude modulation (CAM) and it is an important organizational cue in human audi- tory perception [7] and has been used in computational au- ditory scene analysis [8]. Although CAM has been utilized for stereo music separation [9, 10], to our knowledge, this cue has not been applied in existing monaural systems. In this paper we demonstrate how CAM can be used to resolve overlapping harmonics in monaural music separation. Many existing monaural music separation systems oper- ate only in the amplitude/magnitude domain [5, 6, 11, 12]. However, the relative phase of overlapping harmonics plays a critical role in the observed amplitude of the mixture and must be considered in order to accurately recover the ampli- tudes of individual harmonics. We will show that the phase change of each harmonic can be accurately predicted from the signal\u2019s pitch. When this and the CAM observation are combined within a sinusoidal signal model, both the ampli- tude and phase parameters of overlapping harmonics can be accurately estimated. 538 ISMIR 2008 \u2013 Session 4c \u2013 Automatic Music Analysis and Transcription This paper is organized as follows. Section 2 presents the sinusoidal model for mixtures of harmonic instruments. In Section 3 we justify the CAM and phase change prediction assumptions and propose an algorithm where these assump- tions are used in a least-squares estimation framework for resolving overlapping harmonics. In Section 4 we present a monaural music separation system which incorporates the proposed algorithm. Section 5 shows quantitative evalua- tion results of our separation system and Section 6 provides a \ufb01nal discussion. 2 SINUSOIDAL MODELING Modeling a harmonic sound source as the summation of in- dividual sinusoidal components is a well established tech- nique in musical instrument synthesis and audio signal pro- cessing [13, 14]. Within an analysis frame m where sinu- soids are assumed constant, the sinusoidal model of a mix- ture consisting of harmonic sounds can be written as xm(t) = \u0002 n Hn \u0002 hn=1 ahn n (m)cos(2\u03c0f hn n (m)t + \u03c6hn n (m)), (1) where ahn n (m), f hn n (m), and \u03c6hn n (m) are the amplitude, frequency, and phase of sinusoidal component hn, respec- tively, of source n at time frame m. Hn denotes the number of harmonics in source n. The sinusoidal model of xm(t) can be transformed to the spectral domain by using the dis- crete Fourier transform (DFT). With an appropriately cho- sen time-domain analysis window (in terms of frequency resolution and sidelobe suppression), and assuming perfect harmonicity, the spectral value of xm(t) at frequency bin k can be written as X(m, k) = \u0002 n Shn n (m)W(kfb \u2212hnFn(m)). (2) Here, W is the complex-valued DFT of the analysis win- dow, fb is the frequency resolution of the DFT, and Fn(m) denotes the pitch of source n at time frame m. We call Shn n (m) the sinusoidal parameter of harmonic hn of source n, where Shn n (m) = ahn n (m) 2 ei\u03c6hn n (m). As a proof of con- cept, we further assume that pitches of individual sources are known. 3 RESOLVING OVERLAPPING HARMONICS Given ground truth pitches of each source, one can identify non-overlapped and overlapped harmonics. When a har- monic of a source is not overlapped, the estimation of the sinusoidal parameter, Shn n (m), from observed spectral val- ues, X(m, k), in corresponding frequency bins is straight- forward (see Section 4). However, when harmonics from different sources overlap, \ufb01nding Shn n (m) for each active harmonic is an ill-de\ufb01ned problem. To address this, we make use of non-overlapped harmonics of the same source 1 3 5 7 9 11 13 15 17 19 1 6 11 16 21 26 0 1 2 3 4 5 6 Time Frame Harmonic Number Log\u2212Amplitude Figure 1. Logarithm of the amplitude envelopes for the \ufb01rst 20 harmonics of a clarinet playing a G#. 0 \u22123 \u22126 \u22129 \u221212 \u221215 \u221218 \u221221 \u221224 \u221227 \u221230 \u22120.4 \u22120.2 0",
        "zenodo_id": 1417279,
        "dblp_key": "conf/ismir/WoodruffLW08"
    },
    {
        "title": "Analyzing Afro-Cuban Rhythms using Rotation-Aware Clave Template Matching with Dynamic Programming.",
        "author": [
            "Matthew Wright 0002",
            "W. Andrew Schloss",
            "George Tzanetakis"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1416356",
        "url": "https://doi.org/10.5281/zenodo.1416356",
        "ee": "https://zenodo.org/records/1416356/files/WrightST08.pdf",
        "abstract": "The majority of existing research in Music Information Re- trieval (MIR) has focused on either popular or classical mu- sic and frequently makes assumptions that do not generalize to other music cultures. We use the term Computational Eth- nomusicology (CE) to describe the use of computer tools to assist the analysis and understanding of musics from around the world. Although existing MIR techniques can serve as a good starting point for CE, the design of effective tools can bene\ufb01t from incorporating domain-speci\ufb01c knowledge about the musical style and culture of interest. In this pa- per we describe our realization of this approach in the con- text of studying Afro-Cuban rhythm. More speci\ufb01cally we show how computer analysis can help us characterize and appreciate the complexities of tracking tempo and analyz- ing micro-timing in these particular music styles. A novel template-based method for tempo tracking in rhythmically complex Afro-Cuban music is proposed. Although our ap- proach is domain-speci\ufb01c, we believe that the concepts and ideas used could also be used for studying other music cul- tures after some adaptation. 1 INTRODUCTION We present a set of techniques and tools designed for study- ing rhythm and timing in recordings of Afro-Cuban music with particular emphasis on \u201cclave,\u201d a rhythmic pattern used for temporal organization. In order to visualize timing in- formation we propose a novel graphical representation that can be generated by computer from signal analysis of audio recordings and from listeners\u2019 annotations collected in real time. The proposed visualization is based on the idea of Bar Wrapping, which is the breaking and stacking of a linear time axis at a \ufb01xed metric location. The techniques proposed in this paper have their origins in Music Information Retrieval (MIR) but have been adapted and extended in order to analyze the particular music cul- ture studied. Unlike much of existing work in MIR in which the target user is an \u201caverage\u201d music listener, the focus of this work is people who are \u201cexperts\u201d in a particular music culture. Examples of the type of questions they would like to explore include: how do expert players differ from each other, and also from competent musicians who are not fa- miliar with the particular style; are there consistent timing deviations for notes at different metric positions; how does tempo change over the course of a recording etc. Such ques- tions have been frequently out of reach because it is tedious or impossible to explore without computer assistance. Creating automatic tools for analyzing micro-timing and tempo variations for Afro-Cuban music has been challeng- ing. Existing beat-tracking tools either don\u2019t provide the re- quired functionality (for example only perform tempo track- ing but don\u2019t provide beat locations) or are simply not able to handle the rhythmic complexity of Afro-Cuban music because they make assumptions that are not always appli- cable, such as expecting more and louder notes on met- rically \u201cstrong\u201d beats. Finally the required precision for temporal analysis is much higher than typical MIR appli- cations. These considerations have motivated the design of a beat tracker that utilizes domain-speci\ufb01c knowledge about Cuban rhythms. The proposed techniques fall under the general rubric of what has been termed Computational Ethnomusicology (CE), which refers to the design and usage of computer tools that can assist ethnomusicological research [14] . Futrelle and Downie argued for MIR research to expand to other domains beyond Western pop and classical music [9]. Re- trieval based on rhythmic information has been explored in the context of Greek and African traditional music [1]. Our focus here is the analysis of music in which percus- sion plays an important role, speci\ufb01cally, Afro-Cuban mu- sic. Schloss [13] and Bilmes [4] each studied timing nu- ances in Afro-Cuban music with computers. Beat tracking and tempo induction are active topics of research, although they have mostly focused on popular music styles [11]. Our work follows Collins\u2019 suggestion [5] to build beat trackers that embody knowledge of speci\ufb01c musical styles. The clave is a small collection of rhythms embedded in virtually all Cuban music. Clave is a repeated syncopated rhythmic pattern that is often explicitly played, but often only implied; it is the essence of periodicity in Cuban music. An instrument also named \u201cclave\u201d (a pair of short sticks hit together) usually plays this repeating pattern. Clave is found mainly in two forms: rumba clave and son clave. (One way of notating clave is shown in Figure 1.) 647 ISMIR 2008 \u2013 Session 5c \u2013 Rhythm and Meter Figure 1. Son (left) and rumba (right) clave Our study of timing requires knowing the exact time of every note played by the clave. We can then decompose this data into an estimate of how tempo changes over time (what is called the tempo curve) and a measure of each individual note\u2019s deviation from the \u201cideal\u201d time predicted by a metro- nomic rendition of the patterns shown in Figure 1. Unfortunately, we do not know of any databases of Afro- Cuban music with an exact ground-truth time marked for every clave note or even for every downbeat. 1 Therefore we constructed a small four-song database 2 and gathered ground truth clave timing data by having an expert percus- sionist with Afro-Cuban experience tap along with the clave part. Custom sample-accurate tap detection/logging soft- ware automatically timestamps the taps. Recordings of Afro-Cuban music challenge existing state- of-the-art beat-tracking algorithms because of the complex and dense rhythm and the lack of regular approximately isochronous pulses. Figure 2 shows how two recent state- of-the-art beat-tracking systems (BeatRoot [7] and a beat tracker using dynamic programming proposed by Ellis [8]) do not generate an accurate tempo curve for the recording CB. The plots in the \ufb01gure are shown only in order to moti- vate the proposed approach. The comparison is not fair, as the other algorithms are more generally applicable and de- signed with different assumptions, but in any case it demon- strates the advantage of a domain-speci\ufb01c method to deal with these recordings: our method is speci\ufb01cally designed to take into account clave as the rhythmic backbone. 2 DATA PREPARATION It is common for Afro-Cuban songs to begin with just the sound of the clave for one or two repetitions to establish the initial tempo. However as other instruments (both percus- sive and pitched) and voices enter the mix the sound of the clave tends to become masked. The \ufb01rst step of data prepa- ration is to enhance the sound of the clave throughout the song using a matched \ufb01lter approach. In addition onset de- tection is performed. 1 Bilmes recorded about 23 minutes of Afro-Cuban percussion at MIT in 1992, and performed sophisticated analysis of the timing of the guagua and conga (but not clave) instruments [4]; unfortunately these analog record- ings are not currently available to the research community. 2 Here is the name, artist, and source recording for each song, along with the two-character ID used later in the paper: LP: La Polemica, Los Mu\u02dcnequitos de Matanzas, Rumba Caliente 88. CB: Cantar Bueno, Yoruba Andabo, El Callejon De Los Rumberos. CH: Chacho, Los Mu\u02dcnequitos de Matanzas, Cuba: I Am Time (Vol. 1). PD: Popurrit de Sones Orientales, Conjunto de Sones Orientales, Son de Cuba. Figure 2. Four estimates of the tempo curve for our record- ing CB: Ground truth calculated from a human expert\u2019s tap times (upper left), curve from our method (top right), curve from BeatRoot (lower left), and curve from Ellis\u2019 dynamic programming approach (lower right).",
        "zenodo_id": 1416356,
        "dblp_key": "conf/ismir/WrightST08"
    },
    {
        "title": "Using Statistic Model to Capture the Association between Timbre and Perceived Tempo.",
        "author": [
            "Linxing Xiao",
            "Aibo Tian",
            "Wen Li",
            "Jie Zhou 0001"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1414922",
        "url": "https://doi.org/10.5281/zenodo.1414922",
        "ee": "https://zenodo.org/records/1414922/files/XiaoTLZ08.pdf",
        "abstract": "The estimation of the perceived tempo is required in many MIR applications. However, automatic tempo estimation itself is still an open problem due to the insuf\ufb01cient un- derstanding of the inherent mechanisms of the tempo per- ception. Published methods only use the information of rhythm pattern, so they may meet the half/double tempo error problem. To solve this problem, We propose to use statistic model to investigate the association between tim- bre and tempo and use timbre information to improve the performance of tempo estimation. Experiment results show that this approach performs at least comparably to existing tempo extraction algorithms. 1 INTRODUCTION The perceived tempo is an apparent and descriptive feature of songs, and it gives the listener a direct impression of the music emotion. According to this feature, one can search for songs with his expectant speed quickly. Therefore, tempo is a useful property of songs. The automatic extraction of tempo information directly from digital audio signals is still an open problem, because the association between tempo and other music attributes such as rhythm pattern, melody and timbre, is not yet very well understood. Thus it attracts a lot of researchers, and many perceived tempo identi\ufb01cation algorithms are publish- ed. State-of-the-art methods regard tempo identi\ufb01cation as an optimization problem. They search the best tempo esti- mation of a song under the constraints of the rhythm pat- terns, like periodicity and strength of onsets. For example, [3] uses dynamic programming to \ufb01nd the set of beat times that optimize both the onset strength at each beat and the spacing between beats (tempo). The formulation of tempo estimation in terms of a constraint optimization problem re- quires a clear understanding of the inherent mechanisms of tempo perception. However, people barely know what fac- tors and how these factors affect the perceived tempo. And that is why sometimes, the most salient tempo estimation is not the perceived tempo but half or double of it. Fortunately in spite of the insuf\ufb01cient understanding of perceived tempo, it is still possible to solve the tempo esti- mation problem by using statistic models, whose effective- ness has been proved in the \ufb01elds of speech recognition and natural language processing. In [2], Seyerlehner et al. refor- mulates the tempo estimation in terms of a nearest neighbor classi\ufb01cation problem. For a testing song, [2] compares its rhythm pattern with those in the training set. And the tempo of the most similar song in the training set is assigned to the testing song. The promising reported results show that, the statistic model, which is the nearest neighbor algorithm in [2], is able to capture the complex association between rhythm pattern and perceived tempo. And more importantly, statistic models facilitate the investigation of the factors that in\ufb02uence the perception of tempo, without knowing how they work. In fact, except for the rhythm pattern, timbre is one of the factors that can affect the human perception of tempo. It can be noted that, songs with high perceived tempo are usually quite noisy while those with low perceived tempo are usu- ally quiet. One explanation is that noisy timbre can intensify the tension of fast songs while smooth timbre can make slow songs more peaceful. In this paper, we try to mine the as- sociation between timbre and perceived tempo by using a parameterized statistic model. And a novel tempo estima- tion method is proposed by taking advantage of the learned statistic model. Our approach \ufb01rst uses a state-of-the-art method to generate several tempo candidates. Then, the likelihood of each candidate is computed based on the tim- bre feature of the song. Finally, the candidate with the high- est likelihood is regarded as the tempo of the song. Exper- iments show that signi\ufb01cant improvement is achieved com- paring with the state-of-the-art algorithm we use. The remainder of this paper is organized as follows: the motivation of our research is presented in Section 2. De- tails of our algorithm is introduced in Section 3. Section 4 is the experiments and discussion of the results. Section 5 concludes the paper. 2 MOTIVATION The perceived tempo of certain pieces might vary depending on different human subjects [5]. Although this ambiguity makes it dif\ufb01cult to estimate the perceived tempo, many ef- forts have been made on the automatical tempo estimation. 659 ISMIR 2008 \u2013 Session 5c \u2013 Rhythm and Meter Hz Bark scale frequency banks 0 5 10 5 10 15 20 Hz Bark scale frequency banks 0 5 10 5 10 15 20 Figure 1. Illustration of FPs of two songs. The upper pane is the FP of a song with tempo value 208 bpm (3.4 Hz); the lower pane is the FP of a song with tempo value 104 bpm (1.7 Hz). A common approach of state-of-the-art methods [1] is to analyze the extracted rhythm patterns under the assumption that one of the periodicities present in the rhythm patterns corresponds to the perceived tempo. [1] evaluates eleven algorithms by using two evaluation metrics as follows: \u2022 Accuracy 1: the percentage of tempo estimates within 4 % (the precision window) of the ground-truth tempo. \u2022 Accuracy 2: the percentage of tempo estimates within 4 % of either the ground-truth tempo, or half, double, three times or one third of the ground-truth tempo. In the evaluation of [1], for all eleven algorithms, accu- racies 2 are much higher than accuracies 1. The average increment is about 30%. This result shows that state-of-the- art methods mainly suffer from the half/double and one \u2212 third/triple tempo error problems (We will use \u201dhalf/do\u2212 uble\u201d for both kind of errors). One possible reason is that sometimes it is quite hard to pick the \u201dmost salient tempo\u201d from the rhythm pattern, as shown in the upper pane of Fig- ure 1, which illustrates the Fluctuation Pattern (FP)[4] of a song. The left light bar corresponds to a possible tempo of 104 bpm (1.7 Hz) and right light bar corresponds to a pos- sible tempo of 208 bpm (3.4 Hz). It is hard to determine which tempo is more salient. Merely using rhythm pattern as the feature, the instance based method [2] may meet the half/double tempo error problem as well, because sometimes the rhythm pattern of a song is very similar to that of a song with double tempo, but different from a song with the same tempo. As shown in Figure 1, the upper pane illustrates the FP of a song whose tempo is 208 bpm and the lower one illustrates the FP of a song whose tempo is 104 bpm. Since the tempo is an important parameter that re\ufb02ects the mood of a song, an estimated tempo which is half or double of the ground truth might give the user a terrible ex- perience. For example, a sad user intends to search a slow song, but gets a bright one with double of the desired tempo. In order to solve the half/double tempo error problem, we decide to investigate possible factors that in\ufb02uence the per- ception of tempo except for rhythm patterns. And we hope these factors can be used to improve the accuracy of tempo estimation. Noticing the phenomenon that songs with high perceived tempo are usually quite noisy while those with low perceived tempo are usually quiet, we choose timbre as the factor into which we want to get deeper insights. 3 OUR METHOD Without knowing how the timbre affects the perception of tempo, we build a statistic model to capture the association between tempo and timbre. Then, the statistic model is com- bined with a state-of-the-art method to estimate the tempo of a piece of music.",
        "zenodo_id": 1414922,
        "dblp_key": "conf/ismir/XiaoTLZ08"
    },
    {
        "title": "Music Thumbnailer: Visualizing Musical Pieces in Thumbnail Images Based on Acoustic Features.",
        "author": [
            "Kazuyoshi Yoshii",
            "Masataka Goto"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1415936",
        "url": "https://doi.org/10.5281/zenodo.1415936",
        "ee": "https://zenodo.org/records/1415936/files/YoshiiG08.pdf",
        "abstract": "This paper presents a principled method called MusicThumb- nailer to transform musical pieces into visual thumbnail im- ages based on acoustic features extracted from their audio signals. These thumbnails can help users immediately guess the musical contents of audio signals without trial listening. This method is consistent in ways that optimize thumbnails according to the characteristics of a target music collection. This means the appropriateness of transformation should be de\ufb01ned to eliminate ad hoc transformation rules. In this pa- per, we introduce three top-down criteria to improve mem- orability of thumbnails (generate gradations), deliver infor- mation more completely, and distinguish thumbnails more clearly. These criteria are mathematically implemented as minimization of brightness differences of adjacent pixels and maximization of brightness variances within and be- tween thumbnails. The optimized parameters of a modi\ufb01ed linear mapping model we assumed are obtained by minimiz- ing a uni\ufb01ed cost function based on the three criteria with a steepest descent method. Experimental results indicate that generated thumbnails can provide users with useful hints as to the musical contents of musical pieces. 1 INTRODUCTION Music recommender systems are increasingly important in online music-distribution services to help users discover their favorite pieces among a huge music collection. For instance, recommender systems based on collaborative \ufb01ltering [1,2] recommend musical pieces to the user by taking into ac- count someone else\u2019s ratings of those pieces. Content-based \ufb01ltering systems [3,4] select musical pieces that are similar to the user\u2019s favorites in terms of musical content (acous- tic features). Recently, several hybrid systems that integrate these two techniques have been proposed to enable more ac- curate recommendations [5,6]. An important problem that has not been resolved is that users cannot immediately grasp the musical contents of rec- ommended pieces after these pieces are listed. Users have to listen to all listed pieces, including those they do not like, to \ufb01nd which pieces are worth listening to. This often pre- vents users from seamlessly listening to their favorite pieces. Worse still, trial listening is time-consuming because the in- formation of audio signals (temporal media) is not simul- taneously delivered to users whereas visual images (spatial W. Houston M. Carey J. Jackson Mozart Beethoven Haydn Thumbnails the user has ever seen before This piece would have pops features The generated thumbnail of an unknown piece Figure 1. Expected scenario: A user can roughly guess the musical contents of unknown pieces by seeing only thumb- nails and without time-consuming trial listening. media) can be easily skimmed through. To solve this problem, we propose an audio-visual trans- formation method called MusicThumbnailer that generates compact images corresponding to the audio signals of indi- vidual pieces. This helps users guess the musical contents of audio signals without trial listening. For example, this method will work well in recommender systems, as shown in Fig. 1. Initially, users only glance at compact thumbnails attached to recommended pieces when they actually listen to these pieces. While accumulating this experience, users will unconsciously associate particular types of thumbnail with their preferred music. Users thus learn to understand the musical meanings of the thumbnails\u2019 features. Finally, users will be able to ef\ufb01ciently select audio signals of their desired pieces by using their eyes rather than their ears. One advantage of our method is that the visual features (colors and patterns) of thumbnails are automatically opti- mized for a given collection. To achieve this, it is necessary to eliminate ad hoc rules that arbitrarily associate acoustic features with visual features, because these rules lack a prin- cipled justi\ufb01cation that is consistent for different collections. In this paper, we de\ufb01ne some top-down criteria on generated thumbnails from the viewpoint of usability, independently of the characteristics of music collections. We then mathe- matically represent these criteria in a uni\ufb01ed cost function. Audio-visual associations are obtained in a self-organized way so that the cost function is minimized. 211 ISMIR 2008 \u2013 Session 2b \u2013 Music Recognition and Visualization The rest of this paper is organized as follows. Section 2 introduces related work. Section 3 explains the principles of our audio-visual transformation method and its implemen- tation. Section 4 reports on our experiment using the RWC Music Database [7]. Section 5 summarizes the key \ufb01ndings of this paper. 2 RELATED WORK Many visualization methods have been proposed for spatial representation, organization, and browsing of music collec- tions [8\u201316]. These methods typically locate musical pieces in a two- or three-dimensional space so that musical pieces that have similar musical contents (acoustic features) are ar- ranged close to each other. This enables users to easily un- derstand relationships between musical pieces because sim- ilarity in acoustic features can be observed as spatial dis- tance. From a mathematical viewpoint, this kind of visu- alization can be interpreted as information compression of high-dimensional feature vectors according to some crite- ria. The self-organizing map (SOM) is often used for this purpose (e.g., Islands of Music [8]). In a music-playback interface called Musicream [17], in- dividual musical pieces are visualized as colored discs using an ad hoc rule. The disc color (hue and saturation) is deter- mined from the color circle whose circumference and radius correspond to hue and saturation, respectively. Each piece is mapped into the circle according to its feature vector. Princi- pal component analysis (PCA) is used to reduce the dimen- sionality of acoustic feature vectors to a two-dimensional vector on a plane. Musicovery [18] uses arbitrary rules for associating genres with colors speci\ufb01ed in advance. In the work described in this paper, we aimed to visualize individual pieces, rather than a collection, as compact im- ages (thumbnails) without using ad hoc rules. In general, vi- sual images are represented as super-high-dimensional vec- tors that contain the color values of numerous pixels. There- fore, our objective was to \ufb01nd an appropriate mapping from a low-dimensional acoustic space (several-tens dim.) to a high-dimensional visual space (several-thousands dim.). A unique feature of this problem lies in this drastic increase in degrees of freedom. To solve such an ill-posed problem by using an optimization method, it is necessary to incorporate some criteria on the appropriateness of the mapping. 3 MUSIC THUMBNAILER This section describes our method of generating thumbnails of musical pieces based on acoustic features.",
        "zenodo_id": 1415936,
        "dblp_key": "conf/ismir/YoshiiG08"
    },
    {
        "title": "Chord Recognition using Instrument Voicing Constraints.",
        "author": [
            "Xinglin Zhang",
            "David Gerhard"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.1414826",
        "url": "https://doi.org/10.5281/zenodo.1414826",
        "ee": "https://zenodo.org/records/1414826/files/ZhangG08.pdf",
        "abstract": "This paper presents a technique of disambiguation for chord recognition based on a-priori knowledge of probabilities of chord voicings in the speci\ufb01c musical medium. The main motivating example is guitar chord recognition, where the physical layout and structure of the instrument, along with human physical and temporal constraints, make certain chord voicings and chord sequences more likely than others. Pitch classes are \ufb01rst extracted using the Pitch Class Pro\ufb01le (PCP) technique, and chords are then recognized using Arti\ufb01cial Neural Networks. The chord information is then analyzed using an array of voicing vectors (VV) indicating likelihood for chord voicings based on constraints of the instrument. Chord sequence analysis is used to reinforce accuracy of in- dividual chord estimations. The speci\ufb01c notes of the chord are then inferred by combining the chord information and the best estimated voicing of the chord. 1 INTRODUCTION Automatic chord recognition has been receiving increasing attention in the musical information retrieval community, and many systems have been proposed to address this prob- lem, the majority of which combine signal processing at the low level and machine learning methods at the high level. The goal of a chord recognition system may also be low- level (identify the chord structure at a speci\ufb01c point in the music) or high level (given the chord progression, predict the next chord in a sequence).",
        "zenodo_id": 1414826,
        "dblp_key": "conf/ismir/ZhangG08"
    },
    {
        "title": "ISMIR 2008, 9th International Conference on Music Information Retrieval, Drexel University, Philadelphia, PA, USA, September 14-18, 2008",
        "author": [
            "Juan Pablo Bello",
            "Elaine Chew",
            "Douglas Turnbull"
        ],
        "year": "2008",
        "doi": "10.5281/zenodo.5651429",
        "url": "https://doi.org/10.5281/zenodo.5651429",
        "ee": null,
        "abstract": "Multi-modal dataset for music genre recognition based on six different modalities for the LMD-aligned [1] and SLAC [2] datasets. Further details are provided in [3].\n\nDescriptions of files\n\n\n\t\n\t\t\n\t\t\tLink\n\t\t\tDescription\n\t\t\n\t\n\t\n\t\t\n\t\t\tLMD-aligned_Filelist.arff\n\t\t\tFile list with 1575 music tracks selected from the LMD-aligned dataset [1] with tagtraum genre annotations [4] (only a subset of LMD-aligned is used, which includes only pieces for which all six modalities were accessible, and which includes only well-represented genres)\n\t\t\n\t\t\n\t\t\tLMD-aligned_ExtractedFeatures.tar.gz\n\t\t\tRaw audio signal and model-based features extracted with AMUSE [5]\n\t\t\n\t\t\n\t\t\tLMD-aligned_ProcessedFeatures.tar.gz\n\t\t\tProcessed features: audio signal and model-based features aggregated for 4 s time frames with 2 s step size / all other features (see the table below) with the same values for all time frames\n\t\t\n\t\t\n\t\t\tLMD-aligned_Datasets.tar.gz\n\t\t\tTraining, optimization, and test datasets for 3 splits for the recognition of 5 genres in [3]\n\t\t\n\t\t\n\t\t\tSLAC_Filelist.arff\n\t\t\tFile list with 250 music tracks from the SLAC dataset [2] (genres and sub-genres are provided in the folder structure)\n\t\t\n\t\t\n\t\t\tSLAC_ExtractedFeatures.tar.gz\n\t\t\tRaw audio signal and model-based features extracted with AMUSE [5]\n\t\t\n\t\t\n\t\t\tSLAC_ProcessedFeatures.tar.gz\n\t\t\tProcessed features: audio signal and model-based features aggregated for 4 s time frames with 2 s step size / all other features (see the table below) with the same values for all time frames\n\t\t\n\t\t\n\t\t\tSLAC_Datasets.tar.gz\n\t\t\tTraining, optimization, and test datasets for 3 splits for the recognition of 5 genres and 10 sub-genres in [3]\n\t\t\n\t\n\n\nModalities and feature sub-groups\n\n\n\t\n\t\t\n\t\t\tModality\n\t\t\tSub-group\n\t\t\t\n\t\t\tDimensions in processed\n\n\t\t\tfeatures of LMD-aligned\n\t\t\t\n\t\t\t\n\t\t\tDimensions in processed\n\n\t\t\tfeatures of SLAC\n\t\t\t\n\t\t\n\t\n\t\n\t\t\n\t\t\tAudio signal\n\t\t\tLow-level\n\t\t\t1-524\n\t\t\t1-524\n\t\t\n\t\t\n\t\t\tAudio signal\n\t\t\tSemantic\n\t\t\t525-810\n\t\t\t525-810\n\t\t\n\t\t\n\t\t\tAudio signal\n\t\t\tStructural complexity\n\t\t\t811-908\n\t\t\t811-908\n\t\t\n\t\t\n\t\t\tModel-based\n\t\t\tInstruments\n\t\t\t909-1018\n\t\t\t909-1018\n\t\t\n\t\t\n\t\t\tModel-based\n\t\t\tMoods\n\t\t\t1019-1146\n\t\t\t1019-1146\n\t\t\n\t\t\n\t\t\tModel-based\n\t\t\tVarious\n\t\t\t1147-1402\n\t\t\t1147-1402\n\t\t\n\t\t\n\t\t\tPlaylists\n\t\t\tGenres\n\t\t\t1403-1973\n\t\t\t1403-1973\n\t\t\n\t\t\n\t\t\tPlaylists\n\t\t\tStyles\n\t\t\t1974-1695\n\t\t\t1974-1695\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tPitch\n\t\t\t1696-1757\n\t\t\t1696-1757\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tMelodic\n\t\t\t1758-1781\n\t\t\t1758-1781\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tChords\n\t\t\t1782-1836\n\t\t\t1782-1836\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tRhythm\n\t\t\t1837-1935\n\t\t\t1837-1935\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tTempo\n\t\t\t1936-1963\n\t\t\t1936-1963\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tInstrument presence\n\t\t\t1964-2441\n\t\t\t1964-2441\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tInstruments\n\t\t\t2442-2456\n\t\t\t2442-2456\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tTexture\n\t\t\t2457-2480\n\t\t\t2457-2480\n\t\t\n\t\t\n\t\t\tSymbolic\n\t\t\tDynamics\n\t\t\t2481-2484\n\t\t\t2481-2484\n\t\t\n\t\t\n\t\t\tAlbum covers\n\t\t\tSIFT\n\t\t\t2485-2584\n\t\t\t2485-2584\n\t\t\n\t\t\n\t\t\tLyrics\n\t\t\tjLyrics descriptors\n\t\t\t2585-2603\n\t\t\t2585-2671\n\t\t\n\t\t\n\t\t\tLyrics\n\t\t\tBag-of-Words\n\t\t\t2604-2703\n\t\t\t\n\t\t\n\t\t\n\t\t\tLyrics\n\t\t\tDoc2Vec\n\t\t\t2704-2803\n\t\t\t\n\t\t\n\t\n",
        "zenodo_id": 5651429,
        "dblp_key": "conf/ismir/2008"
    }
]