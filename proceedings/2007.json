[
    {
        "title": "Influence of Tempo and Subjective Rating of Music in Step Frequency of Running.",
        "author": [
            "Teemu Tuomas Ahmaniemi"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418107",
        "url": "https://doi.org/10.5281/zenodo.1418107",
        "ee": "https://zenodo.org/records/1418107/files/Ahmaniemi07.pdf",
        "abstract": "The objective of this work was to study how the tempo and subjective motivational rating of personal music influence in step frequency during an exercise session. The participants (n=8) were requested to bring their own music to the test and rate it according to the motivational effect of each song. The test was conducted on a sports field where the participants were asked to perform a 30 minute exercise without paying attention to the test setup. Significant correlation was found between the subjective motivational rating of music and step frequency, while tempo did not have any influence.",
        "zenodo_id": 1418107,
        "dblp_key": "conf/ismir/Ahmaniemi07"
    },
    {
        "title": "Methodological Considerations in Studies of Musical Similarity.",
        "author": [
            "Hamish Allan",
            "Daniel Müllensiefen",
            "Geraint A. Wiggins"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416956",
        "url": "https://doi.org/10.5281/zenodo.1416956",
        "ee": "https://zenodo.org/records/1416956/files/AllanMW07.pdf",
        "abstract": "There are many different aspects of musical similarity [7]. Some relate to acoustic properties, such as melodic [5], rhythmic [10], harmonic [9] and timbral [2]. Others are bound up in cultural aspects: artists involved in creation, year of ﬁrst release, subject matter of lyrics, demographics of listeners, etc. In judgments about musical similarity, the relative importance of each of these aspects will change, not only for different listeners, but also for the same listener in different contexts [11]. Extra care must therefore be taken when designing studies in musical similarity to ensure that the context is an explicit variable. This paper describes the methodology behind our work in context-based musical similarity; introduces a novel system through which users can specify by example the context and focus of their retrieval needs; and details the design of a study to ﬁnd parameters for our system which can also be adapted to test the system as a whole. 1 INTRODUCTION Though musical similarity is multi-faceted, it is sometimes considered useful to distill it into a single measure; for example, to present a simple ordered list of results from a search-engine-style query. There are various measures for different aspects of similarity in the literature (for a full exploration of these, see [7]): melodic and timbral measures have generally received the most attention, but rhythmic and harmonic ones have also been considered, and metadata such as artist, lyrics, year of release, sales ﬁgures, chart position and label classiﬁcation may also be examined. A single measure might combine several of these, but the relative weighting of the components of such a combination has a great impact of the utility of the measure. For a start, the perceptual prominence of aspects may vary across listeners; in addition, the context of the query exercise has an impact on which aspects are perceived as most salient [11]. We present a novel technique for allowing users to create example sets of pieces of music which are used to determine a weighting for various existing similarity measures. This allows for users lacking a musical vocabulary c⃝2007 Austrian Computer Society (OCG). (or unwilling to constrict their query into one) to make queries like, “I think all these pieces of music are similar in a way that appeals to me. Find me music that is similar to them in the same way.” The technique also incorporates a negative example set and a feedback loop through which the user can train the aspect more accurately (see Figure 1). We believe this dynamic approach addresses the subjectivity and context issues whilst retaining a good balance between expressive power and ease of use. It differs fundamentally from almost all existing approaches in the genre classiﬁcation and similarity retrieval literature, in which parameters are ﬁxed after an initial training phase rather than being iteratively tailored to the needs of individual users. The technique only provides a relative weighting for the individual measures. Therefore, we also propose an experimental study to determine one or more initial weightings according to their perceptual salience for an average user (or several classes of user according to a clustering exercise). This is closely related to the task of weighting features to create a perceptually valid similarity measure [6]. However, when the separation of different aspects is considered, certain problematic assumptions are revealed which must be carefully examined. These assumptions, and a rigorous method which overcomes them, are the primary topic of this paper. 2 CONCEPTUAL OUTLINE OF ASPECT WEIGHTING To demonstrate the concept of aspect weighting, we present an example using four distance measures and a corpus of 20 pieces of music. (Our full study will actually use a database of over 14,000 MIDI-encoded western pop songs and over 40 distance measures.) The selection and normalisation of these measures will be covered in detail in the next section, once we have explained our motivation. Figure 2 shows two-dimensional projections of the example measures. For instance, these data might correspond to melodic (D1), rhythmic (D2), harmonic (D3) and timbral (D4) similarities. The projection itself is merely for demonstration: we assume only the pairwise distances between songs, rather than any of the underlying features. The shorter the distance, the greater the similarFigure 1. Choosing positive (left) and negative (right) examples by adding tracks to a playlist. ity, with zero distance between identical pieces. Most of the measures will be metric spaces, though this does not necessarily have to be the case. In our example, the user decides that tracks 2, 4 and 6 are all similar in her chosen aspect. A straightforward way for her to indicate this might be to drag them from a library of titles into a playlist marked “positive” (see Figure 1). Intuitively, this means that D1 and D3, in which those tracks are closer together than in D2 and D4 (see the solid thick lines in Figure 2) should be weighted more heavily. One way of achieving this is to divide the mean distance between all the tracks in Dk by the mean distance between only the tracks in Dk in the positive example set (i.e., dividing the mean of all the line lengths by the mean of only the three solid thick line lengths for each Dk in Figure 2). To avoid division by zero, a small constant should be added to both means. In this example, this gives an overall weighting for D(+{2, 4, 6}, −φ) of",
        "zenodo_id": 1416956,
        "dblp_key": "conf/ismir/AllanMW07"
    },
    {
        "title": "Virtual Communities for Creating Shared Music Channels.",
        "author": [
            "Amelie Anglade",
            "Marco Tiemann",
            "Fabio Vignoli"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414710",
        "url": "https://doi.org/10.5281/zenodo.1414710",
        "ee": "https://zenodo.org/records/1414710/files/AngladeTV07.pdf",
        "abstract": "We present an approach to automatically create virtual communities of users with similar music tastes. Our goal is to create personalized music channels for these communities in a distributed way, so that they can for example be used in peer-to-peer networks. To ﬁnd suitable techniques for creating these communities we analyze graphs created from real-world recommender datasets and identify speciﬁc properties of these datasets. Based on these properties we select and evaluate different graph-based community-extraction techniques. We select a technique that exploits identiﬁed properties to create clusters of music listeners. We validate the suitability of this technique using a music dataset and a large movie dataset. On a graph of 6,040 peers, the selected technique assigns at least 85% of the peers to optimal communities, and obtains a mean classiﬁcation error of less than 0.05 over the remaining peers that are not assigned to the best community. 1 INTRODUCTION Music recommender systems and algorithms continue to attract scientiﬁc and commercial interest. Generally, music recommender research focuses on either service-based recommender systems, that can be used over the Internet, or on autonomous recommender systems for non networkenabled devices such as portable music players. In this paper we are concerned with providing music recommendations and personalized music radio channels directly between connected systems and devices using peer-to-peer networks. The concept of peer-to-peer radio emerged recently. Such systems are already publicly available and prove its technical feasibility (e.g. [9]). Regarding personalized recommendation over peer-to-peer networks relatively little research has been conducted to date. One such research project on distributed recommendation is 1 Now with Queen Mary University of London, Centre for Digital Music, Mile End Road, London E1 4NS, UK; M.Sc. student at Chalmers University of Technology, Department of Applied Physics, G¨oteborg, Sweden during the study. c⃝2007 Austrian Computer Society (OCG). the TRIBLER project (cf. Wang et al. [12]). TRIBLER provides users with personalized recommendations for items shared in peer-to-peer networks using a distributed version of the widely-used collaborative ﬁltering approach (Resnick et al. provide an introduction to collaborative ﬁltering in [11]). We describe in this paper our approach to cluster peers into groups that share similar music preferences and potentially other criteria, and to provide music recommendations and shared music radio channels to these groups. Music shared by peers in such channels can then be transmitted directly between peers within the groups or broadcasted to the groups of peers depending on the underlying infrastructure. The paper is organized as follows: In Section 2, we discuss the speciﬁc problems that need to be addressed to realize distributed clustering of peers by music preferences. In Section 3, we perform an analysis of typical datasets for recommender systems, we identify properties of those datasets which can be exploited by using graphbased clustering techniques. In Section 4, we comparatively evaluate several clustering techniques and identify the most suitable according to its performance on optimal assignment of peers to communities. We close with ﬁnal remarks and pointers to future work in Section 5. 2 PROBLEM DEFINITION AND SOLUTION APPROACH The main challenge when automatically creating groups or communities of users in the desired setting is to identify clusters of similar peers in a peer-to-peer network so that every peer in the system can enjoy a selection of music that suits her music preferences. Creating communities of peers can be expressed as an optimization problem which is composed of several potentially complementary, but also potentially competitive objectives described in the following Section.",
        "zenodo_id": 1414710,
        "dblp_key": "conf/ismir/AngladeTV07"
    },
    {
        "title": "Music Retrieval by Rhythmic Similarity Applied on Greek and African Traditional Music.",
        "author": [
            "Iasonas Antonopoulos",
            "Aggelos Pikrakis",
            "Sergios Theodoridis",
            "Olmo Cornelis",
            "Dirk Moelants",
            "Marc Leman"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417503",
        "url": "https://doi.org/10.5281/zenodo.1417503",
        "ee": "https://zenodo.org/records/1417503/files/AntonopoulosPTCML07.pdf",
        "abstract": "This paper presents a method for retrieving music recordings by means of rhythmic similarity in the context of traditional Greek and African music. To this end, Self Similarity Analysis is applied either on the whole recording or on instances of a music thumbnail that can be extracted from the recording with an optional thumbnailing scheme. This type of analysis permits the extraction of a rhythmic signature per music recording. Similarity between signatures is measured with a standard Dynamic Time Warping technique. The proposed method was evaluated on corpora of Greek and African traditional music where human improvisation plays a key role and music recordings exhibit a variety of music meters, tempi and instrumentation. 1 INTRODUCTION In the context of Music Information Retrieval (MIR), ﬁnding music recordings with similar rhythmic characteristics is a highly desired task both for the untrained listener and the musicologist. Over the years, several methods have been proposed in the context of Western music for retrieving music with similar rhythmic characteristics, e.g. tempo, meter and rhythmic patterns. Here a short overview of relevant papers is given: The work in [1] measures the similarity between rhythmic patterns extracted from music recordings and artiﬁcially generated percussive sounds. The approach in [2] extracts temporal patterns from the energy envelop of the signal in an attempt to classify music recordings to predeﬁned classes. In [3] a set of classiﬁcation schemes are proposed that are based on extracting rhythmic patterns from the signal’s spectrum. The method proposed in [4] focuses on ballroom dances and is based on features stemming from the histogram of Inter-Onset Intervals. Finally, the work in [5] evolves around self similarity analysis of the music recording. In some of the above methods, the term “rhythmic signature” is used to as a means to encode fundamental rhythmic characteristics of the music recordings. This paper focuses on rhythmic similarity in non Western music, i.e., Traditional Greek and African music, whic⃝2007 Austrian Computer Society (OCG). ch have so far received little attention in the ﬁeld of MIR. Such traditions impose a number of research challenges, mainly due to the complexity of the music meters, the system of music intervals and the highly improvisational attitude of the music performers. The latter gives an additional research challenge as serves the preservation of cultural heritage and also highlights the importance of MIR systems to apply to corpora that fall outside the traditional Western schemes. In an attempt to measure rhythmic similarity in such music corpora, this paper exploits the repetitive nature of the music recordings by means of Self Similarity Analysis. This type of analysis reveals periodicities that are inherent in the music signal. Such periodicities are expressed as a sequence of values to which we also refer by the term rhythmic signature. To this end, we investigate the possibility of applying an optional thumbnailing scheme as a preprocessing step to extracting rhythmic signatures. Similarity measurement between signatures is performed by means of a standard Dynamic Time Warping technique. Section 2 presents the proposed audio thumbnailing scheme and Section 3 describes how rhythmic signatures are extracted from the music signal. The proposed similarity measure is presented in Section 4. Results and implementation details are given in Section 5 and conclusions are drawn in Section 6. 2 THUMBNAILING SCHEME The proposed audio thumbnailing scheme is optional and is considered to be a variation of the method proposed in [6], in the sense that a different feature extraction scheme is used in this paper.",
        "zenodo_id": 1417503,
        "dblp_key": "conf/ismir/AntonopoulosPTCML07"
    },
    {
        "title": "Signal + Context = Better Classification.",
        "author": [
            "Jean-Julien Aucouturier",
            "François Pachet",
            "Pierre Roy",
            "Anthony Beurivé"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416852",
        "url": "https://doi.org/10.5281/zenodo.1416852",
        "ee": "https://zenodo.org/records/1416852/files/AucouturierPRB07.pdf",
        "abstract": "Typical signal-based approaches to extract musical descriptions from audio only have limited precision. A possible explanation is that they do not exploit context, which provides important cues in human cognitive processing of music: e.g. electric guitar is unlikely in 1930s music, children choirs rarely perform heavy metal, etc. We propose an architecture to train a large set of binary classiﬁers simultaneously, for many different musical metadata (genre, instrument, mood, etc.), in such a way that correlation between metadata is used to reinforce each individual classiﬁer. The system is iterative: it uses classiﬁcation decisions it made on some classiﬁcation problems as new features for new, harder problems; and hybrid: it uses a signal classiﬁer based on timbre similarity to bootstrap symbolic inference with decision trees. While further work is needed, the approach seems to outperform signal-only algorithms by 5% precision on average, and sometimes up to 15% for traditionally difﬁcult problems such as cultural and subjective categories. 1 INTRODUCTION: BOOTSTRAPPING SYMBOLIC REASONING WITH ACOUSTIC ANALYSIS People routinely use many varied high-level descriptions to talk and think about music. Songs are commonly said to be “energetic”, to make us “sad” or ”nostalgic”, to sound “like ﬁlm music” and to be perfect to “drive a car on the highway” among a possible inﬁnity of similar metaphors. The Electronic Music Distribution industry is in demand of robust computational techniques to extract such descriptions from musical audio signals. The majority of existing systems to this aim rely on a common model of the signal as the long-term accumulative distribution of frame-based spectral features. Musical audio signals are typically cut into short overlapping frames (e.g. 50ms with a 50% overlap), and for each frame, a feature vector is computed. Features usually consists of generic, all-purpose spectral representations such as melfrequency cepstrum coefﬁcients (MFCCs), but can also be e.g. rhythmic features [1]. The features are then fed to a statistical model, such as a Gaussian mixture model (GMM), which estimates their global distribution over the c⃝2007 Austrian Computer Society (OCG). total length of the extract. Global distributions can then be used to compute decision boundaries between classes (to build e.g. a genre classiﬁcation system such as [2]) or directly compared to one another to yield a measure of acoustic similarity [3]. While such signal-based approaches are by far the most dominant paradigm currently, recent research increasingly suggests they are plagued with important intrinsic limitations [3, 5]. One possible explanation is that they take an auditory-only approach to music classiﬁcation. However, many of our musical judgements are not low-level immediate perceptions, but rather high-level cognitive reasoning which accounts for the evidence found in the signal, but also depends on cultural expectations, a priori knowledge, interestingness and “remarkability” of an event, etc. Typical musical descriptions only have a weak and ambiguous mapping to intrinsic acoustic properties of the signal. In [6], subjects were asked to rate the similarity between pairs of 60 sounds and 60 words. The study concludes that there is no immediately obvious correspondence between single acoustic attributes and single semantic dimensions, and go as far as suggesting that the sound/word similarity judgment is a forced comparison (“to what extent would a sound spontaneously evoke the concepts that it is judged to be similar to?”). Similarly, we studied in [4] the performance of a typical classiﬁer on a heterogeneous set of more than 800 high-level musical symbols, manually annotated for more than 4,000 songs. We observed that surprisingly few of such descriptions can be mapped with reasonable precision to acoustic properties of the corresponding signals. Only 6% of the attributes in the database are estimated with more than 80% precision, and more than a half of the database’s attributes are estimated with less that 65% precision (which hardly better than a binary random choice, i.e. 50%). The technique provides very precise estimates for attributes such as homogeneous genre categories or extreme moods like “aggressive” or “warm”, but typically fails on more cultural or subjective attributes which bear little correlation with the actual sound of the music being described, such as “Lyric Content”, or complex moods or genres (such as “Mysterious” or “Electronica”). This does not mean human musical judgements are beyond computational approximation, naturally. The study in [4] shows that there are large amounts of correlation between musical descriptions at the symbolic level. Table 1 shows a selection of pairs of musical metadata items (from a large manually-annotated set), which were found Table 1. Selected pairs of musical metadata with their Φ score (χ2 normalized to the size of the population), between 0 (corresponding to statistical independence between the variables) and 1 (complete deteministic association). Data analysed on a a set of 800 metadata values manually annotated for more than 4,000 songs, used in previous study [4] Attribute1 Attribute2 Φ Music-independant Textcategory Christmas Genre Special Occasions",
        "zenodo_id": 1416852,
        "dblp_key": "conf/ismir/AucouturierPRB07"
    },
    {
        "title": "Variable-Size Gaussian Mixture Models for Music Similarity Measures.",
        "author": [
            "Wietse Balkema"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415506",
        "url": "https://doi.org/10.5281/zenodo.1415506",
        "ee": "https://zenodo.org/records/1415506/files/Balkema07.pdf",
        "abstract": "An algorithm to efﬁciently determine an appropriate number of components for a Gaussian mixture model is presented. For determining the optimal model complexity we do not use a classical iterative procedure, but use the strong correlation between a simple clustering method (BSAS [13]) and an MDL-based method [6]. This approach is computationally efﬁcient and prevents the model from representing statistically irrelevant data. The performance of these variable size mixture models is evaluated with respect to hub occurrences, genre classiﬁcation and computational complexity. Our variable size modelling approach marginally reduces the number of hubs, yields 3-4% better genre classiﬁcation precision and is approximately 40% less computationally expensive. 1 INTRODUCTION In the current boom of web 2.0, the market for music recommender systems seems to have taken off. Last.fm, iLike, myStrands and others analyze a user’s listening behaviour and compare it with other user’s proﬁles. Music can be tagged with personal tags which allows new ways to explore music collections. One of the major problems of these community based recommender systems is their robustness in new databases and dealing with underrepresented data. Once a song is chosen as favourite, a loop mechanism can keep this song as favourite for a long time. Community based recommenders can be sensitive to attacks that try to inﬂuence a speciﬁc song’s rating. Two kinds of attack strategies are shilling (promoting an item) and nuking (demoting an item) [8]. Another approach for recommender systems that do not suffer from loops, shilling or nuking is content-based recommendation. The acoustical content of a song is analyzed, and songs found to be ‘similar’ to songs a user likes are recommended. Audio similarity is multifaceted, so a common approach to evaluate audio similarity measures is to perform a genre classiﬁcation task. Pampalk et al. [11] c⃝2007 Austrian Computer Society (OCG). found that a combination of 70% timbral and 30% temporal features provide a good audio similarity measure. Hubs, songs that are found to be very similar to a very large number of other songs, are a major problem for audio-based music recommender systems. Aucouturier and Pachet [3] showed that in a purely timbre-based nearest neighbor retrieval system, the number of hubs signiﬁcantly increases when discarding the 5% least signiﬁcant clusters from a Gaussian mixture model. The computational complexity for calculating the distance between Gaussian mixture models scales linearly with the number of clusters in a mixture model for most distance measures. Reducing the number of clusters in a model thus has great impact in computational complexity, but inﬂuences performance. We present a method to reduce the number of mixture components without sacriﬁcing retrieval performance. The required number of Gaussians in a Gaussian mixture model is estimated for each song individually. The number of clusters is then used by the Estimation Maximization algorithm to model the song data. Using individual song complexity estimation prevents overﬁtted models on ‘simple’ songs with too complex models, while still offering ‘complex’ songs an adequate model complexity. The remainder of this paper is organized as follows: In section 2 we present a short overview of related work. Section 3 describes our feature modelling approach in detail. This section is followed by a performance analysis with respect to the effect of our algorithm on hubs and on a genre classiﬁcation task. In our last section we summarize our results and give some recommendations for further research. 2 RELATED WORK Berenzweig et al. [4] compare anchor space based and GMM based similarity measures with a similarity matrix retrieved from a user survey. The anchor space method performs very similarly to the GMM method. Aucouturier and Pachet [2] systematically explore feature parameter space for timbre similarity experiments and evaluate performance with a nearest neighbour retrieval task. Optimal R-precision was found with 20 dimensional MFCCs and a Gaussian mixture model with 50 components. The number of model components however is of less inﬂuence than the number of feature dimensions. Their conclusion is that ‘Everything performs the same’ and that there seems to be a glass ceiling in R-precision. Flexer et al. [7] compare Hidden Markov Models (HMMs) with Gaussian Mixture Models (GMM) describing spectral similarity of songs. It is shown that HMMs are capable of representing the underlying data better than GMMs, even if the GMM has more degrees of freedom. In a genre classiﬁcation task, both methods show very similar results. 3 FEATURE MODELLING We calculate song similarity on 15 dimensional MFCC vectors (without the 0th coefﬁcient), modelled with a Gaussian Mixture Model: p (x, Θ) = k X i=1 αiG(x, µ, ΣX) (1) with x a single feature vector and Θ the model parameters: cluster mean µ and cluster covariance Σ. The mixture weights αi are nonnegative and add up to one.",
        "zenodo_id": 1415506,
        "dblp_key": "conf/ismir/Balkema07"
    },
    {
        "title": "High Time-Resolution Estimation of Multiple Fundamental Frequencies.",
        "author": [
            "Jayme Garcia Arnal Barbedo",
            "Amauri Lopes",
            "Patrick J. Wolfe"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415972",
        "url": "https://doi.org/10.5281/zenodo.1415972",
        "ee": "https://zenodo.org/records/1415972/files/BarbedoLW07.pdf",
        "abstract": "This paper presents a high time-resolution strategy to estimate multiple fundamental frequencies in musical signals. The signal is ﬁrst divided into overlapping blocks, and a high-resolution estimate made of the short-term spectrum. The resulting spectrum is modiﬁed such that only the most relevant spectral components are considered, and an iterative algorithm based on earlier work by Klapuri is used to identify candidate fundamental frequencies. Finally, a context-based rule is used to improve the accuracy of fundamental frequency estimates. The performance of this technique is investigated under both noiseless and noisy conditions, and its accuracy is examined in cases where the polyphony is known and unknown a priori. 1 INTRODUCTION The estimation of fundamental frequencies (F0) of mixtures of several sound sources is a problem whose solution can beneﬁt several areas of digital audio processing, including automatic music transcription, music information retrieval, and sound source separation, among others. Early work relating to this problem aimed to solve the problem of transcribing polyphonic music [1, 2]; however, such methods worked well only under very restrictive constraints. A new phase in multiple-F0 estimation started with the work of Meddis and Hewitt [3], which has provided the foundation for most approaches used in more recent methods. Cheveign´e explored the model proposed in [3] to develop an iterative procedure in which the sound component corresponding to a particular estimated F0 is removed, and a new round of F0 estimation then proceeds using the residual [4]. Tolonen and Karjalainen simpliﬁed the approach of [3] to create a strategy reported to be both accurate and computationally efﬁcient [5], and statistical inference was used by Davy and Godsill to estimate c⃝2007 Austrian Computer Society (OCG). multiple fundamental frequencies [6]. Klapuri proposed a method based in the harmonicity and spectral smoothness of the signals [7], as well as a more recent perceptually motivated strategy that uses an iterative estimationcancellation approach [8]. Like other approaches, the method proposed here begins with the division of the musical signal into overlapping short-time frames. The high-resolution spectral estimation proposed in [9] is then applied to each frame in turn, in order to allow a ﬁner analysis of the spectral structure of the signal. The resulting spectrum is modiﬁed in such a way only the most relevant frequency components are considered. Additionally, the remaining components are quantized into only two levels, making the data more homogeneous. The modiﬁed spectrum is then analyzed using an iterative algorithm based on the procedure presented in [8], but which also introduces some further processing intended to reﬁne the selection of the correct F0. If the polyphony is known a priori, the iterations stop as soon as the number of sound sources has been reached. If the polyphony is unknown, the iterations are interrupted if one out a set of rules is fulﬁlled. After the fundamental frequencies have been determined for all frames, a further procedure is applied to improve the estimates. All frames contained in a segment between two events 1 are analyzed, and the estimates are all made the same according to majority rules. In the cases where the polyphony is known a priori, this last procedure only changes the values of F0, but in cases where the polyphony is unknown at the outset, there can be changes in the estimated number of sound sources. Those context-corrected F0 estimates comprise the output of the algorithm. The remainder of this paper describes this algorithm and its application in more detail, and is organized as follows. Section 2 presents a description of the algorithm. The analysis of the results is presented in Section 3, and Section 4 presents the conclusions and ﬁnal remarks. 1 An event, in the context of this work, is any change in the number of sound sources and/or fundamental frequencies present in the signal. Division into 46.4 ms Frames Input Signal High-Resolution Modified Spectrum Iterative Determination of Multiple F0 Context-Based Correction Final F0 Figure 1. Block diagram of the estimation strategy 2 ALGORITHM DESCRIPTION Figure 1 shows the basic structure of the method. As can be seen, there are four basic procedures, which will be described in the following.",
        "zenodo_id": 1415972,
        "dblp_key": "conf/ismir/BarbedoLW07"
    },
    {
        "title": "Audio-Based Cover Song Retrieval Using Approximate Chord Sequences: Testing Shifts, Gaps, Swaps and Beats.",
        "author": [
            "Juan Pablo Bello"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417911",
        "url": "https://doi.org/10.5281/zenodo.1417911",
        "ee": "https://zenodo.org/records/1417911/files/Bello07.pdf",
        "abstract": "This paper presents a variation on the theme of using string alignment for MIR in the context of cover song identiﬁcation in audio collections. Here, the strings are derived from audio by means of HMM-based chord estimation. The characteristics of the cover-song ID problem and the nature of common chord estimation errors are carefully considered. As a result strategies are proposed and systematically evaluated for key shifting, the cost of gap insertions and character swaps in string alignment, and the use of a beat-synchronous feature set. Results support the view that string alignment, as a mechanism for audiobased retrieval, cannot be oblivious to the problems of robustly estimating musically-meaningful data from audio. 1 INTRODUCTION The term musical similarity can be used to imply a relationship between songs that goes beyond texture, genre or artist, and that is more akin to purely musicological comparisons between songs, e.g. in terms of their melody, harmony and/or rhythm. In this context, cover song identiﬁcation in popular music can be seen as a good, albeit limited, test of the ability to model musical similarity. The task of identifying cover songs poses many difﬁculties for audio-based music retrieval since renditions are often quite different from the original in one or many attributes including instrumentation, key or genre to name a few. In this paper we propose an approach to cover song identiﬁcation based on the use of string alignment for the scoring of approximate chord sequences. These sequences are extracted from audio using chroma features and hidden Markov Models [2]. They are approximate because chord estimation from audio is never 100% accurate. Song sequences in a collection are ranked according to the score of their alignment with a query sequence. The use of approximate string matching is favored as the sequential ordering of events in the signal is taken into account. We argue that in order to maximize retrieval results, one has to consider not only the key or tempo differences between cover song sequences, but also the ways in which these sequences approximate (or not) the songs they represent. c⃝2007 Austrian Computer Society (OCG).",
        "zenodo_id": 1417911,
        "dblp_key": "conf/ismir/Bello07"
    },
    {
        "title": "Audio Identification Using Sinusoidal Modeling and Application to Jingle Detection.",
        "author": [
            "Michaël Betser",
            "Patrice Collen",
            "Jean-Bernard Rault"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416890",
        "url": "https://doi.org/10.5281/zenodo.1416890",
        "ee": "https://zenodo.org/records/1416890/files/BetserCR07.pdf",
        "abstract": "This article presents a new descriptor dedicated to Audio Identification (audioID), based on sinusoidal modeling. The core idea is an appropriate selection of the sinusoidal components of the signal to be detected. This new descriptor is robust against usual distortions found in audioID tasks. It has several advantages compared to classical subband-based descriptors including an increased robustness to additive noise, especially non-random noise such as additional speech, and a robust detection of short audio events.  This descriptor is compared to a classical subband-based feature for a jingle detection task on broadcast radio. It is shown that the new introduced descriptor greatly improves the performance in terms of recall/precision.",
        "zenodo_id": 1416890,
        "dblp_key": "conf/ismir/BetserCR07"
    },
    {
        "title": "A Comparative Survey of Image Binarisation Algorithms for Optical Recognition on Degraded Musical Sources.",
        "author": [
            "John Ashley Burgoyne",
            "Laurent Pugin",
            "Greg Eustace",
            "Ichiro Fujinaga"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418203",
        "url": "https://doi.org/10.5281/zenodo.1418203",
        "ee": "https://zenodo.org/records/1418203/files/BurgoynePEF07.pdf",
        "abstract": "Binarisation of greyscale images is a critical step in optical music recognition (OMR) preprocessing. Binarising music documents is particularly challenging because of the nature of music notation, even more so when the sources are degraded, e.g., with ink bleed-through from the other side of the page. This paper presents a comparative evaluation of 25 binarisation algorithms tested on a set of 100 music pages. A real-world OMR infrastructure for early music (Aruspix) was used to perform an objective, goaldirected evaluation of the algorithms’ performance. Our results differ signiﬁcantly from the ones obtained in studies on non-music documents, which highlights the importance of developing tools speciﬁc to our community. 1 INTRODUCTION Binarising music documents, that is separating the foreground from the background in order to prepare for other tasks such as optical music recognition (OMR), is much more challenging than binarising text documents. In text documents, the letters are all of approximately the same size and are regularly and uniformly distributed throughout the page. Music symbols, on the other hand, exhibit a wide range of sizes and markedly uneven distribution: they are clustered around musical staves. Large black areas, such as note heads, are conducive to ink accumulation during printing, which often results in strong bleedthrough (elements from the verso visible through the paper), especially for early sources. Large blank areas without foreground elements can disturb some binarisation techniques because bleed-through is often considered to be foreground. We will show in this paper that because of these conditions, the most widely used binarisation methods fail to produce suitable results for OMR. Evaluating the performance of binarisation algorithms is a difﬁcult task. Very often, due to a lack of any evaluation infrastructure, researchers use subjective approaches, e.g., marking output as “better”, “same” or “worst” [3, 4]. When the binarisation is performed for the purpose of further image processing tasks, such as optical character c⃝2007 Austrian Computer Society (OCG). recognition (OCR) or OMR, it makes more sense to use an objective evaluation. Evaluating the algorithms within the context of a real-world application enables goal-directed evaluation, which rates a binarisation algorithm on its ability to improve the post-binarisation task [10]. Furthermore, it has been shown that when document images have graphical particularities like music documents do, the use of goal-directed evaluation can lead to signiﬁcant performance improvements [7]. 2 METHODS For our experiments, we used Aruspix, a software application for OMR on early music prints [7]. We selected ﬁve 16th-century music books (RISM 1520-2, 1532-10, 15385, M-0579 and M-0582 [8]) that suffer from severe degradation and transcribed 20 pages from each (100 total) to obtain ground-truth data for the evaluation. We tested 25 different binarisation algorithms over a range of parameters, which resulted in a set of 8,000 images. The images were deskewed and normalised to a consistent staff height (100 pixels) by Aruspix before applying the binarisation algorithm, and after binarisation, Aruspix was used again for the OMR evaluation. Binarisation methods can be categorised according to differences in the criteria used for thresholding. Sezgin and Sankur have proposed a taxonomy of thresholding techniques, including those based on the shape of the greyvalue histogram, measurement-space clustering, image entropy, connected-component attributes, spatial correlation, and the properties of a small, local windows around each pixel [9]. We chose a range of top-performing algorithms for both document images and what Sezgin and Sankur call non-destructive testing (NDT) images, which have more photo-like qualities; for the reasons mentioned above, music documents would be expected to fall somewhere between these two extremes. Methods based on histogram shape include those proposed by Sezan (1985) and Ramesh et al. (1995). Popular measurement-space clustering methods include those proposed by Ridler and Calvard (1978), Otsu (1979), Lloyd (1985), Kittler and Illingworth (1986), Yanni and Horne (1994), and Jawahar et al. (1997). Some entropy-based methods, also popular, are those of Dunn et al. (1984), Kapur et al. (1985), Li and Lee (1993), Shanbhag (a) RISM 1520-2 (b) RISM 1538-5 (c) RISM M-0582 Figure 1. Three sample images from the test set. (a) Brink and Pendock 1996 (b) Gatos et al. 2004 (c) Otsu 1979 Figure 2. Binarisation output from three algorithms on RISM 1538-5. Algorithm Win. Recall Precision Rate Odds multiplier Rate Odds multiplier Brink and Pendock 1996 79.2",
        "zenodo_id": 1418203,
        "dblp_key": "conf/ismir/BurgoynePEF07"
    },
    {
        "title": "A Cross-Validated Study of Modelling Strategies for Automatic Chord Recognition in Audio.",
        "author": [
            "John Ashley Burgoyne",
            "Laurent Pugin",
            "Corey Kereliuk",
            "Ichiro Fujinaga"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416816",
        "url": "https://doi.org/10.5281/zenodo.1416816",
        "ee": "https://zenodo.org/records/1416816/files/BurgoynePKF07.pdf",
        "abstract": "Although automatic chord recognition has generated a number of recent papers in MIR, nobody to date has done a proper cross validation of their recognition results. Cross validation is the most common way to establish baseline standards and make comparisons, e.g., for MIREX competitions, but a lack of labelled aligned training data has rendered it impractical. In this paper, we present a comparison of several modelling strategies for chord recognition, hidden Markov models (HMMs) and conditional random ﬁelds (CRFs), on a new set of aligned ground truth for the Beatles data set of Sheh and Ellis (2003). Consistent with previous work, our models use pitch class proﬁle (PCP) vectors for audio modelling. Our results show improvement over previous literature, provide precise estimates of the performance of both old and new approaches to the problem, and suggest several avenues for future work. 1 INTRODUCTION The task of automatic, continuous chord recognition is an area of active study in the MIR community. When working with audio, chord recognition is an especially difﬁcult task because a chord represents such a wide range of possible musical events. Recent studies have shown the beneﬁt of applying stochastic modelling to this task [1, 7, 8, 11, 13]. The most commonly used model is the hidden Markov model (HMM) [10], but more recent work has also explored discriminative models [9] such as the conditional random ﬁeld (CRF) [14]. In this paper, we use the work of Sheh and Ellis as our departure point [13]. These authors used HMMs to perform chord recognition on a set of 20 Beatles songs. Although their recognition rates were poor, they laid a foundation for future study. We use the same data set, but in addition to their tests, we perform a 10-fold cross validation to verify the validity of our results, training on 18 songs for each run and testing on the remaining 2. Cross validation is essential for obtaining unbiased estimates of model performance when data is limited [5], but because c⃝2007 Austrian Computer Society (OCG). it is so time-consuming to label audio ﬁles, no previous studies of audio chord recognition have tried it. Crossvalidated recognition rates will be lower than the best possible test rate on a single song, e.g., the metric used in [7], but they give a more realistic depiction of the state of the art and are the only fair way to compare different models. Another problem that is often encountered when building HMMs is a lack of aligned, labelled training data. When the training data is not aligned, the model must be initialised using a so-called ﬂat start. In a ﬂat start, training audio is uniformly segmented based on an unaligned transcription. One hopes that enough of the uniformly segmented labels in the ﬂat start will match the correct alignment so that the model parameters will improve during successive training iterations, but this is unlikely in musical applications because chord lengths vary so widely. Using a ﬂat-start with this training data has indeed been shown to result in poor recognition performance [13], and so for our training, we avoided it. Our results demonstrate the usefulness of stochastic modelling and highlight the beneﬁts of CRFs, which until now have received very little attention in the MIR community. 2 PITCH CLASS PROFILE VECTORS Pitch class proﬁle (PCP) vectors, which were introduced by Fujishima in 1999 [3], have been widely used in chord recognition systems [1, 4, 6, 13]. In essence, PCP vectors represent a logarithmically warped and wrapped version of the short time frequency spectrum. The following equations show the computation of a PCP vector: PCP[i] ≜ X {k : p[k]=i} ∥X[k]∥2 (1) p[k] ≜ \u001a round \u0014 D log2 \u0012 k N · fs fref \u0013\u0015\u001b mod D (2) In equation 2, k is an FFT bin, fref is the reference pitch, N is the FFT window size, fs is the sampling rate, and D is the dimensionality of the PCP vector. The log2 operation warps the frequency spectrum to a logarithmic scale, while the modulo operation wraps the frequency spectrum at integer multiples (octaves) of the reference frequency. The frequency components in each of the warped and wrapped frequency bands are then summed (equation 1). In this study, we used D = 12 and fref = 261.6 Hz (C4). Thus, each PCP vector represents 12 semi-tones of a chromatic scale under the same modulus as most Western music theorists as well as the MIDI note number speciﬁcation.",
        "zenodo_id": 1416816,
        "dblp_key": "conf/ismir/BurgoynePKF07"
    },
    {
        "title": "Monaural Source Separation from Musical Mixtures Based on Time-Frequency Timbre Models.",
        "author": [
            "Juan José Burred",
            "Thomas Sikora"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416396",
        "url": "https://doi.org/10.5281/zenodo.1416396",
        "ee": "https://zenodo.org/records/1416396/files/BurredS07.pdf",
        "abstract": "We present a system for source separation from monaural musical mixtures based on sinusoidal modeling and on a library of timbre models trained a priori. The models, which rely on Principal Component Analysis, serve as time-frequency probabilistic templates of the spectral envelope. They are used to match groups of sinusoidal tracks and assign them to a source, as well as to reconstruct overlapping partials. The proposed method does not make any assumptions on the harmonicity of the sources, and does not require a previous multipitch estimation stage. Since the timbre matching stage detects the instruments present on the mixture, the system can also be used for classiﬁcation and segmentation. 1 INTRODUCTION Separation of a musical mixture into its sources can greatly facilitate content analysis for Music Information Retrieval purposes, and allows other applications like remixing or upmixing to a larger number of channels than the original if multitrack recordings are not available. We address separation from a single channel, which is a highly underdetermined problem that requires either strong assumptions about the nature of the sources, a fair amount of a priori information, or a combination of both. The main assumption taken in underdetermined separation is the sparsity of the sources, which leads to the use of elaborate signal models. An example thereof is the use of Nonnegative Sparse Coding [1]. Other approaches are based on sinusoidal modeling, which allows a detailed handling of overlapping partials and is also a highly sparse model. They are based on grouping the extracted partials according to Auditory Scene Analysis cues. In [2], amplitude smoothness is modeled by performing basis decomposition on the harmonic structures and their evolution in time. In [3], spectral ﬁltering techniques are used to resolve overlapping sinusoids. Most approaches based on sinusoidal modeling rely either on a previous multipitch estimation stage or on the knowledge of the pitch score of the mixture [2, 3]. c⃝2007 Austrian Computer Society (OCG). The above methods are unsupervised (i.e., there is no training phase) and are based on generic source models. To further improve separation, statistical models of the sources can be trained beforehand on a database of isolated source samples. Examples of this supervised approach include the use of learnt spectral priors with bayesian harmonic models [4] and the derivation of templates for timbral features [5]. In the present contribution, we propose a system for the separation of sources from single-channel mixtures of musical instruments based on sinusoidal modeling and on a library of pre-trained timbre models. Since it also outputs onset/offset information and the instrument each note belongs to, it can also be used for segmentation or polyphonic instrument recognition. The timbre models are time-frequency templates that describe in detail spectral shapes and their evolution in time. In contrast to most previously existing approaches, no assumptions on harmonicity are made, which allows to separate highly inharmonic sounds or to separate chords played by a single instrument. Furthermore, no previous multipitch estimation or any kind of a priori pitch-related score is needed. Instead, separation is solely based on common onset properties of the partials, and on the analysis of the evolution in time of the spectral envelope they deﬁne. The knowledge of the number and names of the instruments is not mandatory, but will obviously increase the performance. Figure 1 shows an overview of the proposed separation system. First, the mixture signal is subjected to sinusoidal modeling, obtaining a set of sinusoidal tracks. A simple onset detector based on identifying synchronously starting tracks then allows to select the partial tracks that are going to be matched with the timbre models in the next stage. After each common-onset group of partial tracks has been assigned to an instrument, the overlapping part of the tracks is retrieved from the models. Finally, the separated tracks are synthesized using additive synthesis. 2 TRAINING OF THE TIMBRE MODELS The used timbre models are based on the spectral envelope and its evolution in time. Their design and training process has been described in detail in [6]. It is based on performing Principal Component Analysis (PCA) on the set of training spectral envelopes extracted from a database Sinusoidal Modeling Onset detection Track grouping Timbre matching Timbre model library Track extension Resynthesis MIXTURE ... Segmentation results ... ... SOURCES Figure 1. System overview. of isolated notes. The ﬁnal result is a set of prototype curves in a reduced-dimensional timbre space. When projected back to the t-f domain, each prototype trajectory corresponds to a prototype envelope consisting of a mean surface and a variance surface, which we will denote by Mi(k, r) and Σi(k, r), respectively, where i = 1, . . . , I is the instrument index, k = 1, . . . , K is the frequency bin index, and r = 1, . . . , R is the frame index (we will consider the same number of frames R for all models). Analogously, this can be interpreted as a Gaussian Process with parameters varying in the time-frequency plane. 3 SEGMENTATION AND SEPARATION",
        "zenodo_id": 1416396,
        "dblp_key": "conf/ismir/BurredS07"
    },
    {
        "title": "Singing Melody Extraction in Polyphonic Music by Harmonic Tracking.",
        "author": [
            "Chuan Cao",
            "Ming Li 0026",
            "Jian Liu",
            "Yonghong Yan 0002"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414708",
        "url": "https://doi.org/10.5281/zenodo.1414708",
        "ee": "https://zenodo.org/records/1414708/files/CaoLLY07.pdf",
        "abstract": "This paper proposes an effective method for automatic melody extraction in polyphonic music, especially vocal melody songs. The method is based on subharmonic summation spectrum and harmonic structure tracking strategy. Performance of the method is evaluated using the LabROSA database 1 . The pitch extraction accuracy of our method is 82.2% on the whole database, while 79.4% on the vocal part. 1 INTRODUCTION Melody is widely considered as a concise and representative description of polyphonic music and it can be used in numerous applications such as “Query-by-humming” system, music structure analysis and music classiﬁcation. However, the automatic melody extraction is recognized to be very tough and remains unsolved up to now. Yet, amount of remarkable work has been done recently. In 1999, Goto for the ﬁrst time used a monophonic pitch sequence to represent music melody and achieved transcription from real world music with his famous PreFEst algorithm [5]. Klapuri [1] then proposed a perceptual motivated algorithm in 2005. Poliner and Ellis introduced a novel classiﬁcation approach using SVM theory for the transcription task [2]. Also, Paiva et al. [6] and Dressler [4] proposed methods generally based on spectral peaks picking and post-tracking. In most methods above, pitch information (pitch candidates, instantaneous frequency (IF) estimations or others) is analyzed frame by frame, and then integrated with temporal/spectral restrictions. However in polyphonic music, especially vocal melody songs, some local frames are inevitably dominated by non-melody intrusions and thus local pitch information is polluted somewhat, or even destroyed sometimes. So integration process based on the polluted information could hardly ﬁnd the true melody at those local frames. In this paper, we propose a harmonic tracking method attempting to solve this problem. Brieﬂy speaking, we 1 The database can be downloaded from the web site of: http://labrosa.ee.columbia.edu/projects/melody/ c⃝2007 Austrian Computer Society (OCG). trace a sound with its harmonic structure in frequency domain. Here harmonic structure mainly refers to the harmonic partials’ frequencies and their relative amplitudes. If given target harmonic structure for a speciﬁc local frame, we could ﬁnd the partials from the same sound in adjacent frames, by a tracking strategy. In real applications, target harmonic structure is not known priorly and thus has to be estimated from the mixed signal. We analyze the predominant pitch of the mixture to ﬁnd stable harmonic structure seeds and then use them to track forward and backward. Also, rather than tracing all the harmonic partials, we use subharmonic-summation (SHS) spectrum as the tracking feature for simplicity, which can be considered as an integrative representation of the whole harmonic family. And a veriﬁcation procedure is needed to make up the gap between full partial tracking and integrative feature tracking. 2 METHOD DESCRIPTION",
        "zenodo_id": 1414708,
        "dblp_key": "conf/ismir/CaoLLY07"
    },
    {
        "title": "Raag Recognition Using Pitch-Class and Pitch-Class Dyad Distributions.",
        "author": [
            "Parag Chordia",
            "Alex Rae"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416888",
        "url": "https://doi.org/10.5281/zenodo.1416888",
        "ee": "https://zenodo.org/records/1416888/files/ChordiaR07.pdf",
        "abstract": "We describe the results of the ﬁrst large-scale raag recognition experiment. Raags are the central structure of Indian classical music, each consisting of a unique set of complex melodic gestures. We construct a system to recognize raags based on pitch-class distributions (PCDs) and pitch-class dyad distributions (PCDDs) calculated directly from the audio signal. A large, diverse database consisting of 20 hours of recorded performances in 31 different raags by 19 different performers was assembled to train and test the system. Classiﬁcation was performed using support vector machines, maximum a posteriori (MAP) rule using a multivariate likelihood model (MVN), and Random Forests. When classiﬁcation was done on 60s segments, a maximum classiﬁcation accuracy of 99.0% was attained in a cross-validation experiment. In a more difﬁcult unseen generalization experiment, accuracy was 75%. The current work clearly demonstrates the effectiveness of PCDs and PCDDs in discriminating raags, even when musical differences are subtle. 1 BACKGROUND",
        "zenodo_id": 1416888,
        "dblp_key": "conf/ismir/ChordiaR07"
    },
    {
        "title": "A Dynamic Programming Approach to the Extraction of Phrase Boundaries from Tempo Variations in Expressive Performances.",
        "author": [
            "Ching-Hua Chuan",
            "Elaine Chew"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416584",
        "url": "https://doi.org/10.5281/zenodo.1416584",
        "ee": "https://zenodo.org/records/1416584/files/ChuanC07.pdf",
        "abstract": "We present an approach to phrase segmentation that starts with an expressive music performance. Previous research has shown that phrases are delineated by tempo speedups and slowdowns. We propose a dynamic programming algorithm for extracting phrases from tempo information. We test two hypotheses for modeling phrase tempo shapes: a quadratic model, and a spline curve. We test the two models on phrase extraction from performances of entire classical romantic pieces namely, Chopin’s Preludes Nos. 1 and 7. The algorithms determined 21 of the 26 phrase boundaries correctly from Arthur Rubinstein’s and Evgeny Kissin’s performances. We observe that not all tempo slowdowns signify a boundary (some are agogic accents), and multiple levels of phrasing strategies should be considered for detailed interpretation analyses. 1 INTRODUCTION Musical phrasing in expressive performance groups the notes in a piece so as to present a coherent interpretation of its ideas. A performer’s tasks include the determining of some viable groupings of the piece that make musical sense, and the communication of this grouping in performance through the manipulation of expressive parameters such as tempo and loudness. The problem we are concerned with is the automatic extraction of phrases −the groupings of notes in a piece. Phrase structure analysis can begin with the score, or from a performance of a piece. The ﬁrst focuses on features such as motives, melodies and chord progressions. The latter begins with an expressive performanceof a piece. The interpretation is manifested as a set of grouping strategies inherent in the performance. A goal of this paper is to propose a computational approach to automatically extract phrase boundaries from expressive performances based on tempo variations. Our approach ﬁnds the best ﬁt sequence of phrase tempo curves using dynamic programming (DP). This paper also explores the relation between c⃝2007 Austrian Computer Society (OCG). tempo variation and phrase structure. The phrase extraction steps consist of: tempo extraction from audio recordings, tempo smoothing to obtain trajectories, and determination of phrase boundaries from tempo information. We present a Java program for extracting tempo from manually tapping to beat-level onsets. We introduce a DP algorithm for determining the phrase boundaries by curve ﬁtting. We test two quadratic curve types for modeling phrase-level tempo variations: asymmetric concave curves, and splines. 2 RELATED WORK Most researchers focus on three dimensions of expressive musical performance: tempo, dynamics, and articulation. Gabrielsson [2], Kendall & Carterette [3], Todd [7, 8], and Palmer [5] have found that performers tend to indicate phrase boundaries by lengthening note values at these boundaries, and by increasing the time between successive tones. Similarly, Palmer & Hutchins [6] noted that the phrase is a musical unit that is often demarcated by prosodic cues. Large & Palmer [4] used oscillator models and the product of the probabilities that an onset deviates from expected and that it is late as a measure of phrase boundary likelihood. The oscillator models require initial phase and period information. Cheng & Chew [1] used local maxima in the loudness time series to detect phrases. Our present approach uses DP to ﬁt tempo trajectories using a sequence of quadratic curves so as to determine phrase boundaries. No prior information is required in the latter two techniques. 3 SYSTEM DESCRIPTION The system consists of two parts: tempo extraction and phrase boundary determination. Figure 1 shows the system diagram. We describe each part in this section.",
        "zenodo_id": 1416584,
        "dblp_key": "conf/ismir/ChuanC07"
    },
    {
        "title": "Evaluation of Real-Time Audio-to-Score Alignment.",
        "author": [
            "Arshia Cont",
            "Diemo Schwarz",
            "Norbert Schnell",
            "Christopher Raphael"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416304",
        "url": "https://doi.org/10.5281/zenodo.1416304",
        "ee": "https://zenodo.org/records/1416304/files/ContSSR07.pdf",
        "abstract": "This article explains evaluation methods for real-time audio to score alignment, or score following, that allow for the quantitative assessment of the robustness and preciseness of an algorithm. The published ground truth data base and the evaluation framework, including ﬁle formats for the score and the reference alignments, are presented. The work, started for MIREX 2006, is meant as a ﬁrst step towards a standardized evaluation process contributing to the exchange and progress in this ﬁeld. 1 INTRODUCTION Score following is the real-time alignment of a known musical score to the audio signal produced by a musician playing this score, usually in order to synchronise an electronic accompaniment of the music to the performer, leaving him with all possibilities of expressive performance. Despite its history of more than 20 years [1], only very few attempts of a systematic evaluation of the result of score following have been made, and therefore, representation of research work is usually limited to demonstration of short examples or subjective ways to assess the quality of such systems. Evaluation gives an indication of the quality of an alignment algorithm and allows the comparison of different",
        "zenodo_id": 1416304,
        "dblp_key": "conf/ismir/ContSSR07"
    },
    {
        "title": "How Many Beans Make Five? The Consensus Problem in Music-Genre Classification and a New Evaluation Method for Single-Genre Categorisation Systems.",
        "author": [
            "Alastair J. D. Craft",
            "Geraint A. Wiggins",
            "Tim Crawford"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414982",
        "url": "https://doi.org/10.5281/zenodo.1414982",
        "ee": "https://zenodo.org/records/1414982/files/CraftWC07.pdf",
        "abstract": "Genre deﬁnition and attribution is generally considered to be subjective. This makes evaluation of any genrelabelling system intrinsically difﬁcult, as the ground-truth against which it is compared is based upon subjective responses, with little inter-participant consensus. This paper presents a novel method of analysing the results of a genre-labelling task, and demonstrates that there are groups of genre-labelling behaviour which are selfconsistent. It is proposed that the evaluation of any genre classiﬁcation system uses this modiﬁed analysis method. 1 INTRODUCTION Several genre-classiﬁcation systems have been proposed in the literature (surveyed in [1, §4.2]). There has not been a corresponding interest in the evaluation of such systems, or in the actual genre-labelling behaviour of people. Whilst it is generally accepted that genre labels are subjective, with little industry [6], or inter-participant [3] consensus, this is rarely, if ever, included in the evaluation of any genre-categorisation system: most evaluations assume some ground truth, be it industry deﬁned or just the categorisation of the experimenter. Despite the acceptance of the ground-truth problems there has been relatively little work on how people categorise genre (see §3). Whilst this partially excuses experimenters’ reliance on some form of absolute ground truth in system evaluation, it does mean that the results of any studies are questionable, as there is little understanding of what such systems are trying to model. In addition the most commonly referenced ground-truth study, [7], is as yet unpublished, and was not designed as a study of inter-participant consensus, but rather of how much audio a participant needed to establish a consistent genre label [2]. The approach we propose towards ground truth in genre classiﬁcation is as follows: ground truth is an artefact of an individual’s response to music, not an artefact of the audio itself. Therefore the establishment of any ground truth will be the study of responses to music, and is therec⃝2007 Austrian Computer Society (OCG). fore predominately a cultural study. Any unity of response is because of the widespread agreed nature of the musical cues to genre in particular pieces, but the expected response from a group of individuals will be a diversity. If there is a diversity of responses in terms of genre labels to any particular piece, or set of pieces, the standard evaluation methodology that uses single genres as ground truth will, necessarily, not describe all the dataset adequately. 2 WHAT TO EXPECT OF GENRE LABELS? We propose there are two main factors at play in how a listener assigns a single genre label to a piece of music: the number of musical cues associated with different genres in the piece; and the participant’s knowledge and experience of the genres involved. The ﬁrst of these factors is a feature of what the composer intended of the piece in question: as an intentional act a composer can draw upon stylistic elements from one, or more, genres. In cases of this type the assignment of multiple or compound genre labels is justiﬁed. The second factor is a feature of the social and cultural",
        "zenodo_id": 1414982,
        "dblp_key": "conf/ismir/CraftWC07"
    },
    {
        "title": "Finding New Music: A Diary Study of Everyday Encounters with Novel Songs.",
        "author": [
            "Sally Jo Cunningham",
            "David Bainbridge 0001",
            "Dana McKay"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417083",
        "url": "https://doi.org/10.5281/zenodo.1417083",
        "ee": "https://zenodo.org/records/1417083/files/CunninghamBM07.pdf",
        "abstract": "This paper explores how we, as individuals, purposefully or serendipitously encounter “new music” (that is, music that we haven’t heard before) and relates these behaviours to music information retrieval activities such as music searching and music discovery via use of recommender systems. 41 participants participated in a three-day diary study, in which they recorded all incidents that brought them into contact with new music. The diaries were analyzed using a Grounded Theory approach. The results of this analysis are discussed with respect to location, time, and whether the music encounter was actively sought or occurred passively. Based on these results, we outline design implications for music information retrieval software, and suggest an extension of “laid back” searching.",
        "zenodo_id": 1417083,
        "dblp_key": "conf/ismir/CunninghamBM07"
    },
    {
        "title": "Finding Music in Scholarly Sets and Series: The Index to Printed Music (IPM).",
        "author": [
            "Elizabeth Davis"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417747",
        "url": "https://doi.org/10.5281/zenodo.1417747",
        "ee": "https://zenodo.org/records/1417747/files/Davis07.pdf",
        "abstract": "The Index to Printed Music (IPM) provides access to sets and series of  music published beginning  in the 19th century.  Prepared by scholars and researchers, these titles  vary  considerably  in length  (single  to multiple volumes), types (topical, pedagogical, historical, etc.), format   (treatises,  dissertations,  editions,  etc.),  and geographic origin (chiefly Europe and North America). Bibliographical access to their contents is not readily available through library cataloging or reference works. IPM provides title, format, genre, instrumentation, and other  metadata  access  to  these  publications  in  three inter-connected  databases:  Bibliography,  Index,  and Names,  available  by  subscription  through  NISC International, Inc.  The Index Database now contains over 255,000 entries, the Bibliography Database over 10,000 entries, and the Names Database over 15,000 authority records.  Having built this groundwork, future steps involve linking to full-text score images where available  through  non-commercial  projects,  and partnerships with publishers and commercial vendors.",
        "zenodo_id": 1417747,
        "dblp_key": "conf/ismir/Davis07"
    },
    {
        "title": "Bayesian Aggregation for Hierarchical Genre Classification.",
        "author": [
            "Christopher DeCoro",
            "Zafer Barutçuoglu",
            "Rebecca Fiebrink"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416012",
        "url": "https://doi.org/10.5281/zenodo.1416012",
        "ee": "https://zenodo.org/records/1416012/files/DeCoroBF07.pdf",
        "abstract": "Hierarchical taxonomies of classes arise in the analysis of many types of musical information, including genre, as a means of organizing overlapping categories at varying levels of generality. However, incorporating hierarchical structure into conventional machine learning systems presents a challenge: the use of independent binary classiﬁers for each class in the hierarchy can produce hierarchically inconsistent predictions. That is, an example may be assigned to a class, and not assigned to the parent of that class. This paper applies a Bayesian framework to combine, or aggregate, a hierarchy of multiple binary classiﬁers in a principled manner, and consequently improves performance over the hierarchy as a whole. Furthermore, such an approach allows for an arbitrarily complex hierarchy, and does not suffer from classes that are too broad or too reﬁned. Experiments on the MIREX 2005 symbolic genre classiﬁcation dataset show that our Bayesian Aggregation algorithm provides signiﬁcant improvement over independent classiﬁers, and demonstrates superior performance compared to previous work. Our method also improves similarity search by ranking songs by similarity of hierarchical predictions to those of a query song. 1 INTRODUCTION Many musical concepts are inherently hierarchical. Some notion of hierarchy is implicitly or explicitly at play in concepts such as genre and mood (which have overlapping coarse and ﬁne categories), instrument timbre (which is grouped by instrument families), and meter. When hierarchy has been accommodated in automatic classiﬁcation of these concepts, it has generally been in quite simple ways (such as the top-down approach of [9]) or in ways that are acutely tailored to the learning task and/or classiﬁcation method at hand (e.g. [8, 14]). We have developed a technique called Bayesian Aggregation, which uses the output predictions of arbitrary independent classiﬁers (such as k-nearest neighbor, support vector machines, etc.) which we refer to as the base classiﬁers, and aggregates them in such a way as to take advantage of the hierarchical nature of the predictions to improve classiﬁcation accuracy. We demonstrate performance of this method on genre c⃝2007 Austrian Computer Society (OCG). classiﬁcation, a popular classiﬁcation task in music information retrieval. Genre is a culturally relevant and practically useful concept, and genre classiﬁcation systems have the potential to be quite useful in organizing and allowing efﬁcient access to music databases as shown by McKay in [12]. Further, the same work argued that multiple class assignments and user-speciﬁed ontological structure are beneﬁcial in principle; both of these are inherently supported by Bayesian Aggregation. We describe the genre dataset and features used in Section 2.1, and provide background on classiﬁcation algorithms in Section 2.2. We describe our algorithm in Section 3, and demonstrate in Section 4 that the use of Bayesian Aggregation allows for a signiﬁcant improvement in genre classiﬁcation accuracy when compared either to existing methods, or to the predictions of independent classiﬁers without aggregation. Further, it does so in a way that guarantees that the predictions will be consistent with the constraints of the hierarchy; that is, once an instance is assigned to a leaf class, it will also be assigned to the leaf’s parent classes. We also demonstrate that the outputs of such a classiﬁcation system may offer improvements to similarity search systems built on genre classiﬁers. 2 BACKGROUND",
        "zenodo_id": 1416012,
        "dblp_key": "conf/ismir/DeCoroBF07"
    },
    {
        "title": "Fuzzy Song Sets for Music Warehouses.",
        "author": [
            "François Deliège",
            "Torben Bach Pedersen"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415770",
        "url": "https://doi.org/10.5281/zenodo.1415770",
        "ee": "https://zenodo.org/records/1415770/files/DeliegeP07.pdf",
        "abstract": "The emergence of music recommendation systems calls for the development of new data management technologies able to query vast music collections. In this paper, we deﬁne fuzzy song sets and an algebra to manipulate them. We present a music warehouse prototype able to perform efﬁcient nearest neighbor searches in an arbitrary song similarity space. Using fuzzy song sets, the music warehouse offers a practical solution to the all musical data management scenarios provided: song comparisons, user musical preferences and user feedback. We investigate three practical approaches to tackle the storage issues of fuzzy song sets: tables, arrays and bitmaps. Finally, we confront theoretical estimates to concrete implementation results and prove that, from a storage perspective, arrays and bitmaps are both effective data structure solutions. 1 INTRODUCTION Music recommendation systems have recently gained a tremendous popularity. Music lovers discover new ways of searching and sharing their favorite music. However, at such growing speed, the database element of any recommendation systems will soon become a bottleneck. Hence, appropriate musical data management tools are needed. Music Warehouses (MWs) are dedicated data warehouses optimized for the storage and analysis of music content. They are currently developed to respond to this is call. The contributions of this paper are threefold. First, motivated by a case study [2], we propose three generic usage scenarios illustrating the current demands in musical data management. To answer these demands, we deﬁne fuzzy song sets and develop an algebra. Second, to demonstrate the usefulness of fuzzy song sets, a prototypical MW composed of two multidimensional cubes is presented. For each cube, concrete examples of queries inspired by the usage scenarios are provided. Fuzzy song sets prove to be an adequate data structure to manipulate musical information. Third, we discuss three solutions for storing fuzzy song sets and we construct theoretical estimates. A practical implementation shows that the structure overhead represents a major part of the storage consumption and that two solutions are viable for very large music collections. A lot of attention was drawn to enable music lovers to explore individual music collections [6, 7]. Within this context, several research projects have been conducted in c⃝2007 Austrian Computer Society (OCG). order to pursue a suitable similarity measure for music [8, 9]. A music data model, an algebra and a query language are introduced by Wang et al. [10]. However, the model lacks an adequate framework to perform similarity searches. Jensen et al. address this issue and offer a model that supports dimension hierarchies [5]. This paper tackles the storage issues when the scalability does not remain limited to a few hundred thousands songs. Nearest neighbor searches are a popular topic in the database community for their usage in content based retrieval and similarity searches. Work on both high and low dimensional spaces can be found in the literature. However, existing indexing techniques do not apply to high dimensional musical features due to the subjective nature of musical perception, i.e., similarities do not form a metric. Work on indexes for non-metric space is presented in the literature [12]. Though the similarity function is nonmetric, it remains conﬁned in a pair of lower and upper bounds speciﬁcally constructed. MWs, however, should not be tightened to any similarity function. The use of bitmaps in multidimensional databases is frequent. Different compression schemes exist to reduce the storage consumption of bitmaps. The Word Aligned Hybrid [11], WAH, and the Byte aligned Bitmap Compression [1], BBC, are two very common compression algorithms. BBC offers a very good compression ratio and performs bitwise logical operations efﬁciently. WAH performs bitwise operations much faster than BBC but consumes more storage space. The remainder of this paper is organized as follows. Section 2 presents three search for information scenarios that could be treated by music recommendation systems. We proceed in Section 3 by deﬁning fuzzy song sets and an algebra. In Section 4, two prototypical multidimensional cubes are presented and use of the algebra is illustrated through queries examples. Storage solutions are discussed in Section 5 and implementation results are shown in Section 6. Finally, we conclude in Section 7. 2 USAGE SCENARIO Three examples of data obtained from music recommendations system are presented below. The User Feedback The user’s opinion about the previously suggested songs is a valuable piece of information. For each song played, the user can grade if the suggestion was wise based on the criteria provided, referred to as the query context. The query context can be the artist similarity, the genre similarity, the beat similarity, or any other similarity measure available to the system to perform a selection. The grading reﬂects if a proposed song was relevant in the given query context. For example, it is possible to retrieve the list of songs John liked when he asked for a list of rock songs or the ten songs Alice liked the most when she asked for songs similar to “U2: Where the streets have no name”. Typically, the data obtained should contain: (i) a reference to the proﬁle of a registered user in the system, (ii) a reference to a query context provided by the user, and (iii) a list of songs and marks so that for each song proposed, the user can grade how much he liked a particular song being part of the proposition. Grades are given on a per song basis, they reﬂect if the user believes the song deserved its place among the suggested list of songs: strongly disagrees, neutral, likes, and loves. While the grade must not be a numerical value, we assume in the rest of the article that a mapping function to [0, 1] has to be provided. When a user believes a song deﬁnitely deserves its place in the list, a high mark should be given. The User Preferences Some songs should never be proposed to the user independently of the query context. On the contrary, some songs should be proposed more often as they are marked as the user’s favorites. Therefore, a user should be able to grade any song on a fan-scale ranging from “I love it” to “I hate it” depending if he likes the song or not. For example, the music recommendation system database should be able to retrieve the list of songs Maria likes, and the songs she hates the most. The User Musical Preferences contains two different pieces of information: (i) a reference to a user registered, and (ii) a list of songs associated with their respective grades on the fan-scale. If Rico hates a song, a low value should be used; if he loves it, a value close to 1 should be used instead. Musical proﬁles modify the frequency a given song appears in a music recommendation list. The Songs Comparisons Finally, the music recommendation system should be able to compare any pair of songs. For each pair of songs, the system is able to provide a similarity value with respect to a given aspect of the song. The similarity values should indicate if two songs are “very different”, “different”, “somewhat similar”, or “very similar” from the perspective of an given aspect of the song. For example, the song “We will rock you” by Queen is “very different” from the song “Twinkle, twinkle little star” with respect to their genre similarity aspect. To compare songs, three pieces of information are necessary: (i) a reference to the ﬁrst song of the pair being compared, (ii) a reference to the second song of the pair, (iii) a reference to the deﬁnition of a similarity function that maps to any pair of songs to a similarity value, and (iv) the similarity value reﬂecting how similar the two songs are. If two songs are very different, a value close to 0 should be used, if they are very similar, a value close to 1 should be used instead. To keep the scenario as generic as possible, very few assumptions are made about the properties of the functions used to compute the similarity values. In particular, the similarity functions do not have to fulﬁll the mathematical properties of a metric, e.g., the non-negativity, the identity of indiscernibles, the triangular inequality, and the symmetry properties. 3 AN ALGEBRA FOR FUZZY SONG SETS In this section, we introduce song sets as well as operators and functions to manipulate them. Let X be the set of all songs. Then, a fuzzy song set, A, is a fuzzy set deﬁned over X such that A = {µA(x)/x : x ∈X, µA(x) ∈[0, 1]} (1) and is deﬁned as a set of pairs µA(x)/x, where x is a song, µA(x), referred to as the membership degree of x, is a real number belonging to [0, 1], and / denotes the association of the two values as commonly expressed in the fuzzy logic literature [3]. µA(x) = 0 when song x does not belong to A, and µA(x) = 1 when x completely belongs to A.",
        "zenodo_id": 1415770,
        "dblp_key": "conf/ismir/DeliegeP07"
    },
    {
        "title": "The Probado Music Repository at the Bavarian State Library.",
        "author": [
            "Jürgen Diet",
            "Frank Kurth"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417527",
        "url": "https://doi.org/10.5281/zenodo.1417527",
        "ee": "https://zenodo.org/records/1417527/files/DietK07.pdf",
        "abstract": "In this paper, we describe the Probado music repository which is currently set up at the Bavarian State Library, Munich, as part of the larger German Probado digital library initiative. Based on the FRBR approach, we propose a novel work-centric metadata model for organizing the document collection. The primary data contained in the repository currently consists of scanned sheet music and digitized audio recordings. The repository can be searched using both classical and content-based retrieval mechanisms. To this end, we propose a workflow for automated content-based document analysis and indexing.",
        "zenodo_id": 1417527,
        "dblp_key": "conf/ismir/DietK07"
    },
    {
        "title": "Music Recommendation Mapping and Interface Based on Structural Network Entropy.",
        "author": [
            "Justin Donaldson",
            "Ian Knopke"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417329",
        "url": "https://doi.org/10.5281/zenodo.1417329",
        "ee": "https://zenodo.org/records/1417329/files/DonaldsonK07.pdf",
        "abstract": "Recommendation systems generally produce the results of their output to their users in the form of an ordinal list. In the interest of simplicity, these lists are often obscure, measured strength of the recommendations or the relationships the recommended items share with each other. This information is often useful for coming to a better understanding of the nature of how the items are structured according to the recommendation data. This paper describes the ZMDS algorithm, a novel way of analyzing the fundamental network structure of recommendation results. Furthermore, it also describes a dynamic plot interaction method as a recommendation browsing utility. A novel “Recommendation Map” web application implements both the ZMDS algorithm and the plot interface and are offered as an example of both components working together. 1 INTRODUCTION Item based recommendation systems such as the music recommendation services offered by MyStrands 1 are based on networks of associations between items. These associations are formed from aggregate observations or records of sets that occur between the items. In the context of the MyStrands music recommendation network, these sets are playlists. A network can be constructed from this data where each item is a node and the connection weights between the items are the number of times that the two items appear in any set. Recommendation for music in this context involves providing one or more songs as “input” into the system. The system resolves the songs against its list of known songs, and then retrieves a set of recommended songs that have strong associations for the provided songs. For the purposes of this project, a dataset of 1.6 million songs from the MyStrands recommendation database was used. The data was constructed from hundreds of thousands of playlists, which in turn formed millions of links between the individual songs. Recommendation results were retrieved through a modiﬁed network neighborhood 1 http://www.mystrands.com c⃝2007 Austrian Computer Society (OCG). extraction routine. This process resolves the input songs in the network, and then retrieves each of the direct neighbors of these songs. 2 MUSIC NETWORK VISUALIZATIONS Since the recommendation data is a network, it is possible to visualize the prominent structure of the network in two or three dimensions using conventional network visualization techniques. Many recent projects have attempted to visualize music relationships in a low dimensional space according to association or content based similarity metrics [2, 1]. Visualizations such as these are extremely valuable for understanding the various relationships in music. 3 ZMDS AND STRUCTURAL NETWORK ENTROPY ZMDS is a novel modiﬁcation of the standard Euclidean distance based multidimensional scaling method. It involves the following steps: (1) Construction of a modiﬁed association matrix from the recommendation result, (2) row-wise z-score normalization of node edge weights (The Z in ZMDS is taken from this step.), (3) row-wise Euclidean distance calculation for dissimilarity matrix, and (4) classic multidimensional scaling method on dissimilarity matrix for required number of dimensions. According to this method, in step 1 a matrix of associations from a recommendation result is constructed. This matrix is similar to a Laplacian matrix, where each nondiagonal entry is the association weight between two of the songs in the recommendation result, and each diagonal entry is the global edge weight for the song. Each song’s connection weights in the matrix are modiﬁed by a function of the songs total participation in the network in step 2. In order to view the resulting structure in Euclidean space, it is necessary to convert the asymmetric weighted matrix into a symmetric version. This can be done by calculating row-wise Euclidean distances and constructing a symmetric dissimilarity matrix. Applying a multidimensional reduction algorithm on the dissimilarity matrix will provide a low dimensional representation ﬁt for visualization. A z-score normalization method is preferred which uses variance (unit standard deviation) as Figure 1. Artist, genre, and popularity clustering in ZMDS graph the basis for edge weighting: wi,j = ki,j −¯k σ (1) With wi,j being the node weight between nodes i and j, ki,j , being the co-occurrence weight, and σ being the standard deviation for row k. By encoding the size of the node by its global edge degree, it is clear to see how the large hub-pop songs are marginalized on the left side of the distribution. The right side of the distribution describes the two tail structures for the recommendation set. In the context of the recommendation set retrieved, these tails correspond to songs by the artist Bruce Springsteen and Tori Amos (See Figure 1). These artists have long histories and loyal fan bases. As a result, they have a catalog of songs that share more intracatalog edges than extra-catalog edges. However, the associations do not form a cluster in the traditional sense. Instead, the tail structures can be thought of as a “gradient” cluster feature. Nodes along the ends of the tails will include regions of songs that are less widely popular for a given artist. However, while most of these songs are only associated with other Springsteen and Amos songs, there are songs that act as bridges or gateways into the rest of the music network. Logically, these songs are the most widely popular songs for both artists. Near the center of the distribution where these tails start is the “low entropy hub” which ties together the tails into the larger high entropy fan region on the left. It is important to note that one or more of these features may be missing from a ZMDS distribution. If there is no signiﬁcant recommendation set structure, there will be no tails. Likewise, low entropy hubs may be missing as well. 4 INTERACTIVE VISUALIZATION A node repulsion technique was used to handle the node occlusion that occurs in such representations. This technique is very similar to a class of techniques collectively known as liquid browsing [3]. The motivation for the interactive implementation is to allow a user to recognize Figure 2. Recommendation mapping applet and investigate occlusion as it occurs in the low dimensional representation, as well as to retrieve non-dimensional information about the node, such as artist or track title. The visual interface was created in Macromedia Flash as a lightweight web application, suitable for visualizing and investigating music recommendation sets of around two hundred elements. The actual ZMDS process is performed using the PDL matrix data language 2 , and passed to the Flash applet as an XML data set (See Figure 2). The overall effect of this behavior is easy to perceive in the context of the web interface itself, which is available online 3 . 5 REFERENCES [1] F. J. Igo, M. Brand, K. Wittenburg, D. Wong, and S. Azuma. Multidimensional visualization for collaborative ﬁltering recommender systems, 2002. [2] E. Pampalk. Islands of music: Analysis, organization, and visualization of music archives. Journal of the Austrian Soc. for Artiﬁcial Intelligence, 22:20–23, 2003. [3] C. Waldeck, D. Balfanz, C. G. Center, and G. ZGDV. Mobile liquid 2d scatter space (ml2dss). Information Visualisation, 2004. IV 2004. Proceedings. Eighth International Conference on, pages 494–498, 2004. 2 PDL: The Perl Data Language: http://pdl.perl.org/ 3 Recommendation Mapping Applet: http://labs.mystrands.com/cgibin/recmap.cgi",
        "zenodo_id": 1417329,
        "dblp_key": "conf/ismir/DonaldsonK07"
    },
    {
        "title": "Tuning Frequency Estimation Using Circular Statistics.",
        "author": [
            "Karin Dressler",
            "Sebastian Streich"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416294",
        "url": "https://doi.org/10.5281/zenodo.1416294",
        "ee": "https://zenodo.org/records/1416294/files/DresslerS07.pdf",
        "abstract": "In this document a new approach on tuning frequency estimation based on circular statistics is presented. Two",
        "zenodo_id": 1416294,
        "dblp_key": "conf/ismir/DresslerS07"
    },
    {
        "title": "Towards Query by Singing/Humming on Audio Databases.",
        "author": [
            "Alexander Duda",
            "Andreas Nürnberger",
            "Sebastian Stober"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414804",
        "url": "https://doi.org/10.5281/zenodo.1414804",
        "ee": "https://zenodo.org/records/1414804/files/DudaNS07.pdf",
        "abstract": "Current work on Query-by-Singing/Humming (QBSH) focusses mainly on databases that contain MIDI ﬁles. Here, we present an approach that works on real audio recordings that bring up additional challenges. To tackle the problem of extracting the melody of the lead vocals from recordings, we introduce a method inspired by the popular “karaoke effect” exploiting information about the spatial arrangement of voices and instruments in the stereo mix. The extracted signal time series are aggregated into symbolic strings preserving the local approximated values of a feature and revealing higher-level context patterns. This allows distance measures for string pattern matching to be applied in the matching process. A series of experiments are conducted to assess the discrimination and robustness of this representation. They show that the proposed approach provides a viable baseline for further development and point out several possibilities for improvement. 1 INTRODUCTION Query-by-Singing/Humming (QBSH) as introduced in [6] is a popular content-based music retrieval method where the user enters a search query by singing, humming, or whistling it into a microphone. So far, work on QBSH systems has mainly focused on databases containing pieces of music in a symbolic description, usually MIDI. We describe an approach for QBSH on audio recordings instead. From these recordings, descriptive features are extracted and aggregated in symbolic strings which allow using distance measures for string pattern matching. However, music in general consists of several instruments or voices playing harmonically or in opposition to each other at the same time. In MIDI, each instrument usually has its own track, allowing straightforward separation of the individual voices. In real audio recordings, however, audio information of all instruments and voices is mixed and stored in all channels. Nevertheless, users of a QBSH system usually want to query the melody sung by the lead voice or played by a solo instrument. Consequently, the recordings need to be reduced to a somewhat more precise representation of components related to melody or lyrics. Before we introduce our methodology in Section 3, we c⃝2007 Austrian Computer Society (OCG). brieﬂy discuss related work. Section 4 describes the experiments conducted to evaluate our approach. The results of the experiments are presented in Section 5. 2 RELATED WORK In [17] a retrieval method for audio is proposed that restricts the frequency range to 22 semitones. Furthermore, the songs need to be manually segmented into semantically meaningful phrases. The authors argue that usually a query starts at the beginning of such a phrase and thus conﬁne query comparison to the beginning of phrases without further local alignment. We make a similar assumption to improve performance but matching is not restricted to that case. In [14] an automated way for segmentation is presented using a normal CD recording as well as a karaoke track of the same song, which is usually not available and thus, we avoid using such additional information. Instead, we try to infer higher-level representations directly. Current techniques of melody extraction from polyphonic recordings as [3, 5, 13] are still vulnerable to interferences from instruments. Here, source-separation approaches such as [4, 15] could help but still have many limitations: The method proposed in [15] works well on all single-note harmonic instruments including voice but has problems when drums are present or multiple instruments play the same note or an octave. The approach presented in [4] only works for vocals and up to three voices. In contrast to these approaches, we do not aim to achieve a perfect separation into individual voices. Instead, we are mainly interested in characteristics that are reproducable by a human singer. In this work we focus on the lead vocals or solo instruments. 3 METHODOLOGY",
        "zenodo_id": 1414804,
        "dblp_key": "conf/ismir/DudaNS07"
    },
    {
        "title": "Autotagging Music Using Supervised Machine Learning.",
        "author": [
            "Douglas Eck",
            "Thierry Bertin-Mahieux",
            "Paul Lamere"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417869",
        "url": "https://doi.org/10.5281/zenodo.1417869",
        "ee": "https://zenodo.org/records/1417869/files/EckBL07.pdf",
        "abstract": "Social tags are an important component of “Web2.0” music recommendation websites. In this paper we propose a method for predicting social tags using audio features and supervised learning. These automatically-generated tags (or “autotags”) can furnish information about music that is untagged or poorly tagged. The tags can also serve to smooth the tag space from which similarities and recommendations are made by providing a set of comparable baseline tags for all tracks in a recommender system. 1 INTRODUCTION In this paper we investigate the automatic generation of tags with properties similar to those generated by social taggers. Speciﬁcally we introduce a machine learning algorithm that takes as input acoustic features and predicts social tags mined from the web (in our case, Last.fm). The model can then be used to tag new or otherwise untagged music, thus providing a (partial) solution to the cold-start problem. We believe these autotags might also serve to dampen feedback loops which occur when certain songs in a social recommender become over-popular and thus over-tagged. Recently, there has been increasing interest in social tagging including the social tagging of music. Music tagging sites such as QLoud (www.qloud.com) and Last.fm (www.last.fm) and allow music listeners to apply free-text labels (tags) to songs, albums or artists. The real strength of a tagging system is seen when the tags of many users are aggregated. When the tags created by thousands of different listeners are combined, a rich and complex view of the song or artist emerges. Table 1 show the top 20 tags and frequencies of tags applied to the band “The Shins.” From these tags and their frequencies we learn much more about “The Shins” than we would from a traditional single genre assignment of “Indie Rock”. Additionally, in previous work [3] it was shown that social tags (in this case from the freedb CD track listing service at www.freedb.org) can predict canonical music-industry genre with good accuracy. Thus we lose little and gain a lot by moving from genres to tags. c⃝2007 Austrian Computer Society (OCG). Tag Freq Tag Freq Indie 2375 Mellow 85 Indie rock 1138 Folk 85 Indie pop 841 Alternative rock 83 Alternative 653 Acoustic 54 Rock 512 Punk 49 Seen Live 298 Chill 45 Pop 231 Singer-songwriter 41 The Shins 190 Garden State 39 Favorites 138 Favorite 37 Emo 113 Electronic 36 Table 1. Top 20 tags applied to The Shins 2 AN AUTOTAGGING ALGORITHM We now describe a machine learning model which uses the meta-learning algorithm AdaBoost [4] to predict tags from acoustic features. This model is an extension of a previous model [2] which performed well at predicting music attributes from acoustic features: at MIREX 2005 (ISMIR conference, London, 2005) the model won the Genre Prediction Contest and was the 2nd place performer in the Artist Identiﬁcation Contest. The model has two principal advantages. First it performs automatic feature selection based on a feature’s ability to minimize empirical error. Thus we can use the model to eliminate useless feature sets. Second, it’s performance is linear in the number of inputs. Thus it has the potential to scale well to large datasets. Both of these properties are general to AdaBoost and are not explored further in this short paper. See [4] for more. Acoustic feature extraction: We obtained MP3s from a subset of the tagged artists described above. From these MP3s we extracted several popular acoustic features. Due to space limitations, we do not cover feature extraction in depth here. Please see [2] for details. The features used included 20 Mel-Frequency Cepstral Coefﬁcients, 176 autocorrelation coefﬁcients computed for lags spanning from 250msec to 2000msec at 10ms intervals, and 85 spectrogram coefﬁcients sampled by constant-Q (or log-scaled) frequency. We also tried 12 chromagram coefﬁcients but discarded them because they contributed very little to the ﬁnal result. For those not familiar with these standard acoustic features, please see [5]. The features were extracted with high temporal precision to preserve spectral and timbral information. Following the strategy of [2] coarser “aggregate” features were generated by taking means and standard deviations of high-temporal precision features over longer timescales, here 5 sec. Tagging as a classiﬁcation problem: Intuitively, automatic labeling would be a regression task where a learner would try to predict tag frequencies for artists or songs. However, because tags are sparse (many artist are not tagged at all) this proves to be too difﬁcult using our current Last.fm / Audioscrobbler dataset. Instead we chose to treat the task as a classiﬁcation one. Speciﬁcally, for each tag we try to predict if a particular artist has “none”, “some” or “a lot” of a particular tag relative to other tags. We label training examples as being in one of these three classes based on the relative number of times that tag has been applied. Tag prediction with AdaBoost: Using MultiBoost.MH a booster is trained to predict the tag (“none”, “some”, “a lot”) directly from the aggregate feature values. The value for a song is taken by voting over the predictions for each aggregate feature. Voting can take place in two ways: we can choose segment winners and then select as global winner the class receiving the most segment votes or we can sum the weak learner values over segments and then take the class with the maximum sum. 3 EXPERIMENTS To test our model we extracted tags and tag frequencies for more than 50, 000 artists from the social music website Last.fm using the Audioscrobbler web service [1]. From the full set of tags we selected 13 tags corresponding to popular genres. We selected these particular tags to be relatively easy to analyze (i.e. it’s not clear how to analyze the performance of a predictor of “fun” or “mellow”). Results: We compare our results against a baseline computed using the one-versus-all boosted model from [2]. Unlike our current approach, this model assumes that one and only one tag can be applied to a single song. In order to train and test the model we needed to select a winner. We simply chose the most frequent tag. Mean classiﬁcation error rate over all classes except Classical was 42% (std=2.22) error by segment and 39% (std=2.32) by song. The classical was not counted because only two signiﬁcant classes “some/all” and “none” could be generated using available data, yielding error of 13%. These results compare favorably to the baseline one-versus-all results of 62% error by segment and 59% error by song. As an example of model performance see Table 2 where we compare the nearest neighbors for our predicted tags to those for the original tags. In general it seems that our predicted tags are comparable in quality to the original tags. That is, our tags have some surprising errors (Marvin Gaye as a near neighbor to the Beatles?) yet so do the original tags (John Williams as a near neighbor to Mozart?). Overall, these preliminary results suggest that autotagging helps solve the cold start problem seen in social-tag-based music recommenders. Near-neighbor artists Seed Artist Last.fm Tags Our Prediction The Prodigy Chemical Brothers Fatboy Slim Basement Jaxx Apollo 440 Apollo 440 Beck John Lennon Eric Clapton The Beatles The Beach Boys Marvin Gaye The Doors The Rolling Stones Bach Schubert Mozart Beethoven Haydn John Williams Brahms Table 2. A comparison of 3 nearest neighbors for Last.fm tags versus our model predictions. Euclidean distance was used. 4 CONCLUSIONS With these preliminary results conclude that a supervised learning approach to autotagging has merit. Our predictions are noisy and lead to sometimes-counterintuitive near artist predictions. However social tags themselves share these properties. There is much future work to do. One next step (already underway) is to learn a much larger number of tags and combine them using a second stage of learning for similarity prediction. A second step is to compare the performance of our boosted model to other approaches such as SVMs and neural networks. Most importantly, the machine-generated autotags need to be tested in a social recommender. It is only in such a context that we can explore whether autotags, when blended with real social tags, will in fact yield improved recommendations. 5 REFERENCES [1] Audioscrobbler. Web Services described at http://www.audioscrobbler.net/data/webservices/. [2] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and B. K´egl. Aggregate features and AdaBoost for music classiﬁcation. Machine Learning, 65(2-3):473–484, 2006. [3] J. Bergstra, A. Lacoste, and D. Eck. Predicting genre labels for artists using freedb. In Proceedings of the 7th International Conference on Music Information Retrieval (ISMIR 2006), 2006. [4] Y. Freund and R.E. Shapire. Experiments with a new boosting algorithm. In Machine Learning: Proceedings of the Thirteenth International Conference, pages 148–156, 1996. [5] B. Gold and N. Morgan. Speech and Audio Signal Processing: Processing and Perception of Speech and Music. Wiley, Berkeley, California., 2000.",
        "zenodo_id": 1417869,
        "dblp_key": "conf/ismir/EckBL07"
    },
    {
        "title": "The Music Information Retrieval Evaluation Exchange &quot;Do-It-Yourself&quot; Web Service.",
        "author": [
            "Andreas F. Ehmann",
            "J. Stephen Downie",
            "M. Cameron Jones"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417949",
        "url": "https://doi.org/10.5281/zenodo.1417949",
        "ee": "https://zenodo.org/records/1417949/files/EhmannDJ07.pdf",
        "abstract": "The Do-It-Yourself (DIY) web service of the Music Information Retrieval Evaluation eXchange (MIREX) represents a means by which researchers can remotely submit, execute, and evaluate their Music Information Retrieval (MIR) algorithms against standardized datasets that are not otherwise freely distributable. Since its inception in 2005 at the International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL), MIREX has, to date, required heavy interaction by IMIRSEL team members in the execution, debugging, and validation of submitted code. The goal of the MIREX DIY web service is to put such responsibilities squarely into the hands of submitters, and also enable the evaluations of algorithms yearround, as opposed to annual exchanges.",
        "zenodo_id": 1417949,
        "dblp_key": "conf/ismir/EhmannDJ07"
    },
    {
        "title": "Classifying Music Audio with Timbral and Chroma Features.",
        "author": [
            "Daniel P. W. Ellis"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416906",
        "url": "https://doi.org/10.5281/zenodo.1416906",
        "ee": "https://zenodo.org/records/1416906/files/Ellis07.pdf",
        "abstract": "Music audio classiﬁcation has most often been addressed by modeling the statistics of broad spectral features, which, by design, exclude pitch information and reﬂect mainly instrumentation. We investigate using instead beat-synchronous chroma features, designed to reﬂect melodic and harmonic content and be invariant to instrumentation. Chroma features are less informative for classes such as artist, but contain information that is almost entirely independent of the spectral features, and hence the two can be proﬁtably combined: Using a simple Gaussian classiﬁer on a 20-way pop music artist identiﬁcation task, we achieve 54% accuracy with MFCCs, 30% with chroma vectors, and 57% by combining the two. All the data and Matlab code to obtain these results are available. 1 1 INTRODUCTION Classifying music audio (for instance by genre or artist) has been most successful when using coarse spectral features (e.g. Mel-Frequency Cepstral Coefﬁcients or MFCCs [3]), which has been dubbed “timbral similarity” [1]. Such features reﬂect mainly the instruments and arrangements of the music rather than the melodic or harmonic content. Although a wide range of more musically-relevant features has been proposed, they have rarely afforded much improvement on the overall system performance. This paper describes the results of using beat-synchronous chroma features in place of frame-level MFCCs in statistical classiﬁcation of artist identiﬁcation. This feature representation was developed for matching “cover songs” (alternative performances of the same musical piece), and thus reﬂects harmonic and melodic content with the minimum of inﬂuence from instrumentation and tempo [2]. Chroma features consist of a twelve-element vector with each dimension representing the intensity associated with a particular semitone, regardless of octave; our implementation uses instantaneous frequency to improve frequency resolution and rejection of non-tonal energy. By storing just one chroma vector for each beat-length segment of the 1 http://labrosa.ee.columbia.edu/projects/ timbrechroma/ c⃝2007 Austrian Computer Society (OCG). original audio (as determined by a beat tracker), the representation is both compact and somewhat tempo invariant – yet still sufﬁcient, since chroma content rarely changes within a single beat, provided the beats are chosen near the bottom of the metrical hierarchy. Using chroma features directly in place of MFCCs means that we are looking only at the global distribution of chroma use within each piece, and assuming this is somewhat consistent within classes. A chroma vector reﬂects all current notes and is thus roughly correlated with a particular chord (such as Bmin7). Thus, the kind of regularity that such a model might capture would be if a given artist was particularly fond of a certain subset of chords. The model in this form would not learn characteristic chord sequences, and would also be defeated by simple transposition. To overcome these problems we tried (a) modeling sequences of chroma vectors for several beats, and (b) using a key-normalization front-end that attempts to align every piece to a common chroma transposition. Although we believe that beat-chroma distribution models can capture something speciﬁc about individual composition style, we do not expect them to perform as well as timbral features, since artists are likely to vary their melodic-harmonic palette more readily than their instrumentation. However, we expect the information from these two sources to be complementary, since harmonic “signatures”, to the extent they exist, have no reason to be associated with instrumentation. For this reason, we experiment with fusing the two results in ﬁnal classiﬁcation. 2 CLASSIFIER We adopt an artist identiﬁcation task to illustrate the utility of beat-chroma features. We opt for the simple approach of ﬁtting the distributions of random subsets of pooled frame-level features for each artist with either a single full-covariance Gaussian, or a mixture of diagonalcovariance Gaussians. Classiﬁcation of an unknown track is achieved by ﬁnding the model that gives the best total likelihood for a random subset of the features from that track (treated as independent). Although previous work has shown that the SVM classiﬁer applied to track-level features is a more powerful approach to this problem [3], our interest here is in comparing the usefulness of the different features. The simple and quick Gaussian models are adequate for this purpose. 3 KEY NORMALIZATION The goal of key normalization was to ﬁnd a per-track rotation of the circular, 12-bin chroma representation that made all the data within each training class as similar as possible. To do this we ﬁrst ﬁt a single, full-covariance Gaussian to the chroma representation of ﬁrst track in the class. Then the likelihood of each subsequent track was evaluated using this model under each of the 12 possible chroma rotations; the most likely rotation was retained. After one pass through the entire set, a new Gaussian was ﬁt to all the tracks after applying the best rotations from the ﬁrst pass. The search for the best rotations was repeated based on this new model, and the process iterated until no changes in rotations occurred. The ﬁnal, global model was also used on test tracks to choose the best rotation for them prior to classiﬁcation. 4 DATA We collected and used a set of 1412 tracks, composed of six albums from each of 20 artists. It is based on the 18 artist set used on [3] (drawn from “uspop2002”) with some additions and enhancements. We used 6-fold testing, with ﬁve albums from each artist used for training and one for testing in each fold. For this test set, statistical signiﬁcance at 5% requires a difference of just over 3% in classiﬁcation accuracy (one-tailed binomial). Guessing the most common class gives a baseline accuracy of 6%. 5 EXPERIMENTS For both MFCC and beat-chroma features, we experimented with varying the number of frames used to train and test the models, and the number of Gaussians used to model the distributions. We used 20 MFCC coefﬁcients including the zero’th, based on a 20-bin Mel spectrum extending to 8 kHz. For the beat-chroma features, we varied the number of temporally-adjacent frames modeled from 1 to 4, and with using key normalization. The results are summarized in table 1. We notice that MFCCs are intrinsically more useful than chroma features (as expected, since the instrumentation captured by MFCCs is well correlated with artist), that Gaussian mixture models are preferable for the chroma features (which are likely to be multimodal) but not for MFCCs, that key normalization gives a small but signiﬁcant improvement, but concatenating multiple beats into a single feature vector does not. Fusing the best MFCC and Chroma systems (shown in bold in the table) based on a weighted sum of separate model likelihoods tuned on a small tuning subset, we see a substantial but statistically insigniﬁcant improvement that results from including chroma information. 6 CONCLUSIONS We have shown that the simple approach of modeling the distribution of per-beat chroma vectors very much as cepFeature Model T win Acc Exec time MFCC20 FullCov 1 54% 181 s MFCC20 64 GMM 1 54% 1282 s Chroma FullCov 1 15% 31 s Chroma FullCov 4 15% 50 s Chroma 64GMM 1 25% 581 s Chroma 64GMM 4 16% 1501 s ChromaKN FullCov 1 23% 113 s ChromaKN FullCov 4 23% 255 s ChromaKN 64GMM 1 30% 957 s ChromaKN 64GMM 4 23% 1717 s MFCC + Chroma fusion 57% Table 1. Artist identiﬁcation accuracy. “T win” is the size of temporal context window. “FullCov” designates single, full-covariance Gaussian models. “ChromaKN” refers to per-track key-normalized chroma data. Execution times are per fold on a 2.8 GHz Xeon CPU. stral vectors have been modeled in the past is a viable approach for artist identiﬁcation. Rotating the chroma vectors in order to transpose all pieces to a common tonal framework is necessary to realize the full beneﬁts. Our future plans are to pursue the idea of modeling small fragments of beat-chroma representation to identify the most distinctive and discriminative fragments characteristic of each composer/artist. 7 ACKNOWLEDGEMENTS This work was supported by the Columbia Academic Quality Fund, and by the NSF under Grant No. IIS-0238301. Any opinions, ﬁndings, etc. are those of the authors and do not necessarily reﬂect the views of the NSF. 8 REFERENCES [1] Jean-Julien Aucouturier and Francois Pachet. Music similarity measures: What’s the use? In Proc. 3rd International Symposium on Music Information Retrieval ISMIR, Paris, 2002. [2] D. P. W. Ellis and G. Poliner. Identifying cover songs with chroma features and dynamic programming beat tracking. In Proc. ICASSP, pages IV–1429– 1432, Hawai’i, 2007. [3] M. I. Mandel and D. P. W. Ellis. Song-level features and support vector machines for music classiﬁcation. In Proc. International Conference on Music Information Retrieval ISMIR, pages 594–599, London, Sep 2005.",
        "zenodo_id": 1416906,
        "dblp_key": "conf/ismir/Ellis07"
    },
    {
        "title": "A Closer Look on Artist Filters for Musical Genre Classification.",
        "author": [
            "Arthur Flexer"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415668",
        "url": "https://doi.org/10.5281/zenodo.1415668",
        "ee": "https://zenodo.org/records/1415668/files/Flexer07.pdf",
        "abstract": "Musical genre classiﬁcation is the automatic classiﬁcation of audio signals into user deﬁned labels describing pieces of music. A problem inherent to genre classiﬁcation experiments in music information retrieval research is the use of songs from the same artist in both training and test sets. We show that this does not only lead to overoptimistic accuracy results but also selectively favours particular classiﬁcation approaches. The advantage of using models of songs rather than models of genres vanishes when applying an artist ﬁlter. The same holds true for the use of spectral features versus ﬂuctuation patterns for preprocessing of the audio ﬁles. 1 INTRODUCTION Music information retrieval (MIR) is the science of extracting information from music. Probably the most popular form of such information is musical genre (see [2] and [15] for comprehensive overviews). Genre information can be used to describe music in interpersonal communication, in publications about music as well as to structure music databases, libraries and music stores. Although musical genre is a somewhat poorly deﬁned concept, automation of the genre classiﬁcation process remains an important topic in MIR [9]. Besides being a goal in its own right, genre classiﬁcation results are often used as a means to quantify success in modelling musical similarity. A problem inherent to genre classiﬁcation experiments in MIR research is the use of songs from the same artist in both training and test sets. It can be argued that in such a scenario one is doing artist classiﬁcation rather than genre classiﬁcation. Speciﬁc mastering and production effects could also play a role in such a scenario. In [14] the use of a so-called “artist ﬁlter” ensuring that all songs from an artist are in either the training or the test set is proposed. The authors found that the use of such an artist ﬁlter can lower the classiﬁcation results quite considerably (with one of their music collection even from 71% down to 27%). These over-optimistic accuracy results due to not c⃝2007 Austrian Computer Society (OCG). using an artist ﬁlter have been conﬁrmed in other studies [12] [5]. In extending these results, we show that the failure to use an artist ﬁlter also selectively favours particular genre classiﬁcation approaches. In two genre classiﬁcation experiments we are able to show that: (i) the advantage of using models of songs rather than models of genres vanishes when applying an artist ﬁlter; (ii) the same holds true for the use of spectral features versus ﬂuctuation patterns for preprocessing of the audio ﬁles. 2 DATA For our experiments we used a data set of the ISMIR 2004 genre classiﬁcation contest. The data base consist of S = 729 songs from A = 128 artists belonging to G = 6 genres. The different genres plus the numbers of artists and songs belonging to each genre are given in Table 2. Genre No. artists No. songs % of songs Classical 40 320 43.9 Electronic 30 115 15.8 Jazz Blues 5 26",
        "zenodo_id": 1415668,
        "dblp_key": "conf/ismir/Flexer07"
    },
    {
        "title": "A Demonstration of the SyncPlayer System.",
        "author": [
            "Christian Fremerey",
            "Frank Kurth",
            "Meinard Müller",
            "Michael Clausen"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415586",
        "url": "https://doi.org/10.5281/zenodo.1415586",
        "ee": "https://zenodo.org/records/1415586/files/FremereyKMC07.pdf",
        "abstract": "The SyncPlayer system is an advanced audio player for multimodal presentation, browsing, and retrieval of music data. The system has been extended signiﬁcantly in the last few years. In this contribution, we describe the current state of the system and demonstrate the functionalities and interactions of the novel SyncPlayer components including combined interand intra-document music browsing. 1 SYNCPLAYER OVERVIEW The SyncPlayer is a client-server based software framework integrating various MIR-techniques such as music synchronization, content-based retrieval, and multimodal presentation of audio recordings and associated annotations [3]. The framework basically consists of three components: a server component, a client component, and a toolset for data administration (see also Figure 1): • The user operates the client component, which in its basic mode acts like a standard software audio player for *.mp3 and *.wav ﬁles. Additional interfaces, e.g., for performing content-based queries as well as various visualization tools, are provided through plug-ins (see Figure 2). • A remote computer system runs the server component, which is capable of identifying audio recordings played by the client and which supplies the client with metadata and annotations for those recordings. Furthermore, the server comprises several types of music search engines. • Several server-side administration tools are used for maintaining the databases and indexes underlying the SyncPlayer system. In a typical SyncPlayer scenario we assume that, on the server-side, there exists a large collection of music documents. Here, for each given piece of music, various digital representations (e. g., audio, MIDI, MusicXML, scanned images of sheet music) as well as associated metadata should be accessible by the server. In the following, this kind of data will be referred to as raw data. The raw data is further processed to generate what we refer to as derived c⃝2007 Austrian Computer Society (OCG). Raw Data Audio Data MIDI Data Lyrics Metadata · · · Derived Data Features Annotations Alignment Data · · · SyncPlayer Framework Administration Data Conversion Annotation & Data Management Search Indexes · · · Server Audio Identiﬁcation Delivery of Metadata & Annotations Search Engines (e. g. Lyrics, MIDI, Score, Metadata) · · · Client Audio Playback Visualization Navigation Search Interfaces · · · | {z } | {z } Ofﬂine (Preprocessing) Online (Runtime) Figure 1. Overview of the SyncPlayer framework. data. The derived data comprises high-level audio features, various kinds of synchronization and linking data, or structural data, which reveals musically relevant characteristics as well as existing relations within the underlying raw data. Such data may be generated efﬁciently in a purely automatic fashion by means of MIR techniques. Other types of derived data may include textual annotations of audio recordings aligning the lyrics to a corresponding recorded song or synchronization data linking scanned sheet music with a corresponding audio recording. Using the SyncPlayer administration tools, the raw data as well as the derived data are indexed and stored in databases, which can then be efﬁciently accessed by the SyncPlayer server. The generation of the derived data as well as the data organization and indexing can be done ofﬂine in some preprocessing step (Figure 1). For further technical details concerning the data administration and the SyncPlayer implementation, we refer to [2, 3]. The SyncPlayer framework offers two basic modes for accessing audio documents and corresponding contentbased data such as annotations. First, a user operating the client system may choose locally available audio recordings for playback. The client then extracts features from the audio recordings which are sent to the remote SyncPlayer server. The server subsequently attempts to identify the audio recording based on the submitted features. Upon success, the server searches its database for available annotations (such as lyrics or notes) which are then sent back to the client. The client system offers the user several visualization types for the available annotations. Examples are indicated by Figure 2, which shows from top to bottom the SyncPlayer client, a visualization plug-in for a karaokelike display for lyrics information, a piano-roll style display for note (MIDI) information, and a display for the repetitive audio structure. Further plug-ins are available for displaying the waveform, the spectogram, or scanned Figure 2. The SyncPlayer with three visualization plugins showing different annotations (lyrics, piano roll, structure) for Schubert’s Winterreise D911 No. 11. images of sheet music synchronously to audio playback. The plug-in displaying the audio structure allows the user to instantly switch between blocks of musically similar content within a single music document. For example, musically similar blocks might be repetitions or variations of a musical theme. We call this application intradocument browsing because the user browses through particular blocks of a single music document. In addition, we have developed the Audio Switcher plug-in (see Figure 3), which offers inter-document browsing similar to the MATCH System by Dixon et al. [1]. It allows the user to open several synchronized interpretations of the same piece of music. The user may listen to one of the selected interpretations and then, at any time during playback, switch to another interpretation. The playback in the target interpretation will continue at the position that musically corresponds to the position inside the previously selected interpretation. The second method for accessing audio documents using the SyncPlayer is by means of appropriate query enFigure 3. The SyncPlayer on top of the Audio Switcher plug-in. Here, ﬁve different interpretations of Beethoven’s Fifth Symphony are selected in the Audio Switcher. gines. In this scenario, the user operates a query plugin offered by the client. Queries are submitted to the SyncPlayer server which, depending on the query type, schedules the queries to an appropriate query engine. A ranked list of retrieval results is returned to the client and displayed to the user. The user may then select particular query results for playback which are subsequently streamed from the server along with available annotation data. Currently, query engines for lyricsand melodybased retrieval exists for performing symbolic queries. For searching audio recordings, a query engine for audio matching [4] has been implemented. For detailed information on the SyncPlayer framework we refer to [2, 3]. A demo version of the SyncPlayer is available for download at the SyncPlayer Homepage [5]. 2 REFERENCES [1] S. DIXON AND G. WIDMER, MATCH: A Music Alignment Tool Chest, in Proc. ISMIR, London, GB, 2005. [2] C. FREMEREY, SyncPlayer – a Framework for ContentBased Music Navigation. Master Thesis, Dept. of Computer Science, University of Bonn, 2006. [3] F. KURTH, M. M ¨ULLER, D. DAMM, C. FREMEREY, A. RIBBROCK, AND M. CLAUSEN, Syncplayer—an advanced system for content-based audio access, in Proc. ISMIR, London, GB, 2005. [4] M. M ¨ULLER, F. KURTH, AND M. CLAUSEN, Audio matching via chroma-based statistical features, in Proc. ISMIR, London, GB, 2005. [5] SYNCPLAYER, An advanced system for multmodal music access. Website, January 2007. http://www-mmdb. iai.uni-bonn.de/projects/syncplayer.",
        "zenodo_id": 1415586,
        "dblp_key": "conf/ismir/FremereyKMC07"
    },
    {
        "title": "Visualizing Music on the Metrical Circle.",
        "author": [
            "Klaus Frieler"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416798",
        "url": "https://doi.org/10.5281/zenodo.1416798",
        "ee": "https://zenodo.org/records/1416798/files/Frieler07.pdf",
        "abstract": "In this paper we propose a novel method, called Metrical Circle Map, for exploring the cyclic aspects of musical time. To this end, we give a short formalization introducing the notion of Metrical Markov Chains as transition probabilities of segments on the metrical circle. As an illustration we present a compact visualization of the zerothand ﬁrst order metrical Markov transitions of 61 Irish folk songs. 1 INTRODUCTION One important and distinctive feature of metrically-bound music is the double nature of its musical time, linear on one hand, cyclic on the other. However, in most of musicological and other music-related research the focus is laid on linear aspects of musical time. The cyclic nature of musical time is mainly investigated in the context of genuine meter and rhythm research, where rhythm and meters are occasionally visualized or operationalized using a circle representation ([3], [4], [5]). In this paper, we will extend this approach by introducing the so-called Metrical Circle Map and Metrical Markov Chains, opening up several interesting posibilities of visualizing and analyzing metrically bound music, including single monophonic or polyphonic pieces as well as entire corpora. 2 METRICAL CIRCLE MAP The onsets of metrically-bound music are organized around underlying beats (or pulses), which are grouped into higher level units. This is even true for un-quantized music with tempo and meter variations, as for the concept of meter only a set of discrete time-points is needed along with a grouping prescription. We restrict ourselves here to the bar as the main metrical unit, and futhermore to the simplest form of a bar as a constantly-recurring time-span. For a rhythm conceived as a sequence of time-points ti, and a bar time T possibly inferred from the original sequence by some beat and meter induction algorithm (e.g. [2]), or given by annotation the Metrical Circle Map MT is deﬁned as a mapping from the reals into the complex unit circle S1: MT (ti) = zi = e2πi ti T (1) c⃝2007 Austrian Computer Society (OCG). Figure 1. Metrical circle map of two Irish melodies. The left one (Essen Folksong Collection, I0501) shows 2/4 time, the right one (Essen Folksong Collection, I0504) 3/4 time. Thickness of lines corresponds to the frequencies of the transitions. Time is running counter-clockwise and the downbeat is located an the point (1,0) in the complex plane (three o’clock) In this form we already normalized the map, so that time is running in the mathematical direction counter-clockwise and the beginning of bars always lie at three o’clock. In Figure 1 two simple examples of the Metrical Circle Map (MCM) for two Irish folk songs can be seen, one of them being in 2/4 and one in 3/4 time.",
        "zenodo_id": 1416798,
        "dblp_key": "conf/ismir/Frieler07"
    },
    {
        "title": "A Music Information Retrieval System Based on Singing Voice Timbre.",
        "author": [
            "Hiromasa Fujihara",
            "Masataka Goto"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416228",
        "url": "https://doi.org/10.5281/zenodo.1416228",
        "ee": "https://zenodo.org/records/1416228/files/FujiharaG07.pdf",
        "abstract": "We developed a music information retrieval system based on singing voice timbre, i.e., a system that can search for songs in a database that have similar vocal timbres. To achieve this, we developed a method for extracting feature vectors that represent characteristics of singing voices and calculating the vocal-timbre similarity between two songs by using a mutual information content of their feature vectors. We operated the system using 75 songs and conﬁrmed that the system worked appropriately. According to the results of a subjective experiment, 80% of subjects judged that compared with a conventional method using MFCC, our method ﬁnds more appropriate songs that have similar vocal timbres. 1 INTRODUCTION This paper describes a music information retrieval (MIR) system that searches for songs that have similar voice timbres of vocals to a query song presented by a user. By using this system, we can ﬁnd a song by using its musical content in addition to traditional bibliographic information. This kind of retrieval is called content-based MIR, and our system, which focuses on singing voices as content, falls into this category. There is a growing demand for such content-based MIR. Because of rapid and widespread diffusion of portable audio players and online music stores, many users can have an access to a large amount of music tracks and listen to any music they want anytime, anywhere. This trend has triggered a demand for an MIR that takes favorite songs from a large amount of music and uses them to discover new songs that the user has never heard before. When the query of the target song is not known and only vague information such as ”preference” is available, the conventional method of searching for music that only uses bibliographic information is useless. Although a number of studies on content-based MIR have been undertaken [2, 8, 1, 3, 9, 12, 10, 4, 11], they use low-level acoustic features, such as MFCC, spectral centroid, rolloff, and ﬂux, for expressing musical content and do not use higher-level features such as the timbre of vocals. We therefore developed an MIR system that can retrieve songs by using vocal timbres. To achieve this sysc⃝2007 Austrian Computer Society (OCG). Vocal analysis Similarity calculation Vocal feature vectors Database construction Operation Query Retrieval result Database Target songs Vocal feature vectors Vocal feature vectors (Ranked list of songs) Vocal analysis Vocal analysis Vocal analysis Figure 1. Overview of our MIR system. tem, we have to extract, from the polyphonic audio signal of a song, vocal-based feature vector that represent the vocal characteristics and to calculate the vocal-timbre similarity between two songs by using the feature vectors. The vocal-based feature vectors were also used in our singer identiﬁcation method proposed earlier [5]. We used the mutual information content as their similarity measure. 2 ARCHITECTURE OF THE SYSTEM Among many songs registered in a database, our system searches for songs that have similar singing voice timbres to a song query given by a user. The overview of this system is shown in Figure 1. The system consists of a database construction (audio analysis) part and an operation (song retrieval) part. In the database construction part, target songs are stored in the database after being ripped or downloaded, and then each song is analyzed to extract feature vectors that represent vocal characteristics. When the user enters a (favorite) song as a query to the system, the system analyses the query song and extracts feature vectors that also represent the query’s vocal characteristics. The system then calculates the vocal-timbre similarity between the query song and each song in the database and shows the list of songs with high similarity. 3 IMPLEMENTATION OF THE SYSTEM To implement the MIR system described in Section 2, we have to deﬁne the vocal-based feature vector and the vocal-timbre similarity measure.",
        "zenodo_id": 1416228,
        "dblp_key": "conf/ismir/FujiharaG07"
    },
    {
        "title": "Using Pitch Stability Among a Group of Aligned Query Melodies to Retrieve Unidentified Variant Melodies.",
        "author": [
            "Jörg Garbers",
            "Peter van Kranenburg",
            "Anja Volk",
            "Frans Wiering",
            "Remco C. Veltkamp",
            "Louis P. Grijp"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418191",
        "url": "https://doi.org/10.5281/zenodo.1418191",
        "ee": "https://zenodo.org/records/1418191/files/GarbersKVWVG07.pdf",
        "abstract": "Melody identiﬁcation is an important task in folk song variation research. In this paper we develop methods and tools that support researchers in ﬁnding melodies in a database that belong to the same variant group as a set of given melodies. The basic approach is to derive from the pitches of the known variants per onset a weighted pitch distribution, which quantiﬁes pitch stability. We allow for partial matching and AND and OR queries. Technically we do so by deﬁning a distance measure between weighted pitch distribution sequences. It is based on two applications of the Earth Mover’s Distance, which is a distribution distance. We set up a distance framework and discuss musically meaningful parameterizations for two tasks: a) Study the inner-group distances between the group as a whole and single members of the group. b) Use the group’s weighted pitch distribution sequence to query for variant melodies. The ﬁrst experimental results seem very promising: a) The inner-group distances correlate to expert assigned subgroups. b) For variant retrieval our method works better than last year’s MIREX winner. 1 INTRODUCTION In folk song variation research, collection items are associated with each other and grouped by different methods. These include text analysis, meta-information analysis and score-analysis. In the WITCHCRAFT project (What Is Topical in Cultural Heritage: Content-based Retrieval Among Folksong Tunes) we try to aid this process with computational methods based on the musical content. Our initial approach was to use a melody query in – ranked melodies out (short: M2M, Melody to Melody) search engine, based on a transportation distance (EMD) in the real valued onset-pitch domain. The actual M2M system, described in [7], proved to be the most effective retrieval system in the 2006 MIREX competition. We hoped that the result list would inspire folk song researchers by presenting items of the collection in new, similarity driven orderings. However, the users were rather disturbed by the amount of false positives (items c⃝2007 Austrian Computer Society (OCG). on top of the ranking list that should not be there), so we tried to ﬁnd more informed ways to query the database. An important task in folksong research is the grouping of related melodies, generally because they might derive from a common melodic ancestor. To do so, one employs features that are stable accross sets of melodies. However, already in 1951, Bronson [1] pointed out, that ﬁnding and describing these stable aspects can be musicologically quite challenging. He proposes to use punch cards and a sorting machine to quantify stability and to describe the relationships between variants. In a similar approach [2] we have developed computer aided methods and tools to study pitch and harmonic stability. In the current paper we follow up on this approach by making use of the by-products of such kind of studies, namely sets of (manually) aligned melodies: we formally develop the idea to query a database for unclassiﬁed variants with a group of known melodic variants (short: G2M, Group to Melody). In this paper we assume that a set of melodic variants or variant phrases is aligned with respect to the temporal axis and that musically reasonable transformations of the material have been applied. The latter includes translations to a common key, selection of suitable bars and reduction to a particular metrical level, as done in a semi-automatic way in [2]. For a given onset time we thus get the pitches and rests that occur in any of the melodies. By querying ual variations that resulted in the different melodies but still maintain stable pitch requirements. In section 2 we ﬁrst set up a mathematical and computational framework and deﬁne the mapping of musical notes to mathematical values. In section 3 we discuss musically reasonable parameterizations of the framework and in section 4 we conduct and evaluate computational experiments with selected parameterizations and compare the results with previous approaches. 2 THE COMPUTATIONAL FRAMEWORK This research builds on the Earth Mover’s Distance (EMD), whose musical application was studied in [7]. It is also an important building block in Typke’s M2M search engine, which we used in our ﬁrst approach to folk song variation retrieval. However, instead of combining the onset and pitch dimensions into one Euclidean ground distance, as done in Typke’s contour matching, we compute the EMD on the onset and pitch dimensions separately. In section 2.1 we introduce the reader into the parameters of the EMD. In the following sections we deﬁne musically and task speciﬁc values.",
        "zenodo_id": 1418191,
        "dblp_key": "conf/ismir/GarbersKVWVG07"
    },
    {
        "title": "A Symmetry Based Approach for Musical Tonality Analysis.",
        "author": [
            "Gabriel Gatzsche",
            "Markus Mehnert",
            "David Gatzsche",
            "Karlheinz Brandenburg"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416214",
        "url": "https://doi.org/10.5281/zenodo.1416214",
        "ee": "https://zenodo.org/records/1416214/files/GatzscheMGB07.pdf",
        "abstract": "We present a geometric approach for tonality analysis called symmetry model. To derive the symmetry model, Carol L. Krumhansl and E.J. Kessler’s toroidal Multi Dimensional Scaling (MDS) solution is separated into a key spanning and a key related component. While the key spanning component represents relationships between diﬀerent keys, the key related component is suitable for the analysis of inner relationships of diatonic keys, for example tension or resolution tendencies, or functional relationships. These features are directly related to the symmetric organisation of tones around the tonal center, which is particularly visualized by the key related component. 1 INTRODUCTION Tonality is the basis of western tonal music. Tonality comprises several aspects like stability of tones within a given tonal context, aspects of consonance or dissonance, aesthetic properties of a given musical piece, or the prediction of tensions or resolution. Tonality also helps to explain the development and usage of chords and keys. To this day a plenty of theories have been developed to explain these different aspects of tonality. An intuitive and uniﬁed theory of tonality is required that not only supports the development of extended music information retrieval methods, e.g. chord and key recognition, transcription and similarity estimation, but also helps to understand the way the human auditory system processes tonal information. 2 RELATED WORK The Analysis of musical tonality generally operates at three stages: 1. Frequency analysis, signal transformation, 2. Complexity, irrelevancy, redundancy reduction, 3. the analysis of the preprocessed audio signal by menans of a tonality model. The present publication is exclusively devoted to tonality models. For this aspects of frequency transformation and c⃝2007 Austrian Computer Society (OCG). preprocessing (e.g. transient location, noise reduction, consonance ﬁltering, pitch tracking, horizontal segmentation, dimensionality reduction) shall not be regarded in more detail. The Description of musical tonality with geometric tonality models has a long tradition. Early approaches are for example Heinichen’s (1728) or Kellner’s (1737) regional circles, the harmonic network proposed by Leonhard Euler (1739), and Weber’s (1767) regional chart [6, p.43]. Known as cirle of ﬁfth (Kellner’s regional circle), Riemann’s “Tonnetz” (Euler’s harmonic network) or Schönberg’s chart of key regions (Webers regional chart), these models are of great interest till this day. In the meantime, advanced geometric tonality models have been developed. Roger Shepard [12] proposes several helix models, which primarily describe aspects of octave equivalence or ﬁfth and chroma proximity. Elaine Chew [2] proposes a so called Spiral Array. The model’s core is the, harmonic network inspired, geometric arrangement of pitches on a spiral. The great breakthrough of Chew’s model is an uniﬁed description of the relationship between tones, chords and keys within one model and the observation of functional relationships that build a tonal center. Fred Lerdahl’s [6] “diatonic space” consists of “basic space”, “chordal space” and “regional space”. These spaces help to model diﬀerent aspects of tonality. While “basic space” describes the relationships between diﬀerent tonal hierarchies (octave, ﬁfth, triadic, diatonic and chromatic), “chordal space” is specialized to model tonal relationships between chords (chord proximity, chord progressions, ...) and “regional space” helps to describe tonal relationships between keys (e.g. aspects of modulation). Dmitri Tymoczko [14] represents musical chords in a geometric space called “orbifold space”. The mapping of the notes from one chord to those of another are represented with the help of line segments . The similarity of chords is represented by the length of these line segments. Aline Honingh [3] introduces the property of star convexity to describe and visualize the principle of shapeliness of tonal pitch structures. Another relevant geometric tonality model is Krumhansl and Kessler’s [5] four dimensional MDS solution which is subject of the next chapter in which the symmetry model is derived. 3 THE SYMMETRY MODEL Like Elaine Chew’s Spiral Array [2], the symmetry model is a geometric tonality model. A particular feature is the organisation of tones in a way that tonal symmetries within western tonal music become apparent. The model diﬀerentiates between key related 1 and key spanning tonal phenomena 2 and additionally reveals the relationship between tones, major and minor chords and keys. There are different ways to derive the symmetry model 3 , but to provide evidence for the close relationship between the symmetry model’s output and the psychological reality, we derive the symmetry model from Carol L. Krumhansl and E.J.Kessler’s pure cognition inspired MDS solution [5].",
        "zenodo_id": 1416214,
        "dblp_key": "conf/ismir/GatzscheMGB07"
    },
    {
        "title": "Tool Play Live: Dealing with Ambiguity in Artist Similarity Mining from the Web.",
        "author": [
            "Gijs Geleijnse",
            "Jan H. M. Korst"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416430",
        "url": "https://doi.org/10.5281/zenodo.1416430",
        "ee": "https://zenodo.org/records/1416430/files/GeleijnseK07.pdf",
        "abstract": "As methods in artist similarity identiﬁcation using Web Music Information Retrieval perform well on known evaluation sets, we investigate the application of such a method to a more realistic data set. We notice that ambiguous artist names lead to unsatisfying results. We present a simple, efﬁcient and unsupervised method to deal with ambiguous artist names. 1 INTRODUCTION Web Music Information Retrieval (Web-MIR) is a novel and promising ﬁeld of research. Recently, excellent performances (with precision rates around 90%) are reported on artist genre classiﬁcation and artist similarity identiﬁcation using dedicated test sets [2, 1, 3]. In this work, we focus on the task of identifying and scoring artist similarities using web data. Although our method performs well on two commonly used test sets, the results on a more realistic collection of artists are less encouraging. Contrary to the two common benchmark sets for Web-MIR [2, 3], this set of artists contains ambiguous names of less famous artists. When querying for artist names, within the search results the ambiguous name Nirvana can be expected to refer to the band. However, the meaning of Play – also a band in our collection – is less obvious. We observe that such ambiguous artist names tend to be found similar to a large number of artists. Especially for lesser known artists, this leads to unsatisfying results. We therefore present an unsupervised method to deal with the phenomenon of ambiguous artist names, making the method more robust and suited for realistic tasks. 2 FINDING ARTIST SIMILARITIES Using two efﬁcient methods introduced in earlier work [1], we compute the measure of similarity T(a, b) of artist a to another artist b using co-occurrences on the web. With PM, we extract artist names from search engine snippets after querying with relation patterns, where with DM we scan full documents for artist names. We use a set of 224 c⃝2007 Austrian Computer Society (OCG). 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 5 10 15 20 25 30 precision k Precision for k-NN Artist Similarity pm-224 pm-1995 dm-224 dm-1995 Figure 1. Precision for the sets of 224 and 1995 artists. artists, equally divided over 14 genres [2] and a large set of 1995 artists divided over 9 genres [3]. We consider two artists to be similar, if they share a genre in the test set. Figure 1 shows the average precision of the similarity of the artists and their k-NN. We can conclude that the pattern based method gives good results and outperforms DM in both sets. The experiments show that PM both outperforms DM and is less time consuming. Hence, we applied PM to a collection of 1732 artists, used in music recommender experiments. Since no ground truth is available for this collection of artist, we cannot evaluate the precision. We observe that the names that often not refer to the intended artist frequently occur amidst the most related artists. Although the queried expressions contain an artist name by construction, the retrieved snippets do not always handle musical artists and their relations. 3 DEALING WITH AMBIGUITY Ideally, for each occurrence of an artist name in a text we want to observe whether the occurrence indeed reﬂects the intended artist. However, the automatic parsing of sentences and term recognition is troublesome as the snippets contain broken sentences and may be multilingual. Moreover if an artist name is identiﬁed as a subject or object within a sentence, then we still do not know whether the term indeed reﬂects the artist. We therefore aim for an unsupervised method where we estimate the probability that a term a indeed reﬂects the intended artist named a. If we know the probability p(a) that a reﬂects the artist, we can redeﬁne the artist similarity function T as follows. T ′(a, b) = co′(a, b) P c,c̸=b co′(c, b) (1) with co′(a, b) = co(a, b) · p(a) · p(b) (2) Note that for p(a) = p(b) = 1, we have the baseline function [1] as applied in Section 2. We use the deﬁne functionality in Google to obtain the number of senses of a term. For example, by querying define: Tool, we obtain a list of 31 deﬁnitions for the term Tool, collected from various online dictionaries and encyclopedias. This indicates that Tool is an ambiguous term. On the contrary, terms such as Daft Punk, Fatboy Slim and Johannes Brahms lead to precisely one deﬁnition. We deﬁne n(a) to be the number of deﬁnitions for a to be returned by Google. If no deﬁnitions are returned, we consider n(a) to be 1. Based on the number of deﬁnitions n(a) returned by Google, we investigate the following two alternatives to estimate p(a). linear. As we do not know anything about the distributions of the use of the deﬁnitions for term a, we estimate that each deﬁnition has a equal probability to be used and that only one of the deﬁnitions reﬂects the artist. plin(a) = 1 n(a) (3) sqrt. Especially for terms with many deﬁnitions, we observe some overlap between the deﬁnitions. Moreover, two distinct deﬁnitions can be close related. For example, Red Hot Chili Peppers is the name of a band and the name of their self-titled debut album. We therefore investigate a second method to estimate p(a) by using the square root of the number of deﬁnitions found. psqrt(a) = 1 p n(a) (4) 4 EXPERIMENTAL RESULTS For both the sets of 224 and 1995 artists, we collected the numbers of deﬁnition for all the artist names. We recomputed the artist similarities using the linear approach (3) and the sqrt approach (4) and compared the two with the baseline as applied in Section 2. We present the results for the 1995 artists in Figure 2. For the set of 224 the performance of the methods using disambiguation is slightly less than that of the baseline approach. For the set of 1995 artists however, the results improve using either the linear or the sqrt approach. We note that contrary to the set of 224 artists, the 1995 set does contain ‘difﬁcult’ ambiguous names such Autograph, Gamma Ray and Hypocrisy. For the set of 1732 artists in our own collection, we compare the number of times that ambiguous artist names occur among the 5 nearest neighbors for the other artists 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0 5 10 15 20 25 30 35 40 45 50 precision k Precision for k-NN Artist Similarity linear sqrt baseline Figure 2. Precision for the sets of 1995 artists using the three ambiguity estimators. (Table 1). We note that for the term Juli only one definition is found. Although the distribution of ambiguous names is quite different for plin and psqrt, we cannot draw conclusions which approach is better suited as currently no ground truth for artist similarity ranking is available. Artist baseline plin psqrt Live 1227 1 54 Tool 1334 0 642 Fish 724 0 7 Juli 691 1251 1207 Table 1. Number of times an ambiguous artist name occurs among the top 5 nearest neighbors of the 1731 other artists. 5 CONCLUSIONS AND FUTURE WORK We have shown that a method that performs well on a ‘clean’ set with few ambiguous names leads to unsatisfying results on a more realistic data set. Terms that do not have the intended artist as dominant meaning (e.g. Boston, Play and Live) are likely to be found similar to other artists. We observe this problem especially for lesser known artists, where the data collected is more sparse. We have shown that a simple, efﬁcient and unsupervised method using the number of deﬁnitions of a term can compensate for this phenomenon. 6 REFERENCES [1] G. Geleijnse and J. Korst. Web-based artist categorization. In Proceedings of ISMIR’06, pages 266 – 271, 2006. [2] P. Knees, E. Pampalk, and G. Widmer. Artist classiﬁcation with web-based data. In Proceedings of ISMIR’04, pages 517 – 524, 2004. [3] M. Schedl, T. Pohle, P. Knees, and G. Widmer. Assigning and visualizing music genres by web-based co-occurrence analysis. In Proceedings of ISMIR’06, pages 260 – 265, 2006.",
        "zenodo_id": 1416430,
        "dblp_key": "conf/ismir/GeleijnseK07"
    },
    {
        "title": "The Quest for Ground Truth in Musical Artist Tagging in the Social Web Era.",
        "author": [
            "Gijs Geleijnse",
            "Markus Schedl",
            "Peter Knees"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416582",
        "url": "https://doi.org/10.5281/zenodo.1416582",
        "ee": "https://zenodo.org/records/1416582/files/GeleijnseSK07.pdf",
        "abstract": "Research in Web music information retrieval traditionally focuses on the classiﬁcation, clustering or categorizing of music into genres or other subdivisions. However, current community-based web sites provide richer descriptors (i.e. tags) for all kinds of products. Although tags have no well-deﬁned semantics, they have proven to be an effective mechanism to label and retrieve items. Moreover, these tags are community-based and hence give a description of a product through the eyes of a community rather than an expert opinion. In this work we focus on Last.fm, which is currently the largest music community web service. We investigate whether the tagging of artists is consistent with the artist similarities found with collaborative ﬁltering techniques. As the Last.fm data shows to be both consistent and descriptive, we propose a method to use this community-based data to create a ground truth for artist tagging and artist similarity. 1 INTRODUCTION Researchers in music information retrieval widely consider musical genre to be an ill-deﬁned concept [2, 14, 9]. Several studies also showed that there is no consensus on genre taxonomies [1, 10]. However, automatic genre classiﬁcation is a popular topic of research in music information retrieval (e.g. [3, 17, 8, 11, 16, 6]). In their 2006 paper [9], McKay and Fujinaga conclude that musical genre classiﬁcation is worth pursuing. One of their suggestions is to abandon the idea that only one genre is applicable to a recording. Hence, multiple genres can be applicable to one recording and a ranked list of genres should be computed per recording. Today, the content of web sites such as del.icio.us, flickr.com and youtube.com is generated by their users. Such sites use community-based tags to describe the available items (photos, ﬁlms, music, (scientiﬁc) literature, etc.). Although tags has proven to be suitable descriptors for items, no clear semantics are deﬁned. Users can label an item with any term. The more an item is lac⃝2007 Austrian Computer Society (OCG). beled with a tag, the more the tag is assumed to be relevant to the item. Last.fm is a popular internet radio station where users are invited to tag the music and artists they listen to. Moreover, for each artist, a list of similar artists is given based on the listening behavior of the users. In [4], Ellis et al. propose a community-based approach to create a ground truth in musical artist similarity. The research question was whether artist similarities as perceived by a large community can be predicted using data from All Music Guide and from shared folders for peer-to-peer networks. Now, with the Last.fm data available for downloading, such community-based data is freely available for non-commercial use. In Last.fm, tags are terms provided by users to describe music. They “are simply opinion and can be whatever you want them to be” 1 . For example, Madonna’s music is perceived as pop, glamrock and dance as well as 80s and camp. When we are interested in describing music in order to serve a community (e.g. in a recommender system), community-created descriptors can be valuable features. In this work we investigate whether the Last.fm data can be used to generate a ground truth to describing musical artists. Although we abandon the idea of characterizing music with labels with deﬁned semantics (i.e. genres), we follow the suggestion in [9] to characterize music with a ranked list of labels. We focus on the way listeners perceive artists and their music, and propose to create a ground truth using community data rather than to deﬁne one by experts. In line with the ideas of Ellis et al. [4], we use artist similarities as identiﬁed by a community to create a ground truth in artist similarity. As tastes and opinions change over time, a ground truth for music characterization should be dynamic. We therefore present an algorithm to create a ground truth from the dynamically changing Last.fm data instead of deﬁning it once and for all. This paper is organized as follows. In the next section we investigate whether the data from Last.fm can indeed be used as a ground truth for describing artists and artist similarities. As the experiments in Section 2 give rise to further research, we present a method to create a ground 1 http://www.Last.fm/help/faq/?category=Tags truth for a set of artists in Section 3. Using this ground truth, we propose an approach to evaluate the performance of arbitrary methods for ﬁnding artist similarities and tags. Finally, we draw conclusions and indicate directions for future work in Section 4. 2 ANALYZING THE LAST.FM META-DATA Last.fm users are invited to tag artists, albums, and individual tracks. The 100 top-ranked tags (with respect to the frequency a tag is assigned) for these three categories are easily accessible via the Audioscrobbler web services API 2 . By analyzing the listening behavior of its users, Last.fm also provides artist similarities via Audioscrobbler 3 . Per artist, a list of the 100 most similar artists is presented. We analyze tags for artists in the next subsection. As the lists of the top-ranked tags tend to contain noise, we propose a simple mechanism to ﬁlter out such noise (Section 2.2). In order to check the consistency of the tags, we inspect, in Section 2.3, whether users label similar artists with the same tags. In Section 2.4, we compare the performance of the Last.fm data with results from previous work on a traditional genre classiﬁcation task. Section 2 ends with conclusions on the suitability of Last.fm data to create a ground truth in artist tagging and similarity.",
        "zenodo_id": 1416582,
        "dblp_key": "conf/ismir/GeleijnseSK07"
    },
    {
        "title": "Supervised and Unsupervised Sequence Modelling for Drum Transcription.",
        "author": [
            "Olivier Gillet",
            "Gaël Richard"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417237",
        "url": "https://doi.org/10.5281/zenodo.1417237",
        "ee": "https://zenodo.org/records/1417237/files/GilletR07.pdf",
        "abstract": "We discuss in this paper two post-processings for drum transcription systems, which aim to model typical properties of drum sequences. Both methods operate on a symbolic representation of the sequence, which is obtained by quantizing the onsets of drum strokes on an optimal tatum grid, and by fusing the posterior probabilities produced by the drum transcription system. The ﬁrst proposed method is a generalization of the N-gram model. We discuss several training and recognition strategies (style-dependent models, local models) in order to maximize the reliability and the speciﬁcity of the trained models. Alternatively, we introduce a novel unsupervised algorithm based on a complexity criterion, which ﬁnds the most regular and wellstructured sequence compatible with the acoustic scores produced by the transcription system. Both approaches are evaluated on a subset of the ENST-drums corpus, and yield performance improvements. 1 INTRODUCTION Many useful applications can be derived from the knowledge of a semantic description of music signals. As a result, the ﬁeld of music information retrieval (MIR) is receiving a continuously growing interest from the scientiﬁc community. MIR has primarily focused on the extraction of melodic and tonal information, though it is now acknowledged that the rhythmic content, and the drum track in particular, is of primary importance for a number of applications such as drum track remixing, automatic genre recognition, automatic DJing or query by beatboxing. The problem of drum transcription has already been addressed in several studies (see [1] for a review of existing systems). However, most of the studies on drum track transcription only use short-term acoustic information (as carried in acoustic features, or for example, in the coefﬁcients of a non-redundant decomposition), and thus consider each drum event independently from the other adjacent events. Nevertheless, by analogy with speech, where a sequence of random phonemes does not constitute a syntactically correct sentence, most of the sequences of drum events do not represent musically interesting drum tracks. c⃝2007 Austrian Computer Society (OCG). In fact, as already shown in previous works [2, 3, 4], the acoustic clues should be combined with sequence models to take into account the structural speciﬁcities of drum sequences. Some of these speciﬁcities are listed here: some drum subsequences may never be played (either because they are musically irrelevant or because they are too complex to be played by a drummer), some subsequences are frequent, independently of the style (a tom ﬁll for example), and some subsequences are typical of a given style (for example a disco rhythm has the bass drum played on each beat). Furthermore, a drum sequence may contain repetitive patterns that span several hierarchical levels. For instance, a simple one-bar-long pattern may be repeated to create a musical phrase, this phrase may be repeated several times during the chorus, which itself is played several times during the piece. The aim of this paper is to present two strategies to include such information in drum transcription systems: a supervised strategy, based on a generalization of N-gram models (described in section 3); and an unsupervised strategy (described in section 4) which aims to ﬁnd the drum sequence that exhibits the largest degree of repetitivity and structure, while still being compatible with the acoustic scores. Both these methods operate on a symbolic representation of the drum sequence. We therefore brieﬂy discuss in section 2 how to obtain such a representation from a list of unquantized onsets and posterior probabilities produced by a drum transcription system. Finally, experimental results are given in section 5, and some conclusions are suggested in the last section. 2 SYMBOLIC REPRESENTATION EXTRACTION Drum transcription systems output a sequence of pairs (ti, πij)1≤i≤N where πij expresses the probability that the drum instrument Ij is played at time ti 1 . In this study, we focus on three drum instruments: I1 = bass drum, I2 = snare drum and I3 = hi-hat. 1 In some drum transcription systems, the (ti) are obtained by an onset detector, and the posterior probabilities (πij) by probabilistic models trained for each class of drum instrument (bass drum, snare drum...) to detect. Some other systems may require additional post-processings to output a probability score, for example from a detected amplitude or a distance to a template.",
        "zenodo_id": 1417237,
        "dblp_key": "conf/ismir/GilletR07"
    },
    {
        "title": "An Analysis of the Mongeau-Sankoff Algorithm for Music Information Retrieval.",
        "author": [
            "Carlos Gómez",
            "Soraya Abad-Mota",
            "Edna Ruckhaus"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417931",
        "url": "https://doi.org/10.5281/zenodo.1417931",
        "ee": "https://zenodo.org/records/1417931/files/GomezAR07.pdf",
        "abstract": "An essential problem in music information retrieval is to determine the similarity between two given melodies; there are several melodic similarity measures that have been proposed, among others, the Mongeau-Sankoff measure. In this work we implemented a modiﬁed version of the Mongeau-Sankoff measure. We conducted an experimental study to compare the implemented measure with other similarity measures; this evaluation was done in the context of the 2005 edition of the MIREX symbolic melodic similarity competition. The most relevant result of our work is an implementation of the Mongeau-Sankoff measure that presents greater effectiveness when compared to other current melodic similarity measures. 1 PROPOSED APPROACH Throughout the development of music information retrieval systems, several melodic similarity measures have been proposed. A melodic similarity measure is a function that given two melodies, estimates the degree of identity that can be established between them, expressing it as a real number. This similarity measure is ideally a “continuous” function that is expressed in musical terms. This work studies the Mongeau-Sankoff measure [2] in the context of music information retrieval, and compares it with some of the existing measures. The MongeauSankoff measure is based on the alignment of musical sequences, where the elements of the sequences are notes with a speciﬁc pitch and duration. The similarity between the sequences is expressed in terms of the edit distance between them, which depends on the series of edit operations of least cost that is needed to transform one sequence into the other. In addition to the traditional edit operations (insertion, deletion and substitution), the MongeauSankoff measure deﬁnes two operations that apply specifically to music: fragmentation and consolidation. The measure was designed in 1990, and even though it has been applied previously to music information retrieval systems, to the best of our knowledge, no recent implementation of the measure has been developed with the speciﬁc purpose of comparing it to the algorithms evaluated c⃝2007 Austrian Computer Society (OCG). in a Music Information Retrieval Evaluation eXchange (MIREX) edition, a speciﬁc goal of our project. In this work we propose variants to the measure that are related to several aspects: achieving transposition invariance when the key of queries and the key of documents is not known, computing the interval weight according to its distance, and representing the relative cost of the operations. Mongeau and Sankoff originally applied their measure to the study of musical variation; therefore, the measure was not conceived speciﬁcally for retrieving musical documents in a collection. The original measure assumes that the key of the two melodies that are being compared is known; based on this knowledge, the measure attains transposition invariance. In the general case of a music information retrieval system, both the key of the query and the key of the documents could be unknown, therefore, it is necessary to adapt the measure in this aspect. In this work, we adopt the approach of making all the possible transpositions of the query with respect to the target, and consider the similarity result as the best value between all the transpositions. Other researchers in this area have applied this approach to Mongeau-Sankoff, and also to other measures. Another variant of the measure pertains to the weight of the operations that involve the substitution of a note. In the original measure, this weight depends partially on the interval between the note that is being substituted and the new note, and it is deﬁned based on the consonance of the interval. In this work we adopt another approach, which consists of assigning a weight to the interval in terms of the distance between the two notes. This approach has been applied in previous works [1]. Finally, a change was made to the cost functions of the measure. The weight of an operation is the sum of two quantities, that represent the weight associated to the pitch difference between the notes involved in the operation, and that of the duration difference. A parameter of the measure determines the proportion in which the duration difference contributes to the weight, versus that of pitch difference. This constant was originally the same for all operations. In our work, we associate a separate constant to the weight formula of insertions and deletions with respect to the other operations. This allows to vary the relative weight of insertions and deletions with respect to these other operations. Thus, in this work we propose a Alignment ADR NRGB Average Prec. R-Prec. Total time Local∗",
        "zenodo_id": 1417931,
        "dblp_key": "conf/ismir/GomezAR07"
    },
    {
        "title": "Mood-ex-Machina: Towards Automation of Moody Tunes.",
        "author": [
            "Sten Govaerts",
            "Nik Corthaut",
            "Erik Duval"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415918",
        "url": "https://doi.org/10.5281/zenodo.1415918",
        "ee": "https://zenodo.org/records/1415918/files/GovaertsCD07.pdf",
        "abstract": "In 2006, the rockanango system was developed for music annotation by music experts. The system allows these experts to create new musical parameters within a flat data structure [1]. Rockanango is deployed in a commercial environment of hotels, restaurants and cafés. One of the main concerns is the time it takes to manually annotate the music and to introduce new parameters. In this paper, we investigate the possibilities to assist the experts by means of automatic metadata generation. Two case studies are described. One focuses on the use of association rules, in combination with lower level metadata like mode and key. The other case study concerns the generation of a topic or subject marker for songs through harvested lyrics and a keyword generator. From our evaluation, we conclude that the generated keywords are relevant and that the music experts value them higher then laymen. Data mining techniques provide means for monitoring the metadata in terms of interparametric relationships that can be used to generate metadata.",
        "zenodo_id": 1415918,
        "dblp_key": "conf/ismir/GovaertsCD07"
    },
    {
        "title": "Phoneme Recognition in Popular Music.",
        "author": [
            "Matthias Gruhne",
            "Christian Dittmar",
            "Konstantin Schmidt"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417111",
        "url": "https://doi.org/10.5281/zenodo.1417111",
        "ee": "https://zenodo.org/records/1417111/files/GruhneDS07.pdf",
        "abstract": "Automatic lyrics synchronization for karaoke applications is a major challenge in the ﬁeld of music information retrieval. An important pre-requisite in order to precisely synchronize the music and corresponding text is the detection of single phonemes in the vocal part of polyphonic music. This paper describes a system, which detects the phonemes based on a state-of-the-art audio information retrieval system with harmonics extraction and synthesizing as pre-processing method. The extraction algorithm is based on common speech recognition low-level features, such as MFCC and LPC. In order to distinguish phonemes, three different classiﬁcation techniques (SVM, GMM and MLP) have been used and their results are depicted in the paper. 1 INTRODUCTION During the last years, users of personal computers have aquired huge amounts of digital music due to efﬁcient audio compression techniques. One of the most fascinating leisure activities, especially in asian countries is the karaoke application. It is however, difﬁcult and timeconsuming to create karaoke ﬁles at the moment. Since a manual tagging of the song is required, in order to convey the information, when a certain word is played to display and mark it on the screen. It might be commercially interesting to create karaoke applications for the home user, that is able to create karaoke ﬁles automatically from the personal music collection and corresponding texts from the internet. A basis of such system is the automatic synchronisation between music and corresponding lyrics. There have been scientiﬁc papers in the past, which described such methods [5] [1], but there is still room to improve the results signiﬁcantly. The authors think, that phoneme recognition is a basis for synchronizing lyrics and corresponting text, since the most prominent phonemes give a good indication of the time label within a currently sung word. Thus, the system presented in this paper describes a novel approach for automatically detecting pho-nemes in music. Section 2 describes the setup of the overall system, illustrates the state of the art and shows c⃝2007 Austrian Computer Society (OCG). own extensions. Subsequently, the test setup is described and results are depicted. The paper ﬁnishes with conclusions and an explanation of the future work. 2 PROPOSED SYSTEM The proposed system uses techniques of a common state of the art information retrieval system, but makes additionally use by a harmonics extraction algorithm at the beginning. The overall design has been inspired by the method used by Fujihara [3], who described a singer identiﬁcation method based on a music information retrieval system with a previous harmonics extraction. Since singer identiﬁcation addresses a similar task, the results of detecting phonemes from polyphonic music have been expected to increase as well. The proposed method starts with a fundamental frequency estimation as described in [2] in order to improve the harmonic extraction results. Dressler uses a Multi-Resolution Fast Fourier Transform (MRFFT) to compute the spectra in different time-frequency resolutions efﬁciently. In order to discriminate frequencies, an instantaneous frequency (IF) is estimated from successive phase spectra. Due to the fact, that sinusoidal components of the audio signal contain the most relevant melody information, the harmonics are identiﬁed using a psychoacoustic model under distinction of spectral features. After estimating the fundamental frequency, the partials are retrieved from a spectrogram. The ﬁnal sinusoidal resynthesizing of the audio signal is determined by transforming the spectrum into the time domain by using an Inverse Discrete Fourier Transform (IDFT). Only the previously calculated harmonic components of the spectrum are considered for a resynthesis. After constructing the signal, common speech recognition features had been extracted and assembled. The applied features are Mel Frequency Cepstral Coefﬁcients (MFCC), Linear Prediction Coefﬁcients (LPC), Perceptual Linear Prediction (PLP) and Warped Linear Prediction Coefﬁcients (WarpedLPC) [4]. Before the actual classiﬁcation the dimensions are reduced by using a linear discriminant analysis. The resulting feature matrix is classiﬁed with common classiﬁer techniques, namely Gaussian Mixture Models (GMM), Support Vector Machines (SVM) and Multilayer Perceptron (MLP). Classiﬁer Pr. Rc. CCI Results with harmonics analysis MLP",
        "zenodo_id": 1417111,
        "dblp_key": "conf/ismir/GruhneDS07"
    },
    {
        "title": "ATTA: Implementing GTTM on a Computer.",
        "author": [
            "Masatoshi Hamanaka",
            "Keiji Hirata 0001",
            "Satoshi Tojo"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418269",
        "url": "https://doi.org/10.5281/zenodo.1418269",
        "ee": "https://zenodo.org/records/1418269/files/HamanakaHT07.pdf",
        "abstract": "We have been discussing the design principle for the implementation of GTTM and presented the semiautomatic generation techniques of grouping structure, metrical structure, and time-span tree, and the searching method for the optimal parameter value assignments. In ISMIR2007, we organize a tutorial session on the techniques for implementing music theory GTTM for summarizing our work and report it to relevant participants of the conference.  Since the time of the tutorial session is not enough, we demonstrate a working automatic timespan tree analyzer ATTA in a demo session.  ATTA is an integration of our work done so far; by looking at the ATTA demonstration or using ATTA, people will be able to understand the techniques for implementing GTTM as well as GTTM itself in more detail.",
        "zenodo_id": 1418269,
        "dblp_key": "conf/ismir/HamanakaHT07"
    },
    {
        "title": "Desoloing Monaural Audio Using Mixture Models.",
        "author": [
            "Yushen Han",
            "Christopher Raphael"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417507",
        "url": "https://doi.org/10.5281/zenodo.1417507",
        "ee": "https://zenodo.org/records/1417507/files/HanR07.pdf",
        "abstract": "We describe a new approach to the “desoloing” problem, in which one tries to isolate the accompanying instruments from a monaural recording of a soloist with accompaniment. Our approach is based on explicit knowledge of the audio in the form of a score match – a correspondence between a symbolic score and the music audio, giving the times of all musical events. We employ the familiar idea of masking the short time Fourier transform to eliminate the solo part. The ideal mask is estimated by ﬁtting a model to the data, whose note-based components are derived from the score match. The parameters for our probabilistic model are estimated using the EM algorithm. 1 INTRODUCTION We focus here on the problem of isolating an accompanying instruments from a monaural recording of music for soloist and accompaniment. We call this problem “desoloing.” The primary application for desoloing, at least in terms of numbers, would likely be karaoke. Desoloing would produce an accompaniment for any song of interest, thus increasing the range of music on which both singers and listeners could enjoy karaoke. Our interest in this problem, however, stems from our work with musical accompaniment systems. The idea here seems, at ﬁrst, painfully close to karaoke, except that the accompanying instruments must follow the soloist, rather than the other way around. This change adds a great deal of complexity to the problem, while also making it attractive to “classical” musicians. Our preferred method of orchestral resynthesis is from actual audio. While commercial orchestral accompaniments are available for some of the solo literature, they tend to be poorly recorded with variable playing. A successful desoloing algorithm would harvest a wide world of beautifully played and expertly recorded orchestras for the accompaniment system. Desoloing serves an MIR need by allowing one to access the “sources” of an audio ﬁle independently. The desoloing problem takes an asymmetric view of the familiar source separation idea, which has received much attention in the signal processing community over c⃝2007 Austrian Computer Society (OCG). Figure 1. Spectrogram of opening of Samuel Barber Violin Concerto. Vertical lines mark the solo note onset. the last decade. Much of this work is called “blind” source separation, meaning that one tries to separate the sources with little or no knowledge of their contents [1] [2], [3], [4]. The general area of blind source separation includes several efforts that are explicitly devoted to music audio [5] and [6]. Our framing of the problem is distinct from most work in source separation due to the explicit knowledge we have of the audio — we assume that we are given a symbolic score to the piece of music, giving the pitches and instruments of all notes in the music, as well as a score match, giving a precise correspondence between these notes and the audio ﬁle. The present work is enabled by our previous work in orchestral score following with very minor adjustment done manually in case of mismatch. Figure 1 demonstrates this correspondence between score and audio that forms the basis of our approach. A similar problem statement was deﬁned in [7]. While this problem is, no doubt, highly challenging, our particular needs make the goal somewhat more attainable. Any desoloing procedure will almost certainly result in the loss or disﬁgurement of certain aspects of the orchestral audio we wish to isolate. However, in our accompaniment application, the live soloist will be playing at the precise time-frequency regions where our desoloing procedure does the most damage. Thus, much of the harm done by desoloing will be masked by the live soloist. This brings our desoloing into the realm of tractable problems. 2 MASKING IN A TIME-FREQUENCY DOMAIN Our approach operates on the short time Fourier transform (STFT) X of the audio signal x in the time domain [8]. We use binary masking to decompose X into X = ˆ Xs + ˆ Xa. ˆ Xs and ˆ Xs are our estimates for the solo and the accompaniment. We denote this by ˆ Xs ≈ 1SX ˆ Xa ≈ 1AX where 1C(t, k) = ½ 1 if (t, k) ∈C 0 otherwise We deﬁne ˆA to be the complement of ˆS. Convincing audio signals ˆxs and ˆxa in the time domain are reconstructed by STFT−1 ˆ Xs and STFT−1 ˆ Xs. We include Hann window in STFT with L = N/H = 4 hops per FFT length, as it fulﬁlls the constant overlap-add (COLA) constraint for perfect recovery of x from X [13]. It is well-known that perceptually good results can be constructed using the ideal mask that can be computed when the sets, S and A are known, as when x is artiﬁcially constructed by adding together two known signals xs and xa. For instance, see [9]. Moreover, with known x, xs and xa, we can derive a percentage that describes how much binary masking we can estimate correctly. This will serve as a quantitative evaluation other than the audible result. Roweis [14] described the idea of binary masking from a ﬁlter bank point of view. 3 MODEL-BASED DECOMPOSITION",
        "zenodo_id": 1417507,
        "dblp_key": "conf/ismir/HanR07"
    },
    {
        "title": "Music Browsing Using a Tabletop Display.",
        "author": [
            "Stephen Hitchner",
            "Jennifer Murdoch",
            "George Tzanetakis"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415896",
        "url": "https://doi.org/10.5281/zenodo.1415896",
        "ee": "https://zenodo.org/records/1415896/files/HitchnerMT07.pdf",
        "abstract": "The majority of work in Music Information Retrieval (MIR) follows a search/retrieval paradigm. More recently, the importance of browsing as an interaction paradigm has been realized, and several novel interfaces have been proposed. In this paper, we describe two novel interaction schemes for content-aware browsing of music collections that use a graphical tabletop interface. We further present ﬁndings from qualitative user studies. We describe our work in the context of two primary themes: music collection browsing, and collaborative (multiple simultaneous users) interaction and involvement during the browsing/selection process. 1 INTRODUCTION Similarity and clustering algorithms may be used to automatically incorporate meaningful structure into large, diverse, and disorganized collections of music. In particular, Self-Organizing Maps (SOMs) [3] are sought after for their unsupervised dimensionality-reduction applications. A growing body of research aims to enhance the effectiveness of automatic content-based analysis algorithms via intuitive user interface (UI) design. We believe the solution to truly effective music collection browsing combines the use of automatic techniques for structuring collections, with a framework for interaction that facilitates systematic and satisfying exploration of music collections. SOMs of music clusters are particularly amenable to satisfying visualization [4], but may also facilitate purely auditory exploration of the cluster map via tangible UIs such as the Radio Drum and Kaoss pad [5]. In particular, we incorporate use of a SOM for clustering of audio-based features, described in [9], automatically extracted from each track in a digital audio collection. Our primary aim is to support both the systematic exploration of music collections, and collaborative interaction between individuals with potentially diverse musical tastes The perception and appreciation of music is an interactive process that we feel is complimented by rich interaction among multiple individuals within social contexts. Music browsing is also an inherently interactive activity, requiring constant interplay between the user and the sysc⃝2007 Austrian Computer Society (OCG). Figure 1. User study participants interacting with the tabletop interface. tem. We believe that the (collaborative) browsing of music collections via existing systems is limited to a large extent by the use of the often cumbersome monitor / keyboard / mouse UI paradigm. The Jukola, a collaborative system for democratic music selection tested in natural communal setting, is presented in [6]. The use of large horizontal displays has been established as a means of supporting collaborative interaction and social cohesion [7]. As such, tabletop UIs are beginning to be explored for a number of music-related applications [2, 8]. We use the multi-touch front-projected DiamondTouch table developed in 2001 by the Mitsubishi Electric Research Laboratories [1]. Through qualitative user studies, we assess the relative effectiveness of two novel music-collection-browsing paradigms. We investigate consensus-reaching and processes whereby individuals tend to guide each other during exploration of a music collection. 2 SYSTEM OVERVIEW The MarGrid application uses the Marsyas 0.2 1 software framework to extract audio features[9] from a collection of digital audio tracks, and a SOM[3] is employed to organize the tracks within a 2D grid. A user can browse the 2D grid using a mouse and keyboard, MIDI controller pad, or tabletop UI (described below). Music collections 1 http://marsyas.sourceforge.net are imported via parsing of iTunes (XML) library ﬁles. Serveral UIs, providing different approaches to browsing/ interacting with music collections, were explored.",
        "zenodo_id": 1415896,
        "dblp_key": "conf/ismir/HitchnerMT07"
    },
    {
        "title": "Content-Based Music Retrieval Using Query Integration for Users with Diverse Preferences.",
        "author": [
            "Keiichiro Hoashi",
            "Hiromi Ishizaki",
            "Kazunori Matsumoto",
            "Fumiaki Sugaya"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416004",
        "url": "https://doi.org/10.5281/zenodo.1416004",
        "ee": "https://zenodo.org/records/1416004/files/HoashiIMS07.pdf",
        "abstract": "This paper proposes content-based music information retrieval (MIR) methods based on user preferences, which aim to improve the accuracy of MIR for users with “diverse” preferences, i.e., users whose preferences range in songs with a wide variety of features. The proposed MIR method dynamically generates an optimal set of query vectors from the sample set of songs submitted by the user to express their preferences, based on the similarity of the songs in the sample set. Experiments conducted on a music collection with subjective user ratings verify that our proposal is effective to improve the accuracy of contentbased MIR. Furthermore, by implementing a two-step MIR algorithm which utilizes song clustering results, the efﬁciency of the proposed MIR method is signiﬁcantly improved. 1 INTRODUCTION Recent popularity of online music distribution services have provided the opportunity to access to millions of songs, and also have enabled common users to accumulate a largescaled music collection. This rapid growth of both online and personal music collections has made it increasingly difﬁcult for users to efﬁciently ﬁnd songs which they want to listen to. Development of an effective music information retrieval (MIR) system is, therefore, essential to realize satisfactory music distribution services, and improve usability of music applications. Due to these recent developments, various research have been conducted in the area of content-based MIR based on user preferences. Logan has proposed a content-based MIR method which extracts the acoustic features from a set of songs, e.g., an album, which is deﬁned as an expression of user preferences[8]. Grimaldi et al. have extended feature extraction techniques that have been proved effective for music genre classiﬁcation, to conduct retrieval of music based on user preferences[5]. Furthermore, the authors have proposed a content-based MIR method, which generates a vector expression of user preferences from a sample set of songs submitted by the user[6][7]. c⃝2007 Austrian Computer Society (OCG). Although the above content-based MIR methods have been proved to be reasonably effective, the effectiveness of the methods are limited on conditions that the preferences of the users are focused. For example, the evaluations in [8] have utilized individual albums, which typically consist of similar songs, as an expression of user preferences. Furthermore, evaluations in [5] conclude that their MIR method is ineffective when user preferences are not focused on a speciﬁc genre. The main objective of this paper is to propose a method which is capable of providing accurate content-based MIR results to users with diverse preferences. Namely, this paper proposes a method which automatically generates an optimal number of queries from a sample set of songs submitted by the user, based on the similarity between the songs in the set. Effectiveness of the proposed method is veriﬁed by experiments conducted on a music collection with subjective user ratings. 2 CONVENTIONAL MIR METHODS In [6], the authors have proposed a content-based MIR method, which retrieves songs that ﬁt music preferences based on a small set of example songs (hereafter referred as the “sample set”) provided by the user. Our MIR method applies the tree-based vector quantization (TreeQ) algorithm developed by Foote[2]. Furthermore, in [7], the authors have proposed a feature space modiﬁcation (FSM) method, which utilizes song vector clustering results to automatically generate a training set for TreeQ, from any music collection. By implementing this method, the MIR system can build a feature space optimized to the songs in the music collection. By utilizing the tree-based vector quantizer (hereafter referred to as the “VQ tree”) generated by the above methods, our system is able to retrieve a list of songs which ﬁt user preferences, based on the sample set of “good” songs submitted by the user. The system ﬁrst generates a vector expression of the users’ preferences (hereafter referred to as the “user proﬁle”), by calculating the vector sum of all vectors of the songs in the sample set. Scores of all songs in the collection are calculated based on the cosine similarity between the user proﬁle and song vectors, and songs with high similarity to the query are presented to the user as the MIR result. 3 PROBLEMS As previously described, existing content-based MIR methods are able to achieve reasonable success in retrieving songs which ﬁt user preferences. However, the accuracy of existing MIR methods, including our methods in Section 2, are dependent on the information submitted by the user. For instance, if the user inputs his/her preferences to the MIR system by specifying a single song or musical genre, it is fairly easy to derive accurate MIR results, since the user preference is well-expressed by the submitted query. It is obvious, though, that the preferences of users are not always focused on a speciﬁc genre and/or song. If anything, user preferences are usually diverged in multiple genres. In such cases, the accuracy of existing content-based MIR methods are expected to be degraded. For example, consider a situation where a noisy rock song and a soothing classical song are both included as “good” songs in the sample set. As mentioned in Section 1, most existing MIR methods are not capable to output accurate MIR results for users with such diverse preferences. Our previously proposed methods in [6] and [7] are capable of conducting MIR, even from such a diverse sample set. However, the user proﬁle generated from the sample set is the sum vector of the two songs, meaning that the user proﬁle points to the area in the middle of the two songs. Naturally, the MIR result based on the query is expected to consist of songs located in that area. In other words, the system will not be able to retrieve songs that are similar to either rock or classical songs, which are assumably a better representation of the user preferences than the songs that will be retrieved by these methods. A naive way to solve this problem is to utilize the vectors of all songs in the sample set as independent queries, and merge all MIR results obtained from each query. However, this approach obviously will increase the computational cost to conduct MIR. 4 QUERY INTEGRATION In order to solve the previously described problems, we propose the query integration method. The objective of this method is to automatically determine an optimal number of queries to be generated from the sample set, based on the similarity of the features of the songs in the set. A conceptual illustration of the proposed method is shown in Figure 1. Figure 1 illustrates a situation where the sample set consists of six songs, S1, · · · , S6. There are two sets of songs in the sample set that are highly similar to each other, {S1, S2, S3}, and {S4, S5}. In this case, the intuitionally optimal set of queries can be generated by integrating the songs in these two sets to generate a single query, which represents each set respectively, and utilizing S6 as an independent query. The proposed query integration method is implemented by conducting hierarchical clustering of the song vectors included in the sample set. First, the similarity between all song vectors in the sample set are calculated. Next, the Music feature space S1 S2 S3 S4 S5 S6 : song vector : integrated query Figure 1. Conceptual illustration of query integration two songs with the highest similarity are extracted. If the similarity between these two songs exceed a predeﬁned threshold τq, the songs are integrated to a single vector, by summing the vectors of the two songs. This procedure is repeated until all song vectors in the sample set are integrated to a single vector, or when the maximum similarity fails to exceed τq. Query integration is expected to solve the diverse user preference problem, since the generated queries are able to represent the features of the songs in the sample set if they are diverse, and, simultaneously, can generate a focused query whenever appropriate. Furthermore, by integrating vectors of highly similar songs, this method can reduce the increase of computational cost, compared to conducting MIR individually for all songs in the sample set. 5 EXPERIMENTS In order to evaluate the accuracy and efﬁciency of the proposed method, an experiment is conducted based on a large music collection with subjective user ratings. Details of the experiments are as follows.",
        "zenodo_id": 1416004,
        "dblp_key": "conf/ismir/HoashiIMS07"
    },
    {
        "title": "Creating a Simplified Music Mood Classification Ground-Truth Set.",
        "author": [
            "Xiao Hu 0001",
            "Mert Bay",
            "J. Stephen Downie"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416920",
        "url": "https://doi.org/10.5281/zenodo.1416920",
        "ee": "https://zenodo.org/records/1416920/files/HuBD07.pdf",
        "abstract": "A standardized mood classification testbed is needed for formal cross-algorithm comparison and evaluation. In this poster, we present a simplification of the problems associated with developing a ground-truth set for the evaluation of mood-based Music Information Retrieval (MIR) systems. Using a dataset derived from Last.fm tags and the USPOP audio collection, we have applied a K-means clustering method to create a simple yet meaningful cluster-based set of high-level mood categories as well as a ground-truth dataset. 1",
        "zenodo_id": 1416920,
        "dblp_key": "conf/ismir/HuBD07"
    },
    {
        "title": "Exploring Mood Metadata: Relationships with Genre, Artist and Usage Metadata.",
        "author": [
            "Xiao Hu 0001",
            "J. Stephen Downie"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415126",
        "url": "https://doi.org/10.5281/zenodo.1415126",
        "ee": "https://zenodo.org/records/1415126/files/HuD07.pdf",
        "abstract": "There is a growing interest in developing and then evaluating Music Information Retrieval (MIR) systems that can provide automated access to the mood dimension of music. Mood as a music access feature, however, is not well understood in that the terms used to describe it are not standardized and their application can be highly idiosyncratic. To better understand how we might develop methods for comprehensively developing and formally evaluating useful automated mood access techniques, we explore the relationships that mood has with genre, artist and usage metadata. Statistical analyses of term interactions across three metadata collections (AllMusicGuide.com, epinions.com and Last.fm) reveal important consistencies within the genre-mood and artist-mood relationships. These consistencies lead us to recommend a cluster-based approach that overcomes specific term-related problems by creating a relatively small set of data-derived “mood spaces” that could form the ground-truth for a proposed MIREX “Automated Mood Classification” task. 1",
        "zenodo_id": 1415126,
        "dblp_key": "conf/ismir/HuD07"
    },
    {
        "title": "Performance of Philips Audio Fingerprinting under Desynchronisation.",
        "author": [
            "Neil J. Hurley",
            "Félix Balado",
            "Elizabeth P. McCarthy",
            "Guenole C. M. Silvestre"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416068",
        "url": "https://doi.org/10.5281/zenodo.1416068",
        "ee": "https://zenodo.org/records/1416068/files/HurleyBMS07.pdf",
        "abstract": "An audio ﬁngerprint is a compact representation (robust hash) of an audio signal which is linked to its perceptual content. Perceptually equivalent instances of the signal must lead to the same hash value. Fingerprinting ﬁnds application in efﬁcient indexing of music databases. We present a theoretical analysis of the Philips audio ﬁngerprinting method under desynchronisation for correlated stationary Gaussian sources. 1 INTRODUCTION There are inevitable trade-offs between the size and the properties of a robust hash or ﬁngerprint. An audio ﬁngerprinting scheme which has proved to be remarkably robust is the Philips method, proposed by Haitsma et al [1] based on quantizing differences of energy measures from overlapped short-term power spectra. In this paper we examine the theoretical performance of the Philips method under desychonisation through a statistical model. This approach allows the inﬂuence of the system parameters to be studied and optimization strategies to minimize the probability of bit error of the hash to be tackled. Some previous work has tackled performance analysis of the Philips method through a statistical model. For example a model was proposed by Doets and Lagendijk [2], for the case in which the signal to be hashed is uncorrelated Gaussian noise. This was used to evaluate the performance of the ﬁngerprinting method under distortion, but the results only apply to i.i.d. sources. The important issue of performance analysis under desynchronization, which to our knowledge has not been previously tackled, constitutes the main contribution of this paper. 2 DESYNCHRONIZATION ERROR ANALYSIS In the Philips method, the length N input signal x is divided into overlapped frames before hashing. Let L be the number of samples in a single frame, ∆be the number of non-overlapping samples between two frames and xn be the input signal corresponding to the nth frame. We deﬁne the degree of overlap as θ ≜1 −∆/L, where θ ∈(0, 1), and higher θ corresponds to greater overlap. c⃝2007 Austrian Computer Society (OCG). A window w is applied to xn before computing the power spectrum. The spectrum is divided into 32 frequency bands on a logarithmic scale. Denoting by En(m) the energy of frequency band m for input frame xn, an unquantised hash value is given by Dn(m) ≜[En(m) − En(m + 1)] −[En−1(m) −En−1(m + 1)], with m = 0, 1, · · · , 31 and frames n = 0, 1, 2, · · ·. The variables Dn(m) completely determine the system, as the binary hash value Fn(m) ∈{0, 1} corresponding to frame n and band m is computed as Fn(m) ≜u (Dn(m)) , with u(·) the unit step function. This method can be expressed as a quadratic form on the extended vector ˜xn ≜(x[(n −1) · ∆+ 1], · · · , x[n · ∆+ L])T for n = 0, 1, 2, . . ., which includes all the components of the overlapping vectors xn and xn−1 and which is of length M ≜L + ∆, such that Dn(m) = ˜xT n Q(m) ˜xn, for a band and window dependent matrix Q(m), whose derivation is described in [3]. Desynchronization is the potential lack of alignment between the original framing used in the acquisition stage and the framing that takes place in the identiﬁcation stage. The Philips algorithm has a high degree of overlapping in order to counteract desynchronization. Nevertheless, this strategy has a cost of generating a long hash sequence, which may be costly to store and compare. Consider a situation in which the signal fed to the system is desynchronized by k samples, with k ∈{−∆/2+1, · · · , ∆/2} and assuming ∆/2 integer for simplicity. It is sufﬁcient to consider this range since a desynchronization of ∆just shifts all the ﬁngerprint bits one position. A desynchronization by k samples results in a distorted hash value D′ n(m) and then a certain probability of bit error. It is convenient to write Dn(m) and D′ n(m) as quadratic forms in the same extended Gaussian vector xn ≜(x[(n −1)∆−∆/2 −1], · · · , x[n∆+ L + ∆/2])T , (1) of length M + ∆−1, which we assume is distributed as xn ∼N(0, Z). We write these quadratic forms as Dn(m) = xT n Q0(m) xn and D′ n(m) = xT n Qk(m) xn. Letting S ≜Dn(m) and V ≜D′ n(m), we note that stationarity implies that Z is Toeplitz and hence the mean of S and V is zero. Assume that S and V can be jointly modeled as a bivariate normal distribution centered at the origin with correlation coeﬁcient ρ, which can be written as ρk(m) = tr \u0002 Z Q0(m) Z Qk(m) \u0003 tr [(Z Q(m))2] . (2) Deﬁning ǫk n(m) ≜{F ′ n(m) ̸= Fn(m) | k} we have that Pr[ǫk n(m)] = 1 2(Pr[S > 0|V ≤0] + Pr[S ≤0|V > 0]) = 1 π arccos(ρk(m)) In order to average over k, we assume that k is uniformly distributed. An upper bound is based on assuming that ρk(m) ≥0 which holds as long as Pr \u0002 ǫk n(m) \u0003 ≤1/2. Hence, arccos(·) is a concave function and we may apply Jensen’s inequality [4] to upper bound the probability of bit error at frame n and band m as Pr[ǫn(m)] = E \u0014 1 π arccos(ρ(m)) \u0015 ≤1 π arccos(E[ρ(m)]). (3)",
        "zenodo_id": 1416068,
        "dblp_key": "conf/ismir/HurleyBMS07"
    },
    {
        "title": "Localized Key Finding from Audio Using Nonnegative Matrix Factorization for Segmentation.",
        "author": [
            "Özgür Izmirli"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417197",
        "url": "https://doi.org/10.5281/zenodo.1417197",
        "ee": "https://zenodo.org/records/1417197/files/Izmirli07.pdf",
        "abstract": "A model for localized key finding from audio is proposed. Besides being able to estimate the key in which a piece starts, the model can also identify points of modulation and label multiple sections with their key names throughout a single piece. The front-end employs an adaptive tuning stage prior to spectral analysis and calculation of chroma features. The segmentation stage uses groups of contiguous chroma vectors as input and identifies sections that are candidates for unique local keys in relation to their neighboring key centers. Nonnegative matrix factorization with additional sparsity constraints and additive updates is used for segmentation. The use of segmentation is demonstrated for single and multiple key estimation problems. A correlational model of key finding is applied to the candidate segments to estimate the local keys. Evaluation is given on three different data sets and a range of analysis parameters.",
        "zenodo_id": 1417197,
        "dblp_key": "conf/ismir/Izmirli07"
    },
    {
        "title": "Evaluation of Distance Measures Between Gaussian Mixture Models of MFCCs.",
        "author": [
            "Jesper Højvang Jensen",
            "Daniel P. W. Ellis",
            "Mads Græsbøll Christensen",
            "Søren Holdt Jensen"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415752",
        "url": "https://doi.org/10.5281/zenodo.1415752",
        "ee": "https://zenodo.org/records/1415752/files/JensenECJ07.pdf",
        "abstract": "In music similarity and in the related task of genre classiﬁcation, a distance measure between Gaussian mixture models is frequently needed. We present a comparison of the Kullback-Leibler distance, the earth movers distance and the normalized L2 distance for this application. Although the normalized L2 distance was slightly inferior to the Kullback-Leibler distance with respect to classiﬁcation performance, it has the advantage of obeying the triangle inequality, which allows for efﬁcient searching. 1 INTRODUCTION A common approach in computational music similarity is to extract mel-frequency cepstral coefﬁcients (MFCCs) from a song, model them by a Gaussian mixture model (GMM) and use a distance measure between the GMMs as a measure of the musical distance between the songs [2, 3, 5]. Through the years, a number of distance measures between GMMs have been suggested, such as the Kullback-Leibler (KL) distance [2], optionally combined with the earth movers distance (EMD) [3]. In this article, we evaluate the performance of these two distance measures between GMMs together with the normalized L2 distance, which to our knowledge has not previously been used for this application. 2 MEASURING MUSICAL DISTANCE In the following, we shortly describe the Gaussian mixture model and the three distance measures between GMMs we have tested. Note that if a distance measure satisﬁes the triangle inequality, i.e., d(p1, p3) ≤d(p1, p2) + d(p2, p3) for all values of p1, p2 and p3, then a nearest neighbor search can be speeded up by precomputing some distances. Assume we are searching for the nearest neighbor to p, and that we have just computed the distance to p1. If we already know the distance between p1 and p2, then the distance to p2 is bounded by d(p, p2) ≥d(p1, p2) − This research was supported by the Intelligent Sound project, Danish Technical Research Council grant no. 26–04–0092, and the Parametric Audio Processing project, Danish Research Council for Technology and Production Sciences grant no. 274–06–0521. c⃝2007 Austrian Computer Society (OCG). d(p1, p). If the distance to the currently best candidate is smaller than d(p1, p2) −d(p1, p), we can discard p2 without computing d(p, p2).",
        "zenodo_id": 1415752,
        "dblp_key": "conf/ismir/JensenECJ07"
    },
    {
        "title": "Human Similarity Judgments: Implications for the Design of Formal Evaluations.",
        "author": [
            "M. Cameron Jones",
            "J. Stephen Downie",
            "Andreas F. Ehmann"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416126",
        "url": "https://doi.org/10.5281/zenodo.1416126",
        "ee": "https://zenodo.org/records/1416126/files/JonesDE07.pdf",
        "abstract": "This paper presents findings of a series of analyses of human similarity judgments from the Symbolic Melodic Similarity, and Audio Music Similarity tasks from the Music Information Retrieval Evaluation Exchange (MIREX) 2006. The categorical judgment data generated by the evaluators is analyzed with regard to judgment stability, inter-grader reliability, and patterns of disagreement, both within and between the two tasks. An exploration of this space yields implications for the design of MIREX-like evaluations.",
        "zenodo_id": 1416126,
        "dblp_key": "conf/ismir/JonesDE07"
    },
    {
        "title": "Pedagogical Transcription for Multimodal Sitar Performance.",
        "author": [
            "Ajay Kapur",
            "Graham Percival",
            "Mathieu Lagrange",
            "George Tzanetakis"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417127",
        "url": "https://doi.org/10.5281/zenodo.1417127",
        "ee": "https://zenodo.org/records/1417127/files/KapurPLT07.pdf",
        "abstract": "Most automatic music transcription research is concerned with producing sheet music from the audio signal alone. However, the audio data does not include certain performance data which is vital for the preservation of instrument performance techniques and the creation of annotated guidelines for students. We propose the use of modiﬁed traditional instruments enhanced with sensors which can obtain such data; as a case study we examine the sitar. 1 INTRODUCTION Historically, musical traditions were preserved via oral transmission. With the invention of written music, audio recordings, and video, more information can be retained. However, valuable performance data must still be passed by oral means. There will never be a technological replacement for face-to-face teaching, but new methods for archiving performance data will let us retain and disseminate more information. Automatic music transcription is a well-researched area [3, 4, 5]. The novelty of our work is that we look beyond the audio data by using sensors to avoid octave errors and problems caused from polyphonic transcription. In addition, our work does not share the bias of most research that focuses only on Western music. This paper describes a music transcription system for sitar performance. The sitar is a fretted stringed instrument from North India. Unlike many Western fretted stringed instruments (classical guitar, viola de gamba, etc) sitar performers pull (or “bend”) their strings to produce higher pitches. In normal performance, the bending of a string will produce notes as much as a ﬁfth higher than the same fret-position played without bending. In addition to simply showing which notes were audible, our framework also provides information about how to produce such notes. A musician working from an audio recording (or transcription of an audio recording) alone will need to determine which fret they should begin pulling from. This can be challenging for a skilled performer, let alone a beginner. By representing the fret information on the sheet music, sitar musicians may overcome these problems. c⃝2007 Austrian Computer Society (OCG). Figure 1. Block diagram of the system 2 METHOD The Electronic Sitar (ESitar) [2] is an instrument which gathers gesture data from a performing artist. For the research described in this paper, fret data was captured by a network of resistors connecting each fret. The fret sensor is translated into MIDI pitch values based on equivalent resistance induced by left hand placement on the neck of the instrument. Each fret has a “bucket” of values, converting raw sensor data into discrete pitch values seen in ﬁgure 2. Data was recorded at a sampling rate of (44100 ÷ 512) Hz. The synchronized audio signal was recorded with a Shure Beta-57 microphone at 44100 Hz. The entire system is displayed in Figure 1.",
        "zenodo_id": 1417127,
        "dblp_key": "conf/ismir/KapurPLT07"
    },
    {
        "title": "VISA: The Voice Integration/Segregation Algorithm.",
        "author": [
            "Ioannis Karydis",
            "Alexandros Nanopoulos",
            "Apostolos N. Papadopoulos",
            "Emilios Cambouropoulos"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416552",
        "url": "https://doi.org/10.5281/zenodo.1416552",
        "ee": "https://zenodo.org/records/1416552/files/KarydisNPC07.pdf",
        "abstract": "Listeners are capable to perceive multiple voices in music. Adopting a perceptual view of musical ‘voice’ that corresponds to the notion of auditory stream, a computational model is developed that splits musical scores (symbolic musical data) into different voices. A single ‘voice’ may consist of more than one synchronous notes that are perceived as belonging to the same auditory stream; in this sense, the proposed algorithm, may separate a given musical work into fewer voices than the maximum number of notes in the greatest chord. This is paramount, among other, for developing MIR systems that enable pattern recognition and extraction within musically pertinent ‘voices’ (e.g. melodic lines). The algorithm is tested against a small dataset that acts as groundtruth.",
        "zenodo_id": 1416552,
        "dblp_key": "conf/ismir/KarydisNPC07"
    },
    {
        "title": "A Simple Algorithm for Automatic Generation of Polyphonic Piano Fingerings.",
        "author": [
            "Alia Al Kasimi",
            "Eric Nichols",
            "Christopher Raphael"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415880",
        "url": "https://doi.org/10.5281/zenodo.1415880",
        "ee": "https://zenodo.org/records/1415880/files/KasimiNR07.pdf",
        "abstract": "We present a novel method for assigning fingers to notes in a polyphonic piano score. Such a mapping (called a “fingering”) is of great use to performers. To accommodate performers’ unique hand sha our method relies on a simple, user function. We use dynamic programming to search the space of all possible fingerings for the optimal fingering under this cost function. Despite the simplicity of the algorithm we achieve reasonable and useful results.",
        "zenodo_id": 1415880,
        "dblp_key": "conf/ismir/KasimiNR07"
    },
    {
        "title": "Search &amp; Select Intuitively Retrieving Music from Large Collections.",
        "author": [
            "Peter Knees"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416456",
        "url": "https://doi.org/10.5281/zenodo.1416456",
        "ee": "https://zenodo.org/records/1416456/files/Knees07.pdf",
        "abstract": "A retrieval system for large-scale collections that allows users to search for music using natural language queries and relevance feedback is presented. In contrast to existing music search engines that are either restricted to manually annotated meta-data or based on a query-by-example variant, the presented approach describes audio pieces via a traditional term vector model and allows therefore to retrieve relevant music pieces by issuing simple free-form text queries. Term vector descriptors for music pieces are derived by applying Web-based and audio-based similarity measures. Additionally, as the user selects music pieces that he/she likes, the subsequent results are adapted to accommodate to the user’s preferences. Real-world performance of the system is indicated by a small user study. 1 MOTIVATION AND CONTEXT The recent achievements of the MIR community have led to many innovative and possibly unconventional approaches to support users in ﬁnding desired music. Frequently, content-based analysis of the audio ﬁles or collaborative recommendations to point users to music they might like (e.g. [2]) are applied. Some approaches also incorporate information from different sources to build interactive interfaces (e.g. [6]). However, to retrieve music from large databases, many approaches rely on query-by-example methods where the query must consist of a piece of information that has a representation similar to the records in the database, e.g. in query-by-humming/singing systems. Since users are accustomed to text-based search engines, which have become the “natural” way to ﬁnd and access all other types of content like images, videos, and text, query-by-example music search systems often lack broad acceptance. On the other hand, systems that offer textual queries, e.g. catalog search engines of commercial music re-sellers, permit only ﬁltering based on attributes like artist, album, track name, year, or subjective labels like genre or style. To address some of these limitations, in [1], a system is presented that relies on a semantic ontology containing relations between meta-data and automatically extracted c⃝2007 Austrian Computer Society (OCG). acoustic properties and that can be queried via natural language phrases. In the retrieval process, textual queries must then be mapped to the semantic concepts. With this system, semantic queries like “something fast from...” or “something new from...” can be processed. However, as with the traditional systems, the cultural context of the indexed music pieces is ignored. For real-world applicability of a music search engine, it must in fact behave like a Web search engine like Google or Yahoo! and allow arbitrary queries like rock with great riffs or even melodic metal with opera singer as front woman. Our ﬁrst steps into this direction can be found in [3]. By using Web-based features to describe music pieces in a collection, each piece can be represented by a term vector. Furthermore, audio-based similarity is incorporated to describe also those pieces for which no information can be found on the Web. Thus, we combine information about the context with information about the content. For retrieval, queries are sent to Google and the resulting Web pages are used to construct a query vector which can be compared to the term vectors of the pieces in the collection. In [4], we have modiﬁed this approach to include relevance feedback and to use a local Web page index for query vector construction instead of Google. In this work, the applicability of the proposed methods is demonstrated. The user can simply initiate the search for desired music by typing some descriptive terms. From the returned items, those after the user’s fancy are selected and transferred into a list of “harvested music pieces” (analogous to e.g. a shopping cart in an on-line shop). Based on the chosen music pieces, the consecutively presented results are modiﬁed such that they tend to contain more pieces similar to the ones in the “harvest list”. The user can continue searching by selecting (or ignoring) more results or by issuing the next query. 2 TECHNICAL FUNDAMENTALS In this section, we brieﬂy review the technical basis of the presented application. Details can be found in [3, 4]. We derive track speciﬁc information from the Web by combining the results of three queries issued to Google:",
        "zenodo_id": 1416456,
        "dblp_key": "conf/ismir/Knees07"
    },
    {
        "title": "Towards Musicdiff: A Foundation for Improved Optical Music Recognition Using Multiple Recognizers.",
        "author": [
            "Ian Knopke",
            "Donald Byrd"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416522",
        "url": "https://doi.org/10.5281/zenodo.1416522",
        "ee": "https://zenodo.org/records/1416522/files/KnopkeB07.pdf",
        "abstract": "This paper presents work towards a “musicdiff” program for comparing ﬁles representing different versions of the same piece, primarily in the context of comparing versions produced by different optical music recognition (OMR) programs. Previous work by the current authors and others strongly suggests that using multiple recognizers will make it possible to improve OMR accuracy substantially. The basic methodology requires several stages: documents must be scanned and submitted to several OMR programs, programs whose strengths and weaknesses have previously been evaluated in detail. We discuss techniques we have implemented for normalization, alignment and rudimentary error correction. We also describe a visualization tool for comparing multiple versions on a measure-by-measure basis. 1 INTRODUCTION This paper describes work on “musicdiff” program for music notation. There are many potential applications for such a program, just as there are for the text-ﬁle comparison programs that are ubiquitous these days. However, the one we are most interested in here is for use in an “engine” for a multiple-recognizer Optical Music Recognition (MROMR) system of the type described by Byrd and Schindele (2006). In fact, their work was part of a feasibility study for our MeTAMuSE project. A large part of the musicdiff problem is alignment, where the musical documents to be aligned can be expected to be relatively similar. In particular, they should have exactly the same structure, except in terms of details. So, if sections A and B in one version appear in reverse order in the other, ideally, the program would detect that; but if it doesn’t and instead says a section A before B was deleted and a new section (in fact, A) was inserted after it, it is not a serious problem. The ﬁrst well-known UNIX diff program dates back to the seventies (Hunt and McIlroy, 1976), and many variations have been created since. Some do line-level comparisons, some do character-level, and some, e.g., Microsoft c⃝2007 Austrian Computer Society (OCG). Word, do both. The rough analogue of lines in this context is probably measures. But a measure can contain a lot of information. For direct use by people, something with ﬁner granularity is highly desirable; for use in a MROMR engine, it is absolutely essential. For MROMR, we need to compare music-notation ﬁles generated by different programs. As of this writing, MusicXML is far and away the best choice because of its widespread support: we will soon say more about this. While we are most interested in a fully-automatic musicdiff as a component of an MROMR engine, we believe it also has great promise for interactive use by musicians. We shall say more about this application at the end of the paper. 2 MUSICXML AS AN OMR COMPARISON FORMAT Clearly a diff program requires its input ﬁles to be in formats it can compare. Until the recent widespread adoption of MusicXML for programs handling music notation, in practical terms, such formats did not exist. Now, with an intermediate conversion step, it is possible to write MusicXML from just about any OMR program, and to read it for editing or error-checking with most notation programs. XML is rapidly becoming one of the most prominent formats for data interchange, especially in the context of the Internet. An XML document can specify a format it uses; document formats are deﬁned by speciﬁcations like the Document Type Deﬁnition (DTD). A DTD speciﬁes the available element types, the relationships between them, and the attributes and ranges of values for elements and attributes. MusicXML is an XML document format, deﬁned by its DTD. It takes its inspiration primarily from the MuseData (Selfridge-Field, 3 4) and Humdrum music encodings (Huron, 1997). MusicXML seeks to create a common interchange format between programs that use symbolic music data. The adoption of MusicXML by many popular notation programs has accelerated its usage signiﬁcantly, to the point where it is showing signs of becoming a de facto standard. The nested-element layout of XML ﬁles is best represented as a tree data structure. Tree structures and tree \\ \\ \\ \\ \\ l KKK ö \\ \\ \\  \\ \\ KKK ö \\ l \\ \\ \\ \\ \\ Figure 1. Correct and Incorrect Encoding comparisons are one of the most-studied problems in computer science, so it would seem on the surface that creating a “diff” program for MusicXML documents would be easy, especially since there has been a fair amount of research speciﬁcally on comparing XML (Cobena et al., 2001). However, this assumption is incorrect. In all, three types of comparison difﬁculties can be identiﬁed:",
        "zenodo_id": 1416522,
        "dblp_key": "conf/ismir/KnopkeB07"
    },
    {
        "title": "Towards Integration of MIR and Folk Song Research.",
        "author": [
            "Peter van Kranenburg",
            "Jörg Garbers",
            "Anja Volk",
            "Frans Wiering",
            "Louis P. Grijp",
            "Remco C. Veltkamp"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414754",
        "url": "https://doi.org/10.5281/zenodo.1414754",
        "ee": "https://zenodo.org/records/1414754/files/KranenburgGVWGV07.pdf",
        "abstract": "Folk song research (FSR) often deals with large collections of tunes that have various types of relations to each other. Computational methods can support the study of the contents of these collections. Music Information Retrieval (MIR) research provides such methods. Yet a fruitful cooperation of both disciplines is difﬁcult to achieve. We present a role-model to structure this cooperation in which tasks and responsibilities are distributed among the roles of MIR, Computational Musicology (CM) and FSR. 1 INTRODUCTION The goal of the WITCHCRAFT project (“What Is Topical in Cultural Heritage: Content-based Retrieval Among Folksong Tunes”) is to develop a content-based retrieval system for folk song melodies stored as audio and notation. This system will give access to the collection of folksong recordings and transcriptions of the Meertens Institute (a research institute for Dutch language and culture in Amsterdam). Its purposes are on the one hand to support Folk Song Research (FSR) 1 in classifying and identifying variants of folk songs and on the other hand to allow the general public to search for melodic content in the database of the Meertens Institute. In the current paper we focus on the former purpose. ‘Folk songs’ were sung by common people during work or social activities. One of their most important characteristics is that they are part of oral culture. The melodies and the texts are learned by imitation and participation rather than from books. In the course of this oral transmission, changes occur to the melodies. The resulting set of variants of a song form a so called ‘tune family’. Although attention has been paid to folk songs in the Music Information Retrieval (MIR) community, 2 very few studies focus on the particularities of orally transmitted melodies. In most cases folk songs were simply used because they were available as a test collection. As folk song melody research belongs to the domain of ethnomusicology, serious attempts to build software for process1 The equivalent of the German ‘Volksliedkunde’. 2 In this paper ‘MIR’ is taken in a very broad sense: not only speciﬁc retrieval research, but also other research that has computational processing of music as its subject. c⃝2007 Austrian Computer Society (OCG). ing folk song melodies should model concepts and methods that were developed in ethnomusicology. But this is not yet standard practice. Major impediments for fruitful collaboration are the unfamiliarity of researchers in both ﬁelds with each other’s methods and traditions, and the non-formalised nature of FSR concepts and theories. Therefore we need to ﬁnd an approach to bridge this gap. In this paper we ﬁrst give overviews of relevant work that has been done in both disciplines, and after that we describe an approach that may lead to better cooperation. 2 FOLK SONGS IN MIR Only a limited number of MIR applications and studies are speciﬁcally aimed at searching folk song collections. Some online search engines allow the user to search in a large collection of folk song melodies. The Danish Folklore Archives [3] and the Digital Archive of Finnish Folk Tunes [6] are primarily meant for folk songs, while engines like Themeﬁnder [19], MELDEX [12] and Musipedia [13] have a more general scope. Only the Danish search engine posesses a query method that is motivated by FSR. One can search for a sequence of accented notes, which are assumed to be rather stable across variants of a melody. Folk song melodies have been used as data in a considerable number of MIR studies. In some cases folk songs were chosen because of their availability and not because of an interest in folk music as such. This applies to all eight papers in the complete ISMIR proceedings from 2001–2006 that employ the Essen folk song collection [16]. In none of these papers the implications of the choice of this data set is discussed. In most cases it is simply stated that this collection is used, or a pragmatic reason is provided, e.g., the need for a large music database, or the need for a collection of monophonic songs. The results of the more general questions addressed, such as meter classiﬁcation, benchmark establishing or segmentation, have not been interpreted concerning their potential to contribute to folk song research. In broader MIR circles some more studies have been done that particularly focus on folk songs. The work of Zolt´an Juh´asz [10] is highly relevant in this respect. He selects his algorithms for their ability to answer questions about the data (mainly Hungarian folk song melodies) instead of employing the data to answer questions about his algorithms. By clustering contour representations of the melodies in various ways, his studies reveal differences between oral traditions in various countries. Another relevant publication is an article by David Huron in which he proposes to visualize geographic differences in music by showing densities on a map [9]. As an example, using the Essen collection, Huron visualizes the geographic density of certain types of cadence notes, showing that Western European songs are nearly three times as likely to have most of their phrases end on a note other than the tonic compared to Eastern European songs. Of the mentioned papers only those of Juh´asz explicitly state an interest in folk songs as part of oral traditions. 3 SOME PAST AND CURRENT APPROACHES TO MELODY IN FOLK SONG RESEARCH During the last century, the availability of collected folk song tunes has generated a considerable amount of musicological research. One of the primary concerns is how to deal with the speciﬁc type of variation caused by the process of oral transmission. Therefore we will ﬁrst discuss oral transmission. Then classiﬁcation and identiﬁcation of melodies in the context of FSR will be discussed.",
        "zenodo_id": 1414754,
        "dblp_key": "conf/ismir/KranenburgGVWGV07"
    },
    {
        "title": "Automated Synchronization of Scanned Sheet Music with Audio Recordings.",
        "author": [
            "Frank Kurth",
            "Meinard Müller",
            "Christian Fremerey",
            "Yoon-ha Chang",
            "Michael Clausen"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416918",
        "url": "https://doi.org/10.5281/zenodo.1416918",
        "ee": "https://zenodo.org/records/1416918/files/KurthMFCC07.pdf",
        "abstract": "In this paper, we present a procedure for automatically synchronizing scanned sheet music with a corresponding CD audio recording, where suitable regions (given in pixels) of the scanned digital images are linked to time positions of the audio ﬁle. In a ﬁrst step, we extract note parameters and 2D position information from the scanned images using standard software for optical music recognition (OMR). We then use a chroma-based synchronization algorithm to align the note parameters to the given audio recording. Our experiments show that even though the output of current OMR software is often erroneous, the music parameters extracted from the digital images still sufﬁce to derive a reasonable alignment with the audio data stream. The resulting link structure can be used to highlight the current position in the scanned score or to automatically turn pages during playback of an audio recording. Such functionalities have been realized as plug-in for the SyncPlayer, which is a free prototypical software framework for bringing together various MIR techniques and applications. 1 INTRODUCTION Modern digital music libraries contain large amounts of textual, visual, and audio data as well as a variety of associated data representations. In particular for Western classical music, two prominent examples of widely-used and digitally available types of music representations are scanned sheet music (available as digital images) and audio recordings (e. g., in CD or MP3 format). These two representations complement each other describing music on different semantic levels. On the one hand, sheet music, which in our context denotes a printed form of musical score notation, is used to visually describe a piece of music in a compact and human readable from. Sheet music not only allows a musician to create a performance but it also reveals structural, harmonic, or melodic aspects of the music that may not be obvious from mere listening. On the other hand, an audio recording encodes the soundwave of an acoustic realization, which allows the listener to play-back a speciﬁc interpretation. c⃝2007 Austrian Computer Society (OCG). Given various representations of musically relevant information, e. g., as encoded by sheet music or as given by a speciﬁc audio recording, the identiﬁcation of semantically related events is of great relevance for music retrieval and browsing applications [1, 4, 5]. Here, we will discuss the problem of scan-audio synchronization, which refers to the problem of linking regions (given as pixel coordinates) within the scanned images of given sheet music to semantically corresponding physical time positions within an audio recording. Such linking structures can be used to highlight the current position in the scanned score during playback of the recording, thus enhancing the listening experience as well as providing the user with tools for intuitive and multimodal music exploration. The importance of such a functionality, which is illustrated by Figure 4, has been emphasized in the literature, see, e. g., [4]. In this paper, we present a procedure for scan-audio synchronization, which is, to the best of the author’s knowledge, the ﬁrst algorithm for performing this task in a fully automated fashion. The general idea is to transform both the scanned images as well as the corresponding audio recording into chroma-based mid-level representations, which can then be time-aligned via dynamic time warping. The details of our synchronization approach will be explained in Section 2. As one important subtask, we extract musical note events as well as the corresponding 2D pixel regions from the scanned sheet music using standard software for optical music recognition (OMR). In Section 3, we will discuss this OMR step and the problems arising from OMR recognition errors. The mid-level representations, as our experiments show, are robust enough to be successfully used for the scan-audio synchronization task even when corrupted by erroneous note events or by missing signatures. In Section 4, as a further contribution of this paper, we present a novel visualization plug-in for the SyncPlayer framework [6]. This interface synchronously displays the score position within the scanned images along with the audio playback. In the SyncPlayer context, the scan-audio synchronization constitutes a key component for a comprehensive tool facilitating multimodal navigation and retrieval in complex and inhomogeneous music collections. We conclude this paper with Section 5. Further references to related work as well as prospects on future work are given in the respective sections. 2 SYNCHRONIZATION PROCEDURE In this section, we describe our approach to scan-audio synchronization and summarize the background on the underlying techniques from MIR and audio signal processing. As discussed in the introduction, the input of the scanaudio synchronization algorithm consists of a scanned version of a musical score and a corresponding audio recording of the same piece of music. As an example, Figure 1 shows the ﬁrst few measures of Beethoven’s Piano Sonata No. 23, Op. 57 (“Appassionata”) as well as the waveform of a recording by Barenboim of the same measures. In the ﬁrst step of our synchronization algorithm, we transform the scanned score as well as the audio recording into a common mid-level representation, which allows for comparing and relating music data in various realizations and formats. To be more speciﬁc, we use chroma-based features, where the chroma correspond to the twelve traditional pitch classes of the equaltempered scale [2]. In Western music notation, the chroma correspond to the set {C, C♯, D, . . . , B} consisting of the twelve pitch spelling attributes. The audio recording is transformed into a sequence of normalized 12-dimensional chroma vectors, where each vector expresses the signal’s local energy distribution among the 12 pitch classes. Such a chroma representation can be obtained from a spectrogram by suitably pooling Fourier coefﬁcients; for details, we refer to [2]. Figure 1 shows the resulting audio chromagram for our Beethoven example. Chroma-based audio features absorb variations in parameters such as dynamics, timbre, and articulation and closely correlate to the short-time harmonic content of the underlying audio signal. The transformation of the scanned score representation into a chroma representation consists of several steps. First, the score data such as note events, the key signature, the time signature, and other musical symbols are extracted from the scanned images using standard software for optical music recognition (OMR). Note that in addition to the musical score data, the OMR process provides us with pixel coordinates of the extracted data, allowing us to exactly localize all musical symbols within the scanned images. The technical details and the OMRinvolved problems of this step will be discussed separately in Section 3. Then, in the second step, a sequence of chroma features is synthesized from the OMR result. This is done in a straightforward fashion by using the sequence of the extracted note events, which are encoded by parameters for pitch as well as musical onset time and duration. Physical onset times and durations are calculated assuming a constant tempo. Note that in our case the particular choice of tempo is not crucial because differences in tempo will be compensated in the subsequent synchronization step. We choose a standard value of 120 bpm. A sequence of chroma features is then synthesized by sliding across the time axis with a temporal window while adding energy to the chroma bands that correspond to pitches that Scanned Score OMR and conversion Chroma",
        "zenodo_id": 1416918,
        "dblp_key": "conf/ismir/KurthMFCC07"
    },
    {
        "title": "Vivo Visualizing Harmonic Progressions and Voice-Leading in PWGL.",
        "author": [
            "Mika Kuuskankare",
            "Mikael Laurson"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418189",
        "url": "https://doi.org/10.5281/zenodo.1418189",
        "ee": "https://zenodo.org/records/1418189/files/KuuskankareL07.pdf",
        "abstract": "This paper describes a novel tool called VIVO (VIsual VOice-leading) that allows to visually deﬁne harmonic progressions and voice-leading rules. VIVO comprises of a compiler and a collection of specialized visualization devices. VIVO takes advantage of several music related applications collected under the umbrella of PWGL (PWGL is a free cross-platform visual programming language for music and sound related applications). Our music notation application–Expressive Notation Package or ENP–is used here to build the user-interface used to visually deﬁne harmony and voice-leading rules. These visualizations are converted to textual rules by the VIVO compiler. Finally, our rule-based compositional system, PWGLConstraints, is used generate the ﬁnal musical output using these rules. 1 BACKGROUND The musical problems that are interesting in terms of musical constraints programming are typically very demanding. Here, harmony, melody, voice-leading, and counterpoint provide challenges not only in an aesthetic sense but also in terms of a formal deﬁnition. Deﬁning textually rules that solve a certain compositional or music analytical problem is a time consuming task and requires a lot of both musical and programming expertise. VIVO allows to visually deﬁne rudimentary rules of counterpoint. The approach presented here resembles the traditional teaching situation where a teacher or a textbook gives out examples of correct use of harmonic progressions or voice-leading. Using music notation as a framework for constructing the VIVO user-interface provides several advantages: (1) it is easy to write the rules, i.e., the user sees the exact musical context the rule is applied to; (2) it is possible to verify the correctness of the data by ’listening’ to the rules; (3) it is straightforward to edit, add and remove data; and (4) the user can easily edit the rule set and make subsets of it. Our environment for computer assisted composition, PWGL ([3]; http://www.siba.fi/pwgl/), is used to implement VIVO. PWGL offers a unique combination of graphical and textual programming tools. The two of c⃝2007 Austrian Computer Society (OCG). which are of primary importance in terms of VIVO are PWGLConstraints [4] and ENP [2]. VIVO uses an ENP score as a user-interface component. By entering musical material into the score makes it possible to deﬁne given aspects of harmony and voiceleading. The visual representation is then fed to the VIVO compiler which in turn generates textual rules that are suitable for PWGLConstraints. The ﬁnal output generated by PWGLConstraints can then be shown in common music notation using ENP. There are also other rule-based systems that have been used to solve musical constraint satisfaction problems, e.g., Situation, Arno, OMClouds, Strasheela (see [1] for more information) and the choral harmonizer system by Kemal Ebcioglu. Furthermore, integrating visual tools in constraints programming has been studied, for example, in [5]. However, using music notation to deﬁne rules of counterpoint, in the way the VIVO does, is unique and cannot be found elsewhere. This paper gives a brief overview of some of the currently available VIVO tools. 2 THE VIVO TOOLS",
        "zenodo_id": 1418189,
        "dblp_key": "conf/ismir/KuuskankareL07"
    },
    {
        "title": "Metadata Infrastructure for Sound Recordings.",
        "author": [
            "Catherine Lai",
            "Ichiro Fujinaga",
            "David Descheneau",
            "Michael Frishkopf",
            "Jenn Riley",
            "Joseph Hafner",
            "Brian McMillan"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415224",
        "url": "https://doi.org/10.5281/zenodo.1415224",
        "ee": "https://zenodo.org/records/1415224/files/LaiFDFRHM07.pdf",
        "abstract": "This paper describes the first iteration of a working model for searching heterogeneous distributed metadata repositories for sound recording collections, focusing on techniques used for real-time querying and harmonizing diverse metadata models. The initial model for a metadata infrastructure presented here is the first of its kind for sound recordings.",
        "zenodo_id": 1415224,
        "dblp_key": "conf/ismir/LaiFDFRHM07"
    },
    {
        "title": "Using 3D Visualizations to Explore and Discover Music.",
        "author": [
            "Paul Lamere",
            "Douglas Eck"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415022",
        "url": "https://doi.org/10.5281/zenodo.1415022",
        "ee": "https://zenodo.org/records/1415022/files/LamereE07.pdf",
        "abstract": "This paper presents Search Inside the Music an application for exploring and discovering new music. Search Inside the Music uses a music similarity model and 3D visualizations to provide a user with new tools for exploring and interacting with a music collection. With Search Inside the Music, a music listener can ﬁnd new music, generate interesting playlists, and interact with their music collection. 1 INTRODUCTION Tools that help listeners ﬁnd new music, re-ﬁnd forgotten music, and create coherent playlists become increasingly important as music collections grow in size. At the same time, it is important to remember that the primary purpose of music is entertainment. As such, activities surrounding music such as music discovery and playlist generation should be engaging and entertaining activities. In this paper we describe the Search Inside the Music application. SITM uses interactive 3D visualizations of a music similarity space to allow a music listener to explore their music collection, to receive recommendations for new music, to generate interesting and coherent playlists, and to interact with the album artwork of a music collection. The resulting user interface is arguably more engaging and enjoyable to use than currently available commercial music interfaces. 2 MUSIC SIMILARITY The visualizations in the Search Inside the Music system rely on a music similarity model. The system is decoupled from the similarity model such that any music similarity model that can quickly determine the distance between any two songs in a music collection can be used. We’ve successfully used models based on the Marysyas system [3] as well as models developed here at Sun Microsystems [4]. Experience has shown that similarity models based upon maximum-margin classiﬁers such a support vector machines generate poor visualizations since these classic⃝2007 Austrian Computer Society (OCG). Figure 1. 3D View of the Music Space ﬁers tend to result in a number of tight clusters that are not well-suited for exploration and visualization. 3 VISUALIZATIONS Having a model of the musical similarity space allows us to represent a music collection in new and interesting ways. Figure 1 shows a 3-dimensional visualization of the music space. In this visualization songs are represented by spheres ﬂoating in space. Spheres that are close together in this space represent songs that are musically similar. The current playlist is shown by a connected sequence of ﬂoating album covers. Larger spheres represent songs that are favored by the listener. The music listener can interact with this visualization to explore the music space. Clicking on a sphere will cause the represented song to play. Clicking on the ‘More like this’ button will add more songs to the visualization that sound similar to the current song. The listener can generate interesting and varied playlists by selecting two end point songs and having the browser generate a path through the music collection that best connects the songs, minimizing jarring song transitions. Interacting in this way with the music collection can provide interesting insights into its contents and composition and provides an arguably more enjoyable interaction than is typically found in a more traditional music browser that relies primarily on lists of song titles and artist names. SITM uses a multi-dimensional scaling technique to project the high-dimensional music space into three diFigure 2. The Album Cloud Figure 3. The Album Grid mensions similar to the approaches used in [2] with optimizations described in [1].",
        "zenodo_id": 1415022,
        "dblp_key": "conf/ismir/LamereE07"
    },
    {
        "title": "Enabling Access to Sound Archives Through Integration, Enrichment and Retrieval: The EASAIER Project.",
        "author": [
            "Christian Landone",
            "Joseph Harrop",
            "Josh Reiss"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415584",
        "url": "https://doi.org/10.5281/zenodo.1415584",
        "ee": "https://zenodo.org/records/1415584/files/LandoneHR07.pdf",
        "abstract": "Many  digital  sound  archives  suffer  from  problems concerning on-line access: sound materials are often held separately from other related media, they are not easily browsed and little opportunity to search the actual audio content of the material is provided. The  EASAIER  project  aims  to  alleviate  these problems, offering a number of solutions to support sound archive  managers  and  users.  EASAIER  will  enable enhanced access to sound archives, providing multiple",
        "zenodo_id": 1415584,
        "dblp_key": "conf/ismir/LandoneHR07"
    },
    {
        "title": "A Digital Collection of Brazilian Lundus.",
        "author": [
            "Rosana S. G. Lanzelotte",
            "Adriana O. Ballesté",
            "Martha Ulhoa"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415634",
        "url": "https://doi.org/10.5281/zenodo.1415634",
        "ee": "https://zenodo.org/records/1415634/files/LanzelotteBU07.pdf",
        "abstract": "Lundu is a typical Brazilian popular musical form at the 19th century. The distinguished musicologist Mozart de Araújo devoted himself to studying lundus and other forms of that period. He collected 48 lundus, which are nowadays stored in a private library, unavailable to public access. The present work describes the implementation of a digital collection of those lundus, using Dspace as the repository. Dspace is chosen in order to guarantee interoperability through the OAIPMH protocol. Metadata is generated using Dublin Core elements, fully compatible with Dspace. The digital collection provides access to the lundu score images, incipits and midi files, as well as metadata. It is the first time such a rare collection of 19th Brazilian popular music will be available on the web. As Dspace enables interoperation among repositories, a broad community may access the collection.",
        "zenodo_id": 1415634,
        "dblp_key": "conf/ismir/LanzelotteBU07"
    },
    {
        "title": "MIR in Matlab (II): A Toolbox for Musical Feature Extraction from Audio.",
        "author": [
            "Olivier Lartillot",
            "Petri Toiviainen"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417145",
        "url": "https://doi.org/10.5281/zenodo.1417145",
        "ee": "https://zenodo.org/records/1417145/files/LartillotT07.pdf",
        "abstract": "We present the MIRtoolbox, an integrated set of functions written in Matlab, dedicated to the extraction of musical features from audio ﬁles. The design is based on a modular framework: the different algorithms are decomposed into stages, formalized using a minimal set of elementary mechanisms, and integrating different variants proposed by alternative approaches – including new strategies we have developed –, that users can select and parametrize. This paper offers an overview of the set of features, related, among others, to timbre, tonality, rhythm or form, that can be extracted with the MIRtoolbox. One particular analysis is provided as an example. The toolbox also includes functions for statistical analysis, segmentation and clustering. Particular attention has been paid to the design of a syntax that offers both simplicity of use and transparent adaptiveness to a multiplicity of possible input types. Each feature extraction method can accept as argument an audio ﬁle, or any preliminary result from intermediary stages of the chain of operations. Also the same syntax can be used for analyses of single audio ﬁles, batches of ﬁles, series of audio segments, multi-channel signals, etc. For that purpose, the data and methods of the toolbox are organised in an object-oriented architecture. 1 MOTIVATION AND APPROACH MIRtoolbox is a Matlab toolbox dedicated to the extraction of musically-related features from audio recordings. It has been designed in particular with the objective of enabling the computation of a large range of features from databases of audio ﬁles, that can be subjected to statistical analyses. Few softwares have been proposed in this area. One particularity of our own approach relies in the use of the Matlab computing environment, which offers good visualisation capabilities and gives access to a large variety of other toolboxes. In particular, the MIRtoolbox makes use of functions available in public-domain toolboxes such as the Auditory Toolbox [6], NetLab [5] and SOMtoolbox [10]. Other toolboxes, such as the Statistics toolbox or the Neural Network toolbox from MathWorks, can be directly used for further analyses of the features extracted c⃝2007 Austrian Computer Society (OCG). by MIRtoolbox without having to export the data from one software to another. Such computational framework, because of its general objectives, could be useful to the research community in Music Information Retrieval (MIR), but also for educational purposes. For that reason, particular attention has been paid concerning the ease of use of the toolbox. In particular, complex analytic processes can be designed using a very simple syntax, whose expressive power comes from the use of an object-oriented paradigm. The different musical features extracted from the audio ﬁles are highly interdependent: in particular, as can be seen in ﬁgure 1, some features are based on the same initial computations. In order to improve the computational efﬁciency, it is important to avoid redundant computations of these common components. Each of these intermediary components, and the ﬁnal musical features, are therefore considered as building blocks that can been freely articulated one with each other. Besides, in keeping with the objective of optimal ease of use of the toolbox, each building block has been conceived in a way that it can adapt to the type of input data. For instance, the computation of the MFCCs can be based on the waveform of the initial audio signal, or on the intermediary representations such as spectrum, or mel-scale spectrum (see Fig. 1). Similarly, autocorrelation is computed for different range of delays depending on the type of input data (audio waveform, envelope, spectrum). This decomposition of all feature extraction algorithms into a common set of building blocks has the advantage of offering a synthetic overview of the different approaches studied in this domain of research. 2 FEATURE EXTRACTION",
        "zenodo_id": 1417145,
        "dblp_key": "conf/ismir/LartillotT07"
    },
    {
        "title": "TagATune: A Game for Music and Sound Annotation.",
        "author": [
            "Edith L. M. Law",
            "Luis von Ahn",
            "Roger B. Dannenberg",
            "Mike Crawford"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415568",
        "url": "https://doi.org/10.5281/zenodo.1415568",
        "ee": "https://zenodo.org/records/1415568/files/LawADC07.pdf",
        "abstract": "Annotations of audio ﬁles can be used to search and index music and sound databases, provide data for system evaluation, and generate training data for machine learning. Unfortunately, the cost of obtaining a comprehensive set of annotations manually is high. One way to lower the cost of labeling is to create games with a purpose that people will voluntarily play, producing useful metadata as a by-product. TagATune is an audio-based online game that aims to extract descriptions of sounds and music from human players. This paper presents the rationale, design and preliminary results from a pilot study using a prototype of TagATune to label a subset of the FreeSound database. 1 INTRODUCTION Human computation, the idea of channeling the collective human presence over networks to solve difﬁcult AI problems, has had great success. One of the ﬁrst realizations of this idea is an online game called ESP [10], later adopted as the Google image labeler, where two players collaborate to label images on the internet. The result is one of the largest existing databases of images with labeled content. More than half of the nation now has access to the internet, and 42% of those with access play games online [6]. The so-called games with a purpose take advantage of this burgeoning human interest in online games to solve important technological problems. This paper presents the rationale and iterative design of a new game called TagATune, which elicits from human players descriptions of sounds and music (collectively referred to as tunes). 2 RATIONALE TagATune is a human computation tool that is capable of gathering perceptually meaningful descriptions for audio data that are agreed upon by multiple players. Such data are useful for several purposes described below. Large audio databases have become invaluable resources for listeners, sound designers and composers. Current audio retrieval systems are primarily text-based, relying on accurate and comprehensive annotation of the data. However, keywords that describe a particular audio ﬁle are often subjective, based on one person’s opinion [11]. c⃝2007 Austrian Computer Society (OCG). TagATune has the potential to produce better labels at lower cost because the labor is essentially free and the validity of each label is conﬁrmed by many players. Researchers have long been interested in what makes a tune warm or cold, scary or pleasant, bright or dull, happy or sad and to what extent. For example, ongoing studies attempt to relate the perceptions [2, 7] of sounds to their acoustical and physical properties. A better understanding of auditory perception can yield important insights for retrieval tasks and auditory-enhanced interfaces. Part of the labeled data collected by TagATune is based on shared perception of sounds and are useful towards this kind of psychoacoustic or phenomenological research. Another application is creating CAPTCHAs for the visually impaired. As a line of defense against bots, many websites now require humans to pass a visual CAPTCHA [9], which usually involves reading distorted characters against cluttered backgrounds. Common imageand textbased CAPTCHAs are inaccessible to the visually impaired. In addition, audio CAPTCHAs is a much needed alternative to visual CAPTCHAs, which have been successfully broken by various algorithms [4, 5]. Audio CAPTCHAs based on recorded speech exist, but as automatic speech recognition systems improve, labeled audio may offer more effective alternatives. TagATune differs from two recently developed music annotation games, MajorMiner [3] and The Listen Game [8], in several ways. First, in TagATune, players are immersed in an audio environment that is not limited to music, but also a variety of sound clips, which are essential for the production of usable sound CAPTCHAs. Second, instead of playing ofﬂine against a database [3] or simultaneously against multiple players [8], players of TagATune are paired with a partner with whom they must collaborate to label a given tune. This enhanced level of rapport shown to be critical in the popularity of the ESP game. Finally, in addition to tags, TagATune collects comparative information about sounds and music, which allows for more discriminative audio search. 3 TAGATUNE: A PROTOTYPE A prototype of TagATune is built in order to experiment with the game mechanics and to evaluate the usability and reception of the game. TagATune prototype is a simpliﬁed version of the game which uses sound clips only, lacking a blind-accessible interface, safeguards against cheating and inappropriate content, and other text matching and veriﬁcation capability such as spell checking and synonym matching. In addition, the prototype is a simulated two-player game. In it, players have the illusion that they are playing with human partners, whereas in reality, they are playing with a simulated player, or a bot, whose moves and timing come from recorded sequences of previous gameplay. The use of a bot is an indispensable component of the game, since it ensures that every player is paired when there is an odd number of them. The audio used in the TagATune prototype are sound clips provided by the FreeSound Project. Within two months of its release, the project had “attracted over 2,300 members with over 1,650 sound contributions totaling more than 300 minutes of sound” (freesound.iua.upf.edu) and tens of thousands of downloads. Clips can be ﬁeld recordings or synthesized audio containing a wide range of possible sounds, including music, rhythm, effects, ambience noise and speech. The collection of 23,084 sounds in the database vary between 0 and 4,522s in duration. For the prototype, 100 sound clips that are approximately 10 seconds long were randomly chosen. Limiting the amount of data makes it feasible to evaluate manually the quality and universality of the descriptions independent players submitted for each sound.",
        "zenodo_id": 1415568,
        "dblp_key": "conf/ismir/LawADC07"
    },
    {
        "title": "Audio Fingerprint Identification by Approximate String Matching.",
        "author": [
            "Jérôme Lebossé",
            "Luc Brun"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417927",
        "url": "https://doi.org/10.5281/zenodo.1417927",
        "ee": "https://zenodo.org/records/1417927/files/LebosseB07.pdf",
        "abstract": "An audio ﬁngerprint is a small digest of an audio ﬁle which allows to identify it among a database of candidates. This paper ﬁrst presents a ﬁngerprint extraction algorithm. The identiﬁcation task is performed by a new identiﬁcation scheme which combines string matching algorithms and q-grams ﬁltration. 1 ROBUST FINGERPRINT EXTRACTION METHOD Given an input signal sample eventually corrupted, its ﬁngerprint allows to quickly retrieve the original ﬁle among a database of known audio ﬁles when it exists. Since an input audio sample should be identiﬁed, audio ﬁngerprints should be composed of elementary keys (called subﬁngerprints) computed continuously along the signal. Fingerprints construction schemes are usually based on a sequence of overlapping windows on which sub-ﬁngerprint values are computed [3]. This method called enframing insures a ﬁxed detection rate of sub-ﬁngerprints but is sensitive to cropping or shifting operations (Section 3). On the other hand, onset methods [1] are less sensitive to cuts alterations. However, these methods don’t insure any minimal detection rate of frames per second which may allow to bound the time required for the identiﬁcation of an audio ﬁle. Our idea is to combine the respective advantages of enframing and onsets methods by selecting a small time interval (Iemax) with maximal energy within a larger one (Io) (Figure 1). Then, this signal segmentation process allows to quickly synchronize two ﬁngerprints despite eventual shift operations. We then deﬁne the sub-ﬁngerprint values as the lengths (in ms) between two consecutive Iemax. More details about our ﬁngerprint extraction algorithm may be found in [4]. 2 FINGERPRINT RECOGNITION Several recognition techniques [3] may be used to compare an input ﬁngerprint to a database. Within our framec⃝2007 Austrian Computer Society (OCG). Figure 1. Audio Segmentation Method work, string matching methods provide a natural framework to account for shifted or wrongly detected Iemax intervals between two ﬁngerprints. Highly similar sequences between two strings are identiﬁed by computing an edit distance. Using a ﬁnite alphabet, the edit distance is often scored using a simple metric where a match is awarded a positive score while a mismatch is awarded a negative score. However within the ﬁngerprint recognition framework, two co-derivative ﬁngerprints should share long sequences of common symbols with few mismatches. We thus designed a scoring function which increases non linearly when a sequence of matches is encountered. This scoring function is deﬁned as: S(i, j) =        αS(i −1, j −1) + β If si = sj 1 γ max   0, S(i, j −1), S(i −1, j)  −β otherwise (1) The constants α, β, γ are determined experimentally (Section 3) and satisfy 1 < γ < α in order to decrease the score when mismatches are encountered at a lower step than it increases during a sequence of matching symbols. A comparison of an input ﬁngerprint with all the database ﬁngerprints may be performed using string matching",
        "zenodo_id": 1417927,
        "dblp_key": "conf/ismir/LebosseB07"
    },
    {
        "title": "Preliminary Analyses of Information Features Provided by Users for Identifying Music.",
        "author": [
            "Jin Ha Lee 0001",
            "J. Stephen Downie",
            "M. Cameron Jones"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415860",
        "url": "https://doi.org/10.5281/zenodo.1415860",
        "ee": "https://zenodo.org/records/1415860/files/LeeDJ07.pdf",
        "abstract": "This paper presents preliminary findings based on the analyses of user-provided information features found in 566 queries seeking help in the identification of particular music works or artists. Queries were drawn from the answers.google.com (Google Answers) website. The types and frequency of occurrences of different information features are compared with the results from previous studies of music queries. New feature types have also been developed to obtain a more comprehensive understanding of the kinds of information present in queries including such things as indications of uncertainty, associated use, and the “aboutness” of the underlying musical work. The presence of erroneous information in the queries is also discussed.",
        "zenodo_id": 1415860,
        "dblp_key": "conf/ismir/LeeDJ07"
    },
    {
        "title": "A Unified System for Chord Transcription and Key Extraction Using Hidden Markov Models.",
        "author": [
            "Kyogu Lee",
            "Malcolm Slaney"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415208",
        "url": "https://doi.org/10.5281/zenodo.1415208",
        "ee": "https://zenodo.org/records/1415208/files/LeeS07.pdf",
        "abstract": "A new approach to acoustic chord transcription and key extraction is presented. As in an isolated word recognizer in automatic speech recognition systems, we treat a musical key as a word and build a separate hidden Markov model for each key in 24 major/minor keys. In order to acquire a large set of labeled training data for supervised training, we ﬁrst perform harmonic analysis on symbolic data to extract the key information and the chord labels with precise segment boundaries. In parallel, we synthesize audio from the same symbolic data whose harmonic progression are in perfect alignment with the automatically generated annotations. We then estimate the model parameters directly from the labeled training data, and build 24 key-speciﬁc HMMs. The experimental results show that the proposed model not only successfully estimates the key, but also yields higher chord recognition accuracy than a universal, key-independent model. 1 INTRODUCTION A musical key and a chord are among important attributes of Western tonal music. A key deﬁnes a referential point or a tonal center upon which other musical phenomena such as melody, harmony, and cadence are arranged. Particularly, a key and succession of chords over time, or chord progression based on the key forms the core of harmony in a piece of music. Hence analyzing the overall harmonic structure of a musical piece often starts with labeling every chord at every beat or measure based on the key. Finding the key and labeling the chords automatically from audio are of great use for those who want to do harmonic analysis of music. Once the harmonic content of a piece is known, a sequence of chords can be used for further higher-level structural analysis where themes, phrases or forms can be deﬁned. Chord sequences and the timing of chord boundaries are also a compact and robust mid-level representation of musical signals, and have many potential applications such as music identiﬁcation, music segmentation, music c⃝2007 Austrian Computer Society (OCG). similarity ﬁnding, audio summarization, and mood classiﬁcation. Chord sequences have been successfully used as a front end to the audio cover song identiﬁcation system [1]. For these reasons and others, automatic chord recognition has recently attracted a number of researchers in the Music Information Retrieval ﬁeld. Some systems use a simple pattern matching algorithm [2, 3, 4] while others use more sophisticated machine learning techniques such as hidden Markov models or Support Vector Machines [5, 6, 7, 8, 9]. Hidden Markov models (HMMs) are very successful for speech recognition, whose high performance is largely attributed to gigantic databases with labels accumulated over decades. Such a huge database not only helps estimate the model parameters appropriately, but also enables researchers to build richer models, resulting in better performance. However, there are very few such database available for music. Furthermore, the acoustical variance in music is far greater than that in speech in terms of its frequency range, timbre due to different instrumentations, dynamics, and/or duration, and thus even more data is needed to build generalized models. It is very difﬁcult to obtain a large set of training data for music, however. First of all, it is very hard for researchers to acquire a large collection of music. Secondly, hand-labeling the chord boundaries in a number of recordings is not only an extremely time consuming and tedious task but also is subject to errors made by humans. In order to automate the extremely laborious task of obtaining labeled training data for supervised learning algorithm, we use symbolic data such as MIDI ﬁles to generate chord names and precise time boundaries, as well as to create audio. Audio and chord-boundary information generated from the same symbolic ﬁles are in perfect alignment, and we can use them to directly estimate the model parameters. In doing so, we build 24 key-speciﬁc models, one for each for 24 major/minor keys. In this paper, we present a new approach to key estimation and chord recognition at the same time by borrowing the idea from isolated word recognizers. In an isolated word recognizer using HMMs, as described in [10], V number of word models are ﬁrst built, and when an unknown word is given, the observation sequence is obtained via a proper feature analysis. Then model likelihoods are computed for all V models, and word recognition is done by selecting a model whose likelihood is highest. Likewise, once we trained 24 key-speciﬁc HMMs, key estimation is accomplished by selecting the key model with the highest likelihood given the observation sequence of input audio; i.e., key = argmax k Pr(O, Q|λk), (1) where key is an estimated key, O is an observation sequence, Q is a state path, and λk is a key model for key k. Once the key model is selected from Equation 1, we can obtain the chord sequence by taking the optimal state path QOP T = Q1Q2 · · · QT returned by the Viterbi decoder. The overall system for key estimation and chord recognition is shown in Figure 1. Input: = optimal state path = chord sequence Pr(O, Q|λ1) key = argmaxk Pr(O, Q|λk) Pr(O, Q|λK) Pr(O, Q|λ2) λ1 λ2 λK O = O1O2 · · · OT Q = Q1Q2 · · · QT ... Figure 1. System for key estimation and chord recognition. There are several advantages to this approach. First, a great number of symbolic ﬁles are freely available. Second, we do not need to manually annotate key names or chord boundaries with chord names to obtain training data. Third, sufﬁcient training data enables us to build a model for each key, which not only results in increased performance for chord recognition but also provides key information. This paper is organized as follows. A review of related work is presented in Section 2; in Section 3, we explain the method of obtaining the labeled training data, and describe the procedure of building our models and the feature set we used to represent the state observation in the models; in Section 4, we present empirical results with discussions, and draw conclusions followed by directions for future work in Section 5. 2 RELATED WORK Several systems have been previously described for chord recognition from the raw audio waveform. Sheh and Ellis proposed a statistical learning method for chord segmentation and recognition using the chroma features [5]. They used the hidden Markov models (HMMs) trained by the Expectation-Maximization(EM) algorithm, and treated the chord labels as hidden values within the EM framework. In training the models, they used only the chord sequence as an input to the models, and applied the forward-backward algorithm to estimate the model parameters. The frame accuracy they obtained was about 76% for segmentation and about 22% for recognition, respectively. The poor performance for recognition may be due to insufﬁcient training data compared with a large set of classes (147 chord types trained on 20 songs). It is also possible that the ﬂat-start initialization of training data yields incorrect chord boundaries resulting in poor parameter estimates. Bello and Pickens also used the chroma features and HMMs with the EM algorithm to ﬁnd the crude transition probability matrix for each input [6]. What was novel in their approach was that they incorporated musical knowledge into the models by deﬁning a state transition matrix based on the key distance in a circle of ﬁfths, and avoided random initialization of a mean vector and a covariance matrix of observation distribution. In addition, in training the model’s parameter, they selectively updated the parameters of interest on the assumption that a chord template or distribution is almost universal regardless of the type of music, thus disallowing adjustment of distribution parameters. The accuracy thus obtained was about 75% using beat-synchronous segmentation with a smaller set of chord types (24 major/minor triads only). In particular, they argued that the accuracy increased by as much as 32% when the adjustment of the observation distribution parameters is prohibited. The present paper expands our previous work on chord recognition [8, 9], where we used symbolic data to obtain a large amount of labeled training data from which model parameters can be directly estimated without using an EM algorithm where model initialization is critical. In this paper, however, we ﬁrst propose a uniﬁed model to accomplish simultaneously two closely related musical tasks – key estimation and chord transcription – by building key-speciﬁc HMMs. Furthermore, we use a feature vector called Tonal Centroid instead of chroma vector, which has been the feature set of choice in chord recognition systems. To authors’ knowledge, this work is the ﬁrst to use a tonal centroid vector for key extraction as well as for chord transcription. 3 SYSTEM Our chord transcription system starts off by performing harmonic analysis on symbolic data to obtain label ﬁles with chord names and precise time boundaries. In parallel, we synthesize the audio ﬁles with the same symbolic ﬁles using a sample-based synthesizer. We then extract appropriate feature vectors from audio which are in perfect sync with the labels, and use them to train our models.",
        "zenodo_id": 1415208,
        "dblp_key": "conf/ismir/LeeS07"
    },
    {
        "title": "Globe of Music Music Library Visualization Using Geosom.",
        "author": [
            "Stefan Leitich",
            "Martin Topf"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416930",
        "url": "https://doi.org/10.5281/zenodo.1416930",
        "ee": "https://zenodo.org/records/1416930/files/LeitichT07.pdf",
        "abstract": "Music collections are commonly represented as plain textual lists of artist, title, album etc. for each contained music track. The large volume of personal music libraries makes them difﬁcult to browse and access for users. In respect to possible information visualization techniques, no established convenient user interfaces exist. By using a spherical self-organizing map algorithm on low level audio features and processing the resulting map data, a Geographic Information System is used to visualize a music collection. This results in an aspiring music library visualization, which can be handled intuitively by the user and even provides new possibilities for accessing a music collection in the digital domain. 1 INTRODUCTION The advent of digital media libraries has deﬁnitely passed. Nearly every media consumer is in possession of a digital library of media documents nowadays. This of course includes private music collections. In previous days, one browsed through her own music collection by picking CDs from shelves, rearranging stacks and heaps, while searching for the desired track. This was on the one hand an annoying task, but on the other hand somehow part of the listening experience. After having handled a CD a few hundred times, while searching for another one, the artwork as visual experience becomes strongly connected to the tracks the CD contains. In the digital domain the dominant interface for browsing a personal media library was, and partly still is, a search task in a long list of plain textual information. Those lists contain known metadata of a song like artist, title, album, etc. In commercially available media playback applications, the textual playlist is still the most prominent kind of library representation that the user has to deal with. One exception is Apple iTunes with its “cover ﬂow view”. In this visualization, the user has the possibility of browsing through the tracks of her media library, while the artwork is presented in a neatly animated horizontal lineup, with the selected song’s artwork in the center. c⃝2007 Austrian Computer Society (OCG). The coinciding popularity of Geographic Information Systems (GIS), for example Google Earth or NASA WorldWind 1 , means that users are familiar with the interactions possible in such a system. One can easily navigate by rotating, tilting and zooming to access the desired view. Our approach, Globe of Music, makes use of a spherical self-organizing map (SOM) algorithm, to arrange the items of a digital music library on a sphere in context of a GIS. 2 RELATED WORK An early publication in the research domain of Music Information Retrieval (MIR) by Tzanetakis, tackled not only feature extraction aspects for audio and related classiﬁcation methods, but also possible visualization techniques in this domain [9]. He proposed these techniques as interaction methods to enhance audio editors supported by automatic feature extraction, but also browsers for audio collections to visualize timbre or genre information of audio in a 2D and 3D manner. Similar in structure to this approach is the work by Pampalk [7], who developed a psychoacoustically motivated approach for feature extraction and combined it with a visualization technique using a landscape metaphor the so-called Islands of Music. This approach was further developed by Pampalk himself, using aligned SOMs to support multiple, user-adjustable views of a music collection [5], still using the landscape metaphor. Knees et al. [2] interpreted the height proﬁle of the landscape metaphor to create a 3D representation of the Islands of Music. Additionally, they introduced an anisotropic auralization in this approach, by rendering a 5.1 surround sound model and augmenting the environment by related images and terms retrieved from the web in this user interface. Contrasting visualization techniques are presented in the work by Torrens et al. [8]. Here, 2D visualizations in the form of a disc, a rectangle, and a tree-map are presented, using criterias like genre, artist, year of creation, and a deﬁnable quantitative criterion such as playcount. Goto proposes in [1] an user interface Musicream combining the query-by-example and browsing paradigm. 1 http://worldwind.arc.nasa.gov Music tracks are visualized as music-discs and described by feature vectors. The so-called “streaming function” creates a steady ﬂow of music-discs over the screen. If the user picks one of these discs and approaches another disc with it, similar music-discs, in terms of feature vector distance, will stick together and form a playlist. Another approach is the joined work by Goto and Pampalk [6] called MusicRainbow. This approach exploits a combination of signal based information (spectral similarity and ﬂuctuation patterns) and information from the web. Visually, this depicts a “circular rainbow” with similar artists placed next to each other using a one-dimensional circular SOM. High-level terms (e.g.“rock”) on the inside and on the outside music-related terms (e.g. instruments, style attributes, etc.) are used for labeling. Colored rings encode different styles of music, and are sorted by the size of the styles part within the collection. A turning knob is used as an input device to browse the music collection. Our approach is motivated by the landscape metaphor, but augments this with a spherical representation using a spherical SOM Globe of Music. The spherical SOM intrinsically abolishes the border effect of a SOM, and the sphere or globe serves as the user interface metaphor of intuitive perception. A more detailed description is given in the next section. 3 APPROACH Our approach is composed of three steps: (1) extracting signal-based features from audio, (2) using the GeoSOM algorithm [11] to arrange music tracks according to feature vectors, and ﬁnally (3) transforming this information into a GIS renderable format, which depicts the user interface.",
        "zenodo_id": 1416930,
        "dblp_key": "conf/ismir/LeitichT07"
    },
    {
        "title": "Towards a Human-Friendly Melody Characterization by Automatically Induced Rules.",
        "author": [
            "Pedro J. Ponce de León",
            "David Rizo",
            "José Manuel Iñesta Quereda"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414902",
        "url": "https://doi.org/10.5281/zenodo.1414902",
        "ee": "https://zenodo.org/records/1414902/files/LeonRQ07.pdf",
        "abstract": "There is an increasing interest in music information retrieval for reference, motive, or thumbnail extraction from a piece in order to have a compact and representative representation of the information to be retrieved. One of the main references for music is its melody. In a practical environment of symbolic format collections the information can be found in standard MIDI ﬁle format, structured as a number of tracks, usually one of them containing the melodic line, while the others contain the accompaniment. The goal of this work is to analyse how statistical rules can be used to characterize a melody in such a way that one can understand the solution of an automatic system for selecting the track containing the melody in such ﬁles. 1 INTRODUCTION Digital music scores can be found in digital collections in a number of ways. One of the most popular formats is the standard MIDI ﬁle, where the information is structured in such a way that the melody part is stored in one or more tracks, often separately from the rest of the musical content. This is frequently the case of modern popular music. The literature about melody voice identiﬁcation in the symbolic domain is quite poor. Ghias et al. [2] built a system to process MIDI ﬁles extracting a sort of melodic line using simple heuristics. Tang et al. [7] present a work where the aim is to propose candidate melody tracks, given a MIDI ﬁle. They take decisions based on single features derived from informal assumptions about what a melody track should be. Other works in this line have been recently published at the light of recent developments [6] like that of Madsen and Widmer [4] that uses information theory measures like entropy to approach the problem. The large-term goal of this work is to pose the general question What is a melody? under a statistical approach. The answer would be given as sets of rules that are both human-readable and automatically learnt from score corpora. This could be of interest for musicologists or helpful in applications such as melody matching, motif extraction, melody extraction (ringtones), etc. In this paper the ﬁrst stages of this work are presented. c⃝2007 Austrian Computer Society (OCG). 2 METHODOLOGY The steps performed to obtain sets of rules that characterize melody tracks can be outlined as follows: Feature extraction Describe a MIDI track by a set of low-level statistical descriptors. Decision tree learning Learn a set of decision trees from a tagged corpus of MIDI-tracks. Rule extraction Extract rules from decision tree branches. Rule simpliﬁcation Prune rule antecedents. Rule selection Select best rules by ranking. The details about the ﬁrst two steps can be found in [6]. The low-level statistical descriptors utilized to describe musical content are based on several categories of features that assess melodic and rhythmic properties of a music sequence, such as pitch, note interval, and note duration distribution descriptors, as well as track related properties such as polyphony rate, duration, etc. Most distribution descriptors are also present in normalized form with respect to the value range for that descriptor in the whole song. This allows to know the context in which a particular descriptor value is computed. The sets of rules that characterize melody tracks are initially extracted from an ensemble of decision trees that has shown good performance at this task [6]. The rules are simpliﬁed by pruning non useful antecedents and then, for each rule set, rules are ranked according to different metrics. Note that there is no feature selection stage, as the Random Forest [1] method used to learn decision tree ensembles performs its own feature selection process. Expressing a concept by rules has the advantage of being a human-readable description of the characterization process. The simpliﬁcation and ranking steps focus on obtaining a small, manageable rule system. A side goal has been to test whether a small number of selected rules can perform comparably to an ensemble of decision trees, typically containing hundreds or thousands of nodes.",
        "zenodo_id": 1414902,
        "dblp_key": "conf/ismir/LeonRQ07"
    },
    {
        "title": "Automatic Instrument Recognition in a Polyphonic Mixture Using Sparse Representations.",
        "author": [
            "Pierre Leveau",
            "David Sodoyer",
            "Laurent Daudet"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414984",
        "url": "https://doi.org/10.5281/zenodo.1414984",
        "ee": "https://zenodo.org/records/1414984/files/LeveauSD07.pdf",
        "abstract": "In this paper, we introduce a method to address automatic instrument recognition in polyphonic music. It is based on the decomposition of the music signal with instrumentspeciﬁc harmonic atoms, yielding an approximate object representation of the signal. A post-processing is then applied to exhibit ensemble saliences that give clues about the number of instruments and their labels. The whole algorithm is then applied on artiﬁcial mixes of solo performances. The identiﬁcation of the number of instrument reaches 73 % on 10-s segments and the fully blind problem of identiﬁcation of the ensemble label without prior knowledge on the number of instruments is 17 %. 1 INTRODUCTION Orchestration is a critical information for the automatic indexing of music. It gives an important clue about the music genres, and is often necessary for the query of sound samples for electronic music composing. Automatic Instrument Recognition has raised some interest over the latest years (see [1] for an overview). Early studies have addressed the recognition of isolated music notes, then of solos phrases. For these two contexts, machines now reach the performance of expert musicians. However, mono-instrument music is only a small part of the overall recorded music, that involves natural or artiﬁcial mixes of instruments. To deal with multi-instrument music, several strategies have been adopted. Template-based approaches have ﬁrst been proposed [2, 3]. Other approaches adapt “bag-offrames” approaches to polyphony [4]. Other techniques consist in estimating jointly the instrument sources activated in a probabilistic framework [5], at a heavy computational cost. A recent work [6] presents a representation showing the instrument presence probabilities in the timepitch plane without note detection. Ensemble classes can also be modeled using standard feature-based representations in addition with a hierarchical taxonomy [7], when the number of instrument combinations is tractable. c⃝2007 Austrian Computer Society (OCG). In this paper a recent development in the decomposition of music signals is studied for the recognition of music instrument in ensemble music. It relies on principles coming from the sparse approximations domain. To get a useful sparse representation of a signal, two aspects have to be investigated: the building of a signal model (dictionary design), and, given a dictionary, the choice of an algorithm and its optimization towards a faster or better approximation. Techniques from the sparse approximation domain have already been used for automatic music transcription in an unsupervised way [8]: the building of the dictionary was done in an data-driven way, prohibiting the signal analysis in view of prior knowledge of the sources. The introduction of prior knowledge about the sources in dictionaries has been presented in [9]: this knowledge is put in the amplitudes of the note partials. In section 2, the decomposition algorithm is brieﬂy described, then a post-processing is introduced to take a decision on the orchestration. The experiments of ensemble recognition are detailed in section 3. 2 ALGORITHM",
        "zenodo_id": 1414984,
        "dblp_key": "conf/ismir/LeveauSD07"
    },
    {
        "title": "A Semantic Space for Music Derived from Social Tags.",
        "author": [
            "Mark Levy",
            "Mark B. Sandler"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415628",
        "url": "https://doi.org/10.5281/zenodo.1415628",
        "ee": "https://zenodo.org/records/1415628/files/LevyS07.pdf",
        "abstract": "In this paper we investigate social tags as a novel highvolume source of semantic metadata for music, using techniques from the ﬁelds of information retrieval and multivariate data analysis. We show that, despite the ad hoc and informal language of tagging, tags deﬁne a low-dimensional semantic space that is extremely well-behaved at the track level, in particular being highly organised by artist and musical genre. We introduce the use of Correspondence Analysis to visualise this semantic space, and show how it can be applied to create a browse-by-mood interface for a psychologically-motivatedtwo-dimensional subspace representing musical emotion. 1 INTRODUCTION Social tags are of interest as a potential high-volumesource of descriptive metadata for music. Such metadata can either be used directly to drive search applications, as already happens to some extent in the commercial domain, or as a source of groundtruth to train audio content-based classiﬁcation and search engines. In the academic literature, comparable text metadata for music has previously been found by mining web-pages such as blogs and music reviews [2, 19, 9]. Although some interesting preliminary results have been reported, two signiﬁcant problems are associated with this approach. Firstly, text retrieved from the web is often noisy, i.e. it unavoidably contains a great deal of irrelevant content. Secondly, for computational reasons, and because the noise problem becomes insuperable, text has to be mined on a per-artist rather than per-track basis: as a result it offers only low-quality groundtruth for learning the characteristics of audio content. Social tags as applied to individual tracks appear to offer a solution to both of these issues. At the time of writing there is no previous relevant academic literature on social tags for music. The tags discussed here were all applied to individual tracks. They were aggregated from the last.fm 1 and MyStrands 2 web services during January and February 2007. 1 http://ws.audioscrobbler.com 2 https://www.musicstrands.com c⃝2007 Austrian Computer Society (OCG). Music in general has no semantics, in the strict sense of representing or being ‘about’ something, and, perhaps as a result, tags for music are often discursive. Of some 45,000 distinct tags in our dataset, over a third of consist of three or more words, while over 10% contain 5 or more words: these are frequently complete phrases. In our experiments we therefore treat tags as regular text, tokenizing them with a standard stop-list (to remove common words such as ‘it’, ‘and’, ‘the’, etc.). We then create a conventional document-term matrix, tabulating the number of occurrences of each word in tags applied to each track. We do not use a stemmer, because of the idiosyncratic vocabulary of social tagging and the large number of words used as proper nouns (particularly artist names). Working with words rather than tags nonetheless goes some way towards capturing the common meaning of alternate forms such as ‘female vocalist’, ‘female vocals’, ‘good female vocals’, ‘sexy female vocals’, ‘lovely female vocals’, etc. The language of tags for music is ad hoc and often highly informal, as shown by the following few tags selected at random: ‘all my hope is gone’, ‘oregon trips’, ‘my favourite muse songs’, ‘french-canadian’,‘Tool Mix’, ‘comp1’, ‘ragga rhythm’, ‘Dave Brubeck Quartet’, ‘american wedding’, ‘fora do mundo’, ‘space trucking’, ‘right in two’, ‘desert island songs songs which keep me alive or otherwise enraged’, ‘heard on 96wave’, ‘put on mikey cds’. We might indeed question whether the collaborative tagging model can be applied successfully to music: beyond standard metadata like artist or title, which are already likely to be known in any real application, it may be far from obvious which tags are appropriate for any particular track. This study provides evidence, however, that despite the vagaries of individual tags, patterns of co-occurrences of words in tags can reveal terms or combinations of terms which are signiﬁcantly grounded in the music they describe (rather than expressing arbitrary personal reactions) and generalisable across tracks. In particular we show that tags deﬁne a vector space with highly attractive properties for music retrieval, and which appears to have genuine semantics. Table 1. Top terms describing Portishead Tags Web-mined text trip-hop cynical electronic produced portishead smooth female vocalists dark downtempo particular alternative loud mellow amazing chillout vocal sad unique 90s simple 2 THE SEMANTIC SPACE OF TAGS",
        "zenodo_id": 1415628,
        "dblp_key": "conf/ismir/LevyS07"
    },
    {
        "title": "Alternative Digitization Approach for Stereo Phonograph Records Using Optical Audio Reconstruction.",
        "author": [
            "Beinan Li",
            "Simon de Leon",
            "Ichiro Fujinaga"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415746",
        "url": "https://doi.org/10.5281/zenodo.1415746",
        "ee": "https://zenodo.org/records/1415746/files/LiLF07.pdf",
        "abstract": "This paper presents the first Optical Audio Reconstruction (OAR) approach for the long-term digital preservation of stereo phonograph records. OAR uses precision metrology and digital image processing to obtain and convert groove contour data into digital audio for access and preservation. This contactless and imaging-based approach has considerable advantages over the traditional mechanical methods, such as being the only optical method with the potential to restore broken stereo records. Although past efforts on monophonic phonograph records have been successful, no attempts on 33rpm long-playing stereo records (LPs) have been reported. By using a white-light interferometry optical profiler, we are able to extract stereo audio information encoded in the 3D profile of the phonograph record grooves.",
        "zenodo_id": 1415746,
        "dblp_key": "conf/ismir/LiLF07"
    },
    {
        "title": "Improving Genre Classification by Combination of Audio and Symbolic Descriptors Using a Transcription Systems.",
        "author": [
            "Thomas Lidy",
            "Andreas Rauber",
            "Antonio Pertusa",
            "José Manuel Iñesta Quereda"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416344",
        "url": "https://doi.org/10.5281/zenodo.1416344",
        "ee": "https://zenodo.org/records/1416344/files/LidyRPQ07.pdf",
        "abstract": "Recent research in music genre classiﬁcation hints at a glass ceiling being reached using timbral audio features. To overcome this, the combination of multiple different feature sets bearing diverse characteristics is needed. We propose a new approach to extend the scope of the features: We transcribe audio data into a symbolic form using a transcription system, extract symbolic descriptors from that representation and combine them with audio features. With this method, we are able to surpass the glass ceiling and to further improve music genre classiﬁcation, as shown in the experiments through three reference music databases and comparison to previously published performance results. 1 INTRODUCTION Audio genre classiﬁcation is an important task for retrieval and organization of music databases. Traditionally the research domain of genre classiﬁcation is divided into the audio and symbolic music analysis and retrieval domains. The goal of this work is to combine approaches from both directions that have proved their reliability in their respective domains. To assign a genre to a song, audio classiﬁers use features extracted from digital audio signals, and symbolic classiﬁers use features extracted from scores. These features are complementary; a score can provide very valuable information, but audio features (e.g., the timbral information) are also very important for genre classiﬁcation. To extract symbolic descriptors from an audio signal it is necessary to ﬁrst employ a transcription system in order to detect the notes stored in the signal. Transcription systems have been investigated previously but a wellperforming solution for polyphonic music and a multitude of genres has not yet been found. Though these systems might not be in a ﬁnal state for solving the transcription problem, our hypothesis is that they are able to augment the performance of an audio genre classiﬁer. In this work, a new transcription system is used to get a symbolic representation from an audio signal. c⃝2007 Austrian Computer Society (OCG). Figure 1. General framework of the system The overall scheme of our proposed genre classiﬁcation system is shown in Figure 1. It processes an audio ﬁle in two ways to predict its genre. While in the ﬁrst branch, the audio feature extraction methods described in Section",
        "zenodo_id": 1416344,
        "dblp_key": "conf/ismir/LidyRPQ07"
    },
    {
        "title": "A Query by Humming System that Learns from Experience.",
        "author": [
            "David Little 0001",
            "David Raffensperger",
            "Bryan Pardo"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416642",
        "url": "https://doi.org/10.5281/zenodo.1416642",
        "ee": "https://zenodo.org/records/1416642/files/LittleRP07.pdf",
        "abstract": "Query-by-Humming (QBH) systems transcribe a sung or hummed query and search for related musical themes in a database, returning the most similar themes. Since it is not possible to predict all individual singer profiles before system deployment, a robust QBH system should be able to adapt to different singers after deployment. Currently deployed systems do not have this capability. We describe a new QBH system that learns from user provided feedback on the search results, letting the system improve while deployed, after only a few queries.  This is made possible by a trainable note segmentation system, an easily parameterized singer error model and a straight-forward genetic algorithm. Results show significant improvement in performance given only ten example queries from a particular user.",
        "zenodo_id": 1416642,
        "dblp_key": "conf/ismir/LittleRP07"
    },
    {
        "title": "A Web-Based Game for Collecting Music Metadata.",
        "author": [
            "Michael I. Mandel",
            "Daniel P. W. Ellis"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414768",
        "url": "https://doi.org/10.5281/zenodo.1414768",
        "ee": "https://zenodo.org/records/1414768/files/MandelE07.pdf",
        "abstract": "We have designed a web-based game to make collecting descriptions of musical excerpts fun, easy, useful, and objective. Participants describe 10 second clips of songs and score points when their descriptions match those of other participants. The rules were designed to encourage users to be thorough and the clip length was chosen to make judgments more objective and speciﬁc. Analysis of preliminary data shows that we are able to collect objective and speciﬁc descriptions of clips and that players tend to agree with one another. 1 MOTIVATION AND GAME PLAY The easiest way for people to ﬁnd music is by describing it with words. Whether hearing about a new band from a friend, browsing a large catalog, or locating a speciﬁc song, verbal descriptions, although imperfect, generally sufﬁce. While there are notable community efforts to verbally describe large corpora of music, only an automatic music description system can adequately label brand new, obscure, or unknown music. To train such a system, however, requires human generated descriptions. Thus in this project, we endeavor to collect ground truth about speciﬁc, objective aspects of music by asking humans to describe short musical excerpts, which we call clips, in the context of a web-based game 1 . Such a game entertains people while simultaneously collecting useful data. Not only is the data collected interesting, but the game itself makes novel contributions to the ﬁeld of “human computation.” Here is an example of a how a player experiences the game. First she requests a new clip to be tagged. This clip could be one that other players have seen before or one that is brand new, she does not know which she will receive. She listens to the clip and describes it with a few words: harp, female, and sad. The word harp already has been used by exactly one other player, so it scores her one point. In addition, the player who ﬁrst used it scores two points. The word female has already been used by at least two players, so it scores our player zero points. The word sad has not been used by anyone before, so it scores 1 The game is available to play at: http://game.majorminer.com c⃝2007 Austrian Computer Society (OCG). no points immediately, but has the potential to score two points should another player subsequently use it. The player then goes to her game summary. The summary shows both clips that she has recently seen and those that she has recently scored on, e.g. if another user has agreed with one of her tags. It also reveals the artist, album, and track names of each clip and allows the user to see another user’s tags for each clip. The next time she logs in, the system informs her that three of her descriptions have been used by other players in the interim, scoring her six points while she was gone. A number of authors have explored the link between music and text, especially Whitman [4]. More recently, [2] has applied ideas from the image retrieval literature to associate text with music. In the ESP Game [3] pairs of players describe the same image and score points when they agree. This game popularized the idea of allowing free form responses that only score points when veriﬁed. 2 DESIGN CONSIDERATIONS We designed the game with many goals in mind. Our main goal, which shaped the design of the scoring rules, was to encourage users to describe the music thoroughly, to be original, yet relevant. Our second goal, which informed the method for picking clips to show, was for the game to be fun for both new and veteran users. We also wanted to avoid cheating, collusion, or other manipulations of the scoring system or, worse, the data collected. While games like the ESP game pair a player with a single partner, ours in a sense teams a player with all of the other players who have ever seen a particular clip. It is possible that a pair of players could vary widely in skill level or familiarity with the clip under consideration, frustrating both players. The non-paired format allows the most creative or expert players to cooperate with each other asynchronously. It also allows the systematic",
        "zenodo_id": 1414768,
        "dblp_key": "conf/ismir/MandelE07"
    },
    {
        "title": "Visualizing Music: Tonal Progressions and Distributions.",
        "author": [
            "Arpi Mardirossian",
            "Elaine Chew"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417773",
        "url": "https://doi.org/10.5281/zenodo.1417773",
        "ee": "https://zenodo.org/records/1417773/files/MardirossianC07.pdf",
        "abstract": "This paper presents a music visualization tool that shows the tonal progression in, and tonal distribution of, a piece of music on Lerdahl’s two-dimensional tonal pitch space. The method segments a piece into uniform time slices, and determines the most likely key in each slice. It then generates the visualization by dynamically showing the sequence of keys as translucent, growing discs on the twodimensional plane. The frequency of a key is indicated by the size of its colored disc. Each color and position corresponds to a key, and related keys are shown in proximity with related colors. The visual result effectively presents the changing distribution of the keys employed. The proposed visualization is an improvement over more basic charting methods, such as histograms, and it maintains standards of information design in the form of added dimensionality, color, and animation. We show that the visualization is invariant under music transformations that preserve the piece’s identity. We conclude by illustrating how this method may be used to visually distinguish between tonal progression and distribution patterns in western classical versus Armenian folk music. 1 INTRODUCTION Music visualization literature can be broadly grouped into two categories: visualization of individual pieces of music (our focus), and of collections of pieces. It can be said that the ﬁrst form of music visualization created for individual pieces was music notation itself. An experienced musician can often look at the score of a piece and “see” what the music sounds like. Music notation cannot be used readily as a mainstream form of visualization because it can take years of training to learn to decipher the subtleties of the encoded information. Our goal is to create a more intuitive visualization that reveals important features of the music that may not be readily audible to the inexperienced ear. The challenge with developing such a visualization is that music is complex, consisting of multiple inter-related features. A successful visualization must strike a balance between simplicity and comprehensiveness. We aim to create imagery c⃝2007 Austrian Computer Society (OCG). that is both intuitive and informative. In this paper, we propose a visual interface based on Lerdahl’s Tonal Pitch Space [12], which portrays all major and minor keys on a two-dimensional (2D) plane. The distributions of keys are indicated as growing colored discs, where the colors correspond to the keys detected, and the size of the discs to the key frequency. Figure 1 shows the visual interface. The user selects the piece and the granularity of analysis through the graphical user interface. Figure 1. Snapshot of Visualization Interface In our previous work ([14, 15]), we investigated how key distributions could be successfully used to assess similarity between pieces, demonstrating that key distributions, although a summarization of the musical content, can serve as good representations of pieces. The current visualization method is an extension and improvement of the key distribution approach, expanding and adding richness to the simple histogram representation through an increase in dimensionality, addition of color, and animation. According to Tufte [21], an acknowledged expert in information design and visual literacy, increasing the number of dimensions of a visualization sharpens the information resolution. In the histogram, the keys were shown on a one-dimensional line, while in the new visual interface, the keys (all major and minor keys) are shown on a 2D plane, thus capturing the network of inter-relations amongst keys. The frequency of the keys (the third dimension) is shown in the size of the discs. Furthermore, the progression of disc growth shows the range of movement of keys within the piece over time. Hence, we have essentially four dimensions of information captured in a dynamic 2D interface. Tufte states that representations that progress, such as the proposed animated visualization, can be referred to as ‘small multiple designs’ and “answer directly [the question of ‘compared to what?’] by visually enforcing comparisons of changes, of the differences among objects, of the scope of alternatives.” The proposed visual interface incorporates these ideas of small multiple design by taking a sequence of keys and showing the evolution frame-byframe. This dynamic visualization allows one to see the sequential progression of keys, an important component in communicating with music. Third, Tufte states the fundamental uses of color in information design as being: to label (color as noun), to measure (color as quantity), to imitate reality (color as representation), and to enliven or decorate (color as beauty). In our visualization, color labels by distinguishing between keys, measures by displaying the amount of time spent in each key, imitates reality by showing the relationship between keys, and decorates since the same visualization in black and white would not be nearly as visually pleasing. The remainder of the paper is organized as follows: Section 2 presents related work in music visualization, Section 3 describes our visualization system, followed by validation of the visualization through translation analysis in Section 4, and demonstration of the visualization system in Section 5. Section 6 presents our conclusions. 2 RELATED WORK This section reviews a selection of the many music visualization systems developed so as to put the work presented here in perspective. As noted above, visualizations can be broadly categorized into visualizations of collections and individual pieces. Since our work does not consider collections, this review will be limited to visualizations of individual pieces. These systems may be further subcategorized as follows: representations of direct versus interpreted data, and static versus dynamic presentations. Direct data refers to data that is extracted directly from the music (such as pitch and onset time), while interpreted data refers to information that must be determined from extracted data (for example, tempo and key). Let us consider visualizations of direct data. The most basic visualizations in this category are 2D waveforms and spectrograms which usually show time on the x-axis, and have primary values of interest on the y-axis. Additional mappings of these primary values are often shown using color or greyscale ranges. Misra, Wang, and Cook [17] present such visualizations (real time) with some added features and dimensionality. Another example of direct data visualization is Malinowski’s “Music Animation Machine” [13], which dynamically shows notes in a simpliﬁed piano roll representation. We now turn our attention to systems that visualize interpreted data. We ﬁrst review static systems. One approach to music visualization is to create self-similarity maps. In the work developed by Cooper & Foote [8], the acoustic similarity between all instants of an audio recording is calculated and displayed on a 2D grid. Similar or repeating elements are visually distinct, allowing identiﬁcation of structural and rhythmic characteristics. Another self-similarity visualization by Wattenberg et. al. [19] displays musical form as a sequence of translucent arches. Each arch connects two repeated, identical passages of a composition. By using repeated passages as landmarks, the maps reveal deep structures in musical compositions. An early work by Cohn [7] established mappings of music onto the harmonic network (also known as the tonnetz). We now transition to visualizations of interpreted data that are also dynamic. Related to the harmonic network visualization is Toiviainen & Krumhansl’s [20] visualization of listeners’ continuous ratings of tonal contexts on a toroid representation of keys (shown in 2D). Their work measures and models real-time responses using selforganizing maps. Chouvel [5] showed tonal analyses of a number of pieces on a hexagonal version of the tonnetz. Gomez & Bonada [10] developed a tool to visualize the tonal content of polyphonic audio signals. This tool includes different views that may be used for the analysis of tonal content of a music piece through visualization of chord and key estimation, and tonal similarity assessment. Sapp [18] developed a multi-timescale visualization technique for displaying the output from key-ﬁnding algorithms. In his visualization, the horizontal axis represents time in the score, while the vertical axis represents the duration of an analysis window used to select music for the key-ﬁnding algorithm. Each analysis window result is colored according to the determined key. The following works also maintain history information. Langer & Goebl [11] introduced a method for displaying tempo and loudness variations of expressive music performance. In this visualization, a dot moves through a 2D space representing tempo (x-axis) and loudness (yaxis), leaving behind a trace of the recent trajectory that may be interpreted as the performance path. Chew & Franc¸ois [4] developed a visual environment in which tonal information from musical performances are mapped, in real time, to a three-dimensional representation of tonal space. Their MuSA.RT analysis and visualization system also portrays musical memory as a trajectory that touches on the recently visited tonal regions. Our current approach can be considered a 2D counterpart of this work, with the difference that it shows not only the keys as they unfold, it also portrays the cumulative key information as dynamically varying spatial distributions of colored discs. 3 SYSTEM DESCRIPTION This section describes the components of our music visualization method, which displays the progression of the tonal content of a music piece. We begin by slicing a piece of music into segments of uniform time length, and determining the key for each segment using a key-ﬁnding algorithm. We then map the sequence of keys onto a 2D space that contains points representing all possible keys.",
        "zenodo_id": 1417773,
        "dblp_key": "conf/ismir/MardirossianC07"
    },
    {
        "title": "Automatic Derivation of Musical Structure: A Tool for Research on Schenkerian Analysis.",
        "author": [
            "Alan Marsden"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415814",
        "url": "https://doi.org/10.5281/zenodo.1415814",
        "ee": "https://zenodo.org/records/1415814/files/Marsden07.pdf",
        "abstract": "This paper describes software to facilitate research on the automatic derivation of hierarchical (Schenkerian) musical structures from a musical surface. Many MIR tasks require information about musical structure, or would perform better if such information were available. Automatic derivation of musical structure faces two significant obstacles. Firstly, the solution space of possible structural analyses of a piece is very large. Secondly, pieces can have more than one valid structural analysis, and there is little firm agreement among music theorists about how to distinguish a good analysis. To circumvent the first of these obstacles, software has been developed which derives a tractable ‘matrix’ of possibilities from a musical surface (i.e., MIDI-like note-time information). The matrix is somewhat like the intermediate results of a dynamic-programming algorithm, and in a similar way it is possible to extract a particular structural analysis from the matrix by following the appropriate path from the top level to the surface. It therefore provides a tool to facilitate research on the second obstacle by allowing candidate ‘goodness’ metrics to be incorporated into the software and tested on actual music.",
        "zenodo_id": 1415814,
        "dblp_key": "conf/ismir/Marsden07"
    },
    {
        "title": "Polyphonic Instrument Recognition Using Spectral Clustering.",
        "author": [
            "Luis Gustavo Martins",
            "Juan José Burred",
            "George Tzanetakis",
            "Mathieu Lagrange"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415074",
        "url": "https://doi.org/10.5281/zenodo.1415074",
        "ee": "https://zenodo.org/records/1415074/files/MartinsBTL07.pdf",
        "abstract": "The identiﬁcation of the instruments playing in a polyphonic music signal is an important and unsolved problem in Music Information Retrieval. In this paper, we propose a framework for the sound source separation and timbre classiﬁcation of polyphonic, multi-instrumental music signals. The sound source separation method is inspired by ideas from Computational Auditory Scene Analysis and formulated as a graph partitioning problem. It utilizes a sinusoidal analysis front-end and makes use of the normalized cut, applied as a global criterion for segmenting graphs. Timbre models for six musical instruments are used for the classiﬁcation of the resulting sound sources. The proposed framework is evaluated on a dataset consisting of mixtures of a variable number of simultaneous pitches and instruments, up to a maximum of four concurrent notes. 1 INTRODUCTION The increasing quantity of music titles available in digital format added to the huge amount of personal music storage capacity available today has resulted in a growing demand for more efﬁcient and automatic means of indexing, searching and retrieving music content. The computer identiﬁcation of the instruments playing in a music signal can assist the automatic labeling and retrieval of music. Several studies have been made on the recognition of musical instruments on isolated notes or in melodies played by a single instrument. A comprehensive review of those techniques can be found in [1]. However, the recognition of musical instruments in multi-instrumental, polyphonic music is much more complex and presents additional challenges. The main challenge stands from the fact that tones from performing instruments can overlap in time and frequency. Therefore, most of the isolated note recognition techniques that have been proposed in the literature are inappropriate for polyphonic music signals. Some of the proposed techniques for the instrument c⃝2007 Austrian Computer Society (OCG). note 1 note n ... Sound Source Formation note 1 / inst 1 note n / inst i ... Timbre Models Matching Matching Peak Picking Sinusoidal Analysis ... ... ... Figure 1. System diagram block. recognition on polyphonic signals consider the entire audio mixture, avoiding any prior source separation [2, 3]. Other approaches are based on the separation of the playing sources, requiring the prior knowledge or estimation of the pitches of the different notes [4, 5]. However, robustly extracting the fundamental frequencies in such multiple pitch scenarios is difﬁcult. In this paper, we propose a framework for timbre classiﬁcation of polyphonic, multi-instrumental music signals using automatically separated sound sources. Figure 1 presents a block-diagram of the complete system. It starts by taking a single-channel audio signal and uses a sinusoidal analysis front-end for estimating the most prominent spectral peaks over time. The detected spectral peaks are then grouped into clusters according to cues inspired from Computational Auditory Scene Analysis (i.e. frequency, amplitude and harmonic proximity) and formulated as a graph partitioning problem. The normalized cut, a technique from the Computer Vision ﬁeld, is then used as a global criterion for segmenting graphs. Contrary to other approaches [6, 7], this source separation technique does not require any prior knowledge or pitch estimation. As demonstrated in previous works by the authors [8, 9] and later in section 4, the resulting clusters capture reasonably well the underlying sound sources and events (i.e. notes, in the case of music signals) present in the audio mixture. After the sound source separation stage, each identiﬁed cluster is matched to a collection of six timbre models namely piano, oboe, clarinet, trumpet, violin and alto sax. These models are a compact description of the spectral envelope and its evolution in time, and were previously trained using isolated note audio recordings. The design of the models, as well as their application to isolated note classiﬁcation, were described in [10]. The outline of the paper is as follows. In section 2 we describe the sound source separation technique, which starts from a sinusoidal representation of the signal followed by the application of the normalized cut for source separation. In section 3 we brieﬂy describe the training of the timbre models and focus on the matching procedure used to classify the separated clusters. We then evaluate the system performance in section 4 and close with some ﬁnal conclusions. 2 SOUND SOURCE SEPARATION Computational Auditory Scene Analysis (CASA) systems aim at identifying perceived sound sources (e.g. notes in the case of music recordings) and grouping them into auditory streams using psycho-acoustical cues [11]. However, as remarked in [6] the precedence rules and the relevance of each of those cues with respect to a given practical task is hard to assess. Our goal is to use a ﬂexible framework where these perceptual cues can be expressed in terms of similarity between time-frequency components. The separation task is then carried out by clustering components which are close in the similarity space (see Figure 2). Once identiﬁed, those clusters will be matched to timbre models in order to perform the instrument identiﬁcation task.",
        "zenodo_id": 1415074,
        "dblp_key": "conf/ismir/MartinsBTL07"
    },
    {
        "title": "Discovering Chord Idioms Through Beatles and Real Book Songs.",
        "author": [
            "Matthias Mauch",
            "Simon Dixon",
            "Christopher Harte",
            "Michael A. Casey",
            "Benjamin Fields"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415564",
        "url": "https://doi.org/10.5281/zenodo.1415564",
        "ee": "https://zenodo.org/records/1415564/files/MauchDHCF07.pdf",
        "abstract": "Modern collections of symbolic and audio music content provide unprecedented possibilities for musicological research, but traditional qualitative evaluation methods cannot realistically cope with such amounts of data. We are interested in harmonic analysis and propose key-independent chord idioms derived from a bottom-up analysis of musical data as a new subject of musicological interest. In order to motivate future research on audio chord idioms and on probabilistic models of harmony we perform a quantitative study of chord progressions in two popular music collections. In particular, we extract common subsequences of chord classes from symbolic data, independent of key and context, and order them by frequency of occurrence, thus enabling us to identify chord idioms. We make musicological observations on selected chord idioms from the collections. 1 INTRODUCTION Traditional musicology consists of qualitative studies using small data sets, so that it is not possible to ascertain whether the conclusions drawn from the study are representative of a broader corpus of music. Harmonic analysis is no exception to that rule, and there is plenty of literature on harmony in Western music and Jazz (e.g. [3]). One of the disadvantages of this approach is that the choice of data and its interpretation are subjective. Music Information Retrieval methods provide us with increasingly powerful tools that can be applied to strip some of such subjectivity from the analyses by quantitatively evaluating features over large collections of music. Only in recent years has considerable effort been put into the automatic analysis of chord changes in audio and symbolic representations. Most of the efforts in audio chord analysis are concerned with the actual extraction of chords (e.g. [5], [2] and [8]) – which is an interesting and very difﬁcult task – but do not address musicological questions. In the symbolic domain too, efforts have mainly been directed towards chord transcription. Examples include MIDI-driven harmony retrieval such as the Melisma Harmony Program [10] or harmonic labelling with Bayesian c⃝2007 Austrian Computer Society (OCG). Model Selection [9]. On the other hand, Pachet [7] considers hand-annotated chord labels and infers a notion of surprise in chord sequences and a set of chord substitution rules. 2 THE STUDY We follow Pachet by using manually extracted chord labels for our analyses. We deliberately avoid any analysis of tonality or other high-level features because we believe that they are coded implicitly in chord sequences of sufﬁcient length. That is, the chord sequences themselves represent the evolution of harmony over time. Our aim is to create an inventory (or: dictionary) of chord sequences together with an analysis of their statistical frequencies in order to discover chord idioms, prominent chord sequences in a particular style, genre or historical period. This inventory will also function as a basis for further probabilistic research into harmonic structures. We examine two collections of manually labelled chord symbols in text form. One is Harte’s Beatles Chord Database [6] consisting of all the chords of all 180 songs featured on original Beatles studio albums. The database includes start and end times of chords in songs relative to the original Beatles’ recordings. The other collection is a transcription of the chords of 244 Jazz standards from the Real Book [12].",
        "zenodo_id": 1415564,
        "dblp_key": "conf/ismir/MauchDHCF07"
    },
    {
        "title": "Sociology and Music Recommendation Systems.",
        "author": [
            "Daniel McEnnis",
            "Sally Jo Cunningham"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414854",
        "url": "https://doi.org/10.5281/zenodo.1414854",
        "ee": "https://zenodo.org/records/1414854/files/McEnnisC07.pdf",
        "abstract": "Music recommendation systems have centred on two different approaches: content based analysis and collaborative ﬁltering. Little attention has been paid to the reasons why these techniques have been effective. Fortunately, the social sciences have asked these questions. One of the ﬁndings of this research is that social context is much more important than previously thought. This paper introduces this body of research from sociology and its relevance to music recommendation algorithms. 1 INTRODUCTION Traditionally, music has been recommended by trusted friends. Automating this process is a challenging problem since social factors play such a major role in determining what makes a good recommendation. Very little information about social factors has been utilized in the MIR music recommendation literature or why people choose to listen to the music that they do and in what contexts—correct guesses under idealized listening situations are sufﬁcient information for MIR. Fortunately, the social science literature provides concrete research on these questions. 2 MUSIC RECOMMENDATION SYSTEMS There have been a number of music recommendation systems proposed. Generally, these have used either contentbased analysis or collaborative ﬁltering for generating play lists. More recent systems have used user-supplied metadata and web-based data to augment content-based approaches. Logan [7] uses a purely content based approach, producing play lists either directly from similarity, or similarity to a set of songs. Pampalk et al. [10] also uses contentbased analysis but the play lists are altered by users with explicit ratings, as did Pauws [11]. Pandora.com is a commercial example of a content-based recommendation system. Two of Chen and Chen’s [3] recommendation algorithms utilize purely content-based approaches. One of Chen and Chen’s [3] algorithms utilizes a pure collaborative ﬁltering approach. Crossen [4] utilizes user c⃝2007 Austrian Computer Society (OCG). recommendations to determine the music to play in a shared space, ﬁltering over hand-picked genre classiﬁcations. Celma et al. [2] constructs networks of artists based on their FOAF (friend of a friend) proﬁles. Sandvold et al. [12] extended this with tagging and content based analysis. 3 EXISTING CONTEXT ANALYSIS WITHIN MIR Earlier work has explored the importance of context and culture in MIR. Lee et al. [6] examined the challenges of cross-language music queries. Uitdenbogerd and Schyndel provided an overview of social context, culture, and psychological foundations [14]. Moelants et al. [9] demonstrate the dangers of context-insensitive content-based analysis and the importance of context-aware tagging. 4 SOCIOLOGY Bennett [1] sums it up: ’Consumers take the structures of meaning the musical and extra-musical resources associated with particular genres of pop and combine them with meanings of their own to produce distinctive patterns of consumption and stylistic expression.’ This sociological approach approaches assume from the outset that musical preferences will differ from culture to culture and that even the meanings of the same music in different cultures will be different. This philosophy of the non-universality of both music and its meaning is inherent in all modern research in sociology.",
        "zenodo_id": 1414854,
        "dblp_key": "conf/ismir/McEnnisC07"
    },
    {
        "title": "jWebMiner: A Web-Based Feature Extractor.",
        "author": [
            "Cory McKay",
            "Ichiro Fujinaga"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417679",
        "url": "https://doi.org/10.5281/zenodo.1417679",
        "ee": "https://zenodo.org/records/1417679/files/McKayF07.pdf",
        "abstract": "jWebMiner is a software package for extracting cultural features from the web. It is designed to be used for arbitrary types of MIR research, either as a stand-alone application or as part of the jMIR suite. It emphasizes extensibility, generality and an easy-to-use interface. At its most basic level, the software operates by using web services to extract hit counts from search engines. Functionality is available for calculating a variety of statistical features based on these counts, for variably weighting web sites or limiting searches only to particular sites, for excluding hits that do not contain particular filter terms, for defining synonym relationships between certain search strings, and for applying a number of additional search configurations.",
        "zenodo_id": 1417679,
        "dblp_key": "conf/ismir/McKayF07"
    },
    {
        "title": "Singer Identification in Polyphonic Music Using Vocal Separation and Pattern Recognition Methods.",
        "author": [
            "Annamaria Mesaros",
            "Tuomas Virtanen",
            "Anssi Klapuri"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417395",
        "url": "https://doi.org/10.5281/zenodo.1417395",
        "ee": "https://zenodo.org/records/1417395/files/MesarosVK07.pdf",
        "abstract": "This paper evaluates methods for singer identiﬁcation in polyphonic music, based on pattern classiﬁcation together with an algorithm for vocal separation. Classiﬁcation strategies include the discriminant functions, Gaussian mixture model (GMM)-based maximum likelihood classiﬁer and nearest neighbour classiﬁers using Kullback-Leibler divergence between the GMMs. A novel method of estimating the symmetric Kullback-Leibler distance between two GMMs is proposed. Two different approaches to singer identiﬁcation were studied: one where the acoustic features were extracted directly from the polyphonic signal and one where the vocal line was ﬁrst separated from the mixture using a predominant melody transcription system. The methods are evaluated using a database of songs where the level difference between the singing and the accompaniment varies. It was found that vocal line separation enables robust singer identiﬁcation down to 0dB and -5dB singer-to-accompaniment ratios. 1 INTRODUCTION Singing voice is the main focus of attention in musical pieces with a vocal part; most people use the singers voice as the primary cue for identifying a song. Also, a natural classiﬁcation of music, besides genre, is the artist name (often equivalent to singers name). A singer identiﬁcation system would be useful for MIR (music information retrieval) systems in case of identifying singers for songs. The inherent difﬁculties lie in the nature of the problem: the voice is usually accompanied by other musical instruments and even though humans are extremely skilful in recognizing sounds in acoustic mixtures, interfering sounds usually make the automatic recognition very difﬁcult. Two main approaches to singer identiﬁcation have been studied: one where features are computed directly from the polyphonic signal and another using separation and analysis of the vocal source. Treating the polyphonic mix directly and extracting the features for classiﬁcation relies on the assumption that the singing voice is sufﬁciently dominating in the feature values. As preprocessing, the authors of [9, 10] located the time segments where vocals c⃝2007 Austrian Computer Society (OCG). are present. After endpoint detection, in [10] the author used a ﬁxed-length segment of 25 s to compute the features. Reported results were 82% on a number of 45 songs from 8 singers, using MFCCs as features and GMM models and maximum likelihood classiﬁcation. The second approach is the separation of vocals from the polyphonic mixture. A statistical approach to vocals separation is presented in [5]. Another method to accomplish vocals separation is extracting the harmonic components of the predominant melody from the sound mixture and then resynthesizing the melody by using a sinusoidal model [1, 8]. In addition, the authors of [1] selected reliable frames of the obtained melody to get classiﬁcation between the vocal and non-vocal frames. Reported results are 95% correct classiﬁcation on a number of 40 songs from 10 singers, using 15 linear prediction mel cepstral coefﬁcients and 64 components GMM maximum likelihood classiﬁcation. The question that arises is which of the former methods is more robust to accompaniment inﬂuences, and to which degree. This paper gives an evaluation of different classiﬁcation methods in polyphonic case and also separation of the vocal line. Mixtures with various relative levels of the singing and accompaniment were used in order to evaluate the robustness of the methods. 65 songs from 13 singers were mixed at levels starting with clean voice to 0dB and -5dB singing-to-accompaniment ratio (SAR). Classiﬁcation strategies include linear and quadratic discriminant functions, GMM based maximum likelihood classiﬁer and nearest neighbor classiﬁers using Kullback-Leibler divergence between GMMs of the song under analysis and the singers. The acoustic material was produced so that the accompaniment does not provide any information about the singer’s identity. This ensures that the evaluation is based on singer identiﬁcation and not on the accompaniment. The paper is organised as follows. Section 2 gives general guidelines about the features and the classiﬁcation",
        "zenodo_id": 1417395,
        "dblp_key": "conf/ismir/MesarosVK07"
    },
    {
        "title": "A Methodology for the Segmentation and Identification of Music Works.",
        "author": [
            "Riccardo Miotto",
            "Nicola Orio"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415952",
        "url": "https://doi.org/10.5281/zenodo.1415952",
        "ee": "https://zenodo.org/records/1415952/files/MiottoO07.pdf",
        "abstract": "The identiﬁcation of unknown recordings is a challenging problem that has several applications. In this paper, we focus on the identiﬁcation of alternative releases of a given music work. To this end, a statistical model of the possible performances of a given score is built from the recording of a single performance. The methodology is based on the automatic segmentation of audio recordings, exploiting a technique that has been proposed for text segmentation. The segmentation is followed by the automatic extraction of a set of relevant audio features from each segment. Identiﬁcation is then carried out using an application of hidden Markov models. The approach has been tested with a collection of orchestral music, showing good results in the identiﬁcation of acoustic performances. 1 INTRODUCTION The automatic identiﬁcation of music works has a number of applications, that range from digital right management, to automatic metadata extraction, and to music access and retrieval. Given the amount of music recordings that are continuously released, manual identiﬁcation of music works is an unfeasible task. A common approach to music identiﬁcation is to extract, directly from a recording in digital format, its audio ﬁngerprint, which is a unique set of features that allows for the identiﬁcation of digital copies even in presence of noise, distortion, and compression. It can be seen as a content-based signature that summarizes an audio recording. A comprehensive tutorial about audio ﬁngerprinting techniques and applications can be found in [3]. Audio ﬁngerprinting systems are normally designed to identify at the same time the music score, which is the symbolic notation of the music events, and the particular recording of a performance, which is an audio signal as captured by one or more microphones [15]. On the other hand, the identiﬁcation of a music work may be carried out also without linking the process to a particular performance. There are some cases where this approach may be required. Music identiﬁcation of broadcasted live performances may not beneﬁt from the ﬁngerprints of other performances, because most of the acoustic parameters may be different. c⃝2007 Austrian Computer Society (OCG). In the case of classical music, the same works may have hundreds of different recordings, and it is not feasible to collect all of them to create a different ﬁngerprint for each recording. An alternative approach to music identiﬁcation is audio watermarking. In this case, research on psychoacoustics is exploited to embed an arbitrary message, the watermark, in a digital recording without altering the human perception of the sound [2]. The message can provide metadata about the recording (such as title, author, performers), the copyright owner, and the user that purchases the digital item [6]. Similarly to ﬁngerprints, audio watermarks should be robust to distortions, additional noise, A/D and D/A conversions, and compressions. On the other hand, watermarking techniques require that the message is embedded in the recording before its distribution, a situation that can be applied only on newly released material. This paper reports a novel methodology for automatic identiﬁcation of music works from the recording of a performance, yet independently from the particular performance. Unknown music works are identiﬁed through a collection of indexed audio recordings, ideally stored in a music digital library. The approach can be considered a generalization of audio ﬁngerprinting, because the relevant features used for identiﬁcation are not linked to a particular performance of a music work. This work extends previous work on music identiﬁcation based on audio to score matching [11], where performances were modeled starting from the corresponding music scores. Also in this case, identiﬁcation is based on hidden Markov models (HMMs). The application scenario is the automatic labeling of performances of tonal Western music through a match with pre-labeled recordings that are already part of an incremental music collection. Audio to audio matching has been proposed in [9, 5] for classical music audio to audio matching and audio to audio alignment respectively, and in [7] for pop music. 2 HIGH LEVEL DESCRIPTION OF MUSIC PERFORMANCES The identiﬁcation of music performances is based on a audio to audio matching process, which goal is to retrieve all the audio recordings from a database that represents the same musical content as the audio query. This is typically the case when the same piece of music is available in several interpretations and arrangements. The basic idea of the proposed approach is that, even if two different performances of the same music work may dramatically differ in terms of acoustic features, it is still possible to generalize the music content of a recording to model the acoustic features of other, alternative, performances of the same music work. A recording can thus be used to statistically model other recordings, providing that they are all performed from the same score. With the aim of creating a statistical model of the score directly from the analysis of a performance, the proposed methodology is based on a number of different steps. In a ﬁrst step, segmentation extracts audio subsequences that have a coherent acoustic content. Audio segments are likely to be correlated to stable parts in a music score, where there is no change in the number of different voices in a polyphony. Coherent segments of audio are analyzed through a second step, called parameter extraction, which aims at computing a set of acoustic parameters that are general enough to match different performances of the same music work. In a ﬁnal step described in Section 3, modeling, a HMM is automatically built from segmentation and parametrization to model music production as a stochastic process. At matching time, an unknown recording of a performance is preprocessed in order to extract the features modeled by the HMMs. All the models are ranked according to the probability of having generated the acoustic features of the unknown performance.",
        "zenodo_id": 1415952,
        "dblp_key": "conf/ismir/MiottoO07"
    },
    {
        "title": "Robust Music Identification, Detection, and Analysis.",
        "author": [
            "Mehryar Mohri",
            "Pedro J. Moreno 0001",
            "Eugene Weinstein"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418043",
        "url": "https://doi.org/10.5281/zenodo.1418043",
        "ee": "https://zenodo.org/records/1418043/files/MohriMW07.pdf",
        "abstract": "In previous work, we presented a new approach to music identiﬁcation based on ﬁnite-state transducers and Gaussian mixture models. Here, we expand this work and study the performance of our system in the presence of noise and distortions. We also evaluate a song detection method based on a universal background model in combination with a support vector machine classiﬁer and provide some insight into why our transducer representation allows for accurate identiﬁcation even when only a short song snippet is available. 1 INTRODUCTION Automatic detection and identiﬁcation of music have been the subject of several recent studies both in research [5, 6, 1] and industry [16, 4]. Music identiﬁcation consists of determining the identity of the song matching a partial recording supplied by the user. In addition to allowing the user to search for a song, it can be used by content distribution networks such as Google YouTube to identify copyrighted audio within their systems and for recording labels to monitor radio broadcasts to ensure correct accounting. Music identiﬁcation is a challenging task because the partial recording supplied may be distorted due to noise or channel effects. Moreover, the test recording may be short and consist of just a few seconds of audio. Since the size of the database is limited another crucial task is that of music detection, that is that of determining if the recording supplied contains an in-set song. Previous work in music identiﬁcation (see [2] for a recent survey) can be classiﬁed into hashing and non-hashing approaches. The hashing approach involves computing local ﬁngerprints, that is feature values over a window, retrieving candidate songs matching the ﬁngerprints from a database indexed by a hash table, and picking amongst the candidates using some accuracy metric. Haitsma et al. [5] used hand-crafted features of energy differences between Bark-scale cepstra. The ﬁngerprints thus computed were looked up in a large hash table of ﬁngerprints for all songs in the database. Ke et al. [6] used a similar approach, but selected the features automatically using boosting. Covell et al. [4] further improve on Ke and extend the technique c⃝2007 Austrian Computer Society (OCG). beyond music to broadcast news identiﬁcation. Two main limitations of hashing approaches are the requirement to match a ﬁngerprint exactly or almost exactly, and the need for a disambiguation step to reject false positive matches. In contrast, the use of Gaussian mixtures allows our system to tolerate variations in acoustic conditions more naturally. Our use of ﬁnite state transducers (FSTs) allows us to index music event sequences in an optimal and compact way and, as demonstrated in this work, is highly unlikely to yield a false positive match. Finally, this representation permits the modeling and analysis of song structure by locating similar sound sequences within a song or across multiple songs. An example of a non-hashing approach is the work of Batlle et al [1]. They proposed decoding MFCC features over the audio stream directly into a sequence of audio events, as in speech recognition. Both the decoding and the mapping of sound sequences to songs is driven by hidden Markov models (HMMs). However, the system looks only for atomic sound sequences of a particular length, presumably to control search complexity. Our own music identiﬁcation system was ﬁrst presented in Weinstein and Moreno [17]. Our approach is to automatically select an inventory of music sound events using clustering and train acoustic models for each such event. We then use ﬁnite-state transducers to represent music sequences and guide the search process efﬁciently. In contrast to previous work, ours allows recognition of an arbitrarily long song segment. In previous work, we reported the identiﬁcation accuracy of our music processing system in ideal conditions. Here, we examine the problem of music detection and identiﬁcation under adverse conditions, such as additive noise, time stretching and compression, and encoding at low bit rates. 2 MUSIC IDENTIFICATION",
        "zenodo_id": 1418043,
        "dblp_key": "conf/ismir/MohriMW07"
    },
    {
        "title": "Drum Transcription in Polyphonic Music Using Non-Negative Matrix Factorisation.",
        "author": [
            "Arnaud Moreau",
            "Arthur Flexer"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417455",
        "url": "https://doi.org/10.5281/zenodo.1417455",
        "ee": "https://zenodo.org/records/1417455/files/MoreauF07.pdf",
        "abstract": "We present a system that is based on the non-negative matrix factorisation (NMF) algorithm and is able to transcribe drum onset events in polyphonic music. The magnitude spectrogram representation of the input music is divided by the NMF algorithm into source spectra and corresponding time-varying gains. Each of these source components is classiﬁed as a drum instrument or non-drum sound and a peak-picking algorithm determines the onset times. 1 INTRODUCTION The transcription of percussive instruments in music signals is an important step to analyzing its rhythmical content, which is useful in genre classiﬁcation or beat/meter detection. The detection of drum occurrences is a less difﬁcult task than the transcription of harmonic instruments because percussive instruments in general stay constant in pitch throughout the recording. In the case of pure percussive music different approaches give reasonably accurate results [4], but if pitched instruments are also present in the signal, the task becomes very difﬁcult because they disturb the transcription process. There are two different approaches to the problem that appear in literature: (i) Onset detection based systems [5] ﬁrst search the input signal for potential drum onsets and then classify them. (ii) Separation based systems [6, 3] ﬁrst use source separation",
        "zenodo_id": 1417455,
        "dblp_key": "conf/ismir/MoreauF07"
    },
    {
        "title": "Evaluating a Chord-Labelling Algorithm.",
        "author": [
            "Daniel Müllensiefen",
            "David Lewis 0001",
            "Christophe Rhodes",
            "Geraint A. Wiggins"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417779",
        "url": "https://doi.org/10.5281/zenodo.1417779",
        "ee": "https://zenodo.org/records/1417779/files/MullensiefenLRW07.pdf",
        "abstract": "This paper outlines a method for evaluating a new chordlabelling algorithm using symbolic data as input. Excerpts from full-score transcriptions of 40 pop songs are used. The accuracy of the algorithm’s output is compared with that of chord labels from published song books, as assessed by experts in pop music theory. We are interested not only in the accuracy of the two sets of labels but also in the question of potential harmonic ambiguity as reﬂected the judges’ assessments. We focus, in this short paper, on outlining the general approach of this research project. 1 INTRODUCTION AND BACKGROUND Our motivation comes from the need to derive sequences of chord labels from transcriptions of pop songs for a current project 1 hosted at Goldsmiths College, one subtask of which is to provide a summary of the harmonic structure of a song in the form of a sequence of chord labels, of the sort used in lead sheet notation. Several algorithms have been proposed that assign chord labels to points in time, based on note events in a score-like data structure (see e.g. [6]), but none of these algorithms proved fully suited for our purpose. We require the algorithm not only to give the chord root and chord type, functional bass note and extensions for the note events in a time window but also to decide on the optimal width of the time window itself and, furthermore, deal with music where the structures of classical harmony may apply to only a limited extent. We have proposed [5] using Bayesian model selection to tackle segmentation into appropriate time windows and chord label assignment simultaneously. An initial evaluation using manually-generated ground truth showed an accuracy of around 75% for root and type of the chord at each beat of the test set. This preliminary evaluation raised some concerns and questions that motivated this paper; chief among these was the way in which the ambiguity of the task is not considered in ground-truth-based evaluation. 1 http://doc.gold.ac.uk/isms/m4s c⃝2007 Austrian Computer Society (OCG).",
        "zenodo_id": 1417779,
        "dblp_key": "conf/ismir/MullensiefenLRW07"
    },
    {
        "title": "Transposition-Invariant Self-Similarity Matrices.",
        "author": [
            "Meinard Müller",
            "Michael Clausen"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414814",
        "url": "https://doi.org/10.5281/zenodo.1414814",
        "ee": "https://zenodo.org/records/1414814/files/MullerC07.pdf",
        "abstract": "Self-similarity matrices have become an important tool for visualizing the repetitive structure of a music recording. Transforming an audio data stream into a feature sequence, one obtains a self-similarity matrix by pairwise comparing all features of the sequence with respect to a local cost measure. The basic idea is that similar audio segments are revealed as paths of low cost along diagonals in the resulting self-similarity matrix. It is often the case, in particular for classical music, that certain musical parts are repeated in another key. In this paper, we introduce the concept of a transposition-invariant self-similarity matrix, which reveals the repetitive structure even in the presence of key transpositions. Furthermore, we introduce an associated transposition index matrix displaying harmonic relations within the music recording. As an application, we sketch how our concept can be used for the task of audio structure analysis. 1 INTRODUCTION The general concept of self-similarity matrices, which has been introduced to the music context by Foote [3], reveals the repetitive structure of a time-dependent data streams. One ﬁrst transforms a given audio recording into a sequence V := (v1, v2, . . . , vN) of feature vectors vn ∈F, 1 ≤n ≤N, where F denotes a suitable feature space (e. g., a space of spectral, MFCC, or chroma vectors). Then, based on a suitable local cost measure c : F × F →R, one forms an N-square selfsimilarity matrix S deﬁned by S(n, m) := c(vn, vm), 1 ≤n, m ≤N, comparing all features in a pairwise fashion. The crucial observation is that a pair of similar segments in the audio recording is revealed as a path of low cost along diagonals in the resulting self-similarity matrix. As the running example of this paper, we consider the ﬁrst movement of Beethoven’s piano sonata Op. 31, No. 2 (“Tempest”) in a recording by Barenboim. The rough musical form of this movement is given by A1A2BA3C, where A1 corresponds to the exposition (measures 0–90), A2 to the repetition of the exposition, B to the development (measures 93–142), A3 to the recapitulation (measures 143–217), and C to a short coda (measures 218– 228). The musical parts A1 and A2, which are mere repetitions in the score, are played by Barenboim in the same c⃝2007 Austrian Computer Society (OCG). fashion and correspond to the time intervals [0 : 124] and [130 : 251] (measured in seconds) of the recording, respectively. However, even though A3 semantically corresponds to A1, there are signiﬁcant variations in structure and key. A musical analysis shows that A1 has the substructure A1 = R1S1T1U1, where R1 represents the ﬁrst measure, S1 measures 2–7 (part of the ﬁrst theme), T1 measures 8–40 (continuation of the ﬁrst theme and the transfer to the second theme), and U1 measures 41–90 (second theme). Similarly, one has substructures A2 = R2S2T2U2 and A3 = R3X3S3T ′ 3U3. Here, the three Rand S-parts more or less coincide. Similarly, the three U-parts closely correspond to each other, however, with one difference: U3 is a modulated version of U1 transposed ﬁve semitones upwards (and later transposed seven semitones downwards). Furthermore, A3 contains an additional part X3 and part T ′ 3 signiﬁcantly differs from its counterpart T1 in structure and key. A conventional self-similarity matrix as shown in Figure 1 (a) (with respect to chroma-based audio features as discussed in Section 2), reveals only parts of the musical structure. In particular, the path starting at coordinate (0, 130) and ending at (124, 251) indicates the similarity of the time intervals [0 : 124] (part A1) and [130 : 251] (part A2). Similarly, there are paths reﬂecting the similarity of the three Rand S-parts. However, repetitive segments that differ by some transposition are not reﬂected by the self-similarity matrix. In Section 2, we introduce the concept of transpositioninvariant self-similarity matrices that are invariant under all transpositions. In particular, we adopt an idea by Goto [4], which is based on the observation that the transpositions can be handled by cyclically shifting the chroma. Here, the chroma correspond to the twelve traditional pitch classes of the equal-tempered scale [1]. In Section 3, we sketch how the transposition-invariant selfsimilarity matrices can be used for automated audio structure analysis. In Section 4, we conclude this paper and give prospects on future work. Further references to related work are given in the respective sections. 2 TRANSPOSITION-INVARIANT SELF-SIMILARITY MATRIX The properties of a self-similarity matrix S depend on the kind of audio features extracted from the audio recording as well as on the local cost measure c. In the following, we use chroma-based audio features as described, 100 200 300 400 500 50 100 150 200 250 300 350 400 450 500",
        "zenodo_id": 1414814,
        "dblp_key": "conf/ismir/MullerC07"
    },
    {
        "title": "Automatic Transcription of Music Audio Through Continuous Parameter Tracking.",
        "author": [
            "Eric Nichols",
            "Christopher Raphael"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416162",
        "url": "https://doi.org/10.5281/zenodo.1416162",
        "ee": "https://zenodo.org/records/1416162/files/NicholsR07.pdf",
        "abstract": "We present a method for transcribing arbitrary pitched music into a piano-roll-like representation that also tracks the amplitudes of the notes over time. We develop a probabilistic model that gives the likelihood of a frame of audio data given a vector of amplitudes for the possible notes. Using an approximation of the log likelihood function, we develop an objective function that is quadratic in the timevarying amplitude variables, while also depending on the discrete piano-roll variables. We optimize this function using a variant of dynamic programming, by repeatedly growing and pruning our histories. We present results on a variety of different examples using several measures of performance including an edit-distance measure as well as a frame-by-frame measure. 1 INTRODUCTION Polyphonic audio transcription has received considerable attention in recent years and is holds promise in MIR for its potential for automatically creating symbolic music representations from audio. Research in this area has produced a wide variety of approaches [1], [3]-[11] with signiﬁcant contributions, though the problem is deeply challenging and remains open. A recurring theme in this work is that of representing a music spectrogram as a superposition of ﬁxed (but trainable) templates modeling various note aspects. Examples of such template-based approaches are non-negative matrix representations for frames over notes [1], [10] or fundamental frequencies [6], or note-based models involving time-extent as well [7]. Our approach shares some methodology with [11], although that work uses peak detection instead of templates. While these local model-ﬁtting problems are difﬁcult, perhaps even more challenging is the problem of assembling a global interpretation of the data, beyond the frame or note level. Such full transcription problems [3], [8], require the integration of more global musical knowledge. Our work is in this latter category. In addition to attempting polyphonic transcription from unknown sources, we simultaneously estimate the time-varying note amplitude parameters, hoping their knowledge will lead to more © 2007 Austrian Computer Society (OCG). discriminating models. This work has overlap with [4], though our approach is note-based, rather than harmonicbased. The amplitude envelopes themselves may be of primary interest for some applications. 2 A PROBABILISTIC MODEL We present here a probabilistic model describing the likelihood of a frequency spectrum given an assumed conﬁguration of sounding pitches. We denote our sampled time signal as x(m) for m = 0, . . . M −1. Our entire analysis is based on the spectrogram of the time data which we deﬁne as st(k) =",
        "zenodo_id": 1416162,
        "dblp_key": "conf/ismir/NicholsR07"
    },
    {
        "title": "Assessment of Perceptual Music Similarity.",
        "author": [
            "Alberto Novello",
            "Martin F. McKinney"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415050",
        "url": "https://doi.org/10.5281/zenodo.1415050",
        "ee": "https://zenodo.org/records/1415050/files/NovelloM07.pdf",
        "abstract": "This paper extends a study on music similarity perception presented at ISMIR last year, in which subjects ranked the similarity of excerpt-pairs presented in triads [1]. The larger number of subjects and stimuli in the current study required a modiﬁcation of the methodological strategy. We use here two nested incomplete block designs in order to cover the full set of song-excerpts comparisons (triads) while limiting the experimental time per subject. In addition to the two variable factors of the previous experiment, tempo and genre, we examine here the effect of prevalent instrument timbre. We found that 69 of 78 subjects where signiﬁcantly consistent in their judgments of repeated triads. Furthermore, we found signiﬁcant acrosssubject consistency on all 10 repeated triads. A significant difference was found in the distributions of interand intra-genre excerpt distances. The stress values in the Shepard’s plot shows evidence of increased complexity in the present study compared to the previous smaller study. 1 INTRODUCTION Recently, there has been an increasing interest in music similarity, both in the applicative [2] and research [5][6] ﬁelds. Various theoretical [3][4] and experimental works [5][6] have concentrated on which dimensions underlie listeners’ perception of similarity. These studies were run on a small number of stimuli or on a limited number of genres, making it difﬁcult to extend conclusions to the large corpus of Western music. One of the most challenging problems in conducting an experiment on music similarity perception is dealing with the trade off between experimental time and the number of stimuli required for a complete representation of the complexity of the musical world. Our recent study showed that a method combining triadic comparisons and Balanced Incomplete Block Design (BIBD) limited the reasonable experiment duration per subject to a reasonable length (< 1 hour) while examining 18 excerpts. Here, we show how it is possible to further optimize the experimental design using two nested BIBDs to increase the number of stimuli, and thus to examine a broader range of musical styles. c⃝2007 Austrian Computer Society (OCG). 2 METHOD We employed a method using triadic comparisons of songexcerpts, because it is a straightforward procedure for subjects and it alleviates problems associated with scale interpretation. We used two nested BIBD to achieve triad reduction: one to create an incomplete but overlapping set of genres for each subject, the second to create a set of triads within each genre-set. The ﬁrst BIBD was calculated to determine the musical genres for each subject: we used quadratic comparisons (4 genres) per subject. The BIBD formula shows in this case, the number of genre-sets, b: b = λn(n −1) k(k −1) . (1) With n=13 genres, k=4 (quadratic) and λ=3 (each genre pair appeared in three subject designs), we obtain b=39 genre-set, one for each subject. For each genre-set, a BIBD on excerpts was generated. With 6 excerpts per genre, the number of excerpts per genre-set is 24. We used k=3 (triadic) and λ=2 (each excerpt-pair appears twice) reaching 184 triads per genreset (subject). We added ten repeated triads for each subject for evaluation of within and across subject consistency.",
        "zenodo_id": 1415050,
        "dblp_key": "conf/ismir/NovelloM07"
    },
    {
        "title": "A Stochastic Representation of the Dynamics of Sung Melody.",
        "author": [
            "Yasunori Ohishi",
            "Masataka Goto",
            "Katunobu Itou",
            "Kazuya Takeda"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414732",
        "url": "https://doi.org/10.5281/zenodo.1414732",
        "ee": "https://zenodo.org/records/1414732/files/OhishiGIT07.pdf",
        "abstract": "In this paper, we propose a stochastic representation of a sung melodic contour, called stochastic phase representation (SPR), which can characterize both musical-note information and the dynamics of singing behaviors included in the melodic contour. The SPR is constructed by ﬁtting probability distribution functions to F0 trajectories in the F0-∆F0 phase plane. Since ﬂuctuations in singing can be easily separated by using SPR, we applied SPR to a melodic similarity measure for query-by-humming (QBH) applications. Our experimental results showed that the SPR-based similarity measure was superior to a conventional dynamic-programming-based method. 1 INTRODUCTION The goal of this study is to build a model that can represent the dynamics of various singing behaviors (e.g., ﬂuctuations in a musical note and continuous transitions between notes) in a sung melodic contour. Although a symbolic melodic contour (a sequence of musical notes) can be easily modeled by a discrete-time stochastic representation such as n-grams, this representation cannot be used for modeling a sung melody because it is difﬁcult to represent the singing dynamics of its melodic contour, such as vibrato and overshoot. The dynamic representation for modeling a sung melody is important for deﬁning an appropriate melodic similarity between sung melodies, which is useful for various applications such as query-byhumming (QBH) and automatic clustering of songs. Most previous studies including symbolic melodic similarities [1, 2] and melodic similarities for sung melodies [3, 4, 5, 6] focused on the retrieval performance. For sung melodies, for example, a melodic contour was represented by a discrete symbolic sequence of musical notes [3, 4] or a sequence of pitch histograms for unstable pitch contours [5, 6]. Since they did not model the dynamics at all, their melodic similarities are sometimes too sensitive to singing behaviors that may differ among singers. Therefore, we propose a novel stochastic graphical representation of the dynamic properties of sung melodic contours, called stochastic phase representation (SPR). This representation is a generative model of melodic contours and can separate the dynamics of various singing behaviors from an original musical note sequence. By using this c⃝2007 Austrian Computer Society (OCG). 0 1 2 3 4 5 6 7 4500 5000 5500 6000 6500 F0 [cent] 4500 5000 5500 6000 4500 6000 1000 500 0 500 -1000 1000 500 0 -500 -1000 6500 Circle (Vibrato) Spiral (Overshoot) [cent] ∆F0 [cent / sec] (a)  F0 contour 5000 8 [sec] (b)  Phase plane 5500 6500 [cent] (c)  Stochastic phase representation ∆F0 [cent / sec] Figure 1. Schematic view of constructing stochastic phase representation (SPR). The original F0 contour (a) is mapped onto the F0-∆F0 phase plane (b). By ﬁtting Gaussian mixture models to trajectories on the phase plane, stochastic representation of the F0 dynamics (c) can be constructed. representation, we also deﬁne a melodic similarity measure for QBH applications. In our experiments, we show the effectiveness of this similarity measure based on SPR. 2 STOCHASTIC PHASE REPRESENTATION (SPR) FOR MELODIC CONTOUR Figure 1 shows an example of an SPR constructed from singing melodic contours represented as trajectories of the fundamental frequency (F0). We assume that the F0 trajectories are generated by a dynamic system and represented in a two-dimensional phase plane, ⃗f(x, ˙x), where x is the F0 and ˙x is its differential. That is, ⃗f(x, ˙x) represents the local direction of an F0 trajectory. A ﬂuctuation in a sung melody can be modeled by a damped oscillation of the dynamic system and appears as a curling trajectory around a certain target point, i.e., an attractor of the system. The advantage of this modeling is that typical singing behaviors can be characterized by the shape of curling trajectories. As shown in Fig. 1(b), for example, a vibrato within a musical note appears as a circular pattern because it has the quasi-periodic modulation of the F0, and an overshoot after a note change appears as a spiral pattern because the F0 of the overshoot transitionally exceeds the F0 of a (target) musical note just after the note change. Here, the location of each attractor corresponds to the F0 of its target musical note. Therefore, we model the curling trajectories by ﬁtting a Gaussian mixture model (GMM) so that the likelihood of observing the given trajectories becomes the maximum. We refer to this GMM-based representation of the F0 trajectories (sung melodic contours) as stochastic phase representation (SPR) shown in Fig. 1(c). The F0 of musical notes is represented by the location of the local maxima of the SPR, and the singing behavior of those notes is represented by the shape around the local maxima. Because each (target) note and its relative length in a melodic contour are captured as the location and its height of the corresponding local maximum, respectively, the divergence between GMM-based distributions in the phase plane is expected to be a robust melodic similarity measure that can reduce variations by singing behaviors and focuses on the original (target) melodic information. 3 EXPERIMENTS The potential of SPR was preliminarily evaluated on a small QBH application. The song database consists of 50 short excerpts from 25 pop songs of the RWC Music Database (RWC-MDB-P-2001) [7]. The average length of those excerpts is 12 s. For query melodies, 75 subjects listened to each of the above 50 excerpts and then sang its melody with lyrics [8]. The number of recorded samples was 3,750 (75 × 50), but we used 3,257 samples after excluding samples whose melody was extremely different from the original melody. 1 The F0 contour of the query melodies was estimated for every 10 ms by using YIN [9]. The F0 contour of the 50 excerpts in the song database was manually annotated [10]. Both F0 contours were represented in cents so that one equal-tempered semitone corresponds to 100 cents, and then normalized by subtracting the average F0 value over each contour. Finally, the similarity between a query melody and each excerpt in the song database was calculated by using a histogram-intersection distance [11] between their discretized SPRs. SPRs were modeled by 16-mixture GMMs and converted into discretized SPRs where F0 and ∆F0 were uniformly partitioned into square cells (100 cent F0 × 25 cent/sec ∆F0) and relative occurrences (frequencies) within square cells were calculated. However, since this discretized-SPR-based distance did not take into account the temporal order of notes, we divided a long contour into several short segments so that short segments of the query can be compared with the corresponding short segments of each database excerpt in order. Their similarity was calculated by the cumulative sum of their distances. We thus investigated the performance improvement by increasing the number of segments. As for the baseline performance, we also evaluated a tradi1 Since all songs in RWC-MDB-P-2001 were original compositions, the subjects were not familiar with these melodies. Table 1. Percentage of Mean Reciprocal Rank (MRR) DTW Proposed # of segment 1 2 4 8 MRR [%] 64.3 45.6 57.1 65.6 71.1 0",
        "zenodo_id": 1414732,
        "dblp_key": "conf/ismir/OhishiGIT07"
    },
    {
        "title": "MusicSun: A New Approach to Artist Recommendation.",
        "author": [
            "Elias Pampalk",
            "Masataka Goto"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417487",
        "url": "https://doi.org/10.5281/zenodo.1417487",
        "ee": "https://zenodo.org/records/1417487/files/PampalkG07.pdf",
        "abstract": "MusicSun is a graphical user interface to discover artists. Artists are recommended based on one or more artists selected by the user. The recommendations are computed by combining 3 different aspects of similarity. The users can change the impact of each of these aspects. In addition words are displayed which describe the artists selected by the user. The user can select one of these words to focus the search on a speciﬁc direction. In this paper we present the techniques used to compute the recommendations and the graphical user interface. Furthermore, we present the results of an evaluation with 33 users. We asked them, for example, to judge the usefulness of the different interface components and the quality of the recommendations. 1 INTRODUCTION Popular music recommendation services include Amazon’s personalized recommendation lists, personalized Internet radio (e.g. Last.fm and Pandora), and services that make music blogs more accessible (e.g. the Hype Machine). 1 In contrast to most existing recommendation services MusicSun uses neither collaborative ﬁltering nor manually annotated data. Instead we use techniques to automatically extract information from audio and web pages. MusicSun is basically a query-by-example interface. A user selects one or more artists and is given a list of similar artists. In addition, MusicSun offers the users several options to modify the recommendations. In particular, the users can (1) choose which vocabulary the interface uses to describe artists, (2) select a word from a list of words summarizing the query which they consider most relevant to their search, and (3) choose which aspects of similarity they are most interested in (the options are: audiobased sound similarity, web-based sociocultural similarity, or similarity with respect to the selected word). Related work in terms of discovering music includes a number of very different approaches. One example is the query-by-example interface (for songs) presented in [1] where an “aha” slider can be used to ﬁlter results from the same genre and obtain more interesting recommendations. Related work also includes work on visualizing music collections, enabling the user to easily browse and discover new music, e.g., [5, 7, 8, 9, 10, 12, 15, 16]. In terms of discovering music a notable approach is the work 1 http://last.fm, http://pandora.com, http://hypem.com c⃝2007 Austrian Computer Society (OCG). presented in [3] which efﬁciently combines web services to give high quality music recommendations. MusicSun is closely related to the MusicRainbow user interface [14] with respect to the techniques used and the focus on discovering new artists. However, MusicSun gives the users more options to focus their search and also requires them to make more choices. 2 TECHNIQUES The MusicSun interface is generated using audio (tracks) and the associated artist names as input. Using the artist names we crawl the web (using Google), and parse the retrieved pages using 4 different vocabularies. Using this data extracted from the web we compute two of the three aspects of similarity we use and summarize individual artists and groups of artists with words. Using the tracks and audio similarity techniques we compute the third similarity aspect. In this section we brieﬂy describe these techniques and how we combine them. Finally, we list some example recommendations.",
        "zenodo_id": 1417487,
        "dblp_key": "conf/ismir/PampalkG07"
    },
    {
        "title": "Combining Temporal and Spectral Features in HMM-Based Drum Transcription.",
        "author": [
            "Jouni Paulus",
            "Anssi Klapuri"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417257",
        "url": "https://doi.org/10.5281/zenodo.1417257",
        "ee": "https://zenodo.org/records/1417257/files/PaulusK07.pdf",
        "abstract": "To date several methods for transcribing drums from polyphonic music have been published. Majority of the features used in the transcription systems are “spectral”: parameterising some property of the signal spectrum in a relatively short time frames. It has been shown that utilising narrow-band features describing long-term temporal evolution in conjunction with the more traditional features can improve the overall performance in speech recognition. We investigate similar utilisation of temporal features in addition to the HMM baseline. The effect of the proposed extension is evaluated with simulations on acoustic data, and the results suggest that temporal features do improve the result slightly. Demonstrational signals of the transcription results are available at http://www.cs.tut.ﬁ/sgn/arg/paulus/demo/. 1 1 INTRODUCTION Systems for automatic transcription of music have gained a considerable amount of research effort during the last few years. From the point of view of music information retrieval, these can be considered as tools for rising from the acoustic signal to a higher level of abstraction that correlates better with the content of interest. Here we focus on the transcription of drums: locating and recognising sound events created by drum instruments in music. Several methods have been proposed for drum transcription. Some of them are based on locating the onsets of prominent sound events, extracting a set of features from the locations of the onsets, and classifying the events using the features. Systems of this category include the method by Tanghe et al. [11] using support vector machines (SVMs) as classiﬁers, and a system using template adaptation and iterative musical pattern based error correction by Yoshii et al. [13]. As an extension to the systems relying only on acoustic data, Gillet and Richard have proposed a multi-modal system which uses also visual information of the drummer playing [4]. A system using hidden Markov models (HMMs) was presented in [8]. 1 This work was supported by the Academy of Finland, project No. 5213462 (Finnish centre of Excellence program 2006 2011). c⃝2007 Austrian Computer Society (OCG). Frequency Time Classiﬁer Classiﬁer Conventional features TRAPS Figure 1. The basic idea of temporal features illustrated. Instead of short wide-band frames of data, features are calculated from long narrow-band frames. (After [6].) In polyphonic music the presence of other instruments makes the transcription more difﬁcult, since they are effectively noise for drum transcription systems. An alternative to the previous approaches is to try to separate the drums from polyphonic music, or to separate each drum to its own stream. The separation can be done blindly without any prior templates for the drums, as is done by Dittmar and Uhle [2], or by using a dictionary for different drums [9]. Several methods, both blind and dictionarybased, developed by FitzGerald et al. are detailed in [3]. For a more description of the earlier methods refer to [3]. Majority of the features used in the recognisers are “spectral”: parameterising some property of the signal spectrum in a relatively short (e.g., 10 ms) time frames. Features describing the temporal evolution of the signal are usually limited to the ﬁrst temporal derivatives of spectral features, and in essence they still are short-time features. Some systems hand the responsibility of modelling the temporal evolution of the features over to an HMM architecture: different states describe different time instants of the modelled event. Hermansky and Sharma proposed an alternative for this in [6] in the form of using TRAPS (TempoRAl PatternS) features describing energy evolution at different subbands. The main idea behind TRAPS is illustrated in Figure 1. They showed that utilising the information of how the energy evolves on several subbands in one-second frames it was possible to improve the performance of a baseline speech recogniser which used only short-time cepstral features. Features from subband envelopes have been used earlier also in other music related applications, such as muinput signal features features spectral TRAPS TRAPS GMMs observation observation likelihoods likelihoods GMMs drum HMMs transition probabilities decoding model sequence proposed extension Figure 2. Block diagram of the full system with the proposed extension circled with dashed line. sical piece structure analysis [10], genre classiﬁcation [7], and automatic record reviews [12]. However, the features used in these works were concentrated on the modulations of the envelopes whereas we are interested in certain events: the drum hits. We propose to utilise the information from temporal features in addition to the earlier HMM-based system [8], and show that they do increase the performance. Temporal features suit for drums, because drums are usually short events and do not have any “stable” state as e.g. harmonic sounds may have. The baseline HMM system is described in Section 2.1. The added temporal features are detailed in Section 2.2. Methods for combining the information from temporal features to the baseline system are described in Section 2.3. The performance of the resulting system is evaluated with simulations described in Section 3. Finally, conclusions are given in Section 4. 2 PROPOSED METHOD The block diagram of the system including the proposed extension is illustrated in Figure 2. The baseline system extracts a set of spectral features from the input signal and estimates observation likelihoods for all HMM states using Gaussian mixture models (GMMs). Finally, the transcription is obtained by ﬁnding out the best state and model sequence to explain the observed features. The extension adopts the idea from [6] and assumes that temporal features can provide information which can correct some of the errors made by the baseline system. The information provided by the proposed extension is added to the baseline system in the observation likelihood stage before decoding.",
        "zenodo_id": 1417257,
        "dblp_key": "conf/ismir/PaulusK07"
    },
    {
        "title": "A Probabilistic Framework for Matching Music Representations.",
        "author": [
            "Paul H. Peeling",
            "Ali Taylan Cemgil",
            "Simon J. Godsill"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417677",
        "url": "https://doi.org/10.5281/zenodo.1417677",
        "ee": "https://zenodo.org/records/1417677/files/PeelingCG07.pdf",
        "abstract": "In this paper we introduce a probabilistic framework for matching different music representations (score, MIDI, audio) by incorporating models of how one musical representation might be rendered from another. We propose a dynamical hidden Markov model for the score pointer as a prior, and two observation models, the ﬁrst based on matching spectrogram data to a trained template, the second detecting damped sinusoids within a frame of audio by subspace methods. The resulting Bayesian framework is robust to local variations in tempo, and can be used for a wide variety of applications. We evaluate both methods in a score alignment context by inferring the posterior distribution of the current position in the score exactly. The spectrogram method is shown to infer the score position reliably with minimal computation, and the damped sinusoid model is able to pinpoint the positions of score events in the audio with a high level of timing accuracy. 1 INTRODUCTION Musical information is roughly represented in one of three ways: a score, which is a symbolic representation, a MIDI ﬁle, which represents discrete musical events with more precise timing information, and sampled audio, which is the most faithful representation of the sound produced. There are many applications for which we would like to match a number of pieces of music with different representations together. For example, score alignment [14, 10, 9] is the matching of a score representation to the audio representation of the same music. Often in practice, this problem can be reformulated as matching a MIDI representation to audio, assuming the MIDI is quantized to discrete positions and accurately represents the score. In all these applications, the underlying factor which is responsible for causing mismatches between different representations is an unknown tempo process. For example, in the score alignment problem, the tempo of a MIDI representation evolves independently from that of the audio, hence dynamic time warping (DTW) strategies have been popular [19, 4, 11]. Audio synchronization, where two audio representations with different tempi are matched, can c⃝2007 Austrian Computer Society (OCG). also be treated by these strategies [15]. Dynamic time warping (DTW) schemes rely on minimizing an explicit matching function by dynamic programming and may encounter difﬁculties when unexpected events occur, which are not captured in the matching criteria, for example, mistakes made by a player when performing a piece from a score, repeats made in a concert but not during rehearsals, improvisation sections, pauses and reruns, and so on. A complete probabilistic model for music representation enables inclusion of such types of events as a priori information and facilitates learning from data, hence potentially a more robust matching performance can be obtained. Moreover, modern and powerful inference techniques can be developed in cases where the model size becomes large so as not to admit exact computation. In this paper we introduce a probabilistic framework which will allow us to match different music representations in a Bayesian setting. We begin by considering the fundamental representation of music as the score, and construct a prior model of how this representation evolves in time during a performance. One such approach has been developed by Raphael [18], where a probabilistic dynamical model is applied to the tempo of the audio, with the expected timing of events based on the score in a score alignment context. Here we consider the evolution of the position of a ‘score position pointer’ through time, adopting an approach similar to that of [8, 2, 20]. We deﬁne the ‘score pointer’ as an unobserved random variable over score positions evolving according to unknown velocities (tempi). This model differs from previous approaches in the way the tempi are represented. In Section 2 we describe this dynamical model for the score pointer, formulated as a hidden Markov model [17]. Given the formulation it is conceptually straightforward to develop online, ofﬂine and ﬁxed-lag applications using standard exact or approximate inference methodology. In the Bayesian setting, we also require an observation model which assigns a likelihood value to observed data from a different music representation given the current state of the score position pointer. In Section 3 we present two such probabilistic models for the generation of music audio from a score, or where practically more appropriate, MIDI. Our approach in this paper will be to constrain our models to allow for exact inference of the posterior dis-",
        "zenodo_id": 1417677,
        "dblp_key": "conf/ismir/PeelingCG07"
    },
    {
        "title": "Sequence Representation of Music Structure Using Higher-Order Similarity Matrix and Maximum-Likelihood Approach.",
        "author": [
            "Geoffroy Peeters"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416748",
        "url": "https://doi.org/10.5281/zenodo.1416748",
        "ee": "https://zenodo.org/records/1416748/files/Peeters07.pdf",
        "abstract": "In this paper, we present a novel method for the automatic estimation of the structure of music tracks using a sequence representation. A set of timbre-related (MFCC and Spectral Contrast) and pitch-related (Pitch Class Proﬁle) features are ﬁrst extracted from the signal leading to three similarity matrices which are then combined. We then introduce the use of higher-order (2nd and 3rd order) similarity matrices in order to reinforce the diagonals corresponding to common repetitions and reduce the background noise. Segments are then detected and a maximum-likelihood approach is proposed in order to derive simultaneously the underlying sequence representation of the music track and the most representative segment of each sequence. The proposed method is evaluated positively on the MPEG-7 “melody repetition” test set. 1 INTRODUCTION Music structure discovery (MSD) aims at estimating automatically the structure of a music track by analyzing its audio signal. It has become a major topic of interest in the recent years because it allows the development of new paradigms: active music listening (intra-document browsing [4]), acoustic browsing of music catalogues (fast browsing using automatically generated audio summaries [20] or using automatically located chorus, key-phrase, audiothumbnail [15] [5] [3]), music creation (automatic segmentation into cognitively similar parts [12], music mosaicing), media compression [12] and automatic music analysis (understanding music structure through acoustic analysis). MSD algorithms always start by extracting a set of features from the audio signal. The features are then used to detect repetitions of the signal content over time. This notion of “repetition” and “detection of repetition” is the basis of all MSD algorithms developed so far. It is also their main limitation since it does not allow detecting variations or evolutions of a part 1 . The choice of the features therefore plays a central role since it guides the kind of repetitions that can be observed: repetitions can be based on instrument-background repetitions (timbre-related features are for example used by [8]), repetitions of melody or chord-succession (pitch-related used by [3]) or repetitions 1 Note however that [11] takes tonality modulation into account. c⃝2007 Austrian Computer Society (OCG). of rhythm patterns (rhythm-related used by [20] [13]). In the current work both timbre-related and pitch-related features are used. The temporal structure of a music track can then be visualized using a recurrence plot more often called a similarity matrix in the case of music [9] which represents the similarity between each pair of features over time. In order to extract from this visual representation, a numerical representation of the structure of a track, two kinds of representation can be used leading to two different approaches [19]: the state and the sequence representation. The “state” representation (see left part of Fig. 1) considers that a music track is a succession of parts called states and that each time of a music track has emitted a speciﬁc state. A state is deﬁned as a set of contiguous times, which contains similar acoustical information. A state does not need to be repeated later in the track. The notion of states is closely related to the notion of parts in popular music (introduction, verse, chorus and bridge) because for popular music the musical background is often constant during the duration of each part. In this case, the goal of MSD algorithms is to ﬁnd the states that have been emitted at each time. The algorithms rely mainly on segmentation (novelty measure of [8]), partitional, agglomerative or spectral clustering algorithms [15] [6] [1] or hidden Markov models ([15], [20]). The “sequence” representation (see right part of Fig. 1) considers that there exist sequences of time in the music track that are repeated over the track. A sequence is deﬁned as a set of successive times, which is similar to another set of successive times. However the times inside a given sequence do not need to be necessarily identical to each other. All the times of a music track do not belong necessarily to a sequence. The notion of sequence is closely related to the notion of melody (sequence of notes) or chord succession in popular music. These sequences are visible in a similarity matrix through the diagonals, which represent succession of pairs of times with high similarity. The sequence approach allows a more precise description than the state approach, since it allows to detect only the parts which are repeated melodies and are therefore cognitively more memorable. When considering the sequence representation, most approaches only attempts to detect the most representative audio extract from the similarity matrix in order to create a thumbnail [3], [5]. Few papers address the problem of estimating the actual sequence representation from the similarity matrix. When dealing with this problem most authors use Dynamic Time Warping or pattern matching techniques [7] [2] [16] [11]. Recent approaches combine Figure 1. Structure representation in a similarity matrix as [left part] states: we observe three states noted A, C and D. The times noted A belong to the state A. Note that the state C is not repeated later in the track. [right part] sequences: we observe two sequences noted abc and de. Note that a sequence cannot exist if it is not repeated later in the track. DTW with a hierarchical approach of the structure detection [18] [17]. Despite its efﬁciency, the DTW approach remains very heavy in computation time. In this paper, we propose a fast method for the estimation of the sequence of a music track based on a maximum-likelihood approach (parts 2.4 and 2.5). Other contributions of this paper are the simultaneous use of timbre and harmonic-related features combined into a unique similarity matrix (part 2.1) and the use of higher-order similarity matrices (part 2.2). Finally part 3 presents the evaluation of our system on the MPEG7 “melody repetition” test set. 2 PROPOSED METHOD",
        "zenodo_id": 1416748,
        "dblp_key": "conf/ismir/Peeters07"
    },
    {
        "title": "Music Clustering with Constraints.",
        "author": [
            "Wei Peng 0001",
            "Tao Li 0001",
            "Mitsunori Ogihara"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418087",
        "url": "https://doi.org/10.5281/zenodo.1418087",
        "ee": "https://zenodo.org/records/1418087/files/PengLO07.pdf",
        "abstract": "This paper studies the problem of building clusters of music tracks in a collection of popular music in the presence of constraints. The constraints come naturally in the context of music applications. For example, constraints can be generated from the background knowledge (e.g., two artists share similar styles) and the user access patterns (e.g., two pieces of music share similar access patterns across multiple users). We present an approach based on the generalized constraint clustering algorithm by incorporating the constraints for grouping music by “similar” artists. The approach is evaluated on a data set consisting of 53 albums covering 41 popular artists. The “correctness” of the clusters generated is tested using artist similarity provided by All Music Guide. 1 INTRODUCTION For those who listen to music through the Internet, how to navigate in the ocean of on-line music is an important issue. Nowadays, everything about music is on the web — audio, lyrics, artist discographies, artist biographies, reviews, and discussions. This raises an issue of whether the on-line music data can be efﬁciently accessed so that the user can beneﬁt from the existence of such large volumes of data. A solution to the issue can be given by developing efﬁcient music assistance programs, which integrate techniques for analyzing, summarizing, indexing, classifying, and grouping music data. This paper addresses the issue of clustering pop music into groups with respect to the artists. Clustering is the standard, effective tool for efﬁcient organization, summarization, navigation and retrieval of a large amount of data. Information navigation by browsing through data clusters is more suitable for users who have vague information need and/or just wish to discover general contents of the data set. The use of instance-level constraints as the background information to improve data clustering has been widely studied in machine learning in the past few years. Instance-level constraints are generally pairwise and they are of two types: the positive constraint is one that speciﬁes that two instances must belong to the same clusc⃝2007 Austrian Computer Society (OCG). ter, and the negative constraint is one that speciﬁes that two instances must belong to different clusters. These instance-level constraints have been used in learning distance/dissimilarity measures [3,4,7,10,18], modifying objective criteria for cluster evaluation [1], and improving optimization procedures [2,16,17]. The constraints come naturally in the context of music applications. For example, constraints can be generated from the background knowledge (e.g., two artists share similar styles) and the user access patterns (e.g., two pieces of music share similar access patterns across multiple users). In this paper, we investigate the problem of content-based music clustering with such instance-level constraints. In particular, we adapt a generalized constraint clustering algorithm based on K-means and discuss approaches to automatically generate constraints. The rest of the paper is organized as follows: Section 2 introduces the algorithm for constraint-based clustering, Section 3 discusses various approaches to generate constraints in music applications, Section 4 describes the content-based feature extraction, Section 5 show our experimental results, and Section 6 provides conclusions and presents open questions. 2 CONSTRAINT-BASED CLUSTERING This section provides some background on the K-means algorithm and then discusses the constraint-based clustering algorithm following the exposition in [7].",
        "zenodo_id": 1418087,
        "dblp_key": "conf/ismir/PengLO07"
    },
    {
        "title": "An Application of Empirical Mode Decomposition on Tempo Induction from Music Recordings.",
        "author": [
            "Aggelos Pikrakis",
            "Sergios Theodoridis"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418103",
        "url": "https://doi.org/10.5281/zenodo.1418103",
        "ee": "https://zenodo.org/records/1418103/files/PikrakisT07.pdf",
        "abstract": "This paper presents an application of Empirical Mode Decomposition (EMD) on the induction of notated tempo from music recordings. At a ﬁrst stage, EMD is employed as a means to segment music recordings into segments that exhibit similar rhythmic characteristics. At a second stage, EMD is used in order to analyze the diagonals of the Self-Similarity Matrix of each segment, so as to estimate the tempo of the recording. The proposed method has been employed on various music genres with music meters of 2 4, 3 4 and 4",
        "zenodo_id": 1418103,
        "dblp_key": "conf/ismir/PikrakisT07"
    },
    {
        "title": "Indexing Music Collections Through Graph Spectra.",
        "author": [
            "Alberto Pinto",
            "Reinier H. van Leuken",
            "M. Fatih Demirci",
            "Frans Wiering",
            "Remco C. Veltkamp"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416744",
        "url": "https://doi.org/10.5281/zenodo.1416744",
        "ee": "https://zenodo.org/records/1416744/files/PintoLDWV07.pdf",
        "abstract": "Content based music retrieval opens up large collections, both for the general public and music scholars. It basically enables the user to ﬁnd (groups of) similar melodies, thus facilitating musicological research of many kinds. We present a graph spectral approach, new to the music retrieval ﬁeld, in which melodies are represented as graphs, based on the intervals between the notes they are composed of. These graphs are then indexed into a database using their laplacian spectra as a feature vector. This laplacian spectrum is known to be very informative about the graph, and is therefore a good representative of the original melody. Consequently, range searching around the query spectrum returns similar melodies. We present an experimental evaluation of this approach, together with a comparison with two known retrieval techniques. On our test corpus, a subset of a well documented and annotated collection of Dutch folk songs, this evaluation demonstrates the effectiveness of the overall approach. 1 INTRODUCTION Singing songs has always been an important way of passing on stories and expressing emotions, religious beliefs or social values. Most of these folk songs were transferred orally, often signiﬁcantly changing over time and location. This resulted in the existence of many versions of the same songs, often displaying considerable variations. In Onder de groene linde, a collection of Dutch folk songs has been assembled by Ate Doornbosch, a Dutch radio broadcaster and researcher [1]. By recording many singers in the countryside during a period of over three decades, he captured this cultural heritage counting more than 7300 songs on tape. A large part of these melodies and songs has now been transcribed to music notation. The collection is becoming available to the general public and research community [7]. Content-based music retrieval opens up this great resource in such a way that both audiences can access it better. Through music information retrieval, songs belonging to the same class of songs can be grouped, or songs with only slight variations can be found. It can help identify the composer of a song, or asc⃝2007 Austrian Computer Society (OCG). sist in any other scholarly musicological task. We have three main contributions in this paper. Firstly, we introduce a new approach to music retrieval in which the music is represented as graphs, and the matching is based on speciﬁc features of these graphs. Our graph representation encodes the interval structure of a melody; it is a global time-independent signature of the melody, displaying the network of connections that exists between the pitch classes. Secondly, we introduce our indexing approach, which is new to music retrieval. To compute similarity between melodies, an algebraic structure is associated to each graph: an n × n matrix, with n equal to the number of vertices in the graph. Thirdly, we evaluate our method on a test corpus of Dutch folk songs. In this evaluation, we compare our method to two other methods: one approach speciﬁcally targeted towards folk song collections, and one approach using a time-independent structural approach as well. In this comparison, our method outperforms the other methods in terms of three well-known performance measures, namely, nearest neighbor, ﬁrst and second tier. 2 RELATED WORK Melodic similarity has been investigated by many authors from very different points of view, using different kind of song collections as dataset. One of the most complete and recent studies has been performed by M¨ullensiefen and Frieler, who explored the concept melodic similarity within a collection of folk songs [10]. Using a collection of 577 Luxembourg folk songs, they empirically established an optimal similarity measure (the Opti3) that combines several known methods into one unifying expression. Out of 50 implemented musical similarity measures, taking into account all sorts of musical features, a weighted combination of methods was chosen to create one measure that best reﬂected the results of an extensive human listening experiment. Since their approach is speciﬁcally targeted towards a collection of folk songs, we compare it to our method as well. Another example of a representation/matching/indexing paradigm is the weighted point set on which the Earth Mover’s Distance or Proportional Transportation Distance can be applied. This kind of approach has been applied to test music similarity as well [13]. The notes of a melody are encoded as weighted points in a two-dimensional space where pitch and onset time are the axes; the duration of a note determines its weight. Similarity between two melodies can now be computed by measuring the effort it takes to transform one weighted pointset into the other. 3 REPRESENTATION tion of a melodic line that actually makes sense from a musical point of view. With this aim, we start looking just at melodies, not considering the rhythm. Melodies are generally studied from a pitch sequence/contour point of view. Our approach is different: we take as a starting point the interval structure, by which we mean the network of connections between pitches. We remark that melodies use only a subset of all possible connections, and with different frequencies. To model such relationships we use graphs, which have various and signiﬁcant applications throughout mathematics, computer science, and physics. As such, the graph is a projection of the time-dependent concept of melody to a time-independent concept of intervallic structure. The next level of abstraction is to leave out pitch class information so that only the “interval connectivity” of the melody remains, and this means that certain operations such as inversion, transposition, retrogradation, other kind of permutations in the pitch class set and (some) shifting of fragments does not affect the graph. In this perspective what we are modelling is a global, time-independent signature of the melody [11], [8]. Melodies that display a similar interval behaviour have similar graphs, for example melodies in which there are one or two central notes (with many connections) and a number of peripheral notes (few connections). Let M be a melodic sequence of length m = |M| and consider the sequence of pitches {pj}j∈I, {I = 1, ..., m}. Then let V = Z12 be the (metric) space of pitches, or pitch classes, in the 12-tone system. We deﬁne the graph G with vertex set VG = V and edge set whose elements are the edges aj such that aj : \u001a pj →pj+1 for every couple (pj, pj+1) ⊆M pm →p1 for the couple (pm, p1) where j = 1, . . . , m −1 (see also [2] and [5]). The arrow am : pm →p1 does not represent an actual interval in the melody but it has been added for symmetry reasons and in order to take into account the relationship between the last and the ﬁrst note as well, which otherwise would not have been reﬂected in the model. 4 INDEXING The graph representation described up to now is a geometric one. In order to allow computations with this representation, we need to associate an algebraic structure to it. The most common algebraic structure to represent a graph is the adjacency matrix. The adjacency matrix A(G) of a graph G is a square matrix of size equal to the order of the graph and where the entry (i, j) represents the number of oriented edges from vertex i to vertex j. This adjacency matrix therefore contains all the information to reconstruct the connectivity of the graph. A matrix closely related to the adjacency matrix is the laplacian matrix L(G), computed as L(G) = D(G)−A(G), where D(G) is the degree matrix of G. The degree matrix is also a square matrix of size equal to the order of the graph, but all values are zero except for those on the main diagonal. Here, the entry (i, i) represents the number of outgoing edges of vertex i. Given the laplacian matrix of a melody graph, the question remains how to compute the similarity to another melody. For this purpose, we ﬁrst compute the eigenvalues of the laplacian matrix and sort them by magnitude. 1 Hereby, we obtain the laplacian spectrum of the graph, that is known to reﬂect a number of important properties of the graph. These properties include the diameter (related to the second smallest eigenvalue), mean distance, minimum degree and algebraic connectivity. Furthermore, the spectrum is invariant under permutations of the matrix (i.e. swapping columns or rows). Together with the absence of pitch information stored in the matrix, this makes the representation invariant under transpositions and note permutation. This is an important property, because as pointed out before, our concept of similarity is also independent from note permutation and transposition. Our main motivation for encoding the topology of a graph using the laplacian matrix comes from the fact that laplacian matrices are more natural, more important, and more informative than other matrices about the input graphs [9]. Previously, Godsil and McKay [4] and more recently Haemers and Spence [6] have also shown that the laplacian matrix has more representational power than the adjacency matrix, in terms of resulting in fewer cospectral graphs. Recall that two graphs are called cospectral (or, isospectral) if they have the same eigenvalues. Given a query graph and a large database, the objective of an indexing algorithm is to efﬁciently retrieve a small set of candidate matches, that share topological similarity with the query. As pointed out, we encode the topology of a graph through its laplacian spectrum, which is used as a signature for the database object. This spectrum can be seen as a point in a high dimensional space. To compute similarity between two graphs, we compute the Euclidean distance between their signatures, which is inversely proportional to the structural similarity of the graphs. Therefore, for a given query, retrieving its similar graphs can be reduced to a nearest neighbor search among a set of points. A set of candidate matches can now be found without having to inspect the entire database. For more details on this indexing strategy, the reader is referred to [3]. 1 Since the graphs are directed, the laplacian matrix is not necessarily symmetric. Consequently, some of the eigenvalues may be complex numbers and there exist multiple strategies for sorting these. As in [12], we sort these eigenvalues by modulus. CRITERIA NN 1st tier 2nd tier LAPLACIAN 66% 44% 63% ADJACENCY 58% 28% 48% OPTI3 40% 39% 56% EMD 64% 33% 50% PTD 64% 30% 46% Table 1. Nearest neighbour (NN), ﬁrst tier and second tier results on the Onder de groene linde collection, computed using Laplacian spectra (L) Adjacency spectra (A) of the graphs. The results are compared to the methods Opti3, EMD and PTD. 5 EXPERIMENTS In “Onder de groene linde”, a large number of Dutch folk songs is preserved. This collection consists of more than 7300 songs recorded on tape. These songs are documented and annotated in great detail, and illustrated by sheet music examples. We experimented on a subset of this resource, that consists of 141 songs, of which we used the ﬁrst phrase. These songs have been classiﬁed in 18 classes or melody groups, that relate to the concept of melody norms. At the Meertens Institute (a research institute for Dutch language and culture in Amsterdam) the concept of melody norm 2 is used to group historically or “genetically” related, orally transmitted melodies. Because the contents of folk song collections such as OGL are highly fragmented, it is impossible to trace back the history of melodies and to ﬁnd all variants that are derived from a common ‘ancestor’ melody. What can be done, is to ﬁnd related groups of melodies within the collection, based on both melodic similarity and available meta data, and link them to melody norms. A search engine would speed up this process of relating melodies considerably. As a ground truth in our experiments, we used a classiﬁcation of the melodies into melody groups, that serve as candidates for the melody norms to be assigned in a later stage. For all the melodies in our test corpus, a graph has been constructed as described in Section 3. We evaluated retrieval performance with these graphs using both the adjacency and the laplacian spectra. The results are summarised in Table 1. For both experiments, we computed some retrieval statistics, namely nearest neighbor, ﬁrst and second tiers, each averaged over all possible queries. These are frequently used in information retrieval. The ﬁrst ﬁgure is the percentage of correct nearest neighbors (NN), i.e. the number of cases in which the top ranked database item, discarding the query itself, belongs to the same class as the query. We also computed the ﬁrst tier, i.e. how many melodies of the query’s class are returned within the ﬁrst K −1 matches, where K is the size of the query class. A similar performance ﬁgure is the second tier, i.e. how many melodies of the query’s class are returned within the ﬁrst 2(K −1) matches. The laplacian 2 Equivalent with “tune family” and “Melodietyp”. spectral method performs best with a NN score of 66%, a 1st tier score of 44% and a 2nd tier score of 63%. Although these performance ﬁgures show in general the efﬁcacy of the method, there are some interesting cases in particular we would like to point out here. In Figure 1 there is a special case of an “almost false” positive: for query OGL19205 (belonging to “Heer Halewijn 3rd version”), the nearest neighbor is OGL19107, that belongs to the group “Heer Halewijn 4th version”. However, the nearest neighbor is somehow related to the query; coincidentally they share the same graph representation, as is shown in Figure 3. This example shows how two melodies can be identical from the interval connectivity point of view but can also be perceptually quite different. This may represent the main limitation of this method in perceptual similarity tasks. The second example (Figure 2) shows the nearest neighbors for the query song OGL19406. Both examples may suggest also that in the case of folksongs people tend to remember more the interval connectivity than the actual intervals of the melody. Furthermore, we experimented with weighting the edges based on the interval they represent. For this purpose, two different sets of weights were used: one reﬂecting the difference in notes on the chromatic scale (ignoring differences in octaves) and one reﬂecting the harmonics of the interval, giving larger weights to consonant intervals and smaller weights to dissonant intervals. During this round of experiments, these methods did not improve the results obtained with normal laplacian spectra. Using the same test corpus and performance measures we compared our method to the optimal distance measure that was established by M¨uellensiefen and Frieler [10]. These results are also presented in Table 1, under the name Opti3. This distance measure is a weighted combination of three distance measures, each working on different feature sets. These measures are harmcore (using harmonic correlation), rhythfuzz (using fuzziﬁed rhythm values) and ngrukkon (taking into account characteristic motives). This combined distance measure was established empirically out of 50 building blocks, by searching for a weighted combination whose performance best reﬂected the results of an extensive human listening experiment. Consequently, this method has been ﬁtted to the data set at hand, explaining why the results are not optimal in our experiment. We also compared our method to the Earth Mover’s Distance (EMD). This distance measure takes two weighted point sets as input, and measures the minimum amount of work needed to transform one into the other by moving weight. The EMD is used in a number of different contexts; in the musical case, as pointed out in [13], the (2 dimensional) weighted point set is represented by the score itself, where the weight assigned to each note is its duration. However, since our method only takes into account the global melodic structure, we projected the weighted points on the pitch axis prior to computing the transportation distances. The “Proportional Transportation Distance” (PTD) is a modiﬁcation of the EMD in order to get a similarity measure based on weight transportation such that the surplus of weight between two point sets is taken into account. Figure 1. Example of false positive for the query song “Heer Halewijn” (3rd version) OGL19205 with its NN, OGL19107, instance of “Heer Halewijn” (4th version). Figure 2. Example of true positive for the query song “In Frankrijk buiten de poorten” (2nd version) OGL19406 with its NN, OGL41709. Figure 3. Graph representation of the folk songs OGL19205 and OGL19107 (see Figure 1). The two letters in each circle represent the pitch classes respectively in the ﬁrst and in the second song. 6 CONCLUDING REMARKS We presented a graph spectral approach that is new to music retrieval. Our method is focussed on the intervallic structure of the melody. This structure is encoded in a graph whose vertices correspond to the 12 pitch classes and whose edges reﬂect the interval sequence of the melody; an edge is added to the graph if the pitch classes of the corresponding vertices appear consecutively in the melody. The graphs are indexed into a database using their laplacian spectra, a feature vector that reﬂects the original topology and graph structure to a large extent. We evaluated our approach using a subset of a large collection of Dutch folk songs. On this test corpus, our method clearly outperforms existing methods. It is our intention to investigate this method further, for instance by weighting the edges with the duration of the target note and to extend the test corpus. Furthermore we feel that the results can improve by incorporating more detailed musical features. 7 ACKNOWLEDGMENTS This research was supported by the FP6 IST project 5115722 PROFI and WITCHCRAFT NWO project 640-003501. The authors wish to thank Peter van Kranenburg (Universiteit Utrecht) and Ellen van der Grijn (Meertens Instituut). Many thanks also to Daniel M¨ullensiefen and Klaus Frieler for making available the SIMILE package, which includes Opti3. 8 REFERENCES [1] Onder de groene linde. Uitgeverij Unipers, 19871991. [2] B´ela Bollob´as. Modern graph theory. SpringerVerlag, New York, 1998. [3] M.F. Demirci, R.H. van Leuken, and R.C. Veltkamp. Indexing through laplacian spectra. Computer Vision and Image Understanding, To appear, 2007. [4] C.D. Godsil and B.D. McKay. Constructing cospectral graphs. In Aequationes Mathematicae, pages 257– 268, 1982. [5] Chris Godsil and Gordon Royle. Algebraic Graph Theory, volume 207 of Graduate Texts in Mathematics. Springer Verlag, 2001. [6] W. H. Haemers and E. Spence. Enumeration of cospectral graphs. Eur. J. Comb., 25(2):199–211, 2004. [7] Meertens Instituut. Nederlandse liederenbank. http://www.liederenbank.nl/. [8] P. Leonardo. A Graph Topological Representation of Melody Scores. Leonardo Music Journal, 12(1):33– 40, 2002. [9] B. Mohar. The laplacian spectrum of graphs. In Sixth International Conference on the Theory and Applications of Graphs, pages 871–898, 1988. [10] D. M¨ullensiefen and K. Frieler. Optimizing measures of melodic similarity for the exploration of a large folk song database. In Proc. of ISMIR, 2004. [11] Alberto Pinto and Goffredo Haus. A novel xml music information retrieval method using graph invariants. ACM Transactions on Information Systems, To appear, 2007. [12] A. Shokoufandeh, D. Macrini, S. Dickinson, K. Siddiqi, and S.W. Zucker. Indexing hierarchical structures using graph spectra. Pattern Analysis and Machine Intelligence, 27(7), 2005. [13] Rainer Typke, Frans Wiering, and Remco C. Veltkamp. Transportation distances and human perception of melodic similarity. Musicae Scientiae, Discussion Forum 4A, 2007 (special issue on similarity perception in listening to music), p. 153-182.",
        "zenodo_id": 1416744,
        "dblp_key": "conf/ismir/PintoLDWV07"
    },
    {
        "title": "Meaningfully Browsing Music Services.",
        "author": [
            "Tim Pohle",
            "Peter Knees",
            "Markus Schedl",
            "Gerhard Widmer"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417777",
        "url": "https://doi.org/10.5281/zenodo.1417777",
        "ee": "https://zenodo.org/records/1417777/files/PohleKSW07.pdf",
        "abstract": "We present a browser application that offers the user an enhanced access to the content of music web services. Most importantly, the technique we apply aims at making it feasible to add to the automated suggestion of similar artists some intentional spin, or direction. At the heart of the algorithm, automatically derived artist descriptions are analyzed for common topics or aspects, and each artist is described by the extent to which it is associated with each of these topics. The browser application enables the user to formulate a query by means of these underlying topics by simply adjusting slider positions. The best matching artist is shown, and its web page found on the web music service is displayed. 1 INTRODUCTION Today, it is common for online music shops and music web services to have functions for convenient browsing and discovery of new music. One of these functions is the suggestion of similar artists. Such a similar artists function is quite useful, but it has the drawback that the process of discovering new artists may turn into a trial-and-error process, as it may not be immediately obvious if the suggested artists are closer to the user’s particular likes (or dislikes). Recently, we have suggested a procedure to describe artists by the aspects (or topics) they are commonly associated with [1]. Based on such a description, it is possible to give the similar artist suggestions an intentional spin, or direction. In the demonstration presented here, we have implemented a browser application that enables the user to access the content of a music web service with this technique. The recommendation tools offered by the music service’s web interface are also available, so that the application allows the user to get an overall impression how the new technique could complement existing approaches. 2 RELATED WORK There are music recommendation services (such as musiclens.de) that allow for browsing music by adjusting slider c⃝2007 Austrian Computer Society (OCG). values. The main difference is that the approach presented here automatically derives the browsable categories from the given data, thus no time-consuming manual annotation is necessary. Also, our approach does not work on the track level but on the artist level. 3 ALGORITHM In this section, we give an overview of the implemented procedure. Details can be found in [1]. The outcome of the procedure when applied to the data used in this demonstration is given in the next section. The outline of the implemented procedure is as follows: 1) Obtain artist descriptions. For each artist that is in the repository, a description is automatically extracted from the web. This is done by querying a web search engine with +“artist name” +music + review. The found pages are analyzed for the occurrence of particular music-related words, and a TF×IDF vector is calculated [2]. The output of this step is a long list of words associated with each artist. Each of the words has a weight associated. 2) Analyse artist descriptions for common properties. The artist descriptions (long vectors of weighted terms) are compressed to few – ideally meaningful – concepts that make a high-level interaction feasible. For this step, we apply Non-Negative Matrix Factorization (NMF) to yield eight main concepts [3]. The input of NMF are the TF×IDF vectors of all artists, and the most important output are eight vectors that allow a projection of each highdimensional artist vector down to eight dimensions. 3) Represent each artist as a mixture of the common properties. This means to apply the transformation calculated in the previous step to obtain a compressed representation for each artist. After this transformation, each artist is described by the extent to which it is associated with each of the eight concepts. The last step above yields a vector for each artist. The user query also is represented by a vector of the same length, so it is possible to calculate a similarity between the query vector and each artist by applying the cosine similarity measure. The user query can be seen as a usergenerated artist description. Figure 1. Demonstration: The music service browser. Each artist is represented as a vector giving the extent to which this artist is associated with each of the categories represented as sliders on the top. When the user selects an artist (via Quick Selection), its representation is transferred to the sliders and its last.fm web page is shown. When the user modiﬁes the slider positions, the suggested artists (in the Matches box) are changed and a different artist’s page is shown. 4 IMPLEMENTATION For the demonstration, we use list of more than 2.000 artist names that have associated web pages on the music web service last.fm 1 . For each of these artists, up to 100 web pages are retrieved and analyzed as described in Section 3. The artist data is analyzed for eight concepts. For each of the found concepts, the terms with the highest weights are given in Table 1. In the browser application, each of these categories is represented by a slider that is labelled with a manually deﬁned description of the respective concept (Figure 1).",
        "zenodo_id": 1417777,
        "dblp_key": "conf/ismir/PohleKSW07"
    },
    {
        "title": "Musical Memory of the World Data Infrastructure in Ethnomusicological Archives.",
        "author": [
            "Polina Proutskova"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416316",
        "url": "https://doi.org/10.5281/zenodo.1416316",
        "ee": "https://zenodo.org/records/1416316/files/Proutskova07.pdf",
        "abstract": "Ethnomusicological archives build the musical memory of the world, covering the geographical and the historical aspects of music worldwide. This article gives a brief description of the nature and the functionality of ethnomusicological archives. It reflects the current state of data infrastructure (policy and technology), addressing issues of access to archives’ holdings, of online visibility of music collections and of interoperability between archives. An outlook of a mutual involvement and a resulting influence at each others work between MIR community and ethnomusicological archives is given.",
        "zenodo_id": 1416316,
        "dblp_key": "conf/ismir/Proutskova07"
    },
    {
        "title": "MAP Adaptation to Improve Optical Music Recognition of Early Music Documents Using Hidden Markov Models.",
        "author": [
            "Laurent Pugin",
            "John Ashley Burgoyne",
            "Ichiro Fujinaga"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415922",
        "url": "https://doi.org/10.5281/zenodo.1415922",
        "ee": "https://zenodo.org/records/1415922/files/PuginBF07.pdf",
        "abstract": "Despite steady improvement in optical music recognition (OMR), early documents remain challenging because of the high variability in their contents. In this paper, we present an original approach using maximum a posteriori (MAP) adaptation to improve an OMR tool for early typographic prints dynamically based on hidden Markov models. Taking advantage of the fact that during the normal usage of any OMR tool, errors will be corrected, and thus ground-truth produced, the system can be adapted in real-time. We experimented with ﬁve 16th-century music prints using 250 pages of music and two procedures in applying MAP adaptation. With only a handful of pages, both recall and precision rates improved even when the baseline was above 95 percent. 1 INTRODUCTION Optical music recognition (OMR) systems create encodings of the musical content in digital images automatically. For libraries, they constitute very promising solutions for building searchable digital libraries of previously inaccessible material, especially historical documents. Using OMR tools in large-scale digitisation project remains a challenge, however [4], mainly because of the inconsistent performance of most OMR tools. Learning-based approaches, such as the one adopted by Gamera [9], are well suited to such projects, but other approaches, e.g., coupling multiple recognisers, have also been considered recently [3]. When performing OMR on early music sources, one major problem is the extremely high variability exhibited in the data. In printed documents, the font shape may vary considerably from one print to another, and the printing techniques of the time as well as the texture of the paper used resulted in frequent printing irregularities. The physical documents are often degraded, introducing various kinds of noise, and the scanning settings (e.g., brightness or contrast) are not necessarily consistent across all documents. Five examples of 16th-century music prints we used for this study, shown in Figure 1, demonstrate c⃝2007 Austrian Computer Society (OCG). the variability in font shape, document degradation, and scanning parameters. Given such a range of documents, there is no guarantee that an OMR system can be trained to perform well on new document, even with a learningbased approach. Furthermore, as a considerable amount of labelled data is required to train a sufﬁciently reliable system [10], building a system from scratch for every new document encountered is not practical. Similar problems have been encountered previously in speech recognition, where the amount of data and time needed to build a recogniser is considerable. One common approach to solve the problem is to use so-called adaptation techniques. With these techniques, when a new speaker has to be recognised, a system that was previously trained on a large set of other speakers can be optimised for the new speaker using only a small set of new examples. One widely used approach for adaptation in speech is maximum a posteriori (MAP) adaptation [8], a technique that has also been applied in other domains such as handwriting recognition (on-line [2] or off-line [14]), audio transcription [7], and video annotation [1]. In OMR, it is difﬁcult to imagine an application where the recognition errors would not have to be hand-corrected before using the output. For example, if the tool is used to build a digital library or to perform music analysis, it is absolutely necessary to have a correct representation of the musical text. This property makes adaptation techniques of prime interest for OMR because the normal usage of any OMR application software provides the handcorrected data for adaptation and performance improvement. As soon as a page has been recognised and corrected by the user during an OMR process, adaptation can be run so that the subsequent pages will require fewer corrections. In this paper, we present our experiments in using MAP adaptation in Aruspix, an OMR system for early typographic prints based on hidden Markov models (HMMs) [11]. The main goals of the study were to see whether MAP adaptation works in this context, how the adaptation process has to be organised, and how much data is needed to reap beneﬁts from adaptation. We also compared the results obtained with those obtained when training the system from scratch, i.e., without using an adaptation technique. (a) RISM 1528-2 (Attaignant, Paris, 1528) (b) RISM 1532-10 (Moderne, Lyon, 1532) (c) RISM V-1421 (Figliuoli di Gardano, Venezia, 1572) (d) RISM V-1433 (Basa, Roma, 1585) (e) RISM M-0582 (Le Roy & Ballard, Paris, 1598) Figure 1: Examples of prints used for the experiments 2 OMR INFRASTRUCTURE Aruspix provides an infrastructure that handles the complete OMR process. It performs the indispensable preprocessing operations, such as deskewing the image, normalising the size, removing image borders, binarising and cleaning the image, detecting staff positions, and pre-classifying some of the elements (music, ornate letters, lyrics, and title elements). The core of the recognition is performed using HMMs, an original approach to OMR. The Aruspix infrastructure also includes an editor designed especially for early music, which makes it an end-user application as well as a research tool. The machine learning component of Aruspix (training and recognition) is based on the Torch machine learning library [5]. Recognition is performed using continuousdensity HMMs with a 2-pixel sliding window (for a normalised staff height of 100 pixels). At each position, a feature vector of 7 values is extracted [12], and the sequences of feature vectors are then used to build a set of HMMs using embedded training over the whole staff. 3 MAP ADAPTATION The general schema to enable MAP adaptation in Aruspix is to build, in a preliminary phase, a book-independent (BI) system using as large a set of learning data as possible, taken from many different books. The BI system gives acceptable results in general but is not optimised for any book in particular. When a page has been recognised and corrected by the user, the BI system is then adapted with MAP adaptation using the corrected page as an example. This means that, through usage, a book-dependent (BD) system for the book currently being processed can be obtained very quickly. When training the HMMs for the BI system, the expectation-maximisation (EM) algorithm is used to determine the parameter vector λ that maximises P(X|λ), where X is the observed data, i.e.: λ = argmax λ P(X|λ) (1) The principle of MAP adaptation [6] is to ﬁnd the parameters λM that maximises the posterior probability P(λ|X) using the prior knowledge about the model parameter distribution of the already trained model P(λ): λM = argmax λ P(λ|X) = argmax λ P(X|λ)P(λ) (2) To obtain a λM estimate for the BD model, the EM algorithm is applied, and the value obtained enables the means µ of the BI HMMs to be adapted, while the variances, the transitions, and weights are usually unchanged [14]. EM runs with a heuristic weighting factor τ on the relative importance of the new adaptation data. High values of τ privilege the BI model, while low values privilege the new adaptation data. This weighting factor has to be determined empirically. 4 EXPERIMENTS For our experiments, we used microﬁlms of sixteenthcentury music prints held at the Marvin Duchow Music Library at McGill University and the Isham Memorial Library at Harvard University. They were scanned as 8-bit greyscale TIFFs at a resolution of 400 dots per inch. The BI system was trained using 457 pages taken from music books produced by printers from Italy, France, Belgium, and Germany between 1529 and 1595. This set of pages was transcribed and represents a total of 2,710 staves and 95,845 characters. Using this model, Aruspix was trained to recognise 220 different musical symbols (note values from longa to semi-fusa, rests, clefs, accidentals, custodes, dots, bar lines, coloured notes, ligatures, etc.). To experiment with MAP adaptation, we used 5 books printed in France and Italy between 1528 and 1598 (RISM 1528-2, 1532-10, V-1421, V-1433 and M-0582) [13]. For each of them, 50 pages were transcribed and corrected in Aruspix (250 pages in total). We took the ﬁrst 40 pages for the training set and reserved the 10 remaining pages for the test set. We decided to select the ﬁrst pages for adaptation because it is in this order that the data would become available in a digitisation workﬂow, but to verify our results, we performed a 5-fold cross-validation on one of the books, M-0582. In two books, the pages transcribed contain new symbols not represented in the BI model, 10 in M-0582 and 7 in V-1433, which required special treatment. We experimented with MAP adaptation using cumulative procedures, common when experimenting with this technique in an off-line architecture [14]. Unlike incremental MAP adaptation, which generates new BD models for every page of adaptation data using only the new page and the BD model from the previous page, in cumulative MAP adaptation, the BD model is generated using the BI model and the complete set of adaptation data up to that point. We tried two approaches in particular. The ﬁrst is using embedded adaptation with the Viterbi algorithm on whole staves, which is similar to embedded training when training HMMs from scratch [11]. We call it the embedded cumulative MAP (EC-MAP) adaptation. The second approach is to adapt the models for each symbol individually, taking the advantage of the fact that our ground-truth data are aligned. This approach, which we call isolated cumulative MAP (IC-MAP) adaptation, is uncommon because in other domains, the adaptation data are not usually aligned. Finally, we trained a new model from scratch for each of the ﬁve books, using the data in a cumulative way, in order to compare with the MAP adaptation results. The MAP factor τ was empirically optimised in both EC-MAP and IC-MAP. The best results were obtained when the factor was decreased as the amount of data increased, reﬂecting the intuitive assumption that the more data we have for MAP adaptation, the less we need to rely on the original model. During the MAP adaptation process, the new symbols in M-0582 and V-1433 were necessarily trained separately before being inserted into the system. 5 RESULTS The results were evaluated by calculating recall and precision on the best-aligned subsequence of recognised symbols [10]. We computed a baseline by testing the BI model on the ﬁve test sets.",
        "zenodo_id": 1415922,
        "dblp_key": "conf/ismir/PuginBF07"
    },
    {
        "title": "Multipitch Analysis with Harmonic Nonnegative Matrix Approximation.",
        "author": [
            "Stanislaw Andrzej Raczynski",
            "Nobutaka Ono",
            "Shigeki Sagayama"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417809",
        "url": "https://doi.org/10.5281/zenodo.1417809",
        "ee": "https://zenodo.org/records/1417809/files/RaczynskiOS07.pdf",
        "abstract": "This paper presents a new approach to multipitch analysis by utilizing the Harmonic Nonnegative Matrix Approximation, a harmonically-constrained and penalized version of the Nonnegative Matrix Approximation (NNMA) method. It also includes a description of a note onset, offset and amplitude retrieval procedure based on that technique. Compared with the previous NNMA approaches, speciﬁc initialization of the basis matrix is employed – the basis matrix is initialized with zeros everywhere but at positions corresponding to harmonic frequencies of consequent notes of the equal temperament scale. This results in the basis containing nothing but harmonically structured vectors, even after the learning process, and the activity matrix’s rows containing peaks corresponding to note onset times and amplitudes. Furthermore, additional penalties of mutual uncorrelation and sparseness of rows are placed upon the activity matrix. The proposed method is able to uncover the underlying musical structure better than the previous NNMA approaches and makes the note detection process very straightforward. 1 INTRODUCTION The problem of automatic polyphonic music transcription (extracting underlying musical structure from sampled music) has been addressed numerous times, and it still seems there is a long way to go before arriving at a robust and universal technique. This paper tries to lay another brick towards this goal. Automatic music transcription of recorded music is usually a two stage process. The ﬁrst stage is the event detection phase, where music events (note onsets, note offsets, pitch changes) are detected and identiﬁed. In the second stage, these events are transformed into a musical score. This paper focuses on the event detection stage, main part being multipitch analysis, which aims to uncover the fundamental frequencies of simultaneously played harmonic sounds. It is a difﬁcult task, since each sound, besides the fundamental tone, consists of many harmonic tones, some of them having the same frequencies as the fundamental frequencies of other sounds (e.g. in the case of c⃝2007 Austrian Computer Society (OCG). tonal music). It is necessary to distinguish between the fundamental tones and their overtones. A large variety of methods has been used to tackle the multipitch analysis problem (e.g. [1], [2], [4], [9], [11], [12], [13]; an exhaustive list of methods would be very long and we are not going to include it here, but for a good summary, see [6]), but so far none of them solving the problem in a satisfactorily precise and universal way. While our lab has recently developed a powerful method based on Harmonic Temporal Structured Clustering (HTC) for this purpose [4], the procedure proposed in this paper is built upon a method from the family of Nonnegative Matrix Approximations (NNMA), which, under different names and in different varieties, has recently received much attention, also from the music transcription community. To the best of our knowledge, however, none of the NNMA-based methods were developed speciﬁcally for analysis of musical signals. As it will be shown later in this paper, nature of music can be exploited to increase the transcription potential of the algorithm. The goal of this paper was to propose a NNMA variation most suitable for multipitch analysis. Different variations and extensions of the NNMA algorithm have been used for multipitch analysis: the regular NNMA [13], its penalized versions, such as the Nonnegative Sparse Coding (NNSC) [2, 1], or NNMA with basis vectors extended to contain spectrotemporal signatures (a number of consequent data frames), such as Nonnegative Matrix Factor 2-D Deconvolution (NMF2D) and Sparse Nonnegative Matrix Factor 2-D Deconvolution (SNMF2D) [11]. These methods have, however, a few drawbacks. They do not guarantee to yield basis vectors with harmonic structure. NMF2D and SNMF2D use a single signature for every note (of a single instrument), making use of the shift-similarity of logarithmic frequency scale spectra of notes played on a single instrument. This might be an oversimpliﬁcation resulting in an inadequate model. The note spectra are similar, but not identical, with significant differences for some speciﬁc instruments (e.g. ﬂute). Moreover, spectrotemporal atoms cannot account for different note lengths, which results in multiple activity peaks when notes are longer than the signature, and lower activity peaks when notes are shorter than the signature. All of the previous work published on that subject do not propose a complete transcription procedure, simply reporting good results after visual comparison of the activities and symbolic data used to generate the analyzed music. The paper is organized as follows. Section 2 presents a theoretical introduction to the Nonnegative Matrix Approximation and its extension through the addition of constraints and penalties placed upon both the basis matrix and the activity matrix. An overview of the proposed procedure in given in section 3, including description of the proposed Harmonic Nonnegative Matrix Approximation (HNNMA) technique (3.3) and note detection method (3.4). The procedure is evaluated and compared with the results of regular NNMA methods in section 4. 2 THEORETICAL BACKGROUND",
        "zenodo_id": 1417809,
        "dblp_key": "conf/ismir/RaczynskiOS07"
    },
    {
        "title": "The Music Ontology.",
        "author": [
            "Yves Raimond",
            "Samer A. Abdallah",
            "Mark B. Sandler",
            "Frederick Giasson"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1323755",
        "url": "https://doi.org/10.5281/zenodo.1323755",
        "ee": "http://ismir2007.ismir.net/proceedings/ISMIR2007_p417_raimond.pdf",
        "abstract": "<p>LITMUS (Linked Irish Traditional Music) is a two-year cultural heritage linked data project at the Irish Traditional Music Archive in Dublin, Ireland. It focuses on the creation of a linked data ontology specific to Irish traditional music and dance&ndash;the first such ontology based around a music primarily propagated by oral transmission. While efforts to accurately represent, describe, and organise traditional music have numerous challenges, the methodologies used to develop the ontology centre around text-based sources from traditional musicians&rsquo; album notes, mirroring language used by musicians when introducing tunes and songs in both formal and informal performance settings. Using practitioners&rsquo; own language will benefit the eventual application of the ontology within traditional music collections in Ireland, and have potential applications to other European and non-European folk/traditional music collections with similar considerations.</p>",
        "zenodo_id": 1323755,
        "dblp_key": "conf/ismir/RaimondASG07"
    },
    {
        "title": "A Study on Attribute-Based Taxonomy for Music Information Retrieval.",
        "author": [
            "Jeremy Reed",
            "Chin-Hui Lee"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414910",
        "url": "https://doi.org/10.5281/zenodo.1414910",
        "ee": "https://zenodo.org/records/1414910/files/ReedL07.pdf",
        "abstract": "We propose an attribute-based taxonomy approach to providing alternative labels to music.  Labels, such as genre, are often used as ground-truth for describing song similarity in music information retrieval (MIR) systems. A consistent labelling scheme is usually a key in determining quality of classifier learning in training and performance in testing of an MIR system.  We examine links between conventional genre-based taxonomies and acoustical attributes available in text-based descriptions of songs. We show that the vector representation of each song based on these acoustic attributes enables a framework for unsupervised clustering of songs to produce alternative labels and quantitative measures of similarity between songs. Our experimental results demonstrate that this new set of labels are meaningful and classifiers based on these labels achieve similar or better results than those designed with existing genrebased labels.",
        "zenodo_id": 1414910,
        "dblp_key": "conf/ismir/ReedL07"
    },
    {
        "title": "Algorithms for Determining and Labelling Approximate Hierarchical Self-Similarity.",
        "author": [
            "Christophe Rhodes",
            "Michael A. Casey"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418095",
        "url": "https://doi.org/10.5281/zenodo.1418095",
        "ee": "https://zenodo.org/records/1418095/files/RhodesC07.pdf",
        "abstract": "We describe an algorithm for ﬁnding approximate sequence similarity at all scales of interest, being explicit about our modelling assumptions and the parameters of the algorithm. We further present an algorithm for producing section labels based on the sequence similarity, and compare these labels with some expert-provided ground truth for a particular set of recordings. 1 INTRODUCTION",
        "zenodo_id": 1418095,
        "dblp_key": "conf/ismir/RhodesC07"
    },
    {
        "title": "Algorithms for Polyphonic Music Retrieval: The Hausdorff Metric and Geometric Hashing.",
        "author": [
            "Christian André Romming",
            "Eleanor Selfridge-Field"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417615",
        "url": "https://doi.org/10.5281/zenodo.1417615",
        "ee": "https://zenodo.org/records/1417615/files/RommingS07.pdf",
        "abstract": "We consider two formulations of the computational problem of transposition-invariant, time-offset tolerant, meterinvariant, and time-scale invariant polyphonic music retrieval. We provide algorithms for both that are scalable in the sense that space requirements are asymptotically linear and queries are efﬁcient for large databases of music. The focus is on cases where a query pattern M consisting of m events is to be matched against a database N consisting of n events, and m ≪n. The database is assumed to be polyphonic, and the algorithms support polyphonic queries. We are interested in ﬁnding exact and proximate occurrences of the query pattern. The ﬁrst problem considered is that of ﬁnding the minimum directed Hausdorff distance from M to N. We give a (2 + ǫ)-approximation algorithm that solves this problem in O (nm) query time and O (n) space. The second problem is that of ﬁnding all maximal subset matches of M in N, and we give an algorithm that solves this problem in O \u0000m3 (k + 1) \u0001 query time and O \u0000w2n \u0001 space, where w represents the maximum window size and k is the number of matches. Using the same method, the problem can be solved in O (m (k + 1)) query time and O (wn) space if we do not require the time-scale invariance property. The latter query time is asymptotically optimal for the given problem. 1 INTRODUCTION We consider two formulations of the computational problem of content-based, polyphonic music retrieval. We suggest algorithms for both, and they are linked in the sense that they employ the same point-set representation of music. The ﬁrst problem formulation, referred to as the minimum Hausdorff distance problem, considers the dissimilarity of two point sets to be related to the Euclidian distance between points in the set. The second, which we call the maximal subset matching problem, takes the similarity to be related to the number of points in one set that are in the same location as points in the other. Although these two problems might seem similar, the methods required to solve them are inherently different. c⃝2007 Austrian Computer Society (OCG).",
        "zenodo_id": 1417615,
        "dblp_key": "conf/ismir/RommingS07"
    },
    {
        "title": "Improving the Classification of Percussive Sounds with Analytical Features: A Case Study.",
        "author": [
            "Pierre Roy",
            "François Pachet",
            "Sergio Krakowski"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417875",
        "url": "https://doi.org/10.5281/zenodo.1417875",
        "ee": "https://zenodo.org/records/1417875/files/RoyPK07.pdf",
        "abstract": "There is an increasing need for automatically classifying sounds for MIR and interactive music applications. In the context of supervised classification, we conducted experiments with so-called analytical features, an approach that improves the performance of the general bag-of-frame scheme without loosing its generality. These analytical features are better, in a sense we define precisely than standard, general features, or even than ad hoc features designed by hand for specific problems. Our method allows us to build a large number of these features, evaluate and select them automatically for arbitrary audio classification problems. We present here a specific study concerning the analysis of Pandeiro (Brazilian tambourine) sounds. Two problems are considered: the classification of entire sounds, for MIR applications, and the classification of attack portions of the sound only, for interactive music applications. We evaluate precisely the gain obtained by analytical features on these two problems, in comparison with standard approaches.",
        "zenodo_id": 1417875,
        "dblp_key": "conf/ismir/RoyPK07"
    },
    {
        "title": "Comparative Analysis of Multiple Musical Performances.",
        "author": [
            "Craig Stuart Sapp"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417693",
        "url": "https://doi.org/10.5281/zenodo.1417693",
        "ee": "https://zenodo.org/records/1417693/files/Sapp07.pdf",
        "abstract": "A technique for comparing numerous performances of an identical selection of music is described. The basic methodology is to split a one-dimensional sequence into all possible sequential sub-sequences, perform some operation on these sequences, and then display a summary of the results as a two-dimensional plot; the horizontal axis being time and the vertical axis being sub-sequence length (longer lengths on top by convention). Most types of timewise data extracted from performances can be compared with this technique, although the current focus is on beat-level information for tempo and dynamics as well as commixtures of the two. The primary operation used on each sub-sequence is correlation between a reference performance and analogous segments of other performances, then selecting the best correlated performances for the summary display. The result is a useful navigational aid for coping with large numbers of performances of the same piece of music and for searching for possible inﬂuence between performances. 1 INTRODUCTION In the Mazurka Project 1 conducted at CHARM during the past two years along with Nicholas Cook and Andrew Earis, we have collected over 2,500 recorded performances for 49 of Chopin’s mazurkas—on average over 50 performances for each mazurka. Keeping track of differences and similarities between numerous performances is difﬁcult when comparing recordings heard weeks, months or even years apart. And remembering the distinguishing features of 50 individual performances of a composition would be taxing on anyone’s memory. Often the surface acoustics of a performance (such as reverb, microphone placement, piano model, recording/playback noise) are more noticeable and memorable than the actual performance, so identifying related performances solely by ear can sometimes be difﬁcult. A written score contains only the most basic of expressive instructions. The composer relies on the performer to interpret the work according to implicit rules as well as the written instructions. The unwritten rules of a composition are transmitted aurally between performers as well as passed down from teacher to student. These performance conventions can apply to speciﬁc pieces, genres, composers or entire time periods. Performances may involve combining interpretations from several sources, such as 1 http://mazurka.org.uk c⃝2007 Austrian Computer Society (OCG). teachers or other admired pianists; or conversely, it could be a reaction against convention. To help in the exploration of inﬂuences between performances, basic descriptions of tempo and dynamics are extracted from each performance of a work which can then be correlated against each other. A single global similarity measurement for this data could miss interesting smallerscale structures. Therefore, the following plots were developed which display the closest performance to the reference at all possible timescales. In the most interesting variation of the plot, each performance is assigned a color, and when a particular performance is most similar to the reference, its color is ﬁlled in the corresponding point in the plot. As a result of looking at all time spans, patterns of color emerge which can give clues to the relative importance of other performances to the reference performance of the plot. 2 RAW DATA Two types of data are used for comparative analysis: beat duration and loudness. There are many other facets of performance which are being ignored, such as individual note timings, voicing, pedaling, and articulation. However, tempo and overall loudness level at the beats are easier to extract from audio data than many other expressive features and form a reasonable expressive baseline. Both tempo and loudness data are extracted beat by beat throughout a performance, and the data can be plotted against the sequence of beats as illustrated in Figure 1. While the data is extracted by beat from the performances for this paper, we are also working on extracting individual note times and dynamics (including off-beats as well as hand synchrony). Such ﬁne-grained performance information may prove useful in characterizing similarities or differences between performances. Beat durations are extracted by ﬁrst recording taps in real-time while listening to a performance in an audio editor called Sonic Visualiser developed at the Centre for Figure 1. Average tempo and dynamic graphs for 35 performances of mazurka in B minor, 30/2. Digital Music at Queen Mary, University of London. 2 The resulting taps are not aligned precisely to true beat onsets in mazurkas due to a lag in response by the listener— typically with a standard deviation of 60–80 ms (compared to about 30 ms for following a steady tempo). Therefore, audio analysis plugins are used to assist in adjusting the taps onto the exact attack times of notes played on the beats. 3 By repeating data entry for the same performance in an independent manner, the alignment error is reduced to a standard deviation of around 11 ms. Deﬁning a data error as a difference in beat localization by more than 50 ms, the measured data-entry error rate was about 1% for recordings made after 1980 and 3% for recordings in good condition from the early 1920’s. At timing resolutions around 10 ms, deﬁning beat location can become difﬁcult in piano music, particularly due to attack-time differences between the left and right hands (hand synchrony). In these cases, the best procedure is to deﬁne the beat location in a consistent manner in the analogous places in each performance. Since the melody usually contains more expressive timing, it is useful to deﬁne the beat as the time at which the melody note is played rather than using the less-expressive accompaniment. For comparisons of musical dynamics between performances, a smoothed version of the raw power calculated for the audio signal every 10 ms is sampled at each beat location. The raw power in decibels in a sample of audio is given by the equation: raw power = 10 log10",
        "zenodo_id": 1417693,
        "dblp_key": "conf/ismir/Sapp07"
    },
    {
        "title": "Web-Based Detection of Music Band Members and Line-Up.",
        "author": [
            "Markus Schedl",
            "Gerhard Widmer",
            "Tim Pohle",
            "Klaus Seyerlehner"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418325",
        "url": "https://doi.org/10.5281/zenodo.1418325",
        "ee": "https://zenodo.org/records/1418325/files/SchedlWPS07.pdf",
        "abstract": "We present ﬁrst steps towards the automatic detection of music band members and instrumentation using web content mining techniques. To this end, we combine a named entity detection method with rule-based linguistic text analysis. We report on preliminary evaluation results and discuss limitations of the current method. 1 INTRODUCTION AND CONTEXT Automatic extraction of textual information about music artists can be used, for example, to enrich music information systems, for automatic biography generation, to build relationship networks, or to deﬁne similarity measures between artists, a key concept in music information retrieval. Here, we present an approach to ﬁnding the members of a given music band and the respective instruments they play. In this preliminary work, we restrict instrument detection to the standard line-up of most Rock bands, i.e. we only check for singer(s), guitarist(s), bassist(s), drummer(s), and keyboardist(s). 2 METHODS Basically, our approach comprises four steps: web retrieval, named entity detection, rule-based linguistic analysis, and rule selection. Web Retrieval Given a band name B, we use Google to retrieve the URLs of the 100 top-ranked web pages, whose content we then retrieve via wget 1 . Trying to restrict the query results to those web pages that actually address the music band under consideration, we add domain-speciﬁc keywords to the query, which yields the following four query schemes: • “B”+music (abbreviated as M in the following) • “B”+music+review (MR) • “B”+music+members (MM) • “B”+lineup+music (LUM) Discarding all markup tags, we eventually obtain a plain text representation of each web page. 1 http://www.gnu.org/software/wget c⃝2007 Austrian Computer Society (OCG). Named Entity Detection There is a large amount of literature on the topic of named entity detection. A good introduction can be found, for example, in [1]. For this work, we follow a quite simple approach. First, we extract all 2-, 3-, and 4-grams from the plain text representation of the web pages. 2 Subsequently, some basic ﬁltering is performed. We exclude those N-grams whose substrings contain only one character and retain only those N-grams whose tokens all have their ﬁrst letter in upper case and all remaining letters in lower case. Finally, we use the iSpell English Word Lists 3 to ﬁlter out those N-grams which contain at least one substring that is a common speech word. The remaining Ngrams are regarded as potential band members. Rule-based Linguistic Analysis Having determined the potential band members, we perform a simple linguistic analysis to obtain the actual instrument of each member. Similar to the approach proposed in [3] for ﬁnding hyponyms in large text corpora, we deﬁne the following rules and apply them on the potential band members.",
        "zenodo_id": 1418325,
        "dblp_key": "conf/ismir/SchedlWPS07"
    },
    {
        "title": "A Qualitative Assessment of Measures for the Evaluation of a Cover Song Identification System.",
        "author": [
            "Joan Serrà"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415174",
        "url": "https://doi.org/10.5281/zenodo.1415174",
        "ee": "https://zenodo.org/records/1415174/files/Serra07.pdf",
        "abstract": "The evaluation of effectiveness in Information Retrieval systems has been developed in parallel to its evolution, generating a great amount of proposals to achieve this process. This paper focuses on a particular task of Music Information Retrieval: a system for Cover Song Identiﬁcation. We present a concrete example and then try to elucidate which metrics work best to evaluate such a system. We end up with two evaluation measures suitable for this problem: bpref and Normalized Lift Curves. 1 INTRODUCTION Before the ﬁnal implementation of any Information Retrieval (IR) engine, we must carefully consider the quality of the end-product of our efforts. This step can be described as a performance evaluation of a proposed solution. IR techniques can be essentially seen as heuristics: we try to guess something as similar as possible to the right answer. So we have to measure how close to it we can come. Furthermore, evaluation methods are used in a comparative way to measure whether certain changes lead to any improvement in system performance. In particular, when tuning algorithm parameters, it is important to choose the evaluation measure that rewards what we think is a right answer (choosing a valid measure). In the next sections we concentrate on evaluating an IR engine. More precisely, we focus on the evaluation of the effectiveness of a particular Music Information Retrieval (MIR) system. Our goal is to decide which measures are valid (construct validity) for a speciﬁc situation which is presented in subsequent sections. This concrete case is related with Cover Song 1 Identiﬁcation, a very active research topic within the last few years in the MIR community [3, 4, 6], as it provides a direct way of evaluating music similarity algorithms. Some efforts are then being devoted to compare and evaluate different alternatives for this purpose (as MIREX 2 ). 1 According to Wikipedia, in popular music, a cover song (a cover version, or simply cover) is a new rendition, performance or recording of a previously recorded song. 2 http://www.music-ir.org/mirex2006/index.php/Audio Cover Song c⃝2007 Austrian Computer Society (OCG). 2 EVALUATION MEASURES We focus on the situation where a retrieval engine has an input query and it provides an output list of documents (preferably relevant to the query). We ﬁnd in the literature some measures from binary classiﬁcation that might be useful for our purpose: False / True Positives and Negatives (TP, FP, TN, FN), Sensitivity and Speciﬁcity. We also consider here the Fallout Rate, the Receiver Operating Characteristic (ROC) curve, and the Lift Curve [8]. Finally, some popular IR measures we analyze are: Precision, Recall, the Precision-Recall curve, the Break-even Point, the F-measure and Average Precision (AP). We also consider Reciprocal Rank (RR), Discounted Cumulative Gain (DCG) and Binary Preferencebased measure (bpref and bpref-10) [1, 2, 5, 7]. We do not study here other measures such as Spearman’s Rho or Kendall’s Tau, because our data does not ﬁt to the models they were thought for. Basically, we do not have a true measure of similarity for the ground truth (our cover songs, originally, are not ranked from more similar to less similar, we just only know if they are a cover of a given query or not). 3 CASE STUDY: COVER SONG IDENTIFICATION SYSTEM In this section we study the situation where there is a song database (D, the document collection), and we have to come up with an algorithm that, given a song title (query q), yields a list of potential cover songs (A, a list of their titles in descending order of similarity). Here, the query song is not retrieved (that is: q ̸∈A). We should note that there is not a ground truth for song similarity. As a ground truth data, we label 3 all songs, indicating if they correspond to the same group (the same label is attached to the original song and covers of it) or not. Thus, our judgements are based on binary relevance. For our concrete problem, we have a database of 2054 songs (|D| = 2054), labelled into 451 different groups (or “canonical” song versions). The average number of covers per song is 4.24, ranging from 1 (the original song + 1 cover) to 14. Since the maximum number of covers 3 Do not confuse the song titles (which are not relevant for us), with the label we attach to them after listening. a1 a2 a3 a4 a5 a6 a7 a8 a9 a10 a11 a12 a13 a14 |Rq| q1 ⇒A1 ⋆ 1 q2 ⇒A2 ⋆ ⋆ ⋆ ⋆ 7 q3 ⇒A3 ⋆ ⋆ ⋆ ⋆ 7 q4 ⇒A4 ⋆ ⋆ ⋆ ⋆ 14 q5 ⇒A5 ⋆ ⋆ ⋆ ⋆ 14 q6 ⇒A6 4 Table 1. Test answer set example. It consists of 6 manually labelled answer sets (Ai) answering 6 hypothetical queries (qi). These answer sets are composed of 14 ranked documents (Ai = {a1, . . . , a14}), and they are ordered from most valuable (A1) to less valuable (A6). The “⋆” symbol in (i, j) cell denotes that the aj document is relevant for the i-th query. Last column (|Rq|) denotes the total number of covers for the query qi that can be found in the database. per canonical version is 14, the length of the answer set is set to this number in order to be able to present to a potential user all the relevant songs in a single output list (|A| = 14). A cutoff like this is typically introduced in an IR system because of the paginated presentation of search results.",
        "zenodo_id": 1415174,
        "dblp_key": "conf/ismir/Serra07"
    },
    {
        "title": "From Rhythm Patterns to Perceived Tempo.",
        "author": [
            "Klaus Seyerlehner",
            "Gerhard Widmer",
            "Dominik Schnitzer"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418373",
        "url": "https://doi.org/10.5281/zenodo.1418373",
        "ee": "https://zenodo.org/records/1418373/files/SeyerlehnerWS07.pdf",
        "abstract": "There are many MIR applications for which we would like to be able to determine the perceived tempo of a song automatically. However, automatic tempo extraction itself is still an open problem. In general there are two tempo extraction methods, either based on the estimation of interonset intervals or based on self similarity computations. To predict a tempo the most signiﬁcant time-lag or the most signiﬁcant inter-onset-interval is used. We propose to use existing rhythm patterns and reformulate the tempo extraction problem in terms of a nearest neighbor classiﬁcation problem. Our experiments, based on three different datasets, show that this novel approach performs at least comparably to state-of-the-art tempo extraction algorithms and could be useful to get a deeper insight into the relation between perceived tempo and rhythm patterns. 1 INTRODUCTION The tempo is a basic and highly descriptive property of a song, and it is a feature that music users perceive in an intuitive and direct way. Tempo is one of the parameters a user would love to have under control. For instance, one can imagine that depending on her mood, a user would like to be able to choose faster or slower music. Therefore the perceived tempo, the perceived speed of music, would be a perfect and highly intuitive parameter for various new interfaces to music collections. The automatic extraction of tempo information directly from digital audio signals has attracted a lot of research. Published tempo determination methods generally proceed in two stages: extracting low-level information related to apparent periodicities in the signal (we will use the generic term ‘‘rhythm patterns” for this information), and determining some assumed tempo from this information. To be useful in MIR applications, the tempo that is inferred by an algorithm should match the tempo human listeners intuitively perceive when listening to the music, not some ‘theoretical’ tempo that might be deduced from a written score of the piece. It is in this sense that we will use the term perceived tempo here, to denote the tempo that most human listeners would assign to a piece. (Of course, tempo may be perceived differently and may be c⃝2007 Austrian Computer Society (OCG). ambiguous in certain cases, but in general, there will be substantial agreement.) In the evaluation of our tempo extraction method in this paper, we will use music collections manually annotated with tempo values; we will have to assume that the annotations correspond to the tempo that most listeners would perceive. Despite a lot of literature on the subject, the human perception of rhythms, periodicity and pulsation is not yet very well understood. The same is true for the relation between rhythm and tempo perception. Compared to rhythm patterns, which we consider to be low-level features, the common approach in tempo identiﬁcation is to analyse the extracted rhythm patters (even if they not usually called so – see section 2.2 below), assuming that one of the periodicities present in the rhythm patterns corresponds to the perceived tempo. This is usually determined by using some simple peak picking algorithm to predict the most salient tempo. However, the relation between rhythmic patterns and tempo perception might be more complex, and simple peak picking algorithm might not be the most appropriate choice. While a lot of scientiﬁc work has focused on onset detection and rhythm pattern extraction, there has been little effort to gain further insight into this relation between rhythm and perceived tempo to improve the tempo estimation step. We propose to view this relation as a sort of machine learning problem. We model the overall tempo extraction process as a two stage process of rhythm pattern extraction and tempo estimation, and will investigate whether the prediction of perceived tempo from rhythm patterns can be learned by the computer. 2 BASIC NOTIONS",
        "zenodo_id": 1418373,
        "dblp_key": "conf/ismir/SeyerlehnerWS07"
    },
    {
        "title": "A Demonstrator for Automatic Music Mood Estimation.",
        "author": [
            "Janto Skowronek",
            "Martin F. McKinney",
            "Steven van de Par"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417669",
        "url": "https://doi.org/10.5281/zenodo.1417669",
        "ee": "https://zenodo.org/records/1417669/files/SkowronekMP07.pdf",
        "abstract": "Interest in automatic music mood classiﬁcation is increasing because it could enable people to browse and manage their music collections by means of the music’s emotional expression complementary to the widely used music genres. We continue our work on designing a well deﬁned ground-truth database for music mood classiﬁcation and show a demonstrator of automatic mood estimation. While a subjective evaluation of this algorithm on arbitrary music is ongoing, the initial classiﬁcation results are encouraging and suggest that an automatic predicition of music mood is possible. 1 INTRODUCTION In the development of an automatic music mood classiﬁer, we must treat the high degree of subjectivity associated with mood. One can do this by developing personalized models requiring comprehensive user feedback for training, or one can develop generalized models by minimizing the subjectivity involved in the evaluation of music mood. Here we apply the latter approach. A ﬁrst step in minimizing subjectivity is to deliberately deﬁne the goal of the classiﬁcation algorithm: it should model how people would describe the mood expressed in the music (affect attribution) and not how they would actually feel when they are listening to the music (affect induction). See also [1] for a detailed discussion on affect attribution and induction. A second step is to deﬁne mood classes on which users show relative agreement when applying them to music. In addition from an application point of view the mood classes should be easy to use and important to the users. Our previous work [8] reported on the identiﬁcation of those consistent-, importantand easy-mood classes; some issues will be recapitulated in Section 2. As a third step the ground-truth database used for training and testing the desired classiﬁer should contain only music that people can clearly and consistently assign to or exclude from a mood category. This issue will be discussed in more detail in Sections 3 and 4. c⃝2007 Austrian Computer Society (OCG). 2 CLASS DEFINITION In previous work [8] we searched for mood classes that enable the development of the above mentioned generalized mood classiﬁcation models. The basic idea was to ask subjects to evaluate music excerpts using 33 candidate labels used in literature (e.g. [1, 6]), followed by a data analysis in order to identify those mood classes on which subjects show a certain agreement and which subjects assessed as important and easy to use. As a measure for across subject consistency we computed the Cronbach’s coefﬁcient α and as a measure for the importance and easiness of the classes we used questionnaires. In [8] we identiﬁed 8 mood classes that fulﬁlled our criteria (α ≥0.7, at least important, at least easy to use). A more detailed analysis suggested a slight modiﬁcation to that selection of mood classes. First, some mood classes that were highly appreciated in the questionnaires got an α slightly below 0.7. Therefore we lowered our threshold to 0.65. Second, there were very high correlations between some classes, whose meaning of the class names (adjectives) were quite close. Thus we decided to merge them. Third, we loosened the criteria importance and easiness due to the small number of subjects (10): a mood class was selected if at least one of the two criteria was fulﬁlled. Hence the new set of mood classes comprised 12 categories: arousing-awakening, angry-furiousagressive, calming-soothing, carefree-lighthearted-lightplayful, cheerful-festive, emotional-passionate-touchingmoving, sad, loving-romantic, powerful-strong, restlessjittery-nervous, peaceful, tender-soft. 3 MATERIAL COLLECTION In a follow-up experiment subjects labelled a large number of music excerpts using the 12 identiﬁed mood classes. The idea was to select those excerpts that got a clear and consistent rating by the subjects to compile a ground-truth database for training and testing a mood classiﬁcation algorithm. In our previous experiment [8] many excerpts did not fulﬁll our criteria for conveying a clear mood. Hence the new set-up aimed at obtaining a large number of labelled excerpts to ensure a sufﬁcient number of training material, resulting in compromises with respect to the number of involved subjects and an unbalanced distribution of excerpts across music genres. As in the ﬁrst experiment, we selected the excerpts such that their moods are likely to be constant by avoiding drastic changes in the music (structure, tempo, instrumentation etc.) We distributed 1059 excerpts from 12 music genres across 12 subjects such that each excerpt was rated by 6 subjects; each subject had to rate 530 excerpts and the sets per subject were mutually overlapping. The subjects were students and employees working at our laboratory(age between 23 and 40, 8 different nationalities, musical practise from 0 to 21 years). Three of them participated already in the previous experiment. We collected the subjective judgments on a 4-point scale: not, slightly, moderately, deﬁnitively that mood. 4 CLASSIFICATION ALGORITHM In contrast to the mood classes deﬁned by Lu et al. [2], our mood classes are not mutually exclusive. Consequently, we implemented an individual detector for each mood class (binary decision: 1 = that class, 2 = not that class). For that purpose we had to decide for each excerpt and each mood category, whether the excerpt belongs to class 1 or class 2 or whether it should be excluded. An excerpt was accepted if it was relatively consistently rated across subjects (standard deviation ≤1 point on the scale) and if its mean rating was not in an area of ambiguity, which we deﬁned between not that mood and slightly that mood. Depending on the mood label between 5% (sad) and 20% (restless) of the 1059 excerpts were accepted for class 1, between 27% (emotional) and 66% (angry) for class 2. While the consistency criterion was often fulﬁlled, the unambiguity criterion excluded most of the excerpts. The used feature extraction algorithm computes every 743ms a feature vector comprising 4 general feature types: basic signal describing features and their temporal modulation [3], perceptually relevant tempo and rhythm based features [4], features based on chroma and key information [5] as well as features that evaluate the occurences of percussive sound events in the music [7]. The classiﬁcation stage is based on quadratic discriminant analysis and used a randomized 80/20 split of the training and test data in combination with bootstrap repetitions in order to estimated the classiﬁcation performance. Results are shown in Table 1. While some classes show about 75-80 % correct classifciation (e.g. carefree, loving), the performance of other classes (e.g. angry, calming) is with about 90% rather good. In general these results show that an automatic estimation of music mood, as we deﬁned it in Section 1, is possible. 5 CONCLUSIONS The aim of this work is the development of a music mood classiﬁcation algorithm. The applied procedure of groundtruth database design was intended to minimize the subjectivity that is involved in the perception of music mood. Under these constraints the achieved classiﬁcation results as well as the behavior of our real-time demonstrator show that music mood estimation is to a certain extend possible. While a subjective evaluation of the classiﬁcation alMood Class Performance arousing-awakening 85.1 ± 1.7 % angry-furious-agressive 90.1 ± 1.6 % calming-soothing 90.9 ± 2.4 % carefree-lighthearted-light-playful 77.1 ± 1.7 % cheerful-festive 79.8 ± 1.9 % emotional-passionate-touching-moving 82.2 ± 1.1 % loving-romantic 80.2 ± 1.7 % peaceful 88.5 ± 1.7 % powerful-strong 80.7 ± 1.8 % sad 85.0 ± 1.8 % restless-jittery-nervous 90.6 ± 1.5 % tender-soft 89.5 ± 0.6 % Table 1. Classiﬁcation performance (mean ± standard error across bootstrap repetitions) of the individual mood detectors. gorithm on abitrary music is currently being performed, open issues for future research are to further investigate why so many excerpts seem to be ambiguous and how to deal with such music pieces. 6 REFERENCES [1] M. Leman, V. Vermeulen, L. De Voogdt, D. Moelants, M. Lesaffre, Prediction of Musical Affect Using a Combination of Acoustic Structural Cues, J. of New Music Research, Vol. 34(1), 39-67, 2005. [2] L. Lu, D. Liu, H. Zhang, Automatic Mood Detection and Tracking of Music Audio Signals, IEEE transactions on audio, speech, and language processing, Vol. 14(1), 5-18, 2006. [3] M. McKinney, J. Breebart, Features for Audio Music Classiﬁcation, Proceedings of 4th International Conference on Music Information Retrieval, 2003. [4] M. McKinney, D. Moelants, Extracting the perceptual tempo from music audio, Proceedings of 5th International Conference on Music Information Retrieval, Barcelona, 2004. [5] S. van de Par, M. McKinney, A. Redert, Musical Key Extraction from Audio using Proﬁle Training, Proceedings of 7th International Conference on Music Information Retrieval, Victoria, 2006. [6] J. Russell, A circumplex model of affect, J. Personality & Social Psychology, Vol. 39, 1161-1178, 1980. [7] J. Skowronek, M. McKinney, Features for audio classiﬁcation: Percussiveness of sounds, Intelligent Algorithms in Ambient and Biomedical Computing, Philips research Book Series Vol. 7, Springer, 2006. [8] J. Skowronek, S. van de Par, M. McKinney, Groundtruth for automatic music mood classiﬁcation, Proceedings of 7th International Conference on Music Information Retrieval, Victoria, 2006.",
        "zenodo_id": 1417669,
        "dblp_key": "conf/ismir/SkowronekMP07"
    },
    {
        "title": "Similarity Based on Rating Data.",
        "author": [
            "Malcolm Slaney",
            "William White"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416402",
        "url": "https://doi.org/10.5281/zenodo.1416402",
        "ee": "https://zenodo.org/records/1416402/files/SlaneyW07.pdf",
        "abstract": "This paper describes an algorithm to measure the similarity of two multimedia objects, such as songs or movies, using users’ preferences. Much of the previous work on query-by-example (QBE) or music similarity uses detailed analysis of the object’s content. This is difﬁcult and it is often impossible to capture how consumers react to the music. We argue that a large collection of user’s preferences is more accurate, at least in comparison to our benchmark system, at ﬁnding similar songs. We describe an algorithm based the song’s rating data, and show how this approach works by measuring its performance using an objective metric based on whether the same artist performed both songs. Our similarity results are based on 1.5 million musical judgments by 380,000 users. We test our system by generating playlists using a content-based system, our rating-based system, and a random list of songs. Music listeners greatly preferred the ratings-based playlists over the content-based and random playlists. 1 INTRODUCTION This paper describes a means to evaluate the similarity of two multimedia objects using users’ stated preference or rating data about items in the collection. We use a large music collection to illustrate this work, but the same idea applies to any collection of objects where many users report their preference of an object. Most work on similarity uses content-based algorithms. A specialized algorithm looks at the content (usually music) calculates various musically-inspired measures of the sound to form a feature vector, and then compares two features vectors to make a decision about similarity [1]. This similarity measurement is the heart of conventional queryby-example (QBE) systems, such as QBIC [2]. But, even measuring similarity with human raters is difﬁcult [4]. Our work ignores the content. Instead we look at a large group of users and ask if these users, with a wide range of personal musical interests, rate two pieces of music in the same manner. If jazz, classical, blues, and hiphop lovers all rate two songs in the same way, whether c⃝2007 Austrian Computer Society (OCG). they hate it or love it, then the songs are most certainly similar. This paper hypothesizes that ratings data from a large number of users, averaging over many different kinds of tastes, produces an accurate measure of similarity. We compare our approach to a traditional content-based approach. We test this approach by generating a list of songs which are most similar to the query. We show the superiority of our approach by asking for human judgments and by counting songs identiﬁed as similar by virtue that they are performed by the same artist (a weak, but highly objective measure of similarity, as we will discuss in Section",
        "zenodo_id": 1416402,
        "dblp_key": "conf/ismir/SlaneyW07"
    },
    {
        "title": "Annotating Music Collections: How Content-Based Similarity Helps to Propagate Labels.",
        "author": [
            "Mohamed Sordo",
            "Cyril Laurier",
            "Òscar Celma"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415708",
        "url": "https://doi.org/10.5281/zenodo.1415708",
        "ee": "https://zenodo.org/records/1415708/files/SordoLC07.pdf",
        "abstract": "In this paper we present a way to annotate music collections by exploiting audio similarity. Similarity is used to propose labels (tags) to yet unlabeled songs, based on the content–based distance between them. The main goal of our work is to ease the process of annotating huge music collections, by using content-based similarity distances as a way to propagate labels among songs. We present two different experiments. The ﬁrst one propagates labels that are related with the style of the piece, whereas the second experiment deals with mood labels. On the one hand, our approach shows that using a music collection annotated at 40% with styles, the collection can be automatically annotated up to 78% (that is, 40% already annotated and the rest, 38%, only using propagation), with a recall greater than 0.4. On the other hand, for a smaller music collection annotated at 30% with moods, the collection can be automatically annotated up to 65% (e.g. 30% plus 35% using propagation). 1 INTRODUCTION Manual annotations of multimedia data is an arduous task, and very time consuming. Automatic annotation methods, normally ﬁne-tuned to reduced domains such as musical instruments or limited to sound effects taxonomies, are not mature enough to label with great detail any possible sound. Yet, in the music domain the annotation becomes more complex due to the time domain frame. The purpose of making music easily accessible implies a condition of describing music in such a way that machine learning can understand it [1]. Speciﬁcally, these two steps must be followed: to build music descriptions which can be easily maintained, and to exploit these descriptions to build efﬁcient music access systems that help users ﬁnd music in large collections. There are a lot of ways to describe music content, but we can basically classify the descriptors in three groups: editorial meta-data, cultural meta-data, and acoustic meta-data [1]. As a paradigmatic example, the Music Genome Project is a big effort to “capture the essence of music at the fundamental level” by using over 400 attributes to describe c⃝2007 Austrian Computer Society (OCG). songs. To achieve this, more than 40 musicologists have been annotating thousands of ﬁles since 2000. Based on this knowledge, a well–known system named Pandora 1 creates playlists by exploiting these human–based annotations. It is clear that helping these musicologists can reduce both time and cost of the annotation task. Thus, the main goal of our work is to ease the process of annotating music collections, by using content-based similarity distance as a way to propagate labels among songs. 2 RELATED WORK Nowadays, content-based retrieval systems can not classify, identify and retrieve as well as humans can. This is a common problem in the multimedia ﬁeld, like in image or video annotation. But in the latter ﬁelds many attempts have been made [2][3]. Semantic audio annotation, however, has not been as studied as image or video annotation, except the work by Whitman [4] or Barrington et al. [5][6]. Barrington et al. have made signiﬁcant advances in semantic annotation of songs for music information retrieval (MIR) using MFCC’s to describe music content and HMM’s trained on timbre and rhythm for computing similarity between songs. Their idea was basically based on other work that represented image semantic annotation as a supervised multiclassiﬁcation problem [7] . In the MIR ﬁeld, only a few works are dealing with the problem of detecting mood using audio content. Although some results are promising (e.g [8], [9]), there is no standard or clearly deﬁned proposals about the categories and the features to use. In the following experiments, we will check if tags about styles and moods can be propagated using content–based (CB) similarity. 3 EVALUATION The goal of this paper is to prove empirically how contentbased similarity can help to propose labels to yet unlabeled songs, and thus reducing the hard effort of manually annotating songs. For our purpose, the content–based similarity can be seen as a black box. That is to say, given a 1 http://www.pandora.com seed song, the module returns a list of the ith most similar songs. This study employs a CB module that considers not only timbrical features (e.g. MFCC), but some musical descriptors related to rhythm, tonality, etc. [11]. We present two different experiments. The ﬁrst one propagates labels that are related with the style of the piece, whereas the second experiment deals with mood labels. The problem with the Magnatune collection is that there is only one human that annotated the tracks, when normally a ground truth of this nature should be pair–reviewed. Yet, we validated a large amount of the annotated songs by listening to them.",
        "zenodo_id": 1415708,
        "dblp_key": "conf/ismir/SordoLC07"
    },
    {
        "title": "Pitch Spelling with Conditionally Independent Voices.",
        "author": [
            "Gabi Teodoru",
            "Christopher Raphael"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1414946",
        "url": "https://doi.org/10.5281/zenodo.1414946",
        "ee": "https://zenodo.org/records/1414946/files/TeodoruR07.pdf",
        "abstract": "We introduce a new approach for pitch spelling from MIDI data based on a probabilistic model. The model uses a hidden sequence of variables, one for each measure, describing the local key of the music. The spellings in the voices evolve as conditionally independent Markov chains, given the hidden keys. The model represents both vertical relations through the shared key and horizontal voice-leading relations through the explicit Markov models for the voices. This conditionally independent voice model leads to an efﬁcient dynamic programming algorithm for ﬁnding the most likely conﬁguration of hidden variables — spellings and harmonic sequence. The model is also straightforward to train from unlabeled data, though we have not been able to demonstrate any improvement in performance due to training. Our results compare favorably with others when tested on Meredith’s corpus, designed speciﬁcally for this problem. 1 INTRODUCTION We consider here the problem of pitch spelling from MIDI, as has been addressed by several others, including Meredith, [1], [2], Cambouropoulos [3], [4], Chew and Chen, [5], Longuet-Higgins [6], and our previous work [7]. The goal here is to provide the pitch spellings (is it F♯or G♭?) necessary to notate common practice music using a data source, such as MIDI, that does not distinguish between alternate spellings. The most immediate use for such an algorithm is to produce more readable music scores from MIDI data — at present, the pitch spellings from the commercial music notation programs we know of leave much to be desired. But pitch spelling will also be a part of the inevitable migration from MIDI, which, at present, constitutes the lion’s share of symbolically represented music, to more expressive symbolic representations. While, perhaps, not as deep a problem as harmonic analysis, pitch spelling is the most obvious observable attribute of harmony — thus pitch spelling provides a means to quantify the accuracy of a harmonic analysis in objective terms. We introduce a model that uses the notion of conditionally independent voices. That is, we model the musical voices as conditionally independent sequences, while dec⃝2007 Austrian Computer Society (OCG). pending on a common collection of key variables. More explicitly, we model the inﬂuence of harmony by a hidden Markov chain of local keys, one for each measure. Given the keys, the evolution of each voice in each measure occurs independently of the others, but is dependent on the key sequence. The voices are also modeled as Markov Chains. Thus we allow for interaction between the voices while clearly articulating the way in which this interaction occurs. Our model assumes that there are two primary issues that explain pitch spellings: voice leading and local harmony. These two sources of information are articulated in the music theory text [8]. One principle therein suggests using accidentals to show the direction of chromatic passing tones, thus capturing the “yearning” ascribed to accidentals in informal discussions by musicians. (This same notion is also discussed by the Russian composer Nikolai Rimsky-Korsakov in [9].) Another principle from [8] advises avoiding “remote” accidentals; thus B♭is preferred to A♯in C major, since the former is in the scale of the F major, which is near to C. This principle is captured by the key conditional nature of our model, with its implicit notion of the likelihood of various pitches in different keys. Of course, there are times when these two principles come into conﬂict with each other, as in the spelling of chromatic scales. While notational conventions prescribe solutions here, and in other cases, our model can only explain the spelling in terms of local key and voice leading. The most obvious distinction between our approach and the others mentioned is our formulation in terms of a generative probabilistic model. Within this context, we believe that the merits of various pitch spellings can best be weighed within the context of a hidden key, so we explicitly model key and simultaneously estimate this key sequence along with spelling. Furthermore, we clearly articulate our objective as the globally most likely conﬁguration. All of the algorithms cited use some notion of “pitch closeness” in choosing spellings, as in Temperley’s line of ﬁfths and Chew’s spiral array, though, in our case, this notion of closeness is in terms of the hidden key variable. These algorithms differ in their incorporation of voice leading. Temperley and Sleater, Meredith, and, to some extent Longuet-Higgins use voice leading, while Cambouropoulos, and Chew and Chen do not. Our approach is computationally efﬁcient while capturing both notions of horizontal and, to some extent, vertical interaction between voices. Our model is automatically \u0000\u0001\u0000\u0000\u0001\u0000\u0002\u0001\u0002 \u0002\u0001\u0002 \u0003\u0001\u0003\u0001\u0003 \u0003\u0001\u0003\u0001\u0003 \u0004\u0001\u0004 \u0004\u0001\u0004 \u0005\u0001\u0005\u0001\u0005 \u0005\u0001\u0005\u0001\u0005 \u0006\u0001\u0006\u0001\u0006 \u0006\u0001\u0006\u0001\u0006 \u0007\u0001\u0007\u0001\u0007 \u0007\u0001\u0007\u0001\u0007 \b\u0001\b\u0001\b \b\u0001\b\u0001\b \u0001\t\u0001 \u0001\t\u0001",
        "zenodo_id": 1414946,
        "dblp_key": "conf/ismir/TeodoruR07"
    },
    {
        "title": "Ensemble Learning for Hybrid Music Recommendation.",
        "author": [
            "Marco Tiemann",
            "Steffen Pauws",
            "Fabio Vignoli"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417781",
        "url": "https://doi.org/10.5281/zenodo.1417781",
        "ee": "https://zenodo.org/records/1417781/files/TiemannPV07.pdf",
        "abstract": "We investigate ensemble learning methods for hybrid music recommenders, combining a social and a content-based recommender algorithm in an initial experiment by applying a simple combination rule to merge recommender results. A ﬁrst experiment suggests that such a combination can reduce the mean absolute prediction error compared to the used recommenders’ individual errors. 1 INTRODUCTION Music recommender systems that are publicly available today apply one of two recommender paradigms: social recommenders predict preferences based purely on user preference data. Content-based recommenders represent songs as feature vectors, where the features can be assigned manually or extracted automatically directly from audio data. Users are characterized by proﬁles, and songs that are similar to a user’s proﬁle are recommended. Hybrid recommenders combine recommender paradigms to improve the overall accuracy of predictions and reduce problems of speciﬁc recommender algorithms. To this end, hybrid recommenders usually combine social and content -based recommendation using one of a number of possible combination methods [1]. They can be categorized into recommenders that 1) integrate social and contentbased recommendation in a single algorithm or 2) combine the outputs of independent recommender algorithms. We follow the second approach. Our goal is to integrate many distinct independent recommenders (referred to as base recommenders in this paper) ﬂexibly in a generalized hybrid recommender, using ensemble learning methods to create diverse base recommenders and to combine their output with established and novel combination methods. 2 APPLYING ENSEMBLE LEARNING In ensemble learning [3], several weak learners are created and used in regression or classiﬁcation tasks. Their individual output is combined into a uniﬁed single output by applying a combination rule. Ensemble learning methods can often outperform single strong learners in practic⃝2007 Austrian Computer Society (OCG). cal applications. However, ensemble methods can only be applied successfully when the used weak learners are sufﬁciently diverse – weak learners must be able to correct errors made by other weak learners. Hence many ensemble learning methods actively select or create suitably diverse weak learners by techniques such as training data resampling. Some ensemble learning methods can also iteratively adapt to newly presented evidence by modifying either combination rules or weak learners as new evidence is successively presented. Ensemble learning is a promising concept for hybrid music recommendation. It can be used to ﬂexibly integrate heterogeneous data sources and different algorithms. Weak learner diversiﬁcation methods and combination rules with well explored properties are readily available in existing work on ensemble learning, and iterative algorithms can be used to optimize recommender accuracy as additional observations of user preferences becomes available. 3 A HYBRID MUSIC RECOMMENDER In this paper we focus on combining recommender output. We investigate to what extend commonly used social and content-based recommenders provide sufﬁciently diverse output to improve recommendations by combining them without applying diversiﬁcation methods. A frequently used social recommender method is item-based collaborative ﬁltering [4]. Using this method, ﬁrst the similarity between two items i and j is computed using preference data of all users u ∈U considered. The Pearson correlation coefﬁcient is frequently used to determine this similarity: s(i, j) = P u \u0000Ru,i −¯Ri \u0001 \u0000Ru,j −¯Rj \u0001 qP u \u0000Ru,i −¯Ri \u00012 P u \u0000Ru,j −¯Rj \u00012 (1) for all users u ∈U, where Ru,i, Ru,j are preferences of user u for items i and j respectively, and ¯Ri, ¯Rj are mean preferences for these items. Then, a predicted preference Pu,i for a target item i is computed as the weighted sum of preference values of user u for all items j ∈J that are correlated to item i and for which the user i’s preferences are known, scaled by the sum of similarity terms: Pu,i = P j s(i, j)Ru,j P j s(i, j) (2) A content-based recommender can be implemented similarly to this social recommender algorithm. We ﬁrst construct a similarity matrix of songs using a song similarity measure introduced in [5]. Analogous to the social recommendation algorithm described above, recommendations are computed as given in Formula 2 using the referenced content-based similarity function s(i, j) instead of the correlation of user preference data. A combination rule using decision templates [2] is applied to combine the base recommender output. A decision template is a record of weak learner estimates for an observed value. It can be expressed as a decision rule in the form {Pr1, ..., Prn} →R for a set of base recommenders {r1, ..., rn}, where Prn is the prediction of recommender n and R is the observed preference for a particular recommendation. For classiﬁcation, decision templates are created as averaged class probabilities over all instances classiﬁed into a particular class. This method has been shown to perform well for different classiﬁcation tasks [2]. For the regression task of predicting preference values, we apply a variant of this method. Decision template rules are retained without averaging them for each user. To combine base recommenders for a new instance, their individual predictions are computed. Then the Euclidean distance of the vector of computed results to each stored decision template rule head is computed, and the nearest neighbor rule’s tail (the observed preference) is the resulting preference prediction. 4 EXPERIMENT We conducted a preliminary experiment to evaluate the performance of the described hybrid recommender. For this a music dataset containing a collection of 63,949 popular music pieces was used. For each song manually assigned metadata are provided and perceptual audio features were extracted for the content-based recommender described in Section 3. The dataset contains 1,139,979 play counts, the number of times that a song has been played by a user, for 6,939 participants. Participants that played less than 100 distinct songs, played less than 50 songs more than once, or played no song at least 10 times were discarded for the experiment to ensure that sufﬁcient test and training data for cross-validation were available. After this procedure 735 participants remained. In order to avoid bias introduced by large play counts, these were capped at a value of 10. The evaluation task was to predict play counts for songs in the range [1,10] for each of the 735 users. For each user, 10-fold cross validation was used to split the available play counts into training and validation sets. The Mean Absolute Error (MAE) and the standard deviation σ of errors, averaged over all users u ∈U and scaled by the total count of evaluated instances n were computed for each recommender r. The MAE indicates the mean deviation of predicted from observed values and is the most frequently used recommender algorithm evaluation measure [4]. MAEr = 1 n X u X i ∥Ru,i −Pr,u,i∥ (3) In this initial experiment, the item-based social recommender reached a MAE of 2.608 (2.736) and σ 3.374, the content-based recommender reached a MAE of 2.884 (2.993) and σ 3.651, and the hybrid recommender a MAE of 2.349 1 and σ 2.817. We attribute the higher measured MAE for the social recommender compared to MAE values for other, ratings-based datasets (such as [4]) to the usage of implicit listening data and the high sparsity of the dataset. The presented hybrid recommender reduces the MAE by 0.259 compared to the best performing base recommender. 5 CONCLUSION AND FUTURE WORK We have introduced a hybrid recommender that uses a combination rule adapted from ensemble learning methods to combine base recommenders. In a preliminary experiment performed on observed listening data the technique leads to a MAE that is about 10% smaller than that of the best base recommender. This implies that a degree of diversity between social and content-based recommenders exists that can be exploited. Future work will focus on integrating more hetereogeneous data sources, applying diversiﬁcation techniques on them, developing speciﬁcally suited combination rules for hybrid music recommenders, and collecting explicit user ratings for a comparative evaluation of such approaches. 6 REFERENCES [1] R. Burke. ”Hybrid Recommender Systems: Survey and Experiments.” User Modeling and User-Adapted Interaction, Volume 12, Issue 4, 2002, 331-370. [2] L. Kuncheva, J. Bedzek and R. Duin. ”Decision Templates for Multiple Classiﬁer Fusion: an Experimental Comparison” Pattern Recognition, Volume 34, Issue 2, 2001, 299-314. [3] R. Polikar. ”Ensemble Based Systems in Decision Making.” IEEE Systems Magazine, Issue 3, 2006, 2145. [4] B. Sarwar, G. Karypis, J. Konstan and J. Riedl. ”Itembased Collaborative Filtering Recommendation Algorithms.” Proc. 10th International World Wide Web Conference, 2001. [5] F. Vignoli and S. Pauws. ”A Music Retrieval System Based on User-Driven Similarity and its Evaluation.” Proc. Sixth International Symposium on Music Information Retrieval, 2005. 1 MAE values for the base recommenders are rounded to the nearest possible play count to compensate for the inherent rounding effect of the decision template combination rule; original values are given in brackets.",
        "zenodo_id": 1417781,
        "dblp_key": "conf/ismir/TiemannPV07"
    },
    {
        "title": "Strike-A-Tune: Fuzzy Music Navigation Using a Drum Interface.",
        "author": [
            "Adam R. Tindale",
            "David W. Sprague",
            "George Tzanetakis"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418243",
        "url": "https://doi.org/10.5281/zenodo.1418243",
        "ee": "https://zenodo.org/records/1418243/files/TindaleST07.pdf",
        "abstract": "A traditional music library system controlled by a mouse and keyboard is precise, allowing users to select their desired song. Alternatively, randomized playlist or shufﬂes are used when users have no particular music in mind. We present a new interface and visualization system called StrikeA-Tune for fuzzy music navigation. Fuzzy navigation is an imprecise navigation approach allowing users to choose preference related items. We believe this will help users to play music they want to hear and re-discover infrequently played songs in their music library, thus combining the best aspects of precision navigation and shufﬂes. We have designed an interface using an electronic drum to communicate with a visualization and playback system.",
        "zenodo_id": 1418243,
        "dblp_key": "conf/ismir/TindaleST07"
    },
    {
        "title": "Identifying Words that are Musically Meaningful.",
        "author": [
            "David A. Torres",
            "Douglas Turnbull",
            "Luke Barrington",
            "Gert R. G. Lanckriet"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417175",
        "url": "https://doi.org/10.5281/zenodo.1417175",
        "ee": "https://zenodo.org/records/1417175/files/TorresTBL07.pdf",
        "abstract": "A musically meaningful vocabulary is one of the keystones in building a computer audition system that can model the semantics of audio content. If a word in the vocabulary is inconsistently used by human annotators, or the word is not clearly represented by the underlying acoustic representation, the word can be considered as noisy and should be removed from the vocabulary to denoise the modeling process. This paper proposes an approach to construct a vocabulary of predictive semantic concepts based on sparse canonical component analysis (sparse CCA) . Experimental results illustrate that, by identifying musically meaningful words, we can improve the performance of a previously proposed computer audition system for music annotation and retrieval. 1 INTRODUCTION Over the past two years we have been developing a computer audition system that can annotate songs with semantically meaningful words and retrieve relevant songs based on a text query. This system learns a joint probabilistic model between a vocabulary of words and acoustic feature vectors using a heterogeneous data set of song and song annotations. However, if a speciﬁc word is inconsistently used when annotating songs or is not represented well by the acoustic features, the system will not be able to model this noisy word well. In this paper, we explore the problem of vocabulary selection for semantic music annotation and retrieval. We will consider two concepts, human agreement and acoustic correlation, as indicators for picking candidate words. Previously, we collected semantic annotations of music using various methods: text-mining song reviews [15], conducting a human survey [16], and exploring the use of a human computation game [17, 19]. In all cases, we are forced to choose a vocabulary using ad-hoc methods. For example, text-mining the song reviews resulted in a list of over 1,000 candidate words which the authors manually pruned if there was a general consensus that a word was not ‘musically-relevant’. To collect the survey and game data, we built, a priori, a two-level hierarchical vocabulary by ﬁrst considering a set of high-level semantic c⃝2007 Austrian Computer Society (OCG). categories (‘Instrumentation’, ‘Emotion’, ‘Vocal Characteristic’, ‘Genre’) and then listing low-level words (‘Electric Guitar’, ‘Happy’, ‘Breathy’, ‘Bebop’) for each semantic category. In both cases, a vocabulary required manual construction and included some noisy words that degraded the performance of our computer audition system. In this paper, we highlight two potential reasons why a word causes problems for our system. The ﬁrst is related to the notion that aspects of music are subjective. That is, two individual listeners will use different words to describe the same piece of music. For example, a pre-teen girl might consider a Backstreet Boys song to be ‘touching and powerful’ whereas a dj at an indie radio station may consider it ‘abrasive and pathetic’. If we consider them as one population, the annotations will be in conﬂict with one another. To address this issue we introduce in Section 2 a measure of human agreement to evaluate how consistently our population uses a word to label a large set of songs. A second reason a word may be hard to model involves the expressive power of our chosen audio feature representation. For example, if we are interested in words related to long-term music structure (e.g., ‘12-bar blues’) and we only represent the audio using short-term ( < 1 sec) audio feature vectors, we may be unable to model such concepts. Another example is words that relate to a geographical association (e.g., ‘British Invasion’, ‘Woodstock’) which may have strong cultural roots, but are poorly represented in the audio content. Given an audio feature representation, we would like to identify the words that are represented well by the audio content before we try to model such words. To do this we propose the use of a method based on canonical correlation analysis (CCA) to measure acoustic correlation. CCA is a method of exploring dependencies across two different, but related, vector spaces and has been used in applications dealing with multi-language text analysis [18], learning a semantic representations between images and text [4], and localizing pixels which are correlated with audio from a video stream [6]. Similar to how principal component analysis (PCA) ﬁnds informative directions in one feature space by maximizing the variance of projected data, CCA ﬁnds directions (projections of the data) across multiple spaces that maximize correlation. Given music data represented in both a semantic feature space and an acoustic feature space, we propose that these directions of high correlation can be used to ﬁnd words that are strongly characterized by an audio representation. We do so by imposing constraints on CCA that explicitly turn it into a vocabulary selection mechanism. This CCA variant is called sparse CCA. 2 HUMAN AGREEMENT Recently, we collected the Computer Audition Lab 500 (CAL500) data set [16]: 500 songs by 500 unique artists each of which has been annotated according to a 173-word vocabulary by a minimum of three individuals. Most of the participants were paid, American, undergraduate students and the testing was conducted in a computer laboratory at UC San Diego. We purposely collected multiple annotations for songs so that we could gauge how consistently a population of college students label music. Using this data set, we can calculate a statistic we refer to as human agreement for each word in our vocabulary. The agreement of a word-song pair (w, s) is: Aw,s = #(positive associations)w,s #( annotations)s . (1) For example, if 3 out of 4 students label Elvis Presley’s ‘Heartbreak Hotel’ as being a ‘blues’ songs then A‘blues’, ‘heartbreak hotel’ = 0.75. We calculate the human agreement for a word by averaging over all the songs in which at least one subject has used the word to describe the song. This can be written as Aw = P s Aw,s P s I[Aw,s > 0] (2) where I is an indicator function that is 1 if Aw,s is greater then zero, and 0 otherwise. That is, all word-song pairs are valid except the word-song pair that nobody associates with one another. We expect human agreement to be close to 1 for more ‘objective’ words such as words associated with instrumentation (‘cow bell’), and close to 0 for words that are more ‘subjective’ such as those that related to song usages (‘driving music’). 3 ACOUSTIC CORRELATION WITH CCA Canonical Correlation Analysis, or CCA, is a method of exploring dependencies between data which are represented in two different, but related, vector spaces. For example, consider a set of songs where each song is represented by both a semantic annotation vector and an audio feature vector. An annotation vector for a song is a real-valued (or binary) vector where each element represents the strength of association (e.g., Equation 1) between the song and a word from our vocabulary. An audio feature vector is a real-valued vector of statistics calculated from the digital audio signal. It is assumed that the two spaces share some joint information which can be captured in the form of correlations between the music data that live in these spaces. CCA ﬁnds a one-dimensional projection of the data in each space such that the correlations between the projections is maximized. More formally, consider two data matrices, A and S, from two different feature spaces. The rows of A contain music data represented in the audio feature space A. The corresponding rows of S contain the music data represented in the semantic annotation space S (e.g., annotation vectors). CCA seeks to optimize max wa∈A,ws∈S w′ aA′Sws (3) s.t. w′ aA′Awa = 1 w′ sS′Sws = 1. The objective in Problem 3 is the dot product between projections of data points. By itself, the objective function is unbounded since we can scale the w terms arbitrarily. Thus, we add the constraints to bound the length of the w terms and ensure the result is proportional to a correlation score. By analyzing the Lagrangian dual function of Problem 3, we ﬁnd that it is equivalent to a pair of maximum eigenvalue problems, S−1 ss SsaS−1 aa Sasws = λ2ws (4) S−1 aa SasS−1 ss Ssawa = λ2wa (5) where \u0012 Saa Sas Ssa Sss \u0013 = \u0012 A′A A′S S′A S′S \u0013 and λ is the maximum of Problem 3. Note that the solution vector ws can be interpreted as a linear combination of words, learned from the music data, which are highly correlated with the audio representation. In the next section we modify Problem 3 so that a subset of words in our vocabulary is explicitly selected.",
        "zenodo_id": 1417175,
        "dblp_key": "conf/ismir/TorresTBL07"
    },
    {
        "title": "A Game-Based Approach for Collecting Semantic Annotations of Music.",
        "author": [
            "Douglas Turnbull",
            "Ruoran Liu",
            "Luke Barrington",
            "Gert R. G. Lanckriet"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416464",
        "url": "https://doi.org/10.5281/zenodo.1416464",
        "ee": "https://zenodo.org/records/1416464/files/TurnbullLBL07.pdf",
        "abstract": "Games based on human computation are a valuable tool for collecting semantic information about images. We show how to transfer this idea into the music domain in order to collect high-quality semantic information about songs. We present Listen Game, a online, multiplayer game that measures the semantic relationship between music and words. In the normal mode, a player sees a list of semantically related words (e.g., instruments, emotions, usages, genres) and is asked to pick the best and worst word to describe a song. In the freestyle mode, a user is asked to suggest a new word that describes the music. Each player receives realtime feedback about the agreement amongst all players. We show that we can use the data collected during a two-week pilot study of Listen Game to learn a supervised multiclass labeling (SML) model. We show that this SML model can annotate a novel song with meaningful words and retrieve relevant songs from a database of audio content. 1 INTRODUCTION Collecting high-quality, semantic annotations of music is a difﬁcult and time-consuming task. Previous methods have included hand-labeling music [3, 9], conducting surveys [14, 6, 8] and text-mining web documents [7, 15]. Each approach has drawbacks: human annotation methods are time consuming, costly, and as such, do not scale when attempting to annotate large music collections. Information mined automatically from web documents is often inconsistent with a true semantic description of the audio content. To collect large amounts of high quality annotation data at low cost, we propose using web-based games. von Ahn et. al. have created a suite of games (ESP Game [11], Peekaboom [13], Phetch [12]) for collecting semantic information about images. These ‘games with a purpose’ offer users an engaging platform for competition and collaboration while also collecting useful data about the image content. This data analysis technique is called human computation because it harnesses the collective intelligence of a large number of human participants to solve a c⃝2007 Austrian Computer Society (OCG). task that can not easily be automated. Using a game-based approach, a population of users can solve large problems (i.e., labeling all the images on the Internet) using voluntary contributions from individuals (i.e., playing a game to label a single image.) In this paper, we describe Listen Game, a multi-player, web-based game designed to collect associations between audio content and words. We show that this game is a powerful tool for collecting semantic music information by using the collected data to build a music information retrieval (MIR) application. In previous work [8], we presented a computer audition system that can automatically both annotate novel music with semantically meaningful words and retrieve relevant songs from a large database. Our system learns a supervised multi-class labeling (SML) model [1] by training on a set of audio content labeled with semantic annotations. We use the data collected from Listen Game to train our SML model. We then quantitatively evaluate the quality of the data by examining the accuracy of the SML model on the tasks of music annotation and retrieval. 2 COLLECTING MUSIC ANNOTATIONS A supervised learning approach to semantic music annotation and retrieval requires a large corpus of song-word associations. Early work in music classiﬁcation (by genre [9, 5], emotion [4], instrument [2]) either used music corpora hand-labeled by the authors or made use of existing song metadata. While hand-labeling generally results in high quality labels, it does not easily scale to hundreds of labels per song over thousands of songs. Companies such as Pandora [14] employ dozens of musical experts whose full-time job is to tag songs with a large vocabulary of musically relevant words but, unfortunately, have little incentive to make their data publicly available. In [15], Whitman and Ellis collect a large number of web-documents and summarize their content using textmining techniques. From web-documents associated with artists, they learned binary classiﬁers for musically relevant words by associating words in the documents with the artists’ songs. In previous work [7], we mined expert music reviews associated with songs and demonstrated that we could learn a supervised multi-class labeling (SML) model over a large vocabulary of words. While web-mining is a more scalable approach than handlabeling, we found that the data collected was of low quality since the extracted words did not necessarily provide a good description of a song. In general, when writing reviews of songs, albums or artists, authors do not make explicit decisions about the relevance of each single word. In addition, many reviews contain social, historical or opinionated information that is not related to the song’s audio content [15]. A third approach uses surveys to collect semantic information about music. Moodlogic [6] customers annotate music using a standard survey containing questions about genre, instrumentation, emotional characteristics, etc. We used a similar approach [8] to collect the CAL500 data set of 500 songs, each of which has been annotated using a vocabulary of 174 words by a minimum of three people. Data collection took over 200 person-hours and resulted in approximately 300,000 individual word-song associations. Using a survey produced higher quality annotations than the web data but required that we pay test subjects for their time. Furthermore, surveys are tedious and time consuming. Despite ﬁnancial motivation, test subjects quickly tire of lengthy surveys, resulting in inaccurate annotations. Human computation games motivate players to generate reliable annotations based on incentives built into the game. In the ESP Game [11] for example, a pair of unacquainted players are partnered up and each shown the same image. Both players are asked to “type what your partner is thinking”. Since they have no means of communicating, players invariably type words that have something to do with the common image they see. When two people independently suggest the same word to describe an image, the annotation is assumed to be reliable. Human computation games also address the issue of collecting lots of data by turning annotation into an entertaining task. The ESP Game has gathered over 10 million image annotations. Games build a sense of community and loyalty in users and can be highly addictive. Statistics from the ESP Game highlight that some people played in multiple 40 hour per week spans. Since they require little maintenance and run 24 hours a day, games can constantly collect new information from multiple players. Developing human computation games for annotating music is a useful approach for collecting semantic information. We believe that this approach has the potential for large-scale success because people enjoy talking about, sharing, discovering, arguing about and listening to music. 3 LISTEN GAME Image annotation often makes objective binary associations between an image and the objects (‘sailboat’), scene information (‘landscape’), and visual characteristics (‘red’) it represents. Our human computation game broaches the subjectivity inherent in many semantic labels that could be applied to music by allowing users to share their opinions, rather than be judged as correct or incorrect. Listen Game collects the strength of association between a word and a song, rather than an all-or-nothing binary label.",
        "zenodo_id": 1416464,
        "dblp_key": "conf/ismir/TurnbullLBL07"
    },
    {
        "title": "A Supervised Approach for Detecting Boundaries in Music Using Difference Features and Boosting.",
        "author": [
            "Douglas Turnbull",
            "Gert R. G. Lanckriet",
            "Elias Pampalk",
            "Masataka Goto"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415082",
        "url": "https://doi.org/10.5281/zenodo.1415082",
        "ee": "https://zenodo.org/records/1415082/files/TurnbullLPG07.pdf",
        "abstract": "A musical boundary is a transition between two musical segments such as a verse and a chorus. Our goal is to automatically detect musical boundaries using temporallylocal audio features. We develop a set of difference features that indicate when there are changes in perceptual aspects (e.g., timbre, harmony, melody, rhythm) of the music. We show that many individual difference features are useful for detecting boundaries. By combining these features and formulating the problem as a supervised learning problem, we can further improve performance. This is an alternative to previous work on music segmentation which has focused on unsupervised approaches based on notions of self-similarity computed over an entire song. We evaluate performance using a publicly available data set of 100 copyright-cleared pop/rock songs, each of which has been segmented by a human expert. 1 INTRODUCTION Most pop/rock songs have a standard structure: an introduction followed by alternating verses, choruses, and solos/bridges segments, and concluding with an outro. We deﬁne a musical boundary as the point in time where a song transitions between two of these segments. A boundary is often associated with one or more perceptual cues such as the introduction of a new instrument, a key change, or a drum ﬁll. Our goal is to ﬁrst design low-level features that encode information related to such cues, and second, use these features to develop a system that can automatically detect musical boundaries. Automatic boundary detection is useful for generating music thumbnails [10], efﬁcient music browsing [9], music information retrieval [16], and as a front-end for semantic music analysis systems [14, 11]. There are three main contributions of our work. First, we describe a set of local difference features each of which is a time series that is designed to indicate (e.g., ‘peak’) when some aspect of the music changes. Each feature is calculated by sliding a difference window across the audio signal and comparing the information extracted from the ﬁrst half of the window with the second half of the window. Each low-level feature is loosely related to a highc⃝2007 Austrian Computer Society (OCG). level musical concept such as timbre, harmony, melody, and rhythm. We individually evaluate each difference feature and report how well each feature can be used to detect musical boundaries. Second, we combine these features and propose a supervised learning approach based on the AdaBoost algorithm [3, 15, 5]. For each of our features, we create a large number of additional features by ﬁrst smoothing the feature and then calculating the instantaneous derivatives of the smoothed versions of the feature. We then simultaneously sample each feature in this enlarged set of features to create a high-dimensional (∼800 dimensions) vector of feature values for each sample 1 . The samples that correspond to musical boundaries are labeled as belonging to the ‘boundary’ class while all others are labeled as belonging to the ‘non-boundary’ class. We learn a boosted decision stump (BDS) classiﬁer using training data and report performance on test data. Viola and Jones [15] pioneered this technique and Dollar et al. [3] applied this to image boundary detection. Third, we propose two evaluation metrics, Median Time Difference and Boundary Hit Rate, in order to quantitatively evaluate musical boundary detection. We quantitatively evaluate both individual difference features and BDS classiﬁers using the RWC music database of 100 pop songs [6]. This data set now includes human annotated segmentations and can serve as a common test bed for future research [8]. Although we are unaware of previous work that explicitly addresses musical boundary detection, there has been a signiﬁcant amount of work on music segmentation [9, 13, 10, 1, 11, 2, 4]. These two closely related problems are different in that musical boundary detection involves ﬁnding musical boundaries from temporally-local features whereas segmentation involves ﬁnding coherent segments within a song using notions of self-similarity (e.g. ﬁnding repetitive structures [9, 2, 4] or identifying similar spectral characteristics [13, 10, 1]). Our features are designed to indicate dissimilarity in the audio signal. In addition, our supervised approach has the beneﬁt of allowing the user to explicitly specify their deﬁnition of a ‘boundary’ through the labels that they provide for the training data. 1 Throughout this paper, we will consider a sample to be a ‘feature sample’ rather than a ‘audio sample’ that is calculated 10 per second. 2 MUSICAL BOUNDARY FEATURES In this section, we describe various features that are useful for detecting musical boundaries. We loosely relate each to a high-level music concept for clarity, though these features are largely derived from a low-level spectral representation (e.g., short-time Fourier transforms) and often can be related to more than one high-level concept. Each feature is time series that is sampled at a given feature sampling rate (e.g., 10 samples/sec). We create the time series by sliding a window (5-10 seconds in length) across the audio signal with a hopsize equal to the feature sampling rate. The audio signal in the window is summarized with a statistic. The statistic may be a scalar, a vector, a matrix, a set of scalars/vectors, or a time series of scalars/vectors. A difference feature is time series of scalar values that is derived by sliding a window over an audio signal. A difference feature is computed by comparing the statistic calculated in the ﬁrst half of the window with the statistic calculated in the second half of the window. When our statistic is a scalar, vector, or matrix, we will compute the Euclidean norm of the difference. When our statistic is a set or times series, we ﬁrst estimate one Gaussian distribution (with full covariance) from values in the ﬁrst half of the window and a second Gaussian distribution from the values in the second half of the window. We then compute the symmetric Kullback-Leibler (sKL) divergence between the two Gaussian distributions (See Section 2.2.3.4 of [12] for details). The sKL is a nonnegative scalar value that is large when the two estimated distributions greatly differ. One should note that by estimating Gaussians, we discards all temporal information when our statistic is a time series. We normalize each feature so that, over an entire song, the mean of the samples is equal to 0 and variance of the samples equal to 1. Normalization is needed for generalization of features across the different songs in our corpus. In addition, we ﬁnd empirically that ‘smoothing’ a feature by passing a Gaussian window over the time series improves performance.",
        "zenodo_id": 1415082,
        "dblp_key": "conf/ismir/TurnbullLPG07"
    },
    {
        "title": "Stereo Panning Features for Classifying Recording Production Style.",
        "author": [
            "George Tzanetakis",
            "Randy Jones",
            "Kirk McNally"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417537",
        "url": "https://doi.org/10.5281/zenodo.1417537",
        "ee": "https://zenodo.org/records/1417537/files/TzanetakisJM07.pdf",
        "abstract": "Recording engineers, mixers and producers play important yet often overlooked roles in deﬁning the sound of a particular record, artist or group. The placement of different sound sources in space using stereo panning information is an important component of the production process. Audio classiﬁcation systems typically convert stereo signals to mono and to the best of our knowledge have not utilized information related to stereo panning. In this paper we propose a set of audio features that can be used to capture stereo information. These features are shown to provide statistically important information for non-trivial audio classiﬁcation tasks and are compared with the traditional Mel-Frequency Cepstral Coefﬁcients. The proposed features can be viewed as a ﬁrst attempt to capture extra-musical information related to the production process through music information retrieval techniques. 1 INTRODUCTION Starting in the 1960s the recording process for rock and popular music moved beyond the convention of recreating as faithfully as possible the illusion of a live performance. Facilitated by technological advances including multi-track recording, tape editing, equalization and compression, the creative contributions of record producers became increasingly important in deﬁning the sound of artists, groups, and styles [4]. Although not as well known as the artists they worked with, legendary producers including Phil Spector, George Martin, Quincy Jones, and Brian Eno changed the way music was created. So far, research in music information retrieval has largely ignored information about the recording process, focusing instead on capturing information about pitch, rhythm and timbre. A common methodology is to extract features, quantiﬁable attributes of music signals, from recordings, then to classify these features into distinct groups using machine learning techniques. This two-part process has enabled tasks such as automatic identiﬁcation of genres, albums and artists. The inﬂuence of the recording process on automatic classiﬁcation has been acknowledged and termed the alc⃝2007 Austrian Computer Society (OCG). bum effect. The performance of artist identiﬁcation systems degrades when music from different albums is used for training and evaluation [7]. Therefore, the classiﬁcation results of such systems are not based entirely on the musical content. Various stages of production of the recorded artifact, including recording, mixing, and mastering, all have the potential to inﬂuence classiﬁcation. This has led to research which attempts to quantify the effects of production on acoustic features. By detecting equalization curves used in album mastering, it is possible to compensate for the effects of mastering so that multiple instances of the same song on different albums can be better compared [3]. We believe that other information related to the recording process, speciﬁcally mixing, is an important component of understanding modern pop and rock music and should be incorporated rather than being removed from music information retrieval systems. Our goal is to explore stereo panning information as an aspect of the recording and production process. Stereo information has been utilized for source separation purposes [2, 9]. However, to the best of our knowledge, it has not been used in classiﬁcation systems for audio signals. In this paper we show that stereo panning information is indeed useful for automatic music classiﬁcation. 2 STEREO PANNING INFORMATION EXTRACTION In this section we describe the process of calculating stereo panning information for different frequencies based on the short-time Fourier transform (STFT) of the left and right channels. Using the extracted Stereo Panning Spectrum we propose features for classiﬁcation.",
        "zenodo_id": 1417537,
        "dblp_key": "conf/ismir/TzanetakisJM07"
    },
    {
        "title": "Assessment of State-of-the-Art Meter Analysis Systems with an Extended Meter Description Model.",
        "author": [
            "Matthias Varewyck",
            "Jean-Pierre Martens"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417367",
        "url": "https://doi.org/10.5281/zenodo.1417367",
        "ee": "https://zenodo.org/records/1417367/files/VarewyckM07.pdf",
        "abstract": "An extended meter description model capturing the hierarchical metrical structure of Western music is proposed. The model is applied for the quantitative evaluation of four state-of-the-art automatic meter analysis algorithms of musical audio. Evaluation results suggest that the best beat trackers reach a reasonable level of performance, but that none of the tested algorithms has the potential to perform a reliable bar onset tracking. Moreover, the frontends of the best over-all systems not necessarily seem to have the front-ends best encoding the time signature in their output. Therefore, further improvements of these systems should be attainable by a better combination of ideas that can be borrowed from existing algorithms. 1 INTRODUCTION The temporal characteristics of music reside in an ensemble of perceivable periodic patterns at different time scales. These can be captured in a hierarchical metrical structure, called the meter [6]. The automatic extraction of metrical characteristics in musical audio is of direct importance to applications such as intelligent synchronization, standard editing, semi-automatic mixing and synchronizing audio effects but also higher-order applications such as chord recognition, structure detection and genre classiﬁcation. Western music usually exhibits a prominent periodicity, called the beat. Many meter analysis algorithms try to track these beats or to determine the corresponding tempo (e.g. [1, 2, 3]). However, tapping experiments have demonstrated [9] that the beat level is a subjective concept. Consequently, focusing too much on the beat level, may not be such a good idea. Performing an analysis involving multiple levels therefore seems a better approach. Although a number of studies describe meter analysis on symbolic (e.g. MIDI, score) data, those based on musical audio remain rather limited. In this study, we focus on the latter. Goto & Muraoka [5, 11] considered a binary meter model with a bar, beat and intermediate level. Obviously, this model only works well if the Inter Timestamp Interval (ITI) between successive timestamps on one level is equal to two times the ITI on the lower level. Klapuri et al. [6] proposed a meter analysis involving three other levels: the c⃝2007 Austrian Computer Society (OCG). bar, beat and tatum level, with ITI ratios of one to nine between subsequent levels. Klapuri’s method can be applied to music with non-binary meters, but it may produce ambiguous irregular time signatures. In this paper, we propose an extended meter description model offering a more complete representation of the time signature of a polyphonic audio excerpt, hereafter called a song. The generality of the model was ﬁrst inspected by creating meter annotated data. Now, these annotations are used for the quantitative assessment of four state-of-theart automatic meter analysis systems. The meter description model is introduced in section",
        "zenodo_id": 1417367,
        "dblp_key": "conf/ismir/VarewyckM07"
    },
    {
        "title": "Applying Rhythmic Similarity Based on Inner Metric Analysis to Folksong Research.",
        "author": [
            "Anja Volk",
            "Jörg Garbers",
            "Peter van Kranenburg",
            "Frans Wiering",
            "Remco C. Veltkamp",
            "Louis P. Grijp"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416830",
        "url": "https://doi.org/10.5281/zenodo.1416830",
        "ee": "https://zenodo.org/records/1416830/files/VolkGKWVG07.pdf",
        "abstract": "In this paper we investigate the role of rhythmic similarity as part of melodic similarity in the context of Folksong research. We deﬁne a rhythmic similarity measure based on Inner Metric Analysis and apply it to groups of similar melodies. The comparison with a similarity measure of the SIMILE software shows that the two models agree on the number of melodies that are considered very similar, but disagree on the less similar melodies. In general, we achieve good results with the retrieval of melodies using rhythmic information, which demonstrates that rhythmic similarity is an important factor to consider in melodic similarity. 1 INTRODUCTION In this paper we study rhythmic similarity in the context of melodic similarity as a ﬁrst step within the interdisciplinary enterprise of the WITCHCRAFT 1 project (Utrecht University and Meertens Institute Amsterdam). The project aims at the development of a content based retrieval system for a large collection of Dutch folksongs that are stored as audio and notation. The retrieval system will give access to the collection Onder de groene linde hosted by the Meertens Institute to both the general public and musical scholars. The collection Onder de groene linde (short: OGL) consists of songs transmitted through oral tradition, hence it contains many variants for one song. In order to describe these variants the Meertens Institute has developed the concept of melody norm 2 which groups historically or ‘genetically’ related melodies into one norm (for more details see [4]). The retrieval system to be designed should assist in deﬁning melody norms for the collection OGL based on the similarity of the melodies in order to support the study of oral transmission. In a ﬁrst step similar melodies from a given test corpus have been manually classiﬁed into groups. These melody groups serve as pos1 What is Topical in Cultural Heritage: Content-based Retrieval Among Folksong Tunes 2 similar to “tune family” and “Melodietyp” c⃝2007 Austrian Computer Society (OCG). sible candidates for the melody norms to be assigned in a later stage. According to cognitive studies, metric and rhythmic structures play a central role in the perception of melodic similarity. For instance, in the immediate recall of a simple melody studied in [8] the metrical structure was the most accurately remembered structural feature. In this paper we demonstrate that melodies belonging to the same melody group can successfully be retrieved based on rhythmic similarity. Therefore we conclude that rhythmic similarity is a useful characteristic for the classiﬁcation of folksongs. Furthermore, our results show the importance of rhythmic stability within the oral transmission of melodies, which conﬁrms the impact of rhythmic similarity on melodic similarity suggested by cognitive studies. 2 DEFINING A MEASURE FOR SYMBOLIC RHYTHMIC SIMILARITY This section introduces our rhythmic similarity measure that is based on Inner Metric Analysis (IMA).",
        "zenodo_id": 1416830,
        "dblp_key": "conf/ismir/VolkGKWVG07"
    },
    {
        "title": "Keyword Generation for Lyrics.",
        "author": [
            "Bin Wei",
            "Chengliang Zhang",
            "Mitsunori Ogihara"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1418369",
        "url": "https://doi.org/10.5281/zenodo.1418369",
        "ee": "https://zenodo.org/records/1418369/files/WeiZO07.pdf",
        "abstract": "This paper proposes a scheme for content based keyword generation of song lyrics. Syntactic as well semantic similarity is used for sentence level clustering to separate the topic from the background of a song. A method is proposed to search for a center in the semantic graph of WordNet for generating keywords not contained in original text. 1 INTRODUCTION The growth of Web lyrics databases such as lyrics.com and absolutelyric.com raises the issue of processing and understanding lyrics automatically. However, previous work (see [4]) suggests that lyrics are tough for natural language processing. There are three issues that are speciﬁc to keyword extraction, First, the “correct” outcome is usually subtle. Unlike other text data such as news, topic words in lyrics have a very small number of occurrences. Instead, a large portion will be devoted to the background. This makes the frequency information not so useful, or even misleading. Second, because lyrics are free-style, approaches based on word positions also face difﬁculty. Third, for some lyrics, meaningful keywords may not even be included in the original text. Some sort of induction is needed to ﬁnd suitable keywords. To handle the ﬁrst two issues, we propose to use a sentence-level clustering so as to separate the topic from the background and eliminate the position information irrelevant. To handle the third issues, we propose to uses various relation links of WordNet 1 , assuming that the center of the discovered links serves as the central keyword. Keywords generated for each intra-lyric cluster are compared among lyrics, enabling separation of lyrics on similar topics with different backgrounds. Our use of WordNet is more comprehensive than the existing WordNet lexical-cohesion-based approach of (Silber and McCoy [5]) called Lexical Chain, in that we not only focus on nouns and concept hierarchy but try to combine verbs and adjectives and logical relations in the analysis. The sentence-level clustering enables us to constrain our search in a compact subgraph of WordNet rather than the whole net. Also, our analysis is different in that the goal is not only to ﬁnd the relations among words in 1 http://WordNet.princeton.edu c⃝2007 Austrian Computer Society (OCG). the lyrics but to introduce new words as the suitable keywords that may not necessarily occur in the original text. 2 KEYWORD GENERATION We use the Stanford parser for obtaining dependencies, where a dependency is a triple of the form (relation, governor, dependent). We then use the Lesk measurement for word similarity inthe WordNet, and the algorithm in [2] for word sense disambiguation.",
        "zenodo_id": 1418369,
        "dblp_key": "conf/ismir/WeiZO07"
    },
    {
        "title": "An Experiment on the Role of Pitch Intervals in Melodic Segmentation.",
        "author": [
            "Tillman Weyde",
            "Jens Wissmann",
            "Kerstin Neubarth"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417591",
        "url": "https://doi.org/10.5281/zenodo.1417591",
        "ee": "https://zenodo.org/records/1417591/files/WeydeWN07.pdf",
        "abstract": "This paper presents the results of an experiment to test the inﬂuence of IOI, dynamics, pitch change, and pitch direction change on melodic segmentation, extending an an earlier experiment [4]. The new results show little to no signiﬁcant inﬂuence of pitch, when evaluated by a linear or log-linear statistical model with regression. This supports the earlier ﬁndings, which are in contrast to the commonly made assumption that greater pitch intervals lead to melodic segmentation. 1 INTRODUCTION The segmentation of melodies plays an important role in melody perception, cognition, and retrieval (e.g. [3], [5]) and applications in music e-learning, like automated generation of exercises from a given annotated music score. A common assumption is that the Gestalt Principles of proximity and similarity can be applied to the different musical dimensions of pitch, time, dynamics, and to derived quantities such as the change of direction in successive pitch intervals (see [2], [1]). This assumption has been tested empirically in [4] for pitch intervals up to 5 semitones, where neither pitch intervals nor changes of pitch interval direction were signiﬁcant in linear or log-linear regression models. There was however the question whether there could be a signiﬁcant effect for larger pitch intervals. 2 EXPERIMENTAL DESIGN The experiment uses mostly the same design as [4], which is therefore described here only brieﬂy: The experiment uses a forced-choice design, where subjects listen to a melody and are asked whether the length of segments in the melody is 2 or 3 notes. Subjects were presented short melodic sequences, which were designed to be completely isochronous and uniform except for two conﬂicting segmentation cues, of which one indicated a segmentation into groups of two notes and the other into groups of three. The intensity of the cues was varied. This approach was chosen to approximate the situation of actual melodies, where there is normally more than one cue present. Pairs of cue types were used, as all combinations of values for three or more factors would have led to huge numbers of stimuli. c⃝2007 Austrian Computer Society (OCG). Four cues were tested: inter-onset-intervals, loudness accents, pitch intervals, and changes in pitch direction. Each of the cues was varied in several steps. The following values were used: • additional inter-onset-interval values of 30, 60, 90, 120, and 150 ms • loudness accents of 15, 30, 45, 60, and 75 MIDI velocity units • pitch intervals between notes of size 2, 4, 6, 7, 8, 9, 10, 11, and 12 semitones, alternating up and down • changing direction, with pitch intervals of 1, 2, 3, and 4 semitones between every pair of successive notes Each of these was used to cue segments of both two and three notes length. For the smaller pitch intervals not all values were used, as they had already been completely tested in [4]. In addition, a set of examples with only one factor used in segmentation cues was created for each factor and two melodies without segmentation cues, i.e. completely uniform and isochronous sequences. Some additional parameters were varied at random in this experiment: the initial pitch and loudness, the assignment of factors to group lengths, and the total length of the melody. They were also used as independent variables in the regression analyses, to ﬁnd out if and how they inﬂuence the segmentation. The experiments were conducted in one session where each subject listened to all stimuli. The stimuli were presented via MIDI with a piano-like sound on a personal computer with a program that asked to choose of either ’2’ or ’3’ as preferred segment length. The stimuli were presented in random order with a short break of randomised length between the presentations. The subjects were ten music students between 20 and 23 years of age, ﬁve male and ﬁve female. 3 RESULTS In the following selected results and regression analyses of the experiments are presented.",
        "zenodo_id": 1417591,
        "dblp_key": "conf/ismir/WeydeWN07"
    },
    {
        "title": "Synthesized Polyphonic Music Database with Verifiable Ground Truth for Multiple F0 Estimation.",
        "author": [
            "Chunghsin Yeh",
            "Niels Bogaards",
            "Axel Röbel"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1415732",
        "url": "https://doi.org/10.5281/zenodo.1415732",
        "ee": "https://zenodo.org/records/1415732/files/YehBR07.pdf",
        "abstract": "To study and to evaluate a multiple F0 estimation algorithm, a polyphonic database with veriﬁable ground truth is necessary. Real recordings with manual annotation as ground truth are often used for evaluation. However, ambiguities arise during manual annotation, which are often set up by subjective judgements. Therefore, in order to have access to veriﬁable ground truth, we propose a systematic method for creating a polyphonic music database. Multiple monophonic tracks are rendered from a given MIDI ﬁle, in which rendered samples are separated to prevent overlaps and to facilitate automatic annotation. F0s can then be reliably extracted as ground truth, which are stored using SDIF. 1 INTRODUCTION F0 (fundamental frequency) is an essential descriptor for periodic signals such as speech and music. Multiple F0 estimation aims at extracting the fundamental frequencies of concurrent sources. In the ﬁeld of MIR (Music Information Retrieval), the multiple F0s serve as low-level acoustic features for building up high-level representation of music notes. In recent years, quite a few multiple F0 estimation algorithms have been developed for music signals but no common database (corpus + ground truth) for evaluation exists. In order to study and to evaluate a multiple F0 estimation algorithm, a polyphonic music database with reliable ground truth is necessary. There are mainly three types of polyphonic signals used as evaluation corpus: mixtures of monophonic samples, synthesized polyphonic music and real recordings. Mixtures of monophonic samples allow a diversity of combinations among different notes and instruments [1]. The ground truth is extracted by means of single F0 estimation, which can be veriﬁed more easily. The concern, however, is that the ﬁnal mixtures may not have the same statistical properties as those found in real music. To increase the relevance of the test corpus for real world applications, the corpus should take into account musical structures. Synthesized polyphonic music can be rendered from MIDI [2] ﬁles by sequencers with sound modules c⃝2007 Austrian Computer Society (OCG). or samplers. Real recordings can be recordings of multitracks or stereo/mono mix-down tracks. Despite the wide availability of music corpora, establishing their ground truth remains an issue. We propose a systematic method to synthesize polyphonic music that allows to use existing single F0 estimation algorithms to establish the ground truth. In addition, the availability and the interchangeability of ground truth data together with the corpus is a main concern for evaluation. Therefore, we plan to distribute this polyphonic database with ground truth available in the SDIF (Sound Description Interchange Format) format. This paper is organized as follows. In section 2, we present innovative tools developed within IRCAM’s AudioSculpt application [3] for manual annotation or score alignment and discuss the issues. In section 3, we present a systematic method for creating a synthesized polyphonic music database. The method is reproducibleand the ground truth is veriﬁable. The format in which to store the ground truth is described in section 4. Finally, we discuss the evaluation concerns and the possibility of extending this method for different MIR evaluation tasks. 2 MANUAL ANNOTATION OF REAL RECORDINGS Nowadays, more and more evaluations use real recordings of mix-down tracks with manually annotated ground truth [4] [5]. The annotation process usually starts with a reference MIDI ﬁle and then aligns the note onsets and offsets to the observed spectrogram. Under the assumption that the notes in the reference MIDI ﬁle correspond exactly to what have been played in the real performance, the annotation process is in fact “score alignment” with audio signals. At IRCAM, innovative tools in AudioSculpt (see Figure 1) have been developed to facilitate veriﬁcation and modiﬁcation of signal analysis and manual annotation. Given a reference MIDI ﬁle and a real recording of the same musical piece, we ﬁrst align the MIDI notes to the real recording automatically [6]. Then, details like note offs, slow attacks, etc., are manually corrected using AudioSculpt according to the following procedure:",
        "zenodo_id": 1415732,
        "dblp_key": "conf/ismir/YehBR07"
    },
    {
        "title": "Improving Efficiency and Scalability of Model-Based Music Recommender System Based on Incremental Training.",
        "author": [
            "Kazuyoshi Yoshii",
            "Masataka Goto",
            "Kazunori Komatani",
            "Tetsuya Ogata",
            "Hiroshi G. Okuno"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1416880",
        "url": "https://doi.org/10.5281/zenodo.1416880",
        "ee": "https://zenodo.org/records/1416880/files/YoshiiGKOO07.pdf",
        "abstract": "We aimed at improving the efﬁciency and scalability of a hybrid music recommender system based on a probabilistic generative model that integrates both collaborative data (rating scores provided by users) and content-based data (acoustic features of musical pieces). Although the hybrid system was proved to make accurate recommendations, it lacks efﬁciency and scalability. In other words, the entire model needs to be re-trained from scratch whenever a new score, user, or piece is added. Furthermore, the system cannot deal with practical numbers of users and pieces on an enterprise scale. To improve efﬁciency, we propose an incremental method that partially updates the model at low computational cost. To enhance scalability, we propose a method that ﬁrst constructs a small “core” model over fewer virtual representatives created from real users and pieces, and then adds the real users and pieces to the core model by using the incremental method. The experimental results revealed that the proposed system was not only efﬁcient and scalable but also outperformed the original system in terms of accuracy. 1 INTRODUCTION Music recommender systems play important roles in current e-commerce to help users discover their favorites in huge databases [1]. For example, many e-commerce sites (e.g., Last.fm and Amazon.com [2]) use collaborative ﬁltering techniques to recommend musical pieces to the user by examining how someone else has rated them. Although these techniques have been considered to be effective, they suffer from the famous “new item” problem. That is, nonrated pieces cannot be recommended. In addition, the variety of recommendations tends to be poor because most users mainly rate musical pieces by a small number of popular artists. This indicates that there still remains much room for enhancing the Long-Tail effect [3]. To overcome these limitations, content-based ﬁltering techniques, which recommend musical pieces similar to user’s favorites in terms of musical content, are attracting attention of many researchers. However, a major approach based on automatic content analysis for musical c⃝2007 Austrian Computer Society (OCG). audio signals [4, 5] has not fully gained the popularity of end users. In contrast, Pandora, which depends on manual content annotation for commercial titles, is a well-known successful radio station on the Internet. This indicates that we should take into account important factors, such as cultural backgrounds and popularity on the market, that contribute to making reasonable recommendations but cannot be obtained from audio signals. However, this annotationbased approach lacks portability because it is not practical to manually annotate all compositions by amateurs in social networking services (e.g., MySpace.com). To make reasonable recommendations under any conditions, it is necessary to take a ﬂexible hybrid approach that can integrate various types of available data. This improves robustness against inappropriate data in real world (e.g., malicious rating scores, erroneous automatic annotations, and inconsistent manual annotations). We therefore developed a hybrid recommender system using a probabilistic model that integrates both collaborative and content-based data in a theoretical way [6]. Although our system overcame the shortcomings of conventional techniques, critical problems in efﬁciency and scalability emerged when we tried to apply our system to ecommerce where several millions of users and pieces are managed. The system can neither promptly adapt recommendations to each user according to changes in his or her rating scores nor incrementally register new users and pieces. This is because the model should always be trained from scratch, where the time for training is proportional to both numbers of users and pieces. To improve efﬁciency, we propose an online method that updates only partial parameters of the model related to changes in the observed data. This enables the system to incrementally incorporate these changes into the model. To improve scalability, we propose a method that builds a small “core” model over ﬁxed numbers of virtual representatives created from large numbers of real users and pieces. The core model is then updated while incrementally registering real users and pieces. The rest of this paper is organized as follows. Section 2 reviews our recommender system. Section 3 explains the proposed methods. Section 4 reports on the experiments. Section 5 summarizes the key ﬁndings of this study. 2 HYBRID MUSIC RECOMMENDER SYSTEM We will ﬁrst deﬁne a recommendation task and then explain the original version of our recommender system [6].",
        "zenodo_id": 1416880,
        "dblp_key": "conf/ismir/YoshiiGKOO07"
    },
    {
        "title": "Polyphonic Music Note Onset Detection Using Semi-Supervised Learning.",
        "author": [
            "Wei You",
            "Roger B. Dannenberg"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1417385",
        "url": "https://doi.org/10.5281/zenodo.1417385",
        "ee": "https://zenodo.org/records/1417385/files/YouD07.pdf",
        "abstract": "Automatic note onset detection is particularly difﬁcult in orchestral music (and polyphonic music in general). Machine learning offers one promising approach, but it is limited by the availability of labeled training data. Score-toaudio alignment, however, offers an economical way to locate onsets in recorded audio, and score data is freely available for many orchestral works in the form of standard MIDI ﬁles. Thus, large amounts of training data can be generated quickly, but it is limited by the accuracy of the alignment, which in turn is ultimately related to the problem of onset detection. Semi-supervised or bootstrapping techniques can be used to iteratively reﬁne both onset detection functions and the data used to train the functions. We show that this approach can be used to improve and adapt a general purpose onset detection algorithm for use with orchestral music. 1 INTRODUCTION Finding the beginning of notes, or note onsets, in music audio is a problem that is widely studied. By ﬁnding note onsets, we can segment continuous music into discrete note events, beneﬁting tempo estimation, beat ﬁnding, automatic music transcription, and other analysis tasks. These in turn are often used as components in systems for music indexing and retrieval, music ﬁngerprinting, and music similarity. Thus onset detection is a fundamental task for music information retrieval. However, because of the variability within and between musical instruments, ﬁnding note onsets is not trivial. In polyphonic pieces, note onsets may be difﬁcult to separate from other notes, and in large ensembles such as an orchestra, masses of note onsets can be difﬁcult to handle. Our focus is on massively polyphonic music, e.g. orchestra music. One reason previous work has focused on monophonic and polyphonic piano music is the availability of test data. Hand labeling music onsets is tedious work, and it would be very expensive to label all note onsets in large polyphonic works. For example, Beethoven’s Symphony no. 5 in C minor, ﬁrst movement, has more than 10,000 notes and over 2,000 separate onset times over a duration of about 435s. If we assume labeling one note c⃝2007 Austrian Computer Society (OCG). takes 1 minute (quite optimistic in our experience), then more than 30 hours will be needed to label one movement. To solve this dilemma, audio-to-score alignment is used to estimate note onsets automatically. Typically, score alignment is performed with chroma features, which summarize 50 to 250ms windows of audio, but this does not provide the high time resolution one would like for onset labeling. For example, it is not unusual to see an average inter-onset time of 200ms in a polyphonic work. Therefore, we use a semi-supervised learning method to enhance the performance of the audio-to-score alignment, leading to improved onset detector training [6]. Thus, the task of acquiring training data is simply to locate corresponding audio and MIDI ﬁles and run a relatively fast alignment algorithm. Vastly increased numbers of training examples improve functions for onset detection. We compare the results of our trained onset detector to a high-quality, open-source onset detector, Aubio (http:// aubio.piem.org). To measure only improvements due to semi-supervised training, we used exactly the same features as Aubio, and we also used the Aubio peak-picking algorithm for our system. We also tried adding new features, hoping that machine learning would be able to take advantage of the additional information. After the related work section that follows, we explain the techniques used by our onset detection system. Section 4 presents an evaluation. Our ﬁndings are discussed in Section 5, which is followed by a concluding section. 2 RELATED WORK Current practice in note onset detection can be separated into two general approaches: 1. Apply a detection function, often based on change in spectrum and overall power; then use a temporal peak-picking algorithm to ﬁnd local maxima in output from the detection function [2, 4, 1, 3].",
        "zenodo_id": 1417385,
        "dblp_key": "conf/ismir/YouD07"
    },
    {
        "title": "Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007, Vienna, Austria, September 23-27, 2007",
        "author": [
            "Simon Dixon",
            "David Bainbridge 0001",
            "Rainer Typke"
        ],
        "year": "2007",
        "doi": "10.5281/zenodo.1284501",
        "url": "https://doi.org/10.5281/zenodo.1284501",
        "ee": null,
        "abstract": "<p>This release contains the annotations and the scores to test the audio-score alignment methodology explained in:</p>\n\n<blockquote>\n<p><em>Şent&uuml;rk, S., Gulati, S., and Serra, X. (2014). <strong>Towards alignment of score and audio recordings of Ottoman-Turkish makam music.</strong> In Proceedings of 4th International Workshop on Folk Music Analysis, pages 57&ndash;60, Istanbul, Turkey.</em></p>\n</blockquote>\n\n<p>The dataset in this release is derived from the transcription test dataset used in the paper:</p>\n\n<blockquote>\n<p><em>Benetos, E. &amp; Holzapfel, A. (2013). <strong>Automatic transcription of Turkish makam music.</strong> In Proceedings of 14th International Society for Music Information Retrieval Conference, 4 8 Nov 2013, Curitiba, PR, Brazil.</em></p>\n</blockquote>\n\n<p>The scores for each composition are obtained from the SymbTr collection explained in:</p>\n\n<blockquote>\n<p><em>Karaosmanoğlu, K. (2012). <strong>A Turkish makam music symbolic database for music information retrieval: SymbTr.</strong> In Proceedings of 13th International Society for Music Information Retrieval Conference (ISMIR), pages 223&ndash;228.</em></p>\n</blockquote>\n\n<p>From the&nbsp; annotated score onsets for some of the above recordings only the main singing voice segments have been selected. Further separately only a subset of vocal onsets crresponding to phoneme transitions rules have been explicitly annotated as annotationOnsets.txt</p>\n\n<blockquote>\n<p><a href=\"http://mtg.upf.edu/biblio/author/810\">Dzhambazov, G.</a>, <a href=\"http://mtg.upf.edu/biblio/author/644\">Srinivasamurthy A.</a>, <a href=\"http://mtg.upf.edu/biblio/author/494\">Şent&uuml;rk S.</a>, &amp; <a href=\"http://mtg.upf.edu/biblio/author/1012\">Serra X.</a> (2016).&nbsp;&nbsp;<a href=\"http://mtg.upf.edu/node/3492\">On the Use of Note Onsets for Improved Lyrics-to-audio Alignment in Turkish Makam Music</a>. 17th International Society for Music Information Retrieval Conference (ISMIR 2016</p>\n</blockquote>\n\n<p><strong>Using this dataset</strong></p>\n\n<p>Please cite the above publications if you use this dataset in a publication.</p>\n\n<p>We are interested in knowing if you find our datasets useful! If you use our dataset please email us at <a href=\"mailto:mtg-info@upf.edu\">mtg-info@upf.edu</a> and tell us about your research.</p>\n\n<p>&nbsp;</p>\n\n<p><a href=\"http://compmusic.upf.edu/node/233\">http://compmusic.upf.edu/node/233&nbsp;</a></p>",
        "zenodo_id": 1284501,
        "dblp_key": "conf/ismir/2007"
    }
]