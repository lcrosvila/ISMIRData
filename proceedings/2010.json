[
    {
        "title": "Bass Playing Style Detection Based on High-level Features and Pattern Similarity.",
        "author": [
            "Jakob Abeßer",
            "Paul Bräuer",
            "Hanna M. Lukashevich",
            "Gerald Schuller"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418213",
        "url": "https://doi.org/10.5281/zenodo.1418213",
        "ee": "https://zenodo.org/records/1418213/files/AbesserBLS10.pdf",
        "abstract": "In this paper, we compare two approaches for automatic classiﬁcation of bass playing styles, one based on high- level features and another one based on similarity mea- sures between bass patterns. For both approaches, we com- pare two different strategies: classiﬁcation of patterns as a whole and classiﬁcation of all measures of a pattern with a subsequent accumulation of the classiﬁcation results. Fur- thermore, we investigate the inﬂuence of potential tran- scription errors on the classiﬁcation accuracy, which tend to occur when real audio data is analyzed. We achieve best classiﬁcation accuracy values of 60.8% for the feature-based classiﬁcation and 68.5% for the classiﬁca- tion based on pattern similarity based on a taxonomy con- sisting of 8 different bass playing styles.",
        "zenodo_id": 1418213,
        "dblp_key": "conf/ismir/AbesserBLS10"
    },
    {
        "title": "Combining Chroma Features For Cover Version Identification.",
        "author": [
            "Teppo E. Ahonen"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414766",
        "url": "https://doi.org/10.5281/zenodo.1414766",
        "ee": "https://zenodo.org/records/1414766/files/Ahonen10.pdf",
        "abstract": "We present an approach for cover version identiﬁcation which is based on combining different discretized features derived from the chromagram vectors extracted from the audio data. For measuring similarity between features, we use a parameter-free quasi-universal similarity metric which utilizes data compression. Evaluation proves that combined feature distances increase the accuracy in cover version identiﬁcation.",
        "zenodo_id": 1414766,
        "dblp_key": "conf/ismir/Ahonen10"
    },
    {
        "title": "Discovering Metadata Inconsistencies.",
        "author": [
            "Bruno Angeles",
            "Cory McKay",
            "Ichiro Fujinaga"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415550",
        "url": "https://doi.org/10.5281/zenodo.1415550",
        "ee": "https://zenodo.org/records/1415550/files/AngelesMF10.pdf",
        "abstract": "This paper describes the use of fingerprinting-based querying in identifying metadata inconsistencies in music libraries, as well as the updates to the jMusicMeta- Manager software in order to perform the analysis. Test results are presented for both the Codaich database and a generic library of unprocessed metadata. Statistics were computed in order to evaluate the differences between a manually-maintained library and an unprocessed collection when comparing metadata with values on a MusicBrainz server queried by fingerprinting.",
        "zenodo_id": 1415550,
        "dblp_key": "conf/ismir/AngelesMF10"
    },
    {
        "title": "Is There a Relation Between the Syntax and the Fitness of an Audio Feature?.",
        "author": [
            "Gabriele Barbieri",
            "François Pachet",
            "Mirko Degli Esposti",
            "Pierre Roy"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416014",
        "url": "https://doi.org/10.5281/zenodo.1416014",
        "ee": "https://zenodo.org/records/1416014/files/BarbieriPER10.pdf",
        "abstract": "Feature generation has been proposed recently to generate feature sets automatically, as opposed to human-designed feature sets. This technique has shown promising results in many areas of supervised classiﬁcation, in particular in the audio domain. However, feature generation is usually performed blindly, with genetic algorithms. As a result search performance is poor, thereby limiting its practical use. We propose a method to increase the search perfor- mance of feature generation systems. We focus on ana- lytical features, i.e. features determined by their syntax. Our method consists in ﬁrst extracting statistical proper- ties of the feature space called spin patterns, by analogy with statistical physics. We show that spin patterns carry information about the topology of the feature space. We exploit these spin patterns to guide a simulated annealing algorithm speciﬁcally designed for feature generation. We evaluate our approach on three audio classiﬁcation prob- lems, and show that it increases performance by an order of magnitude. More generally this work is a ﬁrst step in using tools from statistical physics for the supervised clas- siﬁcation of complex audio signals.",
        "zenodo_id": 1416014,
        "dblp_key": "conf/ismir/BarbieriPER10"
    },
    {
        "title": "SongWords: Exploring Music Collections Through Lyrics.",
        "author": [
            "Dominikus Baur",
            "Bartholomäus Steinmayr",
            "Andreas Butz"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416682",
        "url": "https://doi.org/10.5281/zenodo.1416682",
        "ee": "https://zenodo.org/records/1416682/files/BaurSB10.pdf",
        "abstract": "The lyrics of a song are an interesting, yet underused type of symbolic music data. We present SongWords, an ap- plication for tabletop computers that allows browsing and exploring a music collection based on its lyrics. Song- Words can present the collection in a self-organizing map or sorted along different dimensions. Songs can be ordered by lyrics, user-generated tags or alphabetically by name, which allows exploring simple correlations, e.g., between genres (such as gospel) and words (such as lord). In this paper, we discuss the design rationale and implementation of SongWords as well as a user study with personal music collections. We found that lyrics indeed enable a different access to music collections and identiﬁed some challenges for future lyrics-based interfaces.",
        "zenodo_id": 1416682,
        "dblp_key": "conf/ismir/BaurSB10"
    },
    {
        "title": "Scalable Genre and Tag Prediction with Spectral Covariance.",
        "author": [
            "James Bergstra",
            "Michael I. Mandel",
            "Douglas Eck"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416942",
        "url": "https://doi.org/10.5281/zenodo.1416942",
        "ee": "https://zenodo.org/records/1416942/files/BergstraME10.pdf",
        "abstract": "Cepstral analysis is effective in separating source from ﬁl- ter in vocal and monophonic [pitched] recordings, but is it a good general-purpose framework for working with mu- sic audio? We evaluate covariance in spectral features as an alternative to means and variances in cepstral features (par- ticularly MFCCs) as summaries of frame-level features. We ﬁnd that spectral covariance is more effective than mean, variance, and covariance statistics of MFCCs for genre and social tag prediction. Support for our model comes from strong and state-of-the-art performance on the GTZAN genre dataset, MajorMiner, and MagnaTagatune. Our clas- siﬁcation strategy based on linear classiﬁers is easy to im- plement, exhibits very little sensitivity to hyper-parameters, trains quickly (even for web-scale datasets), is fast to ap- ply, and offers competitive performance in genre and tag prediction.",
        "zenodo_id": 1416942,
        "dblp_key": "conf/ismir/BergstraME10"
    },
    {
        "title": "Clustering Beat-Chroma Patterns in a Large Music Database.",
        "author": [
            "Thierry Bertin-Mahieux",
            "Ron J. Weiss",
            "Daniel P. W. Ellis"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416720",
        "url": "https://doi.org/10.5281/zenodo.1416720",
        "ee": "https://zenodo.org/records/1416720/files/Bertin-MahieuxWE10.pdf",
        "abstract": "A musical style or genre implies a set of common con- ventions and patterns combined and deployed in different ways to make individual musical pieces; for instance, most would agree that contemporary pop music is assembled from a relatively small palette of harmonic and melodic patterns. The purpose of this paper is to use a database of tens of thousands of songs in combination with a com- pact representation of melodic-harmonic content (the beat- synchronous chromagram) and data-mining tools (cluster- ing) to attempt to explicitly catalog this palette – at least within the limitations of the beat-chroma representation. We use online k-means clustering to summarize 3.7 mil- lion 4-beat bars in a codebook of a few hundred prototypes. By measuring how accurately such a quantized codebook can reconstruct the original data, we can quantify the de- gree of diversity (distortion as a function of codebook size) and temporal structure (i.e. the advantage gained by joint quantizing multiple frames) in this music. The most popu- lar codewords themselves reveal the common chords used in the music. Finally, the quantized representation of mu- sic can be used for music retrieval tasks such as artist and genre classiﬁcation, and identifying songs that are similar in terms of their melodic-harmonic content.",
        "zenodo_id": 1416720,
        "dblp_key": "conf/ismir/Bertin-MahieuxWE10"
    },
    {
        "title": "Decomposition Into Autonomous and Comparable Blocks: A Structural Description of Music Pieces.",
        "author": [
            "Frédéric Bimbot",
            "Olivier Le Blouch",
            "Gabriel Sargent",
            "Emmanuel Vincent 0001"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414734",
        "url": "https://doi.org/10.5281/zenodo.1414734",
        "ee": "https://zenodo.org/records/1414734/files/BimbotBSV10.pdf",
        "abstract": "The structure of a music piece is a concept which is often referred to in various areas of music sciences and technolo- gies, but for which there is no commonly agreed definition. This raises a methodological issue in MIR, when designing and evaluating automatic structure inference algorithms. It also strongly limits the possibility to produce consistent large-scale annotation datasets in a cooperative manner. This article proposes an approach called decomposition into autonomous and comparable blocks, based on principles inspired from structuralism and generativism. It specifies a methodology for producing music structure annotation by human listeners based on simple criteria and resorting solely to the listening experience of the annotator. We show on a development set that the proposed approach can provide a reasonable level of concordance across anno- tators and we introduce a set of annotations on the RWC da- tabase, intended to be released to the MIR community.",
        "zenodo_id": 1414734,
        "dblp_key": "conf/ismir/BimbotBSV10"
    },
    {
        "title": "Music Genre Classification via Compressive Sampling.",
        "author": [
            "Kaichun K. Chang",
            "Jyh-Shing Roger Jang",
            "Costas S. Iliopoulos"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418289",
        "url": "https://doi.org/10.5281/zenodo.1418289",
        "ee": "https://zenodo.org/records/1418289/files/ChangJI10.pdf",
        "abstract": "Compressive sampling (CS) is a new research topic in signal processing that has piqued the interest of a wide range of researchers in different ﬁelds recently. In this pa- per, we present a CS-based classiﬁer for music genre clas- siﬁcation, with two sets of features, including short-time and long-time features of audio music. The proposed clas- siﬁer generates a compact signature to achieve a signiﬁcant reduction in the dimensionality of the audio music signals. The experimental results demonstrate that the computation time of the CS-based classiﬁer is only about 20% of SVM on GTZAN dataset, with an accuracy of 92.7%. Several experiments were conducted in this study to illustrate the feasibility and robustness of the proposed methods as com- pared to other approaches.",
        "zenodo_id": 1418289,
        "dblp_key": "conf/ismir/ChangJI10"
    },
    {
        "title": "ThumbnailDJ: Visual Thumbnails of Music Content.",
        "author": [
            "Ya-Xi Chen",
            "René Klüber"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418271",
        "url": "https://doi.org/10.5281/zenodo.1418271",
        "ee": "https://zenodo.org/records/1418271/files/ChenK10.pdf",
        "abstract": "Musical perception is non-visual and people cannot de- scribe what a song sounds like without listening to it. To facilitate music browsing and searching, we explore the automatic generation of visual thumbnails for music. Tar- geting an expert user groups, DJs, we developed a con- cept named ThumbnailDJ: Based on a metaphor of music notation, a visual thumbnail can be automatically gener- ated for an audio file, including information of tempo, volume, genre, aggressiveness and bass. We discussed ThumbnailDJ and other 3 selected concepts with DJs, and our concept was preferred most. Based on the results of this interview, we refined ThumbnailDJ and conducted an evaluation with DJs. The results confirmed that ThumbnailDJ can facilitate expert users browsing and searching within their music collection.",
        "zenodo_id": 1418271,
        "dblp_key": "conf/ismir/ChenK10"
    },
    {
        "title": "Multiple Viewpoints Modeling of Tabla Sequences.",
        "author": [
            "Parag Chordia",
            "Avinash Sastry",
            "Trishul Mallikarjuna",
            "Aaron Albin"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416540",
        "url": "https://doi.org/10.5281/zenodo.1416540",
        "ee": "https://zenodo.org/records/1416540/files/ChordiaSMA10.pdf",
        "abstract": "We describe a system that attempts to predict the con- tinuation of a symbolically encoded tabla composition at each time step using a variable-length n-gram model. Us- ing cross-entropy as a measure of model ﬁt, the best model attained an entropy rate of 0.780 in a cross-validation ex- periment, showing that symbolic tabla compositions can be effectively encoded using such a model. The choice of smoothing algorithm, which determines how information from different-order models is combined, is found to be an important factor in the models performance. We extend the basic n-gram model by adding viewpoints, other streams of information that can be used to improve predictive per- formance. First, we show that adding a short-term model, built on the current composition and not the entire corpus, leads to substantial improvements. Additional experiments were conducted with derived types, representations derived from the basic data type (stroke names), and cross-types, which model dependencies between parameters, such as duration and stroke name. For this database, such exten- sions improved performance only marginally, although this may have been due to the low entropy rate attained by the basic model.",
        "zenodo_id": 1416540,
        "dblp_key": "conf/ismir/ChordiaSMA10"
    },
    {
        "title": "Quantifying the Benefits of Using an Interactive Decision Support Tool for Creating Musical Accompaniment in a Particular Style.",
        "author": [
            "Ching-Hua Chuan",
            "Elaine Chew"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417337",
        "url": "https://doi.org/10.5281/zenodo.1417337",
        "ee": "https://zenodo.org/records/1417337/files/ChuanC10.pdf",
        "abstract": "We present a human-centered experiment designed to measure the degree of support for creating musical ac- companiment provided by an interactive composition de- cision-support system. We create an interactive system with visual and audio cues to assist users in the choosing of chords to craft an accompaniment in a desired style. We propose general measures for objectively evaluating the effectiveness and usability of such systems. We use melodies of existing songs by Radiohead as tests. Quanti- tative measures of musical distance – percentage correct and closely related chords, and average neo-Riemannian distance – compare the user-created accompaniment with the original, with and without decision support. Numbers of backward edits, unique chords explored, and repeated chord choices during composition help quantify composi- tion behavior. We present experimental data from musi- cians and non-musicians. We observe that decision sup- port reduces the time spent in composition, the number of revisions of earlier choices, redundant behavior such as repeated chord choices, and the gap between musicians’ and non-musicians’ work, without significantly limiting the range of users’ choices.",
        "zenodo_id": 1417337,
        "dblp_key": "conf/ismir/ChuanC10"
    },
    {
        "title": "Computational Analysis of Musical Influence: A Musicological Case Study Using MIR Tools.",
        "author": [
            "Nick Collins"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416756",
        "url": "https://doi.org/10.5281/zenodo.1416756",
        "ee": "https://zenodo.org/records/1416756/files/Collins10.pdf",
        "abstract": "Are there new insights through computational methods to the thorny problem of plotting the ﬂow of musical inﬂu- ence? This project, motivated by a musicological study of early synth pop, applies MIR tools as an aid to the inves- tigator. Web scraping and web services provide one an- gle, sourcing data from allmusic.com, and utilising python APIs for last.fm, EchoNest, and MusicBrainz. Charts of inﬂuence are constructed in GraphViz combining artist sim- ilarity and dates. Content based music similarity is the sec- ond approach, based around a core collection of synth pop albums. The prospect for new musical analyses are dis- cussed with respect to these techniques.",
        "zenodo_id": 1416756,
        "dblp_key": "conf/ismir/Collins10"
    },
    {
        "title": "A Comparative Evaluation of Algorithms for Discovering Translational Patterns in Baroque Keyboard Works.",
        "author": [
            "Tom Collins",
            "Jeremy Thurlow",
            "Robin C. Laney",
            "Alistair Willis",
            "Paul H. Garthwaite"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416070",
        "url": "https://doi.org/10.5281/zenodo.1416070",
        "ee": "https://zenodo.org/records/1416070/files/CollinsTLWG10.pdf",
        "abstract": "We consider the problem of intra-opus pattern discovery, that is, the task of discovering patterns of a speciﬁed type within a piece of music. A music analyst undertook this task for works by Domenico Scarlattti and Johann Sebas- tian Bach, forming a benchmark of ‘target’ patterns. The performance of two existing algorithms and one of our own creation, called SIACT, is evaluated by comparison with this benchmark. SIACT out-performs the existing algo- rithms with regard to recall and, more often than not, pre- cision. It is demonstrated that in all but the most care- fully selected excerpts of music, the two existing algo- rithms can be affected by what is termed the ‘problem of isolated membership’. Central to the relative success of SIACT is our intention that it should address this particu- lar problem. The paper contrasts string-based and geomet- ric approaches to pattern discovery, with an introduction to the latter. Suggestions for future work are given.",
        "zenodo_id": 1416070,
        "dblp_key": "conf/ismir/CollinsTLWG10"
    },
    {
        "title": "Discovery of Contrapuntal Patterns.",
        "author": [
            "Darrell Conklin",
            "Mathieu Bergeron"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417413",
        "url": "https://doi.org/10.5281/zenodo.1417413",
        "ee": "https://zenodo.org/records/1417413/files/ConklinB10.pdf",
        "abstract": "This paper develops and applies a new method for the dis- covery of polyphonic patterns. The method supports the notes that overlap in time without being simultaneous. Such relations are central to understanding species counterpoint. The method consists of an application of the vertical view- point technique, which relies on a vertical slicing of the musical score. It is applied to two-voice contrapuntal tex- tures extracted from the Bach chorale harmonizations. Re- sults show that the new method is powerful enough to rep- resent and discover distinctive modules of species coun- terpoint, including remarkably the suspension principle of fourth species counterpoint. In addition, by focusing on two voices in particular and setting them against all other possible voice pairs, the method can elicit patterns that il- lustrate well the unique treatment of the voices under in- vestigation, e.g. the inner and outer voices. The results are promising and indicate that the method is suitable for computational musicology research.",
        "zenodo_id": 1417413,
        "dblp_key": "conf/ismir/ConklinB10"
    },
    {
        "title": "Automatic Music Tagging With Time Series Models.",
        "author": [
            "Emanuele Coviello",
            "Luke Barrington",
            "Antoni B. Chan",
            "Gert R. G. Lanckriet"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415274",
        "url": "https://doi.org/10.5281/zenodo.1415274",
        "ee": "https://zenodo.org/records/1415274/files/CovielloBCL10.pdf",
        "abstract": "State-of-the-art systems for automatic music tagging model music based on bag-of-feature representations which give little or no account of temporal dynamics, a key characteristic of the audio signal. We describe a novel approach to automatic music annotation and retrieval that captures temporal (e.g., rhythmical) aspects as well as tim- bral content. The proposed approach leverages a recently proposed song model that is based on a generative time series model of the musical content — the dynamic tex- ture mixture (DTM) model — that treats fragments of au- dio as the output of a linear dynamical system. To model characteristic temporal dynamics and timbral content at the tag level, a novel, efﬁcient hierarchical EM algorithm for DTM (HEM-DTM) is used to summarize the common in- formation shared by DTMs modeling individual songs as- sociated with a tag. Experiments show learning the seman- tics of music beneﬁts from modeling temporal dynamics.",
        "zenodo_id": 1415274,
        "dblp_key": "conf/ismir/CovielloBCL10"
    },
    {
        "title": "A Segmentation-based Tempo Induction Method.",
        "author": [
            "Maxime Le Coz",
            "Hélène Lachambre",
            "Lionel Koenig",
            "Régine André-Obrecht"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416766",
        "url": "https://doi.org/10.5281/zenodo.1416766",
        "ee": "https://zenodo.org/records/1416766/files/CozLKA10.pdf",
        "abstract": "The automatized beat detection and localization have been the subject of multiple research in the ﬁeld of music infor- mation retrieval. Most of the methods are based on onset detection. We propose an alternative approach: Our method is based on the “Forward-Backward seg- mentation”: the segments may be interpreted as attacks, decays, sustains and releases of notes. We process the seg- ment boundaries as a weighted Dirac signal. Three meth- ods devived from its spectral analysis are proposed to ﬁnd a periodicity which corresponds to the tempo. The experiments are carried out on a corpus of 100 songs of the RWC database. The performances of our system on this base demonstrate a potential in the use of a “ Forward- Backward Segmentation” for temporal information retrieval in musical signals.",
        "zenodo_id": 1416766,
        "dblp_key": "conf/ismir/CozLKA10"
    },
    {
        "title": "Recognising Classical Works in Historical Recordings.",
        "author": [
            "Tim Crawford",
            "Matthias Mauch",
            "Christophe Rhodes"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417429",
        "url": "https://doi.org/10.5281/zenodo.1417429",
        "ee": "https://zenodo.org/records/1417429/files/CrawfordMR10.pdf",
        "abstract": "In collections of recordings of classical music, it is normal to find multiple performances, usually by different artists, of the same pieces of music. While there may be differences in many dimensions of musical similarity, such as timbre, pitch or structural detail, the underlying musical content is essentially and recognizably the same. The degree of divergence is generally less than that found between ‘cover songs’ in the domain of popular music, and much less than in typical performances of jazz standards. MIR methods, based around variants of the chroma representation, can be useful in tasks such as work identification especially where disco/bibliographical metadata is absent or incomplete as well as for access, curation and management of collections. We describe some initial experiments in work-recognition on a test-collection comprising c. 2000 digital transfers of historical recordings, and show that the use of NNLS chroma, a new, musically-informed chroma feature, dramatically improves recognition.",
        "zenodo_id": 1417429,
        "dblp_key": "conf/ismir/CrawfordMR10"
    },
    {
        "title": "Music21: A Toolkit for Computer-Aided Musicology and Symbolic Music Data.",
        "author": [
            "Michael Scott Cuthbert",
            "Christopher Ariza"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416114",
        "url": "https://doi.org/10.5281/zenodo.1416114",
        "ee": "https://zenodo.org/records/1416114/files/CuthbertA10.pdf",
        "abstract": "Music21 is an object-oriented toolkit for analyzing, searching, and transforming music in symbolic (score- based) forms. The modular approach of the project allows musicians and researchers to write simple scripts rapidly and reuse them in other projects. The toolkit aims to pro- vide powerful software tools integrated with sophisticated musical knowledge to both musicians with little pro- gramming experience (especially musicologists) and to programmers with only modest music theory skills. This paper introduces the music21 system, demon- strating how to use it and the types of problems it is well- suited toward advancing. We include numerous examples of its power and flexibility, including demonstrations of graphing data and generating annotated musical scores.",
        "zenodo_id": 1416114,
        "dblp_key": "conf/ismir/CuthbertA10"
    },
    {
        "title": "Real-time Polyphonic Music Transcription with Non-negative Matrix Factorization and Beta-divergence.",
        "author": [
            "Arnaud Dessein",
            "Arshia Cont",
            "Guillaume Lemaitre"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414824",
        "url": "https://doi.org/10.5281/zenodo.1414824",
        "ee": "https://zenodo.org/records/1414824/files/DesseinCL10.pdf",
        "abstract": "In this paper, we investigate the problem of real-time poly- phonic music transcription by employing non-negative ma- trix factorization techniques and the β-divergence as a cost function. We consider real-world setups where the mu- sic signal arrives incrementally to the system and is tran- scribed as it unfolds in time. The proposed transcription system is addressed with a modiﬁed non-negative matrix factorization scheme, called non-negative decomposition, where the incoming signal is projected onto a ﬁxed basis of templates learned off-line prior to the decomposition. We discuss the use of non-negative matrix factorization with the β-divergence to achieve the real-time decomposition. The proposed system is evaluated on the speciﬁc task of piano music transcription and the results show that it can outperform several state-of-the-art off-line approaches.",
        "zenodo_id": 1414824,
        "dblp_key": "conf/ismir/DesseinCL10"
    },
    {
        "title": "Modified Ais-based Classifier for Music Genre Classification.",
        "author": [
            "Noor Azilah Draman",
            "Campbell Wilson",
            "Sea Ling"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417437",
        "url": "https://doi.org/10.5281/zenodo.1417437",
        "ee": "https://zenodo.org/records/1417437/files/DramanWL10.pdf",
        "abstract": "Automating human capabilities for classifying different genre of songs is a difficult task. This has led to various studies that focused on finding solutions to solve this problem. Analyzing music contents (often referred as con- tent-based analysis) is one of many ways to identify and group similar songs together. Various music contents, for example beat, pitch, timbral and many others were used and analyzed to represent the music. To be able to mani- pulate these content representations for recognition: fea- ture extraction and classification are two major focuses of investigation in this area. Though various classification techniques proposed so far, we are introducing yet anoth- er one. The objective of this paper is to introduce a possi- ble new technique in the Artificial Immune System (AIS) domain called a modified immune classifier (MIC) for music genre classification. MIC is the newest version of Negative Selection Algorithm (NSA) where it stresses the self and non-self cells recognition and a complementary process for generating detectors. The discussion will de- tail out the MIC procedures applied and the modified part in solving the classification problem. At the end, the re- sults of proposed framework will be presented, discussed and directions for future work are given.",
        "zenodo_id": 1417437,
        "dblp_key": "conf/ismir/DramanWL10"
    },
    {
        "title": "Tunepal - Disseminating a Music Information Retrieval System to the Traditional Irish Music Community.",
        "author": [
            "Bryan Duggan",
            "Brendan O&apos;Shea"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416312",
        "url": "https://doi.org/10.5281/zenodo.1416312",
        "ee": "https://zenodo.org/records/1416312/files/DugganO10.pdf",
        "abstract": "In this paper we present two new query-by-playing (QBP) music information retrieval (MIR) systems aimed at musicians playing traditional Irish dance music. Firstly, a browser hosted system - tunepal.org is pre- sented. Secondly, we present Tunepal for iPhone/iPod touch devices - a QBP system that can be used in situ in traditional music sessions. Both of these systems use a backend corpus of 13,290 tunes drawn from community sources and “standard” references. These systems have evolved from academic research to become popular tools used by musicians around the world. 16,064 queries have been logged since the systems were launched on 31 July, 2009 and 11 February, 2010 respectively to 18 May 2010. As we log data on every query made, including geocoding queries made on the iPhone, we propose that these tools may be used to follow trends in the playing of traditional music. We also present an analysis of the data we have collected on the usage of these systems.",
        "zenodo_id": 1416312,
        "dblp_key": "conf/ismir/DugganO10"
    },
    {
        "title": "Universal Onset Detection with Bidirectional Long Short-Term Memory Neural Networks.",
        "author": [
            "Florian Eyben",
            "Sebastian Böck",
            "Björn W. Schuller",
            "Alex Graves"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417131",
        "url": "https://doi.org/10.5281/zenodo.1417131",
        "ee": "https://zenodo.org/records/1417131/files/EybenBSG10.pdf",
        "abstract": "Many different onset detection methods have been pro- posed in recent years. However those that perform well tend to be highly specialised for certain types of music, while those that are more widely applicable give only mod- erate performance. In this paper we present a new onset detector with superior performance and temporal precision for all kinds of music, including complex music mixes. It is based on auditory spectral features and relative spectral differences processed by a bidirectional Long Short-Term Memory recurrent neural network, which acts as reduction function. The network is trained with a large database of onset data covering various genres and onset types. Due to the data driven nature, our approach does not require the onset detection method and its parameters to be tuned to a particular type of music. We compare results on the Bello onset data set and can conclude that our approach is on par with related results on the same set and outperforms them in most cases in terms of F1-measure. For complex music with mixed onset types, an absolute improvement of 3.6% is reported.",
        "zenodo_id": 1417131,
        "dblp_key": "conf/ismir/EybenBSG10"
    },
    {
        "title": "Timbral Qualities of Semantic Structures of Music.",
        "author": [
            "Rafael Ferrer",
            "Tuomas Eerola"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416484",
        "url": "https://doi.org/10.5281/zenodo.1416484",
        "ee": "https://zenodo.org/records/1416484/files/FerrerE10.pdf",
        "abstract": "The rapid expansion of social media in music has pro- vided the ﬁeld with impressive datasets that offer insights into the semantic structures underlying everyday uses and classiﬁcation of music. We hypothesize that the organiza- tion of these structures are rather directly linked with the ”qualia” of the music as sound. To explore the ways in which these structures are connected with the qualities of sounds, a semantic space was extracted from a large collec- tion of musical tags with latent semantic and cluster anal- ysis. The perceptual and musical properties of 19 clus- ters were investigated by a similarity rating task that used spliced musical excerpts representing each cluster. The re- sulting perceptual space denoting the clusters correlated high with selected acoustical features extracted from the stimuli. The ﬁrst dimension related to the high-frequency energy content, the second to the regularity of the spec- trum, and the third to the ﬂuctuations within the spectrum. These ﬁndings imply that meaningful organization of mu- sic may be derived from low-level descriptions of the ex- cerpts. Novel links with the functions of music embedded into the tagging information included within the social me- dia are proposed.",
        "zenodo_id": 1416484,
        "dblp_key": "conf/ismir/FerrerE10"
    },
    {
        "title": "Combining Features Reduces Hubness in Audio Similarity.",
        "author": [
            "Arthur Flexer",
            "Dominik Schnitzer",
            "Martin Gasser",
            "Tim Pohle"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416360",
        "url": "https://doi.org/10.5281/zenodo.1416360",
        "ee": "https://zenodo.org/records/1416360/files/FlexerSGP10.pdf",
        "abstract": "In audio based music similarity, a well known effect is the existence of hubs, i.e. songs which appear similar to many other songs without showing any meaningful per- ceptual similarity. We verify that this effect also exists in very large databases (> 250000 songs) and that it even gets worse with growing size of databases. By combining different aspects of audio similarity we are able to reduce the hub problem while at the same time maintaining a high overall quality of audio similarity.",
        "zenodo_id": 1416360,
        "dblp_key": "conf/ismir/FlexerSGP10"
    },
    {
        "title": "Handling Repeats and Jumps in Score-performance Synchronization.",
        "author": [
            "Christian Fremerey",
            "Meinard Müller",
            "Michael Clausen"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415942",
        "url": "https://doi.org/10.5281/zenodo.1415942",
        "ee": "https://zenodo.org/records/1415942/files/FremereyMC10.pdf",
        "abstract": "Given a score representation and a recorded performance of the same piece of music, the task of score-performance synchronization is to temporally align musical sections such as bars speciﬁed by the score to temporal sections in the performance. Most of the previous approaches as- sume that the score and the performance to be synchro- nized globally agree with regard to the overall musical structure. In practice, however, this assumption is often violated. For example, a performer may deviate from the score by ignoring a repeat or introducing an additional re- peat that is not written in the score. In this paper, we introduce a synchronization approach that can cope with such structural differences. As main technical contribu- tion, we describe a novel variant of dynamic time warping (DTW), referred to as JumpDTW, which allows for han- dling jumps and repeats in the alignment. Our approach is evaluated for the practically relevant case of synchronizing score data obtained from scanned sheet music via optical music recognition to corresponding audio recordings. Our experiments based on Beethoven piano sonatas show that JumpDTW can robustly identify and handle most of the oc- curring jumps and repeats leading to an overall alignment accuracy of over 99% on the bar-level.",
        "zenodo_id": 1415942,
        "dblp_key": "conf/ismir/FremereyMC10"
    },
    {
        "title": "Automated Music Slideshow Generation Using Web Images Based on Lyrics.",
        "author": [
            "Shintaro Funasawa",
            "Hiromi Ishizaki",
            "Keiichiro Hoashi",
            "Yasuhiro Takishima",
            "Jiro Katto"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415996",
        "url": "https://doi.org/10.5281/zenodo.1415996",
        "ee": "https://zenodo.org/records/1415996/files/FunasawaIHTK10.pdf",
        "abstract": "In this paper, we propose a system which automatically generates slideshows for music, by utilizing images re- trieved from photo sharing web sites, based on query words extracted from song lyrics. The proposed system consists of two major steps: (1) query extraction from song lyrics, (2) image selection from web image search results. Moreover, in order to improve the display dura- tion of each image in the slideshow, we adjust image tran- sition timing by analyzing the duration of each lyric line in the input song. We have conducted subjective evalua- tion experiments, which prove that the proposal can gen- erate impressive music slideshows for any input song.",
        "zenodo_id": 1415996,
        "dblp_key": "conf/ismir/FunasawaIHTK10"
    },
    {
        "title": "Evaluation of a Score-informed Source Separation System.",
        "author": [
            "Joachim Ganseman",
            "Paul Scheunders",
            "Gautham J. Mysore",
            "Jonathan S. Abel"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416062",
        "url": "https://doi.org/10.5281/zenodo.1416062",
        "ee": "https://zenodo.org/records/1416062/files/GansemanSMA10.pdf",
        "abstract": "In this work, we investigate a method for score-informed source separation using Probabilistic Latent Component Analysis (PLCA). We present extensive test results that give an indication of the performance of the method, its strengths and weaknesses. For this purpose, we created a test database that has been made available to the public, in order to encourage comparisons with alternative methods.",
        "zenodo_id": 1416062,
        "dblp_key": "conf/ismir/GansemanSMA10"
    },
    {
        "title": "Singing / Rap Classification of Isolated Vocal Tracks.",
        "author": [
            "Daniel Gärtner"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417089",
        "url": "https://doi.org/10.5281/zenodo.1417089",
        "ee": "https://zenodo.org/records/1417089/files/Gartner10.pdf",
        "abstract": "In this paper, a system for the classiﬁcation of the vo- cal characteristics in HipHop / R&B music is presented. Isolated vocal track segments, taken from acapella ver- sions of commercial recordings, are classiﬁed into classes singing and rap. A feature-set motivated by work from song / speech classiﬁcation, speech emotion recognition, and from differences that humans perceive and utilize, is presented. An SVM is used as classiﬁer, accuracies of about 90% are achieved. In addition, the features are an- alyzed according to their contribution, using the IRMFSP feature selection algorithm. In another experiment, it is shown that the features are robust against utterance-speci- ﬁc characteristics.",
        "zenodo_id": 1417089,
        "dblp_key": "conf/ismir/Gartner10"
    },
    {
        "title": "Tempo Induction Using Filterbank Analysis and Tonal Features.",
        "author": [
            "Aggelos Gkiokas",
            "Vassilis Katsouros",
            "George Carayannis"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415210",
        "url": "https://doi.org/10.5281/zenodo.1415210",
        "ee": "https://zenodo.org/records/1415210/files/GkiokasKC10.pdf",
        "abstract": "This paper presents an algorithm that extracts the tempo of a musical excerpt. The proposed system assumes a constant tempo and deals directly with the audio signal. A sliding window is applied to the signal and two feature classes are extracted. The first class is the log-energy of each band of a mel-scale triangular filterbank, a common feature vector used in various MIR applications. For the second class, a novel feature for the tempo induction task is presented; the strengths of the twelve western musical tones at all octaves are calculated for each audio frame, in a similar fashion with Pitch Class Profile. The time- evolving feature vectors are convolved with a bank of resonators, each resonator corresponding to a target tempo. Then the results of each feature class are com- bined to give the final output. The algorithm was evaluated on the popular ISMIR 2004 Tempo Induction Evaluation Exchange Dataset. Re- sults demonstrate that the superposition of the different types of features enhance the performance of the algo- rithm, which is in the current state-of-the-art algorithms of the tempo induction task.",
        "zenodo_id": 1415210,
        "dblp_key": "conf/ismir/GkiokasKC10"
    },
    {
        "title": "A Probabilistic Subspace Model for Multi-instrument Polyphonic Transcription.",
        "author": [
            "Graham Grindlay",
            "Daniel P. W. Ellis"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416842",
        "url": "https://doi.org/10.5281/zenodo.1416842",
        "ee": "https://zenodo.org/records/1416842/files/GrindlayE10.pdf",
        "abstract": "In this paper we present a general probabilistic model suit- able for transcribing single-channel audio recordings con- taining multiple polyphonic sources. Our system requires no prior knowledge of the instruments in the mixture, al- though it can beneﬁt from this information if available. In contrast to many existing polyphonic transcription sys- tems, our approach explicitly models the individual instru- ments and is thereby able to assign detected notes to their respective sources. We use a set of training instruments to learn a model space which is then used during transcrip- tion to constrain the properties of models ﬁt to the target mixture. In addition, we encourage model sparsity using a simple approach related to tempering. We evaluate our method on both recorded and synthe- sized two-instrument mixtures, obtaining average frame- level F-measures of up to 0.60 for synthesized audio and",
        "zenodo_id": 1416842,
        "dblp_key": "conf/ismir/GrindlayE10"
    },
    {
        "title": "What Makes Beat Tracking Difficult? A Case Study on Chopin Mazurkas.",
        "author": [
            "Peter Grosche",
            "Meinard Müller",
            "Craig Stuart Sapp"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415852",
        "url": "https://doi.org/10.5281/zenodo.1415852",
        "ee": "https://zenodo.org/records/1415852/files/GroscheMS10.pdf",
        "abstract": "The automated extraction of tempo and beat information from music recordings is a challenging task. Especially in the case of expressive performances, current beat track- ing approaches still have signiﬁcant problems to accurately capture local tempo deviations and beat positions. In this paper, we introduce a novel evaluation framework for de- tecting critical passages in a piece of music that are prone to tracking errors. Our idea is to look for consistencies in the beat tracking results over multiple performances of the same underlying piece. As another contribution, we further classify the critical passages by specifying musi- cal properties of certain beats that frequently evoke track- ing errors. Finally, considering three conceptually different beat tracking procedures, we conduct a case study on the basis of a challenging test set that consists of a variety of piano performances of Chopin Mazurkas. Our experimen- tal results not only make the limitations of state-of-the-art beat trackers explicit but also deepens the understanding of the underlying music material.",
        "zenodo_id": 1415852,
        "dblp_key": "conf/ismir/GroscheMS10"
    },
    {
        "title": "Learning Features from Music Audio with Deep Belief Networks.",
        "author": [
            "Philippe Hamel",
            "Douglas Eck"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414970",
        "url": "https://doi.org/10.5281/zenodo.1414970",
        "ee": "https://zenodo.org/records/1414970/files/HamelE10.pdf",
        "abstract": "Feature extraction is a crucial part of many MIR tasks. In this work, we present a system that can automatically ex- tract relevant features from audio for a given task. The fea- ture extraction system consists of a Deep Belief Network (DBN) on Discrete Fourier Transforms (DFTs) of the au- dio. We then use the activations of the trained network as inputs for a non-linear Support Vector Machine (SVM) classiﬁer. In particular, we learned the features to solve the task of genre recognition. The learned features per- form signiﬁcantly better than MFCCs. Moreover, we ob- tain a classiﬁcation accuracy of 84.3% on the Tzanetakis dataset, which compares favorably against state-of-the-art genre classiﬁers using frame-based features. We also ap- plied these same features to the task of auto-tagging. The autotaggers trained with our features performed better than those that were trained with timbral and temporal features.",
        "zenodo_id": 1414970,
        "dblp_key": "conf/ismir/HamelE10"
    },
    {
        "title": "Informed Source Separation of Orchestra and Soloist.",
        "author": [
            "Yushen Han",
            "Christopher Raphael"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416750",
        "url": "https://doi.org/10.5281/zenodo.1416750",
        "ee": "https://zenodo.org/records/1416750/files/HanR10.pdf",
        "abstract": "A novel technique of unmasking to repair the degradation in sources separated by spectrogram masking is proposed. Our approach is based on explicit knowledge of the musi- cal audio at note level from a score-audio alignment, which we termed Informed Source Separation (ISS). Such knowl- edge allows the spectrogram energy to be decomposed into note-based models. We assume that a spectrogram mask for the solo is obtained and focus on the problem of repair- ing audio resulting from applying the mask. We evaluate the spectrogram as well as the harmonic structure of the music. We either search for unmasked (orchestra) partials of the orchestra to be transposed onto a masked (solo) re- gion or reshape a solo partial with phase and amplitude imputed from unmasked regions. We describe a Kalman smoothing technique to decouple the phase and amplitude of a musical partial that enables the modiﬁcation to the spectrogram. Audio examples from a piano concerto are available for evaluation.",
        "zenodo_id": 1416750,
        "dblp_key": "conf/ismir/HanR10"
    },
    {
        "title": "An Interchange Format for Optical Music Recognition Applications.",
        "author": [
            "Andrew Hankinson",
            "Laurent Pugin",
            "Ichiro Fujinaga"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417633",
        "url": "https://doi.org/10.5281/zenodo.1417633",
        "ee": "https://zenodo.org/records/1417633/files/HankinsonPF10.pdf",
        "abstract": "Page appearance and layout for music notation is a critical component of the overall musical information contained in a document. To capture and transfer this information, we outline an interchange format for OMR applications, the OMR Interchange Package (OIP) format, which is designed to allow layout information and page images to be preserved and transferred along with semantic musical content. We identify a number of uses for this format that can enhance digital representations of music, and introduce a novel idea for distributed optical music recognition system based on this format.",
        "zenodo_id": 1417633,
        "dblp_key": "conf/ismir/HankinsonPF10"
    },
    {
        "title": "String Quartet Classification with Monophonic Models.",
        "author": [
            "Ruben Hillewaere",
            "Bernard Manderick",
            "Darrell Conklin"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417619",
        "url": "https://doi.org/10.5281/zenodo.1417619",
        "ee": "https://zenodo.org/records/1417619/files/HillewaereMC10.pdf",
        "abstract": "Polyphonic music classiﬁcation remains a very challeng- ing area in the ﬁeld of music information retrieval. In this study, we explore the performance of monophonic mod- els on single parts that are extracted from the polyphony. The presented method is speciﬁcally designed for the case of voiced polyphony, but can be extended to any type of music with multiple parts. On a dataset of 207 Haydn and Mozart string quartet movements, global feature models with standard machine learning classiﬁers are compared with a monophonic n-gram model for the task of composer recognition. Global features emerging from feature selec- tion are presented, and future guidelines for the research of polyphonic music are outlined.",
        "zenodo_id": 1417619,
        "dblp_key": "conf/ismir/HillewaereMC10"
    },
    {
        "title": "Solving Misheard Lyric Search Queries Using a Probabilistic Model of Speech Sounds.",
        "author": [
            "Hussein Hirjee",
            "Daniel G. Brown 0001"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414942",
        "url": "https://doi.org/10.5281/zenodo.1414942",
        "ee": "https://zenodo.org/records/1414942/files/HirjeeB10.pdf",
        "abstract": "Music listeners often mishear the lyrics to unfamiliar songs heard from public sources, such as the radio. Since standard text search engines will ﬁnd few relevant results when they are entered as a query, these misheard lyrics require phonetic pattern matching techniques to identify the song. We introduce a probabilistic model of mishear- ing trained on examples of actual misheard lyrics, and develop a phoneme similarity scoring matrix based on this model. We compare this scoring method to simpler pattern matching algorithms on the task of ﬁnding the correct lyric from a collection given a misheard query. The probabilistic method signiﬁcantly outperforms all other",
        "zenodo_id": 1414942,
        "dblp_key": "conf/ismir/HirjeeB10"
    },
    {
        "title": "Fast vs Slow: Learning Tempo Octaves from User Data.",
        "author": [
            "Jason Hockman",
            "Ichiro Fujinaga"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416110",
        "url": "https://doi.org/10.5281/zenodo.1416110",
        "ee": "https://zenodo.org/records/1416110/files/HockmanF10.pdf",
        "abstract": "The widespread use of beat- and tempo-tracking methods in music information retrieval tasks has been marginalized due to undesirable sporadic results from these algorithms. While sensorimotor and listening studies have demon- strated the subjectivity and variability inherent to human performance of this task, MIR applications such as rec- ommendation require more reliable output than available from present tempo estimation models. In this paper, we present a initial investigation of tempo assessment based on the simple classiﬁcation of whether the music is fast or slow. Through three experiments, we provide performance results of our method across two datasets, and demonstrate its usefulness in the pursuit of a reliable global tempo estimation.",
        "zenodo_id": 1416110,
        "dblp_key": "conf/ismir/HockmanF10"
    },
    {
        "title": "Parataxis: Morphological Similarity in Traditional Music.",
        "author": [
            "Andre Holzapfel",
            "Yannis Stylianou"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416896",
        "url": "https://doi.org/10.5281/zenodo.1416896",
        "ee": "https://zenodo.org/records/1416896/files/HolzapfelS10.pdf",
        "abstract": "In this paper an automatic system for the detection of sim- ilar phrases in music of the Eastern Mediterranean is pro- posed. This music follows a speciﬁc structure, which is re- ferred to as parataxis. The proposed system can be applied to audio signals of complex mixtures that contain the lead melody together with instrumental accompaniment. It is shown that including a lead melody estimation into a state- of-the-art system for cover song detection leads to promis- ing results on a dataset of transcribed traditional dances from the island of Crete in Greece. Furthermore, a general framework that includes also rhythmic aspects is proposed. The proposed method represents a simple framework for the support of ethnomusicological studies on related forms of traditional music.",
        "zenodo_id": 1416896,
        "dblp_key": "conf/ismir/HolzapfelS10"
    },
    {
        "title": "Pitch Class Set Categories as Analysis Tools for Degrees of Tonality.",
        "author": [
            "Aline K. Honingh",
            "Rens Bod"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417533",
        "url": "https://doi.org/10.5281/zenodo.1417533",
        "ee": "https://zenodo.org/records/1417533/files/HoninghB10.pdf",
        "abstract": "This is an explorative paper in which we present a new method for music analysis based on pitch class set cate- gories. It has been shown before that pitch class sets can be divided into six different categories. Each category inher- its a typical character which can “tell” something about the music in which it appears. In this paper we explore the pos- sibilities of using pitch class set categories for 1) classiﬁca- tion in major/minor mode, 2) classiﬁcation in tonal/atonal music, 3) determination of a degree of tonality, and 4) de- termination of a composer’s period.",
        "zenodo_id": 1417533,
        "dblp_key": "conf/ismir/HoninghB10"
    },
    {
        "title": "Combined Audio and Video Analysis for Guitar Chord Identification.",
        "author": [
            "Alex Hrybyk",
            "Youngmoo E. Kim"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417465",
        "url": "https://doi.org/10.5281/zenodo.1417465",
        "ee": "https://zenodo.org/records/1417465/files/HrybykK10.pdf",
        "abstract": "This paper presents a multi-modal approach to automat- ically identifying guitar chords using audio and video of the performer. Chord identiﬁcation is typically performed by analyzing the audio, using a chroma based feature to extract pitch class information, then identifying the chord with the appropriate label. Even if this method proves per- fectly accurate, stringed instruments add extra ambiguity as a single chord or melody may be played in different positions on the fretboard. Preserving this information is important, because it signiﬁes the original ﬁngering, and implied “easiest” way to perform the selection. This chord identiﬁcation system combines analysis of audio to deter- mine the general chord scale (i.e. A major, G minor), and video of the guitarist to determine chord voicing (i.e. open, barred, inversion), to accurately identify the guitar chord.",
        "zenodo_id": 1417465,
        "dblp_key": "conf/ismir/HrybykK10"
    },
    {
        "title": "Singing Pitch Extraction by Voice Vibrato / Tremolo Estimation and Instrument Partial Deletion.",
        "author": [
            "Chao-Ling Hsu",
            "Jyh-Shing Roger Jang"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417357",
        "url": "https://doi.org/10.5281/zenodo.1417357",
        "ee": "https://zenodo.org/records/1417357/files/HsuJ10.pdf",
        "abstract": "This paper proposes a novel and effective approach to extract the pitches of the singing voice from monaural polyphonic songs. The sinusoidal partials of the musical audio signals are first extracted. The Fourier transform is then applied to extract the vibrato/tremolo information of each partial. Some criteria based on this vibrato/tremolo information are employed to discriminate the vocal par- tials from the music accompaniment partials. Besides, a singing pitch trend estimation algorithm which is able to find the global singing progressing tunnel is also pro- posed. The singing pitches can then be extracted more robustly via these two processes. Quantitative evaluation shows that the proposed algorithms significantly improve the raw pitch accuracy of our previous approach and are comparable with other state of the art approaches submit- ted to MIREX.",
        "zenodo_id": 1417357,
        "dblp_key": "conf/ismir/HsuJ10"
    },
    {
        "title": "When Lyrics Outperform Audio for Music Mood Classification: A Feature Analysis.",
        "author": [
            "Xiao Hu 0001",
            "J. Stephen Downie"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415540",
        "url": "https://doi.org/10.5281/zenodo.1415540",
        "ee": "https://zenodo.org/records/1415540/files/HuD10.pdf",
        "abstract": "This paper builds upon and extends previous work on multi-modal mood classification (i.e., combining audio and lyrics) by analyzing in-depth those feature types that have shown to provide statistically significant improve- ments in the classification of individual mood categories. The dataset used in this study comprises 5,296 songs (with lyrics and audio for each) divided into 18 mood categories derived from user-generated tags taken from last.fm. These 18 categories show remarkable consistency with the popular Russell’s mood model. In seven catego- ries, lyric features significantly outperformed audio spec- tral features. In one category only, audio outperformed all lyric features types. A fine grained analysis of the signifi- cant lyric feature types indicates a strong and obvious semantic association between extracted terms and the cat- egories. No such obvious semantic linkages were evident in the case where audio spectral features proved superior.",
        "zenodo_id": 1415540,
        "dblp_key": "conf/ismir/HuD10"
    },
    {
        "title": "Automatic Characterization of Digital Music for Rhythmic Auditory Stimulation.",
        "author": [
            "Eric Humphrey"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418135",
        "url": "https://doi.org/10.5281/zenodo.1418135",
        "ee": "https://zenodo.org/records/1418135/files/Humphrey10.pdf",
        "abstract": "A computational rhythm analysis system is proposed to characterize the suitability of musical recordings for rhyth- mic auditory stimulation, a neurologic music therapy tech- nique that uses rhythm to entrain periodic physical motion. Current applications of RAS are limited by the general in- ability to take advantage of the enormous amount of dig- ital music that exists today. The system aims to identify motor-rhythmic music for the entrainment of neuromuscu- lar activity for rehabilitation and exercise, motivating the concept of musical “use-genres.” This work builds upon prior research in meter and tempo analysis to establish a representation of rhythm chroma and alternatively describe beat spectra.",
        "zenodo_id": 1418135,
        "dblp_key": "conf/ismir/Humphrey10"
    },
    {
        "title": "Upbeat and Quirky, With a Bit of a Build: Interpretive Repertoires in Creative Music Search.",
        "author": [
            "Charlie Inskip",
            "Andy MacFarlane",
            "Pauline Rafferty"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417617",
        "url": "https://doi.org/10.5281/zenodo.1417617",
        "ee": "https://zenodo.org/records/1417617/files/InskipMR10.pdf",
        "abstract": "Pre-existing commercial music is widely used to accom- pany moving images in films, TV commercials and com- puter games. This process is known as music synchronisa- tion. Professionals are employed by rights holders and film makers to perform creative music searches on large catalogues to find appropriate pieces of music for syn- chronisation. This paper discusses a Discourse Analysis of thirty interview texts related to the process. Coded ex- amples are presented and discussed. Four interpretive re- pertoires are identified: the Musical Repertoire, the Soundtrack Repertoire, the Business Repertoire and the Cultural Repertoire. These ways of talking about music are adopted by all of the community regardless of their interest as Music Owner or Music User. Music is shown to have multi-variate and sometimes conflicting meanings within this community which are dynamic and negotiated. This is related to a theoretical feedback model of communication and meaning making which proposes that Owners and Users employ their own and shared ways of talking and thinking about music and its context to determine musical meaning. The value to the music information retrieval community is to inform system design from a user information needs perspective.",
        "zenodo_id": 1417617,
        "dblp_key": "conf/ismir/InskipMR10"
    },
    {
        "title": "Understanding Features and Distance Functions for Music Sequence Alignment.",
        "author": [
            "Özgür Izmirli",
            "Roger B. Dannenberg"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418353",
        "url": "https://doi.org/10.5281/zenodo.1418353",
        "ee": "https://zenodo.org/records/1418353/files/IzmirliD10.pdf",
        "abstract": "We investigate the problem of matching symbolic representations directly to audio based representations for applications that use data from both domains. One such application is score alignment, which aligns a sequence of frames based on features such as chroma vectors and distance functions such as Euclidean distance. Good representations are critical, yet current systems use ad hoc constructions such as the chromagram that have been shown to work quite well. We investigate ways to learn chromagram-like representations that optimize the classification of “matching” vs. “non-matching” frame pairs of audio and MIDI. New representations learned automatically from examples not only perform better than the chromagram representation but they also reveal interesting projection structures that differ distinctly from the traditional chromagram.",
        "zenodo_id": 1418353,
        "dblp_key": "conf/ismir/IzmirliD10"
    },
    {
        "title": "Querying Improvised Music: Do You Sound Like Yourself?.",
        "author": [
            "Michael O. Jewell",
            "Christophe Rhodes",
            "Mark d&apos;Inverno"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417227",
        "url": "https://doi.org/10.5281/zenodo.1417227",
        "ee": "https://zenodo.org/records/1417227/files/JewellRd10.pdf",
        "abstract": "Improvisers are often keen to assess how their performance practice stands up to an ideal: whether that ideal is of tech- nical accuracy or instant composition of material meeting complex harmonic constraints at speed. This paper reports on the development of an interface for querying and navi- gating a collection of recorded material for the purpose of presenting information on musical similarity, and the ap- plication of this interface to the investigation of a set of recordings by jazz performers. We investigate the retrieval performance of our tool, and in analysing the ‘hits’ and particularly the ‘misses’, provide information suggesting a change in one of the authors’ improvisation style.",
        "zenodo_id": 1417227,
        "dblp_key": "conf/ismir/JewellRd10"
    },
    {
        "title": "Melody Extraction from Polyphonic Audio Based on Particle Filter.",
        "author": [
            "Seokhwan Jo",
            "Chang D. Yoo"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414978",
        "url": "https://doi.org/10.5281/zenodo.1414978",
        "ee": "https://zenodo.org/records/1414978/files/JoY10.pdf",
        "abstract": "This paper considers a particle ﬁlter based algorithm to ex- tract melody from a polyphonic audio in the short-time Fourier transforms (STFT) domain. The extraction is fo- cused on overcoming the difﬁculties due to harmonic / per- cussive sound interferences, possibility of octave mismatch, and dynamic variation in melody. The main idea of the al- gorithm is to consider probabilistic relations between melody and polyphonic audio. Melody is assumed to follow a Markov process, and the framed segments of polyphonic audio are assumed to be conditionally independent given the parameters that represent the melody. The melody pa- rameters are estimated using sequential importance sam- pling (SIS) which is a conventional particle ﬁlter method. In this paper, the likelihood and state transition are deﬁned to overcome the aforementioned difﬁculties. The SIS algo- rithm relies on sequential importance density, and this den- sity is designed using multiple pitches which are estimated by a simple multi-pitch extraction algorithm. Experimen- tal results show that the considered algorithm outperforms other famous melody extraction algorithms in terms of the raw pitch accuracy (RPA) and the raw chroma accuracy (RCA).",
        "zenodo_id": 1414978,
        "dblp_key": "conf/ismir/JoY10"
    },
    {
        "title": "An Improved Hierarchical Approach for Music-to-symbolic Score Alignment.",
        "author": [
            "Cyril Joder",
            "Slim Essid",
            "Gaël Richard"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417883",
        "url": "https://doi.org/10.5281/zenodo.1417883",
        "ee": "https://zenodo.org/records/1417883/files/JoderER10.pdf",
        "abstract": "We present an efﬁcient approach for an off-line alignment of a symbolic score to a recording of the same piece, us- ing a statistical model. A hidden state model is built from the score, which allows for the use of two different kinds of features, namely chroma vectors and an onset detec- tion function (spectral ﬂux) with speciﬁc production mod- els, in a simple manner. We propose a hierarchical prun- ing method for an approximate decoding of this statistical model. This strategy reduces the search space in an adap- tive way, yielding a better overall efﬁciency than the tested state-of-the art method. Experiments run on a large database of 94 pop songs show that the resulting system obtains higher recognition rates than the dynamic programming algorithm (DTW), with a signiﬁcantly lower complexity, even though the rhyth- mic information is not used for the alignment.",
        "zenodo_id": 1417883,
        "dblp_key": "conf/ismir/JoderER10"
    },
    {
        "title": "Music Structure Discovery in Popular Music using Non-negative Matrix Factorization.",
        "author": [
            "Florian Kaiser",
            "Thomas Sikora"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418085",
        "url": "https://doi.org/10.5281/zenodo.1418085",
        "ee": "https://zenodo.org/records/1418085/files/KaiserS10.pdf",
        "abstract": "We introduce a method for the automatic extraction of musical structures in popular music. The proposed algo- rithm uses non-negative matrix factorization to segment re- gions of acoustically similar frames in a self-similarity ma- trix of the audio data. We show that over the dimensions of the NMF decomposition, structural parts can easily be modeled. Based on that observation, we introduce a clus- tering algorithm that can explain the structure of the whole music piece. The preliminary evaluation we report in the the paper shows very encouraging results.",
        "zenodo_id": 1418085,
        "dblp_key": "conf/ismir/KaiserS10"
    },
    {
        "title": "Looking Through the &quot;Glass Ceiling&quot;: A Conceptual Framework for the Problems of Spectral Similarity.",
        "author": [
            "Ioannis Karydis",
            "Milos Radovanovic",
            "Alexandros Nanopoulos",
            "Mirjana Ivanovic"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417283",
        "url": "https://doi.org/10.5281/zenodo.1417283",
        "ee": "https://zenodo.org/records/1417283/files/KarydisRNI10.pdf",
        "abstract": "Spectral similarity measures have been shown to exhibit good performance in several Music Information Retrieval (MIR) applications. They are also known, however, to pos- sess several undesirable properties, namely allowing the existence of hub songs (songs which frequently appear in nearest neighbor lists of other songs), “orphans” (songs which practically never appear), and difﬁculties in distin- guishing the farthest from the nearest neighbor due to the concentration effect caused by high dimensionality of data space. In this paper we develop a conceptual framework that allows connecting all three undesired properties. We show that hubs and “orphans” are expected to appear in high-dimensional data spaces, and relate the cause of their appearance with the concentration property of distance / similarity measures. We verify our conclusions on real mu- sic data, examining groups of frames generated by Gaus- sian Mixture Models (GMMs), considering two similar- ity measures: Earth Mover’s Distance (EMD) in combi- nation with Kullback-Leibler (KL) divergence, and Monte Carlo (MC) sampling. The proposed framework can be useful to MIR researchers to address problems of spectral similarity, understand their fundamental origins, and thus be able to develop more robust methods for their remedy.",
        "zenodo_id": 1417283,
        "dblp_key": "conf/ismir/KarydisRNI10"
    },
    {
        "title": "Locating Tune Changes and Providing a Semantic Labelling of Sets of Irish Traditional Tunes.",
        "author": [
            "Cillian Kelly",
            "Mikel Gainza",
            "David Dorran",
            "Eugene Coyle"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418303",
        "url": "https://doi.org/10.5281/zenodo.1418303",
        "ee": "https://zenodo.org/records/1418303/files/KellyGDC10.pdf",
        "abstract": "An approach is presented which provides the tune change loca- tions within a set of Irish Traditional tunes. Also provided are semantic labels for each part of each tune within the set. A set in Irish Traditional music is a number of individual tunes played segue. Each of the tunes in the set are made up of structural seg- ments called parts. Musical variation is a prominent characteris- tic of this genre. However, a certain set of notes known as ‘set accented tones’ are considered impervious to musical variation. Chroma information is extracted at ‘set accented tone’ locations within the music. The resulting chroma vectors are grouped to represent the parts of the music. The parts are then compared with one another to form a part similarity matrix. Unit kernels which represent the possible structures of an Irish Traditional tune are matched with the part similarity matrix to determine the tune change locations and semantic part labels.",
        "zenodo_id": 1418303,
        "dblp_key": "conf/ismir/KellyGDC10"
    },
    {
        "title": "State of the Art Report: Music Emotion Recognition: A State of the Art Review.",
        "author": [
            "Youngmoo E. Kim",
            "Erik M. Schmidt",
            "Raymond Migneco",
            "Brandon G. Morton",
            "Patrick Richardson",
            "Jeffrey J. Scott",
            "Jacquelin A. Speck",
            "Douglas Turnbull"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.6424213",
        "url": "https://doi.org/10.5281/zenodo.6424213",
        "ee": "http://ismir2010.ismir.net/proceedings/ismir2010-45.pdf",
        "abstract": "<p>This deliverable presents models for the impedance of biological synapses and electrode kinetics. Section 3 describes the concept of neural stimulation and possible approaches to the problem. Section 4 focuses on electrical stimulation, the state-of-the-art and specifications for the system to be developed. Section 5 discusses the state-of-the-art of materials for electrodes. Section 6 concludes the deliverable.</p>",
        "zenodo_id": 6424213,
        "dblp_key": "conf/ismir/KimSMMRSST10"
    },
    {
        "title": "Supervised and Unsupervised Web Document Filtering Techniques to Improve Text-Based Music Retrieval.",
        "author": [
            "Peter Knees",
            "Markus Schedl",
            "Tim Pohle",
            "Klaus Seyerlehner",
            "Gerhard Widmer"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417173",
        "url": "https://doi.org/10.5281/zenodo.1417173",
        "ee": "https://zenodo.org/records/1417173/files/KneesSPSW10.pdf",
        "abstract": "We aim at improving a text-based music search engine by applying different techniques to exclude misleading infor- mation from the indexing process. The idea of the original approach is to index music pieces by “contextual” informa- tion, more precisely, by all texts to be found on Web pages retrieved via a common Web search engine. This represen- tation allows for issuing arbitrary textual queries to retrieve relevant music pieces. The goal of this work is to improve precision of the retrieved set of music pieces by ﬁltering out Web pages that lead to irrelevant tracks. To this end we present two unsupervised and two supervised ﬁltering ap- proaches. Evaluation is carried out on two collections pre- viously used in the literature. The obtained results suggest that the proposed ﬁltering techniques can improve results signiﬁcantly but are only effective when applied to large and diverse music collections with millions of Web pages associated.",
        "zenodo_id": 1417173,
        "dblp_key": "conf/ismir/KneesSPSW10"
    },
    {
        "title": "Collaborative Filtering Based on P2P Networks.",
        "author": [
            "Noam Koenigstein",
            "Gert R. G. Lanckriet",
            "Brian McFee",
            "Yuval Shavitt"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415620",
        "url": "https://doi.org/10.5281/zenodo.1415620",
        "ee": "https://zenodo.org/records/1415620/files/KoenigsteinLMS10.pdf",
        "abstract": "Peer-to-Peer (P2P) networks are used by millions of peo- ple for sharing music ﬁles. As these networks become ever more popular, they also serve as an excellent source for Music Information Retrieval (MIR) tasks. This paper re- views the latest MIR studies based on P2P data-sets, and presents a new ﬁle sharing data collection system over the Gnutella. We discuss several advantages of P2P based data-sets over some of the more “traditional” data sources, and evaluate the information quality of our data-set in com- parison to other data sources (Last.fm, social tags, biog- raphy data, and MFCCs). The evaluation is based on an artists similarity task using Partial Order Embedding (POE). We show that a P2P based Collaborative Filtering data- set performs at least as well as “traditional” data-sets, yet maintains some inherent advantages such as scale, avail- ability and additional information features such as ID3 tags and geographical location.",
        "zenodo_id": 1415620,
        "dblp_key": "conf/ismir/KoenigsteinLMS10"
    },
    {
        "title": "On the Applicability of Peer-to-peer Data in Music Information Retrieval Research.",
        "author": [
            "Noam Koenigstein",
            "Yuval Shavitt",
            "Ela Weinsberg",
            "Udi Weinsberg"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414784",
        "url": "https://doi.org/10.5281/zenodo.1414784",
        "ee": "https://zenodo.org/records/1414784/files/KoenigsteinSWW10.pdf",
        "abstract": "Peer-to-Peer (p2p) networks are being increasingly adopted as an invaluable resource for various music information re- trieval (MIR) tasks, including music similarity, recommen- dation and trend prediction. However, these networks are usually extremely large and noisy, which raises doubts re- garding the ability to actually extract sufﬁciently accurate information. This paper evaluates the applicability of using data orig- inating from p2p networks for MIR research, focusing on partial crawling, inherent noise and localization of songs and search queries. These aspects are quantiﬁed using songs collected from the Gnutella p2p network. We show that the power-law nature of the network makes it relatively easy to capture an accurate view of the main-streams using relatively little effort. However, some applications, like trend prediction, mandate collection of the data from the “long tail”, hence a much more exhaustive crawl is needed. Furthermore, we present techniques for overcoming noise originating from user generated content and for ﬁltering non informative data, while minimizing information loss.",
        "zenodo_id": 1414784,
        "dblp_key": "conf/ismir/KoenigsteinSWW10"
    },
    {
        "title": "A Multi-Perspective Evaluation Framework for Chord Recognition.",
        "author": [
            "Verena Konz",
            "Meinard Müller",
            "Sebastian Ewert"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415686",
        "url": "https://doi.org/10.5281/zenodo.1415686",
        "ee": "https://zenodo.org/records/1415686/files/KonzME10.pdf",
        "abstract": "The automated extraction of chord labels from audio recordings constitutes a major task in music information retrieval. To evaluate computer-based chord labeling pro- cedures, one requires ground truth annotations for the un- derlying audio material. However, the manual generation of such annotations on the basis of audio recordings is te- dious and time-consuming. On the other hand, trained mu- sicians can easily derive chord labels from symbolic score data. In this paper, we bridge this gap by describing a pro- cedure that allows for transferring annotations and chord labels from the score domain to the audio domain and vice versa. Using music synchronization techniques, the gen- eral idea is to locally warp the annotations of all given data streams onto a common time axis, which then allows for a cross-domain evaluation of the various types of chord labels. As a further contribution of this paper, we extend this principle by introducing a multi-perspective evaluation framework for simultaneously comparing chord recogni- tion results over multiple performances of the same piece of music. The revealed inconsistencies in the results do not only indicate limitations of the employed chord labeling strategies but also deepen the understanding of the under- lying music material.",
        "zenodo_id": 1415686,
        "dblp_key": "conf/ismir/KonzME10"
    },
    {
        "title": "Unsupervised Accuracy Improvement for Cover Song Detection Using Spectral Connectivity Network.",
        "author": [
            "Mathieu Lagrange",
            "Joan Serrà"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416998",
        "url": "https://doi.org/10.5281/zenodo.1416998",
        "ee": "https://zenodo.org/records/1416998/files/LagrangeS10.pdf",
        "abstract": "This paper introduces a new method for improving the accuracy in medium scale music similarity problems. Re- cently, it has been shown that the raw accuracy of query by example systems can be enhanced by considering pri- ors about the distribution of its output or the structure of the music collection being considered. The proposed ap- proach focuses on reducing the dependency to those priors by considering an eigenvalue decomposition of the afore- mentioned system’s output. Experiments carried out in the framework of cover song detection show that the proposed approach has good performance for enhancing a high accu- racy system. Furthermore, it maintains the accuracy level for lower performing systems.",
        "zenodo_id": 1416998,
        "dblp_key": "conf/ismir/LagrangeS10"
    },
    {
        "title": "Users&apos; Relevance Criteria in Music Retrieval in Everyday Life: An Exploratory Study.",
        "author": [
            "Audrey Laplante"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415578",
        "url": "https://doi.org/10.5281/zenodo.1415578",
        "ee": "https://zenodo.org/records/1415578/files/Laplante10.pdf",
        "abstract": "The paper presents the findings of a qualitative study on the way young adults make relevance inferences about music items when searching for music for recreational purposes. Data were collected through in-depth interviews and analyzed following the constant comparative method. Content analysis revealed that participants used four types of clues to make relevance inferences: bibliographic metadata (e.g., names of contributors, labels), relational metadata (e.g., genres, similar artists), associative meta- data (e.g., cover arts), and recommendations/reviews. Relevance judgments were also found to be influenced by the external context (i.e., the functions music plays in one’s life) and the internal context (i.e., individual tastes and beliefs, state of mind).",
        "zenodo_id": 1415578,
        "dblp_key": "conf/ismir/Laplante10"
    },
    {
        "title": "Crowdsourcing Music Similarity Judgments using Mechanical Turk.",
        "author": [
            "Jin Ha Lee 0001"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416478",
        "url": "https://doi.org/10.5281/zenodo.1416478",
        "ee": "https://zenodo.org/records/1416478/files/Lee10.pdf",
        "abstract": "Collecting human judgments for music similarity evalua- tion has always been a difficult and time consuming task. This paper explores the viability of Amazon Mechanical Turk (MTurk) for collecting human judgments for audio music similarity evaluation tasks. We compared the simi- larity judgments collected from Evalutron6000 (E6K) and MTurk using the Music Information Retrieval Evaluation eXchange 2009 Audio Music Similarity and Retrieval task dataset. Our data show that the results are highly comparable, and MTurk may be a useful method for col- lecting subjective ground truth data. Furthermore, there are several benefits to using MTurk over the traditional E6K infrastructure. We conclude that using MTurk is a practical alternative of music similarity when it is used with some precautions.",
        "zenodo_id": 1416478,
        "dblp_key": "conf/ismir/Lee10"
    },
    {
        "title": "Towards More Robust Geometric Content-Based Music Retrieval.",
        "author": [
            "Kjell Lemström"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415150",
        "url": "https://doi.org/10.5281/zenodo.1415150",
        "ee": "https://zenodo.org/records/1415150/files/Lemstrom10.pdf",
        "abstract": "This paper studies the problem of transposition and time-scale invariant (ttsi) polyphonic music retrieval in symbolically encoded music. In the setting, music is represented by sets of points in plane. We give two new algorithms. Applying a search window of size w and given a query point set, of size m, to be searched for in a database point set, of size n, our algorithm for exact ttsi occurrences runs in O(mwn log n) time; for partial occurrences we have an O(mnw2 log n) algo- rithm. The framework used is ﬂexible allowing devel- opment towards even more robust geometric retrieval.",
        "zenodo_id": 1415150,
        "dblp_key": "conf/ismir/Lemstrom10"
    },
    {
        "title": "Hierarchical Co-Clustering of Artists and Tags.",
        "author": [
            "Jingxuan Li",
            "Tao Li 0001",
            "Mitsunori Ogihara"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416718",
        "url": "https://doi.org/10.5281/zenodo.1416718",
        "ee": "https://zenodo.org/records/1416718/files/LiLO10.pdf",
        "abstract": "The user-assigned tag is a growingly important research topic in MIR. Noticing that some tags are more speciﬁc versions of others, this paper studies the problem of orga- nizing tags into a hierarchical structure by taking into ac- count the fact that the corresponding artists are organized into a hierarchy based on genre and style. A novel clus- tering algorithm, Hierarchical Co-clustering Algorithm (HCC), is proposed as a solution. Unlike traditional hi- erarchical clustering algorithms that deal with homoge- neous data only, the proposed algorithm simultaneously organizes two distinct data types into hierarchies. HCC is additionally able to receive constraints that state certain ob- jects “must-be-together” or “should-be-together” and build clusters so as to satisfying the constraints. HCC may lead to better and deeper understandings of relationship between artists and tags assigned to them. An experiment ﬁnds that by trying to hierarchically cluster the two types of data better clusters are obtained for both. It is also shown that HCC is able to incorporate instance-level constraints on artists and/or tags to improve the clustering process.",
        "zenodo_id": 1416718,
        "dblp_key": "conf/ismir/LiLO10"
    },
    {
        "title": "A Cartesian Ensemble of Feature Subspace Classifiers for Music Categorization.",
        "author": [
            "Thomas Lidy",
            "Rudolf Mayer",
            "Andreas Rauber",
            "Pedro J. Ponce de León",
            "Antonio Pertusa",
            "José Manuel Iñesta Quereda"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415598",
        "url": "https://doi.org/10.5281/zenodo.1415598",
        "ee": "https://zenodo.org/records/1415598/files/LidyMRLPQ10.pdf",
        "abstract": "We present a cartesian ensemble classiﬁcation system that is based on the principle of late fusion and feature sub- spaces. These feature subspaces describe different aspects of the same data set. The framework is built on the Weka machine learning toolkit and able to combine arbitrary fea- ture sets and learning schemes. In our scenario, we use it for the ensemble classiﬁcation of multiple feature sets from the audio and symbolic domains. We present an extensive set of experiments in the context of music genre classiﬁ- cation, based on numerous Music IR benchmark datasets, and evaluate a set of combination/voting rules. The results show that the approach is superior to the best choice of a single algorithm on a single feature set. Moreover, it also releases the user from making this choice explicitly.",
        "zenodo_id": 1415598,
        "dblp_key": "conf/ismir/LidyMRLPQ10"
    },
    {
        "title": "Boosting for Multi-Modal Music Emotion Classification.",
        "author": [
            "Qi Lu",
            "Xiaoou Chen",
            "Deshun Yang",
            "Jun Wang"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417085",
        "url": "https://doi.org/10.5281/zenodo.1417085",
        "ee": "https://zenodo.org/records/1417085/files/LuCYW10.pdf",
        "abstract": "With the explosive growth of music recordings, automatic classification of music emotion becomes one of the hot spots on research and engineering. Typical music emotion classification (MEC) approaches apply machine learning",
        "zenodo_id": 1417085,
        "dblp_key": "conf/ismir/LuCYW10"
    },
    {
        "title": "Accurate Real-time Windowed Time Warping.",
        "author": [
            "Robert Macrae",
            "Simon Dixon"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416156",
        "url": "https://doi.org/10.5281/zenodo.1416156",
        "ee": "https://zenodo.org/records/1416156/files/MacraeD10.pdf",
        "abstract": "Dynamic Time Warping (DTW) is used to ﬁnd alignments between two related streams of information and can be used to link data, recognise patterns or ﬁnd similarities. Typically, DTW requires the complete series of both in- put streams in advance and has quadratic time and space requirements. As such DTW is unsuitable for real-time applications and is inefﬁcient for aligning long sequences. We present Windowed Time Warping (WTW), a variation on DTW that, by dividing the path into a series of DTW windows and making use of path cost estimation, achieves alignments with an accuracy and efﬁciency superior to other leading modiﬁcations and with the capability of synchro- nising in real-time. We demonstrate this method in a score following application. Evaluation of the WTW score fol- lowing system found 97.0% of audio note onsets were cor- rectly aligned within 2000 ms of the known time. Results also show reductions in execution times over state-of-the- art efﬁcient DTW modiﬁcations.",
        "zenodo_id": 1416156,
        "dblp_key": "conf/ismir/MacraeD10"
    },
    {
        "title": "Query-by-conducting: An Interface to Retrieve Classical-music Interpretations by Real-time Tempo Input.",
        "author": [
            "Akira Maezawa",
            "Masataka Goto",
            "Hiroshi G. Okuno"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416614",
        "url": "https://doi.org/10.5281/zenodo.1416614",
        "ee": "https://zenodo.org/records/1416614/files/MaezawaGO10.pdf",
        "abstract": "This paper presents an interface for ﬁnding interpretations of a user-speciﬁed music, Query-by-Conducting. In clas- sical music, there are many interpretations to a particular piece, and ﬁnding “the” interpretation that matches the lis- tener’s taste allows a listener to further enjoy the piece. The critical issue in ﬁnding such an interpretation is the way or interface to allow the listener to listen through differ- ent interpretations. Our interface allows a user, by swing- ing a conducting hardware interface, to conduct the desired global tempo along the playback of a piece, at any time in the piece. The real-time conducting input by the user dy- namically switches the interpretation being played back to the one closest to how the user is currently conducting. At the end of the piece, our interface ranks each interpretation according to how close the tempo of each interpretation was to the user input. At the core of our interface is an automated tempo es- timation method based on audio-score alignment. We im- prove tempo estimation by requiring the audio-score align- ment of different interpretations to be consistent with each other. We evaluate the tempo estimation method using a solo, chamber, and orchestral repertoire. The proposed tempo estimation decreases the error by as much as 0.94 times the original error.",
        "zenodo_id": 1416614,
        "dblp_key": "conf/ismir/MaezawaGO10"
    },
    {
        "title": "Similarity Measures for Chinese Pop Music Based on Low-level Audio Signal Attributes.",
        "author": [
            "Chun-Man Mak",
            "Tan Lee 0001",
            "Suman Senapati",
            "Yu Ting Yeung",
            "Wang-Kong Lam"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415636",
        "url": "https://doi.org/10.5281/zenodo.1415636",
        "ee": "https://zenodo.org/records/1415636/files/MakLSYL10.pdf",
        "abstract": "In this article a method of computing similarity of two Chinese pop songs is presented. It is based on five attributes extracted from the audio signal. They include music instrument, singing voice style, singer gender, tempo, and degree of noisiness. We compare the com- puted similarity measures with similarity scores obtained with subjective listening by over 200 human subjects. The results show that rhythm and mood related attributes like tempo and degree of noisiness are most correlated to human perception of Chinese pop songs. Instrument and singing style are relatively less relevant. The results of subjective evaluation also indicate that the proposed method of similarity computation is fairly correlated with human perception.",
        "zenodo_id": 1415636,
        "dblp_key": "conf/ismir/MakLSYL10"
    },
    {
        "title": "Learning Tags that Vary Within a Song.",
        "author": [
            "Michael I. Mandel",
            "Douglas Eck",
            "Yoshua Bengio"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416130",
        "url": "https://doi.org/10.5281/zenodo.1416130",
        "ee": "https://zenodo.org/records/1416130/files/MandelEB10.pdf",
        "abstract": "This paper examines the relationship between human gener- ated tags describing different parts of the same song. These tags were collected using Amazon’s Mechanical Turk ser- vice. We ﬁnd that the agreement between different people’s tags decreases as the distance between the parts of a song that they heard increases. To model these tags and these relationships, we describe a conditional restricted Boltz- mann machine. Using this model to ﬁll in tags that should probably be present given a context of other tags, we train automatic tag classiﬁers (autotaggers) that outperform those trained on the original data.",
        "zenodo_id": 1416130,
        "dblp_key": "conf/ismir/MandelEB10"
    },
    {
        "title": "It&apos;s Time for a Song - Transcribing Recordings of Bell-playing Clocks.",
        "author": [
            "Matija Marolt",
            "Marieke Lefeber"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416704",
        "url": "https://doi.org/10.5281/zenodo.1416704",
        "ee": "https://zenodo.org/records/1416704/files/MaroltL10.pdf",
        "abstract": "The paper presents an algorithm for automatic transcrip- tion of recordings of bell-playing clocks. Bell-playing clocks are clocks containing a hidden bell-playing me- chanism that is periodically activated to play a melody. Clocks from the eighteenth century give us unique insight into the musical taste of their owners, so we are interested in studying their repertoire and performances - thus the need for automatic transcription. In the paper, we first present an analysis of acoustical properties of bells found in bell-playing clocks. We propose a model that describes positions of bell partials and an algorithm that discovers the number of bells and positions of their partials in a given recording. To transcribe a recording, we developed a probabilistic method that maximizes the joint probabili- ty of a note sequence given the recording and positions of bell partials. Finally, we evaluate our algorithms on a set of recordings of bell-playing clocks.",
        "zenodo_id": 1416704,
        "dblp_key": "conf/ismir/MaroltL10"
    },
    {
        "title": "Recognition of Variations Using Automatic Schenkerian Reduction.",
        "author": [
            "Alan Marsden"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418179",
        "url": "https://doi.org/10.5281/zenodo.1418179",
        "ee": "https://zenodo.org/records/1418179/files/Marsden10.pdf",
        "abstract": "Experiments on techniques to automatically recognise whether or not an extract of music is a variation of a giv- en theme are reported, using a test corpus derived from ten of Mozart‘s sets of variations for piano. Methods which examine the notes of the ‗surface‘ are compared with methods which make use of an automatically derived quasi-Schenkerian reduction of the theme and the extract in question. The maximum average F-measure achieved was 0.87. Unexpectedly, this was for a method of match- ing based on the surface alone, and in general the results for matches based on the surface were marginally better than those based on reduction, though the small number of possible test queries means that this result cannot be regarded as conclusive. Other inferences on which factors seem to be important in recognising variations are dis- cussed. Possibilities for improved recognition of match- ing using reduction are outlined.",
        "zenodo_id": 1418179,
        "dblp_key": "conf/ismir/Marsden10"
    },
    {
        "title": "YAAFE, an Easy to Use and Efficient Audio Feature Extraction Software.",
        "author": [
            "Benoît Mathieu",
            "Slim Essid",
            "Thomas Fillon",
            "Jacques Prado",
            "Gaël Richard"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418321",
        "url": "https://doi.org/10.5281/zenodo.1418321",
        "ee": "https://zenodo.org/records/1418321/files/MathieuEFPR10.pdf",
        "abstract": "Music Information Retrieval systems are commonly built on a feature extraction stage. For applications involv- ing automatic classiﬁcation (e.g. speech/music discrimi- nation, music genre or mood recognition, ...), traditional approaches will consider a large set of audio features to be extracted on a large dataset. In some cases, this will lead to computationally intensive systems and there is, therefore, a strong need for efﬁcient feature extraction. In this paper, a new audio feature extraction software, YAAFE 1 , is presented and compared to widely used li- braries. The main advantage of YAAFE is a signiﬁcantly lower complexity due to the appropriate exploitation of re- dundancy in the feature calculation. YAAFE remains easy to conﬁgure and each feature can be parameterized inde- pendently. Finally, the YAAFE framework and most of its core feature library are released in source code under the GNU Lesser General Public License.",
        "zenodo_id": 1418321,
        "dblp_key": "conf/ismir/MathieuEFPR10"
    },
    {
        "title": "Approximate Note Transcription for the Improved Identification of Difficult Chords.",
        "author": [
            "Matthias Mauch",
            "Simon Dixon"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416598",
        "url": "https://doi.org/10.5281/zenodo.1416598",
        "ee": "https://zenodo.org/records/1416598/files/MauchD10.pdf",
        "abstract": "The automatic detection and transcription of musical chords from audio is an established music computing task. The choice of chord proﬁles and higher-level time-series modelling have received a lot of attention, resulting in",
        "zenodo_id": 1416598,
        "dblp_key": "conf/ismir/MauchD10"
    },
    {
        "title": "Learning Similarity from Collaborative Filters.",
        "author": [
            "Brian McFee",
            "Luke Barrington",
            "Gert R. G. Lanckriet"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416198",
        "url": "https://doi.org/10.5281/zenodo.1416198",
        "ee": "https://zenodo.org/records/1416198/files/McFeeBL10.pdf",
        "abstract": "Collaborative ﬁltering methods (CF) exploit the wis- dom of crowds to capture deeply structured similarities in musical objects, such as songs, artists or albums. When CF is available, it frequently outperforms content-based",
        "zenodo_id": 1416198,
        "dblp_key": "conf/ismir/McFeeBL10"
    },
    {
        "title": "Evaluating the Genre Classification Performance of Lyrical Features Relative to Audio, Symbolic and Cultural Features.",
        "author": [
            "Cory McKay",
            "John Ashley Burgoyne",
            "Jason Hockman",
            "Jordan B. L. Smith",
            "Gabriel Vigliensoni",
            "Ichiro Fujinaga"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415706",
        "url": "https://doi.org/10.5281/zenodo.1415706",
        "ee": "https://zenodo.org/records/1415706/files/McKayBHSVF10.pdf",
        "abstract": "This paper describes experimental research investigating the genre classification utility of combining features ex- tracted from lyrical, audio, symbolic and cultural sources of musical information. It was found that cultural features consisting of information extracted from both web searches and mined listener tags were particularly effec- tive, with the result that classification accuracies were achieved that compare favorably with the current state of the art of musical genre classification. It was also found that features extracted from lyrics were less effective than the other feature types. Finally, it was found that, with some exceptions, combining feature types does improve classification performance. The new lyricFetcher and jLyrics software are also presented as tools that can be used as a framework for developing more effective classi- fication methodologies based on lyrics in the future.",
        "zenodo_id": 1415706,
        "dblp_key": "conf/ismir/McKayBHSVF10"
    },
    {
        "title": "Geoshuffle: Location-Aware, Content-based Music Browsing Using Self-organizing Tag Clouds.",
        "author": [
            "Scott Miller",
            "Paul Reimer",
            "Steven R. Ness",
            "George Tzanetakis"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417703",
        "url": "https://doi.org/10.5281/zenodo.1417703",
        "ee": "https://zenodo.org/records/1417703/files/MillerRNT10.pdf",
        "abstract": "In the past few years the computational capabilities of mobile phones have been constantly increasing. Frequently these smartphones are also used as portable music players. In this paper we describe GeoShufﬂe – a prototype system for content-based music browsing and exploration that tar- gets such devices. One of the most interesting aspects of these portable devices is the inclusion of positioning ca- pabilities based on GPS. GeoShufﬂe adds location-based and time-based context to a user’s listening preferences. Playlists are dynamically generated based on the location of the user, path and historical preferences. Browsing large music collections having thousands of tracks is challenging. The most common method of inter- action is using long lists of textual metadata such as artist name or genre. Current smartphones are characterized by small screen real-estate which limits the amount of tex- tual information that can be displayed. We propose self- organizing tag clouds, a 2D tag cloud representation that is based on an underlying self-organizing map calculated using automatically extracted audio features. To evalute the system the Magnatagatune database is utilized. The evaluation indicates that location and time context can im- prove the quality of music recommendation and that self- organizing tag clouds provide faster browsing and are more engaging than text-based tag clouds.",
        "zenodo_id": 1417703,
        "dblp_key": "conf/ismir/MillerRNT10"
    },
    {
        "title": "Improving Auto-tagging by Modeling Semantic Co-occurrences.",
        "author": [
            "Riccardo Miotto",
            "Luke Barrington",
            "Gert R. G. Lanckriet"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415590",
        "url": "https://doi.org/10.5281/zenodo.1415590",
        "ee": "https://zenodo.org/records/1415590/files/MiottoBL10.pdf",
        "abstract": "Automatic taggers describe music in terms of a multino- mial distribution over relevant semantic concepts. This pa- per presents a framework for improving automatic tagging of music content by modeling contextual relationships be- tween these semantic concepts. The framework extends ex- isting auto-tagging methods by adding a Dirichlet mixture to model the contextual co-occurrences between semantic multinomials. Experimental results show that adding con- text improves automatic annotation and retrieval of music and demonstrate that the Dirichlet mixture is an appropri- ate model for capturing co-occurrences between semantics.",
        "zenodo_id": 1415590,
        "dblp_key": "conf/ismir/MiottoBL10"
    },
    {
        "title": "A Probabilistic Approach to Merge Context and Content Information for Music Retrieval.",
        "author": [
            "Riccardo Miotto",
            "Nicola Orio"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415256",
        "url": "https://doi.org/10.5281/zenodo.1415256",
        "ee": "https://zenodo.org/records/1415256/files/MiottoO10.pdf",
        "abstract": "An interesting problem in music information retrieval is how to combine the information from different sources in order to improve retrieval effectiveness. This paper intro- duces an approach to represent a collection of tagged songs through an hidden Markov model with the purpose to de- velop a system that merges in the same framework both acoustic similarity and semantic descriptions. The former provides content-based information on song similarity, the latter provides context-aware information about individ- ual songs. Experimental results show how the proposed model leads to better performances than approaches that rank songs using both a single information source and a their linear combination.",
        "zenodo_id": 1415256,
        "dblp_key": "conf/ismir/MiottoO10"
    },
    {
        "title": "Evidence for Pianist-specific Rubato Style in Chopin Nocturnes.",
        "author": [
            "Miguel Molina-Solana",
            "Maarten Grachten",
            "Gerhard Widmer"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417793",
        "url": "https://doi.org/10.5281/zenodo.1417793",
        "ee": "https://zenodo.org/records/1417793/files/Molina-SolanaGW10.pdf",
        "abstract": "The performance of music usually involves a great deal of interpretation by the musician. In classical music, the ﬁnal ritardando is a good example of the expressive aspect of music performance. Even though expressive timing data is expected to have a strong component that is determined by the piece itself, in this paper we investigate to what de- gree individual performance style has an effect on the tim- ing of ﬁnal ritardandi. The particular approach taken here uses Friberg and Sundberg’s kinematic rubato model in or- der to characterize performed ritardandi. Using a machine- learning classiﬁer, we carry out a pianist identiﬁcation task to assess the suitability of the data for characterizing the in- dividual playing style of pianists. The results indicate that in spite of an extremely reduced data representation, when cancelling the piece-speciﬁc aspects, pianists can often be identiﬁed with accuracy above baseline. This fact suggests the existence of a performer-speciﬁc style of playing ritar- dandi.",
        "zenodo_id": 1417793,
        "dblp_key": "conf/ismir/Molina-SolanaGW10"
    },
    {
        "title": "Characterization and Similarity in A Cappella Flamenco Cantes.",
        "author": [
            "Joaquín Mora",
            "Francisco Gómez 0001",
            "Emilia Gómez",
            "Francisco Escobar-Borrego",
            "José Miguel Díaz-Báñez"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415242",
        "url": "https://doi.org/10.5281/zenodo.1415242",
        "ee": "https://zenodo.org/records/1415242/files/MoraGGED10.pdf",
        "abstract": "This paper intends to research on the link between musi- cal similarity and style and sub-style (variant) classifica- tion in the context of flamenco a cappella singing styles. Given the limitation of standard computational models for melodic characterization and similarity computation in this particular context, we have proposed a specific set of melodic features adapted to flamenco singing styles. In order to evaluate them, we have gathered a collection of music recordings from the most representative singers and have manually extracted those proposed features. Based on those features, we have defined a similarity measure between two performances and have validated their usefulness in differentiating several styles and vari- ants. The main conclusion of this work is the need to in- corporate specific musical features to the design of simi- larity measures for flamenco music so that flamenco- adapted MIR systems can be developed.",
        "zenodo_id": 1415242,
        "dblp_key": "conf/ismir/MoraGGED10"
    },
    {
        "title": "Monophonic Instrument Sound Segregation by Clustering NMF Components Based on Basis Similarity and Gain Disjointness.",
        "author": [
            "Kazuma Murao",
            "Masahiro Nakano",
            "Yu Kitano",
            "Nobutaka Ono",
            "Shigeki Sagayama"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417113",
        "url": "https://doi.org/10.5281/zenodo.1417113",
        "ee": "https://zenodo.org/records/1417113/files/MuraoNKOS10.pdf",
        "abstract": "This paper discusses a method for monophonic instrument sound separation based on nonnegative matrix factoriza- tion (NMF). In general, it is not easy to classify NMF com- ponents into each instrument. By contrast, monophonic in- strument sound gives us an important clue to classify them, because no more than one sound would be activated simul- taneously. Our approach is to classify NMF components into each instrument based on basis spectrum vector sim- ilarity and temporal activity disjointness. Our clustering employs a hierarchical clustering algorithm: group average method (GAM). The efﬁciency of our approach is evalu- ated by some experiments.",
        "zenodo_id": 1417113,
        "dblp_key": "conf/ismir/MuraoNKOS10"
    },
    {
        "title": "A Multi-pass Algorithm for Accurate Audio-to-Score Alignment.",
        "author": [
            "Bernhard Niedermayer",
            "Gerhard Widmer"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415910",
        "url": "https://doi.org/10.5281/zenodo.1415910",
        "ee": "https://zenodo.org/records/1415910/files/NiedermayerW10.pdf",
        "abstract": "Most current audio-to-score alignment algorithms work on the level of score time frames; i.e., they cannot differen- tiate between several notes occurring at the same discrete time within the score. This level of accuracy is sufﬁcient for a variety of applications. However, for those that deal with, for example, musical expression analysis such micro- timings might also be of interest. Therefore, we propose a method that estimates the onset times of individual notes in a post-processing step. Based on the initial alignment and a feature obtained by matrix factorization, those notes for which the conﬁdence in the alignment is high are chosen as anchor notes. The remaining notes in between are re- vised, taking into account the additional information about these anchors and the temporal relations given by the score. We show that this method clearly outperforms a reference method that uses the same features but does not differenti- ate between anchor and non-anchor notes.",
        "zenodo_id": 1415910,
        "dblp_key": "conf/ismir/NiedermayerW10"
    },
    {
        "title": "IBT: A Real-time Tempo and Beat Tracking System.",
        "author": [
            "João Lobato Oliveira",
            "Fabien Gouyon",
            "Luis Gustavo Martins",
            "Luís Paulo Reis"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416470",
        "url": "https://doi.org/10.5281/zenodo.1416470",
        "ee": "https://zenodo.org/records/1416470/files/OliveiraGMR10.pdf",
        "abstract": "This paper describes a tempo induction and beat track- ing system based on the efﬁcient strategy (initially intro- duced in the BeatRoot system [Dixon S., “Automatic ex- traction of tempo and beat from expressive performances.” Journal of New Music Research, 30(1):39-58, 2001]) of competing agents processing musical input sequentially and considering parallel hypotheses regarding tempo and beats. In this paper, we propose to extend this strategy to the causal processing of continuous input data. The main rea- sons for this are threefold: providing more robustness to potentially noisy input data, permitting the parallel consid- eration of a number of low-level frame-based features as input, and opening the way to real-time uses of the system (as e.g. for a mobile robotic platform). The system is implemented in C++, permitting faster than real-time processing of audio data. It is integrated in the MARSYAS framework, and is therefore available under GPL for users and/or researchers. Detailed evaluation of the causal and non-causal ver- sions of the system on common benchmark datasets show performances reaching those of state-of-the-art beat track- ers. We propose a series of lines for future work based on careful analysis of the results.",
        "zenodo_id": 1416470,
        "dblp_key": "conf/ismir/OliveiraGMR10"
    },
    {
        "title": "Sparse Multi-label Linear Embedding Within Nonnegative Tensor Factorization Applied to Music Tagging.",
        "author": [
            "Yannis Panagakis",
            "Constantine Kotropoulos",
            "Gonzalo R. Arce"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417036",
        "url": "https://doi.org/10.5281/zenodo.1417036",
        "ee": "https://zenodo.org/records/1417036/files/PanagakisKA10.pdf",
        "abstract": "A novel framework for music tagging is proposed. First, each music recording is represented by bio-inspired audi- tory temporal modulations. Then, a multilinear subspace learning algorithm based on sparse label coding is devel- oped to effectively harness the multi-label information for dimensionality reduction. The proposed algorithm is re- ferred to as Sparse Multi-label Linear Embedding Non- negative Tensor Factorization, whose convergence to a sta- tionary point is guaranteed. Finally, a recently proposed method is employed to propagate the multiple labels of training auditory temporal modulations to auditory tem- poral modulations extracted from a test music recording by means of the sparse ℓ1 reconstruction coefﬁcients. The overall framework, that is described here, outperforms both humans and state-of-the-art computer audition systems in the music tagging task, when applied to the CAL500 dataset.",
        "zenodo_id": 1417036,
        "dblp_key": "conf/ismir/PanagakisKA10"
    },
    {
        "title": "Improving Markov Model Based Music Piece Structure Labelling with Acoustic Information.",
        "author": [
            "Jouni Paulus"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416732",
        "url": "https://doi.org/10.5281/zenodo.1416732",
        "ee": "https://zenodo.org/records/1416732/files/Paulus10.pdf",
        "abstract": "This paper proposes using acoustic information in the la- belling of music piece structure descriptions. Here, mu- sic piece structure means the sectional form of the piece: temporal segmentation and grouping to parts such as cho- rus or verse. The structure analysis methods rarely pro- vide the parts with musically meaningful names. The pro- posed method labels the parts in a description. The base- line method models the sequential dependencies between musical parts with N-grams and uses them for the labelling. The acoustic model proposed in this paper is based on the assumption that the parts with the same label even in dif- ferent pieces share some acoustic properties compared to other parts in the same pieces. The proposed method uses mean and standard deviation of relative loudness in a part as the feature which is then modelled with a single multi- variate Gaussian distribution. The method is evaluated on three data sets of popular music pieces, and in all of them the inclusion of the acoustic model improves the labelling accuracy over the baseline method.",
        "zenodo_id": 1416732,
        "dblp_key": "conf/ismir/Paulus10"
    },
    {
        "title": "State of the Art Report: Audio-Based Music Structure Analysis.",
        "author": [
            "Jouni Paulus",
            "Meinard Müller",
            "Anssi Klapuri"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.13926925",
        "url": "https://doi.org/10.5281/zenodo.13926925",
        "ee": "http://ismir2010.ismir.net/proceedings/ismir2010-107.pdf",
        "abstract": "<p><strong><span>Abstract:</span></strong><span> In this article, information is given about Yunus Rajabi's life path, his role in Uzbek music, and his work as a composer. Opinions were also expressed about the creative works created by Yunus Rajabi and their content and role in our national music today.</span></p>\n<p><strong><span>Key words:</span></strong><span> hafiz, playing instruments, music school, creation, work, melody, song</span></p>\n<p><span>&nbsp;</span></p>",
        "zenodo_id": 13926925,
        "dblp_key": "conf/ismir/PaulusMK10"
    },
    {
        "title": "Eigenvector-based Relational Motif Discovery.",
        "author": [
            "Alberto Pinto"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415906",
        "url": "https://doi.org/10.5281/zenodo.1415906",
        "ee": "https://zenodo.org/records/1415906/files/Pinto10.pdf",
        "abstract": "The development of novel analytical tools to investigate the structure of music works is central in current music information retrieval research. In particular, music sum- marization aims at ﬁnding the most representative parts of a music piece (motifs) that can be exploited for an efﬁ- cient music database indexing system. Here we present a novel approach for motif discovery in music pieces based on an eigenvector method. Scores are segmented into a network of bars and then ranked depending on their cen- trality. Bars with higher centrality are more likely to be relevant for music summarization. Results on the corpus of J.S.Bach’s 2-part Inventions demonstrate the effectiveness of the method and suggest that different musical metrics might be more suitable than others for different applica- tions.",
        "zenodo_id": 1415906,
        "dblp_key": "conf/ismir/Pinto10"
    },
    {
        "title": "Multiple Pitch Transcription using DBN-based Musicological Models.",
        "author": [
            "Stanislaw Andrzej Raczynski",
            "Emmanuel Vincent 0001",
            "Frédéric Bimbot",
            "Shigeki Sagayama"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415198",
        "url": "https://doi.org/10.5281/zenodo.1415198",
        "ee": "https://zenodo.org/records/1415198/files/RaczynskiVBS10.pdf",
        "abstract": "We propose a novel approach to solve the problem of estimating pitches of notes present in an audio signal. We have  developed  a  probabilistically  rigorous model  that takes  into  account  temporal  dependencies  between musical notes and between the underlying chords, as well as the instantaneous dependencies between chords, notes and  the  observed  note  saliences.  We  investigated  its modeling  ability  by  measuring  the  cross-entropy  with symbolic (MIDI) data and then proceed to observe the model's performance in multiple pitch estimation of audio data.",
        "zenodo_id": 1415198,
        "dblp_key": "conf/ismir/RaczynskiVBS10"
    },
    {
        "title": "Symbol Classification Approach for OMR of Square Notation Manuscripts.",
        "author": [
            "Carolina Ramirez",
            "Jun Ohya"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415122",
        "url": "https://doi.org/10.5281/zenodo.1415122",
        "ee": "https://zenodo.org/records/1415122/files/RamirezO10.pdf",
        "abstract": "Researchers in the field of OMR (Optical Music Recogni- tion) have acknowledged that the automatic transcription of medieval musical manuscripts is still an open problem [2, 3], mainly due to lack of standards in notation and the physical quality of the documents. Nonetheless, the amount of medieval musical manuscripts is so vast that the consensus seems to be that OMR can be a vital tool to help in the preserving and sharing of this information in digital format. In this paper we report our results on a preliminary approach to OMR of medieval plainchant manuscripts in square notation, at the symbol classification level, which produced good results in the recognition of eight basic symbols. Our preliminary approach consists of the pre- processing, segmentation, and classification stages.",
        "zenodo_id": 1415122,
        "dblp_key": "conf/ismir/RamirezO10"
    },
    {
        "title": "Concurrent Estimation of Chords and Keys from Audio.",
        "author": [
            "Thomas Rocher",
            "Matthias Robine",
            "Pierre Hanna",
            "Laurent Oudre"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417485",
        "url": "https://doi.org/10.5281/zenodo.1417485",
        "ee": "https://zenodo.org/records/1417485/files/RocherRHO10.pdf",
        "abstract": "This paper proposes a new method for local key and chord estimation from audio signals. A harmonic content of the musical piece is ﬁrst extracted by computing a set of chroma vectors. Correlation with ﬁxed chord and key templates then selects a set of key/chord pairs for every frame. A weighted acyclic harmonic graph is then built with these pairs as vertices, and the use of a musical distance to weigh its edges. Finally, the output sequences of chords and keys are obtained by ﬁnding the best path in the graph. The proposed system allows a mutual and beneﬁcial chord and key estimation. It is evaluated on a corpus com- posed of Beatles songs for both the local key estimation and chord recognition tasks. Results show that it performs better than state-of-the art chord analysis algorithms while providing a more complete harmonic analysis.",
        "zenodo_id": 1417485,
        "dblp_key": "conf/ismir/RocherRHO10"
    },
    {
        "title": "Autoregressive MFCC Models for Genre Classification Improved by Harmonic-percussion Separation.",
        "author": [
            "Halfdan Rump",
            "Shigeki Miyabe",
            "Emiru Tsunoo",
            "Nobutaka Ono",
            "Shigeki Sagayama"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418239",
        "url": "https://doi.org/10.5281/zenodo.1418239",
        "ee": "https://zenodo.org/records/1418239/files/RumpMTOS10.pdf",
        "abstract": "In this work we improve accuracy of MFCC-based genre classiﬁcation by using the Harmonic-Percussion Signal Sep- aration (HPSS) algorithm on the music signal, and then calculate the MFCCs on the separated signals. The choice of the HPSS algorithm was mainly based on the observa- tion that the presence of harmonics causes the high MFCCs to be noisy. A multivariate autoregressive (MAR) model was trained on the improved MFCCs, and performance in the task of genre classiﬁcation was evaluated. By combin- ing features calculated on the separated signals, relative er- ror rate reductions of 20% and 16.2% were obtained when an SVM classiﬁer was trained on the MFCCs and MAR features respectively. Next, by analyzing the MAR features calculated on the separated signals, it was concluded that the original signal contained some information which the MAR model was capable of handling, and that the best per- formance was obtained when all three signals were used. Finally, by choosing the number of MFCCs from each sig- nal type to be used in the autoregressive modelling, it was veriﬁed that the best performance was reached when the high MFCCs calculated on the harmonic signal were dis- carded.",
        "zenodo_id": 1418239,
        "dblp_key": "conf/ismir/RumpMTOS10"
    },
    {
        "title": "The Standardized Variogram as a Novel Tool for Music Similarity Evaluation.",
        "author": [
            "Simone Sammartino",
            "Lorenzo J. Tardón",
            "Cristina de la Bandera",
            "Isabel Barbancho",
            "Ana M. Barbancho"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417195",
        "url": "https://doi.org/10.5281/zenodo.1417195",
        "ee": "https://zenodo.org/records/1417195/files/SammartinoTBBB10.pdf",
        "abstract": "Most of methods for audio similarity evaluation are based on the Mel frequency cepstral coefﬁcients, employed as main tool for the characterization of audio contents. Such approach needs some way of data compression aimed to optimize the information retrieval task and to reduce the computational costs derived from the usage of cluster ana- lysis tools and probabilistic models. A novel approach is presented in this paper, based on the standardized vario- gram. This tool, inherited from Geostatistics, is applied to MFCCs matrices to reduce their size and compute compact representations of the audio contents (song signatures), ai- med to evaluate audio similarity. The performance of the proposed approach is analyzed in comparison with other alternative methods and on the base of human responses.",
        "zenodo_id": 1417195,
        "dblp_key": "conf/ismir/SammartinoTBBB10"
    },
    {
        "title": "On the Use of Microblogging Posts for Similarity Estimation and Artist Labeling.",
        "author": [
            "Markus Schedl"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418215",
        "url": "https://doi.org/10.5281/zenodo.1418215",
        "ee": "https://zenodo.org/records/1418215/files/Schedl10.pdf",
        "abstract": "Microblogging services, such as Twitter, have risen enor- mously in popularity during the past years. Despite their popularity, such services have never been analyzed for MIR purposes, to the best of our knowledge. We hence present ﬁrst investigations of the usability of music artist-related microblogging posts to perform artist labeling and simi- larity estimation tasks. To this end, we look into different text-based indexing models and term weighting measures. Two artist collections are used for evaluation, and the dif- ferent methods are evaluated against data from last.fm. We show that microblogging posts are a valuable source for musical meta-data.",
        "zenodo_id": 1418215,
        "dblp_key": "conf/ismir/Schedl10"
    },
    {
        "title": "What&apos;s Hot? Estimating Country-specific Artist Popularity.",
        "author": [
            "Markus Schedl",
            "Tim Pohle",
            "Noam Koenigstein",
            "Peter Knees"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415870",
        "url": "https://doi.org/10.5281/zenodo.1415870",
        "ee": "https://zenodo.org/records/1415870/files/SchedlPKK10.pdf",
        "abstract": "Predicting artists that are popular in certain regions of the world is a well desired task, especially for the music indus- try. Also the cosmopolitan and cultural-aware music aﬁ- cionado is likely be interested in which music is currently “hot” in other parts of the world. We therefore propose four approaches to determine artist popularity rankings on the country-level. To this end, we mine the following data sources: page counts from Web search engines, user posts on Twitter, shared folders on the Gnutella ﬁle sharing net- work, and playcount data from last.fm. We propose meth- ods to derive artist rankings based on these four sources and perform cross-comparison of the resulting rankings via overlap scores. We further elaborate on the advantages and disadvantages of all approaches as they yield interestingly diverse results.",
        "zenodo_id": 1415870,
        "dblp_key": "conf/ismir/SchedlPKK10"
    },
    {
        "title": "Prediction of Time-varying Musical Mood Distributions from Audio.",
        "author": [
            "Erik M. Schmidt",
            "Youngmoo E. Kim"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416238",
        "url": "https://doi.org/10.5281/zenodo.1416238",
        "ee": "https://zenodo.org/records/1416238/files/SchmidtK10.pdf",
        "abstract": "The appeal of music lies in its ability to express emotions, and it is natural for us to organize music in terms of emo- tional associations. But the ambiguities of emotions make the determination of a single, unequivocal response label for the mood of a piece of music unrealistic. We address this lack of speciﬁcity by modeling human response labels to music in the arousal-valence (A-V) representation of af- fect as a stochastic distribution. Based upon our collected data, we present and evaluate methods using multiple sets of acoustic features to estimate these mood distributions parametrically using multivariate regression. Furthermore, since the emotional content of music often varies within a song, we explore the estimation of these A-V distributions in a time-varying context, demonstrating the ability of our system to track changes on a short-time basis.",
        "zenodo_id": 1416238,
        "dblp_key": "conf/ismir/SchmidtK10"
    },
    {
        "title": "Islands of Gaussians: The Self Organizing Map and Gaussian Music Similarity Features.",
        "author": [
            "Dominik Schnitzer",
            "Arthur Flexer",
            "Gerhard Widmer",
            "Martin Gasser"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415248",
        "url": "https://doi.org/10.5281/zenodo.1415248",
        "ee": "https://zenodo.org/records/1415248/files/SchnitzerFWG10.pdf",
        "abstract": "Multivariate Gaussians are of special interest in the MIR ﬁeld of automatic music recommendation. They are used as the de facto standard representation of music timbre to compute music similarity. However, standard algorithms for clustering and visualization are usually not designed to handle Gaussian distributions and their attached metrics (e.g. the Kullback-Leibler divergence). Hence to use these features the algorithms generally handle them indirectly by ﬁrst mapping them to a vector space, for example by deriv- ing a feature vector representation from a similarity matrix. This paper uses the symmetrized Kullback-Leibler cen- troid of Gaussians to show how to avoid the vectorization detour for the Self Organizing Maps (SOM) data visualiza- tion algorithm. We propose an approach so that the algo- rithm can directly and naturally work on Gaussian music similarity features to compute maps of music collections. We show that by using our approach we can create SOMs which (1) better preserve the original similarity topology and (2) are far less complex to compute, as the often costly vectorization step is eliminated.",
        "zenodo_id": 1415248,
        "dblp_key": "conf/ismir/SchnitzerFWG10"
    },
    {
        "title": "Vocalist Gender Recognition in Recorded Popular Music.",
        "author": [
            "Björn W. Schuller",
            "Christoph Kozielski",
            "Felix Weninger",
            "Florian Eyben",
            "Gerhard Rigoll"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415984",
        "url": "https://doi.org/10.5281/zenodo.1415984",
        "ee": "https://zenodo.org/records/1415984/files/SchullerKWER10.pdf",
        "abstract": "We introduce the task of vocalist gender recognition in popular music and evaluate the beneﬁt of Non-Negative Matrix Factorization based enhancement of melodic com- ponents to this aim. The underlying automatic separation of drum beats is described in detail, and the obtained sig- niﬁcant gain by its use is veriﬁed in extensive test-runs on a novel database of 1.5 days of MP3 coded popular songs based on transcriptions of the Karaoke-game UltraStar. As classiﬁers serve Support Vector Machines and Hidden Naive Bayes. Overall, the suggested methods lead to fully auto- matic recognition of the pre-dominant vocalist gender at 87.31 % accuracy on song level for artists unkown to the system in originally recorded music.",
        "zenodo_id": 1415984,
        "dblp_key": "conf/ismir/SchullerKWER10"
    },
    {
        "title": "An Audio Processing Library for MIR Application Development in Flash.",
        "author": [
            "Jeffrey J. Scott",
            "Raymond Migneco",
            "Brandon G. Morton",
            "Christian M. Hahn",
            "Paul J. Diefenbach",
            "Youngmoo E. Kim"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414740",
        "url": "https://doi.org/10.5281/zenodo.1414740",
        "ee": "https://zenodo.org/records/1414740/files/ScottMMHDK10.pdf",
        "abstract": "In recent years, the Adobe Flash platform has risen as a credible and universal platform for rapid development and deployment of interactive web-based applications. It is also the accepted standard for delivery of streaming me- dia, and many web applications related to music informa- tion retrieval, such as Pandora, Last.fm and Musicovery, are built using Flash. The limitations of Flash, however, have made it difﬁcult for music-IR researchers and de- velopers to utilize complex sound and music signal pro- cessing within their web applications. Furthermore, the real-time audio processing and synchronization required for some music-IR-related activities demands signiﬁcant computational power and specialized audio algorithms, far beyond what is possible to implement using Flash script- ing. By taking advantage of features recently added to the platform, including dynamic audio control and C cross- compilation for near-native performance, we have devel- oped the Audio-processing Library for Flash (ALF), pro- viding developers with a library of common audio pro- cessing routines and affording Flash developers a degree of sound interaction previously unavailable through web- based platforms. We present several music-IR-driven ap- plications that incorporate ALF to demonstrate its utility.",
        "zenodo_id": 1414740,
        "dblp_key": "conf/ismir/ScottMMHDK10"
    },
    {
        "title": "Beat Critic: Beat Tracking Octave Error Identification By Metrical Profile Analysis.",
        "author": [
            "Leigh M. Smith"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417891",
        "url": "https://doi.org/10.5281/zenodo.1417891",
        "ee": "https://zenodo.org/records/1417891/files/Smith10.pdf",
        "abstract": "Computational models of beat tracking of musical au- dio have been well explored, however, such systems often make “octave errors”, identifying the beat period at dou- ble or half the beat rate than that actually recorded in the music. A method is described to detect if octave errors have occurred in beat tracking. Following an initial beat tracking estimation, a feature vector of metrical proﬁle sep- arated by spectral subbands is computed. A measure of subbeat quaver (1/8th note) alternation is used to compare half time and double time measures against the initial beat track estimation and indicate a likely octave error. This er- ror estimate can then be used to re-estimate the beat rate. The performance of the approach is evaluated against the RWC database, showing successful identiﬁcation of octave errors for an existing beat tracker. Using the octave error detector together with the existing beat tracking model im- proved beat tracking by reducing octave errors to 43% of the previous error rate.",
        "zenodo_id": 1417891,
        "dblp_key": "conf/ismir/Smith10"
    },
    {
        "title": "Musical Instrument Recognition using Biologically Inspired Filtering of Temporal Dictionary Atoms.",
        "author": [
            "Steven K. Tjoa",
            "K. J. Ray Liu"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416166",
        "url": "https://doi.org/10.5281/zenodo.1416166",
        "ee": "https://zenodo.org/records/1416166/files/TjoaL10.pdf",
        "abstract": "Most musical instrument recognition systems rely entirely upon spectral information instead of temporal information. In this paper, we test the hypothesis that temporal informa- tion can improve upon the accuracy achievable by the state of the art in instrument recognition. Unlike existing tem- poral classiﬁcation methods which use traditional features such as temporal moments, we extract novel features from temporal atoms generated by nonnegative matrix factoriza- tion by using a multiresolution gamma ﬁlterbank. Among isolated sounds taken from twenty-four instrument classes, the proposed system can achieve 92.3% accuracy, thus im- proving upon the state of the art.",
        "zenodo_id": 1416166,
        "dblp_key": "conf/ismir/TjoaL10"
    },
    {
        "title": "Improving the Generation of Ground Truths Based on Partially Ordered Lists.",
        "author": [
            "Julián Urbano",
            "Mónica Marrero",
            "Diego Martín 0001",
            "Juan Lloréns"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417525",
        "url": "https://doi.org/10.5281/zenodo.1417525",
        "ee": "https://zenodo.org/records/1417525/files/UrbanoMML10.pdf",
        "abstract": "Ground truths based on partially ordered lists have been used for some years now to evaluate the effectiveness of Music Information Retrieval systems, especially in tasks related to symbolic melodic similarity. However, there has been practically no meta-evaluation to measure or improve the correctness of these evaluations. In this paper we revise the methodology used to generate these ground truths and disclose some issues that need to be addressed. In particular, we focus on the arrangement and aggrega- tion of the relevant results, and show that it is not possi- ble to ensure lists completely consistent. We develop a measure of consistency based on Average Dynamic Re- call and propose several alternatives to arrange the lists, all of which prove to be more consistent than the original method. The results of the MIREX 2005 evaluation are revisited using these alternative ground truths.",
        "zenodo_id": 1417525,
        "dblp_key": "conf/ismir/UrbanoMML10"
    },
    {
        "title": "AMUSE (Advanced MUSic Explorer) - A Multitool Framework for Music Data Analysis.",
        "author": [
            "Igor Vatolkin",
            "Wolfgang M. Theimer",
            "Martin Botteck"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.5651429",
        "url": "https://doi.org/10.5281/zenodo.5651429",
        "ee": "http://ismir2010.ismir.net/proceedings/ismir2010-8.pdf",
        "abstract": "<p>Multi-modal dataset for music genre recognition based on six different modalities for the LMD-aligned [1] and SLAC [2] datasets. Further details are provided in [3].</p>\n\n<p><strong>Descriptions of files</strong></p>\n\n<table>\n\t<thead>\n\t\t<tr>\n\t\t\t<th scope=\"col\">Link</th>\n\t\t\t<th scope=\"col\">Description</th>\n\t\t</tr>\n\t</thead>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td><a href=\"https://zenodo.org/record/5651429/files/LMD-aligned_Filelist.arff\">LMD-aligned_Filelist.arff</a></td>\n\t\t\t<td>File list with 1575 music tracks selected from the LMD-aligned dataset [1] with tagtraum genre annotations [4] (only a subset of LMD-aligned is used, which includes only pieces for which all six modalities were accessible, and which includes only well-represented genres)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><a href=\"https://zenodo.org/record/5651429/files/LMD-aligned_ExtractedFeatures.tar.gz\">LMD-aligned_ExtractedFeatures.tar.gz</a></td>\n\t\t\t<td>Raw audio signal and model-based features extracted with AMUSE [5]</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><a href=\"https://zenodo.org/record/5651429/files/LMD-aligned_ProcessedFeatures.tar.gz\">LMD-aligned_ProcessedFeatures.tar.gz</a></td>\n\t\t\t<td>Processed features: audio signal and model-based features aggregated for 4 s time frames with 2 s step size / all other features (see the table below) with the same values for all time frames</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><a href=\"https://zenodo.org/record/5651429/files/LMD-aligned_Datasets.tar.gz\">LMD-aligned_Datasets.tar.gz</a></td>\n\t\t\t<td>Training, optimization, and test datasets for 3 splits for the recognition of 5 genres in [3]</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><a href=\"https://zenodo.org/record/5651429/files/SLAC_Filelist.arff\">SLAC_Filelist.arff</a></td>\n\t\t\t<td>File list with 250 music tracks from the SLAC dataset [2] (genres and sub-genres are provided in the folder structure)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><a href=\"https://zenodo.org/record/5651429/files/SLAC_ExtractedFeatures.tar.gz\">SLAC_ExtractedFeatures.tar.gz</a></td>\n\t\t\t<td>Raw audio signal and model-based features extracted with AMUSE [5]</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><a href=\"https://zenodo.org/record/5651429/files/SLAC_ProcessedFeatures.tar.gz\">SLAC_ProcessedFeatures.tar.gz</a></td>\n\t\t\t<td>Processed features: audio signal and model-based features aggregated for 4 s time frames with 2 s step size / all other features (see the table below) with the same values for all time frames</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><a href=\"https://zenodo.org/record/5651429/files/SLAC_Datasets.tar.gz\">SLAC_Datasets.tar.gz</a></td>\n\t\t\t<td>Training, optimization, and test datasets for 3 splits for the recognition of 5 genres and 10 sub-genres in [3]</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n<p><strong>Modalities and feature sub-groups</strong></p>\n\n<table>\n\t<thead>\n\t\t<tr>\n\t\t\t<th scope=\"col\">Modality</th>\n\t\t\t<th scope=\"col\">Sub-group</th>\n\t\t\t<th scope=\"col\">\n\t\t\t<p>Dimensions in processed</p>\n\n\t\t\t<p>features of LMD-aligned</p>\n\t\t\t</th>\n\t\t\t<th scope=\"col\">\n\t\t\t<p>Dimensions in processed</p>\n\n\t\t\t<p>features of SLAC</p>\n\t\t\t</th>\n\t\t</tr>\n\t</thead>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td>Audio signal</td>\n\t\t\t<td>Low-level</td>\n\t\t\t<td>1-524</td>\n\t\t\t<td>1-524</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Audio signal</td>\n\t\t\t<td>Semantic</td>\n\t\t\t<td>525-810</td>\n\t\t\t<td>525-810</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Audio signal</td>\n\t\t\t<td>Structural complexity</td>\n\t\t\t<td>811-908</td>\n\t\t\t<td>811-908</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Model-based</td>\n\t\t\t<td>Instruments</td>\n\t\t\t<td>909-1018</td>\n\t\t\t<td>909-1018</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Model-based</td>\n\t\t\t<td>Moods</td>\n\t\t\t<td>1019-1146</td>\n\t\t\t<td>1019-1146</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Model-based</td>\n\t\t\t<td>Various</td>\n\t\t\t<td>1147-1402</td>\n\t\t\t<td>1147-1402</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Playlists</td>\n\t\t\t<td>Genres</td>\n\t\t\t<td>1403-1973</td>\n\t\t\t<td>1403-1973</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Playlists</td>\n\t\t\t<td>Styles</td>\n\t\t\t<td>1974-1695</td>\n\t\t\t<td>1974-1695</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Symbolic</td>\n\t\t\t<td>Pitch</td>\n\t\t\t<td>1696-1757</td>\n\t\t\t<td>1696-1757</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Symbolic</td>\n\t\t\t<td>Melodic</td>\n\t\t\t<td>1758-1781</td>\n\t\t\t<td>1758-1781</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Symbolic</td>\n\t\t\t<td>Chords</td>\n\t\t\t<td>1782-1836</td>\n\t\t\t<td>1782-1836</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Symbolic</td>\n\t\t\t<td>Rhythm</td>\n\t\t\t<td>1837-1935</td>\n\t\t\t<td>1837-1935</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Symbolic</td>\n\t\t\t<td>Tempo</td>\n\t\t\t<td>1936-1963</td>\n\t\t\t<td>1936-1963</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Symbolic</td>\n\t\t\t<td>Instrument presence</td>\n\t\t\t<td>1964-2441</td>\n\t\t\t<td>1964-2441</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Symbolic</td>\n\t\t\t<td>Instruments</td>\n\t\t\t<td>2442-2456</td>\n\t\t\t<td>2442-2456</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Symbolic</td>\n\t\t\t<td>Texture</td>\n\t\t\t<td>2457-2480</td>\n\t\t\t<td>2457-2480</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Symbolic</td>\n\t\t\t<td>Dynamics</td>\n\t\t\t<td>2481-2484</td>\n\t\t\t<td>2481-2484</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Album covers</td>\n\t\t\t<td>SIFT</td>\n\t\t\t<td>2485-2584</td>\n\t\t\t<td>2485-2584</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Lyrics</td>\n\t\t\t<td>jLyrics descriptors</td>\n\t\t\t<td>2585-2603</td>\n\t\t\t<td>2585-2671</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Lyrics</td>\n\t\t\t<td>Bag-of-Words</td>\n\t\t\t<td>2604-2703</td>\n\t\t\t<td>&nbsp;</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Lyrics</td>\n\t\t\t<td>Doc2Vec</td>\n\t\t\t<td>2704-2803</td>\n\t\t\t<td>&nbsp;</td>\n\t\t</tr>\n\t</tbody>\n</table>",
        "zenodo_id": 5651429,
        "dblp_key": "conf/ismir/VatolkinTB10"
    },
    {
        "title": "Using jWebMiner 2.0 to Improve Music Classification Performance by Combining Different Types of Features Mined from the Web.",
        "author": [
            "Gabriel Vigliensoni",
            "Cory McKay",
            "Ichiro Fujinaga"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416590",
        "url": "https://doi.org/10.5281/zenodo.1416590",
        "ee": "https://zenodo.org/records/1416590/files/VigliensoniMF10.pdf",
        "abstract": "This paper presents the jWebMiner 2.0 cultural feature extraction software and describes the results of several musical genre classification experiments performed with it. jWebMiner 2.0 is an easy-to-use and open-source tool that allows users to mine the Internet in order to extract features based on both Last.fm social tags and general web search string co-occurrences extracted using the Yahoo! API. The experiments performed found that the features based on social tags were more effective at classifying music into a small (5-genre) genre ontology, but the features based on general web co-occurrences were more effective at classifying a moderate (10-genre) ontology. It was also found that combining the two types of features resulted in improved performance overall.",
        "zenodo_id": 1416590,
        "dblp_key": "conf/ismir/VigliensoniMF10"
    },
    {
        "title": "A Roadmap Towards Versatile MIR.",
        "author": [
            "Emmanuel Vincent 0001",
            "Stanislaw Andrzej Raczynski",
            "Nobutaka Ono",
            "Shigeki Sagayama"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1418141",
        "url": "https://doi.org/10.5281/zenodo.1418141",
        "ee": "https://zenodo.org/records/1418141/files/VincentROS10.pdf",
        "abstract": "Most MIR systems are speciﬁcally designed for one appli- cation and one cultural context and suffer from the seman- tic gap between the data and the application. Advances in the theory of Bayesian language and information process- ing enable the vision of a versatile, meaningful and accu- rate MIR system integrating all levels of information. We propose a roadmap to collectively achieve this vision.",
        "zenodo_id": 1418141,
        "dblp_key": "conf/ismir/VincentROS10"
    },
    {
        "title": "Predicting High-level Music Semantics Using Social Tags via Ontology-based Reasoning.",
        "author": [
            "Jun Wang",
            "Xiaoou Chen",
            "Yajie Hu",
            "Tao Feng"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417785",
        "url": "https://doi.org/10.5281/zenodo.1417785",
        "ee": "https://zenodo.org/records/1417785/files/WangCHF10.pdf",
        "abstract": "High-level semantics such as “mood” and “usage” are very useful in music retrieval and recommendation but they are normally hard to acquire. Can we predict them from a cloud of social tags? We propose a semantic iden- tification and reasoning method: Given a music taxonomy system, we map it to an ontology’s terminology, map its finite set of terms to the ontology’s assertional axioms, and then map tags to the closest conceptual level of the referenced terms in WordNet to enrich the knowledge base, then we predict richer high-level semantic informa- tion with a set of reasoning rules. We find this method predicts mood annotations for music with higher accuracy, as well as giving richer semantic association information, than alternative SVM-based methods do.",
        "zenodo_id": 1417785,
        "dblp_key": "conf/ismir/WangCHF10"
    },
    {
        "title": "An Improved Query by Singing/Humming System Using Melody and Lyrics Information.",
        "author": [
            "Chung-Che Wang",
            "Jyh-Shing Roger Jang",
            "Wennen Wang"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414802",
        "url": "https://doi.org/10.5281/zenodo.1414802",
        "ee": "https://zenodo.org/records/1414802/files/WangJW10.pdf",
        "abstract": "This paper proposes an improved query by sing- ing/humming (QBSH) system using both melody and lyr- ics information for achieving better performance. Sing- ing/humming discrimination (SHD) is first performed to distinguish singing from humming queries. For a hum- ming query, we apply a pitch-only melody recognition method that has been used for QBSH task at MIREX with rank-1 performance. For a singing query, we combine the scores from melody recognition and lyrics recognition to take advantage of the extra lyrics information. Lyrics rec- ognition is based on a modified tree lexicon that is com- monly used in speech recognition. The performance of the overall QBSH system achieves 39.01% and 23.53% error reduction rates, respectively, for top-20 recognition under two experimental settings, indicating the feasibility of the proposed method.",
        "zenodo_id": 1414802,
        "dblp_key": "conf/ismir/WangJW10"
    },
    {
        "title": "Are Tags Better Than Audio? The Effect of Joint Use of Tags and Audio Content Features for Artistic Style Clustering.",
        "author": [
            "Dingding Wang 0001",
            "Tao Li 0001",
            "Mitsunori Ogihara"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417543",
        "url": "https://doi.org/10.5281/zenodo.1417543",
        "ee": "https://zenodo.org/records/1417543/files/WangLO10.pdf",
        "abstract": "Social tags are receiving growing interests in informa- tion retrieval. In music information retrieval previous re- search has demonstrated that tags can assist in music clas- siﬁcation and clustering. This paper studies the problem of combining tags and audio contents for artistic style clus- tering. After studying the effectiveness of using tags and audio contents separately for clustering, this paper pro- poses a novel language model that makes use of both data sources. Experiments with various methods for combining feature sets demonstrate that tag features are more useful than audio content features for style clustering and that the proposed model can marginally improve clustering perfor- mance by combing tags and audio contents.",
        "zenodo_id": 1417543,
        "dblp_key": "conf/ismir/WangLO10"
    },
    {
        "title": "Identifying Repeated Patterns in Music Using Sparse Convolutive Non-negative Matrix Factorization.",
        "author": [
            "Ron J. Weiss",
            "Juan Pablo Bello"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1415934",
        "url": "https://doi.org/10.5281/zenodo.1415934",
        "ee": "https://zenodo.org/records/1415934/files/WeissB10.pdf",
        "abstract": "We describe an unsupervised, data-driven, method for auto- matically identifying repeated patterns in music by analyz- ing a feature matrix using a variant of sparse convolutive non-negative matrix factorization. We utilize sparsity con- straints to automatically identify the number of patterns and their lengths, parameters that would normally need to be ﬁxed in advance. The proposed analysis is applied to beat- synchronous chromagrams in order to concurrently extract repeated harmonic motifs and their locations within a song. Finally, we show how this analysis can be used for long- term structure segmentation, resulting in an algorithm that is competitive with other state-of-the-art segmentation algo- rithms based on hidden Markov models and self similarity matrices.",
        "zenodo_id": 1415934,
        "dblp_key": "conf/ismir/WeissB10"
    },
    {
        "title": "Predicting Development of Research in Music Based on Parallels with Natural Language Processing.",
        "author": [
            "Jacek Wolkowicz",
            "Vlado Keselj"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1416808",
        "url": "https://doi.org/10.5281/zenodo.1416808",
        "ee": "https://zenodo.org/records/1416808/files/WolkowiczK10.pdf",
        "abstract": "The hypothesis of the paper is that the domain of Nat- ural Languages Processing (NLP) resembles current re- search in music so one could beneﬁt from this by employ- ing NLP techniques to music. In this paper the similarity between both domains is described. The levels of NLP are listed with pointers to respective tasks within the research of computational music. A brief introduction to history of NLP enables locating music research in this history. Pos- sible directions of research in music, assuming its afﬁnity to NLP, are introduced. Current research in generational and statistical music modeling is compared to similar NLP theories. The paper is concluded with guidelines for music research and information retrieval.",
        "zenodo_id": 1416808,
        "dblp_key": "conf/ismir/WolkowiczK10"
    },
    {
        "title": "Infinite Latent Harmonic Allocation: A Nonparametric Bayesian Approach to Multipitch Analysis.",
        "author": [
            "Kazuyoshi Yoshii",
            "Masataka Goto"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1414912",
        "url": "https://doi.org/10.5281/zenodo.1414912",
        "ee": "https://zenodo.org/records/1414912/files/YoshiiG10.pdf",
        "abstract": "This paper presents a statistical method called Inﬁnite La- tent Harmonic Allocation (iLHA) for detecting multiple fundamental frequencies in polyphonicaudio signals. Con- ventional methods face a crucial problem known as model selection because they assume that the observed spectra are superpositions of a certain ﬁxed number of bases (sound sources and/or ﬁner parts). iLHA avoids this problem by assuming that the observed spectra are superpositions of a stochastically-distributed unbounded (theoretically inﬁ- nite) number of bases. Such uncertainty can be treated in a principled way by leveraging the state-of-the-art paradigm of machine-learning called Bayesian nonparametrics. To represent a set of time-sliced spectral strips, we formulated nested inﬁnite Gaussian mixture models (GMMs) based on hierarchical and generalized Dirichlet processes. Each strip is allowed to contain an unbounded number of sound sources (GMMs), each of which is allowed to contain an unbounded number of harmonic partials (Gaussians). To train the nested inﬁnite GMMs efﬁciently, we used a mod- ern inference technique called collapsed variational Bayes (CVB). Our experiments using audio recordings of real pi- ano and guitar performances showed that fully automated iLHA based on noninformative priors performed as well as optimally tuned conventional methods.",
        "zenodo_id": 1414912,
        "dblp_key": "conf/ismir/YoshiiG10"
    },
    {
        "title": "Automatic Mood Classification Using TF*IDF Based on Lyrics.",
        "author": [
            "Menno van Zaanen",
            "Pieter Kanters"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.1417287",
        "url": "https://doi.org/10.5281/zenodo.1417287",
        "ee": "https://zenodo.org/records/1417287/files/ZaanenK10.pdf",
        "abstract": "This paper presents the outcomes of research into using lingual parts of music in an automatic mood classiﬁcation system. Using a collection of lyrics and corresponding user-tagged moods, we build classiﬁers that classify lyrics of songs into moods. By comparing the performance of different mood frameworks (or dimensions), we examine to what extent the linguistic part of music reveals adequate information for assigning a mood category and which as- pects of mood can be classiﬁed best. Our results show that word oriented metrics provide a valuable source of information for automatic mood clas- siﬁcation of music, based on lyrics only. Metrics such as term frequencies and tf*idf values are used to measure rel- evance of words to the different mood classes. These met- rics are incorporated in a machine learning classiﬁer setup. Different partitions of the mood plane are investigated and we show that there is no large difference in mood predic- tion based on the mood division. Predictions on the va- lence, tension and combinations of aspects lead to similar performance.",
        "zenodo_id": 1417287,
        "dblp_key": "conf/ismir/ZaanenK10"
    },
    {
        "title": "Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010, Utrecht, Netherlands, August 9-13, 2010",
        "author": [
            "J. Stephen Downie",
            "Remco C. Veltkamp"
        ],
        "year": "2010",
        "doi": "10.5281/zenodo.5651429",
        "url": "https://doi.org/10.5281/zenodo.5651429",
        "ee": null,
        "abstract": "<p>Multi-modal dataset for music genre recognition based on six different modalities for the LMD-aligned [1] and SLAC [2] datasets. Further details are provided in [3].</p>\n\n<p><strong>Descriptions of files</strong></p>\n\n<table>\n\t<thead>\n\t\t<tr>\n\t\t\t<th scope=\"col\">Link</th>\n\t\t\t<th scope=\"col\">Description</th>\n\t\t</tr>\n\t</thead>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td><a href=\"https://zenodo.org/record/5651429/files/LMD-aligned_Filelist.arff\">LMD-aligned_Filelist.arff</a></td>\n\t\t\t<td>File list with 1575 music tracks selected from the LMD-aligned dataset [1] with tagtraum genre annotations [4] (only a subset of LMD-aligned is used, which includes only pieces for which all six modalities were accessible, and which includes only well-represented genres)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><a href=\"https://zenodo.org/record/5651429/files/LMD-aligned_ExtractedFeatures.tar.gz\">LMD-aligned_ExtractedFeatures.tar.gz</a></td>\n\t\t\t<td>Raw audio signal and model-based features extracted with AMUSE [5]</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><a href=\"https://zenodo.org/record/5651429/files/LMD-aligned_ProcessedFeatures.tar.gz\">LMD-aligned_ProcessedFeatures.tar.gz</a></td>\n\t\t\t<td>Processed features: audio signal and model-based features aggregated for 4 s time frames with 2 s step size / all other features (see the table below) with the same values for all time frames</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><a href=\"https://zenodo.org/record/5651429/files/LMD-aligned_Datasets.tar.gz\">LMD-aligned_Datasets.tar.gz</a></td>\n\t\t\t<td>Training, optimization, and test datasets for 3 splits for the recognition of 5 genres in [3]</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><a href=\"https://zenodo.org/record/5651429/files/SLAC_Filelist.arff\">SLAC_Filelist.arff</a></td>\n\t\t\t<td>File list with 250 music tracks from the SLAC dataset [2] (genres and sub-genres are provided in the folder structure)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><a href=\"https://zenodo.org/record/5651429/files/SLAC_ExtractedFeatures.tar.gz\">SLAC_ExtractedFeatures.tar.gz</a></td>\n\t\t\t<td>Raw audio signal and model-based features extracted with AMUSE [5]</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><a href=\"https://zenodo.org/record/5651429/files/SLAC_ProcessedFeatures.tar.gz\">SLAC_ProcessedFeatures.tar.gz</a></td>\n\t\t\t<td>Processed features: audio signal and model-based features aggregated for 4 s time frames with 2 s step size / all other features (see the table below) with the same values for all time frames</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td><a href=\"https://zenodo.org/record/5651429/files/SLAC_Datasets.tar.gz\">SLAC_Datasets.tar.gz</a></td>\n\t\t\t<td>Training, optimization, and test datasets for 3 splits for the recognition of 5 genres and 10 sub-genres in [3]</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n<p><strong>Modalities and feature sub-groups</strong></p>\n\n<table>\n\t<thead>\n\t\t<tr>\n\t\t\t<th scope=\"col\">Modality</th>\n\t\t\t<th scope=\"col\">Sub-group</th>\n\t\t\t<th scope=\"col\">\n\t\t\t<p>Dimensions in processed</p>\n\n\t\t\t<p>features of LMD-aligned</p>\n\t\t\t</th>\n\t\t\t<th scope=\"col\">\n\t\t\t<p>Dimensions in processed</p>\n\n\t\t\t<p>features of SLAC</p>\n\t\t\t</th>\n\t\t</tr>\n\t</thead>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td>Audio signal</td>\n\t\t\t<td>Low-level</td>\n\t\t\t<td>1-524</td>\n\t\t\t<td>1-524</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Audio signal</td>\n\t\t\t<td>Semantic</td>\n\t\t\t<td>525-810</td>\n\t\t\t<td>525-810</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Audio signal</td>\n\t\t\t<td>Structural complexity</td>\n\t\t\t<td>811-908</td>\n\t\t\t<td>811-908</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Model-based</td>\n\t\t\t<td>Instruments</td>\n\t\t\t<td>909-1018</td>\n\t\t\t<td>909-1018</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Model-based</td>\n\t\t\t<td>Moods</td>\n\t\t\t<td>1019-1146</td>\n\t\t\t<td>1019-1146</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Model-based</td>\n\t\t\t<td>Various</td>\n\t\t\t<td>1147-1402</td>\n\t\t\t<td>1147-1402</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Playlists</td>\n\t\t\t<td>Genres</td>\n\t\t\t<td>1403-1973</td>\n\t\t\t<td>1403-1973</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Playlists</td>\n\t\t\t<td>Styles</td>\n\t\t\t<td>1974-1695</td>\n\t\t\t<td>1974-1695</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Symbolic</td>\n\t\t\t<td>Pitch</td>\n\t\t\t<td>1696-1757</td>\n\t\t\t<td>1696-1757</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Symbolic</td>\n\t\t\t<td>Melodic</td>\n\t\t\t<td>1758-1781</td>\n\t\t\t<td>1758-1781</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Symbolic</td>\n\t\t\t<td>Chords</td>\n\t\t\t<td>1782-1836</td>\n\t\t\t<td>1782-1836</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Symbolic</td>\n\t\t\t<td>Rhythm</td>\n\t\t\t<td>1837-1935</td>\n\t\t\t<td>1837-1935</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Symbolic</td>\n\t\t\t<td>Tempo</td>\n\t\t\t<td>1936-1963</td>\n\t\t\t<td>1936-1963</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Symbolic</td>\n\t\t\t<td>Instrument presence</td>\n\t\t\t<td>1964-2441</td>\n\t\t\t<td>1964-2441</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Symbolic</td>\n\t\t\t<td>Instruments</td>\n\t\t\t<td>2442-2456</td>\n\t\t\t<td>2442-2456</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Symbolic</td>\n\t\t\t<td>Texture</td>\n\t\t\t<td>2457-2480</td>\n\t\t\t<td>2457-2480</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Symbolic</td>\n\t\t\t<td>Dynamics</td>\n\t\t\t<td>2481-2484</td>\n\t\t\t<td>2481-2484</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Album covers</td>\n\t\t\t<td>SIFT</td>\n\t\t\t<td>2485-2584</td>\n\t\t\t<td>2485-2584</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Lyrics</td>\n\t\t\t<td>jLyrics descriptors</td>\n\t\t\t<td>2585-2603</td>\n\t\t\t<td>2585-2671</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Lyrics</td>\n\t\t\t<td>Bag-of-Words</td>\n\t\t\t<td>2604-2703</td>\n\t\t\t<td>&nbsp;</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>Lyrics</td>\n\t\t\t<td>Doc2Vec</td>\n\t\t\t<td>2704-2803</td>\n\t\t\t<td>&nbsp;</td>\n\t\t</tr>\n\t</tbody>\n</table>",
        "zenodo_id": 5651429,
        "dblp_key": "conf/ismir/2010"
    }
]