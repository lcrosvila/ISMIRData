[
    {
        "title": "Comparing RNN Parameters for Melodic Similarity.",
        "author": [
            "Tian Cheng 0001",
            "Satoru Fukayama",
            "Masataka Goto"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492529",
        "url": "https://doi.org/10.5281/zenodo.1492529",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/61_Paper.pdf",
        "abstract": "Melodic similarity is an important task in the Music Information Retrieval (MIR) domain, with promising applications including query by example, music recommendation and visualisation. Most current approaches compute the similarity between two melodic sequences by comparing their local features (distance between pitches, intervals, etc.) or by comparing the sequences after aligning them. In order to find a better feature representing global characteristics of a melody, we propose to represent the melodic sequence of each musical piece by the parameters of a generative Recurrent Neural Network (RNN) trained on its sequence. Because the trained RNN can generate the identical melodic sequence of each piece, we can expect that the RNN parameters contain the temporal information within the melody. In our experiment, we first train an RNN on all melodic sequences, and then use it as an initialisation to train an individual RNN on each melodic sequence. The similarity between two melodies is computed by using the distance between their individual RNN parameters. Experimental results showed that the proposed RNN-based similarity outperformed the baseline similarity obtained by directly comparing melodic sequences.",
        "zenodo_id": 1492529,
        "dblp_key": "conf/ismir/0001FG18"
    },
    {
        "title": "On the Relationships between Music-induced Emotion and Physiological Signals.",
        "author": [
            "Xiao Hu 0001",
            "Fanjie Li",
            "Tzi-Dong Jeremy Ng"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492425",
        "url": "https://doi.org/10.5281/zenodo.1492425",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/115_Paper.pdf",
        "abstract": "to optimize emotion-aware music retrieval.  Emotion-aware music information retrieval (MIR) has been difficult due to the subjectivity and temporality of emotion responses to music. Physiological signals are regarded as related to emotion and thus could potentially be exploited in emotion-aware music discovery. This study explored the possibility of using physiological signals to detect users' emotion responses to music, with consideration of individual characteristics (personality, music preferences, etc.). A user experiment was conducted with 23 participants who searched for music in a novel MIR system. Users' listening behaviors and self-reported emotion responses to a total of 628 music pieces were collected. During music listening, a series of peripheral physiological signals (e.g., heart rate, skin conductance) were recorded from participants unobtrusively using a researchgrade wearable wristband. A set of features in the time- and frequency- domains were extracted from the physiological signals and analyzed using statistical and machine learning methods. Results reveal 1) significant differences in some physiological features between positive and negative arousal and mood categories, and 2) effective classification of emotion responses based on physiological signals for some individuals. The findings can contribute to further improvement of emotion-aware intelligent MIR systems exploiting physiological signals as an objective and personalized input.",
        "zenodo_id": 1492425,
        "dblp_key": "conf/ismir/0001LN18"
    },
    {
        "title": "Investigating Cross-Country Relationship between Users&apos; Social Ties and Music Mainstreaminess.",
        "author": [
            "Christine Bauer 0001",
            "Markus Schedl"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492507",
        "url": "https://doi.org/10.5281/zenodo.1492507",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/130_Paper.pdf",
        "abstract": "We investigate the complex relationship between the factors (i) preference for music mainstream, (ii) social ties in an online music platform, and (iii) demographics. We define (i) on a global and a country level, (ii) by several network centrality measures such as Jaccard index among users' connections, closeness centrality, and betweenness centrality, and (iii) by country and age information. Using the LFM-1b dataset of listening events of Last.fm users, we are able to uncover country-dependent differences in consumption of mainstream music as well as in user behavior with respect to social ties and users' centrality. We could identify that users inclined to mainstream music tend to have stronger connections than the group of less mainstreamy users. Furthermore, our analysis revealed that users typically have less connections within a country than cross-country ones, with the first being stronger social ties, though. Results will help building better user models of listeners and in turn improve personalized music retrieval and recommendation algorithms.",
        "zenodo_id": 1492507,
        "dblp_key": "conf/ismir/0001S18"
    },
    {
        "title": "Improving Bass Saliency Estimation Using Transfer Learning and Label Propagation.",
        "author": [
            "Jakob Abeßer",
            "Stefan Balke",
            "Meinard Müller"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492411",
        "url": "https://doi.org/10.5281/zenodo.1492411",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/143_Paper.pdf",
        "abstract": "In this paper, we consider two methods to improve an algorithm for bass saliency estimation in jazz ensemble recordings which are based on deep neural networks. First, we apply label propagation to increase the amount of training data by transferring pitch labels from our labeled dataset to unlabeled audio recordings using a spectral similarity measure. Second, we study in several transfer learning experiments, whether isolated note recordings can be beneficial for pre-training a model which is later fine-tuned on ensemble recordings. Our results indicate that both strategies can improve the performance on bass saliency estimation by up to five percent in accuracy.",
        "zenodo_id": 1492411,
        "dblp_key": "conf/ismir/AbesserBM18"
    },
    {
        "title": "A Data-driven Approach to Mid-level Perceptual Musical Feature Modeling.",
        "author": [
            "Anna Aljanaki",
            "Mohammad Soleymani 0001"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492491",
        "url": "https://doi.org/10.5281/zenodo.1492491",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/183_Paper.pdf",
        "abstract": "Musical features and descriptors could be coarsely divided into three levels of complexity. The bottom level contains the basic building blocks of music, e.g., chords, beats and timbre. The middle level contains concepts that emerge from combining the basic blocks: tonal and rhythmic stability, harmonic and rhythmic complexity, etc. High-level descriptors (genre, mood, expressive style) are usually modeled using the lower level ones. The features belonging to the middle level can both improve automatic recognition of high-level descriptors, and provide new music retrieval possibilities. Mid-level features are subjective and usually lack clear definitions. However, they are very important for human perception of music, and on some of them people can reach high agreement, even though defining them and therefore, designing a hand-crafted feature extractor for them can be difficult. In this paper, we derive the mid-level descriptors from data. We collect and release a dataset 1 of 5000 songs annotated by musicians with seven mid-level descriptors, namely, melodiousness, tonal and rhythmic stability, modality, rhythmic complexity, dissonance and articulation. We then compare several approaches to predicting these descriptors from spectrograms using deep-learning. We also demonstrate the usefulness of these mid-level features using music emotion recognition as an application.",
        "zenodo_id": 1492491,
        "dblp_key": "conf/ismir/AljanakiS18"
    },
    {
        "title": "Music Generation and Transformation with Moment Matching-Scattering Inverse Networks.",
        "author": [
            "Mathieu Andreux",
            "Stéphane Mallat"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492415",
        "url": "https://doi.org/10.5281/zenodo.1492415",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/131_Paper.pdf",
        "abstract": "We introduce a Moment Matching-Scattering Inverse Network (MM-SIN) to generate and transform musical sounds. The MM-SIN generator is similar to a variational autoencoder or an adversarial network. However, the encoder or the discriminator are not learned, but computed with a scattering transform defined from prior information on sparse time-frequency audio properties. The generator is trained by jointly minimizing the reconstruction loss of an inverse problem, and a generation loss which computes a distance over scattering moments. It has a similar causal architecture as a WaveNet and provides a simpler mathematical model related to time-frequency decompositions. Numerical experiments demonstrate that this MMSIN generates new realistic musical signals. It can transform low-level musical attributes such as pitch with a linear transformation in the embedding space of scattering coefficients.",
        "zenodo_id": 1492415,
        "dblp_key": "conf/ismir/AndreuxM18"
    },
    {
        "title": "Audio-to-Score Alignment using Transposition-invariant Features.",
        "author": [
            "Andreas Arzt",
            "Stefan Lattner"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492485",
        "url": "https://doi.org/10.5281/zenodo.1492485",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/166_Paper.pdf",
        "abstract": "Audio-to-score alignment is an important pre-processing step for in-depth analysis of classical music. In this paper, we apply novel transposition-invariant audio features to this task. These low-dimensional features represent local pitch intervals and are learned in an unsupervised fashion by a gated autoencoder. Our results show that the proposed features are indeed fully transposition-invariant and enable accurate alignments between transposed scores and performances. Furthermore, they can even outperform widely used features for audio-to-score alignment on 'untransposed data', and thus are a viable and more flexible alternative to well-established features for music alignment and matching.",
        "zenodo_id": 1492485,
        "dblp_key": "conf/ismir/ArztL18"
    },
    {
        "title": "Empirically Weighting the Importance of Decision Factors for Singing Preference.",
        "author": [
            "Michael D. Barone",
            "Karim M. Ibrahim",
            "Chitralekha Gupta",
            "Ye Wang 0007"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492469",
        "url": "https://doi.org/10.5281/zenodo.1492469",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/117_Paper.pdf",
        "abstract": "Although music cognition and music information retrieval have many common areas of research interest, relatively little work utilizes a combination of signal- and humancentric approaches when assessing complex cognitive phenomena. This work explores the importance of four cognitive decision-making factors (familiarity, genre preference, ease of vocal reproducibility, and overall preference) influence in the perception of \"singability\", how attractive a song is to sing. In Experiment One, we develop a model to validate and empirically determine to what degree these factors are important when evaluating its singability. Results indicate that evaluations of how these four factors impact singability strongly correlate with pairwise evaluations (ρ = 0.692, p &lt; 0.0001), supporting the notion that singability is a measurable cognitive process. Experiment Two examines the degree to which timbral and rhythmic features contribute to singability. Regression and random forest analysis find that some selected features are more significant than others. We discuss the method we use to empirically assess the complex decisions, and provide a preliminary exploration regarding what acoustic features may motivate these choices.",
        "zenodo_id": 1492469,
        "dblp_key": "conf/ismir/BaroneIGW18"
    },
    {
        "title": "Main Melody Estimation with Source-Filter NMF and CRNN.",
        "author": [
            "Dogac Basaran",
            "Slim Essid",
            "Geoffroy Peeters"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492349",
        "url": "https://doi.org/10.5281/zenodo.1492349",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/273_Paper.pdf",
        "abstract": "Estimating the main melody of a polyphonic audio recording remains a challenging task. We approach the task from a classification perspective and adopt a convolutional recurrent neural network (CRNN) architecture that relies on a particular form of pretraining by source-filter nonnegative matrix factorisation (NMF). The source-filter NMF decomposition is chosen for its ability to capture the pitch and timbre content of the leading voice/instrument, providing a better initial pitch salience than standard timefrequency representations. Starting from such a musically motivated representation, we propose to further enhance the NMF-based salience representations with CNN layers, then to model the temporal structure by an RNN network and to estimate the dominant melody with a final classification layer. The results show that such a system achieves state-of-the-art performance on the MedleyDB dataset without any augmentation methods or large training sets.",
        "zenodo_id": 1492349,
        "dblp_key": "conf/ismir/BasaranEP18"
    },
    {
        "title": "Relevance of Musical Features for Cadence Detection.",
        "author": [
            "Louis Bigo",
            "Laurent Feisthauer",
            "Mathieu Giraud",
            "Florence Levé"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492423",
        "url": "https://doi.org/10.5281/zenodo.1492423",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/243_Paper.pdf",
        "abstract": "Cadences, as breaths in music, are felt by the listener or studied by the theorist by combining harmony, melody, texture and possibly other musical aspects. We formalize and discuss the significance of 44 cadential features, correlated with the occurrence of cadences in scores. These features describe properties at the arrival beat of a cadence and its surroundings, but also at other onsets heuristically identified to pinpoint chords preparing the cadence. The representation of each beat of the score as a vector of cadential features makes it possible to reformulate cadence detection as a classification task. An SVM classifier was run on two corpora from Bach and Haydn totaling 162 perfect authentic cadences and 70 half cadences. In these corpora, the classifier correctly identified more than 75% of perfect authentic cadences and 50% of half cadences, with low false positive rates. The experiment results are consistent with common knowledge that classification is more complex for half cadences than for authentic cadences.",
        "zenodo_id": 1492423,
        "dblp_key": "conf/ismir/BigoFGL18"
    },
    {
        "title": "Timbre Discrimination for Brief Instrument Sounds.",
        "author": [
            "Francesco Bigoni",
            "Sofia Dahl"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492361",
        "url": "https://doi.org/10.5281/zenodo.1492361",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/249_Paper.pdf",
        "abstract": "Timbre discrimination, even for very brief sounds, allows identification and separation of different sound sources. The existing literature on the effect of duration on timbre recognition shows high performance for remarkably short time window lengths, but does not address the possible effect of musical training. In this study, we applied an adaptive procedure to investigate the effect of musical training on individual thresholds for instrument identification. A timbre discrimination task consisting of a 4-alternative forced choice (4AFC) of brief instrument sounds with varying duration was assigned to 16 test subjects using an adaptive staircase method. The effect of musical training has been investigated by dividing the participants into two groups: musicians and non-musicians. The experiment showed lowest thresholds for the guitar sound and highest for the violin sound, with a high overall performance level, but no significant difference between the two groups. It is suggested that the test subjects adjust the weightings of the perceptual dimensions of timbre according to different degrees of acoustic degradation of the stimuli, which are evaluated both by plotting extracted audio features in a feature space and by considering the timbral specificities of the four instruments.",
        "zenodo_id": 1492361,
        "dblp_key": "conf/ismir/BigoniD18"
    },
    {
        "title": "MIDI-VAE: Modeling Dynamics and Instrumentation of Music with Applications to Style Transfer.",
        "author": [
            "Gino Brunner",
            "Andres Konrad",
            "Yuyi Wang 0001",
            "Roger Wattenhofer"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492525",
        "url": "https://doi.org/10.5281/zenodo.1492525",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/204_Paper.pdf",
        "abstract": "We introduce MIDI-VAE, a neural network model based on Variational Autoencoders that is capable of handling polyphonic music with multiple instrument tracks, as well as modeling the dynamics of music by incorporating note durations and velocities. We show that MIDI-VAE can perform style transfer on symbolic music by automatically changing pitches, dynamics and instruments of a music piece from, e.g., a Classical to a Jazz style. We evaluate the efficacy of the style transfer by training separate style validation classifiers. Our model can also interpolate between short pieces of music, produce medleys and create mixtures of entire songs. The interpolations smoothly change pitches, dynamics and instrumentation to create a harmonic bridge between two music pieces. To the best of our knowledge, this work represents the first successful attempt at applying neural style transfer to complete musical compositions.",
        "zenodo_id": 1492525,
        "dblp_key": "conf/ismir/BrunnerKWW18"
    },
    {
        "title": "Camera-PrIMuS: Neural End-to-End Optical Music Recognition on Realistic Monophonic Scores.",
        "author": [
            "Jorge Calvo-Zaragoza",
            "David Rizo"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492395",
        "url": "https://doi.org/10.5281/zenodo.1492395",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/33_Paper.pdf",
        "abstract": "The optical music recognition (OMR) field studies how to automate the process of reading the musical notation present in a given image. Among its many uses, an interesting scenario is that in which a score captured with a camera is to be automatically reproduced. Recent approaches to OMR have shown that the use of deep neural networks allows important advances in the field. However, these approaches have been evaluated on images with ideal conditions, which do not correspond to the previous scenario. In this work, we evaluate the performance of an end-to-end approach that uses a deep convolutional recurrent neural network (CRNN) over non-ideal image conditions of music scores. Consequently, our contribution also consists of Camera-PrIMuS, a corpus of printed monophonic scores of real music synthetically modified to resemble camera-based realistic scenarios, involving distortions such as irregular lighting, rotations, or blurring. Our results confirm that the CRNN is able to successfully solve the task under these conditions, obtaining an error around 2% at music-symbol level, thereby representing a groundbreaking piece of research towards useful OMR systems.",
        "zenodo_id": 1492395,
        "dblp_key": "conf/ismir/Calvo-ZaragozaR18"
    },
    {
        "title": "Using Musical Relationships Between Chord Labels in Automatic Chord Extraction Tasks.",
        "author": [
            "Tristan Carsault",
            "Jérôme Nika",
            "Philippe Esling"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492329",
        "url": "https://doi.org/10.5281/zenodo.1492329",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/231_Paper.pdf",
        "abstract": "Recent research on Automatic Chord Extraction (ACE) has focused on the improvement of models based on machine learning. However, most models still fail to take into account the prior knowledge underlying the labeling alphabets (chord labels). Furthermore, recent works have shown that ACE performances have reached a glass ceiling. Therefore, this prompts the need to focus on other aspects of the task, such as the introduction of musical knowledge in the representation, the improvement of the models towards more complex chord alphabets and the development of more adapted evaluation methods. In this paper, we propose to exploit specific properties and relationships between chord labels in order to improve the learning of statistical ACE models. Hence, we analyze the interdependence of the representations of chords and their associated distances, the precision of the chord alphabets, and the impact of performing alphabet reduction before or after training the model. Furthermore, we propose new training losses based on musical theory. We show that these improve the results of ACE systems based on Convolutional Neural Networks. By analyzing our results, we uncover a set of related insights on ACE tasks based on statistical models, and also formalize the musical meaning of some classification errors.",
        "zenodo_id": 1492329,
        "dblp_key": "conf/ismir/CarsaultNE18"
    },
    {
        "title": "Document Analysis of Music Score Images with Selectional Auto-Encoders.",
        "author": [
            "Francisco J. Castellanos 0001",
            "Jorge Calvo-Zaragoza",
            "Gabriel Vigliensoni",
            "Ichiro Fujinaga"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492397",
        "url": "https://doi.org/10.5281/zenodo.1492397",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/93_Paper.pdf",
        "abstract": "The document analysis of music score images is a key step in the development of successful Optical Music Recognition systems. The current state of the art considers the use of deep neural networks trained to classify every pixel of the image according to the image layer it belongs to. This process, however, involves a high computational cost that prevents its use in interactive machine learning scenarios. In this paper, we propose the use of a set of deep selectional auto-encoders, implemented as fully-convolutional networks, to perform image-to-image categorizations. This strategy retains the advantages of using deep neural networks, which have demonstrated their ability to perform this task, while dramatically increasing the efficiency by processing a large number of pixels in a single step. The results of an experiment performed with a set of high-resolution images taken from Medieval manuscripts successfully validate this approach, with a similar accuracy to that of the state of the art but with a computational time orders of magnitude smaller, making this approach appropriate for being used in interactive applications.",
        "zenodo_id": 1492397,
        "dblp_key": "conf/ismir/CastellanosCVF18"
    },
    {
        "title": "Functional Harmony Recognition of Symbolic Music Data with Multi-task Recurrent Neural Networks.",
        "author": [
            "Tsung-Ping Chen",
            "Li Su 0004"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492351",
        "url": "https://doi.org/10.5281/zenodo.1492351",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/178_Paper.pdf",
        "abstract": "Previous works on chord recognition mainly focus on chord symbols but overlook other essential features that matter in musical harmony. To tackle the functional harmony recognition problem, we compile a new professionally annotated dataset of symbolic music encompassing not only chord symbols, but also various interrelated chord functions such as key modulation, chord inversion, secondary chords, and chord quality. We further present a novel holistic system in functional harmony recognition; a multi-task learning (MTL) architecture is implemented with the recurrent neural network (RNN) to jointly model chord functions in an end-to-end scenario. Experimental results highlight the capability of the proposed recognition system, and a promising improvement of the system by employing multi-task learning instead of single-task learning. This is one attempt to challenge the end-to-end chord recognition task from the perspective of functional harmony so as to uncover the grand structure ruling the flow of musical sound. The dataset and the source code of the proposed system is announced at https://github.com/ Tsung-Ping/functional-harmony.",
        "zenodo_id": 1492351,
        "dblp_key": "conf/ismir/ChenS18"
    },
    {
        "title": "A Flexible Approach to Automated Harmonic Analysis: Multiple Annotations of Chorales by Bach and Prætorius.",
        "author": [
            "Nathaniel Condit-Schultz",
            "Yaolong Ju",
            "Ichiro Fujinaga"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492345",
        "url": "https://doi.org/10.5281/zenodo.1492345",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/283_Paper.pdf",
        "abstract": "Despite being a core component of Western music theory, harmonic analysis remains a subjective endeavor, resistant automation. This subjectivity arises from disagreements regarding, among other things, the interpretation of contrapuntal figures, the set of \"legal\" harmonies, and how harmony relates to more abstract features like tonal function. In this paper, we provide a formal specification of harmonic analysis. We then present a novel approach to computational harmonic analysis: rather than computing harmonic analyses based on one specific set of rules, we compute all possible analyses which satisfy only basic, uncontroversial constraints. These myriad interpretations can later be filtered to extract preferred analyses; for instance, to forbid 7th chords or to prefer analyses with fewer non-chord tones. We apply this approach to two concrete musical datasets: existing encodings of 371 chorales by J.S. Bach and new encodings of 200 chorales by M. Prætorius. Through an online API users can filter and download numerous harmonic interpretations of these 571 chorales. This dataset will serve as a useful resource in the study of harmonic/functional progression, voice-leading, and the relationship between melody and harmony, and as a stepping stone towards automated harmonic analysis of more complex music.",
        "zenodo_id": 1492345,
        "dblp_key": "conf/ismir/Condit-SchultzJ18"
    },
    {
        "title": "Searching Page-Images of Early Music Scanned with OMR: A Scalable Solution Using Minimal Absent Words.",
        "author": [
            "Tim Crawford",
            "Golnaz Badkobeh",
            "David Lewis 0003"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492391",
        "url": "https://doi.org/10.5281/zenodo.1492391",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/210_Paper.pdf",
        "abstract": "We define three retrieval tasks requiring efficient search of the musical content of a collection of ~32k pageimages of 16th-century music to find: duplicates; pages with the same musical content; pages of related music.  The images are subjected to Optical Music Recognition (OMR), introducing inevitable errors. We encode pages as strings of diatonic pitch intervals, ignoring rests, to reduce the effect of such errors. We extract indices comprising lists of two kinds of 'word'. Approximate matching is done by counting the number of common words between a query page and those in the collection.  The two word-types are (a) normal ngrams and (b) minimal absent words (MAWs). The latter have three important properties for our purpose: they can be built and searched in linear time, the number of MAWs generated tends to be smaller, and they preserve the structure and order of the text, obviating the need for expensive sorting operations.  We show that retrieval performance of MAWs is comparable with ngrams, but with a marked speed improvement. We also show the effect of word length on retrieval. Our results suggest that an index of MAWs of mixed length provides a good method for these tasks which is scalable to larger collections.",
        "zenodo_id": 1492391,
        "dblp_key": "conf/ismir/CrawfordB018"
    },
    {
        "title": "Methodologies for Creating Symbolic Corpora of Western Music Before 1600.",
        "author": [
            "Julie Cumming",
            "Cory McKay",
            "Jonathan Stuchbery",
            "Ichiro Fujinaga"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492459",
        "url": "https://doi.org/10.5281/zenodo.1492459",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/46_Paper.pdf",
        "abstract": "The creation of a corpus of compositions in symbolic formats is an essential step for any project in systematic research. There are, however, many potential pitfalls, especially in early music, where scores are edited in different ways: variables include clefs, note values, types of barline, and editorial accidentals. Different score editors and optical music recognition software have their own ways of storing and exporting musical data. Choice of software and file formats, and their various parameters, can thus unintentionally bias data, as can decisions on how to interpret potentially ambiguous markings in original sources. This becomes especially problematic when data from different corpora are combined for computational processing, since observed regularities and irregularities may in fact be linked with inconsistent corpus collection methodologies, internal and external, rather than the underlying music.  This paper proposes guidelines, templates, and workflows for the creation of consistent early music corpora, and for detecting encoding biases in existing corpora. We have assembled a corpus of Renaissance duos as a sample implementation, and present machine learning experiments demonstrating how inconsistent or naïve encoding methodologies for corpus collection can distort results.",
        "zenodo_id": 1492459,
        "dblp_key": "conf/ismir/CummingMSF18"
    },
    {
        "title": "Music Mood Detection Based on Audio and Lyrics with Deep Neural Net.",
        "author": [
            "Rémi Delbouys",
            "Romain Hennequin",
            "Francesco Piccoli",
            "Jimena Royo-Letelier",
            "Manuel Moussallam"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492427",
        "url": "https://doi.org/10.5281/zenodo.1492427",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/99_Paper.pdf",
        "abstract": "1.1 Related work We consider the task of multimodal music mood prediction based on the audio signal and the lyrics of a track. We reproduce the implementation of traditional feature engineering based approaches and propose a new model based on deep learning. We compare the performance of both approaches on a database containing 18,000 tracks with associated valence and arousal values and show that our approach outperforms classical models on the arousal detection task, and that both approaches perform equally on the valence prediction task. We also compare the a posteriori fusion with fusion of modalities optimized simultaneously with each unimodal model, and observe a significant improvement of valence prediction. We release part of our database for comparison purposes.",
        "zenodo_id": 1492427,
        "dblp_key": "conf/ismir/DelbouysHPRM18"
    },
    {
        "title": "Vocals in Music Matter: the Relevance of Vocals in the Minds of Listeners.",
        "author": [
            "Andrew M. Demetriou",
            "Andreas Jansson 0001",
            "Aparna Kumar",
            "Rachel M. Bittner"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492465",
        "url": "https://doi.org/10.5281/zenodo.1492465",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/98_Paper.pdf",
        "abstract": "In music information retrieval, we often make assertions about what features of music are important to study, one of which is vocals. While the importance of vocals in music preference is both intuitive and anticipated by psychological theory, we have not found any survey studies that confirm this commonly held assertion. We address two questions: (1) what components of music are most salient to people's musical taste, and (2) how do vocals rank relative to other components of music, in regards to whether people like or dislike a song. Lastly, we explore the aspects of the voice that listeners find important. Two surveys of Spotify users were conducted. The first gathered open-format responses that were then card-sorted into semantic categories by the team of researchers. The second asked respondents to rank the semantic categories derived from the first survey. Responses indicate that vocals were a salient component in the minds of listeners. Further, vocals ranked high as a self-reported factor for a listener liking or disliking a track, among a statistically significant ranking of musical attributes. In addition, we open several new interesting problem areas that have yet to be explored in MIR.",
        "zenodo_id": 1492465,
        "dblp_key": "conf/ismir/DemetriouJKB18"
    },
    {
        "title": "The NES Music Database: A multi-instrumental dataset with expressive performance attributes.",
        "author": [
            "Chris Donahue",
            "Huanru Henry Mao",
            "Julian J. McAuley"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492455",
        "url": "https://doi.org/10.5281/zenodo.1492455",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/265_Paper.pdf",
        "abstract": "Existing research on music generation focuses on composition, but often ignores the expressive performance characteristics required for plausible renditions of resultant pieces. In this paper, we introduce the Nintendo Entertainment System Music Database (NES-MDB), a large corpus allowing for separate examination of the tasks of composition and performance. NES-MDB contains thousands of multi-instrumental songs composed for playback by the compositionally-constrained NES audio synthesizer. For each song, the dataset contains a musical score for four instrument voices as well as expressive attributes for the dynamics and timbre of each voice. Unlike datasets comprised of General MIDI files, NES-MDB includes all of the information needed to render exact acoustic performances of the original compositions. Alongside the dataset, we provide a tool that renders generated compositions as NESstyle audio by emulating the device's audio processor. Additionally, we establish baselines for the tasks of composition, which consists of learning the semantics of composing for the NES synthesizer, and performance, which involves finding a mapping between a composition and realistic expressive attributes.",
        "zenodo_id": 1492455,
        "dblp_key": "conf/ismir/DonahueMM18"
    },
    {
        "title": "Convolutional Generative Adversarial Networks with Binary Neurons for Polyphonic Music Generation.",
        "author": [
            "Hao-Wen Dong",
            "Yi-Hsuan Yang"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492377",
        "url": "https://doi.org/10.5281/zenodo.1492377",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/218_Paper.pdf",
        "abstract": "It has been shown recently that deep convolutional generative adversarial networks (GANs) can learn to generate music in the form of piano-rolls, which represent music by binary-valued time-pitch matrices. However, existing models can only generate real-valued piano-rolls and require further post-processing, such as hard thresholding (HT) or Bernoulli sampling (BS), to obtain the final binaryvalued results. In this paper, we study whether we can have a convolutional GAN model that directly creates binaryvalued piano-rolls by using binary neurons. Specifically, we propose to append to the generator an additional refiner network, which uses binary neurons at the output layer. The whole network is trained in two stages. Firstly, the generator and the discriminator are pretrained. Then, the refiner network is trained along with the discriminator to learn to binarize the real-valued piano-rolls the pretrained generator creates. Experimental results show that using binary neurons instead of HT or BS indeed leads to better results in a number of objective measures. Moreover, deterministic binary neurons perform better than stochastic ones in both objective measures and a subjective test. The source code, training data and audio examples of the generated results can be found at https://salu133445. github.io/bmusegan/.",
        "zenodo_id": 1492377,
        "dblp_key": "conf/ismir/DongY18"
    },
    {
        "title": "Learning to Listen, Read, and Follow: Score Following as a Reinforcement Learning Game.",
        "author": [
            "Matthias Dorfer",
            "Florian Henkel",
            "Gerhard Widmer"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492535",
        "url": "https://doi.org/10.5281/zenodo.1492535",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/45_Paper.pdf",
        "abstract": "Score following is the process of tracking a musical performance (audio) with respect to a known symbolic representation (a score). We start this paper by formulating score following as a multimodal Markov Decision Process, the mathematical foundation for sequential decision making. Given this formal definition, we address the score following task with state-of-the-art deep reinforcement learning (RL) algorithms such as synchronous advantage actor critic (A2C). In particular, we design multimodal RL agents that simultaneously learn to listen to music, read the scores from images of sheet music, and follow the audio along in the sheet, in an end-to-end fashion. All this behavior is learned entirely from scratch, based on a weak and potentially delayed reward signal that indicates to the agent how close it is to the correct position in the score. Besides discussing the theoretical advantages of this learning paradigm, we show in experiments that it is in fact superior compared to previously proposed methods for score following in raw sheet music images.",
        "zenodo_id": 1492535,
        "dblp_key": "conf/ismir/DorferHW18"
    },
    {
        "title": "Audio-Aligned Jazz Harmony Dataset for Automatic Chord Transcription and Corpus-based Research.",
        "author": [
            "Vsevolod Eremenko",
            "Emir Demirel",
            "Baris Bozkurt",
            "Xavier Serra"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492457",
        "url": "https://doi.org/10.5281/zenodo.1492457",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/206_Paper.pdf",
        "abstract": "In this paper we present a new dataset of time-aligned jazz harmony transcriptions. This dataset is a useful resource for content-based analysis, especially for training and evaluating chord transcription algorithms. Most of the available chord transcription datasets only contain annotations for rock and pop, and the characteristics of jazz, such as the extensive use of seventh chords, are not represented. Our dataset consists of annotations of 113 tracks selected from \"The Smithsonian Collection of Classic Jazz\" and \"Jazz: The Smithsonian Anthology,\" covering a range of performers, subgenres, and historical periods. Annotations were made by a jazz musician and contain information about the meter, structure, and chords for entire audio tracks. We also present evaluation results of this dataset using stateof-the-art chord estimation algorithms that support seventh chords. The dataset is valuable for jazz scholars interested in corpus-based research. To demonstrate this, we extract statistics for symbolic data and chroma features from the audio tracks.",
        "zenodo_id": 1492457,
        "dblp_key": "conf/ismir/EremenkoDBS18"
    },
    {
        "title": "Bridging Audio Analysis, Perception and Synthesis with Perceptually-regularized Variational Timbre Spaces.",
        "author": [
            "Philippe Esling",
            "Axel Chemla-Romeu-Santos",
            "Adrien Bitton"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492373",
        "url": "https://doi.org/10.5281/zenodo.1492373",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/219_Paper.pdf",
        "abstract": "Generative models aim to understand the properties of data, through the construction of latent spaces that allow classification and generation. However, as the learning is unsupervised, the latent dimensions are not related to perceptual properties. In parallel, music perception research has aimed to understand timbre based on human dissimilarity ratings. These lead to timbre spaces which exhibit perceptual similarities between sounds. However, they do not generalize to novel examples and do not provide an invertible mapping, preventing audio synthesis. Here, we show that Variational Auto-Encoders (VAE) can bridge these lines of research and alleviate their weaknesses by regularizing the latent spaces to match perceptual distances collected from timbre studies. Hence, we propose three types of regularization and show that they lead to spaces that are simultaneously coherent with signal properties and perceptual similarities. We show that these spaces can be used for efficient audio classification. We study how audio descriptors are organized along the latent dimensions and show that even though descriptors behave in a non-linear way across the space, they still exhibit a locally smooth evolution. We also show that, as this space generalizes to novel samples, it can be used to predict perceptual similarities of novel instruments. Finally, we exhibit the generative capabilities of our spaces, that can directly synthesize sounds with continuous evolution of timbre perception.",
        "zenodo_id": 1492373,
        "dblp_key": "conf/ismir/EslingCB18"
    },
    {
        "title": "Generalized Skipgrams for Pattern Discovery in Polyphonic Streams.",
        "author": [
            "Christoph Finkensiep",
            "Markus Neuwirth",
            "Martin Rohrmeier"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492473",
        "url": "https://doi.org/10.5281/zenodo.1492473",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/202_Paper.pdf",
        "abstract": "The discovery of patterns using a minimal set of assumptions constitutes a central challenge in the modeling of polyphonic music and complex streams in general. Skipgrams have been found to be a powerful model for capturing semi-local dependencies in sequences of entities when dependencies may not be directly adjacent (see, for instance, the problems of modeling sequences of words or letters in computational linguistics). Since common skipgrams define locality based on indices, they can only be applied to a single stream of non-overlapping entities. This paper proposes a generalized skipgram model that allows arbitrary cost functions (defining locality), efficient filtering, recursive application (skipgrams over skipgrams), and memory efficient streaming. Further, a sampling mechanism is proposed that flexibly controls runtime and output size. These generalizations and optimizations make it possible to employ skipgrams for the discovery of repeated patterns of close, nonsimultaneous events or notes. The extensions to the skipgram model provided here do not only apply to musical notes but to any list of entities that is monotonic with respect to a given cost function.",
        "zenodo_id": 1492473,
        "dblp_key": "conf/ismir/FinkensiepNR18"
    },
    {
        "title": "Two Web Applications for Exploring Melodic Patterns in Jazz Solos.",
        "author": [
            "Klaus Frieler",
            "Frank Höger",
            "Martin Pfleiderer",
            "Simon Dixon"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492533",
        "url": "https://doi.org/10.5281/zenodo.1492533",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/177_Paper.pdf",
        "abstract": "This paper presents two novel user interfaces for investigating the pattern content in monophonic jazz solos and exemplifies how these interfaces could be used for research on jazz improvisation. In jazz improvisation, patterns are of particular interest for the analysis of improvisation styles, the oral transmission of musical language, the practice of improvisation, and the psychology of creative processes. The ongoing project \"Dig That Lick\" is devoted to addressing these questions with the help of a large database of jazz solo transcriptions generated by automated melody extraction algorithms. To expose these transcriptions to jazz researchers, two prototypes of user interfaces were designed that work currently with the 456 manually transcribed jazz solos of the Weimar Jazz Database. The first one is a Shiny application that allows exploring a set of 653 of the most common patterns by eminent players. The second one is a web interface for a general two-staged pattern search in the Weimar Jazz Database featuring regular expressions. These applications aim on the one hand at an expert audience of jazz researchers to facilitate generating and testing hypotheses about patterns in jazz improvisation, and on the other hand at a wider audience of jazz teachers, students, and fans.",
        "zenodo_id": 1492533,
        "dblp_key": "conf/ismir/FrielerHPD18"
    },
    {
        "title": "Analysis of Common Design Choices in Deep Learning Systems for Downbeat Tracking.",
        "author": [
            "Magdalena Fuentes",
            "Brian McFee",
            "Hélène C. Crayencour",
            "Slim Essid",
            "Juan Pablo Bello"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492355",
        "url": "https://doi.org/10.5281/zenodo.1492355",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/203_Paper.pdf",
        "abstract": "Downbeat tracking consists of annotating a piece of musical audio with the estimated position of the first beat of each bar. In recent years, increasing attention has been paid to applying deep learning models to this task, and various architectures have been proposed, leading to a significant improvement in accuracy. However, there are few insights about the role of the various design choices and the delicate interactions between them. In this paper we offer a systematic investigation of the impact of largely adopted variants. We study the effects of the temporal granularity of the input representation (i.e. beat-level vs tatum-level) and the encoding of the networks outputs. We also investigate the potential of convolutional-recurrent networks, which have not been explored in previous downbeat tracking systems. To this end, we exploit a state-of-the-art recurrent neural network where we introduce those variants, while keeping the training data, network learning parameters and postprocessing stages fixed. We find that temporal granularity has a significant impact on performance, and we analyze its interaction with the encoding of the networks outputs.",
        "zenodo_id": 1492355,
        "dblp_key": "conf/ismir/FuentesMCEB18"
    },
    {
        "title": "A Confidence Measure For Key Labelling.",
        "author": [
            "Roman B. Gebhardt",
            "Michael Stein",
            "Athanasios Lykartsis"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492333",
        "url": "https://doi.org/10.5281/zenodo.1492333",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/72_Paper.pdf",
        "abstract": "We present a new measure for automatically estimating the confidence of musical key classification. Our approach leverages the degree of harmonic information held within a musical audio signal (its \"keyness\") as well as the steadiness of local key detections across the its duration (its \"stability\"). Using this confidence measure, musical tracks which are likely to be misclassified, i.e. those with low confidence, can then be handled differently from those analysed by standard, fully automatic key detection methods. By means of a listening test, we demonstrate that our developed features significantly correlate with listeners' ratings of harmonic complexity, steadiness and the uniqueness of key. Furthermore, we demonstrate that tracks which are incorrectly labelled using an existing key detection system obtain low confidence values. Finally, we introduce a new method called \"root note heuristics\" for the special treatment of tracks with low confidence. We show that by applying these root note heuristics, key detection results can be improved for minimalistic music.",
        "zenodo_id": 1492333,
        "dblp_key": "conf/ismir/GebhardtSL18"
    },
    {
        "title": "Jazz Solo Instrument Classification with Convolutional Neural Networks, Source Separation, and Transfer Learning.",
        "author": [
            "Juan S. Gómez",
            "Jakob Abeßer",
            "Estefanía Cano"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492481",
        "url": "https://doi.org/10.5281/zenodo.1492481",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/145_Paper.pdf",
        "abstract": "Predominant instrument recognition in ensemble recordings remains a challenging task, particularly if closelyrelated instruments such as alto and tenor saxophone need to be distinguished. In this paper, we build upon a recentlyproposed instrument recognition algorithm based on a hybrid deep neural network: a combination of convolutional and fully connected layers for learning characteristic spectral-temporal patterns. We systematically evaluate harmonic/percussive and solo/accompaniment source separation algorithms as pre-processing steps to reduce the overlap among multiple instruments prior to the instrument recognition step. For the particular use-case of solo instrument recognition in jazz ensemble recordings, we further apply transfer learning techniques to fine-tune a previously trained instrument recognition model for classifying six jazz solo instruments. Our results indicate that both source separation as pre-processing step as well as transfer learning clearly improve recognition performance, especially for smaller subsets of highly similar instruments.",
        "zenodo_id": 1492481,
        "dblp_key": "conf/ismir/GomezAC18"
    },
    {
        "title": "Matrix Co-Factorization for Cold-Start Recommendation.",
        "author": [
            "Olivier Gouvert",
            "Thomas Oberlin",
            "Cédric Févotte"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492537",
        "url": "https://doi.org/10.5281/zenodo.1492537",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/142_Paper.pdf",
        "abstract": "Song recommendation from listening counts is now a classical problem, addressed by different kinds of collaborative filtering (CF) techniques. Among them, Poisson matrix factorization (PMF) has raised a lot of interest, since it seems well-suited to the implicit data provided by listening counts. Additionally, it has proven to achieve state-ofthe-art performance while being scalable to big data. Yet, CF suffers from a critical issue, usually called cold-start problem: the system cannot recommend new songs, i.e., songs which have never been listened to. To alleviate this, one should complement the listening counts with another modality. This paper proposes a multi-modal extension of PMF applied to listening counts and tag labels extracted from the Million Song Dataset. In our model, every song is represented by the same activation pattern in each modality but with possibly different scales. As such, the method is not prone to the cold-start problem, i.e., it can learn from a single modality when the other one is not informative. Our model is symmetric (it equally uses both modalities) and we evaluate it on two tasks: new songs recommendation and tag labeling.",
        "zenodo_id": 1492537,
        "dblp_key": "conf/ismir/GouvertOF18"
    },
    {
        "title": "Semi-supervised Lyrics and Solo-singing Alignment.",
        "author": [
            "Chitralekha Gupta",
            "Rong Tong",
            "Haizhou Li 0001",
            "Ye Wang 0007"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492487",
        "url": "https://doi.org/10.5281/zenodo.1492487",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/30_Paper.pdf",
        "abstract": "We propose a semi-supervised algorithm to align lyrics to the corresponding singing vocals. The proposed method transcribes and aligns lyrics to solo-singing vocals using the imperfect transcripts from an automatic speech recognition (ASR) system and the published lyrics. The ASR provides time alignment between vocals and hypothesized lyrical content, while the non-aligned published lyrics correct the hypothesized lyrical content. The effectiveness of the proposed method is validated through three experiments. First, a human listening test shows that 73.32% of our automatically aligned sentence-level transcriptions are correct. Second, the automatically aligned sung segments are used for singing acoustic model adaptation, which reduces the word error rate (WER) of automatic transcription of sung lyrics from 72.08% to 37.15% in an open test. Third, another iteration of decoding and model adaptation increases the amount of reliably decoded segments from 44.40% to 91.96% and further reduces the WER to 36.32%. The proposed framework offers an automatic way to generate reliable alignments between lyrics and solosinging. A large-scale solo-singing and lyrics aligned corpus can be derived with the proposed method, which will be beneficial for music and singing voice related research.",
        "zenodo_id": 1492487,
        "dblp_key": "conf/ismir/GuptaT0W18"
    },
    {
        "title": "Instrument Activity Detection in Polyphonic Music using Deep Neural Networks.",
        "author": [
            "Siddharth Gururani",
            "Cameron Summers",
            "Alexander Lerch 0001"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492479",
        "url": "https://doi.org/10.5281/zenodo.1492479",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/275_Paper.pdf",
        "abstract": "Although instrument recognition has been thoroughly research, recognition in polyphonic music still faces challenges. While most research in polyphonic instrument recognition focuses on predicting the predominant instruments in a given audio recording, instrument activity detection represents a generalized problem of detecting the presence or activity of instruments in a track on a fine-grained temporal scale. We present an approach for instrument activity detection in polyphonic music with temporal resolution ranging from one second to the track level. This system allows, for instance, to retrieve specific areas of interest such as guitar solos. Three classes of deep neural networks are trained to detect up to 18 instruments. The architectures investigated in this paper are: multi-layer perceptrons, convolutional neural networks, and convolutional-recurrent neural networks. An in-depth evaluation on publicly available multi-track datasets using methods such as AUC-ROC and Label Ranking Average Precision highlights different aspects of the model performance and indicates the importance of using multiple evaluation metrics. Furthermore, we propose a new visualization to discuss instrument confusion in a multi-label scenario.",
        "zenodo_id": 1492479,
        "dblp_key": "conf/ismir/GururaniSL18"
    },
    {
        "title": "Towards Full-Pipeline Handwritten OMR with Musical Symbol Detection by U-Nets.",
        "author": [
            "Jan Hajic Jr.",
            "Matthias Dorfer",
            "Gerhard Widmer",
            "Pavel Pecina"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492389",
        "url": "https://doi.org/10.5281/zenodo.1492389",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/175_Paper.pdf",
        "abstract": "Detecting music notation symbols is the most immediate unsolved subproblem in Optical Music Recognition for musical manuscripts. We show that a U-Net architecture for semantic segmentation combined with a trivial detector already establishes a high baseline for this task, and we propose tricks that further improve detection performance: training against convex hulls of symbol masks, and multichannel output models that enable feature sharing for semantically related symbols. The latter is helpful especially for clefs, which have severe impacts on the overall OMR result. We then integrate the networks into an OMR pipeline by applying a subsequent notation assembly stage, establishing a new baseline result for pitch inference in handwritten music at an f-score of 0.81. Given the automatically inferred pitches we run retrieval experiments on handwritten scores, providing first empirical evidence that utilizing the powerful image processing models brings content-based search in large musical manuscript archives within reach.",
        "zenodo_id": 1492389,
        "dblp_key": "conf/ismir/HajicDWP18"
    },
    {
        "title": "A Generalized Parsing Framework for Generative Models of Harmonic Syntax.",
        "author": [
            "Daniel Harasim",
            "Martin Rohrmeier",
            "Timothy J. O&apos;Donnell"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492367",
        "url": "https://doi.org/10.5281/zenodo.1492367",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/258_Paper.pdf",
        "abstract": "Modeling the structure of musical pieces constitutes a central research problem for music information retrieval, music generation, and musicology. At the present, models of harmonic syntax face challenges on the tasks of detecting local and higher-level modulations (most previous models assume a priori knowledge of key), computing connected parse trees for long sequences, and parsing sequences that do not end with tonic chords, but in turnarounds. This paper addresses those problems by proposing a new generative formalism Probabilistic Abstract Context-Free Grammars (PACFGs) to address these issues, and presents variants of standard parsing algorithms that efficiently enumerate all possible parses of long chord sequences and to estimate their probabilities. PACFGs specifically allow for structured non-terminal symbols in rich and highly flexible feature spaces. The inference procedure moreover takes advantage of these abstractions by sharing probability mass between grammar rules over joint features. The paper presents a model of the harmonic syntax of Jazz using this formalism together with stochastic variational inference to learn the probabilistic parameters of a grammar from a corpus of Jazz-standards. The PACFG model outperforms the standard context-free approach while reducing the number of free parameters and performing key finding on the fly.",
        "zenodo_id": 1492367,
        "dblp_key": "conf/ismir/HarasimRO18"
    },
    {
        "title": "An Energy-based Generative Sequence Model for Testing Sensory Theories of Western Harmony.",
        "author": [
            "Peter M. C. Harrison",
            "Marcus T. Pearce"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492369",
        "url": "https://doi.org/10.5281/zenodo.1492369",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/215_Paper.pdf",
        "abstract": "The relationship between sensory consonance and Western harmony is an important topic in music theory and psychology. We introduce new methods for analysing this relationship, and apply them to large corpora representing three prominent genres of Western music: classical, popular, and jazz music. These methods centre on a generative sequence model with an exponential-family energy-based form that predicts chord sequences from continuous features. We use this model to investigate one aspect of instantaneous consonance (harmonicity) and two aspects of sequential consonance (spectral distance and voice-leading distance). Applied to our three musical genres, the results generally support the relationship between sensory consonance and harmony, but lead us to question the high importance attributed to spectral distance in the psychological literature. We anticipate that our methods will provide a useful platform for future work linking music psychology to music theory.",
        "zenodo_id": 1492369,
        "dblp_key": "conf/ismir/HarrisonP18"
    },
    {
        "title": "Onsets and Frames: Dual-Objective Piano Transcription.",
        "author": [
            "Curtis Hawthorne",
            "Erich Elsen",
            "Jialin Song",
            "Adam Roberts",
            "Ian Simon",
            "Colin Raffel",
            "Jesse H. Engel",
            "Sageev Oore",
            "Douglas Eck"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492341",
        "url": "https://doi.org/10.5281/zenodo.1492341",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/19_Paper.pdf",
        "abstract": "We advance the state of the art in polyphonic piano music transcription by using a deep convolutional and recurrent neural network which is trained to jointly predict onsets and frames. Our model predicts pitch onset events and then uses those predictions to condition framewise pitch predictions. During inference, we restrict the predictions from the framewise detector by not allowing a new note to start unless the onset detector also agrees that an onset for that pitch is present in the frame. We focus on improving onsets and offsets together instead of either in isolation as we believe this correlates better with human musical perception. Our approach results in over a 100% relative improvement in note F1 score (with offsets) on the MAPS dataset. Furthermore, we extend the model to predict relative velocities of normalized audio which results in more natural-sounding transcriptions.",
        "zenodo_id": 1492341,
        "dblp_key": "conf/ismir/HawthorneESRSRE18"
    },
    {
        "title": "Audio Based Disambiguation of Music Genre Tags.",
        "author": [
            "Romain Hennequin",
            "Jimena Royo-Letelier",
            "Manuel Moussallam"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492499",
        "url": "https://doi.org/10.5281/zenodo.1492499",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/163_Paper.pdf",
        "abstract": "In this paper, we propose to infer music genre embeddings from audio datasets carrying semantic information about genres. We show that such embeddings can be used for disambiguating genre tags (identification of different labels for the same genre, tag translation from a tag system to another, inference of hierarchical taxonomies on these genre tags). These embeddings are built by training a deep convolutional neural network genre classifier with large audio datasets annotated with a flat tag system. We show empirically that they makes it possible to retrieve the original taxonomy of a tag system, spot duplicates tags and translate tags from a tag system to another.",
        "zenodo_id": 1492499,
        "dblp_key": "conf/ismir/HennequinRM18"
    },
    {
        "title": "OpenMIC-2018: An Open Data-set for Multiple Instrument Recognition.",
        "author": [
            "Eric Humphrey",
            "Simon Durand",
            "Brian McFee"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492445",
        "url": "https://doi.org/10.5281/zenodo.1492445",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/248_Paper.pdf",
        "abstract": "Identification of instruments in polyphonic recordings is a challenging, but fundamental problem in music information retrieval. While there has been significant progress in developing predictive models for this and related classification tasks, we as a community lack a common data-set which is large, freely available, diverse, and representative of naturally occurring recordings. This limits our ability to measure the efficacy of computational models. This article describes the construction of a new, open data-set for multi-instrument recognition. The dataset contains 20,000 examples of Creative Commons-licensed music available on the Free Music Archive. Each example is a 10-second excerpt which has been partially labeled for the presence or absence of 20 instrument classes by annotators on a crowd-sourcing platform. We describe in detail how the instrument taxonomy was constructed, how the dataset was sampled and annotated, and compare its characteristics to similar, previous data-sets. Finally, we present experimental results and baseline model performance to motivate future work.",
        "zenodo_id": 1492445,
        "dblp_key": "conf/ismir/HumphreyDM18"
    },
    {
        "title": "Frame-level Instrument Recognition by Timbre and Pitch.",
        "author": [
            "Yun-Ning Hung",
            "Yi-Hsuan Yang"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492363",
        "url": "https://doi.org/10.5281/zenodo.1492363",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/55_Paper.pdf",
        "abstract": "Instrument recognition is a fundamental task in music information retrieval, yet little has been done to predict the presence of instruments in multi-instrument music for each time frame. This task is important for not only automatic transcription but also many retrieval problems. In this paper, we use the newly released MusicNet dataset to study this front, by building and evaluating a convolutional neural network for making frame-level instrument prediction. We consider it as a multi-label classification problem for each frame and use frame-level annotations as the supervisory signal in training the network. Moreover, we experiment with different ways to incorporate pitch information to our model, with the premise that doing so informs the model the notes that are active per frame, and also encourages the model to learn relative rates of energy buildup in the harmonic partials of different instruments. Experiments show salient performance improvement over baseline methods. We also report an analysis probing how pitch information helps the instrument prediction task. Code and experiment details can be found at https://biboamy. github.io/instrument-recognition/.",
        "zenodo_id": 1492363,
        "dblp_key": "conf/ismir/HungY18"
    },
    {
        "title": "A Timbre-based Approach to Estimate Key Velocity from Polyphonic Piano Recordings.",
        "author": [
            "Dasaem Jeong",
            "Taegyun Kwon",
            "Juhan Nam"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492359",
        "url": "https://doi.org/10.5281/zenodo.1492359",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/196_Paper.pdf",
        "abstract": "Estimating the key velocity of each note from polyphonic piano music is a highly challenging task. Previous work addressed the problem by estimating note intensity using a polyphonic note model. However, they are limited because the note intensity is vulnerable to various factors in a recording environment. In this paper, we propose a novel method to estimate the key velocity focusing on timbre change which is another cue associated with the key velocity. To this end, we separate individual notes of polyphonic piano music using non-negative matrix factorization (NMF) and feed them into a neural network that is trained to discriminate the timbre change according to the key velocity. Combining the note intensity from the separated notes with the statistics of the neural network prediction, the proposed method estimates the key velocity in the dimension of MIDI note velocity. The evaluation on Saarland Music Data and the MAPS dataset shows promising results in terms of robustness to changes in the recording environment.",
        "zenodo_id": 1492359,
        "dblp_key": "conf/ismir/JeongKN18"
    },
    {
        "title": "Evaluating a Collection of Sound-Tracing Data of Melodic Phrases.",
        "author": [
            "Tejaswinee Kelkar",
            "Udit Roy",
            "Alexander Refsum Jensenius"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492347",
        "url": "https://doi.org/10.5281/zenodo.1492347",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/209_Paper.pdf",
        "abstract": "Melodic contour, the 'shape' of a melody, is a common way to visualize and remember a musical piece. The purpose of this paper is to explore the building blocks of a future 'gesture-based' melody retrieval system. We present a dataset containing 16 melodic phrases from four musical styles and with a large range of contour variability. This is accompanied by full-body motion capture data of 26 participants performing sound-tracing to the melodies. The dataset is analyzed using canonical correlation analysis (CCA), and its neural network variant (Deep CCA), to understand how melodic contours and sound tracings relate to each other. The analyses reveal non-linear relationships between sound and motion. The link between pitch and verticality does not appear strong enough for complex melodies. We also find that descending melodic contours have the least correlation with tracings.",
        "zenodo_id": 1492347,
        "dblp_key": "conf/ismir/KelkarRJ18"
    },
    {
        "title": "Aligned Sub-Hierarchies: A Structure-based Approach to the Cover Song Task.",
        "author": [
            "Katherine M. Kinnaird"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492483",
        "url": "https://doi.org/10.5281/zenodo.1492483",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/81_Paper.pdf",
        "abstract": "Extending previous structure-based approaches to the song comparison tasks such as the fingerprint and cover song tasks, this paper introduces the aligned sub-hierarchies (AsH) representation. Built by applying a post-processing technique to the aligned hierarchies of a song, the AsH representation is the set of unique aligned hierarchies for repeats (called AHR) encoded in the original aligned hierarchies of the whole song. Effectively each AHR within AsH is a section of the aligned hierarchies for the original song. Like aligned hierarchies, the AsH representation can be embedded into a classification space with a natural metric that makes inter-song comparisons based on sections of the songs. Experiments addressing a version of the cover song task on score-based data using AsH as the basis of inter-song comparison demonstrate potential of AsH-based approaches for MIR tasks.",
        "zenodo_id": 1492483,
        "dblp_key": "conf/ismir/Kinnaird18"
    },
    {
        "title": "Improved Chord Recognition by Combining Duration and Harmonic Language Models.",
        "author": [
            "Filip Korzeniowski",
            "Gerhard Widmer"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492335",
        "url": "https://doi.org/10.5281/zenodo.1492335",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/300_Paper.pdf",
        "abstract": "Chord recognition systems typically comprise an acoustic model that predicts chords for each audio frame, and a temporal model that casts these predictions into labelled chord segments. However, temporal models have been shown to only smooth predictions, without being able to incorporate musical information about chord progressions. Recent research discovered that it might be the low hierarchical level such models have been applied to (directly on audio frames) which prevents learning musical relationships, even for expressive models such as recurrent neural networks (RNNs). However, if applied on the level of chord sequences, RNNs indeed can become powerful chord predictors. In this paper, we disentangle temporal models into a harmonic language model—to be applied on chord sequences—and a chord duration model that connects the chord-level predictions of the language model to the frame-level predictions of the acoustic model. In our experiments, we explore the impact of each model on the chord recognition score, and show that using harmonic language and duration models improves the results.",
        "zenodo_id": 1492335,
        "dblp_key": "conf/ismir/KorzeniowskiW18"
    },
    {
        "title": "Genre-Agnostic Key Classification With Convolutional Neural Networks.",
        "author": [
            "Filip Korzeniowski",
            "Gerhard Widmer"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492399",
        "url": "https://doi.org/10.5281/zenodo.1492399",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/7_Paper.pdf",
        "abstract": "We propose modifications to the model structure and training procedure to a recently introduced Convolutional Neural Network for musical key classification. These modifications enable the network to learn a genre-independent model that performs better than models trained for specific music styles, which has not been the case in existing work. We analyse this generalisation capability on three datasets comprising distinct genres. We then evaluate the model on a number of unseen data sets, and show its superior performance compared to the state of the art. Finally, we investigate the model's performance on short excerpts of audio. From these experiments, we conclude that models need to consider the harmonic coherence of the whole piece when classifying the local key of short segments of audio.",
        "zenodo_id": 1492399,
        "dblp_key": "conf/ismir/KorzeniowskiW18a"
    },
    {
        "title": "VenueRank: Identifying Venues that Contribute to Artist Popularity.",
        "author": [
            "Emmanouil Krasanakis",
            "Emmanouil Schinas",
            "Symeon Papadopoulos",
            "Yiannis Kompatsiaris",
            "Pericles A. Mitkas"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492513",
        "url": "https://doi.org/10.5281/zenodo.1492513",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/91_Paper.pdf",
        "abstract": "An important problem in the live music industry is finding venues that help expose artists to wider audiences. However, it is often difficult to obtain live music audience data to tackle this task. In this work, we investigate whether important venues can instead be inferred through social media data. Our approach consists of employing bipartite graph ranking algorithms to help discover important venues in artist-venue graphs mined from Facebook. We use both well-established algorithms, such as BiRank, and a modification of their common iterative scheme that avoids the impact of possibly erroneous heuristics to the ranking, which we call VenueRank. Resulting venue ranks are compared to those obtained from feature extraction for predicting the most listened artists and large listener increments in Spotify. This comparison yields high correlation between venue importance for listener prediction and bipartite graph ranking algorithms, with VenueRank found more robust against overfitting.",
        "zenodo_id": 1492513,
        "dblp_key": "conf/ismir/KrasanakisSPKM18"
    },
    {
        "title": "Visualization of Audio Data Using Stacked Graphs.",
        "author": [
            "Mathieu Lagrange",
            "Mathias Rossignol",
            "Grégoire Lafay"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492531",
        "url": "https://doi.org/10.5281/zenodo.1492531",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/96_Paper.pdf",
        "abstract": "In this paper, we study the benefit of considering stacked graphs to display audio data. Thanks to a careful use of layering of the spectral information, the resulting display is both concise and intuitive. Compared to the spectrogram display, it allows the reader to focus more on the temporal aspect of the time/frequency decomposition while keeping an abstract view of the spectral information. The use of such a display is validated using two perceptual experiments that demonstrate the potential of the approach. The first considers the proposed display to perform an identification task of the musical instrument and the second considers the proposed display to evaluate the technical level of a musical performer. Both experiments show the potential of the display and potential applications scenarios in musical training are discussed.",
        "zenodo_id": 1492531,
        "dblp_key": "conf/ismir/LagrangeRL18"
    },
    {
        "title": "A Predictive Model for Music based on Learned Interval Representations.",
        "author": [
            "Stefan Lattner",
            "Maarten Grachten",
            "Gerhard Widmer"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492331",
        "url": "https://doi.org/10.5281/zenodo.1492331",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/179_Paper.pdf",
        "abstract": "Connectionist sequence models (e.g., RNNs) applied to musical sequences suffer from two known problems: First, they have strictly \"absolute pitch perception\". Therefore, they fail to generalize over musical concepts which are commonly perceived in terms of relative distances between pitches (e.g., melodies, scale types, modes, cadences, or chord types). Second, they fall short of capturing the concepts of repetition and musical form. In this paper we introduce the recurrent gated autoencoder (RGAE), a recurrent neural network which learns and operates on interval representations of musical sequences. The relative pitch modeling increases generalization and reduces sparsity in the input data. Furthermore, it can learn sequences of copy-and-shift operations (i.e. chromatically transposed copies of musical fragments)—a promising capability for learning musical repetition structure. We show that the RGAE improves the state of the art for general connectionist sequence models in learning to predict monophonic melodies, and that ensembles of relative and absolute music processing models improve the results appreciably. Furthermore, we show that the relative pitch processing of the RGAE naturally facilitates the learning and the generation of sequences of copy-and-shift operations, wherefore the RGAE greatly outperforms a common absolute pitch recurrent neural network on this task.",
        "zenodo_id": 1492331,
        "dblp_key": "conf/ismir/LattnerGW18"
    },
    {
        "title": "Learning Interval Representations from Polyphonic Music Sequences.",
        "author": [
            "Stefan Lattner",
            "Maarten Grachten",
            "Gerhard Widmer"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492503",
        "url": "https://doi.org/10.5281/zenodo.1492503",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/172_Paper.pdf",
        "abstract": "Many music theoretical constructs (such as scale types, modes, cadences, and chord types) are defined in terms of pitch intervals—relative distances between pitches. Therefore, when computer models are employed in music tasks, it can be useful to operate on interval representations rather than on the raw musical surface. Moreover, interval representations are transposition-invariant, valuable for tasks like audio alignment, cover song detection and music structure analysis. We employ a gated autoencoder to learn fixed-length, invertible and transposition-invariant interval representations from polyphonic music in the symbolic domain and in audio. An unsupervised training method is proposed yielding an organization of intervals in the representation space which is musically plausible. Based on the representations, a transposition-invariant self-similarity matrix is constructed and used to determine repeated sections in symbolic music and in audio, yielding competitive results in the MIREX task \"Discovery of Repeated Themes and Sections\".",
        "zenodo_id": 1492503,
        "dblp_key": "conf/ismir/LattnerGW18a"
    },
    {
        "title": "Revisiting Singing Voice Detection: A quantitative review and the future outlook.",
        "author": [
            "Kyungyun Lee",
            "Keunwoo Choi",
            "Juhan Nam"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492463",
        "url": "https://doi.org/10.5281/zenodo.1492463",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/38_Paper.pdf",
        "abstract": "Since the vocal component plays a crucial role in popular music, singing voice detection has been an active research topic in music information retrieval. Although several proposed algorithms have shown high performances, we argue that there is still room for improving the singing voice detection system. In order to identify the area of improvement, we first perform an error analysis on three recent singing voice detection systems. Based on the analysis, we design novel methods to test the systems on multiple sets of internally curated and generated data to further examine the pitfalls, which are not clearly revealed with the currently available datasets. From the experiment results, we also propose several directions towards building a more robust singing voice detector.",
        "zenodo_id": 1492463,
        "dblp_key": "conf/ismir/LeeCN18"
    },
    {
        "title": "Skeleton Plays Piano: Online Generation of Pianist Body Movements from MIDI Performance.",
        "author": [
            "Bochen Li",
            "Akira Maezawa",
            "Zhiyao Duan"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492387",
        "url": "https://doi.org/10.5281/zenodo.1492387",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/109_Paper.pdf",
        "abstract": "Generating expressive body movements of a pianist for a given symbolic sequence of key depressions is important for music interaction, but most existing methods cannot incorporate musical context information and generate movements of body joints that are further away from the fingers such as head and shoulders. This paper addresses such limitations by directly training a deep neural network system to map a MIDI note stream and additional metric structures to a skeleton sequence of a pianist playing a keyboard instrument in an online fashion. Experiments show that (a) incorporation of metric information yields in 4% smaller error, (b) the model is capable of learning the motion behavior of a specific player, and (c) no significant difference between the generated and real human movements is observed by human subjects in 75% of the pieces.",
        "zenodo_id": 1492387,
        "dblp_key": "conf/ismir/LiMD18"
    },
    {
        "title": "On the Impact of Music on Decision Making in Cooperative Tasks.",
        "author": [
            "Elad Liebman",
            "Corey N. White",
            "Peter Stone"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492511",
        "url": "https://doi.org/10.5281/zenodo.1492511",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/298_Paper.pdf",
        "abstract": "Numerous studies have demonstrated that mood affects emotional and cognitive processing. Previous work has established that music-induced mood can measurably alter people's behavior in different contexts. However, the nature of how decision-making is affected by music in social settings hasn't been sufficiently explored. The goal of this study is to examine which aspects of people's decision making in inter-social tasks are affected when exposed to music. For this purpose, we devised an experiment in which people drove a simulated car through an intersection while listening to music. The intersection was not empty, as another simulated vehicle, controlled autonomously, was also crossing the intersection in a different direction. Our results indicate that music indeed alters people's behavior with respect to this social task. To further understand the correspondence between auditory features and decision making, we have also studied how individual aspects of music affected response patterns.",
        "zenodo_id": 1492511,
        "dblp_key": "conf/ismir/LiebmanWS18"
    },
    {
        "title": "Controlled Vocabularies for Music Metadata.",
        "author": [
            "Pasquale Lisena",
            "Konstantin Todorov",
            "Cécile Cecconi",
            "Françoise Leresche",
            "Isabelle Canno",
            "Frédéric Puyrenier",
            "Martine Voisin",
            "Thierry Le Meur",
            "Raphaël Troncy"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492441",
        "url": "https://doi.org/10.5281/zenodo.1492441",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/68_Paper.pdf",
        "abstract": "We present a set of music-specific controlled vocabularies, formalized using Semantic Web languages, describing topics like musical genres, keys, or medium of performance. We have collected a number of existing vocabularies in various formats, converted them to SKOS and performed the interconnection of their equivalent terms. In addition, novel vocabularies, not available online before, have been designed by an editorial team. Next to multilingual labels and definitions, we provide hierarchical relations as well as links to external resources. We also show the application of those vocabularies for the production of vector embeddings, allowing for the calculation of distances between keys or between instruments.",
        "zenodo_id": 1492441,
        "dblp_key": "conf/ismir/LisenaTCLCPVMT18"
    },
    {
        "title": "Vocal Melody Extraction with Semantic Segmentation and Audio-symbolic Domain Transfer Learning.",
        "author": [
            "Wei Tsung Lu",
            "Li Su 0004"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492467",
        "url": "https://doi.org/10.5281/zenodo.1492467",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/286_Paper.pdf",
        "abstract": "The melody extraction problem is analogue to semantic segmentation on a time-frequency image, in which every pixel on the image is classified as a part of a melody object or not. Such an approach can benefit from a signal processing method that helps to enhance the true pitch contours on an image, and, a music language model with structural information on large-scale symbolic music data to be transfer into an audio-based model. In this paper, we propose a novel melody extraction system, using a deep convolutional neural network (DCNN) with dilated convolution as the semantic segmentation tool. The candidate pitch contours on the time-frequency image are enhanced by combining the spectrogram and cepstral-based features. Moreover, an adaptive progressive neural network is employed to transfer the semantic segmentation model in the symbolic domain to the one in the audio domain. This paper makes an attempt to bridge the semantic gaps between signal-level features and perceived melodies, and between symbolic data and audio data. Experiments show competitive accuracy of the proposed method on various datasets.",
        "zenodo_id": 1492467,
        "dblp_key": "conf/ismir/LuS18"
    },
    {
        "title": "Transferring the Style of Homophonic Music Using Recurrent Neural Networks and Autoregressive Model.",
        "author": [
            "Wei Tsung Lu",
            "Li Su 0004"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492523",
        "url": "https://doi.org/10.5281/zenodo.1492523",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/107_Paper.pdf",
        "abstract": "Utilizing deep learning techniques to generate musical contents has caught wide attention in recent years. Within this context, this paper investigates a specific problem related to music generation, music style transfer. This practical problem aims to alter the style of a given music piece from one to another while preserving the essence of that piece, such as melody and chord progression. In particular, we discuss the style transfer of homophonic music, composed of a predominant melody part and an accompaniment part, where the latter is modified through Gibbs sampling on a generative model combining recurrent neural networks and autoregressive models. Both objective and subjective test experiment are performed to assess the performance of transferring the style of an arbitrary music piece having a homophonic texture into two different distinct styles, Bachs chorales and Jazz.",
        "zenodo_id": 1492523,
        "dblp_key": "conf/ismir/LuS18a"
    },
    {
        "title": "Learning Domain-Adaptive Latent Representations of Music Signals Using Variational Autoencoders.",
        "author": [
            "Yin-Jyun Luo",
            "Li Su 0004"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492501",
        "url": "https://doi.org/10.5281/zenodo.1492501",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/169_Paper.pdf",
        "abstract": "In this paper, we tackle the problem of domain-adaptive representation learning for music processing. Domain adaptation is an approach aiming to eliminate the distributional discrepancy of the modeling data, so as to transfer learnable knowledge from one domain to another. With its great success in the fields of computer vision and natural language processing, domain adaptation also shows great potential in music processing, for music is essentially a highly-structured semantic system having domaindependent information. Our proposed model contains a Variational Autoencoder (VAE) that encodes the training data into a latent space, and the resulting latent representations along with its model parameters are then reused to regularize the representation learning of the downstream task where the data are in the other domain. The experiments on cross-domain music alignment, namely an audioto-MIDI alignment, and a monophonic-to-polyphonic music alignment of singing voice show that the learned representations lead to better higher alignment accuracy than that using conventional features. Furthermore, a preliminary experiment on singing voice source separation, by regarding the mixture and the voice as two distinct domains, also demonstrates the capability to solve music processing problems from the perspective of domain-adaptive representation learning.",
        "zenodo_id": 1492501,
        "dblp_key": "conf/ismir/LuoS18"
    },
    {
        "title": "The Northwestern University Source Separation Library.",
        "author": [
            "Ethan Manilow",
            "Prem Seetharaman",
            "Bryan Pardo"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492407",
        "url": "https://doi.org/10.5281/zenodo.1492407",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/37_Paper.pdf",
        "abstract": "Audio source separation is the process of isolating individual sonic elements from a mixture or auditory scene. We present the Northwestern University Source Separation Library, or nussl for short. nussl (pronounced 'nuzzle') is an open-source, object-oriented audio source separation library implemented in Python. nussl provides implementations for many existing source separation algorithms and a platform for creating the next generation of source separation algorithms. By nature of its design, nussl easily allows new algorithms to be benchmarked against existing algorithms on established data sets and facilitates development of new variations on algorithms. Here, we present the design methodologies in nussl, two experiments using it, and use nussl to showcase benchmarks for some algorithms contained within.",
        "zenodo_id": 1492407,
        "dblp_key": "conf/ismir/ManilowSP18"
    },
    {
        "title": "Conditioning Deep Generative Raw Audio Models for Structured Automatic Music.",
        "author": [
            "Rachel Manzelli",
            "Vijay Thakkar",
            "Ali Siahkamari",
            "Brian Kulis"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492375",
        "url": "https://doi.org/10.5281/zenodo.1492375",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/192_Paper.pdf",
        "abstract": "Existing automatic music generation approaches that feature deep learning can be broadly classified into two types: raw audio models and symbolic models. Symbolic models, which train and generate at the note level, are currently the more prevalent approach; these models can capture long-range dependencies of melodic structure, but fail to grasp the nuances and richness of raw audio generations. Raw audio models, such as DeepMind's WaveNet, train directly on sampled audio waveforms, allowing them to produce realistic-sounding, albeit unstructured music. In this paper, we propose an automatic music generation methodology combining both of these approaches to create structured, realistic-sounding compositions. We consider a Long Short Term Memory network to learn the melodic structure of different styles of music, and then use the unique symbolic generations from this model as a conditioning input to a WaveNet-based raw audio generator, creating a model for automatic, novel music. We then evaluate this approach by showcasing results of this work.",
        "zenodo_id": 1492375,
        "dblp_key": "conf/ismir/ManzelliTSK18"
    },
    {
        "title": "SE and SNL diagrams: Flexible data structures for MIR.",
        "author": [
            "Melissa R. McGuirl",
            "Katherine M. Kinnaird",
            "Claire Savard",
            "Erin H. Bugbee"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492419",
        "url": "https://doi.org/10.5281/zenodo.1492419",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/105_Paper.pdf",
        "abstract": "to interpret. The matrix-based representations commonly used in MIR tasks are often difficult This work introduces start-end (SE) diagrams and start(normalized)length (SNL) diagrams, two novel structure-based representations for sequential music data. Inspired by methods from topological data analysis, both SE and SNL diagrams come equipped with efficiently computable and stable metrics. Utilizing SE or SNL diagrams as input, we address the cover song task for score-based data with high accuracy. While both representations are concisely defined and flexible, SNL diagrams in particular address issues introduced by commonly used resampling methods.",
        "zenodo_id": 1492419,
        "dblp_key": "conf/ismir/McGuirlKSB18"
    },
    {
        "title": "JSYMBOLIC 2.2: Extracting Features from Symbolic Music for use in Musicological and MIR Research.",
        "author": [
            "Cory McKay",
            "Julie Cumming",
            "Ichiro Fujinaga"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492421",
        "url": "https://doi.org/10.5281/zenodo.1492421",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/26_Paper.pdf",
        "abstract": "jSymbolic is an open-source platform for extracting features from symbolic music. These features can serve as inputs to machine learning algorithms, or they can be analyzed statistically to derive musicological insights.  jSymbolic implements 246 unique features, comprising 1497 different values, making it by far the most extensive symbolic feature extractor to date. These features are designed to be applicable to a diverse range of musics, and may be extracted from both symbolic music files as a whole and from windowed subsets of them. Researchers can also use jSymbolic as a platform for developing and distributing their own bespoke features, as it has an easily extensible plug-in architecture.  In addition to implementing 135 new unique features, version 2.2 of jSymbolic places a special focus on functionality for avoiding biases associated with how symbolic music is encoded. In addition, new interface elements and documentation improve convenience, ease-of-use and accessibility to researchers with diverse ranges of technical expertise. jSymbolic now includes a GUI, command-line interface, API , flexible configuration file format, extensive manual and detailed tutorial.  The enhanced effectiveness of jSymbolic 2.2's features is demonstrated in two sets of experiments: 1) genre classification and 2) Renaissance composer attribution.",
        "zenodo_id": 1492421,
        "dblp_key": "conf/ismir/McKayCF18"
    },
    {
        "title": "Evaluating Automatic Polyphonic Music Transcription.",
        "author": [
            "Andrew McLeod",
            "Mark Steedman"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492339",
        "url": "https://doi.org/10.5281/zenodo.1492339",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/148_Paper.pdf",
        "abstract": "Automatic Music Transcription (AMT) is an important task in music information retrieval. Prior work has focused on multiple fundamental frequency estimation (multi-pitch detection), the conversion of an audio signal into a timefrequency representation such as a MIDI file. It is less common to annotate this output with musical features such as voicing information, metrical structure, and harmonic information, though these are important aspects of a complete transcription. Evaluation of these features is most often performed separately and independent of multi-pitch detection; however, these features are non-independent. We therefore introduce M V 2H, a quantitative, automatic, joint evaluation metric based on musicological principles, and show its effectiveness through the use of specific examples. The metric is modularised in such a way that it can still be used with partially performed annotation— for example, when the transcription process has been applied to some transduced format such as MIDI (which may itself be the result of multi-pitch detection). The code for the evaluation metric described here is available at https://www.github.com/apmcleod/MV2H.",
        "zenodo_id": 1492339,
        "dblp_key": "conf/ismir/McleodS18"
    },
    {
        "title": "Meter Detection and Alignment of MIDI Performance.",
        "author": [
            "Andrew McLeod",
            "Mark Steedman"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492357",
        "url": "https://doi.org/10.5281/zenodo.1492357",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/136_Paper.pdf",
        "abstract": "Metrical alignment is an integral part of any complete automatic music transcription (AMT) system. In this paper, we present an HMM for both detecting the metrical structure of given live performance MIDI data, and aligning that structure with the underlying notes. The model takes as input only a list of the notes present in a performance, and labels bars, beats, and sub beats in time. We also present an incremental algorithm which can perform inference on the model efficiently using a modified Viterbi search. We propose a new metric designed for the task, and using it, we show that our model achieves state-of-the-art performance on a corpus of metronomically aligned MIDI data, as well as a second corpus of live performance MIDI data. The code for the model described in this paper is available at https://www.github.com/apmcleod/met-align.",
        "zenodo_id": 1492357,
        "dblp_key": "conf/ismir/McleodS18a"
    },
    {
        "title": "StructureNet: Inducing Structure in Generated Melodies.",
        "author": [
            "Gabriele Medeot",
            "Srikanth Cherla",
            "Katerina Kosta",
            "Matt McVicar",
            "Samer Abdallah",
            "Marco Selvi",
            "Ed Newton-Rex",
            "Kevin Webster"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492519",
        "url": "https://doi.org/10.5281/zenodo.1492519",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/126_Paper.pdf",
        "abstract": "We present the StructureNet - a recurrent neural network for inducing structure in machine-generated compositions. This model resides in a musical structure space and works in tandem with a probabilistic music generation model as a modifying agent. It favourably biases the probabilities of those notes that result in the occurrence of structural elements it has learnt from a dataset. It is extremely flexible in that it is able to work with any such probabilistic model, it works well when training data is limited, and the types of structure it can be made to induce are highly customisable. We demonstrate through our experiments on a subset of the Nottingham dataset that melodies generated by a recurrent neural network based melody model are indeed more structured in the presence of the StructureNet.",
        "zenodo_id": 1492519,
        "dblp_key": "conf/ismir/MedeotCKMASNW18"
    },
    {
        "title": "DALI: A Large Dataset of Synchronized Audio, Lyrics and notes, Automatically Created using Teacher-student Machine Learning Paradigm..",
        "author": [
            "Gabriel Meseguer-Brocal",
            "Alice Cohen-Hadria",
            "Geoffroy Peeters"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.3576083",
        "url": "https://doi.org/10.5281/zenodo.3576083",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/35_Paper.pdf",
        "abstract": "<p>The DALI dataset is&nbsp;a large Dataset of synchronised audio, lyrics and notes for the audio&nbsp;full-duration, with &ndash; its time-aligned lyrics and &ndash; its time-aligned notes (of the vocal melody). Lyrics are described according to four levels of granularity: notes (and textual information un- derlying a given note), words, lines and paragraphs. For each song, we also provide additional multimodal information such as genre, language, musician, album covers or links to video clips.&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>Go to&nbsp;<a href=\"https://github.com/gabolsgabs/DALI\">https://github.com/gabolsgabs/DALI</a>&nbsp;where you can find all the tools to work with the DALI dataset and a&nbsp;detailed description of how to use it.</p>\n\n<p>&nbsp;</p>\n\n<p>For this version cite the <a href=\"https://transactions.ismir.net/articles/10.5334/tismir.30/\">article</a>:</p>\n\n<blockquote>\n<pre>@article{meseguer2020creating,\n  title={Creating DALI, a Large Dataset of Synchronized Audio, Lyrics, and Notes},\n  author={Meseguer-Brocal, Gabriel and Cohen-Hadria, Alice and Peeters, Geoffroy},\n  journal={Transactions of the International Society for Music Information Retrieval}, volume={3}, number={1}, year={2020},\n  publisher={Ubiquity Press}\n}\n</pre>\n</blockquote>\n\n<p>&nbsp;</p>\n\n<p>and the original <a href=\"https://zenodo.org/record/1492443\">paper</a>:</p>\n\n<blockquote>\n<pre>@inproceedings{meseguer2019dali,\n  title={Dali: A large dataset of synchronized audio, lyrics and notes, automatically created using teacher-student machine learning paradigm},\n  author={Meseguer-Brocal, Gabriel and Cohen-Hadria, Alice and Peeters, Geoffroy},\n  journal={arXiv preprint arXiv:1906.10606},\n  year={2019}\n}\n</pre>\n</blockquote>\n\n<p>&nbsp;</p>\n\n<p>This research has received funding from the French National Research Agency under the contract ANR-16-CE23-0017-01 (WASABI project)&nbsp;</p>",
        "zenodo_id": 3576083,
        "dblp_key": "conf/ismir/Meseguer-Brocal18"
    },
    {
        "title": "Understanding a Deep Machine Listening Model Through Feature Inversion.",
        "author": [
            "Saumitra Mishra",
            "Bob L. Sturm",
            "Simon Dixon"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492527",
        "url": "https://doi.org/10.5281/zenodo.1492527",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/272_Paper.pdf",
        "abstract": "Methods for interpreting machine learning models can help one understand their global and/or local behaviours, and thereby improve them. In this work, we apply a global analysis method to a machine listening model, which essentially inverts the features generated in a model back into an interpretable form like a sonogram. We demonstrate this method for a state-of-the-art singing voice detection model. We train up-convolutional neural networks to invert the feature generated at each layer of the model. The results suggest that the deepest fully connected layer of the model does not preserve temporal and harmonic structures, but that the inverted features from the deepest convolutional layer do. Moreover, a qualitative analysis of a large number of inputs suggests that the deepest layer in the model learns a decision function as the information it preserves depends on the class label associated with an input.",
        "zenodo_id": 1492527,
        "dblp_key": "conf/ismir/MishraSD18"
    },
    {
        "title": "Shared Generative Representation of Auditory Concepts and EEG to Reconstruct Perceived and Imagined Music.",
        "author": [
            "André Ofner",
            "Sebastian Stober"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492433",
        "url": "https://doi.org/10.5281/zenodo.1492433",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/101_Paper.pdf",
        "abstract": "Retrieving music information from brain activity is a challenging and still largely unexplored research problem. In this paper we investigate the possibility to reconstruct perceived and imagined musical stimuli from electroencephalography (EEG) recordings based on two datasets. One dataset contains multichannel EEG of subjects listening to and imagining rhythmical patterns presented both as sine wave tones and short looped spoken utterances. These utterances leverage the well-known speech-to-song illusory transformation which results in very catchy and easy to reproduce motifs. A second dataset provides EEG recordings for the perception of 10 full length songs. Using a multi-view deep generative model we demonstrate the feasibility of learning a shared latent representation of brain activity and auditory concepts, such as rhythmical motifs appearing across different instrumentations. Introspection of the model trained on the rhythm dataset reveals disentangled rhythmical and timbral features within and across subjects. The model allows continuous interpolation between representations of different observed variants of the presented stimuli. By decoding the learned embeddings we were able to reconstruct both perceived and imagined music. Stimulus complexity and the choice of training data shows strong effect on the reconstruction quality.",
        "zenodo_id": 1492433,
        "dblp_key": "conf/ismir/OfnerS18"
    },
    {
        "title": "Optical Music Recognition in Mensural Notation with Region-based Convolutional Neural Networks.",
        "author": [
            "Alexander Pacha",
            "Jorge Calvo-Zaragoza"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492393",
        "url": "https://doi.org/10.5281/zenodo.1492393",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/32_Paper.pdf",
        "abstract": "In this work, we present an approach for the task of optical music recognition (OMR) using deep neural networks. Our intention is to simultaneously detect and categorize musical symbols in handwritten scores, written in mensural notation. We propose the use of region-based convolutional neural networks, which are trained in an end-toend fashion for that purpose. Additionally, we make use of a convolutional neural network that predicts the relative position of a detected symbol within the staff, so that we cover the entire image-processing part of the OMR pipeline. This strategy is evaluated over a set of 60 ancient scores in mensural notation, with more than 15000 annotated symbols belonging to 32 different classes. The results reflect the feasibility and capability of this approach, with a weighted mean average precision of around 76% for symbol detection, and over 98% accuracy for predicting the position.",
        "zenodo_id": 1492393,
        "dblp_key": "conf/ismir/PachaC18"
    },
    {
        "title": "Exploring Musical Relations Using Association Rule Networks.",
        "author": [
            "Renan de Padua",
            "Veronica Oliveira de Carvalho",
            "Solange O. Rezende",
            "Diego Furtado Silva"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492435",
        "url": "https://doi.org/10.5281/zenodo.1492435",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/268_Paper.pdf",
        "abstract": "Music information retrieval (MIR) has been gaining increasing attention in both industry and academia. While many algorithms for MIR rely on assessing feature subsequences, the user normally has no resources to interpret the significance of these patterns. Interpreting the relations between these temporal patterns and some aspects of the assessed songs can help understanding not only some algorithms' outcomes but the kind of patterns which better defines a set of similarly labeled recordings. In this work, we present a novel method to assess these relations, constructing an association rule network from temporal patterns obtained by a simple quantization process. With an empirical evaluation, we illustrate how we can use our method to explore these relations in a varied set of data and labels.",
        "zenodo_id": 1492435,
        "dblp_key": "conf/ismir/PaduaCRS18"
    },
    {
        "title": "Musical Texture and Expressivity Features for Music Emotion Recognition.",
        "author": [
            "Renato Panda",
            "Ricardo Malheiro",
            "Rui Pedro Paiva"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492431",
        "url": "https://doi.org/10.5281/zenodo.1492431",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/250_Paper.pdf",
        "abstract": "We present a set of novel emotionally-relevant audio features to help improving the classification of emotions in audio music. First, a review of the state-of-the-art regarding emotion and music was conducted, to understand how the various music concepts may influence human emotions. Next, well known audio frameworks were analyzed, assessing how their extractors relate with the studied musical concepts. The intersection of this data showed an unbalanced representation of the eight musical concepts. Namely, most extractors are low-level and related with tone color, while musical form, musical texture and expressive techniques are lacking. Based on this, we developed a set of new algorithms to capture information related with musical texture and expressive techniques, the two most lacking concepts. To validate our work, a public dataset containing 900 30-second clips, annotated in terms of Russell's emotion quadrants was created. The inclusion of our features improved the F1-score obtained using the best 100 features by 8.6% (to 76.0%), using support vector machines and 20 repetitions of 10-fold cross-validation.",
        "zenodo_id": 1492431,
        "dblp_key": "conf/ismir/PandaMP18"
    },
    {
        "title": "Identifying Emotions in Opera Singing: Implications of Adverse Acoustic Conditions.",
        "author": [
            "Emilia Parada-Cabaleiro",
            "Maximilian Schmitt",
            "Anton Batliner",
            "Simone Hantke",
            "Giovanni Costantini",
            "Klaus R. Scherer",
            "Björn W. Schuller"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492429",
        "url": "https://doi.org/10.5281/zenodo.1492429",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/22_Paper.pdf",
        "abstract": "The expression of emotion is an inherent aspect in singing, especially in operatic voice. Yet, adverse acoustic conditions, as, e. g., a performance in open-air, or a noisy analog recording, may affect its perception. State-of-the art methods for emotional speech evaluation have been applied to operatic voice, such as perception experiments, acoustic analyses, and machine learning techniques. Still, the extent to which adverse acoustic conditions may impair listeners' and machines' identification of emotion in vocal cues has only been investigated in the realm of speech. For our study, 132 listeners evaluated 390 nonsense operatic sung instances of five basic emotions, affected by three noises (brown, pink, and white), each at four Signal-to-Noise Ratios (-1 dB, -0.5 dB, +1 dB, and +3 dB); the performance of state-of-the-art automatic recognition methods was evaluated as well. Our findings show that the three noises affect similarly female and male singers and that listeners' gender did not play a role. Human perception and automatic classification display similar confusion and recognition patterns: sadness is identified best, fear worst; low aroused emotions display higher confusion.",
        "zenodo_id": 1492429,
        "dblp_key": "conf/ismir/Parada-Cabaleiro18"
    },
    {
        "title": "Musical-Linguistic Annotations of Il Lauro Secco.",
        "author": [
            "Emilia Parada-Cabaleiro",
            "Maximilian Schmitt",
            "Anton Batliner",
            "Björn W. Schuller"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492451",
        "url": "https://doi.org/10.5281/zenodo.1492451",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/11_Paper.pdf",
        "abstract": "In madrigals, The Italian madrigal, a polyphonic secular a cappella composition of the 16th century, is characterised by a strong musical-linguistic relationship, which has made it an icon of the 'Renaissance humanism'. lyrical meaning is mimicked by the music, through the utilisation of a composition technique known as madrigalism. The synergy between Renaissance music and poetry makes madrigals of great value to musicologists, linguists, and historians—thus, it is a promising repertoire for computational musicology. However, the application of computational techniques for automatic detection of madrigalisms within scores of such repertoire is limited by the lack of annotations to refer to. In this regard, we present 30 madrigals of the anthology Il Lauro Secco encoded in two symbolic formats, MEI and **kern, with hand-encoded annotations of madrigalisms. This work aims to encourage the development of algorithms for madrigalism detection, a composition procedure typical of early music, but still underrepresented in music information retrieval research.",
        "zenodo_id": 1492451,
        "dblp_key": "conf/ismir/Parada-Cabaleiro18a"
    },
    {
        "title": "Music Source Separation Using Stacked Hourglass Networks.",
        "author": [
            "Sungheon Park",
            "Taehoon Kim",
            "Kyogu Lee",
            "Nojun Kwak"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492405",
        "url": "https://doi.org/10.5281/zenodo.1492405",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/138_Paper.pdf",
        "abstract": "In this paper, we propose a simple yet effective method for multiple music source separation using convolutional neural networks. Stacked hourglass network, which was originally designed for human pose estimation in natural images, is applied to a music source separation task. The network learns features from a spectrogram image across multiple scales and generates masks for each music source. The estimated mask is refined as it passes over stacked hourglass modules. The proposed framework is able to separate multiple music sources using a single network. Experimental results on MIR-1K and DSD100 datasets validate that the proposed method achieves competitive results comparable to the state-of-the-art methods in multiple music source separation and singing voice separation tasks.",
        "zenodo_id": 1492405,
        "dblp_key": "conf/ismir/ParkKLK18"
    },
    {
        "title": "Representation Learning of Music Using Artist Labels.",
        "author": [
            "Jiyoung Park",
            "Jongpil Lee",
            "Jangyeon Park",
            "Jung-Woo Ha",
            "Juhan Nam"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492517",
        "url": "https://doi.org/10.5281/zenodo.1492517",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/168_Paper.pdf",
        "abstract": "In music domain, feature learning has been conducted mainly in two ways: unsupervised learning based on sparse representations or supervised learning by semantic labels such as music genre. However, finding discriminative features in an unsupervised way is challenging and supervised feature learning using semantic labels may involve noisy or expensive annotation. In this paper, we present a supervised feature learning approach using artist labels annotated in every single track as objective meta data. We propose two deep convolutional neural networks (DCNN) to learn the deep artist features. One is a plain DCNN trained with the whole artist labels simultaneously, and the other is a Siamese DCNN trained with a subset of the artist labels based on the artist identity. We apply the trained models to music classification and retrieval tasks in transfer learning settings. The results show that our approach is comparable to previous state-of-the-art methods, indicating that the proposed approach captures general music audio features as much as the models learned with semantic labels. Also, we discuss the advantages and disadvantages of the two models.",
        "zenodo_id": 1492517,
        "dblp_key": "conf/ismir/ParkLPHN18"
    },
    {
        "title": "End-to-end Learning for Music Audio Tagging at Scale.",
        "author": [
            "Jordi Pons",
            "Oriol Nieto",
            "Matthew Prockup",
            "Erik M. Schmidt",
            "Andreas F. Ehmann",
            "Xavier Serra"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492497",
        "url": "https://doi.org/10.5281/zenodo.1492497",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/191_Paper.pdf",
        "abstract": "The lack of data tends to limit the outcomes of deep learning research, particularly when dealing with end-to-end learning stacks processing raw data such as waveforms. In this study, 1.2M tracks annotated with musical labels are available to train our end-to-end models. This large amount of data allows us to unrestrictedly explore two different design paradigms for music auto-tagging: assumption-free models – using waveforms as input with very small convolutional filters; and models that rely on domain knowledge – log-mel spectrograms with a convolutional neural network designed to learn timbral and temporal features. Our work focuses on studying how these two types of deep architectures perform when datasets of variable size are available for training: the MagnaTagATune (25k songs), the Million Song Dataset (240k songs), and a private dataset of 1.2M songs. Our experiments suggest that music domain assumptions are relevant when not enough training data are available, thus showing how waveform-based models outperform spectrogrambased ones in large-scale data scenarios.",
        "zenodo_id": 1492497,
        "dblp_key": "conf/ismir/PonsNPSES18"
    },
    {
        "title": "Analysis by Classification: A Comparative Study of Annotated and Algorithmically Extracted Patterns in Symbolic Music Data.",
        "author": [
            "Iris Yuping Ren",
            "Anja Volk",
            "Wouter Swierstra",
            "Remco C. Veltkamp"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492471",
        "url": "https://doi.org/10.5281/zenodo.1492471",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/75_Paper.pdf",
        "abstract": "Musical patterns are salient passages that repeatedly appear in music. Such passages are vital for compression, classification and prediction tasks in MIR, and algorithms employing different techniques have been proposed to find musical patterns automatically. Human-annotated patterns have been collected and used to evaluate pattern discovery algorithms, e.g., in the Discovery of Repeated Themes &amp; Sections MIREX task. However, state-of-the-art algorithms are not yet able to reproduce human-annotated patterns. To understand what gives rise to the discrepancy between algorithmically extracted patterns and human-annotated patterns, we use jSymbolic to extract features from patterns, visualise the feature space using PCA and perform a comparative analysis using classification techniques. We show that it is possible to classify algorithmically extracted patterns, human-annotated patterns and randomly sampled passages. This implies: (a) Algorithmically extracted patterns possess different properties than human-annotated patterns (b) Algorithmically extracted patterns have different structures than randomly sampled passages (c) Human-annotated patterns contain more information than randomly sampled passages despite subjectivity involved in the annotation process. We further discover that rhythmic features are of high importance in the classification process, which should influence future research on automatic pattern discovery.",
        "zenodo_id": 1492471,
        "dblp_key": "conf/ismir/RenVSV18"
    },
    {
        "title": "An End-to-end Framework for Audio-to-Score Music Transcription on Monophonic Excerpts.",
        "author": [
            "Miguel A. Román",
            "Antonio Pertusa",
            "Jorge Calvo-Zaragoza"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492337",
        "url": "https://doi.org/10.5281/zenodo.1492337",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/87_Paper.pdf",
        "abstract": "In this work, we present an end-to-end framework for audio-to-score transcription. To the best of our knowledge, this is the first automatic music transcription approach which obtains directly a symbolic score from audio, instead of performing separate stages for piano-roll estimation (pitch detection and note tracking), meter detection or key estimation. The proposed method is based on a Convolutional Recurrent Neural Network architecture directly trained with pairs of spectrograms and their corresponding symbolic scores in Western notation. Unlike standard pitch estimation methods, the proposed architecture does not need the music symbols to be aligned with their audio frames thanks to a Connectionist Temporal Classification loss function. Training and evaluation were performed using a large dataset of short monophonic scores (incipits) from the RISM collection, that were synthesized to get the ground-truth data. Although there is still room for improvement, most musical symbols were correctly detected and the evaluation results validate the proposed approach. We believe that this end-to-end framework opens new avenues for automatic music transcription.",
        "zenodo_id": 1492337,
        "dblp_key": "conf/ismir/RomanPC18"
    },
    {
        "title": "Disambiguating Music Artists at Scale with Audio Metric Learning.",
        "author": [
            "Jimena Royo-Letelier",
            "Romain Hennequin",
            "Viet-Anh Tran",
            "Manuel Moussallam"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492493",
        "url": "https://doi.org/10.5281/zenodo.1492493",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/211_Paper.pdf",
        "abstract": "We address the problem of disambiguating large scale catalogs through the definition of an unknown artist clustering task. We explore the use of metric learning techniques to learn artist embeddings directly from audio, and using a dedicated homonym artists dataset, we compare our method with a recent approach that learn similar embeddings using artist classifiers. While both systems have the ability to disambiguate unknown artists relying exclusively on audio, we show that our system is more suitable in the case when enough audio data is available for each artist in the train dataset. We also propose a new negative sampling method for metric learning that takes advantage of side information such as music genre during the learning phase and shows promising results for the artist clustering task.",
        "zenodo_id": 1492493,
        "dblp_key": "conf/ismir/Royo-LetelierHT18"
    },
    {
        "title": "Zero-Mean Convolutions for Level-Invariant Singing Voice Detection.",
        "author": [
            "Jan Schlüter",
            "Bernhard Lehner"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492413",
        "url": "https://doi.org/10.5281/zenodo.1492413",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/189_Paper.pdf",
        "abstract": "State-of-the-art singing voice detectors are based on classifiers trained on annotated examples. As recently shown, such detectors have an important weakness: Since singing voice is correlated with sound level in training data, classifiers learn to become sensitive to input magnitude, and give different predictions for the same signal at different sound levels. Starting from a Convolutional Neural Network (CNN) trained on logarithmic-magnitude mel spectrogram excerpts, we eliminate this dependency by forcing each first-layer convolutional filter to be zero-mean – that is, to have its coefficients sum to zero. In contrast to four other methods – data augmentation, instance normalization, spectral delta features, and per-channel energy normalization (PCEN) – that we evaluated on a largescale public dataset, zero-mean convolutions achieve perfect sound level invariance without any impact on prediction accuracy or computational requirements. We assume that zero-mean convolutions would be useful for other machine listening tasks requiring robustness to level changes.",
        "zenodo_id": 1492413,
        "dblp_key": "conf/ismir/SchluterL18"
    },
    {
        "title": "A Single-Step Approach to Musical Tempo Estimation Using a Convolutional Neural Network.",
        "author": [
            "Hendrik Schreiber 0001",
            "Meinard Müller"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.3553592",
        "url": "https://doi.org/10.5281/zenodo.3553592",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/141_Paper.pdf",
        "abstract": "<p>Global tempo annotations used for training of the tempo estimation CNN presented in&nbsp;<a href=\"https://doi.org/10.5281/zenodo.1492353\">A Single-step Approach to Musical Tempo Estimation using a Convolutional Neural Network</a>.</p>",
        "zenodo_id": 3553592,
        "dblp_key": "conf/ismir/SchreiberM18"
    },
    {
        "title": "A Crowdsourced Experiment for Tempo Estimation of Electronic Dance Music.",
        "author": [
            "Hendrik Schreiber 0001",
            "Meinard Müller"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.3553625",
        "url": "https://doi.org/10.5281/zenodo.3553625",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/220_Paper.pdf",
        "abstract": "<p>Raw beat and derived tempo annotations&nbsp;from&nbsp;<a href=\"https://doi.org/10.5281/zenodo.1492437\">A Crowdsourced Experiment for Tempo Estimation of Electronic Dance Music</a>.</p>",
        "zenodo_id": 3553625,
        "dblp_key": "conf/ismir/SchreiberM18a"
    },
    {
        "title": "Evaluating Language Models of Tonal Harmony.",
        "author": [
            "David R. W. Sears",
            "Filip Korzeniowski",
            "Gerhard Widmer"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492385",
        "url": "https://doi.org/10.5281/zenodo.1492385",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/262_Paper.pdf",
        "abstract": "This study borrows and extends probabilistic language models from natural language processing to discover the syntactic properties of tonal harmony. Language models come in many shapes and sizes, but their central purpose is always the same: to predict the next event in a sequence of letters, words, notes, or chords. However, few studies employing such models have evaluated the most stateof-the-art architectures using a large-scale corpus of Western tonal music, instead preferring to use relatively small datasets containing chord annotations from contemporary genres like jazz, pop, and rock. Using symbolic representations of prominent instrumental genres from the common-practice period, this study applies a flexible, data-driven encoding scheme to (1) evaluate Finite Context (or n-gram) models and Recurrent Neural Networks (RNNs) in a chord prediction task; (2) compare predictive accuracy from the best-performing models for chord onsets from each of the selected datasets; and (3) explain differences between the two model architectures in a regression analysis. We find that Finite Context models using the Prediction by Partial Match (PPM) algorithm outperform RNNs, particularly for the piano datasets, with the regression model suggesting that RNNs struggle with particularly rare chord types.",
        "zenodo_id": 1492385,
        "dblp_key": "conf/ismir/SearsKW18"
    },
    {
        "title": "Automatic, Personalized, and Flexible Playlist Generation using Reinforcement Learning.",
        "author": [
            "Shun-Yao Shih",
            "Heng-Yu Chi"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492371",
        "url": "https://doi.org/10.5281/zenodo.1492371",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/18_Paper.pdf",
        "abstract": "Songs can be well arranged by professional music curators to form a riveting playlist that creates engaging listening experiences. However, it is time-consuming for curators to timely rearrange these playlists for fitting trends in future. By exploiting the techniques of deep learning and reinforcement learning, in this paper, we consider music playlist generation as a language modeling problem and solve it by the proposed attention language model with policy gradient. We develop a systematic and interactive approach so that the resulting playlists can be tuned flexibly according to user preferences. Considering a playlist as a sequence of words, we first train our attention RNN language model on baseline recommended playlists. By optimizing suitable imposed reward functions, the model is thus refined for corresponding preferences. The experimental results demonstrate that our approach not only generates coherent playlists automatically but is also able to flexibly recommend personalized playlists for diversity, novelty and freshness.",
        "zenodo_id": 1492371,
        "dblp_key": "conf/ismir/ShihC18"
    },
    {
        "title": "Summarizing and Comparing Music Data and Its Application on Cover Song Identification.",
        "author": [
            "Diego Furtado Silva",
            "Felipe Falcão",
            "Nazareno Andrade"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492521",
        "url": "https://doi.org/10.5281/zenodo.1492521",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/36_Paper.pdf",
        "abstract": "While there is a multitude of music information retrieval algorithms that have distance functions as their core procedure, comparing the similarity between recordings is a costly procedure. At the same, the recent growth of digital music repositories makes necessary the development of novel time- and memory-efficient algorithms to deal with music data. One particularly interesting idea on the literature is transforming the music data into reduced representations, improving the memory usage and reducing the time necessary to assess the similarity. However, these techniques usually add other issues, such as an expensive preprocessing or a reduced retrieval performance. In this paper, we propose a novel method to summarize a recording in small snippets based on its self-similarity information. Besides, we present a simple way to compare other recordings to these summaries. We demonstrate, in the scenario of cover song identification, that our method is more than one order of magnitude faster than state-of-the-art adversaries, at the same time that the retrieval performance is not affected significantly. Additionally, our method is incremental, which allows the easy and fast update of the database when a new song needs to be inserted into the retrieval system.",
        "zenodo_id": 1492521,
        "dblp_key": "conf/ismir/SilvaFA18"
    },
    {
        "title": "Player Vs Transcriber: A Game Approach To Data Manipulation For Automatic Drum Transcription.",
        "author": [
            "Carl Southall",
            "Ryan Stables",
            "Jason Hockman"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492343",
        "url": "https://doi.org/10.5281/zenodo.1492343",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/24_Paper.pdf",
        "abstract": "State-of-the-art automatic drum transcription (ADT) approaches utilise deep learning methods reliant on timeconsuming manual annotations and require congruence between training and testing data. When these conditions are not held, they often fail to generalise. We propose a game approach to ADT, termed player vs transcriber (PvT), in which a player model aims to reduce transcription accuracy of a transcriber model by manipulating training data in two ways. First, existing data may be augmented, allowing the transcriber to be trained using recordings with modified timbres. Second, additional individual recordings from sample libraries are included to generate rare combinations. We present three versions of the PvT model: AugExist, which augments pre-existing recordings; AugAddExist, which adds additional samples of drum hits to the AugExist system; and Generate, which generates training examples exclusively from individual drum hits from sample libraries. The three versions are evaluated alongside a state-of-the-art deep learning ADT system using two evaluation strategies. The results demonstrate that including the player network improves the ADT performance and suggests that this is due to improved generalisability. The results also indicate that although the Generate model achieves relatively low results, it is a viable choice when annotations are not accessible.",
        "zenodo_id": 1492343,
        "dblp_key": "conf/ismir/SouthallSH18"
    },
    {
        "title": "Improving Peak-picking Using Multiple Time-step Loss Functions.",
        "author": [
            "Carl Southall",
            "Ryan Stables",
            "Jason Hockman"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492409",
        "url": "https://doi.org/10.5281/zenodo.1492409",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/25_Paper.pdf",
        "abstract": "The majority of state-of-the-art methods for music information retrieval (MIR) tasks now utilise deep learning methods reliant on minimisation of loss functions such as cross entropy. For tasks that include framewise binary classification (e.g., onset detection, music transcription) classes are derived from output activation functions by identifying points of local maxima, or peaks. However, the operating principles behind peak picking are different to that of the cross entropy loss function, which minimises the absolute difference between the output and target values for a single frame. To generate activation functions more suited to peak-picking, we propose two versions of a new loss function that incorporates information from multiple time-steps: 1) multi-individual, which uses multiple individual time-step cross entropies; and 2) multi-difference, which directly compares the difference between sequential time-step outputs. We evaluate the newly proposed loss functions alongside standard cross entropy in the popular MIR tasks of onset detection and automatic drum transcription. The results highlight the effectiveness of these loss functions in the improvement of overall system accuracies for both MIR tasks. Additionally, directly comparing the output from sequential time-steps in the multidifference approach achieves the highest performance.",
        "zenodo_id": 1492409,
        "dblp_key": "conf/ismir/SouthallSH18a"
    },
    {
        "title": "Influences on the Social Practices Surrounding Commercial Music Services: A Model for Rich Interactions.",
        "author": [
            "Louis Spinelli",
            "Josephine Lau",
            "Liz Pritchard",
            "Jin Ha Lee 0001"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492505",
        "url": "https://doi.org/10.5281/zenodo.1492505",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/52_Paper.pdf",
        "abstract": "Music can play an important role in social experiences and interactions. Technologies in-use affect these experiences and interactions and as they continue to evolve, social behaviors and norms surrounding them also evolve. In this paper, we explore the social aspects of commercial music services through focus group observation and interview data. We seek to better understand how existing services are used for social music practices and can be improved. We identified 9 social practices and 24 influences surrounding commercial music services. Based on the user data, we created a model of these practices and influences that provides a lens through which social experiences surrounding commercial music services can be understood. An understanding of these social practices within their contextual ecosystem help inform what influences should be considered when designing new technologies. Our findings include the identification of: the underlying relationships between practices and their influences; practices and influences that inform the weight of relationships in social networks; social norms to be considered when designing social features; influences that add additional insight to previously observed behaviors; and a detailed explanation of how music selection and listening practices can be supported by commercial music services.",
        "zenodo_id": 1492505,
        "dblp_key": "conf/ismir/SpinelliLPL18"
    },
    {
        "title": "Wave-U-Net: A Multi-Scale Neural Network for End-to-End Audio Source Separation.",
        "author": [
            "Daniel Stoller",
            "Sebastian Ewert",
            "Simon Dixon"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492417",
        "url": "https://doi.org/10.5281/zenodo.1492417",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/205_Paper.pdf",
        "abstract": "Models for audio source separation usually operate on the magnitude spectrum, which ignores phase information and makes separation performance dependant on hyperparameters for the spectral front-end. Therefore, we investigate end-to-end source separation in the time-domain, which allows modelling phase information and avoids fixed spectral transformations. Due to high sampling rates for audio, employing a long temporal input context on the sample level is difficult, but required for high quality separation results because of long-range temporal correlations. In this context, we propose the Wave-U-Net, an adaptation of the U-Net to the one-dimensional time domain, which repeatedly resamples feature maps to compute and combine features at different time scales. We introduce further architectural improvements, including an output layer that enforces source additivity, an upsampling technique and a context-aware prediction framework to reduce output artifacts. Experiments for singing voice separation indicate that our architecture yields a performance comparable to a stateof-the-art spectrogram-based U-Net architecture, given the same data. Finally, we reveal a problem with outliers in the currently used SDR evaluation metrics and suggest reporting rank-based statistics to alleviate this problem.",
        "zenodo_id": 1492417,
        "dblp_key": "conf/ismir/StollerED18"
    },
    {
        "title": "Concert Stitch: Organization and Synchronization of Crowd Sourced Recordings.",
        "author": [
            "Vinod Subramanian",
            "Alexander Lerch 0001"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492489",
        "url": "https://doi.org/10.5281/zenodo.1492489",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/182_Paper.pdf",
        "abstract": "The number of audience recordings of concerts on the internet has exploded with the advent of smartphones. This paper proposes a method to organize and align these recordings in order to create one or more complete renderings of the concert. The process comprises two steps: first, using audio fingerprints to represent the recordings, identify overlapping segments, and compute an approximate alignment using a modified Dynamic Time Warping (DTW) algorithm and second, applying a cross-correlation around the approximate alignment points in order to improve the accuracy of the alignment. The proposed method is compared to two baseline systems using approaches previously proposed for similar tasks. One baseline cross-correlates the audio fingerprints directly without DTW. The second baseline replaces the audio fingerprints with pitch chroma in the DTW algorithm. A new dataset annotating real-world data obtained from the Live Music Archive is presented and used for evaluation of the three systems.",
        "zenodo_id": 1492489,
        "dblp_key": "conf/ismir/SubramanianL18"
    },
    {
        "title": "Instrudive: A Music Visualization System Based on Automatically Recognized Instrumentation.",
        "author": [
            "Takumi Takahashi",
            "Satoru Fukayama",
            "Masataka Goto"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492477",
        "url": "https://doi.org/10.5281/zenodo.1492477",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/63_Paper.pdf",
        "abstract": "A music visualization system called Instrudive is presented that enables users to interactively browse and listen to musical pieces by focusing on instrumentation. Instrumentation is a key factor in determining musical sound characteristics. For example, a musical piece performed with vocals, electric guitar, electric bass, and drums can generally be associated with pop/rock music but not with classical or electronic. Therefore, visualizing instrumentation can help listeners browse music more efficiently. Instrudive visualizes musical pieces by illustrating instrumentation with multi-colored pie charts and displays them on a map in accordance with the similarity in instrumentation. Users can utilize three functions. First, they can browse musical pieces on a map by referring to the visualized instrumentation. Second, they can interactively edit a playlist that showing the items to be played later. Finally, they can discern the temporal changes in instrumentation and skip to a preferable part of a piece with a multi-colored graph. The instruments are identified using a deep convolutional neural network that has four convolutional layers with different filter shapes. Evaluation of the proposed model against conventional and state-of-the-art methods showed that it has the best performance.",
        "zenodo_id": 1492477,
        "dblp_key": "conf/ismir/TakahashiFG18"
    },
    {
        "title": "Cover Song Synthesis by Analogy.",
        "author": [
            "Christopher J. Tralie"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492381",
        "url": "https://doi.org/10.5281/zenodo.1492381",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/103_Paper.pdf",
        "abstract": "In this work, we pose and address the following \"cover song analogies\" problem: given a song A by artist 1 and a cover song A' of this song by artist 2, and given a different song B by artist 1, synthesize a song B' which is a cover of B in the style of artist 2. Normally, such a polyphonic style transfer problem would be quite challenging, but we show how the cover songs example constrains the problem, making it easier to solve. First, we extract the longest common beat-synchronous subsequence between A and A', and we time stretch the corresponding beat intervals in A' so that they align with A. We then derive a version of joint 2D convolutional NMF, which we apply to the constant-Q spectrograms of the synchronized segments to learn a translation dictionary of sound templates from A to A'. Finally, we apply the learned templates as filters to the song B, and we mash up the translated filtered components into the synthesized song B' using audio mosaicing. We showcase our algorithm on several examples, including a synthesized cover version of Michael Jackson's \"Bad\" by Alien Ant Farm, learned from the latter's \"Smooth Criminal\" cover.",
        "zenodo_id": 1492381,
        "dblp_key": "conf/ismir/Tralie18"
    },
    {
        "title": "Listener Anonymizer: Camouflaging Play Logs to Preserve User&apos;s Demographic Anonymity.",
        "author": [
            "Kosetsu Tsukuda",
            "Satoru Fukayama",
            "Masataka Goto"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492509",
        "url": "https://doi.org/10.5281/zenodo.1492509",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/78_Paper.pdf",
        "abstract": "When a user signs up with an online music service, she is often requested to register her demographic attributes such as age, gender, and nationality. Even if she does not input such information, it has been reported that user attributes can be predicted with high accuracy by using her play log. How can users enjoy music when using an online music service while preserving their demographic anonymity? To solve this problem, we propose a system called Listener Anonymizer. Listener Anonymizer monitors the user's play log. When it detects that her confidential attributes can be predicted, it selects songs that can decrease the prediction accuracy and recommends them to her. The user can camouflage her play logs by playing these songs to preserve her demographic anonymity. Since such songs do not always match her music taste, selecting as few songs as possible that can effectively anonymize her attributes is required. Listener Anonymizer realizes this by selecting songs based on feature ablation analysis. Our experimental results using Last.fm play logs showed that Listener Anonymizer was able to preserve anonymity with fewer songs than a method that randomly selected songs.",
        "zenodo_id": 1492509,
        "dblp_key": "conf/ismir/TsukudaFG18"
    },
    {
        "title": "Interactive Arrangement of Chords and Melodies Based on a Tree-Structured Generative Model.",
        "author": [
            "Hiroaki Tsushima",
            "Eita Nakamura",
            "Katsutoshi Itoyama",
            "Kazuyoshi Yoshii"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492365",
        "url": "https://doi.org/10.5281/zenodo.1492365",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/1_Paper.pdf",
        "abstract": "We describe an interactive music composition system that assists a user in refining chords and melodies by generating chords for melodies (harmonization) and vice versa (melodization). Since these two tasks have been dealt with independently, it is difficult to jointly estimate chords and melodies that are optimal in both tasks. Another problem is developing an interactive GUI that enables a user to partially update chords and melodies by considering the latent tree structure of music. To solve these problems, we propose a hierarchical generative model consisting of (1) a probabilistic context-free grammar (PCFG) for chord symbols, (2) a metrical Markov model for chord boundaries, (3) a Markov model for melody pitches, and (4) a metrical Markov model for melody onsets. The harmonic functions (syntactic roles) and repetitive structure of chords are learned by the PCFG. Any variables specified by a user can be optimized or sampled in a principled manner according to a unified posterior distribution. For improved melodization, a long short-term memory (LSTM) network can also be used. The subjective experimental result showed the effectiveness of the proposed system.",
        "zenodo_id": 1492365,
        "dblp_key": "conf/ismir/TsushimaNIY18"
    },
    {
        "title": "Deep Watershed Detector for Music Object Recognition.",
        "author": [
            "Lukas Tuggener",
            "Ismail Elezi",
            "Jürgen Schmidhuber",
            "Thilo Stadelmann"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492401",
        "url": "https://doi.org/10.5281/zenodo.1492401",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/225_Paper.pdf",
        "abstract": "Optical Music Recognition (OMR) is an important and challenging area within music information retrieval, the accurate detection of music symbols in digital images is a core functionality of any OMR pipeline. In this paper, we introduce a novel object detection method, based on synthetic energy maps and the watershed transform, called Deep Watershed Detector (DWD). Our method is specifically tailored to deal with high resolution images that contain a large number of very small objects and is therefore able to process full pages of written music. We present state-of-the-art detection results of common music symbols and show DWD's ability to work with synthetic scores equally well as with handwritten music.",
        "zenodo_id": 1492401,
        "dblp_key": "conf/ismir/TuggenerESS18"
    },
    {
        "title": "Deep Neural Networks with Voice Entry Estimation Heuristics for Voice Separation in Symbolic Music Representations.",
        "author": [
            "Reinier de Valk",
            "Tillman Weyde"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492403",
        "url": "https://doi.org/10.5281/zenodo.1492403",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/304_Paper.pdf",
        "abstract": "In this study we explore the use of deep feedforward neural networks for voice separation in symbolic music representations. We experiment with different network architectures, varying the number and size of the hidden layers, and with dropout. We integrate two voice entry estimation heuristics that estimate the entry points of the individual voices in the polyphonic fabric into the models. These heuristics serve to reduce error propagation at the beginning of a piece, which, as we have shown in previous work, can seriously hamper model performance. The models are evaluated on the 48 fugues from Johann Sebastian Bach's The Well-Tempered Clavier and his 30 inventions—a dataset that we curated and make publicly available. We find that a model with two hidden layers yields the best results. Using more layers does not lead to a significant performance improvement. Furthermore, we find that our voice entry estimation heuristics are highly effective in the reduction of error propagation, improving performance significantly. Our best-performing model outperforms our previous models, where the difference is significant, and, depending on the evaluation metric, performs close to or better than the reported state of the art.",
        "zenodo_id": 1492403,
        "dblp_key": "conf/ismir/ValkW18"
    },
    {
        "title": "Comparison of Audio Features for Recognition of Western and Ethnic Instruments in Polyphonic Mixtures.",
        "author": [
            "Igor Vatolkin",
            "Günter Rudolph"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492475",
        "url": "https://doi.org/10.5281/zenodo.1492475",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/139_Paper.pdf",
        "abstract": "Studies on instrument recognition are almost always restricted to either Western or ethnic music. Only little work has been done to compare both musical worlds. In this paper, we analyse the performance of various audio features for recognition of Western and ethnic instruments in chords. The feature selection is done with the help of a minimum redundancy - maximum relevance strategy and a multi-objective evolutionary algorithm. We compare the features found to be the best for individual categories and propose a novel strategy based on non-dominated sorting to evaluate and select trade-off features which may contribute as best as possible to the recognition of individual and all instruments.",
        "zenodo_id": 1492475,
        "dblp_key": "conf/ismir/VatolkinR18"
    },
    {
        "title": "Precision of Sung Notes in Carnatic Music.",
        "author": [
            "Venkata Subramanian Viraraghavan",
            "Rangarajan Aravind",
            "Hema A. Murthy"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492461",
        "url": "https://doi.org/10.5281/zenodo.1492461",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/120_Paper.pdf",
        "abstract": "Carnatic music is replete with continuous pitch movement called gamakas and can be viewed as consisting of constant-pitch notes (CPNs) and transients. The stationary points (STAs) of transients – points where the pitch curve changes direction – also carry melody information. In this paper, the precision of sung notes in Carnatic music is studied in detail by treating CPNs and STAs separately. There is variation among the nineteen musicians considered, but on average, the precision of CPNs increases exponentially with duration and settles at about 10 cents for CPNs longer than 0.5 seconds. For analyzing STAs, in contrast to Western music, r¯aga (melody) information is found to be necessary, and errors in STAs show a significantly larger standard deviation of about 60 cents. To corroborate these observations, the music was automatically transcribed and re-synthesized using CPN and STA information using two interpolation techniques. The results of perceptual tests clearly indicate that the grammar is highly flexible. We also show that the precision errors are not due to poor pitch tracking, singer deficiencies or delay in auditory feedback.",
        "zenodo_id": 1492461,
        "dblp_key": "conf/ismir/ViraraghavanAM18"
    },
    {
        "title": "Driftin&apos; Down the Scale: Dynamic Time Warping in the Presence of Pitch Drift and Transpositions.",
        "author": [
            "Simon Waloschek",
            "Aristotelis Hadjakos"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492495",
        "url": "https://doi.org/10.5281/zenodo.1492495",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/97_Paper.pdf",
        "abstract": "Recordings of a cappella music often exhibit significant pitch drift. This drift may accumulate over time to a total transposition of several semitones, which renders the canonical 2-dimensional Dynamic Time Warping (DTW) useless. We propose Transposition-Aware Dynamic Time Warping (TA-DTW), an approach that introduces a 3rd dimension to DTW. Steps in this dimension represent changes in transposition. Paired with suitable input features, TA-DTW computes an optimal alignment path between a symbolic score and a corresponding audio recording in the presence of pitch drift or arbitrary transpositions.",
        "zenodo_id": 1492495,
        "dblp_key": "conf/ismir/WaloschekH18"
    },
    {
        "title": "Computational Corpus Analysis: A Case Study on Jazz Solos.",
        "author": [
            "Christof Weiss",
            "Stefan Balke",
            "Jakob Abeßer",
            "Meinard Müller"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492439",
        "url": "https://doi.org/10.5281/zenodo.1492439",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/23_Paper.pdf",
        "abstract": "For musicological studies on large corpora, the compilation of suitable data constitutes a time-consuming step. In particular, this is true for high-quality symbolic representations that are generated manually in a tedious process. A recent study on Western classical music has shown that musical phenomena such as the evolution of tonal complexity over history can also be analyzed on the basis of audio recordings. As our first contribution, we transfer this corpus analysis method to jazz music using the Weimar Jazz Database, which contains high-level symbolic transcriptions of jazz solos along with the audio recordings. Second, we investigate the influence of the input representation type on the corpus-level observations. In our experiments, all representation types led to qualitatively similar results. We conclude that audio recordings can build a reasonable basis for conducting such type of corpus analysis.",
        "zenodo_id": 1492439,
        "dblp_key": "conf/ismir/WeissBAM18"
    },
    {
        "title": "VocalSet: A Singing Voice Dataset.",
        "author": [
            "Julia Wilkins",
            "Prem Seetharaman",
            "Alison Wahl",
            "Bryan Pardo"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.10265401",
        "url": "https://doi.org/10.5281/zenodo.10265401",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/114_Paper.pdf",
        "abstract": "There has been a persistent lack of publicly accessible data in singing voice research, particularly concerning the diversity of languages and performance styles. In this paper, we introduce SingStyle111, a large studio-quality singing dataset with multiple languages and different singing styles, and present singing style transfer examples. The dataset features 111 songs performed by eight professional singers, spanning 12.8 hours and covering English, Chinese, and Italian. SingStyle111 incorporates different singing styles, such as bel canto opera, Chinese folk singing, pop, jazz, and children. Specifically, 80 songs include at least two distinct singing styles performed by the same singer. All recordings were conducted in professional studios, yielding clean, dry vocal tracks in mono format with a 44.1 kHz sample rate. We have segmented the singing voices into phrases, providing lyrics, performance MIDI, and scores with phoneme-level alignment. We also extracted acoustic features such as Mel-Spectrogram, F0 contour, and loudness curves. This dataset applies to various MIR tasks such as Singing Voice Synthesis, Singing Voice Conversion, Singing Transcription, Score Following, and Lyrics Detection. It is also designed for Singing Style Transfer, including both performance and voice timbre style. We make the dataset freely available for research purposes. Examples and download information can be found at https://shuqid.net/singstyle111.",
        "zenodo_id": 10265401,
        "dblp_key": "conf/ismir/WilkinsSWP18"
    },
    {
        "title": "From Labeled to Unlabeled Data - On the Data Challenge in Automatic Drum Transcription.",
        "author": [
            "Chih-Wei Wu",
            "Alexander Lerch 0001"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492447",
        "url": "https://doi.org/10.5281/zenodo.1492447",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/185_Paper.pdf",
        "abstract": "Automatic Drum Transcription (ADT), like many other music information retrieval tasks, has made progress in the past years through the integration of machine learning and audio signal processing techniques. However, with the increasing popularity of data-hungry approaches such as deep learning, the insufficient amount of data becomes more and more a challenge that concerns the generality of the resulting models and the validity of the evaluation. To address this challenge in ADT, this paper first examines the existing labeled datasets and how representative they are of the research problem. Next, possibilities of using unlabeled data to improve general ADT systems are explored. Specifically, two paradigms that harness information from unlabeled data, namely feature learning and student-teacher learning, are applied to two major types of ADT systems. All systems are evaluated on four different drum datasets. The results highlight the necessity of more and larger annotated datasets and indicate the feasibility of exploiting unlabeled data for improving ADT systems.",
        "zenodo_id": 1492447,
        "dblp_key": "conf/ismir/WuL18"
    },
    {
        "title": "GuitarSet: A Dataset for Guitar Transcription.",
        "author": [
            "Qingyang Xi",
            "Rachel M. Bittner",
            "Johan Pauwels",
            "Xuzhou Ye",
            "Juan Pablo Bello"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.12674910",
        "url": "https://doi.org/10.5281/zenodo.12674910",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/188_Paper.pdf",
        "abstract": "<p>This dataset is used for additional experiments for our work: <a href=\"https://arxiv.org/abs/2407.16639\">Distortion Recovery: A Two-Stage Method for Guitar Effect Removal</a>, published at <a href=\"https://dafx24.surrey.ac.uk/\">DAFx 2024</a>.&nbsp;We randomly rendered dry (direct input) guitar tracks from the <a href=\"https://ss12f32v.github.io/Guitar-Transcription/\">EGDB dataset</a>&nbsp;using BIAS FX2 and fine-tuned both our model and Demucs V3 on this&nbsp;rendered dataset.</p>\n<p>The EGDB dataset consists of 240 tracks (~1.5 hours) and was randomly split by tracks into training, validation, and test sets, following an 8/1/1 split. Our model was then compared with Demucs V3. During both the training and testing phases, each track was randomly segmented into 4-second clips.</p>\n<p>For reproducibility, the segmented clips for testing, including wet/dry signal, results of our model and Demucs V3, are available <a href=\"https://drive.google.com/drive/folders/1_NOPoomWlIWAks_41EpMvWx7sboGJF4I?usp=drive_link\">here</a>. The test code for evaluating the following metrics is available at this&nbsp;<a href=\"https://github.com/y10ab1/guitar_effect_removal\">GitHub repository</a>.</p>",
        "zenodo_id": 12674910,
        "dblp_key": "conf/ismir/XiBPYB18"
    },
    {
        "title": "Part-invariant Model for Music Generation and Harmonization.",
        "author": [
            "Yujia Yan",
            "Ethan Lustig",
            "Joseph VanderStel",
            "Zhiyao Duan"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492383",
        "url": "https://doi.org/10.5281/zenodo.1492383",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/293_Paper.pdf",
        "abstract": "Automatic music generation has been gaining more attention in recent years. Existing approaches, however, are mostly ad hoc to specific rhythmic structures or instrumentation layouts, and lack music-theoretic rigor in their evaluations. In this paper, we present a neural language (music) model that tries to model symbolic multi-part music. Our model is part-invariant, i.e., it can process/generate any part (voice) of a music score consisting of an arbitrary number of parts, using a single trained model. For better incorporating structural information of pitch spaces, we use a structured embedding matrix to encode multiple aspects of a pitch into a vector representation. The generation is performed by Gibbs Sampling. Meanwhile, our model directly generates note spellings to make outputs human-readable. We performed objective (grading) and subjective (listening) evaluations by recruiting music theorists to compare the outputs of our algorithm with those of music students on the task of bassline harmonization (a traditional pedagogical task). Our experiment shows that errors of our algorithm and students are differently distributed, and the range of ratings for generated pieces overlaps with students' to varying extents for our three provided basslines. This experiment suggests some future research directions.",
        "zenodo_id": 1492383,
        "dblp_key": "conf/ismir/YanLVD18"
    },
    {
        "title": "The Many Faces of Users: Modeling Musical Preference.",
        "author": [
            "Eva Zangerle",
            "Martin Pichl"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492515",
        "url": "https://doi.org/10.5281/zenodo.1492515",
        "ee": "http://ismir2018.ircam.fr/doc/pdfs/128_Paper.pdf",
        "abstract": "User models that capture the musical preferences of users are central for many tasks in music information retrieval and music recommendation, yet, it has not been fully explored and exploited. To this end, the musical preferences of users in the context of music recommender systems have mostly been captured in collaborative filtering-based approaches. Alternatively, users can be characterized by their average listening behavior and hence, by the mean values of a set of content descriptors of tracks the users listened to. However, a user may listen to highly different tracks and genres. Thus, computing the average of all tracks does not capture the user's listening behavior well. We argue that each user may have many different preferences that depend on contextual aspects (e.g., listening to classical music when working and hard rock when doing sports) and that user models should account for these different sets of preferences. In this paper, we provide a detailed analysis and evaluation of different user models that describe a user's musical preferences based on acoustic features of tracks the user has listened to.",
        "zenodo_id": 1492515,
        "dblp_key": "conf/ismir/ZangerleP18"
    },
    {
        "title": "Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018, Paris, France, September 23-27, 2018",
        "author": [
            "Emilia Gómez",
            "Xiao Hu 0001",
            "Eric Humphrey",
            "Emmanouil Benetos"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1432913",
        "url": "https://doi.org/10.5281/zenodo.1432913",
        "ee": null,
        "abstract": "<p>The OpenMIC-2018 dataset is made available through&nbsp;a collaboration between <a href=\"https://spotify.com/\">Spotify</a>&nbsp;and <a href=\"https://steinhardt.nyu.edu/marl/\">MARL@NYU</a>. Additionally, the cost of annotation was sponsored by Spotify, whose contributions to open-source research can be found online at the <a href=\"http://developer.spotify.com/\">developer site</a>, <a href=\"https://labs.spotify.com/\">engineering blog</a>, and <a href=\"https://spotify.github.io/\">public GitHub</a>.</p>\n\n<p>If you use this dataset, please cite the following work:</p>\n\n<blockquote>\n<p>Humphrey, Eric J., Durand, Simon, and McFee, Brian. &quot;OpenMIC-2018: An Open Dataset for Multiple Instrument Recognition.&quot; in Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR), 2018. [<a href=\"https://bmcfee.github.io/papers/ismir2018_openmic.pdf\">pdf</a>]</p>\n</blockquote>\n\n<p>The dataset is made available by Spotify AB&nbsp;under a <a href=\"https://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International (CC BY 4.0)</a>&nbsp;license. The full terms of this license are included alongside this dataset.</p>\n\n<p>This dataset contains the following:</p>\n\n<ul>\n\t<li>10 second snippets of audio, in a directory format like &#39;audio/{0:3}/{0}.ogg&#39;.format(sample_key)</li>\n\t<li>VGGish features as JSON objects,&nbsp;in a directory format like &#39;vggish/{0:3}/{0}.json&#39;.format(sample_key)</li>\n\t<li>MD5 checksums for each OGG and JSON file</li>\n\t<li>Anonymized individual responses, in &#39;openmic-2018-individual-responses.csv&#39;</li>\n\t<li>Aggregated labels, in &#39;openmic-2018-aggregated-labels.csv&#39;</li>\n\t<li>Track metadata, with licenses for each audio recording, in &#39;openmic-2018-metadata.csv&#39;</li>\n\t<li>A Python-friendly NPZ file of features and labels, &#39;openmic-2018.npz&#39;</li>\n\t<li>Sample partitions for train and test, in &#39;partitions/*.txt&#39;</li>\n</ul>",
        "zenodo_id": 1432913,
        "dblp_key": "conf/ismir/2018"
    }
]