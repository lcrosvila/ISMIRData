[
    {
        "title": "Polyphonic transcription by non-negative sparse coding of power spectra.",
        "author": [
            "Samer M. Abdallah",
            "Mark D. Plumbley"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415072",
        "url": "https://doi.org/10.5281/zenodo.1415072",
        "ee": "https://zenodo.org/records/1415072/files/AbdallahP04.pdf",
        "abstract": "We present a system for adaptive spectral basis decompo- sition that learns to identify independent spectral features given a sequence of short-term Fourier spectra. When ap- plied to recordings of polyphonic piano music, the indi- vidual notes are identified as salient features, and hence each short-term spectrum is decomposed into a sum of note spectra; the resulting encoding can be used as a ba- sis for polyphonic transcription. The system is based on a probabilistic model equivalent to a form of noisy inde- pendent component analysis (ICA) or sparse coding with non-negativity constraints. We introduce a novel mod- ification to this model that recognises that a short-term Fourier spectrum can be thought of as a noisy realisation of the power spectral density of an underlying Gaussian process, where the noise is essentially multiplicative and non-Gaussian. Results are presented for an analysis of a live recording of polyphonic piano music.",
        "zenodo_id": 1415072,
        "dblp_key": "conf/ismir/AbdallahP04",
        "keywords": [
            "adaptive spectral basis decomposition",
            "identifies independent spectral features",
            "polyphonic piano music",
            "salient features",
            "probabilistic model",
            "noisy independent component analysis",
            "sparse coding",
            "non-negativity constraints",
            "Gaussian process",
            "multiplicative noise"
        ],
        "content": "POLYPHONICMUSIC TRANSCRIPTIONBYNON-NEGATIVE SPARSE\nCODING OF POWERSPECTRA\nSamerA. AbdallahandMarkD. Plumbley\nCentre forDigitalMusic,\nQueen Mary,UniversityofLondon\nABSTRACT\nWe presenta systemforadaptivespectralbasisdecompo-\nsition that learnsto identify independentspectral featur es\ngivena sequenceofshort-termFourierspectra. Whenap-\nplied to recordings of polyphonic piano music, the indi-\nvidual notes are identiﬁed as salient features, and hence\neach short-term spectrum is decomposed into a sum of\nnote spectra; the resulting encoding can be used as a ba-\nsis for polyphonic transcription. The system is based on\na probabilistic model equivalent to a form of noisy inde-\npendent component analysis (ICA) or sparse coding with\nnon-negativity constraints. We introduce a novel mod-\niﬁcation to this model that recognises that a short-term\nFourier spectrum can be thought of as a noisy realisation\nof the power spectral density of an underlying Gaussian\nprocess, where the noise is essentially multiplicative and\nnon-Gaussian. Results are presented for an analysis of a\nliverecordingofpolyphonicpianomusic.\n1. INTRODUCTION\nIn this paper we describe a method of spectral basis de-\ncompositionthatcanbeappliedtopolyphonicmusictran-\nscription. The approach belongs to and combines ele-\nments of a family of related methods that includes in-\ndependent component analysis (ICA) [8], sparse coding\n[6], non-negative matrix factorisation (NMF) [10], and\nnon-negativevariantsof bothICA [14] andsparse coding\n[7]. In the context of polyphonic transcription, the over-\nall methodology is to identify the extracted components\nwiththespectralproﬁlesofthedifferentnotes,andthusto\nachievethedecompositionofagivenmixedspectruminto\na sum ofthose belongingto the currentlysoundingnotes.\nThe fact that the basis is adaptive means that the spectral\nproﬁle of each note is learned by training the system on\nexamples of polyphonic music, not on isolated notes (as\nin,e.g.,[15]).\nSimilar approaches have been described in [1, 2, 16].\nThe technical novelty in this paper is that the underly-\ningprobabilisticmodelspeciﬁcallyaddressesissuestodo\nPermission tomakedigital orhard copies ofallorpartofthi swork for\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributed forproﬁtorcommercialadvantag e andthat\ncopies bear this notice and the full citation on the ﬁrstpage .\nc/circlecopyrt2004 Universitat Pompeu Fabra.with spectral estimation (and more generally, the estima-\ntion of variance) in a Bayesian context. Thus, quite apart\nfrom applications in polyphonictranscription and feature\nextraction,themodelformsatheoreticalbasisforspectra l\nestimationanddenoisingusinganICAmodeltoprovidea\nstrong but adaptive prior, which essentially plays the role\nofaschemaorstatistical summaryofpastexperience,en-\nabling the system to produce low-variance spectral esti-\nmatesfromlimiteddata.\nWe tested the system on a recordings of Bach’s G-\nminor fugue (No.16) from Book I of the Well Tempered\nClavier; some results from one of these experiments are\npresented in§5. Before that, the following sections de-\nscribe some of the theoretical aspects of adaptive basis\ndecomposition, Bayesian estimation of variance, and the\ncombinedsystemfornon-negativesparsecodingofpower\nspectra.\nWe adopt the following typographical conventions:\nvectors and matrices are written in boldface x,A; ran-\ndomvariablesandvectorsaredenotedbyuppercaseletters\nX,Y, while their realisations are denoted by lowercase\nletters x,y. Where these conventions clash, the intended\nmeaningshouldbeclearfromthecontext. Anglebrackets\n/angbracketleft·/angbracketrightwill denotetheexpectationoraveragingoperator.\n2. ADAPTIVEBASIS DECOMPOSITION\nSystems for adaptive basis decomposition generally as-\nsume a linear generative model of the form x=As, or,\nwritingoutthesumexplicitly,\nxi=m/summationdisplay\nj=1ajsj, (1)\nwherex= (x1, . . . , x n)denotes an n-component multi-\nvariate observation, the aj(for1≤j≤m) denote a dic-\ntionaryofm‘atomic’ features (which form the columns\nofthe n×m)dictionarymatrix A, ands= (s1, . . ., s m)\ncontains the weighting coefﬁcients. The purpose of the\nsystem is to learn, from examples of x, a dictionary ma-\ntrixAwhich containsa suitable collectionof atomic fea-\ntures, andthence to encode optimallyanygiven xas ans\nsuch that x≈As. The learningprocesscan be drivenby\na number of desiderata for the dictionary matrix and the\ncomponentsof s,someofwhichweoutlinebelow.\nAnassumptionofstatisticalindependencebetweenthe\nsj, motivated by considerations of redundancy reduction0 1 2 3 400.511.5\nxp(x|v=1)Gamma(d/2,2/d)\nd=2\nd=6\nd=18\n0 1 2 3 400.20.40.60.8\nxp(x|v=2)Gamma(d/2,4/d)\nd=2\nd=6\nd=18\nFigure 1.Some examples of Gamma distribution probability\ndensityfunctions. Notehowshiftingthemeanofthedistrib ution\nfrom 1 to 2 also doubles the standard deviation: this is becau se\nxistheproductof vwithaGamma-distributedrandomvariable,\nwhichtherefore plays the role of a multiplicativenoise.\nand efﬁcient representation [4], leads to ICA. Reducing\nthedependencebetweenthecomponentsrepresentingdif-\nferent notes should in principle reduce the need to ex-\namine several componentsin order to make one note on-\nset/offsetdecision.\nInsparsecoding,weassumethatmostobservationscan\nbeencodedwithonly‘afew’non-zeroelementsof s,that\nis, only a few atomic features are required to account for\ntypically observedpatterns. This ﬁts well with the notion\nthat, in music, a relatively small fraction of the available\nnotes will (usually) be sounding at any one time. Sparse\ncodingcan be facilitated in two distinct (but not mutually\nexclusive)ways: (a) byusinga verylargedictionarycon-\ntaining a wide variety of specialised features, or (b) by\nassuminganoisygenerativemodelsuchthat,after‘afew’\ndictionary elements have been activated, any remaining\ndiscrepanciescanbetreatedasnoise.\nInsomeapplications,thequantitiesinvolvedareintrin-\nsically non-negative; this is certainly the case for power\nspectra and variance estimates in general. Placing non-\nnegativityconstraintsontheatomicfeatures(theelement s\nofA), their weightingcoefﬁcients(the componentsof s),\nor indeedboth,can be enoughto achievemeaningfulfea-\nturedetectioninsomeapplicationswithoutanyadditional\nassumptions,asdemonstratedin[10].\nRelationships between these different requirements\nhave been investigated in, for example, [14, 7]. A recent\nand thorough treatment of sparse decomposition and dic-\ntionarylearningcanbefoundin [9].3. BAYESIAN ESTIMATION OFVARIANCE\n3.1. Univariatecase\nConsiderasysteminwhichwehave dindependentidenti-\ncally distributed (i.i.d.) Gaussian randomvariables(r.v s.)\nZkof zero mean and unknown variance v, that is, Zk∼\nN(0, v)for all 1≤k≤d. To estimate the variance v,\none would compute the mean-square of a sample of the\nvariables. It is a standard result (e.g., [5]) that, taken as\na random variable itself, this estimate has a Gamma (or\nscaledChi-squared)distribution:\nX=1\ndd/summationdisplay\nk=1Z2\nk∼Γ(d\n2,2\ndv)∼1\ndvχ2\nd.(2)\nSince/angbracketleftX/angbracketright=v, thisestimator isunbiased,but notingthat\nvappears only in the scale parameter of the Gamma dis-\ntribution, we can see that, as a noisy estimate of v, it in-\nvolves what is effectively a multiplicative, rather than an\nadditive, noise model, the standard deviation of the error\nbeingproportionalto v,thetruevariance. Theprobability\ndensityoftheestimategivena particularvarianceis\np(x|v) =(dx\n2v)d/2exp−dx\n2v\nxΓ(d/2), (3)\nwhere Γdenotesthe Gammafunction. Someexamplesof\nGamma densities are illustrated in ﬁg. 1. When inferring\nvfrom observed values of x, we interpret the conditional\ndensity p(x|v)as the likelihood of vgivenx; this can be\ncombinedwithanypriorexpectationsabout vin theform\nofa priordensity p(v)toyieldtheposteriordensity\np(v|x) =p(x|v)p(v)\np(x). (4)\nThemaximumaposteriori(MAP)estimateofthevariance\nistherefore\nˆv= argmax\nv{logp(x|v) + log p(v)}.(5)\nUsingtheGammadensity(3),thelog-likelihoodtermex-\npandsto\nlogp(x|v) =d\n2logdx\n2v−dx\n2v−logx−log Γ(d\n2).(6)\nConsidered as a function of v(illustrated in ﬁg. 2), this\nexpression playsthe role of a statistically motivatederro r\nmeasure,ordivergence,from vtox:\nlogp(x|v) =−E(v;x) +{Termsin xandd},(7)\nwithE(v;x) =d\n2/parenleftbigx\nv−1 + logv\nx/parenrightbig\n. (8)\nThe divergenceE(v;x)reachesa minimum of zero when\nv=x,butunlikethequadraticerrormesure (v−x)2,itis\nstrongly asymmetric, with a much higher ‘cost’ incurred\nwhenv < xthan when v > x. This expressesmathemat-\nically the intuition that samples from a Gaussian rv are\nquite likely to be much smaller than the standard devia-\ntion,butveryunlikelyto bemuchlarger.01234567811.522.53\nv−log p(x|v)Gamma log−likelihood\nx=1\nx=2\nFigure 2.Gamma distribution (negative) log-likelihood func-\ntion. The asymptotic behaviour is as 1/vasv→0and as logv\nasv→ ∞.\n3.2. Multivariatecase\nAssume nowthat Yis a multivariateGaussian(i.e.a ran-\ndom vector) with components Yk,1≤k≤N. Diagonali-\nsationofthecovariancematrix/angbracketleftbig\nYYT/angbracketrightbig\n=UΛUTyields\nthe orthogonaltransformation U(whose columns are the\neigenvectors ui),andthe eigenvaluespectrumencodedin\nthe diagonal matrix Λkl=δklλk. If there are degenerate\neigenvalues(i.e. of the same value), then the correspond-\ning eigenvectors, though indeterminate, will span some\ndeterminate subspace of RNsuch that the distribution of\nYprojected into that subspace is isotropic (i.e. spherical\norhyperspherical).\nNowletusassumethattheeigenvectors ukandthede-\ngeneracy structure of the eigenvalue spectrum are known\n(i.e.,the‘eigen-subspaces’areknown),buttheactualval -\nues of the λkare unknown, and are to be estimated from\nobservations. We are now in a situation where we have\nseveral independent copies of the problem described in\nthe preceding section. Speciﬁcally, if there are ndistinct\neigenvalues, then the subspaces can be deﬁned by nsets\nof indicesIisuch that the ith subspace is spanned by the\nbasis{uk|k∈Ii}. A maximum-likelihood estimate of\nthe variance in that subspace, vi=λk∀k∈Ii, can be\ncomputedasin(2):\nˆvi=xi=1\n|Ii|/summationdisplay\nk∈Ii(uT\nky)2, (9)\nwhere|Ii|, the dimensionality of the ith subspace, plays\nthe role of d, the Chi-squared ‘degrees-of-freedom’ pa-\nrameter in (2). Assuming the subspaces are statistically\nindependentgiven the variances vi, the rest of the deriva-\ntion of§3.1can be extended to yield the multivariate di-\nvergence\nE(v;x) =n/summationdisplay\ni=1|Ii|\n2/parenleftbiggxi\nvi−1 + logvi\nxi/parenrightbigg\n,(10)\nwherexandvdenote the arrays formed by the compo-\nnentsxiandvirespectively.3.3. Applicationtopower spectra\nOne of the effectsof a Fouriertransformis to diagonalise\nthecovarianceofastationaryGaussianprocess,theeigen-\nvalue spectrum being in this case equivalentto the power\nspectral density (PSD) of the Gaussian process. The dis-\ncrete short-termFourier transformis an approximationto\nthis (the windowing process makes it inexact) for time-\nvarying Gaussian processes; this is how the periodogram\nmethodofspectralestimationworks.\nPSDestimationisaspeciﬁcinstanceofamoregeneral\nclass of covariance estimation problem where the ‘eigen-\nsubspaces’happento beknownin advance: the diagonal-\nisingtransformationistheFouriertransform,andthesinu -\nsoidal eigenvectors (except those encoding the DC com-\nponentandpossiblytheNyquistfrequency)comeinpairs\nof equal frequency but quadrature phase. These pairs of\neigenvectorswillhavethesameeigenvalueandwillthere-\nforespananumberof2-Dsubspaces,oneforeachdiscrete\nfrequency. The problem of spectral estimation is equiv-\nalent to that of estimating the variance in each of these\nknown subspaces. If no further assumptions are made\nabout these variances (that is, if the prior p(v)is ﬂat and\nuninformative) then any estimated PSD will have a large\nstandard-deviation proportional to the true PSD as illus-\ntrated by the d= 2curves in ﬁg. 1. Our system aims to\nimprove these estimates by using a structured prior, but\nunlike those implicit in parametric methods such as au-\ntoregressivemodels(whichessentiallyamounttosmooth-\nness constraints), we use an adaptive prior in the form of\nanexplicitICA model.\n4. GENERATIVE MODEL\nThe prior p(v)on the subspace variancesis derivedfrom\nthe linear generative model used in adaptive basis de-\ncomposition (1): we assume that v=As, where the\ncomponents of sare assumed to be non-negative, inde-\npendent, and sparsely distributed. If Ais square and\nnon-singular,then we have p(v) = det A−1/producttextm\nj=1p(sj),\nwheres=A−1sandp(sj)istheprioronthe jthcompo-\nnent. Thesepriorsareassumedtobesingle-sided( sj≥0)\nand sharply peaked at zero, to express the notion that we\nexpect components to be ‘inactive’ (close to zero) most\nof the time. In the case that Ais not invertible, the situ-\nation is a little more complicated; we circumvent this by\ndoinginferenceinthe s-domainratherthanthe v-domain,\nthat is we estimate sdirectly by consideringthe posterior\np(s|x,A), rather than p(v|x). The elements of A, rep-\nresenting as they do a set of atomic power spectra, are\nalso required to be non-negative. The complete probabil-\nity modelis\np(x,s|A) =p(x|As)p(s)\n=n/productdisplay\ni=1p(xi|vi)m/productdisplay\nj=1p(sj),(11)\nwherev=Asandp(xi|vi)is deﬁned as in (3). An\nimportant point is that this linear model, combined withthe multiplicative noise model that determines p(x|v), is\nphysically accurate for the composition of power spectra\narising from the superposition of phase-incoherentGaus-\nsian processes, barring discretisation and windowing ef-\nfects. It is not accurate for magnitude spectra or log-\nspectra. On the otherhand, additiveGaussian noise mod-\nelsarenotaccuratein anyofthesecases.\n4.1. Sparse decomposition\nIt is straightforward to extend the analysis of §3.2to ob-\ntain a MAP estimate of the components of srather than\nthoseof v:\nˆs= arg min\nsE(As;x)−logp(s),(12)\nwhere p(s)is the factorial prior p(s) =/producttextm\nj=1p(sj).\nWhen the p(sj)have the appropriate form (see [9]), the\n−logp(s)termsplaysthe role ofa ‘diversity’cost which\npenalises non-sparse activity patterns. If we assume the\np(sj)to becontinuousand differentiablefor sj≥0, then\nlocal minima can be found by searching for zeros of the\ngradient of the objectivefunction in (12). Using (10) this\nexpandsto aset of mconditions\n∀j,n/summationdisplay\ni=1Aij|Ii|\n2/parenleftbiggvi−xi\nv2\ni/parenrightbigg\n+ϕ(sj) = 0,(13)\nwhere ϕ(sj)def=−d logp(sj)/dsj. The optimisation can\nbe achieved by standard 2nd order gradient based meth-\nodswithnon-negativityconstraints,butthesetendtocon-\nverge poorly, and for large systems such as we intend to\ndeal with, each individual step is rather expensive com-\nputationally. Steepest descent is worse still, tending to\nbecomeunstableasanycomponentof vapproacheszero.\nWe found that a modiﬁed form of Lee and Seung’s\nnon-negativeoptimisationalgorithm[11]givesmuchbet-\nter overallperformance. Theiralgorithmminimisesa dif-\nferent measure of divergence between Asandx, with\nno additional diversity cost. Our modiﬁcation accommo-\ndates both the diversity cost (as in [7]) and the Gamma-\nlikelihood based divergence(10). The iterative algorithm\nis as follows. (To simplify the notation, we will assume\nthat∀i,|Ii|=d, i.e.,that all theindependentsubspaces\nof the original Gaussian rv are of the same dimension d.)\nThesjare initialised to some positive values, afterwhich\nthefollowingassignmentismadeateachiteration:\n∀j, s j←sj/summationtextn\ni=1Aijxi/v2\ni\n(2/d)ϕ(sj) +/summationtextn\ni=1Aij/vi(14)\nThis is guaranteed to preserve the non-negativity of the\nsjas long as the Aijare non-negative and ϕ(sj)≥0\nforsj≥0, though some care must be taken to trap\nany division-by-zero conditions which sometimes occur.\nStates that satisfy (13) can easily be shown to be ﬁxed\npointsof(14),andalthoughtheconvergenceproofsgiven\nin[11,7]donotapply,wehavefoundthatinpractice,the\nalgorithmconvergesveryreliably. Note that the subspacedimensionality parameter d(i.e. the degrees-of-freedom\nin the Gamma-distributed noise model) reduces to con-\ntrolling the relative weighting between the requirements\nofgoodspectralﬁt ononehandandsparsityontheother.\n4.2. Learningthedictionarymatrix\nWe adoptamaximum-likelihoodapproachtolearningthe\ndictionary matrix A. Due to well known difﬁculties [9]\nwithmaximisingtheaveragelog-likelihood /angbracketleftlogp(x|A)/angbracketright,\n(that is, treating the components of sas ‘nuisance vari-\nables’ to be integrated out) we instead aim to maximise\nthat average jointlog-likelihood/angbracketleftlogp(x,s|A)/angbracketright. Let\nx1:T≡(x1, . . . ,xT)denotea sequenceof Ttrainingex-\namples,with s1:Tthecorrespondingsequenceofcurrently\nestimated componentsobtained by one or more iterations\nof (14), and vt=Ast, where Ais the current estimated\ndictionarymatrix. Then,thecombinedprocessesofinfer-\nenceandlearningcanbewrittenasthejointoptimisation\n(ˆA,ˆs1:T) = arg max\n(A,s1:T)T/summationdisplay\nt=1logp(xt,st|A),(15)\nwhere p(x,s|A)is deﬁned in (11). Both multiplicative\nand additive update algorithms were investigated. Addi-\ntive updates were found to be adequate for small prob-\nlems(e.g. n= 3),butunstablewhenappliedtorealpower\nspectra ( n= 257,n= 513). The following multiplica-\ntive update (modelled on those in [10]) was found to be\neffective (the sequence index thas been appended to the\ncomponentindices iandj):\n∀i, j, A ij←Aij/parenleftBigg/summationtextT\nt=1sjtxit/v2\nit/summationtextT\nt=1sjt/vit/parenrightBiggη\n,(16)\nA←normalise pA, (17)\nwhere η≤1isastepsizeparametertoenableanapproxi-\nmateformofonlinelearning,andtheoperator normalise p\nrescalestounit p−normeachcolumnof Aindependently.\nFor example, if p= 1, the column sums are normalised.\nThe values of s(and hence v=As) inside the sums\nmay be computed either by interleaving these dictionary\nupdates with a few incremental iterations of (14), or by\nre-initialisingthe standapplyingmany(typically,around\n100) iterations of (14) for each iteration of (17). Clearly,\nthelatteralternativeismuchslower,butistheonlyoption\nif onlinelearningisrequired.\n5. APPLICATION TOPIANOMUSIC\nThe system was tested on a recording of J. S. Bach’s\nFugue in G-minor No. 16. The input was a sequence of\n1024 Fourier spectra computed from frames of 512 sam-\nples each, with a hop size of 256 samples, covering the\nﬁrst91\n2bars of the piece. The only preprocessing was a\nspectral normalisationor ﬂattening (that is, a rescaling o f\neach row of the spectrogram) computed by ﬁtting a gen-\neralised exponential distribution to the activities in eac h(a)\nfrequency/kHz\n20 40 60 80012345\n−6−4−20\n(b)\nfrequency/kHz\n20 40 60 80012345\n−50−40−30−20−100\nFigure3.Dictionarymatrixbefore (a)andafter(b)trainingon\nan extract of piano music. The remnants of the original struc -\ntured dictionary are still visible, but each pitched spectr um has\nbecome adapted to the actual spectrum of the piano notes in th e\npiece. The colour scale islogarithmic and indB.\nspectralbin,asoutlinedin[2]. Inthesparsecodingsystem\ndescribedtherein,spectral ﬂattening was used to improve\nthe ﬁt between the assumed additive white noise model\nand the data. The multiplicative noise system described\nin thispaperdoesnotrequiresucha rescalingof thedata,\nbut the preprocessing was retained both as an aid to vi-\nsualisation (so that, for exampe, the detail in the upper\nfrequencies of ﬁg. 4(a,c) is visible) and to enable future\ncomparisonsbetween the additive and multiplicative sys-\ntems.\nExperiments were performed both with random and\nstructuredinitialdictionaries. Thestructuredinitiald ictio-\nnary,illustratedinﬁg.3(a),consistedofanorderedcolle c-\ntion of roughlypitchedspectra on a quarter-tonespacing.\nEach column of the dictionary matrix was a constructed\nasasequenceofbroadharmonicsincreasinginwidthwith\nfrequency,toallowforinharmonicityorvariationsininto -\nnationinthetonestobeanalysed. Aftertraining,bothdic-\ntionaries converged to qualitatively similar solutions, b ut\ninitialisation with the structureddictionarytends to res ult\ninacorrectlyorderedﬁnaldictionary. Thisordering,how-\never, is not essential to the system, and can be recovered\naftertraining.\nTrainingwasaccomplishedbyalternateapplicationsof\nthemultiplicativeupdaterules(14)and(17),with p= 1inthe column normalisation step; that is, each atomic spec-\ntrumwasnormalisedtohaveunittotalenergy. Inaddition,\na small constant offset was periodically added to all ele-\nments of Aands1:Tin order to nudge the system out of\nlocal minima caused by zero elements, which, of course,\nthemultiplicativeupdatesareunabletoaffect. Theresult -\ningdictionaryisillustratedin ﬁg.3(b).\nThe next step in the process was the categorisation\nof the atomic spectra in the learned dictionary as either\npitched or non-pitched, followed by an assignment of\npitchtoeachofthepitchedspectra. The‘pitchedness’cat-\negorisationwas donebya visual inspectionofthe spectra\nin combination with a subjective auditory assessment of\nthe sounds reconstructed from the atomic spectra by ﬁl-\ntering white Gaussian noise, as described in [2]. We are\ncurrently investigating quantitative measures of ‘pitche d-\nness’ sothatthisprocesscanbeautomated.\nOnce pitches had been assigned to eached of the\npitched spectra in the dictionary, we found that many\npitches were represented by more than one dictionary el-\nement, which elements can therefore be arranged into\ngroups by pitch. The different elements in a particular\ngrouprepresentdifferentspectral realisationsof the sam e\npitch, which may occur during different instances of the\nsame note or at different stages in the temporal evolution\nof a single note. For example, the fourth note (an F /sharp3) in\nthe extract in ﬁg. 4(b) can be seen to involve activity in\ntwo dictionaryelements.\nIn order to obtain the pitch traces in ﬁg. 4(d), the ac-\ntivities (i.e. the componentvalues sj) in each pitch group\nweresummed. Speciﬁcally,if Pkisthesetofcomponents\nin the kth pitch class, then the activity of that pitch class\nis\nσk=/summationdisplay\nj∈Pksj. (18)\nSince the dictionary matrix is column normalised using a\n1-norm, each atomic spectrum has the same total energy,\nso the sum of the activities in each group has a direct in-\nterpretation as the total energy attributable to that note.\nThese energies have a very wide dynamic range, so, for\ndisplaypurposes,weplot log(σ2\nk+ 1)foreachpitchclass\nin ﬁg.4(d).\nThepitcheddictionaryelementscorrespondedtonotes\ninathreeoctaverangefromE2toG5. We haveyettoim-\nplementtheﬁnalstagesofeventdetectionandtime quan-\ntisation, so an evaluation was done by visual comparison\nof ﬁg. 4 with the original score. All the notes in the ﬁrst\n91\n2barswere correctlydetected,exceptfor a repeatedG4\nin bar 5, which is coalesced into the preceding G4 (cir-\ncledinﬁg.4(d),attime10.5s). Givenasufﬁcientlyrobust\npeak-pickingalgorithm,mostof theerrorswouldbe false\ndetections of repeated notes, (two of which are circled in\nﬁg. 4) though we cannot provide any quantitative results\nat this stage. The manual evaluation can be summarised\nasfollows:\nNotesinoriginalextract 163\nCorrectlydetectednotes 162\nFalse detections 2(a) Input spectrogram x1:T\nfrequency/kHz\n0 5 10 15 20012345\n−40−2002040\n(b) Sparse coding s1:T\ncomponent\n0 5 10 15 2020406080\n0246810\n(c) Reconstructed estimatedspectrogram v1:T=As1:T\nfrequency/kHz\n0 5 10 15 20012345\n−40−2002040\n(d) Totalactivityineachpitchgroup\n0 5 10 15 20eff#gabbcc#debeff#gabbcc#debeff#gabbcc#debf#g\nFigure 4.Analysis of the ﬁrst 91\n2bars of Bach’s Fugue in G-minor, BWV861: (a) input spectrogr am; (b) sparse coding using the\ndictionary in ﬁg. 3; (c) reconstruction, which can be though t of as a ‘schematised’ version of the input—note the denoisi ng effect and\nthe absence of beating partials. Finally(d) graphs the pitc h-group activities as log(σ2\nk+ 1), where kranges over the pitches and σkis\ndeﬁned in(18). Errorsintranscriptionare circled(see tex t). Inallﬁgure, the x-axisindicates timeinseconds.\u0000\n\u0001\n\u0001\n\u0002\u0000\u0001\n\u0001\u0003\n\u0004 \u0005\u0005\n\u0005\u0005\n\u0005\u0005\n\u0006\n\u0005\u0005\u0005\u0004\n\u0007\u0005\u0005\u0005\n\u0004\u0005\n\u0005\u0001\u0005\u0005\u0005\u0004\n\u0005\u0005\u0005\n\b\n\t\n\u0005\u0005\n\u0005\u0005\u0005\n\u0005\u0005\n\u0005\u0005\u0005\n\u0005\u0005\n\u0005\u0006\u0007\n\u0005\u0005\n\u0005\u0005\u0005\n\t\n\u0005 \u000b\n\u0005\u0005\u0004 \u0005\u0005\u0006\n\u0005\u0005\n\u0005\u0006\u0005\u0005\n\u0005\b\n\u0005\u0005\u0005\u0005\n\u0005\n\u0005\n\u0004\n\t\n\b\n\u0005\n\u0005\u0005\u0005\n\u0005\u0005\b\n\u0005\n\u0005\n\u0005\u0005\u0005\n\t\n\b\f\n\u0005\u0004 \u0005\n\u0005\n\u0005\u0005\n\t\u0003\u0001\n\u0001\u0000\n\b\n\t\n\u0002\u0001\n\u0001\u0000\n\r\u0003\u0001\n\u0001\n\u0002\u0001\n\u00016\u0005\u0005\n\u0006 \u000e\u0005\n\t\n\u0005\u0005\n\u0006\u0005\u0005\u0005\u0006\n\u0004\u0005\u0005\n\u0005\u0005\u0006\u0005\u0005\n\u0005\b\u0005\n\u0005\u0005\u0005\n\u0005\u0005\u0005\n\u0005\n\u0006\u0005\u0005\n\u0004\b\u0005\n\b\n\u0005\u0005\u0005\n\u000f\u000f\n\u0005\u0005\b\u0005\u0005\n\u0005\n\u0004\u0005\u0005\u0005\n\u0004\u0005\u0005\u0005\n\u0005\u0005\u0005\n\b\f\u0005\u00056\u0001\n\u0001\n\u0002\u0001\n\u0001\u0003\u0005\n\u0001\u0005\n\u0005\u0001\u0005\u0005\u0005\n\u0004\u0005\u0005\n\u0006\u0005\u0005\n\u0005\u0005\u0005\n\u0005\u0005\u0004 \u0005\u0005\n\u0005\n\u0005\u0006\u0005\n\u0010\n\u000b\n\n\u0005\n\u0005\n\u0005\n\u0005\u0005\n\u0005\u0005\n\u0005\u0005\u0004\n\u0005\u0005\u0005\n\u0005\n\u0005\n\u0005\u0005\n\u0005\n\r\nFigure5.The ﬁrst10bars of the Fugue inG-minor, BWV861.\n6. RELATIONS WITH OTHERMETHODS\nThe algorithm presented here is largely derivedfrom Lee\nand Seung’s multiplicativenon-negativematrix factorisa -\ntion(NMF)algorithms[10,11],whicharefoundedontwo\ndivergencemeasures,oneofwhichisquadraticandcanbe\ninterpretedin terms of an additive Gaussian noise model;\ntheother,inthepresentnotation,canbewrittenas\nELS(v;x) =n/summationdisplay\ni=1xilogxi\nvi−xi+vi(19)\nThisdivergencemeasureisrelatedtotheKullback-Leibler\ndivergence,and can be derivedby interpreting vas a dis-\ncreteprobabilitydistribution(overwhateverdomainisin -\ndexed by i) andxas a data distribution. The divergence\nmeasuresthelikelihoodthat that thedata distributionwas\ndrawn from the underlying probability distribution spec-\niﬁed by v. Smaragdis [16] applies this form of NMF to\npolyphonictranscriptionandachievesresultsverysimila r\ntothosepresentedinthispaper. However,wewouldargue\nthat power spectra are notprobability distributions over\nfrequency, and that the resulting system does not have a\nformalinterpretationintermsofspectralestimation.\nHoyer[7]modiﬁedthequadratic-divergenceversionof\nLee and Seung’s method to include the effect of a sparse\nprior on the components s, and applied the resulting sys-\ntem to image analysis. He used an additive update step\n(with post-hoc rectiﬁcation to enforcenon-negativity)fo r\nthe adaptingthe dictionarymatrix A, rather than a multi-\nplicative step. In the present work, additive updates were\nfound to be rather unstable due to singularities in the di-\nvergence measure (10) as any component viapproaches\nzero.\nAbdallah and Plumbley [1, 2] applied sparse coding\nwith additive Gaussian noise and no non-negativity con-\nstraints to the analysis of magnitude spectra. The algo-\nrithm was based on the overcomplete, noisy ICA meth-\nods of [13]. The system was found to be effective for\ntranscribingpolyphonicmusic rendered using a synthetic\nharpsichordsound,but lessable todeal with thewide dy-\nnamicrangeandspectralvariabilityofarealpianosound.It is interesting to note some parallels between the\npresent work and the polyphonic transcription system of\nLepain [12]. His system was built aroundan additive de-\ncomposition of log-power spectra into a manually cho-\nsen basis of harmoniccombs. This basis includedseveral\nversions of each pitch with different spectral envelopes.\nThe error measure used to drive the decomposition was\nan asymmetric one. If, using the current notation, we let\nwi= log vi, and zi= log xi, Lepain’s error measure\nwouldbe\nEL(w;z) =n/summationdisplay\ni=1(wi−zi), w i≥zi∀i.(20)\nFor comparison, the log-likelihood logp(z;w)can be\nderived from the Gamma-distributed multiplicative noise\nmodel(3),yielding\n−logp(z|w) =n/summationdisplay\ni=1di\n2{exp(zi−wi) + (wi−zi)}\n+{Termsin di},(21)\nwhere didenotesthedegrees-of-freedomforthe ithcom-\nponent. The exponential term means the error measure\nrises steeply when wi< zi, but approximately linearly\nwhen wi> zi, and thus Lepain’s error measure can be\nseen as a rough approximation to this, using a hard con-\nstraint instead of the exponential ‘barrier’ found in the\nprobabilisticallymotivatedmeasure.\n7. SUMMARY ANDCONCLUSIONS\nA system for non-negative, sparse, linear decomposition\nof power spectra using a multiplicative noise model was\npresented and applied to the problem of polyphonictran-\nscription froma live acoustic recording. The noise model\nwas derived from a consideration of the estimation of the\nvariance of a Gaussian random vector, of which spectral\nestimation is a special case, while the generative model\nforpowerspectrabelongstoaclassofICA-basedmodels,\nin which the power spectra are assumed to the result ofalinearsuperpositionofindependentlyweighted‘atomic’\nspectrachosenfromadictionary. Thisdictionaryisinturn\nlearned from, and adapted to, a given set of training data.\nThesetheoreticalunderpinningsmeanthatthesystemhas\na formal interpretation as a form of spectral estimation\nfor time-varying Gaussian processes using a sparse fac-\ntoriallineargenerativemodelasanadaptiveprioroverthe\npowerspectra.\nThe learned dictionary can be thought of as an en-\nvironmentally determined ‘schema’, a statistical sum-\nmary of past experiences with power spectra, which en-\nables the system to make better inferences about newly-\nencountered spectra. When exposed to polyphonic mu-\nsic, this schema quickly adapts to the consistent presence\nof harmonically structured notes. The internal coding of\nspectra (i.e. the components of s) therefore reﬂects the\npresence or absence of notes quite accurately, while the\nreconstructedspectra(thevectors v=As)areessentially\na ‘schematised’ (cleaned up, denoised, and ‘straightened\nout’)versionoftheinput( x).\nTheencodingproducedbythesystem,thoughnotaﬁn-\nished transcription, should provide a good basis for one,\nonce the ﬁnal stages of (a) automatic grouping of dictio-\nnary elements into subspaces by pitch and (b) event de-\ntectionon theper-pitchtotal energytraces, havebeenim-\nplemented. The manual evaluation ( §5) suggests that a\ntranscription accuracy of 99% could be achievable given\na sufﬁciently robust and adaptable ‘peak-picking’ algo-\nrithm;werefertheinterestedreaderto[3]foranoverview\nofourinitial effortsinthat direction.\n8. ACKNOWLEDGMENTS\nThisworkwassupportedbyEPSRCgrantGR/R54620/01\n(Automaticpolyphonicmusictranscriptionusingmultiple\ncause models and independent component analysis) and\nEU-FP6-IST-507142 project SIMAC (Semantic Interac-\ntionwithMusicAudioContents).\n9. REFERENCES\n[1] Samer A. Abdallah. Towards Music Perception by\nRedundancy Reduction and Unsupervised Learning\nin Probabilistic Models . PhD thesis, Department\nof Electronic Engineering, King’s College London,\n2002.\n[2] Samer A. Abdallah and Mark D. Plumbley. Unsu-\npervised analysis of polyphonic music using sparse\ncodinginaprobabilisticframework. IEEETrans.on\nNeuralNetworks , 2003. Submittedforreview.\n[3] Samer A. Abdallah and Mark D. Plumbley. Unsu-\npervised onset detection: a probabilistic approach\nusing ICA and a hidden Markov classiﬁer. In Cam-\nbridge Music Processing Colloquium , Cambridge,\nUK,2003.[4] Horace B. Barlow. Sensory mechanisms, the reduc-\ntionofredundancy,andintelligence. In Proceedings\nof a Symposium on the Mechanisation of Thought\nProcesses ,volume2,pages537–559,NationalPhys-\nical Laboratory, Teddington, 1959. Her Majesty’s\nStationeryOfﬁce,London.\n[5] William Feller. An Introduction to Probability The-\nory andits Applications ,volumeII. JohnWiley and\nSons,NewYork,1971.\n[6] David J. Field andBrunoA. Olshausen. Emergence\nof simple-cell receptive ﬁeld propertiesby learing a\nsparse code for natural images. Nature, 381:607–\n609,1996.\n[7] Patrik O. Hoyer. Non-negative sparse coding. In\nNeural Networks for Signal Processing XII (Proc.\nIEEEWorkshoponNeuralNetworksforSignalPro-\ncessing),Martigny,Switzerland,2002.\n[8] Aapo Hyv¨ arinen. Survey on Independent Compo-\nnent Analysis. Neural Computing Surveys , 2:94–\n128,1999.\n[9] Kenneth Kreutz-Delgado, Joseph F. Murray, and\nBhaskar D. Rao. Dictionary learningalgorithmsfor\nsparserepresentation. NeuralComputation ,15:349–\n396,2003.\n[10] DanielD.LeeandH.SebastianSeung. Learningthe\npartsofobjectsbynon-negativematrixfactorization.\nNature,401:788–791,1999.\n[11] Daniel D. Lee and H. Sebastian Seung. Algorithms\nfor non-negative matrix factorization. In Todd K.\nLeen, Thomas G. Dietterich, and Volker Tresp, ed-\nitors,Advances in Neural Information Processing\nSystems, volume 13, pages 556–562, Cambridge,\nMA,2001.MITPress.\n[12] Philippe Lepain. Polyphonic pitch extraction from\nmusical signals. Journal of New Music Research ,\n28(4):296–309,1999.\n[13] Michael S. Lewicki and Terrence J. Sejnowski.\nLearning overcomplete representations. Neural\nComputation ,12:337–365,2000.\n[14] Mark Plumbley. Algorithms for nonnegative inde-\npendentcomponentanalysis. IEEE Transactionson\nNeuralNetworks , 14(3):534–543,2003.\n[15] L. Rossi, G. Girolami, and L. Leca. Identiﬁcation\nof polyphonic piano signals. Acustica, 83(6):1077–\n1084,1997.\n[16] Paris Smaragdis. Non-negative matrix factorization\nfor polyphonic music transcription. In 2003 IEEE\nWorkshop on Applications of Signal Processing to\nAudio and Acoustics , pages 177–180, New Paltz,\nNY, 2003."
    },
    {
        "title": "Time Series Alignment for Music Information Retrieval.",
        "author": [
            "Norman H. Adams",
            "Mark A. Bartsch",
            "Jonah Shifrin",
            "Gregory H. Wakefield"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415694",
        "url": "https://doi.org/10.5281/zenodo.1415694",
        "ee": "https://zenodo.org/records/1415694/files/AdamsBSW04.pdf",
        "abstract": "Time series representations are common in MIR appli- cations such as query-by-humming, where a sung query might be represented by a series of ‘notes’ for database retrieval. While such a transcription into a sequence of (pitch, duration) pairs is convenient and musically intu- itive, there is no evidence that it is an optimal represen- tation. The present work explores three time series repre- sentations for sung queries: a sequence of notes, a ‘smooth’ pitch contour, and a novel sequence of pitch histograms. Dynamic alignment procedures are described for the three representations. Multiple continuity constraints are ex- plored and a modified dynamic alignment procedure is de- scribed for the histogram representation. We measure the performance of the three representations using a collection of naturally sung queries applied to a target database of varying size. The results show that the note representation lends itself to rapid retrieval whereas the contour represen- tation lends itself to robust performance. The histogram representation yields performance nearly as robust as the contour representation, but with computational complex- ity similar to the note representation.",
        "zenodo_id": 1415694,
        "dblp_key": "conf/ismir/AdamsBSW04",
        "keywords": [
            "time series representations",
            "MIR applications",
            "query-by-humming",
            "sung queries",
            "sequence of notes",
            "pitch contour",
            "pitch histograms",
            "dynamic alignment procedures",
            "continuity constraints",
            "performance measurement"
        ],
        "content": "TIME SERIES ALIGNMENT FOR MUSIC INFORMA TION RETRIEV AL\nNorman H.Adams, Mark A.Bartsc h,Jonah B.Shifrin, Gregory H.Wake\u0002eld\nEECS Dept., University ofMichig an\nAnn Arbor ,MI48109-2110\nnhadams@umich.edu\nABSTRA CT\nTimeseries representations arecommon inMIR appli-\ncations such asquery-by-humming, where asung query\nmight berepresented byaseries of`notes' fordatabase\nretrie val.While such atranscription into asequence of\n(pitch, duration) pairs isconvenient andmusically intu-\nitive,there isnoevidence thatitisanoptimal represen-\ntation. Thepresent workexplores three time series repre-\nsentations forsung queries: asequence ofnotes, a`smooth'\npitch contour ,andanovelsequence ofpitch histograms.\nDynamic alignment procedures aredescribed forthethree\nrepresentations. Multiple continuity constraints areex-\nplored andamodi\u0002ed dynamic alignment procedure isde-\nscribed forthehistogram representation. Wemeasure the\nperformance ofthethree representations using acollection\nofnaturally sung queries applied toatargetdatabase of\nvarying size. Theresults showthatthenote representation\nlends itself torapid retrie valwhereas thecontour represen-\ntation lends itself torobustperformance. The histogram\nrepresentation yields performance nearly asrobustasthe\ncontour representation, butwith computational comple x-\nitysimilar tothenote representation.\n1.INTR ODUCTION\nTimeseries representations areubiquitous ininformation\nretrie valapplications [14,27]. MIR isnoexception; se-\nquences ofnotes, pitch estimates, orMFCCs, forexam-\nple,arecommon [5,6,16]. Itiswell established thatfor\ncomparing twotime series, adirect comparison, such as\ntheEuclidean distance, yields abrittle metric [13,27]. A\nsimilarity metric thatisrobusttoelastic shifts andscales\nofthetime indexisrequired. This hasreinvigorated in-\nterest indynamic time warping (DTW), string-matching,\nandother ef\u0002cient time series alignment methods within\ntheIRcommunity .Allsuch alignment procedures have\ncomple xityO(NK),where NandKarethelengthes of\nthetwosequences. Accordingly ,itisdesirable tokeep\nthelength, ordimension, oftherepresentation assmall\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forpro\u0002t orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation onthe\u0002rstpage.\nc\r2004 Universitat Pompeu Fabra.aspossible. However,thisoften comes attheexpense of\nretrie valperformance.\nInthepresent workwecompare therelati vemerits of\nthree time series representations forsung queries. Two\nrepresentations havebeen previously proposed: apitch\ncontour1[16,28],andasequences ofnotes [2,6,17,20].\nAuni\u0002ed presentation ofthealignment procedure isgiven\nforthetworepresentations. Forthecontour representa-\ntion weexplore multiple continuity constraints and\u0002nd\nthatjudicious slope constraints impro veperformance con-\nsiderably .Wealsopropose anovelsequence ofpitch his-\ntograms derivedfrom thepitch contour .Unlik eother pitch\nhistogram representations [9,24], which were proposed\ntoallowforerrors inpitch detection, weemplo ypitch\nhistograms toeliminate ambiguous timing information,\nthus quick ening thealignment procedure. This represen-\ntation violates thelocal comparison constraint ofthecom-\nmon DTW algorithm, hence amodi\u0002ed dynamic align-\nment procedure ispresented.\nWeevaluate theperformance ofthethree representa-\ntions intheconte xtofaquery-by-humming (QBH) sys-\ntem. Early QBH successes [11,17]havesuggested nu-\nmerous alternati vetechniques [6,9,20], which upon fur-\nther investig ation haveyielded results that areoften in-\nconclusi veanddif\u0002cult togeneralize [6,7].Inthepresent\nworkwefocus onrelati veperformance trends between the\nthree representations rather than theabsolute performance\nofanyone. Weapply ourmethods toavariable database\nofthemes, similar to[6]. Insodoing, weobserv esome\ninteresting trends between thedifferent methods.\nAsforanyexperiment, numerous simpli\u0002cations are\nmade tofacilitate measurement andeliminate distracting\ndetails. Our hope isthat, byconsidering howtheper-\nformance decreases astargetdatabase sizeincreases, we\narriveatameasure ofhowthemethods would perform\ninabroader conte xt.That is,manyofthecomplicating\nfactors thataffectQBH performance inareal-w orld sce-\nnario canbeaddressed byexpanding thesizeofthetarget\ndatabase [6].\nThenextthree sections describe, inturn, thecomputa-\ntionandalignment ofthecontour representation, thenote\nrepresentation andthehistogram representation. Section 5\n1Inthepresent work, thepitch contour isde\u0002ned astheoutput of\napitch tracking algorithm. SeeFig.1forasample pitch contour .Other\nauthors usepitch contour torefer toacoarsely quantized sequence of\npitch differences ofanote sequence, asinParson' sdirectory ofthemes,\ni.e.,UDUDR Uforexample [19].0 1 2 3 4 5 6 7 834363840424446485052MIDI pitch\nTime (s)\nFigur e1.Sample query andtargetpitch contours forthe\nmain theme from Richard Rodgers' Sound ofMusic. \nThequery pitch contour isgivenbythesolid lineandthe\ntargetpitch contour isgivenbythedashed line.\npresents ourmethods forevaluation, along with results\nanddiscussion. Section 6concludes thepaper .\n2.CONT OUR REPRESENT ATION\nThe present workispredicated ontheassumption that\nqueries should becoded using primarily pitch informa-\ntion. Singers cansing thesame melody with varying am-\nplitude envelopes, lyrics andstyle; wedesire ourquery\nrepresentation tobeinvariant tosuch variables, however.\nAssuch, allthree query representations considered here\narederivedfrom thepitch contour .The pitch contour is\nestimated using atime-domain method [2,4].This algo-\nrithm computes theautocorrelation foroverlapping win-\ndowsofrecorded data. Thebiasofthewindo wfunction is\nmitig ated bythenormalization ~r(\u001c)=r(\u001c)\nrw(\u001c),where r(\u001c)\nistheautocorrelation ofthewindo wed data andrw(\u001c)is\ntheautocorrelation ofthewindo wfunction. Asetofcan-\ndidate peaks isselected foreveryframe andtheViterbi\nalgorithm isused toconstruct asmooth contour .Weusea\nstep-size of10msthroughout thiswork. Thefundamen-\ntalfrequenc yvalues arethen converted toMIDI pitch2,\nyielding the\u0002nal pitch contour .Fig. 1showsasample\npitch contour forthemain theme from Sound ofMu-\nsic.Included inFig. 1isthepiece wise constant ideal\ncontour forthesame theme stored inourtargetdatabase.\nNote thatinthis\u0002gure thetargetcontour hasbeen time-\nscaled tohaveequal duration asthequery .Furthermore,\nnote thatthequery contour contains gapscorresponding to\nshort pauses takenbythesinger; thepitch track algorithm\nperforms animplicit voiced/ unvoiced segmentation.\nThequery pitch contour caneither beused directly by\ntheretrie valsystem, asproposed in[15,16,28],orfurther\ncoded. Inthepresent workweextend thealignment pro-\ncedure presented in[15,16]andobserv esome interesting\ntrends relati vetoother query representations. Theremain-\n2Thereal-v alued MIDI pitch number pisrelated toasignal' sfunda-\nmental frequenc yinHz,f0,asp=12log2(f0=261)+60.General\nConstraintItakura-Saito\nConstraintSakoe-Chiba\nConstraint1b\n11\n11 11\n1b\nbb1 2b1\nb21\nFigur e2.Three local continuity constraints forDTW .The\nSakoe-Chiba continuity constraint isshownwith aslope\nconstraint ofp=1\n2[23].\nderofthissection describes thealignment procedure for\nthecontour representation.\nTheretrie valcomponent compares thequery pitch con-\ntour with thepiece wise constant contour foreverytheme\nstored inthetargetdatabase. Letthequery pitch contour\nbegivenbyQ=(q1;q2;\u0001\u0001\u0001qK)andthetargetcontour\ngivenbyT=(t1;t2;\u0001\u0001\u0001tN).Adirect, albeit `brittle, '\ndissimilarity measure isgivenby[14]\nD=min(K;N)X\nk=1jqk\u0000tkjp: (1)\nAmore robustmetric isyielded with dynamic time warp-\ning(DTW), analignment procedure popularized inthe\n1970' sforspeech recognition [10,22,23]. Theincreasing\nnumber ofmultimedia applications forlargecollections\noftime series hasrecently reinvigorated interest inDTW\nwithin thedatabase community [14,21,2628].\nDTW isused to\u0002nd aminimum cost alignment be-\ntween thequery andtargetcontours. Let\u0000=[\rn;k]bean\nN\u0002Kmatrix ofminimum pre\u0002x alignment costs; \rn;kis\ntheminimum alignment costfor(q1\u0001\u0001\u0001qk)and(t1\u0001\u0001\u0001tn).\nLetawarping path begivenbyw=(w1;w2;\u0001\u0001\u0001wT),\nwhere wt=(n;k)indicates thatqkisaligned withtn.\nThewarping path must adhere toseveralconstraints tobe\nphysically meaningful. Thepath must beginwith the\u0002rst\nelements ofthequery andtargetcontour ,andendwith\nthelastelements ofthequery andtarget;w1=(1;1)\nandwT=(N;K).Thepath must bemonotonic nonde-\ncreasing andadhere toalocal continuity constraint. Nu-\nmerous continuity constraints havebeen explored [10,21\n23,26]. Inthepresent workweconsider three common\ncontinuity constraints; thegeneral constraint [15,22],the\n`Itakura' constraint [10] andthe`Sakoe'constraint [23].\nTheItakura andSakoecontinuity constraints place bounds\nontheslope ofthealignment path. Thethree continuity\nconstraints areshowninFig.2.Starting inthelowerleft\ncorner of\u0000,everyelement of\u0000isfound recursi velyby\n\rn;k=min0\n@\rn\u00001;k\n\rn;k\u00001\n\rn\u00001;k\u000011\nA+Match Cost(qk;tn)(2)\nforthegeneral continuity constraint, where\nMatch Cost(qk;tn)=jqk\u0000tnjp: (3)Therecursi veequations for\rn;kfortheItakura andSakoe\ncontinuity constraints aresimilar [1,23,26], with edge\nweights showninFig. 2.\f\u00151isanextracost penalty\napplied tofavormore direct alignment paths. Tospeed\ncomputation andpreventpathological warpings, thealign-\nment path isnotallowed tostray toofarfrom thediagonal\nn=k.That is,8n;k:jn\u0000kj>r!\rn;k=1,where\nristhewindo wlength, orradius, ofthealignment path.\nThe\u0002nal alignment costforQandTisgivenby\rN;K,\nandisinterpreted asadissimilarity measure. Byperform-\ningthisalignment oneverytargetinthedatabase wecan\nrank order thetargetthemes. Thecomple xityofthisalign-\nment procedure isO(NK).Note thatthe\u0002nal alignment\ncost isnotnormalized bythetotal length ofthewarping\npath. Using \rN;K=Tasthe\u0002nal metric iscommon in\nDTW systems toavoidpenalizing long targets[15,22,23].\nHowever,asdiscussed below,oursystem time-scales all\ntargetstothelength ofthequery prior toalignment. Nor-\nmalizing bythetotal warping path length prohibits theuse\nofmanycost schemes, astheDPalgorithm isnolonger\nguaranteed to\u0002nd theoptimal alignment path after nor-\nmalization. Thecost scheme weusefortheSakoeconti-\nnuity constraint, forexample, violates theDPconstraint if\nthe\u0002nal alignment costs arenormalized. Assuch, wedo\nnotnormalize the\u0002nal alignment costs. However,whether\ntheuseofacostscheme thatviolates theDPconstraint (in\nwhich case the\u0002nal alignment path issuboptimal) detri-\nments retrievalperformance isanunanswered question.\n2.1. Implementation issues\nAscanbeseen inFig. 1,thequery pitch contour con-\ntains gapswhere verthesinger isnotarticulating acoher -\nentpitch. Itisunclear howtomatch such `null' query\nregions tothetargetcontour .Accordingly ,nullquery re-\ngions are\u0002lled inwith aconstant pitch, computed asthe\nweighted average ofthelastfewcontour values ofthepre-\nvious real-v alued region.\nQueries arerarely sung inthesame key,orwith the\nsame tempo, asthetargettheme. Toachie vetempo invari-\nance, thetargetnote durations arescaled toyield acon-\ntouroftotal length equal tothequery contour .Toachie ve\ntransposition invariance, theaverage pitch difference be-\ntween thequery andtargetisremo vedprior toalignment.\nNote thatthese twonormalizations achie vethedesired in-\nvariance only ifthetwocontours haveapproximately the\nsame shape; asdiscussed inSec. 5.1,weassume theuser\nsings thesame partofthetheme asisstored inthetarget\ndatabase.\nDirect useofthepitch contour yields aquery repre-\nsentation ofrelati velylargedimension. Forastep-size\nof10ms, typical pitch contours areoflength \u0018103,\nwhereas thenote sequences discussed inthenextsection\nareoflength \u0018101.Direct useofthecontour provedto\nbecomputationally burdensome. Accordingly ,weemplo y\nasimple dimension reduction technique proposed in[14];\nquery pitch contours aredecimated byafactor of10,yield-\ningarepresentation oflength \u0018102[15].3.NOTEREPRESENT ATION\nAmusically intuiti vequery representation isasequence\nof(pitch, duration) pairs, e.g., anestimate ofthenote se-\nquence sung bytheuser.Inthiscase thedimension of\nthequery representation isrelati velysmall, ontheorder\nof101.\nIn[2,3]severalsung melody transcription methods are\nexplored foruseinQBH systems. Twoofthemethods\ndiscussed in[2]areincluded inthepresent study: thebest\nandworst performers. The primary difference between\nthetwonote estimators isthenote segmentation. Thepoor\nnote estimator uses asmoothed derivativeofthepitch con-\ntour todetect note boundaries [6,17]. Thegood note es-\ntimator uses anHMM with 12pitch states peroctaveand\n\u0002nds themost likelystate sequence fortheobserv edcon-\ntour.Thetwonote estimators areincluded inthepresent\nstudy toindicate therange ofperformance possible using\nasequence ofnotes torepresent asung query .Because\nnote off-set time isanunreliable statistic, inter-onset in-\nterval(IOI) isused fornote duration [18].\nAcommon method forcomparing twonote sequences\nistakenfrom biological sequence analysis [8]. Letthe\nquery sequence ofKnotes begivenbyQ=(q1;q2;\u0001\u0001\u0001\nqK),whereqk=(qk;pit;qk;dur),andletthetargetse-\nquence ofNnotes begivenbyT=(t1;t2;\u0001\u0001\u0001tN).An\nalignment between thetwosequences isachie vedbyin-\nserting, deleting, andreplacing elements ofthequery in\norder tomatch thetarget. Numerous cost schemes were\nexplored, andwefound asimple approximation toanedit-\ndistance DTW metric tobeparticularly effective[1]. In-\nserting ordeleting anote yields acostequal totheduration\nofthenote. Replacing qkwithtnhascost\nReplace Cost(qk;tn)=jqk;dur\u0000tn;durj+\n2\n\u000bmin(qk;dur;tn;dur)jqk;pit\u0000tn;pitj:(4)\nThe replacement cost hastwocomponents, acost pro-\nportional tothedifference indurations (tomakethedu-\nrations equal), andacost proportional tothepitch differ-\nence times theminimum ofthetwodurations (tomakethe\npitches equal, either before insertion orafter deletion). \u000b\nisthecross-o verpitch difference thatrelates thereplace-\nment costwith insertion anddeletion costs. Replacing two\nequal-duration notes with apitch difference equal to\u000bhas\nequal costtoaninsertion-deletion pair.\nSimilar toDTW ,anoptimal alignment between Qand\nTisfound byrecursi velybuilding amatrix\u0000=[\rn;k]of\nminimum pre\u0002x alignment costs. Letthealignment path\nbegivenbyw=(w1;w2;\u0001\u0001\u0001wT).The path must be\nmonotonic nondecreasing andadhere tothegeneral local\ncontinuity constraint; 8t;wt\u0000wt\u000012f(0;1);(1;1);\n(1;0)g.Starting inthelowerleftcorner of\u0000,everyele-\nment of\u0000isfound recursi velyby\n\rn;k=min0\n@\rn\u00001;k+Insert Cost(tn);\n\rn;k\u00001+Delete Cost(qk);\n\rn\u00001;k\u00001+Replace Cost(qk;tn)1\nA:\n(5)The\u0002nal alignment cost forQandTisgivenby\rN;K.\nThecomple xityofthisalignment procedure isO(NK).\nToachie vetempo invariance, allquery andtargetnote\ndurations arenormalized bythemean query andtarget\nnote duration, respecti vely.Toensure transposition in-\nvariance, weiterati velysubtract themean pitch difference\nbetween thealigned sequences andthen realign these-\nquences.\n4.HIST OGRAM REPRESENT ATION\nAswill beshowninthenextsection, thecontour repre-\nsentation yields more robustperformance than thenote\nrepresentation. This comes atthecost ofamuch larger\nquery representation, however.Indeed, forourMATLAB\nimplementation, aligning apair ofnote sequences takes\nabout 0.001s andaligning apair ofpitch contours takes\nabout 0.1s, similar to[6]. The contour representation is\nimpractical foranyreal-w orld QBH system with alarge\ntargetdatabase. This observ ation motivatesanalternati ve\nrepresentation thatyields similarly robustperformance as\nthecontour representation, butwithout thecomputational\nburden. Wepropose anovelsequence ofpitch histograms.\nPerhaps themost dif\u0002cult component ofsung query\ntranscription isnote segmentation [2,3,17,18,28].There is\naninherent tradeof fbetween insertion anddeletion errors\ninanysegmentation problem [2,12]. Most QBH systems\nareimplicitly tuned tohaveroughly equal note insertion\nanddeletion rates. Aninteresting exception ispresented\nin[21], inwhich anote estimator with ahigh insertion\nrateiscoupled with analignment procedure thataccounts\nformanyinsertion errors. Asdiscussed insection 2,the\npitch detection algorithm emplo yedinthepresent work\nperforms apartial segmentation, which canbeinterpreted\nasanote segmenter with ahigh note deletion rate3.Inthis\ncase, each contour region represents oneormore notes.\nEach region iscollapsed intoasingle histogram ofpitches.\nInsodoing, wemodel thesung query asapartially or-\ndered set4.\nPitch histograms havebeen proposed before forMIR.\nTzanetakis andCook [25] proposed pitch histograms for\ngenre classi\u0002cation andHeoet.al.[9]andSong et.al.[24]\nhaveproposed sequences ofpitch histograms forquery-\nby-humming. There aresome important distinctions how-\never.In[9,24]pitch histograms areemplo yedtoaccount\nforuncertainty inpitch detection aswell aspolyphonic\nsources. Incontrast, weemplo ypitch histograms todis-\ncard ambiguous timing information within each contour\nregion. Furthermore, [9,24]compute pitch histograms for\naconstant frame size, whereas wecompute asingle his-\ntogram foreverycontour region. Inthiscase theduration\nrepresented byeach histogram varies, andamodi\u0002ed DP\nalignment algorithm isrequired.\n3Indeed, applying such anote segmenter toourdatabase ofsung\nqueries (that is,using thegapsinthepitch contour toindicate newnotes)\nyields asegmentation with adeletion rateofabout 35% [2].\n4The order ofcontour regions isde\u0002ned, buttheorder ofpitches\nwithin each region isnot.0 50 100 150 200 250 300 350 400 450 5005657585960616263\nSample NumberMIDI pitch\nFigur e3.Sample query contour forYankeeDoodle. \n051015Region 1\n0246Region 2\n01020Region 3\n0510Region 4\n0102030Region 5\n56 57 58 59 60 61 62 6302040\nMIDI PitchRegion 6\nFigur e4.Pitch histograms forcontour inFig.3.\n4.1. Query &targethistograms\nAsample query pitch-contour forthetune YankeeDoo-\ndle isshowninFigure 3,andthecorresponding sequence\nofpitch histograms isshowninFigure 4.\nLetthesequence ofquery histograms begivenbyQ=\n(q1;q2;\u0001\u0001\u0001;qK),andthetargetsequence givenbyT=\n(t1;t2;\u0001\u0001\u0001;tN).Each query histogram isgivenbyqk=\n[q1\nk;q2\nk;\u0001\u0001\u0001;qM\nk],andeverytargethistogram bytn=[t1\nn;\nt2\nn;\u0001\u0001\u0001;tM\nn],where thenumber ofpitch contour values in\nthemthbinofthekthquery histogram isgivenbyqm\nk.\nBoth sequences arenormalized tounit`volume, '\nKX\nk=1MX\nm=1qm\nk=NX\nn=1MX\nm=1tm\nn=1; (6)\nthus guaranteeing tempo-in variance. Tomaketherep-\nresentation transposition-in variant, themean pitch differ-\nence between thequery andtargetcontour sissubtractedfrom thecontours prior tocomputing thehistograms, as\ndescribed inSection 2.1.\nThetargetsequence isrepresented using aseparate his-\ntogram foreverynote;8n;9!mS.T.tm\nn>0.Users\noften sing portions ofamelody continuously ,hence typi-\ncallyK<N.Because qkmay represent more than one\nnote, itisnecessary tocompare qktoacollection oftarget\nhistograms. Accordingly ,lett(n;p)bethecumulati vesum\noftnthrough tp.\n4.2. Match cost\nWeuseamusically intuiti vematch costforcomparing qk\nandt(n;p)thatshares some features with quantization er-\nror.The match cost iscomputed bypartitioning qkinto\np\u0000n+1cells; each query cellisthen `quantized' tothe\ncorresponding non-zero binint(n;p).AVoronoi partition\nisused tode\u0002ne cellboundaries5.Thecostofeach cellis\ngivenbythesum ofa`duration' component anda`quanti-\nzation' component. Theduration component isthediffer-\nence between thetotal duration represented bythequery\nandtargetcells. Thequantization component isgivenby\ntheabsolute error incurred by`quantizing' thequery bins\ntothenon-zero bininthetargetcell. Adetailed descrip-\ntionofthecellcost canbefound in[1].The\u0002nal match\ncostisgivenbythesum ofallcellcosts.\n4.3. Alignment procedur e\nForthecontour representation, everyquery pitch is\nmatched tooneofNpossible targetpitches. Forthehis-\ntogram representation however,everyquery histogram, qk,\nismatched tooneofN2+N\n2possible cumulati vetargethis-\ntograms, t(n;p).To\u0002nd theoptimal alignment between\nsequences QandTweconstruct aN\u0002N\u0002Kma-\ntrix\u0000=[\rn;p;k].The(n;p;k)thelement of\u0000repre-\nsents theminimum alignment cost forthepre\u0002x subse-\nquences (q1\u0001\u0001\u0001qk)and(t1\u0001\u0001\u0001tp)withqkmatched to\nt(n;p).Note thecritical assumption, thateveryhistogram\naligns with atleast onetargethistogram. Starting with\nk=n=1,everyelement of\u0000isfound recursi velyby[1]\n\rn;p;k=min\nr\rr;n\u00001;k\u00001+Match Cost(qk;t(n;p)):(7)\nFigure 5showsanexample ofthepoints in\u0000thatthe\nalignment path canvisit. This \u0002gure showsthecase for\nN=12andK=6.The dark squares inthekthpanel\nofthe\u0002gure showallthepossible cumulati vetargethis-\ntograms thatthekthquery histogram canbematched to.\nForeach panel, thevertical indexgivesthestarting point\nnforthecumulati vehistogram andthehorizontal index\ngivestheending pointp.Forexample, the\u0002rstquery his-\ntogram, k=1,must match toacumulati vetargethis-\ntogram beginning withn=1,butcanendanywhere from\np=1top=7(p=8\u0001\u0001\u000112isdisallo wed because atleast\n\u0002vetargethistograms areneeded tomatch totheremain-\ning\u0002vequery histograms).\n5Cell boundaries aregivenbythemidpoint between non-zero bins in\nt(n;p)Query Region\nk = 2\nQuery Region\nk = 6Query Region\nk = 5Query Region\nk = 3Query Region\nk = 1Target Ending IndexTarget Beginning Index\nQuery Region\nk = 4\nFigur e5.This \u0002gure showstheallowable alignment\npoints inthecoast matrix \u0000.Each panel showsoneslice\nofthematrix, corresponding tooneregion ofquery pitch\ncontour .Dark squares represent points thealignment path\nmay visit. Inthisexample thequery isrepresented bya\nsequence ofsixhistograms andthetargetbyasequence\noftwelv ehistograms.\n5.EVALU ATION\n5.1. Query Database\nForperformance evaluation, weemplo yaquery testset\ncontaining manysample queries offourteen popular tunes,\nfrom theBeatles' Hey,Jude toRichard Rodgers' Sound\nofMusic. Atotal of480queries were collected from \u0002f-\nteen participants inourstudy .Each participant wasasked\ntosing afamiliar portion ofasubset ofthefourteen tunes\nfour times each. The participants hadavariety ofmusi-\ncalbackgrounds; some hadconsiderable musical orvo-\ncaltraining while most hadnone atall.Participants were\ninstructed tosing each query asnaturally aspossible us-\ningthelyrics ofthetune6.Thequeries aremonophonic,\n16bitrecordings sampled at44.1 kHz andresampled to\n8kHz toreduce processing time. The queries arebe-\ntween 5and20seconds inlength. Alldata were collected\ninaquiet classroom setting andparticipants were freeto\nprogress attheir ownpace.\nNote thatforallfourteen melodies, everyparticipant\nsang thesame setofmeasures. Forareal-w orldQBH sys-\ntem, thisisanunreasonable assumption. Some systems\naddress thisproblem byspeci\u0002cally allowing forinserted\nanddeleted notes atthebeginning andendofthetheme in\ntheretrie valcomponent [18]. Another common practice\nistoinclude multiple themes foreach tune inthetarget\ndatabase [6],increasing thesizeofthetargetdatabase.\n6This contrasts substantially from thecommon practice ofhaving\nparticipants sing isolated pitches onaneutral vowel, requiring partici-\npants toperform note segmentation [2].5.2. TargetDatabase\nMeasuring theretrie valperformance ofourQBH methods\nonatargetdatabase ofonly thefourteen themes forwhich\nwehavesample queries would notindicate howwell the\nmethods would perform inabroader conte xt.Augmenting\nthetargetdatabase with extrathemes thatarenotincluded\ninthequery testsetwasproposed in[6]. Inthepresent\nworkwefollowsuitandinvestig atehowthevarious QBH\nmethods perform asthesizeofthetargetdatabase increases.\nHowthetargetdatabase ofthemes isaugmented isof\ncritical importance. Theadditional themes must besuf\u0002-\nciently similar totheoriginal themes tosubstantially detri-\nment retrie valperformance. That is,augmenting ourtar-\ngetdatabase with themes thatareverydifferent from those\nrepresented inthetestsetwould notaffectperformance.\nScaling thetargetdatabase toinclude thousands ofsim-\nilar`authentic' themes isdif\u0002cult. Accordingly ,weaug-\nment thefourteen authentic themes with avarying number\nof`synthetic' targetthemes. Itshould benoted from the\noutset thatusing synthetic targets limits howtheresults\ncanbeinterpreted; weareconcerned with relati vetrends\nrather than absolute performance, however.Wegener -\natesynthetic themes using aMark ovmodel designed to\nyield note sequences with similar \u0002rst-order statistics as\nthefourteen authentic themes. Foreach synthetic target,\ntwoMark ovmodels arebuilt; oneforgenerating ase-\nquence ofnote pitches, inwhich everystate represents a\nunique pitch, andoneforgenerating asequence ofnote\ndurations, inwhich everystate represents aunique du-\nration. The fourteen authentic targets areused toassign\nstate transition probabilities. Adetailed description ofthe\nMark ovmodels canbefound in[1]. Note thatmanyof\nthesynthetic themes aremusically unsatisfying, since they\nobeyonly the\u0002rst-order Mark ovproperties oftheoriginal\nthemes. However,because none oftheQBH methods we\nexplored consider higher -order statistics ofthequery ,the\nsynthetic targetsdonotfavoronemethod overtheothers.\n5.3. Results\nTwoperformance metrics arereported: theclassi\u0002cation\naccurac yandmean reciprocal rank. Theclassi\u0002cation ac-\ncurac y(CA)isthefraction ofqueries thatareclassi\u0002ed\nasthecorrect target. The mean reciprocal rank (MRR )\nistheaverage inverse rank ofthecorrect target. Note\nthat CA\u0014MRR \u00141+CA\n2.Using thequery and tar-\ngetdatabases described above,theCAandMRR iscom-\nputed forthesevenQBH implementations presented in\nSections 2through 4.Fourimplementations using the\ncontour representation areincluded: thedirect compari-\nsonof(1)aswell asDTW using thegeneral, Itakura and\nSakoecontinuity constraints. Twoimplementations using\nthenote representation areincluded: thepitch-deri vative\nnote estimator (labelled Note: \u0001PSeg.)andtheHMM\nnote estimators. Lastly ,thehistogram representation isin-\ncluded.\nFig.6displays theclassi\u0002cation accurac yforthe7QBH\nmethods. The targetdatabase size isrepresented alongtheabscissa andtheclassi\u0002cation accurac yalong theor-\ndinate. The fourteen `authentic' themes areincluded in\neverydatabase size, hence forthelargest targetdatabase\nsize, 3570 ofthe3584 themes are`synthetic. 'Forallbut\nthesmallest andlargesttargetdatabases, thepoints shown\nareanaverage across multiple targetdatabases. Data are\nshownalong with best-\u0002tting linear curves7.Clearly ,the\nlinear \u0002tcannot beextrapolated inde\u0002nitely ,CAcannot\nbecome negative.Nonetheless, alinear \u0002tprovides aprag-\nmatic visual aidandimplies a`slope, 'orrateofperfor -\nmance degradation. Fig. 7displays themean reciprocal\nrank forthesame7QBH systems across thesame range of\ntargetdatabases. Forboth \u0002gures, thecontour representa-\ntions areshownwith asolid line, thenote representations\nwith adashed line, andthehistogram representation with\nadash-dot line.\n5.4. Discussion\nInFig. 6,therelati veperformance ofthevarious QBH\nmethods demonstrate several interesting trends. Asex-\npected, direct comparison ofpitch contours, without align-\nment, yields thepoorest performance. Itisstriking how-\neverthatusing acommon note estimator with alignment\nonly yields marginally better performance, andthatthis\nimpro vement quickly vanishes astargetdatabase sizein-\ncreases. That is,melody transcription coupled with align-\nment does notnecessarily perform anybetter than Eu-\nclidean distance applied directly tothepitch contours.\nThe HMM note estimator yields considerably better\nperformance than thesimple pitch derivative.Indeed, for\nasmall targetdatabase size, thenote representation com-\nputed using theHMM estimator yields thebest perfor -\nmance, CA\u001992%.However,therateofperformance\ndegradation forthismethod isconsiderably faster than\nthatofthebest contour representations. Thecontour rep-\nresentation using theItakura andSakoecontinuity con-\nstraints yield themost robustperformance, theSakoecon-\nstraint inparticular .Forthelargesttargetdatabase sizethe\nbestcontour representations yield CA\u001980% whereas for\nthebestnote representations CA\u001975%.That thecontour\nrepresentation ismore robusttoincreasing targetdatabase\nsizeisnotsurprising; asthenumber oftargetsgrows,tar-\ngetsplaced in\u001810-Dspace willinevitably becloser than\ntargetsin\u0018100-Dspace.\nThehistogram representation does notoutperform the\nbest note orcontour representations. However,therate\nofperformance degradation forthehistogram represen-\ntation isconsiderably slowerthan thatofthenote repre-\nsentations. The CAslope forthehistogram representa-\ntion isequal tothat ofthecontour representation using\ntheItakura continuity constraint. Forthelargest target\ndatabase, thehistogram representation yields equal CAas\nthebest note representation. This isanintriguing obser -\nvation because while thecontour representation yields the\nmost robustperformance, itiscomputationally burden-\n7This isincontrast to[6],who found a1\nx\u0002t(onalog-scale) tobetter\nmatch their data.0.60.650.70.750.80.850.90.95\nTarget Database SizeClassification Accuracy\n14 28 56112 224 448 8961792 3584Contour: Direct\nContour: General\nContour: Itakura\nContour: Sakoe\nNote: HMM\nNote: D Pitch Seg.\nHistogram\nFigur e6.Classi\u0002cation accurac yfor8QBH methods.\nTheindependent variable isthesizeofthetargetdatabase.\nForeverytargetdatabase, the14themes represented inthe\nquery testsetareincluded.\nsome. Alignment ofhistogram sequences isonly mod-\nestly more time consuming than forthenote representa-\ntion8.Asthetargetdatabase isscaled tomassi vepropor -\ntions, thehistogram representation could provide thebest\ncompromise between performance andretrie valspeed.\nTheperformance ofthehistogram representation could\nbeimpro vedbymore judicious `continuity' constraints\nandhistogram match costs. Forthecontour representa-\ntion, thebest performance isachie vedwith theItakura\nandSakoeconstraints. Noalternati veconstraints were ex-\nplored forthehistogram representation, there being noob-\nvious interpretation forthe`slope' ofthealignment path.\nFurthermore, thealignment procedure assumes thatevery\nquery histogram represents atleast onetargetnote, inser -\ntionerrors aredisallo wed. Some ofthequery pitch con-\ntours contain spurious contour regions which result from\nenvironmental noise, however.Manyofthese spurious re-\ngions could bediscarded using aminimum duration con-\nstraint, butthose thatremain cause thealignment proce-\ndure to\u0002ndimplausible alignments.\nComparing Figures 6&7,thenote representations yield\nrelati velybetter performance interms ofMRR than CA.\nIndeed, forthelargesttargetdatabase, themedian rank of\nallmisclassi\u0002ed queries using thenote representation is\nabout 30,whereas themedian rank isabout 100forthe\ncontour representation. This isduetothedifferent cost\nschemes. Thecontour andhistogram costschemes donot\n8ForourMATLAB implementation, aligning apairofhistogram se-\nquences takesabout 0.003s. Indeed, forthehistogram representation, \u0000\ncontains \u0018103elements. Whereas forthenote andcontour representa-\ntions,\u0000contains \u0018102and\u0018104elements, respecti vely.0.60.650.70.750.80.850.90.95\nTarget Database SizeMean Reciprocal Rank\n14 28 56112 224 448 8961792 3584Contour: Direct\nContour: General\nContour: Itakura\nContour: Sakoe\nNote: HMM\nNote: D Pitch Seg.\nHistogram\nFigur e7.Mean reciprocal rank for8QBH methods.\nallowforportions ofthequery tobeexplicitly `deleted'\nor`inserted, 'everyelement ofthequery must bematched\ntoatleast oneelement inthetarget. This occasionally\nresults inpathological alignments, radically warping the\nquery sequence toaccount forashort portion ofincongru-\nousdata. Insuch situations thenote representation align-\nment simply deletes orinserts theappropriate element.\nForthetargetdatabase of14themes, thebest classi\u0002-\ncation accurac yobserv edisabout92%.Wenote thatclas-\nsi\u0002cation accurac yof98% isachie vedbyremo ving the\nsample queries ofthree subjects from ourtestdatabase.\nThese queries were veryinaccurate andsome were vir-\ntually monotone; thelyrics were their only recognizable\nfeature. Itisunclear thatanyQBH system should bede-\nsigned toaccommodate such queries. Speci\u0002c modeling\nofsinger andtranscription error isbecoming amore active\narea ofQBH research [18,21]. Asmentioned attheout-\nset,ourquery representations arebased exclusi velyonthe\nestimated pitch contour .Emplo ying other features such\nasbroad spectral orphonetic information may further im-\nproveperformance.\n6.CONCLUSION\nInthisworkwehaveexplored severalimplementations of\nthree different query representations: asequence ofnotes,\napitch contour (asequence ofuniformly sampled pitch\nestimates), andasequence ofpitch histograms. Wefound\nthatthenote representation lends itself tothemost rapid\nalignment yettheleast robustperformance. Conversely ,\nthecontour representation lends itself totheslowest align-\nment butthemost robustperformance. The histogram\nrepresentation yields acompromise between thenote and\ncontour representations.7.REFERENCES\n[1]N.H. Adams, Time Series Repr esentations for\nMusic Information Retrie val,University of\nMichig an (2004), http://www .eecs.umich.edu/\ntechreports/systems/cspl/cspl-349.ps.gz.\n[2]N.H. Adams, M.A. Bartch, and G.H. Wake\u0002eld,\nNote Segmentation andQuantization forMusic In-\nformation Retreival ,submitted toIEEE Transactions\nonSpeech andAudio Processing.\n[3] ,Coding ofSung Queries forMusic Infor -\nmation Retreival ,IEEE Workshop onApplication of\nSignal Processing toAudio andAcoustics (2003).\n[4]PaulBoersma, Accur ateShort-T erm Analysis of\ntheFundamental Frequency andtheHarmonics-to-\nNoise Ratio ofaSampled Sound ,Proceedings ofthe\nInstitute ofPhonetic Sciences oftheUniversity of\nAmsterdam, vol.17,1993, pp.97110.\n[5]P.Cano, E.Batlle, H.Mayer ,andH.Neuschmied,\nRobustSound Modeling forSong Identi\u0002cation in\nBroadcast Audio,AES Convention (Munich, Ger-\nmany),May 2002.\n[6]R.B.Dannenber g,W.P.Birmingham, andG.Tzane-\ntakis et.al.,The MUSART Testbed forQuery-By-\nHumming Evaluation ,Proceedings ofISMIR, 2003.\n[7]J.S.Downie, Towar dtheScienti\u0002c Evaluation of\nMusic Information Retrie valSystems ,Proceedings\nofISMIR, 2003, Baltimore, MD.\n[8]D.Gus\u0002eld, Algorithms onStrings, Treesand Se-\nquences ,Cambridge University Press, Cambridge,\nUK, 1999.\n[9]S.P.Heo, M.Suzuki, A.Ito,andS.Makino, Three\nDimensional Continuous DPAlgorithm forMulti-\nplePitchCandidates inMusic Information Retrie val\nSystem ,Proceedings ofISMIR, 2003, Baltimore.\n[10] F.Itakura, Minimum Prediction Residual Princi-\npleApplied toSpeec hReco gnition ,IEEE Transac-\ntions onAcoustics, Speech andSignal Processing 23\n(1975), no.1,6772.\n[11] T.Kage yama, K.Mochizuki, and Y.Takashima,\nMelody Retrie valwith Humming ,Proceedings ofInt.\nComputer Music Conference (ICMC), 1993, Tokyo.\n[12] S.Kay,Fundamentals ofStatistical Signal Process-\ning: Detection Theory ,Prentice Hall Ptr,Upper Sad-\ndleRiver,NJ,1998.\n[13] E.Keogh, Exact Indexing ofDynamic TimeWarping ,\nProceedings ofthe28th VLDB Conference (2002).\n[14] E.Keogh, K.Chakrabarti, M. Pazzani, and\nS.Mehrotra, Dimensionality Reduction forFast\nSimilarity SearchinLargeTimeSeries Databases ,\nKnowledge andInformation Systems 3(3) (2000).[15] D.Mazzoni, TechReport: Melody Matc hing Using\nTimeWarping ,Technical Report, Carne gieMellon\nUniversity (2002).\n[16] D.Mazzoni andR.B.Dannenber g,Melody Matc h-\ningDirectly fromAudio,Proceedings ofISMIR 2001\n(2001), Bloomington, IN.\n[17] R.J.McNab, L.A.Smith, andI.H.Witten etal,\nTowar dstheDigital Music Library: TuneRetrie val\nfromAcoustic Input ,Proceedings ofACM Digital\nLibraries Conference (1996), Bethesda, MD.\n[18] C.Meek andW.Birmingham, Johnny can'tsing: A\ncompr ehensive errormodel forsung music queries ,\nProceedings ofISMIR (2002).\n[19] D.Parsons, TheDirectory ofTunes,Spencer Brown\nandCo., Cambridge, England, 1975.\n[20] S.Pauws, Cubyhum: AFully Oper ational Query\nbyHumming System ,Proceedings ofISMIR, 2002,\nParis, France, pp.187196.\n[21] A.Pikrakis, S.Theodoridis, and D.Kamarotos,\nReco gnition ofIsolated Musical Patterns Using\nConte xtDependent Dynamic TimeWarping ,IEEE\nTransactions onSpeech and Audio Processing 11\n(2003), no.3,175183.\n[22] L.R. Rabiner and B.H. Juang, Fundamentals of\nSpeec hReco gnition ,Prentice Hall, Upper Saddle\nRiver,NJ,1993.\n[23] H.SakoeandS.Chiba, Dynamic Programming Al-\ngorithm Optimization forSpok enWordReco gnition ,\nIEEE Transactions onAcoustics, Speech andSignal\nProcessing 26(1978), no.1,4349.\n[24] J.Song, S.Y.Bae, and K.Yoon, Mid-Le velMu-\nsicMelody Repr esentation ofPolyphonic Audio for\nQuery-by-Humming System ,Proceedings ofISMIR\n(2002), Paris, France.\n[25] G.Tzanetakis, A.Ermolinsk yi,andP.Cook, Pitch\nHisto grams inAudio andSymbolic Music Informa-\ntion Retrie val,Proceedings ofISMIR, 2002, Paris,\nFrance.\n[26] R.YanivandD.Burshtein, AnEnhanced Dynamic\nTime Warping Model forImpr ovedEstimation of\nDTW Parameter s,IEEE Transactions onSpeech and\nAudio Processing 11(2003), no.3,216228.\n[27] B.K. Yi,H.Jagadish, andC.Faloustsos, Ef\u0002cient Re-\ntrievalofSimilar TimeSequences Under TimeWarp-\ning,Proc. IEEE International Conference onData\nEngineering, 1998, pp.201208.\n[28] Y.Zhu andD.Shasha, Warping Indexeswith Enve-\nlope Transforms forQuery byHumming ,Proc. Inter-\nnational Conference ofManagement ofData (SIG-\nMOD) (San Diego,CA), 2003."
    },
    {
        "title": "Whose future is it?",
        "author": [
            "Philippe Aigrain"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416404",
        "url": "https://doi.org/10.5281/zenodo.1416404",
        "ee": "https://zenodo.org/records/1416404/files/Aigrain04.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1416404,
        "dblp_key": "conf/ismir/Aigrain04",
        "content": "Whose futur eisit?\nPhilippe Aigrain\u0003\n12October 2004, keynote speech attheISMIR 2004\nConfer ence, Barcelona\nc\rPhilippe Aigrain, 2004. This textandtheassociated slides canbe\nused under thetermsoftheCreative Commons Attribution License\n2.0http://creativecommons.org/licenses/ by/2.0/\nGood morning. Itisagreathonour todeliver thistalktoaresear ch\ncommunity ofwhich Ihave been amember when wewerenotsurethat\nitwasadomain ofitsown. Ithassince then developed inanimpres-\nsivemanner ,andtoday itoffersallthesigns ofamaturing scienti\u0002c\ncommunity inatechnical \u0002eld: ayearly confer ence with growing at-\ntendance, some structuring paradigms, progressinmodelling itschal-\nlenges, attraction forbright graduate students, evaluation testbeds and\nbenchmarks. That's exactly when acommunity generally puts itself on\ntracks that willlead ittodeliver little ormuch progressforscience,\nlittle ormuch usefulness forsociety. That's when itisworth forsuch\nacommunity tothink awhile about wher eittries togo,totake [Slide:\nonestepaside] .\nThis honour isalso achallenge. IfIweretotally ignorant about\ninformation retrieval and content processing ofmusic, itwould be\nmuch morecomfortable. Iwould sharewithyousome general thoughts\nabout property andcommons forinformation andinformation technol-\nogy,about howresear chfunding mechanisms andintellectual property\nrules in\u0003uence thetargets ofresear chefforts. Andfrommyblissful ig-\nnorance, Iwould leave youwith thetask of\u0002guring outwhat itcan\nmean foryouandtheISMIR community. ButIcan't. Iknow just\nenough about music information retrieval tofeelforcedtoconnect my\ngeneral thinking withconcr eteexamples inthisdomain andneighbour -\ning\u0002elds. Inevitably, Iwillsayafewstupidities inthisprocess. Ihope\nyouwon't stop there,atidentifying these mistakes, that youwillstill\ntrytounderstand theissues Iamraising, andonlythen, decide ifthey\nareworth your consideration.\n\u0003Theauthor ispresently CEO ofSopinspace, Society forPublic Information Spaces\nwww.sopinspace.com .Hispersonal page isathttp://www.publicdebate.net/\nMembers/paigrain/\n1Little boxes\nIn1998, Andr ewOdlyzko1andmyself2hadthesame outcry ofintellec-\ntualrevolt: [Slide: Content isnotking!] .Weactually meant something\ndifferent.Imeant simply people are,while hemeant communication\nis.However ourmessage wasconsistent, andthough itwasnotin-\ntended speci\u0002cally formusic, itapplies toit.Music isarelationship\nbetween people, between gestur eandsound, between mind andper-\ncept. Music isnotcontents. Ofcourse, therehave been music boxes\n(Imean tools that automatically produce music) formany centuries,\nandboxed music (Imean recordedmusic oncarriers thatissoldasa\ncommodity) foralittle morethan acentury. However ,weshould not\nletourminds beframed inthelittle boxes thatthefather ofAnthony\nSeeger (last year's keynote speaker) hasmade famous byinterpr eting\nMalvina Reynolds' song [Slide] .Don't worry ifyouthink myimplicit\nstatement isdemagogic, thereisalsoaversion byWoody Guthrie (the\nbeatnik version) wher ehemakes funofthecritics. Anyway, music\nisnotboxed music. Music issomuch arelationship thatifsomeone\nknows toplayanyofit,heorshecanearnsome money inthestreets\nofmany cities intheworld, which issortofamiracle since atthesame\ntime, boxed music isforcedupon usinthestoresandrestaurants of\nthesame cities. Ofcourse thismiracle ispossible only because and\nwhen streetmusicians don't payperformance rights.\nSowhat hashappened around usinthepast 5years? What has\nhappened topeople, thewaythey access music anduseit?What has\nhappened topeople, thewaythey doresear chanduseit?\nTheground beneath ourfeet\n[Slide: Societal peer-to-peer \u0002lesharing networks have become amore\nef\u0002cient sourceofaccess topublished andbroadcasted material than\nlibraries andarchives] .Raise your hand ifyoudon't believe me. Of\ncourse thisisavery general statement. Itisabsolutely nottrue for\nunpublished primary sourcematerial. Butformaterial thathasbeen\npublished inanyform,ifyoutakearealbenchmark, notanhypotheti-\ncalresear cherthatwould instantly travel toalllibraries andarchives of\ntheworld andhave credentials there,butarealperson, whosuddenly\nwants access tooneperformance ofLaUltima Noche, oradocumen-\ntary movie that wasbroadcasted 2years agoonsome channel, the\nchances heorshewillsucceed ingetting itaremuch higher onP2P\nnetworks. Ofcourse thisisonly recently developing, anditisunder\nthreat.Legal andpolice threats, butalsothreatsfromtheprivate war\ngangs who inject fakes under thename ofP2Pwarfare.Iwillcome\nback later tohowonecanregain thebene\u0002ts ofthiswonder fulsocietal\nmechanism ofP2P\u0002lesharing, while appeasing some ofthepains itis\n1Andrew Odlyzko, Content isnotking, First Monday, 6(2), February 2001, revised\nversion onanoriginal 1998 paper .\n2Philippe Aigrain, The Best ofBoth Worlds: Can Mesia Quality beCombined with\nOpen Contents?, invited talkafIFTA's world conference, Vienna, 1998,http://www.\ndebatpublic.net/Members/paigrain/texts/IFTA. pdf.\n2claimed tocause.\n[Slide: Remix andhiphop scenes arecompeting with of\u0002cial con-\ntemporary music composition aspromising users forcontent-based\nsynchr onisation andinteraction] Iguess Paull Miller's quote speaks\nforitself. This isnotanewdevelopment, butresear chandcreation or-\nganisations areslow toreacttoit.Many people inyour community are\ninvolved asmusicians ortechnicians inthese communities, andsome\nareeven deriving inspiration fromthem, such astheideaofquery by\nbeat boxing .\n[Slide: Creative commons have gone fromidea toproject toreality\ninmany domains] .Many people inthiscommunity areactive sup-\nporters orwould-be supporter ofmusical commons. However ,veryfew\nareactively supported bytheir organisations inbuilding them. Cre-\native commons licenses aremoving usoutofthefalse dilemna between\nrespecting music byrestricting therelationship thatpeople build can\nbuild toit,andmaking itacommodity thatitisfreelylootable forthe\nboxed music business.\nInWIPO andUNESCO, thereispresently adebate about theso-\ncalled protection oftraditional knowledge, folklor eandgenetics re-\nsources. This issupposed tobeananswer tothewell-gr ounded com-\nplaints bydeveloping countries that power fulbusinesses arepirating\n(intherealsense) their traditional cultur eandnatural resour ces.What\nWIPO -and atalesser extent UNESCO- propose tothem istopartici-\npateintotheloot. Enclose them intoproperty, andbecome little looters\nofthecommon goods. Ofcourse thelittle looters willbelocally pow-\nerfulguys, andthey willhave tomake deals with thebiglooters for\nderiving trueincome, andyoucanbecertain thatthisincome will\u0003ow\ntoabout anywher eexcept intothepockets ofthepoor people whocre-\nated andcaredforthese treasur es.Creative commons, andfairglobal\nredistribution arenotaneasy path, butatleast itisapath intheright\ndirection. Thesame istrue inthedeveloped world. Intheeraofin-\nformation, inequality ofpower justasinequality ofwealth isfractal:\nyou\u0002nditbetween countries, between regions inacountry, between\nneighbour hoods inacity,andeven between companies andmusicians.\n[Slide: Softwar eandinformation patentability aretheobject of\u0002erce\npolitical debates inEuropeandglobally] .ISMIR isacommunity of\nopen exchange, wher eallproceedings areaccessible freeofcharge,\nandusable freelyforsome usage (even ifitwould bebetter de\u0002ned in\nmyopinion through aCreative Commons license than bythepresent\nclause). Butthisopen exchange isstopped attheborderofbecom-\ningfact. Technology transfer departments ofpublic resear chcentr es\narepatenting algorithms likehell,andintellectual property divisions of\ncompanies arepatenting softwar esystems components andapplication\nfeatur es.\nAfter 20years, reviews oftheimpact oftheBayh-Dole actconclude\ntoanunsigni\u0002cant contribution oflicensing asafunding sourcefor\npublic labs.However ,thesame technology transfer departments con-\nclude fromtheir failur ethatyoushould helpthem domoreofthesame.\nThefunny thing isthatwhile they carefully enclose thepublic domain\nofresear ch,innovation disseminates fromother sources,even ifthey\n3areinitially much lesssophisticated technically, simply because they\nhave chosen cooperation andsharing instead ofenclosur es.\nInventory of\u0002xtur es\nWhile preparing thistalk, Ireadthrough thesummaries ofcommu-\nnications atthisconfer ence, andinanumber ofcases through the\nproceedings paper version. Iproduced ataxonomy [Slide] ,which is\nslightly morere\u0002ned than theonetheprogram committee hasdone to\norganise thesessions. Which insight canwederived fromit?\nDon't worry, youdonothave toreadallthisstuff.Thetable isin\nthewritten version ofmytalk. Let's look atthetop-level categories.\nFirst, good news, theISMIR community isdoing lotsofbasic technical\nwork, morethan athirdofallpapers, moreifyoucount theevaluation\nbenchmarks. However ,themass ofaccumulated know-how isfarfrom\nbeing available ascomponents forpractical music applications. Thisis\nwher ethephenomena highlighted inmyslide onsoftwar epatents and\ntechnology transfer hitsstronger .Nothing speci\u0002c ofthemusic infor-\nmation \u0002eld: assoon assomething seems tohave some realpotential,\nthemotto seems tobelet's make surethan nobody canbecome rich\nwith itother than us. Why isthissodetrimental? Because then no-\nbody does. Because theideathatina\u0002eldlikemusic information any\nsingle organisation, even withitsindustry partners, willsucceed indis-\nseminating forusage aglobal platfor misridiculous. Nobody canhave\naclue ofwhat people willreally dowith afullchain ofmusic infor-\nmation processing, retrieval, re-creation, exchange. Thelastwhocan\nknow, ormoreprecisely thelastwhocanaccept toknow arethemajor\ncompanies. Youdon't believe me,wellwhydoyouthink thatforyears\nSony andPhilips have carefully buried alltheniceideas andtechnol-\nogythatweallknow exist intheir labs? Only when wehave asetof\nfreelyre-usable andadaptable music information technology modules,\nthataremoreorlesscombinable, andwith which people canexplor e\nnewways ofusage, willwestart toseesome realusage. Iwillcome\nback tothatlater.\nAttheother endofthetechnology spectrum, thesystems andtestbeds\nend, thereisalsosome verynicework, butitsuffersfromarelated dis-\nease: restrictive rights andtheir implementation byDRMs. Don't mis-\ntake thedreams ofmajors andtheir trusted technology suppliers for\nreality. DRMs have nofutur e,though they canwreckours. Only cre-\native commons anddigital rights information areworthwhile, because\nonlythem enable therelationship which ismusic. Don't letyourselves\nbeboxed byboxed music. What's wrongwith DRMs? Access andus-\nagecontr olbytechnology. Theuser asdanger ousenemy. Andtheir\ncomplexity. Noidea thatisthatcomplex, andthatinteracts with hu-\nman behaviour ever worked. TheInternetwould notwork ifithad\nbeen designed with thatmuch deter minism, thatmuch a-priori rules\nabout what people canandcan't do.Anddecreasing returns.DRMs\nand\u0002ngerprinting surveillance have acostperitem, andthereturns\naredecreasing. DRMs maybesoldunder thename offairreward,but\n4Topic Number Topic Number\nFeature extraction, clas-\nsi\u0002cation inonefeature\nspace, similarity mea-\nsures, structure extrac-\ntion45Systems 12\nTempo andbeats (including\ntracking)7Distributed music process-\ning1\nTimbreincl. drum sounds\nandgestur es5Metadata databases and\nbrowsers2\nInstrument 2Digital libraries and Web\nservices MIRsystems3\nDrum patter ns,rhythm 4Fingerprinting forDRMs 3\nVoice (singing detection),\nsinging language2Real-time synchr o\n(w/contextual info, w/\naccompaniment)2\nTonality, key 3Toolboxes (Matlab) 2\nChor ds 2Evaluation, testbeds, and\nbenchmarking9\nMotivic andmelodic lines 10 Featur eextraction andsim-\nilarity2\nOrnementation 1Query-by-humming based\nsystems2\nOther segmentation 2MIRsystems 2\nStructur eextraction 5MIRsystems with DRMs as-\npects3\nSound synthesis meta-\nlanguage1Classi\u0002cation (artists,\nsongs, genres)7\nOptical music recognition 1Clustering pieces andcollec-\ntions2\nMulti-featur eandsequence\nmatching5Artists 1\nUsage paradigms and\ntheir support23 Genr e 4\nSampling toMIDI instru-\nment1Empirical studies ofus-\nage6\nQuery byhumming 1Personal music collections 2\nQuery byvoice percussion 2Digital libraries MIR sys-\ntems2\nAnnotation 3P2PMIRsystems 1\nNotation, pitch spelling 2Social networks (artists) 1\nTranscription, music to\nscore,mapping topatter ns4\nAlignment toscore 4\nProduction andusage oftex-\ntualcontextual info3\nDocument space represen-\ntation3\nTable1:Topics atISMIR'20204\n5allthey willdoisaugment thealready huge concentration ofreward\nonbestsellers. Icanhear youthinking asweweretheones deciding.\nYes,youare.Many ofyour resear chcentr esdoproduce music, allof\nit,fromcomposition topublishing. Many ofyour organisations carefor\npublic archives, andhave accepted foryears without enough reaction\nthatthevarious extensions ofcopyright duration would stop anything\ntoenter thepublic domain. Many ofyour organisations areproducing\nmusic education softwar ethatisused inschools onsome prede\u0002ned\nmusic corpora. Isitacceptable thatthisisoften proprietary softwar e\n(paid bypublic money), andmusic corpora with termsofusethatfor-\nbidanyreal-world usage? Doyouthink music learning stops atthe\nwalls oftheclass-r oom?\nInbetween, wehave what Ihave called usage paradigms andtheir\nsupport. This isoften themost important item inatechnical commu-\nnity: thestructuring meeting point. TheISMIR community hasgone a\nlong waythere,fromthedominance oftheinitial Query byHumming\nparadigm tothings likealignment toscore(often agreatenabler), and\ndocument space representation, thathasproved apower fulapproach\ninother \u0002elds ofinformation retrieval. Thereisclearly stillalong way\ntogo,many other paradigms todiscover ,soIcanaddressyouwith a\nlesscontentious statement: justlet'skeep ourminds open.\nFinally, thereisanewcategory: realworld usage studies andthe\nconnected music classi\u0002cation. This isavery welcome category, at\nleast when work therestarts with anopen mind. Forinstance, this\nistheonly domain ofMIR wher epeople consider P2P\u0002lesharing as\naninteresting element ofreality, andnotacompr omising devil. Ihave\nbeen surprised however about how many people consider automatic\ngenreclassi\u0002cation. Itseems tomethatgenr eisaconcept thatmu-\nsicinformation technology isdestr oying quickly. Itisaconcept that\nwasinvented forformatting (forinstance inradio) andmarketing pur-\nposes (forinstance inrecordstores),because therewasnobetter way\ntopoint people towhat they might like. Sothereisariskthatthetar-\ngetofrecognition willevaporate faster than recognition technology will\ndevelop.\nAselection ofmusic infor mation dreams\nIwillconclude ona[slide] ofdreams. User dreams, Iamnolonger\ntrying tobuild these things (incase youwonder ,that's notbecause I\namretired,that's because Iamtrying tobuild other things).\nIwould liketoseeasetoffreesoftwar emodules, forvarious music\ncontent processing, indexing &retrieval, andinteraction functionality,\nwith abasic framework forcombining them intopractical chains ofus-\nage. Iknow various people areworking onit,both inthiscommunity\nandoutside it,soIhave atleast onerealistic dream. Onechallenge here\nisthatweneed aframework forcombining them, butthisframework\nmust beminimalistic. Ifthecomplexity ofunderstanding theframe-\nwork, ofadapting amodule toitistoohigh, wewilllosethebene\u0002ts\nofwide cooperation, ofexploratory usage. Another challenge isthat\n6weneed togetridofanynotion thatthealgorithms orthefunctional-\nityinthese modules could bepatented. This may beobtained inthe\nEuropean legal framework, though itisahard\u0002ght.\nConsidering thelimited resour cesthatareavailable tobuild large-\nscale MIRsystems andexperiment with them, itisnecessary tofocus\nonmusic thatbeaccessed, exchanged andre-used without high trans-\naction costs. Public domain (when thereisone), voluntarily created\nmusical commons, andmusic thatcanbereused under legal licensing\nschemes aretheonlycandidates. Andthewaytostart isbyconstruct-\ningmusic commons. Once again, therearepeople inyour community\nwho work onit,butthey arefaced with many obstacles. SoIdream\nthattherewould bemoreofacommunity-wide approach atit.\nThetragedy ofthepresent repressive approach toP2P\u0002leshar-\ningisthatitprevents itfrommaturing byputting itunder siege, and\nthen uses thisimmaturity tofurther justify warfare.Installing P2P\u0002le\nsharing asarespected social andtechnical paradigm forarchiving and\nreusing music isavery valuable aim, towhich theMIR community\ncan, ititwishes, contribute keyelements. This isclosely linked tothe\nprevious point (musical commons). Ifyoulove\u0002ngerprinting, useitto\ncontr olinjection instead ofcontr olling usage, forinstance contr olthat\nnofakes areinjected. Develop newschemes ofconnecting contextual\nmetadata with P2Paccess torawmusic (bythewaydon't forget tolook\natwhat theP2Pusers arealready doing inthatrespect). Stand forat-\ntribution, formaking available information about usage rights without\ntrying toforceormonitor their respect byindividual users bytechni-\ncalmeans. Workwith people whoconstruct P2Pservices with added-\nvalue information formedia commons, such astheRéseau National\nd'Echanges thatTariqKrim istrying tocreateinFrance.\nFinally, theretrieval paradigm however valuable itismay betoo\nnarrow.Thereismoretomusic information than retrieval, ascanbe\nseen fromthisconfer ence's topics.\n[Slide] Incase youshareanyofthese dreams, don't believe some-\nbody elsewillremove theobstacles.\nThank you\nAfter many potentially contentious statements, toinstall back some\nserenity, orto\u0002nish destr oying it,Ipropose youaveryshort passage\ninmusic befor equestions, easy listening music thatillustrates howthe\ngenrecategory issuffering abit.Thank youforyour attention.\n7"
    },
    {
        "title": "Tempo And Beat Estimation Of Musical Signals.",
        "author": [
            "Miguel A. Alonso 0002",
            "Gaël Richard",
            "Bertrand David 0002"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415784",
        "url": "https://doi.org/10.5281/zenodo.1415784",
        "ee": "https://zenodo.org/records/1415784/files/AlonsoRD04.pdf",
        "abstract": "Tempo estimation is fundamental in automatic music processing and in many multimedia applications. This paper presents an automatic tempo tracking system that processes audio recordings and determines the beats per minute and temporal beat location. The concept of spec- tral energy flux is defined and leads to an efficient note on- set detector. The algorithm involves three stages: a front- end analysis that efficiently extracts onsets, a periodicity detection block and the temporal estimation of beat loca- tions. The performance of the proposed method is evalu- ated using a large database of 489 excerpts from several musical genres. The global recognition rate is 89.7 %. Results are discussed and compared to other tempo esti- mation systems. Keywords: beat, tempo, onset detection.",
        "zenodo_id": 1415784,
        "dblp_key": "conf/ismir/AlonsoRD04",
        "keywords": [
            "automatic tempo tracking system",
            "audio recordings",
            "beats per minute",
            "temporal beat location",
            "spectral energy flux",
            "note onset detector",
            "front-end analysis",
            "periodicity detection block",
            "temporal estimation",
            "global recognition rate"
        ],
        "content": "TEMPOANDBEATESTIMATIONOFMUSICAL SIGNALS\nMiguel Alonso, Bertr andDavid ,Ga¨elRichard\nENST -GET ,D´epartement TSI\n46,rueBarrault, Paris\n75634 cedex13,France\nfmalonso,bedavid,grichard g@tsi.enst.f r\nABSTRACT\nTempo estimation isfundamental inautomatic music\nprocessing andinmanymultimedia applications. This\npaper presents anautomatic tempo tracking system that\nprocesses audio recordings anddetermines thebeats per\nminute andtemporal beat location. Theconcept ofspec-\ntralenergyﬂuxisdeﬁned andleads toanefﬁcient note on-\nsetdetector .Thealgorithm involvesthree stages: afront-\nendanalysis thatefﬁciently extracts onsets, aperiodicity\ndetection block andthetemporal estimation ofbeat loca-\ntions. Theperformance oftheproposed method isevalu-\nated using alargedatabase of489excerpts from several\nmusical genres. The global recognition rate is89.7 %.\nResults arediscussed andcompared toother tempo esti-\nmation systems.\nKeywords: beat, tempo, onset detection.\n1.INTRODUCTION\nItisverydifﬁcult tounderstand western music without\npercei ving beats, since abeat isafundamental unit of\nthetemporal structure ofmusic [4].Forthisreason, au-\ntomatic beat tracking, ortempo tracking, isanessential\ntaskformanyapplications such asmusical analysis, auto-\nmatic rhythm alignment ofmultiple musical instruments,\ncutandpaste operations inaudio editing, beat drivenspe-\ncialeffects. Although itmight appear simple atﬁrst, tempo\ntracking hasprovedtobeadifﬁcult task when dealing\nwith abroad variety ofmusical genres asshownbythe\nlargenumber ofpublications onthissubject appeared dur-\ningthelastyears [2,5,6,8,9,10,12].\nEarlier tempo tracking approaches focused onMIDI\norother symbolic formats, where note onsets arealready\navailable totheestimation algorithm. More recent ap-\nproaches directly deal with ordinary CDaudio recordings.\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forproﬁt orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation ontheﬁrstpage.\nc\r2004 Universitat Pompeu Fabra.Thesystem thatwepresent inthispaper liesintothiscat-\negory.\nFormusical genres with astraightforw ardrhythm such\nasrap, rock, reggae andothers where astrong percus-\nsivestrikedrivestherhythm, current beat track ersindi-\ncate high performance aspointed outby[5,9,12].How-\never,therobustness ofthebeat tracking systems isoften\nmuch lessguaranteed when dealing with classical music\nbecause oftheweakness ofthetechniques emplo yedin\nattack detection andtempo variations inherent tothatkind\nofmusic.\nInthepresent article, wedescribe analgorithm toesti-\nmate thetempo ofapiece ofmusic (inbeats perminute or\nbpm) andidentify thetemporal locations when itoccurs.\nLikemost ofthesystems available intheliterature, this\nalgorithm relies onaclassical scheme: afront-end proces-\nsorextracts theonset locations from atime-frequenc yor\nsubband analysis ofthesignal, traditionally using aﬁlter\nbank [1,7,10,12]orusing thediscrete Fourier transform\n[3,5,6,8,9].Then, aperiodicity estimation algorithm\nﬁnds therateatwhich these events occur .Alargevariety\nofmethods hasbeen used forthispurpose, forexample, a\nbank ofoscillators which resonate atintegermultiples of\ntheir characteristic frequenc y[6,9,12],pitch detection\nmethods [1,10],histograms oftheinter-onset interv als\n[2,13],probabilistic approaches such asGaussian mix-\nturemodel toexpress thelikelihood oftheonset locations\n[8].\nInthispaper ,follo wing Laroche’ sapproach [9],we\ndeﬁne thequantity so-called spectral energyﬂux asthe\nderivativeofthesignal frequenc ycontent with respect to\ntime. Although thisprinciple hasbeen previously used\n[3,6,8,9],asigniﬁcant impro vement hasbeen obtained\nbyusing anoptimal ﬁlter toapproximate thisderivative.\nWeexploit thisapproach toobtain ahigh performance\nonset detector andintegrate itintoatempo tracking algo-\nrithm. Wedemonstrate theusefulness ofthisapproach by\nvalidating theproposed system using alargemanually an-\nnotated data base thatcontains excerpts from rock, latin,\npop, soul, classical, rap/hip-hop andothers. Thepaper is\norganized asfollo ws: Section 2provides adetailed de-\nscription ofthethree main stages thatcompose thesys-\ntem. InSection 3,testresults areprovided andcompared\ntoother methods. Thesystem parameters used during thePeriodicity\nestimationBeat\nlocationSynchronised\nbeat signalAudio Signal\nOnset detection\n− Spectral analysis (STFT)\n− Spectral energy flux functionDetection\nFigure1.Architecture ofthebeat tracking algorithm.\nvalidation procedure areprovided aswell ascomments\nabout theissues ofthealgorithm. Finally ,Section 4sum-\nmarizes theachie vements ofthepresented algorithm and\ndiscusses possible directions forfurther impro vements.\n2.DESCRIPTION OFTHEALGORITHM\nInthispaper ,itisassumed thatthetempo oftheaudio sig-\nnalisconstant overtheduration oftheanalysis windo w\nandthatiteventually evolvesslowlyfrom onetotheother .\nInaddition, wesuppose that thetempo liesbetween 60\nand200BPM, without lossofgenerality since anyother\nvalue canbemapped intothisrange. Thealgorithm pro-\nposed iscomposed ofthree major steps (seeﬁgure 1):\n\u000fonset detection :itconsists incomputing adetection\nfunction based onthespectral energyﬂuxofthein-\nputaudio signal;\n\u000fperiodicity estimation :theperiodicity ofthede-\ntection function isestimated using pitch detection\ntechniques ;\n\u000fbeat location estimation :theposition ofthecorre-\nsponding beats isobtained from thecross-correlation\nbetween thedetection function andanartiﬁcial pulse-\ntrain.\n2.1.Onsetdetection\nTheaimofonset detection consists inextracting adetec-\ntion function that will indicate thelocation ofthemost\nsalient features oftheaudio signal such asnote changes,\nharmonic changes andpercussi veevents. Asamatter of\nfact,these eventsareparticularly important inthebeatper-\nception process.\nNote onsets areeasily mask edintheoverall signal en-\nergybycontinuous tones ofhigher amplitude [9],while\ntheyaremore likelydetected after separating them infre-\nquenc ychannels. Wepropose tofollo wafrequenc ydo-\nmain approach [3,5,6,8,9]asitprovestooutperform\ntime-domain methods based ondirect processing ofthe\ntemporal waveform asawhole.2.1.1. Spectr alanalysis andspectr alenergyﬂux\nTheinput audio signal isanalyzed using adecimated ver-\nsion oftheshort-time Fourier transform (STFT), i.e.,short\nsignal segments areextracted atregular time interv als,mul-\ntiplied byananalysis windo wandtransformed into the\nfrequenc ydomain bymeans ofaFourier transform. This\nleads to\n˜X(f;m)=N\u00001\n∑\nn=0w(n)x(n+mM)e\u0000j2πfn(1)\nwhere x(n)denotes theaudio signal, w(n)theﬁnite anal-\nysiswindo wofsizeNinsamples, Mthehopsizeinsam-\nples, mtheframe indexandfthefrequenc y.\nMoti vated bytheworkofLaroche [9],wedeﬁne the\nspectral energyﬂux E(f;k)asanapproximation tothe\nderivativeofthesignal frequenc ycontent with respect to\ntime\nE(f;k)=∑\nmh(m\u0000k)G(f;m) (2)\nwhere h(m)approximates adifferentiator ﬁlter with:\nH(ej2πf)'j2πf (3)\nandthetransformation\nG(f;m)=Ffj˜X(f;m)jg (4)\nisobtained viaatwostep process: alow-pass ﬁltering of\nj˜X(f;m)jtoperform energyintegration inawaysimilar to\nthatintheauditory system, emphasizing themost recent\ninputs, butmasking rapid modulations [14]andanon-\nlinear compr ession .Forexample, in[9]Laroche proposes\nh(m)asaﬁrstorder differentiator ﬁlter (h=[1;\u00001]),no\nlow-pass ﬁltering isapplied andthenon-linear compres-\nsion function isG(f;n)=arcsinh (j˜X(f;m)j).In[6]Kla-\npuri uses thesame ﬁrst order differentiator ﬁlter,butfor\nthetransformation, heperforms thelow-pass ﬁltering af-\nterapplying alogarithmic compression function.\nInthepresent workwepropose h(m)tobeaFIRﬁlter\ndifferentiator .Such aﬁlter isdesigned byaRemez optimi-\nsation procedure which leads tothebestapproximation to\nEq. (3)intheminimax sense [11].This newapproach,\ncompared totheﬁrst order difference used in[6,8,9]\nhighly impro vestheextraction ofmusical meaningful fea-\ntures such aspercussi veattacks andchord changes. In\naddition, G(f;k)isobtained vialow-pass ﬁltering with\nasecond half ofaHanning windo wfollo wed byaloga-\nrithmic compression function assuggested byKlapuri [7],\nsince thelogarithmic difference function givestheamount\nofchange inasignal inrelation toitsabsolute level.This\nisapsycho-acoustic relevantmeasure since thepercei ved\nsignal amplitude isinrelation toitslevel,thesame amount\nofincrease being more prominent inaquite signal [7].\nDuring thesystem development, severalorders forthe\ndifferentiator ﬁlter h(m)were tested. Wefound thatusing\nanorder 8ﬁlter wasthebest tradeof fbetween comple x-\nityandefﬁcienc y.Inpractice, thealgorithm uses anNpoint FFT toevaluate theSTFT ,thus thefrequenc ychan-\nnels 1toN\n2ofthesignal’ stime–frequenc yrepresentation\nareﬁltered using h(m)toobtain thespectral energyﬂux.\nThen, allthepositi vecontrib utions ofthese channels are\nsummed toproduce atemporal waveform v(k)that ex-\nhibits sharp maxima attransients andnote onsets, i.e.,\nthose instants where theenergyﬂuxislarge.\nBeat tends tooccur atnote onsets, sowemust ﬁrstdis-\ntinguish the”true beat” peaks from thespurious ones in\nv(k)toobtain aproper detection function p(k).Inaddi-\ntion, weworkunder thesupposition thatthese unwanted\npeaks aremuch smaller inamplitude compared tothenote\nattack peaks. Thus, apeak-picking algorithm thatselects\npeaks aboveadynamic threshold calculated with thehelp\nofamedian ﬁlter isasimple andefﬁcient solution tothis\nproblem. The median ﬁlter isanonlinear technique that\ncomputes thepointwise median inside awindo woflength\n2i+1formed byasubset ofv(k),thus themedian thresh-\noldcurveisgivenbytheexpression:\nθ(k)=C\u0001median (gk) (5)\nwhere gk=fvk\u0000i;:::;vk;:::;vk+igandCisaprede-\nﬁned scaling factor toartiﬁcially risethethreshold curve\nslightly abovethesteady state levelofthesignal. Toen-\nsure accurate detection, thelength ofthemedian ﬁlter\nmust belonger than theaverage width ofthepeaks of\nthedetection function. Inpractice, wesetthemedian ﬁl-\nterlength to200ms. Then, weobtain thesignal ˆp(k)=\nv(k)\u0000θ(k),which ishalf-w averectiﬁed toproduce the\ndetection function p(k):\np(k)=\u001a\nˆp(k)ifˆp(k)>0\n0 otherwise(6)\nInourtests, theonset detector described abovehaspro-\nvedtobearobustscheme thatprovides good results fora\nwide range ofmusical instruments andattacks atarela-\ntivelylowcomputational cost. Forexample, Figure 2-a\nshowsthetime waveform ofapiano recording containing\nsevenattacks. These attacks canbeeasily observ edinthe\nsignal’ sspectrogram inFigure 2-b.Thephysical interpre-\ntation ofFigure 2-ccanbeseen astherateatwhich the\nfrequenc y-content energyoftheaudio signal varies ata\ngiventime instant, i.e.,thespectral energyﬂux. Inthisex-\nample, sevenvertical stripes represent thereinforcement\noftheenergyvariation, clearly indicating thelocation of\ntheattacks (theposition ofthespectrogram edges). When\nallthepositi veenergyvariations aresummed inthefre-\nquenc ydomain andthresholded, weobtain thedetection\nfunction p(k)showninFigure 2-d. Anexample ofanin-\nstrument with smooth attacks, aviolin, isshowninFigure\n3.Largeenergyvariations inthefrequenc ycontent ofthe\naudio signal canstillbeobserv edasvertical stripes inFig-\nure3-c. After summing thepositi vecontrib utions, sixof\nthesevenattacks areproperly detected asshownbythe\ncorresponding largestpeaks inFigure 3-d.\nPSfrag replacements\na)amplitude b)frequenc y(kHz) c)frequenc y(kHz)\ntime (s)d)magnitude\n0 1 2 3 40 1 2 3 40 1 2 3 40 1 2 3 4\n00.510101-101\nFigure2.From toptobottom: time waveform ofapiano\nsignal, itsspectrogram, itsspectral energyﬂuxandthede-\ntection function p(k).\n2.2.Periodicity estimation\nThedetection function p(k)attheoutput oftheonset de-\ntection stage canbeseen asaquasi-periodic andnoisy\npulse-train thatexhibits largepeaks atnote attacks. The\nnextstep istoestimate thetempo oftheaudio signal, i.e.,\ntheperiodicity ofthenote onset pulses. Twomethods\nfrom traditional pitch determination techniques areem-\nployed: thespectral product andtheautocorrelation func-\ntion. These techniques havealready been used forthis\npurpose in[1].\n2.2.1. Spectr alproduct\nThespectral product principle assumes thatthepowerspec-\ntrum ofthesignal isformed from strong harmonics lo-\ncated atintegermultiples ofthesignal’ sfundamental fre-\nquenc y.Toﬁnd thisfrequenc y,thepower spectrum is\ncompressed byafactor m,then theobtained spectra are\nmultiplied, leading toareinforced fundamental frequenc y.\nForanormalized frequenc y,thisisgivenby:\nS(ej2πf)=M\n∏\nm=1jP(ej2πmf)jforf<1\n2M(7)\nwhere P(ej2πf)isthediscrete Fourier transform ofp(k).\nThen, theestimated tempo Tiseasily obtained bypicking\noutthefrequenc yindexcorresponding tothelargestpeak\nofS(ej2πf).Thetempo isconstrained tofallintherange\n60<T<200BPM.PSfrag replacements\na)amplitude b)frequenc y(kHz) c)frequenc y(kHz)\ntime (s)d)magnitude\n0 1 2 3 4 50 1 2 3 40 1 2 3 4 50 1 2 3 4 5\n00.51012012-101\nFigure3.From toptobottom: time waveform ofavio-\nlinsignal, itsspectrogram, itsspectral energyﬂuxandthe\ndetection function p(k).\n2.2.2. Autocorr elation function\nThis isaclassical method inperiodicity estimation. The\nnon-normalized deterministic autocorrelation function of\np(k)iscalculated asfollo ws:\nr(τ)=∑\nkp(k+τ)p(k) (8)\nAgain, wesuppose that60<T<200BPM. Hence, dur-\ningthecalculation oftheautocorrelation, only thevalues\nofr(τ)corresponding totherange of300msto1sare\ncalculated. Toﬁndtheestimated tempo T,thelagofthe\nthree largestpeaks ofr(τ)areanalyzed andamultiplicity\nrelationship between them issearched. Inthecase thatno\nrelation isfound, thelagofthelargestpeak istakenasthe\nbeat period.\n2.3.Beatlocation\nToﬁnd thebeat location, weuseamethod based onthe\ncomb ﬁlter idea thatresembles previous workcarried out\nby[6,9,12].Wecreate anartiﬁcial pulse-train q(t)of\ntempo Tpreviously calculated asexplained inSection 2.2\nandcross-correlate itwith p(k).This operation hasalow\ncomputational cost, since thecorrelation isevaluated only\nattheindices corresponding tothemaxima ofp(k).Then,\nwecallt0thetime indexwhere thiscross-correlation is\nmaximal andweconsider itasthestarting location ofthe\nbeat. Forthesecond andsuccesi vebeats intheanalysis\nwindo w,abeat period Tisadded totheprevious beatGenre Pieces Percentage\nclassical 137 28.0%\njazz 79 16.2%\nlatin 37 7.6%\npop 40 8.2%\nrock 44 9.0%\nreggae 30 6.1%\nsoul 24 4.9%\nrap,hip-hop 20 4.1%\ntechno 23 4.7%\nother 55 11.2%\ntotal 489 100%\nTable1.Genre distrib ution ofthetestdatabase.\nlocation, i.e.,ti=bti\u00001+Tcandacorresponding peak in\np(k)issearched within theareati\u0006Δ.Ifnopeak isfound,\nthebeat isplaced initsexpected position ti.When thelast\nbeat ofthewindo woccurs, itslocation isstored inorder\ntoassure thecontinuity with theﬁrstbeat ofthenewanal-\nysiswindo w.Where thetempo ofthenewanalysis win-\ndowdiffersbymore than 10%from theprevious tempo, a\nnewbeat phase isestimated. Thepeaks aresearched using\nthenewbeat period without referencing theprevious beat\nphase.\n3.PERFORMANCE ANALYSIS\n3.1.Database, annotation andevaluationprotocole\nTheproposed algorithm wasevaluated using acorpus of\n489musical excerpts takenfrom commercial CDrecord-\nings. These pieces were selected tocoverasmanychar-\nacteristics aspossible: various tempi inthe50to200\nBPM range, awide variety ofinstruments, dynamic range,\nstudio/li verecordings, old/recent recordings, with/without\nvocals, male/female vocals andwith/without percussions.\nTheywere also selected torepresent awide diversity of\nmusical genres asshowninTable 1.\nFrom each oftheselected recordings, anexcerpt of20\nseconds having arelati velyconstant tempo, wasextracted\nandconverted toamonophonic signal sampled at16kHz.\nTheprocedure formanually estimating thetempo ofeach\nmusical piece isthefollo wing:\n\u000fthemusician listens toamusical excerpt using head-\nphones (ifrequired, severaltimes inarowtobeac-\ncustomed tothetempo),\n\u000fwhile listening, he/she tapsthetempo,\n\u000fthetapping signal isrecorded andthetempo isex-\ntracted from it.\nAspointed outbyGoto in[4],thebeat isaperceptual\nconcept thatpeople feelinmusic, soitisgenerally difﬁ-\nculttodeﬁne the”correct beat” inanobjecti veway.Peo-\nplehaveatendenc ytotapatdifferent metric levels.ForMethod Recognition\nrate\nPaulus [10] 56.3 %\nScheirer [12] 67.4 %\nSP. 63.2 %\nAC. 73.6 %\nSPusing SEF. 84.0 %\nACusing SEF 89.7 %\nTable2.Tempo estimation performances. SEF stands for\nspectral energyﬂux, SPforspectral product andACfor\nautocorrelation.\nexample, inapiece thathasa4/4time signature, itiscor-\nrecttotapeveryquarter -note oreveryhalf-note. Ingen-\neral, a”ground truth” tempo cannot beestablished unless\nthemusical score ofthepiece isavailable. This isavery\ncommon problem when humans tapalong with themu-\nsic,i.e.,totaptwice asfastortwice asslowthe”true”\ntempo. Whene verthiscase ocurred during thedatabase\nannotation, theslowertempo wastakenasreference TR.\nInasimilar waytohumans, automatic tempo estimation\nmethods also makethisdoubling orhalving ofthe”true”\ntempo. Thus, forevaluation purposes thetempo estima-\ntionTprovided bythealgorithm islabeled ascorrect if\nthere isalessthan 5%disagreement from themanually\nannotated tempo used asreference TRunder theprinciple\n0:95αT<TR<1:05αTwithα2f1\n2;1;2g.\n3.2.Results\nDuring theevaluation, thealgorithm parameters were set\nasfollo ws.Thelength oftheanalysis windo wfortempo\nestimation wassettofour seconds, with anoverlapping\nfactor of50%. Smaller windo wsize values reduce the\nalgorithm performance. Forthespectral energyﬂuxcal-\nculation, thelength oftheanalysis windo wused inthe\ncomputation oftheSTFT was64samples (4ms)with an\noverlapping factor of50% anda128point FFT,thus the\ndetection function v(k)could beseen assignal sampled at\n500Hz.Asmentioned, theorder ofthedifferentiator FIR\nﬁlter wassettoL=8.Inthebeat location stage, theme-\ndian ﬁlter iwassetto25samples, Cwassetto2,andfor\nthepeak location Δwassetto10%ofthebeat period.\nTohaveabetter idea oftheperformance ofoural-\ngorithm, wedecided tocompare itwith ourownimple-\nmententation ofthealgorithms proposed byPaulus [10]\nandScheirer [12].Wealso compared itwith ourprevious\nworkintempo estimation [1].Inthiscase, themain differ-\nence between theprevious andthecurrent system lieson\ntheonset extraction stage. Table 2summarizes theoverall\nrecognition ratefortheevaluated systems. Inthistable,\nSPstands forspectral product, ACforautocorrelation and\nSEF forspectral energyﬂux.\nInmore details, theperformance ofthese methods by\nmusical genre arepresented inTable 3.Inthistable, PLS\nstands forPaulus, SCR forScheirer .Asexpected, resultsMethod PLSSCR SPACSP-SEF AC-SEF\nGenre %%%% % %\nclassical 46.046.248.270.8 71.5 82.4\njazz 57.070.962.069.8 78.4 86.0\nlatin 70.381.162.170.3 91.8 94.5\npop 57.570.075.085.7 92.5 92.5\nrock 40.984.161.384.4 81.8 88.6\nreggae 76.786.786.676.9 96.6 100\nsoul 50.087.570.876.7 100 100\nrap 75.085.075.056.5 100 100\ntechno 69.656.365.295.0 95.6 100\nother 61.869.174.566.7 89.0 90.9\nTable3.Tempo estimation performances bymusical\ngenre. PLS stands forPaulus [10],SCR forScheirer [12].\nindicate that classical music isthemost difﬁcult genre.\nNevertheless, theproposed algorithm displayed promising\nresults. Fortheother genres, itshowsgood performance,\nparticularly formusic with astraightforw ardrhythm.\nSeveralauthors havepointed outthedifﬁculty ineval-\nuating beat tracking systems [4,6,9]duetothesubjecti ve\ninterpretation ofthebeat andtheinexistence ofaconsen-\nsual data base ofbeat-labeled audio tracks. Inourcase,\nthebeat location evaluation wasdone atasubjecti velevel,\nthatis,artiﬁcial ”sound clicks” were superimposed onthe\ntested signal atthecalculated beat locations andtempo.\nDuring thevalidation procedure, wenote thatthepro-\nposed algorithm produces erroneous results under thefol-\nlowing circumstances:\n\u000fwhen dealing with signals having astealthily orlong\nfading-in attacks, thehypothesis thatsupurious peaks\naresmaller than attack peaks does nothold anymore,\nleading tofalseonset detections;\n\u000fthespectral energyﬂux follo wstheprinciple that\nstable spectra regions arefollo wed bytransition re-\ngions. When manyinstruments play simultaneously ,\nasinanorchestra, their ’spectral mixture’ lacks sta-\nbleregions, leading tofalseonset detections;\n\u000fwhen thetempo varies tooquickly inshort time seg-\nments orifthere arelargebeatgaps inthesignal, the\nperiocity estimation stage cannot keepupwith the\nchanges.\nThereader iswelcome tolisten tothesound examples\navailable atwww.tsi.enst.fr/ \u0018malonso/ismir04.\n4.CONCLUSIONS\nInthispaper wehavepresented anefﬁcient beat track-\ningalgorithm thatprocesses audio recordings. Wehave\nalso deﬁned theconcept ofspectral energyﬂuxandused\nittoderiveanewandeffectiveonset detector based onthe\nSTFT ,anefﬁcient differentiator ﬁlter anddynamic thresh-\nolding using amedian ﬁlter.This onset detector displays\nhigh performance foralargerange ofaudio signals. In\naddition, theproposed tempo tracking system isstraight-\nforw ardtoimplement andhasarelati velylowcomputa-\ntional cost. The performance ofthealgorithm presentedwasevaluated onalargedatabase containing 489musi-\ncalexcerpts from severalmusical genres. Theresults are\nencouraging since theglobal success rate fortempo es-\ntimation was89.7%. The method presented works off-\nline. Areal-time implementation isconsidered, butcur-\nrently there arevarious issues toberesolv edsuch asthe\nblock-wise processing thatrequires access tofuture sig-\nnalsamples andthenon-causality ofthethesholding ﬁl-\nter.Future workshould explore other periodicity estima-\ntiontechniques andananalysis oftheresidual partafter a\nharmonic/noise decomposition.\n5.REFERENCES\n[1]Alonso M., DavidB.and Richard G.,”A\nStudy ofTempo Tracking Algorithms from\nPolyphonic Music Signals”, Proceedings of\nthe 4th. COST 276 Workshop ,Bordeaux,\nFrance. March 2003.\n[2]Dixon, M.”Automatic Extraction ofTempo\nand Beat from Expressi vePerformances”,\nJournal ofNewMusic Resear ch,vol.30No.1,\npp.39–58 2001.\n[3]Duxb uryC.,Sandler M.andDavies M.,”A\nHybrid Approach toMusical Note Onset De-\ntection”, Proceedings ofthe5th. Int.Conf .\nonDigital Audio Effects (DAFx) ,pp.33–38,\nHamb urgGerman y,September 2002.\n[4]Goto M.andMuraoka Y.”Issues inEvaluating\nBeat Tracking Systems”, Working Notes ofthe\nIJCAI-97 Workshop onIssues inAIandMusic ,\nAugust 1997.\n[5]Goto, M.”An Audio-based Real-time Beat\nTracking System forMusic WithorWith-\noutDrum-sounds”, Journal ofNewMusic Re-\nsearch,vol.30,No.2,pp.159–171, 2001.\n[6]Klapuri, A.”Musical meter estimation and\nmusic transcription”, Cambridg eMusic Pro-\ncessing Colloquium ,Cambridge University ,\nUK, March 2003.\n[7]Klapuri, A.”Sound Onset Detection byAp-\nplying Psychoacoustic Knowledge”, Proceed-\nings IEEE Int.Conf .Acoustics Speec handSig.\nProc.(ICASSP) ,pp.3089–3092, Phoenix AR,\nUSA March 1999.\n[8]Laroche, J.”Estimating, Tempo, Swing and\nBeat Locations inAudio Recordings”, Pro-\nceedings IEEE Workshop onApplications of\nSignal Processing toAudio and Acoustics\n(WASPAA),pp.135–138, NewPaltz, NY,USA\nOctober 2001.\n[9]Laroche, J.”Efﬁcient Tempo andBeat Track-\ninginAudio Recordings”, J.Audio. Eng.Soc.,\nvol.51,No.4,pp.226–233, April 2003.[10] Paulus J.andKlapuri A.,”Measuring theSim-\nilarity ofRythmic Patterns”, Proceedings of\ntheInternational Symposium onMusic Infor -\nmation Retrie val,Paris, France, 2002.\n[11] Proakis, J.G.andManolakis, D.,Digital Sig-\nnalProcessing: Principles, Algorithms and\nApplications .Prentice Hall, NewYork,1995.\n[12] Scheirer ,E.D.,”Tempo andBeat Analysis of\nAcoustic Music Signals”, J.Acoust. Soc. Am.,\nvol.104, pp.588–601, January 1998.\n[13] Seppa anen, J.”Tatum gridanalysis ofmusical\nsignals”, Proceedings IEEE Workshop onAp-\nplications ofSignal Processing toAudio and\nAcoustics (WASPAA),NewPaltz, NY,USA\nOctober 2001.\n[14] Todd, N.P.McA., ”The Auditory ’Primal\nSketch’: AMultiscale model ofrhythmic\ngrouping”, Journal ofNewMusic Resear ch,\nvol.23,No.1,pp.25–70,1994."
    },
    {
        "title": "Tools and Architecture for the Evaluation of Similarity Measures : Case Study of Timbre Similarity.",
        "author": [
            "Jean-Julien Aucouturier",
            "François Pachet"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416562",
        "url": "https://doi.org/10.5281/zenodo.1416562",
        "ee": "https://zenodo.org/records/1416562/files/AucouturierP04.pdf",
        "abstract": "The systematic testing of the very many parameters and algorithmic variants involved in the design of high-level music descriptors at large, and similarity measure in par- ticular, is a daunting task, which requires the building of a general architecture which is nearly as complex as a full- fledge Music Browsing system. In this paper, we report on experiments done in an attempt to improve the perfor- mance of the music similarity measure described in [2], using the Cuidado Music Browser ([8]). We do not prin- cipally report on the actual results of the evaluation, but rather on the methodology and the various tools that were built to support such a task. We show that many non- technical browsing features are useful at various stages of the evaluation process, and in turn that some of the tools developed for the expert user can be reinjected into the Music Browser, and benefit the non-technical user.",
        "zenodo_id": 1416562,
        "dblp_key": "conf/ismir/AucouturierP04",
        "keywords": [
            "systematic testing",
            "high-level music descriptors",
            "algorithmic variants",
            "Music Browsing system",
            "improving performance",
            "Music Browser",
            "evaluation process",
            "non-technical browsing features",
            "expert user tools",
            "reinjected into Music Browser"
        ],
        "content": "TOOLSAND ARCHITECTURE FORTHE EVALUATIONOF\nSIMILARITYMEASURES : CASE STUDY OF TIMBRESIMILARITY\nJean-JulienAucouturier\nSONY CSL Paris\n6,rueAmyot\n75005Paris, France.FrancoisPachet\nSONY CSL Paris\n6, rueAmyot\n75005Paris,France.\nABSTRACT\nThe systematic testing of the very many parameters and\nalgorithmic variants involved in the design of high-level\nmusic descriptors at large, and similarity measure in par-\nticular,isadauntingtask,whichrequiresthebuildingofa\ngeneral architecture which is nearly as complex as a full-\nﬂedge Music Browsing system. In this paper, we report\non experimentsdone in an attempt to improvethe perfor-\nmance of the music similarity measure described in [2],\nusing the Cuidado Music Browser ([8]). We do not prin-\ncipally report on the actual results of the evaluation, but\nratheronthemethodologyandthe varioustoolsthatwere\nbuilt to support such a task. We show that many non-\ntechnicalbrowsingfeaturesareusefulat variousstagesof\nthe evaluation process, and in turn that some of the tools\ndeveloped for the expert user can be reinjected into the\nMusicBrowser,andbeneﬁtthenon-technicaluser.\n1. INTRODUCTION\nThe domain of Electronic Music Distribution has gained\nworldwideattentionrecentlywithprogressinmiddleware,\nnetworking and compression. However, its success de-\npendslargelyon the existence of robust, perceptuallyrel-\nevant music similarity relations. It is only with efﬁcient\ncontent management techniques that the millions of mu-\nsictitlesproducedbyoursocietycanbemadeavailableto\nitsmillionsofusers.\n1.1. Case study: TimbreSimilarity\nIn[2],we haveproposedto computeautomaticallymusic\nsimilaritiesbetweenmusictitlesbasedontheirglobaltim -\nbre quality. Typical examples of timbre similarity as we\ndeﬁneitare :\n•a Schumann sonata (“Classical”) and a Bill Evans\npiece (“Jazz”) are similar because theyboth are ro-\nmanticpianopieces,\nPermission tomakedigital orhard copies ofallorpartofthi swork for\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributed forproﬁtorcommercialadvantag e andthat\ncopies bear this notice and the full citation on the ﬁrstpage .\nc/circlecopyrt2004 Universitat Pompeu Fabra.•ANickDraketune(“Folk”),anacoustictunebythe\nSmashing Pumpkins (“Rock”), a bossa nova piece\nbyJoaoGilberto(“World”)aresimilarbecausethey\nall consist of a simple acoustic guitar and a gentle\nmalevoice,etc.\nTimbreSimilarityhasseenagrowinginterestintheMusic\nInformationRetrieval communitylately (see [3, 4, 7, 11],\nand[1] fora completereview). Eachcontributionoftenis\nyet another instantiation of the same basic pattern recog-\nnition architecture, only with different algorithm varian ts\nand parameters. The signal is cut into short overlapping\nframes (usually between 20 and 50ms and a 50% over-\nlap), and for each frame, a feature vector is computed,\nwhich usually consists of Mel Frequency cepstrum Co-\nefﬁcients (MFCC). The number of MFCCs is an impor-\ntant parameter,andeach authorcomesup with a different\nnumber. Then a statistical model of the MFCCs’ distri-\nbution is computed, e.g. K-means or Gaussian Mixture\nModels (GMMs). Once again, the number of kmean or\nGMMcentresisadiscussedparameterwhichhasreceived\na vast number of answers in the litterature. Finally, mod-\nels are compared with different techniques, e.g. Monte\nCarlo sampling, Earth Mover’s distance or Asymptotic\nLikelihood Approximation. All these contributions give\nencouragingresultswithalittleeffortandimplythatnear -\nperfect results would just extrapolate by ﬁne-tuning the\nalgorithms’ parameters. However, such extensive testing\nover large, dependent parameter spaces is both difﬁcult\nandcostly.\n1.2. Evaluation\nThe algorithmused fortimbresimilarity comeswith very\nmany variants, and has very many parameters to select.\nThe parameter space for the original algorithm is at least\n6-dimensional: samplerate,numberofMFCCs(N),num-\nber of components (M), distance sample rate (for Monte\nCarlo), alternative distance (EMD, etc.), window size.\nMoreover,some of these parametersare not independent,\ne.g. there is an optimal balanceto be foundbetweenhigh\ndimensionality (N) and high precision of the modeling\n(M).Additionally,theoriginalalgorithmmaybemodiﬁed\nby a number of classical pre/postprocessing, such as ap-\npending delta coefﬁcients or 0th coefﬁcient to the MFCC\nset. Finally, one would also like to test a number of vari-ants,suchasLPCorSpectralContrastinsteadofMFCCs,\nHMMsorSVMsinsteadofGMMs, etc.\nAt the time of [2], the systematic evaluation of the al-\ngorithmwassounpracticalthatthe chosenparametersre-\nsulted from hand-made parameter twitching. In more re-\ncent contributions, such as [3, 11], our measure is com-\npared to other techniques, with similarly ﬁxed parame-\nters that also result from little if any systematic evalu-\nation. More generally, attempts at evaluating different\nmeasuresintheliteraturetendtocompareindividualcon-\ntributions to one another, i.e. particular, discrete choic es\nof parameters, instead of directly testing the inﬂuence of\nthe actual parameters. For instance, [11, 3] compares the\nsettings in [7](19 MFCCs+16Kmeans) to those of [2](8\nMFCCs+3GMM).\nIn [4], Berenzweig et al. describe their experimentsat\na large-scalecomparisonof timbre andcultural similarity\nmeasures, on a large database on songs (8772). The ex-\nperiments notably focus on the gathering of ground truth\ndata for such a large quantity of material. Several au-\nthorshavestudiedtheproblemofchoosinganappropriate\ngroundtruth: [7]considersasagoodmatchasongwhich\nis from the “same album”, “same artist”, “same genre”\nas the seed song. [11] also proposes to use “styles” (e.g.\nThirdWave skarevival)and“tones”(e.g. energetic)cate-\ngories from the All Music Guide AMG1. [4] pushes the\nquestforgroundtruthonestep furtherbyminingthe web\ntocollecthumansimilarityratings. Ontheotherhand,the\nactualnumberofalgorithmicvariantstestedin[4]remains\nsmall, mainlythe dimensionof the statistical model, with\nﬁxedMFCC number,framerate,etc.\nOneofthemainobstaclestoconductingsuchasystem-\natic evaluation is that it requires to build a general archi-\ntecturethatisableto:\n•access and manage the collection of music signals\nthe measuresshouldbetestedon\n•storeeachresultforeachsong(orrathereachduplet\nof songs as we are dealing with a binary operation\ndist(a, b) =dandeachset ofparameters\n•compareresultstoagroundtruth,whichshouldalso\nbe stored\n•build or import this ground truth on the collection\nofsongsaccordingto somecriteria\n•easily specify the computation of different mea-\nsures, and to specify different parameters for each\nalgorithmvariant\n•easily manipulate the resulting similarity matrices,\nso theycanbe comparedandanalysed\nIn the contextof the EuropeanprojectCuidado,which\nended in January 2004, the music team at SONY CSL\nParis has built a fully-ﬂedged EMD system, the Music\nBrowser ([8]), which is to our knowledgethe ﬁrst system\nabletohandlethewholechainofEMDfrommetadataex-\ntractiontoexploitationbyqueries,playlists,etc. Using the\n1www.allmusic.comMusic Browser (MB), we were able to easily specify and\nlaunchalargenumberofexperimentsinanattempttoim-\nprovetheperformanceoftheclassofalgorithmsdescribed\nabove. All theneededoperationsweredonedirectlyfrom\ntheGUI,withoutrequiringanyadditionalprogrammingor\nexternal programto bookkeepthe computationsand their\nresults.\nThis paper does not focus on the actual results of the\nevaluation(althoughwereportona subsetoftheseexper-\niments in section 3.2). A complete account of the results\nof the evaluationcan be found in [1]. Here, we rather fo-\ncus on the methodology and the various tools that were\nbuilt to support such a task. Notably, we discuss how ad-\nvanced evaluation features that proved useful for the ex-\npert user may be reinjected into the MB, and beneﬁt the\nnon-technicaluser.\n2. USINGTHE MUSIC BROWSERAS AN\nEVALUATION TOOL\nFollowing our experiments with building the MB and\nother content-based music systems, we have started de-\nveloping a general JAVA API, the so-called MCM (Mul-\ntimedia Content Management), on which the current im-\nplementationofthe MBrelies.\n2.1. Building atestdatabase\nThanks to MCM, one can use all the browsing function-\nalities of the MB (editorial metadata, signal descriptors,\nautomatic playlist generation, and even other similarity\nfunctions) to select the items which will populate the test\ndatabase.\nFor this study, we have constructed a test database of\n350 song items. In order to use the “same artist” ground\ntruth,weselect clustersofsongsbythesameartist. How-\never, we reﬁne this ground truth by hand, using the MB\nquerypaneltohelpusselect setsof songswhichsatisfy 3\nadditionalcriteria.\nFirst, clusters are chosen so they are as distant as pos-\nsible from one another. This is realized e.g. by using the\nMBtoselectartistswhichdonothavethesamegenreand\ninstrumentationmetadata. For instance, “Beethoven”and\n“Ray Charles” were selected because although they both\nhave the same value for their “instrument”Field (i.e. “pi-\nano”),buttheyhavedistinctvaluesfortheir“genre”Field\n(“jazz”adn“classical”).\nSecond, artists and songs are chosen in order to have\nclusters that are “timbrally” consistent (all songs in each\ncluster sound the same). This is realized by selecting all\nsongs by the chosen artist, and ﬁltering this result set by\nthe available signal descriptors in the MB (subjective en-\nergy, bpm, voice presence, etc.), and by the relevant edi-\ntorialmetadata(e.g. year,album).\nFinally, we only select songs that are timbrally homo-\ngeneous, i.e. there is no big texture change within each\nsong. Thisistoaccountforthefact thatweonlycompute\nandcompareonetimbremodelpersong,which“merges”all the textures found in the sound. The homogeneity of\nthe songs can be assessed with the MB, which is able to\nmeasuresomestatistics onthemetadataofa set ofsongs.\nOncethesongsareselected,theMBoffersadministra-\ntive functions to create a new database (e.g. a separated\ntest database) and add the current result set to this new\ndatabase. This also copies the associated metadata of the\nitems(i.e. theFieldsandtheirvalues),whichisneededto\nautomaticallycomputethe“same artist”groundtruth.\n2.2. Generatingalgorithmicvariants\nAsdescribedinsection1.2,thealgorithmsevaluatedhere\nhave a vast number of parameters that need to be ﬁne-\ntuned. The default parameters used in [2] were based on\nintuition, previous work and limited manual testing dur-\ningthealgorithmdesign. Nofurthertestshadbeenmade,\nbecause it was difﬁcult and very-time consuming to in-\nsert new descriptors in a db, and parameterize these new\ndescriptors. Basically, the DB had to be edited manually\nwitha clientSQL administrativetool.\nThe MCM API makes the whole process of creating\nnew descriptors(orFields) a lot easier. Each Field comes\nwith a number of properties, stored in the db, which can\nbeeditedtocreateanywishednumberofalgorithmicvari-\nants. The Field properties describe the executables that\nneed to be called, as well as the arguments of these exe-\ncutables.\nFigure1showsthepropertiesoftheFieldsavailablein\nthe DescriptorManagerpanelof the MB. Asan example,\nthe executable of the selected Field, mfcc d2, has 3 ar-\nguments, 20 (the number of MFCC), 50 (the number of\nGaussian Components), and 2 (the size of the delta co-\nefﬁcient window). The associated distance function also\nappears with its own parameters, here 2000 (the number\nofsampledpointsforthemonte-carlodistance).\n2.3. Computingsimilaritymatrices\nThe Fields and the corresponding distances can then\nbe computed on the test database using the Browser’s\nDescriptor Manager (Figure 2). When the method\nField.compute() is called,MCM automaticallycre-\natesthedatabasestructuresandthesimilaritycachetable s\ntoaccommodatethenewdescriptors,asspeciﬁedbytheir\nproperties,andlaunchesthecorrespondingexecutable.\n2.4. GeneratingaGroundTruth\nA groundtruthwhichwe wanttocomparethemeasureto\nis simply yet another similarity matrix. Several types of\ngroundtruthcanbe seamlesslyintegratedintheMB:\n•ground truths based on consensual, editorial meta-\ndata about songs and artists, such as artist name,\nalbum,recordingdate,etc. Suchsimilaritymatrices\ncan be computed with the descriptor manager, us-\ning a simple euclidean distance for numerical data,\nandeitherexactorapproximatestringmatchingfor\nstrings.\nFigure1. TheFieldpropertiesasshownintheDescriptor\nManager\n•groundtruths imported from other sources of simi-\nlarity,e.g. inferenceofculturalsimilaritybymining\nofco-occurrencesontheweb([4,9]).\n•ground truths based on user subjective judgments,\ngenerated by experiments. We are currently work-\ningona generalframeworktoautomaticallygener-\nate such user tests (in the form of web pages), and\ncollect the results in the form of a MCM similarity\nmatrix([10]).\nFor the present study, we generate the “same artist”\ngroundtruth on the songs items by testing for equality of\nthe“artist”Fieldineachsong. Thisgroundtruthisstored\nin theformof asimilarity matrix,whichwe will compare\nsuccessively to all the computed timbre similarity matri-\nces.\n3. COMPARINGMATRICES\n3.1. Metrics\nWehaveidentiﬁed2casesinsimilaritymatrixcomparison\n3.1.1. Comparingﬂoatingpointmatrices\nThe two matrices to compare contain ﬂoating-point sim-\nilarity or distances values. In [4], the authors propose to\nuse thetop-N ranking agreement score : the top Nhits\nfromthereferencematrixdeﬁnethegroundtruth,withex-\nponentiallydecayingweightssothatthetophithasweight\n= 1,the 2ndhithasweight αr,the3rdhasweight α2\nr,etc.Figure2. TheDescriptorManager\nThescoreforthe ithqueryisdeﬁnedby:\nSi=N/summationdisplay\nr=1αr\nrαkr\nc (1)\nwhere kris the ranking according to the candidate mea-\nsure of the rth-ranked hit under the ground truth. In [4],\ntheauthorsusethefollowingvalues N= 10,αr= 0.51/3\nandαc=α2\nr.\n3.1.2. Comparingtoaclassmatrix\nA class matrix is a binary similarity matrix (similarity is\neither 0 or 1). This is the case in the evaluation reported\nhere where songsfrom the same artist are consideredrel-\nevant to one another, and songs from different artists are\nnon relevant. This framework is very close to traditional\nIR,whereweknowthenumberofrelevantdocumentsfor\neach query. The evaluation process for such tasks has\nbeen standardized within the Text REtrieval Conference\n(TREC) [13]. Here are the ofﬁcial values printed for a\ngivenretrievaltask :\n•total number of documents over all queries : Re-\ntrieved,Relevant,Relevant& Retrieved\n•interpolated recall-precision averages, at 0.00, at\n0.10, ..., at 1.00 : Measures precision (i.e. the per-\ncentage of retrieveddocumentsthat are relevant)at\nvarious recall level (i.e. after a certain percentage\nof all the relevant docs for that query has been re-\ntrieved).\n•average precision (non interpolated) over all rele-\nvantdocuments\n•precision,at 5docs,at 10docs,...,at 1000docs\nFigure 3. The evaluation tool computes R-precision for\neachquery,andaverageR-precisionfrothewholematrix\n•R-precision:PrecisionafterRdocumentshavebeen\nretrieved, where R=number of relevant documents\nforthe givenquery.\nFigure 3 shows a screenshot of our matrix evaluation\ntool used to computethe evaluationvalues. The tool uses\nthestandardNIST evaluationpackageTREC EVAL.\n3.2. Results\nHere we report on a subset of the evaluation results ob-\ntained with the methodology and tools examined in this\npaper. A complete account and discussion of the results\ncanbefoundin[1].\n3.2.1. (N,M)exhaustivesearch\nAs a ﬁrst evaluation, we explore the space constituted by\nthefollowing2parameters:\n•NumberofMFCCs(N):ThenumberoftheMFCCs\nextractedfromeachframeofdata.\n•Number of Components(M): The number of gaus-\nsian components used in the GMM to model the\nMFCCs.\nFigure 4 shows the results of the complete exploration\nofthe(N,M)space,withNvaryingfrom10to50bysteps\nof 10 and M from 10 to 100 by steps of 10. We use the\nR-precision measure as deﬁned in section 3.1.2. We can\nsee that too many MFCCs ( N≥20) hurt the precision.\nWhen N increases, we start to take greater account of the\nspectrum’sfastvariations,whicharecorrelatedwithpitc h.\nThis creates unwanted variability in the data, as we want\nsimilar timbres with different pitch to be matched nev-\nertheless. We also notice that increasing the number of\ncomponents at ﬁxed N, and increasing N at ﬁxed M is\neventuallydetrimentalto theprecisionaswell. Thisillus -\ntrates the curse of dimensionality mentioned earlier. The\nbest precision p= 0.63is obtainedfor20MFCCs and50\ncomponents. Compared to the original values used in [2]\n(N=8, M=3), this correspondsto an improvementof over\n15%(absolute) R-precision.Figure 4. Inﬂuence of the number of MFCCs and the\nnumberofcomponents\n3.2.2. HiddenMarkovModels\nIn an attempt to model the short-term dynamics of the\ndata,wetryreplacingtheGMMsbyhiddenMarkovmod-\nels (HMMs, see [12]). In ﬁgure 5, we report experiments\nusing a single HMM per song, with a varying number of\nstates in a left-right topology. The output distribution of\neach state is a 4-component GMM (the number of com-\nponent is ﬁxed) (see [1] for more details). From ﬁgure\n5, we see that HMM modeling performs no better than\nstatic GMM modeling. The maximum R-precision of\n0.632isobtainedfor12states. Interestingly,theprecisi on\nachieved with this dynamic model with 4*12=48 gaus-\nsiancomponentsiscomparabletotheoneobtainedwitha\nstatic GMM with 50 states. This suggests that short-term\ndynamics are not a useful addition to model polyphonic\nmixtures.\nFigure5. InﬂuenceofthenumberofstatesinHMMmod-\neling\n4. ADVANCEDFEATURES\nWehaveaddedanumberofadvancedfeaturestotheeval-\nuation tool shown in Figure 3, in order to further discuss\nandanalyseevaluationresults.\n4.1. Hubs\nInterestingly,inourtestdatabase,asmallnumberofsongs\nseems to occur frequentlyas false positives. For instance\n(see [1] for more details), the song MITCHELL, Joni\n- Don Juan's Reckless Daughter occur morethan 6 times more than it should, i.e. is very close to 1\nsong out of 6 in the database (57 out of 350). Among all\nitsoccurrences,manyarelikelytobefalsepositives. This\nsuggeststhattheerrors(about35%)arenotuniformlydis-\ntributed over the whole database, but are rather due to\na very small number of “hubs” (less than 10%) which\nare close to all other songs. Using the evaluation tool,\nwe could redo the simulation without considering the 15\nbiggest hubs (i.e. comparing the similarity matrices only\nonasubsetoftheitems),whichyieldanabsoluteimprove-\nmentof5.2%of R-precision.\nThese hubs are reminiscent of a similar problem in\nSpeech Recognition, where a small fractions of speakers\n(referred to as “goats”, as opposed to sheeps) are respon-\nsible for the vast majority of errors ([6]). They are espe-\nciallyintriguingastheyusuallystandoutoftheircluster s,\ni.e. other songs of the same cluster as a hub are not usu-\nallyhubsthemselves. Afurtherstudyshouldbedonewith\na larger test database, to see if this is only a boundaryef-\nfect due to our small, speciﬁc database or a more general\npropertyofthemeasure.\n4.2. Metasimilarity\nBeingMCMObjects,similaritiescanthemselvesbecom-\nparedandcombinedin anumberofoperations:\n4.2.1. Clustering\nThe evaluation tool has the capacity to represent the var-\nious similarity functions as points in a 2-D space, using\nmultidimensional scaling (MDS, see [5]) based on their\nmutualdistances. Figure6comparessimilaritiesobtained\nwith hmm and delta coefﬁcients (another variant exam-\nined in [1]), using the Top-10 ranking agreement dis-\ntance measure. It appears that while both variants re-\nmain equally close from the ground truth as their order\nincreases, they become more and more distant to one an-\nother. In other words, low order delta and hmm similari-\ntiesarefairlyinterchangeable,whereashigherorderreal ly\ncapturedifferentaspectsofthedata,yieldingquitedisti nct\nsimilaritymatrices.\nSuch similarity at the “meta”level (similarity between\nsimilarity) may prove useful for a non expert user to\nquickly have an idea of how various criteria available for\nbrowsingdiffer.\n4.2.2. Agreementonqueries\nSimilarity measures can be further compared to ﬁnd the\nmost or least agreeing queries, i.e. the line indexesin the\nmatrices which have the smallest distance to one another.\nIf we compare a test matrix to a ground truth, we can\ntherefore estimate the domains on which the test matrix\nis the most accurate : a similarity may be good to com-\npare jazz music, while another more suitable for electro\nmusic.Figure 6 . Multidimensional scaling of the delta-\ncoefﬁcientandhmmsimilaritymatrices\n4.2.3. Agreementonduplets\nMatricescanalso be intersectedwith AND orNAND op-\nerations. Combining similarities by only keeping the re-\nsults on which they agree for each query (e.g. intersect-\ning the 10 nearest neighborsresult sets) results in a more\nconsensual similarity, with a better precision but a worse\nrecall. 2 matrices can also be inspected to ﬁnd duplets of\nitemswhicharecloseaccordingtoonesimilarity,butdis-\ntant according to the other. We have shown in [2] how\nsuch disagreement between similarities could mine rare,\ninterestingrelationsbetweenitems. Forinstance,acoust ic\ncoverversionsofsongshaveadistanttimbresimilarityto\nthe original version, however they are very close accord-\ningtoeditorialmetadata.\n5. CONCLUSION\nInthispaper,wehavereportedontoolsbuiltinanattempt\nto improve the performance of the music similarity mea-\nsure described in [2], using the Cuidado Music Browser\n([8]). We have shown that many non-technical browsing\nfeaturesarecrucialatvariousstagesoftheevaluationpro -\ncess, in ordere.g. to build a test database or to generatea\ngroundtruth. Inturn,wehaveintroducedevaluationtools\nwhich can beneﬁt the non-technical user while browsing\nformusic. Matrixdistancesmayhelpherchoosebetween\ndifferent available search criteria, while matrix interse c-\ntion can help her ﬁnd rare, interesting songs. We believe\nthat this case study shows the importance of designing\ncomplete evaluation environments for MIR researchers.\nMoreover,the convergencein needs and tools of the pro-\ncess of browsing and the process of designing browsing\ntools opens up the way for a uniﬁed language, toward\nwhichtheMCMAPI maybeaﬁrst step.\n6. REFERENCES\n[1] J.-J. Aucouturier and F. Pachet. Improving timbre\nsimilarity: Howhigh’sthesky? JournalofNegative\nResultsin SpeechandAudioSciences ,1(1),2004.[2] J.-J. Aucouturier and F. Pachet. Music similarity\nmeasures: What’s the use ? In Proceedings of the\n3rd International Conference on Music Information\nRetrieval(ISMIR),Paris(France) ,October2002.\n[3] S. Baumann and T. Pohle. A comparison of music\nsimilarity measures for a p2p application. In Pro-\nceedings of the Sixth International Conference on\nDigitalAudioEffectsDAFX,London(UK) ,Septem-\nber2003.\n[4] A. Berenzweig, B. Logan, D. P. W. Ellis, and\nB. Whitman. A large-scale evaluation of acoustic\nand subjective music similarity measures. In Pro-\nceedings of the Fourth International Conference on\nMusic Information Retrieval (ISMIR 2003), Balti-\nmore,Maryland(USA) ,October2003.\n[5] I. Borg and P. Groenen. Modern Multidimensional\nScaling.Springer-Verlag,NewYork.,1997.\n[6] G. Doddington, W. Liggett, A. Martin, M. Przy-\nbocki, and D. Reynolds. Sheep, goats, lambs and\nwolves,astatisticalanalysisofspeakerperformance.\nInProceedings of the 5th International Conference\non Spoken Language Processing (ICSLP), Sydney,\nAustralia. ,December1998.\n[7] B. LoganandA.Salomon. A musicsimilarityfunc-\ntionbasedonsignalanalysis.In inproceedingsIEEE\nInternational Conference on Multimedia and Expo\n(ICME),Tokyo(Japan) , August2001.\n[8] F. Pachet, A. LaBurthe, A. Zils, and J.-J. Aucou-\nturier. Popular music access: The sony music\nbrowser. Journal of the American Society for Infor-\nmation (JASIS),Special Issue on Music Information\nRetrieval,2004.\n[9] F. Pachet, G. Westermann, and D. Laigre. Musical\ndatamining for emd. In Proceedings Wedelmusic ,\n2001.\n[10] F. Pachet and A. Zils. Evolving automaticallyhigh-\nlevel music descriptors from acoustic signals. In\nSpringerVerlagLNCS, 2771 ,2003.\n[11] E. Pampalk,S. Dixon,and G. Widmer. On the eval-\nuation of perceptual similarity measures for music.\nInProceedings of the Sixth International Confer-\nence on Digital Audio Effects DAFX, London (UK) ,\nSeptember2003.\n[12] L. Rabiner. A tutorial on hidden markov models\nandselectedapplicationsinspeechrecognition. Pro-\nceedingsof theIEEE ,77(2),1989.\n[13] E. Voorhes and D. Harman. Overview of the\neighth text retrieval conference. In Proceedings\nof the Eighth Text Retrieval Conference (TREC),\nGaithersburg, Maryland (USA) , November 1999.\nhttp://trec.nist.gov."
    },
    {
        "title": "From Sound Sampling To Song Sampling.",
        "author": [
            "Jean-Julien Aucouturier",
            "François Pachet",
            "Peter Hanappe"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416028",
        "url": "https://doi.org/10.5281/zenodo.1416028",
        "ee": "https://zenodo.org/records/1416028/files/AucouturierPH04.pdf",
        "abstract": "This paper proposes to use the techniques of Music Information Retrieval in the context of Music Interaction. We describe a system, the SongSampler, inspired by the technology of audio sampling, which automatically samples a song to produce an instrument (typically using a MIDI keyboard) that plays sounds found in the original audio file. Playing with such an instrument creates an original situation in which listeners play their own music with the sounds of their favourite tunes, in a constant interaction with a music database.  The paper describes the main technical issues at stake concerning the integration of music information retrieval in an interactive instrument, and reports on preliminary experiments.",
        "zenodo_id": 1416028,
        "dblp_key": "conf/ismir/AucouturierPH04",
        "keywords": [
            "Music Information Retrieval",
            "Music Interaction",
            "SongSampler",
            "Automatic Sampling",
            "Instrumentation",
            "MIDI Keyboard",
            "Original Situation",
            "Listeners",
            "Favourite Tunes",
            "Music Database"
        ],
        "content": "FROM SOUND  SAMPLING TO SONG  SAMPLING \nJean-Julien Aucouturier, Francois Pachet, Peter Hanappe  \nSONY CSL Paris \n6, rue Amyot, 75005 Paris France.   \nABSTRACT \nThis paper proposes to use the techniques of Music \nInformation Retrieval in the context of Music \nInteraction. We describe a system, the SongSampler,  \ninspired by the technology of audio sampling, which \nautomatically samples a song to produce an instrument \n(typically using a MIDI keyboard) that plays sounds \nfound in the original audio file. Playing with such an \ninstrument creates an original situation in which \nlisteners play their own music with the sounds of their  \nfavourite tunes, in a constant interaction with a musi c \ndatabase.  The paper describes the main technical issues  \nat stake concerning the integration of music informatio n \nretrieval in an interactive instrument, and reports on  \npreliminary experiments.  \n1. INTRODUCTION \nMusic information retrieval research so far has not been \nmuch concerned with building interactive music \nsystems. Systems which rely on a real-time music \nperformance generally use it as an input front-end for a \nsearch, in a one-way approach which doesn’t allow any \nsubsequent interaction. In Query by Humming  (QbH, \n[1]), the user sings a melody, and audio files containing \nthat melody are retrieved. However, QbH does not \nexploit the resulting songs to respond to  the original \nmusical expression of the user, like e.g. improvising \njazz musicians quoting or mimicking each others ([2]). \nSimilar paradigms like Query by Rhythm  ([3]) or Query \nby Timbre  ([4]) share the same drawback.  \nTzanetakis in [5] proposes an alternative browsing \nenvironment which offers a direct, continuous \nsonification of the user’s actions. For instance, ch anging \nthe target value of a query on tempo from 60 to 120 \nwould morph the current song into a faster one, whereas \ntraditional settings would require the user to press a \n“submit” button, which would stop the former song, and \ntrigger the next one. While this is one step towards a \nseamless interaction with a music database, the syste m \nstill offers no expressive control on the music as a m usic \ninstrument would do. It remains a sophisticated jukebox. Interactive music systems propose ways of transforming \nin real time musical input into musical output. Such \nresponsiveness allows these systems to participate in \nlive performances, either by transforming the actual \ninput or by  generating new material according to some \nanalysis of the input. Musical interactive systems hav e \nbeen popular both in the experimental field [6] as well \nas in commercial applications, from one-touch chords of  \narranger systems to the recent and popular Korg Karma \nsynthesizer [7]. While some interactive systems, ref erred \nto in [6] as “sequenced techniques”, use pre-recorded \nmusic fragments in response to the user’s input, these \nsequences are usually predetermined, and their mapping \nis predefined (triggered by e.g. dynamics, or specific \nnotes, etc.). With the very large quantity of music \navailable on personal computers, comes the fantasy of \nan interactive instrument able to explore any music \ndatabase, responding to the user’s input with \nautomatically selected extracts or samples. \nThis paper describes a system, the SongSampler, which \nis an attempt at combining both worlds of music \ninteraction and music information retrieval. Using \ntechniques inspired by audio sampling, we propose to \nautomatically produce a music instrument which is able \nto play the same sounds as an arbitrary music file. The  \nSongSampler uses MIR techniques such as content \ndescriptors or similarity measures in order to select t he \nsong(s) to sample in a music database. The resulting \ninstrument influences the user’s performance, which, in  \nturn, is analyzed with MIR tools  to produce queries and \nmodify the sampler’s setting. Playing with such an \ninstrument creates an original situation in which \nlisteners play their own music with the sounds of their  \nfavourite tunes, in a constant interaction with a musi c \ndatabase.   \n2. AUTOMATIC SAMPLING \nAudio Sampling ([8]) is the process of mapping \narbitrary sounds (or samples) to a music instrument. \nEach note played on the instrument (typically a MIDI \nkeyboard) triggers an audio sample corresponding to \nthe pitch of the key (e.g. a C-60) and its loudness. Such \ndigital samplers have been introduced in the 80s, and \nhave been very popular thanks the realistic effect \nachieved by this technique: virtually any sound can be \nproduced by a sampler, by definition. However, the \ncreation of a particular setup for a sampler (e.g. a piano  \nsound) is known to be a tedious and difficult task: \nsamples must be first recorded (e.g. from existing, real Permission to make digital or hard copies of all or  part of this work \nfor  personal or classroom use is granted without fee p rovided that \ncopies are not made or distributed for profit or co mmercial advantage \nand that copies bear this notice and the full citat ion on the first page. \n© 2004 Universitat Pompeu Fabra. \n   \n \n music instruments), then assigned to the keys of a MIDI \ninstrument. Details concerning the actual triggering of \nsounds must be carefully taken into account, such as the \nloop of the sustain part of the samples until the key is \nreleased. This process of specifying a sound for a \nsampler is usually done by hand [9]. \nThe core of the system described here consists in \nproducing automatically setups for software-based \nsamplers, so that the sounds triggered correspond to the \nactual sounds present in a given music title. \nConsequently, a sampler setup produced from a given \naudio file will produce an instrument that plays the \nsame sounds as the original audio file. \nA popular format for what is referred to here as a \n“sampler setup” is the SoundFont® file format [10]. A \ntypical SoundFont® file contains : \n• Samples, which can be digital audio files, e.g. .wav, \nor loaded from ROM on the wavetable device. \nSamples have the option of being looped.  \n• Generators, which control properties of a sample \nsuch as initial pitch and volume as well as how \nthese parameters are affected over time. \n• Instruments, which use one or more samples \ncombined with effects generators to create a sound \nproducing device.  \nThe automatic extraction of a SoundFont®-like setup \nfrom an arbitrary audio file thus requires to : \n• analyse (i.e. segment) the audio data to extract \n“meaningful” samples in the music \n• extract high-level audio descriptors from the \nsamples to select automatically the most appropriate \nsamples to use for a given context. Notably, detect \nthe pitch of each sample so it can be mapped to an \ninstrument note \n• detect parts of the segment that can be looped \nautomatically (or, as it turns out, do more complex \nprocessing to time-stretch the samples) \nEach of these 3 steps have received a vast number of \ntechnical solutions, which we will not review here. The \nSongSampler has a modular architecture, in which any \nsuitable algorithm can fit.  \nHowever, we are dealing here with arbitrary music file s, \nof arbitrary complexity, e.g. polyphonic, containing \npercussion instruments, effects, etc. In the next \nparagraphs, we propose a number of algorithms which \nwe have designed specifically to fit this particular \napplication context.   \n2.1. Multiscale segmentation \n \nThe aim of the segmentation algorithm is to extract \nsamples that can act as well-defined musical events, i. e. \nwhich have a salient note or percussion played by some \ninstrument(s) in the foreground, and a background \nbased on the global sound of the sampled song. For \ninstance, a typical sample from the song “Yesterday” by  \n“The Beatles” ([11]) would be Paul McCartney singing \n“..day…”, with the song’s original background of acoustic guitar, bass and violin. The song is cut in the \ntime domain, which means that each sample contains \nseveral instruments playing at the same time, and not \nseparated individual tracks.  \nTypical segmentation algorithms ([12,13,14]) first \ncomputes a set of features from the signal cut into \nframes, and then detect the segment boundaries by \nlooking for abrupt changes in the trajectory of features.  \nIn this work, we look for the energy variations of the  \nsignal. The signal is cut into frames (2048 points at \n44100Hz), and for each frame, we compute the short–\nterm spectrum. The spectrum itself is processed by a \nMel filterbank of 20 bands. Each band’s energy is \nweighted according to the frequency response of the \nhuman ear, as described e.g. in [14]. Finally, the energy \nis summed across all bands. Change detection is done \nby smoothing the energy profile by a zero-phase \nfiltering by a Hanning window of size wS, and  looking \nfor all the local maxima of the smooth version. The \nsegment boundaries are the deepest valleys in the raw \nenergy profile between 2 adjacent peaks in the smooth \nprofile.  \n \n \n \nFigure 1A : Segmentation of an extract of “The Beatles - Yest erday”. \n(Top) Segmented energy profile using a small Sw (15 0ms) : short events \n(right) get properly detected, while larger events (left) get \noversegmented. (Bottom) Corresponding smoothed ener gy profile, used \nfor peak detection.  \n \nWhile this scheme is effective for simple, percussive \nmusic, we observe that for non percussive, richer \npolyphonic music, the quality of the segmentation \ndepends on the choice of wS. In large events such as a \nsung note lasting for several seconds (e.g. the final “-\nday” in “Yesterday”), there may be several small peaks \nof energy corresponding to the other  instruments \nplaying in the background (e.g. a succession of chords \nplayed on the guitar). With a small  wS, all these peaks \nwould be segmented, and the corresponding atomic \nevent for the sampler application would be cut into \nseveral short identical notes (see Figure 1A). With a \nlarge wS on the other hand, short meaningful events \nlike isolated guitar chords get missed out (Figure 1B). \n   \n \n \n \nFigure 2B : Segmentation of an extract of “The Beatles - Yest erday”. \n(Top) Segmented energy profile using a large Sw (1s ) : large events \n(left) are appropriately recognized, however smalle r events (right) \nare missed out. (Bottom) Corresponding smoothed ene rgy profile \nTherefore we propose a multiscale segmentation \nalgorithm, which adapts the size of the convolution \nwindow to the local shape of the energy profile. More \nprecisely, we compute the STFT of the energy profile \non a running 2-second window (with 90% overlap). As \nthe energy profile is sampled using 50% overlapping, \n2048 point frames (i.e. 43Hz), the FFT describes the \nfrequency content between 0 and 20Hz, with a \nfrequency resolution finer than 1Hz. We select the \npredominant local periodicity of the profile as the \nbarycentre point (spectral centroid) of the spectral \ndistribution within each frame :    \n \u0000\n\u0000\n=\nkk\nkSkkS \nSC )()(\n \n \nwhere S is the magnitude spectrum of a frame. We then \nsmooth the energy profile using a Hanning window size \nwSequal to the inverse of the centroid of the \ncorresponding FFT frame (to ensure continuity, \nHanning window coefficients are normalized so they \nsum to one regardless of their length).  \n \nFigure 2-Bottom shows the SFFT of the energy profile \nused in Figure1. Large events correspond to low \nfrequencies in the energy profile, i.e. small centroid \nfrequencies in the spectrogram (order of 1Hz). \nConsequently, these zones get smoothed with large \nHanning windows (order of 1 sec.). On the other hand, \nshort events in the energy profile  correspond to higher \nfrequency content, higher centroids, and smaller \nwindows size (order of 200ms). Figure 2-Top illustrates \nthe corresponding multiscale segmentation, which \npreserves large, noisy events as well as short, high \namplitude ones. \n \n \n \nFigure 3: (Top) Multiscale segmentation of the same extract , using an \nadaptive convoluation window size : large windows o n the left, and \nsmaller windows on the right. (Bottom) Correspondin g spectrogram of \nthe energy profile, super-imposed (in black) with t he spectral centroid of \neach frame, used to determine the windows size  \n2.2. Automatic Extraction of High-Level Descriptors  \nEach of the segments generated by the previous step \nconstitutes a note, which will be mapped on the \ninstrument. The mapping of the samples is based on \na number of high-level descriptors automatically \nextracted for each sample. The SongSampler relies \non the EDS (Extractor Discovery System) [16] to \ngenerate such descriptors. EDS is a system based on \ngenetic programming which is able to automatically \ngenerate arbitrary perceptual descriptors, given only \na test database and corresponding perceptive tests, \nby discovering and optimizing adapted features and \nmachine learning models. In the current \nimplementation of the SongSampler, EDS was used \nto generate a descriptor of harmonicity (see [16]). \nThis  descriptor is used in the SongSampler to filter \nout samples corresponding to non harmonic events \n(e.g. a snare-drum hit). However, there is an infinity  \nof such descriptor/mapping possibilities, depending \non the application context. For instance, if the \nmapping is done on a digital MIDI drum kit, we \ncould use the EDS to generate a drum sound \nclassifier (see e.g. [17]) in order to affect the \npercussive samples to the right pads. One key \nadvantage of using the EDS is that the descriptors \nare by construction adapted to the type of samples \nused in the SongSampler. We expose further \npossible uses for the EDS in section 5. \n2.3. Salient pitch detection \nThe SongSampler maps the samples to the keyboard key \ncorresponding to their pitch, so a melody can be played. \nIn the current implementation of the system, the pitch  \ndescriptor was not generated by the EDS, but rather was \nmanually designed. We describe this specific algorithm \nin this section. Traditional monophonic, mono-  \n \n instrument pitch detection algorithms are based on peak \ndetection in the signal's spectrum. In more complex \nsound sources, state-of-art approaches look at spectral \nperiodicities, either by a “comb-filter” sample-summing \napproach ([18,19]), or by a FFT approach ([20]). Such \nanalyses done with a high frequency resolution have the \nadvantage of yielding precise estimates of fundamental \nfrequency, which e.g. can be used to study fine \nproperties of instrument timbre. However, such high \nresolutions come at the expense of rather complex \nalgorithmic provisions to cope with a number of signal \ncomplexities like inharmonicity or vibrato : \ninharmonicity factor, subband processing in [19], \ntracking of partial trajectories in [20]. \nIn our context, we are only interested in a rough pitch \nestimate, with limited precision both in frequency \n(precise to the semi-tone, which is the MIDI resoluti on) \nand time (one salient pitch estimate for each sample).  \nWe apply a STFT to the signal, and converts the \nfrequency scale into a midi pitch scale, using a bank of \nband pass filters, on per midi pitch from C0 to C7 with \nthe width of one semitone. The remaining of the \nalgorithm thus deals with a much simpler symbolic \nsignal, a “pitchogram” which represents the energy of \neach potential midi pitch in the signal.  \nFigure 3f shows such a pitchogram (averaged over time) \nfor a sample from “Yesterday”, for midi pitches ranging \nfrom 30 (F#1) to 90 (F#6). The pitchogram is then \nlooked for local maxima, each of which constitute a \npitch hypothesis. \nEach pitch hypothesis receives a harmonic score \naccording to the presence or not of another pitch \nhypothesis at the position of its harmonics : \n• at octave (midi pitch+12) \n• twelfth(octave+fifth) (midi pitch+19) \n• fifteenth(two octaves) (midi pitch+24) \n• major seventeenth (two octaves+major third) (midi \npitch+28), etc. \nThe harmonic score is computed as the weighted sum of \nthe harmonics’ energy. Figure 3 shows the 5 best-score \npitch hypotheses with their harmonics. Each of the \nharmonics’ energy is the energy of the corresponding \npitch hypothesis as found in the pitchogram. \nAdditionally, we reinforce the scores of pitch hypothes is \nwhich uniquely explain one of their harmonics, e.g. the \nfirst harmonic in Figure 3-b. Note that the first \nharmonic of 3-b doesn’t become a pitch hypothesis and \ndoesn’t appear as a new plot in Figure 3, because it has \nno harmonics, and thus receives a minimal score. \nFinally, we pick the pitch hypothesis with the best sco re. \nIn Figure 3, the best hypothesis is 3-c, which both \nincludes a uniquely explained harmonic, and has an \nimportant harmonic score. The corresponding pitch is \nthe midi pitch of its fundamental, i.e. 52 (E3). Note that  \nothers strategies are possible to cope with polyphony \n(e.g. choosing all hypotheses whose score exceeds a \ncertain threshold) \n \n \n \nFigure 4:  the 5 main pitch hypotheses (a-e) corresponding to  the time-\naveraged pitchogram (f). Brackets show the harmonic  relations between \nthe partials of each hypothesis. Dotted-line square s highlight the partials \nwhich are explained by a unique pitch hypothesis. \n \nWe have conducted a small evaluation in order to fine-\ntune the weights involved in the computation of the \nharmonic score. The test database has 50 samples \nextracted automatically from 3 pop songs, and the target \npitch were determined manually. We test 2 parameters : \nthe number of harmonics to be considered (nh), and the \nweights of the harmonics, parameterized by exponential \n).exp( nγβα−  curves, with gamma ranging from –1 \nto 1. We observed that these parameters have little \ninfluence on the algorithm’s precision. The best \nprecision (0.76) was obtained for nh=5 and gamma=0.4 \n(slightly decreasing weights). For better precision, \nharmonic weights could be adapted to specific \ninstrument timbres. \n2.4. Time stretching \nThe samples extracted by segmentation from the \noriginal song have the same duration as in the original \nsong  (e.g. 1.44 second for the above-mentioned “day” \nsample from “yesterday”, in the original recording in \nthe Help album). However, when these samples are \nmapped on a music instrument, we want the duration to \nmatch the musician’s intention: notes can be shorter  or \nlonger than in the original song they were extracted \nfrom. \nThe process of modifying the duration of a note is \ncalled time stretching. Time stretching in traditional \nsamplers is done by looping within the sustain part of \nthe sample for the appropriate (longer) duration. This \nrequires loop start and end points, which are usually \nfound manually, and requires much trials-and-errors. \nLooping is well adapted for clean monophonic, mono \ninstrumental samples. However, in our context of \nsampling arbitrary recording, with complex polyphony, \nbackground percussion, and “real-world” sound \nproduction like reverberation, this approach yields very \npoor results. \nWe time-stretch the samples using a technique know as \nphase-vocoder, which analyses the short term spectrum   \n \n of the signal and synthesizes extra frames to morph \nbetween the original signal’s frames (e.g. adding an \nextra 50 millisecond every 50 milliseconds). The \ntechnique relies on a phase continuity algorithm called \nidentity phase locking ([21]).  \nIn our application context, many samples resulting \nfrom the segmentation algorithm described above are \nnot ideally stable: while each sample is a coherent n ote \n(e.g. “day”), there are still minor events occurring in  \nthe background (e.g. softer notes of the guitar \naccompaniment), which creates discontinuities of \ntimbre, energy, etc…  \n \n \nFigure 5: time stretching with stability zone weighting \nIf we apply the phase-vocoder to the whole sample, the \nalgorithm would also stretch these transient events, \nleading to unrealistic, “slow-motion” sounds, known as \ntransient smearing (e.g. guitar attacks lasting for too \nlong). To avoid stretching zones of discontinuity in the  \nsignal, we first analyze each sample to find zones of \nspectral stability, using the EDS harmonicity descriptor  \n(section 2.2). Each stable zone receives a stability score, \nand we only stretch these zones, by a stretch factor  \nwhich is proportional to the zone’s stability. (Figure 4) \n3. ARCHITECTURE \nFigure 5 describes the architecture of the system. The \ncore of the SongSampler is implemented in Java. It is \ncomposed of 2 concurrent interacting processes, a \nplayer, and a sampler. The interaction occurring \nbetween the Sampler and the Player is at the center o f \nthe application we propose in this work, and is \ndescribed in details in the next section.  In this sect ion, \nwe only describe the nature and medium of the \ncommunication between the components. \nThe SongSampler relies on 2 other components: \n• MidiShare [22] is a real-time multi-task MIDI \noperating system. MidiShare applications are able to \nsend and receive MIDI events, schedule tasks and \ncommunicate in real-time with other MidiShare \napplications.  \n• Fluidsynth [23] is a real-time software synthesizer \nbased on the SoundFont 2 specification. The SongSampler relies on fluidsynth to efficiently play \nthe samples analysed by the Sampler.  \n \n \nFigure 6 : Architecture of the SongSampler, showing the 4 ma in \ncomponents and their Java native interface . \nBoth Fluidsynth and the Sampler process are declared as \nMidiShare applications. MIDI messages coming from \nthe user (e.g. through a MIDI keyboard) are routed via \nMidiShare to the Sampler.  \nAfter analysis, the Sampler forwards the midi messages \nto Fluidsynth, which triggers the corresponding samples \nand renders them to the audio device. \nThe Player communicates with a music database. It can \nautonomously query the db according to various criteria, \nwhich are inferred from the current state of the syst em \nand the user’s actions. It can also play a song, and \ninteract with the sampler by proposing new songs to \nsample.  \nThe Sampler performs the analysis described in section \n2 (possibly prepared in non-real time), assigns samples \nto Fluidsynth, reacts to midi messages coming from the \nuser (e.g. midi program changes), and interacts with the \nplayer by forwarding the incoming user actions. \nFluidsynth is piloted in Java from the Sampler using \nJava native interfaces (JNI) which were developed for \nthis project. Both Fluidsynth and the sampler \ncommunicate with MidiShare via JNI which were \ndeveloped by Grame ([22]). \n4. PRELIMINARY EXPERIMENTS  \nThe SongSampler can be used in a variety of \nplaying/listening modes, which results from the many \npossibilities of interaction between the Player and t he \nSampler process. In this section, we describe our \npreliminary experiments with the system.   \n \n 4.1. Turn Taking \nFigure 6 illustrates a first mode of interaction, where the  user \nand the system’s music player take turns.  \nIn this setting, a song is chosen (e.g. our followed \nexample, “Yesterday”), and analysed by the sampler : \nthe song is segmented into meaningful notes and \nsamples are analysed for pitch. The Sampler then maps \nthe samples in fluidsynth in such a way that the \nsamples are changed after each pressed key, and iterate \nin time order. For instance, pressing a key 3 times \nwould trigger the 3 samples “yes-”, “-ter-”, “-day”. The \nsamples are matched to every note on the keyboard, but \nkeep a relation to their pitch in the original signal. F or \ninstance, the “yes” sample in “yesterday” has an \noriginal detected pitch of “G3”. If the user triggers this \nsample by pressing the “F3” key, fluidsynth \nautomatically pitch-shifts 1 the sample to match the \nwanted pitch, i.e. the sample will be played one tone \nlower than in the original signal. \nWhen the mode is started, the Player starts playing the  \nsong normally. At anytime, the user can press a note o n \nthe keyboard. When the note is received, the Player \nstops, and the Sampler seamlessly triggers the sample \ncorresponding to the current position in the song. The \nuser keeps the lead until he stops playing, i.e. a given \ntime has passed since the last played note, at which \npoint the Player starts playing the song again, at the \nposition of the last triggered sample. Moreover, the new  \nbehavior of the Player depends very closely on the \nuser’s performance : \n• bpm interaction: as the user plays with the Sampler, \nthe bpm of its performance is tracked, using a real-\ntime beat tracker ([24]). Upon restart of the Player, \nthe song is time-stretched (using the technique \nexposed in section 2.3) to match the bpm of the \nuser’s performance.   \n• pitch interaction: the performed pitch of the last \ntriggered sample is compared to the sample’s \noriginal pitch, and upon restart, the Player pitch-\nshifts the song to match the transposition interval \n(using a phase-vocoder like for time-stretching). \nUsing these simple mechanisms, the user can play new \nmelodies with the sounds of the original song. In turn, \nthe original song is modified according to the user’s \nperformance. \nExample on Yesterday \nFigure 6 is a transcription of a turn-taking interaction \nbetween a user and the song “Yesterday” by The \nBeatles. The example starts at the second line of the first \nverse. The Player plays the music corresponding to the \nmelody {D,D,C,Bb,A,G} at a normal rate. At this point  \n(#1), the user takes the lead, and press the {Bb} key, \nwhich is the original pitch of the next sample in queue \n                                                        \n1 In the current version of fluidsynth, pitch-shifting is  \ndone by resampling.  (Paul McCartney singing “here”). Then the user starts \ndeviating from the melody with an ascending pattern \n{C, C#,D}. This successively triggers the samples “to”, \n“stay”, “oh”, at a different, increased pitch than in the \noriginal song. Simultaneously, the user increases the \ntempo from the original bpm of 100 quarter notes per \nminutes to an “allegro” tempo of 140 bpm. After \ntriggering the “oh” sample, the user stops playing. The \nPlayer now takes the lead (#2), and restarts the original  \nsong at the position of the next sample (“I”). As the l ast \ntriggered sample is pitched a perfect fifth higher than \nthe original pitch, the original song is pitch shifted by  a \nfifth, which creates a feeling of continuity with the  \nuser’s phrase. At the same time, as the user bpm is \nhigher than the bpm of the original song, the song is \ntime-stretched to match the new tempo.  \n4.2. Collaborating \nFigure 7 illustrates another mode of interaction, in \nwhich the Player loops on a section of the original f ile \n(the introduction of Yesterday). In the mean time, the  \nSampler processes another section of the song (Paul’s \nvoice on the first verse and chorus). The user is then  \nable to improvise a phrase with the accompaniment of \nthe original song. This mode is an interesting exercise \nfor musicians, as they have to make the best of the \noffered accompaniment, e.g. in Figure 7, although the \nsong is in the key of F major, the guitar vamping  on the \nintroduction does not play the third degree (A). This \nleaves an ambiguity on the major/minor character of t he \nsong 2, which is exploited by the user (alternation of \nminor third Ab and major third A). \n4.3. Exploring the database \nThe Sampler and the Player need not process the same \nsong. For instance, in the previous mode, the Sampler \nmay query a song in the database according to any \nmetadata, e.g. instrument = piano. Consequently, the \nuser would play on top of Yesterday’s guitar comping \nwith e.g. the piano sound of a Beethoven sonata. \nMoreover, the Sampler thread can listen to the end of \neach of the user’s phrases, and change the synth’s \nsettings with another piano song so that the user \nexplores all piano songs/sounds in the database.  \n5. FURTHER WORK \nMany standard techniques of Music Information \nRetrieval can be integrated in the interactive system  \ndescribe above. This section lists some of the \npossibilities we envision : \n                                                        \n2 This ambiguity on the song’s key is actually largely \nexploited in the melody of original song, as analysed \ne.g. in [25]   \n \n • Editorial metadata on the songs : play only with \nsamples from the Beatles, or only “Brasilian \nsounds” \n• High-Level Descriptors on the samples :  Perceived \nEnergy ([16]) (samples with high energy are \ntriggered when keys are pressed with a high \nvelocity, while softer samples are triggered for lower  \nvelocities), Instrument ([27]) (play only samples of \nacoustic guitar). As described in section 2, the \nSongSampler relies on the EDS to automatically \ngenerate such descriptors. \n• Query by Melody ([1]) The user plays the melody of \n“Michelle” with the Samples of “Yesterday”, and \nthe Player replies with “Michelle” \n• Query by Harmony ([28]): The player selects a song \nwhose harmony matches the phrase being played \n• Query by Timbre ([4]): the SongSampler may \ninteract with the user by proposing (i.e. either play \nor sample) songs which sound similar to the song \ncurrently being played/listened to. \n• Structural Analysis of the songs ([29]) : sections to \nsample or to loops (see 4.2.) may be automatically \ndetected.  \n6. CONCLUSION \nThis paper describes a system, The SongSampler, which \nautomatically samples a music file in order to produce \nan instrument that plays the same sounds as the original  \naudio file. This is an attempt at mutually enriching both  \nworlds of Music Information Retrieval and Music \nInteraction. The process of interacting with a music \ncollection creates a novel immersive browsing \nexperience, in which queries are not necessarily \nformulated by the user, but are rather inferred from the  \nuser’s actions. On the other hand, playing with such a \nMIR-enabled interactive instrument enhances the \nfeeling of appropriation by letting listeners play their \nown music with the sounds of their favorite tunes. \n7. ACKNOWLEDGEMENTS \nThe authors wish to thank Tristan Jehan for interesti ng \ndiscussions on the topic of segmentation. The scores in  \nFigure 6 and 7 were obtained with Guido NoteViewer \n(http://www.noteserver.org). \n8. REFERENCES \n[1] Hu, N. and Dannenberg, R. “A Comparison of \nMelodic Database Retrieval Techniques Using Sung \nQueries”' in Joint Conference on Digital Libraries , \nNew York: ACM Press, (2002), pp. 301-307. \n[2] Monson, I. Saying Something: Jazz Improvisation \nand Interaction , Chicago Studies in \nEthnomusicology, The University of Chicago Press, \n1996 \n[3] Chen, J. and Chen, A. “Query by Rhythm: An \nApproach for Song Retrieval in Music Databases”, \nProceedings Eighth International Workshop on Research Issues in Data Engineering, Continuous-\nMedia Databases and Applications , (1998), pp. \n139--146. \n[4] Aucouturier, J.-J. and Pachet F., “Improving Timbre \nSimilarity: How high is the sky?”. \nJournal of Negative Results in Speech and Audio \nSciences , 1(1), 2004. \n[5] Tzanetakis, George, Ermolinskyi, Andreye, and \nCook, Perry \"Beyond the Query-by-Example \nParadigm: New Query Interfaces for Music \nInformation Retrieval\" In  Proc. Int. Computer \nMusic Conference (ICMC) , Gothenburg, Sweden \nSeptember 2002 \n[6] Robert Rowe, Interactive Music Systems, MIT Press \nCambridge, Massachusetts 1993. \n[7] Karma music workstation, Basic guide. Korg Inc. \nAvailable : www.korg.com/downloads/ , 2001. \n[8] Roads, C. “Sampling Synthesis” in The Computer \nMusic Tutorial , pp.117-124, MIT Press, \nCambridge, Massachusetts 1995. \n[9] Duffel, D. The Sampling Handbook , Backbeat \nBooks, May 2004. \n[10] SoundFont® is a registered trademark by E-MU. \nSpecifications available : \nwww.soundfont.com/documents/sfspec21.pdf \n[11] “Yesterday”, John Lennon/ Paul McCartney, in \n“Help”, Parlophone, 1965   \n[12] Tzanetakis, G., and Cook, P., “Multifeature Audio \nSegmentation for Browsing and Annotation”, IEEE \nWorkshop on Applications of Signal Processing to \nAudio and Acoustics , New Paltz, NY, Oct 1999. \n[13] Rossignol, S. et al. “Feature Extraction and \nTemporal Segmentation of Acoustic Signals”, in \nProceedings of the International Computer Music \nConference , 1998. \n[14] Pampalk, E., Dixon, S. and Widmer, G. “Exploring \nmusic collections by browsing different views”, in \nProceedings of the International Symposium on \nMusic Information Retrieval  (ISMIR), Paris, France \n2003 \n[15] Foote, J. and Cooper, M.  “Media segmentation \nusing self-similarity decomposition”, in Proc. SPIE \nStorage and Retrieval for Multimedia Databases , \nVo. 5021, January 2003. \n[16] Pachet, F. and Zils, A. “Evolving Automatically \nHigh-Level Music Descriptors From Acoustic \nSignals.” Springer Verlag LNCS , 2771, 2003. \n[17] Gouyon, F., Pachet, F. and Delerue, O., “On the use \nof zero-crossing rate for an application of \nclassification of percussive sounds”, in Proceedings \nof the Digital Audio Effects (DAFx'00) Conference,  \nVerona, Italy 2000 \n[18] Klapuri, A. et al. “Robust multipitch estimation for \nthe analysis and manipulation of polyphonic \nmusical signals”, in Proceedings of the Digital   \n \n Audio Effects (DAFx'00) Conference,  Verona, Italy \n2000 \n[19] Tadokoro, Y., Matsumoto, W. and Yamaguchi, M. \n“Pitch detection of musical sounds using adaptive \ncomb filters controlled by time delay”, in proc.  \nICME , Lausanne, Switzerland, 2002.. \n[20] Marchand, S.  “ An Efficient Pitch-Tracking \nAlgorithm Using a Combination of Fourier \nTransforms.” In Proceedings of the Digital Audio \nEffects (DAFx'01) Conference , pages 170-174, \nLimerick, Ireland, December 2001. \n[21] Jean Laroche and Mark Dolson \"New Phase \nVocoder Technique for Pitch-Shifting, Harmonizing \nand Other Exotic Effects\". IEEE Workshop on \nApplications of Signal Processing to Audio and \nAcoustics . Mohonk, New Paltz, NY. 1999.  \n[22] Orlarey, Y. and Lequay, H. MidiShare: a Real Time \nmultitasks software module for MIDI applications, \nin Proceedings of the International Computer \nMusic Conference , Computer Music Association, \nSan Francisco, pp. 234-237, 1989. [23] Fluidsynth website http://www.fluidsynth.org  \n[24] Scheirer, E. D. (1998). “Tempo and beat analysis of \nacoustic musical signals,” Journal of the Acoustical \nSociety of America , 103(1), 588–601. \n[25] Pollack, Alan W. (1993), Notes on \"Yesterday\". \nNotes on ... Series  no. 74, 1993. Available : \nhttp://www.recmusicbeatles.com \n[26] Perfecto Herrera, Geoffroy Peeters, Shlomo Dubnov \n“Automatic Classification of Musical Sounds”  \nJournal of New Musical Research  2003, Vol. 32, \nNo. 1, pp 3-21  \n[27] Pickens, J. et al. \"Polyphonic Score Retrieval Using \nPolyphonic Audio Queries: A Harmonic Modeling \nApproach,\" Journal of New Music Research  \n32(2):223-236, June 2003. \n[28] Geoffroy Peeters, Amaury La Burthe, Xavier Rodet \n“Toward Automatic Music Audio Summary \nGeneration from Signal Analysis”, in proc.   \nInternational Conference on Music Information \nRetrieval (ISMIR), Paris (France) October 2002  \n \n \nFigure 7: Turn Taking on “Yesterday – The Beatles”. \n \nFigure 8: Playing on top of the introduction of “Yesterday” , with samples from the verse and chorus"
    },
    {
        "title": "GREENSTONE as a Music Digital Library Toolkit.",
        "author": [
            "David Bainbridge 0001",
            "Sally Jo Cunningham",
            "J. Stephen Downie"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417573",
        "url": "https://doi.org/10.5281/zenodo.1417573",
        "ee": "https://zenodo.org/records/1417573/files/BainbridgeCD04.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1417573,
        "dblp_key": "conf/ismir/BainbridgeCD04",
        "content": "GREENSTONE AS A MUSIC DIGITAL LIBRARY TOOLKIT\nPOSTER/DEMONSTRATION\nDavid Bainbridge\nDept. of Computer Science\nUniversity of Waikato\nHamilton\nNew ZealandSally Jo Cunningham\nDept. of Computer Science\nUniversity of Waikato\nHamilton\nNew ZealandJ. Stephen Downie\nGraduate School of Library\nand Information Science\nUniversity of Illinois\nUrbana-Champaign, IL, USA.\n1. INTRODUCTION\nGreenstone is an open source digital library system that\nhas developed and matured since its inception in 1995.\nToday it is used in over 60 countries, with a strong em-\nphasis on humanitarian aid. The software is also used\nas a framework for research in other ﬁelds such has hu-\nman computer interaction, text-mining, and ethnography.\nThis article provides a summary of Greenstone’s uses to\ndate with music documents. First we discuss incorporat-\ning musical formats into the Greenstone system; then we\ndescribe provision for searching and browsing in a music\ncollection.\n2. DIGESTING SOURCE DOCUMENTS\nCentral to the Greenstone design is a generalised and ex-\ntensible mechanism for importing documents, called plu-\ngins. Herewedescribefoursuchpluginsdesignedformu-\nsicformatsthatspanarangeofrepresentations: symbolic\nnotation, raw audio and sheet music. Plugins are written\ninPerl. Onceabasepluginhasbeenestablishedforanew\ndocument representation, such as symbolic music, much\nof the work for supporting additional formats of a simi-\nlar form can be carried out through external applications,\nwith the Perl code wrappingthings together.\nTheﬁrstmusicpluginwrittenwasRogPlug,aninternal\nformat we have historically developed. With this estab-\nlished, developing MIDIPlug was straightforward. MIDI-\nPlug inherits from RogPlug, and uses a modiﬁed version\nof the open source music notation software RoseGarden\n(www.rosegardenmusic.com)toconvertMIDIdatatothis\nformat. The data is then streamed through to the underly-\ning Rog plugin.\nGreenstone currently does not support direct content\nretrieval of raw audio, and so incorporation of MP3 ﬁles\nin Greenstone is through the text metadata contained in\na ﬁle’s ID3 tags. The role of MP3Plug, therefore, is to\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2004 Universitat Pompeu Fabra.extract this information, which it does through the use of\naPerlpackageMP3::Info,availablefromtheComprehen-\nsivePerlArchiveNetwork(CPAN).Inanextensiontothis\nplugin, artist and title metadata can be used to retrieve re-\nlated images from the web (albeit heuristically). These\ncan then be used in the digital library to evoke additional\ninterest in the work, such as the collaging technique de-\nscribed in Section 4.\nSheetmusicisourﬁnalformofmusicplugin. Herewe\nhave linked the plugin with C ANTOR, a research-based\nOptical Music Recognition (OMR) tool [1], to extract a\npreview image of the ﬁrst staff system, plus any accom-\npanying text above it. In constructing collections based\non this plugin we follow Greenstone’s existing page im-\nagemodelthatsupportsfull-sizeandpreviewimages,and\nnavigation by page number. In an experimental version\nof the plugin, we fed the (possibly erroneous) symbolic\nmusic notation extracted by the OMR tool into RogPlug\nto build a version where direct content queries were sup-\nported [2].\n3. FORMS OF QUERYING\nAny text or metadata extracted from the supported music\nformats can be used to build indexes for querying: search\nby title ﬁeld of MP3 ﬁles, for example. To extend the\nsearchingparadigminGreenstone,M ELDEX[2],hasbeen\nadded as an extension to the runtime system. Moreover,\nmusic and text queries can be combined. Figure 1 shows\none application of its use. Based on a collection of MIDI\nﬁles, a user is formulating a query based on “Gold” ap-\npearing in the song’s title and singing an excerpt of the\nsong they remember. After a short delay upon pressing\nthe search button, a ranked list of tune titles is displayed\n(not shown) with an audio icon next to each for playback.\n4. BROWSING\nCollaging is a visualisation technique whereby a series of\nimagesaredisplayedovertimeatrandomlocationsonthe\nscreen. Imagesalreadyinthevisualisationfadeovertime;\nnew images are added on top of what is already there. If\nat any stage a displayed image sparks some interest, theFigure 1. Direct content querying of symbolic music no-\ntation.\nuser can click on it and have a new window open up dis-\nplaying the context of that image. We have adapted this\nideaasabrowsingfacilityinGreenstoneusingimageslo-\ncated through the MP3Plug extension. The new window\nthat opens up when an image is clicked on is a metaphor\nfor the selected record, and shows two images associated\nwith the track (fashioned on the idea of opening up the\nCD cover) while the audio is played through an embed-\ndedMP3applet. Collaginginadigitalmusiclibraryisthe\nsubject of a longer submission to ISMIR 2004.\nPHIND,aninteractivehierarchicalphraseindexingtech-\nniquealreadyimplementedinGreenstone,isanovelform\nof browsing that makes for a compelling facility in a dig-\nital music library. When the digital library is built, it is a\nsimple matter to direct the text/metadata extracted by the\nmusic-based plugins into the P HINDmodule. Accessing\nthe “phrases” link in a collection based about the Beatles\nand entering “love” as a starting term yields a list includ-\ning “love me do,” “love you to,” “real love,” “words of\nlove” and so on. From this list the user can explore usage\nof a phrase, and variants, in all documents it appears in.\n5. HETEROGENEOUS DATA\nAnother strength of the Greenstone framework is work-\ning with a heterogeneous mix of source documents. As a\nconcluding example, consider extending The Beatles col-\nlectionalludedtoabovewithMP3ﬁles,MIDIﬁles,Guitar\ntablatureintheformofplaintextﬁles,andwebpagescon-\ntaininglyricsanddiscographyinformation(seeFigure2).\nThe navigation bar at the top of this ﬁgure gives the\nuser access to:\nSearching. Direct audio querying of MIDI ﬁles plus text\nand metadata searching over alldocuments;\nFigure 2. Browsing heterogeneous documents by title.\nTitles A-Z. Documentsinthecollectionclusteredbytitle\nand sorted alphabetically;\nBrowsing. Documentsclusteredintogroupsreﬂectingdoc-\numenttype: audio,lyrics,tablature,ordiscography;\nPhrases. The PHINDphrases mentioned above.\nTheﬁgureshowstheL–Msectionoftitles,wheredifferent\nmedia types with the same title have been automatically\ngrouped together. Opening up the cluster for the song Let\nIt Bewe see that there are four items in the collection:\nlyrics, discography, audio and tablature. Clicking on one\nof these links yields the respective document.\nAs part of the poster/demonstration, a variety of col-\nlectionsthatillustratethevariousaspectsdescribedinthis\npaper will be given.\n6. REFERENCES\n[1] D.BainbridgeandT.Bell. Amusicnotationconstruc-\ntion engine for optical music recognition. Software\nPractice and Experience , 33(2):173–200, 2003.\n[2] D. Bainbridge, C. Nevill-Manning, I. Witten,\nL. Smith, and R. McNab. Towards a digital library of\npopularmusic. In The4thACMconferenceonDigital\nLibraries, pages 161–169, 1999."
    },
    {
        "title": "Visual Collaging Of Music In A Digital Library.",
        "author": [
            "David Bainbridge 0001",
            "Sally Jo Cunningham",
            "J. Stephen Downie"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415832",
        "url": "https://doi.org/10.5281/zenodo.1415832",
        "ee": "https://zenodo.org/records/1415832/files/BainbridgeCD04a.pdf",
        "abstract": "This article explores the role visual browsing can play within a digital music library. The context to the work is provided through a review of related techniques drawn from the fields of digital libraries and human computer in- teraction. Implemented within the open source digital li- brary toolkit Greenstone, a prototype system is described that combines images located through textual metadata with a visualisation technique known as collaging to pro- vide a leisurely, undirected interaction with a music col- lection. Emphasis in the article is given to the augmenta- tions of the basic technique to work in the musical domain.",
        "zenodo_id": 1415832,
        "dblp_key": "conf/ismir/BainbridgeCD04a",
        "keywords": [
            "digital music library",
            "visual browsing",
            "digital libraries",
            "human computer interaction",
            "Greenstone",
            "collaging",
            "visualisation technique",
            "leisurely interaction",
            "musical domain",
            "prototype system"
        ],
        "content": "VISUAL COLLAGING OF MUSIC IN A DIGITAL LIBRARY\nDavid Bainbridge\nDept. of Computer Science\nUniversity of Waikato\nHamilton\nNew ZealandSally Jo Cunningham\nDept. of Computer Science\nUniversity of Waikato\nHamilton\nNew ZealandJ. Stephen Downie\nGraduate School of Library\nand Information Science\nUniversity of Illinois\nUrbana-Champaign, IL, USA.\nABSTRACT\nThis article explores the role visual browsing can play\nwithin a digital music library. The context to the work\nis provided through a review of related techniques drawn\nfromtheﬁeldsofdigitallibrariesandhumancomputerin-\nteraction. Implemented within the open source digital li-\nbrary toolkit Greenstone, a prototype system is described\nthat combines images located through textual metadata\nwith a visualisation technique known as collaging to pro-\nvide a leisurely, undirected interaction with a music col-\nlection. Emphasis in the article is given to the augmenta-\ntionsofthebasictechniquetoworkinthemusicaldomain.\n1. INTRODUCTION\nLookingatpatternsofuseinrealmusiccollections—phy-\nsical libraries, music stores—we see people (of their own\nvolition) spending a signiﬁcant amount of time looking,\nbrowsing, sifting through the music/documents [5]. This\nisinstarkcontrasttoobservedbehaviorindigitallibraries;\ntransactionloganalysisrevealsbriefsessions,fewsearches,\nshort queries, and few documents viewed [7, 12]. Is this\na true reﬂection of how users, in general, wish to interact\nwith digital repositories or merely an artifact of browsing\nmechanisms that are ineffectual or too cumbersome to be\nused? Thisissuehasledtovariousresearchinitiativesde-\nveloping richer forms of support for browsing with text-\nbased digital libraries, for instance [3, 15, 14]. Given a\nstrong desire shown by people to browse real music col-\nlections then the ability to support this activity within a\ndigital music library seems especially important.\nThisprovidedthemotivationfortheworkdescribedin\nthis paper: the adaptation of a visual technique known as\ncollaging to the music domain. Collaging is a technique\ndevised by Kerne [9] for use in the more general environ-\nment of the web. In one example of its use, a user keys\ninseveralwebdomains—saymtv.com,archive.org/audio,\nandceolas.org—presses goandthenobservesavisualisa-\ntion of images drawn from crawling these web sites con-\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2004 Universitat Pompeu Fabra.currently. Images already in the visualisation fade over\ntime and as new images are retrieved from the various\nsites, they are added in to the visualisation on top of what\nis already there. If at any stage a displayed image sparks\nsomeinterest,theusercanclickonitandhaveanewweb\nbrowseropenupdisplayingthewebpagetheimagecomes\nfrom.\nThis paper is organized as follows: Section 2 reviews\nrelatedworkincollagingandbrowsingindigitallibraries,\nand the “laid back” approach to interacting with informa-\ntion collections; Section 3 presents a walkthrough of a\nprototype music collection that includes collaging; Sec-\ntion 4 describes implementation issues for creating the\nprototype; and Section 5 is a summary of our work.\n2. RELATED WORK\nPrevious graphics-based browsing tools for music docu-\nment collections primarily support visualization of entire\ncollections; for example, as a self-organising map [14] or\na 3D vector space [15] showing documents clustered by\ngenre. While these visualisations can give an immedi-\nate, powerful overview of relative size of groupings and\na sense of distance between clusters, they are less effec-\ntive in supporting browsing of the documents themselves,\nsince the idea of an overview means that the documents\nmust be minimised and represented at the top (browsing)\nlevelinarelativelyimpoverishedfashion(forexample,by\na single point in a 3D space) [3].\nEarlierworkapplyingcollagingtoinformationseeking\nfocusedonusingtheresultingcollageasasearchtool,for\nexampletobuildacollectionofWebdocumentsbasedon\na CollageMachine search/browse over Google [8]. Based\non user evaluation studies, collaging seems ill-suited to\nsupportingmoreformalinformationseekingactivitiessuch\nas direct search; users objected to the lack of predictabil-\nity in the collaging, and the lack of transparency in the\nrelationship between the search terms that seed the col-\nlage and the collaged “results.” Collaging appears best\nsuited to providing a browsing experience that is engag-\ning and interesting, but that does not require the direc-\ntion and focus on the part of the user that is characteristic\nof searching or even of other types of browsing (for ex-\nample, browsing by metadata or by genre). This type of\nactivity,wherethesystemselectsinformationforviewingbasedonlittle(orevenno!)controlfromtheuser,hasbeen\ntermed “laid back,” to distinguish it from the more active\nanduser-directed“sitforward”informationseekingactiv-\nities [6].\nThe utility of the laid back approach to browsing has\nsupportingevidenceintheHCI,cognitivescienceandpsy-\nchology of memory literatures. It is now a well-known\ntrope in these literatures that “recognition” is greater than\n“recall”[13]. Sincelaidbackbrowsingispremisedonthe\nuser recognising an association presented by the collag-\ning interface rather than having to recall a speciﬁc fact,\nmelody, or set of appropriate search terms, it follows that\nthecollagingtechnique,anditslaidbackapproach,should\nprovideabeneﬁcialbrowsingenhancementformusicseek-\ning users.\nPrevious implementations of collaging have been di-\nrectedatusingextractedimagesortextsnippetstoexplore\nor search document collections that are primarily text [8,\n10]orimages[2]. Howappropriateisthistechniquelikely\nto be when applied to collections of audio documents? In\nanswer, we note that the visual collage-based browsing\nwe describe in this paper is surprisingly close to a com-\nmonbrowsingactivityobservedinmusicstores—ﬂicking\nthrough horizontal displays of music CDs. In this latter\nactivity, shoppers glance at the CD cover images as they\nareﬂickedpast,occasionallypausingtolookmoreclosely\nat an item of interest, perhaps to read its cover notes and\nto “hear the song in my head”[5].\nThe use of visual surrogates for audio ﬁles in a col-\nlage also seems appropriate given evidence that appear-\nance plays a signiﬁcant role in the organization and dis-\nplay of personal music collections—the attractiveness of\nthe CD covers, the arrangement of CDs on shelves, and\nthegeneraleffectachievedbyprominentlypositioningthe\ncollection as the centrepiece of a room [4]. Collaging is\ndesigned to support the display of documents in an inter-\nesting and attractive manner, with the added beneﬁt that\nthe juxtaposition of documents in a novel and serendip-\nitous process may create compositions that the browser\nﬁnds meaningful [11].\n3. PROTOTYPE\nForpedagogicalpurposesaGreenstonedigitallibrarycol-\nlection was built based on a user’s (modest) personal col-\nlection of 80 MP3 ﬁles. The result can be seen in Fig-\nures 1 through 4. In addition to the commonplace brows-\ning structures—such as songs arranged alphabetically by\nTitle or grouped Artist, for instance—the music digital li-\nbrary can provide additional functionality and form. In\na moment we will look at one of the more novel ideas,\ncollaging tailored to music; here we will brieﬂy mention\nsome more prosaic features that come from the existing\ntextual support within the digital library toolkit.\nFigure 1 indeed shows the familiar “browse by artist”\nlisting. Next to the bookshelf icon in the ﬁgure is dis-\nplayedinbracketsthenumberofsongsbythatartist,which\nifclickeduponopensuptoshowthelist. Furtherclickingon a particular item will result in the song being played.\nThe ability to search document content is one of the\npillarsuponwhichdigitallibrariesarebased,andalthough\nGreenstone does not currently include support for direct\ncontent searching of raw audio ﬁles,1in this context the\nmetadatastoredinID3tagsmakesforausefulfoundation\nupon which to base textual queries. The digital library\ntoolkit is ﬂexible over what indexes can be formed: for\nthis collection it was decided to index titles, artist, and\na conglomeration of all text ﬁelds extracted from a ﬁle\n(akintofull-textsearch). Fieldsearchingisalsosupported\nthrough an advanced search page.\nDependingontheirsource,notallMP3ﬁleshavetheir\nmetadata ﬁelds ﬁlled out appropriately. Quality control\nis a hallmark of digital libraries that distinguishes them\nfrom the web at large. To help with selection and main-\ntenanceofdocumentsinadigitallibrary,Greenstonepro-\nvides a graphical interface for adding and editing existing\nmetadata,alongwithmanagingotherworkﬂowaspectsof\na digital library lifecycle [1]. Using the graphical tool a\nuser can shape their music digital library to their taste.\nMetadata need not be restricted to the ﬁelds present in\nthe MP3 ﬁles. A system of metadata sets is supported\nthat allows the use of existing standards, such as Dublin\nCore, as well as ad hoc sets developed by a user or com-\nmunity of users. Through the augmentation of metadata\nﬁelds,richersearchingandbrowsingstructurescanbede-\nveloped, for instance yielding a subject hierarchy for the\ncollection.\nFigure2showsamorenovelformofbrowsingthecol-\nlection. During the building phase of the digital library,\ntitle and artist information contained in each MP3 ﬁle has\nbeen used to locate two images deemed to be relevant to\nthemusicalwork. Collectively,theseformthebasisofthe\ncollage. Inbroadbrushstrokes,oneimageatatimeisran-\ndomly taken from the set and placed at a random location\nwithinthevisualisation. Astimepasses,theimagebegins\ntofadeuntil,eventually,itcannolongerbeseen,atwhich\npoint it is removed from the visualisation and added back\nin to the set of possible choices. In parallel with this, new\nimages are being added at regular intervals. The selec-\ntionmethodofacquiringimagesisheuristicbased,which\nwe explain more fully in the next section. Album covers,\nphotographs of the the actual vinyl disc—some younger\nusersofthedigitallibrarycollectionmayneedthisformat\nexplained!—and publicity shots of artists are most com-\nmon.\nIn Figure 2 the most prominent images in the collage\nare two record covers: The Beatles/1967–1970 (colloqui-\nally known as the blue album) and in front of that the sin-\ngle’s cover for Another one bites the dust by Queen; to\nthe right of these there is also a picture of David Bowie\nlive in concert, although this is not so clearly reproduced\nas a monochrome ﬁgure. If the user clicks on a particular\nimage,thenanewbrowserwindowopensup,asisshown\nin Figure 3 where the aforementioned David Bowie pic-\n1Greenstone does however support direct content querying of sym-\nbolic music.Figure 1. Browsing collection by artist\nFigure 2. Collaging in action.\nturehasbeentargeted. Thenewwindowisametaphorfor\nthe selected record, and shows the two images associated\nwith the track (fashioned on the idea of opening up the\nCD cover) while the audio is played through an embed-\nded MP3 applet.\nAdditional resources relating to the musical track are\nhyperlinked through this page. If the user clicks on either\nof the images, they are taken to the web where the im-\nage was sourced from. In Figure 4 the user has clicked\non the right-hand image of the David Bowie “record” and\nbrought up series of interviews with the artist. In the case\nofTakeachanceonme byABBA,totakeanotherrandom\nexample,theimagestakeyoutoadiscographysiteforthe\nband. Thetitleofthesong,displayedbelowtheimages,is\nalso hyperlinked and is used to automatically formulate a\nquery on Google Images to ﬁnd, with a certain degree of\naccuracy, related items. There is also a small embedded\nJava applet that plays the track, primed to start once the\npage loads and to stream the audio if possible. Follow-ingthatthereisanMP3logothatlinkstotheoriginalﬁle.\nThisallowsausertodownloadtheﬁleiftheywishorplay\nit using a different application. This last detail in itself is\nnot especially astounding, but does help emphasize that\ndigital libraries need to integrate with other aspects of the\nuser’s computer environment.\n4. IMPLEMENTATION\nForrapidprototyping,Greenstone—aversatileopensource\ndigital library toolkit that the authors have considerable\nexperiencewith—waschosenasanimplementationframe-\nwork [16]. The toolkit incorporates a ﬂexible architecture\nfordigestingnewdocumentformatsandcustomisinghow\nthey are indexed, browsed and presented, making it ideal\nforourneeds[17]. Thearchitecturehasfourmaincompo-\nnents: 1) an internal XML document format to which all\ndocuments are converted when they are imported into the\nsystem (native media such as an MPEG video is boundFigure 3. Viewing the work: two images, title and the\naudio playing\nto the format through a linking mechanism); 2) a set of\nparsers called plugins that process document and meta-\ndataformats;3)asetofmodulescalledclassiﬁersthatare\nusedtobuildbrowsingstructures;and4)aschemeforde-\nsigning individual digital library collections by providing\na conﬁguration ﬁle that speciﬁes what kind of documents\nand metadata they are to contain, and what searching and\nbrowsing facilities they provide.\nTo develop the music based prototype, a plugin for the\nMP3 format was needed; its primary purpose to bind the\nsource MP3 ﬁle and digest the metadata stored in its ID3\ntags into the XML representation. To support the collag-\ning metaphor, a new classiﬁer module was also required\nalong with an applet to drive the visualisation. We de-\nscribe each of these in turn.\n4.1. An MP3 Plugin\nPluginsinGreenstonearewritteninPerl,andareaccessed\nwhen a digital library collection is built. A substantial ar-\nrayofpluginsalreadyexists,processingformatsasvaried\nas HTML, Word, PowerPoint, PDF, Images, Email mes-\nsages,ExcelspreadsheetsandMARCdatatonameafew.\nThere is also a plugin known as the UnknownPlug which\nallowsadigitallibrarydesignertodigest,inarudimentary\nfashion, documents of a type that do not have a speciﬁcly\ntargeted plugin by setting basic metadata such as the doc-\nument’s ﬁlename and ﬁle size, and by binding the native\nﬁle into the XML format.\nUnknownPlug was the starting point for writing the\nMP3 plugin, which—inheriting from this plugin—is es-\nsentially a tailored version of this, where more detailed\nmetadata information, such as song title and artist, is ex-\ntracted from the ﬁle. The lion’s share of this work, in\nturn, is implemented by the MP3::Info package, avail-\nable through the Comprehensive Perl Archive Network\n(www.cpan.org). Retrieving an associate array of values\nthrough this package, each entry is added into the Green-\nstone system as metadata bearing the same tag label, pre-\nﬁxed by ‘mp3’ as an XML namespace.\nWith an eye on supporting collaging at runtime within\nthe digital library, an additional routine was added to the\nFigure 4. Original web page from which second David\nBowie image sourced.\npluginthatusedthetitleandartistinformationtoperform\na search on Google Images. To be precise, two exact\nphrasesareformedbasedontheﬁle’sartistandtitlemeta-\ndata and a search, restricted to JPEG ﬁles only, is issued\nto Google Images. If for some reason the ﬁle is missing\neither of these ﬁelds, an alternative search is generated\nbasedontheaudiotrack’sﬁlename,whichduetoinformal\nconventions that have grown up around raw audio ﬁles,\noftenencodessimilartitleandartistinformation. Elemen-\ntary manipulation of the ﬁlename is performed, such as\nconverting underscores ( ) to spaces which are often used\nas a substitute in ﬁlenames.\nFortheresultingimagesidentiﬁedbytheGooglesearch,\nthe ﬁrst two valid matches are mirrored into the collec-\ntion’s document import area. Since the web is inherently\na noisy medium, even if the matching image is available,\nit is sometimes the case that it is still not a valid JPEG\nimage. To help spot such anachronisms, we use identify,\na command-line utility included in ImageMagick’s suite\nof open source image manipulation tools (see imagemag-\nick.org for more details). The necessary Perl code to im-\nplementthistotalledaround300lines. Similarworkcould\nbedoneforcomparableaudioformatssuchasOggVorbis\nand Windows Media Audio (WMA).\n4.2. A Collaging Classiﬁer\nTodevelopthecollagingfacilitywithinGreenstoneamix-\nture of Perl and Java code was needed. The Perl code\nimplementedanewclassiﬁerthatlinkeddocumentidenti-\nﬁers of musical works in the digital library to the associ-\natedimageslocatedthroughGoogleImages. Thiswasnot\nanundulycomplextaskasmuchoftherequiredfunction-\nality resides in a base class for classiﬁers. Roughly 200\nlines of code was needed to develop the necessary struc-\nture. Care was given to capping the number of document\nidentiﬁers clustered together as one block, for runtime ef-\nﬁciency. If the blocks get too large, then there is a no-ticeable delay detectable in the user interface between the\ninformationbeingretrievedandtheresultingimagesbeing\ndisplayed.\nThe Java code took the form of an applet to provide,\nwithin the context of a web-browser based digital library,\nthe necessary ﬁne-grained interaction. The applet was\nmulti-threaded,withoneprinciplethreadaccessingthein-\nformation stored by the Collage classiﬁer: once furnished\nwith a document’s identiﬁer, the applet can make subse-\nquent calls to access metadata and associated ﬁles. Since\nthe images associated with each MP3 ﬁle reside on the\nGreenstone server, these need to be downloaded locally\nby the applet for use in the visualisation. This too took\nplace in a thread. Finally, a third thread was used to man-\nage the graphic canvas in which the collage visualisation\nwas presented, set to refresh at regular intervals.\nAlpha values—a technique in graphics to control the\ntransparency of pixels within an image—was used to im-\nplement the fading of images over time. Earlier we de-\nscribed the placement of images as random. This is an\nover-simpliﬁcation. To make better use of the real es-\ntate available within the visualization, a data-structure is\nmaintained recording the areas currently occupied by the\nimages in the display and how faded they are. When se-\nlecting a new “random” position to place a fresh image,\nseveral random positions are calculated and the one that\nresides in the most dormant area is chosen.\nQueues are kept of the downloaded and currently dis-\nplayed images. Parameters to the applet control aspects\nsuchasthedimensionsandbackgroundcolourofthecan-\nvas, and for conservation of memory (if desired), how\nlarge the various queues can grow. Compared with the\ncode written for the new plugin and classiﬁer, the applet\nwasamoresubstantial—butstillmodestinthelargerscheme\nof things—undertaking requiring 2,500 lines of code.\n4.3. Runtime\nThe ﬁnal component for developing the prototype within\nGreenstone was to specify how “the works” (our MP3\nﬁles) were to be displayed. This is an aspect of digital\nlibrary design that varies greatly depending on the types\nof documents contained in a collection. Control of this in\nGreenstoneisexertedthroughformatstatements,amech-\nanismforcustomising—onacollectionbycollectionbasis—\npresentation and functionality.\nFor the display of MP3 target documents, it was de-\ncided to rekindle the notion of a person perusing the al-\nbum cover sleeve while listening to the music, albeit in a\nsimplistic form. We have already seen the result of this in\nFigure 3. It was accomplished by writing a format state-\nment that sets out a table in HTML with the two sample\nimages (which, incidently, are stored as metadata for the\ndocumentbytheMP3pluginatbuild-time)intheﬁrstrow,\nand the artist, title (more metadata) and embedded MP3\nJava applet in the second.\nAdditional syntax was included to hyperlink images\nandtitleinformationtoexternalresourcesontheweb. The\nintention here is to engage the listener in related material.An anecdotal example of this in use was with one user\nwho, on test-driving the prototype, remarked they’d al-\nways wondered if a particular song by The Beatles was I\nwantto hold your hand orIwannahold your hand . On\nseeing the two sample images displayed while the song\nwas playing, one of which was the single’s cover, they\nwere instantly able to satisfy their curiosity. Intrigued,\nthey then clicked on the imageto explore further.\n4.4. Integration\nThemajorityofthecomponentswrittenfortheprototype—\nthe plugin, classiﬁer, and format statements—are natu-\nrally drawn into Greenstone through a collection’s con-\nﬁguration ﬁle. To operate with the extended functionality\nit is merely a matter of ensuring the modules of code are\nplaced in the correct directories.\nIntegratingthecollagingappletintotheGreenstoneen-\nvironment required the editing of macro ﬁles. This is a\nmechanism within the toolkit that blends a digital library\nsite’slookandfeelwiththespeciﬁcationoffunctionalas-\npects of the web page based interface, to precisely cater\nfor this sort of extension. Macro ﬁles typically set the\nglobal form within the digital library; the aforementioned\nformat statements override this for more local customisa-\ntions.\nWith this done a user can create a new collection, set\nthe collection conﬁguration ﬁle to use the specialist plu-\ngin and classiﬁer, populate it with MP3 ﬁles and issue the\nbuild command. Greenstone even provides a “base this\ncollectionon”featurethatmeansthatonceabaselinecol-\nlectionofthistypehasbeenestablished,futurecollections\ncaninheritthereconﬁgurationsettingsfromthis. Thenthe\ntaskbecomes: startanewcollection,populateitwithdoc-\numents, and then build.\n5. CONCLUSION\nThe development of novel browsing tools in music dig-\nital libraries represents a relatively overlooked research\ndomain. Empirical data suggest that browsing should be\nseen as a fundamental aspect of music information re-\ntrievalandmusicdigitallibraryuse. Toaddressthisshort-\ncoming,wehavedevelopedandpresentedaproof-of-concept\nprototypethatappliescollaging,anovelformofdigitalli-\nbrary browsing, to musical data.\nThere are many directions in which to extend and de-\nvelop the work described here. For example, we plan to\nexplore the utility of affording users the ability to “drag\nand drop” from the presented images to create a user-\ndeﬁnedplaylistinfutureversionsofthistool. Theplaylists\ncould actually form the bases for user-personalised sub-\ncollections or mini-libraries drawn from larger, more het-\nerogeneous music digital libraries.\nBeyond browsing, but closely related to the person-\nalization functions mentioned above, the music collag-\ning technique can be used in conjunction with traditional\nsearch tools. Under this scenario, traditional search toolscould be enhanced to allow users to conduct deliberately\n“broad” or “fuzzy” searches (i.e., music from the Haight-\nAshbury period, horror ﬁlm music, etc.). The search re-\nsults from these broad sets would then be presented via\nthe music collaging technique for “laid back” perusal by\nthe user.\nWe also envision implementations of the music col-\nlaging system that are presented on large, touch-sensitive\nscreens located in record stores and music libraries. On\ntheoppositeendofthespectrum,themusiccollagingtech-\nnique could equally run as a Personal Digital Assistant\n(PDA) enhancement for users to access their MP3 collec-\ntions.\n6. REFERENCES\n[1] D. Bainbridge, J. Thompson, and I. H. Witten. As-\nsembling and enriching digital library collections.\nInProc. of the Third ACM and IEEE Joint Confer-\nence on Digital Libraries , pages 323–334, Houston,\nTexas, 2003.\n[2] M. Chang and J. Leggett. Collection understanding\nthrough streaming collage. In Proc. of the Informa-\ntion Visualization Interfaces for Retrieval and Anal-\nysis (IVARA) Workshop, associated with the Joint\nConference on Digital Libraries , Houston, Texas,\nUSA, May 2003.\n[3] H. Chen, A. Houston, R. Sewell, and B. Schatz. In-\nternet browsing and searching: User evaluations of\ncategory map and concept space techniques. Jour-\nnaloftheAmericanSocietyforInformationScience ,\n49(7):582–603, 1998.\n[4] S.Cunningham,M.Jones,andS.Jones. Organizing\nmusicforuse: personalmusiccollections. Technical\nreport,DepartmentofComputerScience,University\nof Waikato, Hamilton, New Zealand, 2004.\n[5] S. Cunningham, N. Reeves, and M. Britland. An\nethnographic study of music information seeking:\nimplicationsforthedesignofamusicdigitallibrary.\nInProc.oftheJointConferenceonDigitalLibraries ,\npages 5–16, Houston, Texas, USA, May 2003.\n[6] M.Jones,P.Jain,G.Buchanan,andG.Marsden. Us-\ningamobiledevicetovarythepaceofsearch. In In-\nternational Symposium on Mobile HCI , pages 8–11,\nUdine, Italy, September 2003.\n[7] S.Jones,S.Cunningham,R.McNab,andS.Boddie.\nA transaction log analysis of a digital library. Inter-\nnationalJournalonDigitalLibraries ,3(2):152–169,\n2000.\n[8] U. Karadkar, A. Kerne, R. Furuta, L. Francisco-\nRevilla, F. Shipman, and J. Wang. Connecting in-\nterface metaphors to support creation of hyperme-\ndia collections. In Proc. European Conf Digital Li-\nbraries, pages 338–349, 2003.[9] A. Kerne. CollageMachine: an interactive agent\nof web recombination. Leonardo , 33(5):347–350,\n2000.\n[10] A. Kerne, M. Khandelwal, and V. Sundaram. Pub-\nlishing evolving metadocuments on the web. In\nProc. of the 14th ACM Conference on Hypertext\nand Hypermedia , pages 104–105, Nottingham, UK,\n2003.\n[11] A. Kerne and V. Sundaram. A recombinant infor-\nmation space. In Proc. Computational Semiotics\nin Games and New Media (CoSIGN) , pages 48–57,\n2003.\n[12] J. R. McPherson and D. Bainbridge. Usage of the\nMELDEX digital music library. In Proc. of the Sec-\nond Annual International Symposium on Music In-\nformation Retrieval: ISMIR , pages 19–20, 2001.\n[13] J. Preece, Y. Rogers, H. Sharp, D. Benyon, S. Hol-\nland, and T. Carey. Human-Computer Interaction .\nAddison-Wesley, Wokingham, UK, 1994.\n[14] A. Rauber, E. Pampalk, and D. Merkl. Using\npsycho-acousticmodelsandself-organizingmapsto\ncreate a hierarchical structuring of music by sound\nsimilarity. In Proc. of the Third International Con-\nference on Music Information Retrieval: ISMIR ,\npages 71–80, 2002.\n[15] G.Tzanetakis,G.Essl,andP.Cook. Automaticmu-\nsicalgenreclassiﬁcationofaudiosignals. In Proc.of\ntheSecondAnnualInternationalSymposiumonMu-\nsic Information Retrieval: ISMIR , pages 205–210,\n2001.\n[16] I.H.WittenandD.Bainbridge. Howtobuildadigi-\ntallibrary . MorganKaufmann,SanFrancisco,2003.\n[17] I. H. Witten, D. Bainbridge, G. W. Paynter, and S. J.\nBoddie. Importing documents and metadata into\ndigital libraries: Requirements analysis and an ex-\ntensible architecture. In Proceedings of the Euro-\npean Conference on Digital Libraries , pages 390–\n405, Sept. 2002."
    },
    {
        "title": "Classification of musical genre: a machine learning approach.",
        "author": [
            "Roberto Basili 0001",
            "Alfredo Serafini",
            "Armando Stellato"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.6365709",
        "url": "https://doi.org/10.5281/zenodo.6365709",
        "ee": "http://ismir2004.ismir.net/proceedings/p092-page-505-paper239.pdf",
        "abstract": "This record contains raw data related to article  Distinct responses of newly identified monocyte subsets to advanced gastrointestinal cancer and COVID-19\n\nMonocytes are critical cells of the immune system but their role as effectors is relatively poorly understood, as they have long been considered only as precursors of tissue macrophages or dendritic cells. Moreover, it is known that this cell type is heterogeneous, but our understanding of this aspect is limited to the broad classification in classical/intermediate/non-classical monocytes, commonly based on their expression of only two markers, i.e. CD14 and CD16. We deeply dissected the heterogeneity of human circulating monocytes in healthy donors by transcriptomic analysis at single-cell level and identified 9 distinct monocyte populations characterized each by a profile suggestive of specialized functions. The classical monocyte subset in fact included five distinct populations, each enriched for transcriptomic gene sets related to either inflammatory, neutrophil-like, interferon-related, and platelet-related pathways. Non-classical monocytes included two distinct populations, one of which marked specifically by elevated expression levels of complement components. Intermediate monocytes were not further divided in our analysis and were characterized by high levels of human leukocyte antigen (HLA) genes. Finally, we identified one cluster included in both classical and non-classical monocytes, characterized by a strong cytotoxic signature. These findings provided the rationale to exploit the relevance of newly identified monocyte populations in disease evolution. A machine learning approach was developed and applied to two single-cell transcriptome public datasets, from gastrointestinal cancer and Coronavirus disease 2019 (COVID-19) patients. The dissection of these datasets through our classification revealed that patients with advanced cancers showed a selective increase in monocytes enriched in platelet-related pathways. Of note, the signature associated with this population correlated with worse prognosis in gastric cancer patients. Conversely, after immunotherapy, the most activated population was composed of interferon-related monocytes, consistent with an upregulation in interferon-related genes in responder patients compared to non-responders. In COVID-19 patients we confirmed a global activated phenotype of the entire monocyte compartment, but our classification revealed that only cytotoxic monocytes are expanded during the disease progression. Collectively, this study unravels an unexpected complexity among human circulating monocytes and highlights the existence of specialized populations differently engaged depending on the pathological context.",
        "zenodo_id": 6365709,
        "dblp_key": "conf/ismir/BasiliSS04",
        "keywords": [
            "monocytes",
            "immune system",
            "precursors",
            "tissue macrophages",
            "dendritic cells",
            "heterogeneity",
            "transcriptomic analysis",
            "single-cell level",
            "distinct monocyte populations",
            "classical monocytes"
        ]
    },
    {
        "title": "Towards a Socio-cultural Compatibility of MIR Systems.",
        "author": [
            "Stephan Baumann 0001",
            "Tim Pohle",
            "Shankar Vembu"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417733",
        "url": "https://doi.org/10.5281/zenodo.1417733",
        "ee": "https://zenodo.org/records/1417733/files/BaumannPV04.pdf",
        "abstract": "Future MIR systems will be of great use and pleasure for potential users. If researchers have a clear picture about their “customers” in mind they can aim at building and evaluating their systems exactly inside the different socio-cultural environments of such music listeners. Since music is in most cases embedded into a socio-cultural process we propose especially to evaluate MIR applications outside the lab during daily activities. For this purpose we designed a mobile music recommendation system relying on a trimodal music similarity metric, which allows for subjective on-the-fly adjustments of recommendations. It offers online access to large-scale metadata repositories as well as an audio database containing 1000 songs. We did first small- scale evaluations of this approach and came to interesting results regarding the perception of song similarity concerning the relations between sound, cultural issues and lyrics. Our paper will also give insights to the three different underlying approaches for song similarity computation (sound, cultural issues, lyrics), focusing in detail on a novel clustering of album reviews as found at online music retailers. Keywords: Socio-cultural issues in MIR, multimodal song similarity, ecological validation.",
        "zenodo_id": 1417733,
        "dblp_key": "conf/ismir/BaumannPV04",
        "keywords": [
            "Future MIR systems",
            "music listeners",
            "socio-cultural environments",
            "mobile music recommendation",
            "trimodal music similarity",
            "daily activities",
            "large-scale metadata repositories",
            "audio database",
            "online access",
            "song similarity computation"
        ],
        "content": "TOWARDS A SOCIO-CULTURAL COMPATIBILITY OF \nMIR SYSTEMS\nStephan Baumann Tim Pohle Vembu Shankar \nGerm an Research Center for \nArtif icial Intelligence \nErwin Schrödinger Str. \n67663 Kaiserslautern \nGerm any Germ an Research Center for \nArtif icial Intelligence \nErwin Schrödinger Str. \n67663 Kaiserslautern \nGerm any Technical University of \nHam burg  \n \n21071 Ham burg \nGerm any \nABSTRACT \nFuture MIR system s will be of great u se and pleasure \nfor potential users. If researchers have a clear picture \nabout  their “cust omers” i n mind t hey can ai m at \nbuilding and eval uating their systems exact ly inside the \ndifferent  socio-cul tural envi ronm ents of such m usic \nlisteners. Since m usic is in m ost cases em bedded into a \nsocio-cul tural process we propose especi ally to eval uate \nMIR ap plicatio ns outside the lab  during daily activ ities. \nFor t his purpose we desi gned a mobile music \nrecom mendation sy stem relying on a trimodal music \nsimilarity m etric, wh ich allows fo r subjectiv e on-the-fly \nadjustm ents of recom mendations. It offers online access \nto large-scale m etadata repositories as well as an audio \ndatabase containing 1000 songs. W e did first small-\nscale evaluations of this approach and came to \ninteresting resul ts regardi ng t he percept ion of song \nsimilarity concerni ng t he rel ations bet ween sound, \ncultural issues an d lyrics. Ou r paper will also  give \ninsights to the three di fferent  underl ying approaches for \nsong similarity computation (sound, cul tural issues, \nlyrics), focusi ng in detail on a novel  clustering of al bum \nreviews as found at  online music retailers. \nKeywords: Socio-cultural i ssues in MIR, multimodal \nsong si milarity, ecol ogical validation. \n1. INTRODUCTION \nWe propose a socio-cultura l com patibility of MIR \nsystems and achi eved prom ising resul ts by eval uating \nsuch an applicatio n in the field of m obile music \nrecom mendations. W e included t he following aspect s: \n1. The m usical work of ar tists is ex amined from the \nperspect ive of a m usic-consum ing soci ety. \n2. Optionally, users m ay add personal  inform ation \nabout  age, gender, m usical educat ion, personal  \ntaste whi ch refl ects bel onging t o soci al peer \ngroups. \n3. Subject ive music-listening behavi or in soci o-\ncultural environm ents is collected and evaluated \nwith an ecological approach. \n4. Long-t erm observat ions are undert aken usi ng a \nplugin for W inamp MP3 soft ware. 5.  Aspects o f the artist’s creativ e intention being \npartially represent ed in sound, orchest ration, \nproduct ion environm ent, selection of si nger and \nlyrics are covered by  audi o anal ysis and \ninform ation ret rieval methods. \nWe are well aware of the f act that such a holistic \napproach needs for a si gnificant amount of research. \nNevert heless ot her aut hors [1]  have proposed similar \napproaches emphasi zing the soci o-cul tural dimension. \nOur activ ities an d the presented paper focus on the \naspect s (1) and (3) (i n cont rast to our previ ous \npublication [2]  whi ch included no det ails about  the \nclustering t echni ques).  Point (5) i s descri bed very \nshallow and (2), (4) are considered in future work.  \n \n \n \n \n \n \n \nFigu re 1. Ecological evaluation. \n2. RELATED WORK \nOur research asks how we m ight add t o our \nunderst anding of percept ion of m usic similarity through \nan ‘ecol ogical’ approach. Thi s means st udying how \npeople perceiv e music similarity in  their n ormal lives \nbeyond t he artificial worl d of l ab-based experi ments. To \nthis end we want  to find new way s of observi ng users’ \ninteraction wi th our sy stems as they go about  their \neveryday activities. Cogn ition in the wild means \nstudying cognitive phenomena in the natural co ntexts in \nwhich they occur. Th is ap proach relates to  the insight \nthat what  peopl e do i n labs m ay not be ‘ecol ogically \nvalid’: experi mental resul ts may be art efacts of t he lab \nsituation, failing to represent  peopl e’s behavi our i n the \n‘ecologies’ o f their n ormal lives. W hile th e lab-based \napproach can t ell us about  percept ion of m usic \nsimilarity [3], we feel  it is also important to look bey ond \nthe lab and its artificial ex perimental setu ps, to music \nusers’ spont aneous percept ion of m usic similarity in real \nsituations as part  of their everyday lives. This ecological \napproach m ight reveal , for exam ple, how percept ion \nchanges with  time, locatio n, or activ ity, in  ways, wh ich Permission to make digital or  hard copies of all or  part of this wor k \nfor personal or  classr oom use is granted without fee pr ovided that \ncopies ar e not m ade or  distr ibuted for profit or commercial \nadvantage and that copies bear  this notice and the full citation on \nthe first page. \n© 2004 Univer sitat Pom peu Fabr a.   \n \ncould have implications for how systems generate \nrecommendations. For this purpose we designed a \nmobile music recommendation system relying on a \ntrimodal music similarity metric, which allows for \nsubjective adjustments on the fly. We did first small-\nscale evaluations of this approach and came to interesting results regarding the perception of song \nsimilarity concerning the relations between sound, \ncultural issues and lyrics . We will introduce this \nmultimodal similarity metric in the following two sections and present the results in section 5. \n3. MULTIMODAL SONG SIMILARITY \nOur multimodal song similarity measure is realized as a weighted linear combination of three different local \nsimilarity metrics, namely timbral similarity, similarity of lyrics and cultural similarity:  \nS = wso* Sso+ wly* Sly+ wst* Sst  (1)  \nThis section will give insights to the three different underlying approaches for similarity computation. \n3.1. Timbral similarity \nThe computation of timbral similarity has meanwhile a long tradition in the MIR community. Objective \nevaluations based on genre, artist and album metadata \nhave been performed and also being compared against \neach other by differe nt authors [4,5]. A basic finding is \nthat MFCCs and a GMM or k-means clustering and \nEarth Moving Distance behave pretty well for \npredicting similar sounding songs for given anchor \nsongs.  \n \n# of \nNeighbours #of songs \nin the \nsame \nalbum #of songs \nof the \nsame artist # of songs \nin the \nsame \ngenre \n1 0,30 0,41 0,45 \n3 0,75 0,99 1,17 \n5 1,05 1,40 1,78 \nTable 1 . Experimental results of our best timbral \noperator: 13 MFCCs, 16 clus ters, EMD-KL (see [4]). \nWe added to our own operator bank recently an approach relying on ICA of spectral features to see if \nwe could increase previous performance. We could not \nachieve better results acco rding to the objective \nevaluation. There seems to be an upper limit which was reported also in a recent eval uation by Aucouturier [6].  \n3.2. Cultural similarity \nThe processing of cultural descriptions in the context of MIR systems has been introduced by [7]. In our \nprevious work we implemented a minor improvement of \nthis original work to generate artist recommendations \n[2]. From our findings we decided to expand this idea \nby accessing album reviews fro m the Amazon web site  \nand apply a clustering instead of using a simple vector \nspace model for similarity computation between artists. \nThe basic idea of our approach is to spatially organize these reviews that are in th e form of textual documents \nusing an unsupervised learning algorithm called Self-Organizing Maps (SOM) [8] and thus be able to give \nrecommendations for similar artists by making use of \nthe model built by the algorithm. The use of SOMs in \nthe field of text mining and for audio-based MIR \napplications is well understood [9]. The SOM is an unsupervised learning algorithm used to visualize and \ninterpret large high-dimensional data sets. The map \nconsists of a regular grid of processing units called \n“neurons”. Each unit is associ ated with a model of some \nhigh dimensional observation represented by a feature vector. The map attempts to represent all the available \nobservations with optimal accuracy using a restricted set \nof models. Map units that lie nearby on the grid are \ncalled neighbours.  After the formation of a map for a \nparticular data set, the model vectors are arranged in such a manner that nearby map units represents similar \nkind of data and distant map units represent different kinds of data. The literature on Information Retrieval \nprovides techniques to pre-process and represent textual \ndocuments for mining operations. The documents are \nthen represented in the form of a bag-of-words where \neach document is considered as  a point (or vector) in an \nn-dimensional Euclidean space where each dimension \ncorresponds to a word (term) of the vocabulary. The i\nth \ncomponent d i of the document vector expresses the \nnumber of times the word with index i occurs in the \ndocument, or a function of it. Furthermore, each word \ncan be assigned a weight signifying its importance. Commonly used weighting strategy is the tf * idf (term \nfrequency – inverted documen t frequency) scheme, e.g. \nby a variant such as \nwij =  tf ij * idf i  =  tfij * log 2 (N/ df i)  (2) \nwhere \nwij is the tf-idf weight for the ith word in jth document \nin a collection of N documents,  \ntfij is the term frequency of the ith word in the jth \ndocument and  \nidfi = log2 (N/df i) is the inverse document frequency of \nthe ith word over the entire collection. \nThe tf * idf weighting sche me described above does not \ntake into consideration any domain knowledge to determine the importance of a word. But when trying to \nfind similarities between two documents in a musical \ncontext, it is desirable to exploit any domain knowledge \nthat is inherently present in the documents. We propose one such mechanism to accomplish this by introducing \nthe concept of a modified weighting scheme in the \nmusical domain or context. Therefore, in addition to the \nweighting importance given to a word by the tf *idf \nscheme, it would be worthwhile to increase the weight of a word by a certain factor if it is pertaining to the \nmusical domain. We came up with such a word list of \n324 from the genre taxonomies of the All Music Guide . \nThe modified weighting scheme gives rise to a new weight for musical words that is given by   \n \n \nwmij =  tfmij *  idfmi  =  tfmij * log2 (N/dfmi)  * α (3) \nwhere t he superscri pt m indicates words bel onging to \nthe m usical context and α is th e weig hting increm ent. \nThe pre-processed t extual docum ents represent ed in the \nform  of n-di mensional vect ors can be used to train a \nSOM  in an unsupervi sed way . The learning starts with a \nset of reference vectors also called the model vectors \nthat are th e actu al map units o f the network. As the \nlearning proceeds, the m odel vectors gradually change \nor arrange t hemselves so as t o approxi mate the input \ndata space. The final arrangem ent is such that the model \nvectors that are nearby are similar to each other. The \nmodel vect ors are usual ly const rained t o a two-\ndimensional regul ar gri d, and by  virtue of t he learni ng \nalgorithm, follow the distribution of the dat a in a non-\nlinear fashion. The model vectors are fitted  using a \nsequent ial regressi on process. Gi ven a sample vector \nx(t) at iteratio n step t the model vector mi(t) with index i \nis adapted as fo llows:  \n \nmi(t + 1) = m i(t) + h c(x),i(t)[x(t) - m i(t)]  (4) \n \nwhere the index of the “winner” m odel, c for the current \nsample is id entified  by the condition, \n \n,i∀ ||x(t) – m c(t)|| ≤ ||x(t) – m i(t)||  (5) \n \nhc(x),i(t) is called  the neighborhood f unction, which acts \nas a smoothing kernel  over t he gri d, cent ered at  the \n“winner” m odel mc(t) of the current data sam ple. The \nneighborhood funct ion is a decreasi ng funct ion of t he \ndistance between the ith and cth nodes on t he map gri d. \nThe regressio n is usually reiterated  over all th e availab le \nsamples. Thus, with this unsupervi sed learni ng al gorithm \nwe can spatially arran ge all th e documents i.e. th e alb um \nreviews of al l the art ists, resul ting i n a t opological \nordering of the artists. In  addition to this, the SOM \nalgorithm also obtains a clustering of t he dat a onto the \nmodel vectors wherein the artis ts present in a particular \ncluster are sim ilar to each other. Labeling plays an \nimportant role in  the visualizatio n of the SOM. We \nemployed a si mple labeling techni que where a map unit \nis represent ed by a label or a key word t hat has a hi gher \nweight, as calculated by the tf * idf wei ghting schem e, \nwhen com pared to other word s that appear in the m ap \nunit. The modified wei ghting schem e descri bed i n the \nprevi ous sect ions al so ai ded i n labeling the map units. \nSince we increase the weight of  a word that pertains to \nthe musical context, many, if not all, o f the lab els that \nwe obtained were from the musical list o f words. Th is is \nindeed desirable when we ar e lab eling an SOM of artists \nas we would like to see labels that are m usical words \nlike rap, rock, met al, blues and not  plain Engl ish words. \nFigure 2 shows a few di stinct sect ions of the map with \ntheir resp ectiv e lab els. As  can been seen from  the results, we were ab le to obtain a clear categorization of \nartists based on di fferent  musical genres. \n      \n  \nFigu re 2. SOM in HTML format: 7x7 grid, highlighted \nis the rectangle with sim ilar artists of the unit labeled \nwith  blues,fire,clapton  [ 7th row,  5th column] . \nWe show i n the following two exam ples of the data feed \nfrom  Amazon. We used uppercase t o indicate words, \nwhich appear as l abels, and words i n bold to indicate the \noccurrence of the word in  the crisp feature set: \n \n[Artist: Eric Clapton Al bum: Unpl ugged]  \nClapton caught  the \"unpl ugged\" t rend j ust at the right \ntime, when the public was hungry t o hear how  well \nROC K stars and t heir mat erial can hol d up when \nstripped of elaborat e product ion val ues. C lapton hi mself \nseemed baffled by the phenom enon, especially when \npicking up t he arml oad of  Grammys Unpl ugged earned \nhim, including Record and Song of  the Year f or \"Tears \nin Heaven,\" t he heart -rendi ng el egy t o his young son, \nConor. That song and a rew orked versi on of  \"Layl a\" got  \nmost  of the attention, but  the rest  of the album has fine \nversi ons of  acoust ic BLUES numbers such as \"Mal ted \nMilk,\" \"Rollin'  & Tumblin' , and \"Before You Accuse Me\" \nthat make it wo rth investig ating further.  \n \n[Artist: Bob Dy lan Album: Highway  61 R evisited] \n****3/ 4 \"Hi ghway 61 Revi sited\" i s an amazi ngly \noriginal record w hich finds Bob Dylan movi ng \neffortlesly b etween  folk-  ROCK,  BLUES and f lat-out \ngarage ROC K. The songs are among t he best  and most  \nenerget ic in Dyl an's cat alogue, and t he band, w hich \nfeatures Mi chael  Bloomf ield, Al Kooper and BLUES  \ndrummer Sam Lay, careen t hrough t he classic  \"Like A   \n \nRolling St one\", t he acerbi c \"Bal lad Of  A Thin Man\", the \nstylish BLUES \"It  Takes A Lot  To Laugh, It  Takes A \nTrain To C ry\", and t he blistering \"Hi ghway 61\".One of \na handf ul of truly essent ial mid-60s ROC K records, and \none of  Bob Dyl an's very best  and most  cohesi ve al bums. \n \nThe resul ts of our experi ments were val idated usi ng \nwww.echocloud.net , a web-based art ist recom mendation \nengine. It works by  crawl ing peer-t o-peer net works \n(soulseek, gnutella ) to capture users’ file lists in  order to \ndiscover correlations between m usical artists. The \nsimilarity model uses a dat abase of around 120k art ists, \nwhich represent s kind of open w orld approach i n \ncontrast to our reduced t est set of 398 different  artists. \nWe com pared t he Top 10 recom mendations from  \nEchocl oud with our Top 10 recom mendations for al l the \nartists. We also com pared Echocl oud recom mendations \nwith the artists that are present in the 3 and 5 Best \nMatch ing Units (BMUs) of the artist in  question. A Best \nMatch ing Unit for an artist is th e SOM m ap unit that \nbest m odels the set of accord ing textual album  reviews. \nThe Euclidean distance m easur e is used i n finding the \nBMUs fo r an artist.  \n \n Total number of   \nrecom mendation \nmatches Average \nrecom mendation \nmatch in  \npercent age \nTop 10 482 /  3980 12.1 % \n3 BMUs 685 /  3980 17.2 % \n5 BMUs 982 /  3980 24.7 % \n \nTable 2.  Validations of our results for 398 artists with \nEchocloud‘s Top 10 recommendations without \nmodified weighting scheme  \n \n Total number of \nrecom mendation \nmatches Average \nrecom mendation \nmatch in  \npercent age \nTop 10 493 /  3980 12.4 % \n3 BMUs 785 /  3980 19.7 % \n5 BMUs 1038 /  3980 26.1 % \n \nTable 3.  Validations of our results for 398 artists with \nEchocloud‘s Top 10 recommendations with modified \nweighting scheme  \n    As can be seen from  the results in Tables 2 and 3, the \nquality of the recommendations increases b y using the \nmodified weighting schem e incorporat ing dom ain \nknowl edge. Nevert heless the perform ance gai n is rather \nmoderat e which coul d be an indication that we already  \nreached the upper bound of such kind of approaches. In \norder to use t his artist recom mendation approach as a \ncultural facet for the song-based sim ilarity engine, we \ndecided to im plem ent a si mple in feren ce step : the \nsimilarity values between  two art ists are propagat ed to each of their songs. In this way they are accessible for \nthe m ultimodal sim ilarity com putation at song level. In \nthe future we plan to cluster alb um reviews sep arately \nby the SOM to  deliver a m ore fin e-grained similarity \nmodel to propagat e song recom mendations. W e are wel l \naware of the fact that in the long-t erm we shoul d \nincorporat e an ont ology-based reasoni ng engi ne at  this \nstage. Th e explicit sem antic relatio nships between  the \nartist, album  and song as basic ontological entities and \ntheir diverse sub-cat egori es (e.g. “com pilations”, \n“concept album s”, “best of <artist, decade, \ngenre>al bums”) are rather di fficult to map ont o this \napproach.  \n3.3. Similarity of lyrics \nThe last asp ect o f our multimodal similarity engine \ncovers the aspect  of song l yrics. W e included t he sam e \napproach as present ed in our previ ous work [2] . It is \nbased on t he standard t f*idf wei ghting to represent  \nlyrics as docum ent vectors (see form ula 2) and the \ncosine metric to  perform similarity co mputation.  \n \nFigu re 3. Profiler  application.  \nFrom  the techni cal point of vi ew we used t his time a \ncommercial tool provi ding such st andard i nform ation \nretrieval functionalities via a JAVA program ming \ninterface. The out-of-the-shelf  application was used for \nfast subject ive eval uation duri ng implementation phase \n(see Figure 3 for an exam ple of some similar “dancy” \nlyrics to the query  song  dance t onight). \n4. SERVICE-ORIENTED ARCHITECTURE \nWe selected a service-orient ed architecture in order to \ncombine our internal services  and external services from  \nAmazon and Echocl oud. The al bum revi ews for art ists \ncan be accessed from  the Amazon site using the \nAmazon W eb Service interface that is available as a \nstandard devel opment kit. It support s either web servi ce \nSOAP m essages or usi ng XM L over HTTP. Echocl oud \noffers an XM L-based web service for artist \nrecom mendation, whi ch we al so included into our \nfram ework. In addition to the abovem entioned servi ces \nwe pl an to integrate furt her sy mbolic and semantic web-\nrelated  serv ices in  the near fu ture. By fo llowing this \nstrict architecture we are able to offer our own   \n \nmultimodal sim ilarity engine to be invoked from  \nexternal services via standa rd web servi ce descri ption. \n4.1. Wireless LAN for ecological eval uation \nFor an em beddi ng of t he eval uation into most natural \nmusic listening si tuations of every day peopl e we \ndecided t o equi p peopl e wi th a m obile devi ce bei ng \nconnected to the described server. Since several \nhotspots offering free wirele ss LAN access are available \nat our cam pus si te and i n the city we onl y had t o build a \nstandard browser-based appl ication t o set  up t he \nprerequi sites for t he ecological evaluation. \n4.2. MYMO: Mobile application \n \nThe central web site of our services has been optim ized \nfor sm all screen sizes of PDA devices.  \n \nFigu re 4. MYMO application \nIt is possible to search for artists, album s and song titles \nas well as accessing individual item s in a top-down \nselectio n mode. To allow the user interactiv e feed back in \nthe song recom mendation m ode, a vi rtual joystick was \nincluded that can be easily accessed using the pen of the \nPDA. The recom mendation engi ne uses t he song \nsimilarity m easure describe d above. The position of the \njoystick has a di rect influence on the individual weights \nin the linear combination. In this way th e user can  select \ndifferent  settings and fi nd hi s favori te com bination on \nthe fly. The l ogging at  the server allows for storage of \nthe individual interactio ns with  the device. \n \nFigu re 5. User logs.  5. ECOLOGICAL, SUBJECTIVE EVALUATION \n A group of 10 subject s refl ecting the current  \ndistribution of Int ernet users i n Germ any was selected \nby means of vary ing gender, age, educat ion, and \nmusical background. W e used a wi thin-subject s design \nwith two conditions: the lab, and ‘the wild’. In each \ncondition, each subject was asked to find the optim al \njoystick setting that would re turn an acceptable block of \n5 recom mendations for a gi ven anchor song. Thi s \nposition produces a part icular trimodal wei ghting. \nSubjects were instructed that  if th ey d id not like the \nresul ts of a gi ven wei ghting, t hey coul d change t hat \nweig hting immediately to  produce an  altern ative result. \nThey were also asked, if th ey lik ed the resu lts o f the \nweighting to select their favouri te recom mendation. If \nsubject s ended up fi nding not hing, they selected \nnothing. In order t o avoi d learni ng effect s different  sets \nof songs were used for each condition and the order of \npresent ation of condi tions was random ised. W e \ngathered quantitativ e and qualitativ e data. Quantitativ e \ndata was generat ed by  the logging m echani sm, which \ncollected  the joystick  settin gs that led  to the user-\nintended resu lts. Qu alitativ e d ata co nsisted  of \nobservat ions and i nterviews. \n5.1. Anchor songs and session statistics \n \nPeopl e had t o rate 10 anchor  songs and bl ocks of Top5 \nrecommendations in the lab  and in the “wild ”. By \nadjusting the joystick to different positions each subject \nrated at average 1000 recom mendations per sessi on. \n \nRock  Hom ebound Trai n \nGerm an Rock  Jetzt g eht s los \nFolk April Co me She Will \nHipHop Real Love \nGerm an HipHop Michi Beck In Hell \nSoul Caligula \nGerm an Soul  Aus der Dunkel heit \nNuSoul Guidance \nGerm an Pop  Mensch aus Glas \nFunk Eye To Ey e \nElectronica Frozen \nAcid Jazz Stay This W ay \nTable 4.  Anchor songs.  \n5.2. Quanti tative findings \n \nBy averaging the weightings of the different facets over \nall sessions we received the results shown in table 5. \nOur can didates seem  to rely m ost of the time on an \nequal ly rated mixture of sound and cul tural aspect s \n(0,41 and 0,36) whi le lyrics pl ay a m inor rol e (0,22).  \nThe l ab vs. „wi ld“ resul ts shows a decrease of t he sound \nfacet in the ecological envi ronm ent, m aybe because of \nthe noisy en vironment in the wild  settin gs (WLAN \npowered publ ic rest aurant s). The fi ndings were   \n \nstatistically significan t (with  erro r rate 0 .01 using a \npaired si gn test), but  indeed we have t o work on large-\nscale experim ents. \n \nALL Sound Style Lyrics \nAvg 0.41 0.36 0.22 \nLAB  Sound Style Lyrics \nAvg 0.44 0.34 0.21 \nWILD Sound Style Lyrics \nAvg 0.38 0.38 0.24 \nTable 5.  Experimental results of lab-based vs. \necological evaluation. \n5.3. Qual itative findings: typol ogy of users \n \nWithin our sm all group of subject s we could identify 3 \ndifferent  types of user by  averagi ng t heir joy stick \nsettings and perform ing post -experi ment interviews \nasking for t heir introspect ive view on t he experi ment.  \nType 1 users i gnored t he lyrics and showed a cl ear bi as \nto the cultural (style) facet, if they knew the \nrecom mended songs, t hey even di d not  listen to the \nproposed recom mendations. \n \n \n \n \n \nFigu re 6. Type1 us ers prefer “ cultural agreem ents”. \nType 2 users ap preciated  very m uch the capabilities of \nthe timbral sim ilarity an d made heavy use of the fact to \nfind songs soundi ng si milar. They  used the cultural and \nlyrics facet only to m ake sm all corrections. \n \n \n \n \nFigu re 7. Type2 us ers prefer “ what they  hear”. \nWe found one t ype 3 user who was i nterested in finding \nnew and unexpect ed things. He l oved t o be surpri sed by \nnew sound and cul tural unusual  material. Therefore he \nused the lyrics facet to explore the song searchspace \nmost of the time by this dimension.   \n \n \n \n \n \nFigu re 8 . Ty pe3 users prefer “to experiment with \nunusual facets, e.g. the ly rics”. \nAt this point it becom es obvi ous t hat a recom mendation \nengine does not  necessari ly have t o be bui ld on the \nnotion of “sim ilarity”! 6. CONCLUSION AND FUTURE WORK \nOur initial experiments have resu lted in previously \nunknown fi ndings about  individual and common ratings \nof song si milarity based on subject ive eval uation. W e \nhave shown that there are statistically significant \ndifferences bet ween l ab-based and ecol ogical \nvalidations. We have descri bed and val idated how t he \npercept ion of m usic as a soci o-cul tural product  can be \nsupport ed by  MIR technol ogy relying on web m ining. \nWe find surpri sing aspect s by expl oring interviews wi th \nthe subject s bei ng engaged i n the ecological \nexperi ments. As a consequent  next step we want  to see \nif we can fi nd rel ations bet ween users background \n(education, gender, age, etc. which we already \ncollected) and personal  preferences usi ng our dat a \ncollection from  the ecol ogical experi ment.  Addi tionally \nwe want to work on stable  wireless access settings (e.g. \nreplacing W ireless LAN by  UM TS) t o improve t he idea \nof ecological evaluation. W e will conduct further large-\nscale experi ments within these set -ups. Usi ng a plugin \nto W inamp will support the long-term  observation of \nuser preferences .  These efforts have recently been \nstarted . We will co llect d ata fro m users listen ing \nbehavi our for subsequent  data mining in order t o extract \nsequence st ructure for recom mending si milar songs. \nFinally we will o pen the implemented web-service \nfram ework for interested researchers in order to \nintegrate truly semantic serv ices b eing related  to music. \nWe believe that th e “so cio-cultural co mpatibility” is a \nfruitful perspective for futu re research in MIR.  \n7. REFERENCES  \n[1] Leman, M ., “Semantic Des criptions for Musical Audio-\nMining and Information Retrieval”, invited talk at CMMR \n2004, Esbjerg, Denmark, May , 2004 . \n[2] Baumann, S., Halloran, J ., “An ecological approach to \nmultimodal subjective m usic sim ilarity , Proc. of the First \nCIM 2004,  Graz, Austria, 2004. \n[3] Allam anche, E. et al., “ A multiple feature m odel for \nmusical sim ilarity  retrieval”, Proc. of the ISMIR 2003 , \nBaltim ore, USA, October, 2003. \n[4] Baumann, S., Pohle, T.,  “A Comparison of Music \nSimilarity  Measures for a P2P Application”, Proc. of the \n6th DAFX , London, UK, September 8-11, 2003. \n[5] Pampalk, E., Dixon, S., Widmer, G. “On the Evaluation \nof Perceptual S imilarity  Measures for Music”, Proc. of \nthe 6th DAFX-03 , London, UK, September 8-11, 2003. \n[6] Aucouturier, J.-J., Pachet, F., “Improving Timbre \nSimilarity : How high is the sky ?”. Journal of Negative \nResults in Speech and Audio Sciences, 1( 1), 2004. \n[7] Whitman, B., Lawrence S., “Inferring Descriptions and \nSimilarity  for Music from  Com munity  Metadata”, Proc. \nof the 2002 ICMC , Göteborg, Sweden, 16-21 Sep.2002, \npp 591-598. \n[8] Kohonen, T., Self-Organizing Maps , Springer, Berlin, \nHeidelberg, 1995. \n[9] Pampalk, E., Dixon, S., Widmer, G., “Exploring Music \nCollections by  Browsing Different Views”, Proceedings  \nof the 4th International Conference on Music Information \nRetrieval ( ISMIR'03) , Baltim ore, MD, October 26-30, \n2003, pp 201-208."
    },
    {
        "title": "Fast labelling of notes in music signals.",
        "author": [
            "Paul Brossier",
            "Juan Pablo Bello",
            "Mark D. Plumbley"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416132",
        "url": "https://doi.org/10.5281/zenodo.1416132",
        "ee": "https://zenodo.org/records/1416132/files/BrossierBP04.pdf",
        "abstract": "We present a new system for the estimation of note at- tributes from a live monophonic music source, within a short time delay and without any previous knowledge of the signal. The labelling is based on the temporal segmen- tation and the successive estimation of the fundamental frequency of the current note object. The setup, imple- mented around a small C library, is directed at the robust note segmentation of a variety of audio signals. A system for evaluation of performances is also presented. The fur- ther extension to polyphonic signals is considered, as well as design concerns such as portability and integration in other software environments.",
        "zenodo_id": 1416132,
        "dblp_key": "conf/ismir/BrossierBP04",
        "keywords": [
            "note attributes",
            "live monophonic music source",
            "short time delay",
            "fundamental frequency estimation",
            "temporal segmentation",
            "robust note segmentation",
            "audio signal evaluation",
            "polyphonic signals",
            "portability",
            "integration"
        ],
        "content": "Fastlabelling ofnotes inmusic signals\nPaulM.Brossier ,JuanP.Bello, Mark D.Plumble y\nQueen Mary Colle ge,University ofLondon\nCentre forDigital Music\nABSTRA CT\nWepresent anewsystem fortheestimation ofnote at-\ntributes from alivemonophonic music source, within a\nshort time delay andwithout anyprevious knowledge of\nthesignal. Thelabelling isbased onthetemporal segmen-\ntation andthesuccessi veestimation ofthefundamental\nfrequenc yofthecurrent note object. The setup, imple-\nmented around asmall Clibrary ,isdirected attherobust\nnote segmentation ofavariety ofaudio signals. Asystem\nforevaluation ofperformances isalsopresented. Thefur-\ntherextension topolyphonic signals isconsidered, aswell\nasdesign concerns such asportability andintegration in\nother softw areenvironments.\n1.INTR ODUCTION\n1.1. Moti vation\nThereal-time segmentation andattrib uteestimation ofmu-\nsical objects arestill novel\u0002elds ofresearch with pos-\nsible applications inaudio coding, music-oriented tasks\nsuch asscore follo wing andlivecontent-based processing\nofmusic data. In[1]aframe workwaspresented forthe\nreal-time transmission ofaudio contents asobjects, within\ntheconte xtofspectral modelling synthesis. In[2]wein-\ntroduced asystem fortheobject-based construction ofa\nspectral-model ofamusical instrument. This workwas\nextended in[3],when wepresented analgorithm forthe\nreal-time segmentation ofnote objects inmusic signals.\nInthepresent paper wewill concentrate onthereal-\ntime estimation ofattrib utes ofthesegmented note ob-\njects, speci\u0002cally their fundamental frequenc yf0.This\nisanimportant step towards theunderstanding ofhigher -\nlevelstructures inmusic e.g.melody ,harmon y,etc.\nWedescribe anewsystem forthelow-latenc ycharac-\nterisation ofatemporal sonic object, based ontheinfor -\nmation provided byournote segmentation algorithm [3].\nTheaimistoobtain arobustnote labelling onalargeva-\nriety ofmusical signals andinvarious acoustic environ-\nments.\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forpro\u0002t orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation onthe\u0002rstpage.\nc\r2004 Universitat Pompeu Fabra.functionOnset detection\nTemporal\npeak−picking\nNote decisionPitch\ndetectionSilence gateAudio\nFigur e1.Overvie wofthedifferent modules ofthesystem\nAmethod forevaluation ofthislabelling process ispro-\nposed andtested. Special attention willbepaid tothede-\nlayintroduced bythef0computation, andhowthecon-\nstraints imposed bylow-latenc yenvironments affectthe\nreliability ofourestimation.\n1.2.f0Estimation\nThefundamental frequenc yf0isthelowest frequenc yof\naharmonic sound: itspartials appear atfrequencies which\naremultiples off0.Formost sounds thefundamental fre-\nquenc yisstrongly related tothepsychoacoustical concept\nofpitch although insome cases theycanbefound to\ndiffer.This explains why,often intheliterature, theterms\npitch detection andf0estimation areused indistincti vely.\nThere areanumber ofmethods developed forf0es-\ntimation inmonophonic audio mostly forspeech signals.\nThese include approaches such astime-domain andspec-\ntralautocorrelation [4],thetwo-waymismatch algorithm\n[5]andtheuseofperceptual models [6].However,while\nsome ofthese algorithms havealready been successfully\ntested forreal-time f0estimation onaframe-by-frame ba-\nsis,little hasbeen done toestimate thepitch ofsegmented\nnote objects online.\nThecomple xityofthistaskincreases when dealing with\npolyphonic sounds. Inthispaper wewilluseapitch algo-\nrithm [7]tofocus ontheanalysis ofnote objects inmono-\nphonic music, using both onset andpitch detections to-\ngether .Ourchoice ofpitch detection algorithm isdriven\ntowards theextension topolyphonic signals. While other\nmethods forselecting thenote pitch must befurther tested,\nwefocus here onthebeha viour ofthesystem rather than\nonthef0estimation itself, soastotackle thedifferent\nissues ofthenote decision process.1.3. Paper organisation\nThis paper isorganised asfollo ws:inSection 2weexplain\ntheprocess implemented foronset detection andf0esti-\nmation, anddescribe a\u0002rstapproach formaximising per-\nformance based onobject segmentation; inSection 3we\npresent anevaluation method forthissegmentation pro-\ncess;the implementation intheform ofasoftw arelibrary\nisdetailed. Section 4presents quantitati veresults ofthe\nintegration ofthedifferent parts ofthesystem. Finally ,\nwepresent ourconclusions inSection 5.\n2.SYSTEM OVER VIEW\nFigure 1showsanovervie wofthemain system compo-\nnents. Thedifferent elements composing thesystem will\nbedescribed inthefollo wing order: thesilence gate, the\nonset detection functions module, itsassociated peak pick-\ningmodule, thefundamental frequenc yestimation, and\nthe\u0002nal note decision module. Inthislaststep pitch val-\nuesandonset times are\u0002ltered into alistofnote candi-\ndates.\nWeusetwophase vocoders inparallel forboth onset\nandpitch detections. Forasignal xattimen,X[n]de-\n\u0002nes itsShort TimeFourier Transform (STFT), calculated\nusing thephase vocoder .Xk[n],thevalueofthekthbinof\nX[n],canbeexpressed initspolar form asjXk[n]jej\u001ek[n]\nwhere jXk[n]jisthespectral magnitude ofthisbin, and\n\u001ek[n]itsphase.\n2.1. Silence gate\nAsilence gate \u0002rst ensures thesuppression ofspurious\ncandidates inbackground noise. When thesignal drops\nunder acertain level,theonsets arediscarded. Because\nthenoise levelcandramatically change between different\nauditory scenes, thislevelcanbeadjusted tominimise the\nonsets detected during pauses andsilences.\n2.2. Onset detection\nOur onset detection implementation hasbeen previously\ndescribed in[3],along with keyreferences totherelevant\nliterature. The process consists oftheconstruction ofa\ndetection function derivedfrom oneorafewconsecuti ve\nspectral frames ofaphase vocoder .The detection func-\ntionincreases atthebeginning ofthenote attacks. Peak-\npicking isrequired toselect only therelevantonsets.\nDifferent detection functions areavailable inourim-\nplementation, andwehavemeasured howwell theyper-\nform onavariety ofsignals: theaverage rates ofcorrect\ndetections andfalsepositi veshavebeen evaluated atdif-\nferent values ofthepeak picking thresholding parameter .\nAsimple andveryef\u0002cient example istheHigh Frequency\nContent (HFC) [8],which canbederivedfrom onespec-\ntralframe Xk[n]as:\nDH[n]=NX\nk=0kjXk[n]j (1)TheHFC precisely identi\u0002es percussi veonsets, butisless\nresponsi vetonon- percussi vecomponents. Inthecomple x-\ndomain approach [9],tocope with harmonic changes of\nlowtransient timbres, atargetSTFT value isgenerated as\nfollo ws:\n(\n^Xk[n]=jXk[n]jej^\u001ek[n]\n^\u001ek[n]=princar g(2\u001ek[n\u00001]\u0000\u001ek[n\u00002])\n(2)\nwhere ^\u001ek[n]istheestimated phase deviation. The mea-\nsure oftheEuclidean distance, inthecomple x-domain,\nbetween thetargetSTFT ^Xkandtheobserv edframe Xk\nallowtheconstruction ofadetection function as:\nDC[n]=1\nNNX\nk=0\r\r\r^Xk[n]\u0000Xk[n]\r\r\r2\n(3)\nBylooking forchanges atboth energyandphase, thecom-\nplex-domain approach andother methods such asthe\nspectral difference approach quanti\u0002es both percussi ve\nandtonal onsets.\nThedetection functions stillcontain spurious peaks and\nsome pre-processing, such aslowpass \u0002ltering, isrequired\nbefore peak picking. Inorder toselect theonsets inde-\npendently ofthecurrent conte xt,adynamic threshold is\ncomputed overasmall number ofD[n]points. Ame-\ndian \u0002lter isapplied \u0002rstforsmoothing andderivation of\nthedetection function; aproportion ofthemean overthat\nsame number ofpoints isincluded inthethreshold tore-\njectthesmaller peaks:\n\u000et[n]=median (D[n\u0000b]:::D[n+a])\n+ \u000bhD[n\u0000b]:::D[n+a]i(4)\nThevaluesaandbde\u0002ne thewindo wofdetection points\nconsidered, typically oneframe inadvance and\u0002veframes\ninthepast. Increasing theproportion \u000bprevents these-\nlection ofthesmallest peaks inthedetection function and\ndecrease thenumber offalsepositi ves.\nWhile theHFC detects percussi veevents successfully ,\nthecomple x-domain approach andother methods such\nasthespectral difference approach reacts better ontonal\nsounds such asbowed strings buttend toover-detect per-\ncussi veevents. Experimental results haveshownthatthe\ncombined useoftwodetection functions, such asthemul-\ntiplication ofthecomple xdomain andtheHFC functions,\nincrease theoverall reliability oftheresults ontheset\nofrealrecordings, suggesting thecomplementarity ofthe\nfunctions.\n2.3. Pitch estimation\nThefollo wingf0estimation algorithm isderivedfrom [7]\nandtheimpro vements described in[10].Although wefo-\ncusonitsbeha viour with monophonic signals, themethod\nhasbeen designed totackle polyphonic music signals. The\nalgorithm isbased onthespectral frames Xk[n]ofaphase\nvocoder similar asthatused intheonset detection func-\ntions. The input signal is\u0002rst pre-processed through anonset outputdetection functiononset\ntimeline\nnote candidate pitch output first input sampleoverlap size\nonset phase vocoder window\npitch phase vocoder windowsilence gate\nonset peak picking\npitch detection\nFigur e2.Detail ofthesynchronisation oftheonset detec-\ntionmodule andthepitch detection module\nA-weighting IIR\u0002lter soastoenhance medium frequen-\nciesandreduce thehigh andlowparts ofthespectrum.\nThe \u0002ltered signal issent toaphase vocoder using a\nwindo woftypically 4096 samples foranaudio sample-\nrateof44:1kHz. Thelonger windo wing implies alonger\ndelay inthesystem butisrequired foraccurate frequenc y\nestimation ofthemidfrequencies. Theoverlap rateisthe\nsame astheoneused fortheonset detection function. On\neach frame, themagnitude spectrum islowpass \u0002ltered\nandnormalised.\nAfter pre-processing, peaks aredetected inthemag-\nnitude spectral frame andthelistofpeaks ispassed to\nanharmonic comb .Weassume thatoneofthePhigh-\nestpeaks corresponds tooneofthepartials ofthepresent\nnotes formonophonic signals, wewilllimit tothecase\nwhere P=1.Each ofthese peaks generates asetofpitch\nhypotheses de\u0002ned bythe\u0002rstZsub-harmonics as:\nff0\np;z=fp\nzjz2[1:::Z]jp2[1:::P]g (5)\nandwhere fpisthefrequenc yassociated tothebinof\nthepthpeak, computed using aquadratic interpolation\nmethod. Foreach ofthesef0\np;zhypotheses aharmonic\ngridisconstructed overthespectral bins as:\nCp;z(k)=(\n1if9ms.t.\f\f\f1\nmk\nf0\np;z\u0000N\nfs\f\f\f<!b\n0otherwise(6)\nwhere fsisthesampling frequenc y,misanintegerbe-\ntween 1andM,themaximum number ofharmonic con-\nsidered. !b,typically aquarter tone, issettoallowfor\nsome uncertainty intheharmonic match ofthethecomb\n\u0002lter.\nDifferent criteria arecheck edalong theevaluation of\neach candidate combs. The twomost important arethe\nnumber ofpartials matching tothecomb harmonic grid,\nandthecomb energy,estimated asthetotal energycarried\nbythesetofpartials.\n2.4. Note Decision\nThe data incoming from thedifferent modules must be\nordered carefully before thedecision process. Differentapproaches canbetakentoaddress thisissue. Inoursys-\ntem, werelyonthetemporal onset detection, assuming it\niscorrect, andlook fornote pitches overtheframes past\neach onset.\nWhile both pitch andonset vocoders operate atthesame\noverlap rateevery512samples, long windo wsarerequired\nforpitch estimation, andshorter windo wsoftypically 1024\nsamples willfeed theonset detection functions. Synchro-\nnisation ofthepitch candidates totheonset time isre-\nquired. Thetemporal peak picking module ofthedetec-\ntionfunction takesoneoverlap period. When using win-\ndows4times longer forthepitches than fortheonsets,\nthepitch candidates oftheframes aretypically delayed by\nabout 2overlap periods. The process isdepicted inFig-\nure2.\nIntheattack ofthenote, justafter theonset, pitch de-\ntection during strong transient noises willtend tobedif\u0002-\ncult, since thetransient coversmost oftheharmonic com-\nponents. These spurious pitch candidates need tobecare-\nfully discarded. Another source oferror iswhen theam-\nplitude ofthedifferent partial arechanging within thenote.\nOctaveor\u0002fth errors may then occur .\nToevaluate anote pitch candidate inalimited number\nofframes after theonsets, asimple andef\u0002cient system\nhasbeen builtbychoosing themedian overthecandidates\nthatappear intheframes after theonset:\nPnote=median (Pq;Pq+1;:::;Pq+\u000e) (7)\nwhere \u000eisthenumber offrames considered forthepitch\ndecision andwill determine thetotal delay ofthesys-\ntem. The \u0002rstqframes arenotconsidered, soastotake\nintoaccount thedelay between both pitch andonset phase\nvocoders.\nThemedian \u0002lter favorstheselection ofthemost fre-\nquently detected pitch candidate, while \u000eprovides acon-\ntrolofthetrade-of fbetween thesystem delay anditsac-\ncurac y.This simple method provedtobesuccessful at\nselecting themost predominant pitch candidate overafew\nframes after theonset. The note labelling isdone byat-\ntaching theselected pitch candidate totheonset time pre-\nviously detected.\n3.EXPERIMENT ALSETUP\nWedescribe here theevaluation technique used toesti-\nmate theperformance ofournote identi\u0002cation system.\nWewillthen describe oursoftw areimplementation.\n3.1. Performance estimation\nAtestbedfortheevaluation ofoursystem hasbeen imple-\nmented. Audio waveforms aregenerated using MIDI \u0002les\nandanalysed through ournote labelling program. The\nevaluation consist ofthecomparison between theorigi-\nnalMIDI score andthelistofcandidate eventdetections\nweobtain, both against pitch andstart time.\nForourevaluation purposes, thescores were chosen\namongst various single voiced scores forpiano, violin, 60 66 72 78\n 15  20  25Midi pitch\nTime (s)octavefifthlate doubledoriginal midi events\ndetected events\nFigur e3.Example oftypical note segmentation errors:\ndetected eventsareshownin\u0002lled box, theoriginal score\nisoutlined. Extract from Partita inAminor forSolo Flute ,\nJ.S.Bach,BWV1013 ,1stMovement: Allemande.\nclarinet, trumpet, \u0003ute andoboe. Allscores havebeen\nextracted from theMutopia project [11]:some from sepa-\nratetracks ofMozart andBach concertos andsymphonies,\nothers from more recent compositions forsoloinstrument.\nTheMIDI \u0002les contain nuances andexpressi veness. The\ncurrent database currently totalises 1946 notes.\nThemidi \u0002les areconverted intorawwaveforms using\ntheTimidity++ MIDI rendering softw are[12].Each ofthe\nscores canberendered using anyinstrument, while alarge\namount ofsettings andeffects areavailable. Forinstance,\nweused theReverberation andChorus settings, sothatthe\nresults sound more natural.\nNOTE-ON eventsareextracted asapairofMIDI pitch\nandstart time.Ourmain program isthen called, initsoff-\nlinemode, along with asetofparameters, toprocess the\nwaveform andstore thenote candidates inasimilar list.A\nPython script isthen used tocompare theoriginal listto\nthelistofdetected events.\nIfthedetected eventcorresponds toarealnote within\natolerance windo woflength \u000ft(ms) andwith thecorrect\nMIDI pitch rounded tothenearest integer,only then the\neventislabelled asacorrect detection. Incorrect detec-\ntions canbeeasily characterised bytheir frequenc yerror ,\noctaveor\u0002fth jumps, andtheir temporal error ,doubled or\nlatedetections.\nAnexample ofsuch ascore comparison isgiveninFig-\nure3.Pitches areplotted versus times inthispiano-roll\nlikegraph. Theoriginal notes aredrawninsolid lines, the\ndetected eventsin\u0002lled black squares. Theplotillustrates\nvarious types oferrors.\n3.2. Softwar eimplementation\nThis implementation isadevelopment ofthesoftw areli-\nbrary presented in[3].Thef0estimation runs inparallel\nwith theonset functions. Theprocess runs within theJack\nAudio Connection Kit(JACK) serverforexperiments onlivesignals. Inthiscase, anaudio input iscreated forlis-\ntening toincoming data, andaMIDI output portiscreated\ntooutput theresult ofthetranscription toanother MIDI\ndevice.\nThe library hasbeen keptsmall anditsdependancies\nlimited toasmall number ofwidely used libraries.\nWebelie vethatthecurrent setup provides asolid foun-\ndation forthedevelopment oftestbeds forvarious Mu-\nsicInformation Retrie valtechniques, such asblind instru-\nment recognition methods ortesting algorithm robustness\nagainst various compression formats.\n4.INITIAL RESUL TS\nIn[3],thedifferent detection functions were evaluated on\nasetofrealrecordings, representing awide range ofmusic\nstyles andsound mixture. Alldetection functions have\nbeen peak-pick edusing awindo wofsizea=5andb=1\ninEq.4.Theproportions ofcorrect andmisplaced onsets\nhavebeen estimated bycomparing thedetections against\nthehand-labelled onsets.\nWefound theproduct oftheHFC andcomple xdo-\nmain onset functions gavebestperformance. Witha\u0002xed\nthreshold of\u000b=0:300,weobtained typical detection\nrates of96% correct detections and6%offalsepositi ves.\nInthefollo wing twoexperiments, we\u0002xed\u000btoavalue of\n0:300:aslight over-detection isallowed toensure ahigh\ncorrect detection proportion, onwhich werelyinthenote\npitch decision.\nWecanobserv etheperformance ofthenote decision\nprocess byvarying \u000ethenumber offrame considered in\nEq.7.Theresults obtained with different instruments for\nvalues of\u000ebetween 2and25areplotted inFigure 4.\nThe performance forthewhole setofMIDI test\u0002les\n(plus signs inFigure 4)reaches 90% ofcorrect note la-\nbelling at\u000e=20,which givesadecision delay of200ms\nwith atotal rateof12:5%offalsepositi ves.\nSome instruments tend tobesuccessfully labelled within\nasfewas\u000e=3frames, asshownwith theharpsichord\nresults (open squares). Low\u000evalues affect theperfor -\nmance ofthe\u0003ute, which may beexplained bythesoft\nandbreathy attack ofthe\u0003ute. This iscorrected using a\nlonger value of\u000e.\nAnother problem occur when alargevalue of\u000eisused\nonlong notes: theperformance then tends todecrease.\nChanges intherelati veamplitude ofthepartials may cause\nthepitch candidate toswitch toanother harmonic. This\nbeha viour isobserv edonthesecond violin score (aster -\nisks) which hasamoderated tempo andnumerous long\nnotes.\nThegeneral irregular beha viour ofthecurvesisprob-\nably duetothecombined effectofthetransients andthe\nmedian \u0002lter,when only2to9frames areconsidered.\nThetime precision ofoursystem isevaluated using a\n\u0002xedvalue of\u000e=15bychanging thetime tolerance \u000ft\ninthelistcomparison function. Theresults aredisplayed\ninFigure 5.Within a50mstolerance windo w,thesys-\ntemreaches anoptimal score alignment. Overall results 40 50 60 70 80 90 100\n 0  55  110  165  220  275Correct note detections (%)\nDecision delay (ms)flute\nclarinet\nviolin\nharpsichord\noverall\nFigur e4.Correct note estimation results fordifferent val-\nuesof\u000ein(7),thenumber ofpitch frames thedecision is\ntakenon,andfordifferent instruments. \u000bis\u0002xedat0:300.\nareadversely affected bythe\u0003ute performance, especially\nwhen thetime tolerance isdecreased tounder 30ms.\n5.CONCLUSIONS\nWehavepresented acomplete system toperform note ob-\njects labelling ofawide range ofmonophonic music sig-\nnals within ashort delay after theonset detections. Our\nsmall library aims atbeing lightweight, portable, andused\nfrom other softw ares, either realtime oroff-line. Theli-\nbrary willbemade available under theGNU General Pub-\nlicLicense (GPL). Enduser applications havestarted with\nvarious interf aces toother softw areenvironments such as\nCLAM andAudacity .\nUsing avaried setofMIDI \u0002les, theevaluation ofthis\nnote object extraction system hasshownuseful perfor -\nmances canbeobtained with different instruments. Re-\nsults haveenlightened some oftheissues encountered:\nsoftattacks tend todelay thedetection ofonsets; transient\ncomponents affectthepitch decision inthefewframes af-\ntertheonset. The onset detection threshold (\u000b)andthe\nnote decision delay (\u000e)areimportant parameters control-\nling under orover-detection ononehand, andthedelay\nandaccurac yofthesystem ontheother hand.\nEvaluation oftheperformance should alsobetested on\nrealrecordings. Possible impro vements include theuseof\naspeci\u0002c module forbass linedetections andtheelabora-\ntionofadditional features tobeadded tothenote labels.\n6.ACKNO WLEDGEMENTS\nPBissupported byaStudentship from theDepartment of\nElectronic Engineering atQueen Mary Colle ge,Univer-\nsityofLondon. This research hasbeen partially funded by\ntheEU-FP6-IST -507142 project SIMA C(Semantic Inter -\naction with Music Audio Contents) andbyEPSRC grant\nGR=54620 . 0 20 40 60 80 100\n 10  20  30  40  50  60  70  80  90  100Correct detections (%)\nTime tolerance (ms)flute\nclarinet\nviolin\nharpsichord\noverall\nFigur e5.Correct note detections fordifferent instru-\nments plotted against time tolerance \u000ftforvalues from\n\u000ft=10msto\u000ft=100.\n7.REFERENCES\n[1]X.Amatrian andP.Herrera, Transmitting audio\ncontent assound objects, inProc.ofAES22 Inter -\nnat.Confer ence onVirtual Synthetic andEntertain-\nment ,Audio Espoo, Finland, 2002, pp.278288.\n[2]P.M.Brossier ,M.Sandler ,andM.D.Plumble y,\nReal time object based coding, inProceedings\noftheAudio Engeeniring Society ,114th Convention ,\nAmsterdam, TheNetherlands, 2003.\n[3]P.M.Brossier ,J.P.Bello, and M.D.Plumble y,\nReal-time temporal segmentation ofnote objects in\nmusic signals, inProceedings oftheICMC ,Miami,\nFlorida, 2004, ICMA, Conference submission.\n[4]J.C.Brownand B.Zhang, Musical Frequenc y\nTracking using theMethods ofConventional and\n'Narro wed' Autocorrelation, J.Acoust. Soc. Am.,\nvol.89,pp.23462354, 1991.\n[5]P.Cano, Fundamental frequenc yestimation inthe\nSMS analysis, inProc.ofCOST G6Confer ence on\nDigital Audio Effects,Barcelona, 1998, pp.99102.\n[6]M.Slane yandR.F.Lyon, APerceptual Pitch De-\ntector ,inProc.ICASSP ,1990, pp.357360.\n[7]P.Lepain, Polyphonic pitch extraction from music\nsignals, Journal ofNewMusic Resear ch,vol.28,\nno.4,pp.296309, 1999.\n[8]P.Masri, Computer modeling ofSound forTransfor -\nmation andSynthesis ofMusical Signal ,PhD disser -\ntation, University ofBristol, UK, 1996.\n[9]C.Duxb ury,M.E.Davies, and M.B.Sandler ,\nComple xdomain onset detection formusical sig-\nnals,inProc.oftheDAFx Conf .,London, 2003.\n[10] J.P.Bello, Towar dstheAutomated Analysis of\nSimple Polyphonic Music ,PhD dissertation, QueenMary ,University ofLondon, Centre forDigital Mu-\nsic,2003.\n[11] Mutopia Project, apublic domain collection of\nsheet music, http://www .mutopiaproject.or g/.\n[12] T.Toivonen andM.Izumo, Timidity++, aMIDI\ntoWAVEconverter/player ,http://www .timidity .jp/,\n1999, GNU/GPL."
    },
    {
        "title": "The emergence of complex network patterns in music networks.",
        "author": [
            "Pedro Cano",
            "Markus Koppenberger"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417663",
        "url": "https://doi.org/10.5281/zenodo.1417663",
        "ee": "https://zenodo.org/records/1417663/files/CanoK04.pdf",
        "abstract": "Viewing biological, social or technological systems as net- works formed by nodes and connections between them can help better understand them. We study the topol- ogy of several music networks, namely citation in allmu- sic.com and co-occurrence of artists in playlists. The anal- ysis uncovers the emergence of complex network phe- nomena in music information networks built considering artists as nodes and its relations as links. The proper- ties provide some hints on searchability and possible op- timizations in the design of music recommendation sys- tems. It may also provide a deeper understanding on the similarity measures that can be derived from existing mu- sic knowledge sources.",
        "zenodo_id": 1417663,
        "dblp_key": "conf/ismir/CanoK04",
        "keywords": [
            "networks",
            "nodes",
            "connections",
            "emergence",
            "complexity",
            "searchability",
            "optimizations",
            "design",
            "music recommendation",
            "similarity measures"
        ],
        "content": "THE EMERGENCEOF COMPLEXNETWORK PATTERNS INMUSIC\nARTIST NETWORKS\nPedroCano and MarkusKoppenberger\nMusic Technology Group\nInstitut de l’Audiovisual, Universitat PompeuFabra\nOcata1, 08003, Barcelona,Spain\nABSTRACT\nViewingbiological,socialortechnologicalsystemsasnet -\nworks formed by nodes and connections between them\ncan help better understand them. We study the topol-\nogy of several music networks, namely citation in allmu-\nsic.comandco-occurrenceofartistsinplaylists. Theanal -\nysis uncovers the emergence of complex network phe-\nnomena in music information networks built considering\nartists as nodes and its relations as links. The proper-\nties provide some hints on searchability and possible op-\ntimizations in the design of music recommendation sys-\ntems. It may also provide a deeper understanding on the\nsimilaritymeasures thatcan bederived fromexistingmu-\nsicknowledge sources.\n1. INTRODUCTION\nAnetworkisacollectionofitems,namedverticesornodes,\nwith connections between them. The study of the net-\nworks underlying complex systems is easier than study-\ning the full dynamics of the systems. Yet, this analysis\ncan provide insights on the design principles, the func-\ntions and the evolution of complex systems [9, 5]. Signif-\nicant amount of multidisciplinary research on social, bi-\nological, information and technological networks has un-\ncoveredthatcomplexsystemsofdifferentnaturedoshare\ncertain topological characteristics. Indeed, the spread o f\ncertain ideas and religions, the success of companies, the\ntransmission of sexually transmitted diseases such as the\nAIDS epidemic or computer viruses can be better under-\nstood by studying the topologies of the systems where\nthey interact [9].\nAccording to [9], research in complex networks aims\nat three things:\n1. Find statistical properties, such as path length and\ndegreedistributionthatcharacterizethestructureand\ndynamic behavior ofnetworked systems.\nPermissiontomakedigitalorhardcopiesofallorpartofthisw orkfor\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributedforproﬁtorcommercialadvantagean dthat\ncopies bearthis notice andthefull citation ontheﬁrst page .\nc/circlecopyrt2004Universitat PompeuFabra.2. Build models of networks that explain and help un-\nderstandhowtheyarecreatedandhowtheyevolve.\n3. Predictthebehaviorofnetworkedsystemsbasedon\nthe measured statistical properties of the structure\nandthelocalpropertiesofgivenvertices,e.g.: what\nwillhappen totheequilibrium ofanecological net-\nwork ifacertain species disappears.\nComplex network analysis is used to describe a wide\nvarietyofsystemswithinteractingparts: networksofcol-\nlaboratingmovieactors,theWWW,neuralnetworks,meta-\nbolic pathways of numerous living organisms, to name a\nfew. Newinsightscanbeunveiledbyconsideringmusical\nworks and musical artists as parts of a huge structure of\ninterconnecting partsthat inﬂuence each other.\nThe objective of this work is to show the emergence\nofcomplexnetworkphenomenainmusicinformationnet-\nworksbuiltconsideringartistsasnodesandartistrelatio n-\nships as links between nodes. Complex network analysis\nmay enhance our comprehension on some relevant musi-\ncological and information retrieval issues. For example,\nhow much of the network structure is due to content sim-\nilarity and how much to the self-organization of the net-\nwork. This could shed new light on the design and valid-\nityofmusicsimilarityevaluation[6,3]. Secondly,abette r\nunderstandingofthetopologymayhintpossibleoptimiza-\ntions on the design of music information systems [7]. Fi-\nnally,itmayhelptounderstandthedynamicsofofcertain\naspectsofmusicevolution,e.g: howdidanartistgetpop-\nular? Besides the preliminary results and the discussion,\none of the goals of this paper is to call the attention to\nthe MIR community of this body of research which is the\nscience of complex networks.\n2. NETWORK PROPERTIES\nLet us introduce some deﬁnitions and concepts that will\nbe used in this work. A networkor graph is a set of\nvertices connected via edges. Networks connected by di-\nrected edges are called directed networks , networks con-\nnectedbyundirectededgesarecalled undirectednetworks .\nDegree: The degree kior a vertex iis the number of\nconnectionsofthatvertexand /angbracketleftk/angbracketrightistheaverageof kiover\nall the vertices of the network. In an undirected graphwhere each edge contributes to two vertices the average\ndegree is:\n/angbracketleftk/angbracketright=2m\nn(1)\nwhere misthenumberofedgesand nthenumberofver-\ntices.\nClustering coefﬁcient : The clustering coefﬁcient esti-\nmates the probability that two neighboring vertices of a\ngiven vertex are neighbors themselves. In the networks\nthat occupy us, it relates to the probability that if artist A\nis similar to artist B and artist C, B and C are similar as\nwell. Following [10] the clustering coefﬁcient of vertex i\nis the ratio between the total number yiof the edges con-\nnecting its nearest neighbors and the total number of all\npossibleedges between all thesenearest neighbors,\nci=2yi\n/angbracketleftk/angbracketright(/angbracketleftk/angbracketright −1)(2)\nThe clustering coefﬁcient cfor the whole network is the\naverage over the number of nodes n\nc=1\nn/summationdisplay\nici (3)\nComponent : Thecomponenttowhichavertexbelongs\nisthe setof vertices that can be reached from that vertex.\nAverage shortest path : Two vertices iandjare con-\nnected if one can go from itojfollowing the edges in\nthe graph. The path from itojmay not be unique. The\nminimumpathdistanceor geodesicpath dijistheshortest\npath distance from itoj. The average shortest path over\nevery pair of vertices is\n/angbracketleftd/angbracketright=1\n1\n2n(n+ 1))/summationdisplay\ni≥jdij (4)\nwhere dijis the geodesic distance from vertex ito vertex\nj. The maximum geodesic between any two vertices in\nthe graph isknown as diameter.\nDegree distribution : The degree distribution P(k)is\nthe proportion of nodes that have a degree k. The shape\nof the degree distribution can help identify they type of\nnetwork: “scale-free networks” have power-law distribu-\ntionsand“randomnetworks”—asdescribedbytheErd ¨os-\nR´enyimodel—haveaPoissondegreedistribution(seebe-\nlow).\n2.1. Random networks\nTherandomgraphmodel,introducedbyErd ¨osandR´enyi,\nconnects in one of its variants every pair of vertices with\naprobability p(see[4] forareview onrandom graphs).\nThe degree distributionfollows abinomial shape\nP(k) =Ck\nn−1pk/parenleftbig\n1−pk/parenrightbig\n(5)\nwhereCk\nn−1isthenumberofwaysofconnectingavertex\ntoknodesandnotto n−k−1others. Forlarge n/greatermuch1the\ndistributionisapproximated by thePoisson distribution\nP(k)∼e−/angbracketleftk/angbracketright/angbracketleftk/angbracketrightk\nk!(6)Another magnitude that can be computed is the clus-\ntering coefﬁcient. Since the probability of link between\ntwoverticesinarandomgraphisindependentoftheexis-\ntence of other edges and equal to p, in average there will\nbepk(k−1)/2out of the total possible of k(k−1)/2\nneighborsofavertexofdegree k. Combiningequation(1)\nand (2),the cof arandom graph, cr, is\ncr=/angbracketleftk/angbracketright\nn(7)\nAnimportantaspectregardingnetworksreferstounder\nwhich conditions all the vertices are connected in a big\ncomponent1. It can be shown that there exist a thresh-\noldpcunderwhichtheverticesaredisconnectedandover\nwhichagiantconnectedcomponentabruptlyemerges. The\ntransition pcoccurs when /angbracketleftk/angbracketright=npc= 1.\nRandomgraphsreproducethe small-worldeffect ,avery\ncommonphenomenoninrealnetworks,whereverticeson\nanetworkseemtobeconnectedbyshortpaths. Themean\nnumber of vertices at a distance daway from a vertex is\n/angbracketleftk/angbracketrightd. Consequentlythevalue dneededtoreachthewhole\nnetworkis /angbracketleftk/angbracketrightd=n. Atypicaldistance drontherandom\nnetwork willbe:\ndr=log(n)\nlog(/angbracketleftk/angbracketright)(8)\nRandommodelshoweverfailtoreproducecertainprop-\nertiesubiquitousinrealnetworkssuchassigniﬁcantlyhig h\nclustering coefﬁcient or heterogeneity on the degree dis-\ntribution. Additionally,ithasbeenshownthatrandomnet-\nworksareimpossibletonavigateusinglocalalgorithms[7,\n9].\n2.2. Small-World Networks\nIn order to overcome the discrepancy between the small\nclustering coefﬁcient of random networks and those ob-\nserved on real graphs, Watts and Strogatz[10] proposed\nthe Small-world network (SW networks). SW networks\nare made from regular lattices by connecting pairs of ver-\ntices connected at random. The small number of random\nshortcuts produce the short average paths while maintain-\ning the high clustering of regular lattices. SW networks\npresent Poisson-like degree distributions[5].\n2.3. Scale-free networks\nMany complex networks display a high heterogeneity on\nthe degree distributions. It has been found that many net-\nworks follow apower-law or Scale-free (SF) distribution\nP(k)∼k−γ(9)\nSW and random models presented so far aim to ex-\nplain certain properties observed in real networks. How-\nevertheydonotattempttoexplainthepower-lawdistribu-\ntionsoflargenetworkssuchastheWWWorhowthenet-\nworks came to be like they are in the ﬁrst place. Barab ´asi\n1This propertyis knownas percolation.etal.[1]proposedamodelthatexplainthepower-lawde-\ngree distribution of genetic networks or the WWW using\ntwomechanisms:\n1. Networks expand continuously. Indeed new pages\nare added to the WWW or, in our problem, new\nartistsareadded intothe systems.\n2. New vertices attach preferentially to sites already\nwellconnected. Avariation“richgetricher”mech-\nanism.\nSFnetworksarerobusttorandomnoderemovalandfrag-\niletotargetedattacks,i.e.: removalofhubs. Thispropert y,\nknown as resilience is of great interest when studying the\nrobustnessoftheInternetorelectricpowernetworks. Ina\nmusic recommendation system, unknown artists links of-\nfered to a user can be seen as a node removal since the\nuser is unlikely to follow that path. If the hubs artists—\nthe one that keep the network together—are unknown the\nrecommendation network could be fragmented into small\ncomponents.\n(a)Erd¨os-R´enyirandomgraph\n(b) 400 Artist similarity network according to\nAMG\nFigure 1. On the top a random graph generated with the\nsame number of nodes and average degree than the net-\nworkconstructedfromthesimilaritylinkseditedbymusic\nexpertsofAllMusicGuide.com(seeSection3). Theﬁgure\nhas been generated mapping the network to a 3D using a\nspringembedding algorithm available on Pajek [2].\n3. MUSIC ARTISTSIMILARITY NETWORKS\nWehaveconstructedmusicartistsimilaritynetworksbased\nontwosources: expertopinionsandplaylistco-ocurrence.n /angbracketleftk/angbracketrightSGC C C r d d r\nAMix48169 12.5 99.1 0.1.00263.8 4.3\nAMG400 5.4 96.2 0.3 .01354.7 3.6\nTable 1. Summary of basic network properties for the\nArt of the Mix artist network (AMix). nis the number\nor artists, /angbracketleftk/angbracketrightis the average degree, SGCis the size of\nthe giant component as a percentage, Cis the clustering\ncoefﬁcient, Cris the clustering of a random network, d\nis the average shortest path, and dris the corresponding\nshortestpath forthe random network.\nBothsourcesweregatheredandregularizedbytheauthors\nof[6]withthegoalofﬁndingagroundtruthforevaluating\nmusicsimilaritymeasures [6, 3].\nExpert opinion: The data consists of 400 artists along\nwith their relations according the professional editors of\n“All Music Guide”2(AMG). A connection between artist\nis made if the “similar artists” link exists. The network,\noriginally directed, is converted to undirected. See Fig-\nure 1 for a visualization of the AMG network. A random\nnetwork constructed with similar characteristics, i.e.: 4 00\nnodes and an average connectivity between nodes, /angbracketleftk/angbracketrightof\n5.4. As can be observed in Figure 1, the AMG network is\nlesshomogeneous than theequivalent random graph.\nPlaylist Co-occurrence: The data consists of human\nauthoredplaylists(over29.000)from“TheArtoftheMix”3\n(AMix) from early 2003. A connections has been made\nbetween the artists if the co-occur in a playlist. The re-\nsulting graph has over 48.000 artists with an average of\n/angbracketleftk/angbracketright= 12.5links toother artists.\n4. EXPERIMENTAL RESULTS\nProperties of the organization of the networks are sum-\nmarized on Table 1. The ﬁrst thing to note is that both\nnetworks are sparse—on average each artist is connected\ntoasmallpercentageofotherartists. InthecaseofAMix,\neach artist is connected on average to 12.5 (.026%) of the\n48169 possible artists. Despite their sparsity, both net-\nworks contain a single giant component which connects\n99.1% of the artists of AMix and 96.2% on AMG. These\nresultsareinaccordancewithwhathasbeendiscussedon\npercolationonSubsection2.1. Therestofgraphmeasure-\nments restricttothegiant component.\nNot only is it possible to reach a large percentage of\nartistsfromanyartistfollowingtheedges,whichisofob-\nvious interest in an artist recommendation system, it is\nalso possible to do it on small number of steps (small-\nworld effect). The average shortest path dis 4.7 steps on\nthecaseofAMG(withamaximumof13)andonthecase\nof AMix dis 3.8 (with a maximum of 11). The corre-\nsponding values for the equivalent random graph are very\nsimilarand areshown on Table 1.\n2http://www.allmusic.com\n3http://www.artofthemix.orgThe clustering coefﬁcient cfor the AMG is 0.3, mean-\nwhile the overall clustering coefﬁcient for equivalent ran -\ndomgraph cris0.014. ForAMix c= 0.1andcr=.0135.\nBothnetworkshavea dcloseto drandhighclusteringco-\nefﬁcient( c/greatermuchcr)withrespecttotherandomgraph. These\nproperties areindicator of Small Worldstructure.\nAs for the statistical distribution of links, in Figure 2\nwe depict the degree distribution P(k)for the AMix net-\nwork. Thedegreedistributionoftheequivalentpoissonian\nrandom graph is shown for comparison. On the bottom\ngraph of Figure 2, the cumulative Pc(k) =/summationtext\nk/prime>kP(k/prime)\nis displayed. The Pc(k)of power-laws P(k)∼k−γis\nalso a power-law with Pc∼kγ−1and it used to smooth\nthe ﬂuctuations on the tails. The distribution could corre-\nspond to a truncated power-law or a multifractal distribu-\ntion [5]. The existence of a small but signiﬁcant number\nof artists connected to a very large number of artists may\nhint some sort of preferential attachment growing model.\nTheP(k)oftheAMGisnotshownbecauseitdidnotdis-\nplay particular pattern besides a heavy tail probably due\ntoitssmallsize.\n10010110210310410-410-310-210-1100P(k)\n10010310-3100\nArt of Mix Artists\nPoisson\n100101102103104\nk10-410-310-210-1100Pc(k)\n10010310-3100\nFigure 2. Top: Log-log distribution of the empirical de-\ngree distribution P(k)for the Art of the Mix artists net-\nwork. The dashed line shows the degree distribution of\na Poissonian random graph with the same average degree\ndistribution /angbracketleftk/angbracketright= 12.5and it is shown for comparison:\nthe distribution for the real networks shows a heavier tail.\nBottom: Log-log distribution of the cumulative degree\ndistribution Pc(k) =/summationtext\nk/prime>kP(k/prime)which could corre-\nspond to be some sort of truncated power-law or maybe\namultifractal distribution[5]5. DISCUSSION\nWe show that music networks share some statistic prop-\nerties with other complex networks, namely the Small-\nWorldstructure[10].\nSmall-World networks have implications on the nav-\nigability of information systems. MIR systems can be\nstructurally optimized so as to allow surﬁng to any part\nofamusiccollectionwithasmallnumberofmouseclicks\n(short average distance) and so that they are easy to nav-\nigate using only local information [7, 9]. On the other\nhand,adeeperunderstandingoftheunderlyingforcesdriv-\ning playlist creation or music expert knowledge networks\ncan provide new insights on the design of music similar-\nity measures evaluation. Is it possible to quantify how\nmuchofartistsimilaritiesaredueto“populargetpopular”\nmechanisms and how much to actual similarity between\nartists?[6, 3, 8, 9]\nWe are thankful to Oscar Celma, Fabien Gouyon and\nPerfecto Herreraforfruitfuldiscussions.\nThisworkispartiallyfundedbyAUDIOCLASE!2668\nEureka (http://www.audioclas.org).\n6. REFERENCES\n[1] A.-L. Barab ´asi and R. Albert. Emergence of scaling\nin random networks. Science, 286(5439):509–512,\n1999.\n[2] V.BatageljandA.Mrvar. GraphDrawingSoftware ,\nchapter Pajek - Analysis and Visualization of Large\nNetworks, pages 77–103. Springer, Berlin,2003.\n[3] A.Berenzweig,B.Logan,D.P.W.Ellis,andB.Whit-\nman. A large-scale evalutation of acoustic and sub-\njective music similarity measures. In Proc. of the\nISMIR, Baltimore, Maryland, 2003.\n[4] B. Bollob ´as.Random Graphs . London: Academic\nPress, 1985.\n[5] S. N. Dorogovtsev and J.F. F. Mendes. Evolution of\nNetworks: From Biological Nets to the Internet and\nWWW. Oxford University Press,Oxford, 2003.\n[6] D.P.W. Ellis, B. Whitman, A. Berenzweig, and S.\nLawrence. The quest for ground truth in musical\nartistsimilarity. In Proc. of theISMIR , Paris,2002.\n[7] J. M. Kleinberg. Navigation in a small world. Na-\nture, 406:845, 2000.\n[8] F. Menczer. The evolution of document networks.\nProceedings of the National Academy of Science\nUSA, 101:5261–5265, 2004.\n[9] M. E. J. Newman. The structure and function of\ncomplex networks. SIAM Review , 45(2):167–256,\n2003.\n[10] D.J.WattsandS.H.Strogatz. Collectivedynamicsof\n’small-world’networks. Nature, 393:409–10, 1998."
    },
    {
        "title": "Automatic Location And Measurement Of Score-based Gestures In Audio Recordings.",
        "author": [
            "Michael A. Casey",
            "Tim Crawford"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415184",
        "url": "https://doi.org/10.5281/zenodo.1415184",
        "ee": "https://zenodo.org/records/1415184/files/CaseyC04.pdf",
        "abstract": "This paper reports on our first experiments in using the feature extraction tools of the MPEG-7 international stan- dard for multimedia content description on a novel prob- lem, the automatic identification and analysis of score- based performance features in audio recordings of mu- sic. Our test material consists of recordings of two pieces of 17th- and 18th-century lute music in which our aim is to recognise and isolate performance features such as trills and chord-spreadings. Using the audio tools from the MPEG-7 standard facilitates interoperability and al- lows us to share both score and audio metadata. As well as using low-level audio MIR techniques within this MPEG- 7 context, the work has potential importance as an ’or- namentation filter’ for MIR systems. It may also form a useful component in methods for instrumental performer identification.",
        "zenodo_id": 1415184,
        "dblp_key": "conf/ismir/CaseyC04",
        "keywords": [
            "MPEG-7",
            "automatic identification",
            "score-based performance features",
            "audio recordings",
            "music",
            "performance features",
            "trills",
            "chord-spreadings",
            "interoperability",
            "low-level audio MIR techniques"
        ],
        "content": "AUTOMA TIC LOCA TION AND MEASUREMENT OFORN AMENTS IN\nAUDIO RECORDINGS\nMichael Case yandTimCrawfor d\nCentre forCognition, Computation andCulture\nGoldsmiths Colle ge\nUniversity ofLondon\nABSTRA CT\nThis paper reports onour\u0002rstexperiments inusing the\nfeature extraction tools oftheMPEG-7 international stan-\ndard formultimedia content description onanovelprob-\nlem, theautomatic identi\u0002cation andanalysis ofscore-\nbased performance features inaudio recordings ofmu-\nsic.Ourtestmaterial consists ofrecordings oftwopieces\nof17th- and18th-century lute music inwhich ouraim\nistorecognise andisolate performance features such as\ntrills andchord-spreadings. Using theaudio tools from\ntheMPEG-7 standard facilitates interoperability andal-\nlowsustoshare both score andaudio metadata. Aswell as\nusing low-levelaudio MIR techniques within thisMPEG-\n7conte xt,theworkhaspotential importance asan'or-\nnamentation \u0002lter' forMIR systems. Itmay also form a\nuseful component inmethods forinstrumental performer\nidenti\u0002cation.\n1.INTR ODUCTION\nAperennial challenge inMusic Information Retrie valis\nthereconciliation oftheaudio andsymbolic domains of\nmusic representation, [4].While itcanbeargued thatau-\ndiorepresents everything thatmost audiences might rea-\nsonably beexpected topercei veabout apiece ofmusic,\nitlacks signi\u0002cant information thatisexplicitly present in\nasymbolic representation. The task ofrecovering infor -\nmation likeformal andharmonic structure, voice-leading\nandevennote-detail from anybutthesimplest ofaudio\ntextures isstillthesubject ofmuch research activity.[1,\n2,8,9,11,12,13,14,15,18,19].\nOne wayinwhich score andaudio representations of\napiece ofmusic differisinthematter ofthose low-level\nperformance gestures thataregenerally classed under the\nheading of'ornamentation'. Weusetheterm here some-\nwhat broadly toinclude all'extra' note events; thisem-\nbraces some classes ofthetemporal redistrib ution ofnotes,\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forpro\u0002t orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation onthe\u0002rstpage.\nc\r2004 Universitat Pompeu Fabra.inparticular thespreading ofchords which arebasically\nnotated asasingle eventorasetofsimultaneously-starting\nnotes butareperformed asasuccession ofdiscrete note\nonsets.\nScores usually indicate ornaments bytheuseofsym-\nbols from aquite extensi velexicon ofsigns which has\naccumulated andevolvedoverseveralcenturies,[10 ].A\nstandard subset ofatmost afewdozen ismore-or -less\nuniversally accepted intoday' sCommon Music Notation,\nandeach hasaroughly standard conventional interpreta-\ntion.\nIntermediate representations such asMIDI \u0002les may,\normay not,include 'realisations' ofthese ornament signs;\ntheymay beseparated from the'score' notes inthe\u0002le,\northeymight simply beincluded inthegeneral 'mix' of\nnotes. Ornament realisations inMIDI \u0002les might behis-\ntorically appropriate tothemusic ormerely fanciful orin-\nstincti ve,justasthose inaudio recordings might be.\nOften ornamentation gestures areadded tomusic even\nwhen theyarenotprescribed explicitly inascore. This\nistrueforgenres other than classical instrumental music;\nforexample, opera, jazz, blues, soul andallkinds ofpop-\nular andtraditional music havetheir ownlexica oforna-\nments which canbeadded spontaneously ,evenwhen de-\nvices such astempo deviation arenotused, [22,16].\nWewould liketoautomate theprocess ofrecognising\nperformance gestures directly from audio forvarious rea-\nsons. Firstly ,theanalysis ofperformance onaudio record-\nings israpidly gaining ground asanimportant area ofmu-\nsicological research, bearing strongly asitdoes onhow\npeople understand and'feel' music; see[5].Most re-\nsearch inthis\u0002eld hastooperate atthebroad-brush level\noftempo-de viation comparisons, orbymanual examina-\ntion ofspectographic images; see[6];seealso [20].As\nfaraswecansee,verylittle ifanyworkhasbeen done on\ntools thatcanlocate, identify andmeasure detailed orna-\nmental gestures within anaudio recording. Secondly ,an\naudio performance ofapiece ofmusic with ornamenta-\ntionnaturally contains signi\u0002cantly more 'notes' than the\nscore representation; forMIR oranalysis techniques that\ndepend onmatching score andaudio, thisstrongly raises\nthechances offalseorconfused matches. An'ornamentation-\n\u0002lter' might provetobeauseful pre-processing tool for\nMIR systems. Thirdly ,afurther interesting MIR challengeiswhether itispossible tousetheoutput ofanornament-\nrecognition tool toaidintheproblem ofidentifying in-\nstrumental performers directly from audio where timbral\nfeatures aremore-or -less invariant between players (such\nasinpiano ororganrecordings, forexample).\nThis paper reports our\u0002rst steps indeveloping orna-\nment detection andrecognition applications using MPEG-\n7feature extraction tools. Atthisearly stage wehavecon-\ncentrated onaspecialised repertory thatcrucially depends\nonornamentation foritseffect, 18th-century lutemusic.\nThemusic involvedisaverysmall partofaconsiderable\nrepertory oflutemusic being acquired andstudied inthe\nECOLM corpus-b uilding project, funded bytheUKArts\nandHumanities Research Board, [7].Though audio does\nnotform partofthescope ofthatproject, itmay provide\nuswith suitably mark ed-up encodings which represent the\nscores downtoasuf\u0002cient levelofdetail.\nTwocontrasting pieces bySilvius Leopold Weiss(1687-\n1750) were examined forthispaper ,each ofthem existing\nincommercial recordings.\nThe \u0002rst ishisbest-kno wnwork, the'Tombeau pour\nleComte deLogy' (1721). ATombeau isadoleful work\nwhich commemorates thedeath ofafamous person, some-\nwhat inthemanner ofamusical funeral oration, inthis\ncase Count JanAntonin Losy (orLogy) (c1650-1721),\nsaid tobethe\u0002nest aristocratic luteplayer ofhisage. Its\nmarking is'Adagio' anditistakenveryslowly indeed\nbyeach ofourplayers. Attheopening ofthepiece isa\nseries ofrepeated chords mark edtobespread rather than\nplayed inasingle strok e.Thesecond piece isaPassacaille\ninDmajor .This iscomposed onafalling ground bass,\nwhich isrepeated 12times andoverwhich thecomposer\nweavesavaried texture ofchordal andmelodic variations.\nChord-spreading does notform anexplicit feature ofthe\nnotation ofthispiece. Both pieces contain performance\nmarkings such astrills orappoggiaturas (the same sign\nisused forboth), left-hand slurs and(atthebeginning of\ntheTombeau only) explicit chord-spreading. However,in\nfact,allperformances contain examples ofchords thatare\nspread despite notbeing explicitly somark edinthescore.\nSeeFigure 1andFigure 3fortablature andscores ofthe\nopening phrases ofthese works.\nForboth pieces, weprepared special MIDI \u0002les ofthe\nscores; these contain, aswell asthesounding notes indi-\ncated inthetablature, theornament andchord-spreading\nsigns represented asMeta Textevents. This provided a\nsymbolic reference point towhich therecorded perfor -\nmances canberelated.\nFortheTombeau welookedatthree commercial CD\nrecordings played ontypical baroque lutes oftheperiod.\nInthecase ofthePassacaille, wewere able touse\u0002ve\nrecordings inall,twoonbaroque luteandthree onother\ninstruments: modern classical guitar ,piano andlute- harp-\nsichord. The lute- harpsichord wasagut-strung harpsi-\nchord speci\u0002cally designed toimitate thesound ofthe\nlute; JSBach, afriend ofSLWeiss, issaidtohavetakena\nspecial interest intheinstrument' sdevelopment.2.APPR OACH\nOurcorpus consisted ofuncompressed commercial record-\nings ofperformances ofthetwoabove-mentioned works\nbySLWeiss; Tombeau andPassacaille .\nTheworks arepolyphonic andwere written forthe18th-\ncentury 13-course baroque lute, aninstrument played in\nsimilar fashion totheguitar ,butwith aseries ofdiatonically-\ntuned bass strings extending anoctavebelowthesixnor-\nmal playing strings. Allbutthetwohighest-sounding\nstrings arearranged inpairs (knownas'courses'); the\neight lowest-pitched courses (A1-A2) aretuned inoctaves.\nEach piece consists ofmultiple sections that may be\noptionally repeated bytheperformer .There were three\nluteperformances ofeach piece bydifferent performers;\nthere were alsofurther performances ofthePassacaille in\narrangements foranother three instruments: modern clas-\nsical guitar ,modern piano andlute-harpsichord (an18th-\ncentury hybrid gut-strung harpsichord speci\u0002cally designed\nforkeyboard players toimitate thelute).\nThe scores inwhich themusic hascome downtous\narewritten inthespecial form ofnotation knownastabla-\nture, which, aswell asthenotes tobeplayed, indicates\ncertain other performance features bymeans ofspecial\nsigns attheappropriate point. Just aswith harpsichord,\norganorpiano music, most ofthese ornamental markings\ninvolvetheaddition ofnotes which areextratothemain\nnotes. Inthese cases theornament signs canbethought\nofasashort-hand toinstruct theperformer toinsert addi-\ntional notes from anote pattern 'codebook'. Other cases,\nsuch aschord-spreading orvibrato signs involvetheap-\nplication ofprocedures tothenotes, rather than adding\nnote patterns. This codebook varies signi\u0002cantly between\nperiods, composers, performers, instruments andmusical\nstyles. Thus, wehaveingeneral little prior knowledge\nabout howagivenornament will beperformed. Evenif\nconcrete rules foragivenperiod arederivedfrom histori-\ncalsources, these sources tend todisagree indetail, andin\nanycase there isnoguarantee thataperformer would feel\nobliged tofollo wanyparticular setofrules consistently .\nFurthermore, evenwhen follo wing prescribed rules, aper-\nformer hastheliberty toaddornamentation evenwhen not\nexplicitly notated inthescore.\nWetherefore chose tolimit ourinvestigations toasmall\nsubset ofthepossible performance options open toaplayer .\nThese allinvolvetheoccurrence ofnote-e vents extra to\nthose notated inthescore (orMIDI \u0002le). Inthecase of\nappoggiaturas, trills, mordents, turns, etc., theextranotes\ntend tobemanifested asveryshort eventsclustered around\nthepoint where the'main' note isexpected tobe.Inthe\ncase ofchord-spreading, thechord (asingle multi-note\neventinthescore) actually isperformed asasequence of\nnote-onsets (each generating asustained tone) clustered\naround thepoint where thechord might beexpected to\nsound ifitwere asingle event. Thespreading ofchords,\nespecially inslowmusic, iscrucial insuggesting metri-\ncalorharmonic stress andexpressi veintent; forthisrea-\nsonitisoften veryprecisely notated in19th-century pi-anomusic, yetonealwaysexpects aperformer tospread\nallchords tosome extent (indeed itisalmost technically\nimpossible nottodoso).\n2.1. Ornament Classi\u0002er Construction\n2.1.1. MPEG-7 Standar dFeatur es\nWeused standardised features from theMPEG-7 Interna-\ntional Standard forconstructing theornament classi\u0002er ,\n[23].Both Matlab andC++ reference softw areimplemen-\ntations areavailable from musicstructur e.com .From the\naudio low-leveldescriptor frame work(LLD) weused Au-\ndioSpectrumEn velopeD ,AudioSpectrumBasisD andAudioSpec-\ntrumPr ojectionD .The follo wing spectral attrib utes were\nused inalltheexperiments:\nAttrib ute Value\nloEdge 62.5 Hz\nhiEdge 8000 Hz\nresolution 8bands peroctave\nsampleRate 44100 Hz\nwindo wLength 30ms\nwindo w Hamming\nhop 10ms\nFFT Length 2048\nTable 1.MPEG-7 spectral parameters used forornament\ndetection.\nAudioSpectrumEn velopeD wascalculated bytransfor -\nmation ofaShort-T imeFourier Transform (STFT) Power\nSpectrum:\nX=jFTj2\nN\n2+1T (1)\nThe linear transform matrix, T,partitions theSTFT\nvalues into1\n8thoctavelogarithmically-spaced frequenc y\nbands intherange 62:5\u00008000Hzsuch thatthesum of\npowers ineach spectral band ispreserv edunder transfor -\nmation. Inpractice, thelow-frequenc yresolution ofthe\nresulting logfrequenc yspectrum isdependent upon care-\nfulmulti-resolution implementation ofthelinear trans-\nform, detailed discussion ofwhich isoutside thescope\nofthispaper .\nThe primary feature used wasAudioSpectrumPr ojec-\ntionD andwascalculated byconverting AudioSpectrumEn-\nvelopeD toadecibel scale andapplying adecorrelating\nlinear transform matrix, V,consisting ofKcepstral basis\nfunctions. InMPEG-7, thedecorrelating basis iscalled\nAudioSpectrumBasisD :\nY=10log10(X)V (2)\nEquations 1and 2describe afeature that issimilar\ntothewidely-used Mel-frequenc yCepstral Coef \u0002cients\n(MFCC) feature. Ifwechoose Ttobethelinear -to-Mel\nfrequenc ymap andVtobethediscrete cosine transform\n(DCT) then AudioSpectrumPr ojectionD willbeequivalent\ntoMFCC. Most MFCC-based studies usethe\u0002rst fewcepstrum coef\u0002cients, K\u001420,which relate towide-band\nspectral features such asformants.\nItisourviewthatthisfeature ranking disre gards narro w-\nband pitch-based information inthespectrum which is\ncontained inthehigher order coef\u0002cients. Toadmit sen-\nsitivity topitch content, alargenumber ,K>20,of\nMFCC coef\u0002cients would need tobeused. This isdisad-\nvantageous because higher dimensional features perform\npoorly inclassi\u0002cation tasks. This negativeproperty of\nlargerfeature vectors isknownasthecurseofdimension-\nality inmachine learning andpattern recognition litera-\nture, [24].\nToachie veabalance between feature compactness and\nsensiti vity,MPEG-7 uses SVD orICA analysis ofthecep-\nstral feature space, [25][26].Thus AudioSpectrumBasisD\nandthecorresponding AudioSpectrumPr ojectionD coef\u0002-\ncients were derivedusing theSVD:\n[Y;S;V]=SVD(10log10(XT)); (3)\nandtheresulting basis functions, V,andcoef\u0002cients,\nY,were truncated to20dimensions yielding optimally\ncompact cepstral features.\n2.1.2. Hidden Mark ovModels\nOurapproach models neither theinstrument northeper-\nfomer explicitly .Instead, ahidden Mark ovmodel (HMM)\nofeach performance wasinferred from theAudioSpec-\ntrumPr ojectionD data using Bayesian inference. Models\nwere initialised using afully connected 40-state model\nandmodel parameters were inferred using maximum a-\nposteriori (MAP) estimation, model trimming andparam-\neterextinction toforce low-probability parameters toward\nzero, [29].\nGivenanHMM, theViterbi algorithm estimates the\noptimal state sequence f\u000bigN\u000b\ni=12f1;2;\u0001\u0001\u0001;Sg,foran\nHMM withSstates. TheHMM effectivelyquantizes the\nAudioSpectrumPr ojectionD feature vectors toa1dtime\nseries ofintegers. This technique isconsistent with other\ndiscrete-quantization approaches toaudio-based music in-\nformation retrie val,[27][25],andforms apartoftheMPEG-\n7international standard under thedescriptors SoundMod-\nelDS andSoundModelStateP athD .\nFigures 2and4illustrate piano rollsofthestate se-\nquences obtained bytheViterbi algorithm used with each\nperformance' sHMM. Theornament ground truth judge-\nments, provided byexpert listeners, arealso indicated by\nboxedregions. Comparison with thescores illustrates that\nsteady-state sections ofaudio areexpressed asrepeated\nstates anddynamic spectral beha viours, such asnote on-\nsetsandornaments, arecharacterised bythestate transi-\ntions.\n2.1.3. HMM State Transition Count (STC)\nItisourhypothesis thatHMM state transitions occur at\nhigher rates during ornaments than during non-ornamented\nsegments ofanaudio signal. Totestthishypothsis weconstructed anaudio-based ornament classi\u0002er using the\nHMM state transition count (STC) calculated overamov-\ning\u0002xed-length windo w.\nTodetect transitions welocated thenon-zero \u0002rstdif-\nferences oftheHMM state sequence. State transition lo-\ncations were de\u0002ned asthesample points where changes\ninstate occurred. Thestate transition indicator function of\ntheHMM state sequence, I(\u000bi),mark edstate transition\npositions with a1andsteady-state positions with a0:\nI(\u000bi)=\u001a\n1if j\u000bi\u0000\u000bi\u00001j>0\n0otherwise :\nWedivided thestate transition indicator array intoover-\nlapping \u0002xed-length windo wed subsequences. The win-\ndowduration wasvaried from 100ms to1000ms with a\nhopof10ms. Thesum oftheelements inthewindo wed\nsegments provided amoving state transition count f\btgN\u000b\nt=1.\nThestate-transition count wassmoothed with ahamming\nwindo wwhich wasalsovaried between 100ms and1000ms.\nThelocal maxima ofwindo wed transition counts were\nlocated atthepositi ve-to-ne gativezero crossings inthe\n\u0002rst-order difference function \bt\u0000\bt\u00001.Figure 2and\nFigure 4illustrates thetransition counts inrelation toorna-\nment ground truths foraperformance ofboth theTombeau\nandthePassacaille .\nFigur e1.(upper) tablature notation (lower) score tran-\nscription ofTombeau .\nTombeau: Kirchhof PerformanceState Index\n200 400 600 800 1000 1200 1400 1600 1800 20005101520\n200 400 600 800 1000 1200 1400 1600 1800 2000012345\nTime (1/100s)Transition CountState Transition Count and Ground Truth\nOrnament extent\n(ground truth) Transition Count\nLocal Maxima\nFigur e2.(upper) state piano roll(lower) ornament\nground truth andstate transition counts foraperformance\nofTombeau .\nFigur e3.(upper) tablature notation (lower) score tran-\nscription ofPassacaille .\nHeindel Performance:  State OccupancyState Index\n0 100 200 300 400 500 600 700 800 900 1000510152025\n0 100 200 300 400 500 600 700 800 900 1000123456\nTime (1/100s)Transition CountWindowed Transition Counts (0.35s)\nlocal maxima\npositions ornament\nextent (ground truth) \nFigur e4.(upper) state piano roll(lower) ornament\nground truth andstate transition counts foraperformance\nofthePassacaille .\n2.1.4. Ranking andEvaluation\nTotestthetransition count hypothesis weevaluated itsef-\nfectiveness inanumber ofbinary ornament classi\u0002cation\nexperiments. Thetime locations oftransition count max-\nima,\u00162fMg\u001af1:::N\u000bg,were rankedandsorted by\ndescreasing transition count such that\b\u0016i\u0015\b\u0016i+1.\nToevaluate theperformance ofthebinary ornament\nclassi\u0002er wemeasured theprecision andrecall oforna-\nments with respect toaground truth set,f\njgN\nj=1,pro-\nvided byanexpert listener foreach performance. Max-\nimalocations andornament locations were considered in-\ntersecting ifthelocation ofthemaxima occurred between\nthestart andendpoint (inclusi ve)ofanornament ground-\ntruth judgement.\nPrecision wasde\u0002ned asthenumber ofintersections\nbetween each candidate listofmaxima locations andthe\nground truth ornament locations normalised bythelength\nofthecandidate list:\nPL=f\big\u0016L\ni=\u00161Tf\njgN\nj=1\nL;L=f1:::jfMgjg:(4)\nRecall wasde\u0002ned inasimilar manner ,butthenor-\nmalisation term wasthetotal number ofground-truth or-\nnament locations:RL=f\big\u0016L\ni=\u00161Tf\njgN\nj=1\njN\nj;L=f1::NMg:(5)\nClassi\u0002ers were constructed overtheparameter space\noftransition count andsmoothing windo wlengths, each\ntaking values intherange 100ms to1000ms. Todeter -\nmine theperformance ofaclassi\u0002er with parameters Wc\nandWs,thelength ofthecounting windo wandsmooth-\ningwindo wrespecti vely,theprecision and recall were\ncalculated formultiple performances against each perfor -\nmance' sground-truth judgements. Following standard IR\npractice, therecall values were interpolated tostandard-\nised recall levelsof10%,20%,...,andtheprecision at\neach standardised recall levelwasaveraged across thetest\ngroup ofperformances. Themean ofprecision andrecall\natthebreak-e venpoint wasused asaranking oftheclas-\nsi\u0002er' sperformance. Thehighest performing parameters\novereach group oftestperformances arereported below.\n3.RESUL TS\n3.1. Tombeau\nInthefollo wing experiments, thewindo wlengths Wcand\nWswere varied intherange 100ms-1000ms foreach test\ngroup. Inthe\u0002rstexperiment, theclassi\u0002ers were tested\nonthree luteperformances oftheopening oftheTombeau .\nThis sequence contained 10chord spreads andtwoslurs\nwith minor differences inthenumber ofornaments be-\ntween performances. Theresults forthebest performing\nclassi\u0002er ,scored according tothebreak-e vencriterion, are\nshowinFigure 5andTable 2.Theoptimal parameters for\nthistestgroup were atransition count windo wlength of\n400msandasmoothing windo wof900msperforming at\naprecision of77% atastandardised recall rateof80%:\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91Classifier Performance: Tombeau\nRecall (standardised intervals)Precisionperf. 1\nperf. 2\nperf. 3\nbep\nmean\nParameters:\nwindow =0.4s\nsmoothing = 0.9s \nFigur e5.Precision-recall curvesforthree performances\nofTombeau onlutes only.Thebreak-e venpoint isat77%\nmean precision atarecall rateof80%.Recall 0.100 0.200 0.300 0.400 0.500\nPrecision 1.000 1.000 1.000 0.900 0.833\n1.000 0.913 0.713 0.705 0.750\n1.000 0.950 0.830 0.827 0.857\nMean 1.000 0.954 0.848 0.810 0.813\nRecall 0.600 0.700 0.800 0.900 1.000\nPrecision 0.804 0.778 0.764 0.721 0.669\n0.737 0.725 0.678 0.000 0.000\n0.878 0.893 0.862 0.000 0.000\nMean 0.806 0.799 0.768 0.240 0.223\nTable 2.Summary ofprecision-recall results forexperi-\nment 1forthree performances oftheTombeau .\n3.2. Passacaille\nThe second experiment tested classi\u0002er performance on\nthree performances oftheopening phrases ofthePassacaille ,\ntwoonlutes andoneonlute-harpsichord. Alargernum-\nberofornaments with more varied expression occured\ninthese performances than fortheTombeau experiment.\nOrnaments included vibrato, slurs, trills, mordents, chord\nspreads, andappoggiaturas. Thenumber ofground truth\nornaments were 13,19and20respecti velyforeach ofthe\nperformances thus giving atotal relevantsetof52targets.\nThebest performing classi\u0002er inthisexperiment con-\nsisted ofwindo wparameters Wc=350msandWs=\n500ms. The precision was68%atarecall rateof70%,\nseeFigure 6andTable 3.\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91\nParameters:          \nwindow = 0.35s     \nsmoothing = 0.50s\nRecall (standardised intervals)Precisionperf. 1\nperf. 2\nperf. 3\nbep\nmeanOrnament Classifier Performance \nmean\nprecision \nbreak even point \nFigur e6.Precision-recall curvesforthree performances\nofthePassacaille .The break-e venpoint is68% mean\nprecision at70% recall.\nThe third experiment consisted ofamore varied set\nofperformances than thesecond; three lutes, luteharp-\nsichord andmodern acoustic guitar .84ground truth or-\nnaments were identi\u0002ed astargetsinthisexperiment. The\nparameters forthebest performing classi\u0002er wereWc=\n300msandWs=500ms. The mean precision attheRecall 0.100 0.200 0.300 0.400 0.500\nPrecision 0.950 0.783 0.795 0.838 0.866\n1.000 0.815 0.850 0.750 0.760\n0.600 0.747 0.626 0.561 0.593\nMean 0.850 0.782 0.757 0.716 0.740\nRecall 0.600 0.700 0.800 0.900 1.000\nPrecision 0.851 0.750 0.722 0.667 0.000\n0.791 0.758 0.725 0.000 0.000\n0.576 0.535 0.529 0.000 0.000\nMean 0.739 0.681 0.659 0.222 0.000\nTable 3.Summary ofresults ofornament classi\u0002cation\nforthree lute andlute-harpsichord performances ofthe\nPassacaille .\nbreak-e venpoint was60%atarecall rateof60%,seeFig-\nure7andTable 4.\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91Ornament Classifier Performance\nRecall (standardised intervals)Precisionperf. 1\nperf. 2\nperf. 3\nperf. 4\nperf. 5\nbep\nmean\nParameters:\nwindow = 0.30s\nsmoothing = 0.50s \nFigur e7.Precision-recall curvesfor\u0002veperformances\nofthePassacaille tested onarange ofinstruments; three\nlutes, guitar andlute-harpsichord. The break-e venpoint\nwas60% mean precision at60% recall.\n3.3. Discussion\nOur results indicate that thebest performing classi\u0002ers\nperform atareasonable precision-recall rateandtherefore\nsomehw atsupport ourhypothesis that windo wed HMM\nstate transition counts areanestimator ofornamented re-\ngions inaudio recordings ofsolo polyphonic instrument\nperformances. Theperformance does, however,appear to\ndiminish with theaddition ofmore varied ornamentation\nandinstrumentation. This will impact thescalability of\ntheclassi\u0002ers tosigni\u0002cantly largercorpora.\nForamore scalable classi\u0002er ,the\u0002xedwindo wparam-\neterisation applied overallperformances istoorestricti ve\nsofurther workwillbecarried outtoevaluate windo wpa-\nrameters thatadapt toeach performance. Forexample, the\nstate transition count feature isinsensiti vetothesurround-Recall 0.100 0.200 0.300 0.400 0.500\nPrecision 0.950 0.783 0.795 0.838 0.866\n0.833 0.578 0.419 0.438 0.471\n0.833 0.604 0.551 0.470 0.412\n1.000 0.815 0.850 0.789 0.760\n0.600 0.727 0.549 0.552 0.571\nMean 0.843 0.701 0.633 0.617 0.616\nRecall 0.600 0.700 0.800 0.900 1.000\nPrecision 0.819 0.720 0.722 0.629 0.000\n0.476 0.000 0.000 0.000 0.000\n0.410 0.000 0.000 0.000 0.000\n0.791 0.679 0.000 0.000 0.000\n0.506 0.455 0.446 0.000 0.000\nMean 0.600 0.371 0.234 0.126 0.000\nTable 4.Summary ofresults ofornament classi\u0002cation\nfor\u0002veperformances ofthePassacaille onlutes, guitar\nandlute-harpsichord.\ningconte xt;inaperformance with manyshort-duration\nnotes inthenotated score, ornaments might notbeeasily\ndiscriminated from thehighly activebackground. Inthe\nfuture wewillexplore therelationship between theheights\nandthewidths oftheSTC peaks thus seeking toadapt to\nthemean density ofnon-ornamental onsets ineach perfor -\nmance.\n4.CONCLUSIONS\nWedeveloped anHMM-based ornament classi\u0002er thatau-\ntomatically locates ornaments inaudio recordings andeval-\nuated performance against human-judged ground truths\nforasetofrecordings. Wehypothesised thatwindo wed\nHMM state transition counts could beused forlabelling\nregions ofaudio corresponding toornaments andprovided\nsome experimental evidence tosupport thisclaim.\nTheutility ofthemethods presented herein willbeex-\nplored infurther work,including classi\u0002cation ofindivid-\nualornament species andornament '\u0002ltering' foraudio-\nbased information retrie valusing score-based queries.\n5.REFERENCES\n[1]Abdallah, S.andPlumble y,M.,Anindependent\ncomponent analysis approach toautomatic music\ntranscription ,inProc.ofthe114th Convention\nofAudio Engineering Society ,Amsterdam, March\n2003.\n[2]Bello, J.P.,Towards theautomated analysis of\nsimple polyphonic music: Aknowledge-based ap-\nproach, Ph.D. thesis, University ofLondon, 2003.\n[3]Brown,J.C.andSmaragdis, P.,Independent Com-\nponent Analysis forAutomatic Note Extraction\nfrom Musical Trills ,J.Acoust. Soc. Am. 2004.\ninpress.[4]Byrd, D.and Crawford, T.Problems ofmusic\ninformation retrie valinthereal world, Informa-\ntionProcessing andMana gement ,38(2): 249-272,\n2002.\n[5]The Centre for the History and Analysis of\nRecorded Music (CHARM) atGeor getown\n(US) and Southampton (UK) Universities:\nhttp://www .geor getown.edu/departments/AMT/music/\nbowen/CHARM.html\n[6]UK Charm' sAnalytical techniques page at:\nhttp://www .soton.ac.uk/ musicbox/charm5.html\n[7]http://www .ecolm.or g\n[8]Fujishima, T.,Realtime Chord Recognition ofMu-\nsical Sound: aSystem Using Common Lisp Mu-\nsic, Proceedings ofthe1999 International Com-\nputer Music Confer ence,pp.464-467, Beijing,\nChina\n[9]Goto, M.and Hayamizu, S.,Areal-time music\nscene description system: detecting melody and\nbass lines inaudio signals. InProc.IJCAI Work-\nshop onCASA ,pages 31-40, 1999.\n[10] Ornaments ,11.Indextoornaments and table\nofsigns, NewGroveDictionary ofMusic andMu-\nsicians ,NewYorkand London, 2000. Online at\nwww .grovemusic.com\n[11] Hainsw orth, S.and Macloed, M.The Auto-\nmated Music Transcription Problem http://www-\nsigproc.eng.cam.ac.uk/ swh21/ontrans.pdf\n[12] Leman, M.andCarreras, F.,The Self-Or ganization\nofStable Perceptual Maps inaRealistic Musi-\ncalEnvironment, Proceedings ofJIM96, Journees\nd'Informatique Musicale 16-18 May 1996, Caen,\nFrance.\n[13] Martin, K.D.,Automatic Transcription ofSimple\nPolyphonic Music: RobustFront End Process-\ning.M.I.T .Media Lab Perceptual Computing\nTechnical Report 399, November 1996, presented\nattheThird Joint Meeting oftheAcoustical\nSocieties ofAmerica andJapan, December ,1996.\nhttp://x enia.media.mit.edu/ kdm/research/papers/kdm-\nTR399.pdf\n[14] Martin, K.D., ABlackboard System for\nAutomatic Transcription of Simple Poly-\nphonic Music. M.I.T .Media Lab Perceptual\nComputing Technical Report 385, July 1996.\nhttp://x enia.media.mit.edu/ kdm/research/papers/kdm-\nTR385.pdf\n[15] Martin, K.D.andScheirer ,E.D.,Automatic Tran-\nscription ofSimple Polyphonic Music: Integrating\nMusical Knowledge. Presented atSMPC ,1997.\n[16] Mitchell, D.W.Acomparison ofembellishments\ninperformances ofbebop with those inthemusic of\nChopin, MM, Music: University ofArizona, 1992.[17] G.Monti andM.Sandler ,Automatic polyphonic\npiano note extraction using fuzzy logic inablack-\nboard system ,inProc.ofDAFx-02 ,Hamb urg,\nGerman y,2002.\n[18] Pickens, J.,Bello, J.P.,Crawford, T.,Dovey,M.,\nMonti, G.,Sandler ,M.andD.Byrd, Polyphonic\nscore retrie valusing polyphonic audio queries: A\nharmonic modelling approach ,inTheJournal of\nNewMusic Resear ch,vol.32,no.2,June 2003.\n[19] Eric Scheirer: web page onaudio transcription:\nhttp://web .media.mit.edu/ eds/transcription.html\n[20] Sforzando ,Windowssoftw areprogram designed\nforanalysing tempo anddynamic change inrecord-\nings at:http://www-personal.usyd.edu.au/ rjohn-\nson/sforzando/\n[21] Smaragdis, P.,and Brown, J.C., Non-Ne gative\nMatrix Factorization forPolyphonic Music, 2003\nIEEE Workshop onApplications ofsignal Process-\ningtoAudio andAcoustics ,October 19-22, 2003,\nNewPaltz, NY.\n[22] Williams, J.K.,Amethod forthecomputer -aided\nanalysis ofjazz melody inthesmall dimensions.\nAnnual reviewofjazzstudies ,USA Vol.III(1985)\n41-70.\n[23] ISO 15938-4(2002) Multimedia Content Descrip-\ntion Interface Part4(Audio). International Stan-\ndards Organization 2002.\n[24] Duda, R.,Hart, D.andStork, D.Pattern Classi\u0002ca-\ntion,2ndEd.NewYork: Wiley,2002.\n[25] Case y,M.,MPEG-7 Sound Recognition Tools,\nIEEE Transactions onCircuits andSystems: Video\nTechnolo gy.IEEE, June 2001.\n[26] Case y,M.,General Audio Information Retrie val,\ninInnocenti, P.(Ed.), MMIR: Multimedia Informa-\ntionRetrie val.AIDA,2004.\n[27] Foote,J., ASimilarity Measure forAutomatic Au-\ndioClassi\u0002cation. InProc.AAAI 1997 Spring Sym-\nposium onIntellig entIntegration andUse ofText,\nImage,Video, andAudio Corpor a.Stanford, March\n1997.\n[28] Hu, N.,Dannenber g,R.B.and Tzanetakis G.\nPolyphonic Audio Matching and Alignment for\nMusic Retrie valIEEE Workshop onApplications\nofSignal Processing toAudio andAcoustics Octo-\nber19-22, 2003, NewPaltz, NY.\n[29] Brand, M., Pattern disco veryviaentrop ymin-\nimization, inProc.Arti\u0002cial Intellig ence and\nStatistics #7.1998."
    },
    {
        "title": "Architecture for an MPEG-7 Web Browser.",
        "author": [
            "Òscar Celma"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417229",
        "url": "https://doi.org/10.5281/zenodo.1417229",
        "ee": "https://zenodo.org/records/1417229/files/Celma04.pdf",
        "abstract": "The MPEG-7 standard provides description mechanisms and taxonomy management for multimedia documents. There are several approaches to design a multimedia database system using MPEG-7 descriptors. We discuss two of them: relational databases and native XML databases. We have implemented a search and retrieval application for MPEG-7 descriptions based on the latter.",
        "zenodo_id": 1417229,
        "dblp_key": "conf/ismir/Celma04",
        "keywords": [
            "MPEG-7 standard",
            "description mechanisms",
            "taxonomy management",
            "multimedia database system",
            "approaches",
            "relational databases",
            "native XML databases",
            "search and retrieval application",
            "MPEG-7 descriptors",
            "XML databases"
        ],
        "content": "ARCHITECTURE FORANMPEG-7WEBBROWSER\n`Oscar Celma\nMusicTechnology Group\nUniversitatPompeuFabra\nABSTRACT\nThe MPEG-7 standard provides description mechanisms\nand taxonomy management formultimedia documents.\nThere areseveralapproaches todesign amultimedia database\nsystem using MPEG-7 descriptors. Wediscuss twoof\nthem: relational databases andnativeXML databases. We\nhaveimplemented asearch andretrie valapplication for\nMPEG-7 descriptions based onthelatter .\n1.INTRODUCTION\nMultimedia metadata information represents —from the\nservice providers’ point ofview— anice waytoaddvalue\ntomultimedia resources (i.eaudio visual ﬁles) distrib ution.\nManaging audio visual essence implies tostructure itsas-\nsociated metadata andcontent-based descriptors; using de-\nscription schemes, taxonomies andontologies, toorganize\nameaningful data knowledge representation.\nMPEG-7 standard provides content description forau-\ndiovisual content, deﬁning normati veelements asDescrip-\ntors, Description Schemes andaDescription Deﬁnition\nLanguage (DDL). Themain useofMPEG-7 standard is,\nthen, todescribe alltheinformation ofmultimedia assets.\nInthat conte xt,weusually talk about metadata (that is\ndata about data) and, inthat particular case, data about\nthemultimedia information thatisdescribed. Thus, creat-\ningMPEG-7 documents allowauser toquery andretrie ve\n(parts of)multimedia andaudio visual information.\nFormanyyears, database management systems (DBMS)\nhavebeen used toimplement efﬁcient textbased informa-\ntion retrie valsystems. Nevertheless, inthearea ofmul-\ntimedia, andinparticular intheMusic Information Re-\ntrievalﬁeld, there isstill alotofongoing research and\nthere areopen questions concerning architectural andde-\nsign aspects related toDBMS.\nInthispaper wewill expose andreviewdifferent ap-\nproaches tomanage MPEG-7 with DBMS. Moreo ver,a\nweb-based system tosearch andretrie veMPEG-7 descrip-\ntions, willbeshown.Theworkpresented inthispaper has\nbeen carried onunder theframe oftheOpenDrama Euro-\npean ISTproject.\nPermission tomakedigitalorhardcopiesofallorpartofthisworkfor\npersonalorclassroom useisgrantedwithoutfeeprovidedthatcopies\narenotmadeordistributedforpro®torcommercial advantageandthat\ncopiesbearthisnoticeandthefullcitationonthe®rstpage.\nc°2004UniversitatPompeuFabra.2.OPENDRAMA PROJECT\nOpenDrama European ISTproject aims thedeﬁnition, de-\nvelopment andintegration ofanovelplatform toauthor\nandtodeliverrichcross-media digital objects oflyric opera.\nMPEG-7 hasbeen used inOpenDrama asbase technology\nforamusic information retrie valsystem.\nThemost important services derivedfrom OpenDrama\nare:(i)asearch andretrie valsystem tobrowsemutimedia\ncontent and(ii)thestreaming ofmetadata synchronized\nwith multimedia data. System architecture isbased on\nﬁgure 1.Forboth OpenDrama’ sservices, theuseofthe\nMPEG-7 standard ﬁtstheneeds. Inaddition, theuseof\nstandards insuch aproject isakeypoint tomakeitareal\nopen, interoperable, maintainable anddurable system.\n3.OVERVIEWOFMPEG-7STANDARD\nMPEG-7, formally named Multimedia Content Descrip-\ntion Interf ace, aims tocreate astandard forthedescrip-\ntionofthemultimedia content data. Themain goal ofthe\nMPEG-7 standard istoprovide structural andsemantic\ndescription mechanism formultimedia content [1].\nMPEG-7 descriptors aredesigned fordescribing dif-\nferent types ofinformation; from low-levelaudio visual\nfeatures, tohigh-le velsemantic objects. Ideally ,most de-\nscriptors corresponding tolow-levelwould beextracted\nautomatically ,whereas some human interv ention would\nberequired forproducing high-le veldescriptors.\nXML hasbeen adopted astheformat torepresent MPEG-\n7descriptors. Also, MPEG-7 DDL isanextension ofthe\nXML Schema (published byW3C1).XML Schema pro-\nvides themeans fordeﬁning thestructure ofXML docu-\nments, thatis;simple andcomple xdatatypes, type deriva-\ntion andinheritance, element ocurrence constraints and,\nﬁnally ,namespace-a wareforelements andattrib utes dec-\nlarations.\n4.MPEG-7ANDMULTIMEDIA DATABASE\nSYSTEMS\nKosch [2]deﬁnes anarchitecture foramultimedia database\nmanagement system thatincludes MPEG-7 descriptions,\naswell asthestreaming service oftheassociated audio-\nvisual ﬁles. Anarchitecture ofamultimedia database, in-\nspired in[2],isdepicted inﬁgure 1.\n1http://www .w3.org/XML/SchemaFigure1.Multimedia database architecture using MPEG-7.\nStarting from thefeature extraction andannotation pro-\ncess ofamultimedia asset, theMPEG-7 descriptors are\ngenerated andstored inarepository .Typically ,inamulti-\nmedia database system onecandistinguish twoquery sce-\nnarios:pull andpush .Inapull scenario, auser submits\nqueries tothesystem andrecei vesasetofdescriptions\nsatisfying theconstraints ofthequery .Ontheother hand,\ninapush scenario, asoftw areagent selects MPEG-7 de-\nscriptions andperforms asetofactions afterw ards. One\nofthese actions could be,forinstance, proposing tousers\nmedia information anditsdescription based ontheir pref-\nerences. Hence theuser agent isﬁltering audio visual in-\nformation according tometadata description.\nThere areseveralapproaches andparadigms forstruc-\nturing MPEG-7 data intoadatabase system. Inthispaper\nwepoint outtwogeneral solutions: (i)tomodel MPEG-7\ndata intoarelational database system and(ii)touseana-\ntiveXML database (XML:DB) [3]. Theformer isbased\nontheclassic concept ofarelation, while thelatter has\nXML documents asitsfundamental unitof(logical) stor-\nageinthedatabase.\nThenexttwosections explain both approaches.\n4.1.Relational Databases\nThe workthat hasbeen previously done forstructuring\nMPEG-7 data intoadatabase system isbased, mostly ,on\ntheclassical relational model —plus some extensions to\nadapt theXML information into therelations. Forin-\nstance, Jacob [4]hasimplemented adatabase tomanage\ndescriptions ofsound objects —in MPEG-7— using aPost-\ngreSQL database. Asetofextensions (mainly program-\nming triggers) hasbeen designed totakeintoaccount in-\nsertions, updates anddeletions ofelements inanMPEG-7\ndocument. Anextraction rule engine allowstogenerate\ntheMPEG-7 data. Yet,itisnotclear ,from adatabase\nuser point ofview,howtoquery (select )theMPEG-7\ndata inside theDB. D¨oller andKosch [5]havedesigned\nanMPEG-7 Multimedia DataCartridge.This system is\nanextension ofobject-relational DBMS Oracle 9i,pro-\nviding amultimedia query language, access tomedia, and\nindexing capacities. Descriptors intheMPEG-7 schemaaremapped toobject types andtables, thus allowing to\nexpress queries inanhybrid SQL andXPathlanguage.\nBoth systems permit tovalidate XML elements with\ntheXML Schema (i.evalidating MPEG-7 descriptions us-\ningtheMPEG-7 DDL), providing awaytoassure data in-\ntegrity.However,duetothefactthattheMPEG-7 DDL\nistightly associated totheXML Schema deﬁnition, and\nthedifﬁculties ofreverse-engineering themodel, manag-\ningMPEG-7 descriptors isequivalent tomanaging XML\ndocuments [6].Thus, inthisapproach there isabigof\neffortintransducing thewhole MPEG-7 Schema within\nasetofrelations (i.etables). Evenso,possible changes\nontheMPEG-7 standard would imply toredo partofthe\ndatabase schema, which might beunfeasible when asys-\ntemisbeing exploited. Tocope thisproblem inageneral\nsense, there hasbeen considerable research concerning the\nautomatic mapping between schema deﬁnitions ofXML\ndocuments andrelational database schema ([7], [8]), but\nmost oftheworkisfocused onDTD deﬁnitions instead\nofXML Schema2,sotheydonotsufﬁceforthemanage-\nment ofMPEG-7 data.\n4.2.NativeXMLDatabases\nOurapproximation tostructuring MPEG-7 descriptors is\ntouseanativeXML:DB. According toBourret [9],there\naremore than 35(open source andcommercial) native\nXML database systems. NativeXML:DB deﬁne a(logi-\ncal)model foranXML document, andstores andretrie ves\ndocuments according tothatmodel. NativeXML:DB make\nuseofcollections asinternal folders forrepositories of\nXML documents.\nWesterman [6]hasreviewed theexisting database sys-\ntems thatcanmanage MPEG-7 media descriptions. Their\nstudy includes asetofnativeXML database. Atable com-\nparision between systems unveilsalack ofdata integrity\nvalidation bymost ofthem. Data integrity isakeypoint in\nanydatabase system. InanativeXML:DB integrity val-\nidation isdone byparsing anXML document through its\n2Thecriticaldifferencebetween DTDsandXMLSchemaisthat\nXMLSchemausesanXML-based syntax,whereasDTDshaveaunique\nsyntaxheldoverfromSGMLDTDsschema deﬁnition. Data integrity should beveriﬁed after\naninsertion ofanewXML document totheDB,orafter a\nmodiﬁcation ofanalready existent document. None ofthe\nnativeXML:DB presented in[6]allowfullschema vali-\ndation ofMPEG-7 descriptors through MPEG-7 DDL. To\nsolvethisissue, aproposal ofschema validation applied\ntonativeXML:DB ispresented in[10].\nAsnativeXML:DB arestillreaching maturity ,another\nimportant aspect istodeﬁne languages thatallowtoquery ,\ninsert, update anddelete elements inthedocument. The\nmost used languages —by XML:DB implementations—\ntoquery andtoretrie ve(part of)documents aretheW3C\nXPath2.03andXQuery1.04recommendations. XQuery\nisafunctional, strongly typed language thatsatisﬁes the\nrequirements ofadatabase query language. Updating XML\ndata ispossible withXUpdate initiati ve5.XUpdate isa\nsimple XML update language. Itcanbeused tomod-\nifyXML content bysimply declaring, inanXML syntax,\nwhat changes should bemade.\nNextexamples showthepowerofXQuery expressions.\nExample 1showsanXQuery expression toretrie veallthe\nMPEG-7 audio visual segments containing media informa-\ntion. Example 2showsanXQuery expression toretrie ve\nallMPEG-7 person agents, whose roleis’Singer’, andthe\ncharacters theyplay.\nfor$segment in//AudioVisualSegment\nlet$title:=\n$segment/CreationInformation/\nCreation/Title/text()\norderby$title\nreturn\nfor$mediain$segment/MediaInformation/\nMediaProfile\nlet$file:=$media/MediaInstance/\nMediaLocator/MediaUri/text()\nlet$type:=$media/MediaFormat/\nContent/Name/text()\nreturn\n<ahref=\"{$file}\"> {\n$title,\"[\",$type,\"]\"\n}</a>\nExample1.XQuery expression forretrie ving alisting of\nmultimedia items —title andsupport type.\nfor$creator in\n/Mpeg7/Description/MultimediaContent\n/*/CreationInformation/Creation/Creator\nwhere\n$creator/Role[@href=\"OpenDramaSinger%\"]\nand\n$creator/Agent[@xsi:type=\"PersonType\"]\norderby\n$creator/Agent/Name/FamilyName\nreturn\n3http://www .w3.org/TR/2004/WD-xpath20-20040723\n4http://www .w3.org/XML/Query\n5http://www .xmldatabases.or g<agent>\n{\nlet$completeName:= $creator/Agent/Name\nlet$name:= $completeName\n/GivenName/text()\nlet$surname:= $completeName\n/FamilyName/text()\nreturn\n<singer>\n{\n$name,\"\",$surname\n}\n</singer>\n}\n{\nlet$completeName:= $creator/Character\nlet$name:= $completeName\n/GivenName/text()\nlet$surname:= $completeName\n/FamilyName/text()\nreturn\n<character>\n{\n$name,\"\",$surname\n}\n</character>\n}\n</agent>\nExample2.XQuery example todisplay thesingers and\nthecharacters theyplay.\nBoth examples revealthepotential oftheXQuery lan-\nguage. When combined with XHTML, XQuery allowsto\neffortlessly create web applications. Direct access tothe\nXML data, andthen formating ittobedisplayed bythe\nweb browser ispretty straightforw ard.\n5.MPEG-7WEBBROWSERARCHITECTURE\nWehaveimplemented asearch andretrie valweb system,\nbased onMPEG-7. Wefollowthepull application sce-\nnario depicted inﬁgure 1—aclient requests audio visual\ninformation anditsassociated metadata. Inoursystem,\nmetadata information isrelated tothecreation andpro-\nduction process —including media information—, aswell\nasthestructural (temporal) decomposition ofanopera.\nFigure 2depicts thewhole system architecture. Aweb\nbrowser queries theXML:DB according toMPEG-7 de-\nscriptors. Inorder toaccess tothedatabase, theserverap-\nplication transforms theMPEG-7-based query toanXQuery\nexpression. Nextstep istosearch into thedatabase for\nsegments matching thecriteria, toprocess theXML re-\nsults, andtoreturn HTML documents tothebrowser (as\nwell asthemultimedia ﬁles retrie vedfrom thestreaming\nserver).Finally ,eXist6,anativeopen source XML:DB,\nisused tomanage MPEG-7 documents. eXist implements\nbothXQuery andXUpdate languages.\n6http://exist-db.orgFigure2.MPEG-7 WebBrowser architecture.\n6.CONCLUSIONS\nMPEG-7 provides description mechanisms formultime-\ndiacontent; however,existing systems based onMPEG-7\nstandard (specially intheaudio ﬁeld) arestillimmature.\nThere areseveralapproaches tomodel database man-\nagement ofMPEG-7 descriptions. Relational ornative\nXML:DB approach might depend ontheapplication re-\nqueriments. Beforehand, nativeXML:DB seems anatural\nwaytoorganise MPEG-7 descriptors andMPEG-7 DDL\nasitistigthly associated toXML structure. Eventhough,\nthere isstillsome workpending onthedesign ofXML:DB\n—as well asthestandarization ofXQuery 1.0language—\ntoprovide fullfuncionality foranMPEG-7 based appli-\ncation. Using arelational database tomanage MPEG-7\ncanderivetoan“artiﬁcial” orcomple xdatabase schema,\naswell asanoverhead ofprogramming (tovalidate data\nintegrity,forinstance).\n7.REFERENCES\n[1]Manjunath, B.S.,Salembier ,P.andSikora,T.\nIntroduction toMPEG7:Multimedia Content\nDescription Language,Ed.Wiley,2002.\n[2]Kosch, H.DistributedMultimedia Database\nSystemssupported byMPEG-7 andMPEG-\n21,CRC Press, 2004.\n[3]XML:DB. “XML:DB Initiati vefor XML\nDatabases”. Available online: http://xmldb-\norg.sourcefor ge.net/inde x.html\n[4]Jacob, M.“Managing largesound databases\nusing MPEG-7”, Proceedings AES25thInter-nationlConference ,London, UK, 2004 July\n17–19.\n[5]D¨oller,M.andKosch, H.“AnMPEG-7 Mul-\ntimedia Data Cartridge”, Proceedings ofthe\nSPIEConferenceonMultimedia Computing\nandNetworking 2003(MMCN03) ,Santa\nClara, CA, 2003 January 29–31.\n[6]Westermann, U.,Klas, W.“Ananalysis of\nXML database solutions forthemanagement\nofMPEG-7 media descriptions”, ACMCom-\nputing Surveys(CSUR), Volume 35,Issue 4.\nDecember 2003.\n[7]Shanmug asundaram, J.,Tufte, K.,He, G.,\nZhang, C.,DeW ittD.,Naughton, J.“Rela-\ntional Databases forQuerying XML Docu-\nments: Limitations andOpportunities”, Pro-\nceedings of25thInternational Conferenceon\nVeryLargeDataBases, Edinb urgh,Scotland,\nUK, 1999 September 7–10.\n[8]Tian,F.,DeW itt,D.,Chen, J.,Zhang, C.“The\nDesign andPerformance Evaluation ofAlter -\nnativeXML Storage Strate gies”,ACMSigmod\nRecord,Volume 31,Number 1,March 2002.\n[9]Bourret, R. “XML Database Products:\nNativeXML Databases”. Available online:\nhttp://www .rpbourret.com/xml/XMLDatabaseProds.htm\n[10] Hu,G.,Li,Q.“Schema Validation Applied to\nNativeXML Databases”, International Con-\nferenceonAdvances inInfrastructurefor\ne-Business, e-Education, e-Science ,ande-\nMedicine ontheInternet ,L’Aquila, Italy,\n2002 July 29–August 4."
    },
    {
        "title": "A Brazilian Popular Music Oriented Digital Library For Musical Harmony E-Learning.",
        "author": [
            "Fernando William Cruz",
            "Edilson Ferneda",
            "Márcio da Costa P. Brandão",
            "Evandro de Barros Costa",
            "Hyggo Oliveira de Almeida",
            "Murilo Bastos da Cunha",
            "Rafael de Sousa",
            "João Denicol",
            "Carlos da Silva"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417042",
        "url": "https://doi.org/10.5281/zenodo.1417042",
        "ee": "https://zenodo.org/records/1417042/files/CruzFBCACSDS04.pdf",
        "abstract": "This poster presents a digital library proposal conceived for people interested in acquiring knowledge about Brazilian popular music harmony, particularly in Choro. This Brazilian musical style is a complex popular music form based on improvisation, although it contains classical music elements such as the counterpoint. We are proposing two ways of accessing the music virtual library content: a guided navigation mode, in which users interact with a cooperative Web-based learning system; and a free navigation mode, in which users can make their own queries, both through browsers or client applications.",
        "zenodo_id": 1417042,
        "dblp_key": "conf/ismir/CruzFBCACSDS04",
        "keywords": [
            "digital library",
            "knowledge acquisition",
            "Brazilian popular music",
            "Choro",
            "improvisation",
            "classical music",
            "Web-based learning system",
            "free navigation",
            "queries",
            "client applications"
        ],
        "content": "ABRAZILIANPOPULARMUSICDIGITALLIBRARY\nORIENTEDTOMUSICALHARMONYE-LEARNING\nFernandoW.Cruz EdilsonFerneda MárcioBrandão\nCatholicUniversityofBrasília\nComputerScienceDepartment\nBrasília,BRAZIL\nfwcruz@ucb.brCatholicUniversityofBrasília\nComputerScienceDepartment\nBrasília,BRAZIL\neferneda@pos.ucb.brUniversityofBrasília\nComputerScienceDepartment\nBrasília,BRAZIL\nbrandao@unb.br\nEvandrodeB.Costa HyggoO.deAlmeida MuriloB.daCunha\nFederalUniversityofAlagoas\nInformationTechnologyDepartment\nMaceió,BRAZIL\nebc@fapeal.brFederalUniversityofCampinaGrande\nElectricalEngineeringDepartment\nCampinaGrande,BRAZIL\nhyggo@dee.ufcg.edu.brUniversityofBrasília\nInformationScienceDepartment\nBrasília,BRAZIL\nmurilobc@unb.br\nRafaelT.deSousaJr.JoãoRicardoE.DenicolCarlosAlanP.daSilva\nUniversityofBrasília\nElectricalEngineeringDepartment\nBrasília,BRAZIL\ndesousa@ene.unb.brCatholicUniversityofBrasília\nComputerScienceDepartment\nBrasília,BRAZIL\njr@rocho.netFederalUniversityofCampinaGrande\nArtsDepartment\nCampinaGrande,BRAZIL\nalan@liaa.ch.ufpb.br\nABSTRACT\nThisposterpresentsadigitallibraryproposalconceived\nforpeopleinterestedinacquiringknowledgeabout\nBrazilianpopularmusicharmony,particularlyinChoro.\nThisBrazilianmusicalstyleisacomplexpopularmusic\nformbasedonimprovisation,althoughitcontains\nclassicalmusicelementssuchasthecounterpoint.We\nareproposingtwowaysofaccessingthemusicvirtual\nlibrarycontent:aguidednavigationmode,inwhich\nusersinteractwithacooperativeWeb-basedlearning\nsystem;andafreenavigationmode,inwhichuserscan\nmaketheirownqueries,boththroughbrowsersorclient\napplications.\n1.INTRODUCTION\nAlthoughBrazilhasavarietyofstylesinpopularmusic,\ntheprofessionalteachingofnationalmusicaltraditions\nhasbeenpassed,almostcompletely,intraditional\nenvironmentsofmusicalteaching.Anelementary\nhistoricaldescriptionofBrazilianprofessionalmusic\nteachingthatpointstosocialandeconomicaltopicsof\nresourcesapplicationinMusicteachingdemonstratethe\nhegemonyofthesocalledclassicmusicatformal\neducation,whichmakesthepopularmusicteaching\nawayfromtheformalenvironments.\nThereisalargegapintheproductionofgood\neducationalmaterialthatfocusonBrazilianmusic.\nFacingthisfact,digitalandorganizedcontents,thathave\nasmaingoalstherescueandbroadcastofimportant\ncomposersoftheBraziliantradition,bringavery\nimportantcontributiontothecountryculturalidentity.Ontheotherhand,thevertiginousenhancementof\ninformationavailableattheWeb,mostlymultimedia,\nhasgeneratedaconsiderabledemandfortoolsthatcan\naccessandmanipulatethem.Specifically,onmusical\ninformation,despitethemanyproposalswith\nexperimentalandcommercialaspects,theyhave\nfrustratedthepublicexpectations.\nObviously,theamountofmusicalmaterialavailable\nhasfollowedthegeneraltendency.Nowadays,these\nmaterialsareavailabletocomputerusersaroundthe\nworldandnewmusicaldatabasescontinuetobecreated\nandconstantlyupdated.Severalprojectsshowthatscore\nretrievalswithinmusicaldatabasescanbeperformed\nthroughsearchesonthetextdescriptionsofthemusical\npieces,whichusuallyincludesdetailssuchasthetitleor\nthenameofthecomposerorperformer,asintheLester\nLevyCollectionSheetMusic[12].Listeningtothescores\nbeingretrievedisalsoallowedinsomesystemssuchas\ntheVariations2[17].Morecomplexretrievalsbasedon\nparticularmelodiclinescanbedoneinsystemssuchas\ntheThemefinder[16].\nEventhoughthesesystemsareusefultopeople\ninterestedinmusicingeneral,theyarenotableto\nsupportsystemsinapopularmusiceducationalcontext.\nThisworkaimstodesignadigitallibrary[1]\nframeworkabletoprovideanappropriatesupportto\npeopleinterestedinBrazilianmusicalculturebymeans\nofameaningfulsetofmusicalpiecesanitseducational\nuse.\nWepresentinSection2ourmusicalharmonye-\nlearningarchitecturebasedonaBrazilianpopularmusic\ndigitallibrary.InSection3weintroducetheHarmony\nTreesTheory,whichiswherethedidacticalcontentsand\ntheretrievalmethodsarebasedon.InSection4we\npresentsomeconsiderationsconcerningourdigital\nlibraryandtheprincipleswhichguideditsdevelopment.\nFinally,inSection5weintroducesomeconsideration\nregardingthecurrentstatusofourprojectanddrawsome\nconclusions.Permissiontomakedigitalorhardcopiesofallorpartofthiswork\nforpersonalorclassroomuseisgrantedwithoutfeeprovidedthat\ncopiesarenotmadeordistributedforprofitorcommercial\nadvantageandthatcopiesbearthisnoticeandthefullcitationon\nthefirstpage.\n©2004UniversitatPompeuFabra.2.AMUSICALHARMONYE-LEARNING\nARCHITECTURE\nThisworkproposestoputavailablemusicalmaterialina\nwebportaldesignedtohelpavirtualcommunitymade\nupofmusicalharmonye-learningstudents[10]and\npeopleinterestedinBrazilianpopularmusic[11].\nThegeneralarchitectureofthesystemispresentedin\nFig.1.Itsmodulesaremadeofobjectsdisposedina\nhierarchicalmanner.Thehypermediadocumentis\nformedwithwebpages.Inthenon-guidednavigation,\nthelearnerhastotalcontrolofhis/herowndecision.In\ntheguidednavigation,thesystemdetermineswhichis\nthenextdocumentexhibited,basedonthelearner’s\nprofile.Themechanismofdefinitionofthenextstepina\nguidednavigation,aswellasaprofileinspectionbythe\nlearner,isdescribedbyCostaetal[8].\nITS\nFigure1.Generaloverviewofthesystem.\nTheMusicIntelligentTutoringSystem(ITS)\ncomponentiscenteredonamultiagent-basedapproach.\nItisbasedonpreviousworks[4,6,7],andisbeing\ndevelopedonaagentbasedplatformnamedCOMPOR\n[2].TheITSisdefinedinsuchawaythateachagentis\nresponsibleforacertainaspectofdomain’spartitionin\nmusicalharmony[10].\nExercisesandexamplesmaybeconstructedinterms\noftheavailablemusicalobjectsinthedigitalmusical\nlibraryshowninFig.1.Inadidacticway,thereisaneed\ntoassurethatanappropriateexemplificationofthe\nharmonytreesisgiventotheapprentice.Thedidactical\ncontentstobepresentedbytheITSunderdevelopmentis\nrelatedtoMusicalHarmony,anditsbasedonthe\nHarmonyTreesTheorydescribedinthenextsection.As\naconsequence,themechanismsforsearchingand\nindexingexcerptsofsongsarebasedinthistheory.\n3.THEHARMONYTREESTHEORY\nThee-learningsystemwearemakingavailableisrelated\ntotheHarmonyTreesTheory,proposedbyJoséde\nAlencarSoares(a.k.a.AlencarSeteCordas).Heisa\nguitarplayerandamusicteacher,andhasbeenbuilding\nacompilationofthemostcommonharmonicstructures\nfoundinBrazilianpopularmusicformorethanfifteen\nyears.Fromthiscompilation,heconstructedacorpusof\npopularsongs,useditinaninformalstatisticalanalysis\nandgivingtheprobabilityofoccurrenceofparticular\nharmonicsequences,andproposedamusicalharmonytheory.Thistheoryhasbeenusedwithsuccesstoexplain\ntheharmonicstructureswithintonalmusic,particularly\nintheBrazilianpopularmusic,andithasbeenusedboth\ninmusicaleducationandanalysis.Thetheoryis\nrepresentedbydiagrams,whichshowgraphicallythe\nharmonicpossibilitiesinvolved.\nThediagramshowninFig.2depictsthebasic\nharmonytree,whichcontainstheharmonicprogressions\nfoundinmostpopularsongsinthemajorkeys.Roman\nnumeralsareusedtorepresentchordsrelativelytothe\nfirstdegreeofthekey(I).Minorchordsarerepresented\naddingaletter“m”aftertheRomannumeral.Whentwo\nRomannumeralsareinvolved,thefirstonerepresents\nthedegreerelativelytothesecondone.Thesquare\nshapesinthediagramrepresentdominantchordswhich\npreparestotheresolution(roundshapes)followingthem.\nTheshadedpartofthefigurepresentsthepossible\nmodulationsusedinBrazilianpopularmusic.The\nthicknessofthearrowsisassociatedwiththefrequency\nwithwhichaparticularpathinthediagramoccursin\npopulartunes.ItcanbeseenfromFig.2,forexample,\nthatthemostusualprogressioninmajorkeysisthe\nsequenceI–V7/II–IIm–V7–I(itsinstantiationintheC\nmajorkeycorrespondstothesequenceC–A7–Dm–G7–C).\nTherefore,amodulationtoarelativekeyismore\ncommonthenothersmodulationsasin,forexample,the\nsequenceC-E7-AmfortheCmajorkey.\nV7/IV\nII V7IIIV\nV7/IIIIm\nV7/VIVImRelative\nIIIIIIm\nV7/IIIV7/IIIMinorthird\nMajorthird\nV7/IVIVMajorfourth\nV7/IIIbIIIbFlatthird\nV7/VIbVIbMajorsixthModulations\nV7/IImHomonime\nV7/VVMajorfifth\nV7/IIIIIIRelative\nVVm\nV7/VMinorfifth\nMajorfifthModulationsV7/IV\nIm V7IVm\nV7/IIHomonimeIm\nFigure2.Thebasicmajorandminorharmonytree.AsshowninFig.3,morecomplexversionsofthese\ntreescanbeusedtoincorporatecomplexfunctions,such\nasdissonantchordsandmodalborrowing,tothescheme.\nForexample,thesequenceC7M-A7/b13-Dm7/9-G7/9-\nC7Mcanbeseenasadissonantinstantiationofthepath\nI–V7/II–IIm–V7–IontheCmajorkey.Another\ninstantiationforthispathcanbethesequenceC7M-C#º-\nDm7-Ab7-G7-C7M,whichfollowsoneofitsalternative\nroute(I7M-I#º-Iim-VIb-V7-I).\nSubstitution/IIVmV/IV\nVIV\nIIm V/IImV/V\nIo\nI#o\nIIIboIIm7(b5)/IIIV#O\nIIIIm7VIm ISubV/IV\nSubVIM(IIm/IV)IVm\nIIm7(b5)\nVIb\nVIIbModalBorrowing\nV7 IIm V7/IImIIm7(b5)/IVm\nGm7IV#º\nImV7\nImVI7\nFigure3.Theextendedmajorandminorharmony\ntrees.\n4.ABRAZILIANPOPULARMUSICDIGITAL\nLIBRARY\nSeveralworkspointtowardsthedifficultiesinbuilding\nmusicdigitallibraries[9],particularlywithinthepopular\nmusicarea[3].Musicalharmonye-learningsystems\nrequireasetofappropriatesearchmechanismsfora\nmeaningfulretrievalofmusicalobjectsinadigital\nlibrary.\nWeareattemptinginthedesignofourmusicaldigital\nlibrarytotakeintoaccountsomeoftheproblemsrelated\ntomusicalinformationretrievalintroducedbyDownie\n[9],whichrangefromthecomplexityofmusical\ninformationandofthequeriestotheplasticityfoundin\nmusic.Themainguidelinesofourdesignare:\nFocusonasinglemusicalstyle.Thechoiceofastyle\nlessened(butnoteliminates)thedifficultiesrelatedtothe\ncomplexityofmusicalrepresentationandmusical\nqueries.WehavechosentheChoro[5,15,18],whichis\naBrazilianmusicalstyle.Thiscomplexpopularmusic\nstyleisbasedonimprovisation,althoughitcontains\nclassicalmusicelementssuchasthecounterpoint.Asapilotproject,wearemakingavailablea\nrepresentativesetoffiftyChoropiecescontaining,in\nadditiontothemelodyline,thebasslineandthe\nassociatedharmonysequenceasshowninFig.4.The\nchoiceforthisparticularmusicalstyleisbasedonthe\nfactthatthisisoneoftheoldestBrazilianmusicalstyles.\nFigure4.Thefirst4barsfromthechoro“Naquele\nTempo”byPixinguinhaandBeneditoLacerda.\nFocusonaMIRstrategy.Thecomplexityofmusical\ninformationretrievalsystemsisobviouslyrelatedtothe\nmusicalstylesfromtheobjects.Wearefocusingon\nindexingschemesappropriatetobeusedwithinmusical\nharmonyteachingbasedontheHarmonyTreesTheory,\nandwhichareadequatetothechosenmusicalstyle.\nThesearchmechanismwearedevelopingiscentered\nonharmonicsequencesretrievals.Inordertoprovide\nsupporttomusiclearning,wearedesigningoursearch\nmechanismtoaccommodateadirectsearch(e.g.C-C7-\nF)oragenericsearch(e.g.I-I7-IV).\nAsalaterstep,weintendtoprovidemeansto\naccommodatealsochordsubstitutionswhichfollowrules\nsuchas:whenusingmajorkeymodes(I7M-IIm7-\nIIIm7-IV7M-V7-VIm7-VIIm7(b5)),thefirstdegree\ncanbesubstitutedbythethirdandsixthdegrees;the\nsecondwiththefourth;andthefifthwiththeseventhand\nvice-versa.Thishappensbecausechordsbasedonthese\ndegreesshareasignificantnumberofcommonnotes.For\nexample,intheCmajorkey,chordsC(C-E-G)andEm\n(E-G-B)sharetwoofthethreenotes.Infact,when\nconsideringtheusualalterationsemployedbymusicians,\nthisnumberraisestothreeoutofthefournotes(C7M=\nC-E-G-BandEm7=E-G-B-D).Thereforeasearch\nshouldconsidernotonlytheoriginalchordbutalsothese\notherpossiblechordsubstitutions.\nTherefore,ourretrievalsaremadeaccordingtothe\nHarmonyTreesTheoryandthemechanismsbeingmade\navailableareexpectedtoretrievemusicalexcerptsfrom\nmusicalpiecesbasedon(i)simplechordsequences(for\nexample:C-C7-F),(ii)genericsimplechordsequences\n(forexample:I-V/IV7-IV),(iii)chordsequenceswith\ndissonancesorsubstitutions,and(iv)genericchord\nsequenceswithdissonancesorsubstitutions.\nFocusonaparticularaudienceandfunctional\npurpose.Ourmusicdigitallibraryisintendedtobeused\ninitiallybyanaudienceinterestedonMusicalHarmony,\normorespecificallyonBrazilianpopularmusic,because\nofourfocusonqueriesisbasedontheHarmonyTrees\nTheory.\nDesignstrategyfocusedontheuser.Sincethe\nbeginningofthisproject,wearecollectinginformation\nfromthepotentialuserstobeusedinthedesignofthe\nsystemitself.Thisapproachaimstoachieveagood\nacceptabilityofthesystembyitsusers.Choiceofastandardformatforthedigitalobjects.\nMusicXML[13]isbeingusedasourstandardformat\nbecauseofitsportabilityanditsacceptanceinthemusic\ncommunity[14].\n5.CONCLUSIONS\nThispaperdescribesaneffortmadebyamulti-\ndisciplinaryresearchgroupinterestedinmaking\navailableaBrazilianpopularmusicdigitallibrarywhich\ncanhelpvirtualcommunitiesinterestedinacquiring\nknowledgeandsharingexperiences.\nCurrently,oureffortsarefocusedonbuilding(i)a\ndigitallibrarycontainingrepresentativemusicalpieces\nfromtheBrazilianpopularrepertoire,particularlyChoro\npieces;(ii)aWeb-basedcourseonMusicHarmony\nbasedontheHarmonyTreesTheory;and(iii)an\nenvironmenttosupportvirtualcommunitiesofpeople\ninterestedinBrazilianmusic.Theseeffortsaremainly\nbasedonMScandPhDdissertations.\nWeareplanningtoinclude,inthenearfuture,\nretrievalsbasedonmelodiclinesinsuchawayto\naccommodateapplicationsnotonlymusicaleducation\nbutalsomusicalanalysis.Thiswouldallowustobuild\nmechanismstosupportthedevelopmentofdidactical\nmaterialintendedtohelpthemusicteachingofsubjects\nsuchasBrazilianpopularmusicteachingormusical\nanalysis.Weareparticularlyinterestedingetting\nexperimentalevidencewhichwouldallowustovalidate\ntheHarmonyTreesTheory.\n6.REFERENCES\n[1]Arms,W.Y.DigitalLibraries,TheMITPress,\n2000.\n[2]Almeida,H.O.etal“COMPOR:Acomponent-\nbasedframeworkforbuildingmulti-agentsystems”,\nProceedingsofthe2ndInternationalWorkshopon\nSoftwareEngineeringforLarge-ScaleMulti-Agent\nSystems(SELMAS),Portland,USA,2003.\n[3]Bainbridge,D.,etal.\"TowardsaDigitalLibraryof\nPopularMusic\",ProceedingsoftheFourthACM\nConferenceonDigitalLibraries,Berkeley,USA,\n1999,p.61-169.\n[4]Caminha,A.O.etal.“MHITS-Atutoringsystem\ninmusicalharmony”.ProceedingsoftheBrazilian\nSymposiumonComputerMusic(SBCM'2000),\nSBC,Curitiba,Brazil,July2000.\n[5]Cazes,H.Choro–DoQuintalaoMunicipal.2nd\nEdition,editora34,SãoPaulo,Brazil,1999.\n[6]Costa,E.B.etal.“MATHEMA:Alearning\nenvironmentbasedonamulti-agentarchitecture”,\nAdvancesinArtificialIntelligence.12thBrazilian\nSymposiumonArtificialIntelligence-SBIA'95.\nProceedings,Campinas,Brazil,LNAI,Vol.991,\nSpringer1995,pp.141-150.\n[7]Costa,E.B.etal.“Fromatridimensionalviewof\ndomainknowledgetomulti-agentstutoringsystem”,\nAdvancesinArtificialIntelligence.14thBrazilian\nSymposiumonArtificialIntelligence-SBIA'98.Proceedings,PortoAlegre,Brazil,LNAI,Vol.1515,\nSpringer,1998,pp.61-72.\n[8]Costa,E.B.etal.“Towardsamethodologyfor\nbuildingintelligenttutoringsystemsinadistance\nlearningenvironmentbasedonamulti-agent\napproach”,ProceedingsoftheBrazilianSymposium\nonEducationalInformatics(SBIE),SãoLeopoldo,\nBrazil,2002.\n[9]Downie,E.(Org.)“TheMIR/MDLEvaluation\nProjectWhitePaperCollection“Edition#2:\nEstablishingMusicInformationRetrieval(MIR)and\nMusicDigitalLibrary(MDL)Evaluation\nFrameworks.,2002.Availableinhttp://music-ir.org/\nevaluation/wp2/wp2_part1.pdf(accessedin\n05/13/2004).\n[10]Ferneda,E.etal.“Aweb-basedcooperativee-\nlearningenvironmentformusicalharmonydomain”.\nProceedingsoftheIASTEDInternational\nConferenceonWeb-basedEducation(WBE),\nInsbruck,Austria,2004.\n[11]Ferneda,E.etal“AVirtualCommunity\nEnvironmentforBrazilianPopularMusic”.\nProceedingsoftheIEEEInternationalConference\nonAdvancedLearningTechnologies(ETCC/\nICALT),Joensuu,Finland,2004.\n[12]LesterLevyCollectionSheetMusic:http://\nlevysheetmusic.mse.jhu.edu.(accessedin07/29/04).\n[13]MusicXML1.0Tutorial.Availableinhttp://\nwww.recordare.com/xml.html(accessedin\n03/07/2004).\n[14]Sapp,C.ConsiderationsintheRepresentationand\nStorageofMusicalInformation.Availablein\nhttp://www.kcl.ac.uk/humanities/cch/drhahc/drh/abs\nt85.htm(accessedin05/18/2004)\n[15]Sève,M.ChoroVocabulary–Studies&\nCompositions,Lumiar,RiodeJaneiro(Brazil),\n1999.(bilingualedition)\n[16]Themefinder:http://www.themefinder.org(accessed\nin07/29/04).\n[17]Variations2:http://variations2.indiana.edu(accessed\nin07/29/04).\n[18]WhatIsChoroMusic?http://www.\nsaintpaulsunday.org/features/0109_choro/(accessed\nin02/29/04)."
    },
    {
        "title": "Understanding Search Performance in Query-by-Humming Systems.",
        "author": [
            "Roger B. Dannenberg",
            "Ning Hu"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416900",
        "url": "https://doi.org/10.5281/zenodo.1416900",
        "ee": "https://zenodo.org/records/1416900/files/DannenbergH04.pdf",
        "abstract": "Previous work in Query-by-Humming systems has left open many questions. Although a variety of techniques have been explored, there has been relatively little work to compare them under controlled conditions, especially with “real” audio queries from human subjects. Previous work comparing note-interval matching, melodic con- tour matching, and HMM-based matching is extended with comparisons to the Phillips CubyHum algorithm and various n-gram search algorithms. We also explore the sensitivity of note-interval dynamic programming searches to different parameters and consider two-stage searches combining a fast n-gram search with a more precise but slower dynamic programming algorithm. Keywords: Query-by-Humming, N-gram, Dynamic Programming, Evaluation",
        "zenodo_id": 1416900,
        "dblp_key": "conf/ismir/DannenbergH04",
        "keywords": [
            "Query-by-Humming",
            "note-interval matching",
            "melodic contour matching",
            "HMM-based matching",
            "Phillips CubyHum algorithm",
            "n-gram search algorithms",
            "dynamic programming searches",
            "parameters",
            "two-stage searches",
            "evaluation"
        ],
        "content": "UNDERSTANDING SEARCH PERFORMANCE IN  \nQUERY-BY-HUMMING SYSTEMS\n Roger B. Dannenberg and Ning Hu  \n School of Computer Science \nCarnegie Mellon University \nPittsburgh, PA 15213 USA  \nABSTRACT \nPrevious work in Query-by-Humming systems has left \nopen many questions. Although a variety of techniques \nhave been explored, there has been relatively little work \nto compare them under controlled conditions, especially \nwith “real” audio queries from human subjects. Previous \nwork comparing note-interval matching, melodic con-\ntour matching, and HMM-based matching is extended \nwith comparisons to the Phillips CubyHum algorithm \nand various n-gram search algorithms. We also explore \nthe sensitivity of note-interval dynamic programming \nsearches to different parameters and consider two-stage \nsearches combining a fast n-gram search with a more \nprecise but slower dynamic programming algorithm. \nKeywords: Query-by-Humming, N-gram, Dynamic \nProgramming, Evaluation \n1. INTRODUCTION \nThe MUSART project has studied and compared tech-\nniques for query-by-humming (QBH) music retrieval. \n[1] In this work, audio queries are used to search sym-\nbolic (MIDI) targets in a database. Because we use \n“real” queries from non-musicians, the general quality of \nqueries is low, making retrieval quite challenging. The \ndifficulty of our task and configuration can be seen as a \nfeature: we are far from any ceiling effects, so any sig-\nnificant improvement will show up clearly in the results. \nPerhaps the most difficult problem for QBH systems \nis the determination of melodic similarity. Our previous \nwork considered several algorithms for computing \nmelodic similarity. One is based on the fairly well-\nknown dynamic programming algorithms for string \nmatching, which had already been applied to the \nmelodic similarity problem. [2, 3] We use transposition- \nand tempo-invariant representations based on pitch \nintervals and inter-note-onset intervals, so we refer to \nthis as the note-interval search. From previous \nexperience, we knew of numerous shortcomings of this \napproach, and thought we could find better approaches. \nOne of these we call melodic contour matching [4], \nwhich represents melody as a continuous function of \npitch versus time. The advantage of melodic contour \nmatching is that it does not require note segmentation. Instead, only fundamental frequency estimates are used, \neliminating segmentation as a source of errors. \nThe other more sophisticated approach is a hidden \nMarkov model (HMM) [5] in which different categories \nof errors are considered in a unified manner. In contrast \nto the dynamic programming approach, the hidden \nMarkov model approach can consider explicitly differ-\nent error types. One type of error is an incorrect pitch. \nAnother is a transposition, where every note from a \ncertain point onward is transposed. Rhythm and tempo \nerrors can also be considered, and different probabilities \ncan be assigned to different error types. \nEarly experiments with both the melodic contour and \nHMM approaches showed improvements over string-\nmatching approaches. However, Pardo’s work with note \ninterval search [6] resulted in improvements over earlier \nversions. Ultimately, all three approaches showed \nroughly equal performance in our MUSART testbed. [7] \nThe fact that the note interval search was initially \nunimpressive and later delivered quite good perform-\nance led us to investigate the algorithm further to study \nthe sensitivity of different parameter settings. It appears \nthat other researchers may have drawn incorrect \nconclusions from less-than-optimal implementations \nAlong the same lines, there might be other variations \nin the dynamic programming algorithm that could lead \nto even better performance. The CubyHum system [8] \nuses a fairly elaborate set of rules to compute melodic \nsimilarity. These rules are designed to model various \nerror types, somewhat like the way the MUSART HMM \nsystem models errors. We compare the performance of \nthe CubyHum approach to our other algorithms. \nAnother unanswered question is whether there is any \nway to avoid a search cost that is linear in the size of the \ndatabase. All of the algorithms we studied compare the \nquery to each target in the database. It would be much \nbetter to build some sort of index to avoid the cost of \nexhaustive search. A promising approach is to narrow \nthe field of potential targets using an indexing scheme \nbased on n-grams, and then search these candidate \nthemes with a slower, but higher-precision algorithm. \nThus, our intention is to wrap up some “loose ends” \nfrom our previous work and answer some nagging \nquestions about QBH systems. After describing some \nrelated work, we present experiments with a simplified \nnote-interval search algorithm to determine its sensitivity \nand optimal settings. In Section 4, we describe our \nreimplementation and evaluation of CubyHum within \nthe MUSART testbed. In Section 5, we investigate how n-\ngram-based search performs with difficult vocal queries. \nThere will undoubtedly be more questions and research, Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that \ncopies are not made or distributed for profit or commercial \nadvantage and that copies bear this notice and the full citation on \nthe first page. \n© 2004 Universitat Pompeu Fabra.   \n \nbut we feel that we now have a fairly good picture of a \nwide range of options and the potential performance that \ncan be obtained in QBH systems using realistic queries. \n2. RELATED WORK \nOur study of note-interval search uses a simplified \nversion of the note-interval algorithm developed by \nPardo [6], and our findings are consistent with his. \nWhile Pardo emphasizes his optimization process and \noverall results, we are more interested in sensitivity to \nvarying parameter values. We found that search results \nare highly dependent upon the quality of queries. \nLesaffre, et al. [9] collected and studied vocal queries. \nN-grams are studied extensively in the text-retrieval \ncommunity and have also found application in music \ninformation retrieval. Downie [10, 11] evaluated n-gram \ntechniques and explored the effects of different design \nparameters. Errors were simulated by changing one pitch \ninterval randomly. Uitenbogerd and Zobel [12] consid-\nered 4-grams of different pitch interval representations, \nwith queries extracted from MIDI data. Doraisamy and \nRuger [13] studied n-grams with both pitch and rhythm \ninformation in the context of QBH, using simulated \nerrors. Bainbridge, Dewsnip and Witten [14] also \nexplored n-grams along with state-based matching and \ndynamic programming. A synthetic error model was \nused, and their paper also proposed using fast n-gram \nsearch as a filter to reduce the size of the search by \nslower methods. Our work uses audio queries from \nhuman subjects, and further explores the possibility of a \n2-stage search using n-grams and dynamic program-\nming. \n3. OBTAINING THE BEST PERFORMANCE \nWITH NOTE-INTERVAL SEARCH \nWe want to explain the wide variation in observed \nperformance of note-based dynamic programming \nsearches. In order to understand what factors are critical, \nwe tune a search system in different ways and compare \nthe results. We begin by designing a note-based dynamic \nprogramming algorithm called “NOTE-SIMPLE”. \nNOTE-SIMPLE is quite similar to the note-interval \nalgorithm, but simpler in some ways and can take into \naccount different sets of parameters and representations \nfor evaluations. \nAbsolute Pitch:    67             69     71    67\nRelative Pitch:             2           2      −4\nIOI:     1             0.5    0.51\nIOI Ratio:            0.5        1         2\nLog IOI Ratio:            -1          0         1  \nFigure 1. Pitch Interval and IOI Ratio calculation. The NOTE-SIMPLE algorithm can perform search \non different representations of melodic sequence. Pitch \nand Rhythm are the two main components defining each \nnote iN in a melodic sequence n 21 N NNS \u0001 = . As \nshown in Figure 1, the Pitch component can be ex-\npressed in two ways:  \n1. The absolute pitch in MIDI key values: \n{ }127... ,2 ,1)N(Pi abs Î , where ni1 ££  \n2. The relative pitch, or pitch interval: \n) (N P)(N P)(NP1i abs i abs i rel - - = , where ni1£<  \n0)N(P1 rel = \nSimilarly, there are three different kinds of representa-\ntion for the Rhythm component of Ni: \n1. The inter-onset-interval (IOI): \n)N( t)N( t)N(Ti onset 1i onset i IOI - =+ , where ni1 <£ \n)N( t)N( t)N(Tn onset n offset n IOI - =  \n2. The IOI Ratio (IOIR), which is the ratio between the \nIOI values of two succeeding notes: \n) (NT)(NT)N( T\n1iii\nIOIIOI\nIOIR\n-= , where ni1£< \n1)(N T 1 IOIR = \n3. The Log IOI Ratio (LogIOIR) [15], the logarithm of \nthe IOI Ratio: \n))N( Tlog( )N( T i i IOIR LogIOIR = , where ni1££  \nIOIR and LogIOIR are usually quantized by rounding \nthe values to their closest integers [15]. However, we did \nnot quantize in our experiments, and this might be an \noption to explore in the future.  \nLike the note-interval or other common dynamic pro-\ngramming search algorithms, NOTE-SIMPLE computes \nthe melodic edit distance D(A, B) between two melodic \nsequences ma aaA\u000121=  and nb bbB\u000121= by filling the \nmatrix ( n md...0...0,). Each entry jid, denotes the minimal \nmelodic edit distance between the two prefixes iaa\u00021  \nand ). 1( jw b bj w ££\u0002  \n We use a classical calculation pattern for the algo-\nrithm as shown: \nfor 1 \u0001 i \u0001 m and 1 \u0001 j \u0001 n, \n\u0001\u0001\u0001\u0001\n\u0002\u0001\u0001\u0001\u0001\n\u0003\u0004\n££ +££ ++++\n=\n+- --+- - -----\n) (} 2 ), , ,( {) (} 2 ),,,, ( {) ( ),() ( ),() ( ),(\nmin\n1 ,11 1,1,11,,1\n,\nion fragmentatjk b baw dion consolidatik ba aw dt replacemen baw dinsertion bw ddeletion aw d\nd\nj kji kjiji ki jkiji jij jii ji\nji\n\u0002\u0002ff\n \n \nInitial conditions are: \ndi,0 = di-1,0 + w(ai, f), i \u0002 1 (deletion) \nd0,j = d0,j-1 + w(f, bj), j \u0002 1 (insertion) \nand d0,0 = 0. \nIn order to simplify the algorithm, we define  \nw(ai, f)= k1 Cdel and w(f, bj)=k1 Cins,  \nwhere Cdel and Cins are constant values representing \ndeletion cost and insertion cost respectively. We also \ndefine the replacement weight  \n)()( )( )( ),(1 j i j i ji bT aTk bP aP baw - + - = ,    \n \nwhere P() can be Pabs() or Prel(), and T() is either TIOI() \nor TLogIOIR(). If IOIR is used, then \n\u0005\u0005\n\u0006\u0007\n\b\b\n\t\n+ - =)(a T)(b T,)(b T)(a Tmax k)P(b )P(a )b, w(a\ni IOIRj IOIR\nIOIRi IOIR\n1 j i ji\nj \nk1 is the parameter weighting the relative importance of \npitch and time differences. It is quite possible to be \ntuned for better performance. But in this experiment, we \narbitrarily picked k1=1 if the Rhythm form is IOI or \nIOIR and k1=6 if the form is LogIOIR. Those values \nachieved reasonable results in our initial experiments. \nThe equations for computing fragmentation and con-\nsolidation [16, 17] are only used in the calculation pat-\ntern when the Rhythm input is in IOI form, as our previ-\nous experiments based on IOI [16] proves that \nfragmentation and consolidation are beneficial to the \nperformance. We do not use fragmentation or consoli-\ndation for the Rhythm input in IOIR or LogIOIR form, \nsince fragmentation and consolidation do not really \nmake sense when dealing with ratios. \nIf the algorithm is computed on absolute pitches, the \nmelodic contour will be transposed 12 times from 0 to \n11 in case the query is a transposition of the target. [16] \nAlso the contour will be scaled multiple times if IOI is \nused. Both transposition and time scaling increase the \ncomputing time significantly. \nTesting was performed using the MUSART testbed [7], \nwhich has two sets of queries and targets. Database 1 is \na collection of Beatles songs, with 2844 themes, and \nDatabase 2 contains popular and traditional songs, with \n8926 themes. In this section, we report results using \nDatabase 2, which is larger. In most instances, we use \nthe mean reciprocal rank (MRR) to evaluate search per-\nformance. For example if there are only two queries for \nwhich the correct targets are ranked second and fourth, \nthe MRR is (1/2 + 1/4)/2 = 0.375. Thus, MRR ranges \nfrom zero (bad) to one (perfect). \nTable 1 lists some results obtained from the NOTE-\nSIMPLE algorithm for different representations of me-\nlodic sequence. For each of these tests, the insertion and \ndeletion costs were chosen to obtain the best perform-\nance. The combination of Relative Pitch and Log IOI \nRatio results in the best performance.  \nTable 1. Retrieval results using various representations \nof pitch and rhythm. \nRepresentations MRR \nAbsolute Pitch & IOI 0.0194 \nAbsolute Pitch & IOIR 0.0452 \nAbsolute Pitch & LogIOIR 0.0516 \nRelative Pith & IOI 0.1032 \nRelative Pitch & IOIR 0.1355 \nRelative Pitch & LogIOIR 0.2323 \n \nThe relationship between the insertion and deletion \ncosts is another interesting issue to be investigated. Table \n2 shows the results from different combinations of \ninsertion and deletion costs. Note that these values are \nscaled by k1 = 6. The main point of Table 1 and Table 2 is that design \nchoices have a large impact on performance. NOTE-\nSIMPLE does not perform quite as well as Pardo’s note-\ninterval search algorithm [7], perhaps because his note-\ninterval search normalizes the replacement cost function \nto behave as a probability distribution. Further tuning \nmight result in more improvements. Overall, we con-\nclude that dynamic programming is quite sensitive to \nparameters. Best results seem to be obtained with rela-\ntive pitches, Log IOI Ratios, and carefully chosen inser-\ntion and deletion costs. Previous work that did not use \nthese settings may have drawn false conclusions by \nobtaining poor results. \nTable 2. Retrieval results using different insertion and \ndeletion costs. \nCins : Cdel MRR \n0.5 : 0.5 0.1290 \n1.0 : 1.0 0.1484 \n2.0 : 2.0 0.1613 \n1.0 : 0.5 0.1161 \n1.5 : 1.0 0.1355 \n2.0 : 1.0 0.1290 \n0.5 : 1.0 0.1742 \n Cins : Cdel MRR \n1.0 : 1.5 0.2000 \n0.2 : 2.0 0.2194 \n0.4 : 2.0 0.2323 \n0.6 : 2.0 0.2323 \n0.8 : 2.0 0.2258 \n1.0 : 2.0 0.2129 \n  \n  \n4. REIMPLEMENTATION AND TESTING OF \nTHE CUBYHUM SEARCH ALGORITHM \nCubyHum is a QBH system developed at Philips \nResearch Eindhoven. It uses a dynamic programming \nalgorithm for melodic search. We are particularly \ninterested in comparing the performance of its search \nalgorithm with our algorithms. Thus we re-implemented \nthe CubyHum search algorithm (a.k.a. CUBYHUM) in \nour system, following the published description [8]. \nCUBYHUM uses relative pitches in semitones; how-\never, it further quantizes the relative pitches into 9 \nintegers: −4, −3, … 3, 4. The calculation pattern of the \nsearch algorithm is much more complex than the one \nused in NOTE-SIMPLE. However, it does not achieve \nvery satisfying results even compared to our simple \ndynamic programming algorithm NOTE-SIMPLE. (See \nTable 3.) This seems to be in conflict with the common \nnotion that complex calculation patterns yield better \nperformance, but it is consistent with the notion that \nperformance can vary tremendously with different \ndesign choices. \nTable 3. Performance of CubyHum compared to \nNOTE-SIMPLE, as measured by MRR. \nAlgorithm Database 1 Database 2 \nCUBYHUM 0.0229 0.0194 \nNOTE-SIMPLE 0.1221 0.2323 \n5. N-GRAMS FOR QUERY-BY-HUMMING \nAn important problem with all of the approaches we \nhave described so far is their performance in a large \ndatabase. Even with substantial optimization, our fastest   \n \nalgorithm, the NOTE-SIMPLE algorithm, would run for \nmany minutes or even hours on a database with a million \nsongs (and perhaps 10 million themes). One possible \nsolution is to use n-grams, which allow an index to be \nconstructed to speed up searching. \n5.1. Two-stage search. \nIt is unnecessary for the n-gram approach to work as \nwell as note-interval matching or other techniques. The \nimportant thing is for n-grams to have very high recall \nwith enough precision to rule out most of the database \ntargets from further consideration. A more precise \nsearch such as the note-interval matcher can then be \nused to select a handful of final results. This two-stage \nsearch concept is diagrammed in Figure 2. \n \nComplete\nDatabaseN-gram\nSearch\nNote-Interval\nSearch\nResults\n \nFigure 2. A two-stage search using n-gram search for \nspeed and note-interval search for precision. \n5.2. N-gram search algorithms \nOur n-gram search operates as follows: The audio query \nis transcribed into a sequence of notes as in the note-\ninterval search, and note intervals are computed. Pitch \nintervals are quantized to the following seven ranges, \nexpressed as the number of half steps: \n     < −7, −7 to −3, −2 to −1, unison, 1 to 2, 3 to 7, >7 \nIOI Ratios are quantized to five ranges separated by the \nfollowing four thresholds: 4/2, 2/2, 2, 22 . \nThus, the nominal IOI Ratios are ¼, ½, 1, 2, and 4. \nThese fairly coarse quantization levels, especially for \npitch, illustrate an important difference between n-grams \nand other approaches. Whereas other searches consider \nof small differences between query and target intervals, \nn-grams either match exactly or not at all. Hence, coarse \nquantization is used so that small singing or transcription \nerrors are not so likely to cause a mismatch. \nN-grams are formed from sequences of intervals. \nFigure 1 illustrates how a trigram is formed from a \nsequence of four notes. Note that the IOI for the last \nnote is not defined, so we use the last note’s duration \ninstead. In Figure 1, the trigram formed from pitch \nintervals and IOI Ratios is <2, 0.5, 2, 1, −4, 2>. \nA set of n-grams is computed for the query and for \neach target by looking at the n pitch intervals and IOI \nRatios beginning at each successive note. For example, trigrams would be formed from query notes 1, 2, and 3, \nnotes 2, 3, and 4, notes 3, 4, and 5, etc. \nTo compute similarity, we basically count the number \nof n-grams in the query that match n-grams in the target. \nSeveral variations, based on concepts from text retrieval \n[18, 19] were tested. The following are independent \ndesign decisions and can be used in any combination: \n1. Count the number of n-grams in the query that have a \nmatch in the target (e.g. if an n-gram occurs 3 times \nin the query and twice in the target, the score is in-\ncremented by 2). Alternatively, weight each n-gram \nin the query by the number of occurrences in the tar-\nget divided by the number in the query (e.g. if an n-\ngram occurs 3 times in the query and twice in the tar-\nget, the score is incremented by 2/3). This is a varia-\ntion of term frequency (TF) weighting. \n2. Optionally weight each match by the inverse fre-\nquency of the n-gram in the whole database This is \nknown as Inverse Document Frequency (IDF) \nweighting, and we use the formula log(N/d), where N \nis the total number of targets, and d is the number of \ntargets in which the n-gram occurs. \n3. Optionally use a locality constraint: consider only \ntarget n-grams that fall within a temporal window the \nsize of the query. \n4. Choose n-gram features: (a) Incorporate Relative \nPitch and IOI Ratios in the n-grams, (b) use only \nRelative Pitch, or (c) use only IOI Ratios. \n5. Of course, n is a parameter. We tried 1, 2, 3, and 4. \n5.3. N-gram performance on vocal queries \nAlthough we were unable to explore all 96 permutations \nof these design choices, each choice was tested inde-\npendently in at least several configurations. The best \nperformance was obtained with n = 3, using combined \nIOI Ratios and Relative Pitch (3 of each) in n-grams, not \nusing the locality constraint, using inverse document \nfrequency (IDF) weighting, and not using term fre-\nquency (TF) weighting. This result held for both \nMUSART databases. \nFigure 3 show results for different n-gram features and \ndifferent choices of n. As can be seen, note interval \ntrigrams (combining pitch and rhythm information) work \nthe best with these queries and targets. \n5.4. N-grams in a two-stage search \nThe n-gram search (MRRs of 0.09 and 0.11 for the two \ndatabases) is not nearly as good as the note-interval \nsearch algorithm (MRRs of 0.13 and 0.28), but our real \ninterest is the potential effectiveness of a two-stage \nsystem. \nTo study the possibilities, consider only the queries \nwhere a full search with the note-interval search algo-\nrithm will return the correct target ranked in the top 10. \n(If the second stage is going to fail, there is little reason \nto worry about the first stage performance.) Among \nthese “successful” queries, the average rank in the n-\ngram search tells us the average number of results an n-\ngram search will need to return to contain the correct   \n \ntarget. Since the slower second-stage search must look at \neach of these results, the possible speed-up in search \ntime is given by: \nrNs /= , \nwhere s is the speedup (s ³ 1), N is the database size, \nand ris the mean (expected value of) rank. Table 4 \nshows results from our two databases. Thus, in Database \n2, we could conceivably achieve a speedup of 3.45 using \nn-grams to eliminate most of the database from consid-\neration. \nMRR for Database 1\n00.0250.050.0750.1\n1 2 3 4\nLength  (n) of n-gramMRRIOIR\nPitch\nBoth\n \nMRR for Database 2\n00.040.080.12\n1 2 3 4\nLength (n) of n-gramMRRIOIR\nPitch\nBoth\n \nFigure 3. N-gram search results on Databases 1 and 2. \nTable 4. Fraction of database and potential speedup. \nDatabase r/N s \n1 (Beatles) 0.49 2.06 \n2 (General) 0.29 3.45 \nOf course, we have no way to know in advance where \nthe n-gram search will rank the correct target, and n-\ngram searching takes time too, so this theoretical \nspeedup is an upper bound. Another way to look at the \ndata is to consider how results are affected by returning \na fixed fraction of the database from the n-gram search. \nAgain, considering only queries where the second stage \nsearch ranks the correct target in the top 10, we can plot \nthe number of correct targets returned by the first-stage \nn-gram search as a function of the fraction of the data-\nbase returned. \nAs seen in Figure 4, n-gram search is significantly \nbetter than random, but somewhat disappointing as a \nmechanism to obtain large improvements in search \nspeed. If the n-gram search returns 10% of the database, \nwhich would reduce the second-stage search time ten-\nfold, about 50% to 65% of the correct results will be \nlost. Even if the n-gram search returns 50% of the entire \ndatabase, the number of correct results is still cut by \n25% to 40%. These numbers might improve if the n-gram search returns a variable number of results based \non confidence. \nSearch Success vs. Fraction of Database\n00.20.40.60.81\n0 0.2 0.4 0.6 0.8 1\nFraction of Database Returned from SearchFraction of Results Containing Correct \nTarget\nDatabase 1\nDatabase 2\nRandom\n \nFigure 4. Performance of the best n-gram search show-\ning the proportion of correct targets returned as a func-\ntion of the total number of results returned. \nThe n-gram search fails on a substantial number of \nqueries that can be handled quite well by slower \nsearches. Bainbridge, et al. say “It is known that music-\nbased n-gram systems are computationally very efficient \nand have high recall…” [14], but with our data, we see \nthat about 40% of the correct Database 1 targets are \nranked last, and about 20% of the correct Database 2 \ntargets are ranked last. A last-place ranking usually \nmeans that the target tied with many other targets with a \nscore of zero (no n-grams matched). In the event of a tie, \nwe report the highest (worst) rank. Overall, our results \nwith “real” audio queries suggest that singing and tran-\nscription errors place significant limits on n-gram system \nrecall. \n6. SUMMARY AND CONCLUSIONS \nQuery-by-Humming systems remain quite sensitive to \nerrors in queries, and in our experience, real audio \nqueries from human subjects are likely to be full of \nerrors and difficult to transcribe. This presents a very \nchallenging problem for melodic similarity algorithms. \nBy studying many configurations of note-based dy-\nnamic programming algorithms, we have determined \nthat (1) these algorithms are quite competitive with the \nbest techniques including melodic contour search and \nHMM-based searching, but (2) parameters and configu-\nration are quite important. Previous work has both \noverestimated the performance of note-based DP \nsearching by using simple tasks and underestimated the \nperformance by failing to use the best configuration.  \nWe re-implemented the CubyHum search algorithm \nand found that it performs poorly compared to a simpler \nbut well-tuned note-based DP algorithm.  \nWe also studied the use of n-grams for query-by-\nhumming. Overall, n-grams perform significantly worse \nthan other melodic-similarity-based search schemes. The \nmain difference is that n-grams (in our implementation) \nrequire an exact match, so the search is not enhanced by   \n \nthe presence of approximate matches. Also, intervals \nmust be quantized to obtain discrete n-grams, further \ndegrading the information. \nWe considered the use of n-grams as a “front end” in \na two-stage search in which a fast indexing algorithm \nbased on n-grams narrows the search, and a high preci-\nsion algorithm based on dynamic programming or \nHMMs performs the final selection. We conclude that \nthere is a significant trade-off between speed and preci-\nsion. We believe our results form a good indication of \nwhat is possible with n-gram searches applied to “real \nworld” queries of popular music from non-musicians. \n7. FUTURE WORK \nIndexing for melodic contours remains an interesting \nand open question. N-grams suffer because at least small \nerrors are very common in sung queries. A better under-\nstanding of melody recognition by humans might sug-\ngest new strategies. In particular, a better sense of what \nwe recognize and remember might allow vast reductions \nin database size, perhaps even eliminating the need for \nindexing. Query-by-Humming with “real” audio queries \nis difficult even for the best of known algorithms. While \nthe average user may be unable to produce effective \nqueries for unconstrained music searches on the web, \nthere is probably room for creative applications of QBH \nin personal media management systems, music assis-\ntants, education, and entertainment systems yet to be \nenvisioned. \n8. ACKNOWLEDGEMENTS \nThe authors would like to acknowledge the MUSART \nteam for building the testbed used in this study. Special \nthanks are due to Bill Birmingham, George Tzanetakis, \nBryan Pardo, and Collin Meek. Stephen Downie pro-\nvided helpful comments and references that guided our \nwork with n-grams. This paper is based on work sup-\nported by the National Science Foundation under grant \nIIS-0085945. \n9. REFERENCES \n[1] R. B. Dannenberg, W. P. Birmingham, G. Tzanetakis, C. \nMeek, N. Hu, and B. Pardo, \"The MUSART Testbed for \nQuery-by-Humming Evaluation,\" Computer Music \nJournal, vol. 28, pp. 34-48, 2004. \n[2] W. Hewlett and E. Selfridge-Field, \"Melodic Similarity: \nConcepts, Procedures, and Applications,\" in Computing in \nMusicology, vol. 11. Cambridge: MIT Press, 1998. \n[3] R. B. Dannenberg, \"An On-Line Algorithm for Real-Time \nAccompaniment,\" in Proceedings of the 1984 \nInternational Computer Music Conference, Paris, San \nFrancisco:International Computer Music Association, \n1985, pp. 193-198. \n[4] D. Mazzoni and R. B. Dannenberg, \"Melody Matching \nDirectly From Audio,\" in 2nd Annual International \nSymposium on Music Information Retrieval, \nBloomington, Indiana, Bloomington:Indiana University \n2001, pp. 17-18. [5] C. Meek and W. P. Birmingham, \"Johnny Can't Sing: A \nComprehensive Error Model for Sung Music Queries,\" in \nISMIR 2002 Conference Proceedings, Paris, France, \nParis:IRCAM 2002, pp. 124-132. \n[6] B. Pardo, W. P. Birmingham, and J. Shifrin, \"Name that \nTune: A Pilot Studying in Finding a Melody from a Sung \nQuery,\" Journal of the American Society for Information \nScience and Technology, vol. 55, 2004. \n[7] R. B. Dannenberg, W. P. Birmingham, G. Tzanetakis, C. \nMeek, N. Hu, and B. Pardo, \"The MUSART Testbed for \nQuery-by-Humming Evaluation,\" in ISMIR 2003 \nProceedings of the Fourth International Conference on \nMusic Information Retrieval, Baltimore, MD, \nBaltimore:Johns Hopkins University 2003, pp. 41-50. \n[8] S. Pauws., \"CubyHum: A Fully Operational Query-by-\nHumming System,\" in ISMIR 2002 Conference \nProceedings, Paris, France, Paris:IRCAM 2002, pp. 187-\n196. \n[9] M. Lesaffre, K. Tanghe, G. Martens, D. Moelants, M. \nLeman, B. D. Baets, H. D. Meyer, and J.-P. Martens, \"The \nMAMI Query-By-Voice Experiment: Collecting and \nAnnotating Vocal Queries for Music Information \nRetrieval,\" in ISMIR 2003 Proceedings of the Fourth \nInternational Conference on Music Information \nRetrieval, Baltimore, MD, Baltimore:Johns Hopkins \nUniversity 2003, pp. 65-71. \n[10] J. S. Downie and M. Nelson, \"Evaluation of a simple and \neffective music information retrieval method,\" in \nProceedings of ACM SIGIR 2000, pp. 73-80. \n[11] J. S. Downie, Evaluating a Simple Approach to  Music \nInformation Retrieval, Ph.D. Thesis.: Faculty of \nInformation and Media Studies, University of Western \nOntario, 1999. \n[12] A. Uitdenbogerd and J. Zobel, \"Melodic Matching \nTechniques for Large Music Databases,\" in Proceedings \nof the 7th ACM International Multimedia \nConference:ACM 1999, pp. 57-66. \n[13] S. Doraisamy and S. Ruger, \"A Comparative and Fault-\ntolerance Study of the Use of N-grams with Polyphonic \nMusic,\" in ISMIR 2002, Paris, France 2002, pp. 101-106. \n[14] D. Bainbridge, M. Dewsnip, and I. H. Witten, \"Searching \nDigital Music Libraries,\" in Digital Libraries: People, \nKnowledge, and Technology: 5th International \nConference on Asian Digital Libraries, Singapore, New \nYork:Springer-Verlag 2002, pp. 129-140. \n[15] B. Pardo and W. P. Birmingham, \"Encoding Timing \nInformation for Musical Query Matching,\" in ISMIR 2002 \nConference Proceedings, Paris, France, Paris:IRCAM, \nOctober 13-17 2002, pp. 267-268. \n[16] N. Hu and R. B. Dannenberg, \"A Comparison of Melodic \nDatabase Retrieval Techniques Using Sung Queries,\" in \nProceedings of the second ACM/IEEE-CS joint \nconference on Digital libraries, New York:ACM Press \n2002, pp. 301-307. \n[17] M. Mongeau and D. Sankoff, \"Comparison of Musical \nSequences,\" in Melodic Similarity Concepts, Procedures, \nand Applications, vol. 11, Computing in Musicology, W. \nHewlett and E. Selfridge-Field, Eds. Cambridge: MIT \nPress, 1990. \n[18] G. Salton, Automatic Text Processing: The \nTransformation, Analysis, and Retrieval of Information \nby Computer: Addison-Wesley, 1988. \n[19] G. Salton and M. J. McGill, Introduction to Modern \nInformation Retrieval: McGraw-Hill, 1983."
    },
    {
        "title": "Methodology and Tools for the evaluation of automatic onset detection algorithms in music.",
        "author": [
            "Laurent Daudet",
            "Gaël Richard",
            "Pierre Leveau"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417247",
        "url": "https://doi.org/10.5281/zenodo.1417247",
        "ee": "https://zenodo.org/records/1417247/files/DaudetRL04.pdf",
        "abstract": "This paper addresses the problem of the performance eval- uation of algorithms for the automatic detection of note onsets in music signals. Our experiments show that cre- ating a database of reference files with reliable human- annotated onset times is a complex task, since its sub- jective part cannot be neglected. This work provides a methodology to construct such a database. With the use of a carefully designed software tool, called SOL (Sound Onset Labellizer), we can obtain a set of reference onset times that are cross-validated amongst different expert lis- teners. We show that the mean error of annotated times across test subjects is very much signal-dependent. This value can be used, when evaluating automatic labelling, as an indication of the relevant tolerance window. The SOL annotation software is to be released freely for research purposes. Our test library, 17 short sequences contain- ing about 750 onsets, comes from copyright-free music or from the public RWC database. The corresponding vali- dated onset labels are also freely distributed, and are in- tended to form the starting point for the definition of a reliable benchmark.",
        "zenodo_id": 1417247,
        "dblp_key": "conf/ismir/DaudetRL04",
        "keywords": [
            "performance evaluation",
            "automatic detection",
            "note onsets",
            "database creation",
            "subjective part",
            "cross-validation",
            "software tool",
            "SOL annotation",
            "freely distributed",
            "benchmark"
        ],
        "content": "METHODOLOGYAND TOOLSFORTHEEVALUATIONOF\nAUTOMATICONSET DETECTIONALGORITHMSINMUSIC\nPierreLEVEAU, LaurentDAUDET\nLaboratoired’AcoustiqueMusicale\n11,ruedeLourmel\n75015Paris-FRANCE\nleveau,daudet@lam.jussieu.frGa¨elRICHARD\nGET -ENST (T´ el´ ecomParis)\n46,rue Barrault\n75634Paris Cedex 13-FRANCE\ngael.richard@enst.fr\nABSTRACT\nThispaperaddressestheproblemoftheperformanceeval-\nuation of algorithms for the automatic detection of note\nonsets in music signals. Our experiments show that cre-\nating a database of reference ﬁles with reliable human-\nannotated onset times is a complex task, since its sub-\njective part cannot be neglected. This work provides a\nmethodology to construct such a database. With the use\nof a carefully designed software tool, called SOL (Sound\nOnset Labellizer), we can obtain a set of reference onset\ntimesthatarecross-validatedamongstdifferentexpertli s-\nteners. We show that the mean error of annotated times\nacross test subjects is very much signal-dependent. This\nvaluecanbeused,whenevaluatingautomaticlabelling,as\nan indication of the relevant tolerance window. The SOL\nannotation software is to be released freely for research\npurposes. Our test library, 17 short sequences contain-\ningabout750onsets,comesfromcopyright-freemusicor\nfrom the public RWC database. The corresponding vali-\ndated onset labels are also freely distributed, and are in-\ntended to form the starting point for the deﬁnition of a\nreliablebenchmark.\n1. INTRODUCTION\nAn increasing number of studies are concerned with the\nautomatic extractionof note onset times directly from re-\ncorded audio, as this is useful in a wide range of signal\nprocessingapplications: automatictranscription,adapt ive\naudioeffects,object-basedcoding,andmoregenerallyall\ninformation extraction techniques used for MIR (Music\nInformation Retrieval). All these applications try to spli t\nthe audio into segments that have homogeneous proper-\nties, e.g. spectral and / or statistical properties (see for\nexample [1, 2, 3, 4]) While this task is rather straightfor-\nward in the case of isolated notes, thiscan becomea very\ndifﬁcult - and indeed ill-posed - problem for increasingly\nPermission tomakedigital orhard copies ofallorpartofthi swork for\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributed forproﬁtorcommercialadvantag e andthat\ncopies bear this notice and the full citation on the ﬁrstpage .\nc/circlecopyrt2004 Universitat Pompeu Fabra.complicatedsoundﬁles,fromasingleinstrumentmelodic\nline to a full polyphonicorchestra. When many notes are\nplayed together, the notion of sound object may appear\nmorerelevant: forinstanceachordcanbeconsideredasa\nsingle sound object. However, when this chord becomes\nbroken (typically in a guitar slam) or when does it stop\nbeinga singleobjectandstart beinga set ofharmonically\nrelatednotes?\nSo far, the great majorityof note onset detectionsche-\nmesarebasedontheconceptof“detectionfunction”(DF).\nThe DF is a highly sub-sampledversion of the audio that\nexhibits peaks at the time instants where some properties\nchange (e.g. energy, spectral content, etc ...) (see [5] for\na tutorial on onset detection). The performance of such\nschemes is usually evaluated through ROC curves (Re-\nceiver Operating Characteristics), a plot of the ratio of\ncorrectdetectionsasa functionof false alarms. The main\nproblem arises from the deﬁnition of what a “correct de-\ntection” is, since it implies the existence of a reference\nthat gives the time localization of “true onsets” with inﬁ-\nnite precision. Unfortunately,such perfect referencedoe s\nnot exist, except in a very limited set of cases (e.g. syn-\nthesized music). Furthermore, one has to allow for the\nﬁnitetimeresolutionoftheabove-mentioneddetectional-\ngorithms: a given onset candidate at time tis countedas\ncorrect if there exists a “true onset” within a time frame\n[t−τ, t+τ]. Finally, the performances of the different\nschemes proposed in the litterature are not easily com-\npared due to the lack of common database and protocol\nfortheirevaluation.\nThis paper hence addresses the two fundamental (but\npreviouslyunder-considered)followingissues: howtobui ld\na set of reference onset times, and what is a good choice\nfor the time resolution τ. In most cases foundin the liter-\nature, the set of referenceonset times is givenby human-\nannotated data. Amongst our ﬁndings, we have observed\nthat, for a number of test ﬁles, this human annotation ex-\nhibits a signiﬁcant dependency on the employed method,\nthe underlying software, the listener himself, and above\nallonthetypeofmusic. Thisobservationsuggeststhatthe\nreportedperformanceofautomaticonsetdetectionschemes\nis at best over-simpliﬁed and at worst cannot be general-\nized(i.e. areonlytruewithstrictlythesameexperimental\nconditions).The main objective of this paper is a proposal for a\ncommonmethodologyandacommonannotationtool,which\ninturnisusedtobuildacommondatabaseofonset-anno-\ntated ﬁles. These tools and ﬁles are freely available in\nordertobesharedbythewidestcommunity.\n2. ONSETCARACTERIZATION\n2.1. Particularitiesofonsetsin music signals\nBefore labelling the onsets in music signals, we must de-\nﬁne what an onset precisely is. The commonlyused deﬁ-\nnitionisthetimewhenanotebegins . Howeversuchadef-\ninition does not remove all the ambiguities. First, all the\nstudied music signals are recorded. That implies that the\nreal onset time , when the player triggers the production\nof the note, is notnecessarily visible/audiblein the signa l\non which we work. Howeverwe will afterwardsconsider\nthat an onset is the ﬁrst detectable part of a note event in\nthe recording if the note were isolated . Moreover, some\nunwanted or uncontrolled sound events may occur when\nmusicisrecorded. Forinstance,thekeysofthewoodwind\nandthebreathingoftheplayerproducenoisesthatwecan\nhear if we pay attention to it, but they usually bear little\naesthetic or musical meaning. Hence, when someone is\nasked to label onsets in a music signal, it is important to\ntell himifhemust takeintoaccounttheseevents.\nAs mentioned in the introduction, picking out onsets\nwhen the notes are isolated is easy. Things begin to be\nmore difﬁcult when a musical sequence is played, e.g. in\nsolo performances. For monophonic instruments, room\neffects are amongst the phenomena that disturb the deci-\nsion, as the increased release time of a note can mask the\nonset of the following one. Polyphony adds other distur-\nbances to our task: the broken chord can be considered\nas a sequence of notes or as a block. For bowed strings,\nit is also difﬁcult to mark the onset of a note when the\npreviousnote is still played on another string. For mixed\nmusic, these difﬁculties are ampliﬁed. Even if the instru-\nments are supposed to play together on a quantized tem-\nporal grid, most of the time the differences between the\nreal onsets of the different instruments notes are not neg-\nligible,especiallyforslow tempi. All theseelementssug-\ngestthatonsetdetectionisarelativelysubjectivetask,a nd\nthatthespeciﬁcationsonwhatwe arelookingformustbe\npreciselyexpressed.\n2.2. Howtolabel anonset byhand(andby ear)?\nHand-labeling onsets is a strenuous task, that takes time\nand requires extreme concentration. To label onsets in a\nmusicsignal,asubjectcanprincipallyusethreemethods:\n•signal plot : this tool is very efﬁcient to precisely\nand quickly label percussive signals. It can also be\nusedasa secondarymethod: whenanonsetoccurs,\nthe waveshapecanbealtered.\n•spectrogram : it canbeusedasa ﬁrst approach. Be-\ncause of the need to take large enough FFT win-\nFigure1. Interfaceofthe SoundOnset Labellizer\ndows to have a sufﬁcient frequencyresolution, this\nmethod is not very precise, but it helps to localize\nmost onsets globally. Indeed, a common charac-\nterization of music onset is that they are generally\naccompaniedbya burstat all frequencies.\n•listeningtosignalslices : thismethodistheultimate\njudge. Combined with visualizations, it allows an\nefﬁcientlabellingofsignals;thisisthemostprecise\nuser-controlledmethod.\nIt is possible to imagine other representations of the sig-\nnal, e.g. using wavelets, phase, or spectrogram scaled in\nbark. However, by sake of simplicity we have chosen\nto restrict the study to these three most commonly used\nmethods ; and we have compiled these in a software tool\ncalledthe SoundOnsetLabellizer thatispresentedinnext\nsection.\n3. HANDLABELLING\n3.1. AnnotationTool: SoundOnset Labellizer\nThis tool has been developed to provide an easy-to-use\nand portable interface to the different labelling subjects .\nAll annotators(orsubjects)haveusedthesamesoftware.\nThe screen of the GUI is divided into three parts: the\nupper one represents the spectrogram of the signal, the\nmiddle one its time domain waveform, and the lower the\ncontrolstomanipulatesoundﬁles,labelsandvisualizatio n\nwindows(seeFigure1).\nThe spectrogram and waveform parts have the same\ntime axis, and all the zoom operations act on both win-\ndows. Thecursorontherightofthespectrogramallowsasetting of the contrast of the spectrogram, the one on the\nright of the signal waveform allows an amplitude magni-\nﬁcation. The subject can play the sound visualized in the\ncurrentwindow. Thelabels are putwith a cursor,andcan\nbemovedbystepsof5mstoﬁtpreciselythesupposedon-\nset time. Once a few labels are put, the subject can play\nthesignalbetweentwolabelstoevaluateifitcontainsonly\nonenote.\nAfterashortlearningtime,allthreeannotators(theau-\nthors, considered as expert listeners) have spontaneously\nadoptedsimilarmethodstolabeltheonsets,followingthe-\nse steps:\n1.zoomonawindowcontainingafewnotes(typically\n1 or2seconds).\n2.labelwithalowprecision withthehelpofthespec-\ntrogram.\n3.preciseadjustment withthe’autoplay’option. Ital-\nlowssettingthelabel justbeforea newsoundevent\noccurs.\nNotethatnoinstructionnorguidancewasgivenbefore\ntheannotationexceptforthetoolmanipulationitself.\n4. TESTS\n4.1. Databasecontents\nThe labelling is evaluated on a ﬁrst set of 17 sound ﬁles.\nMostofthemhavebeenextractedfromtheRWCdatabase\n[6]. Thesamplingrateusedthroughoutis44.1kHz.\nThe other ones comes from anechoic recordingsmade\nin the laboratory. The contents of our set are shared on\na web site ([7]). The sounds extracted from the RWC\ndatabaseare notfreelyavailable,but the referencesof the\nﬁles are indicated, as well as the start and end samples.\nThe self-made recordings will be shared with a free ac-\ncess and all the rights of use for research purposes. The\ncompletion of our database is in progress and will be up-\ndatedontheweb site.\nThe set is composed of solo performances of mono-\nphonicinstruments(e.g. trumpet,clarinet,saxophone,sy n-\nthetic bass), polyphonic instruments (e.g. cello, violin,\ndistorted and steel guitar, piano) and complex mixes in\ndifferent music genres (e.g. rock, classical, pop, techno,\njazz).\n4.2. EvaluationMethodsforthe annotation\nIn a ﬁrst step, we compare the annotation of the subjects\nby pair. For each subject, the detected labels are counted\non each ﬁle. Then, for each ﬁle, we calculate the time\ndifferencesbetweencorrespondinglabelswherebothsub-\njectsmarkedagivenonset. Themeanofthesedifferences\nrevealsthedifﬁcultytoannotateoneﬁle. Neverthelessthi s\nsecond evaluation requires an arbitrary choice: we must\ndecide to which extent two labels must be assigned to a\nsameonset. Wehavesetthemaximumtimedifferencebe-\ntweentwocorrespondinglabelsat0.1second,considering# Content Ref.duration\n1Solotrumpet ENST 14s\n2Soloclarinet ENST 30s\n3Solosaxophone ENST 12s\n4Solosyntheticbass RWC 7s\n5 Solocello RWC 14s\n6 Soloviolin RWC 15s\n7Solo distortedguitar 6s\n8Solosteel guitar RWC 15s\n9Soloelectricguitar RWC 15s\n10 Solopiano RWC 15s\n11 techno RWC 6s\n12 rock RWC 15s\n13 jazz(octet) RWC 14s\n14jazz (contrabass) RWC 11s\n15 classic 1 RWC 20s\n16 classic 2 RWC 14s\n17 pop1 RWC 15s\nTable1. Descriptionofourdatabase. TheﬁleswithRWC\nreference are taken from the RWC database, those with\nENST reference are recordings made in the lab and are\navailable on our web site. Files are grouped in 3 cate-\ngories: solomonophonicinstruments,solopolyphonicin-\nstruments and complex mix. Full references of the RWC\nﬁlescanbe foundontheproject’swebsite [7]\nthatitrepresentsanupperboundforthedifferenceinboth\nannotators’ estimates of the same onset time. However,\nthe optimalchoice ofthis tolerancetime needsfurtherin-\nvestigations.\nTo know the most reliable labels, we browse all the\nconsistent labels of one comparison, and check that they\nare also consistent for the comparisonsbetween the other\npairs of annotations. For instance, in our case where the\nannotation were conducted by three subjects, the consis-\ntentlabelsofthecomparisonbetweensubjects1and2are\nselected and then it is checked that they are consistent in\nthe comparison between subjects 2 and 3, and ﬁnally be-\ntween subjects 3 and 1. By computing the average times\nof these labels between all the annotators, reliable onset\ntimes can be obtained. It is also possible to keep only\nthe labels of the best labeller (the annotatorwhose labels\ntimesaretheclosest tothese averagelabeltimes).\n4.3. Results\nThe number of labels set by each user for each ﬁle, the\nnumber of reliable labels and the mean of the differences\nbetween each annotation are shown in Table 2. We can\nﬁrstobservethatthenumberoflabelsdetectedbythesub-\njects is more variable when the numberof notes playable\nat the same moment increases. An remarkable excep-\ntion is techno music: the time is so quantized that all\nthe listeners agree to the onset repartition. Some differ-\nences also appear within the onset numbers labeled byFile Numberof Numberof Average\n#labelledonsets consistent timing\n123onsets difference\n1606160 60 3.9ms\n2383846 33 13.6ms\n310913 6 11.9ms\n4252526 25 2.5ms\n5656565 58 14.4ms\n6797979 78 7.2ms\n7202221 20 8.9ms\n8585858 58 7.7ms\n9413941 37 9.9ms\n10202020 19 7.0ms\n11565656 56 4.7ms\n12626266 59 9.9ms\n13565256 47 11.7ms\n14615452 53 9.0ms\n15494953 38 15.8ms\n16121212 4 28.4ms\n17324041 27 11.7ms\nTotal744741765 678 10.5ms\nTable 2. Results of the hand-labelling process. Columns\nmarked1,2and3representthenumberofonsetslabelled\nby each of the test listeners. The next column indicates\nthe number of consistent onsets across listeners, used to\nconstruct our database of reliable onset times. The last\ncolumngivesthemeantimingerroracrosslistenersonthe\nreliableonsets.\neach user in the monophonic performances: the annota-\ntions are subjective when the breathing or the instrument\nkeys can be heared. It emphasize the importance of pre-\ncise ordersto give to the listeners. If some difﬁcultiesare\ncombined (e.g. slow attack instruments and polyphonic\nperformance), the results cannot be exploited. For in-\nstance,only4onsetshavebeenfoundreliableoutofthe12\nonsetslabeledbyeachsubjectinthe“classic2”ﬁle(num-\nber 16). This shows that obtaining statistically meaning-\nful results is very complex for this ﬁle. The low number\nofreliableonsetsisofcoursecorrelatedwitha highmean\ndifferencebetweenthe labelstimeofeachsubject.\n5. CONCLUSION\nIn this paper, a fundamental aspect of the evaluation of\nautomatic onset detection algorithmsis studied. We have\nshown that the number of onsets detected by a listener is\nnot only dependenton the music signal itself, but also on\nthe guidance instructions given to annotators to mark the\nnote onsets. This dependance suggests that onset detec-\ntion algorithms could be evaluated with a different tol-\nerance window for each type of ﬁle. For example, a 20\nmilliseconds tolerance window appears to be acceptable\nfor percussive signals, while it is deﬁnitely too short for\nmusic played by bow strings to take into account the dif-ferencesofonsettimeannotationbetweensubjects. How-\never, this consideration must be also balanced in regards\ntotheapplicationoftheautomaticdetection. Forexample,\nif tempodetectiondoesnotdemanda veryaccurateonset\nlocalization, the estimation of the attack duration (for ex -\nample for instrument recognition)would need a far more\nrobustonsetdetectionfunction.\nFinally, in order to clearly contribute to future mean-\ningful evaluation of onset detection algorithms, the test\ndatabase, the software tool used to annotate the note on-\nsets,andthesetofreliableonsettimesarefreelyavailabl e\nfor research purposes(except for the audio ﬁles extracted\nfromthepublicdatabaseRWCforwhichonlytheposition\noftheusedaudiosegmentsareprovided). Theperspective\nof evolution of the database is to include more anechoic\nrecording of solo performances, and also to be more re-\nliably annotated by performingfurther statistical analys is\nwith alargernumberoflisteners.\n6. REFERENCES\n[1] Klapuri, A. ”Sound Onset Detection by Ap-\nplying Psychoacoustic Knowledge”, Proceed-\ningsIEEEInt.Conf.AcousticsSpeechandSig.\nProc. (ICASSP) ,pp.3089–3092,PhoenixAR,\nUSA March1999.\n[2] Davy M. and Godsill S., ”Detection of abrupt\nspectral changes using Support Vector Ma-\nchines: anapplicationtoaudiosignalsegmen-\ntation”,inProcoftheIEEE-ICASSP,Orlando,\nFlorida,2002\n[3] Goto M. and Muraoka Y., ”A real-time beat\ntracking system for audio signals”, Proc. of\nInternational Computer Music Conference,\n1995.\n[4] Rodet X. and Jaillet F. ”Detection and model-\ning of fast attack transients” in Proc. of Inter-\nnationalComputerMusicConference,2001.\n[5] Bello, J.P, Daudet L., Abdallah S., Duxbury\nC., Davies M. and Sandler M., ”A tutorial on\nonset detection in music signals”, to be pub-\nlished (IEEEtrans.onASSP) .\n[6] Goto M., RWC music database, published at\nhttp://staff.aist.go.jp/m.goto/RWC-MDB/\n[7] Leveau P., Daudet L., G. Richard,\n”Database and tools for onset detec-\ntion evaluation” to be accessible at\nhttp://www.enst.fr/ ˜grichard/ISMIR04/\n[8] Kauppinen I., ”Methods for detecting impul-\nsive noise in speech and audio signals”, in\nProc.ofDSP-2002,July 2002."
    },
    {
        "title": "Causal Tempo Tracking of Audio.",
        "author": [
            "Matthew E. P. Davies",
            "Mark D. Plumbley"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417873",
        "url": "https://doi.org/10.5281/zenodo.1417873",
        "ee": "https://zenodo.org/records/1417873/files/DaviesP04.pdf",
        "abstract": "We introduce a causal approach to tempo tracking for mu- sical audio signals. Our system is designed towards an eventual real-time implementation; requiring minimal high- level knowledge of the musical audio. The tempo track- ing system is divided into two sections: an onset analysis stage, used to derive a rhythmically meaningful represen- tation from the input audio, followed by a beat match- ing algorithm using auto- and cross-correlative methods to generate short term predictions of future beats in the audio. The algorithm is evaluated over a range of musical styles by comparing the predicted output to beats tapped by a musician. An investigation is also presented into three rhythmicallycomplex beat tracking problems, where the tempo is not constant. Preliminary results demonstrate good accuracy for this type of system. Keywords – Tempo tracking, beat analysis, onset detec- tion, rhythmic analysis",
        "zenodo_id": 1417873,
        "dblp_key": "conf/ismir/DaviesP04",
        "keywords": [
            "causal approach",
            "tempo tracking",
            "musical audio signals",
            "real-time implementation",
            "minimal high-level knowledge",
            "onset analysis stage",
            "beat matching algorithm",
            "tempo constant",
            "rhythmically complex beat tracking",
            "tempo accuracy"
        ],
        "content": "CAUSALTEMPOTRACKINGOFAUDIO\nMatthewE.P.DaviesandMarkD.Plumbley\nCentreforDigitalMusic\nQueenMaryUniversityofLondon\nABSTRACT\nWeintroduce acausal approach totempo tracking formu-\nsical audio signals. Our system isdesigned towards an\neventual real-time implementation; requiring minimal high-\nlevelknowledge ofthemusical audio. The tempo track-\ningsystem isdivided intotwosections: anonset analysis\nstage, used toderivearhythmically meaningful represen-\ntation from theinput audio, follo wed byabeat match-\ningalgorithm using auto- andcross-correlati vemethods\ntogenerate short term predictions offuture beats inthe\naudio. Thealgorithm isevaluated overarange ofmusical\nstyles bycomparing thepredicted output tobeats tapped\nbyamusician. Aninvestigation isalso presented into\nthree rhythmically comple xbeattracking problems, where\nthetempo isnotconstant. Preliminary results demonstrate\ngood accurac yforthistype ofsystem.\nKeywords–Tempo tracking, beat analysis, onset detec-\ntion, rhythmic analysis\n1.INTRODUCTION\nInthispaper ,wedescribe anapproach tocausal tempo\ntracking that could form part ofaneventual system for\nreal-time automatic accompaniment .Theaimofourtempo\ntracking system istoproduce abeat prediction from mu-\nsical audio inreal-time: ineffect, to“tap along” tothe\nmusic.\nWhile beat tracking hasperhaps notrecei vedthesame\nlevelofinterest aspolyphonic pitch tracking forautomatic\nmusic transcription, there arenevertheless anumber of\nexisting approaches tobeat tracking andtempo analysis.\nSome arebased onsymbolic input such asMIDI: Brown\n[5],forexample, measures theautocorrelation derivedfrom\nMIDI note onset times toinfer metric structure. Raphael\n[13]uses prior knowledge ofthemusical piece toperform\naccurate score follo wing foranaccompaniment system\nbased ontheobserv ation ofatime-v arying tempo process.\nAllen andDannenber g[1]track beats inreal-time using\nnote onset times with abeam search tomaintain multiple\nhypotheses ofthetempo atanyonetime.\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forproﬁt orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation ontheﬁrstpage.\nc\r2004 Universitat Pompeu Fabra.Thespeciﬁcation ofanaudio input increases thecom-\nplexity oftempo tracking duetothepre-processing re-\nquired toderiveameaningful, symbolic-lik erepresenta-\ntion[6]onwhich toperform theanalysis. Depending on\nthedesired application thismay beperformed ofﬂine to\ncompensate forerrors introduced inpre-processing. Our\nmotivation istowards areal-time application. Acompre-\nhensi vereviewoftempo tracking research, including of-\nﬂine approaches totheproblem isgivenin[6].\nThereal-time beattracking system introduced byScheirer\n[14]emplo ysanonset analysis stage oversixoctavefre-\nquenc ybands which ispassed toparallel comb ﬁlterbanks\ntoextract thebeats. This requires a2-dimensional search\novertempo andphase toﬁnd thebest matching beat po-\nsition. Goto [10]also describes areal-time beat track-\ningsystem using anaudio input. Here arange ofdif-\nferent drum identiﬁcation models areused tomatch an\naudio input todrum beats ondifferent instruments, these\narecombined with harmonic information related tochord\nchanges, toinfer andtrack thebeat structure.\nAnideal beat-tracking system foroureventual aimof\nanautomatic accompaniment system would haveanum-\nberofparticular properties: (i)audio input, rather that\nsymbolic such asMIDI; (ii)causal, such thatabeat pre-\ndiction isproduced from knowledge ofthe“past” signal\nonly; (iii) versatile tochanges intempo andstyle; (iv)\nminimal knowledge required about themusical piece to\nbetrack ed;and(v)computationally efﬁcient tobeimple-\nmented inreal-time. Wetherefore setouttodesign abeat\ntracking approach thatwould besuitable forthispurpose.\nOur approach combines recent workononset detec-\ntion [2]with abeat prediction method based ontheau-\ntocorrelation function (ACF)oftheonset detection func-\ntion. While ithasbeen suggested that theACFwould\nnotbesuitable forbeat tracking, duetothelossofphase\ninformation [14],wecombine thiswith aseparate phase\nmatching stage torecoverthebeat timings. Inthisway\nweseparate thetwo-dimensional search forbeat-period\nandbeat-alignment (asin[14])intotwoone-dimensional\nsearches: oneforbeat-period, follo wed bythesecond for\nbeat-alignment once thebeat period isknown,hence sim-\nplifying thesearch process.\nThepaper isstructured asfollo ws.InSection 2wede-\nscribe theonset detection stage, follo wedbythetempo es-\ntimation inSection 3.Section 4details thebeat alignment\nandprediction stages follo wed byresults andanalysis in\nSections 5and6.Anoverall discussion ofthesystem is\ngiveninSection 7with conclusions inSection 8.Figure1.Algorithm Operation Overview.(HFC=highfre-\nquencycontent,ACF=autocorrelation function)\n2.ONSETANALYSIS\nTheaimoftheonset analysis stage isnottoexplicitly de-\ntectthelocations ofnote onsets, rather togenerate amid-\nlevelrepresentation oftheinput which emphasizes theon-\nsetlocations -similar inprinciple tothemulti-band, enve-\nlope extraction processing in[14].Toreﬂect thisneed we\nchoose theonsetdetection function [2]-acontinuous sig-\nnalwith peaks atonset positions, astheinput tothetempo\nanalysis stage. Anexample detection function isshownin\nFigure 2.\nInoursystem weusetwodetection functions -anHFC\n[12]andacomple xdomain approach [3]which aremulti-\nplied together tocreate asingle input forthetempo analy-\nsisstage (asshowninthelefthand plotofFigure 1).The\ncombination ofthese twodetection functions hasbeen\nshowntogiveimpro vedperformance foronset detection\nthan when used individually [4].Each onset detection\nfunction isgenerated from frame based analysis using a\nwindo wsizeof1024 samples andastepincrement of512\nsamples, from audio sampled at44.1kHz, giving atem-\nporal resolution of11.6 ms. Toaidinthedevelopment\nprocess thetwodetection functions were generated prior\ntoruntime, howeverboth arecomputationally feasible in\nreal-time [4].\n2.1.HighFrequencyContent(HFC)\nMasri andBateman [12]present anapproach toenergy\nbased onset detection, using alinear weighting correspond-\ningtobinfrequenc ykoftheShort TimeFourier Trans-\nform (STFT) frame Xk[n]ofaudio inputx[n]toempha-\nsize thehigh frequenc yenergywithin thesignal, giving\nthedetection function output dfh[n]givenby:\ndfh[n]=NX\nk=0kjXk[n]j (1)\nThis technique isparticularly appropriate foremphasizing\npercussi vetype onsets, most notably cymbals, where the\ntransient region oftheinstrument hitismainly composed\nofenergyathigh frequencies.2.2.ComplexDomainOnsetDetection\nWhile theHFC approach issuited forsignals with strong\npercussi vecontent, itperforms poorly when applied mu-\nsicwith non-percussi veonsets, such asthose created by\nabowed violin. Wetherefore incorporate asecond detec-\ntionfunction, asimplemented byBello etal[3]thatisable\ntocontend with awider variety ofsignal types.\ndfc[n]=1\nNNX\nk=0jj~Xk[n]\u0000Xk[n]jj2(2)\nThecomple xdetection function dfc[n]showninequation\n(2)isacombined energyandphase based approach toon-\nsetdetection. Itportrays thecomple xspectral difference\nbetween thecurrent frame Xk[n]ofaSTFT andapre-\ndicted targetframe, ~Xk[n].Detection function peaks are\ntheresult either ofenergychange ordeviations inphase\nacceleration. These deviations occur intransients aswell\nasduring pitch changes (often calledtonalonsets ,where\nnoperceptual energychange isobserv ed)enabling theap-\nproach todetect onsets inawider range ofsignals. Amore\ncomplete derivation anddiscussion may befound in[2].\n3.TEMPOANALYSIS\nOur tempo analysis process shownintheright ofFig-\nure1,where thebeat period isestimated from theauto-\ncorrelation function (ACF) ofthedetection function. A\nframe based approach isimplemented, using awindo w\nsizeof512detection function samples, with astep incre-\nment of128samples (75% overlap), giving anaudio anal-\nysisframe ofapproximately 6seconds.\n3.1.Auto-correlationfunction(ACF)\nTheﬁrst stage inestimating thebeat period istocalculate\ntheACFoftheinput detection function frame. Giventhe\nassumption thatthedetection function hasstrong peaks at\nnote onsets andtherhythmic content ofthesignal iscoher -\nent(i.e. thatthere issome rhythmic structure) thepeaks\noftheACFwillcorrespond toperiodic events within the\nsignal atcertain time lags, showninFigure 2.TheACF,\nrdf,atlag,l,isderivedasfollo ws:\nrdf[l]=N\u00001X\nn=0df[n]df[n\u0000l] (3)\nwhere df[n]isthecombined detection function, andNis\nthelength ofthedetection function frame. Thelowerplot\ninFigure 2showsalinear trend downfrom thestart of\ntheACF.Itisnecessary tocorrect thissotheanalysis is\nnotbiased. Amodiﬁed ACF,^rdf,with thebias remo ved\niscalculated inequation (4).Thesignal issquared toem-\nphasize thepeaks.\n^rdf[l]=((N\u00001X\nn=0df[n]df[n\u0000l])(1+j(l\u0000N)j))2(4)50100 150 200 250 300 350 400 450 5000.20.40.60.8DF\n50100 150 200 250 300 350 400 450 5000.20.40.60.8\nSample NumberACF\nFigure2.Detection function(upperplot)andcorresponding\nACF(lowerplot)\nByselecting thelagofaparticular peak intheACF,anes-\ntimate ofthetempo canbemade byapplying equation (5),\nwhere \fconvertstheobserv edACFlaglintotherecog-\nnisable units oftempo: beats perminute (bpm).\ntempo=\f\nl(5)\n3.2.BeatPeriodEstimation\nTheinversetempo-lagrelationship results inpoor tempo\nresolution atshort lags, meaning asingle peak isoften\ninsuf ﬁcient toobtain anaccurate value forthebeat pe-\nriod. Figure 2showstheACFhasrhythmically related\npeaks atlonger lags. Inasimilar conte xttoBrown’snar-\nrowedautocorrelationfunction [5],greater accurac ycan\nbeachie vedbyextracting more than onepeak. Wecom-\nbine four related lagobserv ations from theACF,andtake\nthemean toarriveatavalue which compensates forthe\npoor resolution.\nAnefﬁcient means toextract anumber ofarithmeti-\ncally related events from asignal (inthiscase rhythmi-\ncally related lags from anACF)istouseamatrix-based\ncomb ﬁlter approach [7].Because theACFiszero-phase,\nthesearch forthelagcorresponding tothebeat period is\nshift-in variant. This permits theACFtobepassed through\nasingle bank ofcomb ﬁlters. Each ﬁlter ismatched toa\ndifferent fundamental beat period, covering alllags inthe\nrange of1to128samples, where themaximal lagoutput \u001c\ncorresponds tothebestmatching beat period ofthesignal\nframe:\n\u001c=argmax\nl(^rdf\u0002M) (6)\nwhere ^rdfisarowvector oflength NandMisan(N\u0002L)\nmatrix, withLasthenumber ofpossible lags.\n3.3.LagWeightingFunction\nThe implementation ofthecomb ﬁlterbank, asderived\ninsection 3.2, didnotlead totherobustextraction of\nlags, eveninsimple testcases. Anequal weighting for\nalllags gavetoomuch freedom tothepossible beat pe-\nriod output, allowing lags ofjustasingle sample tobe\nconsidered inthebeat period estimation andresulted in\nfrequent inconsistenc yoftheoutput. Anintuiti vesolu-\ntion would betoimpose explicit limits ontherange of\nComb filterbank, MSample number\nLag20406080100120100\n200\n300\n400\n500\n204060801001200.20.40.60.81\nLagWeightingLag Weighting Function, Lw\nFigure3.Matriximplementation ofthebankofcomb®lters\n(leftplot)andtheweighting appliedtoeachrowofthematrix\n(rightplot)\nlagvalues -tohaveminimum andmaximum values de-\nﬁned byadesired range oftempi, derivedfrom equation\n(5). However,theinverse relationship between lagand\ntempo means thatanequal weighting oflagsdoes notcor-\nrespond toequal tempo weighting which results inbias\ntowardsslowertempi. Analternati veistoderiveaweight-\ningfunction fortheallowed lagvalues, imposing implicit\nlimits onanacceptable tempo range andreducing thebias\nintroduced from thenon-linear tempo-lagrelationship.\nInvestigating arange ofpossible weighting functions\nbased ondesired tempo output responses, themost con-\nsistent results were obtained from anempirically derived\nperceptualweighting using theRayleigh distrib ution func-\ntion:\nLw[l]=l\nb2e\u0000l2\n2b2 (7)\nThis gaveaskewed response (shownintheright hand\nplot ofFigure 3)with amaximum atalagof50sam-\nples (where b=50inequation (7))corresponding tothe\npreferredhuman tempo [8]of100bpm.\nBecause theﬁlterbank didnotchange throughout the\noperation ofthealgorithm, itcould becreated ofﬂine. Since\ntheweighting wasalso time-in variant, wechose toincor -\nporate itdirectly into theﬁlterbank bymultiplying each\nrowofMbyLw[l]rather than introduce unnecessary com-\nputation byweighting theACFateach iteration.\n4.BEATALIGNMENT ANDPREDICTION\nWewish toevaluate ourbeat-tracking system bysynthe-\nsizing beats intime with theinput audio. Theresult ofthe\nbeat period estimation alone isinsuf ﬁcient information to\nsuccessfully create abeat output. Equal importance must\nbegiventotheplacement ofthebeats aswell astheir un-\nderlying rate[14],both ofwhich arerequired toenable the\nprediction offuture beats intheinput signal.\n4.1.BeatAlignment\nAsearch forthebestmatching beatalignment overallpos-\nsible shifts ofthebeat period (uptoonewhole period) is\nperformed bycross-correlation ofthedetection function\nwith thecomb ﬁlter matched tothebeat period. InorderTime                              Past Data                                            Now                      Future Signal\nCurrent Detection \nFunction Frame\nBeat Period Comb\nPredicted BeatsAlignmentτ τ\nφγ1γ2\nFigure4.Beatalignment andprediction ofasingleanalysis\nframeofthedetection function. Beatsarepredicted onlyupthe\nendofthesubsequent stepincrement\ntopreserv etheaccurac yofthebeat period, weneed sub-\nsample accurac yinthealignment process. Weupsample\nthedetection function andthecomb ﬁlter byafactor of\n4,calculating anoffset\u001emaxwith respect toareference\npoint inthecurrent detection function frame:\n4\u0002\u001emax=argmax\n\u001e\bX\n\u001e=1\u000e(n\u00004k\u001c+\u001e)df4[n](8)\nwhere k=1:4(tomakefour peaks inthecomb ﬁlter),\n\u001cisthebeat period, \u001erepresents theshifts uptoonepe-\nriod\banddf4[n]istheupsampled detection function. To\nmaintain causality andgivethemost salient alignment, the\noffsetindicates thelocation ofthelastbeat before theend\nofthecurrent frame, seeFigure 4.\n4.2.BeatPrediction\nForthebeat tracking tobecausal, nofuture audio data can\nbeused intheanalysis, therefore future beats must bepre-\ndicted from past data. Our system performs beat predic-\ntionbytriggering synthesized beats atinterv alsspeciﬁed\nbythebeat period from temporal locations deﬁned bythe\nalignment process. Allbeats arealigned with reference\ntotheendofthecurrent analysis frame, andarepredicted\nonly totheendofthenextstep increment. Anypredic-\ntions beyond thispoint should besuperseded inaccurac y\nbytheanalysis ofthefollo wing frame. Agraphical repre-\nsentation ofthebeat prediction process isgiveninFigure\n4,with thebeat predictions calculated using theformula:\n^\rm=(ti\u0000\u001ei)+m\u001ci (9)\nwhere ^\rmislocation ofthemthpredicted beat inthesub-\nsequent analysis frame (i+1),tiisthetime attheendof\nframe i,with\u001ei,theoffsetand\u001ci,thebeat period offrame\ni.Tocompensate forthelong windo wsize(about 6sec-\nonds), afullframe ofwhite noise (tosimulate background\nnoise) isprepended tothedetection function prior torun-\ntime. This istoallowsomeapproximate tempo analysis\ntooccur before afulltime frame haselapsed.Musical Tempo Beat Alignment\nGenre Accuracy(%)Accuracy(%)Loss(%)\nRock(6) 89 87 2\nFunk(4) 89 87 2\nElectronic (4) 90 79 11\nJazz(4) 86 84 2\nClassical (3) 86 63 23\nPop(3) 87 83 4\nTotal(24) 88 81 7\nTable1.Summary ofTempoTrackingPerformance. Thenum-\nberofsongsusedineachgenreisshowninbrackets\n5.RESULTS\nTheperformance ofthetempo tracking system waseval-\nuated bysubjecti vecomparison with beats tapped bya\nmusician. Byassuming perfect beat placement from the\nhuman tapped performance, asimple accurac ymeasure\nisderivedbased ontheratio ofgood predictions tothe\ntotal number ofpredicted beats. The errors correspond\ntospurious beats (including those with notemporal rele-\nvance) andomitted beats. Data isalsopresented based on\nthebeat period estimates, tohighlight theaccurac yinthe\ntempo analysis stage, anddemonstrate thediscrepanc ybe-\ntween thetempo analysis andbeat prediction stages. The\nresults aresummarized inTable 1.The testsetincluded\ncommercial andnon-commercial recordings (sampled at\n44.1kHz) covering arange ofmusical genres each lasting\nbetween 30seconds and1minute inlength.\nAnimmediate observ ation from theresults shownin\nTable 1,isthat, ineverycase, thebeat accurac yislower\nthan thetempo accurac y.This implies that more errors\naremade inthealignment andprediction stages than sim-\nplythose carried forw ardfrom incorrect beat period es-\ntimates. Itappears that thealgorithm isentirely reliant\nonawell structured detection function, andthatthema-\njority oferrors were caused bydeviations from consistent\nrhythmic structure, particularly areas oflowmusical ac-\ntivity such assong intros. Similarly theﬁrst fewbeats\nwere often theleast accurately predicted, duetobeats be-\ningderivedfrom acombination ofthedetection function\nandthepre-pended noise, where thepast data naturally\nlacks structure. These results were notamended todis-\ncount theinitial errors because asimilar approach would\nbeneeded forareal-time implementation ofthealgorithm.\nWehavenotyetperformed acomprehensi veevaluation\nagainst other systems, butinitial tests against aversion of\nScheirer’ s[14]implementation indicate weareobtaining\ncompetiti veresults.\n6.COMPLICA TEDBEATTRACKING\nPROBLEMS\nThe aimofthissection wastotesttherobustness ofthe\nalgorithm when applied toanumber ofdifﬁcult testcases,\ncreated tosimulate thepossible beha viour ofanimpro-10 20 30 40 50 60 70 800100200\nBeat NumberAlgorithm10 20 30 40 50 60 70 800100200Human\n125 bpm \n125 bpm \n85 bpm 85 bpm (BPM) (BPM)\nFigure5.Comparison ofhumantracking(upperplot)with\nalgorithm tracking(lowerplot)forsteptempochange\nvised performance towhich some automatic accompani-\nment might bedesired. The testcases comprised astep\ntempo change, aramp tempo change andanexpressi ve\npiano performance. Subjecti vecomparisons were made\nbetween thealgorithm’ sperformance andbeats tapped by\namusician. Plots aregivenshowing tempo contours, de-\nrivedfrom theinter-onset-interv alsboth forhuman tapped\nandalgorithm predicted beats.\n6.1.StepTempoChange\nTosimulate astep tempo change, twosongs ofdifferent\nfundamental tempo were cuttogether: asong with anelec-\ntronic rhythm track (125 bpm) follo wed byafunk song\n(85bpm). Figure 5showsclearly when thetransition be-\ntween thetwosongs occurs. There isanoticeable delay\nofapproximately 2seconds before thesystem detects the\ntempo ofthesecond segment. Howeveroutside ofthe\ntransition thesystem performs with anaccurac yequiva-\nlenttowhen theinput signal hasnoabrupt tempo change.\n6.2.RampTempoChange\nAramp tempo change may becharacterised byacontin-\nuedacceleration ordeceleration inthetempo ofapiece of\nmusic. Anexample isthesong “L.A. Woman” recorded\nbyThe Doors. Figure 6showstheperformance ofthe\nalgorithm isnotasconsistent asthehuman tapped beats,\nwhich wasnoticeable when thebeattracks were compared\naudibly .The beat prediction stage assumes thetempo is\nconstant across thelength ofthewindo w,thismeans linear\ntempo progressions canonly beapproximated bystep-like\ntransitions. Alistening testconﬁrmed thepredicted beats\nlagged behind thehuman tapped beats intheexcerpt.\nAninteresting result occurred inthiscase, related to\nthetempooctaveproblem [11],exhibited bythesudden\njump intempo inthepredicted beat track. Listening tests\nconﬁrmed thatnosuch discontinuity inthetempo existed.\nThe apparent change intempo wasinduced bythelag\nweighting applied totheﬁlterbank (section 3.3), where\nhalf thebeat period wasfound tobeabetter match tothe\ninput datathan theactual tempo -indicating anupper limit\nforthesystem ofaround 170bpm. Weintend toinvesti-\ngate thiseffectfurther .10 20 30 40 50 60 70 80 90 100 1100100200\nBeat NumberAlgorithm10 20 30 40 50 60 70 80 90 100 110 1200100200Human\nHalf tempo(BPM) (BPM)\nFigure6.Comparison ofhumantracking(upperplot)with\nalgorithm tracking(lowerplot)forramptempochange\n10 20 30 40 50 60 70 800100200Human\n10 20 30 40 50 60 70 800100200Algorithm\nBeat Number(BPM) (BPM)\nFigure7.Comparison ofhumantracking(upperplot)with\nalgorithm tracking(lowerplot)forexpressivepianomusic\n6.3.ExpressivePianoPerformance\nThe ﬁnal case study examines anexcerpt ofexpressi ve\nsolo piano (Mozart’ sSonata K.310). Due totheinconsis-\ntencyinthetempo ofthepiece itisdifﬁcult tomakeavalid\nvisual comparison ofthetempo contours showninFigure\n7.Thecollection ofspikesinthepredicted beat plotare\ntheresult ofachange inrhythmic structure, where thesys-\ntemswitches between tracking attheunderlying rateand\ntwice thetempo. Theapproximate beat accurac ymeasure\nforthepiece wasfound tobe75% which indicates mod-\nerate performance when compared tothetestsubjects in\nTable 1with approximately constant tempo. Thedifﬁculty\ninaccurate evaluation ofthiscase inparticular ,highlights\ntheneed formore robustevaluation, adetailed discussion\nofevaluation procedures related tobeat tracking isgiven\nin[9].\n7.DISCUSSION\nThe algorithm’ sperformance isbased entirely onrobust\nshort term tempo analysis. Therefore themost common\nerrors inbeat period occur when thedetection function,\nandhence theACFarepoorly deﬁned. Inthealignment\nstage, additional errors occur where themaximal cross-\ncorelation isaligned toastrong accent (i.e. alargepeak\ninthedetection function) which isnotrelated tothebeat\nstructure ofthesong. Due tothenature ofthepredic-\ntionprocess, errors ineither beat period oralignment are\nrepeated when more than asingle beat ispredicted per\nframe. Ouranalysis suggests thatatime windo wof6sec-\nonds isenough prior information tocorrectly deduce the\ntempo, butthatthealignment oftapped beats would bemore accurately track edbyincorporating feedback into\nthesystem. Current workisunderw aytoreduce align-\nment errors byusing previously predicted beats tocreate\nanexpectation ofwhere subsequent alignments should oc-\ncur.Theresult ofwhich should also impose some notion\nofcontinuity tothesystem -animportant factor which is\nnotaddressed inthisimplementation.\nAnumber offurther impro vements might increase the\nperformance ofthetempo tracking system. Firstly arhyth-\nmicconﬁdence measure ontheinput ACFcould beused\ntoindicate theextent towhich thedata iswell-structured\nandhence reliable. This could beapplied toamore intel-\nligent useofthetwodetection functions where, instead of\nsimply combining them, adifferential selection could be\nmade based ontheconﬁdence ofeach function. Itwould\nalsobeinteresting toexamine perceptual studies oftempo\ntracking, toconstruct amore theoretically well-founded\nperceptual weighting fortheﬁlterbank aswell asimple-\nmenting alternati veonset analysis stages, such asanenve-\nlope based approach [14]orenergyﬂux [11].Generally\nthesystem would beneﬁt from amore robustevaluation\nstage including adirect comparison with other approaches\ntotheproblem.\n8.CONCLUSIONS\nWehaveintroduced acausal approach totempo tracking\nformusical audio signals. The system hasbeen shown\ntoapproximate human tapped performance overarange\nofmusical styles forsignals with roughly constant tempo,\nwith encouraging results forthemore complicated track-\ningproblems exhibiting tempo change.\nFurther research isunderw aytoevaluate thesystem by\ncomparison with alternati veapproaches andimpro veper-\nformance byincorporating high levelmusical knowledge\ninto thesystem aswell asdeveloping areal-time imple-\nmentation.\n9.ACKNOWLEDGEMENTS\nThis research hasbeen partially funded bytheEU-FP6-\nIST-507142 project SIMA C(Semantic Interaction with Mu-\nsicAudio Contents). More information canbefound at\ntheproject website http://www .semanticaudio.or g.\nMatthe wDavies issupported byaColle geStudentship\nfrom Queen Mary University ofLondon.\n10.REFERENCES\n[1]Allen P.E.and Dannenber gR.B.“Track-\ningMusical Beats inReal Time”,Proceedings\nofInternational Computer MusicConference,\npp.140-143, 1990\n[2]Bello J.P.,Daudet L.,Abdallah S.,Duxb ury\nC.,DaviesM.E.andSandler M.B.“ATutorial\nonOnset Detection inMusic Signals”, IEEETransactions onSpeechandAudioProcessing\n-accepted forpublication ,2004\n[3]Bello J.P.,Duxb uryC.,Davies M.E. andSan-\ndler M.B., “On theuseofPhase andEner gy\nforMusical Onset Detection intheComple x\nDomain”, IEEESignalProcessingLetters,Vol\n11,No.6,pp553-556, June 2004\n[4]Brossier P.,Bello J.P.andPlumble yM.D.\n“Real-time Temporal Segmentation ofNote\nObjects inMusic Signals”, Proceedings ofIn-\nternational Computer MusicConference,2004\n[5]BrownJ.C.“Determination oftheMeter of\nMusical Scores byAutocorrelation” Journalof\ntheAcoustical SocietyofAmerica ,Vol94,No.\n4,1993\n[6]Dixon S.“Automatic Extraction ofTempo and\nBeat from Expressi vePerformances” Journal\nofNewMusicResearch,Vol30,No.1,March\n2001\n[7]Duxb uryC.“Signal Models forPolyphonic\nMusic”Ph.D.Thesis(submitted), Department\nofElectronicEngineering ,QueenMary,Uni-\nversityofLondon ,2004\n[8]Fraisse P.“Rhythm and Tempo”, InD.\nDeutsch, editor ,ThePsychologyofMusic ,\nAcademic Press, 1982\n[9]Goto M.andMuraoka Y.“Issues inEvaluat-\ningBeat Tracking Systems” WorkingNotesof\nIJCAI-97 WorkshoponIssuesinAIandMusic\n-Evaluation andAssessment ,1997\n[10] Goto M.“AnAudio-based Real-time Beat\nTracking System forMusic WithorWith-\noutDrum-sounds” JournalofNewMusicRe-\nsearch,Vol30,No.2,July 2001\n[11] Laroche J.“Efﬁcient Tempo andBeat Track-\ninginAudio Recordings” JournaloftheAudio\nEngineering Society ,Vol51,No.4,2003\n[12] Masri P.and Bateman A.“Impro vedMod-\nelling ofAttack Transients inMusic Analysis-\nResynthesis” Proceedings ofInternational\nComputer MusicConference,pp.100-103,\n1996\n[13] Raphael C.“Automated Rhythm Transcrip-\ntion”Proceedings ofInternational Symposium\nonMusicInformation Retrieval,pp.99-107,\n2001\n[14] Scheirer E.D.“Tempo andBeat Analysis of\nAcoustic Musical Signals ”Journalofthe\nAcoustical SocietyofAmerica ,Vol103, No.\n1,1998"
    },
    {
        "title": "Towards Characterisation of Music via Rhythmic Patterns.",
        "author": [
            "Simon Dixon",
            "Fabien Gouyon",
            "Gerhard Widmer"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416220",
        "url": "https://doi.org/10.5281/zenodo.1416220",
        "ee": "https://zenodo.org/records/1416220/files/DixonGW04.pdf",
        "abstract": "A central problem in music information retrieval is finding suitable representations which enable efficient and accu- rate computation of musical similarity and identity. Low level audio features are ideal for calculating identity, but are of limited use for similarity measures, as many aspects of music can only be captured by considering high level features. We present a new method of characterising mu- sic by typical bar-length rhythmic patterns which are au- tomatically extracted from the audio signal, and demon- strate the usefulness of this representation by its applica- tion in a genre classification task. Recent work has shown the importance of tempo and periodicity features for genre recognition, and we extend this research by employing the extracted temporal patterns as features. Standard classifi- cation algorithms are utilised to discriminate 8 classes of Standard and Latin ballroom dance music (698 pieces). Although pattern extraction is error-prone, and patterns are not always unique to a genre, classification by rhyth- mic pattern alone achieves up to 50% correctness (base- line 16%), and by combining with other features, a classi- fication rate of 96% is obtained.",
        "zenodo_id": 1416220,
        "dblp_key": "conf/ismir/DixonGW04",
        "keywords": [
            "music information retrieval",
            "musical similarity",
            "music identity",
            "low level audio features",
            "high level features",
            "typical bar-length rhythmic patterns",
            "genre classification",
            "tempo and periodicity features",
            "dance music",
            "classification algorithms"
        ],
        "content": "TOWARDS CHARACTERISATION OF MUSIC VIA RHYTHMIC\nPATTERNS\nSimon Dixon\nAustrian Research Institute for AI\nVienna, AustriaFabien Gouyon\nUniversitat Pompeu Fabra\nBarcelona, SpainGerhard Widmer\nMedical University of Vienna\nMedical Cybernetics and AI\nABSTRACT\nAcentralprobleminmusicinformationretrievalisﬁnding\nsuitable representations which enable efﬁcient and accu-\nrate computation of musical similarity and identity. Low\nlevel audio features are ideal for calculating identity, but\nareoflimiteduseforsimilaritymeasures,asmanyaspects\nof music can only be captured by considering high level\nfeatures. We present a new method of characterising mu-\nsic by typical bar-length rhythmic patterns which are au-\ntomatically extracted from the audio signal, and demon-\nstrate the usefulness of this representation by its applica-\ntioninagenreclassiﬁcationtask. Recentworkhasshown\ntheimportanceoftempoandperiodicityfeaturesforgenre\nrecognition,andweextendthisresearchbyemployingthe\nextracted temporal patterns as features. Standard classiﬁ-\ncation algorithms are utilised to discriminate 8 classes of\nStandard and Latin ballroom dance music (698 pieces).\nAlthough pattern extraction is error-prone, and patterns\nare not always unique to a genre, classiﬁcation by rhyth-\nmic pattern alone achieves up to 50% correctness (base-\nline16%),andbycombiningwithotherfeatures,aclassi-\nﬁcation rate of 96% is obtained.\n1. INTRODUCTION\nMostmusiccanbedescribedintermsofdimensionssuch\nas melody, harmony, rhythm, instrumentation and form.\nThese high-level features characterise music and at least\npartiallydetermineitsgenre,buttheyaredifﬁculttocom-\npute automatically from audio. As a result, most audio-\nrelated music information retrieval research has focussed\non extracting low-level features and then using machine\nlearning to perform tasks such as classiﬁcation. This ap-\nproach has met with some success, but it is limited by\ntwo main factors: (1) the low level of representation may\nconceal many of the truly relevant aspects of the music;\nand(2)thediscardingoftoomuchinformationbythefea-\nture extraction process may remove information which is\nneeded for the accurate functioning of the system.\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2004 Universitat Pompeu Fabra.Inthiswork,weaddressoneaspectoftheselimitations\nby a novel approach of extracting rhythmic patterns di-\nrectly from audio in order to characterise musical pieces.\nItishypothesisedthatrhythmicpatternsarenotrandomly\ndistributedamongstmusicalgenres,butrathertheyarein-\ndicative of a genre or small set of possible genres. There-\nfore, if patterns can be extracted successfully, we can test\nthis hypothesis by examining the usefulness of the pat-\nterns as features for genre classiﬁcation.\nAsdancemusicischaracterisedbyrepetitiverhythmic\npatterns, it is expected that the extraction of prominent\nrhythmic patterns would be particularly useful for clas-\nsiﬁcation. However, rhythmic patterns are not necessar-\nily unique to particular dance styles, and it is known that\nthereisacertainamountofoverlapbetweenstyles. Inthis\nwork, rather than assuming a ﬁxed dictionary of patterns,\nwe use an automatic extraction technique which ﬁnds the\nmost salient pattern for each piece. Thus the techniques\nused are generalisable to other musical genres.\nFirst the bar length patterns in the amplitude envelope\narefound andclusteredusingthe k-meansalgorithmwith\na Euclidean distance metric. The centre of the most sig-\nniﬁcant cluster is used to represent the piece, and a fea-\nture vector consisting of this rhythmic pattern and vari-\nous derived features is used for classiﬁcation on a music\ndatabase of the ﬁrst 30 seconds of 698 pieces of Standard\nand Latin ballroom dance music.\nAlthough we focus solely on the rhythmic aspects of\nmusic, we show that for genre classiﬁcation of dance mu-\nsic,averyhighlevelofaccuracyisobtainable. Theresults\nshow an improvement over previous methods which used\nperiodicity or inter-onset interval histograms and features\nderived from these. Other possible applications of auto-\nmatically extracted rhythmic patterns are query and re-\ntrieval of music, playlist generation, music visualisation,\nsynchronisationwithlightsandmultimediaperformances.\nInthefollowingsectionweoutlinethebackgroundand\nrelatedwork,andtheninthesubsequentsectionsdescribe\nthe pattern extraction algorithm and genre classiﬁcation\nexperiments, concluding with a discussion of the results\nand future work.\n2. RELATED WORK\nAudiofeatureextractionwasﬁrstaddressedinthecontext\nof speech recognition, and was later applied to classiﬁ-44\n44Figure 1. The importance of temporal sequence: these\ntwodifferentrhythmpatternshavethesamedistributionof\ninter-onsetintervalsbutaretypicaloftwodifferentgenres,\nCha Cha (above) and Rumba (below).\ncation tasks in order to separate speech from non-speech\nsignals such as music and environmental sounds [19, 26].\nMore recently, several authors have addressed classiﬁca-\ntion tasks speciﬁc to music, such as instrument recogni-\ntion [14] and detection of segments of music that contain\nsinging[2]. Othershavefocussedondeterminingsimilar-\nity judgements for content-based retrieval [8], for the or-\nganisation and navigation of large music collections [16,\n17] and for computation of high level semantic descrip-\ntors [21].\nAutomatic musical genre classiﬁcation has a shorter\nhistory. Tzanetakis et al. [23, 22] used three sets of fea-\ntures representing the timbral texture, rhythmic content\nandpitchcontentofmusicalpieces,andtrainedstatistical\npatternrecognitionclassiﬁerstoachievea61%classiﬁca-\ntionratefortenmusicalgenres. McKinneyandBreebaart\n[15]examinedtheuseofvariouslowlevelfeaturesetsand\nobtained 74% classiﬁcation on 7 musical genres. Dixon\net al. [7] compared two methods of periodicity detection,\nand developed a simple rule based system to classify 17\nstyles of Standard and Latin ballroom dance music based\nonly on the distribution of periodicities, with an accuracy\nof 37%.\nIn the abovementioned work, limited rhythmic infor-\nmation is encoded in the beat histogram [22], modula-\ntion energy [15] or periodicity distribution [7]. Each of\nthese representations provides information about the rel-\native frequency of various time intervals between events,\nbutdiscardstheinformationabouttheirtemporalsequence.\nFor example, consider the two rhythmic patterns in Fig-\nure 1. Both patterns have the same distribution of inter-\nonset intervals (three quarter notes and two eighth notes),\nbut the patterns are perceptually very different. The up-\nper pattern, which is typical of a Cha Cha rhythm, would\nnotbedescribedassyncopated,whereasthelowerpattern,\nmore likely to be found in a Rumba piece, is somewhat\nsyncopated.\nRhythmic patterns were used by Chen and Chen [4]\nfor song retrieval using symbolic queries and a database\nofsymbolicmusic,inwhichapproximatestringmatching\nprovided the similarity measure. The patterns used were\nnot general patterns which summarise a piece or a genre,\nbutspeciﬁcpatternswhichdidnotneedtooccurmorethan\nonce in the piece.Genre PiecesMetre Tempo Tempo\n(nominal) (actual)\nCha Cha 111 4128 92–137\nJive 604176124–182\nQuickstep 824200–208 189–216\nRumba 984104 73–145\nSamba 864200138–247\nTango 864128–132 112–135\nViennese Waltz 653174–180 168–186\nWaltz 110 384–90 78–106\nTable 1. Statistics of the data used in the experi-\nments. Tempo is given in BPM, where a beat corre-\nsponds to a quarter note (except for Samba and some Vi-\nennese Waltzes which have an eighth note beat). Nom-\ninal tempo values are according to the overviews at the\nwww.ballroomdancers.com web site.\nThe only work we are aware of in which rhythmic pat-\nterns were automatically extracted from audio data is by\nPaulusandKlapuri[18],whoextractedbar-lengthpatterns\nrepresented as vectors of loudness, spectral centroid and\nMFCCs, and then used dynamic time warping to measure\nsimilarity. Theirworkdidnotincludegenreclassiﬁcation,\nalthough they did indicate that the similarity of drum pat-\nterns was higher within genre than between genres.\nOther relevant research that involves the extraction of\nrhythmiccontentfromamusicalperformanceisbeattrack-\ning[10,11,20,3,5],thatis,ﬁndingthetimesofthebeats\n(at various metrical levels). If we assume that rhythmic\npatterns exist within a particular metrical unit, e.g. within\nbars, then ﬁnding the boundaries of these metrical units\nbecomes a prerequisite to pattern ﬁnding. The main dif-\nﬁculty in beat tracking is not in ﬁnding the periodicities,\nbuttheirphase. Thatis,thelengthofapatterncanbeesti-\nmated much more reliably than its starting point. We use\naninteractivebeattrackingsystem[6]inordertoannotate\nthe ﬁrst bar of each piece.\n3. PATTERN EXTRACTION\n3.1. Data\nTwo major difﬁculties for developing music information\nretrievalsystemsarethelackofreliablylabelleddatasets,\nand the fuzziness of class boundaries of the attributes.\nBallroom dance music has the advantage of providing a\nset of genres for which there is a high level of agree-\nment among listeners concerning the genre. We collected\n698 samples of Standard and Latin ballroom dance music\n(http://www.ballroomdancers.com ), each con-\nsisting of approximately the ﬁrst 30 seconds of a piece.\nThe music covers the following eight classes: Cha Cha,\nJive, Quickstep, Rumba, Samba, Tango, Viennese Waltz\nand (Slow) Waltz. The distribution of pieces over the\nclasses, the nominal tempo of each class, and the actual\ntempo ranges of the excerpts areshown in Table 1.3.2. Audio Processing\nThesampleswereuncompressedfromRealAudioformat\nto a standard PCM format at the same sampling rate as\ntheoriginalﬁle(either44100,16000or11025Hz,always\nmono). The amplitude envelope was extracted from the\nsignal using an RMS ﬁlter. The frame rate was set so that\na bar would contain a ﬁxed number bof samples at any\ntempo(wherethetempoisalreadyknown,asdescribedin\nthe following subsection).\nIfx(n)is the input signal with sampling rate rand bar\nlength lseconds,thenitsamplitudeenvelopeiscalculated\nwithasamplingrateof bsamplesperbarusingahopsize\nhgiven by:\nh=rl\nb(1)\nThe amplitude envelope y(n)is given by:\ny(n) =/radicalBigg/summationtext(n+k)h−1\ni=nhx(i)2\nkh(2)\nwhere kis the overlap factor. The bar lengths lranged\nfrom 0.97 to 3.30 sec. Best results were obtained with\nb= 72andk= 2, although values of bfrom 48 to 144\ngave similar results.\nTwoalternativerepresentationsfor y(n)werealsotried,\nbytakingrespectivelythesquareandtheabsolutevalueof\nthesignal x(n),passingitthroughaneighthorderCheby-\nshev Type I lowpass ﬁlter, and decimating to a sampling\nrateof bsamplesperbar. Thechoiceofrepresentationhad\nonly a small inﬂuence on results.\n3.3. Bar Finding\nMuch research has been conducted on beat tracking, that\nis, ﬁnding the times of musical beats in audio ﬁles [10,\n11, 20, 3, 5]. Although beat tracking is not a solved prob-\nlem, the extraction of periodicities is reliable, with the\nremaining difﬁculties being the mapping of periodicities\nto metrical levels (e.g. estimating which periodicity cor-\nresponds to the rate of quarter notes), and choosing the\ncorrect phase for a metrical level (e.g. estimating which\nquarternotebeatscorrespondtotheﬁrstbeatofeachbar).\nSince the focus of this work was not to perform beat\nor measure ﬁnding, we used values for the ﬁrst bar gener-\natedbyBeatRoot[6]andcorrectedmanually. Thisalsoal-\nlowed us to skip irregular (i.e. tempo-less) introductions,\nwhicharedifﬁculttodetectautomaticallyinshort(30sec)\nexcerpts.\nOnce the ﬁrst bar was known, the process of ﬁnding\nsubsequent bars was performed automatically, by search-\ning within ±5% of the end of the previous bar for a start-\ningpointwhichhasmaximumcorrelationwiththesumof\nprevious bars. That is, for each bar i, a correction factor\nδ(i)wascalculatedwhichdeterminedtheoffsetofthebe-\nginning of the following bar m(i+ 1)from its expected\nposition ( m(i) +b). If d=/floorleftbigb\n20/floorrightbig\nandm(i)is the index\nof the beginning of the ith bar, where m(1)is given byBeatRoot, then:\nm(i+ 1) = m(i) +b+δ(i) (3)\nwhere\nδ(i) = argdmax\nk=−db−1/summationdisplay\nj=0y(m(i) +b+k+j)∗z(i, j)(4)\nand\nz(i, j) =i/summationdisplay\nk=1y(m(k) +j) (5)\n3.4. Extracting Rhythmic Patterns\nOncethebarpositionsweredetermined,barlengthrhyth-\nmic patterns were then extracted, consisting of the am-\nplitude envelope of the signal between the start and end\npointsofthebar. The ithpattern viisavectoroflength b:\nvi=/angbracketlefty(m(i)), y(m(i) + 1) , . . . , y (m(i) +b−1)/angbracketright(6)\nIn order to remove outliers, k-means clustering (with k=\n4)wasusedtoﬁndclustersofsimilarbars,andthelargest\nclusterwastakenasdeﬁningthemostprominentrhythmic\npattern for each piece.\nIfCjis the cluster containing the most bars, then the\ncharacteristicrhythmicpattern p(n)ofapieceisgivenby:\np(n) =1\n|Cj|/summationdisplay\nk∈Cjy(m(k) +n) (7)\nFurthermore, we can deﬁne the distance D(i, j)between\ntwo rhythmic patterns pi(n)andpj(n)by the Euclidean\nmetric:\nD(i, j) =/radicaltp/radicalvertex/radicalvertex/radicalbtb/summationdisplay\nk=1(pi(k)−pj(k))2(8)\nFor example, Figure 2 shows the pattern vectors of all\n15barsofoneChaChaexcerpt,wherethecoloursindicate\nthe clusters to which the bars belong, and the thick black\ncurve shows the centre of the largest cluster, that is, the\nextracted pattern p(n). The perceptual onset of a sound\noccurs slightly before its energy peak [24], so it is valid\nto interpret peaks occurring immediately after a metrical\nboundary as representing an onset at that metrical posi-\ntion. For example, the extracted pattern in Figure 2 has\na peak at each eighth note, clearly implying a quadruple\nmetre, and if the ﬁve highest peaks are taken, the result-\ning pattern corresponds to the upper rhythmic pattern in\nFigure 1.\nViewing the representative patterns for each song pro-\nvides some feedback as to the success of the pattern ex-\ntraction algorithm. If the measure ﬁnding algorithm fails,\nthe chance of ﬁnding a coherent pattern is reduced, al-\nthough the clustering algorithm might be able to separate\nthepre-errorbarsfromthepost-errorbars. Theremainder 0 1/8 1/4 3/8 1/2 5/8 3/4 7/8  1 00.010.020.030.040.050.060.07Bar by bar energy patterns for track 19: Cha ChaFigure 2. The amplitude envelope of the ﬁfteen bars of\nexcerpt 19 are shown, with the colours representing the\nfour clusters. The thick black line is the centre of the\nlargest cluster, that is, the rhythmic pattern which is ex-\ntracted for this excerpt. This pattern is somewhat typical\noftheChaCha. Thelabelsonthex-axis(showingmusical\nunits) were added for illustrative purposes, and were not\nknown to the system.\nof this section gives examples of extracted rhythmic pat-\nternswhichhavefeaturestypicalofthegenrestheyrepre-\nsent.\nFigure 3 shows another Cha Cha piece which has a\nrhythmic pattern very similar to the one shown in Fig-\nure 2. By thresholding below the level of the highest 5\npeaks,weagainobtaintheprototypicalChaCharhythmic\npattern shown in the upper part of Figure 1.\nMusic for Jive and Quickstep is usually characterised\nbyswingeighthnotes. Thatis,eachquarternoteisbroken\ninto an unequal pair of “eighth” notes, where the ﬁrst is\nlonger than the second. The ratio of the lengths of the\ntwo notes is known as the swing ratio , which is typically\naround 2:1. Figure 4 shows an extracted pattern where a\nswing ratio around 2:1 is clearly visible.\nOne of the characteristics of Rumba is the use of syn-\ncopationinthepercussioninstruments. Accentsonthe4th\nand 6th eighth notes are typical, and this is seen in many\nof the extracted patterns, such as in Figure 5. This pattern\nis similar (but not identical) to the rhythm shown in the\nlower part of Figure 1.\nFinally,thetwoWaltzpatternsinFigure6clearlyshow\na triple metre, distinguishing these pieces from the other\npatternswhichhaveaquadrupleorduplemeter. However,\nwealsonotethatthesetwopatternsarequitedissimilar,in\nthattheupperonehaspeaksforeachquarternote,whereas\nthe lower pattern has peaks for each eighth note. It is also\nnoticeableinFigure6thatthereismuchgreatervariability\nbetweenthebarsofeachpiece. Thelackofprominentper-\ncussion instruments makes the amplitude peaks less pro-\nnounced, making bar ﬁnding and pattern extraction less\nreliable. Asaresult,anumberoftheWaltzpatternsfailed\n 0 1/8 1/4 3/8 1/2 5/8 3/4 7/8  1 00.010.020.030.040.050.060.070.08Bar by bar energy patterns for  track 12: Cha ChaFigure 3. Another Cha Cha piece, which has a pattern\nvery similar to the piece in Figure 2.\n 0 1/8 1/4 3/8 1/2 5/8 3/4 7/8  1 00.020.040.060.080.10.12Bar by bar energy patterns for track 151: Jive\nFigure 4. A Jive pattern showing a swing eighth note\nrhythm.\nto show any regularity at all.\n4. GENRE CLASSIFICATION EXPERIMENTS\nThe relevance of the discovered patterns was evaluated\nin several genre (dance style) classiﬁcation experiments.\nVarioussupervisedlearningalgorithmsanddatarepresen-\ntations (see below) were compared empirically. Classiﬁ-\ncationaccuracywasestimatedviaastandard10-foldcross-\nvalidation procedure: in each experiment, the training ex-\nampleswererandomlysplitinto10disjointsubsets(folds),\n9 of these folds were combined into a training set from\nwhichaclassiﬁerwasinduced,andtheclassiﬁerwasthen\ntested on the remaining tenth fold; this was repeated 10\ntimes, with each fold servingas test set exactly once.\nClassiﬁcation was performed with the software Weka\n(www.cs.waikato.ac.nz/ml/weka )[25],usingthe\nfollowing classiﬁcation algorithms. The simplest method\nusedwasthek-NearestNeighbours(k-NN)algorithm. For 0 1/8 1/4 3/8 1/2 5/8 3/4 7/8  1 00.020.040.060.080.10.120.14Bar by bar energy patterns for track 266: RumbaFigure 5. A Rumba pattern showing a strong emphasis\non the 4th and 6th eighth notes. (Note that the ﬁrst eighth\nnote is at 0.)\n 0 1/8 1/4 3/8 2/4 5/8  1 00.0050.010.0150.020.0250.030.0350.040.0450.05Bar by bar energy patterns for track 628: Waltz\n 0 1/8 1/4 3/8 2/4 5/8  1 00.0050.010.0150.020.0250.030.0350.040.0450.05Bar by bar energy patterns for track 657: Waltz\nFigure 6 . Two Waltz patterns: one in quarter notes\n(above), and one in eighth notes(below).Resolution\nRepresentation 7296120144\nRMS ( k= 1)46.4%45.7%48.1%45.1%\nRMS ( k= 2)47.4%46.0%47.1%46.1%\nABS 43.8%46.1%45.8%46.8%\nSQR 44.7%44.7%50.1%45.1%\nTable 2. Genre classiﬁcation results using the rhythmic\npatterns alone as feature vectors. The rows are different\npattern representations, and the columns are the number\nof points used to represent thepatterns.\nk= 1this amounts to assigning each test set instance to\nthe class of the nearest element in the training set. For\nk >1,theknearestinstancesinthetrainingsetarefound,\nand the greatest number of these neighbours which be-\nlong to the same class determines the class of the test in-\nstance. Various values of kwere used in the experiments.\nThe standard decision tree learning algorithm, J48, was\nalso used, as well as two meta-learning algorithms, Ada-\nBoost and Classiﬁcation via Regression. AdaBoost [9]\nruns a given weak learner (in this case J48) several times\non slightly altered (reweighted) training data and com-\nbines their predictions when classifying new cases. Clas-\nsiﬁcation via regression (using M5P and linear regression\nasbaseclassiﬁers)buildsaregressionmodelforeachclass\nand combines the models viavoting.\n4.1. Classiﬁcation by Rhythmic Pattern Alone\nThe ﬁrst set of classiﬁcation experiments was performed\nwith p(n)asthefeaturevectorforeachexcerpt,thatis,us-\ningtherhythmicpatternaloneforclassiﬁcation. Notethat\nthis representation is totally independent of tempo. The\nclassiﬁcation rates for various pattern representations de-\nscribed in subsection 3.2 are shown in Table 2. The best\nclassiﬁcationrate,50%,wasachievedusingtheAdaBoost\nclassiﬁer, with the decimated squared signal representa-\ntion with b= 120. This is well above the baseline for\nclassiﬁcation of this data, which is 16%.\nTheconfusionmatrixforthisclassiﬁerisshowninTa-\nble 3. Viennese Waltzes were the most poorly classiﬁed,\nwhile classiﬁcation of Cha Cha pieces was the most ac-\ncurate. The greatest mutual confusion was between the\nWaltz and Viennese Waltz, which is to be expected, since\nthey have the same metre and often use the same rhyth-\nmic patterns and instruments, and the clearly distinguish-\ningfeature,thetempo,isnotencodedintherhythmicpat-\ntern.\n4.2. Calculation of Derived Features\nTherhythmicpatternsthemselvesdonotcontaininforma-\ntion about their time span, that is, they are independent\nof the tempo. Since the tempo is one the most impor-\ntantfeaturesindeterminingdancegenre[7,12],wetested\nclassiﬁcationwithacombinationoftherhythmicpatterns,CJQRSTVWRec%\nC746014770367\nJ10231111101338\nQ0113526951443\nR2003531321654\nS809114353750\nT1587653501041\nV026301233035\nW109647196458\nPrec%5846445564464344\nTable 3. Confusion matrix for classiﬁcation based on\nrhythmicpatternsalone. Therowsrefertotheactualstyle,\nand the columns the predicted style. The rightmost col-\numn shows the percentage recall for each class and the\nbottom row shows the percentage precision. The abbre-\nviations for the dance styles are: Cha Cha (C), Jive (J),\nQuickstep (Q), Rumba (R), Samba (S), Tango (T), Vien-\nnese Waltz (V), Waltz (W).\nfeatures derived from the rhythmic patterns, features de-\nrived from the audio data directly, and the tempo.\nThe features derived from the rhythmic patterns were:\nthe mean amplitude of the pattern, the maximum ampli-\ntude of the pattern, the relative maximum amplitude of\nthe pattern (maximum divided by mean), the standard de-\nviationofthepatternamplitudes,anestimateofthemetre,\nasyncopationfactor,andtheswingfactor. Themetrewas\nestimatedbycalculatingtwoweightedsumsofthepattern,\nthe ﬁrst with higher weights around the positions of a di-\nvision of the bar into 4 quarter notes (8 eighth notes), the\nsecond with the weights set for a division of the bar into\n3 quarter notes (6 eighth notes). The greater of the two\nsums determined the metre as a binary attribute, indicat-\ning either a quadruple or ternary metre. The syncopation\nfactorwascalculatedastherelativeweightsoftheoffbeat\neighth notes (i.e. the 2nd, 4th, etc.) to the on-beat eighth\nnotes. Theswingfactorwascalculatedusingapulsetrain\nof Gaussian curves spaced at quarter note intervals, cor-\nrelating with the signal and ﬁnding the highest 2 peaks,\nwhich usually correspond to the positions of the quarter\nnote and eighth note respectively. If the duration of the\nquarternoteis qandtheintervalbetweenthetwopeaksis\nr, then the swing factor sis given by:\ns= max/parenleftbiggr\nq−r,q−r\nr/parenrightbigg\n(9)\nIf only one peak in the correlation was found, the swing\nfactor was set to 0.\nAn additional feature set, containing three groups of\ndescriptors (as described by Gouyon et al. [12]) was also\nused. Theﬁrstgroupofdescriptorswastempo-relatedfea-\ntures, including the measured tempo calculated from the\nbar length. The second group consisted of features de-\nrived from the periodicity histogram representation, and\nthe third group of features were derived from inter-onset\ninterval histograms. Apart from the measured tempo, allCJQRSTVWRec%\nC102105030092\nJ05800020097\nQ00800002098\nR11092040094\nS00208201195\nT10020830097\nV000000650100\nW000110010898\nPrec% 9897989299909699\nTable 4. Confusion matrix for classiﬁcation using rhyth-\nmic patterns and other features. Compare with Table 3.\nofthesevalueswerecalculatedautomatically(seeTable5\nand [12] for more details).\n4.3. Classiﬁcation Using AllFeatures\nIn the following classiﬁcation experiments using all fea-\ntures, a classiﬁcation rate of 96% was achieved with the\nAdaBoostclassiﬁer,usingtheRMSsignalwith k= 2and\nb= 72. This is remarkable, considering that spectral fea-\nturesarenotrepresentedatallinthedata,andthereiscer-\ntainly some ambiguity in the relationship between music\npieces and dance styles. The confusion matrix is shown\nin Table 4. More than half (16 out of 28) of the errors are\ncausedbyconfusionofChaCha,TangoandRumba. From\nTable1,weseethatthesestyleshavestronglyoverlapping\ntempo ranges and the same metre, so other features must\nbe used to distinguish these classes.\nComparisons of classiﬁcation rates with various sub-\nsets of features were performed to determine the relative\ncontribution of each subset (see Table 5). The left hand\ncolumn shows the results from Gouyon et al. [12]; classi-\nﬁcationusingtempoaloneachievedupto82%,classiﬁca-\ntionusingotherfeaturesnotincludingtempoalsoreached\n82%,andbycombiningthesefeatures,aclassiﬁcationrate\nof 93% was obtained. The right hand column shows the\nresults of adding rhythmic patterns and their derived fea-\ntures to the feature vectors: in each case an improvement\nwas made, with overall classiﬁcation rates improving to\n84% (compared with 82%) without the tempo and 96%\n(compared with 93%) including the tempo. For all of\nthese results, the rhythmic patterns were generated with\nthe RMS ﬁlter with k= 2andb= 72, and the AdaBoost\nlearning algorithm was used (hence the difference from\npublished results in [12]).\n5. DISCUSSION AND FURTHER WORK\nItisnottobeexpectedthatasinglerhythmicpatterncould\nuniquely determine the genre of a piece of dance mu-\nsic. Many other features which are not represented in this\nworkarealsorelevanttogenre,suchasthechoiceofmusi-\ncal instruments, which could perhaps be represented with\nstandard timbral features such as MFCCs. ExaminationFeature sets from [12] Without RP With RP\nNone (0) 15.9% 50.1%\nPeriodicity histograms (11) 59.9% 68.1%\nIOI histograms (64) 80.8% 83.4%\nPeriodicity & IOI hist. (75) 82.2% 85.7%\nTempo attributes (3) 84.4% 87.1%\nAll (plus bar length) (79) 95.1% 96.0%\nTable 5. Comparison of classiﬁcation rates using various\nsetsoffeatures. Thecolumnsshowrateswithoutandwith\nthe rhythmic patterns (RP) and their derived features; the\nrowsshowthedifferentfeaturesubsetsfromGouyonetal.\n[12], with the number of features shown in parentheses.\nof the extracted patterns shows that some of the patterns\narequitetrivial,suchasthosewhichshowsharppeakson\neach of the quarter note beats, thus only serving to dis-\ntinguish triple from quadruple metre. Nevertheless, even\nwith these limitations, the results demonstrate that rhyth-\nmic patterns are a usefulfeature for classiﬁcation.\nThe fact that only 30 seconds of each song was used\nmay have adversely inﬂuenced the results, as many songs\nhaveanintroductionwhichdoesnotmatchthestyleofthe\nrest of the piece. Because of the shortness of the tracks, it\nwasconsideredbettertoextractonlyonerhythmicpattern.\nWith longer tracks it would be worthwhile to investigate\nclassiﬁcation using multiple patterns per song. It is also\nexpectedthatthestatisticalreliabilityofpatternextraction\nwould increase with the length of the excerpt.\nOne restriction of the current work is that it relies on\nan accurate estimate of the ﬁrst bar. Automatic methods\nofﬁndingmetricalboundarieshavemadegreatprogressin\nrecent years, but they are still far from perfect, and man-\nual correction for very large music databases is not feasi-\nble. However the errors of such systems are not random;\nthey belong to a very small class of possibilities: tempo\nerrors of a factor of 2 or 3, and phase errors of half (or\noccasionally a third or quarter) of the metrical unit. If we\nallow these cases, no longer considering them as errors,\nthe classiﬁcation algorithm could possibly succeed in im-\nplicitly recognising thesedifferent cases.\nAnother limitation is that although we do not explic-\nitly detect percussive onsets, the methodology assumes\npeaks in energy (e.g. for correlation) for extracting the\npatterns. This limitation is seen in the patterns extracted\nfrom Waltz and Viennese Waltz excerpts. An explicit on-\nsetdetectionstepwhichincludesthedetectionofsoft(i.e.\nnon-percussive) onsets [1] could be used to alleviate this\nproblem. Anotherapproachwouldbetousefeaturesother\nthan amplitude or energy. Paulus and Klapuri [18] found\nthat the spectral centroid, normalised by the energy, pro-\nvided the best feature vector for describing patterns.\nThe high dimensionality of the pattern vectors reduces\nthe ability of learning algorithms to build suitable clas-\nsiﬁers. Dimensionality reduction either by PCA or by a\nmoreexplicitsymbolicencoding(i.e. inmusicalsymbols)\nwould be a step in the direction of solving this problem.If the patterns were quantised and encoded into musical\nunits, they could be matched to explicit patterns such as\nthose found in instructional books. Even without such an\nencoding, matching in the other direction, i.e. from ex-\nplicit patterns to the audio data could be performed as a\nmethod of generating further features.\nA related issue that is yet to be explored is the choice\nof distance metrics between patterns. The Euclidean dis-\ntance is not necessarily ideal, as it treats all time points\nindependently, so that, for example, peaks which almost\nmatcharepenalisedasheavilyaspeakswhicharefarfrom\nbeing aligned.\nAnother avenue of further research would be to ex-\ntractpatternsinvariousfrequencybandsinordertodetect\nbetween-instrument patterns (e.g. bass drum, snare drum,\nhi-hat). Alternatively, recent work on drum sound recog-\nnition [13] could be used to determine multi-dimensional\nrhythmic patterns. These ideas would necessitate the de-\nvelopmentofmorecomplexmethodsofencodingandcom-\nparing patterns.\nTherearenumerousotherdirectionsofpossiblefurther\ndevelopment. The current experiments are limited in the\ngenres of music on which they have been performed. As\nother labelled data sets become available, it will be pos-\nsible to test the generality of this method of pattern ex-\ntractionand comparison forclassiﬁcationof othergenres.\nThe algorithms are general purpose; no domain speciﬁc\nknowledge is encoded in them. The unknown issue is the\nextenttowhichothergenresarecharacterisedbyrhythmic\npatterns.\n6. CONCLUSION\nWe described a novel method of characterising musical\npieces by extracting prominent bar-length rhythmic pat-\nterns. Thisrepresentationisasteptowardsbuildinghigher\nlevel, more musically relevant, parameters which can be\nusedforgenreclassiﬁcationandmusicretrievaltasks. We\ndemonstratedthestrengthoftherepresentationonagenre\nrecognition task, obtaining a classiﬁcation rate of 50%\nusing the patterns alone, 84% when used in conjunction\nwith various automatically calculated features, and 96%\nwhen the correct tempo was included in the feature set.\nThese classiﬁcation rates represent a signiﬁcant improve-\nment over previous work using the same data set [12],\nand higher rates than have been published on other data\nsets [22, 15, 7]. However, we acknowledge the prelimi-\nnary nature of these investigations in the quest to extract\nsemantic information from audio recordings of music.\n7. ACKNOWLEDGEMENTS\nThisworkwasfundedbytheEU-FP6-IST-507142project\nSIMAC(SemanticInteractionwithMusicAudioContents).\nThe Austrian Research Institute for Artiﬁcial Intelligence\nalso acknowledges the ﬁnancial support of the Austrian\nFederal Ministries of Education, Science and Culture and\nof Transport, Innovation and Technology.References\n[1] Bello, J. and Sandler, M. (2003). Phase-based note\nonset detection for musical signals. In International\nConference on Acoustics, Speech and Signal Process-\ning.\n[2] Berenzweig,A.andEllis,D.(2001). Locatingsinging\nvoicesegmentswithinmusicalsignals. In Proceedings\nof IEEE Workshop on Applications of Signal Process-\ning to Audio and Acoustics , pages 119–123, Mohonk,\nNY.\n[3] Cemgil, A., Kappen, B., Desain, P., and Honing, H.\n(2000). On tempo tracking: Tempogram representa-\ntion and Kalman ﬁltering. In Proceedings of the 2000\nInternationalComputerMusicConference ,pages352–\n355,SanFranciscoCA.InternationalComputerMusic\nAssociation.\n[4] Chen, J. and Chen, A. (1998). Query by rhythm: An\napproachforsongretrievalinmusicdatabases. In Pro-\nceedings of the 8th IEEE International Workshop on\nResearch Issues in Data Engineering , pages 139–146.\n[5] Dixon,S.(2001a). Automaticextractionoftempoand\nbeat from expressive performances. Journal of New\nMusic Research , 30(1):39–58.\n[6] Dixon, S. (2001b). An interactive beat tracking and\nvisualisation system. In Proceedings of the Interna-\ntional Computer Music Conference , pages 215–218,\nSan Francisco CA. International Computer Music As-\nsociation.\n[7] Dixon,S.,Pampalk,E.,andWidmer,G.(2003). Clas-\nsiﬁcation of dance music by periodicity patterns. In\n4thInternationalConferenceonMusicInformationRe-\ntrieval (ISMIR 2003) , pages 159–165.\n[8] Foote,J.(1997). Content-basedretrievalofmusicand\naudio.InMultimediaStorageandArchivingSystemsII ,\npages 138–147.\n[9] Freund,Y.andSchapire,R.(1996). Experimentswith\na new boosting algorithm. In Proceedings of the Thir-\nteenthInternationalConferenceonMachineLearning ,\npages 148–156.\n[10] Goto, M. and Muraoka, Y. (1995). A real-time beat\ntrackingsystemforaudiosignals.In Proceedingsofthe\nInternationalComputerMusicConference ,pages171–\n174,SanFranciscoCA.InternationalComputerMusic\nAssociation.\n[11] Goto, M. and Muraoka, Y. (1999). Real-time beat\ntrackingfordrumlessaudiosignals. SpeechCommuni-\ncation, 27(3–4):311–335.\n[12] Gouyon, F., Dixon, S., Pampalk, E., and Widmer,\nG.(2004). Evaluatingrhythmicdescriptorsformusical\ngenre classiﬁcation. In Proceedings of the AES 25th\nInternational Conference , pages 196–204.[13] Herrera, P., Dehamel, A., and Gouyon, F. (2003a).\nAutomaticlabelingofunpitchedpercussionsounds. In\nPresented at the 114th Convention of the Audio Engi-\nneering Society , Amsterdam, Netherlands.\n[14] Herrera, P., Peeters, G., and Dubnov, S. (2003b).\nAutomaticclassiﬁcationofmusicalinstrumentsounds.\nJournal of New Music Research , 32(1):3–22.\n[15] McKinney, M. and Breebaart, J. (2003). Features\nfor audio and music classiﬁcation. In 4th Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR 2003) , pages 151–158.\n[16] Pampalk, E. (2001). Islands of music: Analysis, or-\nganization, and visualization of music archives. Mas-\nter’s thesis, Vienna University of Technology, Depart-\nmentofSoftwareTechnologyandInteractiveSystems.\n[17] Pampalk,E.,Dixon,S.,andWidmer,G.(2004). Ex-\nploring music collections by browsing different views.\nComputer Music Journal , 28(2):49–62.\n[18] Paulus, J. and Klapuri, A. (2002). Measuring the\nsimilarity of rhythmic patterns. In Proceedings of the\n3rd International Conference on Musical Information\nRetrieval, pages 150–156. IRCAM Centre Pompidou.\n[19] Saunders, J. (1996). Real time discrimination of\nbroadcast speech/music. In Proceedings of the Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing , pages 993–996.\n[20] Scheirer, E. (1998). Tempo and beat analysis of\nacousticmusicalsignals. JournaloftheAcousticalSo-\nciety of America , 103(1):588–601.\n[21] Scheirer, E. (2000). Music-Listening Systems . PhD\nthesis, Massachusetts Institute of Technology, School\nof Architecture and Planning.\n[22] Tzanetakis, G. and Cook, P. (2002). Musical genre\nclassiﬁcation of audio signals. IEEE Transactions on\nSpeech and Audio Processing , 10(5):293–302.\n[23] Tzanetakis, G., Essl, G., and Cook, P. (2001). Au-\ntomatic musical genre classiﬁcation of audio signals.\nInInternational Symposium on Music Information Re-\ntrieval.\n[24] Vos, J. and Rasch, R. (1981). The perceptual on-\nset of musical tones. Perception and Psychophysics ,\n29(4):323–335.\n[25] Witten, I. and Frank, E. (1999). Data Mining:\nPracticalMachineLearningToolsandTechniqueswith\nJava Implementations . Morgan Kaufmann, San Fran-\ncisco, CA.\n[26] Wold, E., Blum, T., Keislar, D., and Wheaton, J.\n(1996). Content-based classiﬁcation, search, and re-\ntrieval of audio. IEEE Multimedia , 3(2):7–36."
    },
    {
        "title": "Stochastic Model of a Robust Audio Fingerprinting System.",
        "author": [
            "Peter Jan O. Doets",
            "Reginald L. Lagendijk"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416490",
        "url": "https://doi.org/10.5281/zenodo.1416490",
        "ee": "https://zenodo.org/records/1416490/files/DoetsL04.pdf",
        "abstract": "An audio fingerprint is a compact representation of the perceptually relevant parts of audio content. A suitable audio fingerprint can be used to identify audio files, even if they are severely degraded due to compression or other types of signal processing operations. When degraded, the fingerprint closely resembles the fingerprint of the origi- nal, but is not identical. We plan to use a fingerprint not only to identify the song but also to assess the perceptual quality of the compressed content. In order to develop such a fingerprinting scheme, a model is needed to assess the behavior of a fingerprint subject to compression. In this paper we present the initial outlines of a model for an existing robust fingerprinting system to develop a more theoretical foundation. The model describes the stochastic behavior of the system when the input signal is a station- ary (stochastic) signal. In this paper the input is assumed to be white noise. Initial theoretical results are reported and validated with experimental data.",
        "zenodo_id": 1416490,
        "dblp_key": "conf/ismir/DoetsL04",
        "keywords": [
            "audio fingerprint",
            "perceptually relevant parts",
            "audio content",
            "identifying audio files",
            "degraded audio",
            "compression",
            "signal processing",
            "fingerprinting scheme",
            "perceptual quality",
            "stochastic behavior"
        ],
        "content": "STOCHASTIC MODELOFAROBUSTAUDIOFINGERPRINTING\nSYSTEM\nP.J.O.Doets,R.L.Lagendijk\nDept.ofMediamatics, Information andCommunication Theorygroup,\nDelftUniversityofTechnology ,P.O.Box5031,2600GADelft\nfp.j.doets, r.l.lagendijk g@ewi.tudelft.nl\nABSTRACT\nAnaudio ﬁngerprint isacompact representation ofthe\nperceptually relevantparts ofaudio content. Asuitable\naudio ﬁngerprint canbeused toidentify audio ﬁles, even\niftheyareseverely degraded duetocompression orother\ntypes ofsignal processing operations. When degraded, the\nﬁngerprint closely resembles theﬁngerprint oftheorigi-\nnal,butisnotidentical. Weplan touseaﬁngerprint not\nonly toidentify thesong butalso toassess theperceptual\nquality ofthecompressed content. Inorder todevelop\nsuch aﬁngerprinting scheme, amodel isneeded toassess\nthebehavior ofaﬁngerprint subject tocompression. In\nthispaper wepresent theinitial outlines ofamodel for\nanexisting robustﬁngerprinting system todevelop amore\ntheoretical foundation. Themodel describes thestochastic\nbehavior ofthesystem when theinput signal isastation-\nary(stochastic) signal. Inthispaper theinput isassumed\ntobewhite noise. Initial theoretical results arereported\nandvalidated with experimental data.\n1.INTRODUCTION\nDownloading music isverypopular .Music identiﬁcation\nisusually done bysearching inthemetadata describing the\nmusic content. Metadata likesong title, artist, etc., how-\never,isoften incoherent ormisleading, especially onPeer-\nto-Peer (P2P) ﬁle-sharing netw orks, calling forcontent-\nbased identiﬁcation. Identiﬁcation, however,isoften not\nenough. Theperceptual quality ofasong compressed us-\ningMP3 at32kpbs istotally different from thepercep-\ntualquality oftheCD-recording ofthesame song. There-\nfore, acontent-based indication fortheperceptual quality\nisneeded. TheMusic2Share project proposes touseaudio\nﬁngerprints forboth identiﬁcation andquality assessment\nofunkno wncontent onaP2Pnetw ork[1].\nAudio ﬁngerprints arecompact representations ofthe\nperceptually relevantparts ofaudio content that canbe\nused toidentify music based onthecontent instead ofthe\nPermission tomakedigitalorhardcopiesofallorpartofthisworkfor\npersonalorclassroom useisgrantedwithoutfeeprovidedthatcopies\narenotmadeordistributedforpro®torcommercial advantageandthat\ncopiesbearthisnoticeandthefullcitationonthe®rstpage.\nc\r2004UniversitatPompeuFabra.metadata. Aﬁngerprinting system consists oftwoparts:\nﬁngerprint extraction andamatching algorithm. Theﬁn-\ngerprints ofalargenumber ofsongs areusually stored ina\ndatabase. Asong isidentiﬁed bycomparing itsﬁngerprint\nwith theﬁngerprints inthedatabase. Well-kno wnappli-\ncations ofaudio ﬁngerprinting arebroadcast monitoring,\nconnected audio, andﬁltering forﬁlesharing applications\n[2],[3].Inthiscontrib ution wefocus only ontheﬁnger -\nprint extraction.\nForawell-designed ﬁngerprint itholds thatﬁngerprints\noftwoarbitrary selected pieces ofmusic areverydiffer-\nent, while ﬁngerprints originating from thesame audio\nﬁle,encoded using different coding schemes orbitrates,\nareonly slightly different. Weaimtoexploit these small\nﬁngerprint differences duetocompression toassess the\nperceptual quality ofthecompressed audio ﬁle.\nBymodelling asuccessful existing audio ﬁngerprint-\ningscheme, weaimtogainmore insight inthebehavior\nofﬁngerprint differences when theaudio iscompressed.\nTheﬁngerprint generation ismodelled when thesystem is\nsubject toaninput with knownspectrum. Once thisisbet-\nterunderstood, adifferential model which givesanindi-\ncation oftheperceptual quality ofthecompressed version\nwith respect totheoriginal willbemade.\nSeveralaudio ﬁngerprinting methods exist, using e.g.\nspectral ﬂatness features [4]orFourier coefﬁcients [2],[5]\nand[6].Agood overvie wcanbefound in[3].Wechoose\ntomodel thePhilips audio ﬁngerprinting system [2]be-\ncause itiswell documented, highly robustagainst com-\npression anditcanbemodelled using stochastic models.\nThis paper consists ofﬁvesections: Section 2presents\ndetails oftheaudio ﬁngerprinting system tobemodelled,\nSection 3analyzes howtomodel thefunctional blocks of\ntheﬁngerprinting system, Section 4describes initial re-\nsults fortheﬁngerprint ofwhite noise, while Section 5\ndrawsconclusions andoutlines future work.\n2.DETAILSOFTHEEXISTING SYSTEM\nFigure 1showsanovervie woftheﬁngerprint extraction,\nwhich isfunctionally equivalent tothePhilips ﬁngerprint-\ningsystem [2];some blocks, however,havebeen reshuf-\nﬂedtofacilitate themodelling process described inthefol-\nlowing sections. Theaudio signal isﬁrstsegmented into\nframes of0.37 seconds with anoverlap factor of31/32,Figure1.Functional equivalent ofthePhilips audio ﬁn-\ngerprinting system [2].\nweighted byaHanning windo w.Thecompact represen-\ntation ofasingle audio frame iscalled asub-ﬁngerprint.\nInthisway,itextracts 32-bit sub-ﬁngerprints foreveryin-\ntervalof11.6 ms. Due tothelargeoverlap, subsequent\nsub-ﬁngerprints havealargesimilarity andslowlyvaryin\ntime. Asong consists ofasequence ofsub-ﬁngerprints,\nwhich arestored inadatabase. Thesystem iscapable of\nidentifying asegment ofabout 3seconds ofmusic -gener -\nating 256sub-ﬁngerprints -inalargedatabase, evenifthe\nsegment isdegraded duetosignal processing operations.\nToextract a32-bit sub-ﬁngerprint foreveryframe, 33\nnon-o verlapping frequenc ybands areselected from thees-\ntimated PowerSpectral Density (PSD). These bands range\nfrom 300Hzto2000 Hzandarelogarithmically spaced,\nmatching theproperties oftheHuman Auditory System\n(HAS). Haitsma andKalk erreport thatexperiments have\nshownthatthesign ofenergydifferences isaproperty that\nisveryrobusttomanykinds ofprocessing [2].\nUsing thenotation of[2],wedenote theenergyoffre-\nquenc ybandmofframe nbyE(n;m).Differences in\nthese energiesarecomputed intime andfrequenc y:\nED(n;m)=E(n;m)\u0000E(n;m+1) (1)\n\u0000(E(n\u00001;m)\u0000E(n\u00001;m+1)):\nThebitsofthesub-ﬁngerprint arederivedby\nF(n;m)=(\n1ED(n;m)>0\n0ED(n;m)\u00140; (2)\nwhere F(n;m)denotes themthbitofthesub-ﬁngerprint\nofframe n.\nFigure 2(a) showsanexample ofaﬁngerprint. White\nparts indicate positi veenergydifferences, black parts in-\ndicate negativeenergydifferences. Thesmall side ofthe\nﬁngerprint block isthefrequenc ydirection, consisting of\nthe32bitscorresponding tothedifferences between the\n33frequenc ybands. The long side oftheblock corre-\nsponds tothetemporal dimension.\nThebiterror ratebetween theextracted ﬁngerprint and\ntheﬁngerprint inthedatabase isused asthesimilarity\nmeasure. When thesong issubject tosignal degradations\nsuch ascompression, theﬁngerprint changes slightly .To\nindicate theeffectofMP3 compression ontheﬁngerprint\nextraction, Figure 2showsthedifference pattern ofthe\nﬁngerprint ofarecording atdifferent bit-rates relati veto\n(a)\n (b)\n (c)\n (d)\n (e)\nFigure2.Fingerprints foranexcerpt of’Anarch yinthe\nU.K.’bytheSexPistols. Fingerprints of(a)theoriginal\nand(b)ofanMP3 compressed version encoded at128\nkbps; (c-e) Differences between theﬁngerprints ofthe\noriginal andanMP3 compressed version encoded at(c)\n128kpbs (d)80kpbs and(e)32kbps. Theblack positions\nmark thedifferences.\ntheﬁngerprint oftheCD-quality recording ofthesame\nsong. Theblack sections mark theﬁngerprint differences\nduetocompression, white positions indicate similarity be-\ntween theﬁngerprints. Generally speaking, compression\natalowerbitrateincreases thenumber ofﬁngerprint bit-\nerrors. Thebit-error pattern alsoshowsasigniﬁcant num-\nberofbursts oferrors.\n3.PROPOSED MODEL\nWeanalyze thebehavior ofthesystem when itissubject\ntotheinput ofawell-understood, stationary signal such as\nwhite noise orapthorder auto-re gressi ve(AR-p)process.\nFrom thisanalysis wewillderiveaﬁngerprint model.\nAlthough forthese kinds ofsignals thePSD iswell-\nknown,weneed tokeeptwothings inmind. First, the\nPSD isestimated using theperiodogram, which hascer-\ntainstatistical properties. Second, wehavetotakeintoac-\ncount thestrong overlap between thesubsequent frames,\nwhich causes theslowtime-v ariation oftheﬁngerprint.\nTherefore, eventheﬁngerprint ofawhite noise process is\nnon-white. Infact,white noise ﬁngerprints look similar to\ntheﬁngerprint showninFigure 2(a).\nTheoutcome ofourproposed model istheprobability\nthataﬁngerprint bitisequal toone, giventheprevious\nﬁngerprint bitcorresponding tothesame frequenc yposi-\ntionm,theframe length L,frame overlap length L\u0000\u0001L,\nthenumber offrequenc ybands Nandtheautocorrelation\nfunction oftheinput signal R(l):\nP\u0010\nF(n;m)=1\f\f\fF(n\u00001;m);L;\u0001L;N;R(l)\u0011\n(3)\nOther parameters, such asfrequenc yrange andsampling\nfrequenc yarekeptsimilar totheoriginal system [2](in\nthissystem L=0:37seconds and\u0001L=11:6ms).Themodelling procedure leading toequation (3)follows\nthesteps showinFigure 1.Ignoring the(Hanning) win-\ndowfunction, theperiodogram estimator ofthePSD of\nframe nisgivenby:\nS(n;k)=1\nL\f\f\f\f\fL\u00001X\ni=0x\u0000\ni+n\u0001L\u0001\ne\u0000j2\u0019k\nLi\f\f\f\f\f2\n;(4)\nk=0;:::;L\u00001\nThe energydifference between twosuccessi ve(overlap-\nping) frames perspectral sample isgivenby:\nEDs(n;k)=S(n;k)\u0000S(n\u00001;k) (5)\nThedifferential energywithin asingle frequenc ybandm\nisthen obtained bysumming thesamples belonging tothat\nfrequenc yband:\nE(n;m)\u0000E(n\u00001;m)=X\nk2BmEDs(n;k); (6)\nwhere Bmindicates thesetofsamples belonging tofre-\nquenc ybandm.FromEDs(n;k)theenergydifferences\nbetween frequenc ybands canbecomputed by:\nED(n;m)=X\nk2BmEDs(n;k)\u0000X\nk2Bm+1EDs(n;k)(7)\nTheﬁngerprint bitsarederivedaccording toEquation (2).\nThe statistical properties offour steps oftheﬁngerprint\ngeneration expressed byequations (4)-(7), canbedescribed\ninterms ofProbability Density Functions (PDFs), starting\nwith thePDF describing thestatistical behaviorofindivid-\nualsamples within theperiodogram estimate ofthePSD:\nfS(n;k)(y): (8)\nWemay usethistoderiveaPDF forsamples inEDs(n;k):\nfEDs(n;k)(y): (9)\nFrom thiswecanextend theanalysis tothestatistical be-\nhaviorofsamples fromsuccessive energydifference spec-\ntra.This analysis leads toaPDF oftheform\nfEDs(n;k);EDs(n+1;k)(y;z): (10)\nIntroducing frequenc ybands leads tothefollowing PDF:\nfED(n;m);ED(n+1;m)(y;z): (11)\nWecaneasily convertthisPDF intoprobabilities forthe\nﬁngerprint bitsintwosuccessi vesub-ﬁngerprints, e.g.:\nP[F(n;m)=1;F(n+1;m)=1] (12)\n=Z1\n0Z1\n0fED(n;m);ED(n+1;m)(y;z)dydz:\nEquation (12) yields themodel outlined inEquation (3).0 0.2 0.4 0.6 0.8 105101520253035\nΔL / LLaplace variance for σX = 2Rectangular (Analytical variance)\nRectangular (Experimental variance)\nHannning     (Experimental variance)\nFigure3.Laplace variance asfunction oftheratio\u0001L=L,\nboth forrectangular windo wsandforHanning windo ws.\n4.WHITENOISEFINGERPRINT\nThesection describes thestatistical behavioroftheﬁnger -\nprint generation ofawhite noise input signal interms of\nthePDFs andprobabilities ofEquations (8)-(12). Theﬁn-\ngerprint generation process ofwhite noise isworth mod-\nelling forthree reasons. First, thespectrum ofanAR-\npprocess istheﬁltered version ofcorresponding white\nnoise process. Second, differences intheaudio spectrum\nduetocompression might bemodelled as(ﬁltered) noise.\nThird, ﬁngerprint generation proces ofwhite noise isthe\nmost simple example thatstillprovides insight andshows\nthegenerally applicable ﬁngerprint behavior.\nThis section isorganized asfollows:Section 4.1gives\nanexpression forthemarginal PDFfEDs(n;k)(y);Section\n4.2investig atesthejoint-PDF forfrequenc yband energies\nissuccessi veframes, fED(n;m);ED(n+1;m)(y;z).Itshows\nthatthisdistrib ution canbeapproximated well byaGaus-\nsianPDF,resulting intheP[F(n;m);F(n+1;m)].Rect-\nangular windo wsareassumed unless stated otherwise.\n4.1.MarginalDistribution\nLeon-Garcia showsthatwhen theinput signal x(i)iswhite\nnoise, (x(i)thus isazero-mean, Gaussian random vari-\nable (RV)with variance \u001b2\nX),thesamples oftheperi-\nodogram estimate ofthePSD using rectangular windo ws\nareuncorrelated andfollowanexponential distrib ution:\nfS(n;k)(y)=1\n\u001b2\nXe\u0000y=\u001b2\nX;y\u00150: (13)\nUsing thisresult andtwoproperties oftheLaplace distri-\nbution [8],itcanbeshownthatthesamples ofEDs(n;k)\nfollowaLaplace PDF [9]. Interms oftheparameters in\nSection 3:\nfEDs(n;k)(yjL;\u0001L;R(l)=\u001b2\nX\u000e(l))=1\n2se\u0000jyj=s(14)\nwith thevariance oftheLaplace distrib ution, 2s2:\nVAR[EDs(n;k)]=2s2=2\u001b4\nX(2L\u0000\u0001L)\u0001L\nL2(15)0 0 . 2 0 . 4 0 . 6 0 . 8 10 . 1 50 . 20 . 2 5\nD L  /  LP [  F P ( n , m ) = 1 ,  F P ( n + 1 , m ) = 1  ]\n0 0 . 2 0 . 4 0 . 6 0 . 8 10 . 1 50 . 20 . 2 5\nD L  /  LP [  F P ( n , m ) = 1 ,  F P ( n + 1 , m ) = 1  ]\nFigure4.Analytical (dashed) andexperimental (solid)\nprobability P[F(n;m)=1;F(n+1;m)=1]forwhite\nnoise input using windo ws.\nEquations (14) and(15) havebeen veriﬁed experimen-\ntally.Figure 3showsthevariance asafunction ofthe\nratio\u0001L\nL.Thesame graph also showsthebehavior ofthe\nvariance when Hanning windo wsareused. Thebehavior\nisslightly different, thederivation of(and theexpression\nfor)thevariance much more comple x[9].\n4.2.BivariateDistribution:Gaussian approximation\nDeriving ananalytical expression forthebivariate PDF\nfEDs(n;k);EDs(n+1;k)(y;z)isverydifﬁcult andnotnec-\nessary .Experiments conﬁrm thesuitability oftheGaus-\nsian approximation offED(n;m);ED(n+1;m)(y;z).This\napproximate PDF isthen fully deﬁned bythemarginal\nvariances \u001b2\nED(n;m)=\u001b2\nED(n+1;m)andthecorrelation\ncoefﬁcient \u001a.Inspection ofthesystem learns thatthecor-\nrelation between ED(n;m)andED(n+1;m)canbe\nexpressed interms ofvariances:\n\u001a=1\n2VAR[ED(n;m)j2\u0001L]\nVAR[ED(n;m)j\u0001L]\u00001 (16)\nSo,expressing thevariance ofED(n;m)interms of\u0001L\nL\nfully deﬁnes theGaussian approximation. Tocompute\nthisvariance, however,wealso havetotakethecorrela-\ntionbetween thesamples ofEDs(n;k)intoaccount. Full\nderivations andexpressions forthiscorrelation function\nandtheresulting variance canbefound in[9].\nNowwecancompute themarginal variances andthe\ncorrelation coefﬁcient oftheGaussian approximation, the\nprobabilities P[F(n;m);F(n+1;m)]canbecomputed\naccording toEquation (12). Figure 4showsthecomputed\nprobabilities P[F(n;m)=1;F(n+1;m)=1]along\nwith itsexperimental counterparts. This clearly showsthe\nsuitability oftheGaussian approximation forourpurpose.\n5.CONCLUSION ANDFUTURE WORK\nThis paper presents initial outlines andresults forastochas-\nticmodel ofastate-of-the-art audio ﬁngerprinting system.\nThelonger -term aimofthemodelling istoassess theper-\nceptual quality ofacompressed song with respect tothe\noriginal high quality recording byusing itsﬁngerprint.Amodel fortheﬁngerprint generation ofwhite noise\ninput signals ispresented. Itsrelevance liesinitspossi-\nbleextension toAR-psignals, thepossible modelling of\ncompression effects byadditi ve(ﬁltered) noise anditsrel-\nativesimplicity .Itprovides analytical expressions andap-\nproximations forthestatistics oftheindividual steps ofthe\nﬁngerprint generation process, resulting inﬁngerprint-bit\nprobabilities assuming rectangular windo ws.\nFuture workconcerning thewhite noise model willbe\ntheextension toHanning windo wsandtheincorporation\nofthecorrelation ofﬁngerprint-bits inthespectral dimen-\nsion. Future workconcerning theoverall model willbethe\nextension toAR-pinput signals andthemodelling ofﬁn-\ngerprint differences when theinput signal iscompressed.\n6.REFERENCES\n[1]Kalk er,T.,Epema, D.H.J., Hartel, P.H., La-\ngendijk, R.L. andvanSteen, M.“Music2Share\n-Copyright-Compliant Music Sharing inP2P\nSystems”, vol.92,no.6,pp.961- 970,Pro-\nceedingsoftheIEEE ,June 2004.\n[2]Haitsma, J.andKalk er,T.“AHighly Robust\nAudio Fingerprinting System”, pp.144-148,\nProc.ofthe3rdInt.Symposium onMusicIn-\nformation Retrieval,Oct. 2002.\n[3]Cano, P.,Battle, E.,Kalk er,T.andHaitsma,\nJ.“AReviewofAlgorithms forAudio Finger -\nprinting”, pp.169-173, Proc.oftheInt.Work-\nshoponMultimedia SignalProcessing ,Dec.\n2002.\n[4]Allamanche, E.,Herre, J.,Hellmuth, O.,\nFr¨obach, B.and Cremer ,M.“AudioID: To-\nwards Content-Based Identiﬁcation ofAudio\nMaterial”, Proc.ofthe100thAESConvention ,\nMay 2001.\n[5]Cheng, Y.“Music Database Retrie valBased\nonSpectral Similarity”, pp.37-38,Proc.ofthe\n2ndInt.Symposium onMusicInformation Re-\ntrieval,Oct. 2001.\n[6]Wang, A.“An Industrial Strength Audio\nSearch Algorithm”, Proc.ofthe4thInt.Sym-\nposiumonMusicInformation Retrieval,Oct.\n2003.\n[7]Leon-Garcia, A.Probability andRandomPro-\ncessesforElectrical Engineering .2ndEdition,\nISBN 0-201-50037-X, Addison-W esleyPub-\nlishing Compan y,Inc., 1994.\n[8]Kotz,S.,Kozubo wski, T.J.andPodg ´orski, K.\nTheLaplaceDistributionandGeneralizations .\nISBN 3-7643-4166-1, Birkh ¨auser ,2001.\n[9]Doets, P.J.O.,Modelling aRobustAu-\ndioFingerprinting System .Technical Report,\nhttp://ict.e wi.tudelft.nl, 2004."
    },
    {
        "title": "A Polyphonic Music Retrieval System Using N-Grams.",
        "author": [
            "Shyamala Doraisamy",
            "Stefan M. Rüger"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417961",
        "url": "https://doi.org/10.5281/zenodo.1417961",
        "ee": "https://zenodo.org/records/1417961/files/DoraisamyR04.pdf",
        "abstract": "This paper describes the development of a polyphonic mu- sic retrieval system with the n-gram approach. Musical n-grams are constructed from polyphonic musical perfor- mances in MIDI using the pitch and rhythm dimensions of music. These are encoded using text characters enabling the musical words generated to be indexed with existing text search engines. The Lemur Toolkit was adapted for the development of a demonstrator system on a collection of around 10,000 polyphonic MIDI performances. The in- dexing, search and retrieval with musical n-grams and this toolkit have been extensively evaluated through a series of experimental work over the past three years, published elsewhere. We discuss how the system works internally and describe our proposal for enhancements to Lemur to- wards the indexing of ‘overlaying’ as opposed to index- ing a ‘bag of terms’. This includes enhancements to the parser for a ‘polyphonicmusical word indexer’ to incorpo- rate within document position information when indexing adjacent and concurrent musical words. For retrieval of these ‘overlaying’ musical words, a new proximity-based operator and a ranking function is proposed.",
        "zenodo_id": 1417961,
        "dblp_key": "conf/ismir/DoraisamyR04",
        "keywords": [
            "polyphonic music retrieval system",
            "n-gram approach",
            "MIDI",
            "pitch and rhythm dimensions",
            "text characters",
            "Lemur Toolkit",
            "musical words",
            "text search engines",
            "experimental work",
            "enhancements to Lemur"
        ],
        "content": "APOLYPHONIC MUSICRETRIEV ALSYSTEM USING N-GRAMS\nShyamala Doraisamy\nFac.ofComp.Sc.andIT\nUniversityPutraMalaysiaStefan R¨uger\nDepartment ofComputing\nImperialCollegeLondon\nABSTRACT\nThis paper describes thedevelopment ofapolyphonic mu-\nsicretrie valsystem with then-gram approach. Musical\nn-grams areconstructed from polyphonic musical perfor -\nmances inMIDI using thepitch andrhythm dimensions of\nmusic. These areencoded using textcharacters enabling\nthemusical words generated tobeindexedwith existing\ntextsearch engines. The Lemur Toolkit wasadapted for\nthedevelopment ofademonstrator system onacollection\nofaround 10,000 polyphonic MIDI performances. Thein-\ndexing, search andretrie valwith musical n-grams andthis\ntoolkit havebeen extensi velyevaluated through aseries\nofexperimental workoverthepast three years, published\nelsewhere. Wediscuss howthesystem works internally\nanddescribe ourproposal forenhancements toLemur to-\nwards theindexing of‘overlaying’ asopposed toindex-\ninga‘bag ofterms’. This includes enhancements tothe\nparser fora‘polyphonic musical wordindexer’toincorpo-\nratewithin document position information when indexing\nadjacent andconcurrent musical words. Forretrie valof\nthese ‘overlaying’ musical words, anewproximity-based\noperator andaranking function isproposed.\n1.INTRODUCTION\nN-grams havebeen widely used intextretrie val,where a\nsequence ofsymbols isdivided intooverlapping constant-\nlength subsequences. Acharacter string formed fromn\nadjacent characters within agiventextiscalled ann-gram.\nN-gramming hasrecently been adopted asanapproach\nforindexing sound-related data. Anexperimental system\nby[4]which used adatabase offolksongs, allowed index-\ningoftheentire musical work. Using thisapproach for\nfullmusic indexing ofmonophonic data, each folksong\nofthedatabase wasconverted into asequence ofpitch\ninterv alsignoring individual durations. Using agliding\nwindo w,thissequence wasfragmented into overlapping\nlength- nsubsections. These n-grams were then encoded\nas‘text’words or‘musical words’, aterm coined by[4]\nthat wehavecontinued toadopt. These arebasically a\nstring ofcharacters with nosemantic content. Asacon-\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forproﬁt orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation ontheﬁrstpage.\nc\r2004 Universitat Pompeu Fabra.sequence, each folksong could berepresented asa‘text\ndocument’, andregular textsearch engines canbeused.\nSeveral studies haveinvestigated theuseofn-grams\nandinformation retrie val(IR) approaches formusic in-\nformation retrie val(MIR) [4,9,7].However,thecon-\nstruction ofn-grams hasbeen confined tomonophonic\nsequences. Weintroduced amethod toobtain n-grams\nfrom polyphonic music using thepitch andrhythm dimen-\nsions ofmusic, andthrough aseries ofexperimentation,\ntherobustness ofmusical n-grams with polyphonic music\nretrie valwasshown[2].\nTheIRprocess canbebrieﬂy described based onagen-\neralIRmodel. Information items inacollection arepre-\nprocessed andindexed. During information retrie val,a\nuser’ squery isprocessed andformulated totheformat re-\nquirements oftheindexedcollection. Information items\nthataremost similar tothequery would beretrie vedand\npresented totheuser based onasimilarity computation.\nDevelopments onthisbasic model hasresulted inrather\ndiverse IRmodels, butwith thecommon general empha-\nsisofretrie ving information relevanttoarequest, rather\nthan direct specification ofadocument. Modern IRsys-\ntems include sophisticated indexing, searching, retrie ving\ntechnologies, similarity computation algorithms anduser-\ninterf aces. Weutilise these forourMIR system.\nThis paper isstructured asfollo ws: The approach to\nconstructing n-grams from polyphonic music data using\nthepitch andrhythm dimension ofmusic ispresented in\nSection 2.Section 3presents ourpolyphonic music re-\ntrievalsystem design. The user interf aceofoursystem\ndemonstrator isshowninSection 4.\n2.MUSICAL N-GRAMS\n2.1.Patternextraction\nWithpolyphonic music data, theapproach described in\nSection 1forgenerating n-grams from monophonic se-\nquences would notbeapplicable, since more than one\nnote may besounded atonepoint intime. Theapproach\nproposed byusforn-gram construction from polyphonic\ndata isdiscussed brieﬂy inthissection. Polyphonic mu-\nsicpieces areencoded asanordered pair ofonset time\n(inmilliseconds) andpitch (inMIDI semitone numbers)\nandthese aresorted based ontheonset times. There may\npossibly beafewdifferent pitches corresponding toone\nparticular onset time with polyphonic music data. Pitches\nwith thesame orsimilar onset time together asmusical0719\n=\n;windo w1\n150699\n=\n;windo w2\n300689\n>>=\n>>;windo w3\n450699\n>>>>=\n>>>>;60057 windo w4\n72\n90064\n60\nFigure1.Excerpt from Mozart’ s‘Alla Turca’ andthefirst\nfeweventswith onset times andpitches\nevents aregrouped together .Using thegliding windo wap-\nproach asillustrated inFigure 1,thissequence ofeventsis\ndivided intooverlapping subsequences ofndifferent adja-\ncentevents, each characterised byaunique onset time. For\neach windo w,weextract allpossible monophonic pitch\nsequences andconstruct thecorresponding musical words.\nThese words would beused forindexing, searching and\nretrie ving from apolyphonic music collection.\nInterv alrepresentation ofpitches, i.e.,thedifference of\nadjacent pitch values, areused rather than thepitch values\nthemselv es,owing totheir transposition-in variance. For\nasequence ofnpitches, wedefine asequence ofn\u00001\ninterv alsby\nIntervali=Pitchi+1\u0000Pitchi: (1)\nFigure 1illustrates thepattern extraction mechanism\nforpolyphonic music: The performance data ofthefirst\nfewnotes ofaperformance ofMozart’ s‘Alla Turca’ was\nextracted from aMIDI fileandconverted intoatextfor-\nmat, asshownatthebottom ofFigure 1.Theleftcolumn\ncontains theonset times sorted inascending order ,and\nthecorresponding notes (MIDI semitone numbers) arein\ntheright column. When using awindo wsize of3onset\ntimes, wegetoneinterv alsequence forthefirst windo w\n[\u00002\u00001],oneforthesecond windo w[\u000011]andtwofor\nthethird windo w,[1\u000012]and[13].Thepolyphonic data\ninthefourth windo wgivesriseto4monophonic pitch se-\nquences within thiswindo w.\nRatio i=Onset i+2\u0000Onset i+1\nOnset i+1\u0000Onset i: (2)\nAlthough MIDI files encode theduration ofnotes, we\ndonottaketheactual orpercei vedduration ofnotes into\nconsideration, asthisisnearly impossible todetermine\nfrom actual performances orrawaudio sources. Bycon-\ntrast, onset times canbeidentified more readily with sig-\nnalprocessing techniques.\nForasequence ofnonset times weobtain n\u00002ra-\ntiosusing Eqn 2andn\u00001interv alvalues using Eqn 1.Ann-gram representation which incorporates both pitch\nandrhythm information using interv als(I)andratios (R)\nwould beconstructed intheform of\n[I1R1:::In\u00002Rn\u00002In\u00001]: (3)\nUsing theexample ofFigure 1,thecombined interv al\nandratio sequences from thefirst 3windo wsoflength 3\nare[\u000021\u00001],[\u0000111],[11\u000012]and[113].Note that\nthefirst andlastnumber ofeach tuple areinterv alswhile\nthemiddle number isaratio.\n2.2.Patternencoding\nInorder tobeable tousetextsearch engines, then-gram\npatterns havetobeencoded with textcharacters. One\nchallenge that arises istofind anencoding mechanism\nthat reﬂects thepatterns wefind inmusical data. With\nlargenumbers ofpossible interv alvalues andratios tobe\nencoded, andalimited number ofpossible textrepresenta-\ntions, classes ofinterv alsandratios thatclearly represent a\nparticular range ofinterv alsandratios without ambiguity\nhadtobeidentified. Forthis, thefrequenc ydistrib ution\nforthedirections anddistances ofpitch interv alsandra-\ntiosofonset time differences thatoccur within thedata set\nwere obtained. Thefrequenc ydistrib ution ofalloccurring\ninterv als(inunits ofsemitones) of3096 polyphonic MIDI\nfiles wasanalysed. According tothedistrib ution, thevast\nbulkofpitch changes occurs within oneoctave(i.e., with\nsemitone differences between \u000012and+12),andagood\nencoding should bemore sensiti veinthisarea than out-\nside it.Wechose thecode tobetheintegral part ofa\ndifferentiable continuously changing function, thederiva-\ntiveofwhich closely matches theempirical distrib ution of\ninterv als.\nC(I)=int(Xtanh(I=Y)); (4)\nwhere XandYareconstants andC(I)isthecode as-\nsigned totheinterv alI.Thefunction intreturns theinte-\ngerportion ofitsargument. Xhastheeffectoflimiting\nthenumber ofcodes, andwith26letters (a-z) adopted for\nthestudy ,itisaccordingly setto27inourexperiments.\nYissetto24forthisachie vesa1:1mapping ofsemi-\ntone differences intherange f\u000013;\u000012;:::;13g.Inac-\ncordance with theempirical frequenc ydistrib ution ofin-\ntervalsinthisdata-set, lessfrequent semitone differences\n(which arebigger insize) aresquashed andhavetoshare\ncodes. Thecodes obtained arethen mapped totheASCII\ncharacter values forletters. Inencoding theinterv aldi-\nrection, positi veinterv alsareencoded asuppercase letters\nA\u0000Zandnegativedifferences areencoded with lowercase\nletters a\u0000z,thecode fornodifference being thenumeric\ncharacter 0.\nThefrequenc yofthelogarithm ofalloccurring ratios\nofthedata collection inthesense ofEqn 2wasanal-\nysed. Peaks clearly discriminated ratios thatarefrequent.\nMid-points between these peak ratios were then used as\nthebinboundaries which provide appropriate quantisa-\ntion ranges. Ratio 1hasthehighest peak, asexpected,andother peaks occur inasymmetrical fashion where, for\neverypeak ratior,there isasymmetrical peak value of\n1=r.From ourdata analysis, thepeaks identified asratios\ngreater than 1are6/5,5/4,4/3,3/2,5/3,2,5/2,3,4and5.\nTheratio 1isencoded asZ.Thebins forratios above1,as\nlisted above,areencoded with uppercase letters A\u0000Iand\nanyratio above4.5isencoded asY.The corresponding\nbins forratios smaller than 1aslisted aboveareencoded\nwith lowercase letters a\u0000iandy,respecti vely.\nMusical words obtained from encoding then-grams\ngenerated from polyphonic music pieces with textletters\nareused intheconstruction ofindexfiles. Queries, either\nmonophonic orpolyphonic areprocessed similarly .The\nquery n-grams areused assearch words inatextsearch\nengine.\n2.3.N-gramming strategies\nSeveralproblems hadbeen identified intheuseofn-grams\nwith polyphonic music retrie val.These problems andso-\nlutions were tested. Wecarried outevaluations with sev-\neralpossible indexing mechanisms [2,3].Inthissubsec-\ntion wesummarise ourconclusions from theevaluations\nthatinformed inoursystem design.\n2.3.1.Pathrestrictions\nWhen thewindo wsizenislargeorwhen toomanynotes\ncould besounded simultaneously ,thenumber ofallmono-\nphonic combinations within awindo wbecomes large.Con-\nsider ,forexample, thecase ofn=5,with tendiffer-\nentnotes played ateach ofthe5onset times. Asare-\nsultthere would be105=100;000different monophonic\npaths through thiswindo w:thisappears tobeanimprac-\ntical wayofindexing atinybitofmusic! Inthiscase, we\nsuggest restricting thepossible combinations tovariations\nofupper andlowerenvelopes ofthewindo w,i.e.,weonly\nallowmonophonic sequences which runthrough thehigh-\nesttwonotes perevent(variation oftheupper envelope)\norwhich runthrough thelowest twonotes perevent(vari-\nation ofthelowerenvelope). Intheaboveexample there\nwould only be2\u000125=64different monophonic paths\nthrough thishighly polyphonic passage ofmusic.\n2.3.2.PositionInformation\nProximity-based retrie valhasbeen widely used with text.\nIngeneral, proximity information canbequite effectiveat\nimpro ving precision ofsearches [1].Adopting itsusewith\nmusic data hasbeen verylimited —apreliminary study\nperformed by[7]using monophonic musical sequences.\nApart from juststoring thedocument id,thelocation for\ntheoccurrence ofaterm orwithin document position(s)\ncanbeadded totheindexdata. Withexactpositions where\nawordappears inatext,single-w ordqueries canbeex-\ntended asaphrase. Amore relax edversion ofthephrase\nquery istheproximity query .Inthiscase, asequence of\nsingle words orphrases isgiven,together with amaximum\nallowed distance between them. The words andphrasesPoly/Mono -phonic\nMusic QueriesPoly/Mono -phonic\nMusic Documents\nFormulated Musical\nText QueryMusical Words\nIndexed CollectionDocument Pre -processing and Indexing Query processing and formulation\nSimilarity\ncomputation\nRanked List of\nSimilar Music\nDocumentsMusic -friendly inputs\nMusic -friendly outputs\nFigure2.Polyphonic Music Retrie valSystem Overvie w\nmay ormay notberequired toappear inthesame order\nasthequery .Thefirst word’slocation isidentified andif\nallterms arefound within aparticular distance, theterm\nfrequenc yisincremented forthegivenphrase query [1].\nInconsidering within document position andadjacenc y\nofterms forpolyphonic music, notonly the‘listening or-\nder’/‘playing order’ based onthetimeline thathastocon-\nsidered butalso theconcurrenc yofthis‘order’. Theonly\nsequentially thathasbeen preserv edwith then-gram ap-\nproach formusical words generation when indexing has\nbeenncontiguous notes [7].Polyphonic music would\nrequire anewapproach towards indexing position infor -\nmation using ‘overlaying’ positions ofpolyphonic musi-\ncalwords. This would takeinto consideration thetime-\ndependent aspect ofpolyphonic musical words compared\ntoindexing a‘bag ofterms’. Inusing then-gram ap-\nproach towardsindexing polyphonic music, apart from the\nadjacent musical words generated based onatime line,\nwords may begenerated concurrently ataparticular point\nintime. Preliminary investigation performed [3]empha-\nsizes theneed fora‘polyphonic musical wordposition\nindexer’.\n3.POLYPHONIC MUSICRETRIEV ALSYSTEM\nThe scope ofourMIR system istoretrie veallsimilar\npieces giveneither amonophonic orpolyphonic musical\nquery excerpt. Formusic similarity assumptions, evalua-\ntion oftheapproaches havebeen based ontherelevance\nassumption used by[9]. The polyphonic music retrie val\nsystem design isshowninFigure 2andthefollo wing sub-\nsections describe theprocesses shown.\n3.1.Document PreprocessingandIndexing\nThe document collection used contained almost 10,000\npolyphonic MIDI performances. These were mostly clas-\nsical music performances which hadbeen obtained from\ntheInternet [http://www .classicalarchi ves.com]. Files that\nconverted totextformats with warning messages onthe\nvalidity oftheMIDI filesuch ‘nomatching offset’ fora\nparticular onset, bythemidi-to-te xtconversion utility [6],\nwere notconsidered forthetestcollection.Index Pos. Pitch Rhythm nY #R.Bins\nPPR4 yes yes yes 424 21\nPPR4ENV yes yes yes 424 21\nPR3 no yes yes 324 21\nPR4 no yes yes 424 21\nPR4ENV no yes yes 424 21\nPR5ENV no yes yes 524 21\nTable1.Musical wordformat andindexfilevariants\nDocuments arepreprocessed using ourn-gram approach\nwith severalvariants ofindexesaredeveloped based on\nthestrate gies described inSubsection 2.3. Queries are\nusually subjected tothesame kind ofprocessing. Index\nmechanisms that combine various elements were evalu-\nated andfollo wing arethose recommended from ourin-\nvestigation:\nPR3, PR4: Thepitch andrhythm dimensions areused for\nthen-gram construction, asdescribed inSubsection\n2.1.n=3orn=4were values ofnadopted. For\ninterv alencoding, thevalue ofYinEqn 4issetto\n24.Fortheratio encoding, all21binranges thathad\nbeen identified assignificant, aslisted inSubsection\n2.2,were used.\nENV assuffix: Thegeneration ofn-grams isrestricted to\nthevariations oftheupper andlowerenvelopes of\nthemusic, asdiscussed inSubsection 2.3.1.\nPPR4: Incorporation ofposition information toPR4 as\ndiscussed inSection 2.3.4.\nTable 1showsasummary oftheused wordandindex\nfileformats listed inalphabetical order (with Pos. indi-\ncating ifposition information isincluded with theindex\ndata).\n3.2.QueryProcessingandFormulation\nBoth monophonic andpolyphonic queries canbemade to\ntheindexedpolyphonic collection. Themusical words ob-\ntained from thequery document canbequeried asabag\nofterms against thecollection indexedinthesame way.\nWhat iscurrently being investigated arequeries formu-\nlated asstructured queries tobequeried against thecollec-\ntionindexedwith theincludion ofposition information.\n3.2.1.Bagofterms\nQueries areprocessed inthesame approach totheindexed\ncollection. Themusical wordformat variants arelisted in\nTable 1.Adjacanc yandconcurrenc yinformation ofthe\nmusical words arenotconsidered.\n3.2.2.InternalStructuredQueryFormat\nTheuseofthevarious proximity-based andstructured query\noperators available within [5]arecurrently being investi-\ngated fortheinclusion tothesystem. Query languagesallowauser tocombine thespecification ofstrings (or\npatterns) with thespecification ofstructural components\nofthedocument. Apart from theclassic IRmodels, IR\nmodels thatcombine information ontextcontent with in-\nformation onthedocument structure arecalled structured\ntextretrie valmodels1[1].Following areoperators within\nLemur thatwere tested:\nSum Operator: #sum (T1:::Tn)Theterms ornodes con-\ntained inthesumoperator aretreated ashaving equal\ninﬂuence onthefinal result. Thebelief values pro-\nvided bythearguments ofthesum areaveraged to\nproduce thebelief value ofthe#sum node.\nOrdered Distance Operator: #odN (T1:::Tn)Theterms within\nanODN operator must befound inanyorder within\nawindo wofNwords ofeach other inthetextin\norder tocontrib utetothedocument’ sbelief value.\nAfewinitial tests using theknownitem search with\ntheODN operator showed, asexpected, apoor perfor -\nmance. Retrie valusing more comple xquery formulations\nwere then lookedinto[3].Amonophonic theme extracted\nfrom Figure 1wasencoded as:\nbZaZA aZAZC AZCIB CIBib BibZa bZaZA aZAZD AZ-\nDIA DIAia AiaZa aZaZA aZAZG AZGZb GzbZa bZaZA\naZAZB AZBZb BzbZa bZaZA aZAZC\nThe query wasthen reformulated as(This reformulation\nwould bedone automatically bythesystem internally and\nwould betransparent totheuser):\n#SUM( #ODN3(bZaZA aZAZC)\n#ODN3(AZCIB CIBib)\n...\n#ODN3(bZaZA aZAZC))\nBased ontheanalysis ofourretrie valresults [3],amore\nspecific operator formusic retrie valisrequired andthein-\ntroduction ofMODN (Musical Ordered Distance Opera-\ntor)isdiscussed inthefollo wing subsection.\n3.3.Similarity Computation\nThemain aimofthesystem istoretrie veallsimilar pieces,\ngivenamonophonic orpolyphonic musical excerpt asa\nquery andusing therelevance assumption adopted by[9].\nCurrently thesystem retrie vesbased onthevector -space\nIRmodel. Ongoing workinadapting thestructured re-\ntrievalmodel forproximity-based retrie valisdiscussed in\nthesecond partofthissubsection.\n3.3.1.Vector-spacemodel\nToindex,search andretrie ve,theLemur Toolkit wasse-\nlected astheresearch toolasitsupported anumber ofIR\n1Inusing theBoolean model, aclassic IRmodel, foraquery ’white\nhouse’ toappear near theterm ‘president’, itwould beexpressed as\n[‘white house’ and‘president’]. Classic IRmodels include theBoolean,\nvector -space andprobabilistic models [1]. Aricher expression such as\n‘same-page(near(‘white house’, ’president’)))’ would require astructured\nquery language.Polyphonic Midi FileEnhanced Polyphonic\nText Document\nGeneratorMusical text document\nwith within word\npositions\n1bZaZA2aZAZl2aZAZC3AylFG3AZCFh3AylFC\n3AZCFl4lFGZJ4CFhZJ4lFCZN4CFlZN4 lFGZ0\n4 CFhZ0 4 lFCZD4CFlZD4lFGZd4CfhZd4 lFCZ0\n4 CFlZ0 5 GZJfb5hZJfb5CZNfb5lZNfb5 GZ0fH\n5 hZ0fH 5 CZDfH5lZDfH5GZdfL5hZdfL5 CZ0fL\n5 lZ0fL\nFigure3.Musical textdocument with within document\nwordpositions\nmodels which weinvestigated forMIR. Models supported\ninclude theuseoflanguage models (based onaprobabilis-\nticapproach) andthevector -space model. Similar tothe\nfindings from thestudy by[7],thelanguage modelling ap-\nproach didnotperform aswell with musical n-grams and\ntehvector -space model wasadopted. Details ofadapting\nthismodel inLemur arediscussed indetail in[10].\n3.3.2.Structuredretrievalmodelandpolyphonic position\nindexing\nForproximity-based retrie val,thestructured query lan-\nguage provided byLemur wasinvestigated andinthissec-\ntion wediscuss theenhancements being tested forpoly-\nphonic music retrie val.\nUsing thefirst fiveonsets ofthemusic excerpt inFig-\nure1,thedocument generated from ourpolyphonic text\ndocument generator would besimilar asthebagofterms\nshowninSubsection 3.2.2. Thepolyphonic textdocument\ngenerator isthetoolwedeveloped (modules added toutil-\nities byGNMIDI Solutions [6])fortheconversion ofa\npolyphonic MIDI fileintoapolyphonic musical textdoc-\nument. This wasenhanced tooutput ‘overlaying’ posi-\ntions. Theparser in[5]hadtobemodified toparse these\nnewposition information format. ’Overlaying’ positions\nasshowninFigure 3would need toberecorded bythe\nindex.\nThe existing proximity-based operator ODN retrie ves\nonly documents thatcontainallquery terms inthesimi-\nlarorder within agivenproximity distance areretrie ved.\nIntercepting onsets from apolyphonic document would\ngenerate n-grams thataredissimilar toitscorresponding\nmonophonic query ,resulting innon-retrie valofrelevant\ndocuments. Erroneos queries would alsogenerate dissim-\nilarn-grams from therelevantdocument. A‘musical or-\ndered distance operator’ (MODN) should enable thisdif-\nference between n-grams generated from thequery and\ntherelevantpolyphonic document tobereﬂected byasim-\nilarity measure [3],i.e., retrie valthat partially satisfies\nquery conditions would beneeded. Rank edretrie valshould\nbebased onthenumberofquerytermsfoundwithina\ngivenproximitydistance andnotthecondition thatall\ntermsmustbefoundwithinagivenproximitydistance .\nTherequirement ofMODN therefore would betoretrie ve<DOC q1>\n#MODN\n3\nLPAREN\n1 p\n2 q\n3 r\nRPAREN<DOC q2>\n#MODN\n3\nLPAREN\n1 p\n2 q\n2 r\n3 s\nRPAREN\nFigure4.Query Documents\ndocuments based onarank whereby documents thatmatch\nthequery with theTheHighest number ofquery n-grams\nwithin agivenproximity distance would beretrie vedwith\nthehighest rank, i.e.,rank 1.\nWearecurrently investigating asimple scoring func-\ntionbased onthenotion of‘fuzzy match points’ forMODN.\nTherefore informulating ascoring function forMODN,\nwestart attheverybeginning andlook atthenotion of\nmatch points forterm orphrase frequencies instead [1].\nThe term match point defined by[1]refers totheposi-\ntioninthetextofasequence ofwords which matches (or\nsatisfies) theuser query .Ifastring appears inthree po-\nsitions inthetextofadocument dj,wesaythatthedoc-\nument djcontains three match points. WithMODN, we\ncontinue tolook atmatch points butthematch points for\npolyphonic musical documents would betheposition in\nthetextdocument thatmatches thefirst term available of\nthequery sequence. Apart from assigning amatch point\nonly based onthefirstterm ofthequery sequence, anyof\nthefollo wing terms inthequery sequence canassume the\nfirst position ifanyoftheprior terms donotexistinthat\nparticular sequence. Forthescore calculation, 1point is\ngivenasascore forthefirstterm from thequery sequence\nthatisfound inatextdocument and1point foreach ofthe\nfollo wing term thatisfound within thegivenproximity .\nThis score calculation isshownusing thefollo wing ex-\nample. Twosample queries, q1(amonophonic query) and\nq2(apolyphonic query) aregiveninFigure 4.Thequery\ndocuments areshownintheformat required byLemur’ s\nparser .Foranexample offour relevantdocuments, D1:1\np2q2r3r4s4t,D2:1p2q3x4y,D3:1p2q3p4\nq5randD4: 1p2q3a4b5c6d7r8p9q10r,the\nscores foreach ofthese documents foreach ofthequeries\nwould beasfollo ws:\nTherelevance scores ofthedocuments forq1are: D1\n=5(from sequences (pqr)and(pr)),D2=2(from se-\nquence (pq)),D3=6(from twosequences (pqr)),and\nD4=5(from sequences (pq)and(pqr)).\nTherelevance scores ofthedocuments forq2arebased\nontwomonophonic sequences –(pqs)and(prs):D1=\n9(from sequences (pqs),(prs)and(prs)),D2=2\n(sequence (p,q)), D3=4(2sequences (pq)),andD4=6\n(sequences (pq),(pq)and(pr)).Figure5.User interf ace\nFigure6.QBH System Interf ace\n4.SYSTEM DEMONSTRA TOR\nThescreen shots ofourearly polyphonic music retrie val\nsystem demonstrator isshowninFigure 5.Apart from se-\nlecting apoly/monophonic MIDI filefrom thedialog box\nprovided, aquery input canalso bemade viaa‘graph-\nicalkeyboard’ enabling monophonic performance inputs\nonly.MIDI files generated willbeprocessed bythemusi-\ncaldocument generator discussed Subsection 3.3.2. gen-\nerating query terms inthesame format astheindexeddoc-\numents asshowninTable 1.The screen shot showsre-\ntrievalresults asatextfilewith arankedlistofdocuments.\nFortheretrie valshown,themusical textdocuments were\nindexedwith thePR4ENV format with Lemur version 1.9.\nThesystem iscurrently being tested with Lemur Version\n2.2being enhanced fortheinclusion ofthenewmusical\nwords parser ,MODN anditsscoring module. Future work\nincludes evaluation ofthis.\nThis development workispartoftheMultimedia Knowl-\nedgement Research Group’ sframe workforcontent-based\ninformation retrie val.Figure 6showstheQBH interf ace\ndeveloped where onecanhum aquery which would be\ntranscribed byapitch track erwritten by[8]andconverted\ntoPR4ENV .Future workalsoincludes integrating thisand\nother interf aces tomusic inputs thathavebeen developed\ninclude atextcontour input andpolyphonic audio filein-\nput[http://km.doc.ic.ac.uk] tothecurrent early prototype.5.CONCLUSION\nWehaveoutlined apolyphonic music retrie valsystem de-\nsign anddescribed itsdevelopment indetail. These in-\nclude details ofthedocument preprocessing andindex-\ning, query processing andformulation andthesimilar -\nitycomputation using musical n-grams from polyphonic\nmusic data. Anearly prototype hasbeen shownandwe\narecurrently investigating enhancements forincorporat-\ningproximity-based retrie val.\n6.ACKNOWLEDGEMENTS\nThis workispartially supported bytheEPSRC, UK.\n7.REFERENCES\n[1]Baeza-Y ates, Rand Ribeiro-Neto, B.Mod-\nernInformation Retrieval.ACM Press Addi-\nsonWesley,1999.\n[2]Doraisamy ,Sand R¨uger,S.“Rob ustPoly-\nphonic Music Retrie valwith N-grams”, Jour-\nnalofIntelligentInformation Systems ,21(1),\n53–70.\n[3]Doraisamy ,SandR¨uger,S.“Position Index-\ningofAdjacent andConcurrent N-Grams for\nPolyphonic Music Retrie val”,Proceedings of\ntheFourthInternational ConferenceonMu-\nsicInformation Retrieval,ISMIR2003 ,Balti-\nmore, USA, 2003, pp227–228.\n[4]Downie, S.“Evaluating aSimple Approach\ntoMusic Information Retrie val:Concei ving\nMelodic N-grams asText”.PhDThesis ,Uni-\nversity ofWestern Ontario, 1999.\n[5]Lemur Toolkit. http://www-\n2.cs.cmu.edu/ ˜lemur .\n[6]Nagler , G. GN MIDI Solutions.\nhttp://www2.iicm.edu/Cpub .\n[7]Pickens,J.“AComparison ofLanguage Mod-\neling andProbabilistic TextInformation Re-\ntrieval”,1stAnnualInternational Symposium\nonMusicInformation Retrieval,ISMIR2000 ,\nPlymouth, USA, 2000.\n[8]Tarter,A.“Query byHumming”, Project Re-\nport, Imperial Colle geLondon, 2003.\n[9]Uitdenbogerd, Aand Zobel, J.“Melodic\nMatching Techniques forLargeDatabases”,\nProccedings ofACMMultimedia '99,pp57-\n66.\n[10] Zhai, C.“Notes ontheLemur TF-IDF model”,\nhttp://www-2.cs.cmu.edu/lemir/1.9/tfidf .ps,\nUnpublished report."
    },
    {
        "title": "Micro-level groundtruthing environment for OMR.",
        "author": [
            "Michael Droettboom",
            "Ichiro Fujinaga"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417217",
        "url": "https://doi.org/10.5281/zenodo.1417217",
        "ee": "https://zenodo.org/records/1417217/files/DroettboomF04.pdf",
        "abstract": "A simple framework for evaluating OMR at the symbol level is presented. While a true evaluation of an OMR system requires a high-level analysis, the automation of which is a largely unsolved problem, many high-level er- rors are correlated to these more tractably-analyzed lower- level errors.",
        "zenodo_id": 1417217,
        "dblp_key": "conf/ismir/DroettboomF04",
        "keywords": [
            "OMR",
            "symbol level",
            "evaluation",
            "automation",
            "high-level analysis",
            "tractably-analyzed",
            "lower-level errors",
            "high-level errors",
            "truly",
            "system"
        ],
        "content": "SYMBOL-LEVEL GROUNDTRUTHING ENVIRONMENT FOR OMR\nMichael Droettboom\nDigital Knowledge Center, Sheridan Libraries\nThe Johns Hopkins University\n3400 North Charles Street\nBaltimore, MD 21218, USAIchiro Fujinaga\nCIRMMT, Faculty of Music\nMcGill University\n555 Sherbrooke Street West\nMontr´eal, QC H3A 1E3, Canada\nABSTRACT\nA simple framework for evaluating OMR at the symbol\nlevel is presented. While a true evaluation of an OMR\nsystem requires a high-level analysis, the automation of\nwhich is a largely unsolved problem, many high-level er-\nrorsarecorrelatedtothesemoretractably-analyzedlower-\nlevel errors.\n1. INTRODUCTION\nComplexdocumentunderstandingsystemsaredifﬁcultto\nevaluate [5]. On the other hand, pixel-level and symbol-\nlevelanalysisismuchmorefeasibletoimplementandstill\nquite useful for improving the accuracy of document un-\nderstanding systems [11].\nWe demonstrate that, at least at the level of symbol\nrecognition, the test driven software development tech-\nnique[1]canbeusedtodeveloprobustopticalmusicrecog-\nnition(OMR)systems. Wearguethatdevelopingaperfor-\nmanceevaluationmetricforapatternrecognitionsoftware\nsystem is similar to the test-driven software development\nmethodology. The goal in both cases is to minimize er-\nrors.\nIn many classiﬁcation problems the evaluation metric\nis fairly straightforward. For example at the character\nlevel of OCR, it is simply a matter of ﬁnding the ratio be-\ntween correctly identiﬁed characters and the total number\nof characters. In other classiﬁcation domains, this is not\nsosimple,forexampledocumentsegmentation[7],recog-\nnition of maps [4], mathematical equations [9], graphi-\ncal drawings, and music scores. In these domains, there\nare often multiple correct output representations, which\nmakes the problem of comparing a given output to high-\nlevel groundtruth very difﬁcult [5]. In fact, it could be ar-\ngued that a complete and robust system to evaluate OMR\noutput would be almost as complex and error-prone as an\nOMR system itself.\nSymbol-level analysis may not be directly suitable for\ncomparingcommercialOMRproducts[2,8],becausesuch\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2004 Universitat Pompeu Fabra.systems are usually “black boxes” that take an image as\ninput and produce a score-level representation as output.\nNevertheless, higher-level evaluations such as [8] are of-\nten correlated to lower-level errors, thus studying the ef-\nfectsoflower-levelerrorstohigher-levelerrorswithopen-\nsourcesystemsshouldprovideinsightsforevaluatingcom-\nmercial products.\n2. EVALUATIONS\nTestingisusefulfordevelopingnewapproachestorecog-\nnition, comparing the relative performances (error rate,\nefﬁciency) of alternative approaches and validating incre-\nmental modiﬁcations to algorithms (e.g. regression test-\ning). In general, there are three types of evaluation strate-\ngies [3]: 1) symbol-level evaluation, 2) combination of\nsymbol-level evaluations, and3) an edit cost function.1\nThe symbol-level evaluation involves calculating the\nrecognition rate of basic graphic symbols, which may be\nline segments, arcs, or characters. Sophisticated metrics\nwill reﬁne the recognition rate by counting separately the\ncorrectly identiﬁed symbols, missed symbols, and false\npositive (added) symbols.\nIn many document-understanding systems, including\nOMR, metrics are required at higher-levels. This is often\ncalculatedassomelinearcombinationofweightedlower-\nlevel errors. The assignments of weights are non-trivial,\nhowever [10]. Ng, et al., 2003 [8] provides a ﬁrst attempt\nat developing evaluation weights for OMR. However, as\nmusicisperceptual,someerrorsare“moreimportantthan\nothers” and can rely on levels of complex context. There-\nfore any fully complete evaluation metric must rely, at\nleast on some level, on humanstudies.\nAnother popular metric for the evaluation of complex\ndocument understanding systems is to calculate the cost\nof manually correcting and redrawing graphical objects.\nThis is achieved by calculating the editing time required\nby empirically ﬁnding the times for operations such as\nmouseclicks[12]. Oncetherelationshipbetweenthesymbol-\nlevel errors and the type of operations required to correct\nthemisestablished,thismetriccanbecalculatedautomat-\nically.\n1The term “low-level” used in [3] has been changed to “symbol-\nlevel” for better clarity.To sum up, in all cases, symbol-level error analysis is\nrequired.\n3. SYMBOL-LEVEL ANALYSIS\nThe goal of our symbol-level analysis system is to evalu-\natetheperformanceofourOMRsystem,GAMUT(Gamera-\nbasedAdaptiveMusicUnderstandingTool)2onverysmall\nscore features, before the symbolic representation of the\nscore has been assembled. Some of these are:\n•assignment of accidentals to notes\n•combinationofclefsandaccidentalsintokeysigna-\ntures\n•constructionoftimesignaturesfromindividualdig-\nits\n•number of beams in each beam complex\n•exact location of noteheads relative to the staff and\nledger lines\n•identiﬁcation and disambiguation of dots (augmen-\ntation,staccato, bass clef, fermata, etc.)\n•interpretingrepeat, Dacapoandother“ﬂow-control”\nmarkings\n•recognition of tuplets (triplets, quintuplets etc.)\nIt is clear from each of these cases that each example can\nbe evaluated cleanly, either strictly correct or incorrect,\nrather than having multiple correct representations or a\ngraded scale of correctness.\nThe system works by generating result tables, one for\neach of the evaluation categories, with three columns:\n1. the subject of the test, highlighted with some sur-\nrounding context\n2. GAMUT’s interpretation ofthe excerpt\n3. the groundtruth\nIf the groundtruth is incorrect, it may be edited and sub-\nmitted back to the systemfor re-evaluation.\nFigures1and2areexamplesoftheseevaluationtables.\nThisapproach,whileclearlyveryprimitive,hasalready\nbeen very useful for improving the accuracy of GAMUT.\nFor instance, when the test system was ﬁrst implemented,\nthenumberofcorrectlypitchednoteheadswasaround79%.\nThetestsystemallowedustoimprovethealgorithmtothe\npointwhereitnowoperatesat96%. Itisalsoveryeasyto\nadd new test categories as the need arises.\n2In previous incarnations, this system was known as AOMR (Adap-\ntiveOpticalMusicRecognition),whichhassincebecomeagenericterm.\nFigure1. Excerptofthetableshowingtheperformanceof\naugmentation dots. The ﬁrst three examples do not match\nthe groundtruth, since they are not genuine augmentation\ndots.Figure 2. Excerpt of the table showing beam interpreta-\ntion. Theﬁrstfourelementsdonotmatchthegroundtruth,\nwhich is a list list of numbers representing the number of\nbeams at each point along the set of beams.4. CONCLUSIONS\nAs research domains mature, evaluation metrics become\nmore important. As the MIR community grows, efforts\nshouldbemadetocreatemetricsandgroundtruthdatasets.\nWe presented here a simple but fundamental model for\nevaluating OMR systems and different algorithms within\noneOMRsystem. Automaticallycomparingtheﬁnalsym-\nbolic outputs of OMR systems at the high-level is still\na largely unsolved problem, and one which we hope the\nMIR community will consider as an application of recent\ndevelopments in the searching and analysis of symbolic\nmusic data.\n5. REFERENCES\n[1] Beck, K., 2002. Test driven development.\nBoston: Addison-Wesley.\n[2] Bruno, I., P. Bellini, and P. Nesi. 2003. As-\nsessing optical music recognition tools. Paper\npresented at Second MUSICNETWORK Open\nWorkshop .\n[3] Chhabra, A., and I. Phillips. 1998. A bench-\nmark for graphics recognition systems. Pro-\nceedings of IEEE Workshop on Empirical\nEvaluation Methods in Computer Vision .\n[4] Hinz,S.,C.Wiedemann,andA.Baumgartner.\n2000.Aschemeforreadextractioninruralar-\neasanditsevaluation. ProceedingsoftheIEEE\nWorkshoponApplicationsofComputerVision,\n134–9.\n[5] Hu, J., R. Kashi, D. Lopresti, G. Nagy, and G.\nWilfong. 2001. Why table ground-truthing is\nhard.International Conference on Document\nAnalysis and Recognition (ICDAR), 129–33.\n[6] Kanai, J., S. Rice, T. Nartker, and G. Nagy.\n1995. Automated evaluation of OCR zoning.\nIEEE Transactions on Pattern Analysis and\nMachine Intelligence, 17: 86–90.\n[7] Mao, S., and T. Kanungo. 2001. Empirical\nperformance evaluation methodology and its\napplication to page segmentation algorithms.\nIEEE transactions on Pattern Analysis and\nMachine Intelligence, 23(3): 242–56.\n[8] Ng, K., and A. Jones. 2003. A quick-test\nfor optical music recognition systems. Paper\npresented at Second MUSICNETWORK Open\nWorkshop .\n[9] Okamoto, M., H. Imai, and K. Takagi. 2001.\nPerformance evaluation of a robust method\nfor mathematical expression recognition. Pro-\nceedings of the International Conference on\nDocument Analysis and Recognition, 121–8.[10] Randriamasy,S.andL.Vincent.1994.Bench-\nmarking and page segmentation algorithms.\nProceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 411–6.\n[11] Wenyin, L., and D. Dori. 1997. A protocol for\nperformance evaluation of line detection algo-\nrithms.Machine Vision and Applications, 9:\n240–50.\n[12] Wenyin, L., Z. Liang, T. Long, and D.\nDori. 1999. Cost evaluation of interactively\ncorrecting recognized engineering drawings.\nProceedings of IAPR Workshop on Graphics\nRecognition, 335–40."
    },
    {
        "title": "MIR In Matlab: The MIDI Toolbox.",
        "author": [
            "Tuomas Eerola",
            "Petri Toiviainen"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416234",
        "url": "https://doi.org/10.5281/zenodo.1416234",
        "ee": "https://zenodo.org/records/1416234/files/EerolaT04.pdf",
        "abstract": "The MIDI Toolbox is a compilation of functions for analyzing and visualizing MIDI files in the Matlab computing environment. In this article, the basic issues of the Toolbox are summarized and demonstrated with examples ranging from melodic contour, similarity, key- finding, meter-finding to segmentation. The Toolbox is based on symbolic musical data but signal processing",
        "zenodo_id": 1416234,
        "dblp_key": "conf/ismir/EerolaT04",
        "keywords": [
            "MIDI Toolbox",
            "Matlab computing environment",
            "analyzing",
            "visualizing",
            "melodic contour",
            "similarity",
            "key finding",
            "meter finding",
            "segmentation",
            "symbolic musical data"
        ],
        "content": "MIR IN MATLAB: THE MIDI TOOLBOX\nTuomas Eerola Petri Toiviainen \nDepartment of Music \nUniversity of Jyväskylä, Finland  Department of Music \nUniversity of Jyväskylä, Finland  \nABSTRACT (150-200 words) \nThe MIDI Toolbox is a compilation of functions for \nanalyzing and visualizing MIDI files in the Matlab computing environment. In this article, the basic issues of the Toolbox are summarized and demonstrated with examples ranging from melodic contour, similarity, key-finding, meter-finding to segmentation. The Toolbox is based on symbolic musical data but signal processing methods are applied to cover such aspects of musical behaviour as geometric representations and short-term memory. Besides simple manipulation and filtering functions, the toolbox contai ns cognitively inspired \nanalytic techniques that are suitable for context-\ndependent musical analysis, a prerequisite for many music information retrieval applications. \n1. INTRODUCTION \nMIDI Toolbox provides a set of Matlab functions, \nwhich provide versatile possibilities to analyze and visualize MIDI data. The development of the Toolbox has been part of ongoing research involved in topics relating to musical data-mining, modelling music perception and decomposing the data for and from perceptual experiments. The Toolbox is available free of charge under the GNU General Public License from http://www.jyu.fi/musica/miditoolbox/. Although MIDI data is not necessarily a good representation of music in general, it suffices for many research questions dealing with concepts such as melodic contour, tonality and pulse finding. These concepts are intriguing from the point of view of music perception and the chosen representation greatly affect s the way these issues can \nbe approached. MIDI format is also wide-spread among the research community as well as having a wider group of users amongst the music professionals, artists and amateur musicians. \nThe aim of MIDI Toolbox is to provide the core \nrepresentation and functions for manipulating and analyzing MIDI files in Matlab. These basic tools are designed to be modular to allow easy further development and tailoring for specific analysis needs. Another aim is to facilitate efficient use and to lower the \nthreshold for practical use. For example, the Toolbox can be used as teaching aid in music cognition courses. \n2. GENERAL ISSUES \n2.1 Representation \nIn the Toolbox, a MIDI file is represented as a matrix \n(hereafter notematrix ) containing information about \nonset time, MIDI channel, pitch, velocity and duration of each note. Such notematrices can be created by a conversion function readmidi that reads in a MIDI file: \n \nnm = readmidi('laksin.mid') \n nm = 0  0.9   1  64  82  0     0.55 1  0.9   1  71  89  0.61  0.55 2  0.45  1  71  82  1.22  0.28 . . . \nThe columns of the matrix refer to (1) onset time in \nbeats, (2) duration in beats, (3) MIDI channel, (4) pitch, (5) velocity, (6) onset time in seconds and (7) duration in seconds. The example notematrix given above are the first three notes of Finnish Folk song called Läksin \nMinä Kesäyönä  (the notation is shown in Figure 1). \nLarge corpora of music can be conveniently \nprocessed in the Toolbox using a collection format, which stores multiple notematrices in a compact way and a set of meta functions that process the entire collection using a given analytical procedure or function. For example, the authors have created a collection of 8613 Finnish Folk Songs [1] using this format and also analyzed ot her music corpora using the \nsame representation [2].  \n2.2. Functions \nThere are several categories of functions in the Toolbox \nthat either manipulate, filter, analyze or generate notematrices. These functions are divided into following categories: \n \n• Conversion functions \n• Generation functions \n• Filter functions \n• Meta functions \n• Plotting functions \n• Statistical functions \nPermission to make digital or hard c opies of all or part of this work \nfor personal or classroom use is granted without fee provided that \ncopies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. \n© 2004 Universitat Pompeu Fabra.   \n \n• Key-finding functions \n• Contour functions \n• Segmentation functions \n• Melodic functions \n• Meter-related functions \nConversion functions  read MIDI files into Matlab and \nexport MIDI files from Matlab whereas generation \nfunctions  allow creating and pl aying notematrices from \nwithin Matlab, either by invoking an external MIDI player or synthesizing the notematrix and sending the resulting waveform to a soundcard using Matlab's own soundsc function. Various filter functions  allow \nediting the musical material, including scaling, shifting, \nquantizing, transposing and time-windowing notematrices. Meta functions  are designed to apply any \nfunction for a collection of notematrices. Plotting and \nstatistical functions are also helpful in visualizing the \nstructure of a notematrix, either by displaying it using a pianoroll notation or showing the distribution of certain types of events of a notematrix. These functions facilitate use of the remainder of function categories which could be called analytic functions such as key-\nfinding, meter-finding, segmentation and so on. Most of \nthe functions in these categories involve cognitive models that are applied to a notematrix. Next examples \nof these analytical functions are demonstrated. \n3. EXAMPLES \n3.1. Melodic contour and similarity \nMelodic contour describes the overall shape of the \nmelody. The contour representation of a melody is usually easier to remember than exact interval information [3, 4] and numerous music informational retrieval systems make use of this information [5, 6]. Contour representation is available in the Toolbox using melcontour  function. In Figure 1, two versions of \nmelodic contour are plotted using two resolutions.  \nMelodic contour lends itself to different \napplications. For example, various distance measures \ncan be used to calculate the similarity between the contour of melodic motifs or phrases. In the toolbox, distance calculations are handled by the meldistance  function, which calculates the distance \n(or similarity) between two melodies using a user-\ndefined representation (various distributions or melodic contour) and a distance measure. In this function, similarity can be scaled to range between 0 and 1, the latter indicating perfect similarity although this value does not indicate absolute similarity but is meaningful when compared to other melodic pair ratings.  0 2 4 6 8 10 12 14 16 18606264666870727476\nTime (beats )Midinoteresolution in beats = .25\nresolution in beats = 1.0\n \n \nFigure 1 . Melodic contour and the notation of an \nexample tune, Läksin Minä Kesäyönä . Phrases are \nmarked with capital letters. The contour plot depicts two versions of the melodic contour, sampled at a different resolution, for the first two phrases only (A and B).  \nFor example, the similarity between the four phrases of \nthe example tune (shown in Figure 1), using contour representation (20 samples) and city block distance (scaled between 0 and 1), is calculated as follows: \n \n%% Load the melody from a selection of \n%% reference tunes nm=reftune('laksin_complete');  %% select the first 8 beats ph{1} = onsetwindow(nm,0,8);   %% select the next phrases and trim ph{2} = trim(onsetwindow(nm,9,17));  ph{3} = trim(onsetwindow(nm,18,28)); ph{4} = trim(onsetwindow(nm,29,37));  %% calculate the distances for i=1:4   for j=1:4     dst(i,j)=meldistance(ph{i},ph{j},...     'contour','taxi',20,1);   end end \n \n contour   durdist1 \nPhrase A B C  Phrase A B C \nA     A    \nB .90    B .63   \nC .80 .76   C .63 .95  \nD .90 1.00 .76  D .54 .91 .89 \nTable 1. Melodic similarity of the four phrases of the \nexample tune (notation shown in Figure 1) using different representations (rescaled between 0-1). \nFor the contour representation, phrases B and D are \nidentical (similarity 1.00) and phrase C differs most   \n \nfrom the other phrases (Table 1). This seems intuitively reasonable although the exact numbers should be viewed with caution. However, similarity based on the distribution of note durations indicates greatest similarity between phrases B and C (.95) and lowest similarity between A and D (.54). The results of this simple indicator of rhythmic similarity are in contrast with the contour representation.  These results are, again, \nreadily apparent from the notated score. \n3.2. Key-finding \nThe classic Krumhansl & Schmuckler key-finding \nalgorithm [7], is based on key profiles obtained from empirical work by Krumhansl & Kessler [8]. In the K-S key-finding algorithm, the 24 individual key profiles, 12 major and 12 minor key profiles, are correlated with the pitch-class distribution of the piece weighted according \nto their duration. This gives a measure of the strength of each key. This method can be applied within a time window that runs across the length of the music to explore how tonality changes over time.  \nLet us take the Piano Sonata Nro. 1 in G major \nby F. J. Haydn (Hob. XVI:8). The first 12 measures of this sonata are shown in Figure 2. Below is an example of finding the maximal key correlation and obtaining the key at each two-beat interval: \n \n%% Run K-S key-finding algorithm across %% a 8-beat window moved by 4 beats %% first calculate highest correlations keys = movewindow(sonata,8,4,...        'beat','maxkkcc');  %% then obtain the key names labels = keyname(movewindow(sonata,...         8,4,'beat','kkkey')); \nThe results of this are shown in panel B of the Figure 2. \nThe plot displays the key changes over time, showing the local tonal center moving from G major towards D major. Although the measure shows the strength of the key correlation, it gives a rather simplistic view of the tonality as the dispersion of the key center between the alternate local keys is not shown. A recent dynamic model of tonality induction [9 ] calculates local tonality \nbased on key profiles. The results may be projected onto a self-organizing map (SOM) trained with the 24 key profiles.  \nIn the following example, the keysomanim \nfunction calculates the key strengths and creates the \nprojection either as an animation in Matlab or as separate frames. The resulting maps underlying the tonal strengths are toroid in shape, which means that the opposite edges are attached to each other. This visualization of tonality can be used to show the fluctuations of the key center and key strength over time as an animation. Below is a static example of the \nkeysomanim  function using a window length of 4 beats \nand short-term memory time constant of 8 beats (the \nsecond parameter). The results are shown in panel C of Figure 2.  \nkeysomanim(sonata,8,4,'beat','strip'); \n/MT65\n/MT66\n/MT67/MT66/MT101/MT97/MT116/MT32/MT52/MT46/MT48\n/MT67/MT68/MT98\n/MT68/MT69/MT98/MT69/MT70\n/MT71/MT98/MT71/MT65/MT98/MT65\n/MT66/MT98/MT66/MT99/MT99/MT35/MT100\n/MT100/MT35/MT101/MT102/MT102/MT35\n/MT103/MT97/MT98/MT97/MT98/MT98\n/MT98/MT66/MT101/MT97/MT116/MT32/MT56/MT46/MT48\n/MT67/MT68/MT98\n/MT68/MT69/MT98/MT69/MT70\n/MT71/MT98/MT71/MT65/MT98/MT65\n/MT66/MT98/MT66/MT99/MT99/MT35/MT100\n/MT100/MT35/MT101/MT102/MT102/MT35\n/MT103/MT97/MT98/MT97/MT98/MT98\n/MT98/MT66/MT101/MT97/MT116/MT32/MT49/MT50/MT46/MT48\n/MT67/MT68/MT98\n/MT68/MT69/MT98/MT69/MT70\n/MT71/MT98/MT71/MT65/MT98/MT65\n/MT66/MT98/MT66/MT99/MT99/MT35/MT100\n/MT100/MT35/MT101/MT102/MT102/MT35\n/MT103/MT97/MT98/MT97/MT98/MT98\n/MT98\n/MT66/MT101/MT97/MT116/MT32/MT49/MT54/MT46/MT48\n/MT67/MT68/MT98\n/MT68/MT69/MT98/MT69/MT70\n/MT71/MT98/MT71/MT65/MT98/MT65\n/MT66/MT98/MT66/MT99/MT99/MT35/MT100\n/MT100/MT35/MT101/MT102/MT102/MT35\n/MT103/MT97/MT98/MT97/MT98/MT98\n/MT98/MT66/MT101/MT97/MT116/MT32/MT50/MT48/MT46/MT48\n/MT67/MT68/MT98\n/MT68/MT69/MT98/MT69/MT70\n/MT71/MT98/MT71/MT65/MT98/MT65\n/MT66/MT98/MT66/MT99/MT99/MT35/MT100\n/MT100/MT35/MT101/MT102/MT102/MT35\n/MT103/MT97/MT98/MT97/MT98/MT98\n/MT98/MT66/MT101/MT97/MT116/MT32/MT50/MT52/MT46/MT48\n/MT67/MT68/MT98\n/MT68/MT69/MT98/MT69/MT70\n/MT71/MT98/MT71/MT65/MT98/MT65\n/MT66/MT98/MT66/MT99/MT99/MT35/MT100\n/MT100/MT35/MT101/MT102/MT102/MT35\n/MT103/MT97/MT98/MT97/MT98/MT98\n/MT98/MT48 /MT52 /MT56 /MT49/MT50 /MT49/MT54 /MT50/MT48 /MT50/MT52/MT48/MT46/MT53/MT48/MT46/MT54/MT48/MT46/MT55/MT48/MT46/MT56/MT48/MT46/MT57/MT49\n/MT71 /MT71\n/MT71\n/MT71/MT68/MT68\n/MT84/MT105/MT109/MT101/MT32/MT40/MT98/MT101/MT97/MT116/MT115/MT41/MT77/MT97/MT120/MT46/MT32/MT107/MT101/MT121/MT32/MT99/MT111/MT114/MT114/MT46/MT32/MT99/MT111/MT101/MT102/MT102/MT46\n \n \nFigure 2 . Panel A: Notation of th e F. J. Haydn’s Piano \nSonata Nro. 1 in G major (Hob. XVI:8), first twelve \nmeasures. Panel B: Maximum key correlation coefficients across time (K-S algorithm [7] and the labels for most plausible lo cal keys. Panel C: Dynamic \nmodel of tonality induction [9] portraying the local key centers across the excerpt. \nFrom the separate figures of panel C one can see how \nthe tonal center is first firmly in G major and then it slightly leans towards other regions, mainly D major. Another option in keysomanim  function allows to \nsave the animation as a Matlab movie ('movie'), which \ncan also be written to a file using the avifile  \ncommand. \n3.3. Meter-finding \nInferring the meter is a challenge that involves finding a \nregular beat structure fro m a musical sequence. One \ntechnique is to use the autocorrelation function and to seek peaks from the onset structure corresponding to simple duple (2/4, 2/8, 2/2, 4/4) or simple triple meter (3/4, 3/2). This technique resembles the method used by Brown [10] to estimate meter. Toiviainen and Eerola [2]   \n \ntested the effectiveness of the method in classifying the meters into duple or triple using two large collections of melodies (Essen collection and Finnish Folk Tunes, N = 12368), [11, 1]. With only durational accents, the correct classification rate was around 80%. This method is available as the meter function in the Toolbox and applied to the Finnish folk tune Läksin Minä Kesäyönä : \n \nmeter(nm) \n   ans = 3 \n \nThis indicates the most probable meter is simple triple \n(probably 3/4). When mel odic accent is incorporated \ninto the inference of meter, the correct classification of \nmeter is higher (up to 93% of the Essen collection and 95% of Finnish folk songs were correctly classified [2]). This optimized function is available in toolbox using the 'optimal' parameter in meter function, although the scope of that function is limited to monophonic melodies. Also, discriminating between compound meters (6/8, 9/8, 6/4) presents another challenge for meter-finding that will not be covered here. A plot of \nautocorrelation results – obtained by using \nonsetacorr  function – provides a closer look of how \nthe meter is inferred (Figure 3). In the function, the \nsecond parameter refers to divisions per quarter note. \n \nonsetacorr(nm,4,'fig'); \n \nFigure 3 shows that the zero time lag receives perfect \ncorrelation as the onset distribution is correlated with itself. Time lags at 1-8 quarter notes are stronger than the time lags at other positions. Also, there is a difference between the correlations for the time lag 2, 3 and 4. The lag of 3 beats (marked with A) is higher (although only slightly) than the lags 2 and 4 beats and therefore it is plausible that the meter is simple triple. \nEven if we now know the likely meter we cannot \nbe sure the first event or ev ents in the notematrix are not \npick-up beats. In this dilemma, it is useful to look at the metrical hierarchy, whic h stems from the work by \nLerdahl and Jackendoff [12]. They described the rhythmic structure of Western music as consisting of alteration of weak and strong beats, which are organized in a hierarchical manner. The positions in the highest level of this hierarchy correspond to the first beat of the measure and are assigned highest values, the second highest level to the middle of the measure and so on, depending on meter. It is possible to examine the metrical hierarchy of events  in a notematrix by making \nuse of the meter-finding algor ithm and finding the best \nfit between cyclical permutations of the onset distribution and the Lerdahl and Jackendoff metrical hierarchies (Figure 4). \n 0 2 4 6 800.20.40.60.81\nTime lag (quarter notes)CorrelationAutocorrelation function of onset times\nA\n↓B\n↓B\n↓\n \nFigure 3 . Autocorrelation function of onset times in \nLäksin Minä Kesäyönä . \n \nplothierarchy(nm,'sec'); \n \nThe dots in Figure 4 represen t the metrical hierarchy. \nHigh stacks of dots (connected with a stem) correspond to events with high metrical hierarchy. In this melody, three levels are in use. The meter-finding algorithm infers the meter of the tune correctly (3/4), but the algorithm assumes that the first note is a pick-up note. This probably happens because of the metrical stress caused by the long notes in the second beats in measures three and six. A listener unfamiliar with the song could easily form this interpretation of meter. \n \n0 2 4 6 8 102345\nTime in secondsMetrical hierarchy\n \n \nFigure 4. Notation and inferred metrical hierarchy of \nLäksin Minä Kesäyönä . \n3.4 Melodic segmentation \nOne of the fundamental processes in perceiving music is \nthe segmentation of the auditory stream into smaller units, melodic phrases, motifs and such issues. Various computational approaches to segmentation of monophonic material have been taken. With symbolic representations of music, we can distinguish rule-based and statistical (or memory-based) approaches. An example of the first category is the algorithm by Tenney and Polansky [13], which finds the locations where the changes in “clangs” occur. These clangs correspond to   \n \nlarge pitch intervals and large inter-onset-intervals (IOIs). This idea is partly based on Gestalt psychology. Another segmentation technique uses the probabilities derived from the analysis of melodies (e.g., [14]). In this technique, the probabilities of phrase boundaries have \nbeen derived from pitch-class-, interval- and duration distributions at the segment boundaries in the Essen folk song collection [11]. Finally, a Local Boundary Detection Model by Cambouropoulos [15] is a rule-based model that offers efficient segmentation. These segmentation algorithms are available in the MIDI \nToolbox as individual functions. When applied to a simple folk song, Läksin Minä Kesäyönä , they produce \nsegmentations shown in Figure 5. \n \n0 2 4 6 8 10 12 14 16 18D4 D4#E4 F4 F4#G4 G4#A4 A4#B4 C5 Pitch\n0 2 4 6 8 10 12 14 16 1800.20.40.60.81Boundary\nstrengtha\n0 2 4 6 8 10 12 14 16 1800.20.40.60.81Boundary\nstrengthb\n0 2 4 6 8 10 12 14 16 1800.20.40.60.81Boundary\nstrengthc\nTime (beats)  \n \nFigure 5 . Three segmentations of Läksin Minä \nKesäyönä showing the boundary strengths (0-1) based \non (a) the Gestalt-based algorithm [13], (b) Local Boundary Detection Model [15] and (c) the probabilities of tone, interval, and duration distributions at segment boundaries in the Essen collection [11,14]. \nAll segmentation algorithms produce plausible divisions \nof the example tune although the correct segmentation in the notation is most in line with \nLocal Boundary \nDetection Model . \n4. CONCLUSIONS \nIn this paper, the MIDI Toolbox has been shortly \npresented. The toolbox utilizes the efficient data processing and powerful visualization tools of Matlab. Currently the representation of MIDI files contains only note event information and hold pedal information. Future work will consider storing other information than the one pertaining to note events (e.g., key and time signature, copyright notes, track names, various types of controller data) in the notematrix. To make the Toolbox more convenient to use for novice users, a graphical \nuser interface will be developed. \nThough the Toolbox is based on symbolic \nmusical data, the processing philosophy incorporates signal processing methods to cover such aspects of musical behaviour as short-term memory and geometric representations of contour. Compared with other systems available for manipulation of symbolic music (e.g., Humdrum [16], POCO [17], Melisma [18] and Rubato [19]), MIDI Toolbox connects directly with the powerful computational and visualization tools of Matlab. In addition, Matlab is available for the most operating systems and is widely used in the engineering community and hence it sports many readily available specialized toolboxes that are useful for music related operations (e.g., Statistics, Signal Processing, Neural Network and Fuzzy Logic Toolboxes). \nAlthough many musically relevant tasks (such as \nrecognizing a variant of a tune) may appear trivial for us as listeners, simulating them with a computer is challenging. We believe that cognitively inspired analytic techniques (segmentation, key-finding, meter-finding) are necessary to carry out this context-dependent task convincingly. We hope that the MIDI Toolbox will facilitate the application and development of such techniques within the MIR community. \n \n5. REFERENCES \n[1] Eerola, T., & Toiviainen, P. Suomen kansan \nesävelmät: Digital archive of Finnish Folk songs  \n[computer database]. Jyväskylä: University of Jyväskylä, 2004. URL: http://www.jyu.fi/musica/sks/ \n[2] Toiviainen, P., & Eerola, T. \"The role of accent \nperiodicities in meter i nduction: A cl assification \nstudy\". In S. D. Libscomb, R. Ashley, R. O. Gjerdingen, & P. Webster (Eds.) Proceedings of \nthe 8th International Conference on Music Perception & Cognition , Evanston, IL (pp. 422-\n425). Adelaide, Australia: Causal Productions, 2004. \n[3] Dowling, W. J. \"Scale and contour: Two \ncomponents of a theory of memory for melodies\". Psychological Review, 85(4),  341-354, 1978. \n[4] Dowling, W. J., & Fujitani, D. S. \"Contour, \ninterval, and pitch recognition in memory for melodies\". Journal of the Acoustical Society of \nAmerica, 49 , 524-531, 1971. \n[5] Kim, Y. E., Chai, W., Garcia, R., & Vercoe, B. \n\"Analysis of a contour-based representation for melody\". In International Symposium on Music \nInformation Retrieval , Plymouth, MA: Indiana \nUniversity, 2000. \n[6] Lemström, K., Wiggins, G. A., & Meredith, D. \"A \nthree-layer approach for music retrieval in large databases\". In The Second Annual Symposium on   \n \nMusic Information Retrieval  (pp. 13-14), \nBloomington: Indiana University, 2001. \n[7] Krumhansl, C. L. Cognitive foundations of musical \npitch . New York: Oxford University Press, 1990. \n[8] Krumhansl, C. L., & Kessler, E. J. \"Tracing the \ndynamic changes in perceived tonal organization in a spatial representation of musical keys\". Psychological Review, 89 , 334-368, 1982. \n[9] Toiviainen, P., & Krumhansl, C. L. \"Measuring \nand modeling real-time responses to music: the dynamics of t onality induction\". Perception, 32(6),  \n741-766, 2003. \n[10] Brown, J. C. \"Determination of meter of musical \nscores by autocorrelation\". Journal of Acoustical \nSociety of America, 94(4),  1953-1957, 1993. \n[11] Schaffrath, H. The Essen Folksong Collection in \nKern Format.  [computer database] D. Huron (Ed.). \nMenlo Park, CA: Center for Computer Assisted Research in the Humanities, 1995. \n[12] Lerdahl, F., & Jackendoff, R. A generative theory \nof tonal music. Cambridge: MIT Press, 1983. \n[13] Tenney, J., & Polansky, L. \"Temporal gestalt \nperception in music\". Journal of Music Theory, \n24(2) , 205-41, 1980. \n[14] Bod, R. \"Memory-based models of melodic \nanalysis: challenging the gestalt principles\". Journal of New Music Research, 31 , 27-37, 2002. \n[15] Cambouropoulos, E. \"Musical rhythm: A formal \nmodel for determining local boundaries, accents and metre in a melodic surface\". In M. Leman (Ed.), Music, Gestalt, and Computing: Studies in \nCognitive and Systematic Musicology (pp. 277-\n293). Berlin: Springer Verlag, 1997. \n[16] Honing, H. \"POCO: An Environment for \nAnalysing, Modifying, and Generating Expression in Music.\" In Proceedings of the 1990 \nInternational Computer Music Conference  (pp. \n364-368). San Francisco: Computer Music Association, 1990. \n[17] Huron, D. The Humdrum Toolkit: Reference \nManual . Menlo Park, CA: Center for Computer \nAssisted Research in  the Humanities, 1995. \n[18] Mazzola, G., & Zahorka. O. \"The RUBATO \nPerformance Workstation on NEXTSTEP.\" In Proceedings of the ICMC 1994. Århus, Denmark: \nICMC, 1994. \n[19] Temperley, D., & D. Sleator. The Melisma Music \nAnalyzer . Available from \nhttp://www.link.cs.cmu.edu/music-analysis/, 2001."
    },
    {
        "title": "Extracting Melody Lines From Complex Audio.",
        "author": [
            "Jana Eggink",
            "Guy J. Brown"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418003",
        "url": "https://doi.org/10.5281/zenodo.1418003",
        "ee": "https://zenodo.org/records/1418003/files/EgginkB04.pdf",
        "abstract": "We propose a system which extracts the melody line played by a solo instrument from complex audio. At every time frame multiple fundamental frequency (F0) hypotheses are generated, and later processing uses various knowledge sources to choose the most likely succession of F0s. Knowledge sources include an instrument recognition module and temporal knowledge about tone durations and interval transitions, which are integrated in a probabilistic search. The proposed system improved the number of frames with correct F0 estimates by 14% compared to a baseline system which simply uses the strongest F0 at every point in time. The number of spurious tones was reduced to nearly a third compared to the baseline system, resulting in significantly smoother melody lines.",
        "zenodo_id": 1418003,
        "dblp_key": "conf/ismir/EgginkB04",
        "keywords": [
            "proposed system",
            "extracts melody line",
            "complex audio",
            "multiple fundamental frequency hypotheses",
            "probabilistic search",
            "improved F0 estimates",
            "reduced spurious tones",
            "smooth melody lines",
            "instrument recognition module",
            "temporal knowledge"
        ],
        "content": "Abstract\nWe propose a system which extracts the melody line\nplayed by a solo instrument from complex audio. At everytime frame multiple fundamental frequency (F0)hypotheses are generated, and later processing usesvarious knowledge sources to choose the most likelysuccession of F0s. Knowledge sources include aninstrument recognition module and temporal knowledgeabout tone durations and interval transitions, which areintegrated in a probabilistic search. The proposed systemimproved the number of frames with correct F0 estimatesby 14% compared to a baseline system which simply usesthe strongest F0 at every point in time. The number ofspurious tones was reduced to nearly a third compared tothe baseline system, resulting in significantly smoothermelody lines.\n1.   Introduction\nThis paper is concerned with obtaining an adequatesymbolic representation from musical audio. The word‘adequate’ indicates that we are not trying to extract allpossible information, but only information which isrelevant for a specific task. All information would includea full transcription in form of a musical score, detailedacoustic analysis of the instruments playing, emotionalexpression, harmonic analysis and so on. This is not onlya currently infeasible task, but would also result in a largeamount of what is likely to be irrelevant information for aspecific task. Here, we focus on the extraction of themajor melody line from polyphonic audio, which wedefine as the melody played by the solo instrument in anaccompanied sonata or concerto. While this definition isnot necessarily always accurate in terms of perception ormusic theory (since, for example, another instrument fromthe orchestra could play a short solo), it provides a goodevaluation basis. It is also likely to be an adequatedefinition for most practical applications. Applications forwhich an automatic melody extraction would be usefulinclude automatic indexing and analysis, detection ofcopyright infringement, and ‘query-by-humming’systems. In particular, the latter has attracted increasingattention over the last few years (e.g., see [8] and otherpublications from the same conference). But so far, thesesystems rely on manual coding of the melody line, which\nis then used to match against the melody ‘hummed’ by theuser. The system introduced in this paper attempts tobridge this gap by providing a melody extracted directlyfrom audio, without the need for manual interference.\nPrevious work has tended to concentrate on solving\nthe problem of obtaining a full transcription, i.e.extracting all fundamental frequencies (F0s) frompolyphonic music. But no general solution has beenproposed so far; the musical material has always beenrestricted in terms of the maximum number of concurrentnotes, specific instruments and musical style. Someexamples include the work of Klapuri [7], whoconcentrated on rich polyphonic sounds with relativelyhigh numbers of simultaneous notes, but only usedmixtures of isolated tone samples. Working in thefrequency domain using a subtraction scheme based on\nspectral smoothness he obtained accuracies between 54%\nand 82% for 6 concurrent F0s. Raphael [9] pursuedanother approach, which was restricted to piano music byMozart with no more than 4 concurrent tones, butemployed naturalistic recordings. His system is based ona statistical modelling approach using hidden Markov-models (HMMs), and he achieved an accuracy of 61%. \nGoto [6] pursued an approach in many ways related to\nthe one proposed in this paper. Instead of trying toestimate all F0s, he concentrated on estimating only theF0s of the melody and the bass line in limited frequencyranges. Using adaptive tone models within a multi-agentarchitecture, he obtained good results of around 88%correctly identified F0s on a frame-by-frame basis for thehigher pitched melody, and slightly less for the bass line.His system was mainly evaluated on popular and jazzmusic with the melody line produced by a singer, and noattempt was made to distinguish between sections wherethe singer is present or silent. \nBerenzweig [2] investigated the distinction between\nsections of a piece of music where the singing voice ispresent or absent. He compared statistical models traineddirectly on acoustic features or on the likelihood outputsof a speech recogniser trained on normal speech. He foundlittle advantage of the latter, suggesting that thedifferences of sung words with background music fromnormal speech are too large to be useful; and no attemptwas made to separate the voice from the backgroundaccompaniment. He obtained correct classifications in74% of frames, improving to 81% when averaged over 81frames.\nPermission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided that\ncopies are not made or distributed for profit or commercialadvantage and that copies bear this notice and the full citation on the\nfirst page.\n© 2004 Universitat Pompeu Fabra.EXTRACTING MELODY LINES FROM COMPLEX AUDIO\nJana Eggink & Guy J. Brown\nDepartment of Computer Science, University of Sheffield, UK\n{j.eggink, g.brown}@dcs.shef.ac.ukFor the current system we assume a classical sonata or\nconcerto, where a solo instrument is accompanied byeither a keyboard instrument or an orchestra. Soloinstruments can span the whole pitch range, ranging fromthe low tones of a cello to a high pitched flute, so that adivision of the frequency range in the way Goto proposedfor his system [6] is not feasible. It is also not guaranteedthat the solo instrument will always produce the strongestF0, so that additional knowledge is necessary to extractthe melody line. One of the major sources of information– and a potential advantage of our system compared toexisting ones – is the integration of an instrumentrecognition module. This knowledge can both help tochoose the correct F0 among multiple concurrent F0candidates, and to determine sections where the soloinstrument is actually present as opposed to sectionswhere only the accompanying instruments are playing.\n2.   System Overview\nThe main objective of the present system is to provide aframework for integrating different knowledge sources, inorder to enable the robust extraction of the main melody.Finding the melody is here defined as finding thesuccession of F0s produced by the solo instrument. If thestrongest F0 at every time frame was always played by thesolo instrument, then all that would be needed is analgorithm to find this F0. The strongest F0 might bedefined as the perceptually most salient one, although thatstill leaves open the question for a definition in acoustic orsignal processing terms. Reasoning from a musical pointof view, the solo instrument is more likely to have thestrongest F0 in concertos since, for example, the soloviolin is still perceivable as such even when accompaniedby a whole set of first and second violins within theorchestra. This seems less obvious in a sonata with pianoaccompaniment, where even the title might indicate anequal importance of both instruments by changing theorder in which the instruments are named, for examplesonatas for piano and violin by Beethoven or Brahms.Instead of trying to find only the single most prominent F0at every point in time, we therefore extract multiple F0candidates and use additional knowledge at later\nprocessing steps to decide which of the candidatesbelongs to the melody played by the solo instrument.\nThe knowledge sources can be divided into two\ncategories, local or stationary knowledge and temporal ortransitionary knowledge. Local knowledge includes thestrength of every F0 hypothesis and the likelihood of thatF0 to be produced by the solo instrument, both in terms ofinstrument recognition, and in terms of the pitch range ofthe solo instrument. Temporal knowledge includesmeasures related to tone durations and intervals betweensuccessive tones. This knowledge is combined to find themost likely ‘path’ through the space of all F0 candidatesin time; in other words, to find the melody. As the melodypath occasionally follows the accompaniment, additional‘post processing’ is carried out to eliminate sectionswhere the solo instrument is actually silent.\nIn the following we will assume that the identity of the\nsolo instrument is known. We were able to classify thesolo instrument correctly with high accuracies of over85% in an earlier system [5]. For now we assume that thisinstrument identification has been carried out in advance,although a more integrated solution is of coursepreferable, and will be addressed in further research.\n2.1. F0 and Tone Candidates\n2.1.1. Time-Frequency ConversionThe first processing step is a conversion from the time into\nthe frequency domain. Most commercially availablemusic is recorded at 44100 Hz, and this samplingfrequency is retained. The signal is divided into frames of70 ms (3072 samples) length with 33% (1024 samples)overlap. Every frame is multiplied with a Hanningwindow and a fast Fourier transform (FFT, size 16384) iscomputed. The spectral power is log-compressed for allfurther processing. The window size is relatively largecompared to most audio processing applications, butproved to be necessary to ensure a sufficient resolution forlow frequencies. Other research [7] used even largerwindows of up to 190 ms for F0 estimation in polyphonicmusic, and experienced a strong decrease in accuracy forshorter windows lengths. melody \n‘path’\nsolo instr. \npresent / \nabsentsolo instru-\nment +  \ninstrument \nlikelihoodsF0 / tone \ncandidatesF0 likeli-\nhoodstransitions, \ninterval \nlikelihoodsaudio\nF0 strength\nmelodyinstrument \nrecogniserLOCAL KNOWLEDGE TEMPORAL \nKNOWLEDGE\nrelative \ntone usage\nFigure 1: System OverviewSEARCH2.1.2. Peak Extraction\nThe next processing step is concerned with data reduction.\nInstead of using the complete spectrum for furtherprocessing, only spectral peaks are kept. These are usedboth to estimate F0s, and as a basis for instrumentrecognition. To estimate peaks the spectrum is convolvedwith a 50 samples wide differentiated Gaussian. As aresult the spectrum is smoothed and peaks are transformedinto zero crossings, which are easy to detect. Thefrequency of a peak is then defined by its correspondingFFT bin. A highly zero-padded FFT is used, in order toincrease the accuracy of the frequency estimates for thespectral peaks. Spectral peaks are discarded if their poweris below a threshold. This threshold differs with frequencyto account for the fact that musical instrument tonesgenerally have more energy in lower frequencies. Weemploy a local threshold defined by the mean of thespectral power within a range of ±\n 500 Hz around a peak. \n2.1.3. Estimating F0 CandidatesDifferent methods of F0 estimation were investigated,\nwith the first one searching for prominent harmonic serieswithin the spectral peaks of a frame. While this led togood results if two instruments were playing atapproximately the same level ([4]), it worked less well inthe present study. One specific problem is that theaccompaniment often plays F0s one or two octaves belowthe solo instrument, which regularly resulted in the soloinstrument being estimated one or two octaves below itstrue F0 value. Comparing the search for a prominentharmonic series with an F0 estimation that is based solelyon the power of isolated spectral peaks we found noadvantage of the former for most solo instruments. Sinceour approach depends on the existence of a spectral peakat the fundamental frequency, it cannot account forperceptual phenomena such as the ‘pitch of the missingfundamental’, but proved suitable for the musicalinstrument tones used in the present study. Using thestrongest spectral peaks as F0 candidates also has theadvantage that it has low computational cost. \n2.1.4. Connecting Peaks to Form TonesTo correct isolated errors made by the F0 estimation\nalgorithm, and to provide a more restricted basis for thesearch module, frame-based F0 hypotheses are connectedto form longer tones. The approach is based on a birth-and-death algorithm, where tones continue if there is amatching F0 hypothesis in the next frame, and areterminated otherwise, while unmatched F0s hypothesesstart a new tone. The matching interval for thecontinuation of a tone is based on its average frequency upto that point. The use of an average frequency seemspreferable over an estimated trajectory, as it allows forvibrato while breaking up successive tones even whenthey are separated by only a small interval. To account forpotential errors or misses in the F0 estimation algorithm,tones can continue even if for one frame (or maximallytwo frames) no matching F0 hypotheses are found. Inthese gaps the exact locations and strengths of missing F0s\nare reestimated by searching the spectral peaks for acandidate that is within the matching interval of theaverage F0 of the tone. A minimum tone length of twoframes is imposed, which helps to suppress isolatedrandom errors in the generation of F0 hypothesis. In asecond processing step, tones are split when suddendiscontinuities in F0 strength occur. This allows a newtone to be started if, for example, the solo instrument startson the same F0 as a long note played by theaccompaniment. \n2.2. Local Knowledge\nAll local knowledge is derived from the frame-based F0\ncandidates. Every candidate has three attributes, whichdescribe its spectral power, its fundamental frequency,and a set of features derived from its harmonic overtoneseries used for instrument recognition.\n2.2.1. F0 StrengthAssuming that the solo instrument is commonly played\nlouder than (or at least as loud as) the accompaniment, andthat strong F0 estimates are less likely to be caused byartefacts, the likelihood of an F0 candidate can be directlyinferred from its spectral power. The stronger the spectralpeak that caused the F0 hypothesis, the higher itslikelihood.\n2.2.2. F0 LikelihoodIf the solo instrument is known, the likelihood of an F0\ncandidate in terms of its frequency can be estimated. In afirst step, all candidates with F0s outside of the range ofthe solo instrument are discarded. Additionally, evenwithin the range of an instrument not all F0s are equallylikely; for example, the F0s at the end of the range of aninstrument are normally less common. We computed thelikelihood of an F0 for a specific instrument by countingthe frequency of its occurrence in MIDI (MusicalInstrument Digital Interface) transcriptions of 4-5different sonatas and concertos. To avoid influence by thekeys in which the pieces were written, likelihood valueswere smoothed across frequencies. \n2.2.3. Instrument LikelihoodInstrument recognition is carried out for every frame-\nbased F0 estimate of every tone. This part of the systemwas developed earlier to enable recognition of soloinstruments without imposing any restrictions on thebackground; for details see [5]. It uses a representationbased solely on the F0 and partials of an instrument toneand has been shown to be highly robust againstbackground accompaniment. The features consist of theexact frequency location and the normalised and log-compressed power of the first 10 partials of the targettone. Added to these are frame-to-frame differences(deltas) and differences of differences (delta-deltas)within tones of continuous F0. A Gaussian classifier witha full covariance matrix is trained for every possible F0 ofevery instrument; the classifier which maximises the\nposterior probability of a set of test features determines itsinstrument class.\nInstead of making hard decisions and assigning every\ntone to an instrument class, the likelihood values of itbeing produced by the solo instrument are computed andlater used as one knowledge source among others in themelody search.\n2.3. Temporal Knowledge\n2.3.1. Interval LikelihoodDifferent interval transitions between successive tones\nwithin a melody have different likelihoods; in westernclassical music small intervals are generally morecommon than large ones. Similar to the instrumentdependent F0 likelihood, we computed instrumentdependent interval likelihoods from 4-5 different MIDIfiles. The distributions were close to those expected fromcommon musical knowledge, with a full tone being themost frequent interval, and nearly monotonicallydecreasing likelihoods for larger intervals, slightlyfavouring fifths and octaves. Instrument-dependentvariations were very small and might have been causedprimarily by the selection of examples, but were kept forthe current evaluation.\nIntervals are computed between the average F0s of\nsuccessive tones. The likelihood value for every intervalis multiplied with the length of the second tone, sinceotherwise two or more short tones with relatively likelyinterval transitions would be favoured over one long tone.\n2.3.2. Relative Tone UsageThe measure of relative tone usage is solely used as a\npunishment under special conditions, where the pathincludes the start of a tone but leaves it before the toneends. This is necessary to allow for reverberant conditionswhich can cause an overlap between successive tones. Todiscourage paths that leave a tone very early and while thetone has still strong F0 candidates, the percentage offrames unused within that particular tone is calculated andweighted by the mean F0 strength of these skipped F0hypotheses. If all that is skipped is a reverberant ‘tail’, thenumber of frames will be limited and will have a relativelylow F0 strength, so that the punishment will be small. Inall other cases the path is probably trying to leave a tonethat really continues and therefore the path’s overallprobability is correctly diminished.\n2.4. Search \n2.4.1. Finding the Melody ‘Path’The different knowledge sources outlined in the previous\nsection are combined to find the most likely ‘path’through the maze of all possible F0s candidates over time.This resembles in many ways a tracking problem, such asone in which the changing location of a moving personhas to be identified. A common way of knowledgeintegration for this type of problem is the Bayesian\nframework, for which a number of computationallyefficient algorithms exist to find the optimal path (e.g.[1]). One of the preconditions for most of these algorithmsis that the Markov assumption holds true, e.g. thelikelihood for a state depends on the previous state, but noton earlier or future states. This is not the case within oursystem, where the temporal knowledge sources violatethis precondition. Also, the computational complexity inour application is strongly constrained by the fact that notall positions within the space of all possible F0s areconsidered at every time frame, but only those for whichan F0 hypothesis exists. Nevertheless, evaluating allpossible paths is far too costly, and we therefore applysome pruning. A simple n-best method is used, where at\nevery time step only the best n paths are kept.\nThe final melody path contains one F0 for every\nframe, but the search for this path is restricted and guidedon the basis of longer tones. The rules for allowedmovements within the time-F0 space are mainly derivedfrom general musical knowledge and are as follows:\n Every tone within the melody path has to be included\nfrom the tones’ beginning.\n A tone has to be followed to its end, unless a new tone\nstarts.\n A return to the same tone is prohibited once the path\nhas left this tone.\n After a tone has finished, the path can either go to a\nnew tone or to a silence state.\nWithin all possible paths that obey these restrictions, the\nknowledge sources help to find the most likely melodypath. The overall likelihood of a path at a certain point intime consists of the sum of both the local and the temporalknowledge sources up to that point in time. Theknowledge sources are weighted with differentimportancy factors (‘weights’), to reflect the fact thatdifferent types of knowledge might be of differentimportance for an optimal path finding solution.\nTo enable comparisons across different sources of\ninformation, all probabilities are normalised to have zeromean and a standard deviation of 1. Specifically, thisnormalisation is carried out for each sound file across allinstrument likelihood and F0 strength values. Additionalknowledge sources have fixed values independent of theactual example, but are set within a similar range ofvalues. During frames which are labelled as silent allknowledge sources are set to their mean value of 0. \n2.4.2. Silence Estimation for the Solo InstrumentThe solo instrument in classical sonatas or concertos is not\nnecessarily playing continuously. The melody pathfinding algorithm is able to label a frame as silent if noneof the available tones leads to a higher confidence value.But at the current stage, this is not reliable enough toautomatically detect longer sections where the soloinstrument is silent, where the path frequently picks up F0candidates belonging to the accompaniment. We thereforedeveloped an additional processing step to determine the\nstate of the solo instrument being ‘present’ or ‘silent’ atdifferent moments in time.\nThe melody as estimated by the path finding algorithm\nwith the corresponding likelihoods for the solo instrumentis used to distinguish between these two states, assumingthat this likelihood will be higher for sections where thesolo instrument is present than for those where it is silent.To reduce the influence of random variations and outliersin the likelihood estimates from the instrument recogniser,median smoothing is applied across neighbouringlikelihood values along the melody path. Two thresholdsare used to estimate sections where we can be fairly sureabout the state of the solo instrument. The presentthreshold is set at the median of all solo instrumentlikelihoods, starting from the assumption that the soloinstrument is present at least half the time. The silentthreshold is set to zero, the mean of the likelihoods overall instruments and all F0s. Should the present thresholdbe less than the silent threshold, the silent threshold islowered until it becomes the lower one. This scenarioshould not happen if the other parts of the system areworking well; it might be an indication that the overallinstrument recognition scheme has produced a falseestimate for the solo instrument. \nAfter these two thresholds are defined, every tone is\ninvestigated and if at least 75% of its frames are within thedefinite present or silent range, the whole tone is assignedto that state. In a second processing step, unassigned tonesare investigated. If the previous and the following tone ofa candidate tone are both assigned to the silent state, thecandidate tone in the middle is assigned to the same state,as isolated tones by the solo instrument are unlikely.Remaining tones are assigned according to the median oftheir instrument likelihood values. If this is closer to thepresent threshold, the tone is assigned to this state,otherwise it is assigned to the silent state. Additionally, itis assumed that very short sections of presence for the soloinstrument are unlikely. A minimal duration threshold isapplied, which declares sections where the soloinstrument is declared present as spurious if their lengthfalls below that threshold.\n3.   Evaluation\nThe major part of the evaluation is based on audio filesgenerated from MIDI data. This allows for a direct controlof the stimuli and provides a good basis for automaticevaluation procedures. 5 different solo instruments wereused, flute, clarinet, oboe, violin and cello. The mainreason for this choice was that these are commonly usedas solo instruments, and unaccompanied CD-recordingsfor training purposes were locally available. Bothconcertos with orchestral accompaniment and sonataswith keyboard (piano or cembalo) accompaniment wereused for evaluation; examples were chosen so that everysolo instrument was accompanied by a each type ofaccompaniment. From each of the 10 examples either awhole movement was taken, or, in the case of very long\nexamples, the first 3 minutes. No specific considerationwas given to the tempo of the pieces, but both pieces withfast and slow musical tempi (e.g. allegro, adagio) wereused. No selection according to specific musical style wasperformed; while all pieces can be categorized as westernclassical music, they span a range from the baroque (late17th to early 18th century) to the romantic period (early tolate 19th century). All MIDI files were taken withoutfurther editing from the classical music archives [3]. Theywere played using samples provided by Propellerhead’sReason 2.5 sampler software, which in most cases used 4recorded tones per octave. The solo voice and theaccompaniment were first saved in independent soundfiles and then mixed at 0 dB root-mean-square (rms)energy levels. Sections where the solo instrument is silentwere ignored for the normalisation procedure, asotherwise the solo instrument would be strongercompared to the accompaniment in pieces where it is notplaying continuously. The choice of mixing levels issomewhat arbitrary, and of course, the stronger the soloinstrument, the easier the task of estimating thecorresponding F0s. Informal listening tests by the authorsshowed that a mixing level of 0 dB was acceptable as arealistic option, with the solo instrument beingidentifiable as such. \nThe frequencies provided by the MIDI files were taken\nas ground truth. In some pieces for violin or cello the soloinstrument is in fact playing more than one F0simultaneously; in these cases either one would beconsidered correct. Evaluation was carried out both interms of the percentage of correctly estimated F0s on aframe-by-frame basis, and based on longer tones ofcontinuous F0. No explicit onset detection was carriedout, so that repeated tones without silence in betweenwere automatically merged into one longer tone. A tonewas considered as ‘found’ if at least one frame of it wasmatched by the estimated melody path, while all tones inthe estimated melody which did not at least partiallymatch with the true F0s were taken to be spurious tones.\nAutomatic evaluation of realistic recordings taken\nfrom commercially available CDs is less straight forward.We manually annotated two short examples with thecorrect F0s using a mixture of listening and visualspectrogram inspection to place the notes taken from ascore at the correct time frames. These two examples aremainly considered as proof of concept, as the test set is toolimited for a full quantitative evaluation.\n3.1. F0 Estimation\nAll further processing relies on the F0 and tone candidates\nestimated in the initial processing step. Tones missed cannot be recovered at a later stage, so additional falsecandidates seem preferable over missed ones. Usingsimply the one strongest spectral peak as F0 candidate,this F0 was correct for 52% of frames on average (basedonly on those frames where the solo instrument is actuallypresent). Increasing the number of F0 candidatesimproved the results, counting as correct every frame\nwhere any one of the F0 candidates matches the true F0.With 3 candidates correct F0s were found in 76% offrames, increasing to a plateau of around 95% with 15 ormore F0 candidates. F0 estimation was better forwoodwinds than for strings, which is probably at leastpartially due to the fact that the strongest partial coincidesmore often with the F0 for woodwinds than for strings.Differences between the instruments became smaller withmore F0 candidates, see Table 1. Additionally, F0estimation was better for concertos than for keyboardaccompaniment for all instruments, with an averagedifference of 10% with both 1 and 3 F0 candidates. \n3.2. Instrument Recognition\nThe instrument recognition scheme has been previously\nshown to give good recognition accuracies, with no dropin performance between monophonic and accompaniedphrases [5]. Tested on the melodies withoutaccompaniment and using knowledge about the correctF0s, all examples were correctly classified, except for onepiece for oboe, where the instrument was mistaken for aflute. However, recognition performance in the presenceof accompaniment was, even when evaluated based oncorrect F0s only, well below expectations. While violinand cello were still correctly identified by the system, the3 woodwind instruments were repeatedly confused witheach other or a string instrument.\nThese results indicate that either samples are harder to\nidentify than realistic phrases, maybe because they showless of instrument typical variations such as vibrato, andrecognition performance is therefore less robust in thepresence of interfering sound sources. The other factorlikely to have a strong influence is the mixing levelbetween solo instrument and accompaniment; and thefrequency regions dominated by the accompaniment. Theinstrument recognition results could suggest that themixing used results in a SNR that is actually lessfavourable for the solo instrument than those found inprofessional recordings of real musical performances. \n3.3. Melody Path\nAll knowledge sources used to find the most likely path\nare weighted by individual importancy factors. Thesetting of these weights influences the performance of thesystem to a large degree. The weights were estimated in ahill climbing procedure, where the weights of everyknowledge source were randomly increased or decreased,to find new weight combinations which led to a better\nestimate of the known melody path. Comparing the bestknowledge weights estimated for the different examples,some similarities could be observed. The strength of theF0 was in all examples one of the most important factors,often with a weight more than double that of the nextimportant knowledge source. The tone usage factors wasnot useful in more than one or two examples, but thiscould be partly due to the data, as overlapping tones ortones with inaccurately estimated onsets are less commonin MIDI based audio than in realistic recordings.Knowledge about the instrument dependent likelihood ofa specific F0s was useful in almost all cases, as was thelikelihood of a F0 to be produced by the solo instrument.The latter was true even for examples were the overallinstrument recognition was poor. This suggests that thelikelihood of having been produced by the solo instrumentwas still higher for the true F0s than for spurious ones orthose belonging to the accompaniment. \nFor further evaluation the combination of weights that\ngave the best results independent of the specific examplewas used, with an importancy factor of 7 for the F0strength, 2 for the F0 likelihood, 2 for the instrumentlikelihood, 3 for the interval likelihood, and 0 for therelative tone usage. Adjusting the weights individuallygave only a relatively small improvement of around 5%more correctly identified frames, which suggest that astable parameter set can be obtained which is independentof particular musical examples.\nAnother factor influencing performance is the number\nof F0 candidates considered, which is closely connected tothe amount of pruning used in the search for the best path.To keep computational costs within a reasonable amount,we used only 3 F0 candidates per frame. If the pathfinding algorithm was able to always choose the correctF0s, this allows for an increase of 25% overall correctframes over the baseline of choosing the strongest F0s atevery frame. Again due to necessary constraints incomputation costs, we limited the search algorithm to amaximum of 5 paths. Spot checks using differentknowledge weight combinations showed that less pruningimproved results only for a very limited number ofexamples, and even then only marginally. \nSimply taking the strongest F0 at every time frame can\nbe seen as baseline performance, since no knowledge isinvolved. As with the path finding at this stage, no attemptis made to determine sections where the solo instrument ispresent or absent, and we therefore only use sectionswhere the solo instrument is present for evaluation. Usingthe same set of weights for all examples, the correct F0 ofthe melody path was found in on average 63% of frames,ranging from 34% in the worst case to a best case of 85%.Compared to the baseline of taking the strongest F0 inevery frame, the additional knowledge integration leads toan average increase of 11%. Additionally, even for pieceswhere the improvement in terms of percentage correctframes by the path finding algorithm is only small, theoverall melody contour is significantly smoother. The\nF0 candidates flute clar. oboe violin cello average\n 1 (strongest) 70% 78% 48% 28% 38% 52%\n 3 82% 94% 78% 61% 64% 76%\n15 98% 99% 98% 94% 84% 95%\nTable 1: Frames with correct F0 estimates (based only on \nsections where the solo instrument is present), with the \nnumber of F0 candidates in rows and the different \ninstruments in columns.removal of short tones leads only to a very small increase\nin an overall rating of correct frames, but is not unlikely tohave a more significant influence perceptually. Thisincreased smoothness of the melody contour is moreobvious in a tone based evaluation, where the number ofadditional tones is reduced by more than half compared tothe baseline performance based on F0 strength only. \n3.4. Solo Instrument Present/Absent\nAfter estimating the most likely path for the melody, it is\nstill necessary to distinguish between sections where thesolo instrument is playing or silent. The path findingalgorithm allows for silent states, but in its current staterepeatedly follows the accompaniment when the soloinstrument is absent for more than a few frames. In theexamples used here, the solo instrument is present moreoften than silent (74% present, 26% silent), and since thisis likely to be the case with many musical pieces, it mightbe favourable to choose a setting that favours the presentstate for best overall results. \nDifferent smoothing intervals from 0 to 10 sec and\ndifferent minimal durations of 1 to 30 sec for the presenceof the solo instrument were investigated. The differentsettings had only small influences, unless the smoothinginterval was very short and the minimal presence long, inwhich case the results deteriorated and too many frameswere assigned to the silent state. The best parametersetting for overall accuracy uses a smoothing interval of 2sec (86 frames) and a minimum present time of 10 sec(431 frames). With this setting, 77% of frames wereassigned to the correct state (65% if normalised by therelative number of silent compared to present frames).The path finding search alone already assigns the silentstate more often during sections where the solo instrumentis actually silent. Using the explicit estimation of silentframes improved the accuracy for silence estimation from24% using only the path based search to 39% correctlyassigned silent frames. But as silent frames form onlyaround one quarter of all frames, the improvement for theoverall recognition accuracy was only 3% when evaluatedon a frame-based basis. Using a tone-based evaluation anadditional 16% of superfluous tone were suppressed usingthe explicit search for silent sections (see Table 2).\n3.5. Realistic Recordings\nTwo short excerpts from realistic recordings – the\nbeginnings of Mozart’s clarinet concert and a violinsonata by Bach – were manually annotated with thecorrect F0s. Both were pieces which were also included in\nthe main MIDI based evaluation, so that a directcomparison is possible. \nUsing the same knowledge weight combination that\nwas used for the MIDI generated data, F0s were correctlyidentified in 56% of frames for the violin example, a resultas good for the realistic recording as for the equivalentsection of MIDI based audio. Accuracy for the clarinetexample was 76% correctly labelled frames, around 5%higher for the realistic recording than the MIDI generatedexample. All parameters used within the system wereoptimised on MIDI generated data, and differentparameter combinations might be favourable for realisticrecordings. Adjusting the parameters for the realisticrecordings led to 10%-15% more correctly labeledframes, but it seems premature to draw furtherconclusions from only two examples.\nEven though the realistic examples were limited,\nresults seem very promising. Although additionaldifficulties can normally expected in realistic recordings,such as reverberation and stronger vibrato, accuracieswere if anything higher compared to the MIDI generateddata (see Figure 2).\n4.   Conclusion and Future Work\nUsing the strongest F0 at every frame without furtherprocessing can be seen as a baseline performance, and ledto an accuracy of 40% correctly estimated F0s on a frame-by-frame basis with no distinction between sectionswhere the solo instrument is actually present or only theaccompaniment is playing. The proposed systemimproved results to an accuracy of 54% and also led tooverall smoother melody contours, reducing the numberof spurious tones to nearly a third compared to thebaseline performance.\nA relatively weak point of the system was the\nestimation of the presence or absence of the soloinstrument, which is likely be caused by the low resultsfrom the instrument identification process. Since wepreviously obtained better results for instrumentidentification using realistic recordings, it can besuggested that some of the difficulties encountered wereat least to some extent caused by the MIDI generated testdata, and not the system. This finding is supported by theoverall better results obtained for two short excerpts fromrealistic recordings. While MIDI generated audio has thestrong advantage of automatically providing a ‘true’solution which can be used for evaluation purposes, itdoes not necessarily provide a good evaluation basis interms of the acoustics generated.\nAdditional knowledge sources which could benefit the\nsystem might include an instrument recognition modeltrained on accompaniment. Instead of using only thelikelihood that a specific F0 was produced by the soloinstrument, this could be weighted against the likelihoodthat the same F0 was produced by the accompaniment.Higher level knowledge in terms of short melodic phrases,\nstrongest F0 path path+silence\ncorrect frames 40% 51% 54%\ntones found 78% 76% 72%\nspurious tones 321% 135% 117%\nTable 2: Summary MIDI based evaluation (based on \ncomplete examples).which are learned during the analysis of an example,\nmight also help to more robustly identify recurring motifsand phrases. Expanding the instrument basis, especially toinclude the singing voice, would certainly be interesting.It would also lead to a wider range of music that could beanalysed, as pop and rock music is often dominated by avocalist. \n5.   References\n[1] Arulampalam, M.S., Maskell, S., Gordon, N. &\nClapp, T. “A tutorial on particle filters for onlinenonlinear/non-Gaussion Bayesian tracking”, IEEE\nTransactions on Signal Processing, Vol. 50, No. 2,pp. 174-188, 2002\n[2] Berenzweig, A.L., & Ellis, D.P.W. “Locating\nsinging voice segments within music signals”,Proceedings of the IEEE Workshop on Applicationsof Signal Processing to Audio and Acoustics, 2001\n[3] Classical Music Archives, \nwww.classicalarchives.com\n[4] Eggink, J. & Brown, G.J. “A missing feature\napproach to instrument identification in polyphonicmusic”, Proceedings of the IEEE International\nConference on Acoustics, Speech and SignalProcessing, pp. 553-556, 2003\n[5] Eggink, J. & Brown, G.J. “Instrument recognition in\naccompanied sonatas and concertos”, Proceedings of\nthe IEEE International Conference on Acoustics,Speech and Signal Processing , in print, 2004\n[6] Goto, M. “A predominant-F0 estimation method for\nCD recordings: MAP estimation using EM algorithm\nfor adaptive tone models”, Proceedings of the IEEE\nInternational Conference on Acoustics, Speech andSignal Processing, pp. 3365-3368, 2001\n[7] Klapuri, A.P. “Multiple fundamental frequency\nestimation based on harmonicity and spectralsmoothness”, IEEE Transactions on Speech and\nAudio Processing, Vol. 11, No.6,  pp. 804-816, 2003\n[8] Meek, C. & Birmingham, W.P. “The dangers of\nparsimony in query-by-humming applications”,Proceedings of the International Conference onMusic Information Retrieval,  2003[9] Raphael, C. “Automatic transcription of piano\nmusic”, Proceedings of the International Conference\non Music Information Retrieval,  2002\nAcknowledgements\nJana Eggink acknowledges the financial support provided\nthrough the European Community’s Human PotentialProgramme under contract HPRN-CT-2002-00276,HOARSE. Guy J. Brown was supported by EPSRC grantGR/R47400/01 and the MOSART IHP network.50 100 150 200 250 3004008001600\n0 50 100 150 200 250 3004008001600\nFigure 2: Analysis of a realistic recording (the beginning of Mozart’s clarinet concerto), \nshowing manually annotated F0s (gray) and estimated melody (black).a) Melody based on strongest F0 b) Melody based on knowledge integrating path findingF0s (Hz)\ntime (frames)F0s (Hz)\ntime (frames)"
    },
    {
        "title": "Eigenrhythms: Drum pattern basis sets for classification and generation.",
        "author": [
            "Dan Ellis",
            "John Arroyo"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415948",
        "url": "https://doi.org/10.5281/zenodo.1415948",
        "ee": "https://zenodo.org/records/1415948/files/EllisA04.pdf",
        "abstract": "We took a collection of 100 drum beats from popular music tracks and estimated the measure length and down- beat position of each one. Using these values, we normal- ized each pattern to form an ensemble of aligned drum patterns. Principal Component Analysis on this data set results in a set of basis ‘patterns’ that can be combined to give approximations and interpolations of all the ex- amples. We use this low-dimension representation of the drum patterns as a space for classification and visualiza- tion, and discuss its application to generating continua of rhythms. Our classification results were very modest – about 20% correct on a 10-way genre classification task – but we show that the projection into principal compo- nent space reveals aspects of the rhythm that are largely orthogonal to genre but are still perceptually relevant. Keywords: rhythm, genre, classification, principal compo- nents",
        "zenodo_id": 1415948,
        "dblp_key": "conf/ismir/EllisA04",
        "keywords": [
            "drum beats",
            "measure length",
            "down-beat position",
            "normalized patterns",
            "ensemble of aligned",
            "principal component analysis",
            "basis patterns",
            "low-dimensional representation",
            "genre classification",
            "perceptually relevant"
        ],
        "content": "EIGENRHYTHMS: DRUM PATTERN BASIS SETS FOR\nCLASSIFICATION AND GENERATION\nDaniel P.W. Ellis and John Arroyo\nLabROSA, Dept. of Elec. Eng., Columbia University, NY NY USA\ndpwe@ee.columbia.edu, ja2124@columbia.edu\nABSTRACT\nWe took a collection of 100 drum beats from popular\nmusictracksandestimatedthemeasurelengthanddown-\nbeatpositionofeachone. Usingthesevalues,wenormal-\nized each pattern to form an ensemble of aligned drum\npatterns. Principal Component Analysis on this data set\nresults in a set of basis ‘patterns’ that can be combined\nto give approximations and interpolations of all the ex-\namples. We use this low-dimension representation of the\ndrum patterns as a space for classiﬁcation and visualiza-\ntion, and discuss its application to generating continua of\nrhythms. Our classiﬁcation results were very modest –\nabout 20% correct on a 10-way genre classiﬁcation task\n– but we show that the projection into principal compo-\nnent space reveals aspects of the rhythm that are largely\northogonal to genre but are still perceptually relevant.\nKeywords: rhythm,genre,classiﬁcation,principalcompo-\nnents\n1. INTRODUCTION\nPopular music usually includes a drum ‘track’ providing\ntherhythmicbackboneofthepiece,andthepercussionin-\nstrumentsgenerallyplayashortpatternthatrepeatsevery\nfew beats. This core pattern, along with the rate at which\nit is played (typically measured in beats per minute, or\nBPM)constituteakeyelementinthecharacterofthemu-\nsic.\nWe are interested in describing and extracting such es-\nsentialsubjectivecharacteristicsfrommusicaspartofour\nwider project into music similarity and recommendation\n[1,2]. Ourpreviousworkhasfocusedonaveragespectral\ncontent,pitches,andchords[12,14],buthasnotincluded\nexplicit rhythm-related features. This paper describes an\ninitialstudyintoextractinganddescribingthiskindofin-\nformation.\nManypreviousmusicinformationretrievalsystemshave\ntapped the rhythm dimension. Tzanetakis et al. employ a\nsmallsetofrhythmdescriptorsincludingBPMand“rhythm\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2004 Universitat Pompeu Fabra.strength”[15],andGouyonetal. showthevalueofcosine\ntransform coefﬁcients of a time-warped log-inter-onset-\ninterval histogram [6] for classifying dance music genres.\nThere is, however, a gulf between the very large range of\npossibledrumpatterns–spanningvariationsinbasicnote\npatterns,accent,andsmalltimeshifts(“swing”)–andthe\nsmallnumberofdimensionsdesirableinclassiﬁcationand\nbrowsing systems.\n2. EIGENRHYTHMS\nWe propose to bridge this gulf using the standard dimen-\nsionality reduction tool of Principal Component Analysis\n(PCA)[3]. Anensembleofdatathatcanberepresentedas\npoints in a high-dimensional space can be approximated\nas the weighted sums of a few basis vectors in that space;\nthe covariance matrix of the ensemble provides informa-\ntion about which dimensions are correlated (i.e. exhibit\nco-ordinated changes), and by ﬁnding the eigenvectors of\nthe covariance matrix with the largest eigenvalues PCA\nﬁnds the basis functions that minimize the distortion of a\nlower-dimensional representation. Each point in the orig-\ninal high-dimensional space is represented by a smaller\nnumber of coefﬁcients, which are the weights applied to\neach of the principal component vectors to approximate\nthat point. Individual principal components, ordered ac-\ncording to their contribution to the overall distortion, can\nbe interpreted as the main dimensions of variation among\nthe examples in the set.\nIn this work, we represent drum patterns as a simple\ntwo-dimensionalsurface. Thehorizontaldimensionistime,\ndenselysampledtoprovideaﬁneresolutionofdrum-note\nevents (for the results below, we used 5 ms sampling).\nTheverticaldimensioncorrespondstothedifferentinstru-\nments: we caricature popular music drum tracks as con-\nsisting of three instruments, bass drum, snare, and hi-hat,\nand have one row for each. The values in this surface are\npseudo energy envelopes: each beat event is represented\nby a brief, decaying pulse in the surface. We use half-\nGaussians with a standard deviation of 20 ms, which, un-\nlikesingleimpulses,canglossoversmallamountsofjitter\nin the timing of individual beats, while retaining a sharp,\nwell-deﬁned onset. The principal components of these\nsurfaces, the two-dimensional surfaces that can be com-\nbined to approximate the entire set, constitute our “eigen-\nrhythms”.Our goal is to produce systems that can be applied to\nactual recordings, but to simplify the investigation of un-\nderlying rhythmic information we sidestepped the stage\nof extracting drum events from audio by working directly\nfromMIDIﬁlesinwhichnoteeventtimesanddrumvoice\nidentities are provided explicitly. We do not consider this\na serious limitation given the success reported in auto-\nmatic drum beat extraction from audio: Real-time extrac-\ntion of bass drum and snare events was reported by Goto\ntenyearsago[5],andmorerecentworkhasincludedadap-\ntivelearningofthedifferentdrumsounds[16]amongmany\nother reﬁnements.\nOur principal motivation is to investigate the viability\nof low-dimensional descriptions of rhythm patterns, but\nin order to motivate and evaluate our results we apply the\nrepresentations to a genre classiﬁcation task. However,\nwe do not pay much attention to making a careful and\ncomprehensivedescriptionoftherhythmicpatternineach\nindividual piece: Since we are interested in the gross be-\nhavior of a large collection of different drum patterns, the\nmain thing we want is a large number of diverse patterns\nextracted from different peices. If, for a particular piece,\nwe only extract one of several patterns used, or even if\nour extraction algorithm fails and returns a nonsense pat-\ntern, it is not of great concern as long as the bulk of the\ndatabase – the material that the most signiﬁcant principal\ncomponentsdescribe–isvalid. Thisisanotherreasonfor\nstarting with MIDI: there are tens of thousands of MIDI\nﬁles readily available on the internet, and we would like\ntobeabletomodelrhythmicinformationfromcollections\nofthisscalewithoutthecomputationalloadofprocessing\nan equivalent amount of audio. By limiting ourselves to\npullingthepatternfromasingle,smallexcerpttakenfrom\nthemiddleofeachpiece,wealsoavoidtheissueoftempo\ntracking – we assume the tempo is constant over the 10 s\nexcerpts we use, and estimate a single value.\nThe next section describes in more detail our method\nforﬁndingthe“eigenrhythm”drumpatternprincipalcom-\nponents. Section 4 presents the results of our preliminary\napplication to 100 pieces, giving both the eigenrhythms\nand describing the classiﬁcation experiments. We discuss\nsome other possible applications, including resynthesiz-\ning patterns from arbitrary points in rhythm space, and\npresent our conclusions in section 5\n3. METHOD\nTo apply PCA, we must generate a collection (ensemble)\nofdrumpatternswherecorrespondingbeatsarealignedin\neachitem. Todothis,wemustestimatethepatternlength\nineachoriginaldrumtrack(i.e. itsBPM),andtheposition\nof one reference time point i.e. one pattern-initial down-\nbeatwithintheexcerpt. Giventhesevalues,wecanextract\naﬁxednumberofbeats,startingatadownbeat,fromeach\ntrack, stretch or compress them to a single nominal BPM\nof 120, to form a single entry in our data matrix. The ﬁrst\nstep, however, is to convert the raw MIDI data into our\nbasic drum pattern time-channel surfaces.\nAutocorrelation and peak periods\nOriginal pattern compressed 98 →120 BPM\nCross-correlation of reference against scaled patternOriginal drum pattern (train/hiphop/nEpisode)\nReference pattern (120 BPM)\nExtracted patterntime / sec 123456789\ntime / sec 12345678998 BPM\n49 BPM 32 BPM\n25 BPMBDSNHH\nBDSNHH\nBDSNHH\nBDSNHHFigure1. Tempoestimationanddownbeatdetection. The\ntop pane shows the pattern extracted from an MIDI ﬁle\nmapped to bass drum, snare, and hi-hat; darker gray in-\ndicates a more intense beat; dots above indicate hand-\nmarked downbeats. The second pane shows the autocor-\nrelation of this pattern, with the four highest peaks cir-\ncled and labeled with their equivalent BPMs. Below that\nis the reference pattern (a grand average of aligned pat-\nterns), which is cross-correlated against the original pat-\ntern rescaled to 120 BPM (in this case, assuming the 98\nBPM peak is valid) to give the ﬁfth pane. The largest\npeak in this cross-correlation gives the downbeat hypoth-\nesis,andleadstotheextractedpatterninthebottompanel\nwhich then becomes part of thealigned pattern ensemble.\n3.1. Preprocessing of MIDI data\nWeusepublicly-availabletoolstoreadGeneralMIDIﬁles\n(GM) culled from the internet into Matlab. In the GM\nstandard,channel10isdevotedtodrumsounds,witheach\nMIDInote,normallyusedtospecifythedifferentpitches,\ncorresponding to a different pre-deﬁned drum sound. We\nbuilt a map to convert the 85 common voices to our three\nclasses: bass drum, snare, or hi-hat. The vast majority\nofpopularmusicdrumpatternsconsistonlyofthesethree\nvoices. Tom-toms,crashcymbals,andotherexoticsounds\nwere discarded (mapped to null). The MIDI velocity pa-\nrameters,whichcanbeusedtoconveyamplitudeaccents,\nwere ignored in this work, as were the note offset times.\nInstead,eachonsettimeresultedinashort,decayingenve-\nlopeelementbeingaddedintoappropriatevoice’soverall\ntime envelope, sampled at 200 Hz. In lieu of a more so-\nphisticated approach, we initially extracted 10 s of drum\ntrack starting 30 s into each GM ﬁle. An example of such\na pattern (from a MIDI replica of “The Next Episode” by\nDr. Dre) is shown in the ﬁrst pane of ﬁgure 1.\n3.2. Pattern period estimation\nScheirer [11] contrasts zero-phase autocorrelation tempo\nperiod estimators with his bank of resonators which indi-\ncate both the dominant period and the timing of energypeaks within each channel. However, because we wish\nto use a more complex approach to downbeat detection,\nwe can use simple autocorrelation to ﬁrst obtain several\nperiodestimates,leavingthedownbeatidentiﬁcation(and\nchoice among the period estimate) to a subsequent stage.\nThe second pane of ﬁgure 1 shows the positive-lag half\nof the autocorrelation of the extracted drum-pattern sur-\nfaceshownintheﬁrstpane: eachofthethreetracks(bass\ndrum, snare and hi-hat) ﬁrst has its total energy equalized\n(to reduce the inﬂuence of the most active voice, usually\nthe hi-hat), then their individual autocorrelations are sim-\nplysummed. Inthisexample,weseethestrongestpeakat\na lag of around 1.1 s, corresponding to 98 BPM. The next\nhighest peaks are the higher-order multiples at 2, 3 and 4\ntimes this basic period.\nIn this case, the 98 BPM peak corresponds to the sub-\njective period for this pattern, but in general, the highest\npeakisnotalwaysthebestperiod,andtheremaybestrong\npeaks at subdivisions as well as integer multiples of the\nkey period. We choose among these by considering each\nof the Nhighest peaks from the autocorrelation (where\nN= 4in the results presented here), and keeping the pe-\nriod that gives the highest normalized cross-correlation in\nthe downbeat estimation, described next.\n3.3. Downbeat Location\nTo get sensible results from PCA, the different patterns\nin our ensemble must not only have the same tempo, but\nmustbesomehow‘linedup’tohaveequivalentbeatsatthe\nsame time. Although this concept is not well-deﬁned, in\nmany cases it is possible to identify a particular point in a\nloopingdrumpatternasthe‘beginning’,andourgoalisto\nlocate this point. Regardless of its interpretation, we need\nsomewaytochooseauniqueanchorpointineachpattern:\nifourensembleincludesanarbitrarycirculartimeshiftto\neach pattern, the principal components will be meaning-\nless.\nOur approach is to deﬁne a reference pattern, consist-\ning of some simpliﬁed version of what we are hoping to\nﬁnd, and to cross-correlate this template against the in-\nput patterns once their tempo has been normalized. If the\ninput pattern contains that exact subsequence, the cross-\ncorrelation will peak at the time-skew that aligns them.\nEven if the ideal pattern does not occur exactly in the in-\nputpattern,thehighestpeakinthecross-correlationshows\nthe time offset within the longer segment that begins the\nsegment with greatest similarity to the reference pattern,\nwhichisanunambiguousanchorpoint,andgivesusanap-\npropriate alignment of ‘maximum similarity’ for extract-\ning a segment to use for the PCA.\nFor each of the Nperiod hypotheses extracted from\nautocorrelation,weﬁrsttime-scaletheoriginalMIDIdata\nsothat,ifthehypothesisiscorrect,thenewnotesequence\nwill be at 120 BPM, the tempo of the reference pattern.\nAfterﬁndingthecross-correlationpeakforthesurfacede-\nrivedfromthattime-compressedor-stretchedversion,we\nmake a note of the peak cross-correlation value as well as\nthe time offset where it occurs. We normalize the cross-correlationbytheenergyoftheinputpatternwithinaslid-\ningwindowofthesamelengthasthereferencepattern,so\nthe cross-correlation values are always correctly normal-\nized and can reach unity only when reference and input\nexactlymatch. Wecalculatethecross-correlationonlyfor\npoints where there is full overlap between the short refer-\nence pattern and the longer scaled input pattern.\nWe then choose among the BPM hypotheses the one\nthatgavethehighestpeakcross-correlationvaluei.e.,from\namong the period hypotheses suggested by the autocorre-\nlation, the temporal scaling of the original input pattern\nthat results in a pattern most similar to the reference pat-\ntern appearing. Over-estimates of the original pattern’s\nperiod (i.e. picking the 49 BPM peak in the example)\nwill compress more points into the ﬁxed-length segment\nin the temporally-scaled pattern; while this may lead to\nmore overlap with the peaks in the reference pattern, the\nextrainputnoteswillleadtoahighaverageenergy,sothe\nnormalized cross-correlation value will be hurt. Period\nestimates that are too short will have normalized versions\nthat are too stretched out in time and are unlikely to have\nenough points in common with the reference to achieve\na high cross-correlation. Thus, the cross-correlation ﬁnds\nthe downbeats and chooses the best-matching tempo esti-\nmate in a single stage.\nThe reference template we use is actually the average\nofallthenormalizedpatternsemergingfromouranalysis,\nbut there is a circularity because we need to perform the\ndownbeat alignment before we can calculate this average.\nTo bootstrap, we took a very simple prototype pattern, al-\nternating bass drum and snare with an eighth-note hi-hat\npulse,thensuccessivelyalignedourpatterns,formedtheir\naverage, and re-calculated the downbeat positions using\nthis new average as reference. Once the downbeat posi-\ntions match in two successive iterations, the system has\nconverged and there will be no further changes in later it-\nerations. We observed convergence within 5 cycles.\nThe grandaverage referencepattern templateis shown\nin the third pane of ﬁgure 1, along with one of the time-\nscaled drum patterns, in this case for the correct 98 BPM\nhypothesis; the ﬁfth pane shows the results of cross cor-\nrelation, with the top 10 peak values circled; for now, we\nconsider only the top value in the cross-correlation and\nusethatasourdownbeat,assumingthatitgivesthelargest\npeak value across all the BPMs being considered.\nFinally,weextractashortsegmentfromthe120BPM-\nscaledinputpatterns,correspondingtothe4-beatsegment\nof the reference template, and pass this forward to the\nprincipalcomponentanalysis. Wetakefourbeatsbecause\n2beats(e.g. asinglebassdrum/snarealternation)seemed\ntoo short to capture much interesting structure in the pat-\ntern;afterreviewingthetrainingexamples,manyofwhich\ncontain 8- or 16-beat basic patterns, there could be good\nreason to use a longer excerpt, although this might neces-\nsitate a lower temporal resolution to our surfaces in order\nto keep our PCA computationally tractable.3.4. Principal Component Analysis\nThe processing so far gives, for each input drum track,\none 2 s excerpt of the rhythm pattern after normalization\nto 120 BPM (i.e. four beats in total), starting at a down-\nbeat deﬁned by the best alignment to a reference rhythm.\nThree voices at a sampling rate of 200 samples per sec-\nond for 2 seeconds gives a 1200 point feature for each\npiece. Westackthevectors foreachofour examples,cal-\nculateandsubtractthemeanpattern(whichisjusttheref-\nerence pattern used in extraction, once the analysis has\nconverged)andapplysingularvaluedecompositiontothe\ncovariance matrix of this data to ﬁnd the eigenvectors, as\ndescibed in section 2. In our experiments, we used just\n100 MIDI tracks, giving a maximum of 99 nonzero eigen\ndimensions, although our goal is in using many fewer di-\nmensions than this to get at the ‘essence’ of the rhythms.\nThe projection of each rhythm pattern into a subset of the\nmost signiﬁcant principal components provides for clas-\nsiﬁcation (e.g. by nearest neighbor), and the space pro-\nvides interesting interpolations; by compressing the di-\nmensionality to maximally preserve the structure in the\nreal rhythms, we have a space where unnatural rhythms\nmost likely cannot be represented, and all points corre-\nspond to reasonable-soundingrhythms.\n4. RESULTS\nFor our tests, we collected a set of 100 MIDI tracks, ar-\nrangedas10examplesforeachof10genres–blues,coun-\ntry, disco, hiphop, house, new wave, rock, pop, punk, and\nrhythm & blues – roughly based on contemporary popu-\nlar music radio, and, according to our intuitions, deﬁning\nclasses that might possibly distinguished by their rhythm\npatterns. Weveriﬁedthateachoftheﬁlesweselectedwas\na well-produced replica and a satisfactory representative\nof its class, but did not use any more speciﬁc criteria in\nselecting them.\nTo evaluate the raw tempo and downbeat extraction,\nwe auditioned each tracking result by resynthesizing the\noriginal drum pattern along with added tone pips indicat-\ning the system’s chosen downbeats and cycle length. In\ntwo of the cases the arbitrary initial note extraction re-\nturned irregular drum patterns for which no period could\nbe decided. In nine of the remaining 98 cases (9.2%),\nthe period chosen by the system was wrong, almost al-\nways half the length (i.e. tempo twice as fast) as the per-\nceived period. Where the tempo was correct, about half\nthe tracks had patterns of 4 or 8 beats (rather than the ba-\nsic2-beatbass/snarepattern),andofthose,approximately\nhalf (25 out of 53) had the downbeat in the right point\nwithin that sequence. In the others, the downbeat was\nshifted by an even number of beats. This is a secondary\nerror, since the extracted pattern was basically appropri-\nate, but it would make for a better interpolation space if\nthe automatic downbeat placement could come closer to\nsubjectiveimpression. We return to thisin thediscussion.4.1. Classiﬁcation task\nThe 100 extracted patterns, each represented by a 1200-\npointenvelope,werethenfedtoPCAtoextracttheeigen-\nrhythms;themeanpatternandtopﬁveeigenrhythmbases\nare illustrated in ﬁgure 2; 25 dimensions are required to\nexplain 90% of the variance. Restricting ourselves to the\ntopNeigenrhythms gives an N-dimensional projection\nof the set of rhythms that minimizes squared-error distor-\ntion, and which can also be seen as a kind of generaliza-\ntion, smoothing away ‘insigniﬁcant’ differences between\npatterns. This reduced space can be used for classiﬁca-\ntion, for instance by treating each of the patterns in turn\nas unknown and classifying it on the basis of its knear-\nest neighbors ( k-NN classiﬁcation). Although this is not\nas good a predictor of classiﬁer success as using test data\nseparate from the data used in deriving the eigenrhythm\nspace, we note that the genre labels were not involved\nin that stage i.e. the PCA ‘model’ does not encode prior\nknowledge of the true class of the test examples.\nWe performed this classiﬁcation and searched over the\nnumberofPCAdimensions(from2to40)andthenumber\nofneighborstouseinthe k-NNclassiﬁcation. Ourresults\nweregenerallyweak;oneofthebestperformingcombina-\ntions was the simple case of using 4 dimensions and clas-\nsifyingaccordingtothesinglenearestneighbor. Theover-\nall classiﬁcation accuracy of 21% is signiﬁcantly better\nthanrandomguessing(whichwouldgive10%),andacon-\nfusionanalysisrevealssomecliquesofgreaterdiscrimina-\ntion,including {country,blues }and{rock,hiphop,punk }.\nRhythm&bluesisrecognizedcorrectly4outof10times,\nwhich is also how often disco is recognized as house. All\nofthesedetailsseemtomakesenseinviewofthemusical\ncharacter of the different classes.\n4.2. Eigenrhythms\nFor a greater insight into the classiﬁcation performance,\nand to see what the “eigenrhythm” concept has actually\ncaptured, it is interesting to look at the top eigenrhythms\nindividually, as in ﬁgure 2. The top-left panel shows the\noverall mean pattern, which is subtracted from every pat-\ntern prior to the eigen analysis; this pane is nonnegative,\nwithwhiteshowingregionsofnoenergyanddarkershades\nof gray indicating progressively more intense beats. The\nremainingpatternshave,ingeneral,bothpositiveandneg-\nativeportions,andcanbeassociatedwithpositiveorneg-\native weights to add to or subtract from different beats in\nthe mean pattern – i.e. increasing or reducing the con-\ntrast between their positive and negative extrema. For\ngrayscale presentation, positive values tend to black, neg-\native to white, with zero a mid-gray; a colored version is\nincludedin[4]. Eigenrhythm1ismainlypositive,follow-\ning the mean pattern, but includes emphasis of the 16th\nnotes(atsamples25,75,125etc.) inthesnareandhi-hat,\nthesnarebeatsontheeighthnotesatsamples50,150and\n350, and the bass drum simultaneous with the main snare\nbeatsatsamples100and300(quarternotebeats2and4).\nEigenrhythm 2 provides contrast between the eighth-Mean pattern\nBDSNHH\nBDSNHH\nBDSNHHBDSNHH\nBDSNHH\nBDSNHHEigenrhythm 1\nEigenrhythm 2 Eigenrhythm 3\nEigenrhythm 4\n50 00 samples (@ 200 Hz)\nbeats (@ 120 BPM)100 150 200 250 300 350 400\n1234 1234Eigenrhythm 5\n50 100 150 200 250 300 350-0.100.1Figure 2. Mean drum pattern and top 5 eigenrhythms.\nnote hi-hats, and the snare and bass hits on beats 2 and 4\nalong with a snare eighth-note ‘echo’ at samples 150 and\n350. Thethirdbasiscontrastshi-hatbeatsonthe16thnote\n‘off’beatswithasimplequater-noterhythm,soanegative\ncoefﬁcient here will introduce a double-speed hi-hat, and\na positive weight gives half-speed. Basis 4 contrasts off-\nbeat bass drums at samples 50, 150, 250 and 350 with\nhi-hat beats alongside the main snares at beats 2 and 4, as\nwell as including some fast snare beats between samples\n200 and 300 and some evidence of 6/8 patterns with hi-\nhat features around sample 67, 167, 267 etc. (i.e. two-\nthirds of the way through each quarter-note beat). The\nﬁnaleigenrhythmintheﬁgurecontrastssnareandbasson\nbeats 2 and 4, and also can be seen to provide for more\ncomplex structures in hi-hat and bass drum.\nOverall,webegintoseethattheindividualeigenrhythms\nare encoding particular features of the different rhythmic\npatterns, more about the character of the piece than its\ngenre, but at the level of groups of notes rather than indi-\nvidualevents. Althoughgenreisonlyweaklypredictedby\nnearestneighborsintheeigenspace,wecanaskifthereare\nother perceived properties being preserved. For instance,\nplotting all our tracks on a 2D surface deﬁned by the ﬁrst\ntwo eigenvectors reveals two signiﬁcant clusters (this ﬁg-\nure can be seen in [4]). Listening to tracks from each, we\nﬁndthattheﬁrstclusterconsistsofstraight-ahead4/4pat-\nternswitheighthnotehi-hatpatterns,snareonbeats2and\n4, and some variations in the bass drum within the basic\neighth-note grid. Patterns in the second cluster, by con-\ntrast,havethesamebasicbassdrumonbeats1and3with\nsnareonbeats2and4,buthavea6/8‘syncopated’rhythm\nin the hi-hat, or a simple quarter-note hi-hat beat. Thus,\nwe ﬁnd that the eigenrhythm space does indeed cluster\ndrum patterns with clearly discernible perceptual similar-\nities.\n5. DISCUSSION AND CONCLUSIONS\nOurapproachcanbecontrastedwithotherworkthatlooks\nat the detail of rhythm patterns. Laroche has presented a\nsystem able to extract ‘swing’ represented by slight sys-\ntematic timing shifts of beats 2 and 4; his approach en-\ncodes more musical knowledge (with a correspondingly\nnarrower applicability) than we wished to use [8]. Paulusand Klapuri present a system for comparing the rhythm\npatterns between two different pieces, overcoming varia-\ntions in drum sounds with cepstral normalization and mi-\nnortimingdifferencesthroughdynamictimewarping,but\nthey do not attempt to build a parametric space of rhyth-\nmic variants [10].\nOne natural application for the eigenrhythm represen-\ntation is for the generation of rhythm patterns that inter-\npolate between the different points in eigenrhythm space.\nWe have built a crude Matlab interface to synthesize the\nrhythm patterns corresponding to any value of the ﬁrst\neight eigenrhythms allowing us to investigate the space,\nbut we hope to produce something a bit more interactive.\nIn evaluating our current system, it became clear that\nthe 2 s excerpts used in modeling were too short. Many\ntrackshaddrumpatternsthatrepeatedatalargerscalethat\nthis,andthesystemhadlittlechanceofﬁndingtheappro-\npriate downbeat within such patterns when only part of\nthecycleismodeled. Evenso,thedownbeatdetectionap-\npears to require a more sophisticated approach. Lacking\nsome absolute principle to decide where the cycle starts,\nwe believe that our technique of matching a model de-\nrived from actual data is the right basic approach, but it\nmayneedtobe seededwithground-truthonactualdown-\nbeatsforatleastsomeofthetrainingexamples,andcould\nrequire a family of prototype patterns (e.g. ﬁnding the\ntemporal alignment that supports the best ﬁt from a set\nof eigenrhythms) rather than relying on a single, average\ntemplate.\nAs noted above, our initial interest was simply to col-\nlect a large body of drum patterns to see what the princi-\npal components would be like. However, a more careful\nmusical information extraction technique would consider\ntheentiredrumtrackofapiece,lookingfortheregularly-\nrepeatingpatternsandperhapsalsomodelingthelessrepet-\nitive breaks and ornamentations. We think the eigen anal-\nysis should also be applicable to drum breaks, if they can\nbe effectively extracted, although because their duration\nislessconstrainedsomekindofsequentialstructure(such\nas a hidden Markov model) might be appropriate. One\ncould imagine a Markov model where each state is rep-\nresented by values or a distribution in eigenrhythm space,\nand transition probabilities encode the likely evolution of\nthe entire piece.There are many details even in the work we have de-\nscribed that deserve closer examination. Where we have\ninvestigated alternatives at all our main metric has been\nthe genre classiﬁcation accuracy, which is so low as to\nbe suspect and doesn’t directly address our main interest\nof deﬁning a space of ‘good’ drum patterns. One idea\nis to replace the asymmetric envelopes used to represent\neach drum event with a smoother shape like a full Gaus-\nsian. This might allow small time shifts (like Laroche’s\n‘swing’) to be effectively encoded by eigenvectors that\ncanperformalinearcross-fadebetweentwonearbypeaks.\nIf, on the other hand, we wished to pursue the genre\nclassiﬁcation application, we could look at more discrim-\ninative ways to deﬁne our basis functions, such as us-\ningLinearDiscriminantAnalysis(LDA)inplaceofPCA.\nLDA is another procedure for ﬁnding a low-dimensional\nprojection of a dataset, but it uses class labels associated\nwith each training pattern to ﬁnd projections that maxi-\nmally separate classes, to be the most useful in classiﬁca-\ntion [3].\nThereareseveralotherinterestingprojectionalgorithms\ntoconsider. IndependentComponentAnalysis(ICA)ﬁnds\nbasis projections that are not orthogonal but which max-\nimize the statistical independence of the projected coefﬁ-\ncients,whichcanbeamoresemanticallyrelevantdecom-\nposition [7]. Non-negative Matrix Factorization (NMF)\nﬁnds linear basis sets where all the coefﬁcients are pos-\nitive or zero, so each pattern is approximated by a pro-\ncess of ‘adding in’ parts, rather than the balancing con-\ntrasts seen in our eigenrhythms [9]. When the underlying\ndatasetisintrinsicallynonnegative,asinourcase,thiscan\nbe an interesting alternative transformation; some previ-\nous applications to musical audio are reported in [13].\nIn conclusion, we have introduced a new representa-\ntion for the complex but constrained class of popular mu-\nsic drum patterns, and derived our basic eigenrhythm pat-\nternsbyscalingandaligningacorpusofdrumtracksfrom\nreal pieces, encoded as MIDI ﬁles. We hope to use larger\ndatasetsandadeeperanalysistocomeupwithamoregen-\neral model of the stylistic variations in rhythm patterns,\nand we hope to be able to train from, and apply to, actual\nrecorded waveforms.\n6. ACKNOWLEDGMENT\nThis work was supported in part by the NSF under grant\nno. IIS-0238301, and by a grant from Google. Any opin-\nions, ﬁndings and conclusions or recommendations ex-\npressed in this material are those of the authors and do\nnot necessarily reﬂect theviews of the sponsors.\n7. REFERENCES\n[1] A.Berenzweig,D.P.W.Ellis,andS.Lawrence. An-\nchor space for classiﬁcation and similarity measure-\nment of music. In ICME 2003 , 2003.\n[2] A. Berenzweig, B. Logan, D. P. Ellis, and B. Whit-\nman. A large-scale evalutation of acoustic and sub-jectivemusicsimilaritymeasures. In Proc.Int.Conf.\non Music Info. Retrieval ISMIR-03 , 2003.\n[3] R. Duda, P. Hart, and D. Stork. Pattern Classiﬁca-\ntion. Wiley-Interscience, New York, 2001. 2nd ed.\n[4] D. P. W. Ellis and J. Arroyo. Eigenrhythms: Drum\npattern basis sets for classiﬁcation and generation.\nTechnical report, Columbia Univ. Dept. Elec. Eng.,\n2004. http://www.ee.columbia.edu/\n˜dpwe/pubs/tr04-eigenrhythm.pdf .\n[5] M. Goto and Y. Muraoka. A beat tracking system\nfor acoustic signals of music. In ACM Multimedia ,\npages 365–372, 1994.\n[6] F. Gouyon, S. Dixon, E. Pampalk, and G. Widmer.\nEvaluating rhythmic descriptors for musical genre\nclassiﬁcation. In Proc. 25th Intl. AES Conf. , Lon-\ndon, 2004.\n[7] A. Hyv ¨arinen and E. Oja. Independent component\nanalysis: Algorithms and applications. Neural Net-\nworks, 13(4–5):411–430, 2000.\n[8] J. Laroche. Estimating tempo, swing and beat loca-\ntionsinaudiorecordings. In Proceedingsofthe2001\nIEEE Workshop on Applications of Signal Process-\ning to Audio and Acoustics , Mohonk NY, 2001.\n[9] D. D. Lee and H. S. Seung. Learning the parts of\nobjectsbynon-negativematrixfactorization. Nature,\n401:788–791, 1999.\n[10] J.PaulusandA.Klapuri. Measuringthesimilarityof\nrhythmic patterns. In Proc. ISMIR-02 , Paris, 2002.\n[11] E. D. Scheirer. Tempo and beat analysis of acoustic\nmusicalsignals. J.Acoust.Soc.Am. ,103:1:588–601,\n1998.\n[12] A. Sheh and D. P. Ellis. Chord segmentation and\nrecognition using em-trained hidden markov mod-\nels. InProc. Int. Conf. on Music Info. Retrieval\nISMIR-03 , 2003.\n[13] P. Smaragdis and J. C. Brown. Non-negative ma-\ntrixfactorizationforpolyphonicmusictranscription.\nInProc. IEEE Workshop on Apps. of Sig. Proc. to\nAcous. and Audio , Mohonk NY, 2003.\n[14] R. J. Turetsky and D. P. Ellis. Ground-truth tran-\nscriptionsofrealmusicfromforce-alignedmidisyn-\ntheses. In Proc. Int. Conf. on Music Info. Retrieval\nISMIR-03 , 2003.\n[15] G.Tzanetakis,G.Essl,andP.Cook. Automaticmu-\nsical genre classiﬁcation of audio signals. In Proc.\nISMIR-01 , Bloomington IN, 2001.\n[16] A. Zils, F. Pachet, O. Delerue, and F. Gouyon. Au-\ntomatic extraction of drum tracks from polyphonic\nmusicsignals. In Proc.WEDELMUSIC-02 ,Decem-\nber 2002."
    },
    {
        "title": "Musical instrument recognition based on class pairwise feature selection.",
        "author": [
            "Slim Essid",
            "Gaël Richard",
            "Bertrand David 0002"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418253",
        "url": "https://doi.org/10.5281/zenodo.1418253",
        "ee": "https://zenodo.org/records/1418253/files/EssidRD04.pdf",
        "abstract": "In this work, musical instrument recognition is consid- ered on solo music from real world performance. A large sound database is used that consists of musical phrases ex- cerpted from commercial recordings with different instru- ment instances, different players, and varying recording conditions. The proposed recognition scheme exploits class pairwise feature selection based on inertia ratio maximization. More- over, new signal processing features based on octave band energy measures are introduced that prove to be useful. Classification is performed using Gaussian Mixture Mod- els in a one vs one fashion in association with a data rescal- ing procedure as pre-processing. Experimental results show that substantial improvement in recognition success is thus achieved.",
        "zenodo_id": 1418253,
        "dblp_key": "conf/ismir/EssidRD04",
        "keywords": [
            "musical instrument recognition",
            "solo music",
            "real world performance",
            "large sound database",
            "class pairwise feature selection",
            "inertia ratio maximization",
            "new signal processing features",
            "octave band energy measures",
            "Gaussian Mixture Models",
            "data rescaling procedure"
        ],
        "content": "MUSICAL INSTRUMENT RECOGNITION BASED ON CLASS PAIRWISE\nFEATURE SELECTION\nSlim ESSID, Ga ¨el RICHARD and Bertrand DAVID\nGET-ENST (T ´el´ecom Paris)\nD´epartement Traitement du Signal et des Images (TSI)\nABSTRACT\nIn this work, musical instrument recognition is consid-\nered on solo music from real world performance. A large\nsounddatabaseisusedthatconsistsofmusicalphrasesex-\ncerptedfromcommercialrecordingswithdifferentinstru-\nment instances, different players, and varying recording\nconditions.\nThe proposed recognition scheme exploits class pairwise\nfeatureselectionbasedoninertiaratiomaximization. More-\nover,newsignalprocessingfeaturesbasedonoctaveband\nenergy measures are introduced that prove to be useful.\nClassiﬁcationisperformedusingGaussianMixtureMod-\nelsinaonevsonefashioninassociationwithadatarescal-\ning procedure as pre-processing.\nExperimental results show that substantial improvement\nin recognition success is thus achieved.\n1. INTRODUCTION\nMusical instrument recognition is an important aspect of\nmusic information retrieval. Such a capability may be ex-\ntremely helpful in the framework of automatic musical\ntranscription systems as well as in content-based search\napplications. Both the amateur music lover and the pro-\nfessional musician would appreciate to have a system in-\nforming them of the instruments involved in the musical\npiece which they are listening to.\nHowever, processing complex mixtures of instruments\nof real world music remains a very difﬁcult issue which\nhas been barely addressed. In fact, most effort was dedi-\ncatedtomusicalinstrumentrecognitionbasedonisolated-\nnote content, and to a smaller extent, based on mono-\nphonic musical phrases [1]. In our work, music from real\nsoloperformanceisconsideredsinceitisbelievedthatthis\ndirection could give rise to immediate applications and\nstands as an important intermediate step towards musical\nrecognition in the polyphoniccontext [2].\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2004 Universitat Pompeu Fabra.In marked contrast to other pattern recognition tasks\n(typicallyspeechrecognition),therehasbeennorealcon-\nsensus in choosing a set of signal processing features\namenable to successful instrument recognition. A large\nnumberofpotentiallyusefulfeaturescanbechosenwhich\nare adapted to our task. In such a situation, feature selec-\ntion techniques should be considered [3] in order to fetch\nthemostrelevantfeaturesubset. Classically,featuresfrom\nall instrument classes are processed jointly, which results\nin an optimal set of descriptors that is subsequently used\nto train appropriate classiﬁers [4, 5, 6]. Our contribution\nsuggestsperformingclasspairwisefeatureselectioninor-\ndertoﬁndthemostefﬁcientfeaturesindiscriminatingbe-\ntween a given comparison of 2 instruments. It is shown\nthatcombiningthisapproachwithaonevsoneclassiﬁca-\ntion strategy based on Gaussian Mixture Models (GMM)\nresults in higher recognition success.\nThe outline of the paper is the following. We ﬁrst\npresentthesetofsignalprocessingfeaturesusedandpro-\npose new features that prove to be useful for instrument\nrecognition. The feature selection strategy as well as the\nclassiﬁcation technique are then described. Finally, we\nproceed to the experimental study.\n2. FEATURE EXTRACTION\nManyfeatureshavebeenproposedformusicalinstrument\nrecognition[4,5,1]describingvarioussoundqualities. A\nnumber of these features become quite hard to extract ro-\nbustlywhendealingwithmusicalphrases. Typically,note\nattack characteristics, although surely perceptually very\nimportant, are difﬁcult to evaluate since onset detection\nis already intricate in our case1. Thus, a set of features\nwhich can be extracted in a more or less straightforward\nmanner was chosen. In the following, we present a brief\ndescription of the features used. All of them are extracted\non a frame basis.\n2.1. Commonly used features\n•Temporal . They consist of Autocorrelation Coefﬁ-\ncients(AC)whichwerereportedtobeusefulin[8],\n1althoughtherehasbeenanumberofproposalsaddressingthisissue\n[7], there is no known system able to perform 100% successful onset\ndetection due to the large variety of musical signalsin addition to Zero Crossing Rates (ZCR).\n•AmplitudeModulationfeatures(AM) .Thesefea-\ntures are meant to describe the tremolo when mea-\nsuredinthefrequencyrange4-8Hz,andthe”grain-\niness” or ”roughness” of the played notes if the fo-\ncus is put in the range 10-40 Hz [5]. First, tem-\nporal amplitude envelopes were computed using a\nlow-pass ﬁltering of signal absolute complex en-\nvelopes, then a set of six coefﬁcients was extracted\nas described in Eronen’s work [5], namely AM fre-\nquency,AMstrengthandAMheuristicstrength(for\nthe two frequency ranges). Two coefﬁcients were\nappended to the previous to cope with the fact that\nan AM frequency is measured systematically (even\nwhen there is no actual modulation in the signal);\nthey were the product of tremolo frequency and\ntremolo strength, as well as the product of ”grain-\niness” frequency and ”graininess” strength.\n•Cepstral. Mel-Frequency Cepstral Coefﬁcients\n(MFCC) are considered as well as their time ﬁrst\nand second derivatives which are estimated over 9\nsuccessive frames.\n•Spectral.\nBased on statistical moments . These included the\nSpectralCentroid(Sc),theSpectralWidth(Sw),the\nSpectral Asymmetry (Sa) deﬁned from the spectral\nskewnessandtheSpectralFlatness(Sf)deﬁnedfrom\nthe spectral kurtosis. These features have proven\nto be successful for drum loop transcription [9] but\nalso for musical instrument recognition [10]. They\nare denoted by Sx = Sc, Sw, Sa, Sf. Their time\nderivatives ( δSx) (approximated over 9 successive\nframes) were also computed in order to provide us\nwith an insight into spectral shape variation over\ntime. It is worth to note that δSc can be seen as a\nqualityofthevibratoplayingtechniquesinceitem-\nbeds some frequency modulationinformation [5].\nMPEG7 spectrum ﬂatness . A more precise de-\nscription of the spectrum ﬂatness was also used,\nnamely MPEG-7 Audio Spectrum Flatness (ASF)\n[11]whichisprocessedoveranumberoffrequency\nbands. Indeed, this feature subset was found to be\nvery useful for our task [10].\nBasedonconstant-Qtransform . Frequencyderiva-\ntiveoftheconstant- Qcoefﬁcients(describingspec-\ntral ”irregularity” or ”smoothness”) were extracted\nastheywerereportedtobesuccessfulbyBrown[2].\nAnotherusefulfeatureconsistedinameasureofthe\naudio signal Frequency cutoff (Fc), also called fre-\nquency rolloff in some studies [12]. It was com-\nputedasthefrequencybelowwhich99%ofthetotal\nspectrum energy was accounted.\nFigure 1. Octave band ﬁlterbank frequency response.\n2.2. New features : Octave Band Signal Intensities\nWeintroduceanewfeaturesetwhichhasbeenfoundvery\nuseful. The idea is to capture in a rough manner the har-\nmonic structure of a musical sound, since it is desired to\navoid recurring to pitch-detection techniques. In fact, a\nprecise measure of frequencies and amplitudes of the dif-\nferent partials is not required for our task. One rather\nneeds to represent the differences in harmonic structure\nbetween instruments. This can be achieved by consider-\ning a proper ﬁlterbank, designed in such a way that the\nenergycapturedineachsubbandvaryfortwoinstruments\npresenting different energy distribution of partials. Thus,\nwe consider an octave band ﬁlterbank with triangular fre-\nquency responses. Filter edges are mapped to musical\nnote frequencies starting from the lowest Piano note A1\n(27.5 Hz). For each octave subband the maximum of the\nfrequency response is reached in the middle of the oc-\ntave subband. Important overlap is kept between adjacent\nchannels (half octave). We then measure the log energy\nof each subband (OBSI) and the logarithm of the energy\nRatioofeachsubband sbtotheprevious sb−1(OBSIR).\nAs a result, the energy captured in each octave band\nas well as the energy ratio of one band to the previous\nwill vary for two instruments having different harmonic\nstructures. Additionally, in most cases, coarse locating of\nthe fundamental frequency ( f0) is achieved since its oc-\ntaverangecanbededucedfromtheﬁrstpeakintheOBSI\nfunction. Figure 2.2 gives an illustration of this discus-\nsion with Alto Sax and Bb Clarinet playing the same mu-\nsical note A4. For example, one can easily observe that\nthe Bb Clarinet has more energy in the second subband\nappearing on the plot than the Alto Sax, while the Atlo\nSax has more energy than the Bb Clarinet in the third and\nforth subbands. In fact, it is known that the Bb Clarinet\nis characterized by the prominence of its odd harmonics\nand OBSI/OBSIR attributes allow us to describe such a\ncharacteristic.Figure 2. Amplitude spectrums of Alto Sax (top) and Bb\nClarinet(bottom)playingthesamenoteA4andtheoctave\nband ﬁlterbank.\n3. FEATURE SELECTION\nWhenever an important number of candidate features are\nconsidered for a given classiﬁcation task, it is very ad-\nvantageous, not to say necessary to use feature selection\ntechniques [3]. Such techniques aim at obtaining a ”min-\nimal” set of features which is the most efﬁcient in dis-\ncriminating between the classes under consideration, in\nthe sense that selected features form the most informative\nand non-redundant subset of the original set of features.\nTherehasbeenagreatdealofeffortmadetothisendgiv-\ningrisetoanumberoffeatureselectionalgorithms[3,13].\nWe choose to use a technique proposed by Peeters [6] in\nthe context of musical instrument classiﬁcation. The au-\nthor reported higher performance using the so-called ”In-\nertiaRatioMaximizationusingFeatureSpaceProjection”\n(IRMFSP) approach than the more classic ”Correlation-\nbasedFeatureSelection”(CFS)algorithm. Ourmaincon-\ntribution here lies in adopting a pairwise feature selection\nstrategy. The key idea is to select the subset of features\nthat is the most efﬁcient in discriminating between every\npossible pair of the considered instruments. We start by a\nbrief description of the IRMFSP algorithm.\n3.1. The IRMFSP algorithm\nFeatureselectionismadeiterativelywiththeaimtoderive\nanoptimalsubsetof dfeaturesamongst D,thetotalnum-\nber of features. At each step i, a subset Xiofifeatures isbuilt by appending an additional feature to the previously\nselected subset Xi−1. Let Kbe the number of classes,\nNkthenumberoffeaturevectorsaccountingforthetrain-\ning data from class kandNthe total number of feature\nvectors ( N=/summationtextK\nk=1Nk).\nLetxi,nkbe the nkthfeature vector (of dimension i)\nfromclass k,mi,kandmiberespectivelythemeanofthe\nvectorsoftheclass k(xi,nk)1≤nk≤Nkandthemeanofall\ntraining vectors (xi,nk)1≤nk≤Nk; 1≤k≤K.\nFeatures are selected based on the ratio ri(also known\nastheFisherdiscriminant[14])oftheBetween-classiner-\ntiaBito the ”average radius” of the scatter of all classes\nRideﬁned as:\nri=Bi\nRi=/summationtextK\nk=1Nk\nN/bardblmi,k−mi/bardbl\n/summationtextK\nk=1/parenleftBig\n1\nNk/summationtextNk\nnk=1/bardblxi,nk−mi,k/bardbl/parenrightBig(1)\nThe principle is quite intuitive as we would like to se-\nlect features that enable good separation between classes\nwithrespecttothewithin-classspreads. Thus,theselected\nadditional feature correspondsto the highest ratio ri.\nIn order to ensure the non-redundancy of the subset to\nchoose, an orthogonalization step is introduced consec-\nutive to every Inertia Ratio Maximization-based feature\nselection. At each iteration, ratio rimaximization is per-\nformedyieldinganewfeaturesubset Xi,andthenthefea-\nturespacespannedbyallobservationsismadeorthogonal\ntoXi.\nThe algorithm stops when the ratio rdmeasured at it-\neration dgets much smaller than r1,i.e.whenrd\nr1< /epsilon1for\na chosen /epsilon1, which means that the gain brought by the last\nselected feature has becomenon-signiﬁcant.\n3.2. Class pairwise feature selection\nOur approach consists in performing the IRMFSP algo-\nrithm/parenleftbigK\n2/parenrightbig\ntimes2, one processing for each pair of instru-\nments (this will be referred to as/parenleftbigK\n2/parenrightbig\n-IRMFSP by con-\ntrast to the classic approach denoted by 1-IRMFSP). A\ndifferent set of features that is optimal in discriminating\nbetweentwo giveninstrumentsis searched for,in theper-\nspective of a one vs one classiﬁcation strategy. Hence, as\nmany GMM classiﬁers as instrument pairs will be built\nbased on different feature subsets. Beyond the improve-\nment in recognition success (see section 5), the proposed\nscheme allows us to better understand instrument timbral\ndifferences. Indeed,itenablesonetoformulatestatements\nsuch as ” Instrument i has characteristics A and B quite\ndifferentfrominstrumentj ”,where” characteristicsAand\nB” are deduced from the subset of features selected for\nthepair {i, j}. Additionally,itmakestheanalysisandop-\ntimization of classiﬁcation performance more straightfor-\n2/parenleftbigK\n2/parenrightbig\nbeing the number of combinations of 2 elements from Kpos-\nsible or the binomial coefﬁcientward in the sense that it helps ﬁnding remedies to instru-\nment confusions. For example, if the recognition success\nfor a given instrument iis unsatisfactory because it is of-\ntenconfusedwithinstrument j,itisreasonabletoconsider\noptimizing only the {i, j}classiﬁer.\nThe pairwise solution remains practicable even when\na higher number of instruments are considered since hi-\nerarchicalclassiﬁcation,wherein instruments aregrouped\ninto families, is commonly used with success in this case\n[4, 5, 6]. The number of combinations to be considered\nat a time is then reduced to classes at the same level of\ntaxonomy, rarely more than 4classes.\n4. CLASSIFICATION\n4.1. The Gaussian MixtureModel (GMM)\nThe Gaussian Mixture model (GMM) has been widely\nusedbythespeech/speakercommunitysinceitsintroduc-\ntion by Reynolds for text-independent speaker identiﬁca-\ntion [15]. It was also successful for musical instrument\nrecognition [2, 5]. In such a model, the distribution of the\nP-dimensional feature vectors is modeled by a Gaussian\nmixture density. For a given feature vector x, the mixture\ndensity for instrument Ωkis deﬁned as :\np(x|Ωk) =M/summationdisplay\ni=1wk\nibk\ni(x). (2)\nwheretheweightingfactors wk\niarepositivescalarssat-\nisfying/summationtextM\ni=1wk\ni= 1. The density is then a weighted\nlinear combination of M Gaussian component densities\nbk\ni(x)withmeanvector µk\niandcovariancematrix Σk\nigiven\nby:\nbk\ni(x) =1\n(2π)P/2|Σk\ni|1\n2e(−1\n2(x−µk\ni)/prime(Σk\ni)−1(x−µk\ni))(3)\nThe parameters of the model for the instrument k, de-\nnotedby λk={wk\ni, µk\ni,Σk\ni}i=1,...,Mareestimatedthanks\nto the traditional Expectation-Maximization (EM) algo-\nrithm [16]. Classiﬁcation is then usually made using the\nMaximum a posteriori Probability (MAP) decision rule.\nAs an alternative, one can consider a one vs one decision\nstrategy which can be very proﬁtable as will be discussed\ninsection5. Classiﬁcationisthenperformedusinga”ma-\njority vote” rule applied over all possible class pairs and\noverLconsecutive observations in time. For each pair of\nclasses {Ωi,Ωj}, a positive vote is counted for the class\nΩiif\np(xt|Ωi)> p(xt|Ωj) (4)\nwhere (p(xt|Ωk))k=i,jisgivenin(2), xtisthetestfeature\nvector observed at time t, and Lis the total number of\nobservations considered in taking decisions.4.2. Rescaling and transforming the data\nAs a ﬁrst pre-processing to GMM training, we introduce\na rescaling stage which aims at homogenizing the highly\nvarying dynamics of the different feature subsets. This is\nawellknowntechniqueinquantizationproblemswhereby\nbetterprecisionisachievedbymeansofappropriatescale\nfactors [17]. In our case, one scale factor is chosen for\neach feature subset in such a way that the resulting all-\nfeaturevectorshavecoefﬁcientsconﬁnedintherange [0,1].\nThe second pre-processing consists in using a Princi-\npalComponentAnalysis(PCA)transforminorderto”de-\nnoise” the data [18]. The particularity of the approach\nrests on the fact that one PCA transform is computed for\neachinstrumentclass(basedonitstrainingdata). Thishas\nproven to be more efﬁcient than a global PCA transform\nobtained from all-class data. PCA was performed as fol-\nlows : for each instrument class, the covariance matrix of\nall related training feature vectors was computed and its\nSingular Value Decomposition(SVD) processed yielding\nRx=UDVt,\nwhereRxis the covariance matrix, UandVare respec-\ntively the left and the right singular vector matrices, and\nDisthesingularvaluematrix. ThePCAtransformmatrix\nwasthentakentobe W=Vtandclassiﬁersweretrained\nonthedata Y=WX,whereXisthematrixwhosecolumns\nrepresentthetrainingfeaturevectors. Thesametransform\nmatrixWwas applied on test feature vectors.\n5. EXPERIMENTAL VALIDATION\nLet us ﬁrst give indications on various experimental pa-\nrameters. Theinputsignalwasdown-sampledtoa32-kHz\nsampling rate, it was centered with respect to its temporal\nmeananditsamplitudewasnormalizedwithrespecttoits\nmaximum value. The analysis was performed over slid-\ning overlapping windows. The frame length was 32 ms\nand the hop size 16 ms for the extraction of all features\nexcept tremolo and roughness. Longer analysis length\n(960 ms and 480-ms hopsize) was used for the latter so\nas to measure the AM features properly. The AM fea-\nture values measured over each long window were then\nassigned to each 32-ms frame corresponding to the same\ntime segment. All spectra were computed with a FFT af-\nter a Hamming window had been applied. Frames con-\nsisting of silence signal were detected thanks to a heuris-\ntic approach based on power thresholding then discarded\nfrom both train and test data sets. The frequency ratio for\nthe constant- Qtransform was 1.26. A total of 160 feature\ncoefﬁcients were considered including elements from all\nfeature subsets described earlier.\nTheGMMwastrainedwithteniterationsoftheExpec-\ntation Maximization algorithm. Initialization consisted in\nclustering the observation space of accumulated feature\nvectors into M= 16Vorono¨ı regions thanks to the LBGquantization procedure [19]. Initial means of the compo-\nnent densities were taken to be the centroids of the ob-\ntained clusters. Diagonal covariance matrices were used\nand initialized with empirical covariance coefﬁcients of\nfeatures from every Vorono ¨ı region.\nScoring was performed as follows : for each test sig-\nnal,adecisionregardingtheinstrumentclassiﬁcationwas\ntaken every 0.47 s (30 overlapping frames of 32-ms dura-\ntion). Therecognitionsuccessrateisthen,foreachinstru-\nment,thepercentageofsuccessfuldecisionsoverthetotal\nnumber of 0.47-s test segments.\n5.1. Sound database for solophrase recognition\nTen instruments were considered, namely, Alto Sax, Bas-\nsoon, Bb Clarinet, Flute, Oboe, Trumpet, French Horn,\nViolin, Cello and Piano. We used the same database that\nis described in [10] and presented in table 1. It is impor-\ntant to note that we used larger and more varied musical\ncontent than previous studies. This allowed us to achieve\nbetter training but also to draw statistically valid conclu-\nsionsandassessthegeneralizationcapabilitiesofourclas-\nsiﬁcation scheme.\nTrainSrcsTrksnTests Test\nAlto Sax 9.37 10196825.46\nBassoon 3.33 592872.30\nBb Clarinet 13.13 10261077 8.62\nFlute 17.74 824217317.38\nOboe 18.29 824216217.30\nFrench Horn 4.61 5133692.95\nTrumpet 20.14 973239919.19\nCello 19.26 720233218.66\nViolin 22.67 1131244719.58\nPiano 20.48 815186214.90\nTable 1.Sound database - Srcsis the total number of distinct sources\nusedduringtest; TrksisthetotalnumberoftracksfromCDsduringtest;\nnTestsis the number of tests performed (1 test = 1 class decision over\n0.47 s);Total train andTotal test are the total durations of respectively\ntrain and test material in minutes.\n5.2. Features\nTable2sumsupthefeaturesubsetsusedtogetherwiththe\nfeatures selected in the 1-IRMFSP conﬁguration with a\nstop criterion /epsilon1= 10−5. A total of 19 features were se-\nlected including MFCC, Sx, ASF, OBSI and OBSIR. Not\nonlywereOBSIattributesselectedinprioritybutalsothey\nare the feature subset that is the most largely represented\ninthesetofselectedfeatures. Sincethemostrelevantfea-\nturesareselectedindecreasingorderofimportancebythe\nIRMFSPalgorithm,itcanbededucedthattheseattributes\nare useful for the instrument recognition task. Only the\n4 ﬁrst MFCCs were selected which is quite a low num-\nber compared to the 10 or 12 coefﬁcients usually used for\nsound source recognition.Feature subset SizeSelected\nAC=[A1,...,A49] 49-\nZCR 1-\nMFCC=[C1,..,C10]+ δ+δ230C1,...,C4\nSx=[Sc,Sw,Sa,Sf]+ δ+δ212Sc,Sw,Sa,Sf\nASF=[A1,...,A23] 23A22,A23\nSi=[S1,...,S21] 21-\nFc 1-\nOBSI=[O1,...,O8] 8O4,...,O8\nOBSIR=[OR1,...,OR7] 7OR4,...,OR7\nAM=[AM1,...,AM8] 8-\nTable 2. Feature subsets and 1-IRMFSP results\nUsing the same stop criterion/parenleftbig10\n2/parenrightbig\n-IRMFSP was per-\nformed (for the 45 possible pair combinations) yielding\nanaveragenumberof19featuresperinstrumentpair. The\nnumber of selected features varied from 9 (for the\nPiano/Violin confrontation) to 44 (for Bb Clarinet versus\nFlute). This is another beneﬁt of the chosen methodol-\nogy : features are speciﬁcally tuned to the context, when-\never two instruments are easily distinguished, the number\nof needed features is smaller. Examples of class pairwise\nfeature selection results arepresented in table 3.\nOne can draw the following conclusions.\n•Some features were never selected; this is the case\nfor the ﬁrst and second time derivatives of Spectral\ncentroid ( δSc,δ2Sc), Spectral width ( δSw,δ2Sw),\nSpectral asymmetry ( δSa,δ2Sa) and Spectral ﬂat-\nness ( δSf,δ2Sf). Also, cepstrum ﬁrst time deriva-\ntives (except δC0) and second time derivatives (ex-\nceptδ2C0)andtheproductof”graininess”frequency\nand ”graininess” strength.\n•Sc,Sw,SaandSftogetherwithMPEG-7ASFcoef-\nﬁcientsandOBSI/OBSIRwerethemostsuccessful\nfeatures since an important subset of them was al-\nways selected for every instrument pair. It is worth\nto note that Sc was not considered useful in a num-\nberofcases,probablybecausethesameinformation\nwasembeddedinotherfeaturesdescribingthespec-\ntralshape. TheaveragenumberofselectedMFCCs\nwas 4 (consistent with the 1-IRMFSP ﬁndings).\n•Some other features, although not selected very of-\nten, were useful in particular situations. In fact,\nSpectral ”irregularity” coefﬁcients (Si) were con-\nsideredparticularlyusefulforcombinationsinvolv-\ning the Bb Clarinet and otherwise rarely selected.\nAMfeatureswereparticularlyconsistentwhendeal-\ning with wind instruments, especially with the Bb\nClarinetandtheFrenchHorn. Amaximumof4au-\ntocorrelation coefﬁcients (among 49) were selected\nfor the pair Bb Clarinet/Flute. Zero Crossing Rate\nwas selected 18 times (out of 45) and Frequency\ncutoff 21 times. As for delta-cepstrum attributes,\nonly energy temporal variation ( δC0) and energy\nacceleration ( δ2C0) were found efﬁcient for only a\nfew combinations.Bb Clarinet/Alto Sax Bb Clarinet/Bassoon Bb Clarinet/Flute Bb Clarinet/French Horn\nC1,..,C3,C6,...,C8,C11\nSc,Sw,Sa,Sf\nA16,A22\nS12\nOR5C1,...,C4\nSc,Sw,Sa\nA21,...,A23\nS12,S18\nO5,...,O7R5,R10,R23,R42-ZCR\nC1,..,C3,C6- δ2C0\nSc,Sw,Sa,Sf\nA5,A9,A10,A18,A20,A22,A23\nS7,S8,S15,S16,S18,S19\nFc-O1,..,O8-OR1,...,OR7\nAM5ZCR-C1,..,C6\nSc,Sw,Sa,Sf\nA2,A3,A5,A6,A9,A10,A14,A18,A20,A23\nS9,S13,S14,S15,S16,S20\nFc-OR5,OR6\nAM1,AM2,AM3,AM6\nBb Clarinet/Trumpet Bb Clarinet/Cello Bb Clarinet/Violin Bb Clarinet/Piano Bb Clarinet/Oboe\nA8-C2,C3\nSw,Sa,Sf\nS15,S16,S19\nO1,O5,O6,O7\nOR5,OR7A1-C2,C3\nSw,Sa,Sf\nA22-S19\nO5,O9\nAM1C1,...,C3\nSw,Sa,Sf\nA20,A22,A23\nFc\nO4,O5A1-C1,...,C4\nSw,Sa\nC2,..,C5,C7\nA13,A18,A20,A22,A23\nFc-O2,O6,O7,O8\nOR6,OR7A1,A8,A18\nSc,Sw,Sa-ASF22\nS11,S14-O2,O4,O6,...,O8\nOR5,OR7\nTable 3.Features selected by the/parenleftbig10\n2/parenrightbig\n-IRMFSP algorithm for a few examples.\n5.3. Classiﬁcation results\nFor the one vs one scheme, 45 GMMs were trained based\nonfeaturesselectedforeachcombinationof2instruments\nthrough a/parenleftbig10\n2/parenrightbig\n-IRMFSP approach. No particular strat-\negy was employed at the test stage to cope with the in-\ndeterminationsresultingfromtheuseofthemajorityvote\nrule, typically the cases where two classes received the\nsame number of votes. Under these circumstances, we\nmerelyadoptedrandomselectionindecidingtheclassfor\ntherelatedtestsegment. Itisworthtonotethatbetterper-\nformance could be achieved with the one vs one scheme\nusing more sophisticated techniques in coupling the pair-\nwisedecisionsinordertodecidetheclassassociatedwith\na given test segment (see [20] for example).\nIn parallel, one GMM per instrument was trained using\nthedataobtainedthankstoaclassic1-IRMFSPandMAP\ndecision was used at the test stage.\n% correct oVo-nr oVooVo+PCA MAP\nAlto Sax 58.0353.48 56.36 65.76\nBassoon 60.1464.86 61.23 63.41\nBb Clarinet 63.7880.54 84.49 71.48\nFlute 56.6986.08 84.79 75.70\nOboe 82.2981.87 82.58 75.31\nFrench Horn 62.7865.96 71.49 57.45\nTrumpet 71.5564.02 68.19 79.70\nCello 90.4789.51 90.88 89.14\nViolin 95.5395.48 94.43 88.61\nPiano 92.8296.41 96.35 72.21\nAverage 73,4177,82 79,08 73,88\nTable 4. Instrument recognition success with different\nclassiﬁcation schemes - oVostands for one vs one, nr\nstands for no rescaling\nFirst, the beneﬁt of the rescaling procedure is high-\nlightedincolumns2and3oftable4,presentingtherecog-\nnitionsuccesswiththeonevsoneapproachwhenfeatures\nwererescaled(column3)andwithoutsuchpre-processing\n(column 2). The average improvement with rescaling is\n4.41%. Very signiﬁcant improvement is achieved espe-\nciallyfortheBbClarinet(+16.7%)andtheFlute(+29.4%).Indeed, rescaling seems to help the EM optimization in\nestimating the means of the mixture density-components\nas the data can be quantized more precisely at the initial-\nization stage. Even though a decline in performance is\nobserved in some cases (eg. Trumpet) it remains quite\nsmall compared to the improvement and could probably\nbe avoided with a better optimization of the scale factors.\nResults obtained with PCA-transformed data are given\nin column 4. Some improvement is observed for the Alto\nSax, the Oboe and the French Horn. The success rates\nremain hardly changed for the rest. It appears that, since\nirrelevant features are removed thanks to the feature se-\nlection algorithm, the use of PCA becomes less effec-\ntivecomparedtothecasewherenofeatureselectiontech-\nniques are exploited.\nLet us now compare our classiﬁcation scheme to clas-\nsiﬁcation using 1-IRMFSP in association with a classic\nGMMapproach(withasmanymodelsasinstruments)and\nthe MAP criterion (column 5). Our proposal performs\nsubstantially better in almost all cases. The success rate\nreaches 96.41 % for the Piano with the one vs one ap-\nproachwhileitisonly72.21%withtheclassicmethodol-\nogy(+24.2%). However,therearetwoexceptions,namely\nAlto Sax and Trumpet for which it is believed that rescal-\ning and model training was sub-optimal (see column 1\nand 2). The Alto Sax was confused with the Violin 37%\nof the time and the Trumpet with the Oboe 11% of the\ntime and with the Flute 9% of the time. A great advan-\ntageoftheonevsoneapproachresidesinthefactthatone\ncould consider optimizing only the Alto Sax/Violin clas-\nsiﬁer in order to improve the Alto Sax recognition rate.\nThe optimization should be concerned with both features\nandclassiﬁcationtechniques. Oneshouldfocusonﬁnding\nnew speciﬁc descriptors that could be amenable to better\ndiscrimination between Alto Sax and Violin, but also on\nmoreadaptedclassiﬁers. Infact,itisalsopossibletocon-\nsiderdifferentclassiﬁersforeachinstrumentpairusingfor\nexample the best of GMM and Support Vector Machines\n[21] with kernels speciﬁcally tuned to a certain combina-tionof2instruments. Thiscanyieldmoreeffectivenessin\nrecognition.\nThelastexperimentconsistedinmodifyingtheIRMFSP\nstopcriterionbychoosingasmaller /epsilon1inordertocheckthe\neffect of selecting more features on the recognition task\nsuccess. A value of /epsilon1= 10−6yielded 33 features with 1-\nIRMFSP and an average number of 38 features when us-\ningthe/parenleftbig10\n2/parenrightbig\n-IRMFSP.Resultsaregivenintable5. Scores\nareslightlyhigherandtheonevsoneapproachremainsin\nthe overall performance moreefﬁcient.\n% correct MAP oVo\nAlto Sax 71.82 55.15\nBassoon 61.96 68.84\nBb Clarinet 81.12 84.87\nFlute 82.28 88.87\nOboe 77.83 85.07\nFrench Horn 63.83 71.91\nTrumpet 82.44 71.77\nCello 91.30 87.81\nViolin 94.43 96.64\nPiano 90.80 98.17\nAverage 79,78 80,91\nTable 5. Instrument recognition success with /epsilon1= 10−6,\n33 features for 1-IRMFSP and an average of 38 features\nfor/parenleftbig10\n2/parenrightbig\n-IRMFSP.\n6. CONCLUSION\nIn this work, a one vs one classiﬁcation scheme was pro-\nposed for the recognition of 10 musical instruments on\nreal solo performance. A high number of signal process-\ning features was considered including new proposals that\nhaveproventobesuccessfulforthistask. Moreover,ithas\nbeen shown that it is advantageous to tackle the feature\nselection problem in a pairwise fashion whereby the most\nrelevant features in discriminating between two given in-\nstruments are found. The chosen approach entails higher\nrecognition success and allows us to analyze the recogni-\ntion system performance and look for enhancements in a\nmore straightforward manner.\nAdditionally, a data rescaling procedure has been in-\ntroduced that lead to substantial improvement in classiﬁ-\ncation performance.\nFutureworkwillconsiderclassiﬁersspeciﬁcallyadapted\nto every instrument pair, particularly, Support Vector Ma-\nchines in a scheme where the best kernel is selected for a\ngiven combination of 2 instruments.\n7. REFERENCES\n[1] P. Herrera, G. Peeters, and S. Dubnov. Automatic\nclassiﬁcation of musical instrument sounds. New\nMusic Research , 32.1, 2003.[2] Judith C. Brown, Olivier Houix, and Stephen\nMcAdams. Feature dependence in the auto-\nmatic identiﬁcation of musical woodwind instru-\nments.JournaloftheAcousticalSocietyofAmerica ,\n109(3):1064–1072, March 2000.\n[3] HuanLiuandHiroshiMotoda. Featureselectionfor\ndata mining and knowledge discovery . Kluwer aca-\ndemic publishers, 1998.\n[4] Keith Dana Martin. Sound-Source Recognition : A\nTheoryandComputationalModel . PhDthesis,Mas-\nsachusets Institue of Technology, June 1999.\n[5] Antti Eronen. Automatic musical instrument recog-\nnition. Master’sthesis,TampereUniversityofTech-\nnology, April 2001.\n[6] Geoffroy Peeters. Automatic classiﬁcation of large\nmusical instrument databases using hierarchical\nclassiﬁers with inertia ratio maximization. In AES\n115th convention, New York, USA , October 2003.\n[7] Juan P. Bello, L. Daudet, S. Abdallah, C. Duxbury,\nM. Davies, and M.B. Sandler. A tutorial on onset\ndetection in music signals. IEEE trans. on speech\nand audio processing , to be published.\n[8] J. Brown. Musical instrument identiﬁcation using\nautocorrelation coefﬁcients. In International Sym-\nposiumonMusicalAcoustics ,pages291–295,1998.\n[9] Olivier Gillet and Ga ¨el Richard. Automatic tran-\nscriptionofdrumloops. In IEEEICASSP,Montr ´eal,\nCanada, May 2004.\n[10] Slim Essid, Ga ¨el Richard, and Bertrand David. Ef-\nﬁcient musical instrument recognition on solo per-\nformance using basic features. In AES 25th inter-\nnationalconference,London,UnitedKingdom ,June\n2004.\n[11] Information technology - multimedia content de-\nscription interface - part 4: Audio, June 2001.\nISO/IEC FDIS 15938-4:2001(E).\n[12] G. Peeters. A large set of audio features for\nsound description (similarity and classiﬁcation) in\nthe cuidado project. Technical report, IRCAM,\n2004.\n[13] L. Molina, L. Belanche, and A. Nebot. Feature se-\nlection algorithms :a survey and experimental eval-\nuation. In International conference on data mining,\nMaebashi City, Japan , 2002.\n[14] P. E. Hart Richard Duda. Pattern Classiﬁcation and\nScence Analysis . Wiley-Interscience. John Wiley\nSons, 1973.\n[15] D. Reynolds. Speaker identiﬁcation and veriﬁca-\ntion usinggaussian mixturespeaker models. Speech\nCommunication , 17:91–108, 1995.[16] Todd K. Moon. The expectation-maximization al-\ngorithm. IEEE Signal processing magazine , pages\n47–60, nov 1996.\n[17] A. Gersho and R. Gray. Vector quantization and\nsignal compression . Kluwer academic publishers,\n1992.\n[18] Slim Essid, Ga ¨el Richard, and Bertrand David. Mu-\nsicalinstrumentrecognitiononsoloperformance. In\nEUSIPCO, Vienna, Austria , September 2004.\n[19] Y. Lindo, A. Buzo, and R. M. Gray. An algorithm\nforvectorquantizerdesign. In IEEETrans.Commu-\nnication, pages 84–95, January 1980.\n[20] Trevor Hastie and Robert Tibshirani. Classiﬁcation\nby pairwise coupling. Technical report, Stanford\nUniversity and university of Toronto, 1996.\n[21] ChristopherJ.C.Burges. Atutorialonsupportvector\nmachines for pattern recognition. Journal of Data\nMining and knowledge Discovery , 2(2):1–43, 1998."
    },
    {
        "title": "Beat and meter extraction using gaussified onsets.",
        "author": [
            "Klaus Frieler"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417851",
        "url": "https://doi.org/10.5281/zenodo.1417851",
        "ee": "https://zenodo.org/records/1417851/files/Frieler04.pdf",
        "abstract": "Rhythm, beat and meter are key concepts of music in gen- eral. Many efforts had been made in the last years to au- tomatically extract beat and meter from a piece of music given either in audio or symbolical representation (see e.g. [11] for an overview). In this paper we propose a new method for extracting beat, meter and phase information from a list of unquantized onset times. The procedure re- lies on a novel method called ’Gaussification’ and adopts correlation techniques combined with findings from mu- sic psychology for parameter settings.",
        "zenodo_id": 1417851,
        "dblp_key": "conf/ismir/Frieler04",
        "keywords": [
            "Rhythm",
            "beat",
            "meter",
            "music",
            "automatic",
            "extract",
            "audio",
            "symbolical",
            "representation",
            "unquantized"
        ],
        "content": "BEATANDMETEREXTRACTIONUSINGGAUSSIFIED ONSETS\nKlausFrieler\nUniversityofHamburg\nDepartment ofSystematic Musicology\nkgf@omni versum.de\nABSTRA CT\nRhythm, beat andmeter arekeyconcepts ofmusic ingen-\neral. Manyefforts hadbeen made inthelastyears toau-\ntomatically extract beat andmeter from apiece ofmusic\ngiveneither inaudio orsymbolical representation (seee.g.\n[11]foranovervie w). Inthispaper wepropose anew\nmethod forextracting beat, meter andphase information\nfrom alistofunquantized onset times. Theprocedure re-\nliesonanovelmethod called ’Gaussiﬁcation’ andadopts\ncorrelation techniques combined with ﬁndings from mu-\nsicpsychology forparameter settings.\n1.INTR ODUCTION\nThesearch formethods andalgorithms forextracting beat\nandmeter information from music hasseveralmotivations.\nFirst ofall,onemight wanttoexplain rhythm percep-\ntionorproduction inacogniti vemodel. Most ofclassical\nwestern, modern popular andfolkmusic canbedescribed\nasorganized around aregularly sequence ofbeats, thisis\nofutmost importance forunderstanding thecogniti veand\nproducti vedimensions ofmusic ingeneral. Second, me-\nterandtempo information areimportant meta data, which\ncould beuseful inmanyapplications ofmusic informa-\ntionretrie val.Third, forsome tasks related toproduction\norreproduction such information could also behelpful,\ne.g., foraDJwho wants tomixdifferent tracks inatem-\nporal coherent wayorforahip-hop producer ,who wants\ntoadjust music samples toasong orvice versa.\nInthispaper wedescribe anewmethod, which takes\nalistofonset times asinput, which might come from\nMIDI-data orfrom some kind ofonset detection system\nforaudio data. Thelistofonsets isturned intoaintegrable\nfunction, theso-called Gaussiﬁcation, andtheautocorre-\nlation ofthisGaussiﬁcation iscalculated. From thepeaks\noftheautocorrelation function time base (smallest unit),\nbeat (tactus) andmeter areinferred with thehelp ofﬁnd-\nings from music psychology .Then thebest ﬁtting meter\nandphase areestimated using cross-correlation ofproto-\ntypical meters, which resembles akind ofmatching algo-\nPermission tomakedigitalorhardcopiesofallorpartofthisworkfor\npersonalorclassroom useisgrantedwithoutfeeprovidedthatcopies\narenotmadeordistributedforpro®torcommercial advantageandthat\ncopiesbearthisnoticeandthefullcitationonthe®rstpage.\nc\n\u00002004UniversitatPompeuFabra.rithm. Weevaluated thesystem with MIDI-based data,\neither quantized with added temporal noise orplayed by\nanamateur keyboard player ,showing promising results,\nespecially intheprocessing oftemporal instabilities.\n2.MATHEMA TICAL FRAMEW ORK\nTheconcept ofGaussiﬁcation wasdeveloped inthecon-\ntextofextending autocorrelation methods from quantized\nrhythms tounquantized ones ([1],[4]).Theideabehind is\nthatanyproduced orpercei vedonset canbeviewed asan\nimperfect rendition (orperception) ofapoint onaperfect\ntemporal grid. Asimilar idea wasused byToiviainen &\nSnyder [11],who assumed anormal distrib ution ofmea-\nsured tappings time foranalysis purposes. However,the\nmethod presented here wasdeveloped independently ,and\ntheaims arequite different. Though anormal distrib ution\nisanatural choice itisnottheonly possible one, andthe\nGaussiﬁcation ﬁtinto themore general concept offunc-\ntionalisation.\nDeﬁnition 1(Functionalisation) Let\u0001\u0003\u0002\u0005\u0004\u0007\u0006\t\b\u000b\n\r\f\u000f\u000e\u0010\b\u0011\u000e\u0010\u0012 be\nasetoftime points (arhythm )and \u0004\u0007\u0013\u0014\b\u0015\n\u0016\f\u0017\u000e\u0018\b\u0019\u000e\u0018\u0012 asetof\n(real)coefﬁcients Moreover,let \u001abea \u001b\u001d\u001c-integrable func-\ntion: \u001e \u001f!\n\u001f\n\u001a#\"\u0019\u0006\t$\u0015%&\u0006('*)\nThen\u001a,+-\"\u0019\u0006\t$\u0014\u0002\n\u0012.\b0/1\f\n\u0013\n\b\u001a#\"\u0011\u0006324\u0006\n\b$ (1)\niscalled afunctionalisation of\u0001.\nWedenote by56\"\u0011\u0006\u0017798;:9<=$ thegaussian kernel, i.e.,5\u0010\"\u0019\u0006\u00177\t8;:9<=$(\u0002 >\n? @\u0016A<\u001c\u0018B\n!DCFE\u0019GIHKJMLLON\nL\n: (2)\nThenP+\n\"\u0011\u0006\t$Q\u0002\n\u0012.\b0/3\f\n\u0013;\b\u00115\u0010\"\u0019\u0006\u00177\t\u0006\u0015\b9:9<=$ (3)\u0002 >\n?@\rA<\u001c\n\u0012.\bR/1\f\n\u0013\n\bB\n!\nCME\u0011GIERSTJ\nLLON\nL (4)\niscalled aGaussiﬁcation of\u0001.AGaussiﬁcation isbasically alinear combination of\ngaussians centered atthepoints of \u0001.The advantage of\nafunctionalisation isthatthetransformation ofadiscrete\nsetintoaintegrable (orevencontinous anddifferentiable)\nfunction, sothatcorrelation andsimilar techniques areap-\nplicable. Anadditional advantage ofGaussiﬁcation isthat\nthevarious correlation functions canbeeasily integrated\nout.One has\nProposition 1Let \u0000\u0002\u0001\u0004\u0003\nP\"\u0011\u0006\t$\u0006\u0005\nP\"\u0019\u0006\b\u0007\n\t $bethetime\ntranslation operator.Then thetime-shifted scalarpr oduct\noftwogaussﬁcations\nP\f:\nP\u001cisthecross-corr elation func-\ntion \u000b\b\f\u000e\r\u000f\fL:\u000b\f\n\r\fL\n\"\u0010\t $\u0011\u0003M\u0002 \u0012\nP\f\r:\u0013\u0000\u0002\u0001\nP\u001c\n\u0014(5)\u0002 >\n@<\n?\nA\n\u0012\u0015\r\u0017\u0016 \u0012L\n.\b\u0010\u0016 \u00189/1\f\n\u0013;\b\u001a\u0019\u001b\u0018K56\"\u001c\t67\u001e\u001d \u0006\n\f\u001c\b\u001f\u0018\n:\n? @<=$\nwith \u001d \u0006\n\f\u001c\b\u001f\u0018\n\u0002 \u0006! \n\f\u0013\"\b\n24\u0006! \n\u001c\n\"\u0018.\nTheautocorr elation function #$\f \"\u0010\t $ofaGaussiﬁcation\nP\nisgiven by:#%\f \"\u001c\t $&\u0003M\u0002 \u0012\nP:\u0013\u0000\n\u0001\nP\n\u0014(6)\u0002 >\n@<\n?\nA\n\u0012.\b\u0010\u0016 \u00189/1\f\n\u0013\n\b\u0013\n\u001856\"\u001c\t67\u001e\u001d \u0006\n\f\u001c\b\u001f\u0018\n:\n?@<=$\nThenextthing weneed isthenotion ofatemporal grid.\nDeﬁnition 2(Temporal grid) Let \u001d'\u0000\n(*) bearealpos-\nitive constant, thetimebase .Then thesetP,+\u0015-\u0002 \u0004/.0\u001d'\u0000 :\u0013.214365 \n (7)\niscalled atemporal grid.For7 '98:1:35the\";7\u0010:;8\u0018$ -\nsubgrid of\nPisthesetP,+\u0015-\"\u001a7\u0010:\u00108\u0010$\u0014\u0002 \u0004 \";7,\u0007<.=8\u0018$\u0013\u001d>\u0000 :\u0017.\u0006143(\n (8)\nwith phase 7andperiod 8.Thevalue?\u0002 >@\u001d>\u0000(9)\niscalled thetempo ofthe(sub)grid. Anysubset \u0001BA\nP+\u0015-\nofatempor algrid iscalled aregular rhythm .\nItisnowconvenient todeﬁne thenotion ofametrical hi-\nerarchy .\nDeﬁnition 3(Metrical hierar chy)Let\nP+\u0015-beatempo-\nralgrid, C \u0002 \u0004\n@\b:>\n'\n@\f'\n@\u001c\n'\u0004DEDFD '\n@\u0012\nasetof\norderednatur alnumber sand 7 '\n@\faﬁxed phase .The\nsubgrid\nP,+\u0015-\";7\u0010:;8HG $JI\u0004K \";76:\u0013. $with 8LG \u0002:M\nG\b0/1\f\n@\b:\u0013.ONPiscalled asubgrid oflevel .andphase 7.\nA(regular) metrical hierarchyisthen thecollection of\nallsubgrids oflevel .QN\nP:R\";7\u00107\n@\f\u0016:FSFSESK:\n@\u0012 $\u0014\u0002 \u0004TK \";76:\u0013. $\u0017:>\nNU.VN\nP\n (10)Wearenowable tostate some classic problems ofrhythm\nresearch.\nProblem 1(Quantization) Let \u0001\u0003\u0002 \u0004 \u0006\t\b\u000b\n\u0016\f\u0017\u000e\u0018\b\u0019\u000e\u0018\u0012 beagiven\nrhythm (w.l.o.g . \u0006 \f \u0002\n))and W$(X) .Thetaskofquantiza-\ntionistoﬁndatime constant \u001d'\u0000andasetofquantization\nnumber s \u0004E.\n\b14365 \nsuch,thatY\u0006\u0015\b=2V.=\bZ\u001d>\u0000\nY'[W :]\\L^ (11)\nThemapping _ \"\u0011\u0006\t\bT$ \u0002`.=\b\u001a\u001d>\u0000 iscalled aquantization of\u0001.\nItisevident thatasolution does notnecessarily exist\nandisnotunique. Forany \u001d>\u0000andanynatural number7, \u001d>\u0000\u0002a \u0002\n+\u0015-agivesaanother solution. Therefore the\nrequirement ofminimal quantization, i.e. bc.\n\b\u0002\n@^Z.\nshould beadded. Manyalgorithms canbefound inthelit-\nerature forsolving thequantization problem (see [11]for\nanovervie w)andtherelated problems ofbeat andmeter\nextraction, which canbestated asfollo ws.\nProblem 2(Beat andmeter extraction) Let \u0001bethemea-\nsuredonsets ofarhythm rendition. Furthermor e,assume\nthat asubject was askedtotapregularly totherhythm,\nand thetapping times weremeasur ed,giving arhythm\u0000-\" \u0001 $.Thetask ofbeat extraction istodeduce aquan-\ntization of\u0000 \" \u0001 $from\u0001.Ifthesubject wasfurthermor e\naskedtomark a’one’, i.e.agrouping ofbeats, measur ed\ninto another rhythm\nR\" \u0001 $thetask ofmeter extraction\nistodeduce aquantization of\nRandtoﬁnditsrelative\nposition totheextracted beat.\nWewillpresent anewapproach with theaidofGaussi-\nﬁcation. Formusically reasonable applications more con-\nstraints havetobeadded, which naturally come from mu-\nsicpsychological research.\n3.PSYCHOLOGY OFRHYTHM\nMuch research, empirical andtheoretical, hasbeen done\nintheﬁeld ofrhythm, though ageneral accepted deﬁnition\nofrhythm isstilllacking. Likewise there aremanydif-\nferent terms anddeﬁnitions forthebasic building blocks,\nliketempo, beat, pulse, tactus, meter etc. Wewill only\nassemble some well-kno wnandwidely accepted empiri-\ncalfacts from theliterature, which serveasaninput for\nourmodel. Inaddition wewillrestrict ourselv estoexam-\nples from westen music which willbeconsidered tohave\nasigniﬁcant levelofbeat induction capability ,andcanbe\ndescribed with theusual western concepts ofanunderly-\ningisochronous beat andaregular meter .\nAreviewoftheliterature onmusical rhythm speaks\nforthefact, that there isahierachy oftime scales for\nmusical rhythm related tophysiological processes. (For\nasummary ofthefactspresented here seee.g[10]or[7]\nandreferences therein). Though music comprises arather\nwide range ofpossible tempos, which range roughly from\n60-300 bpm (200 ms-1s),there isnogeneral scale in-\nvariance. The limitations oneither side arecaused fromperceptual andmotorical constraints. The fusion thresh-\nold,ie,theminmal time span atwhich twoevents canbe\npercei vedasdistinct liesaround 5-30 ms,andorder re-\nlation between events canestablished above30-50ms.\nThe maximal frequenc yofalimb motion isreported to\nbearound 6-12 Hz( \u001d>\u0000 \u000280-160 ms), andthemax-\nimum time span between twoconsecuti veevents tobe\npercei vedascoherent, theso-called subjecti vepresent, is\naround\n@2\u0001\u0000 s.Furthermore, subjects askedtotapan\nisochronous beatatarateoftheir choice tend totaparound\n120 bpm (\u001d>\u0000 \u0002\u0003\u0002 ) )ms), theso-called spontaneous\ntempo ([3],[7],[12]).Likewise, thepreferred tempo,\ni.e.thetempo where subjects feelmost comfortably while\ntapping along tomusic liesaround within asimilar range,\nandisoften used synon ymously tospontaneous tempo.\nWiththisfactsinmind, wewillnowformulate anal-\ngorithm forsolving thequantization taskandthebeat and\nmeter extraction problem.\n4.METRICAL HIERARCHY ALGORITHM\nInput toouralgorithm istherhythm \u0001 \u0002 \u0004\u0007\u00069\b\u000b\n\u0016\f\u0017\u000e\u0018\b\u0019\u000e\u0018\u0012 as\nmeasured from amusical rendition. Fortesting purposes\nweused MIDI ﬁles ofsingle melodies from western pop-\nularmusic. Without lossofgenerality weset \u0006\n\f\u0002 )\n@\u0005\u0004.\n1.Prepare aGaussiﬁcation\nP\"\u0011\u0001 $with coefﬁceints com-\ningfrom temporal accent rules.\n2.Calculate theautocorrelation function #\f.\n3.Determine setofmaxima andmaxima points Cof#\f\n4.Find beat \u0000\u0007\u0006andtimebase \u001d>\u0000from C \";#\f\n$\n5.Getalistofpossible meters8\u0018\bwith best phases\b#\b\nandweights \t \bwith cross-correlation.\n4.1. Gaussiﬁcation with accents rules\nThe calculation ofaGaussiﬁcation from alistofonsets\nwasalready describe above.Wechose avalue of < \u0002\n25msforallfurther investigations. The crucial point is\nthesetting ofthecoefﬁcients\u0013\n\b.Wewill consider the\nvalues ofaGaussiﬁcation asaccent values, sotheques-\ntion ishowtoassign (perceptual) meaningful accents to\neach onset. itisknownfrom music psychology thatthere\narealotofsources forpercei vedaccents, ranging from\nloudness, pure temporal information along pitch clues to\ninvolvedharmonical (and therefore highly cultural depen-\ndent) clues. Since wearedealing with purely temporal in-\nformation, only temporal accent rules willbeconsidered.\nInterestingly enough, much ofthetemporal accent rules\n([7],[8],[9])arenotcausal, which seems tobeevidence\nforsome kind oftemporal integration inthehuman brain.\nForsakeofsimplicity weimplemented only some ofthe\nsimplest accent rules, related tointer-onset interv al(IOI)\nratios.00.511.522.53\n-5160516103215482064258030963612412846445160Timeline\nFigur e1.Example: Gaussiﬁcation ofthebeginning ofthe\nLuxembour gian folksong ’Plauderei anderLinde’, at120\nbpm with temporal noise added (< \u0002\n\u0002 )\n@\u000b\u0004).\nLet \f\u000e\r\u0010\u000f\u0012\u0011 (\u0013\f\u0014\r\u0016\u0015\n\u0012(>betwofreeaccent parameters\nformajor andminor accents respecti vely.Furthermore,\nwewrite\u001d-\u0006\n\b\u0002 \u0006\n\b24\u0006\n\b\n!\fforIOIs. Then theaccent algo-\nrithm isgivenby\n1.INITIALIZE\nSet\u0013;\b \u0002>,\u0013 \f \u0002\u0017\f\r\u0010\u0015\n\u0012,\u0013#\u0012 \u0002\u0017\f\r\u0010\u0015\n\u0012\n2.MINORACCENT\nIf \";\u001d \u0006\n\b\u0019\u00183\f2\n@<=$\u001b\u001a \u001d-\u0006\n\b(>then \u0013\n\b\u0002\u0001\f\u0014\r\u0016\u0015\n\u0012\n3.MAJORACCENT\nIf\";\u001d \u0006\u0015\b\u0019\u00183\f \u0007 <=$\u001c\u001a \u001d \u0006\u0015\b (\n@\nthen\u0013;\b1\u0002\u0001\f\r\u0010\u000f\u0012\u0011\nThesecond ruleassigns aminor accent.to everyevent,\nwhich follo wing IOIissigniﬁcantly longer then thepre-\nceding IOI. The third rule assigns amajor accent toan\nevent,ifthefollo wing IOIisaround twotimes aslong as\nthepreceding IOI. Itseems thataccent rules, evensim-\npleonelikethese, areinevitable formusically reasonable\nresults. After some informal testing weused values of\f\u0014\r\u0016\u000f\u0012\u0011 \u0002\u0001\u0000and \f\u0014\r\u0010\u0015\n\u0012\u0002\n@\nthroughout.\n4.2. Calculation of # \fanditsmaximum points\nThecalculation oftheautocorrelation function isdone ac-\ncording toequation 6.Afterw ardsthemaxima aresearched\nandstored forfurther use. Wedenote thesetofmaxima\nandcorresponding maximum points withC \"\u0010# \f $\u0014\u0002\u0005\u0004 \"\u0011\u0006\n\b:\u001e\u001d\n\b$\u0017:\u001b\u001d\n\b\u0002 #%\f \"\u0019\u0006\n\b$\u0014\u0002\n@\f \u001f1: ) N ^ '\nP\n4.3. Determination ofbeat andtime-base\n4.3.1. Determination ofthebeat\nItisawidely observ edfactthatthe’beat’-le velinamu-\nsical performance isthemost stable one. First, weweight\ntheautocorrelation with atempo preference function, and\nthen choose thepoint ofthehighest peaks tobethebeat00.20.40.60.811.2\n0516 1032 1548 2064 2580 3096ACF\nFigur e2.Example: Autocorrelation ofthebeginning of\n’Plauderei anderLinde’. One clearly sees thepeaks atthe\ntimebase of246ms,atthebeat levelof516msandatthe\nnotated meter 2/4(975 ms)\u0000\u0007\u0006.Thetempo preference function canbemodelled fairly\nwell byaresonance curvewith critical damping asin[12].\nParncutt [7]alsouses asimilar curve,derivedfrom aﬁtto\ntapping data ,which hecalls pulse-period salience. Be-\ncause theexact shape ofthetempo preference curveis\nnotimportant, weused theParncutt function, which hasa\nmore intuiti veform:\t-\"\u0011\u0006\t$\u001d\u0002B\n!\u0001\u0000\u0003\u0002 \u0004\u0006\u0005LL\n\u0007\t\b\n\u0007\t\u000b: (12)\nwhere\u0006\n\fdenotes thespontaneous tempo, which isafree\nmodel parameter thatwassetbyusto500msthrough-\nout,and\rbeing adamping factor ,which isanother free\nparameter ranging from\r \u0002>to\r \u0002\n@\n.(See Fig. 3).\nThesetofbeat candidates cannowbedeﬁned as\u0000\u000f\u000e \u0002 \u0004 \u0006\n\b14C \"\u0010#%\f($\u0017:\u001e\t \"\u0011\u0006\n\b$ \u001d\n\b\u0002\u0011\u0010\u0013\u0012\u0015\u0014\u0010\n (13)\nButanother constraint hastobeapplied on \u0000 \u0006toachie ve\nmusical meaningful results, coming from thecorrespond-\ningtimebase. The timebase isdeﬁned asthesmallest\n(ideal) time unit inamusical piece1,andmust beain-\ntegersubdi vision ofthebeat. Butsubdi visions ofthebeat\nareusually only multiples of2(’binary feel’) or3(’ternary\nfeel’), ornosubdi vision atall.So,theﬁnal deﬁnition of\nthebeat is:\u0000\u0007\u0006 \u0002\u0016\u0010\u0018\u0017\u001a\u0019\b\n\u0004\u0007\u0006\u0015\b 1 C\u001c\u001bT\"\u0010#\f\n$\u0017:\u001b\t-\"\u0011\u0006\u0015\b\u000b$ \u001d&\b1\u0002\u0011\u0010\u0013\u0012\u0015\u0014\u0010\n : (14)\nwithC\u001b\n\";#\f\n$(\u0002 \u0004\u0007\u0006\u0015\b\t:\n\u001d\u0006\u0015\b\u001d>\u0000-\"\u0011\u0006\n\b$\u0015\u001e\n\u0002\n@G\u0000 \u001f :\u0017.;:\n@1 35\n\n :(15)\nwhere thesymbol ! D \"denotes thenearest integer(round-\ning) operation, andwetaketheminimal candidate inthe\nextremely rarecase ofmore than onepossibility .\n1sometimes calledpulse00.10.20.30.40.50.60.70.80.91\n0 200 400 600 800 1000b=1\nb=2\nFigur e3.Tempo preference function with different\ndampings\n4.3.2. Determination ofthetimebase\nForagivenbeat candidate \u0000\n\u0006thetimebase \u001d>\u0000 canbe\nderivedfromC \"\u0010# \f($with thefollo wing algorithm.\nConsider thesetofdifferences\u001d C \u0002\u0005\";\u001d \u0006\n\f:\u001e\u001d \u0006\u001c\n:ESFSES :\u001e\u001d \u0006\n\u0012$\nofthepoints from C \";# \f $,with theproperties \u001d \u0006\n\bNU\u0000\n\u0006\nand \u001d \u0006\n\b$#\u0000I<.The second property rules out’unmusi-\ncal’timebases, which might becaused bycomputational\nartifactsorgrace notes. Then thetimebase\u001d>\u0000 1 \u001d C,is\ndeﬁned by%%%%\n\u0000\n\u0006\u001d>\u0000\n2\n\u001d\u0000\n\u0006\u001d>\u0000\u0013\u001e\n%%%%\n\u0002&\u0010'\u0017\u001a\u0019=:\n\u001d\u0000\n\u0006\u001d>\u0000\u0018\u001e\n\u0002\n@G\u0000\n\u001f:\u0017.;:\n@14365 (16)\nIfthere isnosuch atimebase forabeat candidate, the\ncandidate isruled out. Ifforallbeat candidates noap-\npropiate timebase canbefound, thealgorithm stops.\n4.4. Determination ofmeters andphases\nGiventhebeat, thenextlevelinametrical hierarchy isthe\nmeter .Itisdeﬁned asasubgrid ofthebeat grid. Although\nitcanbepresumed thatthetotal duration ofa(regular)\nmeter should notexceed thesubjecti vepresent ofaround@2 \u0000\n\u0004,there arenoclear measurements as,e.g., forthe\npreferred tempo. Likewise, meter ismuch more ambigu-\nousthan thebeat level,ase.g. thedecision between 2/4\nor4/4meter isoften merely amatter ofconvention (or\nnotation).\nSothestrate gyused formeter determination ismore\nheuristic, resulting inalistofpossible meters with aweight,\nwhich canbeinterpreted asarelati veprobability ofper-\nceiving thismeter ,andwhich canbetested empirical. The\nproblem ofdetermining thecorrect phase isthemost dif-\nﬁcult one. One might conjecture thattheinterplay ofpos-\nsible butdifferent phases foragivenmeter ,orevenofdif-\nferent meters, isamusical desirable effect, which might\naccount fornotions likegrooveorswing.Meter period Relati veAccents\n2 \u00042,0 \n3\u00042,0,0\n4 \u00042,0,1,0 \n5\u00042,0,0,1,0\n5 \u00042,0,1,0,0 \n5 \u00042,0,0,0,0 \n6\u00042,0,1,0,1,0\n6 \u00042,0,0,1,0,0 \n7\u00042,0,1,0,2,0,0\n7 \u00042,0,0,2,0,1,0 \n7 \u00042,0,0,2,0,2,0 \nTable 1.Listofprototypical accent structures\nNevertheless, ourstrate gyisstraightforw ardandisba-\nsically apattern matching process with thehelp ofcross-\ncorrelation ofgaussiﬁcations. Forthemost common mu-\nsical meters inwestern music prototypical accent patterns\n([6])aregaussiﬁcated onthebase ofthedetermined beat\u0000\n\u0006,andthen thecross-correlation with therhythm iscal-\nculated overoneperiod ofthemeter .Themaximum value\nofthiscross-correlation isdeﬁned asthematch between\ntheaccent pattern andtherhythm, andalong thiswaywe\nalso acquired thebest phase forthismeter .The match-\ningvalue isthen multiplied with thecorresponding value\noftheautocorrelation function, thisistheﬁnal weight for\nthemeter .\nTheprototypical accent patterns weused canbefound\ninTab.1.Forsome meters severalvariants aregiven,be-\ncause theycanbeviewed ascompound meters.\nSofrom anaccent pattern\u0000\u0002\u0001forameter with period8\nandbeat\u0000\n\u0006wegetthefollo wing Gaussiﬁcation:P\u0001\n\"\u0019\u0006\u00177\u0003\u0000\u0001\n:\u0013\u0000\u0007\u0006 $\u0014\u0002\n\u0012\u0005\u0004\u0005\u0006\b\u0007.a\u0017/1\f\n\u0013;\b 56\"\u0011\u0006\u00177!7 \u0000\u0007\u0006 : <=$\u0017: (17)\nwith\nP\u001f\n\t\f\u000bsuch, that\nP\u001f\n\t\f\u000b\n\u0000\n\u0006\u0011#\u0006\n\u0012\nThematch\n@\u0001isthemaximum ofthecross-correlation@\u0001\n\u0002 \u0010\u0013\u0012\u0015\u00145\n\u000eL\u0001\u000e\r\u0001\n-\u0010\u000f\u000b\f\u0012\u0011F\f\n\"\u001c\t $ (18)\nandthebest phase \bisthecorresponding time-lag. The\nweight \t\u0013\u0001isthevalue\t\u0014\u0001-\u0002X#\u0002\u0015 \" 8 \u0000\n\u0006$\n@\u0001\n5.EXAMPLES\nInFig. 1theGaussiﬁcation ofafolk song from Luxem-\nbourg(’Plauderei anderLinde’) isshown.Theinput was\nquantized butdistorted with random temporal noise of\nmagnitude < \u000250ms.Theoriginal rhythm wasnotated\nin2/4meter with atwoeight-note upbeat. Thegridshown\ninthepicture isbased ontheestimated beat \u0000\n\u0006\u0002516ms.\nFig. 2displays thecorresponding autocorrelation func-\ntion.00.511.522.53\n-51605161032154820642580309636124128464451605676Timeline\nBest 2/4\nFigur e4.Best 2/4Meter for’Plauderei anderLinde’.\nOne canseehowthealgorithm picks thebest balancing\nphase.\nMeter Phase Match Weight\n2 545ms 1.39953 1.55218\n3 540ms 0.882693 0.587578\n4 545ms 1.04741 0.803957\nTable 2.Phases, match andtotal weights for’Plauderei\nanderLinde’\nTheimportant peaks areclearly identiﬁable. InFig. 4\nthebest 2/4meter isshownalong with theoriginal Gaus-\nsiﬁcation. Thecross-correlation algorithm searches fora\ngood interpolating phase. Thecorreponding cross-correlation\nfunction canbeseen inFig. 5Theweights, matches and\nbestphases forthisexample arelisted inTab.2\nWealso tested aMIDI rendition oftheGerman popu-\nlarsong ’Mit 66Jahren’ byUdo J¨urgens (Fig. 6)played\nbyanamateur keyboard player .The autocorrelation can\nbeseen inFig.7.Though thehighest peak oftheautocor -\nrelation isaround 303ms,thealgorithm chooses thevalue\nof618ms( \u001697bpm) forthebeat, cause ofinﬂuence ot\n00.20.40.60.811.21.4\n01002003004005006007008009001000\"T0216_2.0.cc\"\nFigur e5.Cross-correlation function for2/4meter for\n’Plauderei anderLinde’.00.511.522.533.5\n-618 0618 1236 1854 2472 3090Timeline\nBest 2/4\nFigur e6.Gaussiﬁcation of’Mit 66Jahren’ andbest 2/4\nmeter .\nthetempo preference curve.Thetimebase ischosen tobe\n103ms,indicating thettheplayer adopted aternary feelto\nthepiece, which isreasonable, because theoriginal song\nhaskind ofablues shufﬂefeel. Thebest meter is2/4(or\n4/4forthehalfbeat), butthebest phase is738ms.Com-\npared totheoriginal score, which isnotated in4/4, the\ncalculated meter isphase-shifted byhalfameasure.\n6.SUMMAR YAND OUTLOOK\nWepresented anewalgorithm fordetermining ametrical\nhierarchy from alistofonsets.\nTheﬁrstresults arepromising. Forsimple rhythm like\ntheycanbefound in(western) folksongs, thealgorithm\nworks stable giving acceptable results compared tothe\nscore.\nFormore complicated orsyncopated rhythm, aswell\nasforecological obtained data theresults arepromising,\nbutnotperfect inmanycases, especially formeter extrac-\ntion. However,itisthequestion, whether human listener\nareable todetermine beat, meter andphase from those\nrhythms ina’correct’ way,ifpresented without themusi-\ncalconte xtandwith noother accents present. This willbe\ntested inthenear future.\nThealgorithm canbeexpanded inanumber ofways.\nTheextension topolyphonic rhythms should bestraight-\nforw ardandmight evenstabilize theresults. Furthermore,\nawindo wmechanism could beimplemented, which is\nnecessary forlargerpieces andtoaccount fortempo changes\nasaccelerations ordecelerations.\n7.REFERENCES\n[1]Brown, J.”Determination ofthe meter\nof musical scores by autocorrelation”,\nJ.Acoustic.Soc. Am,94(4), 1953-1957, 1993\n[2]Eck, D.Meter through synchrony:Pr ocessing\nrhythmical patterns with relaxation oscilla-00.20.40.60.811.2\n0 618 1236 1854 2472ACF\nFigur e7.Autocorrelation of’Mit 66Jahren’\ntors.Unpublished doctotal dissertation, Indi-\nanaUniversity ,Bloomington, 2000.\n[3]Fraisse, P.”Rhythm andtempo”, inD.Deutsch\n(Ed.), Psyc hologyofmusic ,NewYork: Aca-\ndemic Press, 1982\n[4]Frieler ,K.Mathematical music analysis .Doc-\ntotal dissertation (inpreparation), University\nofHamb urg,Hamb urg.\n[5]Large,E.,&Kolen, J.F.”Resonance andthe\nperception ofmusical meter”, Connection Sci-\nence,6(1), 177-208, 1994\n[6]Lerdahl, F&Jackendof f,R.Agenerative\ntheory oftonal music .MIT Press,Cambridge,\nMA, 1983.\n[7]Parncutt, R.”Aperceptual model ofpulse\nsalience and metrical accents inmusical\nrhythms”, Music Perception ,11, 409-464,\n1994\n[8]Povel,D.J., &Essens, P.”Perception oftem-\nporal patterns”, Music Perception ,2,411-440,\n1985\n[9]Povel,D.J., &Okkermann, H.”Accents in\nequitone sequences”, Perception and Psy-\nchophysics ,30,565-572, 1981\n[10] Seifert, U.,Olk, F.,&Schneider ,A.”On\nrhythm perception: theoretical Issues, empir -\nicalﬁndings”, J.ofNewMusic Resear ch,24,\n164-195, 1995\n[11] Toiviainen, P.&Snyder,J.S.”Tapping to\nBach: Resonance-based modeling ofpulse”,\nMusic Perception ,21(1), 43-80, 2003\n[12] vanNoorden, L.&Moelants, D.”Resonance\ninthetheperception ofmusical pulse”, Jour-\nnalofNewMusic Resear ch,28,43-66, 1999"
    },
    {
        "title": "Estimating The Tonality Of Polyphonic Audio Files: Cognitive Versus Machine Learning Modelling Strategies.",
        "author": [
            "Emilia Gómez",
            "Perfecto Herrera"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418007",
        "url": "https://doi.org/10.5281/zenodo.1418007",
        "ee": "https://zenodo.org/records/1418007/files/GomezH04.pdf",
        "abstract": "In this paper we evaluate two methods for key estimation from polyphonic audio recordings. Our goal is to compare between a strategy using a cognition-inspired model and several machine learning techniques to find a model for tonality (mode and key note) determination of polyphonic music from audio files. Both approaches have as an input a vector of values related to the intensity of each of the pitch classes of a chromatic scale. In this study, both methods are explained and evaluated in a large database of audio recordings of classical pieces.",
        "zenodo_id": 1418007,
        "dblp_key": "conf/ismir/GomezH04",
        "keywords": [
            "key estimation",
            "polyphonic audio recordings",
            "cognition-inspired model",
            "machine learning techniques",
            "tonality determination",
            "polyphonic music",
            "chromatic scale",
            "large database",
            "audio recordings",
            "classical pieces"
        ],
        "content": "ESTIMATING THE TONALITY OF POLYPHONIC AUDIO FILES: \nCOGNITIVE VERSUS MACHINE LEARNING MODELLING \nSTRATEGIES\nEmilia Gómez Perfecto Herrera \nMusic Technology Group, Institut Universitari de l’Audiovisual \nUniversitat Pompeu Fabra \n{emilia.gomez,perfecto.herrera}@iua.upf.es \nhttp://www.iua.upf.es/mtg \nABSTRACT \nIn this paper we evaluate two methods for key estimation \nfrom polyphonic audio recordings. Our goal is to compare between a strategy using a cognition-inspired \nmodel and several machine learning techniques to find a model for tonality (mode and key note) determination of polyphonic music from audio files. Both approaches have as an input a vector of values related to the intensity of each of the pitch classes of  a chromatic scale. In this \nstudy, both methods are explained and evaluated in a large database of audio reco rdings of classical pieces. \n1. INTRODUCTION \nTonality and tonal aspects of  musical pieces are very \nrelevant for its appreciation. There have been attempts to relate those aspects with mood induction in listeners, and some kind of relatedness (or similarity) between different excerpts sharing tonality have been reported. Listeners are sensitive to key changes, which are also related to rhythm, structure, style and mood. Key changes can be used, for instance, as cues about the structure of a song, or as features to query for matching pieces in a database. \nKey and mode can also be used to navigate across digital music collections by computing similarities between the files or selected excerpts from them.  \nIn western music, the term key (or tonality ) is usually \ndefined as the relationship between a set of pitches having a tonic  as its main tone, after which the key is \nnamed. A key is then defined by both its tonic (also called key note, for example: A) and its mode (ex: minor ). \nThe tonic is one in an octave range, within the 12 semitones of the chromatic scale (ex: A, A#/Bb, B, C, \nC#/Db, D, D#/Eb, E, F, F#/Gb, G ). The mode is usually \nminor or major, depending on the used scale. The major and minor keys then rise to a total set of 24 different tonalities. \nHere we compare two approaches for computing the \ntonality from audio files c ontaining poly phonic music. \nThe first one is based on a tonality model that has been established after perceptual studies, and uses some musical knowledge to estimate the global key note and mode attached to a certain audio segment. The second one is based on machine learning algorithms trained on a \ndatabase of labelled pieces. After a description of both \napproaches, we evaluate th em, present the results and \ndiscuss some of our findings. \n2. SYSTEM BLOCK DIAGRAM \nThe overall system block diagram is presented in Figure \n1. In order to estimate the key from polyphonic recordings, we first extract a set of low-level features \nfrom the audio signal. These f eatures are then compared \nto a model of tonality in order to estimate the key of the piece. \n \n \nFigure 1 . System block diagram. \n \nIn this study we have assumed that the key is constant \nover the considered audio segm ent. That means that the \nmodulations we can find do not  affect the overall tonality \nof the piece and we can estimate a tonality for the segment.  \n \n3. FEATURE EXTRACTION \nThe input of the key estimation block in Figure 1 is a \nvector of low-level featur es extracted from the audio \nsignal. The features used  in this study are the Harmonic \nPitch Class Profile  (HPCP), based on de Pitch Class \nProfile descriptor proposed by Fujishima in the context \nof a chord recognition system [1]. HPCP is a vector of low-level signal features meas uring the intensity of each \nof the 12 pitch classes of th e temperate scale within an \nanalysis frame. The feature extraction procedure is summarized as follows. We refer to [2] for a detailed explanation. \n1. Instantaneous HPCP vector is computed for each \nanalysis frame using the magnitude of the spectral peaks that are located w ithin a certain frequency \nband, considered as the most significant frequencies carrying harmonic information. We introduce a Permission to make digital or hard c opies of all or part of this work \nfor personal or classroom use is granted without fee provided that \ncopies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2004 Universitat Pompeu Fabra.   \n \nweight into the HPCP computation to get into account differences in tuning, and the resolution is changed to less than one semitone. The HPCP vector is normalized for each analysis frame in order to discard energy information.  \n2. Global HPCP is computed by averaging \ninstantaneous HPCP within the considered segment.  \n4. TONALITY COMPUTATION USING A \nCOGNITION-INSPIRED MODEL \nThis algorithm is based on a key estimation algorithm \nproposed by Krumhansl et al. and summarized in [3, pp. 77-111]: the probe tone method . It measures the \nexpectation of each of the 12 tones of a chromatic scale after a certain tonal context. This measure is representative to quantify th e hierarchy of notes in a \ngiven tonal context. The output of the model is a rating for each of the 12 semitones of a chromatic scale (starting from the tonic), shown in Figure 2. The data were produced by experienced musicians following tonal contexts that consisted of tonic triads and chord cadences. This profile is used  to estimate the key of a \nMIDI melodic line, by correlating it with a vector containing the relative duration of each of the 12 pitch classes within the MIDI sequence [3].\n \n \nFigure 2 . Probe tone ratings from the study by Krumhansl and \nKessler (1982) shown with reference to a major key (top) and a minor key (bottom). \nOur approach relies on extending this model to deal \nwith audio recordings in a polyphonic situation. We consider the profile value for a given pitch class to represent also the hierarchy of a chord in a given key. \nGiven this assumption, we consider all the chords containing a given pitch class when measuring the relevance of this pitch class within a certain key. For instance, the dominant pitch class (i=8) appears in both \ntonic and dominant chords, so that the profile value for i=8 adds the contribution of the tonic and the dominant chords of the key. We only consider the three main triads of the major/minor key as the most representative chords (tonic, subdominant and dominant).   We also adapt the method to work with audio features \n(HPCP related to energy) in stead of MIDI. The spectrum \nof a note is composed of several harmonics, whose frequencies are multiples of the fundamental frequency f \n(f, 2f, 3f, 4f,  etc.). When a note is played, HPCP increases \nat the pitch classes of the different harmonics. A note has then different associated pitch classes, one for each \nharmonic (not only the considered fundamental frequency). Each of the notes of the considered chords contributes to the profile values of its different harmonics. We make this contribution decrease along frequency using a linear function, in order to simulate that the spectrum amplitude decreases with frequency. \nFinal profiles are represented in Figure 3.  \n \nFigure 3 . Profiles adapted to polyphony and HPCP shown with \nreference to a major key (top) and a minor key  (bottom).  \nIn order to build the profiles for the 24 different keys, \nwe consider that the tonal hierarchy is invariant with respect to the chosen tonic.  For instance, the B major \nprofile is equal to the A major profile but shifted two bins (as A and B from a 2 semitones interval). The global HPCP vector is correlated with the different profiles, computed by circular shifting the adapted profiles. The maximum correlation gives the estimated key note and mode, as well as a correlation factor measuring the proximity of HPCP and the estimated key. More details on the method are found in [2]. \n5. MACHINE LEARNING FOR TONALITY \nMODELLING \nDifferent experiments have been performed, all of them \ninvolving comparisons between different inductive strategies, including the most usual ones like binary trees, bayesian estimation, neural networks, or support vector machines, but also some interesting meta-learning schemes such as boosting, or bagging. Meta-learning can be defined as the enhancement or extension of basic learning algorithms by means of incorporating other learners [5], which, in general, improve the performance and generalization capabilities of the base learners. Most of the experiments were carried out using Weka\n1:  \n                                                           \n1 http://www.cs.waika to.ac.nz/ml/weka/   \n \n1. Learning the tonic or key note using the low-level \ndescriptors HPCP as input. \n2. Learning the mode using HPCP information. \n3. Learning the key note and the mode altogether. \n4. Learning tonality (key note and mode) using the \nHPCP vector and the estimation derived from the perceptual/cognitive model (Section 4), which is considered as a mixed approach. \n6. RESULTS \n6.1. Audio material \n \nWe have built an audio database of 878 excerpts of \nclassical music for eval uation, including many \ncomposers as, for instance, Mozart, Chopin, Scarlatti, \nBach, Brahms, Beethoven, Handel, Pachelbel, Tchaikovsky, Sibelius, Dvorak, Debussy, Telemann, Albinoni, Vivaldi, Pasquini, Glenn Gould, Rachmaninoff, Schubert, Shostakovich, Haydn, Benedetto, Elgar, Bizet, Liszt, Boccherini, Ravel, Debussy, etc. We also incl ude some jazz versions of \nclassical pieces (e.g. Jac ques Lousier, The Swingle \nSingers, etc).  Most of the included titles were first movement (in case that the piece is a multi-movement form as sonata or symphony). All the key note and mode annotations were taken from the FreeDB database\n1. Some \nadditional manual corrections we re made to include other \nmovements or because of FreeDB wrong metadata, although systematic checking has not been performed.  \nWe divided the database in two sets: the training \nset, consisting of 661 audio files and the holdout set including the remaining 217 title s. We kept this holdout \nin order to test the generalization capabilities of the models using none of the instances used in the training phase. The tonality models were then derived using the 661 instances not assigned to the holdout.  Most of the tests involved between 10 and 20 instances for each \ntonality. \n6.2. Model for tonality perception \n \nThe results of the evaluation over the holdout database \nare presented in Figure 4, with 59,5% of correct tonality, 82% of correct mode and 65% of correct key note estimation. The confusion matrix is found in the author’s web page\n2.  \nWe find that the 19% of the estimation errors \ncorrespond to confusions between minor/major relatives (e.g., C major confused with A minor), and other 24% correspond to tuning errors (e.g., E minor confused with E b  m i n o r ) .  I t  c a n  a l s o  b e  s een that  the 5,7 % of the \nerrors have been made by estimating the upper 5th  within the circle of fifths (e.g., C major confused with G major) or the key whose tonic form a 5th ascending \n                                                          \n \n1 http://freedb.freedb.org/ \n2 http://www.iua.upf.es/~egomez/TonalDescription/ \nGomezHerrera-ISMIR2004.html interval  with the correct one (e.g., D minor confused \nwith A minor). 19% of the keys were confused with the near key down on the circle of fifths (A major confused with D major) or the key whose tonic is located at a 5th descending interval (e.g., A minor confused with D minor). Only 44% of the errors correspond to non-related tonality confusions.  \n6.3. Machine learning models \nWe present the results acco rding to the addressed \nsubproblems: mode induction, key note induction, and combined key note and mode induction. We observe, among other things, that there is no single “best learner” capable of optimally approximating the solutions for all of them. \n6.3.1.  Mode induction \nThe best results for mode induction were obtained using \nan instance-based learner which bases its decision on the class assigned to the five nearest neighbour cases (84% of correct decisions). Surprisingly, the rest of studied methods scored far below this family of models. The second best method was a multilayer perceptron  with \none hidden layer containing 20 units, which achieved 71% of correct decisions. In all cases, there were much more errors because of wrong assignment of minor mode than the other way round. \n6.3.2.  Key note induction \nThe application of “agnostic” machine learning strategies \nto the problem of assigning an overall key to a music piece yielded slightly better results than the perceptual/cognitive strategy. In this case, a Bayesian classifier with Density Estimation was the best of the set (72% of correct decisions). The Sequential Maximum Optimization algorithm (a kind of Support Vector Machine) scored close to that (70%), and again a 5 Nearest-Neighbour provided good results (69%).  \n6.3.3.  Simultaneous key note and mode induction \nAchieving a combined answer for key note and mode is \nthe most complex problem addressed in this series, as there were 24 different classes to classify the input patterns. Here, the best approach was that of a multilayer back-propagated perceptron  with a single hidden layer of \n20 units (63% of correct decisions). Again, instance-based strategies scored among the best (60%), although the best results were not quite far from those from the perceptual/cognitive model (59%). The confusion matrix \nfor this approach is shown in the author’s web page\n3. \n6.4. Combination of approaches \nAs it is the case in some meta-learning approaches, the \ncombination of two different algorithms can improve the   \n \nperformance provided both ge nerate different error \npatterns. Our experiments using the output of the perceptual/cognitive model as an additional input for the \nbest machine learning algorithm has yielded no improvement to the presente d results except in the case \nof key estimation, where the Bayesian learner yielded 77% when we included the predicted key, mode and strength from the perceptual/cognitive model. This addition amounts to an improvement of 5% (11% compared to the performance of the perceptual/cognitive model alone).       \nTonality estimation evalu ation\n59,463 6482,5 84 83\n657277\n020406080100\nCognitive Best ML Combined% \nCorrect tonality estimation Correct mode estimation\nCorrect key note estimation\nFigure 4 . Evaluation results. % of correct estimation. \n7. DISCUSSION  \nComparing the tonal cognition-inspired approach to the \nmachine learning techniques th at we can consider as \n“tools of the trade”, modest improvements in performance can be achieved by the latter (7% when computing the key note) or by embedding the former into the latter (12% for key note computation).  \nAs it is pointed out by Krumhansl, the tonal \ndescriptors we have considered are severely restricted, in the sense that they do not cap ture any musical structure. \nThese features take into account neither order information nor the chords’ position in the event hierarchy, as for instance, its place in the rhythmic or \nharmonic structure [3, pp. 66]. In fact, some of the estimation errors may be caused by tonality changes that affect the overall key measures and labelling. We will work on these structural and rhythmic aspects along future research. \n8. CONCLUSIONS \nWe have presented a comparison between two different \napproaches for tonality estimation from polyphonic \naudio. The first one is inspired in the probe tone method  \nand considers some aspects of tonality cognition. The second one uses “blind” machine learning techniques to model key by analyzing a training annotated collection. We have evaluated both methodologies over a large audio database, achieving a 64% of correct overall tonality estimation. Very sm all improvements were found \nby only using machine learning algorithms, which is somehow a puzzling observation that requires further \nexperiments with different da ta representations and more \nintensive parameter tweaking of the algorithms. We have still room for improvement in order to come up with a robust technique that allow us to exploit tonality information for retrieval in a general-purpose popular music database and also for aiding the discovery of music information in a similar way to what Purwins et al. [4] have recently presented.   \n9. ACKNOWLEDGMENTS \nThe authors would like to thank Takuya Fujishima and \nJordi Bonada for their advi ces on the feature extraction \nprocedure. This research has been partially funded by the EU-FP6-IST-507142 project SIMAC (Semantic Interaction with Music Audio Contents)\n1 and by the \nSpanish Government project TIC2003-07776-C02-02 Promusic.  \n10. REFERENCES \n[1] Fujishima, T. “Realtime chord recognition of \nmusical sound: a system using Common Lisp Music”, ICMC , Beijing, China, 1999, pp. 464–467. \n[2] Gómez, E. “Tonal description of polyphonic audio \nfor music content processing”. INFORMS Journal \non Computing. Special Cluster on Music Computing.  Chew, E., Guest Editor , 2004. \n[3] Krumhansl, C. L. Cognitive foundations of musical \npitch . Oxford University Press, New York, 1999, \npp. 16-49.  \n[4] Purwins, H., Blankertz,  B., Dornhege, G., and \nObermayer, K. “Scale degree profiles from audio investigated with machine learning”, 116\nth AES \nConvention , Berlin, Germany, 2004. \n[5] Witten, I. H. and Frank, E. Data mining: practical \nmachine learning tools and techniques with Java implementations . Morgan Kaufmann, San Francisco, \n2000.  \n                                                          \n \n1 http://www.semanticaudio.org"
    },
    {
        "title": "Speech-Recognition Interfaces for Music Information Retrieval: &apos;Speech Completion&apos; and &apos;Speech Spotter&apos;.",
        "author": [
            "Masataka Goto",
            "Katunobu Itou",
            "Koji Kitayama",
            "Tetsunori Kobayashi"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417339",
        "url": "https://doi.org/10.5281/zenodo.1417339",
        "ee": "https://zenodo.org/records/1417339/files/GotoIKK04.pdf",
        "abstract": "This paper describes music information retrieval (MIR) systems featuring automatic speech recognition. Al- though various interfaces for MIR have been proposed, speech-recognition interfaces suitable for retrieving musi- cal pieces have not been studied. We propose two differ- ent speech-recognition interfaces for MIR, speech com- pletion and speech spotter, and describe two MIR-based hands-free jukebox systems that enable a user to retrieve and play back a musical piece by saying its title or the artist’s name. The first is a music-retrieval system with the speech-completion interface that is suitable for mu- sic stores and car-driving situations. When a user can re- member only part of the name of a musical piece or an artist and utters only a remembered fragment, the system helps the user recall and enter the name by completing the fragment. The second is a background-music play- back system with the speech-spotter interface that can en- rich human-human conversation. When a user is talk- ing to another person, the system allows the user to enter voice commands for music-playback control by spotting a special voice-command utterance in face-to-face or tele- phone conversations. Our experimental results from use of these systems have demonstrated the effectiveness of the speech-completion and speech-spotter interfaces. Keywords: speech recognition, MIR interface, hands-free MIR, jukebox, title and artist search Video demonstration: http://staff.aist.go.jp/m.goto/ISMIR2004/",
        "zenodo_id": 1417339,
        "dblp_key": "conf/ismir/GotoIKK04",
        "keywords": [
            "speech recognition",
            "music information retrieval",
            "automatic speech recognition",
            "speech completion",
            "speech spotter",
            "hands-free jukebox",
            "music retrieval system",
            "background music playback",
            "voice commands",
            "human-human conversation"
        ],
        "content": "SPEECH-RECOGNITION INTERFACES FOR MUSIC INFORMATION\nRETRIEVAL: “SPEECH COMPLETION” AND “SPEECH SPOTTER”\nMasataka Goto†, Katunobu Itou‡, Koji Kitayama††, and Tetsunori Kobayashi††\n†National Institute of Advanced Industrial Science and Technology (AIST), Japan\n‡Nagoya University, Japan††Waseda University, Japan\nABSTRACT\nThis paper describes music information retrieval (MIR)\nsystems featuring automatic speech recognition. Al-though various interfaces for MIR have been proposed,speech-recognition interfaces suitable for retrieving musi-\ncal pieces have not been studied. We propose two differ-\nent speech-recognition interfaces for MIR, speech com-\npletion andspeech spotter , and describe two MIR-based\nhands-free jukebox systems that enable a user to retrieveand play back a musical piece by saying its title or the\nartist’s name. The ﬁrst is a music-retrieval system with\nthespeech-completion interface that is suitable for mu-\nsic stores and car-driving situations. When a user can re-member only part of the name of a musical piece or anartist and utters only a remembered fragment, the system\nhelps the user recall and enter the name by completing\nthe fragment. The second is a background-music play-back system with the speech-spotter interface that can en-\nrich human-human conversation. When a user is talk-\ning to another person, the system allows the user to enter\nvoice commands for music-playback control by spotting\na special voice-command utterance in face-to-face or tele-phone conversations. Our experimental results from useof these systems have demonstrated the effectiveness of\nthespeech-completion andspeech-spotter interfaces.\nKeywords: speech recognition, MIR interface, hands-free MIR,\njukebox, title and artist search\nVideo demonstration: http://staff.aist.go.jp/m.goto/ISMIR2004/\n1. INTRODUCTION\nThe purpose of this study is to build a music-retrieval\nsystem with a speech-recognition interface that facilitatesboth identiﬁcation of a musical piece and music playbackin everyday life. We think a speech-recognition interface\nis well-suited to music information retrieval (MIR), espe-\ncially retrieval of a musical piece by entering its title or theartist’s name. At home or in a car, for example, an MIR-based jukebox system with a speech-recognition interfacewould allow users to change background music just by\nsaying the name of a musical piece or an artist. At music-\nlistening stations in music stores or on karaoke machines,\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copiesare not made or distributed for proﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2004 Universitat Pompeu Fabra.\na speech-recognition interface could also help users ﬁnd\nmusical pieces they have been looking for without usingany input device other than a microphone.\nMost previous MIR research, however, has not ex-\nplored how speech recognition can be used for retrieving\nmusic information, although various MIR interfaces us-\ning text, symbols, MIDI, or audio signals have been pro-posed. To retrieve a musical piece, a typical approach isto use a text query related to bibliographic information.This approach requires the use of hand-operated input de-\nvices, such as a computer keyboard, mouse, or stylus pen.\nAnother approach is to use a melody-related query giventhrough symbols, MIDI, or audio signals. In particular,music retrieval through a sung melody is called query byhumming (QBH), and this approach is considered promis-\ning because it requires only a microphone and can easily\nbe used by a novice. However, even though this approachuses a microphone, speech recognition of the names ofmusical pieces and artists has not been considered.\nIn this paper, we describe two original speech-\nrecognition interfaces, speech completion and speech\nspotter , which are suitable for MIR. Using these inter-\nfaces, we have built two MIR-based jukebox systems thatallow users to ﬁnd and play back a musical piece by say-ing its title or the artist’s name.\nMusic-retrieval system with the speech-completion in-\nterface\nThis system enables a user to retrieve a musical piece\nor a list of musical pieces by an artist even if the usercan remember only part of the name of the piece orartist. A user cannot similarly enter an incompletename into current speech recognizers because these\nrecognizers tacitly force the user to utter the entire\nname carefully and precisely. Our system, on the otherhand, allows a user to enter the name by uttering a re-membered fragment of it: the system can complete the\nrest (i.e., ﬁll in the missing part) of the partially uttered\nfragment.\nMusic playback system with the speech-spotter inter-\nface\nThis system enables a user to listen to background mu-sic by uttering the name of a musical piece or artistwhile talking to another person. It has been difﬁcult to\nuse current speech recognizers in the midst of human-\nhuman conversation because it is difﬁcult to judge,from microphone input only, whether a user is speak-ing to another person or a speech recognizer. Our sys-tem, on the other hand, allows a user to enter voice\ncommands for music-playback control in the midst offace-to-face or telephone conversations: the systemcanspot (identify) a special voice-command utterance\nin human-human conversations without otherwise in-\nterrupting the conversations.\nIn the following sections, we explain both of these\nspeech-capable music jukebox systems and describe their\nimplementation. We then show experimental resultswhich have demonstrated the effectiveness of our speech-recognition interfaces.\n2. MUSIC-RETRIEVAL SYSTEM WITH THE\nSPEECH-COMPLETION INTERFACE\nIn this system, a user can retrieve a musical piece by ut-\ntering a fragment of its title or the artist’s name with anintentional ﬁlled pause (the lengthening of a vowel dur-\ning hesitation). In human-human conversation, when a\nspeaker cannot remember the entire name of a piece or\nan artist and hesitates while uttering the name, a listenerwill sometimes help the speaker recall it: the listener sug-gests options by completing the partially uttered fragment\n(i.e., by ﬁlling in the rest of it). For example, when\na speaker cannot remember the last part of a Japanese\nphrase “maikeru jakuson”\n(in English, “Michael Jackson”)1\nand stumbles, saying “maikeru–\n ”2(in English, “Michael,\nuh...\n” or “Michael–\n ”)with a ﬁlled pause “ru–\n ”(“uh...\n ”o r\n“l–\n”),3a listener can help the speaker by asking whether\nthe speaker intends to say “maikeru jakuson” (“Michael\nJackson”) . Our system can provide this completion assis-\ntance to users.\nThe concept of completing a fragment is widely used\nin text-based interfaces. Several text editors (e.g., Emacs)and UNIX shells (e.g., tcsh and bash), for example, pro-vide functions for completing the names of ﬁles and com-\nmands. These functions ﬁll in the rest of a partially typed\nfragment when a completion-trigger key (typically the Tabkey) is pressed. Completion functions for pen-based inter-faces have also been proposed [1]. However, even thoughcompletion is so convenient that it often becomes indis-\npensable to users, an effective completion function for\nspeech input interfaces has not been developed becausethere has been no effective way to trigger the function dur-ing natural speech input and current speech recognizershave difﬁculty recognizing a partially uttered fragment.\nWe therefore propose the speech-completion interface\nwhich completes the names of musical pieces and artistsand displays completion candidates so that a user can se-\nlect the correct one (Figures 1, 2, and 3). The most im-\nportant point is that we use an intentional ﬁlled pause(a vowel-lengthening hesitation like “er...\n”) to trigger this\n1When a foreign name like “Michael Jackson” is written or pro-\nnounced in Japanese, it is regularized to conform to the Japanese style:“maikeru jakuson. ”\n2In this paper, underlining indicates that the underlined syllable is\nprolonged (a ﬁlled pause is uttered).\n3In Japanese, vowel-lengthening hesitations like “maikeru–\n ” (sound-\ning like “Michael–\n ” in English) are very common, while inserted-ﬁller\nhesitations like “Michael, uh...\n ” are usually used in English.speech-completion function. To prevent this kind of com-\npletion assistance becoming annoying, it should be in-voked only when a user wants to obtain completion candi-dates. Because the ﬁlled pause is a typical hesitation phe-\nnomenon that indicates a user is having trouble thinking\nof or recalling a subsequent word [2], its use is natural\n4\nand makes this speech-completion function effective andpractical.\nThis interface provides three beneﬁts:\n1. A user can more easily recall poorly remembered\nnames.\n2. Less labor is needed to input a long name. For ex-\nample, you can enter a song title “Supercalifragilis-\nticexpialidocious” (a song from “Mary Poppins”) byuttering “Supercalifra–\n, No.1”.\n3. The user is not forced to utter the entire name carefully\nand precisely, as is required by most current speech-recognition systems.\n2.1. Search methods\nOur music-retrieval system provides two speech-based\nsearch methods for a music database: a method of specify-ing the musical-piece title and a method of specifying the\nartist’s name. For the latter method, the system shows, on\nthe screen, a numbered list of titles for the speciﬁed artistin the music database, and a user can select an appropriatetitle by uttering either the title or its number (Figure 3).After the musical piece is identiﬁed by either method, the\nsystem plays back its sound ﬁle in our current implemen-\ntation. This interface can also be used for playing back theappropriate standard MIDI ﬁle (SMF) or karaoke track,\nmaking a play list, or editing and downloading music ﬁles.\nWhen a user enters the names of musical pieces or\nartists, the system allows the user to complete speech ineither a forward or backward direction:\n1.Forward speech completion (Figure 1)\nA user who does not remember the last part of a namecan invoke this completion by uttering the ﬁrst part\nwhile intentionally lengthening its last syllable (mak-\ning a ﬁlled pause). Here, the user can insert a ﬁlledpause at an arbitrary position (even within a word) .\nThe user then gets a numbered list of completion can-didates whose beginnings acoustically resemble the\nuttered fragment.\n2.Backward speech completion (Figure 2)\nA user who does not remember the ﬁrst part of a\nname can invoke this completion by uttering the lastpart after intentionally lengthening the last syllable ofa predeﬁned special keyword — called the wildcard\nkeyword (In the current implementation, we use the\nJapanese wildcard “nantoka”\n(in English, “something”) ,\nand a user can utter, for example, “nantoka–\n jaku-\nson” (“something–\n Jackson”)5). The user then gets a\nnumbered list of completion candidates whose end-\n4This is especially true for Japanese, a moraic language in which\nevery mora ends with a vowel that can be lengthened. In fact, speakerstypically use ﬁlled pauses to gain time to recall a word or to wait for alistener to help with word choice.\n5This form of expression is very natural in Japanese.Forward Speech Completion\n(1) Uttering “maikeru–\n .” (3) Uttering “No. 2. ”[Entering the phrase “maikeru jakuson” (“Michael Jackson”)\nwhen its last part ( “jakuson” ) is uncertain.]\n(2) A pop-up window containing\ncompletion candidates appears.\n(4) The second candidate is\nhighlighted and bounces.\n(5) The selected candidate “maikeru jakuson”\nis determined as the recognition result.\nFigure 1 . Screen snapshots of forward speech completion.\nBackward Speech Completion\n(1) Uttering “nantoka–\n .” (3) Uttering “jakuson. ” (5) Uttering “No. 1. ”\n(wildcard keyword)[Entering the phrase “maikeru jakuson” (“Michael Jackson”)\nwhen its ﬁrst part ( “maikeru” ) is uncertain.]\n(2) A pop-up window with colorful ﬂying\ndecorations appears.\n(4) A window containing completion\ncandidates appears.\n(6) The ﬁrst candidate “maikeru jakuson”\nis determined as the recognition result.\nFigure 2 . Screen snapshots of backward speech completion.\nMusic Playback[Playing back a musical piece of the artist “maikeru jakuson” (“Michael Jackson”)\nwhose name is determined by the speech-completion interface (Figures 1 and 2).]\n(1) Continued from\nFigure 1 (5) or 2 (6).\n(2) A pop-up window containing a list of musical pieces appears.(3) Uttering\n“No. 1. ”\n(4) The ﬁrst musical piece is highlighted and played back.\nFigure 3 . Screen snapshots of music playback.\nings acoustically resemble the uttered last part. Com-\npletion candidates are generated by replacing the wild-card keyword (ﬁlling in the ﬁrst part) as if a wildcardsearch was done.\nThe user can see other candidates by uttering the turning-\nthe-page phrases, “next candidates” and “previous candi-dates,” displayed whenever there are too many candidates\nto ﬁt onto the screen. If all the candidates are inappropri-\nate or the user wants to enter another name, the user cansimply ignore the displayed candidates and proceed withthe next utterance. When the user selects one of the can-didates by saying (reading out) either its number, the rest\nof the name, or the entire name, that name is highlightedand used for the music playback (Figure 3).\n2.2. Implementation\nFigure 4 shows a block diagram of the method for imple-\nmenting the speech-completion interface: the two main\nprocesses are the ﬁlled-pause detector (Section 2.2.1) and\nspeech recognizer (Section 2.2.2). All the names of\nartists, musical pieces, and the corresponding sound ﬁlesAudio signal input\nFilled-pause detectorFilled-pause\nperiod Speech recognizer\ncapable of listing\ncompletion candidates\nRecognition resultRecognition resultFeature extractor\nInterface managerCompletion candidatesEndpoint detector\n(Utterance detector)\nJukebox player\nFigure 4 . Method for implementing the speech-\ncompletion interface.\nare stored on an SQL (Structured Query Language) data-\nbase server. Each name of the artists and their pieces isalso registered as a single word in the system vocabularyof the speech recognizer. Note that we cannot convert anuttered fragment of a vocabulary word into text by using\nan up-to-date HMM-based speech recognizer\n6because\nthe recognizer only accepts a combination of vocabularywords: it is therefore necessary to extend the speech rec-ognizer to deal with word fragments.\n2.2.1. Filled-pause detector\nTo detect ﬁlled pauses in real time, we use a robust ﬁlled-\npause detection method [3]. This is a bottom-up method\nthat can detect a lengthened vowel in any word througha sophisticated signal-processing technique. It determinesthe beginning and end of each ﬁlled pause by ﬁnding twoacoustical features of ﬁlled pauses — small fundamental\nfrequency (voice pitch) transitions and small spectral en-\nvelope deformations.\n2.2.2. Speech recognizer capable of listing completion\ncandidates\nWe extended a typical speech recognizer to provide a list\nof completion candidates whenever a ﬁlled pause was de-tected [4, 5]. Because single phonemes cannot be rec-\nognized accurately enough, up-to-date speech recognizers\ndo not determine a word’s phoneme sequence phonemeby phoneme. Instead, they choose the maximum likeli-hood (ML) hypothesis while pursuing multiple hypothe-ses on a vocabulary tree where all vocabulary words (i.e.,\nnames of artists and musical pieces) are stored. When the\nbeginning of a ﬁlled pause is detected, the recognizer de-termines which completion method is to be invoked (for-ward or backward) on the basis of whether the wildcardkeyword is the ML hypothesis at that moment.\nIn forward speech completion, completion candidates\nare obtained by deriving from the vocabulary tree those\nwords that share the preﬁx corresponding to each incom-\nplete word hypothesis for the uttered fragment: the candi-dates are obtained by tracing from the top 15 hypothesesto the leaves on the tree.\nIn backward speech completion, it is necessary to ob-\ntain completion candidates by recognizing a last-part frag-\nment uttered after the wildcard keyword, which is not reg-\n6If this text conversion was possible, the problem we are solving\nwould be similar to text-based completion and much easier. However,there is no such thing as a universal speech (phonetic) typewriter.\nSpeaker A: Let’s change the background music.\nSpeaker B: How about Michael Jackson?\nSpeaker A: Uhm...\n , okay. I like his hit song from 1991.\nSpeaker B: Yeah, the title is “Black or White.”Speaker A: Uhm...\n,\nBlack or White. (with a high pitch)\nThe system then plays the song “Black or White.”\nFigure 5 . An example of using the music playback system\nwith the speech-spotter interface.\nistered as a vocabulary word. We therefore introduced an\nentry node table that lists the roots (notes) from which the\nspeech recognizer starts searching. Just after the wildcard\nkeyword, every syllable in the middle of all the vocabu-\nlary words is temporarily added to the table. Then, afterthe last-part fragment is uttered, the hypotheses that havereached leaves are considered completion candidates.\n3. MUSIC PLAYBACK SYSTEM WITH THE\nSPEECH-SPOTTER INTERFACE\nIn this system, a user can listen to background music by\nsaying its title or the artist’s name while talking to another\nperson, as illustrated in Figure 5. For this system to bepractical, it must be able to monitor human-human conver-sations without disturbing them and provide music play-back only when asked for it. We think such on-demand\nmusic playback assistance in human-human conversation\nis useful and convenient because it does not require theuse of any input device other than a microphone.\nIt has been difﬁcult, however, to achieve a practical\nmeans of providing such assistance by using only micro-\nphone input. Previous approaches using only speech in-formation detected keywords in speech signals by meansof word-spotting technology [6, 7]. These techniques,though, are poor at judging, without the context being re-\nstricted in advance, whether the detected keywords are in-\ntended to be command utterances (voice commands for\nmusic-playback control) for a computer system or conver-\nsational utterances for a conversational partner. Although\nthere have been other spotting approaches which required\nthat an utterance intended for the system be preceded by\na keyword, such as Computer ,Casper ,o rMaxwell , this\nrestricted the usual behavior of a user: the user was forcedto avoid use of the keyword in human-human conversa-tion in front of the microphone. Other previous speech-\ninterface systems have had to use other input devices such\nas a button or a camera [8, 9]. In short, no previousapproach allowed a system to identify a command utter-\nance in conversation without the context being restricted\nor some other device being used.\nWe therefore developed the speech-spotter interface\nwhich enables a user to request music playback only whendesired while talking to another person. This interface re-\ngards a user utterance as a command utterance only when\nit is intentionally uttered with a high pitch just after aﬁlled pause such as “er...\n” or “uh...\n ”. In other words, a\ncomputer system accepts this specially designed unnatu-ral utterance only and ignores other normal utterances in\nhuman-human conversation. For example, when a userenters the title of a musical piece by saying “Er...\n(a\nﬁlled pause) ,\nBlack or White (an utterance with a high\npitch) ”,7the system plays back the corresponding sound\nﬁle. For this speech-spotter utterance, we use the unnatu-ralness of nonverbal speech information — in this case anintentional ﬁlled pause and a subsequent high-pitch utter-ance — because this combination is not normally uttered\nin (Japanese) human-human conversation but nevertheless\ncan be easily uttered.\nThis interface provides three beneﬁts:\n1. In human-human conversation, speech-based assis-\ntance can immediately be used whenever needed.\n2. A hands-free interface system with only a microphone\nis achieved. A user is free regarding body movementand can use the system even during a telephone con-versation.\n3. A user can feel free to use any words in conversation\nwith another person. The user does not have to care-\nfully avoid saying anything that the system will accept\nas input.\n3.1. Search methods\nOur music playback system supports the following search\nmethods:\nSpecifying the musical-piece title\nWhen the title of a musical piece is uttered, such as\n“Er...\n,\nWithout You”, the system plays back the corre-\nsponding sound ﬁle. It also either shows the title onthe screen, has a speech synthesizer read out the title,or both.\nSpecifying the artist’s name\nWhen the name of an artist is uttered, such as “Uhm...\n,\nMariah Carey”, the system shows a numbered list ofmusical-piece titles for that artist on the screen or hasthe speech synthesizer read out the list. After the userselects a musical piece by saying the speech-spotterutterance of either the title or its number, the system\nplays back the piece. It also highlights the selected\ntitle or reads out the title.\nThe system allows the user to say speech-spotter ut-\nterances at any time by overlapping and interrupting thespeech synthesis or music playback. The user, for exam-ple, can stop the music playback by saying “Uh...\n,\nstop”,\nor change the current piece by saying another title.\nThis system is useful not only when a user would like\nto enjoy background music, but also when a user wouldlike to talk about music in telephone conversations whilelistening to it; note that the system does not disturb suchconversations. In particular, this is very effective for peo-\nple who like to listen to music in everyday life because it\nmakes it much easier for them to share background musicand discuss it during playback on the telephone. The sys-tem can also be used to change background music in anactual room where people are talking.\n7In this paper, overlining indicates that the pitch of the underlined\nwords is intentionally raised by a user.Beginning and\nend pointsAudio signal input\nFilled-pause detector\nMFCCFilled-pause period Endpoint detector\n(Utterance detector)\nF0 estimatorSpeech recognizer\n(Modified CSRC toolkit)\nUtterance classifier\n(Pitch classifier)\nJukebox playerPitch (F0)Maximum likeli-\nhood hypothesis\nSpeech-spotter utteranceRecognition resultFeature extractor\nFigure 6 . Method for implementing the speech-spotter\ninterface.\n3.2. Implementation\nFigure 6 shows a block diagram of the method for im-\nplementing the speech-spotter interface. The four main\nprocesses are the ﬁlled-pause detector (Section 2.2.1),\nendpoint detector (Section 3.2.1), speech recognizer\n(Section 3.2.2), and utterance classiﬁer (Section 3.2.3).\nLike the implementation of the speech-completion inter-\nface, all the names are stored on the SQL database server\nand each name is registered as a single word in the systemvocabulary.\nSpeech-spotter utterances can be detected through the\nfollowing four steps:\n1. Each ﬁlled pause is detected by the ﬁlled-pause detec-\ntordescribed in Section 2.2.1.\n2. When triggered by a detected ﬁlled pause, the end-\npoint detector determines the beginning of an utter-\nance.\n3. While the content of the utterance is being recognized\nby the speech recognizer , the end of the utterance is\nautomatically determined by the endpoint detector .\n4. The average pitch of the utterance whose beginning\nand end points were determined above is judged to be\nhigh or normal by the utterance classiﬁer .\n3.2.1. Determining the beginning of an utterance (end-\npoint detector)\nWhenever a ﬁlled pause is detected, the beginning of the\nsubsequent utterance is determined as being 130 ms be-fore the end of the ﬁlled pause — i.e., as being in the\nmiddle of the ﬁlled pause. Every lengthened vowel (and\nsubsequent consonant if necessary) should be inserted atthe beginning of the grammar.\n3.2.2. Determining the end of an utterance (endpoint de-\ntector and speech recognizer)\nAfter the HMM-based speech recognizer starts decoding\nthe current utterance, the endpoint detector checks the\nML hypothesis (intermediate speech-recognition result)for every frame. In the frame-synchronous Viterbi beamsearch, if the ML hypothesis stays at a unique node that is\nnot shared by other words in a tree dictionary or a silence\nnode that corresponds to the silence at the end of a sen-tence, its frame is considered the end of the utterance andthespeech recognizer stops decoding [10].3.2.3. Judging the voice pitch (utterance classiﬁer)\nOn the basis of a speaker-independent pitch-classiﬁcation\nmethod using a threshold relative to the base fundamental\nfrequency (base F0) [11], the utterance classiﬁer ﬁlters\nout normal-pitch utterances to obtain high-pitch speech-\nspotter utterances. The base F0 is a unique pitch referencethat corresponds to the pitch of the speaker’s natural voiceand can be estimated by averaging the voice pitch duringa ﬁlled pause. If the relative pitch value of an utterance,\nwhich is calculated by subtracting the base F0 from the\npitch averaged over the utterance, is higher than a thresh-old, the utterance is judged to be a speech-spotter utter-\nance .\n4. EXPERIMENTAL RESULTS\nWe describe the results from evaluating the effectiveness\nof the system with the speech-completion interface and\nthe system with the speech-spotter interface.\n4.1. Evaluation of the speech-completion interface\nWe tested the system with 45 Japanese subjects (24 male,\n21 female). To evaluate whether the subjects preferred touse speech completion after gaining a good command of\nit, we measured the usage frequencies of speech comple-\ntion under two conditions: (a) when a subject input a set ofname entries from a list by freely using speech completionaccording to personal preference, and (b) when a subjecthad to recall and input vaguely remembered entries.\nWe found that the average usage frequency of speech\ncompletion was 74.2% and 80.4%, respectively, for con-ditions (a) and (b). These results showed that the sub-jects preferred to use the speech-completion function even\nwhen they could choose not to use it. Subjective ques-\ntionnaire results indicated that the speech-completion in-terface was helpful and easy to use, and made it easy torecall and input uncertain phrases.\n4.2. Evaluation of the speech-spotter interface\nWe analyzed the detection performance for speech-spotter\nutterances on a 40-minute corpus consisting of both nor-\nmal utterances of sentences naturally spoken with sponta-neous ﬁlled pauses and speech-spotter utterances of 218names of musicians and songs by 12 Japanese subjects.\nWe found that the recall and precision rates for detect-\ning speech-spotter utterances were 0.78 and 0.77, respec-tively. In our experience with the music playback sys-tem, users without any training were able to start play-back of background music and change it while talking on\ncellular or normal phones. They felt that the practical per-\nformance was much higher than the above rates indicatebecause visual feedback enabled the users to know howlong a vowel should be lengthened during a ﬁlled pause.\nAlthough melody ringers (cellular phone ring-tones) are\nwidely used, our users had no previous experience of lis-tening to music in the midst of telephone conversation,and appreciated its novelty and usefulness.5. CONCLUSION\nWe have described two speech-recognition interfaces suit-\nable for MIR, speech completion andspeech spotter , and\ndemonstrated their usefulness in two different music juke-\nbox systems. The music-retrieval system with the speech-\ncompletion interface enables a user to listen to a musical\npiece even if part of its name cannot be recalled. The mu-sic playback system with the speech-spotter interface en-\nables users to share music playback on the telephone as if\nthey were talking in the same room with background mu-\nsic. As far as we know, this is the ﬁrst system that peoplecan use to obtain speech-based music information assis-tance in the midst of a telephone conversation .\nWe believe that practical speech-recognition interfaces\nfor MIR cannot be achieved by simply applying the cur-\nrent automatic speech recognition to MIR: retrieval of mu-sical pieces just by uttering entire titles or artist namesis not sufﬁcient. The two interfaces described in this pa-per can be considered an important ﬁrst step toward build-\ning the ultimate speech-capable MIR interface. It will be-\ncome more and more important to explore various speech-recognition interfaces for MIR as well as other traditionalMIR interfaces.\nIn the future, we plan to build a uniﬁed system where\na user can use, from a single microphone input, both of\nthe systems described in this paper as well as a query-by-humming (QBH) system to leverage the potential afﬁnitybetween speech-recognition interfaces and QBH systems.\n6. REFERENCES\n[1] T. Masui. An efﬁcient text input method for pen-based\ncomputers. Proc. of CHI’98 , pp. 328–335, 1998.\n[2] E. Shriberg. To ‘errrr’ is human: ecology and acoustics of\nspeech disﬂuencies. Journal of the International Phonetic\nAssociation , 31(1):153–169, 2001.\n[3] M. Goto, K. Itou, and S. Hayamizu. A real-time ﬁlled\npause detection system for spontaneous speech recogni-tion. Proc. of Eurospeech ’99 , pp. 227–230, 1999.\n[4] M. Goto, K. Itou, T. Akiba, and S. Hayamizu. Speech com-\npletion: New speech interface with on-demand completionassistance. Proc. of HCI International 2001 , volume 1, pp.\n198–202, 2001.\n[5] M. Goto, K. Itou, and S. Hayamizu. Speech completion:\nOn-demand completion assistance using ﬁlled pauses for\nspeech input interfaces. Proc. of ICSLP 2002 , pp. 1489–\n1492, 2002.\n[6] J. R. Rohlicek, W. Russell, S. Roukos, and H. Gish. Con-\ntinuous hidden Markov modeling for speaker-independent\nword spotting. Proc. of ICASSP 89 , pp. 627–630, 1989.\n[7] T. Kawahara, K. Ishizuka, S. Doshita, and C.-H. Lee.\nSpeaking-style dependent lexicalized ﬁller model for key-\nphrase detection and veriﬁcation. Proc. of ICSLP 98 , pp.\n3253–3256, 1998.\n[8] K. Nagao and A. Takeuchi. Social interaction: Multimodal\nconversation with social agents. Proc. of AAAI-94 , vol-\nume 1, pp. 22–28, 1994.\n[9] Y . Matsusaka et al. Multi-person conversation via multi-\nmodal interface — a robot who communicate with multi-\nuser. Proc. of Eurospeech ’99 , pp. 1723–1726, 1999.\n[10] K. Kitayama, M. Goto, K. Itou, and T. Kobayashi. Speech\nstarter: Noise-robust endpoint detection by using ﬁlled\npauses. Proc. of Eurospeech 2003 , pp. 1237–1240, 2003.\n[11] M. Goto, Y . Omoto, K. Itou, and T. Kobayashi. Speech\nshift: Direct speech-input-mode switching through inten-\ntional control of voice pitch. Proc. of Eurospeech 2003 ,\npp. 1201–1204, 2003."
    },
    {
        "title": "Dance music classification: A tempo-based approach.",
        "author": [
            "Fabien Gouyon",
            "Simon Dixon"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416636",
        "url": "https://doi.org/10.5281/zenodo.1416636",
        "ee": "https://zenodo.org/records/1416636/files/GouyonD04.pdf",
        "abstract": "Recent research has studied the relevance of various features for automatic genre classification, showing the particular importance of tempo in dance music classifica- tion. We complement this work by considering a domain- specific learning methodology,where the computed tempo is used to select an expert classifier which has been spe- cialised on its own tempo range. This enables the all-class learning task to be reduced to a set of two- and three-class learning tasks. Current results are around 70% classifi- cation accuracy (8 ballroom dance music classes, 698 in- stances, baseline 15.9%).",
        "zenodo_id": 1416636,
        "dblp_key": "conf/ismir/GouyonD04",
        "keywords": [
            "tempo",
            "dance music",
            "genre classification",
            "domain-specific learning",
            "expert classifier",
            "tempo range",
            "all-class learning",
            "two-class learning",
            "three-class learning",
            "classification accuracy"
        ],
        "content": "DANCE MUSIC CLASSIFICA TION: ATEMPO-B ASED APPR OACH\nFabien Gouyon\nUniversitat Pompeu Fabra\nBarcelona, SpainSimon Dixon\nAustrian Research Institute forAI\nVienna, Austria\nABSTRA CT\nRecent research hasstudied therelevance ofvarious\nfeatures forautomatic genre classi\u0002cation, showing the\nparticular importance oftempo indance music classi\u0002ca-\ntion. Wecomplement thisworkbyconsidering adomain-\nspeci\u0002c learning methodology ,where thecomputed tempo\nisused toselect anexpert classi\u0002er which hasbeen spe-\ncialised onitsowntempo range. This enables theall-class\nlearning tasktobereduced toasetoftwo-andthree-class\nlearning tasks. Current results arearound 70% classi\u0002-\ncation accurac y(8ballroom dance music classes, 698in-\nstances, baseline 15.9%).\n1.INTR ODUCTION\nTempo isamusical attrib uteofprime importance. More-\nover,recent research [3]advocated itsrelevance inthe\ntask ofclassifying different styles ofdance music: fo-\ncusing solely onthecorrecttempo (i.e. measured manu-\nally)8classes ofStandard andLatin ballroom dance mu-\nsiccanbeclassi\u0002ed, bymeans ofdiverse classi\u0002cation\ntechniques, with around 80% accurac y(total of698 in-\nstances, classes arelisted inTable 1,baseline is15.9%).\nDecision treeclassi\u0002ers revealed aclear ordering ofdance\nstyles with respect totempi. Therefore, onecanassume\nthat, givenamusical genre, thetempo ofanyinstance is\namong averylimited setofpossible tempi. Forinstance,\nthetempo ofaCha Cha isusually between 116and128\nBPM. Table 1givestempo ranges forthe8dance styles\nused here.\nThis assumption may bearguable, yetitseems tomake\nsense forballroom dance music as,ontheonehand, com-\nmon musical knowledge (e.g. instructional books, dance\nclass websites1)suggests such boundaries, andonthe\nother hand, [4]showsonalargeamount ofdata (more\nthan 90000 instances) that different dance music styles\n(trance, afro-american, house andfast) showclearly dif-\nferent tempo distrib utions, centered around different typ-\nical tempi.\n1seee.g.http://www .ballr oomdance rs.com/Dan ces\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forpro\u0002t orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation onthe\u0002rstpage.\nc\n\u00002004 Universitat Pompeu Fabra.Inthispaper wepropose tofurther exploit thehigh rel-\nevance ofthetempo indesigning aclassi\u0002er thatfocuses\n\u0002rstonthisfeature andthen uses complementary features\ntomakedecisions inpossibly ambiguous situations (i.e.\ntempo overlaps).\nOurapproach istode\u0002ne thetempo range ofeach class\nbyaGaussian probability function. Anillustration isgiven\ninFigure 1.TheGaussian standard deviations arede\u0002ned\nsothattheprobabilities atthelimits speci\u0002ed inTable 1\narehalfthevalue ofthecorresponding probability maxi-\nmum. Puttogether ,these probabilities may overlap incer-\ntaintempo regions (e.g. Samba andRumba, seedashed-\nblue andsolid-black lines around 100BPM inFigure 1).\n80 100 120 140 160 180 20000.20.40.60.81\nFigur e1.Tempo probability functions andoverlaps of8\ndance music styles. X-axis inBPM.\nHence, givenanunkno wninstance, asimple classi\u0002ca-\ntionprocess could be:\n1:Compute itstempo \u0001\n2:Retrie vethe \u0002classes thatoverlap signi\u0002cantly at \u0001\n3:Useaclassi\u0002er tailored tothese \u0002classes\nAssuming that different classes havedifferent tempo\ndistrib utions, itisreasonable toconsider thatmuch less\nthan 8classes dooverlap signi\u0002cantly atanytempo. The\nclassi\u0002er design isconsequently easier than theeight-class\nlearning taskconsidering allexamples.\nHowever,there isaconsensus inthetempo-tracking\nliterature onthefactthatstate-of-the-art tempo induction\nalgorithms typically makeerrors ofmetrical levels(they\noutput e.g.halfthecorrect tempo). Accordingly ,thetempo-\ntracking algorithm weusehere BeatRoot [1]may\noutput thecorrect tempo, twice orhalfofitsvalue (inthe\ncase ofexcerpts with aduple meter) ortwothirds ofits50 100 150 200 25000.511.522.53\nFigur e2.Adapted tempo probability functions of8dance music styles. X-axis inBPM. Solid black lineisthesum ofall\nprobability functions andrepresents overall class overlaps.\nvalue (inthecase ofexcerpts with atriple meter).2We\nadapted each tempo probability function accordingly in\nconcatenating severalGaussians whose means arecorrect\ntempi andrelevantmultiples (seeFigure 2).\nObserving theprobability functions inFigure 2,one\ncanseethateach tempo value corresponds usually totwo\ndifferent potential classes, attheexception ofthree spe-\nci\u0002c tempo regions inwhich three classes overlap. These\nare95to105BPM and193to209BPM forQuickstep,\nRumba andSamba, and117 to127 BPM forCha Cha,\nTango andViennese Waltz. Therefore, wepropose tobuild\n30different classi\u0002ers:\u0003\u0005\u0004\u0007\u0006\t\b\u000b\n\f\u000e\r\n\u0002\u0005\u000f\u0011\u0010\u000e\u0012 two-class classi\u0002ers, \u0013\u0015\u0014\n\u0017\u0016\u0018\u0016\t\u0016\n\u0014\u001a\u0019\u0006\u001c\u001b,\neach expert inaspeci\u0002c pairwise classi\u0002cation task.\u00032three-class classi\u0002ers, \u0014\u001d\u0019\u001f\u001e and \u0014\u001a \"!,each expert\ninathree-class speci\u0002c task\nWhen presented with unkno wninstances, theknowl-\nedge available tothesystem isthissetof30expert clas-\nsi\u0002ers andthetempo probability functions forallpossible\nclasses. Therefore, theoverall classi\u0002cation process is\u0002-\nnally thatdetailed inAlgorithm 1.\nAlgorithm 1Overall classi\u0002cation process\n1:Compute tempo \u0001oftheinstance toclassify\n2:Find theclassi\u0002er \u0014\u001a#whose tempo range includes \u0001\n3:Perform classi\u0002cation with \u0014\u001d#\nIntheremainder ofthispaper ,wegivethedetail of\ntheaudio data used forexperiments. Then weintroduce\nthediverse features computed from thisdata. Wethen\n2Forfurther tempo induction evaluation details, onthedatabase used\nhere, werefer to[3].detail ourexperiments, discuss theresults, compare them\ntotheresults reported in[3]andpropose asummary and\ndirections forfuture work.\n2.DATAAND ASSOCIA TED MET ADATA\nThemusical database weusefortraining andtesting con-\ntains excerpts from 698pieces ofmusic, around 30sec-\nonds long. The audio quality ofthisdata isquite low,it\nwasoriginally fetched inrealaudio format, with acom-\npression factor ofalmost 22with respect tothecommon\n44.1 kHz 16bitsmono WAVformat. Itwassubsequently\nconverted toWAVformat forexperiments. This data is\npublically available ontheworld-wide web atthefollo w-\ningURL:\nhttp://www .ballr oomdancer s.com/Music/style .asp\nChaCha 111inst.$\u000e$\u0018%'&\u0005$\u0015\u0010\u000e\u0012 BPM\nJive 60inst. $\u0018%)('&*$\u0015\u0012\u000e( BPM\nQuickstep 82inst.$\u0018+\u000e\u0012'&,\u0010-$\u0015( BPM\nRumba 98inst. +\u000e(.&\u0005$\u000e$\u0015( BPM\nSamba 86inst.+\u000e%.&\u0005$\u0018(\u000e/ BPM\nTango 86inst. $\u0015\u0010\u000e(0&\u0005$\t/1( BPM\nViennese Waltz 65inst. $324('&\u0005$\u0018+)( BPM\nSlowWaltz 110inst.24\u0012.&5+\u000e\u0012 BPM\nTable 1.Dance music classes, number ofinstances per\nclass andclass tempo ranges.\nForallthose recordings, themusical genre isavailable\n(see Table 1).Inaddition, thecorrect tempo, assigned\nmanually ,ofeach recording isalso available (inbeats per\nminute, BPM). Theminimum value is60BPM, themax-\nimum 224BPM.3.DESCRIPT ORS\nWeconsider 71descriptors, divided intothree groups. All\nareimplemented asopen source softw areunder theGNU\nlicense.\n3.1. Tempo\nInaddition totheground truth tempo, inBPM, asprovided\nwith thedata, wealso computed thetempo using Beat-\nRoot [1].BeatRoot' stempo induction stage yields several\ntempo hypotheses thataresubsequently re\u0002ned, beat by\nbeat, andrankedinatracking process. The\u0002nal tempo is\nthemean ofthewinning agent' sinter-beat interv als.\nThefollo wing 69features describe low-levelcharacter -\nistics of2different periodicity representations (i.e. distri-\nbution statistics ascentroid, \u0003atness, etc.orpeak-related\nquantities).\n3.2. Periodicity Histogram descriptors\n11descriptors arebased ona\u0002rstrepresentation ofsignal\nperiodicities, theperiodicity histogram (PH) [5].This\nrepresentation, isthecollection inahistogram ofthesal-\niences ofdifferent pulses (from 40BPM to240 BPM)\ninsuccessi vechunks ofsignal (12s long, with overlap).\nIneach chunk ofsignal, periodicities arecomputed viaa\ncomb \u0002lterbank.\n3.3. Inter -Onset Inter valHistogram descriptors\nRemaining descriptors arequantities computed from asec-\nond representation ofthesignal periodicities, theInter -\nOnset Interv alHistogram (IOIH) proposed in[2].This\nrepresentation givesameasure ofrecurrence ofthedif-\nferent inter-onset interv alspresent inthesignal (not just\nsuccessi veonsets, butanypairs ofonsets). Inter -onset\ninterv alsareaccumulated inahistogram which isthen\nsmoothed byaGaussian windo w.\nWecomputed thesaliences of10periodicities (promi-\nnent periods intheIOIH) whose periods arethe10\u0002rst\nintegermultiples ofthefastest pulse (computed asin[2]).\nNote thatsolely theperiod salience iskept,nottheperiod\nvalue. Therefore, those descriptors areindependent ofthe\ntempo.\nWealsode\u0002ned 48other descriptors ascommon spec-\ntral descriptors (8distrib ution statistics and40MFCCs),\nbutcomputed ontheIOIH, notonaspectrum. TheMFCC-\nlikedescriptors arecomputed asfollo ws:\u0003IOIH computation\u0003Projection oftheperiod axis from linear scale to\ntheMel scale, oflowerdimensionality (i.e. 40),by\nmeans ofa\u0002lterbank\u0003Magnitude logarithm computation\u0003InverseFourier transformForeach ofthe30classi\u0002cation tasks, wediscarded the\nuseofthetempo andweevaluated therelevances ofthere-\nmaining low-leveldescriptors onanindividual basis (i.e.\nRank ersearch method associated toReliefF attrib uteeval-\nuator), andselected the10most relevantfeatures. That is,\nthe30classi\u0002ers alluse10low-levelfeatures, thatmay\nbedifferent ineach case. Allexperiments havebeen con-\nducted with Weka[6].3\n4.EXPERIMENTS\nForclassi\u0002cation, weuseSupport Vector Machines asitis\ncommonly suggested forproblems with fewclasses (es-\npecially 2). Allpercentages result from 10-fold cross-\nvalidation procedures. Systematic evaluation ofdifferent\nclassi\u0002cation methods islefttofuture work.\nThe majority ofthe28pairwise classi\u0002er accuracies,\nallusing 10descriptors, areabove90%. Theworstclassi-\n\u0002eristhatbetween SlowWaltzandViennese Waltz(81.8%\naccurac y,baseline 63%). Thebestisthatbetween Quick-\nstepandViennese Waltz(100% accurac y,baseline 55.7%).\nRegarding thethree-class classi\u0002ers, alsousing 10descrip-\ntors,\u0014\n\u0019\u001f\u001e(Quickstep vs.Rumba vs.Samba) has84.1%\naccurac y(baseline 36.6%) and\u0014\n \"!(Cha Cha vs.Tango\nvs.Viennese Waltz) 91.9% accurac y(baseline 42.3%).\nTomeasure theoverall accurac yofthe30classi\u0002ers, let\nuscompute aweighted average oftheir individual accu-\nracy.Theweights areproportional tothenumber oftimes\naclassi\u0002er isactually required (giventhetempo estima-\ntions ofthe698excerpts). This yields 89.4% accurac y.\nLetusnowevaluate thewhole classi\u0002cation process.\nRecall thattheprocess involvestwosteps, itsuffersfrom\ntempo estimation errors inaddition tomisclassi\u0002cations.\nIn24.3% ofthecases (i.e. 170excerpts) thetempo es-\ntimation step assigns excerpts topairwise (orthree-class)\nclassi\u0002ers thatdonotaccount foritstrueclass. There isno\nwaytorecoverfrom these errors, whate verthesubsequent\nclassi\u0002cation, theexcerpt willbeassigned toanincorrect\nclass.\nTheoverall accurac yofthesystem istherefore themul-\ntiplication ofboth step accuracies, i.e.(\u0016\n\u0012\u000e+\u001c/\u001d67(\u0016\n2\u000e8)29\u000f\n67.6% .\nOne might wonder whether considering metrical level\nerrors inthedesign ofthetempo probabilities (i.e. using\ntempo probabilitities asde\u0002ned inFigure 2instead ofFig-\nure1)actually results inanyimpro vement. Asreported in\n[3],thetempo induction algorithm hasaround 50% accu-\nracy(considering multiples aserrors). Theresulting over-\nallaccurac yofthemethod presented here would therefore\nbearound (\u0016\n\u0012)+\u001c/:6;(\u0016\n8<\u000f44.7%. Theimpro vement isover\n20%.\nHowever,wenoted thattempo induction isespecially\nbadforSlowWaltzexcerpts, yielding around 75% tobe\nassigned towrong classi\u0002ers. This isbecause onset de-\ntection, inthetempo induction algorithm, isdesigned for\npercussi veonsets, which areoften lacking from waltzes.\nRemo ving theSlowWaltzexcerpts forthedatabase, 587\n3http://www .cs.waikato.ac.nz/ml/w ekaremain, andthenumber ofexcerpts thatareassigned to\nirrele vantclassi\u0002ers fallsto13.9%. Theoverall accurac y\nrises nowto76.5% .\nThose results areencouraging. Theyarehoweverslightly\nlowerthan theresults reported in[3],where therationale\nwastobuild an8-class classi\u0002er (1-Nearest Neighbour\nlearner) with 15MFCC-lik edescriptors andnotempo in-\nformation. There, theclassi\u0002cation accurac yreached 79% .\n5.SUMMAR Y,DISCUSSION AND FUTURE\nWORK\nInthispaper ,weinvestigated theclassi\u0002cation of8dance\nstyles from aparticular viewpoint thatputs aspecial em-\nphasis onthetempo estimation. Theproposed classi\u0002ca-\ntionprocess entails twosubsequent steps: tempo compu-\ntation anduseofexpert (pairwise orthree-class) classi-\n\u0002ers inspeci\u0002c tempo regions. Weshowthatitispossi-\nbletodesign veryaccurate expert classi\u0002ers. However,\nlogically ,inthisframe work,ifthetempo estimation fails,\ntheclassi\u0002cation fails. Weshowed thatconsidering tempo\nmultiples results inasubstantial classi\u0002cation impro ve-\nment.\nThe accurac yonadatabase of698 excerpts from 8\nclasses is67.6%. Restricting tests tothe7classes (587 ex-\ncerpts) onwhich tempo estimation isreasonably reliable,\ntheaccurac yis76.5%. This isslightly worse than results\nreported in[3]with adifferent method (an8-class clas-\nsi\u0002er yielded 79% accurac ywith 15tempo-independent\ndescriptors).\nInconclusion, reducing theproblem from aneight-class\nlearning task toseveraltwo-orthree-class learning tasks\nisonly pertinent when using anextremely reliable tempo\nestimation algorithm.\nToillustrate this, letusconsider using thecorrect tempo\n(assigned manually) instead BeatRoot tempo (computed\nautomatically). There, theclassi\u0002cation accurac yrises to\n82.1%. This corresponds totwofactors: misclassi\u0002ca-\ntions oftheexpert classi\u0002ers (i.e. 0.109%) andthecost\noftheinitial assumption regarding class tempo probabil-\nities (i.e. instances outliers that effectivelyhavea\ntempo outside oftheir class' stempo range, i.e.54of698\ninstances).\nThis opens twoimportant avenues forfuture work: im-\nproving theaccurac yoftheexpert classi\u0002ers (forinstance\ninre\u0002ning theselection ofthemost relevantdescriptors\nforeach classi\u0002er) andstudy thevalidity ofthelimited-\ntempo-ranges assumption onadatabase containing more\ninstances ofalargernumber ofclasses.\nFurther ,astempo-independent dance style classi\u0002ca-\ntion[3]seems tobemore reliable than tempo induction it-\nself(that principally suffersfrom metrical levelerrors)\n79% vs.50%, wewill study infuture workwhether\ngenre classi\u0002cation canbeused toimpro vetempo induc-\ntion.6.ACKNO WLEDGMENTS\nForreading andveryuseful comments, wewish tothank\nGerda Strobl, Elias Pampalk, Gerhard Widmer and(asal-\nways) Pedro Cano, Guenter Geiger andPerfecto Herrera.\nThe Austrian Research Institute forArti\u0002cial Intelli-\ngence alsoackno wledges the\u0002nancial support oftheAus-\ntrian Federal Ministries ofEducation, Science andCulture\nandofTransport, Innovation andTechnology\nThis workwasfunded bytheEU-FP6-IST -507142 project\nSIMA C(Semantic Interaction with Music Audio Contents).\nMore information canbefound attheproject website htt-\np://www .semanticaudio.or g.\n7.REFERENCES\n[1]Dixon S.,Automatic extraction oftempo and\nbeat from expressi veperformances, Journal\nofNewMusic Research, 30(1),2001.\n[2]Gouyon F.,Herrera P.andCano P.,Pulse-\ndependent analyses ofpercussi vemusic,\nProc.AES 22nd International Confer ence,\nHelsinki, 2002.\n[3]Gouyon F.,Dixon S.,Pampalk E.andWidmer\nG.Evaluating rhythmic descriptors formusi-\ncalgenre classi\u0002cation, Proc.25th AES Inter -\nnational Confer ence,London, 2004.\n[4]Moelants D.,Dance music, movement and\ntempo preferences, Proc.5thTriennal ES-\nCOM Confer ence,Hano ver,2003.\n[5]Pampalk E.,Dixon S.andG.Widmer ,Ex-\nploring music collections bybrowsing differ-\nentviews, Proc.International Confer ence on\nMusic Information Retrie val,Baltimore, 2003.\n[6]Witten I.andFrank E.,Data Mining: Prac-\ntical machine learning tools with Javaim-\nplementations, MorganKaufmann ,SanFran-\ncisco, 2000."
    },
    {
        "title": "Melodic Similarity: Looking for a Good Abstraction Level.",
        "author": [
            "Maarten Grachten",
            "Josep Lluís Arcos",
            "Ramón López de Mántaras"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417403",
        "url": "https://doi.org/10.5281/zenodo.1417403",
        "ee": "https://zenodo.org/records/1417403/files/GrachtenAM04.pdf",
        "abstract": "Computing melodic similarity is a very general problem with diverse musical applications ranging from music anal- ysis to content-based retrieval. Choosing the appropriate level of representation is a crucial issue and depends on the type of application. Our research interest concerns the development of a CBR system for expressive music pro- cessing. In that context, a well chosen distance measure for melodies is a crucial issue. In this paper we propose a new melodic similarity measure based on the I/R model for melodic structure and compare it with other existing measures. The experimentation shows that the proposed measure provides a good compromise between discrim- inatory power and ability to recognize phrases from the same song.",
        "zenodo_id": 1417403,
        "dblp_key": "conf/ismir/GrachtenAM04",
        "keywords": [
            "melodic similarity",
            "musical applications",
            "representation choice",
            "CBR system",
            "expressive music processing",
            "distance measure",
            "melodic structure",
            "I/R model",
            "comparison with existing measures",
            "good compromise"
        ],
        "content": "MELODIC SIMILARITY: LOOKING FOR A GOOD ABSTRACTION\nLEVEL\nMaarten Grachten andJosep-Llu ´ıs ArcosandRamon L´opez de M ´antaras\nIIIA-CSIC - Artiﬁcial Intelligence Research Institute\nCSIC - Spanish Council for Scientiﬁc Research\nCampus UAB, 08193 Bellaterra, Catalonia, Spain.\nVox: +34-93-5809570, Fax: +34-93-5809661\nEmail: {maarten,arcos }@iiia.csic.es\nABSTRACT\nComputing melodic similarity is a very general problem\nwithdiversemusicalapplicationsrangingfrommusicanal-\nysis to content-based retrieval. Choosing the appropriate\nlevel of representation is a crucial issue and depends on\nthetypeofapplication. Ourresearchinterestconcernsthe\ndevelopment of a CBR system for expressive music pro-\ncessing. In that context, a well chosen distance measure\nfor melodies is a crucial issue. In this paper we propose\na new melodic similarity measure based on the I/R model\nfor melodic structure and compare it with other existing\nmeasures. The experimentation shows that the proposed\nmeasure provides a good compromise between discrim-\ninatory power and ability to recognize phrases from the\nsame song.\n1. INTRODUCTION\nComputing melodic similarity is a very general problem\nwithdiversemusicalapplicationsrangingfrommusicanal-\nysis to content-based retrieval. Choosing the appropriate\nlevel of representation is a crucial issue and depends on\nthe type of application. For example, in applications such\naspatterndiscoveryinmusicalsequences[1],[4],orstyle\nrecognition [4], it has been established that melodic com-\nparison requires taking into account not only the individ-\nualnotesbutalsothestructuralinformationbasedonmu-\nsic theory and music cognition [12].\nSome desirable features of melodic similarity measure\nare the ability to distinguish phrases from different musi-\ncalstylesandtorecognizephrasesthatbelongtothesame\nsong. We propose a new way of assessing melodic simi-\nlarity, representing the melody as a sequence of I/R struc-\ntures (conform Narmour’s Implication/Realization (I/R)\nmodel for melodic structure [10]). The similarity is then\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2004 Universitat Pompeu Fabra.assessedbycalculatingtheedit-distancebetweenI/Rrep-\nresentationsofmelodies. Wecomparedthisassessmentto\nassessmentsbasedonnoterepresentations[9],andmelodic\ncontour representations [2, 7].\nWeshowthatsimilaritymeasuresthatabstractfromthe\nliteral pitches, but do take into account rhythmical infor-\nmation in some way (like the I/R measure and measures\nthat combine contour information with rhythmical infor-\nmation),provideagoodtrade-offbetweenoveralldiscrim-\ninatory power (using an entropy based deﬁnition) and the\nability to recognize phrases from the same song.\nThe paper is organized as follows: In Section 2 we\nbrieﬂy introduce the Narmour’s Implication/Realization\nModel. In section 3 we describe the four distance mea-\nsures we are comparing — the note-level distance pro-\nposed in [9], two variants of contour-level distance and\ntheI/R-leveldistanceweproposeasanalternative. Insec-\ntion 4 we report the experiments performed using these\nfour distance measures on a dataset that comprises mu-\nsical phrases from a number of well known jazz songs.\nThe paper ends with a discussion of the results, and the\nplanned future work.\n2. THE IMPLICATION/REALIZATION MODEL\nNarmour [10,11]hasproposedatheoryofperceptionand\ncognitionofmelodies,theImplication/Realizationmodel,\nor I/R model. According to this theory, the perception of\namelodycontinuouslycauseslistenerstogenerateexpec-\ntations of how the melody will continue. The sources of\nthose expectations are two-fold: both innate and learned.\nTheinnatesourcesare‘hard-wired’intoourbrainandpe-\nripheral nervous system, according to Narmour, whereas\nlearned factors are due to exposure to music as a cul-\nturalphenomenon,andfamiliaritywithmusicalstylesand\npieces in particular. The innate expectation mechanism\nis closely related to the gestalt theory for visual percep-\ntion [5, 6]. Gestalt theory states that perceptual elements\nare(intheprocessofperception)groupedtogethertoform\na single perceived whole (a ‘gestalt’). This grouping fol-\nlows certain principles ( gestalt principles ). The most im-\nportant principles are proximity (two elements are per-\nceived as a whole when they are perceptually close), sim-\u0000\n\u0000\n\u0000\u0000\u0001\u0000\u0002\u0000 \u0000\n\u0000\u0000 \u0000\n\u0000\u0000\u0000\n\u0000\n\u0000\u0000\n\u0000\u0000\u0000\n\u0000\n\u0000\u0000\n\u0000\u0000D R IRVR ID P VP IP\n443\nP IDPPAll Of MeFigure 1.Top: Eight of the basic structures of the I/R model.\nBottom: First measures of All of Me, annotated with I/R struc-\ntures.\nilarity(two elements are perceived as a whole when they\nhave similar perceptual features, e.g. color or form, in vi-\nsualperception),and goodcontinuation (twoelementsare\nperceivedasawholeifoneisa‘good’or‘natural’contin-\nuation of the other). Narmour claims that similar princi-\nples hold for the perception of melodic sequences. In his\ntheory,theseprinciplestaketheformof implications : Any\ntwoconsecutivelyperceivednotesconstituteamelodicin-\nterval,andifthisintervalisnotconceivedascomplete,or\nclosed,itisan implicativeinterval ,anintervalthatimplies\nasubsequentintervalwithcertaincharacteristics. Inother\nwords,somenotesaremorelikelytofollowthetwoheard\nnotes than others. Two main principles concern registral\ndirection andintervallicdifference . Theprincipleofregis-\ntral direction (PRD) states that small intervals imply an\nintervalinthesameregistraldirection(asmallupwardin-\nterval implies another upward interval, and analogous for\ndownwardintervals),andlargeintervalsimplyachangein\nregistraldirection(alargeupwardintervalimpliesadown-\nwardintervalandanalogousfordownwardintervals). The\nprincipleofintervallicdifference(PID)statesthatasmall\n(ﬁve semitones or less) interval implies a similarly-sized\ninterval (plus or minus two semitones), and a large inter-\nvals (seven semitones or more) implies a smaller interval.\nBased on these two principles, melodic patterns can\nbe identiﬁed that either satisfy or violate the implication\nas predicted by the principles. Such patterns are called\nstructures and labeled to denote characteristics in terms\nofregistraldirectionandintervallicdifference. Eightsuch\nstructures are shown in ﬁgure 1(top). For example, the\nP structure (‘Process’) is a small interval followed by an-\nother small interval (of similar size), thus satisfying both\nthe registral direction principle and the intervallic differ-\nenceprinciple. SimilarlytheIP(‘IntervallicProcess’)struc-\nture satisﬁes intervallic difference, but violates registral\ndirection.\nAdditionalprinciplesareassumedtohold,oneofwhich\nconcerns closure, which states that the implication of an\ninterval is inhibited when a melody changes in direction,\nor when a small interval is followed by a large interval.\nOther factors also determine closure, like metrical posi-\ntion(strongmetricalpositionscontributetoclosure),rhythm\n(notes with a long duration contribute to closure), and\nharmony (resolution of dissonance into consonance con-\ntributes to closure).\nWe have designed an algorithm to automate the anno-\ntation of melodies with their corresponding I/R analyses.StructureInterval\nsizesSame\ndirection?PID\nsatisﬁed?PRD\nsatisﬁed?\nP S S yes yes yes\nD 0 0 yes yes yes\nID S S (eq) no yes no\nIP S S no yes no\nVP S L yes no yes\nR L S no yes yes\nIR L S yes yes no\nVR L L no no yes\nTable 1.Characterization of eight basic I/R structures; In the\nsecond column,‘S’ denotes small, ‘L’ large, and ‘0’ a prime in-\nterval\nThe algorithm implements most of the ‘innate’ processes\nmentioned before. It proceeds by computing the level of\nclosure at each point in the melody using metrical and\nrhythmical criteria, and based on this, decides the place-\nment and overlap of the I/R structures. For a given set of\nclosurecriteria,theprocedureisentirelydeterministicand\nno ambiguities aries. The learned processes, being less\nwell-deﬁnedbythe I/Rmodel,are currentlynotincluded.\nNevertheless, we believe that the resulting analysis have\na reasonable degree of validity. An example analysis is\nshown in ﬁgure 1(bottom).\n3. MEASURING MELODIC DISTANCES\nFor the comparison of the musical material on different\nlevels, we used a measure for distance that is based on\nthe concept of edit-distance (also known as Levenshtein\ndistance [8]). In general, the edit-distance between two\nsequences is deﬁned as the minimum total cost of trans-\nformingonesequence(thesourcesequence)intotheother\n(the target sequence), given a set of allowed edit opera-\ntions and a cost function that deﬁnes the cost of each edit\noperation. The most common set of edit operations con-\ntains insertion, deletion, and replacement. Insertion is the\noperation of adding an element at some point in the tar-\ngetsequence;deletionreferstotheremovalofanelement\nfrom the source sequence; replacement is the substitution\nof an element from the target sequence for an element of\nthe source sequence.\nBecause the edit-distance is a measure for comparing\nsequences in general, it enables one to compare melodies\nnotonlyasnotesequences,butinprincipleanysequential\nrepresentation can be compared. In addition to compar-\ningnote-sequences,wehaveinvestigatedthedistancesbe-\ntween melodies by representing them as sequences of di-\nrectional intervals, directions, and I/R structures, respec-\ntively.\nThese four kinds of representation can be said to have\ndifferent levels of abstraction, in the sense that some rep-\nresentations convey more concrete data about the melody\nthanothers. Obviously,thenoterepresentationisthemost\nconcrete,conveyingabsolutepitch,anddurationinforma-\ntion. The interval representation is more abstract, since\nit conveys only the pitch intervals between consecutive\nnotes. Thedirectionrepresentationabstractsfromthesize44\n4444ID P\nP P ID PBA\nCFigure2. Anexampleillustratingdifferencesofsimilarity\nassessments by the interval, direction and I/R measures.\nof the intervals, maintaining only their sign. The I/R rep-\nresentation captures pitch interval relationships by distin-\nguishing categories of intervals (small vs. large) and it\ncharacterizes consecutive intervals as similar or dissimi-\nlar. The scope of this characterization (not all interval-\npairs are necessarily characterized), depends on metrical\nand rhythmical information.\nAn example may illustrate how the interval, direction\nand I/R measures assess musical material. In ﬁgure 2,\nthreemusicalfragmentsaredisplayed. Thedirectionmea-\nsureratesA–BandA–Casequallydistant,whichisnot\nsurprising since A differs by one direction from both B\nand C. The interval measure rates A as closer to B than\nto C. The most prominent difference between A and C in\nterms of intervals is the jump between the last note of the\nﬁrst measure and the ﬁrst note of the second. In fragment\nAthisjumpisaminorthirddown,andforCitisaperfect\nfourth up. It can be argued that this interval is not really\nrelevant,sincetheﬁrstthreeandthelastthreenotesofthe\nfragments form separate perceptual groups. The I/R dis-\ntanceassessmentdoestakethisseparationintoaccount,as\ncan be seen from the I/R groupings and rates fragment A\ncloser to C than to fragmentB.\nThe next subsections brieﬂy describe our decisions re-\ngardingthechoiceofedit-operationsandweightsofoper-\nations for each type of sequence. We do not claim these\nare the only right choices. In fact, this issue deserves fur-\ntherdiscussionandmightbeneﬁtalsofromempiricaldata\nconveying human similarity ratings of musical material.\n3.1. An edit-distance for note sequences\nIn the case of note sequences, we have followed Mon-\ngeau and Sankoff’s approach [9]. They propose to ex-\ntend the set of basic operations (insertion, deletion, re-\nplacement) by two other operations that are more domain\nspeciﬁc: fragmentation andconsolidation . Fragmenta-\ntion is the substitution of a number of (contiguous) el-\nements from the target sequence for one element of the\nsourcesequence;conversely,consolidationisthesubstitu-\ntionofoneelementfromthetarget-sequenceforanumber\nof (contiguous) elements ofthe source sequence.\nThe weights of the operations are all linear combina-\ntions of the durations and pitches of the notes involved in\nthe operation. The weights of insertion and deletion of a\nnote are equal to the duration of the note. The weight of\na replacement of a note by another note is deﬁned as thesum of the absolute difference of the pitches and the ab-\nsolute difference of the durations of the notes. Fragmen-\ntation and consolidation weights are calculated similarly:\nthe weight of fragmenting a note n1into a sequence of\nnotes n2, n3, ..., n Nisagaincomposedofapitchpartand\na duration part. The pitch part is deﬁned by the sum of\nthe absolute pitch differences between n1andn2,n1and\nn3, etc. The duration part is deﬁned by the absolute dif-\nference between the duration of n1, and the summed du-\nrationsof n2, n3, ..., n N. Justlikethereplacementweight\nthe fragmentation weight is the sum of the pitch and du-\nration parts. The weight of consolidation is exactly the\nconverse of the weight of fragmentation.\n3.2. An edit-distance for contour sequences\nOne way to conceive of the contour of a melody is as\ncomprising the intervallic relationships between consec-\nutive notes. In this case, the contour is represented by a\nsequence of signed intervals. Another idea of contour is\nthatitjustreferstothemelodicdirection(up/down/repeat)\npatternofthemelody,discardingthesizesofintervals(the\ndirections are represented as 1,0,-1, respectively). In our\nexperiment,wehavecomputeddistancesforbothkindsof\ncontour sequences.\nWe have restricted the set of edit operations for both\nkinds of contour sequences to the basic set of insertion,\ndeletion and replacement, thus leaving out fragmentation\nandconsolidation,sincethereisnocorrespondencetofrag-\nmentation/consolidationasmusicalphenomena. Theweights\nfor replacement of two contour elements (intervals or di-\nrections)isdeﬁnedastheabsolutedifferencebetweenthe\nelements, and the weight of insertion and deletion is de-\nﬁnedastheabsolutevalueoftheelementtobeinserted/deleted\n(conform Lemstr ¨om and Perttu [7]).\nAdditionally,onecouldarguethatwhencomparingtwo\nintervals,itisalsorelevanthowfarthetwonotesthatcon-\nstitute each interval are apart in time. This quantity is\nmeasured as the time interval between the starting posi-\ntions of the two notes, also called the Inter Onset Interval\n(IOI). We incorporated the IOI into the weight functions\nbyaddingitasaweightedcomponent. Forexample,let P1\nandIOI 1respectivelybethepitchintervalandtheIOIbe-\ntweentwonotesinsequence1and P2andIOI 2thepitch\ninterval and IOI between to notes in sequence 2, then the\nweightofreplacingtheﬁrstintervalbythesecond,would\nbe|P2−P1|+k· |IOI 2−IOI 1|,where kisaparameter\ntaking positive real values, to control the relative impor-\ntanceofdurationalinformation. Theweightofdeletionof\nthe ﬁrst interval would be 1 +k·IOI 1.\n3.3. An edit-distance for I/Rsequences\nThesequencesof(possiblyoverlapping)I/Rstructures(I/R\nsequences, for short) that the I/R parser generated for the\nmusical phrases, were also compared to each other. Just\nas with the contour sequences, it is not obvious which\nkinds of edit operations could be justiﬁed beyond inser-\ntion,deletionandreplacement. ItispossiblethatresearchinvestigatingtheI/Rsequencesofmelodiesthataremusi-\ncal variations of each other, will point out common trans-\nformations of music at the level of I/R sequences. In that\ncase, edit operations may be introduced to allow for such\ncommontransformations. Presentlyhowever,weknowof\nno such common transformations, so we allowed only in-\nsertion, deletion and replacement.\nAs for the estimation of weights for edit operations\nuponI/Rstructures,notethatunlikethereplacementoper-\nation, the insertion and deletion operations do not involve\nany comparison between I/R structures. It seems reason-\nable to make the weights of insertion/deletion somehow\nproportional to the ‘importance’ or ‘signiﬁcance’ of the\nI/Rstructuretobeinserted/deleted. Lackingabettermea-\nsurefor the(unformalized)notionof I/Rstructuresigniﬁ-\ncance,wetakethesizeofanI/Rstructure,referringtothe\nnumber of notes the structure spans, as an indicator. The\nweightofaninsertion/deletionofanI/Rstructurecanthen\nsimply be the size of the structure.\nTheweightofareplacementoftwoI/Rstructuresshould\nassignhighweightstoreplacementsthatinvolvetwovery\ndifferentI/Rstructuresandlowweightstoreplacementsof\nan I/R structure by a similar one. The rating of distances\nbetween different I/R structures (which to our knowledge\nhas as yet remained unaddressed) is an open issue. Dis-\ntance judgments can be judged on class attributes of the\nI/R structures, for example whether the structure captures\na realized or rather a violated expectation. Alternatively,\nor in addition, the distance judgment of two instances of\nI/R structures can be based on instance attributes, such as\nthenumberofnotesthatthestructurespans(whichisusu-\nallybutnotnecessarilythree),theregistraldirectionofthe\nstructure,and whetheror notthe structureis chainedwith\nneighboring structures.\nAiming at a straight-forward deﬁnition of replacement\nweightsforI/Rstructures,wedecidedtotakeintoaccount\nfour attributes. The ﬁrst term in the weight expression is\nthe difference in size (i.e. number of notes) of the I/R\nstructures. Secondly,acostisaddedifthedirectionofthe\nstructuresisdifferent(wherethedirectionofanI/Rstruc-\nture is deﬁned as the direction of the interval between the\nﬁrst and the last note of the structure). Thirdly, a cost is\naddedifoneI/Rstructureischainedwithitssuccessorand\nthe other is not (this depends on metrical and rhythmical\ninformation). Lastly, a cost is added if the two I/R struc-\ntures are not of the same kind (e.g. PandVP). A special\ncase occurs when one of the I/R structures is the retro-\nspectivecounterpart of the other (a retrospective structure\ngenerally has the same up/down contour as it’s prospec-\ntive counterpart, but different interval sizes; for instance,\na retrospective P structure typically consists of two large\nintervals in the same direction, see [10] for details). In\nthis case, a reduced cost is added, representing the idea\nthatapairofretrospective/prospectivecounterpartsofthe\nsame kind of I/R structure is more similar than a pair of\nstructures of different kinds.3.4. Computing the Distances\nTheminimumcostoftransformingasourcesequenceinto\na target sequence can be calculated using the following\nrecurrence equation for the distance dijbetween two se-\nquences a1, a2, ..., a iandb1, b2, ..., b j:\ndij=min\n\ndi−1,j+w(ai,∅) (a)\ndi,j−1+w(∅, bj) (b)\ndi−1,j−1+w(ai, bj) (c)\ndi−1,j−k+w(ai, bj−k+1, ..., b j),2≤k≤j(d)\ndi−k,j−1+w(ai−k+1, ..., a i, bj),2≤k≤i(e)\nfor all 1≤i≤mand1≤j≤n, where mis the\nlength of the source sequence and nis the length of the\ntarget sequence. The terms on the right side respectively\nrepresent the cases of (a) deletion, (b) insertion, (c) re-\nplacement,(d)fragmentationand(e)consolidation. Addi-\ntionally, the initial conditions for the recurrence equation\nare are: -.3cm\ndi0=di−1,j+w(ai,∅)deletion\nd0j=di,j−1+w(∅, bj)insertion\nd00 = 0\n-.3cm For two sequences aandb, consisting of mand\nnelements respectively, we take dmnas the distance be-\ntween aandb. The weight function w, deﬁnes the cost\nofoperations(whichwediscussedintheprevioussubsec-\ntions). For computing the distances between the contour\nand I/R sequences respectively, the terms corresponding\nto the cost of fragmentation and consolidation are simply\nleft out of the recurrence equation.\n4. EXPERIMENTATION\nA crucial question is how the behavior of each distance\nmeasure can be evaluated. One possible approach could\nbe to gather information about human similarity ratings\nof musical material, and then see how close each distance\nmeasure is to the human ratings. Although this approach\nwouldcertainlybeveryinteresting,ithasthepracticaldis-\nadvantage that it may be hard to obtain the necessary em-\npirical data. For instance, it may be beyond the listener’s\ncapabilities to conﬁdently judge the similarity of musi-\ncal fragments longer than a few notes, or to consistently\njudge hundreds of fragments. Related to this is the more\nfundamental question of whether there is any consistent\n‘groundtruth’concerningthequestionofmusicalsimilar-\nity(see[3]foradiscussionofthisregardingmusicalartist\nsimilarity). Leaving these issues aside, we have chosen\na more pragmatic approach, in which we compared the\nratings of the various distance measures, and investigate\npossibledifferencesinfeatureslikediscriminatingpower.\nAnother criterion to judge the behavior of the measures\nis to see how they assess distances between phrases from\nthe same song versus phrases from different songs. This\ncriterion is not ideal, since it is not universally true that\nphrasesfromthesamesongaremoresimilarthanphrases 0 50 100 150 200 250 300 350 400\n 0 0.2 0.4 0.6 0.8  1notes\n 0 50 100 150 200 250 300 350 400\n 0 0.2 0.4 0.6 0.8  1intervals\n 0 50 100 150 200 250 300 350 400\n 0 0.2 0.4 0.6 0.8  1directions\n 0 50 100 150 200 250 300 350 400\n 0 0.2 0.4 0.6 0.8  1I/R structuresFigure 3.Distribution of distances for four melodic similarity\nmeasures. The x axis represents the normalized values for the\ndistances between pairs of phrases. The y axis represents the\nnumber of pairs that have the distance shown on the x axis.\nfrom different songs, but nevertheless we believe this as-\nsumption is reasonably valid.\nThecomparisonofthedifferentdistancemeasureswas\nperformed using 124 different musical phrases from 40\ndifferent jazz songs from the Real Book. The musical\nphrases have a mean duration of eight bars. Among them\nare jazz ballads like ‘How High the Moon’ with around\n20 notes, many of them with long duration, and Bebop\nthemes like ‘Donna Lee’ with around 55 notes of short\nduration. Jazz standards typically contain some phrases\nthat are slight variations of each other (e.g. only differ-\nent beginning or ending) and some that are more distinct.\nThis is why the structure of the song is often denoted by\na sequence of labels such as A1, A2 and B, where labels\nwith the same letters denote phrases that are similar.\nWith the 124 jazz phrases we performed all the possi-\nble pair-wise comparisons (7626) using the four different\nmeasures. The resulting distance values were normalized\nper measure. Figure 3 shows the distribution of distance\nvalues for each measure. The results for the direction and\ninterval measures were obtained by leaving IOI informa-\ntionoutoftheweightfunction(i.e. settingthe kparameter\nto 0, see section 3.2).\nThe ﬁrst thing to notice from ﬁgure 3 is the difference\ninsimilarityassessmentsatthenote-levelontheonehand,\nandtheinterval,directionandI/R-levelsontheotherhand.\nWhereas the distance distributions of the last three mea-\nsures are more spread across the spectrum with several\npeaks, the note level measure has its values concentrated\naround one value. This suggests that the note-level mea-\nsure has a low discriminatory power. We can validate this\nby computing the entropy as a measure of discriminatory\npower: Let p(x),x∈[0,1]be the normalized distribution\nofadistancemeasure Donasetofphrases S,discretized\nintokbins, then the entropy of DonSis\nH(D) =−1/summationdisplay\n0p(k) lnp(k)\n 4 4.2 4.4 4.6 4.8 5 5.2 5.4\nNoteIntervalDirectionI/RInterval+IOIDirection+IOIEntropy of distance distribution over dataset\n 0.5 0.55 0.6 0.65 0.7 0.75 0.8\nNoteIntervalDirectionI/RInterval+IOIDirection+IOIKL-Divergence between within-song and between-song distributionsFigure 4.Left: Discriminatory power (measured as entropy);\nRight: KL-Divergence between within-song distance distribu-\ntion and between-song distance distribution. the Interval+IOI\nand Direction+IOI measures were computed with k= 2 .0\nwhere p(k), is the probability that the distance between a\npair of phrases is in bin k. The entropy values for each\nmeasure are shown in ﬁgure 4. It can be seen that the dis-\ncriminatory power is substantially higher for the interval,\ndirection, and I/R measures than for the note measure.\nAninterestingdetailofthenotemeasuredistributionis\na very small peak between 0.0and0.2(hard to see in the\nplot). More detailed investigation revealed that the data\npoints in this region were within-song comparisons. That\nis,comparisonsbetween‘partner’phrasesofthesamesong\n(e.g. theA1andA2variants). Thispeakisalsoobservable\nin the I/R measure, in the range 0.0−.05, In the interval\nand direction measure the peak is ‘overshadowed’ by a\nmuchlargerneighboringpeak. Thissuggeststhatthenote\nand I/R measures are better at separating very much re-\nsembling phrases from not much resembling phrases than\ntheintervalanddirectionmeasures. Toverifythis,wecal-\nculated the Kullback-Leibler divergence (KLD) between\nthe distribution of within-song distances and the distribu-\ntion of between-song distances. The KLD is a measure\nfor comparing distributions. High values indicate a low\noverlap between distributions and vice versa. Figure 4\nshows the KLD values per measure. Note that the values\nfor the interval and direction measures are slightly lower\nthan those of the note and I/R measures.\nTheintervalanddirectionmeasuresdonotincludeany\nkind of rhythmical/temporal information. Contour repre-\nsentations that ignore rhythmical information are some-\ntimesregardedastooabstract,sincethisinformationmay\nbe regarded as an essential aspect of melody [13, 14].\nTherefore,wetestedtheeffectofweighingtheinter-onset\ntime intervals (IOI) on the behavior of the interval and\ndistancemeasures. IncreasingtheweightsofIOIsubstan-\ntially improved the ability to separate within-song com-\nparisonsfromthe between-song comparisons. However,it\ndecreased the discriminatory power of the measures (see\nﬁgure 4). In ﬁgure 5, the distance distributions of the di-\nrection measure are shown for different weights of IOI.\nNote that, as the IOI weight increases, the form of the\ndistribution smoothly transforms from a multi-peak form\n(like those of the interval, direction and I/R measures in\nﬁgure 3), to a single-peak form (like the note-level mea-\nsure in ﬁgure 3). That is, the direction level assessments\nwith IOI tend to resemble the more concrete note level\nassessment. 0 50 100 150 200 250 300 350\n 0 0.2 0.4 0.6 0.8  1k=0.0000\n 0 50 100 150 200 250 300 350\n 0 0.2 0.4 0.6 0.8  1k=0.0625\n 0 50 100 150 200 250 300 350\n 0 0.2 0.4 0.6 0.8  1k=0.5000\n 0 50 100 150 200 250 300 350\n 0 0.2 0.4 0.6 0.8  1k=2.0000Figure 5.Distributions of distances of the direction measure\nfor various weights of inter-onset intervals.\n5. CONCLUSIONS AND FUTURE WORK\nIn this paper we have proposed a new way of assessing\nmelodicsimilarityandcompareditwithexistingmethods\nfor melodic similarity assessment, using a dataset of 124\njazz phrases from well known jazz songs.\nThediscriminatorypower(usinganentropybaseddef-\ninition)onthewholedatasetwashighestforthe(mostab-\nstract) contour and I/R level measures and lowest for the\nnote level measure. This suggests that abstract melodic\nrepresentationsservebettertodifferentiatebetweenphrases\nthat are not near-identical (e.g. phrases belonging to dif-\nferent musical styles) than very concrete representations.\nIt is conceivable that the note-level distance measure is\ntoo ﬁne-grained for complete musical phrases and would\nbemoreappropriatetoassesssimilaritiesbetweensmaller\nmusical units (e.g. musical motifs).\nThe experimentation also showed that the note and I/R\nlevel measures were better at clustering phrases from the\nsame song than the contour (i.e. interval and direction)\nlevel measures. This was shown to be due to the fact\nthatrhythmicalinformationismissinginthecontourlevel\nmeasures. Takingintoaccountthisinformation(byweight-\ning the IOI values in the edit operations) in the contour\nlevelmeasuresimprovedtheirabilityseparate within-song\ncomparisons from between-song comparisons, at the cost\nof discriminatory power on the whole dataset.\nIn general, there seems to be a trade-off between good\ndiscriminatory power on the one hand, and the ability to\nrecognize phrases from the same song (that are usually\nvery similar) on the other. Very concrete measures, like\nthe note measure, favor the latter at the cost of the for-\nmer, whereas very abstract measures (like contour mea-\nsures without IOI information), favor the former at the\ncost of the latter. The I/R measure, together with contour\nmeasures that pay heed to IOI information, seem to be a\ngood compromise between the two.\nIn the future, we wish to investigate the usefulness of\nthe similarity measures to cluster phrases from the same\nmusical style. Some initial tests indicated that in partic-\nular the contour and I/R measures separated bebop stylephrases from ballads.\n6. REFERENCES\n[1] David Cope. Computers and Musical Style . Oxford\nUniversity Press, 1991.\n[2] W.J.Dowling. Scaleandcontour: Twocomponents\nof a theory of memory for melodies. Psychological\nReview, 85(4):341–354, 1978.\n[3] D. P. W. Ellis, B. Whitman, A. Berenzweig, and\nS. Lawrence. The quest for ground truth in musical\nartist similarity. In Prooceedings of the 3rd Interna-\ntional Conference on Music Information Retrieval .\nISMIR, 2002.\n[4] D. H¨ornel and W. Menzel. Learning musical struc-\nture and style with neural networks. Computer Mu-\nsic Journal , 22 (4):44–62, 1998.\n[5] K. Koffka. Principles of Gestalt Psychology . Rout-\nledge & Kegan Paul, London, 1935.\n[6] W. K ¨ohler.Gestalt psychology: An introduction to\nnewconceptsofmodernpsychology . Liveright,New\nYork, 1947.\n[7] Kjell Lemstr ¨om and Sami Perttu. Semex - an ef-\nﬁcient music retrieval prototype. In First Interna-\ntional Symposium on Music Information Retrieval\n(ISMIR’2000) , Plymouth, Massachusetts, October\n23-25 2000.\n[8] V.I.Levenshtein.Binarycodescapableofcorrecting\ndeletions, insertions and reversals. Soviet Physics\nDoklady, 10:707–710, 1966.\n[9] M. Mongeau and D. Sankoff. Comparison of mu-\nsical sequences. Computers and the Humanities ,\n24:161–175, 1990.\n[10] E. Narmour. The Analysis and cognition of ba-\nsic melodic structures : the implication-realization\nmodel. University of Chicago Press, 1990.\n[11] E. Narmour. The Analysis and cognition of melodic\ncomplexity: the implication-realization model . Uni-\nversity of Chicago Press, 1992.\n[12] P.Y. Rolland. Discovering patterns in musical se-\nquences. Journal of New Music Research , 28\n(4):334–350, 1999.\n[13] J.Schlichte. Derautomatischevergleichvon83.243\nmusikincipits aus der rism-datenbank: Ergebnisse -\nnutzen-perspektiven. FontesArtisMusicae ,37:35–\n46, 1990.\n[14] R.Typke,P.Giannopoulos,R.C.Veltkamp,F.Wier-\ning, and R van Oostrum. Using transportation\ndistances for measuring melodic similarity. In\nProoceedingsofthe4thInternationalConferenceon\nMusic Information Retrieval . ISMIR, 2003."
    },
    {
        "title": "Extraction of Drum Patterns and their Description within the MPEG-7 High-Level-Framework.",
        "author": [
            "Matthias Gruhne",
            "Christian Uhle",
            "Christian Dittmar",
            "Markus Cremer"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1414888",
        "url": "https://doi.org/10.5281/zenodo.1414888",
        "ee": "https://zenodo.org/records/1414888/files/GruhneUDC04.pdf",
        "abstract": "A number of metadata standards have been published in recent years due to the increasing availability of multimedia content and the resulting issue of sorting and retrieving this content. One of the most recent efforts for a well defined metadata description is the ISO/IEC MPEG-7 standard, which takes a very broad approach towards the definition of metadata. Herein, not merely hand annotated textual information can be transported and stored, but also more signal specific data that can in most cases be automatically retrieved from the multimedia content itself. In this publication an algorithm for the automated transcription of rhythmic (percussive) accompaniment in modern day popular music is described. However, the emphasis here is not a precise transcription, but on capturing the “rhythmic gist” of the piece of music in pieces by their dominant rhythmic patterns. A small-scale evaluation of the algorithm is presented along with an example representation of the thus gained semantically meaningful metadata using description methods currently discussed within MPEG-7.",
        "zenodo_id": 1414888,
        "dblp_key": "conf/ismir/GruhneUDC04",
        "keywords": [
            "metadata standards",
            "multimedia content",
            "ISO/IEC MPEG-7",
            "automated transcription",
            "rhythmic accompaniment",
            "popular music",
            "semantically meaningful metadata",
            "description methods",
            "evaluation",
            "example representation"
        ],
        "content": "EXTRACTION OF DRUM PATTERNS AND THEIR \nDESCRIPTION WITHIN THE MPEG-7 HIGH-LEVEL-\nFRAMEWORK\nMatthias Gruhne \nghe@idmt.fraunhofer.de Christian Uhle \nuhle@idmt.fraunhofer.de Christian Dittmar \ndmr@idmt.fraunhofer.de Markus Cremer \ncre@idmt.fraunhofer.de \nFraunhofer IDMT \nLangewiesener Str. 22 \n98693 Ilmenau, Germany \n \n \nABSTRACT \nA number of metadata standards have been published in \nrecent years due to the increasing availability of multimedia content and the resulting issue of sorting and retrieving this content. One of the most recent efforts for a well defined metadata description is the ISO/IEC MPEG-7 standard, which takes a very broad approach towards the definition of metadata. Herein, not merely hand annotated textual information can be transported and stored, but also more signal specific data that can in most cases be automatically retrieved from the multimedia content itself. \nIn this publication an algorithm for the automated \ntranscription of rhythmic (percussive) accompaniment in modern day popular music is described. However, the emphasis here is not a precise transcription, but on capturing the “rhythmic gist” of the piece of music in order to allow a more abstract comparison of musical pieces by their dominant rhythmic patterns. A small-scale evaluation of the algorithm is presented along with an \nexample representation of the thus gained semantically meaningful metadata using description methods currently discussed within MPEG-7.\n   \n1. INTRODUCTION \nStimulated by the ever-growing availability of musical \nmaterial to the user via new media and content distribution methods an increasing need to automatically categorize audio data has emerged.  Descriptive information about audio data which is delivered together with the actual content represents one \nway to facilitate this search immensely. The aims of so-called metadata  (”data about data”) are to e.g. detect the \ngenre of a song, specify music similarity, perform a segmentation on a song, or simply recognize a song by scanning a database for similar metadata.  There have been a number of publications describing an approach to achieve these aims using features that \nbelong to a lower semantic hierarchy order (“Low-Level-Tools”) [1].   \nThese features are extracted  directly from the signal \nitself in a computationally efficient manner, but carry \nlittle meaning for the human listener. The usage of high level semantic information relates to the human perception of music.  The rhythmic elements of music, determined by the \ndrum and percussion instruments, play an important role especially in contemporary popular music. Therefore, the performance of advanced music retrieval applications will benefit from using mechanisms that allow the search for rhythm ic styles or particular \nrhythmic features. \n1.1. The Metadata Standard MPEG-7 \nOne example of a number of upcoming standards for the \nspecification of metadata for audiovisual data is the MPEG-7 standard, which was finalized in late 2001.  The first version of MPEG-7 Audio (ISO/IEC 15938-4) does not, however, cover high level features in a significant way. Therefore th e standardization committee \nagreed to extend this part of  the standard. The work of \ncontributing high level tools is currently being assembled in MPEG-7 Audio Amendment 2 (ISO/IEC 15938-4 AMD2). One of its features is RhythmicPatternDS . The internal structure of its \nrepresentation depends on the underlying rhythmic structure of the considered pattern. The main advantage \nconsists in the fact that for every pattern the most compact representation can be provided, resulting in an efficient comparison of the patterns and minimal memory needed for storage. The system presented in this paper has been designed to extract an MPEG-7 Audio AMD2 conformant RhythmicPatternDS  out of a musical audio signal. The \nsystem's complexity is low enough to allow real time operation on today's personal computers. \n2. SYSTEM OVERVIEW \nThe system presented in this paper consists of three \ndifferent parts. At the firs t processing stage occurrences \nof un-pitched percussive instruments are detected and classified. Based on the resulting drum transcription actual drum patterns are extr acted. Finally, an MPEG-7 \nAudio conformant XML description is created. \nPermission to make digital or hard c opies of all or part of this work \nfor personal or classroom use is granted without fee provided that \ncopies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. \n© 2004 Universitat Pompeu Fabra.   \n \n 2.1. Transcription of Percussive Instruments \nThis section gives an overview on the method for \ndetection and classification of un-pitched percussive instruments, described in detail in [2]. The detection and classification of percussive events is carried out using a spectrogram-representation \nX of the audio signal. \nDifferentiation and halfway-rectification of X yield a \nnon-negative difference spectrogram Xˆ, from which the \ntimes of occurrence t and the spectral slices tXˆ related \nto percussive events are deduced. Principal Component Analysis (PCA) is applied to \ntXˆ according to (1). \n                                 WXX⋅=tˆ~ (1) \nThereby, transformation matrix W reduces the number \nof slices to d components unifying decorrelation and \nvariance normalization. The principal components X~ \nare subjected to Non-Negative Independent Component Analysis (NNICA) [3], which attempts to find un-mixing matrix \nA by optimizing a cost function describing the \nnon-negativity of the components. The spectral characteristics of un-pitched percussive instruments, especially th e invariance of spectra of \ndifferent notes compared to pitched instruments allows a separation of \nX~ using un-mixing matrix A into \nspectral profiles F according to equation (2).  \n                                  XAF~⋅=  (2) \nThe spectral profiles can be  used to extract the \nspectrogram’s amplitude basis, from here forward referred to as amplitude envelopes \nE according to (3). \n                                  XFE⋅=  (3) \nThis procedure is closely related to the principle of Prior \nSubspace Analysis (PSA) [4],  modified to estimate the \nspectral profiles from the analy zed audio signal itself. In \nfurther contrast to the original procedure introduced in [4] no further ICA-computation is carried out on the amplitude envelopes.  The extracted components are classified using a set of spectral-based and time-based features.  The classification shall provide two sources of information. Firstly, components should be excluded from the rest of the pr ocess which are clearly \nharmonically sustained. Secondly, the remaining dissonant percussive compone nts should be assigned to \npre-defined instrument classes.  A suitable measure for the distinction of the amplitude envelopes is represented by the percussiveness, which is introduced in [5]. A slightly modified version is employed to distinguish the amplitude envelopes related \nto percussive instruments from the ones related to sustained sounds. A spectral-based measure is constituted by introducing the spectral dissonance, earlier described in [5], [6]. A slightly modified version is employed to distinguish the spectra of harmonic sustained sounds from dissonant ones related to percussive sounds. The assignment of spectral profiles to a priori trained \nclasses of percussive instruments is provided by a k-nearest neighbour classifier with spectral profiles of single instruments from a training database. To verify the classification in cases of low reliability or several occurrences of the same inst ruments, additional features \ndescribing the shape of the spectral profile, e.g. centroid, spread, and skewness, are ex tracted. Other features are \nthe center frequencies of the most prominent local partials, their intensity, spread, and skewness.  Drum-like onsets are detected in the amplitude envelopes using conventional peak picking methods.   The intensity of the onset candidate is estimated from the magnitude of the envelope signal. Onsets with intensities exceeding a predetermined dynamic threshold are accepted. This procedure reduces crosstalk \ninfluences of harmonic sustained instruments as well as concurrent percussive instruments. \n2.2. Extraction of Drum Pattern \nThe extraction of recurring pattern (drum pattern) from \na list of automatically detect ed events (see section 2.1.) \nis illustrated in Figure 1.   \nSegmentation \nQuantization Segment \nboundaries\nPattern length estimation \nPattern histogram computation Event list Audio signal\nDrum patternQuantized score\nThresholding \nPhase Correction \n \nFigure 1:  Block diagram of the second stage of drum \npattern extraction \nAt first, the audio signal is segmented into similar and \ncharacteristic regions using a self-similarity method initially proposed by Foote [7]. The segmentation is motivated by the assumption,  that within each region \nnot more than one representative drum pattern occurs, and that the rhythmic featur es are nearly invariant. \nSubsequently, the temporal positions of the events are quantized on a tatum grid. The term tatum grid refers to the pulse series on the lowest metric level [8]. Tatum period and phase is computed by means of a two-way mismatch error procedure, originally proposed for the estimation of the fundamental frequency in [9] and applied to tatum estimation before in [10]. An additional note onset detection process, finding note onsets in the   \n \n audio signal, complements the list of note onsets from \nthe percussive un-pitched instruments. The pattern length is estimated by searching for the prominent periodicity in the quantized score with periods equaling an integer multiple of the bar length. The periodicity function is obtained by calculating a similarity measure between the signal and its time shifted version. The similarity between two score representations is calculated as weighted sum of the number of simultaneously occurring notes and rests in the score. An estimate of the bar length is obtained by comparing the derived periodicity function to a number of so-called metric models, each of them corresponding to a bar length. A metric model is defined here as a vector describing the degree of periodicity per integer multiple of the tatum period, and is illustrated as a number of pulses, where the height of the pulse corresponds to the degree of periodicity. The best match between the periodicity function derived from the input data and pre-\ndefined metric models is computed by means of their correlation coefficient. A periodicity function and two exemplary metric models are illustrated in Figure 2. \n2 4 6 8 10 12 14 16 180.10.20.30.40.50.60.70.80.9\nlag in tatum periodsPeriodicity function\n \nFigure 2:  Periodicity function and two examples of metric \nmodels corresponding to four-f our time (dotted line) and \nfive-four time (dashed line) \n \nA histogram-like representation Hi,j of the score Ti,l is \nobtained by measuring the frequency of occurrence of events per instrument and metric position according (4). \n()∑\n=−+=r\nkbkji ji\n11 , , T H\n                                           (4) \nwhere i=1…n  and j=1…b , n represents the number of \ninstruments, b is the bar length and r equals the number \nof bars. The drum patterns are extracted by choosing the positions whose occurrence exceeds a threshold q\ni (5). \n \n()\n() >=otherwiseqi ji ji\nji0,, ,\n,H HD\n                            (5) \nThe final processing step estimates the start position of the pattern. It is assumed that the start of the pattern corresponds to the position featuring the strongest \noccurrence of kick drum notes. A further strategy is to identify common playing styl es and to compare the \nextracted pattern to various exemplary patterns. 2.3.\n MPEG-7 AudioRhythmicPattern \nThe AudioRhythmicPattern descriptor uses a non-linear \nindexing of the velocity values with help of a so-called PrimeIndex , derived from prime factorization of the grid \nindices. The PrimeIndex  indicates the rhythmic \nsignificance (rhythmic level) within the pattern. In general, velocity values that occur on a beat will be indicated by a PrimeIndex  with a lower integer value \nthan velocity values o ccurring between two beats \n(offbeat). Depending on meter and micro time different levels of rhythmic hierarchy will result.  \nPart of \nthe bar 1 1+ 2 2+ 3 3+ 4 4+ \nRhythmic \nlevel *** * ** * *** * ** * \nGrid \nposition 1 2 3 4 5 6 7 8 \nPrime \nindex 1 5 3 6 2 7 4 8 \nVelocity 100 0 112 0 150 68 120 0 \nTable 1:  Example of a Rhythmic Pattern \nThe term micro time defines as the (close to) integer \nratio between the beat period and the tatum period. An example for a rhythmic pattern is given at Table 1. The meter is 4/4 and the micro time equals 2. This results in a total pattern size of 8.  \nRhythmic \nlevel *** *** ** ** * \nPrime index 1 2 3 4 7 \nVelocity 100 150 112 120 68 \nTable 2:  Rhythmic Pattern after deleting zeros and re-\nordering  \nAll velocity values equal to zero and their corresponding \nprime indices (elements) are deleted. According to the ascending order of the prime indices the elements will be rearranged, resulting in the final representation (see Table 2).        \n    \n… \n      <Audio xsi:type=\"AudioSegmentType\"> \n        <AudioDescriptionScheme              xsi:type=\"AudioPatternType\">           <Meter>             <Numerator>4</Numerator>             <Denominator>4</Denominator>           </Meter>           <TimePoint>PT00N1000F</TimePoint>           <Pattern>             <BarNum>1</BarNum>             <InstrumentID>36</InstrumentID>             <Microtime>2</Microtime>             <PrimeIndex>1 2 3 4 7 </PrimeIndex>             <Velocity>110 150 112 120 68 </Velocity>           </Pattern>                   </AudioDescriptionScheme>       </Audio> … \nFigure 3:  section of an exam ple rhythmic pattern \nXML description      \n \n 3. TEST RESULTS \nAn informal listening test has been conducted in order \nto quantify the abilities of the presented system. Nine human listeners (a mixture of lab members and students with varying degree of musical training ranging from non-musicians to skilled performers) were confronted with 92 excerpts of 40 test songs. Each excerpt features a minimum duration of six seconds. The songs include a wide range of musical genres where the appearance of drum patterns is common, e.g. Rock, Pop, Latin, Soul and House. The human listeners were instructed to compare the original excerpt s to the synthetic rendition \nof the extracted patterns. The rating ranges from five (for a perfect extracted pattern) to one (for an unrecognizable pattern). Details on the results of the listening test are displayed in Figure 4. The solid line shows the mean score value per test item in descending order. The dashed line shows the corresponding standard deviation arranged as a tolerance interval around the mean value. It can be seen that almost 70 percent of the test items have been assigned a score equal or greater three. Another interesting observation is the fact, that the standard deviation does not diverge strongly amongst the test subjects. The presumption that a rating of pattern quality could be a very subjective task is invalidated by a sma ll average standard deviation \nof 0.74 score points. \n0 10 20 30 40 50 60 70 80 90 100 \n5 \n4.5\n4  \n3.5\n3  \n2.5\n2  \n1.5\n1  \n \ntest itemsscore\n \nFigure 4:  Results of Listening Tests \n4. CONCLUSIONS \nIn this publication an algorithm for the automated \nextraction of rhythmic patterns from popular music items has been introduced a nd evaluated. The presented \ntest results verify that the algorithm produces viable results, and can indeed be deployed for the comparison of rhythmic aspects of different music items. The extracted rhythmic patterns are represented using the latest description methods under preparation within the international standard ISO/IEC MPEG-7.  Thus, with the tools and methods presented above the realization of a complete, fully automated system for efficient rhythmic character ization and comparison of \ncontemporary popular music tunes is possible.   \n5.\n REFERENCES \n[1] Allamanche, E. Herre, J. Hellmuth, O. \n''Content-based Identification of Audio Material Using MPEG-7 Low Level Description'', Proceedings of the 2nd Annual \nSymposium on Music Information Retrieval,  \nBloomington, USA, 2001. \n[2]\n Dittmar, C. Uhle, C. ''Further Steps towards \nDrum Transcription of Polyphonic Music'', Proceedings of the AES 116th Convention , \nBerlin, Germany, 2004. \n[3]\n Plumbley, M. ''Algorithms for Non-Negative \nIndependent Component Analysis'' , \nProceedings of the IEEE Transactions on Neural Networks , 14 (3), pp 534- 543, 2003. \n[4]\n Fitzgerald, D. Lawlor, B. and Coyle, E. ''Prior \nSubspace Analysis for Drum Transcription'', Prcoeedings of the 114th AES Convention , \nAmsterdam, Netherlands, 2003. \n[5]\n Uhle, C. Dittmar, C. and Sporer, T. ''Extraction \nof Drum Tracks from polyphonic Music using Independent Subspace Analysis'', Proceedings \nof the Fourth International Symposium on Independent Component Analysis, Nara, Japan, \n2003. \n[6]\n Sethares, W. ''Local Consonance and the \nRelationship between Timbre and Scale'', Journal of the Acoustical Society of America , \n94 (3), pt. 1, 1993. \n[7]\n Foote, J. ''Automatic Audio Segmentation \nUsing a Measure of Audio Novelty'', Proceedings of the IEEE Int. Conf. on Multimedia and Expo , vol. 1, pp. 452-455, \n2000. \n[8]\n Bilmes, J.A.  ''Timing is of Essence'', MSc \nThesis , Massachusetts Institute of Technology, \n1993. \n[9] Maher, R. Beauchamp, J. ''Fundamental \nFrequency Estimation of Musical Signals Using a Two-Way Mismatch Procedure'', Journal of the Acoustical Society of America , \nvol. 95, no. 4, pp. 2254-2263, 1994. \n[10]\n Gouyon, F. Herrera, P. Cano, P. ''Pulse-\nDependent Analysis of Percussive Music'' , \nProceedings of the AES 22nd Int. Conference on Virtual, Synthetic and Entertainment Audio, 2002."
    },
    {
        "title": "Time-Warped Longest Common Subsequence Algorithm for Music Retrieval.",
        "author": [
            "AnYuan Guo",
            "Hava T. Siegelmann"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417165",
        "url": "https://doi.org/10.5281/zenodo.1417165",
        "ee": "https://zenodo.org/records/1417165/files/GuoS04.pdf",
        "abstract": "Recent advances in music information retrieval have en- abled users to query a database by singing or humming into a microphone. The queries are often inaccurate ver- sions of the original songs due to singing errors and errors introduced in the music transcription process. In this pa- per, we present the Time-Warped Longest Common Sub- sequence algorithm (T-WLCS), which deals with singing errors involving rhythmic distortions. The algorithm is employed on song retrieval tasks, where its performance is compared to the longest common subsequence algorithm.",
        "zenodo_id": 1417165,
        "dblp_key": "conf/ismir/GuoS04",
        "keywords": [
            "Music Information Retrieval",
            "Singing or humming queries",
            "Database accuracy",
            "Rhythmic distortions",
            "Time-Warped Longest Common Sub-sequence algorithm",
            "Song retrieval tasks",
            "Performance comparison",
            "Longest common subsequence algorithm",
            "Singing errors",
            "Music transcription process"
        ],
        "content": "TIME-WARPEDLONGEST COMMON SUBSEQUENCEALGORITHM\nFORMUSICRETRIEV AL\nAnYuanGuoHavaSiegelmann\nUniversityofMassachusetts, Amherst\nDepartment ofComputer Science\nABSTRACT\nRecent advances inmusic information retrie valhaveen-\nabled users toquery adatabase bysinging orhumming\nintoamicrophone. Thequeries areoften inaccurate ver-\nsions oftheoriginal songs duetosinging errors anderrors\nintroduced inthemusic transcription process. Inthispa-\nper,wepresent theTime-W arped Longest Common Sub-\nsequence algorithm (T-WLCS), which deals with singing\nerrors involving rhythmic distortions. The algorithm is\nemplo yedonsong retrie valtasks, where itsperformance is\ncompared tothelongest common subsequence algorithm.\n1.INTRODUCTION\nInrecent years, alargeamount ofmusic hasbeen made\npublicly available overtheInternet. Various music collec-\ntions come informats such asMIDI, WAV,MP3, ABC,\nandGUIDO, toname afewpopular ones. Furthermore,\nmanyprograms existthatwillconvertbetween thediffer-\nentformats [15,8,1].The proliferation ofmusic data\nhasdrivenupuser demands foreasy andefﬁcient waysto\nsearch databases forasong ofinterest. Auser caninput a\nquery viaakeyboard orevenbysinging orhumming intoa\nmicrophone [6,9,13].Theusermay haveinaccurate pitch\norrhythm, ormay sing atadifferent speed than theorig-\ninalrendition. Weintroduce analgorithm thatdeals with\nrhythm andspeed variations within theconte xtofstring-\nmatching based music similarity metrics.\nApiece ofmusic iscomposed ofaseries ofsymbols.\nInthispaper ,wefocus onmonophonic music, inwhich\natmost onenote isplayed atanygiventime. This class\nofmusic canbereadily represented asastring overan\nalphabet, where thealphabet includes allthepitches that\nappeared within thatpiece ofmusic. Giventherepresen-\ntation ofapiece ofmusic asastring ofsymbols, theedit-\ndistance based string matching algorithms, already widely\nemplo yedinthespeech andtextprocessing communities\n[3,18,5],naturally lend themselv estotheformation of\nplausible similarity measures.\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forproﬁt orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation ontheﬁrstpage.\nc\r2004 Universitat Pompeu Fabra.Onekeystrength common tostring matching algorithms\nisthattheyaredesigned todeal with insertions, deletions\nandsubstitutions between thequery string andthetarget,\naproperty much needed considering thedifferent types of\nerrors that might beintroduced intheretrie valprocess.\nThere could beinaccuracies intheuser’ srecall orinaccu-\nracies insinging. Furthermore, errors areoften introduced\ninthetranscription process when theacoustic signals are\nturned into musical symbols [10].Indeed, severalstring\nmatching algorithms, such asapproximate string match-\ning,local alignment andthelongest common subsequence\nalgorithm havebeen successfully emplo yedinmusic in-\nformation retrie valsystems [17,13].However,these al-\ngorithms donothaveprincipled waysforhandling speed\nvariations andinaccuracies inrhythm. Inthispaper ,we\nfocus onextending thelongest common subsequence al-\ngorithm with these additional properties.\nGiventwosequences, thelongest common subsequence\nalgorithm ﬁnds thelongest subsequence thetwohavein\ncommon. The subsequence itself canbedispersed arbi-\ntrarily among each ofthestrings with gapsofnon-matching\nsymbols inbetween. Similarity measures based onthisal-\ngorithm havebeen used toperform content-based music\ninformation retrie val[17]. Itshould benoted, however,\nthatedit-distance based algorithms cannot deal with trans-\nposed matches [11].\nAccording tostudies inmusic recognition, therhythm\nofapiece ofmusic isimportant torecognition tasks [4].\nThe inclusion ofrhythmic information hasshowntoim-\nprovemusic retrie val[17].People often sing inaccurately\nwith respect torhythm -some notes areshrunk, others\nexpanded. Furthermore, therendition could befaster or\nslowerthan theoriginal score. Inother words, thedura-\ntionofthenotes sung donotmatch theoriginal song, some\nnonlinear expansion/contractions could beintroduced.\nString matching techniques forsong retrie valsuch asthe\nlongest commons subsequence algorithm lackaprincipled\nwaytodeal with thethese variations [17].\nThe contrib ution ofthispaper istheintroduction of\nthetime-w arped longest common subsequence (T-WLCS)\nalgorithm, which combines thedesirable aspects oftwo\nalgorithms, thedynamic time warping algorithm (DTW)\nandthelongest common subsequence algorithm (LCS).\nTheT-WLCS algorithm augments theLCS algorithm with\ntheability toaccount forsongs played atdifferent speeds,\nwith possibly non-uniform expansions/contractions. Atthesame time, itretains thedesirable properties ofthe\nLCS algorithm, allowing forgapsbetween matching se-\nquences. Finally ,thealgorithm istested inasong retrie val\napplication.\n2.BACKGROUND\nDynamic time warping (DTW) isanalgorithm developed\nbythespeech recognition community tohandle thematch-\ningofnon-linearly expanded orcontracted signals [14].\nThealgorithm ﬁnds theoptimal path through amatrix of\npoints representing possible time alignments between the\nsignals. The optimal alignment canbeefﬁciently calcu-\nlated viadynamic programming.\nDynamic time warping operates asfollows.Giventwo\ntime sequences X=hx1;x2;:::;xmi,and\nY=hy1;y2;:::;yni,itﬁlls anmbynmatrix repre-\nsenting thedistances ofbest possible partial path using a\nrecursi veformula:\nD(i;j)=d(i;j)+min8\n<\n:D(i;j\u00001)\nD(i\u00001;j)\nD(i\u00001;j\u00001)(1)\nwhere 1\u0014i\u0014m;1\u0014j\u0014n,d(i;j)represents\nthedistance between xiandyj.D(1;1)isinitialized to\nd(1;1).The alignment thatresults intheminimum dis-\ntance between thetwosequences hasvalueD(m;n).\nAlthough DTW does havetheﬂavoroftheproperty\nthatwedesire, namely ,itmatches non-linearly stretched\norcompressed sequences, itisnotdirectly applicable to\ntheclass ofmusic retrie valtasks weareinterested in.DTW\naligns twosequences from beginning toend. Often, in\nmusic retrie valtasks, weareonly givenpartial songs. In\nthenextsection, wewill showhowtoaugment theLCS\nalgorithm with thetime stretching properties ofDTW .\n3.TIME-WARPEDLONGEST COMMON\nSUBSEQUENCEALGORITHM\nDue toinaccuracies andspeed variations insinging, and\ntheerrors introduced inthemusic transcription phase, the\nquery string andthestored song might differ.Adesirable\nmatching algorithm thus needs totakethese factors into\naccount.\nString matching algorithms suitable forthistaskbelong\ntotheedit-distance family ,deﬁned asalgorithms thatﬁnd\naminimum-weight sequence ofeditoperations (such as\ndeletions, insertions andsubstitutions) thattransform one\nstring totheother .This family ofalgorithms iswidely em-\nployedinreal-life applications ranging from speech pro-\ncessing tomolecular biology [7,19].\n3.1.TheOriginalLongestCommon Subsequence Al-\ngorithm\nThe longest common subsequence algorithm (LCS) be-\nlongs tothiseditdistance family ofstring matching algo-\nrithms. Speciﬁcally ,theLCS algorithm ﬁnds thelongestsubsequence thattwosequences haveincommon, regard-\nless ofthelength and thenumber ofintermittent non-\nmatching symbols. Forexample, thesequences “abcdefg”\nand“axbydezzz” havealength four sequence “abde” as\ntheir longest common subsequence.\nFormally ,theLCS problem isdeﬁned asfollows.Given\nasequence X=hx1;x2;:::;xmi,andasequence Y=\nhy1;y2;:::;yni,ﬁnd asequence Z,such thatZisthe\nlongest sequence thatisboth asubsequence ofX,anda\nsubsequence ofY.The subsequence isdeﬁned asase-\nquence Z=hz1;z2;:::;zki,where there exists astrictly\nincreasing sequence hi1;i2;:::;ikiofindices ofXsuch\nthatforallj=1:::k;xij=zj[2].\nThesolution totheLCS problem involvessolving the\nfollowing recurrence equation, where thecostfortheedit\noperations isstored inc.\nc(i;j)=8\n>>>>>><\n>>>>>>:0 ifi=0or\nj=0\nc(i\u00001;j\u00001)+1 ifi;j>0and\nxi=Yj\nmax[c(i;j\u00001);c(i\u00001;j)]ifi;j>0and\nxi6=Yj\n(2)\nUsing LCS asasimilarity measure between twose-\nquences hastheadvantage thatthetwosequences weare\ncomparing canbeofdifferent length andhaveintermittent\nnon-matches. Intheconte xtofmusic retrie val,thisallows\nfortheuseofpartial andnoisy inputs.\n3.2.Thenewalgorithm\nWhen apiece ofmusic isexpanded orcompressed, we\nwould liketorecognize them asthesame. Forexam-\nple,if44556677 were matched against 4567, theoutput\nshould beahigh score. The LCS algorithm would out-\nput4,since thesequences havethesubsequence 4567 in\ncommon. Now,what ifthesequences 42536172 and4567\naregiven? The output ofLCS isstill4,since thecom-\nmon subsequence isagain4567. Butsince theﬁrstpairof\nsequences arejustexpanded/contracted versions ofeach\nother ,theyshould beconsidered asmore similar than the\nsecond pair.TheLCS algorithm cannotmakethisdistinc-\ntion.\nAswenoted earlier ,thedynamic time warping algo-\nrithm handles theexpansion andcontraction ofthese-\nquences butrestricts thealignment toandend-to-end fash-\nion,andispoor athandling insertions/deletions thatmight\neasily occur during music transcription. The LCS algo-\nrithm, ontheother hand, handles extra/skipped charac-\ntersandisveryﬂexible inthestarting/end points ofthe\nalignment, butdoes notdeal with expansion/contraction.\nWetakethedesirable properties ofeach andconstruct the\ntime-w arped longest common subsequence algorithm (T-\nWLCS) thatcandeal with both setsofissues.\nTherecurrence formula forT-WLCS is:c(i;j)=8\n>>>>>>>>>><\n>>>>>>>>>>:0 ifi=0or\nj=0\nmax[c(i;j\u00001);c(i\u00001;j);ifi;j>0and\nc(i\u00001;j\u00001)]+1 xi=Yj\nmax[c(i;j\u00001);c(i\u00001;j)]ifi;j>0and\nxi6=Yj\n(3)\nHerec(i;j)denotes theminimum cost T-WLCS path\nleading totheentryi,jintheT-WLCS table .TheT-WLCS\ntable isannbymtable thatkeeps track oftheminimum-\ncost T-WLCS path leading toeach possible alignment so\nfar(see Figure 1foranexample LCS table andT-WLCS\ntable). AT-WLCS path speciﬁes analignment between\ntwostrings. The optimal T-WLCS path istheonethat\nachie vestheminimum costalignment. This costisstored\ninthec(m;n)entry inthetable. The optimal paths for\neach oftheexamples areshownashighlighted squares in\nthetable (seeFigure 2).\n3.3.Examples\nExample1S1=“41516171”, S2=“4567”. Compar e\ntheoutput ofthetwoalgorithms onthesequences S1and\nS2.LCS(S1, S2)=4,T-WLCS(S1, S2)=4.\nFortheexample above,thetwoalgorithms giveiden-\ntical results, since “4567” isthestring theyhaveincom-\nmon. TheT-WLCS table showing therunofthealgorithm\nisshowninFigure 1(a).\nExample2S1=“44556677”, S2=“4567”. Compar e\ntheoutput ofthetwoalgorithm onthesequences S1and\nS2.LCS(S1, S2)=4,andT-WLCS(S1, S2)=8.\nHere, theT-WLCS givesthismatching ahigher score\nthan thepair inExample 1.This makessense since this\npairisactually more similar ,because oneisjustadoubly\nstretched version oftheother .TheT-WLCS table isshown\ninFigure 1(b).\nExample3S1=“4455661111177”, S2=“4567”, com-\nparedtheoutput ofthetwoalgorithms onsequences S1\nandS2.LCS(S1, S2)=4,T-WLCS(S1, S2)=8.\nThis showsthatT-WLCS retains theadvantage ofthe\noriginal LCS algorithm inthatitcanskip asection oftext\nandmatch inanon-continuous fashion. TheT-WLCS ta-\nbleforthisexample isshowninFigure 1(c).\n4.EXPERIMENTS\nWetested theeffectiveness ofoursimilarity measure cal-\nculated bytheT-WLCS algorithm onamusic informa-\ntionretrie valtask. Songs from theDigital Tradition col-\nlection were used because thiscollection consists entirelyQuery SequenceStored Sequence\nStored Sequence\nStored SequenceQuery Sequence\n2\n2 32 2\n2\n2 566\n2 2\n12 6751 4 444441511\n4\n5 6 77671\n5\n6\n711111111\n1122222\n112333\n1122334444556677\n122222\n1344444\n134 6\n123456784\n5\n6\n7\n4\n5\n6\n712222222222\n234444 4\n12346666666\n345666668Query Sequence\n4 111 5611 4\n6\n(c)(b) (a)\nFigure1.The T-WLCS tables containing c(i;j)values\nforExamples 1,2and3\noffolk songs that aremonophonic innature. Asubset\noftwohundred songs were randomly selected tobeused\ninquery/retrie val.Foreach song, wetested arange of\npossible query versions thatsimulate various types ofer-\nrors. Westretched outthesong byfactors ofuptofour,\nalso shortened andlengthened randomly selected notes\ntosimulate inaccuracies inrhythm. Random insertions\nanddeletion ofnotes were also made tosimulate both\nsinging errors anderrors that areintroduced during the\nmusic transcription process when acoustic signals arecon-\nverted tomusical notes. Different length queries were\ntested. Chunks ranging from 40to100percent oftheorig-\ninalsong were randomly selected touseasquery strings\nbefore theerror simulations described abovewere used to\nperturb thesongs.\nTheT-WLCS algorithm isused tocalculate similarity\nscores between agivenquery sequence andeach stored\nsong inthedatabase. The effectiveness oftheretrie val\nsystem ismeasured bycomparing theranking oftheorig-\ninalsong (that thequery isderivedfrom) toallother songs\ninthedatabase.\nWecompared theperformances oftheT-WLCS algo-\nrithm with thetraditional LCS algorithm andfound that\ntheretrie valismore accurate when theT-WLCS algo-\nrithm isused tocalculate thesimilarity score between two\nsongs. SeeFigure 2fordetails.\n5.CONCLUSION ANDFUTURE WORK\nInthiswork,wepresented theT-WLCS algorithm, which\ncalculates similarity scores betweens songs, allowing for\nvariations inspeed andinaccuracies intherhythm between\nthequery andthestored music. The algorithm wasem-\nployedonsong retrie valtasks andcompared favorably to\ntheoriginal LCS algorithm.\nWeplan toextend thisworkinseveraldirections. First,\nwewould liketoﬁnetune thealgorithm toinclude limits\nontheamount ofallowable distortion between songs. On\ntheapplication side, wewould liketoinvestig atetheap-\nplicability oftheT-WLCS algorithm topolyphonic music22.533.544.550.30.40.50.60.70.80.91\nFactors of expansion% of songs with high retrieval ranking (top 5%)Retrieval of Expanded versions of original\n song with random deletion noise\n22.533.544.550.30.40.50.60.70.80.91\nFactors of expansion% of songs with high retrieval ranking (top 5%)Retrieval of Expanded versions of original \nsong with random insertion noise\n22.533.544.550.30.40.50.60.70.80.91\nFactors of expansion% of songs with high retrieval ranking (top 5%)Retrieval of Expanded versions of original \nsong with random repetition noise0.50.60.70.80.9 10.40.50.60.70.80.9\nFraction of the original song used for retrieval% of songs with high retrieval ranking (top 5%)Retrieval using a fraction of the original \nsong with random deletion noise\n0.50.60.70.80.9 10.40.450.50.550.60.650.70.750.8\nFraction of the original song used for retrieval% of songs with high retrieval ranking (top 5%)Retrieval using a fraction of the original \nsong with random insertion noise\n0.50.60.70.80.9 10.450.50.550.60.650.70.750.80.850.9\nFraction of the original song used for retrieval% of songs with high retrieval ranking (top 5%)Retrieval using a fraction of the original\n song with random repetition noiseusing LCS scoring metric\nusing T−WLCS scoring metricusing LCS scoring metric\nusing T−WLCS scoring metric\nusing LCS scoring metric\nusing T−WLCS scoring metricusing LCS scoring metric\nusing T−WLCS scoring metric\nusing LCS scoring metric\nusing T−WLCS scoring metricusing LCS scoring metric\nusing T−WLCS scoring metric\nFigure2.Experimental comparison between theLCS al-\ngorithm andtheT-WLCS algorithm onsong retrie val\nretrie val.Since thealgorithm inherently operates onlinear\nsequences, wewillexperiment with methods that“ﬂatten”\noutapolyphonic score toalinear theme [16,12].Wealso\nplan toapply thealgorithm toclustering tasks thatinvolve\nmusic recordings, forwhich rhythmic differences could\nalsoplay animportant partinthesimilarity measure.\n6.REFERENCES\n[1]Innovativemusic systems, inc. wavto\nmidi, mp3 tomidi converter -intelliscore.\nhttp://www .intelliscor e.net/.\n[2]T.Cormen, C.Leiserson, andR.Rivest. Introduc-\ntiontoAlgorithms .TheMIT Press, Cambridge, MA,\n1990.\n[3]F.Damerau. Atechnique forcomputer detection and\ncorrection ofspelling errors. Comm. oftheACM,\n7(3):171–176, 1964.\n[4]D.Deutsch. Grouping mechanisms inmusic. InPsy-\nchologyofMusic ,Orlando, 1982. Academic Press.\n[5]J.French, A.Powell, andE.Schulman. Applica-\ntions ofapproximate wordmatching ininformation\nretrie val.InProceedings ofACMCIKM’97 ,pages\n9–15, 1997.[6]A.Ghias, J.Logan,D.Chamberlin, andB.Smith.\nQuery byhumming -musical information retrie val\ninanaudio database. InACMMultimedia 95-Elec-\ntronic Proceedings ,1995.\n[7]D.Gusﬁeld. Algorithms onStrings, TreesandSe-\nquences .Cambridge University Press, 1997.\n[8]K.Hamel. Thesalieri project -guido music notation.\nhttp://www .salieri.or g/guido/impl.html .\n[9]T.Kage yama, K.Mochizuki, and Y.Takashima.\nMelody retrie valwith humming. InICMC Proceed-\nings 1993 ,1993.\n[10] A.Klapuri. Automatic transcription ofmusic. In\nProceedings ofStockholm Music Acoustics Confer -\nence,2003.\n[11] K.Lemstrom. String Matc hing Techniques forMusic\nRetrie val.PhD thesis, University ofHelsinki, 2000.\n[12] A.Marsden. Modelling theperception ofmusi-\ncalvoices: acase study inrule-based systems. In\nComputer Repr esentations and Models inMusic ,\npages 239–263, London/San Diego,1992. Academic\nPress.\n[13] R.McNab, L.Smith, I.Witten, C.Henderson, and\nS.Cunningham. Towards thedigital music library:\nTuneretrie valfrom acoustic input. InProceedings\noftheDigital Libraries Confer ence,1996.\n[14] H.SakoeandS.Chiba. Dynamic programming al-\ngorithm optimization forspok enwordrecognition.\nIEEE Trans. Acoustics, Speec handSignal Process-\ning,ASSP-26(1):43–49, 1978.\n[15] S.Shlien andB.Vreck em. The abcmusic project\n-abcmidi. http://abc.sour cefor ge.net/abcMIDI/ ,\n2003.\n[16] A.Uitdenbogerd andJ.Zobel. Manipulation ofmu-\nsicformelody matching. InB.Smith andW.Ef-\nfelsber g,editors, Proceedings oftheACMMultime-\ndiaConfer ence,pages 235–240, Bristol, UK, Sept.\n1998.\n[17] A.L.Uitdenbogerd andJ.Zobel. Matching tech-\nniques forlargemusic databases. InD.Bulterman,\nK.Jeffay,andH.J.Zhang, editors, Proceedings of\ntheACMMultimedia Confer ence,pages 57–66, Or-\nlando, Florida, Nov.1999.\n[18] R.Wagner andM.Fisher .Thestring tostring cor-\nrection problem. Journal oftheACM,21:168–178,\n1974.\n[19] T.Yap,O.Frieder ,and R.Martino. High per-\nformance computational methods forbiolo gical se-\nquence analysis .Kluwer Academic Publishers,\n1996."
    },
    {
        "title": "Survey Of Music Information Needs, Uses, And Seeking Behaviours: Preliminary Findings.",
        "author": [
            "Jin Ha Lee 0001",
            "J. Stephen Downie"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417637",
        "url": "https://doi.org/10.5281/zenodo.1417637",
        "ee": "https://zenodo.org/records/1417637/files/HaD04.pdf",
        "abstract": "User studies focusing upon real-life music information needs, uses and seeking behaviours are still very scarce in the music information retrieval (MIR) and music digital library (MDL) fields. We are conducting a multi- group survey in an attempt to acquire information that can help eradicate false assumptions in designing MIR systems. Our goal is to provide an empirical basis for MIR/MDL system development. In this paper, we present our preliminary findings and analyses based on the 427 user responses we have received to date. Two major themes have been uncovered thus far that could have a significant influence the future development of successful MIR/MDL systems. First, people display “public information-seeking” behaviours by making use of collective knowledge and/or opinions of others about music such as reviews, ratings, recommendations, etc. in their music information-seeking. Second, respondents expressed needs for contextual metadata in addition to traditional bibliographic metadata. Keywords: context metadata, relational metadata, associative metadata, public information-seeking",
        "zenodo_id": 1417637,
        "dblp_key": "conf/ismir/HaD04",
        "keywords": [
            "user studies",
            "music information needs",
            "music information retrieval",
            "music digital library",
            "survey",
            "empirical basis",
            "MIR/MDL system development",
            "public information-seeking",
            "contextual metadata",
            "traditional bibliographic metadata"
        ],
        "content": "SURVEY OF MUSIC INFORMATION NEEDS, USES, AND \nSEEKING BEHAVIOURS: PRELIMINARY FINDINGS\nJin Ha Lee J. Stephen Downie \nUniversity of Illinois at \nUrbana-Champaign  \nGraduate School of Library \nand Information Science  University of Illinois at \nUrbana-Champaign  \nGraduate School of Library \nand Information Science  \nABSTRACT \nUser studies focusing upon real-life music information \nneeds, uses and seeking behaviours are still very scarce in the music information retrieval (MIR) and music digital library (MDL) fields. We are conducting a multi-group survey in an attempt to acquire information that can help eradicate false assumptions in designing MIR systems. Our goal is to provide an empirical basis for MIR/MDL system development. In this paper, we present our preliminary findings and analyses based on the 427 user responses we have received to date. Two major themes have been uncovered thus far that could have a significant influence the future development of successful MIR/MDL systems. First, people display “public information-seeking” behaviours by making use of collective knowledge and/or opinions of others about music such as reviews, ratings, recommendations, etc. in their music information-seeking. Second, respondents expressed needs for contextual metadata in addition to traditional bibliographic metadata.  \nKeywords: context metadata, relational metadata, \nassociative metadata, public information-seeking \n1. INTRODUCTION \nThis survey is being conducted as part of the Human Use of Music Information Retrieval Systems (HUMIRS) project  [7]. The primary goal of the HUMIRS project is the acquisition of real-world user data so that an empirically justifiable framework can be \ndeveloped within which the scientific evaluation of MIR/MDL systems can take pl ace. It is within this \nframework that we hope to create the TREC-like evaluation scenarios discussed in  [7].  \nWhat MIR/MDL development and evaluation \nrequires is a set of properly conducted “user needs and uses” studies as defined by Wilson  [11]. The ultimate goal of any needs and uses study is the capturing of real-world expressions of users’ actual information-seeking behaviours unmediated by any particular set of technologies. Using a variety of techniques including surveys, ethnographic obser vation, qualitative text \nanalysis, etc.,\n  needs and uses studies provide the \ninformation necessary to avoid creating the unbridgeable divides between system features and performance, and user expectations and skills that make system use untenable [2].  \nOnly a small handful of needs and uses studies have \nbeen conducted in the MIR/MDL domain. Thus, existing MIR systems have been designed and evaluated largely based on anecdotal evidence of user needs, \nintuitive feelings for user information-seeking behaviour, and a priori assumptions of typical usage scenarios  [4]. Some work has been done in the area of transaction log analysis of  online music catalogs  which \ncan provide rich information on user behaviours of a specific system or database. However, these queries are \nalready limited by the functions of specific systems so they cannot accurately represen t the real music needs of \nusers  [3]. Qualitative, grounded theory studies have looked at music-related onlin e forums, mailing-lists, and \ncommunities, and investigated various music search questions posted in natural language  [1],  [5]. The categories of needs and uses descriptions presented in  [1],  [5], and  [6] provided a starting point for designing our survey questions asking about people’s music and music information needs. \n2. SURVEY DESIGN AND IMPLEMENTATION \n2.1. Study Population, Sampling and Sample Size \nThere are two population groups examined in our survey. Group I comprises the UIUC campus community and Group II comprises the general population of those over 18 years old. In this paper, we present preliminary data from the responses received thus far from Group I. \nTo ensure the generalizability of our results, we \nadopted a stratified random sampling approach to select candidate respondents from our Group I pool. Group I comprises the 77,532 members of the UIUC campus population including undergraduates, graduate students, faculty and staff. We randomly selected a set of email addresses based upon stratification by sex and academic/professional status (six strata in all). Email invitations were sent out in three batches starting on April 9, 2004. We have collected 427 responses from our sample of 2,100 as of April 30, 2004. This represents a response rate of 20.3%. The number of responses is large enough to achieve a 95% confidence \nPermission to make digital or hard c opies of all or part of this work \nfor personal or classroom use is granted without fee provided that \ncopies are not made or distributed for profit or commercial \nadvantage and that copies bear this notice and the full citation on the first page. \n© 2004 Universitat Pompeu Fabra.   \n \nlevel, with ±5% margin of sampling error in generalizing the results to our study population.  \n2.2. Issues of Methodology \nA Web-based survey method was chosen because \nelectronic communications have become the primary and official communication medium at UIUC. We concede that people who responded to our survey are potentially more interested in music than the ones who did not. However, these are the people who would be the first to use the MIR/MDL systems we develop and therefore it seems appropriate to start with this group’s music information needs, uses, and seeking behaviours. \n2.3. Questionnaire \nThe survey questionnaire was designed based on \nconsultation with Dr. Brechin, Professor of Sociology at UIUC, who specializes in survey methods. The survey consists of four major parts: Demographic information; Respondent’s characteristics; Needs and uses;\n  Search \nbehaviours. \n3. DATA ANALYSIS AND DISCUSSION \n3.1. Introduction \nIn this section, we will discuss the responses from \nGroup I, the UIUC campus population. Our analysis includes a review of preliminary findings, possible interpretations of findings, and implications for MIR system design.\n1  \n3.2. Respondents’ Characteristics \nThe top-ranked music genres among the respondents were Rock, Pop, Classical and Alternative. The open-ended “other” responses include Korean, Japanese Pop, old Hindi, Italian, traditional Irish, etc. This is not surprising given the multicultural makeup of UIUC. \n 73.1% of respondents said they were avid listeners, \nand 36.3% said they were “Musically passionate.” With regard to music literacy and musical ability, 63.6% replied they can read sheet music “OK” to “Very well” \nand 64% expressed their singing ability is average or above. Also 74.5% answered they can play a musical instrument. \n \n3.3. Music Information Needs \nFinding 1. Descriptive metadata and extra-musical information have important commercial and experience enrichment aspects for users. \nThe top three categories in Table 1 are “Title of \nwork(s)” (90%), “Lyrics” (81.0%), and “Artist information” (74.6%). Each of these is either metadata \n                                                          \n \n1 In our survey, we asked questions about both music  and music \ninformation . However, for the purpose of the following discussion, we \nwill use music information  as a broad term for any music-related items \nor information, including recordings , printed and electronic materials, \nmultimedia and computer applications, etc. We will also use the term \nextra-musical information  to refer to information which is “about” \nmusic or music objects such as revi ews, biographies, histories, etc.  or extra-musical informati on. The commercial aspects \ncome to the fore when one looks at the 67.4% positive \nresponse for “Sample tracks for listening,” the 60.7% positive responses to “Price of item,” the positive response rate of 67.2% to “Learn about item before purchase” (Table 2) and the 47.1% positive expression for “Review/rating” information. The “Artist information” numbers along with “Information on genre” (49.1%), the “Influences among artists” (42.6%), and the “Background information” (39.1%) responses all suggest that users are deliberately seeking information to enhance their experience of the music they listen to. \nPositive Negative Don't \nknow                            Response \n \nMusic information % % % \nTitle of work(s) 90.1 7.4 2.5 \nLyrics 81.0 15.4 3.6 \nArtist information 74.6 23.7 1.7 \nSample tracks for listening 67.4 27.3 5.3 \nTrack listing 60.7 33.8 5.5 \nPrice of item 51.7 41.5 6.8 \nInformation on genre 49.1 46.3 4.6 \nReview/Rating by others 47.1 47.3 5.6 \nInfluences among artists 42.6 52.6 4.8 \nBackground information \n(history, theory, etc.) 39.1 55.4 5.6 \nInformation on different \nversion(s) of work(s) 37.3 55.7 7.0 \nArtwork/Album cover 30.8 62.8 6.5 \nLinks to related websites 29.7 62.2 8.0 \nReleased date 21.5 71.2 7.3 \nRecord label 15.0 77.9 7.0 \nTable 1. Responses to “How likely are you to seek \nthe following music information?”2  \n3.4. Reasons for Searching Music information \nFinding 2. Users seek music as an auditory experience. \nFinding 3. Users seek inform ation to assist in the \nbuilding of collections of music. Finding 4. Users seek music information for verifying or \nidentifying works, artists, lyrics, etc. \nMost of the respondents (94.5%) search for music to \nlisten to for entertainment which provides a strong argument for actually delivering the sought-after audio versions of the music in a simple and timely manner. The strongly positive “Build co llection” data, at 89.1%, \nstrikes us as significant for they suggest MIR/MDL uses beyond mere single-item identification. Notwithstanding this finding, the data also show that a large percentage (73.9%) of respondents search for music information, not to obtain an actual item or material, but to have enough information for “Verifying or identifying a work, artist,  lyrics, etc.” for which \n“name that tune” would be one appropriate strategy. \n                                                          \n \n2 Response categories collapsed as follows—Positive: [very likely + \nsomewhat likely];  Negative: [not very likely + not at all likely]    \n \nThe “Learn about artists (70.5%) and music” (54.5%)” data again suggest the important role extra-musical information plays in enrichi ng the music experiences of \nusers.\n \nPositive Never \nFrequency \n(times per month) \n≤ 1 2–4   ≥ 5 Total Total                Response \n \n               \nReason % % % % % \nListen for \nentertainment 18.0 33.4 43.1 94.5 5.5\nBuild collection 28.5 39.7 20.9 89.1 10.9\nVerify or identify \nwork, artist, lyrics 30.9 31.1 11.9 73.9 26.1\nLearn about artists 34.4 27.8 8.3 70.5 29.4\nLearn about item \nbefore purchase 32.9 26.4 7.9 67.2 32.7\nListen for work or \nstudy purposes 15.7 21.7 22.1 59.5 40.5\nLearn about music 31.8 16.0 6.7 54.5 45.5\nUse for special \noccasions  27.3 11.9 1.4 40.6 59.4\nLearn about \ninstrument(s) 23.0 10.5 4.0 37.5 62.4\nPerform with a \nmusical instrument 18.2 9.1 5.5 32.8 67.2\nKaraoke/Sing for \nentertainment  16.2 8.5 7.2 31.9 68.2\nUse for gadgets \n(ringtone, etc.) 19.5 9.1 1.9 30.5 69.6\nPlay at certain \nplaces (café, etc.) 15.5 7.9 2.6 26.0 74.0\nUse in teaching/ \ninstruction 12.6 3.8 1.1 17.5 82.5\nAcademic research  8.6 3.8 1.6 14.0 86.0\nSing professionally 4.5 2.4 1.7 8.6 91.4\nTable 2. Responses to “How often do you seek \nmusic or music information for the following reasons?”  \n3.5. Music-Related Online Activities \nFinding 5. Users value online music reviews, ratings, \nrecommendations, and suggestions. \n92.7% of respondents answered that they have used the \nInternet to search for music information. Among these respondents, reading music information including news, reviews, etc., purchasing recordings and listening to online radio were the most popular activities. About 1 out of 4 respondents (25.4%) said they listen to online radio “A few times a week” to “Almost every day.” 74.7% responded that they search for “Electronic music files” (Table 4), but only 39.4% actually make purchases, while 74.9% looked for free music files.  \nPositive Never \nFrequency  \n(times per month) \n≤ 1 2–4   ≥ 5 Total Total Response \n \n \nActivity \n% % % % % \nRead any kind of \nmusic information  29.4 36.7 16.9 83.0 17.0Purchase music \nrecordings (cd, etc.) 60.4 17.2 0.3 77.9 22.1\nListen to streaming/ \nonline radio 26.2 26.2 25.4 77.8 22.1\nDownload free \nmusic files 27.4 29.2 18.3 74.9 25.1\nVisit music stores 39.6 22.1 7.6 69.3 30.7\nPurchase music \nfiles 18.1 15.0 6.3 39.4 60.6\nDownload scores 23.8 5.1 1.8 30.7 69.2\nVisit music forum, \ncommunity, etc. 14.9 9.8 5.8 30.5 69.4\nRead/Subscribe to \nmusic listservs 9.1 5.1 4.6 18.8 81.2\nTable 3. Responses to “How often do you do the \nfollowing activities online?”  \nPeople gave a variety of responses regarding their \nfavourite music-related websites and the reasons they \nliked them. Respondents clearly chose different websites that are suitable for different purposes. The website mentioned the most was Amazon.com (24 responses). Easy searching, useful extra-musical features such as reviews, ratings, recommendations and Listmania were some of th e reasons they liked the \nwebsite. Amazon.com’s popularity is expected as it definitely meets most of the music needs mentioned in Table 1 except for such things as lyrics, genre and background information, etc. Allmusic.com was the second-most-mentioned website (another site rich in extra-musical information). Even though the counts were much lower, responde nts expressed very strong \nfondness for the site.  \n3.6. Music-Related Materials Sought \nFinding 6. Users prefer online resources for extra-\nmusical information. \nPositive Never \nFrequency \n(times per month) \n≤ 1 2–4   ≥ 5 Total Total                Response \n \n               \nMaterial % % % % % \nMusic recordings  \n(CD, vinyl, etc.) 38.9 36.5 11.6 87.0 13.0 \nElectronic music \nfiles (mp3, etc.) 24.0 30.0 20.7 74.7 25.2 \nMusic multimedia \n(VHS, DVD, etc.) 33.5 24.4 7.2 65.1 34.9 \nMusic news or \nentertainment \nnews 24.2 25.4 14.1 63.7 36.3 \nMusic-related \nsoftware 27.9 9.3 2.4 39.6 60.5 \nMusic magazines 21.9 10.9 2.9 35.7 64.3 \nBooks on music 26.0 7.1 0.7 33.8 66.2 \nSheet \nmusic/Scores 22.1 8.8 1.7 32.6 67.5 \nAcademic journal \narticles 12.1 3.6 0.2 15.9 84.0 \nTable 4. Responses to “How often do you search for \nthe following items both online and offline?”    \n \nA majority of the respondents answered that they search for “Music recordings” (87%),  “Electronic music files” \n(74.7%), “Music multimedia” (65.1%) and “Music and entertainment news” (63.7%).  Traditional paper-based books or journal articles that are the main sources of scholarly information were not sought as much. Even though more than half of the respondents said they search for music information to “Learn more about artists (70.5%) and music (54.5%)” from Table 2, only 33.8% search for “Books on music” and 15.9% search for “Academic journal articles.”  \n3.7. Places Visited for Music Information Search \nFinding 7. Users have definite preferences regarding \nwhere they physically go to seek music information. \n“Record store” (77.5%) and “Acquaintance’s/Friend’s place” (76.6%) are the principal physical places where respondents seek music information. These data are consistent with prior research that found the music store is the most significant physical source of music information for many people  [4].\n \nPositive Never \nFrequency \n(times per month) \n≤ 1 2–4   ≥ 5 Total Total                Response \n \n               \nPlace % % %  %  % \nRecord store 45.4 29.7 2.4 77.5 22.6 \nAcquaintance's/  \nFriend's place 30.5 39.6 6.5 76.6 23.4 \nLibrary 25.4 9.3 1.2 35.9 64.1 \nAcademic \ninstitution 17.9 6.9 2.7 27.5 72.6 \nTable 5. Responses to “How often do you go to the \nfollowing physical places to search for music or music information?”  \n3.8. Persons Consulted for Music Information Search \nFinding 8. Personal familiarity with search helpers is a \nkey determinant for music information seekers. \nPositive Never \nFrequency \n(times per month) \n≤ 1 2–4   ≥ 5 Total Total                  Response \n \n               \nPerson % % %  %  % \nFriend or \nfamily member 27.5 42.4 14.7 84.6 15.4 \nRecord store staff 32.9 11.6 1.2 45.7 54.3 \nMusician 17.3 9.9 4.5 31.7 68.2 \nOnline community \nor forum member 11.0 7.4 1.4 19.8 80.1 \nTeacher/Instructor 13.9 5.0 0.7 19.6 80.4 \nMusic librarian 8.6 2.7 0.2 11.5 88.6 \nTable 6. Responses to “How often do you ask the \nfollowing people for help when you search for music or music information?”  \nA majority of respondents (84.6%) ask friends or family \nmembers for help when they search for music information. Beyond mere knowledge of music, the availability and approachability of the helping person appear to affect respondents’ music searching strategies. We conjecture that a “comfort factor” might be involved in this user behaviour. Music queries can be difficult to express and can involve a certain amount of embarrassment (i.e., inability to sing, exposure of ignorance, etc.). Searchers appear to prefer asking those whom they expect will not j udge nor ridicule them.   \n3.9. Sources That Triggered Music Information \nSearches \nFinding 9. Music information- seeking should be seen as \na socially instigated act.\n \nPositive Never \nFrequency \n(times per month) \n≤ 1 2–4   ≥ 5 Total Total                Response \n \n               \nSource % % % % % \nAcquaintance's/ \nFriend's place 31.9 41.8 13.7 87.4 12.5\nRadio show 35.6 36.5 9.6 81.7 18.4\nTV show, movie, \nor animation 38.4 33.8 8.6 80.8 19.2\nPublic places (café, \nstore, bar, etc.) 32.6 30.5 6.9 70.0 30.0\nConcert/Recital 41.9 23.8 3.1 68.8 31.2\nAdvertisement or \ncommercial 37.3 22.4 4.5 64.2 35.8\nSpecial occasion \n(party, event, etc.) 39.2 13.3 1.9 54.4 45.6\nCultural event 33.3 10.8 2.1 46.2 53.7\nTable 7. Responses to “How often do you search for \nmusic you heard from the following places or events?” \nThat “Acquaintance’s or friend’s place”, with its \n87.45% positive response rate, was named the most \ncommon triggering source for instigating a music information search is quite noteworthy. In conjunction with the “Public places” (70. 0%), “Special occasion” \n(54.4%) and “Cultural event” (46.2%) data, we see a strong contextual association between the social interactions of the seekers and the instigation of their music information searches. Media was also a major source that triggers respondents’ music information-seeking as the positive responses for “Radio show” (81.7%), “TV show, movie, or animation” (80.0%), “Advertisement or commercial” (64.2%) show.  \n3.10. Preferred Search/Browse Options \nFinding 10. Music informa tion seekers employ public \nknowledge and/or opinions for searches.  \nIn analyzing the top 10 positiv e responses from Table 8, \nregarding “Search/Browse opti ons”, we note that all but \none are classified as either  metadata or extra-musical \ninformation. The “Singing/Humming” option is the \nexception as it is based in the music itself.  Despite the rarity of extant MIR systems providing query by a “Singing/Humming” option, 34.8% said they would still be likely to use it.    \n \nWe again observe the social side of music \ninformation-seeking as 62.2% responded that they are \nlikely to use “Recommendations from other people.” Respondents appear to rely on collective knowledge and/or opinions on music in their seeking processes. This corresponds with our earlier observation of the important role friends and family members play in both the triggering and helping with music information-seeking. \n41.9% of respondents said they would search or \nbrowse music information by  “Associated usage.” This \nties in with both the social and media aspects of music information-seeking triggers. This kind of extra-musical information is not traditionally incorporated in MIR systems. This might be a contributing reason why respondents so often consult with friends and family \nmembers who could provide this kind of information. \n \nPositive Negative Don't \nknow                           Response \n \nSearch/Browse by % % % \nSinger/Performer 96.2 2.8 1.0 \nTitle of work(s) 91.7 6.4 1.9 \nSome words of the lyrics 74.0 22.3 3.6 \nMusic style/Genre 62.7 33.0 4.4 \nRecommendations  62.2 34.2 3.6 \nSimilar artist(s) 59.3 36.4 4.3 \nCreator (composer/author) 54.5 40.9 4.6 \nSimilar music 54.2 41.0 4.8 \nAssociated usage (ad, etc.) 41.9 50.9 7.2 \nSinging/humming 34.8 55.1 10.1 \nTheme (main subject) 33.4 59.7 7.0 \nPopularity 31.0 62.8 6.3 \nSpecific version 29.1 60.4 10.6 \nMood/Emotional state  28.2 63.5 8.4 \nLanguage 23.8 69.0 7.2 \nTime period 23.8 68.5 7.7 \nCountry 23.6 69.9 6.5 \nOccasions to use 23.6 68.2 8.2 \nInstrument(s) 20.8 71.7 7.4 \nPlace/Event where heard 20.7 69.1 10.1 \nPurchase patterns  20.6 69.3 10.2 \nStoryline of music 17.9 70.5 11.6 \nVocal range/Genders  16.2 74.9 8.9 \nTempo  14.2 75.4 10.4 \nUsing keyboard input 13.2 72.5 14.4 \nReleased/Composed year 12.3 80.6 7.2 \nRecord label  11.7 81.5 6.7 \nPublisher 6.0 85.4 8.6 \nTable 8. Responses to “When you search for music \nor music information, how likely are you to use the following search/browse options?”\n1 \n4. CONCLUSION \n4.1. Public Information-seeking \nThe survey data illustrate that music information-\nseeking is not just a private and isolated process, but \n                                                           \n1 Response categories collapsed as follows; \nPositive: very + somewhat  likely, Negative: not very + not at all likely  also can be a public and shared process. With 47.1-\n84.6% of respondents s howing positive opinions \ntowards reviews, ratings, recommendations from other people, etc. (i.e., extra-musi cal information), we see a \nclear indication of the impor tance of the social and \ncommunal side of music information-seeking. Respondents make use of collective knowledge or opinions on music created by other community members in their searching processes. We see these behaviours as a variation on the idea of “collaborative information retrieval”  [10]. It is a variation on this theme in the sense that when people are generating or using the collective knowledge in their music information-seeking, it is not  always the case that there \nis a single specific goal or an swer that they have in \nmind and feel necessary to work towards. Rather, this is a more flexible and less directed process of exploration. Future MIR/MDL systems that take this aspect of user \nbehaviour into account s hould provide a successful \nservice to music information seekers.  \n4.2. Need for Context Metadata  \nThroughout the survey, we see the importance of extra-\nmusical information and informal social interactions in music information-seeking. The data suggest that we should start developing new ty pes of metadata as access \npoints that take into account the extra-musical and associative kinds of information which contextualize users’ real-world search es. The necessity for access \npoints that link music with external objects or events \nhas already been mentioned in  [5]. We suggest that serious work begin on designing “context metadata” frameworks. Context metadata is distinct from “content” metadata in that content metadata is intrinsic  \nto an object and relates to what the object is, or contains, whereas context metadata indicates the extrinsic aspects, uses and relationships of an object  [9]. To this end, we suggest the following metadata framework that can serve as a guide for future MIR/MDL development:  \n Content Metadata \n◦ Musical metadata : data derived directly from \nthe music itself (e.g., melody, tempo, etc.)  \n◦ Bibliographic metadata : traditionally-used \nmetadata that describes the item (e.g., title, \nauthor, etc.)  \n Context Metadata \n◦ Relational metadata : data about the item’s \nrelationships (artificially created or socially \nconstructed) with other music related items (e.g., genre; indications of similarity, etc.)  \n◦ Associative metadata : data indicating \nassociations with other works, media or events (e.g., use in TV, movies or commercials; use at special events, etc.)     \n \nThe need for “relational metadata” was highlighted \nas more than half of respondents expressed positive \nopinions towards “Genre” (62.7%), “Similar artist(s)” (59.3%), and “Similar music” (54.2%) as search or browse options. Similarly, the need for “associative \nmetadata” is evident in the data that show the very high percentage of users reporting that their searches were triggered by such things as a “Radio show” (81.7%), a “TV show, movie or animation” (80.8%) or “Advertisement or commercial” (64.2%).  \nCreating useful context metadata will not be an easy \ntask: they are difficult—perhaps impossible—to generate automatically. Furthermore, context metadata cannot be generated solely from an individual item or at the point of the item’s production or creation. Notwithstanding these difficulties, a possible way to achieve the creation of context metadata might be to include music community members or subject enthusiasts  [8] in a form of collective production.  \n5. FUTURE RESEARCH \nIn this paper, we presented descriptive statistics and analyses of our initial Group I (University of Illinois community) data set. Our future papers will provide detailed inferential statistical analyses and explore the relationship between multiple variables (e.g., level of music literacy, musical ability, favourite genre, etc.) and music information needs, uses, and search patterns. We will also compare the results from both the Group I and II (general adult public) samples to uncover any significant differences between them. \nOver the life of the HUMIRS project, we hope to \ncontribute to the success of the next generation of MIR/MDL systems by providing meaningful insights into the music information needs and uses of potential MIR/MDL users. \n6. ACKNOWLEDGEMENTS \nDrs. Don Waters and Suzanne Lodato, both of the Andrew W. Mellon Foundation, are thanked for their moral and financial support. This project is also supported by the National Science Foundation (NSF) under Grant Nos. NSF IIS-0340597 and NSF IIS-0327371. We would like to thank Dr. Brechin, Dr. Estabrook,\n  and Bijan Warner at Library Research \nCenter. Karen Foote Retzer at  the UIC Survey Research \nLab, Dawn Owens-Nicholson at ATLAS, Melissa Cragin, and M. Cameron Jones are also thanked for their valuable contributions to the project.  \n7. REFERENCES \n[1] Bainbridge, D., Cunningham, S.J., and Downie, J.S. “How people describe their music information needs: A grounded theory analysis of music queries”, Proceedings of the International Symposium on Music Information Retrieval , Baltimore, USA, 2003. \n[2] Belkin, N.J., Oddy, R.N., and Brooks, H.M. “ASK for information retrieval: Part I. Background and Theory. Journal of \nDocumentation , 38(2), 61-71, 1982. \n[3] Cunningham, S.J. “User studies: A first step in designing an MIR Testbed”, MIR/MDL \nEvaluation Project White Paper Collection 2 , \n2002 \n[4] Cunningham, S.J., Reeves, N., and Britland, M. “An Ethnographic Study of Music Information Seeking: Implications for the \nDesign of a Music Digital Library”, Proceedings of the 3\nrd ACM/IEEE-CS joint \nconference on Digital Libraries , Houston, \nUSA, 2003.   \n[5] Downie, J.S., Cunningham, S.J. “Toward a Theory of Music Information Retrieval Queries: System Design Implications”, Proceedings of the International Symposium on Music Information Retrieval , Paris, France, \n2002.   \n[6] Downie, J.S. “The Musifind Musical Information Retrieval Project, Phase II: User Assessment Survey”, Proceedings of the 22\nnd \nAnnual Conference of the Canadian Association for Information Science , Toronto, \nCanada, 1994. \n[7] Downie, J. S. “The Creation of Music Query Documents: Framework and Implications of the HUMIRS Project”, Proceedings of \nACH/ALLC, Göteborg, Sweden, 2004. \n[8] Greenberg, J. “Metadata and the World Wide Web”, The Encyclopedia of Library and \nInformation Science , Vol.72, 224-261, Marcel \nDekker, New York, 2002. \n[9] Gilliland-Swetland, A.J., “Setting the Stage”, Introduction to Metadata: Pathways to Digital Information , Getty Research Institute, \nhttp://www.getty.edu/res earch/institute/standar\nds/intrometadata/2_articles/index.html  \n(accessed 5/2004). \n[10] Karamuftuoglu, M., “Collaborative information retrieval: Toward a social informatics view of IR interaction”, Journal of \nthe American Society for Information Science , \n49(12), 1070-1080, 1998. \n[11] Wilson, T.D. “Information needs and uses: fifty years of progress”, in: B. C. Vickery, (Ed.), Fifty years of information progress: a \nJournal of Documentation review , London: \nAslib, 1994."
    },
    {
        "title": "Audio Features for Noisy Sound Segmentation.",
        "author": [
            "Pierre Hanna",
            "Nicolas Louis",
            "Myriam Desainte-Catherine",
            "Jenny Benois-Pineau"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415214",
        "url": "https://doi.org/10.5281/zenodo.1415214",
        "ee": "https://zenodo.org/records/1415214/files/HannaLDB04.pdf",
        "abstract": "Automatic audio classification usually considers sounds as music, speech, silence or noise, but works about the noise class are rare. Audio features are generally specific to speech or music signals. In this paper, we present a new audio feature sets that lead to the definition of four classes: colored, pseudo-periodic, impulsive and sinusoids within noises. This classification relies on works about the per- ception of noises. This audio feature set is experimented for noisy sound segmentation. Noise-to-noise transitions are characterized by means of statistical decision model based on Bayesian framework. This statistical method has been trained and experimented both on synthetic and real audio corpus. Using proposed feature set increases the discriminant power of Bayesian decision approach com- pared to a usual feature set.",
        "zenodo_id": 1415214,
        "dblp_key": "conf/ismir/HannaLDB04",
        "keywords": [
            "audio",
            "classification",
            "sounds",
            "music",
            "speech",
            "silence",
            "noise",
            "perception",
            "noises",
            "segmentation"
        ],
        "content": "AUDIOFEATURESFORNOISYSOUNDSEGMENT ATION\nPierreHanna,NicolasLouis,MyriamDesainte-Catherine ,JennyBenois-Pineau\nSCRIME -LaBRI\nUniversit´edeBordeaux 1\nF-33405 Talence Cede x,France\nABSTRACT\nAutomatic audio classiﬁcation usually considers sounds\nasmusic, speech, silence ornoise, butworks about the\nnoise class arerare. Audio features aregenerally speciﬁc\ntospeech ormusic signals. Inthispaper ,wepresent anew\naudio feature setsthatleadtothedeﬁnition offourclasses:\ncolored, pseudo-periodic, impulsi veandsinusoids within\nnoises. This classiﬁcation relies onworks about theper-\nception ofnoises. This audio feature setisexperimented\nfornoisy sound segmentation. Noise-to-noise transitions\narecharacterized bymeans ofstatistical decision model\nbased onBayesian frame work.This statistical method has\nbeen trained andexperimented both onsynthetic andreal\naudio corpus. Using proposed feature setincreases the\ndiscriminant powerofBayesian decision approach com-\npared toausual feature set.\n1.INTRODUCTION\nAdvances inconsumer home devices andbroadcast tech-\nnologies permit individuals toenjoyalargeamount ofau-\ndio/visual (A/V) content. Tomanage such wide quantity\nofincoming data, users need automatic techniques. Inthe\nlastdecade, different authors such as[1,2]haveproposed\nmethods forstructuring A/V content which arebased on\naudio visual descriptors. Inparallel, asigniﬁcant progress\nhasbeen made inautomatic audio classiﬁcation forvar-\nious application areas [3,4].Namely ,alargeamount\nofworkaddresses theproblem ofclassiﬁcation ofaudio\nintomusic, speech, silence andnoise. Amongst those four\nclasses, thenoise class isthemost comple xandunstruc-\ntured one. Still fewresearch isdevoted totheanalysis of\nnoise inaudio.\nInthispaper ,weareinterested inacharacterization of\nnoisy sounds anditsapplication totheproblem ofdetect-\ningnoise-to-noise transitions insound tracks. Theworkis\nbased ontheassumption thataudio classiﬁcation inspeech,\nmusic, noise andsilence hasalready been done. Infact,\naudio tracks inbroadcast A/V programs orA/V works\ncontain various noise segments andtransitions between\nPermission tomakedigitalorhardcopiesofallorpartofthisworkfor\npersonalorclassroom useisgrantedwithoutfeeprovidedthatcopies\narenotmadeordistributedforpro®torcommercial advantageandthat\ncopiesbearthisnoticeandthefullcitationonthe®rstpage.\nc\r2004UniversitatPompeuFabra.them. Locating those transitions isessential intheﬁeld\nofA/V content structuring. Thus, weareﬁrstly interested\ninaudio features which cancharacterize noisy sounds in\nthebest way.Then, anadequate method fornoise tran-\nsition detection canbeproposed, which weformulate as\nasemi-blind segmentation problem based onselected fea-\ntures. The paper isorganized asfollo ws. The choice of\naudio features andcharacterization ofnoisy sounds ispre-\nsented insection 2.The segmentation method isintro-\nduced insection 3.Some results arepresented insection\n4.Conclusion andperspecti vesaregiveninsection 5.\n2.CLASSES OFNOISYSOUNDS ANDAUDIO\nFEATURES\nAccording toourprevious work[5],noisy sounds canbe\nclassiﬁed into four classes. This classiﬁcation isbased\nonperceptual properties. Inthis section, wefocus on\nthese classes andpropose thechoice ofrelevantfeatures\nfortheir identiﬁcation.\n2.1.ColoredNoise\nThe ﬁrst category ofnoisy sounds isintuiti velythecat-\negory composed ofsounds thatcanperfectly besynthe-\nsized byﬁltering white noise: alltheperceptual properties\nofthese sounds areassumed tobecontained intheshort-\ntime spectral envelopes [6].Thename ofColoredNoises\nisduetotheanalogy with thecolor oflight. The exam-\nplesofsuch sounds arenumerous: sounds from seashore,\nwind, breathing, etc...\nThe main characteristic ofcolored noises istheen-\nvelopes oftheir short-time spectra. Itisuseful todeter -\nmine oneorafewparameters fordescribing thisproperty .\nAfewfeatures havebeen shownasuseful forspeech sig-\nnalormusical sounds. Forexample theSpectral Roll-Of f\n[7]orthespectral centroid areparticularly useful todis-\ncriminate voice from unvoiced music. However,theap-\nplication ofthese features tonoisy sounds islessprecise.\nWepropose here toapply theresearch results ofGood-\nwinabout theresidual modeling ofsounds [8].This work\nrelies onthenoise model ofperception which states that\nabroadband noise iscorrectly represented bythetime-\nvarying energyinEqui valent Rectangular Bands (ERBs).\nHoweverwepropose toadapt thismethod bychoosing\ntheBark scale instead oftheERBs, because thenumber\nofbark bands issmaller than thenumber ofERBs: thischoice implies afewer number offeatures. Short-time\nspectral envelopes ofnoisy sounds arerepresented bythe\nshort-time energieswithin each Bark band. Therefore, one\nfeature (composed of26values) characterizes thecolor of\nthenoisy sounds. Variations ofonly onevalue indicate\namodiﬁcation ofthecolor ,thatmay bepercei ved,since\neach feature isrelated totheperception.\n2.2.Pseudo-periodic Noises\nSeveralnatural noisy sounds arecharacterized bythepitch\nthatcanbepercei ved,forexample machine noises, insect\nﬂies, scratching noise, etc...Thepitch may havedifferent\nstrengths, andseveralpitches canbepercei vedatthesame\ntime. This property may havedifferent explanations: the\nspectral envelope ofsounds iscomposed ofafrequenc y\nband ofhigh energy[9],thenoisy sounds aremixesof\nseveralsounds, thenoisy sounds areconsidered asasum\nofafewsinusoids thatimply noises with percei vedpitch\n[5]orthenoisy sounds areassumed asrippled noise [10].\nThe twoﬁrst cases respecti velyarepresented inthesec-\ntions 2.1and2.4.Inthissection, wepropose twofeatures\nwhich may characterize theclass ofnoisy sounds thatis\ndescribed bythetwoother cases.\nThe feature wedeﬁne hastodescribe thepercei ved\npitch ofasound intwoways: thestrength ofthepitch\nanditsvalue. Afewmethods havebeen proposed inpsy-\nchoacoustics tomeasure thepitch strength ofrippled noise\n[10].The method wechoose relies ontheautocorrela-\ntionfunction \u0000,andmore precisely ontheratio (denoted\nAR) ofthesecond maximum oftheautocorrelation func-\ntion(denoted \u0000(\u001c))toitsﬁrstvalue\u0000(0) (total energyof\nthesignal). This feature isalready knownasatechnique\nforthesegmentation ofspeech into voiced andunvoiced\nparts, forthepitch estimation ofaharmonic sound and\nforspeech recognition [11].Nevertheless itappears tobe\nveryuseful tocharacterize noisy sounds.\nFurthermore, twopseudo-periodic noises may haveone\nsimilar autocorrelation ratio without being thesame sound.\nThat’ swhy wepropose asecond feature torepresent pseu-\ndo-periodic noises: theestimation oftheperiod p=\u001c\nR(R\nsample rate).\n2.3.ImpulsiveNoise\nSeveralnatural noisy sounds arecomposed ofperiodic or\naperiodic impulses. Forexample, onecanthink about ap-\nplauses, walking steps, raindrops, etc...Theyarereferred\nasimpulsive noise [12].Frequenc yofpulses composing\nthistype ofsounds hastobelower than approximately\n20Hz.Otherwise thisfrequenc yisdetected bythehearing\nsystem asapitch.\nThe pulses contained inimpulsi venoises aresimilar\ntothetransients (attacks) ofinstrumental sounds. Some\nmethods forthedetection ofthetransients havebeen de-\nveloped relying onthevariations ofenergyoronthezero-\ncrossing rate(ZCR). However,wethink thatthese meth-\nodscanhardly lead tothedeﬁnition offeatures forthe\ncharacterization ofimpulsi venoises. Indeed, thepresenceofenergyinhigh frequencies may indicate pulses butalso\njustthelevelofnoise.\nWepropose tostudy thedistrib ution ofsamples asex-\nplained in[5].Properties about thisdistrib ution canbe\nquantiﬁed bythekurtosis. Alocal pulse, characterized by\nasharp probability density function, induces ahigh value\nofkurtosis. Akurtosis value isaffected toeach frame. A\nthreshold hastobechosen inorder todeﬁne frames that\nareassumed tocontain onepulse. Each kurtosis value\ngreater than thethreshold isconsidered asanimpulsi ve\nframe. The more important andaudible thepulse is,the\nhigher thekurtosis value is.Therefore, thekurtosis value\nisnotonly anindicator ofthepresence ofpulses, butalso\nafeature thatdescribes thenature ofthepulse.\nWethink thatitisalsoimportant tobeable todiscrim-\ninate impulsi venoises thatdonotdifferbythenature of\nthepulses, butbytheir periodicity .That’ swhy wepro-\npose tocomplete thekurtosis value with theperiodicity of\nthepulses composing impulsi venoises. This periodicity\nisnullifonly onepulse hasbeen detected.\n2.4.Sinusoids withinNoise\nClassiﬁcation systems usually consider natural sounds as\nmusic sound, speech sound ornoise. However,realworld\nnoises canrarely beassumed aspure noises, because they\nmay benothing butmixesofseveralsounds from different\nsources, thatmay beharmonic.\nHere weconsider therealworld sounds which areas-\nsumed asbeing mixesofseveralsound sources. Ifoneor\nsome ofthese sources areharmonic orpseudo-harmonic\nandifthenoise levelisnottoohigh, these harmonic sources\ncanbepercei ved:theyarethus important perceptual char-\nacteristics ofsuch sounds. The examples ofreal world\nsounds ofthisclass arenumerous: street sounds-capes\nwith horns, wind intrees with singing birds, seashore with\nseagulls, etc...\nNatural noises arerepresented byshort time amplitude\nspectra thatarecomposed ofpeaks which correspond to\nsinusoids contained inthesound. Thefeature associated\ntothisclass ofnoisy sounds issimply thenumber ofsinu-\nsoids. Severalanalysis methods havebeen proposed inthe\nconte xtofsound analysis/synthesis. Anoriginal method\nhasbeen proposed in[5].This technique isaccurate with\nnoisy sounds anditisindependent from thegeneral vol-\nume ofthesound. Itisbased onthestatistical analysis\noftheintensity ﬂuctuations. Ameasure foreach binof\ntheamplitude spectra iscomputed andachosen threshold\npermits todeﬁne thenumber ofbins thatcorrespond to\nsinusoids. Therefore, anumber ofsinusoids isassociated\ntoeach analysis frame. Weconsider theanalyzed sound\nasamixofnoise andharmonic sounds ifthisnumber of\nsinusoids isgreater than achosen threshold.\n3.STATISTICAL SEGMENT ATIONOFNOISE\nTRANSITIONS\nAfter thestudy ofnoise signal features weaddress here\ntheproblem ofdetection oftransitions between differentnoisy sounds inatime-v arying audio signal. This tran-\nsition canhappen both between noises ofthesame class\nandbetween noises from different classes asdescribed in\nsection 2.Inanycase wepropose ablind segmentation\nscheme which consists inthefollo wing. Forapair of\nconsecuti vetemporal windo wsonatemporal noise sig-\nnaltheproblem istocheck iftheboundary between the\nwindo wscorresponds toachange from onesound toan-\nother orthesound iscontinuous. This segmentation can\nbecalled ”semi-blind”. Infactwewillusethedescriptors\nwhich characterize noisy sounds thebest (versus speech\nandmusic descriptors), butstillrealize ablind segmenta-\ntionapproach similarly to[1].Weformulate thesegmen-\ntation problem inageneral Bayesian frame work. Inour\nproblem, thestochastic variable x=(x1;:::;xm)Trepre-\nsents avector ofaudio features withmthetotal number of\nfeatures measured along thetime andtwohypotheses are\nconsidered, H1-”the absence ofaudio variation attime\nt0”andH2-”audio variation attimet0”.Wewillsup-\npose thatH1andH2form thepartition ofthespace of\nhypotheses, thatisPr(H1)+Pr(H2)=1(Obviously ,\nfrom thesense ofourproblem Pr(H1\u0002H2)=0).From\nthewell-kno wnBayes formula thefollo wing may bein-\nduced:\n(\nPr(H1=x)=f(x=H1):Pr(H1)\nf(x)\nPr(H2=x)=f(x=H2):Pr(H2)\nf(x)(1)\nwithf(x)andf(x=Hk)representing theprobability den-\nsityfunction andtheconditional probability density re-\nspecti vely.Wewillsuppose Gaussian distrib utions N0(\u00160;\n\u00060)associated toH1fortheinterv al[t0\u0000n;t0+n],\nN1(\u00161;\u00061)andN2(\u00162;\u00062)associated toH2forthein-\ntervals[t0\u0000n;t0]and[t0+1;t0+n]respecti velywithn\naparameter .Then achange ofaudio stream attimet0can\nbeexpressed interms ofthelikelihood ratio as:L1\nL2=A\nB\nwhere A=Qt0+n\nt=t0\u0000n1\n(2\u0019)m\n2\u0002p\ndet(\u00060)\u0002e\u00001\n2(XT\nt\u0002\u0006\u00001\n0\u0002Xt)\nandB=Qt0\u00001\nt=t0\u0000n1\n(2\u0019)m\n2\u0002p\ndet(\u00061)\u0002e\u00001\n2(XT\nt\u0002\u0006\u00001\n1\u0002Xt)\n\u0002Qt0+n\nt=t01\n(2\u0019)m\n2\u0002p\ndet(\u00062)\u0002e\u00001\n2(XT\nt\u0002\u0006\u00001\n2\u0002Xt)\nwithLj=f(x=Hj)\u0002Pr(Hj)thelikelihood function of\nthehypothesis Hj,j=1;2and\u0006kthecovariance matrix,\nk=0;1;2andXacentered measurement vector .\nFollowing theusual development, thatistaking thelog-\narithm ofthelikelihood ratio, weobtain:\nM=n\n2[ln(det(\u00061))+ln(det(\u00062))]\u0000n\u0002ln(det(\u00060))\n+1\n2[Pt0\nt=t0\u0000n(XT\nt\u0002\u0006\u00001\n1\u0002Xt)\n+Pt0+n\nt=t0+1(XT\nt\u0002\u0006\u00001\n2\u0002Xt)\n\u0000Pt0+n\nt=t0\u0000n(XT\nt\u0002\u0006\u00001\n0\u0002Xt)]\nM<\n>2\u0002n\u0002ln(P=(1\u0000P))=>H2\n=>H1\n(2)\nHere, Pistheprobability ofthehypothesis H2-noise\ntransition. The segmentation method isthus asfollo ws.\nFeature vectors xwill bemeasured intwoconsecuti ve\nsliding windo ws. The decision onchange will bemade\naccording to(2).Inthenextsection theresults ontheuseofvarious noise descriptors forsemi-blind segmentation\narepresented.\n4.EXPERIMENTS\nHere wecompare twosetsofdescriptors fornoise seg-\nmentation using statistical method from Section 3.The\nﬁrstsetofdescriptors isconstituted ofSpectral Centroid,\nSpectral Flow,Roll-Of f,Zero-Crossing Rate andMelFre-\nquenc ySpectral Coef ﬁcients (only the13ﬁrstcoefﬁcients)\n[7].The second one, which wepropose, consists inthe\nenergydistrib ution amongst theBark’ sBands, Kurtosis,\nPeriod ofKurtosis, Auto-Correlation Ratio (ACR), Period\noftheACRandNumber ofSinusoids [7,5].\nInorder todetermine therelati vediscriminati vepower\nofthetwosets ofdescriptors weconducted theexperi-\nments both onsynthetic noise testdata setandonex-\ncerpts from audio tracks ofreal-w orld broadcast corpus.\nThe synthetic corpus wascomposed ofimpulsi ve,peri-\nodic, sinusoidal andcolored noises generated with con-\ntrolled parameters bythenoise generation tooldeveloped\nin[5].Various combinations ofnoises inside thesame\nclass, described insection 2such asimpulsi ve,colored,\netc...,were tested. Allcombinations ofnoises from differ-\nentclasses follo wing each other along thetime were pro-\nduced aswell andsubmitted tothechange detector .The\nrealnoise corpus contained alimited number (15) oftran-\nsitions. Thelikelihood ratio normalized bythecardinal of\nfeature setwascomputed forboth setsofdescriptors. The\nresults ofthiscomparison areshowninFigure 1.These\nresults were obtained onasynthetic noisy sound corpus.\nFigure 1apresents thenormalized loglikelihood ratio for\nthedescriptors ofthegroup 1.Results fortheproposed\ngroup 2areshowninﬁgure 1b.Itcanbeseen thatthe\ngroup 2exhibits much stronger minimum ofthenormal-\nized loglikelihood ratio inthecase ofnoise change. This\nsituation istypical. Intable 1wealsoshowthedifferences\nbetween theglobal minimum andtheclosest minimum of\nnormalized loglikelihood ratio curve.The ﬁrst column\nintable 1contains thetype oftransition, e.g. i\u0000imeans\nthatthetransition isobserv edinground truth between two\nimpulsi venoises. Thesecond column contains thesound\nfeature changed inthesynthetic sound generated. Thein-\nterclass transitions arenotated with thekey-word”Class”.\nThelasttwocolumns contain theabsolute difference be-\ntween theglobal minimum andtheclosest minimum of\nthenormalized loglikelihood ratio. Itcanbestated that\nthisdifference incase ofthesecond group offeatures is\nstronger initsabsolute value. Therefore thediscriminati ve\npowerofthesecond group ofdescriptors isstronger .\nAsitcanbeseen from (2)thedecision onanoise change\nisbased onaprobability dependent threshold andonthe\nsizeofmeasurement windo w.Itcanbeseen from theta-\nble1thatthevariability ofthegapbetween the”change”\nminimum andtheclosest minimum israther strong. This\nmakesusconclude, thatthethreshold should beadapti ve\nandwedynamically train itontheﬁrstmeasured windo ws\nsupposing thecontinuity ofnoise. Withthisassumption0 50 100 150 200 250 300 350 400 450−3.5−3−2.5−2−1.5−1−0.50\nThreshold line\ntime (frame)Normalized log of likelihood ratio\n0 50 100 150 200 250 300 350 400 450−10−9−8−7−6−5−4−3−2−10\nThreshold line\ntime (frame)Normalized log of likelihood ratio\na) b)\nFigure1.Normalized loglikelihood ratio forgroups of\naudio features: a)First group, b)Second proposed group\nTransitions Differences Gr.1 Gr.2\ni1\u0000i2 Period 0.50 4.10\ni1\u0000i3 Peaks’ Magnitude 0.32 0.40\ni\u0000p Class 0.50 19.00\ni\u0000s Class 0.30 4.20\ni\u0000c Class 0.35 3.40\np\u0000s Class 1.17 7.80\np1\u0000p2 Magnitude ofACR 10.50 18.65\np1\u0000p3 Period 10.00 18.80\np\u0000c Class 0.52 19.35\ns1\u0000s2 Magnitude ofsin 1.30 4.85\ns1\u0000s3 Number ofsin 0.60 2.55\ns\u0000c Class 0.55 4.50\nc\u0000c Color 0.68 3.86\nTable1.Comparison ofdiscriminati vepower oftwo\ngroups ofdescriptors: (i\u0000impulsi ve,p\u0000pseudo-periodic,\nc\u0000colored noise ands\u0000sinusoidal)\nandontheproposed second descriptor set,thesemi-blind\nsegmentation method performs well onalimited realtest\ndata setwehave.Withdynamic training ofathreshold\nweobtain arecall ﬁgure of86.67%andthesame preci-\nsion of86.67%with regard totheground truth onareal\nbroadcast audio noise transitions using thesecond group\nofdescriptors.\n5.CONCLUSION ANDPERSPECTIVES\nInthispaper ,weproposed anewsetoffeatures tocharac-\nterize noisy sounds. Weproposed asemi-blind segmen-\ntation ofnoise transition based onaclassical Bayesian\napproach. Wehavealso shownthatnoise-to-noise tran-\nsition detection canbeimpro vedusing relevantfeatures.\nForthatpurpose, ourcomparison between classical and\nproposed features illustrates thatthediscriminant power\nofthestatistical segmentation rulehasbeen considerably\nincreased using ourproposed feature setboth onsynthetic\nsound andrealsound corpus.\nHowever,ontheonehand, more tests onrealaudio data\nsethavetobedone tovalidate therobustness ofourdeci-\nsion method andtoconsolidate ourchoice offeature set\nproposed. Ontheother hand, conscious ontheclassical\nchick en-and-e ggproblem, wearenevertheless interested\ninamore extensi vestudy offeatures appropriated tothe\nclasses ofnoisy sounds described inthispaper ,forchangedetection.\nAcknowledgments\nThis research wascarried outintheconte xtoftheSCRI-\nME1project which isfunded bytheDMDTS oftheFrench\nCulture Ministry ,theAquitaine Regional Council, theGen-\neralCouncil oftheGironde Department andIDDACofthe\nGironde Department.\nTheBroadcast Corpus weused forexperiments hasbeen\npartially supplied intheframe workofaprevious bilateral\nresearch contract with Philips Research NL.\n6.REFERENCES\n[1]E.Kijak, G.Gravier,L.Oisel, andP.Gros, “Audio-\nvisual integration fortennis broadcast structuring, ”\nProceedings ofCBMI'03, Rennes,France ,2003.\n[2]L.Chaisorn, T.-S.Chua, C.-K. Koh,Y.Zhao, H.Xu,\nand H.Feng, “Atwo-levelmulti-model approach\nforstory segmentation oflargenewsvideo corpus, ”\nTRECVID WORKSHOP 2003 ,2003.\n[3]M.F.McKinne yandJ.Breebaart, “Features froau-\ndioandmusic classiﬁcation, ”Proc.ofISMIR'03 .\n[4]E.Scheirer andM.Slane y,“Construction andevalu-\nation ofarobustmultifeature speech/music discrim-\ninator ,”inProc.ofICASSP'97 ,Munich, German y.\n[5]P.Hanna, “Mod ´elisation statistique desons bruit ´es:\n´etude deladensit ´espectrale, analyse, transformation\nmusicale etsynth `ese,”Ph.D. dissertation, LaBRI,\nUniversit´eBordeaux I,2003.\n[6]X.Serra andJ.Smith, “Spectral modeling synthesis:\nasound analysis/synthesis system based onadeter -\nministic plus stochastic decomposition, ”Computer\nMusicJournal ,vol.14,no.4,pp.12–24, 1990.\n[7]G.Peeters, “Alargesetofaudio features for\nsound description (similarity andclassiﬁcation) in\nthecuidado project, ”CUIDADOProjectReport .\n[8]M.Goodwin, “Residual modeling inmusic analysis-\nsynthesis, ”Proc.ofICASSP'96, Atlanta,GA,USA.\n[9]E.Zwick erandH.Fastl,Psychoacoustics: factsand\nmodels .Springer ,1999.\n[10] W.Yost,“Pitch ofiterated rippled noise, ”Journalof\nAcoustical SocietyofAmerica ,vol.100, no.1,pp.\n3329–3335, 1996.\n[11] A.Zolnay ,R.Schulter ,and H.Ney,“Extraction\nmethods ofvoicing feature forrobustspeech recog-\nnition, ”inProc.ofEuroSpeech'03,vol.1,Gene va,\nSwitzerland, pp.497–500.\n[12] W.Hartmann, Signals,Sound,andSensation .Mod-\nernAcoustics andSignal Processing, 1997.\n1StudiodeCrÂeationetdeRecherche enInformatique etMusique\nÂelectroacoustique"
    },
    {
        "title": "An Analytical Methodology for Acousmatic Music.",
        "author": [
            "David Hirst"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415656",
        "url": "https://doi.org/10.5281/zenodo.1415656",
        "ee": "https://zenodo.org/records/1415656/files/Hirst04.pdf",
        "abstract": "This paper presents a procedure for the analysis of acousmatic music which was derived from the synthesis of top-down (knowledge driven) and bottom-up (data- driven) cognitive psychological views. The procedure is also a synthesis of research on primitive auditory scene analysis, combined with the research on acoustic, semantic, and syntactic factors in the perception of everyday environmental sounds. The procedure can be summarized as consisting of a number of steps: Segregation of sonic objects; Horizontal integration and/or segregation; Vertical integration and/or segregation; Assimilation and meaning.",
        "zenodo_id": 1415656,
        "dblp_key": "conf/ismir/Hirst04",
        "keywords": [
            "acousmatic music",
            "analysis procedure",
            "top-down",
            "bottom-up",
            "cognitive psychological views",
            "primitive auditory scene analysis",
            "acoustic",
            "semantic",
            "syntactic factors",
            "perception of everyday environmental sounds"
        ],
        "content": "AN ANALYTICAL METHODOLOGY FOR ACOUSMATIC \nMUSIC\nDavid Hirst \nTeaching, Learning and Research Support Dept, University of Melbourne \nemail: d.hirst@unimelb.edu.au \nABSTRACT \nThis paper presents a procedure for the analysis of \nacousmatic music which was derived from the synthesis \nof top-down (knowledge driven) and bottom-up (data-\ndriven) cognitive psychological views. The procedure is \nalso a synthesis of research on primitive auditory scene \nanalysis, combined with the research on acoustic, \nsemantic, and syntactic factors in the perception of \neveryday environmental sounds. The procedure can be \nsummarized as consisting of a number of steps: \nSegregation of sonic objects; Horizontal integration \nand/or segregation; Vertical integration and/or \nsegregation; Assimilation and meaning. \n1. INTRODUCTION \nNot only do we recognize sounds, but we ascribe \nmeanings to sounds and meanings to the relationships \nbetween sounds and other cognate phenomena. Music is \na meaningful and an emotional experience. Acousmatic \nmusic uses recordings of everyday sounds, recordings of \ninstruments and synthesized sounds. They are combined \nin their raw forms or processed then mixed together and \nrecorded to a fixed medium (usually CD or tape). \nAcousmatic music may make use of traditional musical \nrelationships, but there can also be unique abstract \nrelationships between the sonic attributes of sounds and \nthe perceiver of those sonic attributes that we don’t find \nin traditional instrumental music. What is this “syntax” \nof acousmatic music and how does it interact with the \nsemantic references afforded by some of the sonic \nmaterial within acousmatic musical works? \nWe have previously reported on a bottom-up type of \napproach to the analysis of acousmatic music that \nadapted Bigand’s model of Event Structure Processing \nof tonal music (Hirst, 2003) [5], and discussed the \ninterpretation of some of Smalley’s concepts (Hirst, \n2002) [4]. In this paper we will examine some of the \ntop-down processes that may operate in the perception \nof Western tonal music and also consider top-down \nprocesses in the recognition of environmental sounds. \nThrough combining these two bodies of previous work, \nwhere there has been some research activity, we may \nshed some light on the perception of acousmatic music, \nwhere there has been only a meagre amount of research. \nThe combination has resulted in the definition of a \nmethodology for the analysis of acousmatic works.  \n2. DEFINING THE ANALYTICAL \nMETHODOLOGY \nThe following procedure for the analysis of acousmatic \nmusic was derived from the synthesis of top-down \n(knowledge driven) and bottom-up (data-driven) views \nsince it has been found impossible to divorce one \napproach from the other. \nThe procedure can be summarized as consisting of a \nnumber of steps: \nSegregation of sonic objects   \n1. Identify the sonic objects (or events). \n2. Establish the factors responsible for \nidentification (acoustic, semantic, syntactic, \nand ecological) and their relative weightings (if \npossible). \nHorizontal integration and/or segregation \n3. Identify streams (sequences or chains) which \nconsist of sonic objects linked together and \nfunction as a unit that we could call a “pattern”. \nThe role of “trajectories” should also be \nconsidered. (The term “gesture” has sometimes \nbeen used in this context.) \n4. Determine the causal linkages between the \nsonic objects within the chain-type pattern. \n5. Determine the relationships between “pattern \nobjects” – if this level of syntax exists. This \namounts to the investigation of higher-order \nrelationships within a “hierarchy”. \n6. Consider local organization in time – pulse, \nbeat, accent, rhythm, meter. \n7. Consider the horizontal integration of pitch, \nincluding emergent properties relating to timbre \n(vertical overlap). \nVertical integration and/or segregation \n8. Consider vertical integration as a cause of \ntimbre creation and variance: \na. Timbre as a cause of integration \nand/or segregation. \nb. The dimensional approach to timbre, \nincluding emergent properties relating \nto pitch (horizontal overlap). \nc. Texture resulting from contrasting \ntimbres. \n9. Also consider vertical integration or \nsegregation in terms of the potential for \npsychoacoustic dissonance and musical \ndissonance (and consonance). \nAssimilation and meaning  \n10. Consider the nature and type of discourse on \nthe source-cause dominant to typological-Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that \ncopies are not made or distributed for profit or commercial \nadvantage and that copies bear this notice and the full citation on \nthe first page. \n© 2004 Universitat Pompeu Fabra. \n   \n \n relational dominant continuum, and the way it \nvaries over time. \n11. Consider implication-realization, and arousal \nand meaning on a moment-to-moment basis \nthroughout the work. \n12. Consider global organization in time – identify \nformal structures, like sectional or continuous \norganization, and the nature of the relationships \nbetween sections, i.e. hierarchical relationships. \n3. APPLYING THE ANALYTICAL \nMETHODOLOGY \nHaving outlined a methodology, and considered some \npreliminary issues, let us examine each of the steps in \nthe above procedure in more detail. \n3.1. Segregation of sonic objects \nThe first stages in our methodology are the identification \nthe sonic objects (or events), and establishing the factors \nresponsible for identification.  \nBuilding on the work of Bregman, McAdams has \npostulated several stages in the recognition of sound \nsources and events (McAdams, 1993) [8]. The first two \nstages are sensory transduction and auditory grouping of \nfrequencies – both simultaneous and sequential. The \nthird stage involves the analysis of auditory properties \nand features. \nThere may be many factors responsible for \ncategorical matching so the next step in our \nmethodology seeks to explore the varied factors and the \npossible contribution made by each factor in sound \nevent recognition. \n3.1.1.  Factors in the identification of environmental \nsounds \nAcoustic factors \nGygi (2001) presents results of similarity studies \nconducted at the Hearing and Communication \nLaboratory at Indiana University. He points out that for \n“meaningful” sounds, there needs to be a consideration \nof their “psychological space”. [3] \nOne experiment attempted to uncover the structure of \nsuch a psychological space by using multidimensional \nscaling (MDS) procedures. \nGygi conducted the experiment using 50 \nenvironmental sounds. Recordings of 100 instances of \nthe environmental sounds were played to listeners who \nwere asked to rate the similarity of each pair on a seven \npoint scale from least similar to most similar. \nA two-dimensional MDS solution was derived. Gygi \nsearched for a meaningful interpretation of the two \ndimensions. He measured over twenty acoustic variables \ncovering different aspects of spectral distribution. The \nhighest correlations for Dimension 1 were spectral \nspread, and confidence in the pitch of the signal. \nCorrelations with the second dimension were not so clear cut. The most significant correlations were to do \nwith measures of rhythmicity and spectral mean. \nGygi’s identification studies gives us a basis for \nisolating acoustic features of environmental sounds that \nwe may want to single out for analysis. \n \nSyntactic and semantic factors   \nIn an article by Howard and Ballas (1980), they \ndocumented their experiments which tested syntactic \n(temporal structure) and semantic factors (knowledge of \nsource events) in the classification of non-speech \ntransient patterns [6]. What is important about this series \nof experiments is that they are concerned with patterns \nof sounds (i.e. sequences of sounds) – both pure tones \nand environmental sounds. \nHoward and Ballas found that a grammatical group of \nlisteners performed significantly better than did a non-\ngrammatical group for pure tones and for semantically \nsensible sequences. Listeners in the grammatical group \nhad actually learned something about the syntactic rules \n(the grammar) used to generate the target patterns. \n3.2. Horizontal integration and/or segregation \nIn this next phase of our analytical procedure we must \nidentify sequences (streams or chains). Such a sequence \nwould consist of sonic objects linked together and it \nwould function as a single unit that we could call a \n“pattern”. The role of “trajectories” should also be \nconsidered. (The term “gesture” has sometimes been \nused in this context.) \nNext we need to determine the linkages between the \nsonic objects within each chain-type pattern. Which of \nthe relevant factors are operating, and what are the \nrelative weightings of those factors? \nThen we shall determine the relationships between \n“pattern objects” – if this level of syntax exists. This \namounts to the investigation of higher-order \nrelationships within a “hierarchy”. \nFinally we must consider local organization in time – \npulse, beat, accent, rhythm, and meter. \nThis phase is a lot like a melodic analysis in tonal \nmusic. Transformations in pitch or frequency define a \nmelodic form (or gesture). However, we shall treat pitch \nas a quality that is emergent from the frequency \nspectrum of sonic objects within acousmatic music. \nA sequence with large frequency transitions in a short \nspace of time will not remain perceptually coherent. \nAlternations of high and low frequencies will cause \nsegregation between the two different registers and the \nperception of separate streams will result. \nChanges in timbre can effect the integration of a \nhorizontal sequence, for example: Repeated and/or rapid \nchanges in timbre can fragment a sequence; Less rapid \nshifts in timbre can be used to delineate larger horizontal \nunits or “phrases”. \nA distinction can be made between form-defining \nsequential events and subordinate ornamental sound \nevents. Ornamental events group with the form-bearing \nevent they are subordinate to. Gestalt-like principles of   \n \n primitive scene analysis seem to constrain these \nemergent events. The dependent event must be very \nclose to the anchor event in frequency and time, \notherwise the two will not group to form a larger event. \nThe choice of which event is the anchor and which is \nthe dependent one depends on factors such as duration, \nintensity, and rhythm. Dependent events tend to \n“resolve” to stable anchor events. \n3.2.1.  Trajectories \nIs a regular sequence of sounds (where the sequence is \npredictable) easier to recognize than an irregular one? \nAs we listen repeatedly to an auditory pattern, we \nlearn the regularities in it. We can then anticipate \nsegments of the pattern before they occur and integrate \nthe sequence into a coherent mental representation. This \npreparation of our attention through repeated listening \nshould assist the formation of sequential streams. \nThere is evidence both for and against abilities in the \nformation of streams via trajectories. This concept of \ntrajectory is important, but its application is far from \nstraight forward. (See Bregman, 1999: 670) [1] \n3.3. Vertical integration and/or segregation \nIn this next stage we consider vertical integration as a \ncause of timbre creation and variance. We also consider \nvertical integration or segregation in terms of the \npotential for psychoacoustic dissonance and musical \ndissonance. \n3.3.1.  Timbre and texture \nBregman notes that timbre plays a role in the sequential \norganization of music. Timbre is a complex \nphenomenon and no one perspective can encapsulate its \ncomplexity. Bregman tackles timbre from several points \nof view. \n \nTimbre as a cause of segregation and integration \nAs we saw above, large and rapid changes in timbre can \ncause the formation of separate, parallel streams as \nhappens with a “compound melody”. \nConversely, timbre can be used as sequential glue for \nmusical phrases, and to delineate separate units or \nsections. Such delineation exploits scene analysis \nprinciples from nature where a sudden change in timbre \nusually implies a new event has begun. In contrast, a \ncontinuous change implies that a single event is \nchanging in some way – through some form of \nincremental transformation. \n \nThe dimensional approach to timbre \nBregman points out that there is a use of timbre that is \nstronger than the accentuation and reinforcement of \nmelodic forms. \nBregman highlights several approaches to the study \nof the dimensionality of timbre. Lerdahl (1987) has \nattempted some tests of the organization of music according to certain types of dimensionality [7]. Other \nstudies have explored acoustic dimensions such as the \nformant frequencies of vowel sounds. (See Slawson, \n1985) [9].  \n \nTimbre as the result of fusion, texture as a result of \ncontrasting timbres \nKnowledge of fusion and segregation principles can \nbe used to assist the organization of musical texture, for \nexample a number of musical sounds can be fused to \ncreate a global timbre, or a polyphonic texture can be \ncreated where two or more distinct sequential streams \ncan be heard. \nTonal music uses scene analysis principles in a \nnumber of ways that are relevant to musical texture. \nPerhaps these principles could be adapted for the \nanalysis of texture in acousmatic music. \nAn examination of the role of primitive scene analysis \nin counterpoint may be of some benefit too. In \npolyphonic music, the parts must not be totally \nsegregated or totally integrated. Segregation between \nparts is improved by strong sequential organization \nwithin each part. Strong sequential organization is \nachieved through principles such as: using small pitch \nchanges to favour sequential integration; avoiding \n“common fate” by prohibiting synchronous onsets and \noffset (different rhythms); avoiding parallel changes in \npitch between two parts, i.e. encourage contrary motion \nor oblique motion; avoiding harmonic relations between \nsimultaneous events in different parts, e.g. whole \nnumber ratios like 2:1, 3:2, etc. \n \nDissonance \nBregman distinguishes two types of dissonance: \npsychoacoustic dissonance and musical dissonance. \nPsychoacoustic dissonance is the sense of roughness \ncaused when partials combine to produce a large number \nof beats at an unrelated rate [2]. Simple ratios like 3:2 \n(ca. seven semitones) will sound smooth, complex ratios \nlike 45:32 (ca. six semitones) will seem rough. \nMusical consonance is defined by Bregman as a \ncognitive experience. Stable and unstable combinations \nof sounds are defined by a musical style. Unstable \ncombinations are points of tension, whereas stable ones \nare points of rest. Bregman notes that unstable \ncombinations of tones often happen to be \npsychoacoustically dissonant in Western music. \nComposers control such dissonant tones by using \ntechniques such as: avoiding simultaneous start and stop \ntimes; capturing tones into separate streams by \npreceding each with tones close in pitch; capturing each \ninto smooth and different trajectories; capturing each \ninto its own repetitive sequence. An increase in \ndissonance can be created by violating the above \nprinciples. \n3.4. Assimilation and meaning  \nIn this final stage we consider global organization and \nformal structures. This is also the stage in which we   \n \n discuss emotion, arousal and meaning, implication-\nrealization, and the nature and type of discourses \noperating within the work. Space does not allow us to \nconsider these concepts within this paper, but the model \ndisplayed in Figure 1 draws upon the work of Dowling \nand Harwood (1986) [2]. \n4. CONCLUSION \nThe above approach is a reductionist one – tearing the \nmusical materials apart. We must now put the work back \ntogether again in order to get a more holistic view. \n \n \n \n \nFig. 1: Network of relations in the interpretation of \nacousmatic music. \n \nFigure 1 is a pictorial representation of the concepts \nintroduced in this paper and their relationships. Of \nfundamental importance is the differentiation between \n“Environmental Space” associated with the experience \nof “real-world” sounds and “Musical Space” associated \nwith the “fictional sound world” created by the musical \nwork. Note that there is scope for overlap between these \nworlds so that the composer can play with ambiguities \nand chimerical properties resulting from their \ncombination. \nIn assessing the dominant forces at play within a \nwork, a continuum has been constructed according to the \ntype of discourse that may be dominant at any given \ntime. Source-cause discourse (after Smalley, 1994) is \ndominant when sounds from the natural world are \nidentified and related somehow within a semantic \nnetwork. [10] These sounds will have strong source-\nbonding and the syntax of their relationships will be \ndominated, although not exclusively, by semantic and \necological factors. \nAs source-bonding decreases, the acoustic factors \nbegin to dominate and the work adopts a typological-\nrelational discourse based upon more abstract \nrelationships within a syntactic network governed by \nimplications and realizations. The continuum extends \nfrom the purely concrete reproduction of the environment (real space) to the totally abstract world of \na fictional sound space with fictional sources (musical \nspace). \nReferences extend from the indexical to the symbolic, \nfrom simple association at one extreme to directly \neliciting emotions at the other extreme, with a \nrepresentation of emotional ebb and flow in between. \nThe listener’s experience along these various continua \ncan vary from moment to moment within a given work, \nand this becomes one more dynamic aspect within the \ndiscourse. \nWhat remains to be completed is to test this model of \nanalysis on representative repertoire works. \n5. REFERENCES \n \n[1] Bregman, A. S. (1999). Auditory Scene \nAnalysis: The Perceptual Organization of \nSound. (Second MIT Press Paperback edition) \nCambridge, MA: MIT Press. \n[2]  Dowling, W.J. and Harwood, D.L. (1986). \nMusic Cognition. Orlando: Academic Press. \n[3] Gygi, B. (2001). Factors in the Identification \nof Environmental Sounds. PhD Dissertation: \nIndiana University \n[4] Hirst, D. (2002). Developing Analysis Criteria \nBased on Denis Smalley's Timbre Theories. \nProceedings of the Australasian Computer \nMusic Conference 2002, pp 43-52. Australasian \nComputer Music Assoc. Melbourne. \n[5] Hirst, D. (2003). Developing a cognitive \nframework for the interpretation of acousmatic \nmusic. Converging Technologies: Proceedings \nof the Australasian Computer Music \nConference 2003, pp 43-57. Australasian \nComputer Music Assoc. Melbourne. \n[6] Howard, J. H. & Ballas, J. A (1980). Syntactic \nand semantic factors in the classification  of \nnonspeech transient patterns. Perception & \nPsychophysics 28(5), 431-439. \n[7] Lerdahl, F. (1987). Timbral hierarchies. \nContemporary Music Review 2: 135-60. \n[8] McAdams, S. (1993). Recognition of sound \nsources and events. Thinking in Sound: The \nCognitive Psychology of Human Audition. (ed. \nS. McAdams and E. Bigand). Oxford \nUniversity Press, pp. 146-98. \n[9] Slawson, W. (1985).  Sound Color. University \nof California Press, Berkeley, CA. \n[10] Smalley, D. (1994). Defining Timbre - \nRefining Timbre. Contemporary Music Review \nVol. 10 Part 2, Harwood Academic Publishers, \nSwitzerland, pp. 35-48."
    },
    {
        "title": "MusicAustralia: towards a national music information infrastructure.",
        "author": [
            "Robyn Holmes",
            "Marie-Louise Ayres"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418097",
        "url": "https://doi.org/10.5281/zenodo.1418097",
        "ee": "https://zenodo.org/records/1418097/files/HolmesA04.pdf",
        "abstract": "MusicAustralia is a national music discovery service, developed by the National Library of Australia and ScreenSound Australia, National Film and Sound Archive. The service aims to provide seamless access to music and music information resources, in multiple formats, from custodians across all cultural sectors. This paper describes the development of the service, including its architecture, and content base. Service development to date has concentrated on metadata contribution and discovery strategies, together with development of the national digital music collection. In the future, digital content developed to populate the service could be subjected to Music Information Retrieval applications, to further enrich understanding of Australian music. The paper finishes by examining the challenges of achieving these advanced services in an environment where MIR research is relatively undeveloped.",
        "zenodo_id": 1418097,
        "dblp_key": "conf/ismir/HolmesA04",
        "keywords": [
            "MusicAustralia",
            "National Library of Australia",
            "ScreenSound Australia",
            "National Film and Sound Archive",
            "seamless access",
            "music and music information resources",
            "multiple formats",
            "custodians across all cultural sectors",
            "metadata contribution",
            "discovery strategies"
        ],
        "content": "MUSICAUSTRALIA: TOWARDS A NATIONAL MUSIC  \nINFORMATION INFRASTRUCTURE\nRobyn Holmes Marie-Louise Ayres\nMusic and Dance Section\nNational Library of Australia\nParkes Place\nPARKES  ACT 2600\nAustralia\nABSTRACT\nMusicAustralia is a national music discovery service, \ndeveloped by the National Library of Australia and \nScreenSound Australia, National Film and Sound \nArchive. The service aims to provide seamless access to \nmusic and music information resources, in multiple \nformats, from custodians across all cultural sectors. \nThis paper describes the development of the service, \nincluding its architecture, and content base. Service \ndevelopment to date has concentrated on metadata \ncontribution and discovery strategies, together with \ndevelopment of the national digital music collection. In \nthe future, digital content developed to populate the \nservice could be subjected to Music Information \nRetrieval applications, to further enrich understanding \nof Australian music. The paper finishes by examining \nthe challenges of achieving these advanced services in \nan environment where MIR research is relatively \nundeveloped. \n1.INTRODUCTION\nThis paper sets out the challenges, issues and \nmethodologies involved in developing a cooperative \nnational music information infrastructure. \nMusicAustralia has been developed through a \npartnership between the National Library of Australia1\nand ScreenSound Australia, the National Film and \nSound Archive2, in cooperation with a range of\ninstitutions and specialist music projects across sectors3. \nWe want any user, anywhere, to find and access \nAustralian music – across all genres and styles, \nhistorical and contemporary, and whether online or not \n– and to find and access information about Australian \nmusic and music-making in multiple forms, including \nonline entries about people and organisations, or books, \nwebsites, manuscripts, ephemera or pictures. This free \nservice operates on a federated model, with a centralised \nmetadata repository for resource discovery coupled with \ndistributed management and delivery of resources and \n                                                       \n1<http://www.nla.gov.au>\n2 <http://www.screensound.gov.au>\n3  Contributors include the Australian Music Centre (a representative \nbody for Australian contemporary classical composers and performers), \nall Australian State Libraries, Australian university libraries, museums \nand archives, and several online services, including Australian Music \nOnline and Australian Sound Design.information across multiple organisations, small and \nlarge4.\nWhat makes MusicAustralia conspicuous is its \napproach to national cooperation: building a \ncollaborative mechanism that enables institutions and \nthe musical community to create, share, harvest, and \naggregate the nation’s musical data, resources and \ninformation in one virtual, seamless space – and as \ncheaply as possible! The success of such a national \nservice is dependent not only on a sustainable central \ndiscovery service, but fundamentally upon the \nwillingness and capacity of contributors to participate in \nsuch a federation. In addition to building the central \ninfrastructure, the National Library has cultivated desire \nand commitment from a range of organisations across \nsectoral boundaries. And we have built capacity among \ncustodians or creators of music resources and \ninformation – capacity to share metadata about \nresources and people; capacity to build and deliver \ndigital collections to ensure technical and business \nsustainability; and, we hope, capacity to think ahead to \nthe ways in which these collections might be exploited \nfor new business – including ‘musical’ access by Music \nInformation Retrieval (MIR) applications – in the \nfuture. \n2. BACKGROUND\nBuilding a demonstrator5– released at an international \nmusic librarianship conference in August 2002 [4] –\nwas a key and effective strategy in generating \nexcitement and commitment from our stakeholders. \nWhen the project began in late 2001, there were no \ndigital printed music collections in Australia, and \nalthough a small number of national institutions had \nbeen digitally preserving their sound collections for \nsome time, a business case to deliver these collections \nonline had not been made. \nAt the National Library of Australia, the new \nbusiness imperative to digitise and deliver the Library’s \nprinted music and extensive archival sound collections \nto support MusicAustralia had a major effect on \ndevelopment of the Library’s Digital Collections \n                                                       \n4The National Library undertook extensive research into existing and \ndeveloping international music projects. \n<http://www.nla.gov.au/wgroups/projectma/Related_work/register.htm>\n5  <http://www.musicaustralia.org> The demonstrator will be replaced \nby the production service in November 2004.Manager, an in-house application to support collection, \nstorage, management and delivery of the Library’s \ndigital collections across all formats – pictures, printed \nmusic, manuscripts, maps, sound, books. Music specific \nneeds provided solutions for other collection types \nwhere items consist of multiple parts and online \nnavigation must be logical and as easy as possible for \nusers. \nThe Library’s experience has been replicated in \nother institutions; having developed digital systems for \nprinted music, contributors have moved on to digitise \nand deliver archival holdings, newspapers and other \ncomplex formats. Managing and delivering complex \nsound collections – which may require simultaneous \ndelivery of sound and transcript, or navigation to tiny \nmusical fragments in an oral history interview – has \nproved even more challenging. However, the National \nLibrary and ScreenSound Australia are poised to make \nlarge portions of their sound collections available online \nfrom early 2005, thanks to further application \ndevelopment, and effective approaches to the rights and \npermissions issues arising from such collections. \nThe MusicAustralia demonstrator adopted a \nmodel used successfully to deliver the PictureAustralia \nservice1, which provides access to more than 1 million \nonline images delivered by nearly forty Australian \nlibraries, archives and museums. The PictureAustralia \nmodel is extremely simple. Contributors map their local \ndescriptive metadata to unqualified Dublin Core and \nmake it available for harvesting by the PictureAustralia \nrepository. Users discover images through \nPictureAustralia, but immediately navigate to the \ncontributing organisation’s digital delivery system to \nview the image. The PictureAustralia model was \npioneering when it was adopted in 2001, and has been \nwidely replicated by other services around the world. It\nis still an extremely effective and efficient way to \nprovide users with access to many collections, and is \nrelatively cheap both to run, and to contribute to.\nWe needed to test whether this model was \nsuitable for a national music service. We particularly \nneeded to test whether a simple metadata format such as \nDublin Core was adequate for a service incorporating \nalmost any imaginable material format: scores, born-\ndigital scores, sound, multi-media objects, manuscripts, \nwebsites, pictures, text. We also needed to test whether \nthis model would give users the kinds of ‘seamless’ \nnavigation experiences we envisaged, e.g. viewing a \nscore delivered by one institution while listening to a \nseries of potentially related sound recordings delivered \nby one or more other contributors. At the back of our \nminds were the implications of a distributed national \ndigital music collection for future MIR applications. \nOur initial assumptions that the pilot would lead \ndirectly into a full production service of the same kind \nwere overturned as we evaluated the lessons we had \n                                                       \n1  <http://www.pictureaustralia.org>learned. We found that available descriptive metadata \nwas very rich and complex – albeit in a number of \ndifferent formats – and that reducing these complexities \nto simple Dublin Core metadata would do music and \ncontributors a disservice. We found that – in contrast to \nthe situation of pictures in Australia, and of music in \nmany jurisdictions – the majority of the Australian \nnotated music corpus was already well described in \nAustralia’s national union catalogue, the National \nBibliographic Database (NBD). Secondary sources were \nalso well represented. The majority of the nation’s \npublished sound collection, however, was not described \nin the NBD, as its primary custodian, ScreenSound \nAustralia, is an archive employing its own descriptive \nmetadata structures in their custom-built stand-alone \nMAVIS2 database.\nRights management was clearly a key inhibitor to \nthe delivery of contemporary or older but in-copyright \nmusic. We therefore decided that working with \n‘aggregators’ representing particular musical \nconstituencies, and brokering rights on a licensing basis, \nwould necessarily form a major part of the \nMusicAustralia strategy.\nWe also realised that we needed to put more \neffort than expected into assisting other contributors to \ndigitise, store and deliver their music objects, or to \ntransform preservation processes into full delivery \nsystems. ‘Going digital’ required major rethinking and \nredevelopment for all concerned, and few contributors –\neven state libraries – had the kinds of IT resources \navailable to a national library. \nThis realisation had a direct effect on our first \nphase ambitions. In particular, our hopes of enriching \navailable metadata to better expose relationships \nbetween items and their parts, or between versions of \nworks have not yet been realised. Similarly, although \nthe centralised discovery and distributed content \ndelivery model had great advantages as a ‘discovery’ \nstrategy, we acknowledged that the diversity of storage, \nmanagement and delivery systems was likely to be a \nmajor inhibitor to cross-service MIR applications.  \nThe demonstrator did not include any work on \nthe concept of providing information on people and \norganisations associated with Australian music, or our \ndesires to display music resources in a context \nrecognising creators and performers. \nFollowing the pilot, MusicAustralia’s joint \nproject board – including senior staff from the National \nLibrary of Australia and ScreenSound Australia – made \na number of decisions about the production service:\n                                                       \n2  MAVIS was jointly developed by ScreenSound Australia and Wizard \nInformation Systems, Australia, and is deployed in a number of \ninternational audio-visual archives, including the Norwegian National \nLibrary, the BundesArchiv, and the Library of Congress. \n<http://www.wizardis.com.au/ie4/products/mavis/introducingmavis.html\n>\u0001MusicAustralia would collect descriptive \nmetadata only into an XML repository;1\n\u0001for compelling business and cultural \nreasons, the national digital music \ncollection would be a distributed \ncollection;\n\u0001all resource metadata would be routed \nthrough the NBD, requiring new business \nsolutions;\n\u0001a richer descriptive metadata format – the \nLibrary of Congress’ Metadata Object \nDescription Schema   (MODS )[6] would \nbe adopted as the service’s preferred \nexchange format, to retain the richness of \noriginal records;\n\u0001MusicAustralia would provide access to all \nAustralian music, and to all information \nabout Australian music, in all formats, and \nwhether online or not; \n\u0001MusicAustralia would ‘piggyback’ on new \nNBD services being developed, including \nservices allowing users to ‘get’ physical \nformat materials discovered through the \nservice; and\n\u0001the National Library would develop a \nschema to encode information about people \nand organisations, Metadata for Australian \nParties Schema  (MAPS) [9] after \nevaluation of other available schemas. The \nlocally developed schema will support \nconversion  to and from MARC21 Concise \nFormat for Authority Data [7], Encoded \nArchival Description  (EAC)[3]and the \nLibrary of Congress’ Metadata for \nAuthorities Description Schema\n(MADS)[8] and other relevant schemas.\nWe also decided to work from ‘easiest to \nhardest’, that is, to develop the resource database first \n(core and well understood library business), followed by \nthe people database.  Value added services such as MIR \nand ‘interactive’ services such as collection of user \nannotations or individual contributions of data or objects \nare ‘new’ business, and will only be implemented as \nexpertise and resources permit. \n3.ARCHITECTURE\nWhile the business solution to a scattered national music \ncollection – a single discovery service – was driven by \nthe inherent complexity of music as a symbolic and \nperformed art form documented in multiple formats, the \n                                                       \n1  The Library deploys the Teratext Database System from Inquirion Pty \nLtd. <http://www.teratext.com/index.html>architecture we adopted for the service is not music \nspecific. Indeed, we moved away from seeing music as \nhaving special needs needing separate information \ninfrastructure, and towards thinking about how existing \nand developing national infrastructure could be used to \nensure music service sustainability. \nOur core decision was to build on the strengths of \nthe Australian National Bibliographic Database, which \nhas flourished for more than twenty years. More than \none thousand libraries contribute to the service, which \nincludes descriptions of more than 14.2 million items, \nwith 38 million holdings. More than half a million of \nthese items are available online. More than 120 000 \nitems are Australian music2. \nThis level of music representation was not just \nthe result of serendipity. Even before we decided to \nplace the NBD at the heart of our service, the National \nLibrary had worked hard to overcome problems with the \nway in which local music catalogue records appeared in \nthe union catalogue, and to encourage key contributors \nto increase their cataloguing efforts. In fact, throughout \n2002 and 2003, music was the fastest growing area \ncovered by the NBD – evidence of MusicAustralia’s \nsuccess in raising awareness of the need to enhance \naccess to music information and to translate this into \nconcrete actions.\nNevertheless, the NBD was essentially a service \nto which libraries contributed, and which libraries used \non behalf of their clients. We therefore needed to \nrethink what the NBD could be, to find new ways in \nwhich other organisations – whether traditional \ncollecting organisations or specialist online music \nservices – could contribute to the NBD and to find new \nways to exploit its rich data for end-users. \nThe time was ripe for such a rethink. The Library \nwas formulating its future strategies for the NBD \nservice, and in fact commenced a major redevelopment \nin the second half of 2003. New technologies and \nprotocols such as the Open Archives Initiative Protocol \nfor Harvesting Metadata  [10] and eXtensible Stylesheet \nLanguage Transformation (XSLT) offered practical and \naffordable solutions to harvesting and converting data \nfrom many contributors and in many formats. Maturing \nXML data repository platforms also offered us \nopportunities to re-use existing data and to tailor \npresentation of the data to particular audiences. \nMusicAustralia is also pioneering a new kind of \nfederated service for Australia – a federated service for \ninformation about people and organisations. Libraries \nhave long used ‘authority’ data – structured information \nto assist cataloguers to select appropriate name and \nother headings in bibliographic records – to enhance \nretrieval in online systems. However, sharing \nbiographical and organisational history data across \n                                                       \n2  As at 20 August 2004. The documented Australian music corpus is, of \ncourse, relatively small, with 40 000 years of oral tradition pre-dating a \nmere 230 years of European settlement.   sectors has not previously occurred in Australia. Around \nthe world, a number of projects to investigate such \nsharing are currently in place1. Many of these are \nfunded to levels unimaginable in the Australian context, \nso we needed to find a cheap and affordable way to co-\nlocate this disparate data, and to exploit it to provide \npeople-centred context to the resource discovery aspect \nof MusicAustralia. \nOur technical solution, therefore, consists of a \nnumber of modular – and therefore independently \nreplaceable – components:\n\u0001a data harvester, capable of harvesting data \nin any XML format (DC, MODS, \nMARCXML, local schemas) by FTP, OAI \nor HTTP;\n\u0001a contributor profile specifying expected \ncontribution behaviours, formats, \nconversion requirements and data \ndestinations, e.g. the NBD, the \nMusicAustralia people database;\n\u0001a data conversion tool, capable of invoking \nspecified XSLT conversion stylesheets \naccording to contributor profile, e.g. \nconverting ScreenSound Australia’s \nMODS versions of their original MAVIS \nrecords to MARCXML;\n\u0001the existing National Bibliographic \nDatabase utilities, to be replaced with new \nutilities over the next two years;\n\u0001a data extraction profile, capable of \nautomatically extracting records from the \nNBD according to specified criteria, e.g. \nall music materials, and all materials about \nmusic; \n\u0001a maintenance suite, in an early stage of \ndevelopment; and\n\u0001XML data repositories.\n                                                       \n1For example, the European Union’s Linking and Exploring Authority \nFiles (LEAF) project. <http://www.crxnet.com/leaf/>MODS Resource\nDC Resource\nAny SchemaHarvester\nProfile\nHarvest\nAnalyse\nConvert \nPass downstream\nNBD MARC Resource\nMusicAustralia\nPeopleResources PeopleOAI\nMAPS Person\nMusicAustralia\nResources\nMusicAustralia\nInterfaceFTP\nHTTP\nFigure 1.  MusicAustralia Data Workflow\nRecords from the Australian Music Centre2 and \nScreenSound Australia formed the test-bed for this new \ninfrastructure. Indeed, the achievement of the \nAustralian Music Centre – with a total of a mere twelve \nstaff – in redeveloping their business processes so that \nthey could present their rich catalogue records in the \nMODS schema and via an OAI repository for \nharvesting, conversion to MARC and inclusion in the \nNBD was recognised in the form of the Kinetica (NBD \nservice) Innovation Award for 2004. \nThese data-sharing workflows can, of course, be \nused to expose Australian collections to an even wider \naudience. The National Library’s entire digital music \ncollection, for example, is now accessible through the \nUS-based Sheet Music Consortium, and indeed \nmetadata for the Library’s entire digital collection is \nfreely available for harvesting via OAI servers.3\nThe important thing to note here is that – with \nthe exception of the MusicAustralia repositories – these \nnew pieces of infrastructure are not specific to music, or \nindeed to any particular downstream repositories or data \nstructures. The Library can re-use them for any number \nof purposes, and is indeed doing so. Examples include \nharvesting of Dublin Core metadata describing online \ngovernment publications, and harvesting of records \ndescribing archival collections held by large and small \ninstitutions. \n4.PRODUCTION SERVICE\nThe MusicAustralia production service will be released \nin late November 2004. A single interface will allow \nusers to retrieve records from the resource and  people \nrepositories, using simple or advanced searches, or a \nbrowse interface. Users can sort and refine results, limit \ntheir searches to online items only, and save and email \nresults within a single user session.\n                                                       \n2<http://www.amcoz.com.au/>\n3The Sheet Music Consortium is a cooperative service led by UCLA, \n<http://digital.library.ucla.edu/sheetmusic>The resource database includes descriptive \nmetadata for more than 120 000 music and music \ninformation items across all formats. Of these, \napproximately 12 000 items are available online, \nincluding more than 9 000 digitised scores, 2 000 sound \nrecordings and more than 500 Australian music \nwebsites1. \nThe numbers of items findable through the \nservice will increase enormously in 2005, when \nScreenSound Australia will export its MAVIS records \nfor harvesting and conversion to MARC for use in the \nNBD and MusicAustralia and will grow still further as \nother major non-library collecting organisations \ncontribute their data to the service. The proportion of \ncontent available online will certainly increase, \nespecially as institutions find appropriate business and \nrights management models to deliver their enormous \ndigitally preserved sound collections, e.g. the National \nLibrary’s own massive folklore archives. \nWhere content is available online, users will \nnavigate to the contributing organisation’s delivery \nsystem to view, listen, read or perhaps request a higher \nquality copy of the item. Where content is not available \nonline, an integrated ‘Find and Get’ service will enable \nusers to request loans, or pay for document delivery in a \nsingle workflow, rather than having to contact \nindividual contributing organisations. \nThe people database includes descriptive \nmetadata for more than 13 000 people and organisations \nassociated with MusicAustralia resources. More than \nhalf of these records include biographical or \norganisational history information, in addition to \nnames, alternative names and dates. Many people \nrecords will be sourced from contemporary music \nservices, such as Australian Music Online2, an audience \ndevelopment initiative of the Australia Council for the \nArts, and illustrated with images from contributors’ own \ncollections. \nContent highlights will be showcased in ‘themes’ \n– curated collections of digital objects around a \nparticular musical theme, genre, period or person. \nThemes are designed both to give ‘surfing’ users an \nentry point if they do not otherwise know where to start,\nand to highlight content which might otherwise be \n‘overwhelmed’ in a service in which most content is not \navailable online, and in which online scores will \nsignificantly outnumber online sound in the short term.\n5.SCHEDULED ENHANCEMENTS\nIn the six months following release, new user services \nwill be added to MusicAustralia. The Library’s premier \nfederated service – the NBD – is currently being \nredeveloped, and many of its new user services can be \n                                                       \n1Longevity of music websites, many of which include musical content, is \nbeing achieved through archiving these sites in the National Library’s \nPANDORA web archive. <http://pandora.nla.gov.au/index.html>\n2<http://www.amo.org.au>easily migrated to MusicAustralia, as the services will \nemploy the same repository solutions and data \nstructures. This includes extending document delivery \noptions to e-commerce suppliers, such as music \npublishers and retailers. It will also include portal or \n‘My MusicAustralia’ functionality, in which users can \nbecome registered users, able to retain saved searches \nand results, and able to specify alert services letting \nthem know that new music content of interest has been \nadded to the service. \nOnce these services are put into production, the \nbasic MusicAustralia service will essentially be \ncomplete. The databases will be continually refreshed \nwith new content, or new access to digital content, or \nnew themes. The great advantage of the models we have \nselected is that the majority of MusicAustralia’s growth \nwill require little additional service specific effort. This \nis essential for long-term sustainability of such a service \nfor music that is, after all, a very small and specialised \npart of larger institutions’ business.\nHowever there will still be a heavy emphasis on \nidentifying potential content contributors – especially \nthose outside the Library sector – and working with \nthem to ensure that their content can be accessed \nthrough the service via metadata mapping, conversion \nand contribution. Likewise, we anticipate a continuing \nrole in advising contributors developing digital \nrepositories and delivery systems – especially in the \nuniversity sector. We will continue to ‘intervene’ in \nearly stages of new online music projects and services, \nfor example JazzAustralia and the National Indigenous \nRecording Project, aiming to persuade developers and \nfunders of the benefits of using standards based \nsolutions, and structuring their data to support its \ninclusion in MusicAustralia and other federated \nservices. \nAn important service for contributors will also be \ntrialled in the first year of operation. Many contributors \nwish to provide access to in-copyright score and sound \ncontent, much of which is not available in the \nmarketplace, and has little residual economic value. The \nNational Library and ScreenSound are working towards \nbrokering a content licence on behalf of contributors. \nThis is new business for both institutions, and the pilot \nis an opportunity to establish whether this is a viable \nand sustainable model for enhancing access to the \nrichness of our national music collections, especially \nnew music. \n6.DESIRED ENHANCEMENTS\nOther user services, however, are likely to remain on \nour ‘nice to have’ list unless recurrent funding is \nsupplemented by external funds such as grants and \nsponsorship. While we would love to develop these \nservices – and feel passionately about their value to the \nmusic and wider communities – our first imperative is \nto provide access to resources, and our first responsibility is to ensure that our service is sustainable \nin the long term. \nMusicAustralia development has been fully \nfunded from its developers’ recurrent budgets. All \ndigital content – and the complex infrastructure which \nsupports its capture and delivery – has similarly been \ndeveloped from contributors’ recurrent budgets. There is \nno national digitisation strategy or funding in Australia, \nand very few avenues for raising either startup or \nmaintenance funding for such projects and services. One \nconsequence of this dearth of funding options is that \nmusic researchers working in universities have little or \nno access to digital music collections at their home \ninstitutions. This lack of access itself may be a factor in \nAustralia’s relatively low MIR research activity, and \nindeed in the limited exposure to MIR’s possibilities \namong the general music research community.\nMIR applications are, of course, high on this list \nof desired enhancements. But because we operate \nprimarily within a collection, documentation, access and \ndelivery  framework, we are particularly interested in \nthe ways MIR could be coupled with other metadata \nactivities. For example, we wish to apply IFLA’s \nFunctional Requirements for Bibliographic Records  [5] \ninformation model to the resource database, as we \nrecognise and have documented [1] the great value its \nversion modelling offers to music users. We also wish to \nbuild annotation services, supporting users to add their \nscholarly apparatus – or even non-scholarly views – on \nparticular pieces of music. \nWe see the potential for MIR, FRBR and \nannotations to operate together and multiply benefits. \nApplying Musical Character Recognition (MCR) \napplications to printed music to produce machine-\nreadable texts and generate MIDI files could, for \nexample, indicate version differences between printed \nmusic items which human cataloguers do not always \nidentify. Applications to create MIDI files from existing \nsound recordings could similarly reveal melodic \nrelationships between sound recordings or indeed \nbetween scores and recordings which are otherwise \nhidden due to differences in descriptive metadata. We \ncan envisage software applications that translate these \nversion differences into appropriate metadata supporting \nFRBR displays. Similarly, we can imagine annotation \napplications which both allow users to contribute their \nknowledge about music resources or people, and which \ncan trigger maintainers to establish relationships \nbetween music items, or between music items and \npeople, or indeed between people and people. \nApplications to support some of these services \nare already well advanced in the MIR and information \nresearch communities, and indeed in international \ndemonstrator and production services. More importantly \nfor a national service, key infrastructure capability to \nsupport these and other ‘data-mining’ applications is \ncurrently being built in Australian research universities. \nThe National Library is a key partner in three large infrastructure projects recently funded1 by the \nAustralian Commonwealth Government. One is \nprimarily concerned with developing institutional \ndigital repositories, with the National Library \ndeveloping a repository specifically for non-affiliated \nscholars and creators, and the federated discovery \nservice covering all repositories. One aims to ensure that \nthe myriad formats which will be deposited in such \nrepositories will be preserved and accessible into the \nfuture. The third is developing systems to support \nauthentication based access to a range of services –\nincluding repositories – across the higher education \nsystem. \nClearly, these three important infrastructure \ninitiatives could be used effectively to support advanced \nmusic applications across collections, institutions and \neven sectors. The work we have already done with \nuniversities contributing to MusicAustralia has placed \nmusic and its needs firmly on these agendas.\n7.CONCLUSIONS\nThe desire to apply musical solutions to information \nretrieval and interrogation of the resources in \nMusicAustralia has always been part of the long-term \nvision, and investigation of potential applications has \ninformed the context in which MusicAustralia has been \ndeveloped.  However, for reasons outlined in this paper, \nbusiness decisions have been taken to substantially drive \nand adopt generic repository solutions as a first priority. \nThis strategy has successfully positioned digital \nmusic services across formats, from preservation \nthrough to delivery, within the core sustainable business \nenterprise of the key partner institutions, the National \nLibrary of Australia and ScreenSound Australia. Music \nis only one part of the business of large cultural \ninstitutions, which have many competing demands on \ntheir resources and limited access to external or special \nproject funding. However, their momentum to develop\nseamless access to the nation’s documentary resources \nand information, delivered to anyone, anywhere, and for \nany purpose – and this has embraced music! – has \nshaped and guided some key tenets of the \nMusicAustralia service. Its audience is broad; its \nmusical scope and coverage is comprehensive; \ndiscovering and getting musical objects and information \nwith ease are primary goals; cooperation between \ninstitutions, organisations and individuals, across the \nnation and across sectors, is an essential; and its service \nmodel is loosely ‘democratic’, centrally sustained but \nwith each custodial organisation responsible for all \naspects of its content. \n                                                       \n1  The Australian Partnership for Sustainable Repositories (APSR) led by \nthe Australian National University, the Australian Research Resources \nOnline to the World (ARROW) project led by Monash University, and \nthe Meta Access Management System (MAMS), led by Macquarie \nUniversity were awarded multi-year Australian Commonwealth \nGovernment funding in 2003.In the process, however, MusicAustralia has \nforged a national music information strategy and is \ncreating an extraordinarily rich Australian music \n‘content bank’ with enormous potential for exploitation \nfrom a variety of research perspectives. \nKey questions, perhaps even tensions, underpin \nsuch exploitation: whose creative imagination and \nwhich discipline will bring such research inquiry to the \nservice? Whose business is it and who will pay? Who \nwill develop the tools, methodologies and applications –\noften seen as cutting edge, experimental, or risky – to \napply systemically to a production level service that is \ninterdependent with the capacity of its contributing \norganisations? How can the requisite research expertise \nbe developed in a musical community when digital \nmusic collections and repositories in Australia are still \nin their early stages?\nThe opportunities for further MIR applications \nbest lie in identifying the synergies between the needs of \nusers, the needs and aspirations of researchers, and the \ncapacity of the creative and research communities to \nengage with the service in partnerships that can bring \nexternal funding to their development. This project has \nthe potential to move far beyond a national discovery \nservice for Australian music to a powerful research \nenvironment capable of driving major innovation. Its \nstrength lies in its cooperative national strategy, and it \ncould support cooperative, interoperable and standards \nbased MIR activities across the nation. The question \nremains: who will drive this and how might the whole \nvision be achieved?\n8.REFERENCES\n[1] Ayres, Marie-Louise, “Case Studies in Implementing \nFRBR: AustLit and MusicAustralia”, paper presented at \nEvolution or Revolution, The Impact of FRBR,\nMelbourne, Australia this and all subsequent cited \nonline documents viewed 20 August 2004, \n<http://www.nla.gov.au/lis/stndrds/grps/acoc/ayres2004.\ndoc>, 2 February 2004.\n[2] Dublin Core Metadata Initiative, Dublin Core \nLibrary Application Profile , \n<http://dublincore.org/documents/library-application-\nprofile/>, 2002.\n[3] EAC Working Group, Encoded Archival Context \nBeta, <http://www.iath.virginia.edu/eac/>, 2004.\n[4] Holmes, Robyn, “MusicAustralia: A Digital Strategy \nfor Music”, paper presented at the International \nAssociation of Music Libraries (IAML) Annual \nConference, Berkeley, California, USA, \n<http://www.nla.gov.au/nla/staffpaper/2002/iaml8Aug0\n2.html>, 4 August 2002.[5] IFLA Study Group on the Functional Requirements \nfor Bibliographic Records, Functional Requirements for \nBibliographic Records: Final Report , UBCIM \nPublications,\n<http://www.ifla.org/VII/s13/FRBR/FRBR.pdf >, 2004. \n[6] Library of Congress Network and MARC Standards \nOffice, Metadata for Object Description Schema , < \nhttp://www.loc.gov/standards/mods/>, 2003.\n[7] Library of Congress Network and MARC Standards \nOffice, MARC21 Concise Format for Authority Data , \n<http://www.loc.gov/marc/authority/ecadhome.html>, \n2003.\n[8] Library of Congress Network and MARC Standards \nOffice, Metadata for Authority Description Schema ,\n<http://www.loc.gov/standards/mads/>, 2003.\n[9] National Library of Australia, Metadata for \nAustralian Parties Schema ,\n<http://www.musicaustralia.org/schemas/maps/maps-\nv1.1.xsd>, 2004.\n[10] Open Archives Initiative, Open Archives Initiative \nProtocol for Metadata Harvesting v2.0, \n<http://www.openarchives.org/OAI/openarchivesprotoc\nol.html>, 2002."
    },
    {
        "title": "Finding Approximate Repeating Patterns from Sequence Data.",
        "author": [
            "Jia-Lien Hsu",
            "Arbee L. P. Chen",
            "Hung-Chen Chen"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415530",
        "url": "https://doi.org/10.5281/zenodo.1415530",
        "ee": "https://zenodo.org/records/1415530/files/HsuCC04.pdf",
        "abstract": "In this paper, an application of feature extraction from music data is first introduced to motivate our research of finding approximate repeating patterns from sequence data. An approximate repeating pattern is defined as a sequence of symbols which appears more than once under certain approximation types in a data sequence. By using the ‘cut’ and ‘pattern_join’ operators, we develop a level-wise approach to solve the problem of finding approximate repeating patterns.",
        "zenodo_id": 1415530,
        "dblp_key": "conf/ismir/HsuCC04",
        "keywords": [
            "feature extraction",
            "music data",
            "approximate repeating patterns",
            "sequence data",
            "cut operator",
            "pattern_join operator",
            "level-wise approach",
            "approximation types",
            "data sequence",
            "approximate repeating pattern"
        ],
        "content": "Finding Approximate Repeating Patterns from Sequence Data \nJia-Lien Hsu \nDepartment of Computer Science and \nInformation Engineering, Fu Jen \nCatholic University, Taiwan, R.O.C. \nE-mail: alien@csie.fju.edu.tw Arbee L.P. Chen \nDepartment of Computer Science \nNational Chengchi University \nTaipei, Taiwan, R.O.C. \nE-mail: alpchen@cs.nccu.edu.tw Hung-Chen Chen \nDepartment of Computer Science \nNational Tsing Hua University \nHsinchu, Taiwan, R.O.C. \nE-mail: jesse@cs.nthu.edu.tw \n \nABSTRACT  \nIn this paper, an application of feature extraction from music data is \nfirst introduced to motivate our research of finding approximate repeating patterns from sequence da ta. An approximate repeating \npattern is defined as a sequence of  symbols which appears more than \nonce under certain approximation types in a data sequence. By using \nthe ‘ cut’ and ‘ pattern_join ’ operators, we develop a level-wise \napproach to solve the problem of finding approximate repeating patterns.  \n1. INTRODUCTION \nCompared to transactional data, less attention on data mining has \nbeen drawn to the issues of mining sequence data such as traces of web browsing activities and sequences of multimedia data. Although tasks of data mining are usually application-dependent, to discover \nsome universal properties, such as  repetitions and trends, from data \nsequences is still promising.  \n1.1 Application: Feature Extraction from Music Data  \nFor content-based music data re trieval, one of fundamental \ntechniques is to extract music features from the raw data of music objects and organize them as a musi c index for further processing. \nTaking into account of the music characteristics, the music features \ncan be classified into four categories: static music information , \nacoustical feature , thematic feature and structural feature  [Hsu01]. \nAs for the structural feature, classic music objects are composed according to a special structure called musical form  in which there \nare two basic rules: hierarchical rule and repetition rule [Jone74][Krum90][Narm90]. The hierarchical rule says music objects are formed hierarchically. The repetition rule says that some sequences of notes, known as motives , repeatedly appear in a \nmovement. Repetition rule is also meaningful for other music categories. For example, the repetition in pop music is called the refrain .  \nBased on the repetition rule, we de rive the sequences of notes \nappearing more than once in the music object as its structural feature. The sequences are called repeating patterns  [Hsu01][Hsu98]. \nResearchers in the musicology field also agree that repetition is a universal characteristic in music structure modeling [Krum90][Narm90]. Meanwhile, the length of repeating patterns is \nmuch shorter than that of a music object. Choosing repeating patterns as the features to represent the music objects meets both efficiency and semantic-richness re quirements for conten t-based music data \nretrieval. Therefore, techniques for finding the repeating patterns \nfrom the sequence of notes of a music object need are to be developed. \nHowever, patterns may repeat in the music object with some variance. One of the concepts to deal with this variance is the prototypical \nmelody . “The prototypical melody is  a kind of generalization to \nwhich elements of information represented in the actual melody may seem relevant” [Self98]. The prototypical melody suggests the greatest influence on the way the actual melody is remembered and retrieved. For example, consider the five extracts from Mozart’s Piano Sonata K.311, shown in Figure 1(a)-(e). A prototypical melody, \nwhich approximates the five extracts, is identified in Figure 1(f). \n          \nFigure 1:  Five extracts from Mozart’s Piano Sonata K. 311 and \na prototypical melody (excerpted from [Self98]). \nFor the purpose of searching, it is  easier to handle the compositions \nwhich are managed in a consistent way to extract features. An algorithmic approach to the problem of identifying prototypical melody is required for musi c information retrieval. \n1.2 Related Works \nConsidering the previous application on music data, Shih, et al.  \n[Shih01] propose a modified Lempel-Ziv algorithm for automatic extraction of exact repeating patterns in music databases. The music objects are first segmented into bars and the bar index table is then \nconstructed. An adaptive dicti onary-based compression algorithm \n(LZ-78) is then applied to the bar-re presented music scores to extract \nrepetitive patterns. Rolland [Roll98][Roll99] also focus on the pattern extraction problem and propos e a more flexible similarity \nmetrics between music sequences . A dynamic programming-based \napproach, called FlExPat, is also introduced. By pair comparison and then categorization, the melodic patterns can be found. \nIn [Meek01], the authors introduce an algorithm, Melodic Motive \nExtractor (MME), to “extract themes from a piece of music.” Based on the hashing function techniques a nd lattice structure, the MME is \ndevised to identify frequent pa tterns from a sequence of music \ncontour. In [Dann02], the authors apply the dynamic programming technique on music audio data, which is represented as sequences, to \nrecognize the repetition structure.  First, possible pairs of segment  \n(i.e., subsequence) will be identified and considered as candidates. \nSimilar candidates will be clustere d, and the analysis of musical \nstructure will also be produced according. However, the time complexity of proposed method would be as higher as O(n\n4), where n \nis the length of sequence. In [Pie n02], a text-based method is applied \nto extract maximal frequent phrase from music data. The proposed approach is a variant of n-gram method by combining bottom-up and \nf \nPermission to make digital or hard copi es of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that copies \nbear this notice and the full citation on the first page. \n© 2004 Universitat Pompeu Fabra. greedy methods, and first introduced in [Ahon99] for text mining \ntask from documents.  \nWe propose two approaches in di scovering repeating patterns in \nmusic data [Hsu01][Hsu98]. For th e first approach, the repeating \npatterns are found based on a data structure called correlative matrix . \nFor a music object of n notes, an ( n x n) correlative matrix is \nconstructed to keep the intermediate results during the finding process. In the other approach, the longer repeating pattern of a music object is discovered by repeat edly combining shorter repeating \npatterns by a string-join  operation. The storage space and execution \ntime can therefore be reduced. In this paper, we extend the problem of finding exact repeating patterns to finding approximate repeating \npatterns. \n2. PROBLEM FORMULATION \nThe application in Section 1 motivate the problem of finding \napproximate repeating patterns from sequence data. In this section, \nwe formulate the problem and introduce three types of approximations, i.e., longer_length , shorter_length , and equal_length . \nDue to the space limitation, we only discuss the case of  longer_length approximation in the rest of this paper. \n2.1 The Definitions \nWe first define the match operator, i.e., longer_length_match , as \nfollows.  \nDEFINITION 2.1: longer_length_match  (P, LL) \nGiven P = (p1, p2, … , pm) which is a pattern sequence of length m, \nand LL = (s1, s2, … , sn), a data sequence of length n, where n > m.  \n\n\n\n=<<<== =\n=\notherwise.,0  1     where  ,,,2,1 for   ,  if,1)LL,P( _ _\n2 1 n b bb m i s p match length longer\nmb ii\nLL \nDefine r = n − m to indicate the approximation degree  of \nlonger_length_match (P, LL). longer_length_match (P, LL) = 1 \nwhen there exist m symbols in LL, which match the m symbols in P \nin sequence, and s1 = p1, sn = pm. The approximation degree denotes \nthe number of symbols which will not be matched when applying a match operator. \nFor example, let P be a four-symbol pattern, P = (p\n1, p2, p3, p4) = (A, \nB, C, D). Let LL be a six-symbol pattern, LL = (s1, s2, … , s6) = (A, \nB, K, C, M, D). In this case, we can find ( b1, b2, b3, b4) = (1, 2, 4, 6), \nwhich means that p1 matches s1, p2 matches s2, p3 matches s4, and p4 \nmatches s6. Therefore, longer_length_match (P, LL) = 1, and the \napproximation degree is two. \nThe match operator is used to compute the repeating frequency  of a \npattern sequence P in a data seque nce S. The repeating frequency of \nP is the number of appearance of P in data sequence S. Each appearance is identified by a substring of S which makes the match operator satisfied. We discuss th e computation of the repeating \nfrequency of P in S with approximation degree r as follows. \nDenote freq (P, S, r, AT) as the repeating frequency of pattern \nsequence P in a data sequence S with the approximation type AT (i.e., \nAT = longer_length) and approximation degree r.  \nz \n()\n() ∑=i match length longerr freq\nLL,P _ _gth longer_len,,S,P  hold. must  or either      then 1,  )  LL(P, _        1and  ) LL(P, _  if        ,   ],..S[  LLand ]..S[ any LLfor   )3(P LL  )2(S of substringa  is   LL(1)where\nadcbch length_mat longerch length_mat longerjidc bar\njij iii\n<<==≠ = =+= \n \nEach appearance is identified by a substring of S which satisfies the \nlonger_length_match  operator. Moreover, there is no overlap among \nthese appearances as specified in (3).  \nAs discussed in Section 1, the found patterns can be refrains and \nmotives of music objects. The refrains and motives are recognizable patterns which repeat several tim es. Therefore, the overlapped \nappearances cannot be consider ed as recognizable patterns. \nExample 1   \nLet P be “ABC”, and S be “ AKBCDEABLCF”. Consider the \nlonger_length approximation with degree one. Among all substrings \nof length four, we have two substrings, LL 1 and LL 2, which set \nlonger_length_match (P, LL i) to 1, i.e., “AKBC” and “ABLC”. \nTherefore, freq (“ABC”, “AKBCDEABLCF”, 1,  longer_length) = 2. \n2.2 The Problem \nThe problem of finding approximate repeating patterns is formulated \nas follows. Given a data sequence S,  and the parameters of pattern \nlength, approximation degree, mi nimal repeating frequency, and \napproximation type (denoted pa_i, pa_r , pa_f, and AT respectively), \nfind all approximate repeating patterns. The pa_i specifies the range \nof pattern length to be found. The pa_r  specifies the range of \napproximation degree, specifically, pa_r  = {0, 1, …, max_pa_r } . \nThe pa_f specifies the minimal number of pattern appearances to \nform a repeating pattern.  \nWith respect to the longer_length approximation type, the problem is \nto find those patterns that repeatedly appear in S, in which the appearances are identified by the longer_length_match  operator and \nthe repeating frequencies are computed by freq (P, S, r, \nlonger_length). Therefore, the problem of extracting prototypical melody from music data, as shown in  Figure 1, can be formulated as \nthe one of find approximate repeating pattern with AT = \nlonger_length. Note that the patterns found are not necessarily substrings of the data sequence. Ot herwise, the prototypical melody \nwill not be discovered in any way. \nExample 2  \n \nThe data sequence S is “ABFCDLBMABPFCFD”, and the \nparameters are pa_i = {1, 2, 3, 4}, pa_r  = {0, 1}, pa_f = 2, and AT = \nlonger_length. The setting of paramete rs means that we are interested \nin those patterns of length one to four with the approximation type of longer_length. For each appearance of a pattern, at most one symbol of the appearance is not matched when applying longer_length_match  operator. Each of the found patterns has to \nappear at least twice in the data  sequence. The found patterns are as \nfollows. \nP\n1 = {”A”, ”B”, “C”, “D”, “F”},  \nP2 = {”AB”,”BF”,”CD”,”FC”,”FD”},  \nP3 = {”ABF”,”BFC”,”FCD”}, and P 4 = {”ABFC”} \n As an example of the pattern “ABF”, since the parameter pa_r  is set \nto {0, 1}, we have freq (“ABF”, S, 0,  longer_length) = 1 and freq \n(“ABF”, S, 1,  longer_length) = 1. In total, there are two appearances \nof the pattern “ABF”, which satisfies the parameter pa_f.  \n3. OUR APPROACH \nIn this section, we propose our so lutions to the problem of finding \napproximate repeating patterns, as well as the concept of cut and \npattern_join  operator, denoted by PJ.  \n3.1 The Level-wise Approach \nTo find all approximate repeating patterns, intuitively, we can apply \nsliding windows of all possibl e lengths, ranging from one to \n(max_pa_i  + max_pa_r ), over the data sequence S to have a set of \nsubstrings. For these s ubstrings, we check whether an approximate \nrepeating pattern can be formed by the longer_length_match  operator. \nAs in Example 2, we need slidi ng windows of lengths one to six. \nSuch brute-force process has too many substrings for the checking. In the following, we introduce the concept of cut. By carefully dividing S, we can have fewer substrings for the checking.  \nDenote max_pa_i  and max_pa_r  as the maximal values in the range \nof pa_i and pa_r , respectively, and strlen  (S) as the length of S.  \n()()\n() ( ) () ( ) ()\n.  is  and ,S ,1 1 2_ _ _ _  ,1 1 where  ,3 ,2 ,1 ], . . S[\ncut_idistrlen i cw cw minbr pa maxipa max cw i cw aiba cuti\n−×+−×=+ =−×+== = L\n \nExample 3   \nAs in Example 2, the data sequence S = “ABFCDLBMABPFCFD”, \nmax_pa_i = 4, and max_pa_r  = 1. Accordingly, cw = max_pa_i + \nmax_pa_r  = 5, we have three cuts as follows. \ncut1 = “ABFCDLBMA” \ncut2 = “LBMABPFCF” \ncut3 = “PFCFD”  \nSince the length of patterns to be  found is bounded by the parameters \npa_i and pa_r , we first partition the sequence S into substrings of \nlength cw, the summation of the maximal values of pa_i and pa_r . \nHowever, some patterns may span over two adjacent substrings, \ntherefore we add a padding of length ( cw−1) for each substring. Note \nthat in (2), the min function is used for a boundary condition, in case \nthe last cut has fewer than (2 ×cw−1) symbols.  \nBefore providing the definition of pattern_join operator, we \nintroduce a data structure to represent the found patterns and to keep the information needed for processing. The pattern set of length i, \ndenoted by P\ni = {<pat i(1), plist i(1)>, <pat i(2), plist i(2)>, …, <pat i(j), \nplist i(j)>}, where pat i(j) denotes the j-th pattern in P i, and plist i(j) is a \nlist of triplets (cut_id: start, end) . Each triplet indicates an appearance \nof pat i(j) in S. The ‘cut_id’ indicates a cut, and the ‘start and ‘end’ \nindicate where the pattern pat i(j) is located in the cut. For example, P 2 \n= {<”BF”, (1: 2, 3), (2: 5, 7)>, <”FD”, (1: 3, 5), (3: 4, 5)>}. The P 2 \nmeans that we have two patterns, “BF” and “FD”. For the triplet associated to “BF”, (2: 5, 7) means that “BF” is located in the cut\n2 \nranging from the fifth to the seventh position.  \nThe triplet whose ‘start’ value is larger than cw is called a dummy  \ntriplet . The dummy triplets are used for concatenating in succeeding \nprocessing. An appearance of a pattern specified by a dummy triplet will also be specified by a non-dummy triplet. Therefore, the repeating frequency of pat\ni(j) is the number of triplets, excluding dummy triplets, in plist i(j). For example, P 2 = {<”FC”, (1: 3, 4), (2: 7, \n8), (3: 2, 3)>}, and the repeating frequency of “FC” is 2. \nIn the following, we introduce the pattern_join operator. The \npattern_join operator is used for concatenating the found patterns of \nlength i to derive the candidate patterns of length i+1. By applying \nthe pattern_join operator in a level- wise manner, all the patterns will \nbe found. \nFor two patterns of length i, pat  i(a) and pat  i(b), we define the \npattern_join operator as follows. \nDEFINITION 3.1: pattern_join operator \n( )\n\n∅= 〉 〈=〉 〈〉 〈\n+ +\notherwise   ,)]1([1.. pat]..2[ pat if, plist , patplist , pat , plist , pat\n(b) (a) (c)\n1(c)\n1(b) (b) (a) (a)\ni- iPJ\ni i i ii i i i\n \nwhere \n(1) pati+1(c) = pat  i(a)[1..i] + pat  i(b)[i..i], where the ‘+’ denotes the \nstring concatenation  \n(2) for (cut_id(a): start(a), end(a)) and (cut_id(b): start(b), end(b)) from \nplist i(a) and plist i(b), respectively, if  \ni. cut_id(a) = cut_id(b) and start(a) < start(b) \nii. 0 ≤ (end(b) − start(a) + 1) − |pat i+1(c)| ≤ max_pa_r  \nadd (cut_id(a): start(a), end(b)) into plist i+1(c) \n \nFor two patterns of length i, pat  i(a) and pat  i(b), if the two patterns \nhave an overlapping of ( i−1) symbols, we concatenate the two \npatterns as the pattern pat i+1(c). Then, we check the corresponding \ntriplet lists to derive the triplet list of pat i+1(c). The triplet list of \npati+1(c) is constructed as follows. The conditions of (2) are used to \nmake sure that the pattern pat i+1(c) and the substring, indicated by the \ntriplet (cut_id(a): start(a), end(b)), satisfy the longer_length_match  \noperator.  \n(cut_id(a), start(a), end(a))\n(cut_id(b), start(b), end(b))\n \nFigure 2:  An illustration of the pattern_join operator. \nExample 4   \nPJ (<”BF”, (1: 2, 3), (2: 5, 7)>, <”FD”, (1: 3, 5), (3: 4, 5)>) = \n<”BFD”, (1: 2, 5)> \nOur method is a level-wise approach (procedure \nfind_approxi_pattern ). First, we determine cuts from the \ndata sequence S. By concaten ating the patterns of length i from P i \n(procedure find_level ), we derive the candidate patterns of \nlength ( i+1), denoted by C i+1. After checking the repeating frequency \nof candidate patterns, the patterns of length ( i+1), P i+1, will be \nconfirmed (procedure prune ). As for the next level, similar \nprocessing is performed until all patterns are found. \nThe main parts of our algorithm ar e shown as follows. Due to the \nspace limitation, other supporting procedures of our approach are not included in this paper.  \nAlgorithm  find_approxi_pattern (S, pa_i, pa_r, \npa_f) \n//input: the data sequence S, pa_i, pa_r, pa_f \n//output: the approximate pattern set AP Begin \n1. W = cut_dataseq(S, max_pa_i + max_pa_r) \n2. C1 = find_level_1(W) \n3. P1 = prune(C 1) \n4. for (i = 1 to (max_pa_i - 1)) \n5.  Ci+1 = find_level(P i) \n6.  Pi+1 = prune(C i+1) \n7. AP = P 1 ∪ P2 ∪ … ∪ Pmax_pa_i \n8. return AP \nEnd \n \nAlgorithm  find_level (P i) \n//input: the pattern set, P i \n//output: the pattern set, P i+1 \nBegin \n1. Pi+1 = ∅ \n2. for each (A, B) in P i  \n3.  TP = pattern_join(A, B) \n4.  Pi+1 = Pi+1 ∪ TP  // add TP into P i+1 \n5. return P i+1 \nEnd \nExample 5   \nGiven the same S, pa_i, pa_r , and pa_f, as in Example 2, to find all \napproximate repeating pattern s by applying our approach \nfind_approxi_pattern .  \nFirst, we determine three cuts as in Example 3. The following \nprocesses are preceded by a level-wise manner, as shown in Figure 3. Through scanning the data sequence S once, we have the candidate \nset C\n1. By checking their repeating fre quencies, the patterns of length \none are derived. As for the next le vel, we first derive the candidate \nset C 2, followed by the minimal repeating frequency checking. For \neach pair of patterns from P 1, we apply the PJ operators to derive C 2. \nFor each pattern in C 2, we check its repeating frequency to determine \nthe patterns of length two, P 2. Similar processes are repeated until all \nthe patterns whose length is specified in pa_i are obtained.  \nA B C D F L M PAB BA BF CD FC FD CFABF BFC BFD FCDABFC BFCD\nA B C D F L M PAB BA BF CD FC FD CFABF BFC BFD FCDABFC BFCD\n \nFigure 3:  The illustration of processing steps in Example 5. \n4. CONCLUSION \nIn this paper, the application of f eature extraction is first presented to \nmotivate our research on finding approximate repeating patterns from sequence data. In Section 2, follo wed by the definitions of match \noperator and approximation type, we consider the type of longer_length approximation as the fundamental problem. We develop a level-wise approach to the problem of finding approximate \nrepeating patterns with respect to  the longer_length approximation. \nIn addition, we extend the basic a pproach for efficiently finding long \npatterns. We also complete the preliminary investigation of performance study, in which we explore the four factors having \nimpact on the performance and show that our approach is efficient. Likewise, the refined methods and performance study are not covered in this paper because of space limitation. \nThe future work includes the fo llowing. First, the extensive \nexperiments of effectiveness study on real data are still carrying on. \nGiven a corpus of music data, our approach will be applied to discover the prototypical melody of music data, as shown in Figure 1. \nIn our experiment design, the disc overed features will be compared \nwith music catalogs, such as [Barl75], to show the effectiveness. Moreover, we define the probl ems of other two types of \napproximations, \ni.e., shorter_length and equal_length approximations. \nWe are currently working on developing more efficient algorithms to solve the problems. Besides, our appr oach can be directly applied to \nmonophonic music objects, but not polyphonic music objects. We are also working on exploring featur es for polyphonic music objects and \ndeveloping corresponding methods. \nREFERENCE:  \n[Ahon99] Ahonen-Myka, H., “Fi nding All Maximal Frequent \nSequences in Text,” in Proc. of Intl. Conf. on Machine Learning , \n1999. \n[Barl75] Barlow, H. and S. Morgenstern, A Dictionary of Musical \nThemes , Crown Publishers, Inc., New York, 1975 \n[Dann02] Dannerberg, R. B. a nd N. Hu, “Pattern Discovery \nTechniques for Music Audio,” in Proc. of ISMIR , 2002. \n[Hsu98] Hsu, J. L., C. C. Liu,  and A. L. P. Chen, “Efficient \nRepeating Pattern Finding in Music Databases,” in  Proc. of Intl. Conf. \non Information and Knowledge Management (CIKM'98 ), 1998. \n[Hsu01] Hsu, J. L., C. C. Liu, a nd A. L. P. Chen, “Discovering Non-\ntrivial Repeating Patterns in Music Data,” IEEE Transactions on \nMultimedia , Vol. 3, No. 3, 2001.  \n[Jone74] Jones, G. T., Music Theory , Harper & Row, Publishers, \nNew York, 1974. \n[Krum90] Krumhansl, C. L., Cognitive Foundations of Musical Pitch , \nOxford University Press, New York, 1990. \n[Meek01]  Meek, C. and W. P. Birmingham, “Thematic Extractor,” in \nProc. of ISMIR , 2001. \n[Narm90] Narmour, E., The Analysis and Cognition of Basic Melodic \nStructures , The University of Chicago Press, Chicago, 1990. \n[Pien02] Pienimaki, A., “Indexing Music Database Using Automatic \nExtraction of Frequent Phrases, in Proc. of ISMIR  2002. \n[Roll98] Rolland, P. Y., “FlExP at: A Novel Algorithm for Musical \nPattern Discovery,” in Proceedings of the 12th Colloquium on \nMusical Informatics  (XII CIM), 1998. \n[Roll99] Rolland, P. Y. and J. G.  Ganascia, “Musical Pattern \nExtraction and Similarity Assessment,” in Miranda, E. (eds.), \nReadings in Music and Artificial Intelligence , New York and London: \nGordon & Breach - Harwood A cademic Publishers, 1999. \n[Self98] Selfridge-Field, E., “C onceptual and Representational \nIssues in Melodic Comparison,” in Hewlett, W. B. and E. Selfridge-\nField (eds.), Melodic Similarity: C oncepts, Procedures, and \nApplications (Computing in Musicology: 11 ), The MIT Press, 1998. \n[Shih01] Shih, H.-H., S. S. Na rayanan, and C.-C. Jay Kuo, \n“Automatic Main Melody Extraction from MIDI Files with a Modified Lempel-Ziv Algorithm,” in \nProc. of Intl. Symposium on \nIntelligent Multimedia, Video and Speech Processing , 2001."
    },
    {
        "title": "Comparison Of Features For DP-Matching Based Query-by-Humming System.",
        "author": [
            "Akinori Ito",
            "Sung-Phil Heo",
            "Motoyuki Suzuki",
            "Shozo Makino"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416178",
        "url": "https://doi.org/10.5281/zenodo.1416178",
        "ee": "https://zenodo.org/records/1416178/files/ItoHSM04.pdf",
        "abstract": "In this paper, we compared three kinds of similarity measures for DP-matching based query-by-humming music retrieval experiments. First, a DP matching-based algorithm is formulated using the similarity between a deltaPitch of an input humming and that of a song in the database. Then the three similarities are introduced: distance-based similarity, quantization-based similarity and fuzzy quantization-based similarity. The three similarities are compared by experiments. From the experimental results, the distance-based one gave the best recall rate. In addition, we examined the combination of distance-based and fuzzy-quantization-based similarities. The experimental result showed that the recall rate was improved by the combination.",
        "zenodo_id": 1416178,
        "dblp_key": "conf/ismir/ItoHSM04",
        "keywords": [
            "DP matching",
            "query-by-humming",
            "music retrieval",
            "deltaPitch",
            "database",
            "similarity measures",
            "recall rate",
            "distance-based similarity",
            "quantization-based similarity",
            "fuzzy quantization-based similarity"
        ],
        "content": "COMPARISON OF FEATURES FOR DP-MATCHING BASED\nQUERY-BY-HUMMING SYSTEM\nAkinori Ito Sung-Phil Heo Motoyuki Suzuki Shozo Makino\nGraduate School of\nEngineering\nTohoku University\nAoba 05, Aramaki,\nSendai, 980-8579 Japan\n+81 22 217 7084\naito@makino.ecei.tohoku.jpKorea Telecom Research\n& Development Group\n17 Umyeon-dong,\nSeocho-gu, Seoul,\n139-792 Korea\n+82 31 702 9749\nhsphil@hotmail.comGraduate School of\nEngineering\nTohoku University\nAoba 05, Aramaki,\nSendai, 980-8579 Japan\n+81 22 217 7112\nmoto@ecei.tohoku.ac.jpGraduate School of\nEngineering\nTohoku University\nAoba 05, Aramaki,\nSendai, 980-8579 Japan\n+81 22 217 7172\nmakino@ecei.tohoku.ac.jp\nABSTRACT\nIn this paper, we compared three kinds of similarity\nmeasures for DP-matching based query-by-humming\nmusic retrieval experiments. First, a DP matching-based\nalgorithm is formulated using the similarity between a\ndeltaPitch of an input humming and that of a song in\nthe database. Then the three similarities are introduced:\ndistance-based similarity, quantization-based similarity\nand fuzzy quantization-based similarity. The three\nsimilarities are compared by experiments. From\nthe experimental results, the distance-based one\ngave the best recall rate. In addition, we\nexamined the combination of distance-based and\nfuzzy-quantization-based similarities. The experimental\nresult showed that the recall rate was improved by the\ncombination.\n1. INTRODUCTION\nRapid progress of hardware and software technologies\nmakes it possible to manage and access large volumes\nof music data. To access the music contents\nmore easily, content-based music information retrieval\nsystems have been developed. Some of these systems\nuse a user’s humming as a key to information retrieval.\nThe input humming is segmented into notes, and pitch\nfrequencies are extracted. DeltaPitch and inter-onset\ninterval (IOI) ratio are often used as features of the\nhumming. Then the input is matched with the music in\nthe database. The matching method involves two\naspects: the similarity measure and the matching\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for proﬁt or\ncommercial advantage and that copies bear this notice and the\nfull citation on the ﬁrst page.\nc°2004 Universitat Pompeu Fabra.algorithm. Ghias et al. used the deltaPitch quantized in\nthree levels (up, down and same)[1] as a representation\nof a note, and the identity of the quantized codes was\nused as the similarity of the two notes. McNab et\nal.[2] employed a similar technique. Sonoda et al.\nused a similarity based on hierarchical quantization of\ndeltaPitch and IOIratio[3]. For the matching algorithm,\ndynamic programming (DP) based matching algorithm\nis commonly used[1, 3]. The MIRACLE system[4]\nemployed a two-level matching algorithm that utilized\nlinear matching and DP matching.\nIn this paper, we compare several similarity measures\nbetween notes from the retrieval accuracy point of view.\nThe quantization-based similarity is the most popular\napproach, but there seems to be a couple of other\npossibilities of similarity measures. The continuous DP\nmatching is chosen as a matching algorithm. Then\nthree kinds of similarity measures are compared.\nThe MIR system used in this work assumes that a\nmusic database contains information of musical pieces\nwith monophonic melody, and heights and lengths of\nthe notes in the database are taken from MIDI data.\nWhen an input humming is given, the feature sequence\nis extracted from the input[5]. First, the input signal\nis segmented using a band-pass ﬁlter and power\nthreshold. Then pitch frequencies are extracted from the\nhumming. The sequence of the deltaPitch is calculated\nfrom the extracted pitch frequency. Here, a deltaPitch\nvalue is expressed as cent, i.e. if we have contiguous\nnotes of f1(Hz) and f2(Hz), the deltaPitch value ∆fis\n∆f= 1200 log2f2\nf1: (1)\n2. DP-MATCHING BASED MUSIC\nINFORMATION RETRIEVAL\nDP matching is a popular approach to measure the\ndistance between an input humming and a song in thedatabase. The DP matching is a matching algorithm\nthat considers insertions and deletions of notes in a\nhumming input. As there are many variations of the\nDP-based matching, we employ a continuous-DP[6]\nbased algorithm.\nLet a deltaPitch sequence of the input humming be\nh(1); : : : ; h (J), and that of m-th song in the database\nbedm(1); : : : ; d m(I). Let the similarity between the\ni-th note of the m-th song in the database and the j-th\nnote in the input humming be Sm(i; j). This similarity\nis deﬁned in several ways later. Here, Sm(i; j)for\ni·0orj·0is deﬁned as ¡1.\nNow DP-score gm(i; j)are calculated as follows.\nforj= 1\ngm(i;1) = Sm(i;1) (2)\nforj= 2\ngm(i;2) = max½\ngm(i¡1;1) +Sm(i;2)\ngm(i¡2;1) +Sm(i;2) +¯(3)\nforj¸3\ngm(i; j) = max8\n<\n:gm(i¡1; j¡2) + ( Sm(i; j) +¯)=2\ngm(i¡1; j¡1) +Sm(i; j)\ngm(i¡2; j¡1) +Sm(i; j) +¯\n(4)\nHere, ¯is a penalty value for insertion and deletion\nerrors. This algorithm assumes that the insertion errors\nor the deletion errors do not occur successively. Now\ngm(i; J)is an optimum score between the input\nhumming h(1); : : : ; h (J)and the m-th song assuming\nthat the note h(J)corresponds to the note dm(i).\nUsing gm(i; j), the score of song mis calculated as\nVm= max\nigm(i; J): (5)\nFinally, top- Nsongs that have the highest Vmare\nchosen as the retrieval result.\nTo evaluate a retrieval result, we employ the top-10\nrecall rate that is the ratio of the queries for which the\ncorrect song is listed within the top-10 candidates. Let\nNQbe number of the queries and ribe the rank of the\ncorrect song in the retrieval result of the i-th query.\nLet the rank-hit function hk(n)be\nhk(n) =½\n1ifn·k\n0otherwise(6)\nThen the top-10 recall rate R10is calculated as\nR10=1\nNQNQX\ni=1h10(ri): (7)\n3. DISTANCE-BASED MATCHING\n3.1. Distance-based similarity\nThe most straightforward way to calculate the similarity\nSm(i; j)is to observe the difference between dm(i)andmusic\ndatabasenumber of\nmusicalpieceschildren’s song: 155\ngenerated: 10,000\ntotal: 10,155\nnumber of\naverage notes57.8\nhumming\ndatasinger 1 male\nnumber of\nhumming67\nnumber of\naverage notes14.0\nTable 1. Large-scale database.\nFigure 1. Retrieval result by distance-based similarity.\nh(j). If they are similar, then the difference between\nthem is nearly zero. Then we can deﬁne Sm(i; j)as\nSm(i; j) =¡jdm(i)¡h(j)j: (8)\nIf the two deltaPitches are identical, then the maximum\nsimilarity of zero is obtained. When they are different,\nthe similarity value gets smaller.\n3.2. Experiment\nIn order to investigate the performance of the proposed\nmethod, a music retrieval experiment was carried\nout. The large-scale music database used for this\nexperiment is shown in Table 1. The large-scale MIR\nsystem has 10,155 songs that consist of 155 children’s\nsongs and 10,000 pieces automatically generated by the\ntrigram probabilities. The generation was performed as\nfollows. Let the pitch and the length of two contiguous\nnotes be (pi¡1; `i¡1);(pi; `i). Then the pitch pand the\nlength `of the next note are generated according to the\ntrigram probabilities P(pjpi¡1; pi)andP(`j`i¡1; `i)\nrespectively. The trigram probabilities was estimated\nfrom the 155 real songs. Therefore, the generated\n10,000 songs are similar to the 155 songs, from which\ntarget songs are chosen.\nThe top-10 recall rates for various ¯are shown in\nFigure 1. The best result of 80.2% was obtained when\n¯= 600.4. QUANTIZATION-BASED MATCHING\n4.1. Quantization-based similarity\nQuantization-based matching (also known as contour\nmatching) algorithm involves string-matching based\nalgorithm employed by QBH[1]. This algorithm\nconverts a deltaPitch sequence of input humming and\nsongs in the database into sequences of quantized\ncodes. Then the code sequence of the input humming\nis matched with that of songs in the database using an\napproximate string matching algorithm.\nConventional systems use quantized code such as\n‘U’ (up), ‘D’ (down) and ‘S’ (same). In this paper,\nwe express the quantized code as integer values\n0;1; : : : ; K ¡1. Let center values of quantization\nintervals be ¹0; : : : ; ¹ K¡1. Now the quantization\nfunction Q(x)is deﬁned as follows.\nQ(x) =argmin\nkjx¡¹kj (9)\nThen the similarity Sm(i; j)can be deﬁned as follows.\nSm(i; j) =½\n1ifQ(h(j)) =Q(dm(i))\n0otherwise(10)\nThe advantage of quantization-based method is its\nrobustness, as it is not affected by small ﬂuctuation of\npitch frequency. The drawback of this method is that\nthe performance of this method is greatly affected by\nquantization error.\n4.2. Experiment\nAn experiment was carried out to measure the\nperformance of quantization-based similarity. The\nexperimental condition is the same as described in\nSection 3.2. For a certain K, the center value is\nchosen as follows.\n¹i=Dµ\ni¡K¡1\n2¶\n(i= 0;1; : : : ; K ¡1)(11)\nDis a quantization interval. Figure 2 shows the\nresults for quantization level K= 3; : : : ; 11, penalty\n¯=¡1»0andD= 100. From this result, K= 11\nand¯=¡0:8are optimum and 53.3% of top-10 recall\nrate was obtained. Compared to the distance-based\nsimilarity (Figure 1), the quantization-based method\nwas not effective.\nFigure 3 shows the results for various D. IfDis\nlarge, the number of quantization error becomes lower,\nbut the number of ‘synonyms’ in the database becomes\nlarge. This result showed that the highest performance\nof 56.3% was obtained for K= 7andD= 300.\nThere can be a couple of reasons that degrades the\nquantization-based matching. One reason is quantization\nerrors around the quantization boundary, and the other\none is octave errors caused by pitch extraction errors.\nFigure 2 . Retrieval result by quantization-based\nsimilarity.\nFigure 3 . Retrieval result for various quantization\nintervals.\n5. FUZZY-QUANTIZATION-BASED MATCHING\n5.1. Observation of deltaPitch difference between\nhumming and database\nThe pitch error between a query humming and\na database was further investigated. First,\ncorrespondences between deltaPitches of an input query\nand that in the database were determined using\nDP matching. After searching for the optimal\ncorrespondences, deltaPitch differences were calculated.\nThe number of humming that contained 863 notes was\n67. Eight hundred seventy notes were detected from\nthe query by the automatic note segmentation, and 21\ninsertions and 14 deletion of notes were observed.\nNext, differences of the deltaPitch between the\ndatabase and humming were observed. Here, if the\nexpected deltaPitch is equal to the observed deltaPitch,\nthe error is zero. Table 2 shows the distribution of the\ndeltaPitch difference. From this result, it was found\nthat almost 40% of the notes have a difference of\nmore than 50 cents from the notes in the database.\nDifferences more than 1000cent are 1.4% of all notes,\nwhich seem to be caused by pitch extraction error.deltaPitch\ndifference\n(cent)#notes ratio(%)\n-50516 60.8\n50-100 216 25.4\n100-200 79 9.3\n200-300 12 1.4\n300-400 7 0.8\n400-500 1 0.1\n500-600 1 0.1\n600-700 2 0.2\n700-800 1 0.1\n800-900 0 0.0\n900-1000 2 0.2\n1000- 12 1.4\nTable 2. Distribution of deltaPitch difference\nFigure 4. Histogram of deltaPitch error.\nFigure 4 shows a histogram of the deltaPitch difference.\nHere, the y-axis represents the frequency of occurrence\nof deltaPitche errors whose values fall into 10 cent bin.\nFrom these results, it is clear that it is important to\ndeal with quantization error in order to raise the\naccuracy of the quantization-based method.\n5.2. Membership function and fuzzy quantization\nThere are several ways to avoid the effect of a\nquantization error. The most popular method is fuzzy\nquantization[7]. The basic idea of fuzzy quantization is\nto change the feature function of the quantization into\na continuous function.\nFirst, the ordinary quantization is formulated from\nthe feature function point of view. Let the number of\nquantization levels (clusters) be K, and the k-th cluster\nbeCk(0·k·K¡1). The feature function of the\nquantization f(x; k)is deﬁned as\nf(x; k)´½\n1ifx2Ck\n0otherwise(12)\nUsing f(x; k), continuous value xis categorized into\none of the quantization levels. On the other hand, thefuzzy quantization method uses a membership function\ninstead of a feature function. A membership function\nR(x; k)maps the input xinto a continuous value from\n0 to 1.\nThere are many possibilities to construct R(x; k). In\nthis paper, the membership function is constructed\nusing a probabilistic framework. Let us assume that the\ndeltaPitches that corresponds to the level Ckin an input\nhumming follows a certain distribution Ák(x). Now, the\nmembership function R(x; k)is calculated as follows.\nR(x; k)´Ák(x)\nKX\ni=0Ái(x)(13)\nHere, R(x; k)is equivalent to a posteriori probability\nof quantization level Ckgiven xunder an assumption\nthat the occurrence probability of Ckis uniform.\nNext, the distribution function Ák(x)has to be\ndecided. If we assume that the distribution function is\nindependent from kexcept the mean value, Ák(x)can\nbe calculated as\nÁk(x) =Á(x¡¹k) (14)\nwhere Á(x)is a distribution function whose mean is\nzero. As Á(x)is independent from k, it becomes\noptimal when the distribution function properly models\nthe distribution shown in Figure 4. From an observation\nof the distribution in Figure 4 that the center is sharp\nand around the edge is smooth, a distribution of Figure\n4 seems to be modeled by Laplace distribution rather\nthan Gaussian distribution.\nThe density function of the Laplace distribution is a\ntypical supergaussian distribution. The density function\nof Laplace distribution is given as follows.\nÁ(x)´1\n2vexp½\n¡jxj\nv¾\n(15)\nwhere vis a parameter that is related to the variance of\nthe distribution.\nTo generalize Gaussian and Laplace distributions,\nwe introduce another parameter °into the density\nfunction to control the kurtosis of the distribution. The\ndistribution function is\nÁ(x)´°\n2Γ³\n1\n°´\nv1\n°exp½\n¡jxj°\nv¾\n(16)\nwhere °is related to the kurtosis of the distribution.\nFigure 5 shows some examples of density functions for\nvarious vand°. When °= 1, this distribution function\nis identical to Laplace distribution. On the other hand,\nwhen °= 2we obtain Gaussian distribution.\nFrom this distribution function, the membership\nfunction Ris calculated according to formula (13).\nFigure 6 shows examples of membership functions\nwhen K= 9; °= 1andv= 20. 0 0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05\n-100 -50  0  50  100\nxv=10, gamma=1.0\nv=20,gamma=1.0\nv=10,gamma=0.7\nFigure 5. Examples of distributions for various vand°.\n 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n-400 -200  0  200  400R(x,k)\nx (cent)\nFigure 6. An example of membership functions.\nFinally, the value of the membership function that\ncorresponds to h(j)become a similarity.\nSm(i; j) =R(h(j); Q(dm(i))) (17)\n5.3. Experiment\nThe above-mentioned similarity is examined by the\nretrieval experiment. The experimental condition is\nthe same as described in 3.2. In this experiment,\nquantization level Kwas set to 9 and quantization\ninterval Dis set to 100. First, penalty value ¯was\noptimized. Figure 7 shows the result for v= 10and\n°= 0:5. The optimum ¯was around ¡0:2in this\nresult. Therefore ¯=¡0:2was used hereafter. Figure\n8 shows the top-10 recall rate for various vand°.\nFrom this result, it is found that the parameters are\noptimum around v= 50and°= 0:8. At the optimum\npoint, a recall rate of 76.1% was obtained.\n6. EXPERIMENT WITH FIVE USERS\nNext, the above three similarities are compared using\nthe humming data sung by ﬁve users. The experimental\nFigure 7 . Retrieval result by fuzzy-quantization-based\nsimilarity.\nFigure 8. Recall rates for various vand°.\nconditions are shown in Table 3. Various parameters\nare set to the optimal values obtained in the previous\nsections. For the distance-based method, ¯was set to\n600. For the quantization-based method, Dwas set to\n300, Kwas set to 7 and ¯was set to ¡0:8. For the\nfuzzy quantization based method, Dwas set to 100, K\nwas 9, ¯was¡0:2,vwas 50 and °was 0.8.\nFigure 9 shows the experimental results of ﬁve\nusers. The recall rate was different from user to user.\nDistance-based method showed the best performance\nand fuzzy-quantization-based method was the next. The\naverage recall rate for top-10 candidates was 65% by\nthe distance-based method.\nTo improve the recall rate, we tried to combine the\ndistance-based and fuzzy-quantization-based similarities.\nIn this experiment, similarity between the input\nhumming and the m-th song in the database was\ncalculated as\nVm=¸Vdist\nm+ (1¡¸)VFQ\nm (18)\nwhere Vdist\nmandVFQ\nmare similarities obtained by the\ndistance-based and fuzzy-quantization-based methodmusic\ndatabasenumber of\nmusicalpieceschildren’s song: 155\ngenerated: 10,000\ntotal: 10,155\nnumber of\naverage notes57.8\nhumming\ndatasinger 5 males\nnumber of\nhumming320\nnumber of\naverage notes11.7\nTable 3. Large-scale database.\nFigure 9. Retrieval result for ﬁve users.\nrespectively. Figure 10 shows the recall rate for various\n¸. This result is an average recall rate for ﬁve\nusers. By combining these scores, about a 1 point\nimprovement was obtained.\n7. CONCLUSION\nIn this paper, we compared three kinds of\nsimilarity measures through DP-matching based\nquery-by-humming music retrieval. The compared\nrepresentations are distance-based, quantization-based\nand fuzzy-quantization based representations. From\nthe experimental result, the distance-based method\ngave the best recall rate. Besides, we\nexamined the combination of distance-based and\nfuzzy-quantization-based similarity. The experimental\nresult showed that the recall rate was improved by the\ncombination.\n8. REFERENCES\n[1] A. Ghias, J. Logan, D. Chamberlin and B. C.\nSmith,Query by Humming: Musical Information\nRetrieval in an Audio Database, Proc. ACM\nMultimedia, 1995.\n[2] R. J. McNab, L. A. Smith, D. Bainbridge and I.\nFigure 10 . Retrieval results of the combined score.\nH. Witten, The New Zealand Digital Library\nMELody inDEX, D-Lib Magazine, May, 1997.\n[3] T. Sonoda and Y. Muraoka, A WWW-based\nMelody Retrieval System–An Indexing Method for\nA Large Database–, Proc. ICMC, 2000.\n[4] J.-S. Roger Jang, J.-C. Chen and M.-Y. Kao,\nMIRACLE: A Music Information Retrieval System\nwith Clustered Computing Engines, Proc. ISMIR,\n2001.\n[5] S.-P. Heo, M. Suzuki, A. Ito, S. Makino\nand H. Chung, Multiple pitch candidate\nbased music information retrieval method for\nquery-by-humming , Proc. Int. Workshop on\nAdaptive Multimedia Retrieval, 189–200, 2003.\n[6] S. Nakagawa, Connected Spoken Word Recognition\nAlgorithm by Constant Time Delay DP, O(n)\nDP and Augmented Continuous DP Matching,\nInformation Sciences , 33 , 63–85, 1984.\n[7] L. A. Zadeh , ”Fuzzy sets,” Inform. Contr., Vol. 8,\npp. 338-353, 1965"
    },
    {
        "title": "Perceptual Segment Clustering For Music Description And Time-axis Redundancy Cancellation.",
        "author": [
            "Tristan Jehan"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416854",
        "url": "https://doi.org/10.5281/zenodo.1416854",
        "ee": "https://zenodo.org/records/1416854/files/Jehan04.pdf",
        "abstract": "Repeating sounds and patterns are widely exploited throughout music. However, although analysis and mu- sic information retrieval applications are often concerned with processing speed and music description, they typi- cally discard the benefits of sound redundancy cancella- tion. We propose a perceptually grounded model for de- scribing music as a sequence of labeled sound segments, for reducing data complexity, and for compressing audio.",
        "zenodo_id": 1416854,
        "dblp_key": "conf/ismir/Jehan04",
        "keywords": [
            "repeating sounds",
            "patterns",
            "music analysis",
            "music information retrieval",
            "processing speed",
            "music description",
            "sound redundancy cancellation",
            "perceptually grounded model",
            "sound segments",
            "data complexity"
        ],
        "content": "PERCEPTUAL SEGMENT CLUSTERING FOR MUSIC DESCRIPTION\nAND TIME-AXIS REDUNDANCY CANCELLATION\nTristan Jehan\nMassachusetts Institute of Technology\nMedia Laboratory\nABSTRACT\nRepeating sounds and patterns are widely exploited\nthroughout music. However, although analysis and mu-\nsic information retrieval applications are often concerned\nwith processing speed and music description, they typi-\ncally discard the beneﬁts of sound redundancy cancella-\ntion. We propose a perceptually grounded model for de-\nscribing music as a sequence of labeled sound segments,\nfor reducing data complexity,and for compressing audio.\n1. INTRODUCTION\nTypical music retrieval applications deal with large\ndatabases of audio data. One of the major concerns of\nthese programs is the meaningfulness of the music de-\nscription, given solely the audio signal. Another concern\nis the efﬁciency of searching through a large space of in-\nformation. With those considerations, some recent tech-\nniques for annotating audio include psychoacoustic pre-\nprocessingmodels [1],and/or acollection offrame-based\n(i.e., 10-20 ms) perceptual audio descriptors [2] [3]. The\ndata is highly reduced, and the description hopefully rel-\nevant. However, although the annotation is appropriate\nfor sound and timbre, it remains complex and inadequate\nfordescribing music,ahigher-levelcognitivemechanism.\nWe propose a meaningful, yet more compact description\nof music, rooted on the segmentation of audio events.\nIn [4], Jonathan Foote and Matthew Cooper intro-\nduced a novel approach to musical structure visualiza-\ntion. They used self similarity of Mel-frequency cepstral-\ncoefﬁcientfeature vectors asa signature fora givenaudio\npiece. From the resulting matrix could be derived a rep-\nresentation of the rhythmic structures, which they called\nbeat spectrum . In [5], they proposed a statistically-based\nframeworkforsegmentingandclusteringlargeaudioseg-\nments via Singular Value Decomposition . The analysis\ncould for instance return the structural summarization of\na piece, by recognizing its “most representative” chorus\nandversepatterns. Ourapproach,ontheotherhand,starts\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2004 Universitat Pompeu Fabra.\n0 5 10 15 20 25\u0005-2\u0005-10124\nx 10\n0 5 10 15 20 25\nx 104\n\u0005-2\u0005-1012\n0 5 10 15 20 25\n15101525\n2015101525\n20\n0 5 10 15 20 25\n0 5 10 15 20 25\n0 5 10 15 20 2500.20.40.60.81\n00.20.40.60.81[A]\n[B]Figure 1. [A] 26-second audio excerpt of “Watermelon\nMan” by the Headhunters (1974). From top to bottom:\nwaveform,auditoryspectrogram,andloudnesscurvewith\nsegmentation markers (129 segments of about 200 ms).\n[B]resynthesisofthepiecewithonly30%ofthesegments\n(less than 8 seconds of audio). From top to bottom: new\nwaveform,auditoryspectrogram,loudnesscurve,andseg-\nmentation. Note that there are few noticeable differences,\nboth in the time and frequency domains.\nwithaperceptualtechniquefordescribingtheaudiospec-\ntralcontent ﬁrst,derivesameaningfulsegmentationofthe\nmusicalcontent then,andonly latercomputesamatrixof\nsimilarities. Our goals are both description and resynthe-\nsis. Weassumenopriorknowledgeaboutthemusicbeing\nanalyzed. For instance, the segment sizes are automati-\ncally derived from the music itself.2. PSYCOACOUSTICALLY INFORMED\nSEGMENTATION\nSegmenting is the process of dividing the musical signal\ninto smaller units of sounds [6]. A sound is considered\nperceptually meaningful if it is timbrally consistent, i.e.,\nit does not contain any noticeable abrupt changes. We\nbase our segmentation on an auditory model . Its goal is\nto remove the information that is the least critical to our\nhearing sensation, while retaining the important parts.\nWe ﬁrst apply a running STFT, and warp the spectrum\nto a 25-critical-band Bark scale. We then model the non-\nlinear frequency response of the outer and middle ear [7],\nand apply frequency and temporal masking [8], turning\ntheoutcomeintoa“what-you-see-is-what-you-hear”type\nof spectrogram [9] (see ﬁgure 1-[A]-2). A loudness func-\ntioniseasilyderivedbysummingenergyacrossfrequency\nchannels (see ﬁgure 1-[A]-3).\n00.20.40.60.81\n00.20.40.60.810 0.5 1 1.5 2 2.5 3\n0 0.5 1 1.5 2 2.5 3\n0 0.5 1 1.5 2 2.5 300.20.40.60.81\nFigure 2. Short 3.2-second audio excerpt extracted from\nﬁgure 1. From top to bottom: the unﬁltered event detec-\ntionfunction,theeventdetectionfunctionconvolvedwith\na 150-ms Hanning window, the loudness curve. Vertical\nred lines represent onset markers.\nWeconvertthespectrogramintoan eventdetectionfunc-\ntionby ﬁrst calculating the ﬁrst-order difference function\nfor each spectral bands, and by summing across channels\n(see ﬁgure 2-1). Transients are localized by peaks, which\nwesmoothslightlybyconvolvingthefunctionwitha150-\nms Hanning window to combine those perceptually fused\ntogether (i.e., two events separated in time by less than\n50 ms [10]). The required onsets can ﬁnally be found by\nextracting every local maxima within that function (see\nﬁgure 2-2). Since our concern is resynthesis by concate-\nnating audio segments, we reﬁne the onset location by\nsearching the corresponding previous local minimum in\ntheloudnessfunction,andtheclosestzero-crossinginthe\nwaveform (see ﬁgure 2-3).3. LABELING AND SIMILARITIES\nMusic could be described as an event-synchronous path\nwithin a perceptual multidimensional space of audio seg-\nments. Musicalpatternscanberecognizedasloopswithin\nthat path. A perceptual multidimensional scaling (MDS)\nof sound is a geometric model which provides us with\nthe determination of the Euclidean space that describes\nthe distances separating timbres as they correspond to\nlisteners’ judgments of relative dissimilarities. It was\nﬁrst exploited by Grey [11] who found that traditional\nmonophonic pitched instruments could be represented in\na three-dimensional timbre space with axes correspond-\ning roughly to attack quality (temporal envelope), spec-\ntral ﬂux (evolution of the spectral distribution over time),\nand brightness (spectral centroid). However, little work\nhas previously been done on the similarity of rich poly-\nphonic arbitrary sound segments. We seek to label these\nsegments in a way that the perceptually similar ones fall\nin the same region of the space. Redundant segments get\nnaturally clustered, and shallbe codedonly once.\n20 40 60 80 100 12020\n40\n60\n80\n100\n120\n11\n102030405060708090100similar ity %\nFigure 3. Matrix of perceptual self similarities for the\n129 segments of the “Watermelon Man” excerpt of ﬁgure\n1-[A]. White means very similar, and black very dissim-\nilar. Note the black lines, which represent very unique\nsegments, and the white diagonal stripes, which illustrate\npatternredundanciesinthemusic,althoughthemusicwas\nfully performed and not loop-based, i.e., no digital copies\nof the same material.\nOur current representation describes sound segments\nwith 30 normalized dimensions. Because segments are\nsmall and consistent, 25 dimensions are derived from the\naverage amplitude of critical bands of the auditory spec-\ntrogram over time, and 5 are derived from the temporal\nloudness function (normalized loudness at onset and at\noffset,maximumloudness,lengthofthesegment,andrel-\native location of the maximum loudness). A more accu-\nrate representation taking into account the complete dy-\nnamic variations of the spectral envelope, and a dynamic\nprogramming approach is currently under development\n(a collaboration with J.J. Aucouturier from Sony CSL).However, our preliminary results have been satisfactory.\nA very compact, yet perceptually meaningful vector\ndescription of the time structure of musical events (much\nlike an “audio DNA” symbolic sequence) is now estab-\nlished. We can ﬁnally compute the self similarity ma-\ntrix between segments with, for example, a simple mean\nsquared distance measure (see ﬁgure 3). Other distance\nmeasures could very well beconsidered.\n4. CLUSTERING AND COMPRESSION\nSince the space is Euclidean, a simple k-means algorithm\ncan be used for clustering. An arbitrary small number of\nclusters may be chosen depending on the targeted accu-\nracy and compactness. The process is comparable to vec-\ntor quantization: the smaller the number of clusters, the\nsmallerthelexiconandthestrongerthequantization. Fig-\nure 4 depicts the segment distribution for a short audio\nexcerpt at various segment ratios (deﬁned as the number\nof segments retained divided by the number of original\nsegments). Audio examples corresponding to the resyn-\nthesisofthisexcerptatvarioussegmentratiosettings(see\ndescription below), as well as many other examples are\navailable at: www.media.mit.edu/ ∼tristan/ISMIR04/\n20 40 60 80 100 120100\n90\n80\n70\n60\n50\n40\n30\n20\n10% number segments\n1\nFigure 4. Color-coded segment distribution for the 129\nsegments of the “Watermelon Man” piece of ﬁgure 1-[A]\nat various segment ratios. 100% means that all segments\nare represented, while 10% means that only 13 different\nsegments are retained. Note the time-independence of the\nsegment distribution, e.g., here is an example of the dis-\ntribution for the 13 calculated most perceptually relevant\nsegments out of 129:\n33 33 66 66 23 122 23 15 8 112 42 8 23 42 23 15 112 33 33 66 66 66 108 23 8 42 15 8 128 122 23 15 112 33 66\n1156612223158128426612842231511233661158108231584215812812223115112336611586\n1282333115112428128422311511286686610886152342158128122231151128661158612823\n122 8 112 42 8 108 42 23 115 112 8 66 115 66 108 86 122 23 42 122 23 128 122 23 128 128\nCompression is the process by which data is reduced\ninto a form that minimizes the space required to store or\ntransmit it. While modern lossy audio coders efﬁciently\nexploit the limited perception capacities of human hear-\ning in the frequency domain [12], they do not take into\naccount the perceptual redundancy of sounds in the time\ndomain. We believe that by canceling such redundancy,\nwe can reach further compression rates. The segment ra-\ntioindeedhighlycorrelateswiththecompressionratethat\nis gained over traditional audio coders.\nPerceptual clustering allowed us to reduce the audio\nmaterialtothemostperceptuallyrelevantsegments. These\nsegments can be stored along with a list of indexes and\nlocations. Resynthesis of the audio consists of juxtapos-\ning the audio segments from the list at their correspond-\ning locations (see ﬁgure 1-[B]). Note that no cross-fadingbetween segments or interpolations were used in our ex-\namples.\nCurrently, our implementation allows us to deﬁne a\nsegment ratio, regardless of the music content. However,\ntoofewclustersmayresultin musicaldistortions atresyn-\nthesis, i.e., the sound quality is fully maintained, but the\nmusical“syntax”mayaudiblyshiftfromitsoriginalform.\nAmoreusefulsystemwouldinfactadaptitssegmentratio\nto the music being compressed (i.e., the more redundant,\nthe more compressed), and would prefer a perceptual ac-\ncuracycontrol parameter to our static segment ratio set-\nting. This is currently under development.\n5. DISCUSSION AND FUTURE WORK\nReducing audio information beyond current state-of-the-\nart perceptual codecs by structure analysis of its musical\ncontent is arguably a bad idea. Purists would certainly\ndisagree with the beneﬁt of cutting some of the original\nmaterial altogether, especially if the music was entirely\nperformed. There are obviously great risks for music dis-\ntortion currently and the method applies naturally better\ntocertaingenres,includingelectronicmusic,pop,orrock,\nwhererepetitionisaninherentpartofitsqualities. Formal\nexperimentscouldcertainlybedoneonmeasuringthe en-\ntropyof a given piece and the compressibility across sub-\ncategories.\nWebelievethat,witharealadaptivestrategyandanap-\npropriateperceptuallygroundederrorestimation,theprin-\nciple has great potential, primarily in devices such as cell\nphones,andPDAs,wherebitrateandmemoryspacemat-\ntermorethansoundquality. Atthemoment,segmentsare\ncompared and concatenated as raw material. There is no\nattempt to transform the audio itself. However, a much\nmore reﬁned system would estimate similarities indepen-\ndentlyofcertainperceptualdimensions,suchasloudness,\nduration, aspects of equalization or ﬁltering, and possibly\npitch. Resynthesis would consist of transforming para-\nmetrically theretainedsegment(e.g.,amplifying,equaliz-\ning,time-stretching,pitch-shifting,etc.) inordertomatch\nits target more closely. This could greatly improve the\nmusical quality, increase the compression rate, and reﬁne\nthedescription,consequentlyenablingadditionalanalysis\ntasks.\nPerceptualcodershavealreadyprovideduswithavalu-\nablestrategyforestimatingtheperceptuallyrelevantaudio\nsurface (by discarding what we cannot hear). Describing\nmusical structures at the core of the codec is an attrac-\ntive concept that may have great signiﬁcance for many\nhigher-level information retrieval applications, including\nsongsimilarity,genreclassiﬁcation,rhythmanalysis,tran-\nscription tasks, etc.\n6. CONCLUSION\nWe propose a low-rate perceptual description of music\nsignals based on a psychoacoustic approach to segmen-\ntation. The description can be quantized meaningfully byclustering segments, and the audio compressed by retain-\ning only one segment per cluster. Although the technique\nisnotfullydevelopedyet,promisingresultswereobtained\nwith early test examples. We believe that such approach\nhas potential both in the music information retrieval, and\nthe perceptual audio coding domains.\n7. REFERENCES\n[1] Elias Pampalk, Simon Dixon, and Gerhard Widmer,\n“Exploring music collections by browsing different\nviews,” in Proceedings of the International Sympo-\nsium on Music Information Retrieval (ISMIR) , Bal-\ntimore, MD, October 2003.\n[2] Perfecto Herrera, Xavier Serra, and Geoffroy\nPeeters, “Audio descriptors and descriptor schemes\ninthe contextofMPEG-7,” InternationalComputer\nMusic Conference , 1999.\n[3] Martin McKinney and Jeoren Breebaart, “Features\nfor audio and music classiﬁcation,” in Proceed-\ningsoftheInternationalSymposiumonMusicInfor-\nmation Retrieval (ISMIR) , Baltimore, MD, October\n2003.\n[4] Jonathan Foote and Matthew Cooper, “Visualizing\nmusical structure and rhythm via self-similarity,” in\nProceedings International Computer Music Confer-\nence, La Habana, Cuba, 2001.\n[5] Matthew Cooper and Jonathan Foote, “Summariz-\ningpopularmusicviastructuralsimilarityanalysis,”\ninIEEE Workshop on Applications of Signal Pro-\ncessing to Audio and Acoustics , New York, October\n2003.\n[6] George Tzanetakis and Perry Cook, “Multifeature\naudio segmentation for browsing and annotation,”\ninProceedings IEEE Workshop on applications of\nSignal Processing to Audio and Acoustics , October\n1999.\n[7] E. Terhardt, “Calculating virtual pitch,” Hearing\nResearch, vol. 1, pp. 155–182, 1979.\n[8] T. Painter and A. Spanias, “A review of al-\ngorithms for perceptual audio coding of dig-\nital audio signals,” 1997, Available from\nwww.eas.asu.edu/ speech/ndtc/dsp97.ps .\n[9] E.ZwickerandH.Fastl, Psychoacoustics: Factsand\nModels, Springer Verlag, Berlin, 2nd edition, 1999.\n[10] Bob Snyder, Music and Memory: an Introduction ,\nMIT Press, Cambridge, MA,2000.\n[11] J.Grey, “Timbrediscriminationinmusicalpatterns,”\nJournal of the Acoustical Society of America , vol.\n64, pp. 467–472, 1978.[12] Marina Bosi and Richard E. Goldberg, Introduc-\ntiontoDigitalAudioCodingandStandards , Kluwer\nAcademic Publishers, Boston,December 2002."
    },
    {
        "title": "Organizing digital music for use: an examination of personal music collections.",
        "author": [
            "Steve Jones 0002",
            "Sally Jo Cunningham",
            "Matt Jones 0001"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416298",
        "url": "https://doi.org/10.5281/zenodo.1416298",
        "ee": "https://zenodo.org/records/1416298/files/JonesCJ04.pdf",
        "abstract": "Current research on music information retrieval and music digital libraries focuses on providing access to huge, public music collections. In this paper we consider a different, but related, problem:  supporting an individual in maintaining and using a personal music collection. We analyze organization and access techniques used to manage personal music collections (primarily CDs and MP3 files), and from these behaviors, to suggest user behaviors that should be supported in a personal music digital library (that is, a digital library of an individual’s personal music collection).",
        "zenodo_id": 1416298,
        "dblp_key": "conf/ismir/JonesCJ04",
        "keywords": [
            "music information retrieval",
            "personal music collection",
            "digital libraries",
            "access techniques",
            "organizing",
            "supporting users",
            "individual behavior",
            "public music collections",
            "MP3 files",
            "CDs"
        ],
        "content": "ORGANIZING DIGITAL MUSIC FOR USE:AN EXAMINATION OF PERSONAL MUSIC COLLECTIONSSally Jo Cunningham , Matt Jones, Steve JonesDepartment of Computer ScienceUniversity of WaikatoHamilton, New ZealandABSTRACTCurrent research on music information retrieval and musicdigital libraries focuses on providing access to huge,public music collections. In this paper we consider adifferent, but related, problem:  supporting an individualin maintaining and using a personal music collection. Weanalyze organization and access techniques used to managepersonal music collections (primarily CDs and MP3 files),and from these behaviors, to suggest user behaviors thatshould be supported in a personal music digital library(that is, a digital library of an individual’s personal musiccollection).1. INTRODUCTIONThe music retrieval/digital libraries literature has focusedon the problems of supporting large scale, public digitallibraries, nearly to the exclusion of considering howindividuals might organize and access personal collections.This is a surprising omission, considering the popularityof the digital CD and MP3 format; even a child may nowhave a sizeable digital music collection, of a size thatrequires more in the way of access support than a simplelist of filenames or song titles.What features or functions should a music digitallibrary system include, if it is intended to supportindividuals in accessing and managing their own music?We search for clues to answer this question by analyzingthe ways that people currently access and organize theirpersonal music collections. Insight into everyday music-related activities can have practical implications for designof a personal music digital library—that is, a collection ofan individual’s music documents, owned, ‘used’,  andorganized by that person.This paper is organized as follows:  Section 2 discussesprevious research in eliciting music information seekingbehaviors; Section 3 describes the methodology used inthis paper; Section 4 presents the observed musicorganization and usage behaviors, and discusses theirimplications for the design of a personal music digitallibrary; and Section 5 summarizes this work.2. PREVIOUS WORKAt present, there is a dearth of a-priori research on musicinformation behavior. Much of the existing musicinformation retrieval and music digital library research hasbeen technology-driven, and music digital libraries asreported in the research literature are largely developed asproof-of-concept demonstrations of the potential of a giventool or effectiveness of a retrieval algorithm, or are focusedaround providing access to an available set of musicdocuments [8]. Current efforts at studying MIR systemusability focus on user behavior exhibited in specific MIRsystems, for example by examining transaction logs [13].While usability studies can suggest improvements toexisting software, they are impoverished sources ofknowledge about additional features that might be usefulor other information behaviors that could be supported.Research examining human perception and cognition ofmusic is primarily focused on problems in creatingsoftware that can match music elements or extract musicalphrases in such a way as to produce retrieval resultsacceptable to human users [2], or on the factors that mayinfluence a user in creating effective music queries (forexample, in creating a good ‘sung’ query to a query-by-humming interface [15]).A previous, large-scale ethnography of music behavior[4] presents interviews with 41 participants; the focus wasprimarily on the emotional relationships that participantshad with their music, and to a lesser extent on the waysthat people use music in their daily lives.  Theseinterviews are ‘raw’ ethnographies—that is, the data isminimally edited for presentation but is not analyzed toinduce a theory or explanation of the self-reportedbehaviors. This study is not of direct use in suggestingdesign considerations for a music digital library, but couldbe mined for evidence of music-related activities.There is a small but growing body of work on musicbehavior of non-specialists (that is, people who areinteracting with music primarily for personal pleasurerather than professionally). The goal of this research isprimarily to develop an understanding of how toeffectively support access to public collections. In [12], theterminology that participants use to characterize classicalmusic pieces is analyzed and contrasted with formalPermission to make digital or hard copies of all or part of this workfor personal or classroom use is granted without fee provided thatcopies are not made or distributed for profit or commercialadvantage and that copies bear this notice and the full citation onthe first page.© 2004 Universitat Pompeu Fabra.(bibliographic) descriptors. Music queries posted to amusic-focused Usenet newsgroup [7] and to theGoogleAnswers ‘ask an expert’ service [1] provide clues asto the types of music documents that people may beinterested in obtaining from a public music digital libraryand the attributes that people can provide to describe theirmusic information need. The strategies that peoplenatively employ in searching and browsing for music inCD stores and public libraries are detailed in [6].3. METHODOLOGYThe data gathering techniques employed in thisinvestigation were ‘personal ethnographies’, interviews andon-site observations of personal music collections,observations of music store layout and shoppingbehaviors, and focus groups.The bulk of the data was gathered through a projectassigned to students in a third year university human-computer interaction course.  The students were directed toperform a ‘personal ethnography’, in which they examinedtheir own music collections and created a description oftheir collection’s organization, the collection’s contents,when and under what circumstances they use thecollection, and the ways in which they access thecollection (e.g., listening to songs, loaning music tofriends, reading CD inserts, and so forth). In a personalethnography or autoethnography [5], ethnographictechniques of observation and analysis are applied to one’sown experiences; the challenge is to view oneselfobjectively, to see one’s own worldview as freshly aspossible and to then interpret the identified experiences inthe light of applicable theory. The students then performed a similar ethnographicobservation of a friend’s collection and interviewed thefriend to clarify the organizational principles that the friendused in his/her music collection, and to create adescription of how and when that friend uses the musiccollection.In total, the students conducted ten personalethnographies and ten observations/interviews of another.The researchers performed an additional sixobservations/interviews focused on personal musiccollections. We also draw on eight interviews on musicbehavior conducted to support an earlier study [6].Each of the ten students also examined at least onemusic store’s layout, and performed participantobservations of shoppers in the stores.  In an earlier paper[6] we argue that as CD stores are a common source ofmusic for many people, the searching and browsingstrategies observable in these stores can be useful sourcesof data on music information behaviors. In this currentstudy, the music store observations provided a commercialview of how music can be organized to facilitate access(although in this case ease of access is confounded withimpetus to purchase), and additional data on how peoplenavigate a large, public music collection.Three focus groups (one of six individuals, two withthree participants) were also organized by three of thestudents, to solicit experiences with current musicorganization/playing systems such as MP3 players, and tobrainstorm ideas on the functions and features that an idealmusic system would include.Approximately 120 pages of data were gathered from allsources.  The data was analyzed using a grounded theoryapproach [9]. With this technique researchers attempt toapproach the data without prior assumptions, and togenerate theory from the data.  Further qualitative studiesor quantitative experiments can then test the validity of theemergent theory. The aim here is to describe how peoplecurrently organize their music collections, to suggestfeatures and functions that should be included in apersonal music digital library software system.4. HOW PERSONAL MUSIC COLLECTIONS AREORGANIZED AND ACCESSEDThe following sub-sections summarize the characteristics ofmusic collections and the observed ways that participantsorganize, search, browse, and use their personal musiccollections.4.1. Collections vary in size and mediaThe collections varied widely in size—from a single CDowned by an eleven-year old girl (“I don’t get anyallowance at all! I can never ever ever afford anything likeCDs.”) to an estimated seven hundred plus CDsaccumulated over more than a decade. Surprisingly, theorganizational schemes employed were relatively consistentover a range of sizes; once more than a handful of musichad accumulated (Sections 4.4 & 4.5).Music collections included a variety of media:primarily CDs, frequently MP3s (ripped from CDs,emailed from friends, or downloaded from the Internet),and older formats such as cassettes, eight track tapes, andvinyl LPs (with older formats seldom or never accessed;see Section 4.5).At present, music is usually legally obtained as an‘album’—a collection of songs, as released by the artist.Albums may be burned to CD for use as backups, andcompilation CDs may be created, composed of favoritesongs from different albums.  These physical CDs(purchased or burned) are then physically organized, and theorganization and access of these physical collections is thefocus of much of this paper.The organization of individual songs is becoming amore significant activity in personal collections, as theready availability (both legally, through online musicstores such as iTunes or Rhapsody, and illegally, throughmusic sharing services) of single tracks entices musiclovers to obtain only the specific songs in a collection thatthey like. The creation of playlists and compilations[Section 4.6] suggests that even if the album truly does die[3], users will still wish to manipulate groups of songs aswell as to access individual tunes.4.2. Collections are distributedFew participants with music collections of more than anominal size (say, more than 20 CDs) keep their entirecollection in one physical spot.  Collections are generallydivided into the active items (that is, those that see regularor occasional use) and the archival items (music that isseldom or never listened to; for further discussion, seeSection 4.4).The active set is also frequently divided into severalsub-collections:  a small set of very frequently used music,generally placed on top of the CD player or by thecomputer (see Section 4.2); a large set of occasionallylistened to CDs (see Section 4.3) in a CD tower, drawer,or cabinet, beneath or near the main listening device(usually a stereo, occasionally a computer); a set of CDs ina CD wallet, that are played in more than one location andso need to be easily transportable (“[my] CD wallet … isusually situated either underneath my Discman, on top ofthe stereo or in the car…”); a set of CDs stored at theworkplace or at a university computer lab; and if the homecontains several music playing devices, a set of CDs maybe associated with each (for example, CDs in the familyroom, CDs in an individual’s bedroom, and CDs besidethe home computer).  A degree of forward planning isrequired to ensure that the right CDs are in their correctlocations for listening.It is clear that this geographic distribution is almostentirely due to the fact that the CD is a physical object,and so must be toted from place to place, if for no otherreason than to ‘rip’ it and put the copy on an MP3 playeror hard drive. The participants generally viewed havingsubsets of their collection in more than one spot as anannoyance, generally minor but occasionally major, sincelocating a desired CD might involve looking in many,sometimes widely separated, places. Storing a collection in a portable music appliance suchas an MP3 player would finesse this problem ofgeographic distribution, since the entire collection couldbe then be easily transported to wherever the owner wishesto use it. 4.3. Emergent StructureA common realization by the students performing theautoethnographies, and indeed by other participants astheir interviews proceeded, was that even a seeminglydisorganized collection frequently had an implicit structurethat had arisen through use:Before beginning this project I did not think that mycollection was organized in any specific way. Howeverafter examining it … though not organized in atraditional way such as alphabetically or by genre.There is a system, which I have implemented withoutreally realizing it.The most frequently observed emergent structure is asmall stack of CDs that are currently receiving a largeamount of use. This music is typically located close to theplaying device, to make it as easy as possible to quicklyselect the CDs for playing. The last played CD is usuallyplaced on top, so that the less played CDs drift to thebottom of the stack. The size of this set of most activemusic is small—sometimes only three or four CDs,sometimes as many as twenty. Often a limiting factor onsize is that a tall stack is prone to accidentally topple, orthat it looks messy. When the stack becomes too large,then it is pruned and less frequently played CDs arereturned to the main collection.This small stack organization may be associated withthe ‘thrashing’ and ‘sickness’ listening cycle.  A fewparticipants reported that a new CD will be thrashed, “i.e.played over and over, until it eventually looses [sic] itsnovelty”—at which point ‘sickness’ sets in, the CD owner“decides that its [sic] time to listen to something else”,and the CD is moved further from the top of the currentstack, or even put with the main set of CDs.4.4. The Main Active CollectionAs noted in Section 4.1, the bulk of most collections arestored near the primary listening device. Where the mostfrequently listened to items are in a small stack (Section4.2) and the never listened to items are in storage (Section4.4), the remainder—indeed the majority of mostcollections—are only occasionally listened to. A variety oforganizations are used for these items:• by date of purchase, for example with the newest CDsplaced either at the top of a stack or at the end of ashelf• by release or recording date• by artist, with the artists arranged alphabetically• by genre, where the number of genres can be large orsmall (“rap and other”)• by country of origin (e.g., “New Zealand music”)• from most favorite to least favorite• in order of recency in which the CDs have been playedA collection is generally organized into relatively fewcategories (for example, into very broad genres such asJazz and Pop).  A secondary organization may be appliedto each of the broad top-level categories (for example,sorting by artist within genre). The classification schemeis rarely more than two levels deep, so that a linear searchis generally needed to locate a particular CD within acategory or sub-category. This type of loose ordering isprovides acceptable access support, as most collections aresmall enough that CDs can be located relatively quickly.Unfortunately, the structure of most collections tends todeteriorate over time. Few people have the patience toreturn a CD to its proper spot after it is played, guestsmay disturb a collection while browsing it (Section 4.6), aCD tower may be knocked over and hastily shoved backinto place, and so on.  Many participants had abandoned aformer ordering, and were now simply adding in CDs tothe top of a tower as they were purchased (“Once I used tosort by artist, but not any more. It’s too much of a pain.”).Given the initial interest that is shown in ordering adeveloping collection, it seems likely that softwarefacilities that will support the organization of music wouldbe welcomed—if, and this is a big ‘if’, the organizationalmetadata can be quickly and easily added with a new pieceof music. If much effort at all is required beyond a coupleof clicks, then it seems likely that the metadata taggingwill be deferred indefinitely and the digital collection willalso subside into disorganization.4.5. The Archival CollectionOnly two of the participants reported that music which hadfallen from favor was discarded or allowed to drift off(“they usually end up getting misplaced and lost … Idon’t really pay attention to where I put them.”) Typicallyif a personal music collection has been accumulated over asignificant period of time, then it is likely to includeitems that are rarely, if ever, listened to. These form the‘archives’ of a collection, stored away in a closet orotherwise put out of the way.  Sometimes these items arearchived because the media is out of date (eight tracktapes, vinyl LPs), and a player is not available(“approximately seventy vinyl L.P.’s now in permanentstorage due to the lack of a turntable (a.k.a.Gramophone)”.  Other items are simply no longer ofinterest to the owner:  “music that I have grown out of”.Why are these items stored, and not discarded?Sometimes it’s simple inertia on the part of thecollector—an unwillingness to take the time to sort outthe potentially listenable from the completely outgrown.Sometimes the collecting instinct is too strong to resist:Interviewer:  Why are you keeping all of those LPs inyour closet?Husband: I haven’t the faintest idea.Wife:  Because he’s a hoarder!And sometimes the music is kept because of emotionalties or as a memento:  “nothing more than a reminder ofchanges in my personal taste as I have grown.”It appears likely that a digital music collection will alsoeventually include music that the owner no longer wishesto listen to, but is reluctant to delete. An archival facilityis likely to be useful—perhaps semi-automatic, with thesystem suggesting candidate songs that have not beenlistened to in months or years. A secondary use for thissuggestion function would be to remind users about musicthat they had forgotten about, but that they still mightenjoy playing.4.6. Idiosyncratic Genres:  Characterizing Music byIntended UseOne notable way that participants characterized music wasby intended use—that is, based on the event or occasion atwhich they intended to listen to a particular set of music.Music of this type might be listened to as a set ofcomplete CDs, or might consist of individual songspulled together into a playlist or compilation CD.Using the term ‘genre’ loosely, the participantsidentified a diverse set:  programming music (“[technomusic] is great to program to, it keeps you typing, even ifwhat you type is nonsense”); detention music (a highschool teacher described selecting the music she playswhen sitting with students who are serving an in-schoolsuspension: “When I’m working on detention I pull outthe Roger Miller tape, the one with the rankest, mostcountry accent and words, and play it for my hip hop kids,so they never want to serve detention with me again”);music to amuse children (“silly songs for the kids, like‘Please Mr. Custer’, ‘Ahab the Arab’, ‘Transfusion’,‘They’re coming to take me away, haha’”); driving music(“Everything by Jethro Tull, and one or two others [CDs]that rotate”; “[music that will] keep me awake on a latenight drive home from a tiring day on the mountain”);work music (“”more ambient music, not as loud andaggressive as some of the other CDs [in the collection]”),mood altering or matching music (“Browsing through thecollection to select one that suits my mood, either relaxingif at the end of a difficult day, or something exciting if Iam feeling bored”; “[to] cheer me up”; “I only listen tohim [artist] when I’ve split up with someone”), and soforth. One of the more fascinating aspects of this study isthe sheer number of idiosyncratic genres that emerge fromthe interviews and observations.Note that the criteria defining the music intended for aparticular use vary—in the above definitions,programming music includes a well known genre (techno)and detention music is selected as being the antithesis ofhip hop; the first definition of driving music and detentionmusic are identified more or less closely with a particularartist; silly songs for kids have amusing, G-rated lyricsand a sing-able, simple melody; work music is soft, not“aggressive”, and is used as background noise rather thanclosely attended to, while the second type of drivingmusic is loud and fast-paced, to keep a sleepy motoristawake; and  mood music is may be dependent on anynumber of facets, including personal associations withevents experienced while a particular song happened to beplaying (think, for example, of  a couple identifying “oursong” with a romantic mood).A facility to allow a user to create personal genres andto easily add metadata to identify music in these genreswould be useful in a music digital library. This would beparticularly useful as new music is added to a collectionwith an existing set of user-defined categories. It is easy toimagine, however, circumstances in which the collectionowner will miss the opportunity to tag a song with itsappropriate genre—for example, if a new genre is beingadded to a large existing collection. When defining a newpersonal genre, the individual has at hand exemplars ofthat genre; locating additional candidates for that genrecould be supported by a facility that searches within thecollection for ‘more songs like these’. A next step is toclearly identify the musical facets most useful forcharacterizing genres—timbre, instrumentation, rhythm,etc—and to develop interfaces for specifying musicalquery-by-example searches. Research into techniques toautomate the creation of personalized playlists (forexample, by automatically retrieving and ordering songswith features similar to a user-selected ‘seed’ song) showspromise in this direction (for example, [16])Music grouped into such a personal genre may becopied onto one or more compilation CDs. This musicmay be played sequentially by track, if the user has astrong sense how a mood may be developed through aparticular ordering of songs, or the player may be set toplay the songs in random order. Random ordering can adda sense of variety and novelty to a playlist.4.7. Collections May Be SharedWhile individuals have their personal music collections,they may also participate in a shared collection withothers—for example, students sharing accommodationsmay keep a stack of CDs by the living room stereo, orfamilies may have developed a shared collection thateveryone can contribute to and play.A major drawback experienced with shared CDcollections is that they are even more difficult to keep inan intelligible order than individual music collections. Theemergent ‘current favorites’ stack organization fails whenmore than one person is involved; for example, in a familyof seven, the current listening stack by the computerconsists of “the favourite albums of various familymembers and [the stack] is in random order as each usersearches through the stack until they find their currentfavourite disk and return it to the top of the stack whenfinished.” Again, this is a problem tied to the physicalityof CDs; different people could view a set of MP3 files indifferent orderings.Another common form of sharing occurs when guestsare invited to browse a collection to select music to beplayed during their visit. The music is then part of thesocial occasion, listened to together or collectivelyunattended as background to a party. Browsing a friend’smusic collection may provide an opportunity to learn moreabout a new type of music, or to re-think aspects of one’sown tastes.  One student, for example, reported that afterexamining a friend’s collection he re-organized part of hisown collection according to the distinction his friend madebetween New Zealand and international artists.Not all music lovers are comfortable allowing others tobrowse or access their collections.  One notable exceptionwas James, who had the most elaborate and well-maintained organization for his extensive set of CDs:…James is adverse to other people selecting CD’s fromhis collection. For this reason he keeps his collectionin his bedroom to “restrict” access to others. On theodd occasion, for example during a party, that hiscollection is interfered with and the logic disrupted,James will spend time restoring the stand to a state thatis as close as possible to how it was organized beforethe disruption occurred.Perhaps if it were easier to share music and to browsethe collection without running the risk of disturbing itsstructure, collectors such as James would be less averse toexposing their music to others.Another reason cited for reluctance to allow others tobrowse a personal collection is self-consciousness aboutone’s musical tastes:  “My collection also contains …CDs I sometimes play but am embarrassed to possess (see:Chris Isaak).”  One participant even organized his CDs ina set of racks so as to allow him to hide some of hismusic: “I can rotate my rack in a way that “shows off” mybest CDs while partially obscuring the average andembarrassing CDs.” This participant most eloquentlyexpressed the relationship that a music collection can havewith a person’s image:… I feel that my character is partially judged on thecontents of my collection, as I myself consider thecontents of a person’s music collection whenevaluating what type of person they are. From that lastpoint, I can conclude than an important factor of mymusic collection [is] that it displays prominently thebetter/brighter aspects of my personality (see DavidGrey, Coldplay); while partially obscuring the darkerside (see Nine Inch Nails, Deftones).The ability to customize what others see of one’scollection, and how it appears, may be a crucial feature tosome.While allowing others to browse and listen to one’smusic is generally enjoyable, actually loaning a physicalCD is generally avoided: “…I do not lend out CDs, as Imanage to lose and damage them quite well on my own.”This problem would not exist, of course, if a collectionwas entirely held on a computer or music appliance thatsupported easy copying to other digital media—but it isdifficult to imagine that such sharing would be legal in thenear future. In the meantime, those who have copied theirCDs are generally willing to loan the copies (although notthe originals).  Loaning may be seen as a more significantact than simply handing over a bit of plastic, as itinvolves a sharing of an experience that has beenemotionally or intellectually significant, an opportunityfor strengthening bonds between friends, or a chance tobroaden one’s musical horizons:[Lending] allows others to enjoy my music, experiencenew types of music and allows me to share with otherswho have similar tastes. Additionally, it allows me tointroduce lesser-known bands to my friends and allow[sic] them to enjoy the styles of music that I do.4.8. Metadata and Extra-musical Documents AreDesiredGiven the access methods described by participants, theabsolute minimum metadata required to support searchingand browsing in a personal collection appears to be theartist’s name, CD title, and song title.  A simple way toenter these bibliographic details would greatly enhanceusability of a personal music library; ideally, each piece ofmusic would have this metadata associated with it andautomatically loaded into the music digital library withthe song or album itself. (for example, using a servicesuch as Gracenote’s CDDB;     www.gracenote.com    ).Earlier studies of music queries on the Google Answers‘ask an expert’ system and on a music-focused UsenetNewsgroup suggested that a far richer set of metadatawould be desired to enhance the user’s interaction with apersonal music collection. While this present study didnot directly address the question of what metadata userswould like to have available, the observations andinterviews indicate that some users may desire additionalmetadata—for example, one participant wanted the timingsfor albums and songs, to allow him to keep disk changesto a minimum when using a Discman.  The precisemetadata desired is highly likely to vary from user to user,and so as rich a set as possible should be available, withthe user able to select the fields of interest for display. Asan example, consider the spreadsheet developed by one ofthe most avid collectors encountered in this study. Thisindividual entered standard bibliographic details such asartist and CD title, and also details specific to hiscollection such as the year that he acquired each CD.Alas, even this relatively simple set of details proved tooonerous to keep current, and he fell so far behind in dataentry that the spreadsheet was abandoned.The most frequently mentioned additional metadata isthe lyrics of songs. Association of lyrics (or ‘the words’)to songs in a collection is useful in familiarizing oneselfwith a new acquisition: “If the CD is new I willsometimes take the insert out of the case to see if there areany lyrics printed so that I can sing along with the music.”   Several participants reported using their collection toaid in musical performances, either amateur or aprofessional. For singers, printed lyrics are exceptionallyuseful, as it may be difficult to interpret the words as sungin the recording itself. For instrument players who cannotread music, the recording may be repeatedly listened tountil it can be played by ear; the facility to easily repeatdifficult bits until the notes are picked out would behelpful for these users. Musicians who can read musicwould of course benefit from having the score availabletogether with the recording.Enjoyment of a personal music collection may also besignificantly enhanced by ready access to music-relateddocuments, giving background or otherwise augmentingthe listening experience.  One participant describes afriend:For [him], music does not begin and end with listeningto the E.P., but continues onto a complete artistexperience, including investigating the band on theInternet, downloading music videos and investigatingtheir belief and social systems through thoroughinvestigation of their official and unofficial websites.People with such an intense interest in specific artists orgenres are not uncommon; they may participate, forexample, in online ‘interest communities’, as described in[11]. The ability to link specific songs, or groups ofsongs, to the information discovered online would likelybe of great interest to these aficionados. Further, it may beuseful to store documents about artists, albums, genres,etc. that are not directly linked to any music in thecollection; information searching may be conducted priorto purchasing a piece of music, or the information gatheredmay indicate that a particular potential purchase would notbe likely to be enjoyed by the user.4.9. Collections are visual and tactileEarlier work [6] describes the ways that CD cover art canbe useful in searching or browsing a large CDcollection—for example, when searching for a particularCD its cover can be more quickly recognized than its title,and the style of the cover art can provide clues as to aCD’s genre or style. In the personal collection, these cuesare particularly useful in browsing, whether lookingthrough one’s own music to find something to listen to,or when examining a friend’s collection to literally ‘seewhat’s in it’.While some participants expressed no interest in theCDs other than as a container of music (“I don’t really carehow it looks”), the appearance of both individual CDs andthe physical collection as a whole is significant to others.In a personal collection, the CD covers may indeed beused as cover ‘art’: Occasionally CD inserts with effective graphic designare used as decoration, by being U-Tacked to the wall.This allows for ease of lyric recall and adds an aestheticelement to my room.Another participant enjoyed designing CD labels forcompilation CDs that he created, as that allowed him tomake the compilations more visually attractive. Stillanother sorted, stacked, and positioned his collection toprovide an aesthetically pleasing display in his room. Acollection’s appearance as well as its content may besignificant to ‘image management’ [10], how that personpresents him- or herself to the world; yet another reportedthat,To me, the manor [sic] at which I display my music, isalmost as important as the music itself. This is part ofthe reason why I still retain my CD collection (as everysong I have on CD, is also in MP3 format on mycomputer).The sheer physicality of a CD may add to theexperience of collecting music. One participant was askedwhy he browses through CD stores on a regular basis,given that he could simply phone the store to find outwhether it has a CD that he is considering purchasing; hereplied, “it’s important to go and press the flesh, so tospeak”.  Another reported that after purchasing a new CDthat he will “take it round to friends to show it off andmaybe let them hold the case.”  For these people, simplyhaving an MP3 file does not give the same pleasure or thesame sense of ownership, of having a collection, that thepurchase of a physical CD brings.It will be a challenge to the designers of musicappliances or digital libraries, and to the music industry,to bring this sense of joy of possession to the onlinepurchase of an MP3.  One possibility for adding value isto make available other, related documents with a piece ofmusic—for example, images, lyrics, or backgroundinformation about the artist—and then to support the userin viewing or otherwise using these images through themusic digital library. Personalization seems to be the keyhere, for example by allowing the user to easily makebackgrounds or wallpaper, or to set up icons representingthe piece of music.4.10. BrowsingBrowsing through a personal music collection may beextremely undirected, essentially a linear search until apiece of music suddenly strikes the individual as what s/hewants to hear at that moment: “I usually access this part ofmy collection by flicking through (maybe multiple times)my CD wallet looking at each individual CD trying todecide what I feel like listening to.” The end of thisactivity comes not when a predetermined item or type ofmusic is located, but when a song unexpectedly attractsattention and is selected for playing.Browsing involves scanning the CD faces if the CDsare stored in a wallet, or scanning the spines if they arestored in a tower or a stack. Spines are a sparse source ofinformation, giving only the artist and title—although oneparticipant reported that color might aid in recognizing adesired CD. CD faces are not always placed in the walletso that the text is right side up, and copied CDs have onlythe details that the copier has thought to provide (generallyvery little information). Given the variety of features thatmay spark interest in listening to a particular piece—title,artist, genre, rhythm, and mood, to name just a few—arich set of browsing categories is indicated.4.11. CONCLUSIONSA personal music digital library system will need tosupport mix of tasks different from those of a large, publicmusic digital library.  While significant effort is beingexerted in the music information retrieval researchcommunity in developing query-by-humming interfaces,this type of access will likely be less frequently used in apersonal collection. Users who interact with a set of musicthat they themselves have chosen for inclusion willnecessarily be more familiar with its contents than with apublic collection, and so will be less likely to conduct aquery-by-humming search for that song that they can’tquite identify, but can’t get out of their heads. Note theweasel words ‘less likely’; users may be able to identifythe approximate location of a desired song (within aparticular genre, by a given artist, or on a particular CD)but may not know the track number, title, or otheridentifier. In these cases, a query-by-humming interfacemay be useful in selecting the correct item from a set ofcandidates.Commercial services for managing personal collections,such as Itunes (   http://www.apple.com/itunes   ) alreadyinclude a number of the facilities identified in this paper asdesirable in a personal music digital library—for example,searching and browsing support over a rich set of metadata(title, artist, date, lyrics, etc.), facilities for creatingplaylists, and the ability to customize physical media byprinting CD labels.    The insights into personal musicbehavior coming out of this study point to areas in whichthe currently available facilities might be extended; forexample, that the user might be able to easily add newmetadata (and new, idiosyncratic metadata categories, suchas “the person who gave this to me”, “the parties I’veplayed this at”, and so forth).Ease of use is paramount. Members of focus groupswere particularly scathing about the difficulties they hadencountered in using existing music management software:“…perhaps I’m just stupid, but I’m damned if I can makethe thing do what I want. I mean it should be simpleright?”  Learning to use, and using, the system should notinterfere with enjoyment of a music collection.At present there appears to be a tension between designfor small size for portability, and provision of a screendisplay large enough to support searching, browsing, andorganization of a collection.  One focus group wasparticularly emphatic about the need for a larger displayarea than currently exists on MP3 players, and for a crisp,clear display.  Perhaps the current focus amongmanufacturers for designing ever-smaller MP3 players willlead to missed opportunities (for example, the Ipod Minibills itself as “smaller than any cellphone”); small size andportability in an information appliance should not be theprimary goal in the design of an information appliance,but should be secondary as derived from user needs andrequirements of function [14].  In this study, participantsexpressed a keen desire for many functions that wouldrequire a display of at least the size on a PDA, if notlarger; no one referred to small size as important orbeneficial. Not all portable music appliances need to bepocket-sized: in the past, some people took along their“ghetto blasters” to social events; in the future, peoplemay well bring their personal music servers. Given thatmusic collections are typically used in multiple locations,and that people are keen to enjoy a rich interaction withtheir collection whether using a PC or an MP3 player, it isdifficult to envision a tiny-screened music appliancesupporting the full set of features of a music digital librarywith a high degree of usability. Manufacturers who doprovide small appliances meeting these challenges will bewell placed, differentiating themselves in a rapidlyexpanding market.The ‘Smart Playlist’ function of iTunes incorporatesmany of the ordering and selection features encountered inthis study—inclusion by metadata values (such as genre),ordering by attributes such as newness to the collection,and so forth. Additional support for playlist maintenancecould include management of the ‘thrashing’ and‘sickness’ cycle, although it will be a challenging task toset up an appropriate interface for this feature! Given theidiosyncratic nature of genres as described by participants,in creating genre playlists it may be more natural to allowusers to specify a particular song as an example of a genreand then have a playlist automatically generated based onaudio similarity to the example [16], rather than asking theuser to use genre metadata.Given that a music digital library as described in thispaper would be, in the words of one participant, ‘a part ofyour environment’, its appearance would be important toits acceptability. A further desirable extension tocommercial personal music management software/systemswould be more significant ability to personalize  theappearance of individual songs and compilations/playlists,as well as that of the collection as a whole (in a morefundamental manner than through ‘skins’). At present it isnot uncommon for a music collection to have severalowners/users (for example, within a family or in a studentflat); this suggests that a personal music digital libraryshould not be strictly a single user system, but shouldsupport multiple users, each able to personalize thecollection to suit their needs. A collection should also beable to be presented in a form understandable byothers—to allow friends to view the collection, as part ofthe image that an individual presents to the world.5. ACKNOWLEDGMENTSWe wish to thank the students for their work in gatheringethnographic data for this study; the participants for theirpatience and time; and the members of the New ZealandDigital Library Research Group, for their collegial support.6. REFERENCES[1] Bainbridge, D., Cunningham, S. J., and Downie, J.S. “Analysis of queries to a Wizard-of-Oz MIRsystem: Challenging assumptions about what peoplereally want,” Proceedings of the 4th InternationalConference on Music Information Retrieval,Baltimore, MD, USA, 2003, 221-222.[2] Byrd, Donald, and Crawford, Tim. “Problems ofmusic information retrieval in the real world,”Information Processing & Management 38, 2002,249-272.[3] Campbell, Kim.  “The death of the album?” TheChristian Science Monitor, 14 Nov 2003 edition, 14.http://www.csmonitor.com/2003/11114/p15s01-   almp.html   .[4] Crafts, Susan D., Cavicchi, Danial, Keil, Charles, andthe Music in Daily Life Project.  My Music:Explorations of music in daily life. WesleyanUniversity Press, Middletown CT, 1993.[5] Crawford, L. “Personal ethnography”, CommunicationMonographs 63/2, 1996, 158-170.[6] Cunningham, S., Reeves, N., Britland, M.  “Anethnographic study of music information seeking:implications for the design of a music digital library”,Proceedings of the ACM/IEEE Joint Conference onDigital Libraries, 2003, 5-16.[7] Downie, J.S., and Cunningham, S.J.  “Toward atheory of music information retrieval queries:  systemdesign implications”, Proceedings of the 3rdInternational Conference on Music InformationRetrieval, 2002, 299 – 300.[8] Futrelle, Joe, and Downie, J. Stephen,“Interdisciplinary Communities and Research Issuesin Music Information Retrieval”, Proceedings of the3rd International Conference on Music InformationRetrieval, 2002, 215-221.[9] Glaser, B., and Strauss, A.  The Discovery ofGrounded Theory: Strategies for QualitativeResearch.  Chicago, 1967.[10] Goffman, Erving.  The Presentation of Self inEveryday Life. Anchor Press, New York, 1959.[11] Kibby, Marjorie D.  “Home on the page: a virtualplace of music community”, Popular Music 19/1,2000, 91-100.[12] Kim, Ja-Young and Belkin, Nicholas J., “Categoriesof music description and search terms and phrasesused by non-music experts”, Procs. of the 3rdInternational Conference on Music InformationRetrieval, 2002, 209-214.[13] McPherson, J.R. and Bainbridge, D.  “Usage of theMELDEX Digital Music Library”, Proceedings of theInternational Symposium on Music InformationRetrieval, 2001, 19-20.[14] Norman, Don. The Invisible Computer:  why goodproducts can fail, the personal computer is socomplex, and information appliances are thesolution. The MIT Press, Cambridge MA, 1998.[15] Pauws, Steffen. “Effects of song familiarity, singingtraining and recent song exposure on the singing ofmelodies”, Procs. of the 4th International Conferenceon Music Information Retrieval: ISMIR, 2003, 57-64.[16] Pauws, Steffen, and Eggen, B. “PATS:  Realizationand user evaluation of an automatic playlistgenerator”, Procs. of the 3rd International Conferenceon Music Information Retrieval, 2002, 222-230."
    },
    {
        "title": "MusicBLAST - Gapped Sequence Alignment for MIR.",
        "author": [
            "Jürgen Kilian",
            "Holger H. Hoos"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416984",
        "url": "https://doi.org/10.5281/zenodo.1416984",
        "ee": "https://zenodo.org/records/1416984/files/KilianH04.pdf",
        "abstract": "We propose an algorithm, MusicBLAST, for approximate pattern search/matching on symbolic musical data. Mu- sicBLAST is based on the BLAST algorithm, one of the most commonly used algorithms for similarity search on biological sequence data [1, 2]. MusicBLAST can be used in combination with an arbitrary similarity measure (e.g., melodic, rhythmic or combined) and retrieves multiple oc- currences of a given search pattern and its variations. Dif- ferent from many other pattern matching techniques, it can find incomplete and imperfect occurrences of a given pat- tern, and produces a significance measure for the accuracy and quality of its results. Like BLAST — and different from many musical pattern matching approaches — Mu- sicBLAST retrieves heuristically optimised bi-directional alignments searching iteratively in forward and backward direction by starting at a dedicated seed note position of a performance. Keywords: Similarity, pattern matching, retrieval.",
        "zenodo_id": 1416984,
        "dblp_key": "conf/ismir/KilianH04",
        "keywords": [
            "MusicBLAST",
            "approximate pattern search/matching",
            "symbolic musical data",
            "BLAST algorithm",
            "similarity search",
            "arbitrary similarity measure",
            "multiple occurrences",
            "incomplete and imperfect occurrences",
            "significance measure",
            "heuristically optimised bi-directional alignments"
        ],
        "content": "MusicBLAST — GAPPED SEQUENCE ALIGNMENT FOR MIR\nJ¨urgen Kilian\nDarmstadt University of Technology\nFB Informatik / AFS\ne-mail:kilian@noteserver.orgHolger H. Hoos\nUniversity of British Columbia\nDepartment of Computer Science\ne-mail:hoos@cs.ubc.ca\nABSTRACT\nWeproposeanalgorithm,MusicBLAST,forapproximate\npattern search/matching on symbolic musical data. Mu-\nsicBLAST is based on the BLAST algorithm, one of the\nmost commonly used algorithms for similarity search on\nbiologicalsequencedata[1,2]. MusicBLASTcanbeused\nin combination with an arbitrary similarity measure ( e.g.,\nmelodic,rhythmicorcombined)andretrievesmultipleoc-\ncurrencesofagivensearchpatternanditsvariations. Dif-\nferentfrommanyotherpatternmatchingtechniques,itcan\nﬁndincompleteandimperfectoccurrencesofagivenpat-\ntern,andproducesasigniﬁcancemeasurefortheaccuracy\nand quality of its results. Like BLAST — and different\nfrom many musical pattern matching approaches — Mu-\nsicBLAST retrieves heuristically optimised bi-directional\nalignments searching iteratively in forward and backward\ndirectionby startingat adedicated seednote positionof a\nperformance.\nKeywords : Similarity, pattern matching, retrieval.\n1. MOTIVATION\nSearching for a given musical pattern or fragment (based\nonmelodic,rhythmic,orarbitrarytypesofsimilarity)ina\npiece of music or a musical database is a common task in\nthe context of music information retrieval (MIR). Related\nto pattern searching are the problems of pattern induction\nin the context of musical analysis ( i.e.,inferring a global\nstructure,suchasAABA)andthetaskofscore-following.\nTypical queries may be inexact, and consequently, a\nsearch algorithm should be able to support approximate\npatternmatchingandgappedalignmentsbetweenasearch\npattern and given performance data or pieces in a data-\nbase. EspeciallyintheMIRdomain,matchingalgorithms\ntypicallyneedbeabletosearchlargedatabases,andhence\nmust be optimised for performance. A similar situation\narises in the domain of bioinformatics, where the identi-\nﬁcationofapproximatesimilaritiesbetweenbiologicalse-\nquences,suchasgenomicDNA,isanextremelyimportant\ntask; in this ﬁeld, an algorithm called Basic Local Align-\nment Search Tool (BLAST) has become one of the most\nwidely used methods for accomplishing this task.\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2004 Universitat Pompeu Fabra.The similarity between the problem of ﬁnding approx-\nimate matches for a given biological sequence ( e.g.,a ge-\nne) in large biological sequence databases (such as Gen-\nBank) and typical retrieval tasks in the musical domain\nsuggests the adaptation of the basic features of BLAST\nto musical data. In the following we give a short out-\nline about the original BLAST algorithm and explain its\nadaption to retrieval on symbolic musical data. This is\nfollowed by a summary of preliminary results on the per-\nformanceofthenewMusicBLASTalgorithm,andanout-\nlook to future work.\n2. BLAST\nThe gapped BLAST algorithm [2], based on an earlier\nungapped version [1], is a commonly known and widely\nused search tool in biological sequence analysis. Applied\nto amino acid sequences, it works as follows:1\nFirst awindow based similarity matrix of size m×n\nbetweentwoarbitraryaminoacidstringsofsize mandn,\nrespectively, is created. Each entry of the matrix repres-\nentsthesimilaritybetweensubsequencesofthetwoamino\nacid strings with a certain length l;lis also called win-\ndowsize. Thesimilaritybetweentwo singlecharacters of\nthe strings is calculated by using a position speciﬁc score\nmatrixwhich speciﬁes a similarity measure between the\ndifferent amino acids. A commonly used score matrix for\namino acids is the 20×20PAM matrix.2\nNext, alimited number of high-scoring hits within the\nwindow-basedsimilaritymatrix(bestmatchingwindows)\nare selected and used for determining the start positions\n(seeds) for possible alignments. A high-scoring window\nwill only be used for determining a seed position if in the\nsame diagonal of the matrix another high-scoring entry\nappearswithinacertaindistance. Asastartposition(seed),\nany entry of an high-scoring window ( e.g.,the one with\nthe highest score, or the centre of the window as used in\n[2]) might be selected. Starting from the selected seeds, a\nbi-directionalgappedalignments willberetrievedusinga\nperformance optimised version of string matching by dy-\nnamic programming (DP), which produces a cost optim-\nised local alignment.\nAs shown in Figure 1, the two DP tables for the right\nandleftdirectedalignmentsareﬁlledalternatinglyviathe\ninverse diagonals, starting atthe seed position.\n1Whilein[2],BLASTisappliedtoproteinsequences, i.e.,sequences\nof amino acid symbols, in general, it is also frequently applied to DNA\nand RNA sequences.\n2see http://www.cmbi.kun.nl/bioinf/tools/pam.\nshtmlFigure 1. BLAST: iterative ﬁlling of the DP table along\nthe inverse diagonals of the right and left alignment.\nAlthough in principle, the DP procedure guarantees an\noptimal local alignment, because of the use of heurist-\nically selected seed positions and abort criteria, BLAST\nmay not always ﬁnd an optimal alignment; however, in\npractice, it has been found to give an excellent combina-\ntionofaccuracyandefﬁciency. Toachievethisefﬁciency,\nthe number of evaluated cells of each DP table is limited\nby a threshold on the score of cells relative to the cur-\nrent best alignment score. Cells are marked as invalid if\ntheirscorefallsbelowthisthreshold. Forinvalidcellsloc-\nated at the border of the DP table, the remaining cells of\nthe corresponding row or column need not be evaluated.\nDifferent from other efﬁciency improvements to DP ( e.g.,\n[1, 4]), this heuristic does not directly limit the length or\nthe course of the best path of the DP table.\nThe most time consuming task within BLAST is the\ngeneration of gapped alignments; but by using the thresh-\nold optimisations, the complexity for computing a single\nalignmentcantypicallybeminimisedfarbelow O(n·m),\nbecausethenlargepartsoftheDPtablemustnotbeﬁlled.\nFurthermore, the window-based similarity matrix is used\nto limit the number of calls to the DP procedure to prom-\nising start positions.\n3. BLAST ON MUSICAL DATA\nBy using the outline of the original gapped BLAST ap-\nproach and adapting the similarity measures to musical\ndata, it is possible to exploit the performance advantages\nof the BLAST algorithm for pattern retrieval and similar-\nityanalysisinthemusicaldomain. Forcreatingthesimil-\naritymatrixonmusicaldata(aseriesofnotesandchords),\nthe scoring matrix for amino acids (or nucleotides) needs\ntobereplacedbyasimilarityfunctionthatassignsascore\ntoanycombinationoftwonotesorchords. Dependingon\nthepreciseapplicationcontext,thissimilarityfunctioncan\nbe based on any feature of a single note or chord, in par-\nticular: pitch, pitch ratio (interval), duration, inter-onsetinterval (IOI), IOI ratio, or intensity. Analogously to the\nbiological scoring matrix, the similarity function should\ngive positive results for highly similar notes and negat-\nive results for notes that are not similar. In general, any\nsimilarity function can be used that satisﬁes the general\nrequirements of a DP cost function, especially similarity\nfunctionsusedincontextofotherDPbasedmusicalstring\nmatching( e.g.,[4]). Hereeventuallythefunction’soutput\nrange must be normalised to the intended range. By eval-\nuating the IOI ratio and/or the duration ratio instead of\nabsolute IOI and duration, it is possible to allow rhyth-\nmical pattern matching between any combination of un-\nquantised performance data and quantised score data. To\nstart the bi-directional alignment, a seed note needs to be\nselected within each high-scoring window of the similar-\nity matrix; this can be the centre note or any other note\ninside the window ( e.g.,the longest note). For our evalu-\nationweusedthenotewiththehighestsimilarityscoreas\nstart position.\nIt should be noted that standard DP methods for string\nmatching have already been successfully applied to score\nfollowing (see [3]) and MIR ( e.g.,[10]); however, com-\npared to the approach proposed here, these suffer from\nseveral limitations. For example, a bi-directional heur-\nisticalignment(startingfromtheseednoteinforwardand\nbackwarddirection)promisesadvantagesforallscenarios\nwhere two patterns have a high (ungapped) similarity on\na small range of notes only. By starting the alignment\nsearch in both directions from that high similarity region,\nthe number of calculations during the dynamic program-\nming can be decreased signiﬁcantly. In these situations\nstandard approaches such as [10] would require the cal-\nculation of all n·mpositions of the DP table, where the\nquery length mis usually signiﬁcantly smaller than the\nperformance length or database size n. Assuming that\nthe calculation of an alignment will be aborted by the op-\ntimisation features of BLAST if the number of insertions\n(gaps)exceeds k·m. thebi-directionalapproachwouldre-\nquire the calculation of only 2·k·m2entries. The rather\nsimple calculation of the similarity matrix still requires\ntimeO(n·m), but the complexity of the alignment re-\ntrieval depends only on the query length.\nAfter retrieving a set of cost optimised gapped align-\nments—builtbyconcatenatingofpairsofleft-andright-\nalignments — these can be ranked by their signiﬁcance\nor matching quality. This signiﬁcance can be determined\nasthetotalcostalreadycalculatedbytheDPprocedureor\ncalculatedbyapplyingageneralcostfunction,whichneed\nnotsatisfytheconstraintsforDP.Byallowinggapsinthe\nalignment of a search pattern and the performance data,\nthe query will be more robust against extraneous or miss-\ning notes in the pattern and/or the performance data. Us-\ningBLASTonmusicaldata,asdescribedabove,itisalso\npossible to retrieve substring alignments and multiple oc-\ncurrencesofapatterninapieceorperformance. Mongeau\nandSankoffproposedanapproachbasedondynamicpro-\ngramming that allows fragmentation (splitting of notes)\nand consolidation (merging of notes) during retrieving a\ncost optimal alignment [8]. In principle, it should be pos-\nsible to integrate their improvements into BLAST, where\nthis would require a re-design of the advanced aborting\ncriteria of BLAST, which might decrease its efﬁciency.4. PRELIMINARY RESULTS AND DISCUSSION\nTheMusicBLASTalgorithmdescribedintheprevioussec-\ntion has been prototypically implemented within a more\ngeneral system for inferring score level information from\nlowlevelmusicaldatacalled midi2gmn ,whichsupportsas\ninput mechanically or live performed MIDI ﬁles or sym-\nbolic representation in G UIDOMusic Notation. The Mu-\nsicBLASTmodulecanbeusedintwoways: forretrieving\napproximate(completeorpartial)occurrencesofasearch\npatterngivenasasinglevoiceG UIDOﬁle,orforanalysing\nthe overall structure of an arbitrary input ﬁle by perform-\ning a self-similarity analysis. For the example discussed\nin the following, we used a similarity measure that eval-\nuates the absolute (transposition invariant) pitch distance\nbetween two notes; intervals smaller than three semitones\nresulted in positive costs and intervals larger than three\nsemitonesinnegativecosts. Gapswereassignedapenalty\nequal to that of a ﬁve-semitone interval.\n&44X/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚ_X/g1ÚÚÚÚÚÚÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚ\n3aX/g1ÚÚÚÚÚÚbX/g1ÚÚÚÚÚÚn¥‹\n&44X/g1ÚÚÚÚÚÚ.X/g1ÚÚÚÚÚÚJX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚ‹\n3X/g1ÚÚÚÚÚÚJX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚ7\n®X/g2ÛÛÛÛÛÛÛÛ\n#X/g2ÛÛÛÛÛÛÛX/g2ÛÛÛÛÛÛ\nX/g2ÛÛÛÛÛÛÛÛX/g2ÛÛÛÛÛÛ\nX/g2ÛÛÛÛÛÛÛÛ\n‹X/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g2ÛÛÛÛÛÛ\n‹‹‹‹‹‹‹‹‹\n&\n&aX/g1ÚÚÚÚÚÚJaX/g1ÚÚÚÚÚÚJaX/g1ÚÚÚÚÚÚJaX/g1ÚÚÚÚÚÚJ‹\nX/g1ÚÚÚÚÚÚ.X/g1ÚÚÚÚÚÚJX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚ‹\n3aX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚ¥‹\nX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚE/g1ÚÚÚÚÚÚ ‹aX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚJ\n3aX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚaX/g1ÚÚÚÚÚÚJ‹\nX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚ‹‹‹‹‹‹‹‹‹‹‹‹‹\n&\n&5@X/g1ÚÚÚÚÚÚ#X/g1ÚÚÚÚÚÚ____X/g1ÚÚÚÚÚÚÚÚÚÚÚÚÚÚ_X/g1ÚÚÚÚÚÚ\n@X/g1ÚÚÚÚÚÚÚÚÚÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚÚÚÚÚÚÚÚÚÚn\nX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚ ‹\nX/g1ÚÚÚÚÚÚ_X/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚ ‹@X/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚ\n3aX/g2ÛÛÛÛÛÛX/g2ÛÛÛÛÛÛaX/g2ÛÛÛÛÛÛj\n‹\nX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚX/g1ÚÚÚÚÚÚ\nX/g2ÛÛÛÛÛÛX/g2ÛÛÛÛÛÛX/g2ÛÛÛÛÛÛ\n‹aX/g1ÚÚÚÚÚÚ.\n\\\nw\n\\‹‹‹‹‹‹‹‹‹‹\\\\\\\\\\\nFigure 2. Example showing a performance and the ori-\nginal score of Alouette(the same example is discussed\nin [9]). The solid lines connect pairs of notes that have\nbeen aligned by our implementation of the MuscBLAST\nalgorithm. Dashed vertical lines (with arrows on top) in-\ndicatethestartpositionsofthealignmentsincludedinthe\npath of the four overlapping alignments around the main\ndiagonalofthesimilaritymatrix(thetwootheralignments\nare not shown here, see Figure 4). Dashed connections\nbetween notes indicate that here the different alignments\npaths produced different alignments. Note that the align-\nmentatthebeginningoftheﬁrstmeasureisaresultofthe\nfactthatthesimilaritymeasureusedinthisexperimentig-\nnores any rhythmic information.\nFortheinputdatashowninFigure2,ourMusicBLAST\nimplementation selected six high scoring entries of the\nsimilarity matrix, each of which triggered the bi-directio-\nnal DP procedure for retrieving an optimum alignment\nstartingatthebestmatchofthehighscoringwindow(see\nFigure4b). Theevaluationofourtestsshowedthattheuse\noftheBLASToptimisationsforDPdecreasedthenumber\nof cells (of the DP tables) that needed to be evaluated for\ntheretrievalofallsixalignmentsfrom 12 054 (= 41 ·49·6)\nto2 143(17.8%). As shown in the trace of the alignment\npaths in Figure 4c, the start positions of the alignments 3nrlength (l/r/l+r) #cells/length (l/r/l+r)\n17 / 12 / 19 8.8 / 7.9 / 8.3\n25 / 42 / 47 7.5 / 10.17 / 9.85\n325 / 21 / 46 9.32 / 12.23 / 10.67\n413 / 31 / 44 7.31 / 11.9 / 10.52\n536 / 9 / 45 10.86 / 9.78 / 10.64\n62 / 9 / 11 4.5 / 9.0 / 8.18\naverage 1-6: 11.08\naverage 1,2,6: 8.24\nFigure 3. Evaluation of the retrieved alignments for the\nAlouetteexample. Each column shows the values for the\nright-, left-alignments, and the combination of both. The\ncolumn length shows the lengths of the retrieved align-\nments including inserted gaps, the right column gives the\nrelation between the number of evaluated cells of the DP\ntables and the retrieved alignment length.\nto5aresubsetsofthepathofalignment2. Assumingthat\nitispossible(withoutincreasingtheoveralltimecomplex-\nity) to mark start positions as invalid, that are part of an\nalreadyretrievedalignment,thenthenumberofevaluated\ncellswouldhavebeendecreasedtoonly11.8%compared\ntoanon-optimisedDPimplementation. Theroughtlycon-\nstant ratio between the number of evaluated cells and the\nlengthoftheretrievedpatternseeninFigure3givesanin-\ndicationthattheassumptionthatwithusingtheoptimised\nmethod for ﬁlling the DP table in BLAST (marking high\npenaltycellsasinvalid),onaverage,asinglebi-directional\nalignmentcanbecomputedintime O(m),where misthe\nlength of the query.\nOne of the algorithms for pattern matching proposed\nin [5] (named Algorithm 3) appears to be similar to Mu-\nsicBLAST(andgappedBAST,respectively),butthereare\nsome differences between both algorithms: Algorithm 3\nsearchesintwodirectionsforstartandendpointsofadis-\ncovered pattern, but not in the iterative bi-directional way\nusedwithintheBLASTapproach. Inourmodel,theabort\ncriteriondependsonthedistancebetweenlocalalignment\ncosts and the best local alignment costs achieved so far;\nthis seems to be less restrictive than the global threshold\nproposed by Dannenberg and Hu. The dynamic abort cri-\nterionofBLASTavoidstheinsertionsofgapsattheendof\nthe alignment (where they have to be trimmed later). The\nwindow based MusicBLAST selection strategy for start\npositionsofalignmentsseemstobemoreselective(higher\ndiscrimination rate) than the single-note-similarity-based\nstrategy of Algorithm 3. As shown in Figure 4, the win-\ndow based similarity matrix (b) achieves a signiﬁcantly\nhigherdiscriminationrate thanamatrixbased onnote-to-\nnote similarity (a). With the MusicBLAST approach it is\npossible — if desired — to retrieve overlapping matches,\nwhichseemstobenotthecasefortheapproachdescribed\nin [5]. Finally, different from Dannenberg’s and Hu’s im-\nplementation, where a complete second n×mmatrix\nis used for marking invalid cells, MusicBLAST requires\nonly two arrays (with size nandm) for storing the index\nofthelastevaluatedcell(ﬁrstinvalidcell,respectively)in\neach row and each column.\nThere are faster algorithms for exact pattern matching(a)\n (b)\n (c)\nFigure 4.Alouetteexample, score vsperformance, absolute pitch similarity measure: (a) one-by-one similarity matrix;\n(b) similarity matrix (window size 4) the window similarity has been calculated as the product of the single note-to-note\nsimilarityvaluesofallnoteswithininawindow);(c)traceofretrievedalignments. In(a)and(b),thebrightregionshave\nahighsimilarity. In(c),thestartpositionsofthetracesareindicatedbyapairofblacksquares(onefortheleftandonefor\ntherightdirection). Positionsincludedonlyinasinglealignmentarecolouredinlightgrey. Adarkgreycolouredposition\nindicates that it is included in more than one alignment. The slight deviations between the four overlaid traces around\nthe main diagonal (nr 2–5 in Figure 3) are caused by ambiguities of the cost function ( e.g.,for a series of three perfect\nmatches and a single gap, the cost function is independent of the gap position) and the different directions (left/right) in\nwhich they have been passed bythe alignment.\n(such as sufﬁx-tree-based methods with time complexity\nO(m)[7]), but it is somewhat unclear whether the under-\nlying indexing techniques can be applied to approximate\nmatchingapproachesforhandlingquerieswithmissingor\nadditional notes and arbitrary similarity measures. The\nMusicBLASTapproachisrobustagainsttheseerrors,can\nbe adapted to different similarity measures, and can be\nused for quantised and live performed input data.\nBecause of the voice separation (stream segregation)\nfunctionalityavailableinmidi2gmn[6],MusicBLASTcan\nbe applied to non-separated polyphonic data as well as to\nthe single voices (containing notes and chords) obtained\nfrom voice separation. First results from analysing the\nmelodicandrhythmicsimilarityinliveperformedandme-\nchanicalMIDIﬁlesshowedthatMusicBLASTcanretrieve\nsigniﬁcant occurrences of a search pattern and analyse\nself-similarities within a performance with an adequate\nresponse time. A detailed evaluation of MusicBLAST\nwill be conducated in the near future. With the current\ndirection in developing data formats for representing hy-\nbrid combinations audio and symbolic information ( e.g.,\nMPEG7),thenumberofdatabaseswithsymbolicmusical\ndata and the need for performance optimised retrieval al-\ngorithms should increase even more in future. Given the\npopularity and success of BLAST in biological sequence\nanalysis and retrieval, we believe that MusicBLAST has\nsubstantial potential MIRresearch and applications.\n5. REFERENCES\n[1] S. F. Altschul, W. Gish, W. Miller, E. W. Myers, and\nD. J. Lipman. Basic local alignment search tool. Journal\nof Molecular Biology , 215(3):403–410, October 1990.\n[2] S. F. Altschul, T. L. Madden, A. A. Schaffer, J. Zhang,\nZ. Zhang, W. Miller, and D. J. Lipman. Gapped BLASTand PSI–BLAST: a new generation of protein database\nsearch programs. Nucleic Acids Research , 25:3389–3402,\n1997.\n[3] T. Crawford, C. S. Iliopoulos, and R. Raman. String-\nMatching Techniques for Musical Similarity and Melodic\nRecognition. Computing in Musicology , 11:73–100, MIT\nPress, 1998.\n[4] R. B. Dannenberg, An On-Line Algorithm for Real-Time\nAccompaniment. Proceedings of ICMC 1984 , pages 193–\n198, 1984.\n[5] R. B. Dannenberg and N. Hu. Pattern Discovery Tech-\nniques for Music Audio. Proceedings of ISMIR 2002 ,\npages 63–70, 2002.\n[6] J. Kilian and Holger H. Hoos. Voice separation — a local\noptimisationapproach. ProceedingsofISMIR2002 ,pages\n39–46. IRCAM – Centre Pomdidou, Paris, France, 2002.\n[7] K.Lemstr ¨om,A.Haapaniemi,andE.Ukkonen. Retrieving\nmusic — to index or not to index. ACM Multimedia ’98 ,\n1998.\n[8] M. Mongeau and D. Sankoff. Comparison of musical se-\nquences, Computers and the Humanities, 24:161–175,\n1990.\n[9] B. Pardo, W. P. Birmingham. Improved Score Following\nfor Acoustic Performances, Proceedings of ICMC 2002\n[10] L. A. Smith, R. J. McNab, and I. H. Witten. Sequence-\nbased melodic comparison: a dynamic programming ap-\nproach. Computing in Musicology , 11:101–117. MIT\nPress, 1998."
    },
    {
        "title": "Artist Classification with Web-Based Data.",
        "author": [
            "Peter Knees",
            "Elias Pampalk",
            "Gerhard Widmer"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417189",
        "url": "https://doi.org/10.5281/zenodo.1417189",
        "ee": "https://zenodo.org/records/1417189/files/KneesPW04.pdf",
        "abstract": "Manifold approaches exist for organization of music by genre and/or style. In this paper we propose the use of text categorization techniques to classify artists present on the Internet. In particular, we retrieve and analyze webpages ranked by search engines to describe artists in terms of word occurrences on related pages. To classify artists we primarily use support vector machines. We present 3 experiments in which we address the fol- lowing issues. First, we study the performance of our ap- proach compared to previous work. Second, we investi- gate how daily fluctuations in the Internet affect our ap- proach. Third, on a set of 224 artists from 14 genres we study (a) how many artists are necessary to define the con- cept of a genre, (b) which search engines perform best, (c) how to formulate search queries best, (d) which overall performance we can expect for classification, and finally (e) how our approach is suited as a similarity measure for artists. Keywords: genre classification, community metadata, cul- tural features",
        "zenodo_id": 1417189,
        "dblp_key": "conf/ismir/KneesPW04",
        "keywords": [
            "genre classification",
            "community metadata",
            "cultural features",
            "support vector machines",
            "webpages",
            "search engines",
            "artists",
            "support vector machines",
            "experiments",
            "genres"
        ],
        "content": "ARTIST CLASSIFICATION WITH WEB-BASED DATA\nPeterKnees1,Elias Pampalk1,GerhardWidmer1,2\n1AustrianResearchInstitute for Artiﬁcial Intelligence\nFreyung 6/6, A-1010 Vienna, Austria\n2Department ofMedicalCybernetics andArtiﬁcial Intelligen ce\nMedicalUniversity of Vienna,Austria\nABSTRACT\nManifoldapproachesexistfororganizationofmusicby\ngenreand/orstyle. Inthispaperweproposetheuseoftext\ncategorization techniques to classify artists present on t he\nInternet. In particular, we retrieve and analyze webpages\nranked by search engines to describe artists in terms of\nword occurrences on related pages. To classify artists we\nprimarilyusesupport vector machines.\nWepresent3experimentsinwhichweaddressthefol-\nlowing issues. First, we study the performance of our ap-\nproach compared to previous work. Second, we investi-\ngate how daily ﬂuctuations in the Internet affect our ap-\nproach. Third, on a set of 224 artists from 14 genres we\nstudy(a)howmanyartistsarenecessarytodeﬁnethecon-\nceptofagenre,(b)whichsearchenginesperformbest,(c)\nhow to formulate search queries best, (d) which overall\nperformance we can expect for classiﬁcation, and ﬁnally\n(e) how our approach is suited as a similarity measure for\nartists.\nKeywords: genreclassiﬁcation,communitymetadata,cul-\ntural features\n1. INTRODUCTION\nOrganizing music is a challenging task. Nevertheless, the\nvast number ofavailable pieces ofmusicrequiresways to\nstructurethem. Oneofthemostcommonapproachesisto\nclassifymusicintogenres and styles.\nGenreusuallyreferstohigh-levelconceptssuchasjazz,\nclassical, pop, blues, and rock. On the other hand, styles\nare more ﬁne-grained such as drum & bass and jungle in\nthegenreelectronicmusic. Inthispaper,wedonotdistin-\nguish between the terms genre and style. We use the term\ngenreinaverygeneralwaytorefertocategoriesofmusic\nwhich can bedescribed usingthe samevocabulary.\nAlthough even widely used genre taxonomies are in-\nconsistent (for a detailed discussion see, e.g. [18]), they\nPermissiontomakedigitalorhardcopiesofallorpartofthisw orkfor\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributedforproﬁtorcommercialadvantagean dthat\ncopies bearthis notice andthefull citation ontheﬁrst page .\nc/circlecopyrt2004Universitat PompeuFabra.are commonly used to describe music. For example, gen-\nrescanhelplocatedanalbuminarecordstoreordiscover\nsimilar artists. One of the main drawbacks of genres is\nthe time-consuming necessity to classify music manually.\nHowever, recent work (e.g. [25, 29, 2, 15]) suggests that\nthiscan be automatized.\nA closely related topic is overall perceived music sim-\nilarity (e.g. [11, 17, 1, 22, 4]). Although music similarity\nand genre classiﬁcation share the challenge of extracting\ngoodfeatures,theevaluationofsimilaritymeasuresissig -\nniﬁcantlymoredifﬁcult(forrecenteffortsinthisdirecti on\nsee, e.g. [10, 5, 9, 20]).\nSeveral approaches exist to extract features to describe\nmusic. One ﬂexible but challenging approach is to ana-\nlyzetheaudiosignaldirectly. Acomplementaryapproach\nis to analyze cultural features, also referred to as commu-\nnity metadata [28]. Community metadata includes data\nextractedthroughcollaborativeﬁltering,co-occurrence of\nartists in structured, readily available metadata (such as\nCDDB) [19], and artist similarities calculated from web-\nbased data with text-retrieval methods [29, 3, 7]. In the\nfollowing,wewillnotdistinguishbetweenthetermscom-\nmunitymetadata,culturalmetadata,andweb-basedmeta-\ndata.\nIn this paper, we extract features for artists from web-\nbaseddataandclassifytheartistswithsupportvectorma-\nchines (SVMs). In particular, we query Internet search\nengines with artist names combined with constraints such\nas+music+review andretrievethetoprankedpages. The\nretrievedpagestendtobecommonwebpagessuchasfan\npages,reviewsfromonlinemusicmagazines,ormusicre-\ntailers. This allows us to classify any artist present on the\nwebusingtheInternetcommunity’s collectiveknowledge .\nWe present 3 experiments. First, we compare our ap-\nproachtopreviouslypublishedresultsonasetof25artists\nclassiﬁedinto 5genres usingweb-based data [29].\nSecond,weinvestigatetheimpactontheresultsofﬂuc-\ntuations over time of the retrieved content. For this ex-\nperiment we retrieved the top ranked pages from search\nengines for 12 artists every other day for a period of 4\nmonths.\nThird, we classify 224 artists into 14 genres (16 artists\nper genre). Some of these genres are very broad such as\nclassical, others are more speciﬁc such as punk and alter-\nnativerock. WecomparetheperformancesofGoogleand\nYahoo, as well as 2 different constraints on the queries.One of the main questions is the number of artists neces-\nsary to deﬁne a genre such that new artists are correctly\nclassiﬁed. Finally, we demonstrate the possibility of us-\ning the extracted descriptors also for a broader range of\napplications,suchassimilarity-basedorganizationandv i-\nsualization.\nTheremainderofthispaperisorganizedasfollows. In\nthenextsectionwebrieﬂyreviewrelatedwork. InSection\n3 we describe the methods we use. In Section 4 we de-\nscribe our experiments and present the results. In Section\n5we draw conclusions and point out futuredirections.\n2. RELATED WORK\nBasically, related work can be classiﬁed into two groups,\nnamely,artistsimilarityfrommetadata,andgenreclassiﬁ -\ncationfromaudio. First,wereviewmetadata-basedmeth-\nods.\nIn [19] an approach is presented to compute artist and\nsongsimilaritiesfromco-occurrencesonsamplersandra-\ndio station playlists. From these similarities rough genre\nstructures are derived using clustering techniques. The\nﬁndingthatgroupsofsimilarartists(similartogenres)ca n\nbe discovered in an unsupervised manner by considering\nonly cultural datawas further supported by[2].\nWhile the above approaches focus on structured data,\n[28, 3] also consider information available on common\nweb sites. The main idea is to retrieve top ranked sites\nfrom Google queries and apply standard text-processing\ntechniques like n-gram extraction and part-of-speech tag-\nging. Usingtheobtainedwordlists,pairwisesimilarityof\naset of artistsiscomputed.\nTheapplicabilityofthisapproachtoclassifyartistsinto\n5 genres (heavy metal, contemporary country, hardcore\nrap, intelligent dance music, R&B) was shown by Whit-\nman and Smaragdis [29] using a weighted k-NN variant.\nOne of the ﬁndings was that community metadata works\nwell for certain genres (such as intelligent dance music),\nbut not for others (such as hardcore rap). This is dealt\nwith by combining audio-based features with community\nmetadata.\nSince metadata-based and audio signal-based methods\nare not directly related, we just want to give a brief over-\nviewoftheclassiﬁcationcategoriesusedinsystemsbased\nonaudiosignalanalysis. Inoneoftheﬁrstpublicationson\nmusic classiﬁcation, Tzanetakis [26] used 6 genres (clas-\nsic, country, disco, hip hop, jazz, and rock), where clas-\nsic was further divided into choral, orchestral, piano, and\nstring quartet. In [25] this taxonomy was extended with\nblues,reggae,pop,andmetal. Furthermore,jazzwassub-\ndividedinto6subcategories(bigband,cool,fusion,piano ,\nquartet,andswing). Intheexperiments,thesubcategories\nwereevaluatedindividually. Forthe10generalcategories\na classiﬁcation accuracy of 61% was obtained. In [6] a\nhierarchically structured taxonomy with 13 different mu-\nsical genres isproposed.\nOther work usually deals with smaller sets of genres.\nIn[30]and[24]4categories(pop,country,jazz,andclas-\nsic)areusedwithaclassiﬁcationaccuracyof93%,respec-tively 89%. In [15] 7 genres (jazz, folk, electronic, R&B,\nrock,reggae,andvocal)areusedandtheoverallaccuracy\nis74%. Inthepresentpaper,wewilldemonstratehowwe\nachieve upto87% for 14genres.\n3. METHOD\nFor each artist we search the web either with Google or\nYahoo. The query string consists of the artist’s name as\nan exact phrase extended by the keywords +music +re-\nview(+MR)assuggestedin[28]or +music+genre+style\n(+MGS). Without these constraints searching for groups\nsuchasSublimewould resultinmany unrelated pages.\nWeretrievethe50top-rankedwebpagesforeachquery\nand remove all HTML markup tags, taking only the plain\ntext content into account. We use common English stop\nwordliststoremove frequent terms(e.g. a, and, or,the).\nFor each artist aand each term tappearing in the re-\ntrieved pages, we count the number of occurrences tfta\n(term frequency) of term tin documents relating to a.\nFurthermore, we count dftthe number of pages the term\noccurred in (document frequency). These are combined\nusing the term frequency ×inverse document frequency\n(tf×idf)function(weusethe ltcvariant [23]). Theterm\nweight per artistis computed as,\nwta=/braceleftBigg\n(1 + log2tfta) log2N\nd ft,iftfta>0,\n0, otherwise,(1)\nwhere Nis the total number of pages retrieved. Note that\ndue to various reasons (e.g. server not responding) on av-\nerage we were only able to retrieve about 40 from the top\n50ranked pages successfully.\nA web crawl with 200 artists might retrieve more than\n200,000differentterms. Mostoftheseareuniquetyposor\notherwise irrelevant and thus we remove all terms which\ndo not occur in at least 5 of the up to 50 pages retrieved\nper artist. As a result between 3,000 and 10,000 differ-\nent terms usually remain. Note that one major difference\nto previous approaches such as [28, 3] is that we do not\nsearchforn-grams orperformpart-of-speechtagging. In-\nsteadweuseeveryword(withatleast2characters)which\nisnot inastopword list.\nFrom a statistical point of view it is problematic to\nlearn a classiﬁcation model given only a few training ex-\namples (in the experiments below we use up to 112) de-\nscribedbyseveralthousanddimensions. Tofurtherreduce\nthenumberoftermsweusethe χ2testwhichisastandard\nterm selection approach in text classiﬁcation (e.g. [31]).\nTheχ2-value measures the independence of tfrom cate-\ngorycand is computed as,\nχ2\ntc=N(AD−BC)\n(A+B)(A+C)(B+D)(C+D)(2)\nwhere Ais the number of documents in cwhich contain\nt,Bthenumberofdocumentsnotin cwhichcontain t,C\nthe number of documents in cwithout t,Dthe number of\ndocuments not in cwithout t, andNis the total number\nofretrieveddocuments. As Nisequalforallterms,itcanbeignored. Thetermswithhighest χ2\ntcvaluesareselected\nbecause they areleast independent from c.\nNote that the idfpart of tf×idfcan be replaced with\ntheχ2\ntc-valueintextclassiﬁcationassuggestedin[8]. How-\never, inour experiments this didnot improve the results.\nGiven χ2\ntc-values for every term in each category there\nare different approaches to select one global set of terms\nto describe all documents. A straightforward approach is\nto select all terms which have the highest sum or max-\nimum value over all categories, thus using either terms\nwhichperformwellinallcategories,orusingthosewhich\nperform well for one category.\nFor our experiments we select the nhighest for each\ncategory and join them into a global list. We got best\nresults using the top 100 terms for each category, which\ngivesusaglobaltermlistofupto 14×100terms(ifthere\nis no overlap in top terms from different categories). Ta-\nble 2 gives a typical list of the top 100 terms in the genre\nheavymetal/hardrock. Notethatwedonotremovewords\nwhich are part of thequeries.\nWe use the notation Cnto describe the strategy of se-\nlecting nterms per category. In case of C ∞we do not\nremove any terms based on the χ2\ntc-values and thus do\nnot require prior knowledge of which artist is assigned to\nwhich category. (This is of particular interest when using\nthe samerepresentation forsimilaritymeasures.)\nAftertermselectioneachartistisdescribedbyavector\nof term weights. The weights are normalized such that\nthe length of the vector equals 1 (Cosine normalization).\nThisremoves theinﬂuencethatthelengthoftheretrieved\nwebpageswouldotherwisehave. (Longerdocumentstend\nto repeat the same words again and again which results in\nhigher term frequencies.)\nTo classify the artists we primarily use support vec-\ntor machines [27]. SVMs are based on computational\nlearning theory and solve high-dimensional problems ex-\ntremely efﬁciently. SVMs are a particularly good choice\nfor text categorization (e.g. [12]). In our experiments we\nused a linear kernel as implemented in LIBSVM (version\n2.33) withtheMatlab OSU Toolbox.1,2\nIn addition to SVMs we use k-nearest neighbors (k-\nNN) for classiﬁcation to evaluate the performance of the\nextracted features insimilaritybased applications.\nTovisualizetheartistdataspaceweuseself-organizing\nmaps [14] which belong to the larger group of unsuper-\nvised clustering techniques. The SOM maps high-dimen-\nsional vectors onto a 2-dimensional map such that similar\nvectors are located close toeach other.\nWhile the SOM requires a similarity measure it does\nnot require any training data where artists are assigned to\ngenres. Thus, we can use the algorithm to ﬁnd the inher-\nent structure in the data and in particular to automatically\norganize and visualize music collections (e.g. [22, 21]).\nForourexperimentsweusedtheMatlabSOMToolbox.3\n1http://www.ece.osu.edu/˜maj/osu svm\n2http://www.csie.ntu.edu.tw/˜cjlin/libsvm\n3http://www.cis.hut.ﬁ/projects/somtoolboxHeavy Metal\nContemp. Country\nHardcore Rap\nIDM\nR&BGenre Style Review\nFigure 1. Distance matrix for the 25 artists. On the left\nis the matrix published in [29], the other two matrices we\nobtained using tf×idf(with C∞). Black corresponds to\nhigh similarity, white to high dissimilarity. The diagonal s\nof the matrices are set to the largest distance to improve\nthecontrast. Notethattheoveralldifferencesinbrightne ss\nareduetothetwoextremeoutliervaluesincontemporary\ncountry (thus the grayscale in the right matrix needs to\ncover a larger range). However, for k-NN classiﬁcation\nnot the absolute values but merely therank isdecisive.\n1-NN 3-NN 5-NN 7-NN\nWhitman &Smaragdis 68 80 76 72\nGoogle Music Genre Style 96 92 96 92\nGoogle Music Review 80 76 84 80\nTable 1. Results for k-nearest neighbor classiﬁcation for\n25artistsassignedto5genres. Thevaluesarethepercent-\nage of correctly classiﬁed artists computed using leave-\none-out cross validation.\n4. EXPERIMENTS\nWe ran three experiments. First, a very small one with\n25 artists for which genre classiﬁcation results have been\npublished by Whitman and Smaragdis [29]. Second, an\nexperimentovertimewherethesamequeriesweresentto\nsearch engines every second day over a period of almost\n4 months to measure the variance in the results. Third, a\nlargeronewith224artistsfrom14partlyoverlappinggen-\nreswhich are morelikelytoreﬂect a real worldproblem.\n4.1. Whitman & Smaragdis Data\nAlthoughthefocusin[29]wasnotongenreclassiﬁcation\nWhitman and Smaragdis published results which we can\ncompare to ours. They used 5 genres to which they as-\nsigned 5 artists each. The distance matrix they published\nis shown graphically in Figure 1. Using the distance ma-\ntrix we apply k-NN to compare our tf×idfapproach to\ndescribeartistsimilarity. Theclassiﬁcationaccuracies are\nlistedinTable 1.\nAs pointed out in [29], and as can be seen from the\ndistance matrix, the similarities work well for the genres\ncontemporarycountryandintelligentdancemusic(IDM).\nHowever, for hardcore rap, heavy metal, and R&B the re-\nsults are not satisfactory. Whitman and Smaragdis pre-\nsented an approach to improve these by using audio simi-\nlaritymeasures.\nAscanbeseeninTable1ourresultsaregenerallybet-\nter. In particular, when using the constraint +MGS in the\nGoogle queries we only get one or two wrong classiﬁca-\ntions.Lauryn Hill is always misclassiﬁed as hardcore rapSub\n56\nMoz\n56\nMM\n56Em\n24Em\n32RW\n56\nMJ\n56\nDP\n56SO\n54\nSO\n2\nPulp\n56YND\n56\nAK\n21\nAK\n35\nStro\n56\nFigure 2. SOM trained on data retrieved over a period of\nabout 4 months. The full artist names are listed in Fig-\nure 3. The number below the artists abbreviation is the\nnumberofresultsfromdifferentdaysmappedtothesame\nunit.\ninstead of R&B. Furthermore, Outkasttends to be mis-\nclassiﬁed as IDM or R&B instead of hardcore rap. Both\nerrorsare forgivable tosomeextent.\nWhen using +MR as constraint in the Google queries\nthe results do not improve consistently but are on average\n6 percentage points better than those computed from the\nWhitman and Smaragdis similarity matrix. The distance\nmatrix shows that there is a confusion between hardcore\nrap and R&B.\nThe big deviations between the constraint +MGS and\n+MR are also partly time dependent. We study the varia-\ntions over timeinthe next section.\n4.2. Experiment measuring Time Dependency\nIt is well known that contents on the Internet are not per-\nsistent (e.g. [13, 16]) and the top ranked pages of search\nenginesareupdatedfrequently. Tomeasurehowthisinﬂu-\nencesthe tf×idfrepresentationswesentrepeatedqueries\nto Google over a period of almost 4 months every other\nday (56 times)startingon December 18th, 2003.\nWe analyzed 12 artists from different genres (for a list\nseeFigure3). Foreachartistweusedtheconstraints+MR\nor+MGS.Weretrievedthe50toprankedpagesandcom-\nputed the tf×idfvectors (without χ2term selection).\nWe studied the variance by training a SOM on all vec-\ntors. The resulting SOM (using the +MGS constraint) is\nshowninFigure2. Forexample,all56 tf×idfvectorsfor\nSublimeare mapped to the upper left corner of the map.\nThevectorsfor EminemandMarshallMathers arelocated\nnext to each other. Note that there is no overlap between\nartists (i.e. every unit represents at most one artist). Thi s\nindicatesthattheoverallstructureinthedataisnotdrast i-\ncally effected.\nIn addition we measured the variation over time by\ncomputing the following. Given 56 vectors {vad}for an\nartistawhere ddenotes the day the pages were retrieved\nwe compute the artist’s mean vector va. For each artist\nwe measure the daily distance from this mean as dad=\n||va−vad||. Theresultsfor+MGSand+MRareshownin\nFigure3. Wenormalizethedistancessothatthemeandis-\ntance between EminemandMarshall Mathers (Eminem’s\nreal name) equals 1.The results show that in general the deviations from\nthe mean are signiﬁcantly smaller than 1 for all artists.\nHowever,therearesomeexceptions. Forexample,forthe\n+MGSconstraintsomeofthequeriesfor MichaelJackson\nare quite different from the mean. We assume that the\nrecent court case and its attention in the media might be\none of thereasons forthis.\nWe obtained the best results with the smallest variance\nfor the African artist Youssou N’Dour who is best known\nforhishitSevenSeconds(released1994). Thehypothesis\nthat this might be because N’Dour has not done anything\nwhich would have attracted much attention from Decem-\nber 2003 to April 2004 does not hold as this would also\napply, for example, to the alternative ska-punk band Sub-\nlimewho have signiﬁcantly more variance but disbanded\nin1996 after their lead singer died.\nAnother observation is that the variances are quite dif-\nferent for the 2 constraints. For example, Pulphas a very\nlow variance for +MR (median deviation is about 0.45)\nandahighonefor+MGS(mediandeviationisabove0.6).\nHowever, looking at all 12 artists both constraints have a\nsimilaroverall variance.\nWe can conclude that there are signiﬁcant variations\nin the retrieved pages. However, as we can see from the\nSOMvisualizations,thesevariationsaresosmallthatthey\ndonotleadtooverlapsbetweenthedifferentartists. Thus,\nwecanexpectthattheclassiﬁcationresultsarenotgreatly\ninﬂuenced. Furtherresearchisneededtostudytheimpact\nonlarger setsof artists.\n4.3. Experiment with224 Artists\nTo evaluate our approach on a larger dataset we use 14\ngenres (country, folk, rock’n’roll, heavy metal/hard rock ,\nalternative rock/indie, punk, pop, jazz, blues, R&B/soul,\nrap/hiphop, electronic, reggae, and classical). To each\ngenre we assigned 16 artists. The complete list of 224\nartistsisavailable online.4\nFor each artist we compute the tf×idfrepresentation\nas described in Section 3. Table 2 lists the top 100 words\nforheavymetal/hardrockselectedusingthe χ2test. Note\nthatneitheroftheconstraintwords(reviewandmusic)are\ninthelist.\nThe top 4 words are all (part of) artist names which\nwerequeried. However,manyartistswhicharenotpartof\nthequeriesarealsointhelist,suchasPhilAnselmo(Pan-\ntera), Hetﬁeld, Hammett, Trujillo (Metallica), and Ozzy\nOsbourne.\nFurthermore, relatedgroups suchasSlayer,Megadeth,\nIron Maiden, and Judas Priest are found as well as album\nnames (Hysteria, Pyromania, ...) and song names (Para-\nnoid, Unforgiven, Snowblind, St. Anger, ...) and other\ndescriptive words such as evil, loud, hard, aggression and\nheavy metal.\nThe main classiﬁcation results are listed in Table 3.\nTheclassiﬁcationaccuraciesareestimatedvia50holdout\nexperiments. Foreachrunfromthe16artistspergenreei-\n4http://www.oefai.at/˜elias/ismir040.30.40.50.60.70.80.91Alicia Keys  Daft Punk  Eminem  Marshall Mathers  Michael Jackson  Mozart  Pulp  Robbie Williams  Stacie Orrico  Strokes  Sublime  Youssou N'Dour  Genre Style\n0.30.40.50.60.70.80.91Review\nFigure 3. Boxplots showing the variance of the data over time. The x-a xis is the relative distance between the mean per\nartist over time and each day, normalized by the average dist ance between the vectors of EminemandMarshall Mathers .\nTheboxeshavelinesatthelowerquartile,median,andupper quartilevalues. Thewhiskersarelinesextendingfromeach\nend of the box to show the extent of the rest of the data (the max imum length is 1.5 of the inter-quartile range). Outliers\nare datawithvalues beyond theends of thewhiskers.\n100 *sabbath 26 heavy 17 riff 12 butler\n97 *pantera 26 ulrich 17 leaf 12 blackened\n89 *metallica 26 vulgar 17 superjoint 12 bringin\n72 *leppard 25 megadeth 17 maiden 12 purple\n58 metal 25 pigs 17 armageddon 12 foolin\n56 hetﬁeld 24 halford 17 gillan 12 headless\n55 hysteria 24 dio 17 ozzfest 12 intensity\n53 ozzy 23 reinventing 17 leps 12 mob\n52 iommi 23 lange 16 slayer 12 excitable\n42 puppets 23 newsted 15 purify 12 ward\n40 dimebag 21 leppards 15 judas 11 zeppelin\n40 anselmo 21 adrenalize 15 hell 11 sandman\n40 pyromania 21 mutt 15 fairies 11 demolition\n40 paranoid 20 kirk 15 bands 11 sanitarium\n39 osbourne 20 riffs 15 iron 11 *black\n37 *def 20 s&m 14 band 11 appice\n34 euphoria 20 trendkill 14 reload 11 jovi\n32 geezer 20 snowblind 14 bassist 11 anger\n29 vinnie 19 cowboys 14 slang 11 rocked\n28 collen 18 darrell 13 wizard 10 drummer\n28 hammett 18 screams 13 vivian 10 bass\n27 bloody 18 bites 13 elektra 9 rocket\n27 thrash 18 unforgiven 13 shreds 9 evil\n27 phil 18 lars 13 aggression 9 loud\n26 lep 17 trujillo 13 scar 9 hard\nTable 2. The top 100 terms with highest χ2\ntc-values for\nheavy metal/hard rock deﬁned by 4 artists (Black Sab-\nbath, Pantera, Metallica, Def Leppard) using the +MR\nconstraint. Words marked with * are part of the search\nqueries. The values are normalized so that the highest\nscore equals 100.\nther 2, 4, or 8 are randomly selected to deﬁne the concept\nof thegenre. The remaining are usedfor testing.\nThe reason why we experiment with deﬁning a genre\nusing only 2 artists is the following application scenario.\nA user has an MP3 collection structured by directories\nwhichreﬂectgenrestosomeextent. Foreachdirectorywe\nextracttheartistnamesfromtheID3tags. AnynewMP3s\nadded to the collection should be (semi)automatically as-\nsignedtothedirectorytheybestﬁtintobasedontheartist\nclassiﬁcation. Thus, we are interested in knowing howwell thesystemcan workgiven only fewexamples.\nUsingSVMsand8artiststodeﬁneagenrewegetupto\n87% accuracy which is quite impressive given a baseline\naccuracyofonly7%. GenerallytheresultsforGoogleare\nslightlybetterthanthoseforYahoo. For+MGStheresults\nof Yahoo are signiﬁcantly worse. We assume that the rea-\nson is that Yahoo does not strictly enforce the constraints\nif many search terms are given. In contrast to the ﬁndings\nofthedatasetwith25artists(Section4.1)weobservethat\nthe+MRconstraintgenerallyperformsbetterthan+MGS.\nWewouldalsoliketopointoutthatusingonly2artists\nto deﬁne a genre we get surprisingly good results of up\nto 71% accuracy using SVMs with C 100. Performance\nis only slightly worse when using the top 200 words per\ngenre (C 200) or even when not using the χ2test to select\nterms(C∞).\nThe confusion matrix for an experiment with Google\n+MR (SVM, t4, C 100) is shown in Figure 4. Classical\nmusic is not confused with the other genres. In contrast\nto the results published in [29] hip hop/rap is also very\nwell distinguished. Some of the main errors are that folk\nis wrongly classiﬁed as rock’n’roll, and punk is confused\nwithalternativeandheavymetal/hardrock(alldirections ).\nBoth errors “make sense”. On the other hand, any confu-\nsionbetweencountryandelectronic(evenifonlymarginal)\nneeds further investigation.\nIn addition to the results using SVMs we also investi-\ngatedtheperformanceusingk-NN(without χ2cut-off)to\nestimate how well our approach is suited as a similarity\nmeasure. Similarity measures have a very broad applica-\ntion range. For example, we would like to apply a web-\nbasedsimilaritymeasuretoourislandsofmusicapproach\nwere we combine different views of music for interactive\nbrowsing [21]. Accuracies of up to 77% are very encour-\naging. However, one remaining issue is the limitation to\ntheartistlevel,whilewewouldpreferamoreﬁne-grained\nsimilaritymeasure atthe song level.\nTofurthertesttheapplicabilityasasimilaritymeasure,\nwe trained a SOM on all artists (Figure 5). We did notGoogle Yahoo\nGenre Style Review Genre Style Review\nt2 t4 t8 t2 t4 t8 t2 t4 t8 t2 t4 t8Mean\nSVM C 10070±3.780±2.986±2.371±4.381±3.187±3.061±4.372±3.179±2.965±4.978±2.987±2.676±3.3\nSVM C 20067±3.878±3.085±2.768±4.379±3.386±2.656±4.469±3.378±3.062±4.575±3.285±3.274±3.4\nSVM C∞67±3.877±3.184±3.069±4.779±3.584±2.756±4.867±3.774±3.165±4.976±2.185±3.173±3.5\n3-NN C∞54±6.966±4.673±3.856±4.668±4.374±3.339±6.151±5.658±3.951±6.962±4.771±3.760±4.9\n7-NN C∞39±7.767±3.775±3.043±8.268±4.577±3.731±9.051±5.562±3.740±8.563±5.273±3.757±5.5\nMean 59±5.274±3.581±3.061±5.275±3.781±3.049±5.762±4.270±3.357±5.971±3.680±3.3\nMean 71±3.9 73 ±4.0 60±4.4 69 ±4.3\nTable3. Classiﬁcationresultsonthe224artistdataset. Theﬁrstv alueineachcellisthemeanaccuracyfrom50holdout\nexperiments. The second value is the standard deviation. Va lues are given in percent. The number of artists (size of the\ntraining set)used todeﬁne agenre islabeled witht2,t4, t8.\nCountry\nFolk\nJazz\nBlues\nR&B/Soul\nHeavy Metal\nAlt/Indie\nPunk\nRap/HipHop\nElectro\nReggae\nClassic\nRock n' Roll\nPop\nCountry Folk Jazz Blues R&B/S HM A/I Punk R/HH Elctr ReggaeClass R&R Pop91\n±7\n11\n±8\n4\n±42\n±6\n55\n±22\n1\n±2\n2\n±4\n1\n±3\n1\n±2\n3\n±6\n1\n±293\n±41\n±3\n88\n±14\n1\n±3\n2\n±55\n±8\n4\n±4\n4\n±6\n75\n±12\n1\n±2\n1\n±3\n1\n±2\n1\n±3\n3\n±6\n2\n±6\n2\n±52\n±5\n75\n±16\n7\n±10\n8\n±11\n5\n±4\n1\n±22\n±4\n9\n±7\n9\n±11\n57\n±23\n11\n±12\n5\n±7\n1\n±4\n8\n±10\n5\n±61\n±3\n6\n±11\n13\n±14\n69\n±14\n1\n±2\n2\n±5\n1\n±34\n±7\n91\n±9\n4\n±5\n1\n±31\n±3\n1\n±3\n12\n±9\n4\n±4\n94\n±8\n2\n±4\n1\n±3\n2\n±486\n±9\n100\n±02\n±4\n15\n±16\n1\n±2\n8\n±14\n14\n±9\n2\n±4\n1\n±4\n1\n±2\n74\n±15\n1\n±31\n±3\n1\n±3\n6\n±8\n6\n±8\n8\n±10\n10\n±7\n3\n±6\n2\n±4\n1\n±3\n87\n±8Real Class\nClassification Results\nFigure4. ConfusionmatrixofclassiﬁcationresultsusingaSVMwith Google+MRC 100datausing4artistspercategory\nfor training. Values are given in percent. The lower value in each box is the standard deviation computed from 50 hold\nout experiments.\nuse the χ2cut-off as this would require knowledge of the\ngenre of each artist which we do not assume to be given\nintheislandsofmusicscenario. TheSOMconﬁrmssome\nof the results from the confusion matrix. Classic (upper\nright)isclearlyseparatedfromallothers. Jazzandreggae\nare also very well distinguished. Heavy metal, punk, and\nalternative overlap very strongly (lower left). Folk is ver y\nspread out and overlaps with many genres. An interest-\ning characteristic of the SOM is the overall order. Notice\nthat blues and jazz are located closer to classical music\nwhile electronic is close to alternative. Furthermore, the\nSOMoffersanexplanationoftheconfusionbetweenelec-\ntronic and folk. Inparticular, 2artistsfrom electronic an d\nfromfolktogetherwithartistsfrommanyothergenresare\nmapped to the same unit (in the 2nd row, 1st column).\nThe main reason for this is that some of the artists we as-\nsignedtoeachgenrearevery“mainstream”andthustheirtf×idfrepresentations are more similar to other main-\nstream artists than to typical members of their genre that\narenot sopopular.\n5. CONCLUSIONS\nInthispaperwehavepresentedanapproachtoclassifying\nartists into genres using web-based data. We conducted 3\nexperimentsfromwhichwegainedthefollowinginsights.\nFirst, we showed that our approach outperformed a previ-\nously published approach [29]. Second, we demonstrated\nthat the daily ﬂuctuations in the Internet do not signiﬁ-\ncantly interfere with the classiﬁcation. Third, on a set of\n224 artists from 14 genres we showed that classiﬁcation\naccuracies of 87% are possible. We conclude that in our\nexperiments Google outperformed Yahoo. Furthermore,REGGAE (14)\naltindie (3)\nrocknroll (3)\nfolk (2)\npunk (2)\nelectro (2)\ncountry (1)\npop (1)\naltindie (5)\npunk (4)\nrocknroll (2)\nHEAVY (15)\nPUNK (9)\nALTINDIE (6)country (1)\nrnbsoul (1)\nFOLK (5)\naltindie (1)\nelectro (1)\npop (1)\nelectro (2)\naltindie (1)\npunk (1)COUNTRY (14)\nfolk (2)\nrocknroll (1)\nrnbsoul (4)\nfolk (2)\nPOP (5)\nELECTRO (10)\npop (1)ROCKNROLL (8)\nfolk (2)\nblues (1)\nrnbsoul (1)\nrnbsoul (3)\njazz (1)\npop (1)\nRNBSOUL (5)\npop (2)\nRAPHIPHOP (13)\npop (1)BLUES (14)\nfolk (1)\nrnbsoul (1)\nrocknroll (1)\nblues (1)\nraphiphop (2)\nreggae (1)\npop (1)CLASSIC (16)\nrocknroll (1)\nJAZZ (15)\npop (3)\nfolk (2)\nrnbsoul (1)\nheavy (1)\nraphiphop (1)\nelectro (1)\nreggae (1)\nFigure 5. SOM trained on 224 artists. The number of artists from the re spective genre mapped to the unit is given in\nparentheses. Upper case genre names emphasize units which r epresent many artistsfromone genre.\nwe achieved best resultsusing theconstraint +music+re-\nviewin the search engine queries. A particularly inter-\nesting insight we obtained was that deﬁning a genre with\nonly 2 artists results in accuracies of up to 71%. Finally,\nwedemonstratedthatthefeaturesweextractarealsowell\nsuitedfor direct useinsimilaritymeasures.\nNevertheless, with the web-based data we face several\nlimitations. Oneofthemainproblemsisthatourapproach\nheavilyreliesontheunderlyingsearchenginesandtheas-\nsumption that the suggested webpages are highly related\nto the artist. Although some approaches to estimating the\n“quality” of a webpage have been published (e.g. [3]), it\nis very difﬁcult to identify off-topic websites without de-\ntailed domain knowledge. For example, to retrieve pages\nfor the band Slayer, we queried Google with ”slayer”\n+music +genre +style and witnessed unexpectedly high\noccurrences of the terms vampireandbuffy. In this case\na human might have added the constraint −buffyto the\nquerytoavoidretrievingsitesdealingwiththesoundtrack\nof the tv-series “Buffy The Vampire Slayer”. Similarly,\nas already pointed out in [28], bands with common word\nnameslike WarorTexasaremoresusceptibletoconfusion\nwithunrelated pages.\nFurthermore,asartistsorbandnamesoccuronallpages,\nthey have a strong impact on the lists of important words\n(e.g. see Table 2). This might cause trouble with band\nnames such as Daft Punk , where the second half of the\nname indicates a totally different genre. In addition, also\nartists with common names can lead to misclassiﬁcation.\nFor example, if the genre pop is deﬁned through Michael\nJacksonandJanet Jackson , any page including the term\njackson(such as those from country artist Alan Jackson )\nwill be more likely to be classiﬁed as pop. A variation of\nthesameproblemis,e.g,rapartist Nelly,whosenameisa\nsubstringofethno-popartist NellyFurtado . Oneapproachtoovercometheseproblemswouldbetousenounphrases\n(asalreadysuggestedin[28])ortotreatartistnamesnotas\nwords but as special identiﬁers. We plan to address these\nissues in future work using n-grams and other more so-\nphisticatedcontentﬁlteringtechniquesassuggestedin[3 ].\nFurther, we plan to investigate classiﬁcation into hi-\nerarchically structured genre taxonomies similar to those\npresented in [6]. Other plans for future work include us-\ning the information from the Google ranks (the ﬁrst page\nshouldbemorerelevantthanthe50th),experimentingwith\nadditionalqueryconstraints,andcombiningtheweb-based\nsimilarity measure with our islands of music approach to\nexplore different views of musiccollections [21].\n6. ACKNOWLEDGEMENTS\nThis research was supported by the EU project SIMAC\n(FP6-507142) and by the Austrian FWF START project\nY99-INF. The Austrian Research Institute for Artiﬁcial\nIntelligenceissupportedbytheAustrianFederalMinistry\nfor Education, Science, and Culture and by the Austrian\nFederal Ministry for Transport, Innovation, and Technol-\nogy.\n7. REFERENCES\n[1] J.-J. Aucouturier and F. Pachet, “Music similarity\nmeasures: What’s the use?,” in Proc. of the Interna-\ntional Conf. onMusicInformation Retrieval , 2002.\n[2] J.-J. Aucouturier and F. Pachet, “Musical genre: A\nsurvey,” Journal of New Music Research , vol. 32,\nno. 1, 2003.[3] S. Baumann and O. Hummel, “Using cultural meta-\ndata for artist recommendation,” in Proc. of Wedel-\nMusic, 2003.\n[4] A. Berenzweig, D. Ellis, and S. Lawrence, “Anchor\nspace for classiﬁcation and similarity measurement\nof music,” in Proc. ot the IEEE International Conf.\non Multimediaand Expo , 2003.\n[5] A.Berenzweig,B.Logan,D.Ellis,andB.Whitman,\n“A large-scale evaluation of acoustic and subjective\nmusic similarity measures,” in Proc. of the Interna-\ntional Conf. on MusicInformation Retrieval , 2003.\n[6] J.J.Burredand A.Lerch, “AHierarchical Approach\ntoAutomaticMusicalGenreClassiﬁcation,”in Proc.\nof the International Conf. on Digital Audio Effects ,\n2003.\n[7] W.W. Cohen and Wei Fan, “Web-collaborative ﬁl-\ntering: Recommending musicbycrawlingtheweb,”\nWWW9 / Computer Networks , vol. 33, no. 1-6, pp.\n685–698, 2000.\n[8] F. Debole and F. Sebastiani, “Supervised term\nweighting for automated text categorization,” in\nProc. of the ACM Symposium on Applied Comput-\ning, 2003.\n[9] J.S. Downie, “Toward the scientiﬁc evaluation of\nmusicinformationretrievalsystems,” in Proc.ofthe\nInternational Conf. on Music Information Retrieval ,\n2003.\n[10] D. Ellis, B. Whitman, A. Berenzweig, and S.\nLawrence, “The quest for ground truth in musical\nartist similarity,” in Proc. of the International Conf.\non MusicInformationRetrieval , 2002.\n[11] J.T. Foote, “Content-based retrieval of music and\naudio,” in Proc. of SPIE Multimedia Storage and\nArchiving Systems II ,1997, vol. 3229.\n[12] T. Joachims, “Text categorization with support vec-\ntor machines: Learning with many relevant fea-\ntures,” in Proc. of the European Conf. on Machine\nLearning, 1998.\n[13] W.Koehler, “Alongitudinalstudyofwebpagescon-\ntinued: A consideration of document persistence,”\nInformation Research , vol. 9,no. 2, 2004.\n[14] T. Kohonen, Self-Organizing Maps , Springer, 2001.\n[15] M.F. McKinney and J. Breebaart, “Features for au-\ndioandmusicclassiﬁcation,” in Proc.oftheInterna-\ntional Conf. on MusicInformation Retrieval , 2003.\n[16] S.LawrenceandC.L.Giles, “AccessibilityofInfor-\nmation on the Web,” in Nature, vol. 400, no. 6740,\npp. 107–109, 1999.\n[17] B.LoganandA.Salomon, “Amusicsimilarityfunc-\ntion based on signal analysis,” in Proc. of the IEEE\nInternational Conf. on Multimedia and Expo , 2001.[18] F. Pachet and D. Cazaly, “A taxonomy of musical\ngenres,” in Proc. of RIAO Content-Based Multime-\ndia Information Access , 2000.\n[19] F. Pachet, G. Westerman, and D. Laigre, “Musical\ndata mining for electronic music distribution,” in\nProc. of WedelMusic , 2001.\n[20] E.Pampalk,S.Dixon,andG.Widmer, “Ontheeval-\nuation of perceptual similarity measures for music,”\ninProc. of the International Conf. on Digital Audio\nEffects, 2003.\n[21] E. Pampalk, S. Dixon, and G. Widmer, “Explor-\ning music collections by browsing different views,”\nComputer Music Journal , vol. 28, no. 3, pp. 49–62\n2004.\n[22] E. Pampalk, A. Rauber, and D. Merkl, “Content-\nbased organization and visualization of music\narchives,” in Proc. of ACM Multimedia , 2002.\n[23] G. Salton and C. Buckley, “Term-weighting ap-\nproaches in automatic text retrieval,” Information\nProcessing and Management , vol. 24, no. 5, pp.\n513–523, 1988.\n[24] Xi Shao, C. Xu, and M.S. Kankanhalli, “Unsu-\npervised classiﬁcation of music genre using hidden\nmarkov model,” in Proc. of the IEEE International\nConf. of Multimedia Expo , 2004.\n[25] G. Tzanetakis and P. Cook, “Musical genre clas-\nsiﬁcation of audio signals,” IEEE Transactions on\nSpeech and Audio Processing , vol. 10, no. 5, pp.\n293–302, 2002.\n[26] G.Tzanetakis,G.Essl,andP.Cook, “Automaticmu-\nsical genre classiﬁcation of audio signals,” in Proc.\nof the International Symposium on Music Informa-\ntion Retrieval , 2001.\n[27] V.Vapnik, StatisticalLearningTheory , Wiley,1998.\n[28] B. Whitman and S. Lawrence, “Inferring descript-\nionsandsimilarityformusicfromcommunitymeta-\ndata,” inProc. of the International Computer Music\nConf., 2002.\n[29] B.WhitmanandP.Smaragdis, “Combiningmusical\nand cultural features for intelligent style detection,”\ninProc. of the International Conf. on Music Infor-\nmation Retrieval , 2002.\n[30] C. Xu, N.C. Maddage, Xi Shao, and Qi Tian, “Mu-\nsical genre classiﬁcation using support vector ma-\nchines,” in Proc.oftheInternationalConf.ofAcous-\ntics,Speech & Signal Processing , 2003.\n[31] Y.YangandJ.O.Pedersen, “Acomparativestudyon\nfeature selection in text categorization,” in Proc. of\nthe International Conf. on Machine Learning , 1997."
    },
    {
        "title": "Sound, Music and Textual Associations on the World Wide Web.",
        "author": [
            "Ian Knopke"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416144",
        "url": "https://doi.org/10.5281/zenodo.1416144",
        "ee": "https://zenodo.org/records/1416144/files/Knopke04.pdf",
        "abstract": "Sound files on the World Wide Web are accessed from web pages. To date, this relationship has not been ex- plored extensively in the MIR literature. This paper details a series of experiments designed to measure the similar- ity between the public text visible on a web page and the linked sound files, the name of which is normally unseen by the user. A collection of web pages was retrieved from the web using a specially-constructed crawler. Sound file information and associated text were parsed from the pages and analyzed for similarity using common IR techniques such as TFIDF cosine measures. The results are intended to be used in the improvement of a web crawler for audio and music, as well as for MIR purposes in general.",
        "zenodo_id": 1416144,
        "dblp_key": "conf/ismir/Knopke04",
        "keywords": [
            "Sound files",
            "Web pages",
            "Similarity measures",
            "TFIDF cosine",
            "Crawler",
            "MIR literature",
            "IR techniques",
            "Public text",
            "Audio and music",
            "Web access"
        ],
        "content": "SOUND, MUSIC AND TEXTU ALASSOCIA TIONS\nONTHE WORLD WIDE WEB\nIanKnopk e\nMusic Technology\nMcGill University\nian.knopk e@mail.mcgill.ca\nABSTRA CT\nSound \u0002les ontheWorld WideWebareaccessed from\nweb pages. Todate, thisrelationship hasnotbeen ex-\nplored extensi velyintheMIR literature. This paper details\naseries ofexperiments designed tomeasure thesimilar -\nitybetween thepublic textvisible onaweb page andthe\nlinkedsound \u0002les, thename ofwhich isnormally unseen\nbytheuser.Acollection ofweb pages wasretrie vedfrom\ntheweb using aspecially-constructed crawler.Sound \u0002le\ninformation andassociated textwere parsed from thepages\nandanalyzed forsimilarity using common IRtechniques\nsuch asTFIDF cosine measures. Theresults areintended\ntobeused intheimpro vement ofaweb crawler foraudio\nandmusic, aswell asforMIR purposes ingeneral.\n1.INTR ODUCTION\nAshasbeen previously indicated, theWorld WideWeb\nisnotahomogeneous, strictly-or ganized structure [7].\nWhile much ofitmay appear random at\u0002rstglance, there\nisoften anelement ofsimilarity incontent between many\nlinkedweb resources. This isinfactafundamental el-\nement ofthewayinwhich theentire web functions. It\nisprecisely theembedding ofcontent descriptions within\nhyperte xtlinks thatmakesweb browsing possible, andis\noneofthemajor in\u0003uences behind thepopular growthof\ntheInternet.\nThemain purpose ofthispaper istoaddress therela-\ntionship between audio andmusic materials ontheweb,\nandtheassociated web pages thatlinktothem. While this\nrelationship hasbeen studied tosome degree between hy-\nperlink edweb pages, thestructure ofaudio information\nontheweb remains largely unstudied atthepresent time.\nTodate, verylittle attention hasbeen paid totherela-\ntionship between web pages andtheaccompan ying audio\n\u0002les. This issurprising, considering theeffectofP2Psys-\ntems andelectronic deliverysystems ontheentire music\nbusiness. Withthesuccess ofiTunes andother Internet-\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forpro\u0002t orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation onthe\u0002rstpage.\nc°2004 Universitat Pompeu Fabra.based deliverysystems, itseems likelythattheweb will\nbecome animportant deliverysystem formusic andaudio\ninthenear future.\nThehyperte xt/audio relationship isalsopotentially valu-\nable forMusic Information Retrie val(MIR) research, in\nthesense thatweb pages with links tosound \u0002les form\nakind ofuser-created annotation thatisnotavailable in\nmanyother MIR conte xts.This ispotentially anenormous\nsource ofinformation fortheMIR \u0002eld which hasnotgen-\nerally been exploited todate, perhaps because ofalack of\ntools with which tostudy theproblem.\nThis paper isdivided intoseveralsections. After thein-\ntroduction, themotivations behind thestudy andhowthey\nrelate tobuilding afocused web crawler arerelated, as\nwell asprevious andrelated research. Themethod used in\nconducting theexperiment isoutlined, followed byadis-\ncussion oftheresults. Some directions forfuture research\narealsogiven.\n2.MOTIVATION\nOne oftheprimary uses ofthis information isforthe\ncreation ofafocused web crawler [5]. General purpose\nsearch engines, such asGoogle orLycos, havetradition-\nallytended tousebreadth-\u0002rst crawling techniques, fol-\nlowing alllinks inturn from each web page, then follow-\ningalllinks from each child, andsoforth. This technique\nhasbeen showntoeffectively\u0002nd everypage, aswell\nasgiving good results insituations where allinformation\nmust beindexedforgeneral purpose queries. Eventually\nabreadth-\u0002rst crawlwillcapture allpages ontheweb, al-\nthough thismay takeconsiderable time andresources.\nHowever,thismay notbedesirable incases where en-\ngines aredesigned toconcentrate onaspeci\u0002c topic, class,\nortype ofinformation. Analternate procedure knownas\nfocused web crawling works byprioritizing some links\noverothers andfollowing those links which aremost likely\ntoproduce thebest possible results earlier inthecrawl-\ningprocess. This reduces thenumber ofpages thatmust\nbeparsed togivebetter quality indexesofpages contain-\ninginformation about aparticular topic, andultimately\nenhancing theenduser experience. This isparticularly\nvaluable inthecase ofaweb crawler designed tolook for\nandanalyze audio andmusic \u0002les, which require consid-\nerable resources beyond what isusually required byanHTML-only web crawler.Considering thesparseness of\naudio information ontheweb, anunderstanding ofthere-\nlationship between HTML andtheaccompan ying audio\n\u0002les may considerably reduce theamount ofweb infor -\nmation which acrawler must accommodate.\nAnother important consideration isthefreshness of\ntheinformation that hasbeen indexed. Not only isthe\nweb growing atarapid rate, butmuch oftheinformation\nischanging constantly asweb pages arealtered ormoved\ntonewlocations. Aweb-based search engine isfaced with\nthedual challenge ofadding newinformation, aswell as\nkeeping indexesofexisting information uptodate. A\nfocused crawler,based onaparticular topic, canbene-\n\u0002tgreatly byconcentrating onresources that arelikely\ntogivethebest results. This impro vesthequality ofthe\nquery indexesbyreducing thenumber offalsenodes and\nmissing pages returned inresponse touser queries.\n3.PREVIOUS RESEARCH\nMost large-scale search engines areproprietary commer -\ncialventures thatdonotmakedetailed information about\ntheir web crawlers public. Fewsystems aredescribed in\ndetail. Some details oftheoriginal Google search en-\ngine, asitexisted asaStanford University research project\nareavailable [3]. Mercator [11]isahigh-capacity web\ncrawler from Compaq Systems Research Center thatcan\nbeeasily con\u0002gured tohandle different types ofproto-\ncols, such asHTTP ,FTP,orGOPHER. Mercator formed\nthebasis oftheoriginal AltaV istaengine, andhasbeen\nmade available forselect research projects. Another re-\ncent multithreaded Java-based crawler uses multiple ma-\nchines connected using theNetw orkFileSystem (NFS) to\ntransfer data between nodes [18].Other crawler designs\nhavebeen described inlessdetail [20,9,4,2].Crawling\ntechnology formultimedia isstillinitsinfancy,with most\ndevelopment inthisarea applied tostillimages [16,17,\n10].The Cuidado project isanaudio andmusic content\nanalysis system thatisintended toeventually incorporate\naweb crawling system [19].\nThere havebeen severalstudies ofweb structure and\nthehyperte xtlinkages between webpages. Kleinber g[12]\nidenti\u0002ed twoimportant types ofweb structures. Authori-\ntiesarepages which contain anextensi veamount ofhigh-\nquality information about asetofrelated topics. Hubs are\npages which contain multiple links toauthorities onapar-\nticular topic. The twostructures reinforce oneanother:\ngood hubs havemanylinks toauthorities, andgood au-\nthorities arelinkedtobymanyhubs. TheGoogle PageR-\nankalgorithm [15] exploits theability ofusers toaddnew\nlinks tofavored resources, toproduce page rankings based\nonthepopularity ofawebpage. Thetechnique hasproven\ntobeenormously successful, although itrequires thecap-\nture ofasigni\u0002cant portion oftheweb toworkreliably .\nAsimilar method, using textual similarities between mul-\ntiple links totheURL resource wasproposed byAmitay\n[1].Dean andHenzinger [8]proposed amethod for\u0002nd-\ningweb pages based almost-entirely onlinkstructure.4.EXPERIMENT ALMETHOD\nAcrawlofapproximately 2,000,000 unique web pages\nwasconducted using theAROOOGA system, aspecial-\nized web crawler andsearch engine designed tolocate\nandindexboth sound \u0002les andassociated pages onthe\nweb.AROOOGA stands forArticulated Resource forOb-\nsequious Opinionated Observ ations intoGathered Audio.\nDuplicate URLs were remo vedduring thecrawling pro-\ncess, yielding asetofunique pages. Crawling wasdone\nusing abreadth-\u0002rst search method, apractice that has\nbeen showntogiveafairly natural distrib ution ofweb\nresources [13].Crawling wasinitiated from asetof20\nstarting seeds representing McGill University ,aswell as\nseverallargeportals.\nRetrie vedpages were speci\u0002cally analyzed forhyper-\nlinks referencing sound ormusic \u0002les. Each linkalsocon-\ntains anassociated piece ofanchor text;thisisthevis-\nibleelement onaweb page thattheuser clicks onto\naccess thesound \u0002le. Additionally ,studies haveshown\nthatwords surrounding ahyperlink often contain descrip-\ntivetextreferring tothehyperlink [7,6].Inthiscase, a\nmaximum oftenwords preceding andfollowing thelink\nwere also captured andanalyzed, orasmanyascould be\nobtained before theoccurrence ofanother hyperlink.\nThese three pieces ofinformation, thesound \u0002lename,\nanchor text,andsurrounding textareconsidered forthis\nstudy toform asingle document. Allthree type ofinfor -\nmation were then parsed intotokens,lowercased, andsto-\nplisted toremo vecommon terms thathavelittle value for\ncomparison. Parsing sound \u0002lenames intotokenspresents\nspecial problems notpresent inother sources [14], mostly\nduetocontiguous nature ofthewords inthename. Af-\ntersome experimentation, itwasdetermined thattokens\ncould also bederivedbycarefully splitting oncapitaliza-\ntion boundaries, punctuation, anddigits (with some ex-\nceptions). Filename extensions were remo ved,butused\ntodetermine theformat ofthesound \u0002le(WAVE, MP3,\netc.) The extracted information wasthen used todeter -\nmine commonality between pages andsound \u0002les.\n4.1. Similarity Measur es\nAseries oftextual similarity measures, adapted from IR,\nareused here tostudy therelationship between sound \u0002les\nandthelinkedweb pages, similar totheones proposed by\nDavison [7]forthepurpose ofstudying topical locality .\nResults areaveraged across alldocuments. Allmeasures\narecomparable inthattheyproduce values between 0and\n1,with 1indicating identical documents, and0represent-\ningdocuments without anycommon terms.\n4.1.1. TFIDF cosine similarity\nTFIDFkd=TFkd£IDFkpP(TFkd£IDFk)2\nwhere\nIDFk=logDN+1\nDkand\nTFkd=log(fkd+1)\nwhere IDFkistheinversedocument frequenc yofterm\nk,DNisthetotal number ofdocuments, Dkisthenumber\nofdocuments containing keywordk,andfkdisthenum-\nberofoccurrences ofkeywordkindocument d.Compar -\nisons areaccomplished using acosine measure.\nTFIDF¡COSkd=P\nallTFIDFkd1£TFIDFkd1\nsqrt(PTFIDF2\nkd1£PTFIDF2\nkd2)\n4.2. Term Probability\nFk=fkd\ndn\nand\nProbd1;d2=X\nFk\nwhere kmust beanelement ofboth documents.\n4.3. Document Overlap\nOverlapd1;d2=X\nmin(Fk;d1;Fkd2)\n5.RESUL TS\nFrom theinitial crawl,1,726,054 unique pages were found\ntocontain links tovalidpages. These were successfully\ndownloaded andparsed. Ofthisoriginal set,4500 pages\nwere found tohavereferences tosound \u0002les, producing\nunique links to35,481 sound \u0002les, adensity oflessthan\nonepercent (0.26%) ofpages crawled.\nTheaverage number ofsound \u0002lelinks perweb page\nacross theentire corpus is0.024. However,theaverage\nnumber ofsound \u0002lelinks, onpages which contain ref-\nerences tosound \u0002les is7.71, with thelargest number of\nlinks onasingle page being 341.\nThe listofsound \u0002les issegmented by\u0002leformat in\nTable 1.Themajority of\u0002les (66.33%) were found tobe\nMPEG types, with thenextlargest category being WAVE\n\u0002les (31.99%).\nType Number Percent AvgSize(Kb)\nMPEG 23533 66.33 1662\nWAVE 11349 31.99 701\nAIFF 578 1.63 2301\nOTHER 21 0.06 586\nTOTAL 35481 100 1637\nTable 1.Sound FileTypes\n5.1. Common Tokens\nFrom theentire setofextracted tokens, lists ofthemost\ncommon terms were computed foreach ofthethree cat-\negories ofinformation. Byfar,themost common token\nfound wasmp3, most often occurring instatements such\nasclick here togetthemp3. Other terms indicate astrong popandclassical music presence ontheweb.Itis\ninteresting thatmanyofthemost common tokens(click,\ntrack, download, kb,audio, \u0002le) appear todescribe ele-\nments oftheweb or\u0002lestructure, rather than thecontent\nofthe\u0002le.\nSound \u0002lenames\nclassical(1604) archi ves(1602) wav(1304) pre(1304)\nmusic(1302) opus(465) track(411) buzz(409) piano(292)\ndemo(282) bach(261) bass(209) love(204) chopin(197)\nsonata(175) project(167) mozart(167) mix(145)\nbrahms(134) guitar(132)\nAnchor/link text\nmp3(2168) listen(1435) download(712) demo(494)\nsample(405) kb(346) mono(324) click(321) mpeg(285)\nclip(264) wav(260) audio(224) hear(216) guitar(193)\nsound(184) normal(173) song(156) excerpt(139)\nbass(125) tone(124)\nTextsurrounding link\nkb(2804) mp3(1997) sound(1529) wav(993) sample(791)\ncode(770) perform(756) \u0002le(722) audio(637)\ndrum(613) loop(532) guitar(508) cd(495) record(457)\norigin(425) song(420) clip(419) listen(396)\nplain(387) music(386)\nCombined\nmp3(4259) kb(3165) wav(2557) listen(1845) classical(1818)\nsound(1778) archi ve(1625) pre(1342) sample(1324)\nmusic(1302) demo(1144) download(988) audio(913)\ntrack(867) drum(848) guitar(834) \u0002le(830) clip(798)\ncode(774) perform(765)\nTable 2.Common Tokensfrom External Metadata\n5.2. Similarity Measur es\nTFIDF Term Prob.Doc. Overlap\nLink Text 0.28 0.25 0.23\nSurrounding Text 0.37 0.30 0.27\nCombined 0.42 0.36 0.32\nTable 3.General Similarity Comparisons\nTable 3showssimilarity results foreach ofthethree\nanalyzes types, comparing sound \u0002lename tokenswith\nother visible textsources.\nTable 4showsthesame results between theentire doc-\nument ofcombined information andeach subtype, divided\nbysound \u0002letype.\n6.CONCLUSIONS\nSound \u0002les ontheweb aresparser than other types ofin-\nformation, andoccur lessfrequently than than thenumber\nofweb pages parsed. However,sound \u0002les arenotevenly\ndistrib uted across allpages, buttend toexhibit astrong\nhubstructure [12],with particular sources often leading\ntomultiple \u0002les.TFIDF Term Prob.Doc. Overlap\nWAVE\n\u0002lename 0.67 0.56 0.56\nanchor 0.25 0.17 0.17\nsurrounding 0.45 0.33 0.32\nAIFF\n\u0002lename 0.68 0.44 0.43\nanchor 0.49 0.27 0.27\nsurrounding 0.57 0.52 0.51\nMPEG-1\n\u0002lename 0.66 0.46 0.44\nanchor 0.43 0.32 0.32\nsurrounding 0.52 0.46 0.45\nTable 4.Similarity Comparisons byFileType\nPages with sound \u0002les tend tofeature manyofthesame\ntexttokens. Foraweb crawler,page retrie valsbased on\nthese tokensismore likelytoproduce web pages thathave\nlinks tosound \u0002les, andcanconstitute thebasis forapri-\norization scheme forfocused crawling ofaudio informa-\ntion. Also, there aredifferences intokenpatterns anddis-\ntribution between different \u0002letypes, andcanbetakenas\nanindicator ofdifferences inusage patterns fordifferent\ntypes.\nThere arecorrespondences between sound \u0002lenames\nandassociated visible texttokensonweb pages contain-\ninglinks tosound \u0002les. Thegreatest amount ofcorrespon-\ndence occurs when both thesurrounding textandvisible\nanchor textareretrie vedandparsed, indicating that the\nuseofboth types ofinformation should beused toproduce\nbetter metadata descriptions oflinkedsound \u0002les than the\nuseofthesound \u0002lenames alone.\n7.FUTURE WORK\nThis research isintended tobeused intheimpro vement\noftheAROOOGA search engine andweb crawler.Future\ncrawlstesting thevalue ofthisinformation forfocused\ncrawling areplanned. Theresults detailed here also need\ntobetested against alargerportion oftheweb.One ofthe\nnextsteps inthislineofresearch istoundertak eamuch\nlargercrawl,possibly necessitating theuseofadifferent\nclass ofmachinery .\nOneofthefunctions ofAROOOGA istocombine DSP\nandaudio analysis techniques with thecrawling process\ntoimpro vethequality ofinformation available tosearch\nengine users. Additional research willinvolvecomparison\nofDSP analyzes ofretrie vedsound \u0002les with thetextual\ninformation from theassociated webpages, particularly in\nthestudy ofgenre andweb localization.\n8.REFERENCES\n[1]E.Amitay .Using common hyperte xtlinks toiden-\ntifythebest phrasal description oftargetweb doc-\numents. InProceedings oftheSIGIR'98 Post-Confer ence Workshop onHyperte xtInformation Re-\ntrieval.n.p., 1998.\n[2]P.Boldi, B.Codenouti, M.Santini, and S.Vi-\ngna. Ubicra wler: Ascalable fully distrib uted web\ncrawler.InProceedings oftheEighth Australian\nWorld WideWebConfer ence,page n.p., 2002.\n[3]S.Brin andL.Page. The anatomy ofalarge-scale\nhyperte xtual web search engine. Proceedings of\nthe7thInternational World WideWebConfer ence,\npages 10717, 1998.\n[4]R.Burk e.Salticus: guided crawling forpersonal dig-\nitallibraries. InProceedings oftheACM/IEEE Joint\nConfer ence onDigital Libraries ,pages 8889, 2001.\n[5]S.Chakrabarti, M.Berg,and B.Dom. Focused\ncrawling: anewapproach totopic-speci\u0002c Webre-\nsource disco very.Computer Networks (Amster dam,\nNetherlands: 1999) ,31(1116):16231640, 1999.\n[6]S.Chakrabarti, B.Dom, D.Gibson, J.Kleinber g,\nP.Ragha van,andS.Rajagopalan. Automatic re-\nsource listcompilation byanalyzing hyperlink struc-\ntureandassociated text.InProceedings ofthe7thIn-\nternational World WideWebConfer ence,pages 65\n74,1998.\n[7]B.D.Davison. Topical locality intheweb.InPro-\nceedings ofthe23rdAnnual Confer ence onResear ch\nand Development inInformation Retrie val,pages\n27279, 2000.\n[8]J.Dean andM.Henzinger .Finding related pages in\ntheWorld WideWeb.Computer Networks ,31(11\n16):146779, 1999.\n[9]J.Edwards, K.McCurle y,andJ.Tomlin. Anadap-\ntivemodel foroptimizing performance ofanincre-\nmental web crawler.InProceedings oftheTenth In-\nternational WorldWideWebConfer ence,pages 106\n113, 2001.\n[10] V.Harmandas, M.Sanderson, andM.D.Dunlop.\nImage retrie valbyhyperte xtlinks. InProceedings of\nthe20th International Confer ence onResear chand\nDevelopment inInformation Retrie val,pages 296\n303, 1997.\n[11] A.Heydon andM.Najork. Mercator: Ascalable,\nextensible web crawler.World WideWeb,2(4):219\n29,1999.\n[12] J.M.Kleinber g.Authoritati vesources inahyper-\nlinkedenvironment. Journal oftheACM,46(5):604\n32,1999.\n[13] M.Najork andJ.L.Wiener .Breadth-\u0002rst crawl-\ningyields high-quality pages. InProceedings of\nthe10th International World WideWebConfer ence,\npages 1148, 2001.[14] F.Pachet andD.Laigre. Anaturalist approach to\nmusic \u0002lename analysis. InInternational Sympo-\nsium onMusic Information Retrie val.518, 2001.\n[15] L.Page, S.Brin, R.Motw ani, and T.Winograd.\nThe PageRank citation ranking: Bringing order to\ntheweb.Technical report, Stanford Digital Library\nTechnologies Project, 1998.\n[16] N.Rowe. Marie-4: ahigh-recall, self-impro ving\nweb crawler that \u0002nds images using captions. In-\ntellig entSystems, IEEE ,17(4):814, 2002.\n[17] S.Sclarof f,L.Taycher ,and M.La-Cascia. Im-\nagero ver:acontent-based image browser forthe\nworld wide web.InIEEE Workshop onContent-\nbased Access ofImageandVideo Libraries ,pages\n29, 1997.\n[18] V.Shkapen yukandT.Suel. Design andimplementa-\ntionofahigh-performance distrib uted web crawler.\nInProceedings ofthe18th International Confer ence\nonData Engineering ,pages 24954, 2002.\n[19] H.Vinet, P.Herrera, andF.Pachet. The Cuidado\nproject: Newapplications based onaudio andmusic\ncontent description. InProceedings oftheInterna-\ntional Computer Music Confer ence,pages 450454.\nICMA, 2002.\n[20] H.Yan,J.Wang, X.Li,and L.Guo. Architec-\ntural design and evaluation ofanef\u0002cient Web-\ncrawling system. Journal ofSystems andSoftwar e,\n60(3):185193, 2002."
    },
    {
        "title": "Melodic Atoms for Transcribing Carnatic Music.",
        "author": [
            "Arvindh Krishnaswamy"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417859",
        "url": "https://doi.org/10.5281/zenodo.1417859",
        "ee": "https://zenodo.org/records/1417859/files/Krishnaswamy04.pdf",
        "abstract": "We had introduced a set of 2D melodic units to transcribe Carnatic music previously, and we now provide some il- lustrative examples using real pitch tracks to further our discussion on this topic.",
        "zenodo_id": 1417859,
        "dblp_key": "conf/ismir/Krishnaswamy04",
        "keywords": [
            "Carnatic music",
            "Melodic atoms",
            "Pitch tracks",
            "Transcription",
            "2D melodic units",
            "Musical entities",
            "Interval tuning",
            "Intonation",
            "Melodic units extraction",
            "Ragams classification"
        ],
        "content": "MELODIC ATOMSFORTRANSCRIBING CARNATICMUSIC\nArvindhKrishnaswamy\nCenterforComputer Research inMusicandAcoustics\nDeptofElectrical Engineering, StanfordUniversity\narvindh@ccrma.stanford.edu\nhttp://ccrma.stanford.edu/˜arvindh/\nABSTRACT\nWehadintroduced asetof2Dmelodic units totranscribe\nCarnatic music previously ,andwenowprovide some il-\nlustrati veexamples using realpitch tracks tofurther our\ndiscussion onthistopic.\n1.INTRODUCTION\nTwelv enotes (orsixteen with four enharmonic equiva-\nlents) arenotsufﬁcient foranaccurate orfaithful repre-\nsentation ofCarnatic music; theseasoned musician can\neasily identify manymore than 12musical entities inan\noctave.Wehavelisted these various “melodic atoms”\ninprevious works [1,2,3].Inthisarticle, wecontinue\ntodescribe various aspects ofthese melodic entities and\nalso provide some illustrati veexamples using actual, per-\nformed audio segments. Inthiswork, interv altuning or\nintonation isnotdiscussed. Rather ,wesimply talkabout\nmelodic units whose intonation, timing andrendering we\nhaveobserv edtobequite ﬂexible overall. Fordeﬁnitions\nofterms andsymbols, thereader isreferred toourprevi-\nouspublications.\nMethods toautomatically segment Carnatic music pitch\ntracks andextract these entities from them will bepre-\nsented inafuture report. Wewillalsodefer toafuture re-\nporttechniques toidentify andclassify ragamsandpieces\nusing these melodic units.\n2.SAMENOTE,MANYVERSIONS\nAnote inCarnatic music canmean different things orap-\npear indifferent ways. Forexample, what does aCarnatic\nmusician mean when herefers tothenote “R1” or“Sud-\ndhaRishabham?” “R1” could beaparticular melodic en-\ntity,a“swarasthanam” oritcould refer totheentire setof\nentities associated with R1,asillustrated inFigure 1.(A\nswarasthanam, simply put, isthelocation ofthe“ideal”\nconstant-pitch variety ofanote, thatalso supports oris\nPermission tomakedigitalorhardcopiesofallorpartofthisworkfor\npersonalorclassroom useisgrantedwithoutfeeprovidedthatcopies\narenotmadeordistributedforpro®torcommercial advantageandthat\ncopiesbearthisnoticeandthefullcitationonthe®rstpage.\nc\r2004UniversitatPompeuFabra.R1:R1R1\n~R1~S/R1\\SR1− S*R1\n(?)^(?)R1+ R1R1 Swarasthanams? ?Suddha Rishabham\nS\nActual Melodic EntitiesS+\nFigure1.Under thecommon title of“R1” or“Suddha\nRishabham” there exist manydistinct melodic entities,\nsupported bytwodifferent swarasthanams. Allofthese\nentities appear inmost ragamsthatcontain theminor sec-\nond. (The entity denoted bythecarat symbol andquestion\nmarks isafast,upwardinﬂexion, examples ofwhich are\ngiveninFigure 4.)\ntheanchor point forinﬂected notes.) Figures 2,3and4\nshowrealpitch tracks ofthese different entities. Similar\ngraphs anddiagrams canbeproduced forallthenotes in\nCarnatic music.\nThough these entities canbedistinguished from each\nother easily inslowspeeds, with fasttempos andﬂexible\ntiming, theymay morph ormergeintoeach other .Forex-\nample, theentities R1-andR1:R1 areveryclose tobegin\nwith, andmay beverysimilar toother elements likeS/R1\norS*R1 incertain conte xts. Conversely ,givenacertain\npitch contour segment, sometimes itmay notbepossible\ntoclassify itintoaparticular category with absolute cer-\ntainty; apitch segement might ﬁtwell into2oreven3ad-\njacent categories, butfortunately inthose cases, itusually\ndoesn’ treally seem tomatter which category ischosen.\nThese categories ormelodic entities should bethought\nofas“reference points” inamulti-dimensional feature\nspace, andpitch contours would traversecontinuous paths\ninthisspace.\nThese categories arenotarbitrary .Insome phrases,\nonly certain melodic entities areacceptable forcertain notes.\nForexample, inthephrase from Gaulai showninFig-\nure10,only S+would workwhere itappears. This graph\nalsoshowsthisentity morphing slowlyintoanother: S*R1,\nandalso showsthatinanother conte xt,adifferent entity\nmay appear: R1+.\nButinother phrases, awide variety ofmelodic entities0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  −20020406080100120140160180200\nTime (s)Relative Pitch (cents)\nS+ S*R1 R1−S ~S~~R1~\nR1 R1+\nFigure2.Segments drawnfrom realmelodic contours, illustrating some oftheentities listed inFigure. Examples ofS\nandR1played with amild vibrato arealsoincluded. Themeaning orinterpretation ofthesymbols isgivenin[1].\n0 1 2 3 4 5 6 7 8−20020406080100120\nTime (s)Relative Pitch (cents)\nR1:R1 S/R1\\S/R1\\S\nFigure3.Additional entities listed inFigure. R1:R1 is\ncalled a“jandai” (double) phrase andispractically very\nclose toR1-, sometimes evenindistinguishable from or\nreplaceable with it.Thecontour ontheright showsaslide\n(transitions) between theconstant-pitch versions ofSand\nR1,which canbeeasily distinguished from theother enti-\ntiesatslowtempos.\n0 1 2 3 4 5 6050100150200250300350400\nTime (s)Relative Pitch (cents)\nFigure4.Incertain phrases, afastupwardspike,toward\na“ﬂexible” upper endisused forashort-duration ortran-\nsient note. Called “viraladi-s” inviolin playing, aboveare\nexamples ofR1“hit” from S.\nareacceptable tobeused asshowninFigures 5,6and7.\nInragamsthatdonothavecertain notes, entities which\narenormally anchored onthose missing notes will also\nusually beabsent. Forexample, inHindolam, thelack of\nPmeans thatP+doesn’ tappear infavorofD1,while D1-,D1andD1+ areemplo yed. Since P+sounds the“lowest”\ninpercei vedpitch andismissing, thisleads people tostate\nthat theD1inHindolam is“higher ,”which istrue toa\ncertain extent. Similarly ,inSunadha vinodhini, which is\nagainmissing P,P-does notappear forM2,leading people\ntoopine thatitsM2islowerthan theM2ofKalyani where\nP-isallowed toused forM2. Again,there isacertain\nelement oftruth inthisstatement, butitwould bemore\naccurate todescribe each occurrence ofanote interms of\nthemelodic entity category.\nThere arealso ragams which useentities anchored on\nswarasthanams that arenotpart oftheragam. Forex-\nample, inThodi, R2+ appears asaparticular form ofG2\nsometimes. And inMadh yama vati,M1isrender viathe\nG3*P inﬂexion, while G3does notﬁgure inMadh yama-\nvati.\n3.CONCLUDING REMARKS\nWebelie vethatthevarious entities wehavepublished thus\nfarareanecessary andsufﬁcient settobeabletorepresent\norsynthesize anyphrase inCarnatic music. Webelie ve\nthatallourentities arenecessary because foreach one,\nwecanﬁndaphrase where itisrequired. And thus far,we\nhavenotencountered aphrase inCarnatic music which\ncan’tbemodeled ortranscribed using only theentities we\nhavepublished. Whether thiswould hold foreverorifour\nlistsneed updating needs tobeseen.\nMapping apitch contour onto ourentities willremo ve\nanyartistic orexpressi vecontent inthemusic andyield a\nskeletal contour that nevertheless sounds musically cor-\nrect. Formanyapplications, such astranscription and\nclassiﬁcation thisisexactly what isrequired.00.511.522.533.544.555.56050100150200250300350400450500\nTime (s)Relative Pitch (cents)\nFigure5.The “r1” inthephrase “m1g3r1g3m1” in\nragams likeMaayaamaala vagaulai canberendered using\ndifferent melodic entities. Infact,duetoﬂexibilities inin-\ntonation, anddifferent artists’ preferences, awhole contin-\nuum ofcontours canarise, “in-between” theones shown\nabove.\n00.511.522.533.544.555.566.577.58380400420440460480500\nTime (s)Relative Pitch (cents)\nFigure6.The phrase “m1g3m1” canberendered in\nmanyways. Shownaboveare: (i)plain version, (ii)\n“m1:m1:m1” (jandai), (iii)“m1-, ”(iv)“m1 g3+ m1” or\n“m1 g3*m1 g3m1.”There existawhole continuum of\ncontours inbetween (ii),(iii)and(iv),andthedifferences\nbetween these different entities sometimes become fuzzy\norevennon-e xistent especially atfasttempos.\n4.REFERENCES\n[1]A.Krishnasw amy,“Inﬂe xions andmicrotonality in\nsouth indian classical music, ”inProcofFrontiers\nofResearchonSpeechandMusic(FRSM), Anna-\nmalainagar,India ,2004.\n[2]A.Krishnasw amy,“Towards modeling, computer\nanalysis andsynthesis ofindian ragams,”inProcof\nFRSM ,2004.\n[3]A.Krishnasw amy,“Multi-dimensional musical atoms\ninsouth indian classical music, ”inProcofICMPC ,\n2004.0 2 4 6 8 10 12 14200250300350400450500\nTime (s)Relative Pitch (cents)\nFigure7.Different waysofplaying thephrase “R2 G2\nM1” byusing distinct melodic entities inplace ofG2:(i)\naconstant-pitch G2,(ii)asliding transition, (iii)half an\nupwardora“truncated” upwardinﬂexion, (iv)R2+ (v)\nG2-, (vi)R2+M1 (vii) M1-G2. This isnotanexhausti ve\nlist.\n0123456789100100200300400500600700\nTime (s)Relative Pitch (cents)M M\nFigure8.Anexample ofthefamous BegadaM1. This\ncontour involveslarger-sized (much more than 100cents)\ninﬂexions andillustrates twokinds ofM1: “P-G3” and\n“P-M1”\n12 3\n4 5*\nFigure9.Different occurrences ofaninﬂexion likeR2+\ncanbethought ofascoming from orbeing cutfrom a\nlargerstable entity denoted bythe“*”. (1)A“transient\ninﬂexion, ”(2)Half ofaninﬂexion ora“truncated inﬂex-\nion,”(3)Twoperiods ofaninﬂexion, (4)Used inThodi in\nplace ofG2,(5)Used inSankarabaranam inphrases like\n“sR;s”.0123456789101112131415161718−500−400−300−200−1000100200300400500600700\nPMGMRGM −     −     −     −     −     −     R     −     −     −     −     −     − SRSN PM1−R1+M1− S− S+ S*R1 S  S^^\nTime (s)Relative Pitch (cents)\nFigure10.Aphrase from theragamGaulai, illustrating different kinds ofR1used, some ofthem evenmorphing from\noneintoanother .Thebottom lineofnotes isthestandard notation giveninmost music books. Thetoplineuses ourmore\nreﬁned categories. InGaulai, G3appears most often asM1-. Butthemost famous note inthisragamisR1which appears\nasS+.This phrase isa“re-rendition” ofasimilar phrase presented in[3](which does notuseR1+, butisalso equally\nacceptable).\n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5−400−300−200−1000100200300\nSR2+R2S\nN2 D1 D1+ N2R2+ S\nSRGRS−     −     −     −         D         −     −     −     − N R S\nTime (s)Relative Pitch (cents)\nFigure11.Aphrase from theragamDarbari Kaanada, segmented intomelodic atoms andtransitions. Themiddle lineof\nnotes iswhat onewould ﬁndinstandard music books. Thetopandbottom lines emplo yourmore sophisticated categories\nofmelodic atoms. Note thattheR2+ inﬂexion appears once asG2andthen againasanornament onR2! Thetransition\nfrom N2toD1could alsobeinterpreted asN2*D1."
    },
    {
        "title": "CsoundXML: a meta-language in XML for sound synthesis.",
        "author": [
            "Pedro Kröger"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415652",
        "url": "https://doi.org/10.5281/zenodo.1415652",
        "ee": "https://zenodo.org/records/1415652/files/Kroger04.pdf",
        "abstract": "The software sound synthesis is closely related to the Mu- sic N programs started with Music I in 1957. Although Music N has many advantages such as unit generators and a flexible score language, it presents a few problems like limitations on instrument reuse, inflexibility of use of pa- rameters, lack of a built-in graphical interface, and usually only one paradigm for scores. Some solutions concen- trate in new from-scratch Music N implementations, while others focus in building user tools like pre-processors and graphical utilities. Nevertheless, new implementations in general focus in specific groups of problems leaving oth- ers unsolved. The user tools solve only one problem with no connection with others. In this paper we investigate the problem of creating a meta-language for sound synthe- sis. This constitutes an elegant solution for the above cited problems, without the need of a yet new acoustic compiler implementation, allowing a tight integration which is dif- ficult to obtain with the present user tools.",
        "zenodo_id": 1415652,
        "dblp_key": "conf/ismir/Kroger04",
        "keywords": [
            "Music N",
            "unit generators",
            "flexible score language",
            "limitations on instrument reuse",
            "inflexibility of parameter use",
            "built-in graphical interface",
            "single paradigm for scores",
            "new from-scratch Music N implementations",
            "user tools",
            "pre-processors and graphical utilities"
        ],
        "content": "CSOUNDXML: AMET A-LANGU AGEINXML FOR SOUND SYNTHESIS\nPedroKr¨oger\nFederal University atBahia, Brazil\nABSTRA CT\nThesoftw aresound synthesis isclosely related totheMu-\nsicNprograms started with Music Iin1957. Although\nMusic Nhasmanyadvantages such asunitgenerators and\na\u0003exible score language, itpresents afewproblems like\nlimitations oninstrument reuse, in\u0003exibility ofuseofpa-\nrameters, lackofabuilt-in graphical interf ace,andusually\nonly oneparadigm forscores. Some solutions concen-\ntrate innewfrom-scratch Music Nimplementations, while\nothers focus inbuilding user tools likepre-processors and\ngraphical utilities. Nevertheless, newimplementations in\ngeneral focus inspeci\u0002c groups ofproblems leaving oth-\nersunsolv ed.Theuser tools solveonly oneproblem with\nnoconnection with others. Inthispaper weinvestigate\ntheproblem ofcreating ameta-language forsound synthe-\nsis.This constitutes anelegant solution fortheabovecited\nproblems, without theneed ofayetnewacoustic compiler\nimplementation, allowing atight integration which isdif-\n\u0002cult toobtain with thepresent user tools.\n1.INTR ODUCTION\nThe history ofsoftw aresound synthesis isclosely con-\nnected totheprograms written byMax Mathe wsinthe\n50'sand60'ssuch asMusic IVandMusic V.Alargenum-\nberofprograms (e.g. Music 4BF,Music 360, Music 11,\nCsound, Cmusic, Common Lisp Music, only tociteafew)\nwere developed taking Music Vasamodel. Usually these\nprograms arecalled Music N-type programs. (Although\nnotentirely correct wewillcallthese programs Music N\nimplementations).\nDespite itsstrengths, such asunitgenerators, a\u0003exible\nscore language, powerandspeed, Music Nhasafewprob-\nlems thatcanbedivided in:instrument design, score ma-\nnipulation, andintegration between instrument andscore.\nRegarding instrument design, the\u0002rst problem isin-\nstrument reuse. Only afewMusic Vbased programs have\nnamed instruments andtables instead ofnumbered. Only\nveryfewimplementations havegreat communication \u0003ex-\nibility anddata exchange between instruments, andnone\nallowthede\u0002nition ofconte xtdependent sound output.\nThe second problem isthewell-kno wnordered list; all\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forpro\u0002t orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation onthe\u0002rstpage.\nc\r2004 Universitat Pompeu Fabra.parameters arede\u0002ned asanordered list.This makesuti-\nlization more dif\u0002cult fortheuser (it'shard toremember\ntheorder andfunction ofallparameters, specially when\nanunitgenerator uses adozen ofthem) andprograms to\nextract instrument data. Thethird problem isthelack of\nscalability ofthetools developed todescribe instruments\ngraphically .Theyhavetohaveadeep understanding of\nthelanguage syntax, notinfrequently implementing a(yet\nanother) fullparser .Some programs such asSupercollider\nandCsound havespeci\u0002c opcodes forgraphical widgets.\nUnfortunately ,thissolution results inhaving graphical el-\nements inthesame levelofthesound synthesis. This\nisoneofthereasons thissolution isnotscalable; ifthe\ngraphical representation hastobechanged, theinstrument\ncore hastobemodi\u0002ed.\nScore manipulation represents anentirely different prob-\nlembecause acomposition isdescribed onit.And differ-\nentcomposers compose indifferent waysandneed dif-\nferent tools. Some solutions aspreprocessors andgeneric\nprogramming languages areuseful butlimited. Atone\nhand preprocessors usually haveonly one\u0002xedsyntax and\nparadigm, notbeing \u0003exible enough toaccommodate the\ncomposer' sstyle. Ontheother hand, when using ageneric\nprogramming language thecomposer hasallthe\u0003exibil-\nitynotfound with preprocessors, butitisnecessary to\nlearn acomplete programming language before compos-\ning,which isnotreasonable.\nThelastproblem isthelack ofintegration between the\norchestra andthescore, andspecially thelack ofintegra-\ntion between solutions forthescore (i.e. preprocessors)\nandtheorchestra. Tools forscore processing usually de-\n\u0002nemusical representation inahigher levelthen the\u0003at\nnote list. Howeverthis breaks thecommunication be-\ntween thepre-scorethe \u0002letobeprocessed andcon-\nverted inthescoreand theorchestra (\u0002g. 1).Communi-\ncation between thepreprocessor \u0002leandtheorchestra, or\nbetter yet,between thepre-score andapre-orchestra\nwould behighly needed (\u0002g. 2).\nFigur e1.Relationship between score, orchestra, andpre-\nprocessorFigur e2.Relationship between score, orchestra, andpre-\nprocessor\nInthispaper weinvestigate theproblem ofcreating a\nmeta-language forsound synthesis. This constitutes anel-\negant solution fortheabovecited problems, without the\nneed ofayetnewacoustic compiler implementation, al-\nlowing atight integration which isdif\u0002cult toobtain with\nthepresent user tools. The details oftheuseofascore\nlanguage with csoundXML isthesubject foranother pa-\nper.Apreliminar workandintroduction canbefound at\n[5].\n2.CSOUNDXML\nCsoundXML isameta-language inXML forsound syn-\nthesis developed bytheauthor ofthispaper .Ameta-\nlanguage isusually used tode\u0002ne ordescribe another lan-\nguage. Itdescribes theCsound orchestra language with a\nfewadded features.\nTheideal andhighly desirable goal would beaunique\nmeta-language forsound synthesis capable ofdescribing\nallsynthesis algorithms. However,thislanguage isvery\ndif\u0002cult todevelop, ifnotimpossible. The original goal\noftheMPEG-4 Structured Audio [4]wastofunction as\nanintermediate format between anysynthesis program,\nbutitrapidly became clear thatthisidea isunten-\nable. Thedifferent softw aresynthesizersCsound,\nSAOL, SuperCollider ,Nyquist, andthecommer -\ncialgraphical onesall havedifferent underlying\nconceptions ofevents, signals, opcodes, andfunc-\ntions thatmakesitimpossible tohaveasingle for-\nmat thatcaptures anything buttheverysimplest\naspects ofbehavior[9].\nSince auniversal language forsynthesis isnotviable,\nonesolution istocreate astandard andwaitforitsadop-\ntion[9].Another solution istode\u0002ne ageneric andexten-\nsible language with afewtargetlanguages [2].CsoundXML\nisanexample ofthelatter while SAOL[3]isanexample\noftheformer .\n2.1. Advantages\n2.1.1. Langua gesconversion\nXML hasbeen used with success inthecreation ofmeta-\nlanguages forconversion between different languages [6,\n1,8,7].CsoundXML works asastarting point inthesense\nthatinstruments written initcanbeconverted todiffer-\nentsynthesis languages such asCsound, Cmix, andsoon.Although CsoundXML isnotauniversal language, itis\ncompatible with theCsound orchestra format, andconse-\nquently ,other programs intheMusic Nfamily .\n2.1.2. Databases\nTheexistence ofalargecollection ofCsound instruments\nisoneofthemain sources oflearning. Nowthatthenum-\nberofthese instruments ismore then 2000, isnecessary\nthecreation ofamore formal database. Having these\ninstruments converted toCsoundXML allowstheuseof\nmeta-information tags such asauthor ,description, local-\nization, andsoon. This information canbeeasily ex-\ntracted andmanipulated.\n2.1.3. Pretty-print\nPretty-Print ismore than aneyecandy feature. Thepos-\nsibility toprint Csound code with typographical quality\nisanecessity ofbook andarticle authors. Having anin-\nstrument written inXML, theconversion toCsound can\nbedone indifferent ways. One simple example istheuse\nofcomments. One canchoose iftheywillorwillnotbe\nprinted andhow theywillbeprinted; ifabove,below,or\nsidewaysofanexpression.\n2.1.4. Graphical tools\nBecause CsoundXML isaformal andstructured language\nitispossible todescribe instruments graphically automat-\nically .There aretwobasic problems:\n1.design decisions tode\u0002ne howelements willbedrawn.\nSound generators such asoscillators areeasy torep-\nresent while opcodes thatconvertvalues and\u0003ow\ncontrol arehard torepresent graphically .\n2.algorithms todistrib utethesynthesis elements in\nthescreen avoiding collision. Having theprevious\nitem solvedisnecessary tohavesmart algorithms\ntoallowdifferent kinds ofvisualization andcom-\nplexity.\n2.1.5. Integration\nXML allowstheintegration ofdifferent paradigms andvi-\nsualizations modes. Forexample, asystem canbebuilton\ntopofCsoundXML todisplay instruments as\u0003owcharts\norasaparameter editor ,ortoemulate aMusic N-style\nsyntax (\u0002g. 3).\n2.2. CsoundXML syntax\nThis section willshowafewsyntatic elements togivean\nidea ofhowCsoundXML looks like.Inaddition italso\nsupports \u0003owcontrol, different types ofoutput, functions,\nexpressions, andmeta information.Figur e3.XML helps integration\nExample 2.1Atypical Csound opcode\nafoo oscil 10000, 440, 1;some comment here\n2.2.1. Opcodes\nTheheart ofCsound instruments aretheunitgenerators,\nimplemented asopcodes. The ex.2.1showsatypical\nopcode,oscil ,whereafoo isana-variable that will\nhold theopcode output.10000 istheamplitude, 440 is\nthefrequenc y,and1isthefunction number with awave\nshape. The textafter thesemi-colon isacomment that\nwillbedisre garded byCsound.\nInCsoundXML theopcodes arede\u0002ned bytheopcode\nelement anditsparameters bythepar element. Theop-\ncode andparameter name isde\u0002ned bythename attrib ute.\nTheidattrib utede\u0002nes aunique name foreach element.\nItcanalso beused toprovide connection between ele-\nments, likevariables (see 2.2.2). The ex.2.2showsthe\ncode ofex.2.1inCsoundXML.\nInformation about theopcodes (e.g. howmanyand\nwhich parameters) andparameters (e.g. thepossible val-\nues)isde\u0002ned inanXML library forCsound, CXL1,also\ndeveloped bytheauthor ofthispaper .Akind ofcross-\nreference between CsoundXML andCXL isachie vedus-\ningthename attrib ute.\nThetype attrib uteindicates thevariable type (e.g.k,\n1This isthesubject foranother paper ,yettobepublished.\nExample 2.2ACsound instrument inXML\n<opcode name= oscil id=footype=a>\n2<outid=fooout/>\n<parname =amplitude >\n4 <number >10000</number >\n</par>\n6<parname =frequency >\n<number >440</number >\n8</par>\n<parname =function >\n10 <number >1</number >\n</par>\n12<comment >some comment here</comment >\n</opcode >Example 2.3Parameter de\u0002nition\n<defpar id=gain type=i>\n2<default >20</default >\n</defpar >\ni,ora).Variables canhaveanyname, CsoundXML makes\nsure thevariable willstart with theright letter when con-\nverting toCsound. This isavaluable feature forautomatic\nconversion between variables.\nEach parameter may havethree kinds ifinput, asimple\nnumeric value (e.g. 1), avariable (e.g. iamp), oran\nexpression (e.g. iamp+1/idur). Iftheinput isanumeric\nvalue, thenumber element isused (line 4ofex.2.2).\nIftheinput isanexpression, theexpr element isused.\nFinally ,iftheinput isavariable, theparelement willbe\nempty andthevariable will bede\u0002ned bythevvalue\nattrib ute. Vvalue stands forvariable value. The value\nofvvalue must bethesame oftheidofthevariable\nde\u0002ned bydefpar (seesection 2.2.2).\nOne may bebothered bytheverbosity ofXML docu-\nments. Our original example (ex.2.1) hasonly oneline\nwhile theCsoundXML version (ex.2.2) has13! Never-\ntheless, XML verbosity isafeature andnotabug.Itper-\nmits, among other things, more complete searches. Still\nintheexample 2.2,aprogram fordrawing functions could\nquickly andeasily seehowmanyandwhich functions an\ninstrument isusing looking forthefunction attrib utein\nthe<par> tag. Itisimportant tokeepinmind thatre-\ngardless the\u0002rst impression, having structured informa-\ntion inXML makelifeeasier fortheprogrammer/user .\nAlltheprocess ofreading theXML \u0002le,determinating the\nstructure andpropriety ofdata, dividing thedata inpieces\ntosend toother components isdone bytheXML parser .\nSince there aremanyparsers available, both commercially\nandfreely ,thedeveloper does nothavetomakeonefrom\nscratch.\n2.2.2. Parameter sandvariables\nInCsoundXML variables arede\u0002ned with thedefpar\nelement. Italsohastheidandtype attrib utes(ex.2.3).\nAmore comple xexample isshowninex.2.4where the\ngain parameter isde\u0002ned. Thedescription element\ncontains abrief description, thedefault element hasa\nvaliddefaultvalue fortheparameter ,andtherange ele-\nment de\u0002nes thenumerical range. Agraphical toolcould\nextract theinformation inrange toautomatically create\nsliders foreach parameter .\nIftheauto attrib uteisequal toyes itsvalue willbe\nautomatically assigned toap\u0002eld. That is,theCsoundXML\ncode<defpar id=\"notes\" auto=\"yes\"/> isequiv-\nalent tothecodeinote=p4inCsound. The differ-\nence isthattheexactp\u0002eld isnotdetermined bytheinstru-\nment designer butbytheprogram implementing CsoundXML.\nThis isamore \u0003exible solution than theconventional use\nsince theparameters inthescore willbecalled bythevari-\nable names, notbyp\u0002elds.Example 2.4Aparameter with adefaultvalue andrange\n<defpar id=gain >\n2<description >\ngain factor ,usually between 0\u00001\n4</description >\n<default >1</default >\n6<range steps =\u0003oat>\n<from>0</from>\n8<to>1</to>\n</range>\n10</defpar >\n2.3. Parameter editor\nThe \u0002gure 4showsanovervie wofthecreation ofapa-\nrameter editor .After DTD validation (inorder tocheck\nthecorrectness oftheXML \u0002le) theneeded data isex-\ntracted from theinstrument. The program looks forele-\nments with theauto=\"yes\" attrib utetocreate sliders\nforeach parameter andselection boxesforeach function.\nSince thefunctions arede\u0002ned inaseparate \u0002le,thepro-\ngram reads allfunctions inthat\u0002leandshowsallofthem\nintheselection box.\nFigur e4.Parameter editor GUI creation\nThegreatest advantage ofthisapproach isthattheGUI\niscreated from aregular CsoundXML, thatis,nospeci\u0002c\ngraphical information hastobecoded intheinstrument.\nTheGUI isgenerated automatically .\nData caneasily beobtained from theinstrument byus-\ningXpath queries. TheXpath code forextracting allfunc-\ntions is//par[@name='function'] ,forexample.\nThis kind ofdata canbeveryuseful forcreating instru-\nment debuggers, forknowing themost used opcodes ina\ncollection ofinstruments, forcontrolling functions, andso\non.\n3.CONCLUSIONS AND FUTURE WORK\nThecreation ofameta-language forsound synthesis solves\nsome oftheproblems raised insection 1.Instrument reuse ismade possible byahigh-le velde-\nscription, named instruments, \u0003exible signal input andout-\nput,andmainly thepossibility tobeabletode\u0002ne multiple\noutputs depending onconte xt.\nTheuseofastructured syntax (such asXML 's)allows\nbypassing thelimitations ofMusic N's\u0003atlists. Itispos-\nsible toextract informations from theinstrument easily\n(section 2.3).\nUnlik eother solutions thataddgraphical commands in\ntheinstrument, themeta-language XML structure allows\ntheautomatic creation ofgraphical instruments, without\nextraopcodes (section 2.3).\nFinally ,theproblem oflack ofintegration between the\nsolutions forthescore (preprocessors) andtheorchestra is\nsolvedwith adescription ofboth inahigher levelandthe\nuseofparameter automation andconte xt.The proposed\nsolution allowsthecreation ofanintegrated system that\ncanbeaccessed with different interf aces.\nThesolutions presented inthisworkcanbeapplied in\ndifferent conte xt.Theycanbeimplemented astools to\nexpand programs already existent likeCsound, constitute\nthebasis foranewcompositional system, beincorporated\ntoexistent sound synthesis programs, beextended touse\nother synthesis languages than Csound asbasis.\n4.REFERENCES\n[1]Yannis Chicha, Florence Defaix, and Stephen M.\nWatt.AC++ toXML translator .TheFRISCO con-\nsortium, 1999.\n[2]Michael Gogins. Re:SML (synthesis modelling lan-\nguage), Jan2000.\n[3]ISO/IEC. Information technolo gycoding ofaudio-\nvisual objects ,1999.\n[4]Rob Koenen. Overvie wofthempegstandard. Tech-\nnical report, ISO/IEC JTC1/SC29/WG11, 1999.\n[5]Pedro Kr¨oger.Desen volvendo uma meta-lingua gem\nparas´\u0011ntese sonor a[Developing ameta-langua gefor\nsound synthesis] .PhD thesis, Federal University at\nBahia, Brazil, 2004.\n[6]Manuel Lemos. MetaL: XML based meta-\nprogramming engine developed with php. InPHP\nConfer ence 2001 ,Frankfurt, November 2001. PHP-\nCenter andSoftw are&Support Verlag.\n[7]Alagappan Meyyappan. GUI development using\nXML, 2000.\n[8]Soumen Sarkar andCraig Cleaveland. Code genera-\ntionusing xmlbased document transformation, 2001.\nAvailable athttp://www.theserverside.\ncom/resources/articles/XMLCodeGen /\nxmltransform.pdf .\n[9]Eric Scheirer .Re: SML (synthesis modelling lan-\nguage), 2000."
    },
    {
        "title": "A Prototypical Service for Real-Time Access to Local Context-Based Music Information.",
        "author": [
            "Frank Kurth",
            "Meinard Müller",
            "Andreas Ribbrock",
            "Tido Röder",
            "David Damm",
            "Christian Fremerey"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1414926",
        "url": "https://doi.org/10.5281/zenodo.1414926",
        "ee": "https://zenodo.org/records/1414926/files/KurthMRRDF04.pdf",
        "abstract": "In this contribution we propose a generic service for real- time access to context-based music information such as lyrics or score data. In our web-based client-server sce- nario, a client application plays back a particular (wave- form) audio recording. During playback, the client con- nects to a server which in turn identifies the particular piece of audio as well as the current playback position. Subsequently, the server delivers local, i.e., position spe- cific, context-based information on the audio piece to the client. The client then synchronously displays the received information during acoustic playback. We demonstrate how such a service can be established using recent MIR (Music Information Retrieval) techniques such as audio identification and synchronization and present two partic- ular application scenarios. Keywords: Music services, context-based information, fingerprinting, synchronization.",
        "zenodo_id": 1414926,
        "dblp_key": "conf/ismir/KurthMRRDF04",
        "keywords": [
            "Music services",
            "context-based information",
            "fingerprinting",
            "synchronization",
            "audio identification",
            "Music Information Retrieval (MIR)",
            "web-based client-server scenario",
            "real-time access",
            "client application",
            "server"
        ],
        "content": "APROTOTYPICALSERVICEFOR REAL-TIME ACCESSTOLOCAL\nCONTEXT-BASEDMUSIC INFORMATION\nFrankKurth,MeinardM ¨uller,AndreasRibbrock,TidoR ¨oder,David Damm, and Christian Fremerey\nUniversityof Bonn, Germany\nDepartmentof Computer Science III\nABSTRACT\nIn this contribution we propose a generic service for real-\ntime access to context-based music information such as\nlyrics or score data. In our web-based client-server sce-\nnario, a client application plays back a particular (wave-\nform) audio recording. During playback, the client con-\nnects to a server which in turn identiﬁes the particular\npiece of audio as well as the current playback position.\nSubsequently, the server delivers local, i.e., position spe-\nciﬁc, context-based information on the audio piece to the\nclient. Theclientthensynchronouslydisplaysthereceived\ninformation during acoustic playback. We demonstrate\nhow such a service can be established using recent MIR\n(Music Information Retrieval) techniques such as audio\nidentiﬁcation andsynchronization and present two partic-\nular application scenarios.\nKeywords: Musicservices,context-basedinformation,\nﬁngerprinting, synchronization.\n1. INTRODUCTION\nThelastyearshaveseenthedevelopmentofseveralfunda-\nmentalMIRtechniquessuchasaudioﬁngerprinting[3,4],\naudio identiﬁcation [1], score-based retrieval, or synchro-\nnizationofmusicindifferentformats[2,6,7]. Besidesthe\ndevelopment of tools for basic retrieval tasks, the impor-\ntance of using feature-based representations for exchang-\ningcontent-based music information (i.e., any informa-\ntion related to the content of the raw music data) over the\ninternethasbeenrecognizedrecently[8]. Asanimportant\nexample, compact noise-robust audio ﬁngerprints may be\nusedtopreciselyspecifyaplaybackpositionwithinapar-\nticular piece of PCM audio [4].\nAs the online distribution of audio documents evolves,\nthere is an increasing demand for advanced MIR services\nwhich are able to provide content-based information as\nwell as metadata related to particular music documents.\nWhile there are already various services and resources on\ntheinternetproviding globalinformationsuchaslyricsor\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopiesbear this notice and the full citation on the ﬁrst page.\nc°2004UniversitatPompeu Fabra.scores for a particular piece of music, there is still a lack\nofservices providing localinformationforsmall excerpts\nof a given piece of music. However, such local context-\nbasedinformation (e.g.,informationrelatedtoalocaltime\ninterval) can be of great value to a user while listening to\na piece of music. Examples of applications incorporating\nlocal context-based information include score following,\nlyricsfollowing,karaoke,ortheonline-displayoftransla-\ntions or commentaries.\nIn this contribution, we propose a generic framework\nwhich allows users to access and exchange context-based\n(local) information related to particular pieces of audio.\nWe demonstrate the feasibility of our framework by pre-\nsenting two services, one providing context-based lyrics,\nthe other providingscore information.\n2. GENERIC FRAMEWORK\nThegenericscenariooftheproposedserviceconsistsofa\npreprocessingphase andtheruntimeenvironment .\nIn the preprocessing phase, we start with a given data\ncollection of PCM audio pieces. For each of these audio\npieces,weassumetheexistenceofaparticulartypeofad-\nditional, context-based information such as the lyrics in\ncase of pop-songs or score information in case of classi-\ncal music. The preprocessing phase consists of two parts.\nIn the ﬁrst part, we create a ﬁngerprint database (FPDB)\nusing the raw audio material. Employing ﬁngerprinting\ntechniques such as [4], the FPDB allows us to precisely\nidentify a particular (short) excerpt taken from any audio\npiece within the collection. The identiﬁcation provides us\nwiththerespectivesongIDandthecurrentpositionofthe\nexcerpt within that song. The second part of preprocess-\ning consists of linking the context-based information for\neachaudiopiecetotheactualtime-lineofthatpiece. This\namountstoassigningaparticularstartingpositionanddu-\nration to each basic component, e.g., a single word in the\nlyricsscenarioorasinglenoteinthescorescenario. Fig.1\nshowsascore-,PCM-,andMIDI-versionoftheﬁrstmea-\nsures of J.S. Bach’s Aria con variazioni (BWV 988). The\nupper part of the ﬁgure illustrates the concept of score-\nPCM synchronization where a link between a symbolic\nnoteeventanditscorrespondingphysicalrealizationisin-\ndicated by an arrow. Below, a corresponding illustration\nis given for a MIDI-PCM synchronization. Technically,\nthelinkingmaybeperformedbyspecialized synchroniza-Figure 1. Synchronizing score to PCM (top) and MIDI\ntoPCM(bottom). Correspondingeventsarelinkedbyred\narrows.\ntionalgorithmssuchas[2]inthescore-PCMscenario. As\na result we obtain a so-called sync ﬁlewhich allows us\ntoaccesstheavailablecontext-basedinformationforeach\ntimeintervalofaparticularpieceofaudio. BoththeFPDB\nand sync ﬁles are stored on the server.\nAt runtime, we assume that a user is equipped with a\nclient application which basically consists of an extended\naudio player. The user selects a particular audio piece\nforplayback,whichthenundergoesalocalﬁngerprintex-\ntraction at the current playback position. The extracted\nﬁngerprint is transmitted to the server, which then tries\nto identify the audio piece and current playback position.\nIf successful, the server retrieves the context-based infor-\nmation from the sync ﬁle that is available for the cur-\nrent playback position. Using a suitable communication\nprotocol, the information is then transferred to the client.\nDuring playback the client displays the information syn-\nchronously with the acoustic signal. Note that the actual\nvisualizationdependsontheparticulartypeofapplication.\nFor example, in a karaoke-like scenario one would prob-\nably want to display entire lines of lyrics in advance and\nadditionallyhighlightsinglewordswhiletheyareactually\nsung.\nDuring subsequent playback, the client may request\nany further context-based information on the identiﬁed\npieceofaudiobysimplyspecifyingatargettimeinterval.\nThe server then retrieves all available data for that time\ninterval from the sync ﬁle. Note that when using a suit-\nable synchronization protocol between client and server,\na repeated ﬁngerprint-based identiﬁcation of the current\nplayback position is not necessary.\nFigure2. Service offeringcontext-basedlyrics.\n3. APPLICATIONS:PROTOTYPICALSERVICES\n3.1. A LyricsService\nFig. 2 shows an overview of a corresponding service for\nreal-time delivery of context-based lyrics information. In\nourtestsettingtheFPDBiscreatedusingapreviouslypro-\nposed audio ﬁngerprinting technique [4]. Sync ﬁles are\ncreated from a corresponding collection of lyrics for all\nof the songs contained in our audio collection. As there\nare currently no general approaches to automatic PCM-\nlyrics synchronization available, this step is carried out\nmanually. For the client application we implemented an\nenhanced audio player capable of ﬁngerprint extraction,\nclient-server communication, and synchronous display of\nlyrics during playback.\nIn the following we discuss some issues on the im-\nplemented system. Our client-server implementation is\nJava-based using a C++-library for ﬁngerprint extrac-\ntion, index creation and audio identiﬁcation. The server\nimplements a scheduler component which assigns a task\nIDtoeachincomingclientrequest,hencefacilitatingmul-\ntipleconcurrentrequests. Duringplayback,theclientper-\nforms ﬁngerprint extraction and transfers the ﬁngerprints\ntotheserverwhichinturnperformsthetaskofaudioiden-\ntiﬁcation. Following a successful identiﬁcation, the client\nisallowedtoquerytheserverforthedesiredcontext-based\nmetadata by specifying a target time interval. Note again\nthat our audio identiﬁcation technique [4] yields the ex-\nact offset between the queried fragment and the original\naudio signal, hence allowing us to precisely synchronize\ntime-offsets between the client- (query) and server-side\n(original) audio material. The communication between\nclientandserverisperformedusing Java’sRMI(remote\nmethod invocation) mechanism and data access is based\non the assigned task IDs.\nTheclientsystemconsistsofthegeneric SyncPlayer ,Figure 4.SyncFileMaker plug-in during manual synchronization of lyrics (left column, the current word position is\nhighlighted) to a simultaneously played audio track. The center column contains manually speciﬁed word positions, the\nright column is used for previewingthe results of the manual annotation.\nFigure 3. Generic SyncPlayer (top) during playback\nofMy Grown-Up Christmas List (by Amy Grant). The\nbottom part of the ﬁgure shows the Lyrics Display\nplug-in highlighting current text lines and word positions\nsimultaneously to the audio playback.<?xml version=\"1.0\" ?>\n<SyncFile>\n<Header>\n<Type>Lyrics</Type>\n<SamplesPerUnit>500.0</SamplesPerUnit>\n<SamplingFrequency>44100</SamplingFrequency>\n<Tracks>1</Tracks>\n</Header>\n<Body>\n<Track>\n<Description>Track Number One</Description>\n<Events>\n<Event>\n<Start>1509</Start>\n<Duration>17</Duration>\n<Data>Do</Data>\n</Event>\n...\n</Events>\n</Track>\n</Body>\n</SyncFile>\nFigure5. Skeletonofasyncﬁle. Theactualsynchroniza-\ntion information is contained in the <Event> -Tags.\nwhich currently supports basic playback of WAV- (44.1\nkHz, 16 bit, stereo) and MP3audio ﬁles.\nTheSyncPlayer offers a plug-in concept for an ap-\nplication speciﬁc display of metadata. This concept is re-\nalized as a Javainterface. To display lyrics, we imple-\nmented a Lyrics Display plug-in which is depicted\nin Fig. 3 (bottom) along with the SyncPlayer (top).\nDuringplayback,thecontext-basedmetadataistransferred\ntoalloftheactivatedplug-ins,whichareinturnresponsi-\nble for an appropriate visualization. As an example, the\nLyrics Display visualizes a local section of lyrics\nand highlights the current rows of text as well as the cur-\nrently sung words(Fig. 3, bottom).To facilitate manual creation of sync ﬁles in the lyrics\nscenario, we implemented a SyncFileMaker plug-in,\nseeFig.4. Thisplug-inmaybeusedtogeneratesyncﬁles\nby simply pressing a key on a computer keyboard at each\nword position during playback of a song, thus generat-\ning a list of time stamps which may then be exported to a\nsyncﬁle. SyncﬁlesarerealizedusingasimpleXML-style\nformat, see Fig. 5 for a small example. Note that our im-\nplementation and sync ﬁle format conceptually supports\nmultipletracks,whichmaybeusedtostoredifferenttypes\nofmetadata. Timestampswithinthesyncﬁlesarerelated\ntosamplepositionsusingthe <SamplesPerUnit> tag.\nThisallowsustoemploythesametimeresolutionasused\nby the ﬁngerprinting algorithm: ﬁngerprinting is usually\nperformed by a sliding window technique, resulting in a\ndecrease in time resolution by a certain factor.\nTheSyncPlayer and the LyricsPlayer plug-in\nare availablefor downloadat our website\nhttp://www-mmdb.iai.uni-bonn.de/research.php\nbyfollowingthecorrespondinglinkinthe Demossection.\nCurrently, there is a small collection of about ten songs\navailablefor demonstrating the lyrics service.\nWhile the time for ﬁngerprint extraction may be ne-\nglected,thedelaybetweenstartofaudioplaybackanddis-\nplayofcontext-basedlyricsmainlydependsonthequality\noftheinternetconnection,theserverload,andthetimere-\nquiredforqueryingtheFPDB.Aswecurrentlytransferall\nof the available lyrics information for one piece of audio\nto the client directly after identiﬁcation, there is no addi-\ntional latency besides this initial delay, even when skip-\nping through the audio piece during playback. Note that\nadditional delays may occur when playing back MP3au-\ndiotrackswhichareduetotheused MP3softwarelibrary.\n3.2. A ServiceforScore-BasedInformation\nAsasecondscenarioweproposeaprototypicalservicefor\nproviding context-based score information during play-\nback. In contrast to the former lyrics service, in this sce-\nnario we use a novel algorithm for score-PCM synchro-\nnization [5] to automatically generate the sync ﬁles. Fur-\nthermore, the karaoke-like visualization is replaced by a\nscore-following type of display, highlighting the current\nscorepositionsduringplayback. Acorrespondingplug-in\nforourSyncPlayer iscurrentlyunderdevelopmentand\nwill be availablefrom our website upon completion.\n4. CONCLUSIONS AND ONGOING WORK\nWe presented a web-based client-server scenario for real-\ntimeaccesstocontext-basedlocalmusicinformationsuch\nas lyrics or score-related data. The two proposed proto-\ntypical services can be realized using recent MIR tech-\nniques and are a ﬁrst step towards bridging the gap be-\ntween widely available global music information and the\nincreasing demand for selective access to local context-\nbasedinformation. Obviously,theproposedgenericframe-\nworkhasvariousfurtherapplicationssuchassynchronousdisplay of translations, commentaries, or instrument spe-\nciﬁc music information likeguitar chords.\nPart of our future work will be concerned with com-\npleting, reﬁning and evaluating the proposed services. In\nthis we are particularly interested in possible application\nscenarios within existing as well as emerging digital li-\nbraries. Another important challenge will be the design\nof new methods for automatically synchronizing meta-\ndata to audio signals. While there have been recent ad-\nvances allowing for resonable synchronizations of score-\nlike data to polyphonic recordings for a limited class of\ninstruments, those techniques are not yet applicable to ar-\nbitrary pieces of music. Furthermore, the automatic syn-\nchronization of lyrics to audio recordings is a challeng-\ningﬁeld ofresearch andwillrequire, e.g.,advancedalgo-\nrithms for detecting vocalpassages in audio recordings.\n5. REFERENCES\n[1]Eric Allamanche, J ¨urgen Herre, Bernhard Fr ¨oba, and\nMarkus Cremer. AudioID: Towards Content-Based\nIdentiﬁcation of Audio Material. In Proc. 110th AES\nConvention,Amsterdam,NL , 2001.\n[2]Vlora Ariﬁ, Michael Clausen, Frank Kurth, and\nMeinard M ¨uller. Automatic Synchronization of Mu-\nsical Data: A Mathematical Approach. In Walter B.\nHewlett and Eleanor Selfridge-Fields, editors, Com-\nputingin Musicology .MIT Press, in press, 2004.\n[3]Pedro Cano, Eloi Battle, Ton Kalker, and Jaap\nHaitsma. AReviewofAudioFingerprinting. In Proc.\n5. IEEE Workshop on MMSP, St. Thomas, Virgin Is-\nlands,USA ,2002.\n[4]Frank Kurth, Michael Clausen, and Andreas Rib-\nbrock. Identiﬁcation of Highly Distorted Audio Ma-\nterial for Querying Large Scale Data Bases. In Proc.\n112thAES Convention,Munich,Germany , 2002.\n[5]Meinard M ¨uller, Frank Kurth, and Tido R ¨oder. To-\nwardsanEfﬁcientAlgorithmforAutomaticScore-to-\nAudio Synchronization. In International Conference\non Music Information Retrieval, Barcelona, Spain ,\n2004.\n[6]Ferr´eol Soulez, Xavier Rodet, and Diemo Schwarz.\nImprovingpolyphonicandpoly-instrumentalmusicto\nscore alignment. In International Conference on Mu-\nsicInformation Retrieval,Baltimore , 2003.\n[7]Robert J. Turetsky and Daniel P.W. Ellis. Force-\nAligning MIDI Syntheses for Polyphonic Music\nTranscription Generation. In International Con-\nference on Music Information Retrieval, Baltimore,\nUSA, 2003.\n[8]George Tzanetakis, Jun Gao, and Peter Steenkiste. A\nScalable Peer-to-Peer System for Music Content and\nInformationRetrieval. In InternationalConferenceon\nMusicInformation Retrieval,Baltimore , 2003."
    },
    {
        "title": "Expressive Notation Package - an Overview.",
        "author": [
            "Mika Kuuskankare",
            "Mikael Laurson"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415084",
        "url": "https://doi.org/10.5281/zenodo.1415084",
        "ee": "https://zenodo.org/records/1415084/files/KuuskankareL04.pdf",
        "abstract": "The purpose of this paper is to give the reader a concise overview of Expressive Notation Package 2.0 (hencefor- ward ENP). ENP is music notation program that belongs to a family of music and sound related software packages developed at Sibelius Academy in Finland. ENP has been used in various research projects during the past several years.",
        "zenodo_id": 1415084,
        "dblp_key": "conf/ismir/KuuskankareL04",
        "keywords": [
            "Expressive Notation Package",
            "Music notation program",
            "Computer-aided composition",
            "Music analysis",
            "Virtual instrument control",
            "Western musical notation",
            "Typesetting",
            "LispWorks",
            "OpenGL",
            "User interface"
        ],
        "content": "EXPRESSIVE NOTATION PACKAGE - AN OVERVIEW\nMika Kuuskankare\nDocMus\nSibelius Academy\nmkuuskan@siba.ﬁMikael Laurson\nCMT\nSibelius Academy\nlaurson@siba.ﬁ\nABSTRACT\nThe purpose of this paper is to give the reader a concise\noverview of Expressive Notation Package 2.0 (hencefor-\nward ENP). ENP is music notation program that belongs\nto a family of music and sound related software packages\ndevelopedatSibeliusAcademyinFinland. ENPhasbeen\nused in various research projects during the past several\nyears.\n1. OVERVIEW\nENP [3] is a music notation program that has been devel-\noped in order to meet the requirements of computer aided\ncomposition, music analysis and virtual instrument con-\ntrol. ENP is intended to represent Western musical no-\ntation from 17th century onward including 20th century\nnotation. ENP is not a full featured music typesetting\nprogram. It is, however, designed to produce automatic,\nreasonable musical typesetting according to the common\npractices [12]. ENP output should not generally require\nadjustments made by the user.\nTherearealsosomeothernon-commercialLISP-based\nprograms that are aimed at representing complex musical\ndata such as Common Music Notation [13], PatchWork’s\nRhythm-Editor [6] and the musical editors in OpenMusic\n[1].\nENP is programmed with LispWorks ANSI Common\nLisp by Xanalys. LispWorks, in turn, is a Lisp imple-\nmentationthatissourcecodecompatibleacrossWindows,\nLinux, Mac OS X and UNIX platforms. ENP uses the\nOpenGL API for graphical output. OpenGL is a widely\nused3Dgraphicslibrarythatisfastandportablewithim-\nplementations in all of the aforementioned operating sys-\ntems. It is equally suitable for developing 2D or 3D inter-\nactive applications.\nAtthetimeofwritingthedevelopmentversionofENP\nruns in Mac OS X. However, some preliminary attempts\nhave been made to port it to Windows.\nSome of the key concepts behind ENP are:\n1) ENP can be used to represent a wide range of nota-\ntional styles.\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2004 Universitat Pompeu Fabra.2) ENPhasamousedrivenuserinterfacethatreliesondi-\nrect editing, i.e., almost every notational object can be\nedited in the score with synchronized visual feedback.\n3) ENP provides access to its notational data structures,\nthus it can be controlled algorithmically. The user can\ninspectandmodifythepropertiesofthenotationalob-\njects (e.g., time, pitch, duration).\n4) ENP provides a rich library of standard and user-\ndeﬁnable expressions. They range from standard ar-\nticulation markings to fully interactive multi-purpose\ngraphical expressions.\nAlso, ENP is used as a notational front end in a vi-\nsual programming language called PWGL [8]. PWGL\nis a combination of several complex software packages\nbuild on top of Common Lisp. The components include\narule-basedprogramminglanguage(PWConstraints[6]),\nand a sound synthesis program (PWSynth [7, 10]). These\nclosely integrated software packages can be used to fur-\nther analyze, construct, and modify the musical data con-\ntained by ENP scores.\nThisrestofthepaperisdividedintofourmainsections.\nSection 2 contains an outline of the notational structures\nand expression scheme of ENP. Section 3 explains the\nprinciples of ENP user interface. Section 4, in turn, cov-\nerstwoseparateissues,ENP-score-notationandscripting.\nFinally, in the last Section, we discuss the future develop-\nment of ENP.\n2. MUSIC REPRESENTATION\nENP supports a number of notational styles (e.g., men-\nsural or non-mensural notation, frame notation, tape no-\ntation). In the next two subsections we discuss the two\nfundamental notational styles supported by ENP: mensu-\nral and non-mensural notation. The third subsection, in\nturn, discusses frame notation in brief.\n2.1. Mensural Notation\nAnENPscoreisbuiltoutofhierarchicalobjectstructures.\nTypically, a mensurally notated ENP score consists of a\nlistofparts,apartconsistsofalistofvoices,avoicecon-\nsists of a list of measures, and a measure consists of a\nlist of beats. Beats, in turn, can contain either other beats\n(to create complex nested rhythms) or chords. Finally, a\nchord contains a list of notes. Notes and chords can alsocontain information about any additional attributes, such\nas expressions (see Section2.4).\nNextwegiveanexampleofthescorestructureofENP\nin mensural context. Figure 1 contains a simple musical\nexcerpt consisting of two eight-notes and a quarter-note.\nFigure2,inturn,displaysthecorrespondingENPscoreas\na tree structure revealing the score hierarchy.\nFigure 1. A simple ENP score.\nFigure 2. The hierarchical structure of the ENP score\n(shown in Figure 1) displayed as a tree structure.\n2.2. Non-mensural Notation\nThe difference in non-mensural notation, when compared\nto mensural notation, is that internally a voice contains a\nlistofchordswithabsolutestart-timesanddurations. The\nnon-mensural notation can be used, for example, when\nwritingcontemporaryproportionalor’time-notation’( `ala\nBerio). Figure 3 gives an example of an ENP score that\nis written using non-mensural notation. The example in-\ncludes also some expressions and special note-heads, etc.\nFigure3. AnENPscorewrittenusingnon-mensuralnota-\ntion. Theabsolutetime(minutes:seconds)isshownabove\nthe staff.\n2.3. Frame Notation\nApart from the two fundamental notational styles de-\nscribed above ENP can also display musical material us-\ning frame notation. Frame notation can be used in bothmensural and non-mensural context and it is indicated in\nthe score by enclosing a group of pitches or gestures in-\nside a rectangle. These are then to be played either in a\nrandom order or according to a given rhythm. Two exam-\nples of the frame notation canbe found in the Appendix.\n2.4. Expression Scheme\nENP provides a predeﬁned set of both standard and non-\nstandardnotationalattributes,calledENP-expressions[4].\nStandard ENP-expressions include, for example, stac-\ncatos,slurs,playingstyles,etc. Non-standardexpressions,\nin turn, include groups and score-BPF (see Figure 4 for\nexamplesofboth). Score-BPF,forinstance,isamultipur-\npose graphical object that can represent breakpoint func-\ntions as a part of a musical texture. The user can also\ncreate new expressions throughinheritance.\nAllENP-expressionsaredynamic,i.e,theyadjusttheir\nvisual appearance according to their notational context.\nExpressions are also editable directly in the score thus\neditingascore-BPForadjustingtheslopeofaslurisboth\nstraightforward and interactive.\nFigure4givesanexampleofascorecontainingvarious\nENP-expressions.\nFigure 4. A score containing a collection of different\nENP-expressions,suchasascore-BPF(a)andagroup(b).\n2.4.1. Compound Expressions\nOne example of an ENP-expression that inherits proper-\ntiesfromanotherENP-expressionisthecrescendoexpres-\nsion. As it inherits form the score-BPF the user can draw\nan arbitrary break-point function to express the increase\nanddecreaseofloudnessasafunctionoftime. Thebreak-\npoint function can be edited directly in the score:\nFigure 5. The crescendo expression (left) inherits form\nthe score-BPF and thus contains an editable break-point\nfunction (right). At the right the crescendo expression\nis displayed in an editable state (i.e., after the user has\ndouble-clicked it). Notice also how the musical material\nremains in the background while editing.\n2.4.2. Instrument Sensitive Expressions\nAn expression may have a different graphical representa-\ntion depending on the instrument it is written for. In ENPthese kind of expressions are called instrument sensitive\nexpressions [8].\nLet us look at an example where we use an expression\nthat designates that a passage of music is to be played on\na speciﬁc string. In a guitar part it is usually written dif-\nferently than, for example, in a violin part (Figure 6). In\ncasetheuserwouldchangetheinstrumentofapart,allthe\ninstrument sensitive expressions in it would adjust their\nappearance accordingly.\nFigure 6. An instrument sensitive expression attached\nboth to a guitar part and a violin part. The expression has\na different outlook in both parts depending on the instru-\nment in question.\n3. GUI\nNowwediscussthegraphicaluserinterfaceofENP.First,\nweintroducethegeneralideasbehindeditinginENP.Af-\nter that we explain the selection mechanism and also take\na look at how rhythms are edited.\nENPhasagraphicaluserinterfacethatallowsanymu-\nsicalobjecttobeediteddirectlyinthescore[3]. Generally\nall editing operations provide a synchronized visual feed-\nback for the user.\n3.1. Editing\nThere are two edit modes inENP:\n1) General edit mode in which the user can, for example,\nenter and edit pitch information, add and edit expres-\nsions, etc.\n2) Rhythmeditmode. Thismodeisintendedforinputting\nand editing the rhythmic structures and timing infor-\nmation.\nThe underlying idea behind the ENP user interface is\nto allow the user to access the information contained by a\ndigital musical score as straightforward as possible. As a\ngeneral guideline every musical object of any complexity\ncan be edited directly in the score. This reduces the need\nfor any external dialogs or editors. Every musical object\nin the score reacts to a setof operations:\n1) Dragging is the primary way to edit notational objects\nincluding transposing, repositioning, shape adjusting\nand even editing the rhythm.\n2) Zooming is mainly used to scale the score view or to\nadjust the spacing of the score.3) Panning can be used, for example, to displace objects\nor to reposition the score view.\nTheeffectoftheeditingoperationdependsonthetype\noftheobjectinquestionandusuallyonthedirectionofthe\nmouse movement. For example, notes and chords can be\ntransposed (vertical drag) or displaced in time (horizontal\ndrag). The expressions can be repositioned or reshaped,\netc.\nMostoperationsthatcannotbeperformedbydragging\narehandledwiththehelpofcontextsensitivemenus. They\ncan be used to control various notational attributes like\nnote-head shapes, enharmonic spelling or beaming infor-\nmation. Context sensitive menus are also used to add ex-\npressions, make changes to page layout, and so on.\n3.2. Selecting\nENP supports all the widely used ways of selecting ob-\njects. Italsosupportsdiscontinuousselectioninanycom-\nplexity (i.e., notational objects of any kind across voices\nand parts) and any form (e.g., including sweep selection).\nThe novelty in ENP is not the way the actual selection is\nmade but rather the way the selection and especially the\nmultiple selection is managed [3].\nThere are two ways of making a selection in ENP:\n1) Single selection (e.g., clicking to an object).\n2) Multiple selection (e.g., sweep selection, extended se-\nlection).\nIn (1) it is unambiguous to determine which object is af-\nfected by the consequent editing operations. In (2), how-\never, there may be different kinds of objects in the selec-\ntion and hence there must be a mechanism to determine\nwhichsubsetoftheselectionisactiveatthetime. Forthis\npurpose we introduce a new concept, selection ﬁltering.\n3.2.1. Selection Filtering\nThere are two ways that selection ﬁltering can take place\nin ENP:\n1) Explicitly, when the user applies a suitable ﬁlter. Cur-\nrently supported ﬁlters of this kind are: note-, chord-,\nbeat-, and measure-ﬁlter. This approach is usually\nused when applying some keyboard shortcuts to a se-\nlection.\n2) Automatically, in consequence of some editing opera-\ntions. Thisoccurswheneditingobjectswiththemouse\norusingcontextsensitivemenus. Inthiscasetheeven-\ntualeditingoperationsaffectonlytothoseselectedob-\njects that are of the same kind as the one the user is\nmanipulating with the mouse.\nIt is to be noted, however, that selection ﬁltering does\nnot collapse the original selection; all the objects remain\nselected after editing. This is especially useful when the\nuser has to perform a number of operations to different\nsubsets of the selected notational objects.3.3. Rhythm Edit Mode\nThere are two basic operations to modify the beat struc-\ntures:\n1) Theusercandivideanexistingbeatintoarbitrarynum-\nber of sub-beats. This can be done by using keyboard\nshortcuts or context sensitive menus.\n2) The user can change the proportional duration of any\nbeat. This can be done by using keyboard shortcuts,\ncontext sensitive menus or mouse.\nTypically,thenumerickeysareusedtoindicatethecor-\nresponding numeric values. When the user types a num-\nber, the proportional durations of the selected beats are\nchanged. When the user types a number while holding\ndowntheshiftkey,theselectedbeatsaredividedintocor-\nrespondingnumberofsub-beats. Theusercanalsodraga\nbeat at any level to change its proportional duration (Fig-\nure 7). This can also be used to change a note to a rest.\nAs can bee seen in Figure 7 there is also some additional\ninformationdrawnalongwiththestandardnotation. Each\nbeat level in the hierarchy is brought out by drawing a\nthicklinealongwithanumberreferringtoitsproportional\nduration. Thelineservesbothasaneditablehandletothe\nbeat and also as a visual indication of the extent of the\nbeat.\nFigure 7. Editing rhythm in ENP. The user has dragged\nthe beat handle of a quarter-note (left) to change it to a\nhalf-note (right).\n4. ENP-SCORE-NOTATIONAND SCRIPTING\nInthissectionwediscussENP-score-notationandalsode-\nscribe how ENP can be scripted.\n4.1. ENP-score-notation\nENP allows to describe scores in a special text format\ncalled ENP-score-notation [9]. This approach is akin to\nthe XML based MusicXML format [2] or the L ATEX ﬂa-\nvored LilyPond [11] where the user can describe the ele-\nmentsofthescoreratherthanthelayoutitself. Thelayout\nis then handled either by the software itself (as it is the\ncase with LilyPond) or with another software capable of\nimporting the format in question.1\nAlthough ENP scores are saved in a text format it is\nnot, however, very easily readable, especially for the un-\ntrained. The ENP-score-notation is offered as an interme-\ndiate step between the score and the elaborate ﬁle format.\nByusingtheENP-score-notationascorecanbeconverted\n1In case of MusicXML there are several commercial and non-\ncommercial music notation packages that can import MusicXML, in-\ncluding Igor Engraver, Sibelius, Finale, etc.into a more human readable form. ENP-score-notation\ncan also easily be converted back to an ENP score. This\nwaypracticallyalltheuserdeﬁnablepropertiesofanENP\nscore are accessible and deﬁnable.\nThe structure of ENP-score-notation reﬂects the score\nstructure described in Section 2. The syntax is similar to\nthe LISP list syntax. Every level in the hierarchy is col-\nlected into a list.\nNext we give a relatively simple example of an ENP\nscore and its counterpart written in ENP-score-notation\n(Figure 8). The ENP-score-notation example shown here\nisgeneratedautomaticallybyexportingthescoreinENP-\nscore-notation format.\n(((((1\n((1\n:NOTES\n(60)\n:EXPRESSIONS\n(:ACCENT\n(:CRESCENDO/290019720\n:USER-POSITION-Y-CORRECTION\n-0.4)))\n(2\n:NOTES\n(62)\n:EXPRESSIONS\n(:ACCENT\n:CRESCENDO/290019720))\n(1\n:NOTES\n(64)\n:EXPRESSIONS\n(:ACCENT\n:CRESCENDO/290019720))))))\n:STAFF :TREBLE-STAFF))\nFigure 8. A simple ENP score (above) and its equivalent\nwritten in ENP-score-notation (below).\n4.2. Intelligent Scripting\nMost professional music notation programs provide a\nscripting language and/or plug-in interface that allows\nthe user to modify the notational information contained\nby the score. In Sibelius, for example, the user can\nwritescriptsbyusingthebuilt-inscriptinglanguagecalled\nManuScript. In Finale, on the other hand, there is an in-\nterface that allows the user to write plug-ins using C++.\nLilyPond provides a plug-in functionality by its built-in\nScheme interpreter.\nInamusicnotationprogramascriptcouldtypicallybe\nused to apply a certain articulation pattern to a passage of\nmusicortorecalculatetheenharmonicidentityofselected\nnotes, etc.\nENP Script [5] is a scripting language that is derived\nfrom the pattern-matching language of PWConstraints\n[6]. PWConstraints, in turn, is a general-purpose rule-\nbased programming language. The use of the PWCon-\nstraints pattern-matching language as the basis of ENP\nscripting offers several advantages:1) Complex musical patterns can easily be deﬁned with\nthe help of a pattern-matching syntax.\n2) The syntax of the pattern-matching language is com-\npact and powerful and easy tolearn.\n3) PWConstraints contains rich knowledge about the\nmelodic, rhythmic and harmonic properties of the\nscore that can be accessed bythe script.\n4) There is no need to write any control structures (e.g.,\nloops)becausePWConstraintsprovidesthebasicabil-\nity to map through the notational objects in the score.\nWithout going into details of the concepts behind rule\nbased languages, and PWConstraints in particular, we\ngive an example of an ENP script. In the example we\nuse a script to insert a repeating articulation pattern to a\npassageofsixteenthnotes(theresultingscoreisshownin\nFigure 9). The beneﬁts of this kind of a script are natu-\nrally more obvious when the musical passage in question\nis very long.\nThe script is deﬁned as follows:\n(* ?1 ?2 ?3 ?4\n(?if (when (downbeat? ?1)\n(add-expression ’slur ?1 ?2)\n(add-expression ’staccato ?3)\n(add-expression ’staccato ?4))))\nFigure 9. An articulation pattern is applied to passage of\nmusic with the help of a script.\n5. FUTURE DEVELOPMENTS\nThere are still many improvements and features planned\ninto the future. ENP is far from ﬁnished and continues\nto be an active ﬁeld of improvement. For example, there\nis currently no visual synchronization between the men-\nsural and non-mensural notation. This is a visualization\nproblem that should be addressed.\nAlso, one of the main tasks will be to add appropriate\nimport/exportﬁltersinENP.AbilitytoexportatleastMu-\nsicXML or LilyPond formats is planned and the Enigma\nTransportable Format (ETF) support is under considera-\ntion.\nThere have also been some preliminary attempts to in-\ncorporate a Find feature in ENP that would allow to ﬁnd\nand display notational objects in the score according to\nmultiple search criteria.\n6. ACKNOWLEDGMENTS\nThe work of Mikael Laurson has been ﬁnanced by the\nAcademy of Finland.7. REFERENCES\n[1] Gerard Assayag, Camillo Rueda, Mikael Laurson,\nCarlos Agon, and Olivier Delerue. Computer As-\nsisted Composition at IRCAM: From PatchWork to\nOpenMusic. ComputerMusicJournal ,23(3):59–72,\nFall 1999.\n[2] M. Good and G. Actor. Using MusicXML for File\nInterchange. In Third International Conference on\nWEB Delivering of Music , page 153, Los Alamitos,\nCA, September 2003. IEEE Press.\n[3] Mika Kuuskankare and Mikael Laurson. ENP2.0\nA Music Notation Program Implemented in Com-\nmon Lisp and OpenGL. In Proceedings of Interna-\ntional Computer Music Conference , pages 463–466,\nGothenburg, Sweden, September2002.\n[4] Mika Kuuskankare and Mikael Laurson. ENP-\nExpressions, Score-BPF as a Case Study. In Pro-\nceedings of International Computer Music Confer-\nence, pages 103–106, Singapore, 2003.\n[5] Mika Kuuskankare and Mikael Laurson. Intelligent\nScripting in ENP using PWConstraints. In Proceed-\nings of International Computer Music Conference ,\n2004. Accepted for publication.\n[6] Mikael Laurson. PATCHWORK: A Visual Program-\nming Language and some Musical Applications .\nStudia musica no.6, Sibelius Academy, Helsinki,\n1996.\n[7] Mikael Laurson and Mika Kuuskankare. PWSynth:\nA Lisp-based Bridge between Computer Assisted\nComposition and Sound Synthesis. In Proceedings\nof the International Computer Music Conference ,\npages 127–130, Havana, Cuba, September 2001.\n[8] Mikael Laurson and Mika Kuuskankare. PWGL:\nA Novel Visual Language based on Common Lisp,\nCLOS and OpenGL. In Proceedings of Interna-\ntional Computer Music Conference , pages 142–145,\nGothenburg, Sweden, September2002.\n[9] Mikael Laurson and Mika Kuuskankare. From\nRTM-notation to ENP-score-notation. In Journ´ees\nd’Informatique Musicale , Montb ´eliard, France,\n2003.\n[10] Mikael Laurson and Vesa Norilo. RECENT DE-\nVELOPMENTS IN PWSYNTH. In Proceedings\nof DAFx 2003 , pages 69–72, London, England,\nSeptember 2003.\n[11] Han-Wen Nienhuys and Jan Nieuwenhuizen. Lily-\nPond, a system for automated music engraving. In\nXIV Colloquium on Musical Informatics (XIV CIM\n2003), Firenze, Italy, May 2003.\n[12] Gradner Read. Music Notation . Victor Gollancz\nLtd., 1982.\n[13] BillSchottstaedt.ComonMusicNotation.In Beyond\nMIDI, The Handbook of Musical Codes . MIT Press,\nCambridge, Massachusetts,1997.A. APPENDIX\nFigure 10 . An example of the frame notation in ENP. All the frames (the two ﬂute frames and the percussion frame) can\nbe freely dragged both vertically and horizontally (in time).\nFigure 11 . An example of the frame notation in mensural context. There is also a special accelerando-beat in the last\nmeasure."
    },
    {
        "title": "A multi-parametric and redundancy-filtering approach to pattern identification.",
        "author": [
            "Olivier Lartillot"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416426",
        "url": "https://doi.org/10.5281/zenodo.1416426",
        "ee": "https://zenodo.org/records/1416426/files/Lartillot04.pdf",
        "abstract": "This paper presents the principles of a new approach aimed at automatically discovering motivic patterns in monodies. It is shown that, for the results to agree with the listener’s understanding, computer modelling needs to follow as closely as possible the strategies undertaken during the listening process. Motivic patterns, which may progressively follow different musical dimensions, are discovered through an adaptive incremental identification in a multi-dimensional parametric space. The combinatorial redundancy that would logically result from the model is carefully limited with the help of particular heuristics. In particular, a notion of specificity relation between pattern descriptions is defined, unifying suffix relation – between patterns – and inclusion relation – between the multi-parametric descriptions of patterns. This enables to discard redundant patterns, whose descriptions are less specific than other patterns and whose occurrences are included in the occurrences of the more specific patterns. Resulting analyzes come close to the structures actually perceived by the listener. Keywords: motivic analysis, pattern discovery, melodic identification, redundancy filtering, music cognition.",
        "zenodo_id": 1416426,
        "dblp_key": "conf/ismir/Lartillot04",
        "keywords": [
            "motivic patterns",
            "monodies",
            "computer modelling",
            "adaptive incremental identification",
            "multi-dimensional parametric space",
            "specificity relation",
            "combinatorial redundancy",
            "melodic identification",
            "music cognition",
            "listeners understanding"
        ],
        "content": "A MULTI-PARAMETRIC AND REDUNDANCY-FILTERINGAPPROACH TO PATTERN IDENTIFICATIONOlivier LartillotUniversity of JyväskyläDepartment of MusicPL 35(A)40014 University of Jyväskylä, FinlandABSTRACTThis paper presents the principles of a new approachaimed at automatically discovering motivic patterns inmonodies. It is shown that, for the results to agree withthe listener’s understanding, computer modelling needsto follow as closely as possible the strategies undertakenduring the listening process. Motivic patterns, whichmay progressively follow different musical dimensions,are discovered through an adaptive incrementalidentification in a multi-dimensional parametric space.The combinatorial redundancy that would logicallyresult from the model is carefully limited with the helpof particular heuristics. In particular, a notion ofspecificity relation between pattern descriptions isdefined, unifying suffix relation – between patterns –and inclusion relation – between the multi-parametricdescriptions of patterns. This enables to discardredundant patterns, whose descriptions are less specificthan other patterns and whose occurrences are includedin the occurrences of the more specific patterns.Resulting analyzes come close to the structures actuallyperceived by the listener.Keywords: motivic analysis, pattern discovery, melodicidentification, redundancy filtering, music cognition.1. GENERAL SPECIFICATIONS1.1. The Key Role of Musical Patterns in MIR.Musical structures may be decomposed along twogeneral dimensions. On the one hand, temporal gaps andmusical discontinuities (such as pitch leaps, or changesin intensity, timbre, etc.) induce the determination ofboundaries [2] [11] [14]. On the other hand, similarcontexts in one or several musical sequences may beassociated one with the others, and be related to onesingle conceptual description called pattern. Once apattern is inferred, the identification becomes global,since other occurrences of the pattern can be discoveredthroughout the whole musical sequence or inside anentire musical corpus. Contrary to local structures,global patterns offer hence a synthetic description of themusical sequences that can be used for MIR purposes.1.2. An Adaptive Pattern IdentificationIn opposition to similarity-based paradigm [4] [13],cognitive studies [7] have suggested that musicidentification relies on exact identification alongmultiple parametric dimensions, such as pitch, contourand rhythm. The cognitive and computational approachesto melodic identification along ‘multiple viewpoints’ [3]always consider each possible musical dimensionseparately. Resulting patterns are either rhythmic,melodic, or melodico-rhythmic, and melodic patternsresult either from pitch, scale, or contour identifications.However, it seems that heterogeneous patterns may beconstructed through a progressive identification alongdifferent musical dimensions. For instance, the patternrepresented in the first line of  Figure 4 consists of threenotes of same pitch and rhythmic value and a fourth noteof lower pitch. For such patterns to be discovered, allpossible musical dimensions have to be consideredduring each phase of the progressive construction, andrelevant viewpoints have to be selected in an adaptiveway. A computational solution to this core problem isdescribed in this paper.1.3. An Incremental Pattern ConstructionPatterns are usually discovered following two differentpossible strategies. In a first approach, pair-wisecomparisons are made between templates, that areselectively extracted from the musical sequence [4] orthat consists of all possible sub-strings within a definedrange of length [13]. Once templates are identical orsufficiently similar, they are considered as occurrences ofa pattern. In this way, only patterns that are included inthis pre-defined set of templates – particularly, patternsof a limited size – will be discovered.Alternatively, pattern occurrences are discoveredthrough a progressive construction directly from themusical sequence [2] [5] [6]. First, patterns of two notesare discovered. Then the next notes following theiroccurrences are compared. Identifications among thesecontinuations lead to extensions into patterns of threenotes, and so on. Patterns of unlimited size may then bePermission to make digital or hard copies of all or part of this workfor personal or classroom use is granted without fee provided thatcopies are not made or distributed for profit or commercialadvantage and that copies bear this notice and the full citation onthe first page.© 2004 Universitat Pompeu Fabra.discovered in an optimal way, since only the necessarycomparisons are made.1.4. A Non-Selective ApproachIn most current approaches, the automated patterndiscovery mechanism produces a large amount ofpatterns that does not present any interest as such. Theresult need then to be reduced through additionalfiltering mechanisms, which select patterns featuring agood score along particular criteria. Such a global post-filtering process prevents a thorough analysis of themusical pieces. In our approach, we will try to avoid thisfiltering by insuring the pertinence of the patterndiscovery process itself. For this purpose, we will showin particular the necessity of an automated filtering ofredundant patterns, such as suffixes.1.5. A Monodic RestrictionSome approaches [6] [13] take into account musicaltransformations such as note insertion, deletion, etc.Others [12] attempt to analyze polyphonic sequences. Inour system, however, due to the complexity of theproposed paradigm, only monodic sequences will beconsidered in a first approach.2. AN INCREMENTAL MULTIDIMENSIONALMOTIVIC IDENTIFICATION2.1. The Musical DimensionsWith each note may be associated different kinds of pitchvalues (see Figure 1). Theoretical pitch values, such asC#, stem from the existence of pitch scales, or tonalityin particular. Each theoretical pitch value may then bealso expressed as a degree on this scale. This scaledegree can be represented by an integer between 0 and 7,where 0 is the tonic of the scale. In the scale degree maybe included the octave position: With one particulartonic is associated value 0, with the tonic one octavehigher value +7, etc. Diatonically transposed patterns –i.e. patterns that are translated along the scale degreedimension – can be identified along the scale degreeinterval – noted ‘s’ in the remainder of the paper – thatrepresents the scale degree difference between successivenotes.\nFigure 1. Description of a musical sequencefollowing different musical dimensions. Repeatedsequences of values, which form patterns, aresquared. Are highlighted the dimensions integratedin our approach.Alternatively, the pitch of each note may be expressedindependently of any scale. Particularly convenient forthat purpose is the chromatic pitch representation, whichassociates with each enharmonic pitch – say, each key ofa piano keyboard – a position number. Following theMIDI standard, with middle C is associated the value60, and the pitch value of each other note is computed inrelation to its distance in semi-tones to middle C. Thenchromatically transposed patterns – i.e. patterns that aretranslated along the chromatic pitch dimension – can beidentified along the inter-pitch dimension, noted ‘p’,which is the chromatic pitch difference betweensuccessive notes. Finally, contour – noted ‘c’ – simplyrepresents the sense of variation between successivenotes: increasing (+), decreasing (-), or constant (0).Finally, rhythmic values – noted ‘r’ – may beexpressed by a rational number, indicating the quotientbetween the duration of each note and a given pulsation.For instance, as the rhythm of Figure 1 is ternary, value1 is associated with quavers.2.2. Incremental Pattern ConstructionPatterns and their occurrences are discovered in anincremental and recursive way, but in the same timethrough a chronological scanning of the successive notesof the musical sequence. We will first explain theincremental construction of patterns – which generalizesCrochemore’s approach [5] to a multi-dimensional space–, and will then describe its chronological adaptation.2.2.1. Associative MemoryFirst, all the different parameters1 related to each intervalbetween successive notes are stored in associativememories. These memories are content-oriented, in sucha way that a new interval induces a recall of allmemorized intervals that are identical along one orseveral parameters. This can be modeled simply throughhash-tables linked to each possible musical parameter.For instance, each new interval (say, the interval n6Æn7in Figure 2) is stored in a scale degree interval hash-table(named “scale interval”), at the index associated with itsscale degree value (here: s = +1). All the memorizedintervals featuring a same scale degree interval value aredirectly retrieved at the same index of the hash-table(here: the interval n1Æn2).2.2.2. Pattern DiscoveryOnce several intervals share a same identity along oneor several parameters, a new pattern is created (here: nodec pointed by the considered index of the scale intervalhash-table). The description of the pattern is the list ofidentities (here, only s = +1), and the pattern class is thelist of intervals that are considered as occurrences of thispattern (here: n1Æn2 and n6Æn7).Following extensions of patterns follow the sameprinciple. The interval that follows each occurrence of thepattern is stored in an associative memory related to the                                                1 In our approach, each interval also contains a rhythmic dimension,which consists in the rhythmic value of the first note of the interval.pattern (here: the ‘scale interval’ and ‘pitch interval’hash-tables associated with the node c). In this way,whenever two following intervals (here: n2Æn3 andn7Æn8) share an identity along one or severalparameters, a new extension of the pattern is created(here: d), and so on.\nFigure 2. A musical sequence, some of its patternoccurrence trees (below), and the associated patterntree (above), with some of the related associativememories. See the text for a detailed explanation ofthis figure.2.3. Graph-Based Data Representation2.3.1. Pattern ChainsWhen a pattern is progressively extended, its successiveprefixes need to be stored. Each successive extension of anew occurrence of the pattern can be associated with eachsuccessive prefix of the pattern in an increasing order oflength. For certain occurrences, this progressive patternrecognition is not complete and stops at one particularprefix. For these reasons, pattern may be represented as achain of states – called pattern chain (PC) – featuringthe successive prefixes. In Figure 2, the branchaÆcÆdÆe, over the score, is a PC. Similarly, eachpattern occurrence is also represented as a chain of states– called pattern occurrence chain (POC) – featuring thesuccessive prefixes too. Each state of a POC is related toits corresponding PC. In Figure 2, each branchaÆcÆdÆe under the score is a PO of the previouslyshown PC.2.3.2. Pattern TreesNow each state of a PC (for instance, d) can acceptseveral different possible extensions (here: e, f and g). Inthis way, the set of all pattern classes forms a tree, calledpattern tree (PT), and each PC is as a branch of the PT.This is what is represented over the score of Figure 2.Similarly, each state of a pattern occurrence can acceptseveral different possible extensions. Hence the set of allpattern occurrences that are initiated by a same note (forinstance: n1) forms a tree, called pattern occurrence tree(POT), and each POC initiated by the note n1 is abranch of the POT. In Figure 2, the POT initiated by n1is represented underneath. The initial note n1 may berelated to the root node (a) of the PT. Since all notes ofthe sequence can potentially initiate a pattern, they are alloccurrences of this particular pattern a, called notepattern. For instance, under the POT initiated by n1 is alittle POT initiated by n2 (aÆb).2.4. Chronological Pattern ConstructionNow the incremental pattern construction has to beadapted to the chronological perception of notesfounding the listening process. This necessity will beunderstood once we will consider, in the next section,the mechanisms of redundancy filtering. In a word, thesemechanisms prevent the creation of particular patternoccurrences by taking into account the local context ofeach occurrence. If pattern occurrences are not filteredprogressively, redundancy needs to be filtered byadditional algorithms [6]. In the approach developedhere, however, the analysis is so detailed that the simplepattern discovery process, because of the combinatorialredundancy, could not be completed without anintegrated redundancy filter. That is why patternoccurrences need to be discovered chronologically.Each new note that is heard (for instance, n9) isconsidered as an occurrence of the note pattern (a). In thisway, new pattern occurrences may potentially beconstructed from this note. Then, the interval n8Æn9between the previous note and current note is considered.Each occurrence that concludes the note n8 issuccessively considered (here: occurrences of d, b and a).2.4.1. Chronological Pattern DiscoveryThe interval n8Æn9 is memorized in the associativememory of the pattern d, b and a. As the interval n8Æn9is identified with the interval n3Æn4 through the scale-interval and pitch-interval hash-tables associated with thepattern d, a new pattern e is inferred as an extension of d.The occurrence of d concluded by previous note n8 isextended into an occurrence of e concluded by currentnote n9. The occurrences associated with the memorizedintervals (here, only n3Æn4) are also extended. Patternsf and g are discovered in a similar way.When the following note n10 will be considered, thenew interval n9Æn10 will be memorized in theassociative memory of the patterns associated with theprevious note n9 (e, f and g). However, the intervalsn4Æn5, on the contrary, could not be memorized in thesame way. The memorization of these old intervalsshould therefore be done when the new patterns (e, f andg) are discovered.2.4.2. Chronological Pattern RecognitionConsider now note n22, which concludes occurrencesof d, b and a. The pattern d already accepts severalextensions e, f and g. As the interval n22Æn23 meetsthe description of extension e, the occurrence of d issimply extended into an occurrence of e concluded bynote n23. The occurrences of patterns f and g arediscovered in a similar way.The incremental approach proposed here enables amulti-dimensional adaptive discovery of patterns. Theuse of hash-tables insures the computational efficiency ofthe pattern discovery process: thanks to the associativememory, remembering of old similar contexts does notneed a search through the score.3. REDUNDANCY FILTERING3.1. Combinatorial ExplosionThe pattern discovery system, as described in theprevious section, shows important limitations. Inparticular, the number of discovered patterns is huge andthe process easily enters into combinatorial explosion.This is due in particular to the redundancy of the patternclasses, which can be described along two relations.3.1.1. Suffix RelationWhen a pattern is discovered, all the possible suffixes ofthe patterns are also considered as patterns of their own.For instance, in Figure 2, b – which represents theidentityi2: s = -1– is a suffix of d, which represents the sequence ofidentities i1Æi2, wherei1: s = +1.Such redundant inferences should actually not beconsidered, unless the suffix appear alone in the musicalsequence, without being a suffix of the longer pattern.This principle may be formalized with an equalityrelation between pattern classes. We defined the patternclasses as the set of occurrences of a pattern. The classesof pattern d and its suffix b will be considered as equalsince each occurrence of b is a suffix of an occurrence ofd. If, on the contrary, there exists occurrences of b thatare not suffix of occurrences of d, then the pattern classof d would be considered as included in the pattern classof b.3.1.2. Implication RelationThe second dimension of pattern redundancy stems fromthe notion of implication relations between patterndescriptions. Pattern e, for instance, is a succession ofthree identities i1Æi2Æi3, wherei3: s = +2 and p = +3.Each identity may be compared to any other identitywithin any other pattern. A notion of implicationbetween identities can now be defined, as a conjunctionof two mechanisms.Firstly, as the description of i4, where:i4: s = +2,for instance, consists in an element of the description ofi3, then i4 may be considered as implied by i3.Secondly, some parameters are direct consequences ofother parameters. In particular, a contour value c = - isimplied by an enharmonic pitch interval, for instance p =-2, or a scale degree interval value s = -1.Both aspects can be unified into a single concept ofidentity implication. This leads us to the seconddimension of pattern redundancy. Pattern f, for instance,is described by the succession of identities i1Æi2Æi4.As each successive identity of f is implied by thecorresponding identity of same rank in e, then the wholedescription of f is implied by the whole description of e.If all occurrence of f are occurrence of e, f should not beconsidered as a pattern of its own. Else, this wouldproduce a combinatorial set of redundant patterns.3.1.3. Specificity RelationNow suffix and implication relations can be unified,leading to a single specificity relation. The descriptionh: i2Æi4, for instance, is less specific than thedescription e: i1Æi2Æi3, because h is an implied suffixof e. That is: the description of h is a suffix of thedescription f: i1Æi2Æi4, which is implied by thedescription of e.3.2. Avoiding Redundant Description of PatternClassesNow the general principle ruling the pattern redundancycontrol may be stated as follows: If a pattern h is lessspecific than another pattern e, and if, in the same time,the pattern class of h is equal to the pattern class of e,then the pattern h, considered as redundant, should notbe inferred at all.However a pattern that is considered as redundant atone moment of the musical sequence may become non-redundant once it appears alone at a later stage of thesequence. The pattern would then be inferred, as well asall previous pattern occurrences whose existences wereinitially inhibited.Put in another way, a pattern class could be describedby different successions of identities, but only the mostspecific description should be explicitly considered. Allthe less specific descriptions are implicitly representedby the most specific description.3.3. Incremental Redundancy FilteringNow such redundancy filtering mechanism needs to beadapted to our incremental and chronological patterndiscovery framework. As explained in section 2, patternsclasses and occurrences are constructed through aprogressive discovery of the successive intervals thatconstitute them.We may remark that, when a pattern x is consideredas a non-redundant suffix of another pattern y, itsextension x’ may, on the contrary, become redundant.This happens when the pattern class of x’ is smaller thanthat of x and becomes equal to that of a more specificpattern y’ [10]. For this reason, the non-redundancy of apattern should be checked at every phase of its extension.Now the mechanism of incremental redundancyfiltering will be explained through an example. Considernote n8 in Figure 2. The occurrence of pattern a,concluded by the previous note n7, is candidate forextension as an occurrence of a new pattern b. Considerthe occurrence of pattern d concluded by current note n8.Since the pattern class of this more specific pattern d isequal to the pattern class of b – one occurrence concludedby n3, and the other by n8 –, then pattern b will actuallynot be inferred.The trouble is, the more specific pattern d can beconsidered only if its occurrence concluded by n8 hasalready been discovered. First, for the new perceived noten8, all the pattern occurrences that are concluded by theprevious note n7 should be considered in a decreasingorder of specificity. Then, for each of these patternoccurrences, the possible extensions have to beconsidered in a decreasing order of specificity of theiridentities.Thanks to the mechanism presented in this section,the general pattern discovery system offer a morecompact and synthetic, but in the same time lossless,representation of the motivic dimension of musicalpieces. Such reduction was necessary not only for thequality of the results, but also in order to limit thecomputational complexity of the process.Moreover, when a pattern is repeated several timessuccessively, lots of redundant implicit patterns could belogically discovered, leading to another combinatorialexplosion [2] [10]. Although the listener may sometimesfollow some of these redundant patterns, his or herperception more generally catches the successiverepetitions of the simple period. This heuristics has beenincluded in our model [10].4. CURRENT RESULTS4.1. ImplementationThis model is developed as a library of OpenMusic [1]called OMkanthus, and will be integrated intoMIDItoolbox [8]. The analysis is currently undertaken onrhythmically quantified MIDI files. Rhythmic values aredirectly computed with respect to a pre-defined tempo,and scale degree parameters through a straightforwardcorrespondence between pitches values and scale degrees,knowing the tonality. Contour, although theoreticallyincluded in our framework, is not taken into account incurrent analyses: its integration apparently needs athorough modeling of short-term memory.The results of the analysis can be displayed, as inFigures 3 and 4, in a score composed of a superpositionof synchronous staves, each different stave representingthe occurrences of a different pattern. Alternatively, thepatterns progressively inferred by the model during theincremental analysis of the score can be listed. Thisenables to trace the analysis process, to understand thestrategies undertaken, and to find the reasons of thepossible unexpected behaviors.4.2. Some Results4.2.1. Beginning of Mozart Sonata in A, K 331.Figure 3 presents the resulting analysis of the beginningof the upper voice of the first movement of MozartSonata in A, K 331. The first pattern is the main phraseof the main theme that appears twice as antecedent andconsequent. The last notes of each phrase are notidentified because of the little rhythmic variation that oursystem cannot abstract for the moment. The secondpattern is the little motive repeated twice, with diatonictransposition, in the first pattern. The third pattern ispurely rhythmic, and is repeated successively in the mainphrase. The fourth pattern is a very short melodic phrasewith two distant occurrences, whose actual perception bythe listener may be questioned. Two non-pertinentpatterns have also been discovered, that result from badbehaviors of the modeling.4.2.2. Beginning of Beethoven’s Fifth Symphony.Figure 4 presents the analysis of the monodic reductionof the beginning of Beethoven’s Fifth Symphony. Thefirst line represents the different occurrences of thefamous 4-note pattern. The second pattern is a melodico-rhythmic phrase that aggregates three successiveoccurrences of the 4-note pattern. The third pattern is aspecification of the 4-note pattern featuring a thirdinterval between the two last notes and a long lastrhythmic value. The fourth pattern is anotherspecification featuring a major third interval. The fifthpattern is an extension of the 4-note pattern, concludedby an ascending fourth interval.Previous pattern discovery systems cannot offer thiskind of analysis, although evident for the listeners. Theywould indeed include a numerous set of redundantpatterns such as suffixes or redundant extensions. Thisshows the necessity of mechanisms of redundancyfiltering such as those proposed in this paper.4.3. Discussion and Future WorksThis study has shown that the musical patterns actuallydiscovered by the listeners cannot be reduced to simplemathematical definitions. The actual complex strategiesundertaken during the listening process need to bemodeled as carefully as possible.The computational complexity of the model is noteasy to assess. Indeed, due to the complexinterdependencies between the different mechanisms, thebehavior of the model varies extremely with regard to themusical material.Thanks to this perceptive mimicry, the model offerspromising results. Yet bad behaviors need to becontrolled, and a large scope of musical expression –such as polyphony – has not been taken into account yet.Some assessments of the formal definition of patterns,though, have been attempted [9].AcknowledgementsThis study has been initiated during my PhD supervisedby Emmanuel Saint-James (Paris 6 University) andGérard Assayag (Ircam), and is now carried out in theMusic Cognition Group supervised by Petri Toiviainenat the University of Jyväskylä.5. REFERENCES[1] Assayag, G. et al. ''Computer AssistedComposition at Ircam: From Patchwork toOpenmusic'', Computer Music Journal, 23(3),1999.[2] Cambouropoulos, E. 1998. Towards a GeneralComputational Theory of Musical Structure.PhD thesis, University of Edinburgh, 1998.[3] Conklin, D., and C. Anagnostopoulou.''Representation and Discovery of MultipleViewpoint Patterns'', Proceedings of theInternational Computer Music Conference,San Francisco, USA, 2001.[4] Cope, D. Computer and Musical Style. OxfordUniversity Press., 1991.[5] Crochemore, M. ''An optimal algorithm forcomputing the repetitions in a word'',Information Processing Letters, 12(3), 1981.[6] Dannenberg, R. and N. Hu. \"Pattern DiscoveryTechniques for Music Audio.\" Proceedings ofthe International Conference on MusicInformation Retrieval, Paris, France, 2002.[7] Dowling, W.J., and D.L. Harwood. MusicCognition. Academic Press, London, 1986.[8] Eerola, T., and P. Toiviainen. \"MIR In Matlab:The MIDI Toolbox.\" Proceedings of theInternational Conference on MusicInformation Retrieval, Barcelona, Spain, 2004.[9] Lartillot, O., and E. Saint-James. ''AutomatingMotivic Analysis through the Application ofPerceptual Rules'', Music Query: Methods,Strategies, and User Studies (Computing inMusicology 13). MIT Press, 2004.[10] Lartillot, O. ''A Musical Pattern DiscoverySystem Founded on a Modeling of ListeningStrategies'', Computer Music Journal, 28(3),2004.[11] Lerdahl, F., and R. Jackendoff. A GenerativeTheory of Tonal Music. MIT Press, 1983.[12] Meredith, D., K. Lemström and G. Wiggins.''Algorithms for discovering repeated patternsin multidimensional representations ofpolyphonic music'' Journal of New MusicResearch, 31(4), 2002.[13] Rolland, P.-Y. ''Discovering Patterns inMusical Sequences'', Journal of New MusicResearch, 28(4), 1999.[14] Temperley, D. The Cognition of Basic MusicalStructures. MIT Press, 1988.\nFigure 3. Analysis of the upper voice of the beginning of the first movement of Mozart Sonata in A, K 331. Eachdifferent line shows the occurrences, within the same melody, of a different pattern. The successive interval parameterstaking part in the description of each pattern are indicated below each first occurrence, under the note ending eachconsidered interval, and where ‘p’ means pitch, ‘s’ scale degree and ‘r’ rhythm.\nFigure 4. Analysis of the beginning of Beethoven’s Fifth Symphony, in the same representation that in Figure 3."
    },
    {
        "title": "Methodological Considerations Concerning Manual Annotation Of Musical Audio In Function Of Algorithm Development.",
        "author": [
            "Micheline Lesaffre",
            "Marc Leman",
            "Bernard De Baets",
            "Jean-Pierre Martens"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1414874",
        "url": "https://doi.org/10.5281/zenodo.1414874",
        "ee": "https://zenodo.org/records/1414874/files/LesaffreLBM04.pdf",
        "abstract": "In research on musical audio-mining, annotated music databases are needed which allow the development of computational tools that extract from the musical audio- stream the kind of high-level content that users can deal with in Music Information Retrieval (MIR) contexts. The notion of musical content, and therefore the notion of annotation, is ill-defined, however, both in the syntactic and semantic sense. As a consequence, annotation has been approached from a variety of perspectives (but mainly linguistic-symbolic oriented), and a general methodology is lacking. This paper is a step towards the definition of a general framework for manual annotation of musical audio in function of a computational approach to musical audio-mining that is based on algorithms that learn from annotated data.",
        "zenodo_id": 1414874,
        "dblp_key": "conf/ismir/LesaffreLBM04",
        "keywords": [
            "annotated music databases",
            "computational tools",
            "Music Information Retrieval (MIR)",
            "high-level content",
            "musical audio-stream",
            "musical content",
            "semantic sense",
            "syntactic sense",
            "general framework",
            "manual annotation"
        ],
        "content": "METHODOLOGICAL CONSIDERATIONS CONCERNING MANUAL \nANNOTATION OF MUSICAL AUDIO IN FUNCTION OF \nALGORITHM DEVELOPMENT \nMicheline Lesaffre1, Marc Leman1, Bernard De Baets2 and Jean-Pierre Martens3 \n1 IPEM: Department of Musicology, Ghent University, Blandijnberg 2, 9000-Ghent, Belgium \nMicheline.Lesaffre@UGent.be \n2 Department of Applied Mathematics, Biometrics and Process Control, Ghent University \n3 Department of Electronics and Information Systems (ELIS), Ghent University\nABSTRACT \nIn research on musical audio-mining, annotated music \ndatabases are needed which allow the development of computational tools that ex tract from the musical audio-\nstream the kind of high-level content that users can deal with in Music Information Retrieval (MIR) contexts. The notion of musical content, and therefore the notion of annotation, is ill-defined, however, both in the syntactic and semantic sense. As a consequence, annotation has been approached from a variety of perspectives (but mainly linguistic-symbolic oriented), and a general methodology is lacking. This paper is a step towards the definition of a general framework for manual  annotation of musical audio in function of a \ncomputational approach to musical audio-mining that is based on algorithms that learn from annotated data. \n1. INTRODUCTION \nAnnotation1 refers to the act of describing content using \nappropriate space-time markers and labeling. Annotation generates additional information that may be useful in contexts of information retrieval and data-mining, either as indices for retrieval or as training data for the development of computational tools. In that respect, annotation is a broad field that covers semantic content as well as labeling and segmentation in function of algorithm development. In the music domain, annotation pertains to the description of metadata and musical features that users might find particularly relevant in the context of music information retrieval.  \nIn the present paper, we will mainly focus on the \nmanual  annotation of musical audio in function of the \ndevelopment, through computational learning, of tools \n                                                          \n \n1 Music annotation is an open term that integrates any information \n(textual, visual or auditory) that can be added to music. In music \ndescription distinctive music characteristics are described in a way that is close to the original music. \n for the automatic generation of similar annotations from \nthe audio.  \nUp to now, there is a general lack of training data and \nthe methodology for manual annotation of musical audio in function of algorithm development is largely under-estimated and under-developed. This is due to the fact that, unlike speech annotation, music is less determined in terms of its content. Unlike speech \nsounds, music is not defined by a limited set of lexical entities. Its syntax is typically depending on multiple constraints that allow a great and almost unlimited variety of forms and structures. Moreover, its semantics are non-denotative and more depending on subjective appreciation. Consequently, the process of manual annotation is rather complex because it comprises multiple annotation levels a nd different possible types \nof content description. The challenge of seeking common ground in the diverse expressions of music annotation has not been addressed thus far. Manual annotation of musical audio indeed raises questions that point to the nature of musical content processing, the context of MIR, and the relationship between natural and cultural constraints involved in musical engagement. \n This paper deals with some methodological \nconsiderations concerning the manual annotation of musical audio in function of algorithm development. First, the general background in musical audio annotation is reviewed. Then a general framework for annotation is sketched and examples are given of experiments that aim at building up an appropriate methodology. An ongoing large-scale experiment, called Music Annotation for MAMI,  is reported and the \nlast section is devoted to a discussion and ongoing work. \n \n2. BACKGROUND \n2.1. Annotation Forms  \nIn the speech community, a large set of annotated \ndatabases has been constructed that proved to be useful for the development of algorithms for speech recognition. Since these annot ations are based on speech \naudio, it is natural to investigate to what extent these tools may be useful for music annotation.  Permission to make digital or hard c opies of all or part of this work \nfor personal or classroom use is granted without fee provided that \ncopies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. \n© 2004 Universitat Pompeu Fabra.   \n \n The Linguistic Data Consortium (LDC)2, for example, \nprovides a list of tools and formats that create and manage linguistic annotations. The Annotation Graph Toolkit [4] is a formal framework that supports the development of linguistic transcription and annotation tools. Handschuh et al. [9] describe a framework for providing semantic annotation to the Semantic Web. Bigbee et al. [3]  review capabilities of multi-modal \nannotation by examining how linguistic and gesture analysis tools integrate video data. They point to the increasing importance of multi-modal corpora and interfaces in future tools. \nTools based on linguistics may be useful for simple \nannotation tasks, but in general, they are not satisfactory. The main problem  is that musical content \ndescription may require many new description formats, which often go beyond the commonly used metadata-based indexing. A simple example is the annotation of the beat, where the annotator has to tap along with the music, and thus annotates while listening to the played music.  \nIn the music domain, there are but a few initiatives \nthat seem to address this problem, such as the SIMAC\n3, \nMAMI and SEMA4 projects. \nIn extending the concept of annotation to music \nanalysis in general, it appears that the literature on music annotation is mainly concerned with linguistic and symbolic descriptions. Few studies have investigated methods for the time synchronous annotation of a musical audio stream. The media industry and researchers involved in content-based music analysis are activel y discussing the needs for \nmusic representation. Currently the Moving Picture Experts Group (MPEG) is starting a new activity aimed at the systematic support of symbolic forms of music representations by integrating Symbolic Music Representation (SMR) into MPEG multimedia applications and formats. The decoding and rendering should allow the user to add annotations to SMR events.  Here annotations are considered as audiovisual objects, music representation elements or simple URL-links. The idea is that the annotation format will be normative, as well as the way annotations are issued by the end-user.  \nApart from the limited number of annotation tools, \nmost tools focus on music notation\n5, like the \ndevelopment of tools for adding specific interpretation symbols to a score such as bowing, fingering, breathes and simple text. Efforts in this context relate to the development of standards for music description, such as MPEG-4, MPEG-7 and MPEG-21. \nA few initiatives have been taken that focus on the \nannotation of musical audio. Acousmograph (GRM)\n6, \nfor example, is similar to a sonogram, offering the user the opportunity to select part of a graph and listen to the \n                                                          \n \n2 www.ldc.upenn.e du/annotation/  \n3 www.semanticaudio.org  \n4 www.ipem.ugent.be  \n5 Music notation refers to the representation of music by a system of \nsymbols, marks or characters. \n6 www.ina.fr/grm/outils_dev/acousmographe/  chosen image. Timeliner [17] is another example of a \nvisualization and annotation tool for a digital music library. It enables users to create their own annotated visualizations of music retrieved from a digital library.  \nIf we then look at the music databases that have been \nannotated using existing annotation tools, it turns out that the main focus has been on metadata description, and not, or less, on the description of musical content as such. Don Byrd maintains a list\n7 as a work-in-progress \nthat surveys candidate MIR collections. Many of these databases, however, alr eady start from symbolic \nrepresentation of music (scores). The Repertoire International des Sources Musicales (RISM), for example, documents musical sources of manuscripts or printed music, works on music theory and libretti stored in libraries, archives, monasteries, schools and private collections and provides images of musical incipits. The RISM Music Manuscript Database is linked to three other databases providing additional information to specific content: Composer, Library Sigla and Bibliographic Citations. The MELDEX Digital Music Library [16] handles melodic or textual queries and offers twofold access to songs. The results of a query are visualized or presented as an automatically compiled \nlist of metadata, such as titles. At the Center for Computer Assisted Research in the Humanities (CCARH) MuseData\n8  has been designed to represent \nboth notational and sound information (MIDI). The Real World Computing (RWC) Music Database [8] is built in view of meeting the need of commonly available databases for research purposes. It consists of 4 databases containing popular music, classical music, jazz music, and royalty free music. Two other component databases were added with musical genre and musical instrument sounds. RWC contains music in both MIDI and audio form and provides lyrics of songs as text files. Standard MIDI files are generated as substitutes for scores, for genres for which no scores are available.  \nTo sum up, most annotation and analysis tools have \npaid attention to linguistic and symbolic oriented annotation, but the picture of music annotation is rather dispersed. There is no clear methodology, nor is the problem domain very well described.  A theory of music \nannotation is lacking. \n2.2. Problem specification \nThe major task of musical audio-mining is to make a \nconnection between the musical audio stream on the one hand and user-friendl y content descriptions on the other \nhand. The main problem is that audio streams are physical representations , while user-friendly \ndescriptions pertain to high-level human information processing capabilities that involve a complex set of goal-directed cognitive, affective and motor actions. Humans typically process musical information in terms of purposes, goal directed actions, values and meanings. \n                                                          \n \n7 http://www.ismir.net/  \n8 www.ccarh.org    \n \n They handle a subjective (first person) ontology that is \nvery different from the objective (third person) ontology of physical signals (see [14] for a more detailed account). \n A major goal of manual annotation of musical audio \nis therefore to provide data that allows computational systems to learn the task of annotation and therefore to build bridges between first person descriptions and third person descriptions of music. Modelling based on imitation learning is considered a candidate to cope with the gap between the measurable quantities of an audio signal and the intentionality of subjective qualities. \n \n3. GENERAL FRAMEWORK  \n3.1. Context dependencies  \nThere are at least three obs ervations to keep in mind \nwhen dealing with music annotation, namely (1) the intentional nature of human communication, (2) the requirements of music information retrieval contexts, and (3) the development of mediation technology.  \nFirst of all, since annotation aims at making the link \nbetween the musical audio stream and levels of content description that allow humans to access the information stream, it is necessary to take into account the highly focused level of human communication. This level is called the cultural  level because the implied ontology is \nbased on human learning, subjective experiences and symbolization. Due to the fact that this level is characterized by goal-oriented behavior and intentional attitudes (thoughts, beliefs, desi res, …) its descriptions \nare therefore very different from objective or nature-driven descriptions that pertain to physical signals. As a consequence, there are two methodologies involved: \n\u001f Naturalistic approaches  (studied in the natural \nsciences) aim at developing tools that extract nature-driven descriptions from audio. These tools are objective in the sense that  they start from physical \n“energies” and rely upon “universal” principles of human information processing. The resulting descriptions have an inter-subjective basis and do not involve the subjective goal-directed action ontology on which human communication patterns typically rely. Examples are the extraction of pitch from a sung melody, or the extraction of timbre classes from polyphonic audio. \n\u001f Culturalistic approaches  (studied in the human \nsciences), in contrast, tend to describe music in terms of its signification, its meaning, value, and role as cultural phenomenon. Thus far, culture- determined content description has been strongly linguistic-symbolic oriented, based on textual and visual descriptors. Reference is often made to subjective experience and historical and social–\ncultural interactions.  Some culturalist musicologists from the postmodern \nschool tend to claim that links between physical descriptions and subjective descriptions of music are impossible.  Yet, there is no strong proof of evidence for \nsuch statement. The main  argument draws on the idea \nthat signification (attribution of meaning to music) is an arbitrary activity that is depending on the cultural environment, the history and the personal taste. Association and signification can be wild indeed, but we do believe that there are at  least certain aspects of \ndescriptions, including descriptions at the high semantic (first person) levels, that are not completely arbitrary, and that, given a proper analysis of the goals, can be very functional in MIR contexts. When aiming at semantic description of music, this hypothesis is rather fundamental, and skepticism can only be refuted when the proof has been given of a working system.   \nA second observation, and closely connected to the \nfirst point, is that music descriptions serve a goal that is largely determined by the MIR context . Out of a myriad \nof possible natural objective descriptions, and perhaps also subjective descriptions, we should select those that serve the particular goals of the particular context. Hence, research in audio-mining is not purely a matter of bottom-up signal processing and making the link between third person and firs t person descriptions. At a \ncertain moment, a thorough analysis has to be made of the retrieval context, the economical value, the ethical value, the purpose etc… and decisions may have to be taken about the context-based bias of the whole enterprise, including the work on manual annotation. Reference can be made to the DEKKMMA-project \n9 \nwhere researchers are confr onted with a large audio \ndatabase of Central African music, and where little experience is available of how people would tend to \nsearch, and what  they would tend to search, in such a \ndatabase. It is likely, but analysis has to clarify this, that users who are unfamiliar with the Central African idiom behave very differently from users that know the music.  \nA third observation is concerned with the technology \nused for music mediation. Mediation refers here to the ways in which streams of musical information are transmitted and the tools that may be used to specify content and to retrieve that  content. The most recent \ndevelopments seem to go in the direction of networked wireless mobile devices giving access to a large amount of music in databases.  Such technologies may imply certain constraints on the possible types of musical content specification (see e.g.  [2] for experiments with \nPocket PC). \nTo sum up, given the aims of a MIR system, \nannotation should take into  account at least three \ndifferent types of context, namely culture , user, and \nmediation . Given that background, Figure 1 shows the \ngeneral framework for annotation of musical audio that incorporates the naturalistic and culturalistic approaches with their focus on human information processing and \n                                                          \n \n9 www.ipem.ugent.be    \n \n social-cultural context, respectively (Adapted from \n[14]). \n \nFigure 1.  General framework for annot ation of musical audio. \n3.2. Computer Modeling Approach \nApart from the general framework in which annotation \nhas to be carried out, there is another framework that needs careful analysis, name ly that of computational \nmodeling. In the past, the problem of manual annotation of musical content has ofte n been considered from the \nviewpoint of a Cartesian  modeling strategy. The strategy \nconsists in the specification of a set of pre-defined feature extractors (the clear  and distinct ideas of \nDescartes) with limited scope to clear meaning in a restricted context (most often) of stimulus-response experimentation. It is then hoped that through combination of a selected set of weighted pre-defined features, high-level semantic knowledge can be predicted. However, it turns out that this strategy has a number of limitations [12] such as a complicated semantic interpretation when features are summarized or \ncombined (linearly as well as non-linearly). If no significant meaning can be given to these features it may be better to give up the idea of working with many local descriptors and look for alternative methods. Pachet & Zils [18] explore an alternative method using an automated processing operators composition method in the framework of genetic programming. In general, however, we believe that straightforward imitation learning based on an appropriate level of manual annotation may be of help.  \nTaking into account the multiple ways in which users \ncan engage with music, annotation should extend the possibilities of linguistic-symbolic descriptions with non-symbolic and non-linguistic forms of description.  This draws on the understanding that the interaction between subjective experience and objective description is a dynamic process constrained by both natural and cultural determinants and that, somehow, levels of annotation in between what is considered to be natural and cultural processing should be chosen. The ecological approach indeed regards any response to music as the result of a complex interaction of the subject in its social-cultural and physical environment. Levels of annotation can be addressed that indeed lie on \nthe borderline of objective/subjective descriptions and that form the connection points with first person and third person descriptions of music.  \nThis calls for an investigation towards new forms of \nannotation based on mimetic and gesture capabilities of human communication. Performing a manual annotation in the form of motoric action subsumes the interconnectedness between culture-based and nature- based computational music research approaches. \n3.3. Representation levels \nAnnotation comprises diverse types of description, \ndepending on the purpose of the description and the level of annotation. The various types of music annotation are related to syntactic, semantic, structural and articulation elements. Figure 2 shows the distinguished representation levels and associated annotation methods. \n \nFigure 2.  Representation levels and associated annotation \nmethods. \n \n\u001f The symbolic/linguistic-based annotation  has \nmainly a focus on the description of structural and semantic units. The user's interaction to symbolic representation relates to verbal and visual descriptors. This could include a score, or conventional music notation machine code (i.e. MIDI, SASL). The main problem of this approach is that symbols are depriv ed of any semantic \nconnotation and that they require human interpretation based on the inter-subjective semantics. Annotation thus relies on subjective experience of the creator or user who performs an interpretational and intentional action. \n\u001f The sub-symbolic -based annotation  is mainly \nbased on multi-dimensional spaces. Some studies focus on representation forms such as sonification and visualization. Toiviainen\n10 for example explored \n                                                           \n10 http://www.cc.jyu.fi/~ptoiviai/  \n   \n \n the additional value of visual data mining for large \nmusic collections. Through exploration of multiple \nmusic dimensions he found that some musical features are more natural oriented and other more cultural.  Pampalk [19] presents a visualization method that uses self-organizing maps for grouping similar pieces pertaining to  different music genres. \nManual annotation involved motor action that is placing of the pieces on a map according to personal style.  \n\u001f Mimetic annotation  is related to imitative aspects \nof motor actions. Imitation behavior in general is a topic of growing interest within the cognitive sciences [22]. The process of imitative learning is based on perceiving real world behavior and learning from it through action. Applied to music research, imitation is a means for analyzing the perception of similarity aspects within music through motor responses. The now popular Query by Voice paradigm supports the idea of retrieving music by vocal imitation of representative melodies (see e.g. [5], [15], [20]). \n\u001f Gestural  annotation  accounts for representation as \nthe result of multi-modal gesture-based interaction and emotional expressiveness [13]. The distinction with mimetic annotation is that it needs not be learned or rehearsed. Gestural annotation involves motoric action as a physical  manifestation of the \nsonorous, such as body movement or dancing.  \nModeling  gesture annotation takes into account time \ndependencies through which it mainly applies to the representation of  rhythmic features. \nAt the gestic level following annotation forms are distinguished: \n-sound producing action  such as tapping, \nhitting and stroking -sound accompanying action  such as body \nmovement and dancing \n4. EXPERIMENTAL INVESTIGATION \nIn view of a search and retrieval technology based on \ncomputational learning algorithms that draw upon the notion of imitation, a range  of annotation possibilities \nare currently being studied . In what follows some \nexamples of experimental research, mainly conducted at Ghent University, are give n. The focus is on the \nexpertise level of the annotator and the role of different manual annotation methods with relation to the development of computational algorithms.  \n4.1. Annotation Subjects \nA global distinction can be made between annotations \nmade by experts  (musicologists, performers, teachers, \nand librarians) and naive  or common users. Expert \nannotation has the advantage of being precise and consistent, but often it is singular. The first person \ndescriptions of musicologists, performers and composers are not necessarily shared w ith those of naive listeners. \nThe latter may perceive the same music completely different. In a similar way distinctions can be made between users who know the musical idiom and users who don’t know the idiom.  \nFurthermore, an annotator can make personal \nannotations or can use annotations that are provided by others or automatically generated, in a bootstrap process. Tzanetakis & Cook [24] describe a tool for semi-automatic audio segmen tation annotation. A semi-\nautomatic approach combines both manual and automatic annotation into a fl exible user interface.   \nThe background of the annotator is a determining \nfactor in the annotation process. It is in view of the cultural goal of the annotation that decisions can be taken whether expert annotators or naive annotators are most appropriate. Much depends on the goal and the task of the annotation. \n4.2. Annotation Experiments \n4.2.1. Linguistic / Symbolic description \nAt the level of semantic labeling, the use of terminology \nin the form of selecting or naming keywords is a complex issue. Semantic features don't have a clear meaning and are not universally applicable. Beyond metadata such as title, composer, genre and year there are narrative descriptions of music that draw on the appreciation and subjective e xperience of users. Recent \nstudies show interest in the terminology used by non-music experts indicating features of music. Kim [11] investigates people's perception of music by means of categorizing the words they use. Bainbridge et al. [1] analyze the questions and answers by which users of MIR systems express their needs. It was found that users are uncertain as to the accuracy of their descriptions and \nexperience difficulty in coming up with crisp descriptions of musical categories such as genre or date. Using the power of the Internet, MoodLogic\n11 developed \na music meta-database with song information available to users. The MoodLogic user community was questioned through surveys on how they feel about songs and artists, and these answers were collected in a massive database containing information on mood, tempo, genre, sub-genre and beat.  \nLeman et al. [12] present an empirical study on the \nperceived semantic quality of musical content in which subjects had to judge musical fragments using adjectives describing perceived emotions and affects. Subjects had to evaluate semantic qualities, presented as 15 bipolar adjectives, on a 7-point scale. More recent results reveal that prediction of affective qualities attributed to music may be possible and therefore usable \n                                                          \n \n11 www.moodlogic.com/    \n \n in MIR-contexts, but results could possibly be improved \nusing induction-based learning paradigms.   4.2.2.  Melody imitation \n A Query by Voice experiment [15] was conducted that generated an annotated database of 1500 vocal queries, which is freely available on the Internet. For musical imitations in the form of vocal queries user-based and model-based annotation wa s performed. User-based \nannotation provided content about the spontaneous\n \nbehavior of users and model-oriented annotation provided descriptions as a referential framework for \ntesting automatic transcription models. For model-based annotation the PRAAT transcription and annotation tool for speech analysis [6] was used. PRAAT takes in an audio file, allows marking of segments and typing in words. The features inve stigated for vocal query \nannotation were segmentation (events), onset time and onset reliability, frequency, pitch stability, query method and sung words or syllables. The results have been used for training the MAMI melody transcriber [7].\n \nMelody imitation is also a useful method for handling \nthe problem of annotation of polyphonic music in that monophonic melodic imitation might serve as reference material. The capability of professional singers for imitating melodies pertaining to different voices has been tested in a pilot study . Each of the eight singers \ninvolved had to study the same ten songs. Then participants were asked to imitate as well as possible the main melodies, bass lines and possibly other melodies that they considered important. The files were then transcribed using the MAMI automatic transcription tool [7]. Statistical analysis shows that professional singers can imitate main and other relevant melodies quite well but imitations of the bass lines are less accurate. Another problem imposed by perception \nissues is the non-consistency in choosing other relevant melodies. [21] \n4.2.3.  Tonality annotation \nIn a recent study by Toiviainen and Krumhansl [23] \nsubjects manipulated a virtual slider while listening to a musical fragment (Bach) that contained pulsing probe tones. This method has shown to be interesting because \nit has the advantage of being based on the theory of analysis of time-series, whic h is preferable to a scale \nwith a limited amount of steps. At Ghent University a further stage tonality description has been studied which aims at generating manual annotations as natural response [10]. The subjects (26) had to listen to 20 short (60 sec.) fragments of classical and non-classical music (fifty-fifty) and were  asked first to sing the best fit and \nsecond to express their appreciation. In the first part, it was suggested that participants would sing low, soft and long tones. Appreciation was measured by means of judgment on a seven-point scale of pairs of adjectives related to emotion and tonality features. Graphic \nannotation was also explored. While they listen to the same music as before, participants had to draw a line which represents the course of the melody. There was a remarkable correspondence among the patterns in the drawings which points to the inter-subjectivity of mental structures when people are acting in the same context and under equivale nt conditions. \n4.2.4.  Rhythm annotation \nAn ongoing study at our laboratory deals with drum \nannotations of real music recordings. Aiming at providing ground truth measures for drum detection in raw musical audio the method of annotation by imitation has been tested. A professional drummer imitated, by use of an electronic drum kit, the drums he heard in 4 entire pieces of music. Beside s that 5 drum loops were \nalso annotated in the same way. To obtain maximum accuracy the music was given beforehand to the player \nwho was expected to study the drum part. From analyzing the files some pr oblems raised due to cross \ntalk between drums, poor synchronization between the signals (from 20 up to 80 msec. ) and insertion of notes \ngenerated by the pedal as a result of body movement. The recorded files needed additional manual checking. \nManual annotation of pol yphonic musical pieces \ncontaining drums needs to be done by percussion experts and is very time consuming. Future work might include similar studies in which the player himself also manually \ncorrects the recordings using a sequencer program. \n4.2.5.  Multiple annotation forms: Music Annotation \nfor MAMI \nIn context of the MAMI and SEMA project, a new \nlarge-scale annotation experiment has been set up. Music Annotation for MAMI  is a study that investigates \nthe requirements for the development of a system that relies on multiple forms of textual and motoric music annotation. The experiments that are currently conducted aim at collecting a large amount of annotated data that rely on syntax, genre description and appreciation. Focus is on the exploration of the usability of several annotation methodol ogies that may facilitate \nhandling music databases.  \nTo begin with, an explorative study in the form of an \nonline inquiry has been done to recruit a large group of subjects willing to particip ate in diverse annotation \nexperiments spread over se veral months. Until now 717 \npersons, aged between 15 and 75 year, filled in the inquiry of which 663 (300 male and 363 female) are willing to participate in the experiments. The questionnaire provides individual profiles by collecting following information:  \n \n•  socio-demographic info \n•  cultural background \n•  acquaintance with the Internet \n•  musical background    \n \n •  music preferences  \nIn addition people are requested to provide titles of their preferred music together with the composer or performer of the piece. Indication of the genre they think the piece belongs to is asked as well.  For each title two sets of 5 bipolar adjectives related to emotion, expression and style are presented to be judged on a 7-point scale. \nStatistical analysis will lead to the distinction of \nspecific user groups. The main selection criterion is the formation of equally dist ributed groups according to \nage, gender, cultural background, music education, music experience and genre preferences.  \nPeople’s favorite music is the starting point for the \ncreation of a large database containing a large number of music fragments of 30 seconds. For different experimental issues various subsets of this database are used. Ongoing experiments focus on mimetic (rhythm and melody imitation) and symbolic-linguistic (perceived semantic qualities) annotation.  \nTable 1 summarizes music characteristics and \nannotation methods involved in ongoing and future experiments. They relate to  the conceptual framework \nof a taxonomy worked out within the context of audio mining [15].  \n \nTable 1.  Music characteristics and annotation methods \ninvolved in the ‘Annotation for MAMI’ experiment. \n \n5. DISCUSSION AND ONGOING WORK \nAccess to large music databases requires interoperable \ndata representations and methods for handling new description formats for querying and delivering musical data. Nowadays audio databases are still in a stage of simple annotation methodology.  Existing systems only provide tools that allow metadata-based indexing in view of searching by name or format. Most audio is represented in compressed formats such as MP3, RealAudio, QuickTime etc., and only little research concentrates on real audio. Annotations for musical files usually include information about performer, title, year \netc. and are not satisfactory in more sophisticated search and retrieval. For this purpose new descriptors \nassociated with audio signals have to be developed. \nA scan of the literature on annotated databases that \nwould allow the training of computational systems however reveals that probably only a few of such databases are available and moreover that they are limited in scope. The current state-of-the-art suffers \nfrom a lack of a well-defined conceptual frame that supports (learning) the mapping of the interconnectedness of diverse conceptual levels. \nIt has been argued that new music annotation \nmethodology is likely to have strong influence on the improvement of information search and retrieval processes and on the most efficient system’s usability possible. The development of an annotation system that deals with the interconnectedness between culturalistic and naturalistic approaches might benefit from elaborated exploration of manual annotation and new description methods. An attempt is made to define a general framework for modeling based on imitation learning. It is estimated to facilitate easy computerized handling of large music databases. Previous annotation studies have proven that the use of more advanced methods based on mimetic and gesture skills lead to promising results. These studies are the first steps towards modeling of the relationships between high-level content descriptions and stream-based musical audio. However, the value of this paradigm is only estimable when a large amount of annotated musical data from different user groups is available. The currently conducted Music Annotation for MAMI experiments deal with this issue. It involves multiple \nmanual annotation of a music database incorporating linguistic-symbolic and sub-symbolic descriptions.  \nAcknowledgements \n \nThis research has been conducted at IPEM, Department of musicology at Ghent University in the framework of the MAMI project for audio recognition. We are grateful for the financial support given by The Flemish Institute for the Promotion of Scientific and Technical Research in Industry.  \n \n6. REFERENCES \n[1] Bainbridge, D., Cunningham S. J. and Downie \nJ. S. “Analysis of queries to a Wizard-of-Oz MIR system: Challenging assumptions about what people really want”,  Proceedings of the \n4th International Conference on Music Information Retrieval, Baltimore, 2003. \n[2] Baumann, S., and Halloran J. “An ecological \napproach to multimodal subjective music similarity perception ”, Proceedings of the \nConference in Interdisciplinary Musicology (CIM) , Graz, 2004.   \n \n [3] Bigbee, T., Loehr D., and Harper L., \n“Emerging Requirements for Multi-Modal Annotation and Analysis Tools”, Proceedings, \nEurospeech 2001 Special Event: Existing and Future Corpora -- Acoustic, Linguistic, and Multi-modal Requirements, Aalborg, Denmark, \n2001. \n[4] Bird, S. and Liberman, M. ''A formal \nframework for linguistic annotation”, Speech \nCommunication , 33(1,2), 2001.  \n[5] Birmingham W., “MUSART: Music Retrieval \nVia Aural Queries”, Proceedings of the 2nd \nInternational Conference on Music Information Retrieval, Bloomington, 2001. \n[6] Boersma, P., and Weenink,  D. (1996). Praat. \nA system for doing phonetics by computer . \nAmsterdam: Institute of Phonetic Sciences of the University of Amsterdam. Retrieved July 31, 2003, from http://www.praat.org\n. \n[7] De Mulder, T., Martens J.P., Lesaffre M., \nLeman M. and B. De Baets “An auditory model based transriber of vocal queries”,  \nProceedings of the 4th International Conference on Music Information Retrieval, Baltimore, 2003. \n[8] Goto, M., Hashiguchi H., Nishimura T., and \nOka R. “RWC Music Database: Music Genre Database and Musical Instrument Sound Database” . Proceedings of the 4th \nInternational Conference on Music Information Retrieval, Baltimore, 2003. \n[9] Handschuh, S., Staab S. and Volz R., “On \nDeep Annotation”. Proceedings of the 12th \nInternational World Wide Web Conference, WWW 2003, Budapest, Hungary, 2003 . \n[10]  Heylen, E. et al. Paper in progress \n[11] Kim, J.-Y. and Belkin, N. J. “Categories of \nMusic Description and Search Terms and Phrases Used by Non-Music Experts”,  \nProceedings of the 3th International Conference on Music Information Retrieval, Paris, 2002. \n[12] Leman, M., Vermeulen V., De Voogdt L., \nTaelman J., Moelants D. and Lesaffre M., “Correlation of gestural musical audio cues and perceived expressive qualities”, in A. Camurri and G. Volpe (Eds.) Gesture-based \ncommunication in human-computer interaction.  Berlin, Heidelberg, Springer-\nVerlag, 2003, 40-54. \n[13] Leman M., and Camurri A., “Musical content \nprocessing for Interactive Multimedia”, proceedings of the  Conference on \nInterdisciplinary Musicology,  Graz, 2004. [14] Leman, M. From Music Description to its \nIntentional use . Manuscript, 2004. \n[15] Lesaffre M., Tanghe K., Martens G., Moelants \nD., Leman M., De Baets B., De Meyer H. and \nMartens J.-P.  \"The MAMI Query-By-Voice \nExperiment: Collecting and annotating vocal queries for music information retrieval\",  \nProceedings of the 4th International Conference on Music Information Retrieval, Baltimore, 2003. \n[16] McNab, R. J., Smith L. A., Bainbridge D. and \nWitten I. H. “The New Zealand Digital Library MELody inDEX”, D-Lib Magazine , 1997. \n[17] Notess, M., and Swan, M., “Timeliner: \nBuilding a Learning Tool into a Digital Music Library”, Accepted to the 2004 ED-MEDIA \nWorld Conference on Educational Multimedia, Hypermedia & Telecommunications to be held Lugano, Switzerland, 2004. \n[18] Pachet, F. and Zils, “A. Evolving automatically \nhigh-level music descriptors from acoustic signals”, in Computer Music Modeling and \nRetrieval: International Symposium, CMMR 2003 , Berlin, Heidelberg  Springer-Verlag  \nLNCS, 2771, 2003, 42-53. \n[19] Pampalk, E., Dixon S. and Widmer G. \n“Exploring music collections by browsing different views”, Proceedings of the 4th \nInternational Conference on Music Information Retrieval, Baltimore, 2003. \n[20] Pauws S., “CubyHum: a fully operational \n\"query by humming\" system”,  Proceedings of \nthe 3th International Conference on Music Information Retrieval, Paris, 2002. \n[21] Stijns, F. et al. Paper in progress \n[22] Tomasello M.  The Cultural Origins of Human \nCognition. Harvard University Press. \n[23] Toiviainen, P. and Krumhansl, C.L. \n“Measuring and modeling real-time responses to music: The dynamics  of tonality induction”, \nPerception,  32, 6,\n 2003. \n[24] Tzanetakis, G. and Cook P. “Experiments in \ncomputer-assisted annotation of audio”, Proceedings International Conference Auditory Display (ICAD),  Atlanta, Georgia, \n2000."
    },
    {
        "title": "Improving Melody Classification by Discriminant Feature Extraction and Fusion.",
        "author": [
            "Ming Li",
            "Ronan Sleep"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416728",
        "url": "https://doi.org/10.5281/zenodo.1416728",
        "ee": "https://zenodo.org/records/1416728/files/LiS04.pdf",
        "abstract": "We propose a general approach to discriminant feature extraction and fusion, built on an optimal feature transformation for discriminant analysis [6]. Our experiments indicate that our approach can dramatically reduce the dimensionality of original feature space whilst improving its discriminant power. Our feature fusion method can be carried out in the reduced lower- dimensional subspace, resulting in a further improvement in accuracy. Our experiments concern the classification of music styles based only on the pitch sequence derived from monophonic melodies.",
        "zenodo_id": 1416728,
        "dblp_key": "conf/ismir/LiS04",
        "keywords": [
            "discriminant feature extraction",
            "discriminant analysis",
            "optimal feature transformation",
            "dimensionality reduction",
            "discriminant power",
            "feature fusion method",
            "reduced lower-dimensional subspace",
            "accuracy improvement",
            "classification of music styles",
            "pitch sequence"
        ],
        "content": "IMPROVING MELODY CLASSIFICATION BY \nDISCRIMINANT FEATURE EXTRACTION AND FUSION\nMing Li Ronan Sleep \nSchool of Computing Sciences \nUniversity of East Anglia  \nNorwich, NR4 7TJ, U.K. \n(mli,mrs)@cmp.uea.ac.uk \nABSTRACT \nWe propose a general approach to discriminant feature \nextraction and fusion, built on an optimal feature transformation for discriminant analysis [6]. Our experiments indicate that our approach can dramatically \nreduce the dimensionality of  original feature space \nwhilst improving its discriminant power. Our feature fusion method can be carried out in the reduced lower-dimensional subspace, resulting in a further improvement in accuracy. Our experiments concern the classification of music styles based only on the pitch sequence derived from monophonic melodies. \n1. INTRODUCTION \nMonophonic melody is an ab straction of a piece of \nmusic, consisting of a sequence of pitches together with timing information. For the best classification performance, both characteristics will be used. However, it is interesting to note that Barlow and Morgenstern’s dictionary of musical themes [8] is based solely on the pitch sequence (or pitch contour ), ignoring timing \ninformation entirely. We decided to investigate the potential of pitch contour with respect to a music genre classification task. \nEven without timing information,  there remains a \nwealth of possible feature sets to take as a basis for classification. This creates high-dimensional feature spaces, which present problems for many conventional learning algorithms. Thus, dimensionality reduction methods are called for. These map a vector in high dimension space into a lower dimension space using some transformation. \nInstead of using a single transformation, we used a \nhybrid scheme. First we transformed the original feature space into a low-dimensional subspace. Next we computed discriminant vectors in the reduced dimension subspace in terms of separati on of pre-defined classes. \nA similar scheme was previously applied to document classification [1]. The hybrid approach allowed us to explore various combinations of data reduction and feature selection techniques in pursuit of our goals, which are: (1) to \nexplore the discriminatory power of pitch sequences alone; (2) to explore the e ffectiveness of an optimal \ndiscriminant feature extr action technique on accuracy \nimprovement in music classification; (3) to identify a suitable framework of hierar chical classification with \ndiscriminant feature extraction and fusion. \n2. HYBRID SCHEME FOR DISCRIMINANT \nFEATURE EXTRACTION AND FUSION \nA good music style classification cannot be achieved by \nconsidering only a single type of feature such as n-gram pitch histogram and melody contour alone. However, building a composite feature based on simple concatenation of multiple feature sets leads to high (and perhaps variable) data dimensionality. To handle this we developed a hybrid scheme, which aimed to combine only the most informative f eatures from each feature set \nwhile preserving the overall discriminant power of original features. Figure 1 shows the general architecture of our scheme. Note that it allows us to work with a wide range of features, ranging from raw statistical models (the bi-gram model in Fig. 1) to descriptive statistics drawn up by musical experts. \n \nFigure 1 . A general approach for discriminant feature \nextraction and fusion. \n2.1. Pitch-based Feature Construction  \nIn this work, two types of melody features were \nconsidered: bi-gram pitch features and knowledge-based pitch features.  The first feature set consists of statistics taken from a bi-gram model of the sequence. The second is based on 11 top-level properties listed in Permission to make digital or hard c opies of all or part of this work \nfor personal or classroom use is granted without fee provided that \ncopies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. \n© 2004 Universitat Pom\npeu Fabra.  \n \nTable 1. Value distributions of first 8 properties were characterised by descriptive statistics which include: median, maximum, minimum, mean, standard deviation, mean absolute deviation, scope\n1, interquartile range, the \nmost frequent value, the number of distinguished values, dominant interval\n2 and dominant percentage3. Our \nchoice of the second feature set builds on previous work (e.g. [7]). Note that our hybrid scheme is not limited to handling these particular feature sets: .it can be extended naturally to incorporate other information such as timing and harmony. \nTop-level \nproperties: Description: \nAbsolute Pitch  From 1 to 128 in MIDI code.\nPitch Class Represented by numbers \nranging from 1 to 12. \nUpward / Downward Pitch Interval Increased / decreased   value \nbetween two consecutive absolute pitches. \nPerfect Fifth Consecutive pitch classes \nseparated by fifths \nDuration of Upward / Download / Flat Interval The number of consecutive \nnotes with increased / decreased / iden tical value in \nabsolute pitch \nDuration Percentage The percentage of the total \nupward / downward / flat duration within the whole melody. \nTable 1.  Expert-derived Properties of Melody \n2.2. Discriminant Feature Extraction \nFeature extraction techniques can be divided into two \ngroups according to whether or  not they are guided by \nclass labels.  Principal Component Analysis (PCA) and Random Projection (RP) do not consider class information. Consequently there is a risk that the resulting spaces will not c ontain good features for \ndiscrimination. On the other hand techniques which do use class labels such as Linear Discriminant Analysis (LDA) may be unduly affected by errors or bias in the class labelling. \nA further consideration is the ability of such \ntechniques to handle cases where the dimensionality of the raw data is greater than the number of observations available. In our cased, for example, we have over 3000 dimensions but less than 800 midi files in our data set. Moreover, classical Fisher LDA has two limitations: (1) the dimensionality of the samples is set by the number of classes; (2) the basis vectors are not guaranteed to be orthogonal. Okada and Tomita [3] and Duchene and Leclercq [6] point out that th is classical solution is not \nnecessarily the best, and indicate that an orthogonal set of vectors is preferable in terms of both discriminant \n                                                          \n \n1 Interval between maximum and minimum value \n2 The interval between two most frequent values \n3 The percentage of the most frequent value. ratio and mean error probability. Duchene and Leclercq \n[6] give a direct analytic  solution for calculating the \noptimal set of orthogonal discriminant vectors, which can be as many as the dimension of the original feature space. \nMotivated by above concerns, we used our hybrid \nscheme for discriminant feature extraction to break processing down into stages as follows: \n(1) First, we used PCA or RP to scale down the high \ndimensionality of the origin al feature space. We also \nused RP to do a rough and ready dimension reduction, then applied PCA to further reduce the dimensionality in a more directed manner. As shown in Figure 2, this scheme gives similar performance to PCA over all the dimensions. In [2] the author mentions the same idea in the context of removing the problems in the case of highly eccentric clusters to which PCA is particularly susceptible; \n(2) Working in the low-dimensional subspace created \nby the first step, we applied an optimal transformation for discriminant analysis (ODT [7]), which is an \nextension of Fisher’s LDA [6]. In ODT, the optimal set of orthogonal discriminant vectors can be as many as the dimension of the original feature space.\n \n2.3. Discriminant Feature Fusion \nBased on the hybrid scheme described in section 2.2, \nfeature fusion can be perform ed more efficiently and \neffectively by operating in a low-dimensional subspace. We explored two routes: \nz Fusion with Concatenation : concatenate the \ndiscriminant features ex tracted from every single \nfeature set; \nz Fusion with Transformation : apply the hybrid \nscheme for discriminant feature extraction a \nsecond time  over the composite feature set \ngenerated above. This is motivated by a concern that the results generated may not be the projection along the optimal discriminant vectors within the new composite feature space. \n3. EXPERIMENTAL RESULTS \n3.1. Experimental Design \n749 MIDI files falling into 4 categories were collected \nfrom the internet. Two of the categories were western classical music composed by Beethoven (289 files) and Haydn (255 files). The remaining categories were Chinese music (80 files) and Jazz (125 files). Using the principal track from the MIDI file, a pitch sequence was obtained and its element was represented by a number from 1 to 128. If two pitch events overlapped in time, only the highest value was retained. Recall that this removes information about the note length but retains the ordering and pitch of the notes. \nA normalized bi-gram feature set with 3045 \ndimensions was generated after removing elements occurred less than 10 times in  the whole collection. For   \n \nthe knowledge-based set, 99 features were extracted and scaled to range [-1, 1]. \nA standard three-fold stratified cross validation \n(CV)\n1 was carried out to eval uate the classification \nperformance2. The performance of random projection \nwas averaged over 3 runs in all 3 iterations. \nAs the classifier, we used the Support Vector \nMachine (SVM) 3. This is a binary discrimination \ntechnique, which we applied to our multi-class problem by singling out one class at a time as positive examples, treating the other classes as negative. \n3.2. Three Experimental Results and Analysis \n3.2.1.  Hybrid Scheme for Feature Reduction \nThis experiment compared the performance of various \nfeature reduction schemes applied over the high-dimensional bi-gram feature set. In PCA+RP, the starting point for PCA is the 1300-dimensional representation generated by RP. The starting point for ODT is a 450-dimensional representation generated by PCA, applied so as to account for 99% of the variance. As shown in Figure 2: \nz RP at 8 is as good as the baseline performance of \n61% error (all Beethoven). It gets close to the original error rate (32.66%) using less than half the dimensions of the raw data. However, this number is still too large to carry out the eigenvector-based method for discriminant feature extraction. Notice that, the variance is high at low dimensions depending whether and how pertinent information happened  to be captured by RP; \nz Over all the dimensions, RP+PCA gives similar \nperformance to PCA, and similar relation is observed between RP+PCA+ODT and PCA+ODT. This suggests it is advisable to use RP first to reduce original feature space quickly and then \napply PCA to further reduce the dimensionality. This also can save much computational cost without sacrificing too much performance; \nz PCA+ODT generate the best result with the lowest \ndimensionality. Its error rate is even slightly better than original feature se t, which indicates its \nadvantage over PCA/RP in finding good discriminant features and also reveals its ability in removing noisy (or irrelevant) information. This capability is also reflected in Figure 3 (see \n                                                          \n \n1 The dataset is randomly split into 3 mutually exclusive subsets, which \ncontain approximately the same propor tions of labels as the original \ndataset  \n2 10-fold CV is also commonly used in many works over music \nclassification and a better experiment al result could be obtained since \nmore data are used for training. However, fewer test data means that the confidence interval for the accuracy will be wider. Thus, we still \nprefer 3-fold stratif ied CV in this work\n \n3 One implementation called SVMTorch is chosen which is available at \nftp://ftp.idiap.ch/pub/l earning/OldSVMTorch.tgz . The entire \nconfiguration is used as default (e.g. Gaussian kernel with C=1).  pairwise comparison of feature set <3,4> and <5, \n6/7>). \n \nFigure 2 . Classification error ra te over pitch bi-gram \nfeature set  \n3.2.2.  Discriminant Feature Fusion \nOur preliminary experimental results show that, bi-gram \nfeatures and knowledge features result in similar accuracy. However, the former performed better at separating two classical composers. In contrast, the latter did better at distinguishing Chinese music from others. Thus, a better performance can be expected by the combination of both feature sets. In Figure 3, we can see that, on average, the performance of composite feature sets (5-7) is 5% better than that of single un-combined feature sets (1 -4). This validates the \neffectiveness of discrimina nt feature fusion on accuracy \nimprovement. Moreover, some accuracy improvement is also observed in fusion with transformation (set 7 in Figure 3) over fusion with concatenation (set 6 in Figure 3). \nNote that, in this experiment, fusion with \nconcatenation produced a f eature set with a better \nperformance than either. However, this is not always true. Given a noise sensitive classifier like K-Nearest Neighbour, performance degradation may occur. Although there is more useful information in the combined feature vector, ther e is also more noise. That \nis why a further discriminant feature extraction is suggested, in the hope that this may counteract the side effect of feature fusion w ith simple concatenation. \n3.2.3.  Hierarchical Classification of Music Style \nPrevious work in [5] has found that, by utilizing known \nhierarchical structure, a classification task can be \ndecomposed to a sequence of sub-problems that can be solved more efficiently and with improved classification \naccuracy. We took a brief look at this issue. In our experiment, a hierarchical  model for music style \nclassification was manually designed and compared   \n \nwith the flat model. In the hierarchical model, a \nsequence was first categorised as Chinese, jazz or western music. Then, we stern music is further \nsubdivided into Beethoven and Haydn. All the comparisons were based on composite feature vectors generated by both methods desc ribed in section 4.1. As \nshown in Figure 3, a slight improvement in accuracy was obtained by the hierarchical model over the flat model.  \n \nFigure 3 . Performance of discriminant feature fusion and \nhierarchical classification. \n4. CONCLUSION AND FUTURE WORK \nThis work demonstrates the effectiveness of our \nproposed hybrid scheme for discriminant feature extraction and fusion. The classification performance \nwas evaluated using SVM with both flat and hierarchical models. The e xperimental results indicate \nthe improvement in accuracy for extracted features over \noriginal features, for composite feature set over single un-combined feature set, a nd for hierarchical model \nover flat non-hierarchical model. \nWe emphasise that the experiments reported here \nutilise only pitch-based sequence information. Temporal features such as note dura tion were excluded. In the \nfuture, the effectiveness of this discriminant feature \nfusion will be further evaluated when more features (including note duration) are incorporated. Moreover, some techniques for non-linear feature extraction and automatic hierarchical model construction will be investigated. 5. ACKNOWLEDGEMENTS \nThe authors would like to thank the anonymous \nreviewers for their careful reading of the paper and \nsuggestions for improvement. Thanks are also due to Graham Tattersall, Stephen Cox and Kris West for many useful discussions. \n6. REFERENCES \n[1] Kari Torkkola. \"Linear discriminant analysis in \ndocument classification\", Workshop on Text \nMining (TextDM'2001), http://www-ai.ijs.si/ \nDunjaMladenic/TextDM01/.. \n[2] S. Dasgupta. \"Experiments with random \nprojection\", Sixteenth Conference on \nUncertainty in Artificial Intelligence (UAI),  \n2000. \n[3] T. Okada and S. Tomita. \"An optimal \northonormal system for discriminant analysi s\", \nPattern Recognition , 18(2):139--144, 1985. \n[4] Ranan Collobert and Sany Bengio. \"SVMTorch: \nsupport vector machines for large-scale regression problems\", Journal of Machine \nLearning Research , 1:143-160, 2001. \n[5] S. T. Dumais and H. Chen. \"Hierarchical \nclassification of web content\",  Proc. of the 23rd \nInt'l ACM Conf. on Research and Development in Information Retrieval (SIGIR) , pages256-263, \nAthens, Greece, August 2000. \n[6] J. Duchene and S. Leclercq, \"An optimal \ntransformation for discriminant principal component analysis\" , IEEE Trans. On Pattern \nAnalysis and Machine Intelligence , Vol. 10, No \n6, November 1988. \n[7] G. Tzanetakis and P. Cook. \"Musical genre \nclassification of audio signals\", IEEE \nTransactions on Speech and Audio Processing , \n10(5), July 2002 \n[8] Barlow, H and Morgenstern, S. \" A dictionary of \nmusical themes \", London : E. Benn, 1978."
    },
    {
        "title": "Music Recommendation from Song Sets.",
        "author": [
            "Beth Logan"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416658",
        "url": "https://doi.org/10.5281/zenodo.1416658",
        "ee": "https://zenodo.org/records/1416658/files/Logan04.pdf",
        "abstract": "We motivate the problem of music recommendation based solely on acoustics from groups of related songs or ‘song sets’. We propose four solutions which can be used with any acoustic-based similarity measure. The first builds a model for each song set and recommends new songs according to their distance from this model. The next three approaches recommend songs according to the av- erage, median and minimum distance to songs in the song set. For a similarity measure based on K-means models of MFCC features, experiments on a database of 18647 songs indicated that the minimum distance technique is the most effective, returning a valid recommendation as one of the top 5 32.5% of the time. The approach based on the median distance was the next best, returning a valid recommendation as one of the top 5 29.5% of the time.",
        "zenodo_id": 1416658,
        "dblp_key": "conf/ismir/Logan04",
        "keywords": [
            "music recommendation",
            "acoustics",
            "song sets",
            "acoustic-based similarity",
            "four solutions",
            "K-means models",
            "MFCC features",
            "experiments",
            "database",
            "18647 songs"
        ],
        "content": "MUSICRECOMMEND ATIONFROMSONGSETS\nBethLogan\nHewlettPackardLabs\nOneCambridge Center\nCambridge MAUSA\nABSTRACT\nWemotivatetheproblem ofmusic recommendation based\nsolely onacoustics from groups ofrelated songs or‘song\nsets’. Wepropose four solutions which canbeused with\nanyacoustic-based similarity measure. The ﬁrst builds\namodel foreach song setandrecommends newsongs\naccording totheir distance from thismodel. The next\nthree approaches recommend songs according totheav-\nerage, median andminimum distance tosongs inthesong\nset. Forasimilarity measure based onK-means models\nofMFCC features, experiments onadatabase of18647\nsongs indicated that theminimum distance technique is\nthemost effective,returning avalidrecommendation as\noneofthetop532.5% ofthetime. Theapproach based\nonthemedian distance wasthenextbest, returning avalid\nrecommendation asoneofthetop529.5% ofthetime.\n1.INTRODUCTION\nListeners areincreasingly ﬁnding music ofinterest onthe\nWebrather than through traditional distrib ution channels.\nThis represents agreat opportunity fornewandobscure\nartists tointroduce their music tolargeaudiences since\ntheWebhasrelati velylowentry barriers. However,itis\ndifﬁcult forlisteners todisco versuch artists since estab-\nlished automatic music recommendation techniques use\neither opinions orplaylists generated bythepublic, or\nmeta-data generated byexperts. Forlittle-kno wnartists,\nfewexperts areinterested incategorizing their music and\nthegeneral public isunawareoftheir existence. Artists\ncould self-cate gorize their music butsuch asystem isopen\ntoabuse. What isneeded then isawaytorecommend\nsongs orartists based solely onaudio data.\nAutomatically recommending andorganizing music us-\ningaudio properties hasattracted much attention (e.g. see\n[2],[1]andreferences). However,eventhebest systems\ntodate stillfallfarshort ofhuman expectations [2].The\ninclusion ofnon-audio meta-data canhelp overcome such\nshortf alls, yetfornewartists such meta-data does notex-\nist.Insuch cases though, wecanperhaps achie vebetter\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forproﬁt orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation ontheﬁrst page.\nc\r2004 Universitat Pompeu Fabra.performance byincluding moreaudio data. Wepropose\nthen rather than studying recommending Nsongs given\noneexample song toinstead study theeasier butstillvery\nuseful task ofrecommending onesong givenNrelated\nsongs. Thehope isthatifseveralsongs arechosen asrep-\nresentati veofthe‘sound’ theuser isseeking, wewillhave\nmore information onwhich tobase ourautomatic recom-\nmendation. Wecallthisproblem the‘song setcompletion’\nproblem. Weusetheterm ‘song set’rather than ‘playlist’\naswearenotconcerned with theorder inwhich thesongs\nwillbeplayed, merely thattogether theyrepresent asub-\ngenre preferred bytheuser.Thus weconsider howgiven\nasetofuser-selected songs wewould recommend another\nsong with similar properties using merely audio analysis.\nSuch setsofsongs might beauser’ sfavorite songs ora\ngroup ofsongs bytheuser’ sfavorite artist.\nInthispaper wepresent andevaluate four algorithms\ntorecommend songs from song sets. Thealgorithms are\nquite general andcanbeused with anyaudio distance\nmeasure. Wetestthem using ourpreviously published\ntimbre similarity measure.\n2.RECOMMEND ATIONSFROMSONGSETS\nInthissection, weﬁrstbrieﬂy describe ourpreviously pre-\nsented technique todetermine acoustic similarity between\nsongs. Wethen present four algorithms which canbere-\ngarded asextensions ofthisoranysong similarity tech-\nnique todetermine thedistance between songs andsong\nsets. The approaches differbywhether theybuild asin-\nglemodel fortheentire song setoraseries ofmodels for\nitsconstituent songs, andbythemanner ofcomparing the\nmodel ormodels tothesongs toberecommended.\n2.1.Acoustic-Based MusicSimilarity\nInorder toprovide recommendations from song sets, we\nrequire ameans toautomatically determine theacoustic\ndistance between asong andasong set. This issimilar\ntothetaskofdetermining thedistance between twosongs\nforwhich manyalgorithms havebeen proposed.\nWehavepreviously published andachie vedgood re-\nsults with anacoustic similarity measure which captures\ninformation about songs’ instrumentation ortimbre [3].\nThe approach issimilar inspirit toanumber ofother\nmusic similarity algorithms which transform rawaudio\ntoperceptually meaningful features andﬁtaparametricprobability model tothese. Similarity isthen computed\nusing asuitable distance measure between themodels for\neach song.\nInourprevious work, each song isﬁrst converted to\nagroup ofMel-frequenc ycepstral coefﬁcients (MFCCs).\nSuch features capture smoothed spectral information which\nroughly corresponds toinstrumentation andtimbre. We\nthen model these features using K-means clustering, learn-\ningthemean, covariance andweight ofeach cluster .Hav-\ningﬁtmodels tothedata, wecalculate similarity bycom-\nparing themodels. Forthis, weusetheEarth-Mo ver’s\ndistance (EMD) [4]which calculates thecostof‘moving’\nprobability mass between clusters tomakethem equiva-\nlent. Formore details refer to[3].\n2.2.Modeling SongSetsDirectly\nOur ﬁrst technique forrecommending songs from song\nsetsbuilds asingle model torepresent allthesongs inthe\nsetandrecommends similar songs according totheir dis-\ntance tothismodel. This isequivalent totreating thesong\nsetasonelong song. Inthispaper ,weusethemodels\nanddistance measure from ourpreviously proposed tech-\nnique described above.However,anymodel-based acous-\nticsimilarity measure could beused.\n2.3.AverageDistancetotheSongsintheSet\nTheapproach described abovecompares pairs ofmodels\ntrained onquantities ofdata thatcould differbyanorder\nofmagnitude. Since thismay beundesirable, wepresent\nanalternati veapproach. Instead ofbuilding onemodel\nforthesong set,webuild aseparate model foreach of\nitssongs andthen recommend songs according totheir\naverage distance toasong inthesong set.This technique\nismore scalable than theprevious approach; ifweform\nanewsong setfrom adifferent combination ofsongs, we\nneed nottrain anewmodel.\n2.4.MedianDistancetotheSongsintheSet\nThetwotechniques described aboveaverage thedistance\nbetween asong andasong seteither explicitly orbymerg-\ningthecontents ofthesong setintoonesong. However,if\noneortwosongs inthesetareoutliers orunusual, thiswill\naffecttheaverage, probably adversely1.This isequiva-\nlenttosaying thatifthedistrib ution ofdistances between\nasong andasong setisnotGaussian, then taking theav-\nerage distance willbeverysensiti vetooutliers.\nFigures 1,2and3showthehistograms forthedis-\ntance between arandomly selected song andtherestof\nthesongs onthree albums. Asdescribed inSection 3,we\nregard albumsasgood examples ofsong sets. Weseefrom\nthese ﬁgures thattypically ,thedistrib ution ofthedistance\n1Atleast forthesimple distance measure studied inthispaper .One\ncanimagine averysophisticated recommendation technique which takes\nnote ofanunusual song anddecides whether itshould inﬂuence arec-\nommendation.0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.801234567\ndistancenumber\nFigure1.Histogram ofthedistances between arandomly\nchosen song from “20YearsofJethro Tull”andtherestof\nthesongs onthealbum.\n0.08 0.1 0.12 0.14 0.16 0.18 0.200.511.52\ndistancenumber\nFigure2.Histogram ofthedistances between arandomly\nchosen song from “Jagged Little Pill” byAlanis Moris-\nsette andtherestofthesongs onthealbum.\nbetween asong andthesongs inthesong setisnotGaus-\nsian. Wehaveexamined such histograms forover500\nalbums andfound thatveryfewareevenclose tobeing\nGaussian. Wetherefore seek adistance measure between\nsongs andsong setsthatdoes notrelyonthedistrib ution\nofthedistances between songs being Gaussian.\nAstandard technique from statistics used toimpro ve\nrobustness tooutliers when thedata isnon-Gaussian is\ntotakethemedian instead oftheaverage. Wetherefore\nconsider recommending songs using themedian ofthe\ndistances between thesong and each song inthesong\nset.This approach shares thescalability advantages ofthe\nprevious averaging technique butmakeslessassumptions\nabout thenature ofthedistance distrib ution.\n2.5.Minimum DistancetotheSongsintheSet\nFinally ,weconsider computing thedistance between a\nsong andasong setastheminimum ofthedistances be-\n0.08 0.09 0.1 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.1800.511.52\ndistancenumber\nFigure3.Histogram ofthedistances between arandomly\nchosen song from “Backstreet Boys”andtherestofthe\nsongs onthealbum.Genre %Collection\nRock 68.2\nClassical 5.6\nJazz 5.5\nWorld 3.7\nNewage 2.4\nFolk 2.4\nSoundtrack 2.0\nElectronica 1.9\nVocal 1.7\nRap 1.5\nTable1.Percentage ofthecollection covered bythemain\ngenres.\ntween thesong andthesongs intheset. Although this\ntechnique could backﬁre ifthesong matches anoutlier in\nthesong set,onaverage itshould havegood performance.\n3.EXPERIMENTS\nHaving presented arange oftechniques toprovide recom-\nmendations from songs sets, wenowstudy their perfor -\nmance onadatabase of18647 songs.\n3.1.Experimental Setup\nAnatural source ofsong setsisuser-generated playlists\nwhich canbeeasily found ontheWeb.However,our\nanalysis requires data forwhich audio isavailable atthe\nsong levelsince weextract features from theaudio ofeach\nsong. Collecting audio forallthesongs inevenasubset\noftheplaylists ontheWebisunfortunately beyond our\nresources.\nAlbums howeverareasource ofnatural song setsand\naremore readily available inthose sets. Wetherefore\nevaluate ouralgorithms onanin-house database of18647\nsongs from 1523 albums forwhich wehavethefullau-\ndio. Thecollection coversawide variety ofgenres from\nClassical toRock. Table 1showsthepercentage ofthe\ncollection covered bythemain genres.\nWeassume thatthelistofsongs oneach albumisa\nvalidsong set.Foreach album,werandomly choose one\nsong toomit. These omitted songs form ourtestsetand\ntheremainder ofthesongs oneach albumasong set.There\narethus 1523 testsongs and1523 song setsineach exper-\niment.\nForeach song set,werecommend songs from thetest\nsetaccording toouralgorithms. Ideally ,thesong omitted\nfrom theeach song set’salbumshould betheﬁrstrecom-\nmendation forthatsong set,although there could becases\ninwhich other songs arevalidchoices. Wereport twoﬁg-\nuresofmerit. Theﬁrstrecords thepercentage oftimes this\nomitted or“correct” song wasinthetop1,thetop5,the\ntop10andthetop20recommendations. Wealso study a\nmore relax eddeﬁnition ofthecorrect song which includesCorrect Number Top Top Top Top\nSong Clusters 1 5 10 20\nStrict 16 13.7 24.7 31.4 37.5\n64 16.8 27.9 33.5 38.9\n256 15.6 26.8 33.5 39.5\nRelax ed 16 16.2 29.9 38.2 46.8\n64 20.3 33.7 41.2 48.1\n256 19.5 33.2 40.5 47.7\nTable2.Percentage oftimes thecorrect song wasin\nthetop1,5,10and20songs returned according tosong\nsetsmodeled byK-means models with various numbers of\nclusters forvarious deﬁnitions ofthecorrect song. Each\ntestsong ismodeled byaK-means model with 16clusters.\nallsongs bythesame artist who composed thesongs inthe\nsong set.\n3.2.Results\nWeﬁrstconsider recommendations ofsongs according to\ncloseness tothemodels builtforeach song setasdescribed\ninSection 2.2. Weconverttheaudio to19dimensional\nMFCC vectors andcluster these using K-means cluster -\ning. Table 2showsthepercentage oftimes thecorrect\nsong wasinthetop1,top5,top10andtop20recom-\nmendations forvarying numbers ofclusters used tomodel\nthesong set. Each testsong ismodeled byaK-means\nmodel with 16clusters. Weseethatthese results arevery\npromising, being farbetter than chance. Atleast 25% of\nthetime, thecorrect song isoneofthetop5recommenda-\ntions. Thebest result isobtained for64clusters. For256\nclusters theperformance degrades, presumably because\ninsuf ﬁcient data isavailable tolearn somanyclusters.\nIfthedeﬁnition ofthecorrect song isrelax edweobtain\ntheresults inthelowerhalfofTable 2.Here weseethat\nanimpro vement ofabout 20% relati veispossible ifone\nassumes anysong returned bythethesame artist asthe\nsong setwould beasuitable recommendation.\nWenextconsider song recommendations according to\ntheir average distance toasong inthesong setasde-\nscribed inSection 2.3.Wemodel each testsong andeach\nsong inthesong setbyaK-means model with 16clusters\nandaverage theEMD between thetestsong andeach song\ninthesong set.ThetoppartofTable 3showstheresults\nforthisexperiment forboth thestrict andrelax eddeﬁni-\ntions ofthecorrect song. Theresults arecomparable tothe\nprevious case inwhich thesong setwasrepresented bya\nmodel, although asdiscussed averaging ismore scalable\nsowould bepreferred.\nNextwestudy thesystem described inSection 2.4in\nwhich songs arerecommended according totheir median\ndistance tothesongs inthesong set.Themiddle section\nofTable 3showsthese results. Weseethat useofthe\nmedian provides some advantage overusing theaverage\ndistance ormodeling thesong setdirectly .Evenforthe\nstrictest deﬁnition ofcorrect song, almost 30% thetime,Distance Correct Top Top Top Top\nSong 1 5 10 20\nAverage Strict 15.8 28.1 34.1 41.2\nRelax ed 18.4 33.4 41.2 50.2\nMedian Strict 17.4 29.5 35.0 42.7\nRelax ed 20.9 35.0 41.6 51.3\nMinimum Strict 20.1 32.5 37.6 45.1\nRelax ed 26.5 41.2 47.7 56.1\nTable3.Percentage oftimes thecorrect song wasinthe\ntop1,5,10and20songs returned according totheav-\nerage, median andminimum distance between itandthe\nsongs inthesong setforvarious deﬁnitions ofthecorrect\nsong.\nthecorrect song isreturned asoneofthetop5.\nFinally westudy thesystem which recommends songs\naccording totheir minimum distance toallsongs inthe\nsong set. These results areshowninthebottom section\nofTable 3.These indicate thatthisapproach isthebest.\nForthestrictest deﬁnition ofcorrect song, asuitable rec-\nommendation isreturned 32.5% ofthetime. Forthemore\nrelax eddeﬁnition ofcorrect song, thecorrect song ischo-\nseninthetop541.2% ofthetime, compared with only\n35.0% ofthetime forthemedian distance system.\n4.DISCUSSION\nThe results aresome what surprising. The best approach\nforrecommending songs from song setsappears tobesim-\nplychoosing songs according totheminimum distance to\nsongs inthesong set.There appears tobenoadvantage in\nmodeling thesong setorevenconsidering anysong init\nother than theoneclosest tothetestsong.\nThis could beanartifactofourchoice ofsong setand\nourdistance measure. Our song sets arealbums which\ntypically contain veryclosely related songs. Although\nthere areoutliers, wewould beunluck ytochoose one\nofthese asourtestsong. Also, ourdistance measure\nworks best when comparing twomodels trained onthe\nsame amount ofdata. Other distance measures designed\ntomodel thesong setdirectly may bemore effective.\nInanycase, weshould bewaryofdrawing toomany\nconclusions from thispreliminary study .Wehaveonlyconsidered onesetoftestsongs andtwoobjecti vedeﬁni-\ntions ofthecorrect song. More experiments onavariety\nofsong setswith user evaluations areneeded.\n5.CONCLUSION ANDFUTURE WORK\nWehavemotivated andproposed solutions totheproblem\nofmusic recommendation based solely onacoustics from\nsetsofrelated songs. Wefound thatforatimbre-based\nsimilarity measure, thebest recommendations were ob-\ntained byranking songs bytheminimum oftheir distance\ntosongs inthesong set.\nFuture workwillfocus ontheuseofother acoustic dis-\ntance measures, particularly those incorporating rhythmic\ninformation, andlearning which sounds inthesong set\nperceptually distinguish itfrom therestofaudio space.\nWewillalsoconsider recommending groups ofsongs. As\ndescribed, wehope toconduct thisresearch onalarger,\nmore varied collection ofsong setswith greater feedback\nfrom users.\n6.ACKNOWLEDGMENTS\nThanks isduetoDaveGoddeau foruseful discussions and\ntotheanon ymous reviewers fortheir feedback.\n7.REFERENCES\n[1]Berenzweig, A.,Logan, B.,Ellis, D.P.W,\nandWhitman, B.,“Alarge-scale evaluation of\nacoustic andsubjecti vemusic similarity mea-\nsures”,ISMIR ,2003.\n[2]Aucouturier ,J-Jand Pachet, F.“Impro ving\ntimbre similarity: Howhigh’ sthesky”,Jour-\nnalofNegativeResults inSpeech andAudio\nSciences, April 2004.\n[3]Logan, B.andSalomon, A.,“Amusic similar -\nityfunction based onsignal analysis”, ICME\n2001.\n[4]Rubner ,Y.andTomasi C.andGuibas L.“The\nEarth Mover’sDistance asametric forimage\nretrie val”,Stanford University Technical Re-\nportSTAN-CS-TN-98-86, 1998."
    },
    {
        "title": "Timbre Classification Of A Single Musical Instrument.",
        "author": [
            "Mauricio Alves Loureiro",
            "Hugo Bastos de Paula",
            "Hani C. Yehia"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416320",
        "url": "https://doi.org/10.5281/zenodo.1416320",
        "ee": "https://zenodo.org/records/1416320/files/LoureiroPY04.pdf",
        "abstract": "In order to map the spectral characteristics of the large variety of sounds a musical instrument may produce, different notes were performed and sampled in several intensity levels across the whole extension of a clarinet. Amplitude and frequency time-varying curves of partials were measured by Discrete Fourier Transform. A limited set of orthogonal spectral bases was derived by Principal Component Analysis techniques. These bases defined spectral sub-spaces capable of representing all tested sounds and of grouping them according to the distance metrics of the representation. A clustering algorithm was used to infer timbre classes. Preliminary tests with resynthesized sounds with normalized pitch showed a strong relation between the perceived timbre and the cluster label to which the notes were assigned. Self-Organizing Maps lead to results similar to those obtained by PCA representation and K- means clustering algorithm.",
        "zenodo_id": 1416320,
        "dblp_key": "conf/ismir/LoureiroPY04",
        "keywords": [
            "Discrete Fourier Transform",
            "Principal Component Analysis",
            "timbre classes",
            "pitch",
            "resynthesized sounds",
            "self-Organizing Maps",
            "distance metrics",
            "sound",
            "notes",
            "sound"
        ],
        "content": "TIMBRE CLASSIFICATION OF A SINGLE MUSICAL \nINSTRUMENT\nMauricio A. Loureiro Hugo B. de Paula Hani C. Yehia \nSchool of Music  Grad. Program in Electrical Engineering  Dept. of Electronic Engineering  \nCEFALA – Center for Research on Speech, Acoustics Language and Music \nUFMG - Federal University of Minas Gerais - Brazil \nABSTRACT \nIn order to map the spectral characteristics of the large \nvariety of sounds a musical instrument may produce, different notes were  performed and sampled in several \nintensity levels across the whole extension of a clarinet. Amplitude and frequency time-varying curves of partials were measured by Discrete Fourier Transform. A limited set of orthogonal spectral bases was derived by Principal Component Analysis techniques. These bases defined spectral sub-spaces capable of representing all tested sounds and of grouping them according to the distance metr ics of the representation. \nA clustering algorithm was used to infer timbre classes. Preliminary tests with resynthesized sounds with normalized pitch showed a strong relation between the perceived timbre and the cluster label to which the notes were assigned. Self-Organizing Maps lead to results similar to those obtained by PCA representation and K-means clustering algorithm.   \n1. INTRODUCTION \nRepresentation of a musical instrument involves the \nestimation of the physical parameters that contribute to the perception of pitch, intensity levels and timbres of all sounds the instrument is capable of producing. Of these attributes, timbre poses the greatest challenges to the measurement and specification of the parameters involved in its perception, due to its inherently multidimensional nature. Unlike timbre, intensity and pitch time-varying levels can be classified according to \nsoft/loud and low/high one-dimensional scales and are, hence, capable of being quantitatively expressed by the traditional music notation system. Timbre is perceived by means of the interaction of a variety of static and dynamic properties of sound grouped into a complex set of auditory attributes. The identification of the contribution of each one of these competitive factors has been the main subject of psychoacoustics research on timbre perception. \nThe introduction of the notion of \"similarity rate\" of \nhearing judgment responses together with Multidimensional Scaling (MDS) techniques allowed the reduction of this multidimensionality. \"Timbre values\" of different instruments were positioned on low-dimensional timbre space  according to their \nsimilarity/dissimilarity responses between pairs of distinct timbres, providing a quantification of a relatively complex structure upon quite simple data. More recent studies were able to relate measurable physical parameters with the dimensions shared by the timbre represented in these spaces, establishing correlations between purely pe rceptive factors related to \ntimbre and acoustic measurements extracted directly from sound [4, 10]. A historical review of the development of research on musical timbre is found in [8]. \nMost studies on musical timbre research have \napproached comparisons among isolated notes of different musical instruments outside any musical context, focusing on the pe rceptive mechanism that \ndiscriminates a musical instrument from another. Little has been achieved regarding perceptive discrimination within the timbre palette produced by a single musical instrument, or even along the extent of a single note. Focused on the timbre of a single instrument, this study investigates methods for representing the variety of sonorities produced by the instrument.  \n2. TIMBRE SET SPECIFICATION \nTimbre representation here  investigated was built upon \nspectral parameters extracted from samples of sounds performed along the entire pitch range of the instrument. Two major simplifications were considered in defining the timbre set used in this study: (i) it was limited to the sound palette commonly produced on musical instruments in tr aditional classical western \nmusic performance, excluding sonorities produced on the instrument on the context of other musical traditions,  as well as those regularly used in contemporary music known as “extended techniques”; (ii) only the sustained part of relatively long sounds was considered, excluding attack , decay and transitions \nbetween consecutive notes. Due to dependence of timbre on these parts of the sound, the second simplification limits the investigation to the perception of slow variation of timbre, which commonly happens along longer notes.  \nIntentional variations of timbre, together with \nfluctuations of intensity and duration are commonly used by the player, in order to convey his or her expres-sive intentions. Although timbre may vary independ-ently of intensity and duration, its dependence on inten-sity is evident. This high level of correlation facilitates \nPermission to make digital or hard c opies of all or part of this work \nfor personal or classroom use is granted without fee provided that \ncopies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. \n© 2004 Universitat Pompeu Fabra. the sampling of different timbre “values” of the same \nnote upon specification of intensity levels. Thus, four different timbres were samp led for each note by asking \nthe player to perform each not e in four different inten-\nsity levels, with minimal variation: pianissimo  (pp), \nmezzo -piano  (mp), mezzo -forte  (mf) and fortissimo  (ff). \nThe performer was asked to establish the lowest and highest level limits as softer and louder as possible, respectively, within the range of commonly used timbres on western classical music. Intermediate levels were to be defined by comparison with these limits. Samples were obtained thr ough high quality recordings \nof all notes of the two lowest registers of a B flat clarinet, ranging from D3 (147 Hz) through A5 (880 Hz), played at the four levels of intensity defined above, with an average duration of 3 seconds. \n3. SPECTRAL BASES FOR TIMBRE \nCHARACTERIZATION \n3.1. Spectral parameters estimation \nAmplitude curves of partials were estimated according \nto McAulay and Quatieri’s method, which searches for maximum amplitude values (“peak detection”) of a Fourier Transform and establishes a correspondence between the closest peak values in adjacent frames \n(“peak continuation”), associating these values to instantaneous frequency and amplitude values of harmonic components [9, 12]. It was assumed that all sampled sounds could be represented by a weighted sum of sinusoids without abrupt variations of amplitude and \nfrequency values. Components with intensity more than 60 dB below the maximum level were discarded and amplitude curves were smoothed by a low pass filter with cut-off frequency of 10 Hz. \n3.2. Principal Component Analysis   \nThe high correlation of spectral parameters, presented in \nboth frequency and time domains, which is a common characteristic of spectral distribution of sounds of musical instruments, allowed an efficient data reduction using Principal Component Analysis (PCA) [5]. PCA calculates an orthogonal basis determined by the directions of maximum variance of a set of multidimensional variables. The projections of the original data on this basis, denominated principal components (PCs), follow tr ajectories that accumulate \nthe maximum variance of the data in a decreasing order. This allows an approximate representation of the data, using only a reduced numbe r of dimensions. Recent \nworks presented the effectiveness of this kind of representation of musical timbre [1-3, 11]. \n3.3. Physical Timbre space trajectories \nIn order to represent the spectral distributions of the \ntested sounds, spectral basis were calculated using as input data the concatenation of the four samples of each note, pp, mp, mf and ff, as defined in Section 2. Samples \nwere normalized in amplitude and duration, with 75 frames (approx. 870 ms) each, taken from the center of the note. The spectral basis thus obtained constitutes a timbre space  for these notes, where each sound occupies \na unique position, according to its spectral configuration. Figure 1 shows three-dimensional trajectories of four intensity levels of four contiguous notes, A3 (220 Hz), Bb3 (233 Hz), B3 (247 Hz) and C4 (262 Hz), represented in the principal component space defined by them. \nThe correlation between intensity level and the first \nPC is evident as the spectral points belonging to each sound are separated in groups positioned in increasing order from pp to ff along the first PC dimension, while \nthe 2\nnd and 3rd PCs vary differently in different \ndirections for each sound. Moreover, we can identify clustering of different sounds from different notes: softer sounds ( pp and mp) on the right side of the space \nand louder sounds on the left. We can also observe that louder sounds such as A3 ff, A3 mf and C4 ff have their \ntrajectories more spread then softer sounds. \n \n-0.4 -0.2 0 0.2 0.4 0.6 0.8 1-0.500.5-0.4-0.3-0.2-0.100.10.20.3\nA3ffA3mf\n1st PCBb3ff\nBb3mf\nC4ffB3ff\nC4mfA3mpB3mf\nC4mp\nBb3mp\nB3mpBb3pp\nC4pp\nA3pp\nB3pp\n2nd PC3rd PC\n \nFigure 1 : Three-dimensional trajectories of notes A3 \n(220 Hz), Bb3 (233 Hz), B3 (247 Hz) and C4 (262 Hz) \nin the spectral space defined by them. \n4. TIMBRE CLASSIFICATION \n4.1. K-means Cluster Analysis \nAn attempt to investigate the timbre distribution along \nthe entire instrument was made with Cluster Analysis, using the K-means algorithm [6]. Comparison of timbre parameters among notes of different pitch becomes more complex, as timbre may vary significantly as a function of the note played, depending on the instru-ment. Clarinet sounds, as used in this study, present irregular variation of timbre from note to note, which can be very accentuated, de pending on the region of the \ninstrument, like the abrupt timbre change between the \nlow and mid registers, a well known characteristic of the clarinet. At first, a cluster analysis was performed using \nthe 19 notes (76 sounds) from the low register of the clarinet, from D3 (147 Hz) through Ab4 (415 Hz). Nine \nclusters provided the best correlation between auditory tests and the classification obtained for this set of sounds. Very few of these sounds had their principal component coordinates split into  different clusters and, \nwhen this happened, no more than 2 clusters were involved and the cluster assigned to the central part of the sound was always the cluster where the majority of points lied. \nFigure 2 orders all 76 sounds of the low register of \nthe clarinet by pitch and s hows the cluster to which each \none was assigned. Each sound is represented by the location of its central frame on the low register  timbre \nspace. This figure highlights the correlation of the \ncluster to intensity level and shows that intensity level variation spreads the sounds more than pitch variation. Informal auditory tests showed strong coupling between perceived brightness and clusters assignment. Due to the known relationship of spectral centroid to the perception of brightness, cluster labels  were ordered according to \nthe mean spectral centroid of the group of sounds assigned to it. Note that the first 3 clusters group almost every pp and mp sounds of the whole set. Some notes of \nhigher pitch in mf and ff were also assigned to those \nclusters. While higher pitched notes were grouped more tightly into these clusters, the four last clusters contain almost only mf and  ff notes of the lower octave, \nshowing the tendency of timbre variation reduction as pitch increases. \n1 2 3 4 5 6 7 8 9D3Eb3E3F3F#3G3Ab3A3Bb3B3C4C#4D4Eb4E4F4F#4G4Ab4\n pp  mp mf  ff pp mp  ff  mf pp  mp mf ff pp mp  mf ff pp mp  mf ff pp  mp  mf ff pp  mp mf ff pp  mp  mf  ff pp mp  mf  ff pp  mp  mf  ff pp  mp  mf ff pp  mp  ff  mf pp  mp  mf ff pp mp  mf  ff pp  mp  ff  mf pp  mp  mf  ff pp  mp mf ff pp mp  mf ff pp mp mf  ffNote - Cluster Label Scale\nCluster Number  \nFigure 2 : Cluster Label of the 19 notes of the low \nregister of the clarinet, D3 (147 Hz) through Ab4 (415 \nHz). Notes are ordered by pitch and cluster labels by \nthe mean of the spectral centroids. \n4.2. Self-Organizing Maps \nSelf-Organizing Maps are algorithms formalized by \nKohonen for non-supervised neural nets, capable of mapping input data of large dimensions into lower dimensional spaces, preservi ng the essential topological \nrelationships of the original data [7]. Toiviainen [13] compared the efficiency of musical timbre representations in spaces bu ilt by topological distances calculated by SOM to subj ective measurements of \nsimilarity, proving a high correlation degree between the two domains. De Poli [3], Cosi and colleagues [2] developed studies on classification of musical timbre using SOM. This paper used a Matlab Toolbox from Versanto and colleagues [14]. \nAn hexagonal SOM of size 16-by-9 was used to map \nthe 76 sounds (19 notes) of the low register of the clarinet. Figure 3 shows the relation of this mapping to sound intensity levels. pp and mp sounds were more \ntightly clustered than mf and ff sounds, which can be \nverified by the distance metrics distribution of the SOM shown on the graph on the right side of Figure 3, in which distances between hexagons represent distances between map cells. \nAlthough SOM mapping is projected onto two \ndimensions, some consistency between both representations was identified. Like the K-means, SOM was able to map together every frame of a single sound into one or at most two cells. \nmp\nmp\nmp\npp\nmp\npp\nmp\nmp\nmp\nmp\nppmf\npp\nmf\nff\nmp\nmp\nppff\nff\nff\nmf\nmf\nff\npp\npp\nmp\npp\nmf\nmf\nmpmf\nff\nmp\nff\nff\nff\nmf\nmf\nmp\nppff\nmp\nmf\npp\nmpff\nff\nff\nff\nff\nmf\nmp\nmp\npp\nppmf\nmf\nmf\nmf\nmf\nmp\npp\nppff\nff\nff\nmf\npp\nppmf\nmf\nff\nff\nff\nmf\npp\npp \nFigure 3 : SOM mapping of intensity levels (left) of the \n76 sounds (19 notes) of the low register of the clarinet, \n(D3 to Ab4); distribution of the distance between map \ncells (right). Larger cells mean closer matching units. \nFigure 4 shows the trajectories of six notes of the lower \noctave of the instrument: D3, Eb3, F3, F#3, Ab3 and A3. Despite being closer in pitch, they were mapped onto two distinct groups on opposite sides. Comparing Figures 2 and 4 we observe that notes mapped on the upper left corner (D3, Eb3 and Ab3) were assigned to the same clusters by the K-means as were also the notes mapped on the lower right corner (F3, F#3 and A3). Moreover, in both classifications Ab3 sounds were po-sitioned tightly together, while A3 sounds were widely spread over clusters and cells, corroborating the high correlation between the K-means and the Kohonen map. \n5. CONCLUSION \nThis study carries out timbre representation of a musical \ninstrument based on spectral parameters extracted from sounds performed on that instrument. Principal compo-nent analysis is used for dimensionality reduction, and clustering techniques are used to categorize the different D3Eb3\nAb3\nF3\nF#3A3\n \nFigure 4 : SOM mapping of the trajectories of notes \nD3, Eb3, F3, F#3 Ab3 and A3. \ntimbres produced. The construction of spectral sub-\nspaces involving all possibl e sounds produced by the \ninstrument made it possible a compact representation of the whole timbre palette of the instrument. This unified representation allowed a tim bre classification according \nto the distance metrics of the PC timbre space . Both K-\nmeans and Self-Organized Maps provided a descriptive comparison of the dynamic variation of timbre. These representations and clustering techniques showed a strong matching, as they are mapping data from the same timbre space . It was clearly verified across all the \nresults presented in this study that timbre classes tend to be divided as a function of spectral brightness, which is known to be correlated to intensity level in wind instruments. It was also noted that the lower pitched notes of the clarinet exhibit in general much more richness of timbre variation and spectral brightness than higher pitched notes.  \nThe results of this study applied to wider dynamic \ntimbre variation will facilitate the investigation of the use of intentional timbre differentiation by the performer to convey musical expressiveness. Other perspectives for this pr oject are to extend the \ninvestigation to shorter sounds, like staccati and pizzicati, as well as attack, decay and transition between notes, for which auditory models seem to be an adequate analysis tool. \n6. ACKNOWLEDGMENTS \nThis work was supported in part by CAPES (Brazilian \nHigher Education Funding Agency), and by CNPq (Brazilian National Council for Scientific and Technological Development).   7. REFERENCES \n[1] Charbonneau, G., C. Hourdin, and T. Moussa, \"A \nMultidimensional Scaling Analysis of Musical Instrument's Time-Varying Spectra,\" Computer Music Journal, vol. 21, pp. 40-55, 1997. \n[2] Cosi, P., G. De Poli, and G. Lauzzana, \"Auditory \nModelling and Self-Organizi ng Neural Networks for \nTimbre Classification,\" Journal of New Music \nResearch, vol. 23, pp. 71-98, 1994. \n[3] De Poli, G. and P. Prandoni, \"Sonological Models \nfor Timbre Characterization,\" Journal of New Music Research, vol. 26, pp. 170-197, 1997. \n[4] Hajda, J. M., R. A. Kenda ll, E. C. Carterette, and M. \nL. Harshberger, \"Methodological Issues in Timbre Research,\" In Perception and Cognition of Music, I. Deliège and J. Sloboda, Eds. Hove: Psychology \nPress, 1997, pp. 253-306. \n[5] Johnson, R. and D. W. Wichern, Applied \nMultivariate Statistical Analysis. Upper Sadlle, New Jersey, 1998. \n[6] Kaufman, L. and P. J.  Rousseeuw, Finding Groups \nin Data: An Introduction to Cluster Analysis. New York: John Wiley & Sons, 1989. \n[7] Kohonen, T., Self-Organi ng Maps - Springer Series \nin Information Sciences, vol. 30. Berlim: Springer Verlag, 1995. \n[8] McAdams, S., S. Winsbe rg, S. Donnadieu, G. De \nSoete, and J. Krimphoff, \"Perceptual Scaling of Synthesized Musical Timbres: Common Dimensions, Specificities and Latent Subject Classes,\" Psychological Re search, vol. 58, pp. 177-\n192, 1995. \n[9] McAulay, R. J. and T. F. Quatieri, \"Speech \nAnalysis/ Synthesis Based on a Sinusoidal Representation,\" IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 34, pp. 744-754, 1986. \n[10] Misdariis, N. R., B. K. Smith, D. Pressnitzer, P. \nSusini, and S. McAdams, \"Validation of a Multidimensional Distance Model for Perceptual Dissimilarities Among Musical Timbres,\" In Proceediongs of the 16th International Congress on Acoustics, Woodbury, New York, 1998. \n[11] Sandell, G. J. and W. Martens, \"Perceptual \nEvaluation of Principal-Component-Based Synthesis of Musical Timbres,\" Journal of The Audio Engineering Society, vol. 43, pp. 1013-1028, 1995. \n[12] Serra, X., \"Musical S ound Modeling with Sinusoids \nplus Noise,\" In Musical  Signal Processing, A. \nPiccialli, C. Roads, and S.  T. Pope, Eds.: Swets & \nZeitlinger Publishers, 1997. \n[13] Toiviainen, P., M. Kaipainen, and J. Louhivuori, \n\"Musical Timbre: Similarity Ratings Correlate with Computational Feature Space Distances,\" Journal of New Music Research, vol. 24, pp. 282-298, 1995. \n[14] Versanto, J., J. Himb erg, E. Alhoniemi, and J. \nParhankangas, Self-Organiz ing Map in Matlab: the \nSOM Toolbox. Helsinky, Finland,: Helsinky \nUniversity of Technology, 2000."
    },
    {
        "title": "Pattern Matching in Polyphonic Music as a Weighted Geometric Translation Problem.",
        "author": [
            "Anna Lubiw",
            "Luke Tanur"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417969",
        "url": "https://doi.org/10.5281/zenodo.1417969",
        "ee": "https://zenodo.org/records/1417969/files/LubiwT04.pdf",
        "abstract": "We consider the music pattern matching problem—to find occurrences of a small fragment of music called the “pat- tern” in a larger body of music called the “score”—as a problem of translating a set of horizontal line segments in the plane to find the best match in a larger set of horizontal line segments. Our contribution is that we use fairly gen- eral weight functions to measure the quality of a match, thus enabling approximate pattern matching. We give an algorithm with running time O(nm log m), where n is the size of the score and m is the size of the pattern. We show that the problem, in this geometric formulation, is unlikely to have a significantly faster algorithm because it is at least as hard as a basic problem called 3-SUM that is conjec- tured to have no subquadratic algorithm. We present some examples to show the potential of this method for finding minor variations of a theme, and for finding polyphonic musical patterns in a polyphonic score.",
        "zenodo_id": 1417969,
        "dblp_key": "conf/ismir/LubiwT04",
        "keywords": [
            "music pattern matching",
            "finding occurrences",
            "small fragment",
            "larger body",
            "horizontal line segments",
            "quality of a match",
            "algorithm",
            "running time",
            "basic problem",
            "polyphonic musical patterns"
        ],
        "content": "PATTERNMATCHING INPOLYPHONIC MUSICASAWEIGHTED\nGEOMETRIC TRANSLA TIONPROBLEM\nAnnaLubiw\nUniversityofWaterloo\nSchoolofComputer ScienceLukeTanur\nUniversityofWaterloo\nSchoolofComputer Science\nABSTRACT\nWeconsider themusic pattern matching problem—to ﬁnd\noccurrences ofasmall fragment ofmusic called the“pat-\ntern” inalargerbody ofmusic called the“score”—as a\nproblem oftranslating asetofhorizontal linesegments in\ntheplane toﬁndthebestmatch inalargersetofhorizontal\nlinesegments. Ourcontrib ution isthatweusefairly gen-\neralweight functions tomeasure thequality ofamatch,\nthus enabling approximate pattern matching. Wegivean\nalgorithm with running timeO(nmlogm),where nisthe\nsizeofthescore andmisthesizeofthepattern. Weshow\nthattheproblem, inthisgeometric formulation, isunlik ely\ntohaveasigniﬁcantly faster algorithm because itisatleast\nashard asabasic problem called 3-SUM thatisconjec-\ntured tohavenosubquadratic algorithm. Wepresent some\nexamples toshowthepotential ofthismethod forﬁnding\nminor variations ofatheme, andforﬁnding polyphonic\nmusical patterns inapolyphonic score.\n1.INTRODUCTION\nMusic information retrie valisarapidly evolving, multi-\ndisciplinary research area [7,4].One oftheproblems at\nitscore isthe“music pattern matching problem”—to ﬁnd\noccurrences ofasmall fragment ofmusic (the“pattern”)\ninalargerbody ofmusic (the“score”).\nThetechniques required forthisproblem differdepend-\ningonwhether themusic isrepresented symbolically oras\naudio. This paper focuses ontheformer; forliterature on\naudio representations andthepattern matching problem in\nthatconte xt,see[11].\nWithmusic represented symbolically ,there arestilla\nvariety ofapproaches tothemusic pattern matching prob-\nlem. Efforts areunderw aytocompare these approaches\nonlargedata sets, seeDownie [8]. Techniques based\nonstring matching havebeen most heavilyexplored [15].\nThese include editdistance [19]andn-gram [9]techniques.\nSince these algorithms workonsequences, polyphonic\nmusic poses agreat challenge, though there havebeen at-\ntempts tohandle itinthisframe work[16,6].\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forproﬁt orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation ontheﬁrstpage.\nc\r2004 Universitat Pompeu Fabra.Forpolyphonic music, thepattern matching problem\nismore tractable when music isrepresented inaricher ,\nmore geometric format than asa1-dimensional string—\nwhen itisrepresented aslinesegments intheplane [23],\nweighted point setsintheplane [22],ormulti-dimensional\npoint sets[24].\nThis paper explores thepossibilities ofaparticular ge-\nometric approach tomusic pattern matching. Wemodel\neach note asalinesegment intheplane—see Figure 1.\nThevertical axis corresponds topitch andthehorizontal\naxiscorresponds totime; inparticular ,thelength ofaline\nsegment indicates theduration ofthenote. This represen-\ntation isanatural one, andhasbeen used bymanyothers,\nforexample intheMusic Animation Machine [18]andby\nBrinkman andMesiti [3].\nFigure1.J.S.Bach, Invention 1,BWV 772, bars 1–2, and\ntherepresentation aslinesegments. Black linesegments\nindicate anexact match ofapattern (ontheleft) andan\ninexact match ofthesame pattern (ontheright).\nMatc hing thepattern into thescore means translating\nthepattern relati vetothescore, where “translation” is\nused initsmathematical sense. Imagine thepattern drawn\nonatransparent sheet thatcanbeshifted horizontally and\nvertically overthescore toﬁndthebestposition. Thever-\ntical shift corresponds totransposing thepattern. Thehor-\nizontal shift corresponds tolocating thepattern intime.\nSome matches arebetter than others. Anexact matc hisa\ntranslation ofthepattern sothateach linesegment ofthe\npattern exactly matches alinesegment ofthescore. See\nFigure 1.Exact matches havelimited applicability—the y\nencompass transposition, butallownoother variation. For\naricher setofpossibilities, weintroduce weight functions\nandwesearch formatches ofoptimum weight.\nAlgorithms using thisapproach havebeen developedforsome speciﬁc weight functions. Ukkonen etal.[23]\ndeﬁne theweight ofamatch tobethesum ofthelengths\noftheoverlaps ofpattern andscore linesegments. They\ngiveanalgorithm toﬁndmaximum weight matches ofa\nmonophonic pattern inapolyphonic score.\nAseries ofpapers by´OMaid ´ın[17], Francu andNeville-\nManning [12],andAloupis etal.[1]givealgorithms to\nminimize aweight function that measures thearea be-\ntween amonophonic pattern andamonophonic score.\nWeintroduce aweighting scheme that encompasses\nboth ofthese measures, andmanymore. Wecan, for\nexample, assign weights depending ontheinterv albe-\ntween anote ofthepattern andanote ofthescore; for\nexample, matching notes anoctaveapart could contrib ute\nmore weight than matching notes anaugmented 4thapart.\nMongeau andSank off[19]used such aweighting scheme\nintheir edit-distance algorithm. Weinclude examples to\nshowalittle ofthepowerandﬂexibility ofourapproach.\nOurpurpose inthispaper istoexamine theefﬁciency\nandtheefﬁcacy ofthisapproach tomusic pattern match-\ning. Withrespect toefﬁciency ,wegiveanalgorithm in\nSection 2tosolvethemusic pattern matching problem in\ntimeO(nmlogm)where misthesizeofthepattern and\nnisthesizeofthescore. This isthesame running time\nasisachie vedbyUkkonen etal.[23]intheir algorithm to\nmaximize length ofpattern-score overlap, andbyAloupis\netal.[1]intheir algorithm tominimize thearea between\npattern andscore. The running time ofouralgorithm is\nalso competiti vewith other approaches tothemusic pat-\nternmatching problem, such aseditdistance techniques.\nItis,however,disappointing inthesense thatstring pat-\nternmatching canbedone much more efﬁciently ,inlin-\neartimeO(n+m).Thequadratic time behaviorformusic\npattern matching isacceptable forsmall input sizes, butis\nprohibiti velyslowforhuge ones, such asthose envisioned\ningoogle-style music query systems.\nHowever,weargueinSection 3thatforthisgeomet-\nricapproach, quadratic behaviour isthebest thatcanbe\nachie vedwithout asigniﬁcant breakthrough insome ba-\nsicalgorithm design problems. Inparticular ,ourmodel of\nthemusic pattern matching problem includes asaspecial\ncase aproblem about containment ofpoints inlineseg-\nments. This latter problem isknowntobeequivalent, in\nterms ofcomputational comple xity,toother problems for\nwhich noonehasasubquadratic algorithm, andforwhich\nitisconjectured thatnosuch algorithm exists [2]. This\nisnotaprovedlowerbound, butitisevidence towards a\nlowerbound, which, giventhedismal state oflowerbound\ntechniques, issomething. Weknowofnoprevious lower\nbound arguments inmusic pattern matching.\nWithrespect toefﬁcacy ,i.e.whether thisapproach has\nanything tooffermusic theorists andmusicologists, we\ngivesome examples inSection 4toshowwhat ispossi-\nblewith ourmethods. Inparticular ,weconsider some of\ntheexamples thatSelfridge-Field [20]identiﬁes asbeing\nproblematic forautomatic classiﬁcation systems.\nFormore detail onthiswork,seetheMaster’ sthesis of\nthesecond author [21].2.ALGORITHM\n2.1.Overview\nForthemusic pattern matching problem, wearegivena\npattern ofmnotes andascore ofnnotes, represented as\nlinesegments. Wearealso givenaweight function with\nwhich toevaluate atranslation ofthepattern inthescore.\nWewish toﬁndthetranslation ofthepattern inthescore\nthathasmaximum weight. More generally ,wewantnot\nonly “the best” match, butanumber ofgood matches.\nOur algorithm isanefﬁcient version ofthemost ba-\nsicapproach tothismusic pattern matching problem: to\ntryallpossible translations ofthepattern inthescore, and\ncompute theweight ofeach, inorder toﬁnd thetransla-\ntions thathavemaximum weight. Thealgorithm ofUkko-\nnenetal.[23]uses thissame approach, andouralgorithm\ncanbeviewed asanextension oftheirs tomore general\nweight functions.\nThere aretwomain ingredients toanefﬁcient imple-\nmentation. One istoidentify abounded-size setofcandi-\ndate translations thatincludes allpossible optimum solu-\ntions tothemusic pattern matching problem. Weshowa\nbound ofO(nm)onthenumber ofcandidate translations.\nTheother ingredient istoavoidcomputing theweight of\neach translation from scratch, butrather toupdate efﬁ-\nciently from onetranslation tothenext.This ispossible\nformany,though notall,weight functions. Wediscuss the\nallowable weight functions inSection 2.3,andshowhow\ntopreprocess thescore intimeO(n)toachie veanupdate\ntime ofO(logm)toﬁndthenexttranslation andO(1)to\ncompute itsweight.\nPutting these together ,weobtain anO(nmlogm)al-\ngorithm forthemusic pattern matching problem.\nIntheanalysis ofouralgorithm, wemakecrucial use\noftheassumption thatmusical pitches come from adis-\ncrete set.Ourexamples usethe128MIDI values based on\nsemi-tones, butouralgorithm would apply toanydiscrete\nset,forexample scale degrees, orthebase-40 representa-\ntion ofHewlett [14]. Ourrunning time ofO(nmlogm)\nhides thedependence on128. Toputitmore precisely ,\nourrunning time isactually O(nm(d+logm))where d\nisthesize ofthediscrete pitch set. Weremark that, al-\nthough 128isaconstant, itisarather largeconstant, and\nanalgorithm whose running time does notdepend ond\nwould certainly bedesirable. This ispossible forspecial-\nized weight functions and/or monophonic music [23,1].\nItremains anopen problem toachie vethisindependence\nfromdforpolyphonic music andourgeneral weight func-\ntions.\n2.2.NotationandInputData\nAnotesisrepresented byitsstarting time,\u001b(s),itsending\ntime,\u001c(s),anditspitch, \u0019(s).Thenotescorresponds to\nthehorizontal linesegment from thepoint(\u001b(s);\u0019(s))to\nthepoint(\u001c(s);\u0019(s)).\nWeassume thatthenotes ofthescore aregivensorted\nby\u001b(s).This istrue fordata coming from aMIDI ﬁle,butother data may need tobesorted atanextra cost of\nO(nlogn).\nForthepurpose ofouralgorithm, weneed anordered\nlistofallthedistinct \u001b(s)and\u001c(s)values. These are\ncalled thetime points ofthescore. There areatmost2n\ntime points, andforpolyphonic music, generally fewer.\nWecancompute thelistoftime points from thesorted\nscore byscanning through thescore once, keeping aheap\nofthe\u001c-values ofthenotes thatareplaying atthecurrent\ntime. This takestime timeO(nlogl),where listhemax-\nimum polyphon yofthescore—i.e. themaximum num-\nberofnotes being played atanyonetime. Wedonot\nassume thatlisbounded by128, butwedoassume itto\nbebounded byaconstant.\n2.3.TheWeightModel\nWeuseaweight function tomeasure deviations ofthe\ntranslated pattern from thescore. Note thattranslating the\npattern is“free”; only thedifferences between thetrans-\nlated pattern andthescore count. Our weight functions\nareadditi ve—i.e. theweight ofaparticular translation of\nthepattern isthesum oftheweights ofitsnotes.\nItseems natural thatmatching along note should count\nmore than matching ashort note. Weeffectthisbysetting\ntheweight ofatranslated note tobeproportional toits\nduration. Forexample, ahalfnote thatperfectly matches\nintothescore counts twice asmuch asaquarter note that\nperfectly matches into thescore. Thus theweight ofa\ntranslated notepmatching ascore notesthatoccupies the\nsame time interv alwill be(\u001c(p)\u0000\u001b(p))f(\u0019(s);\u0019(p)),\nwhere fisafunction ofthepitches ofsandp.More\ngenerally ,ifsandpoverlap intime, weusethelength of\ntheoverlap instead of(\u001c(p)\u0000\u001b(p)).\nWhen atranslated pattern note overlaps intime with\nseveralnotes ofamonophonic score, weallowpieces of\nthepattern note tomatch with different notes ofthescore.\nThis captures what Mongeau andSank off[19] call“frag-\nmentation”, where onenote isreplaced byseveral. The\nopposite transformation, “consolidation”, iscaptured when\nseveralpattern notes match tothesame note ofthescore.\nAportion ofatranslated pattern note may match aportion\nofanote ofthescore only iftheyoccup ythesame time\nspan. SeeFigure 2(a).\ns1s2p2\np1\nt1 t2 t3 t4π1π2π3π4\npsσ\nsτε'ε'\n(a) (b)\nFigure2.Computing theweight function: (a)weight is\n(t2\u0000t1)f(\u00192;\u00193)+(t3\u0000t2)f(\u00191;\u00193)+(t4\u0000t3)f(\u00191;\u00194);\n(b)theeffectofashift by\u000f0.\nPolyphonic music may haveseveralpattern notes and\nseveralscore notes occup ying thesame time span. Inthiscase wematch each (piece ofa)pattern note tothesingle\nnote ofthescore thatgivesthebestweight.\nAverysimple version ofsuch aweight function sets\nf(\u0019(s);\u0019(p))tobe1if\u0019(s)=\u0019(p),and0otherwise. In\ngeometric terms, theweight ofatranslation ofthepattern\nisthen thesum ofthelengths oftheoverlap ofpattern and\nscore linesegments. This istheweight function used by\nUkkonen etal.[23].\nAmore complicated version ofsuch aweight function\nsetsf(\u0019(s);\u0019(p))tobethedifference between \u0019(s)and\n\u0019(p).Using MIDI pitches, thisisthenumber ofsemi-\ntones intheinterv albetween thetwonotes. Ingeometric\nterms, thisweight function measures thearea between the\ntranslated pattern andthescore, andwewantamatch of\nminimum weight. This weight function wasused forthe\ncase ofmonophonic music by´OMaid ´ın[17],Francu and\nNeville-Manning [12],andAloupis etal.[1].\nMore generally ,wecandeﬁne f(\u0019(s);\u0019(p))todepend\nontheinterv albetween thetwonotes inamore compli-\ncated way.Forexample, wecanassign abetter value toan\ninterv alof7semi-tones (aperfect 5th)than tothesmaller\ninterv alof6semi-tones. Mongeau andSank off[19] use\nascheme likethisintheir edit-distance algorithm, assign-\ningweights tointerv alsinincreasing order ofdissonance.\nThe particular weighting ofinterv alsthatweuseinour\nexamples isshowninTable 1.Note thatanexact match\ngetsaweight of1,andweseek amaximum weight match.\nWemakenoclaim about these weights being ideal; further\nexperimentation would begood—see Section 4.\nOurmethod canextend tofunctions f(s;p)thatdepend\nonother properties ofthenotessandpthan pitch—for\nexample stress, dynamics, relati veposition inthebar,etc.\n2.4.TheSetofCandidate Translations\nWecanthink ofthescore aslying inagrid formed by\nthe128MIDI pitches along thevertical axis, andthetime\npoints ofthescore along thehorizontal axis. This gridhas\nsizeatmost2n\u0002128.\nClaim1Withanyweight function asdescribed above,\ntherewill beanoptimum matc hofthepattern into the\nscorethathassome linesegment ofthepattern starting or\nending atoneofthese grid points.\nThus, foranyweight function asdescribed above,there\nareatmost128\u00014\u0001nmcandidate translations.\nProof:\nConsider anoptimum weight translation ofthepattern\nintothescore. Thetranslation must leavepitches onthe\ngrid, butsuppose that noline segment ofthetranslated\npattern starts orends atatime point. Let\u000fbethemin-\nimum shift leftorright thatcauses thestart orendofa\npattern note toreach atime point. Consider anotepof\nthetranslated pattern. Shifting pbyany\u000f0,\u0000\u000f\u0014\u000f0\u0014\u000f,\ndoes notalter which notes ofthescorepmatches to;it\nonly alters, by\u000f0,thelength oftheportion ofpmatch-\ningateither end. SeeFigure 2(b). Thus there isavalue\n\u000e(p)such thatthechange inweight duetotheshift ofpis\u000f0\u000e(p).More precisely ,iftheinitial portion ofthetrans-\nlated notepmatches toscore notes\u001bandtheﬁnal por-\ntionofpmatches toscore notes\u001c,andphasbeen trans-\nlated topitch\u0019,then\u000e(p)=f(\u0019(s\u001c);\u0019)\u0000f(\u0019(s\u001b);\u0019).\nThe change overallnotes is\u000f0P\np\u000e(p).IfP\np\u000e(p)is\npositi ve,then apositi ve\u000f0would increase theweight; ifP\np\u000e(p)isnegative,then anegative\u000f0would increase the\nweight. Since weassumed thetranslation tobeofopti-\nmum weight,P\np\u000e(p)must be0,andtherefore ashift of\n\u000fdoes notchange theweight, andlines upthestart orend\nofapattern note with atime point. Thus there isanopti-\nmum match onthegrid. 2\nWeremark that thenumber ofcandidate translations\ncanbereduced to4nminsome cases because there will\nbeanoptimum match inwhich some linesegment ofthe\npattern starts [orends] exactly where alinesegment ofthe\nscore starts [orends]. This happens iftheweight function\nonly measures exact overlap ofpattern andscore lineseg-\nments. Withthedistance weight function, thisproperty\nfails, andthenumber ofcandidate translations goes upby\nafactor ofd.Forthepure distance function, Aloupis et\nal.[1]areable toavoidlooking atallthecandidate trans-\nlations, butweseenowaytodothisforourmore general\nweight functions andpolyphonic music.\n2.5.Preprocessing\nInorder toquickly determine theweight ofatranslated\nnote, weprecompute aweight matrix Wbased onthe\nscore andthegivenweight function. Matrix Whasarow\nforeach ofthe128MIDI pitches, andacolumn foreach\nofthetime points ofthescore, ofwhich there areatmost\n2n.Thus ithassizeatmost128\u00022n,which isO(n).\nForpitch\u0019andtime pointt,thecorresponding matrix\nentry ,W(\u0019;t),contains theweight factor tobeapplied\ntoanote ofthepattern translated topitch\u0019,andgoing\nfrom time pointttothenexttime pointt0.Thus such a\ntranslated pattern note contrib utes(t0\u0000t)W(\u0019;t)tothe\nweight ofamatch. Interms ofthefunction fdescribed\nabove,W(\u0019;t)=maxff(\u0019(s);\u0019):sisascore note that\nincludes thetime interv al(t;t0)g:\nWecompute Wbyiterating through thenotessofthe\nscore, andupdating W(\u0019;t)as\u0019ranges through the128\npitch values, andtranges through thetime points from\n\u001b(s)upto,butnotincluding, \u001c(s).\nEach ofthe128\u00022nmatrix positions willbeupdated\natmostltimes, where listhemaximum polyphon yofthe\nscore. Asmentioned above,weassume ltobeaconstant,\nsothetime tosetupWisatmostO(n).More precisely ,\nwith apitch setofsized,andmaximum polyphon yofl,\nthetime tocompute WisO(dln).\nWecanreduce thistoO(dnlogl)bycomputing W\nacross rows,andusing aheap tocompute thebestweight\nforeach matrix entry .Wenote thatifthespace tostore\nthematrix isconsidered prohibiti ve,wecandispense with\nthematrix altogether ,andsimply compute weights aswe\nneed them during themain algorithm.2.6.TheMainMatching Algorithm\nWetryeach possible candidate translation (t;\u0019),where t\nisthetranslation applied tothetime coordinate, and\u0019is\nthetranslation applied tothepitch coordinate.\nWetryeach value oft,inorder .Wecallitanevent\nwhen thestart ortheendofatranslated pattern note lines\nupwith atime point ofthescore. Lining upthestart of\npattern notepwith time pointuoccurs attranslation value\nt=u\u0000\u001b(p).Lining uptheendofthepattern note with\ntime pointuoccurs attranslation valuet=u\u0000\u001c(p).We\ngothrough theevents inorder oftheir translation values.\nNote thatseveraleventsmay occur atthesame translation\nvalue, butforbook-k eeping purposes wehandle them one\natatime. Thenumber ofeventsisatmost4nm.\nTogothrough theevents inorder ,wekeep, foreach\npattern notep,twopointers p\u001bandp\u001cinto thelistof\ntime points. The pointer p\u001bgivesthetime point forthe\nnexteventinvolving thestart ofnotep,andp\u001cgivesthe\ntime point forthenexteventinvolving theendofnotep.\nWemakeaheap ofthetranslation values corresponding\ntothese2mforthcoming events. This allowsustoﬁnd\nthenexteventinO(logm)time. Each ofthe2mpointers\nmakesonepass through thelistoftime points.\nAswegothrough theevents—i.e. thevalues oft—we\nmaintain information foreach value of\u0019—i.e. each trans-\nposition ofthepattern. There areatmost2\u0001128values of\n\u0019.The information wemaintain includes theweight for\nthecandidate translation (t;\u0019),butalsoother information\nthat allowsustoupdate each ofthese weights inO(1)\ntime. Speciﬁcally ,westore, foreach value of\u0019,andeach\npattern notep,avalue\u000e\u0019(p)with theproperty thatasmall\nchange t t+\u000fchanges p’scontrib ution totheweight of\nthematch attransposition value\u0019bytheamount \u000f\u000e\u0019(p).\nSuppose that\u0019translates noteptopitch\u0017=\u0019(p)+\u0019.\nThe start ofphasjustpassed time pointprev(p\u001b)and\ntheendofphasjustpassed time pointprev(p\u001c).Then\n\u000e\u0019(p)=W(\u0017;prev(p\u001c))\u0000W(\u0017;prev(p\u001b)).\nIfw\u0019istheweight ofthepattern translated by(t;\u0019)\nthen thechange tow\u0019caused byt t+\u000fis\u000fP\np\u000e\u0019(p).\nWemaintain \u0001\u0019=P\np\u000e\u0019(p).Ataneventinvolving\npattern notep,wemust update \u000e\u0019(p),\u0001\u0019,andw\u0019for\neach value of\u0019.Ifthiseventisforthestart ofp,then\n\u000e\u0019(p)changes by\u000e\u0019=W(\u0017;prev(p\u001b))\u0000W(\u0017;p\u001b).If\ntheeventisfortheendofpthen\u000e\u0019(p)changes by\u000e\u0019=\nW(\u0017;p\u001c)\u0000W(\u0017;prev(p\u001c)).\nSuppose thatthecurrent eventoccurs attranslation value\nt0,andthattheprevious eventoccurred attranslation value\nt(possibly t=t0).The changes areasfollows:w\u0019 \nw\u0019+(t0\u0000t)\u0001\u0019;\u000e\u0019(p) \u000e\u0019(p)+\u000e\u0019;\u0001\u0019 \u0001\u0019+\u000e\u0019.\nEach eventinvolvesasingle pattern notep,butwemake\nchanges foreach oftheO(d)settings of\u0019,foratotal time\nofO(d).\nAltogether ,wehaveO(nm)events, timeO(logm)to\nﬁndthenextevent,andtimeO(d)toupdate ateach event—\nforatotal running time ofO(nm(d+logm)),andspace\nofO(dn).Finding thekbest matches addsO(k)tothe\ntime andspace. (Weuselinear time median ﬁnding.)3.BARRIERS TOAFASTERALGORITHM\nOur algorithm forthemusic pattern matching problem\ntakestimeO(nmlogm)forascore ofsizenandapat-\ntern ofsizem.Asubquadratic algorithm with running\ntimeO(n+m)(achie vable forstring pattern matching) or\nevenO((n+m)logn),would bevastly preferable, and\nwould makethealgorithm practical foruseinlargemusic\ndatabases. Inthissection weshowthatsuch anefﬁcient\nalgorithm willbeamajor challenge.\nMore speciﬁcally ,weshowthatthemusic pattern match-\ningproblem, inthisgeometric formulation, includes asa\nveryspecial case aproblem called “Segments Containing\nPoints” which isatleast ashard asthe3-SUM problem—\nandthatproblem isconjectured tohavenoalgorithm with\nasubquadratic running time. Weexpand onthese points in\ntheremainder ofthissection. Forbackground onO(\u0001)and\no(\u0001)notation, see[5].Thetwoproblems areasfollows:\n3-SUM: Givenasetofnintegers, does itcontain numbers\na;b;cwitha+b+c=0?\nSegments Containing Points(SCP): GivenasetPofm\nnumbers andasetQofnpairwise-disjoint interv als,is\nthere atranslation usuch thatP+u\u0012Q?\nThegeometric formulation ofthemusic pattern match-\ningproblem includes SCP astheveryspecial case where\nthepattern andthescore havenotes only ononepitch, the\npattern notes areveryshort, andthe0-1weight function\nisused.\nBarequet andHar-Peled [2]provethat analgorithm\nwith running timeo(nm)fortheSCP problem would im-\nplyanalgorithm with running timeo(n2)forthe3-SUM\nproblem. Thus SCP is“3-SUM hard”. The class of3-\nSUM hard problems wasintroduced byGajentaan and\nOvermars [13],who showthatanumber ofdifferent prob-\nlems are3-SUM hard. Although thisisnotaprovedlower\nbound (see [10]) asubquadratic algorithm foranyofthese\nproblems would beamajor breakthrough.\nThus asubquadratic algorithm forthegeometric ver-\nsion ofthemusic pattern matching problem would have\nimplications farbeyond music information retrie val.From\napractical point ofview,atleast forsmall patterns, the\nfactor of128inouralgorithm isprobably more prohibiti ve\nthan thefactor ofm.Certainly ,giventheabove,itisa\nmore tractable challenge toattempt toreduce thedepen-\ndence onthesizeofthepitch set.\n4.EXAMPLES ANDDISCUSSION\nForallourexamples weusetheweights showninTable 1,\nwhich aresome what similar tothose used byMongeau\nandSank off[19].Weassign averygood weight toasemi-\ntone, because wemodel pitches onascale of12semi-\ntones, rather than asscale degrees. Thus apattern repeated\natadifferent scale degree willingeneral beoffbysemi-\ntones. Wealso assign agood weight toanoctave,andto\nsemi-tones oneither sideofanoctave.\nOurﬁrstexample istheBach Two-PartInvention, Num-\nber1,BWV 772, theﬁrsttwobars ofwhich areshowninInterv alapart (insemi-tones) Weight\nperfect unison (0) 1\nminor 2nd(1) 0.9\nmajor 2nd(2) 0.6\nminor 3rd(3) 0.4\nmajor 3rd(4) 0.4\nperfect 4th(5) 0.2\nperfect 5th(7) 0.6\nminor 6th(8) 0.3\nmajor 6th(9) 0.3\nmajor 7th(11) 0.7\nperfect octave(12) 0.8\nminor 9th(13) 0.7\nallother interv als 0\nTable1.Theweighting scheme used forourexperiments\n1 2 3 4 5\nC100.\n97.597.597.597.597.5\n97.596.3 96.3\n92.592.592.592.5\n90.90.90.90.90.\n90.87.5\n19 20 21 22 23\nC100.\n97.597.597.597.597.5\n97.596.3 96.3\n92.592.592.592.5\n90.90.90.90.90.\n90.87.5\nFigure3.Selected bars showing matches oftheﬁrstpat-\nterninBach’ sInvention.\nFigure 1.Ourﬁrstpattern consists oftheﬁrst8notes of\nthepiece, butwith theﬁnal Greplaced bya16thnote D,\nasitappears ininverted form. Figure 3showstheﬁrst\nandlastfour bars oftheresults ofasearch forthebest20\nmatches. There are18“correct” matches inthepiece. Our\nalgorithm assigns them weights between 90% and100% ,\nandnicely separates them from theextraneous matches—\nexcept thatitﬁnds agood match (92:5%)ofthepattern\nintotheﬁnal chord. Since ourpattern hassofewdistinct\npitches, itmatches quite well against thetwonotes ofa\nminor 3rdinachord. Itmight bepossible toavoidsuch\nanomalies byassigning extra weight when thestart ofa\npattern note matches thestart ofascore note.\nWhen wesearch fortheinversion ofourﬁrst pattern,\nthematch intotheﬁnal chord getsalowerweight than the\n19validmatches, soouralgorithm correctly separates the\ngood matches from theextraneous ones. SeeFigure 4.\nOursecond example istheAndante movement ofMozart’ s\nPiano Sonata K311, with thetheme oftheﬁrst2bars as\nthepattern. SeeFigure 5.This isoneoftheexamples1 2 3 4 5\nC100.100.100.100. 100.\n98.898.898.8\n97.5 97.597.5 97.597.597.5 97.597.5\n96.3 96.396.3\n92.5\nFigure4.Matches oftheinversion oftheﬁrstpattern in\nbars 1–4ofBach’ sInvention.\ndiscussed bySelfridge-Field [20]. Shepoints outthatthe\nrecurrences ofthetheme differslightly from each other ,\nmaking automatic classiﬁcation difﬁcult. Some 6-bar seg-\nments oftheresult ofoursearch areshowninFigure 7.\nBars arenumbered asthough therepeat were explicitly\nwritten out. The ﬁrst match does notrecei ve100% be-\ncause thepattern omits thegrace note, andbecause the\nMIDI ﬁle,produced from ahuman performance, hasthe\nnote durations shortened. Thetop9matches found byour\nalgorithm aregood ones, andinclude allthevariants listed\nbySelfridge-Field. Theﬁnal variant shelists isthesyn-\ncopated oneshowninFigure 6(top), which occurs inbar\n86.Ouralgorithm detects thisasthe9thmatch, atweight\n84:4%.Seethelastpane ofFigure 7.The 10thmatch,\nstarting justbefore bar54,isextraneous, butthe11th,at\nbar50,isgood. Seethethird pane ofFigure 7.\nFigure5.W.A.Mozart, Piano Sonata inDmajor K311,\n2ndmovement, bars 1–2, with thetheme intheright hand.\nFigure6.Twooccurrences ofthetheme intheright hand:\nbars 86–87 (top) andbars 90–91 (bottom).\nUnlik eintheBach example, there areoccurrences of\nthetheme thatarerankedpoorly byouralgorithm anddo\nnotmakeitintoeventhetop20matches. Wehaveiden-\ntiﬁed 5such occurrences. Fourofthem arejustartifacts\noftheparticular MIDI ﬁleweused: inbars 38–39 and\n74–75, thetheme occurs inthelefthand, inthirds, but\ntheperformer played these notes staccato, sothelengths\nofthenotes intheMIDI ﬁleareroughly halfofwhat the\nscore indicates (this canbeseen inthe2ndpane ofFigure\n7),andtheweight computed byouralgorithm iscommen-\nsurately reduced. (Actual values areabout 60%.)Whenwechange thelengths ofthese notes toaccurately reﬂect\nthewritten score, these matches recei veweights ofabove\n90%,andwould rank among the“good” matches. The\nﬁfth occurrence ofthetheme thatouralgorithm misses is\nmore interesting. SeeFigure 6(bottom) andbars 90–91 of\nFigure 7.Tohuman eyesandears, thisisavariant ofthe\ntheme, butitposes quite achallenge forautomatic pattern\ndetection!\n1 2 3 4 5 6 7\nC95.3 95.3 93.4 93.4 89.4 88.686.7\n85.5 84.4\n81.9 81.6\n36 37 38 39 40 41 42\nC95.3 95.3 93.4 93.4 89.4 88.686.7\n85.5 84.4\n81.9 81.6\n50 51 52 53 54 55 56\nC95.3 95.3 93.4 93.4 89.4 88.686.7\n85.5 84.4\n81.9 81.6\n86 87 88 89 90 91 92\nC95.3 95.3 93.4 93.4 89.4 88.686.7\n85.5 84.4\n81.9 81.6\nFigure7.Selected bars showing matches ofthetheme in\nMozart’ sPiano Sonata.\nOur third example istheCminor fugue from Bach’ s\nWell-Tempered Clavier,Book I,theﬁrsttwobarsofwhich\nareshowninFigure 8.Our polyphonic pattern comes\nfrom bar11—see Figure 9.Thetopvoice ofthepattern is\ntheﬁrstpartofthefugalsubject transposed intotherela-\ntivemajor key(E[major); thebottom voice ofthepattern\nisthecorresponding countersubject, which starts aminor\n6thandanadditional octavebelowthetopvoice.\nFigure 10showsselected bars oftheresult ofasearch\nforthebest 15matches. Thetop7matches, with weights\nabove94%,allappear inthese bars. Theyareverygood\nmatches, differing inatmost afewsemi-tones from the\npattern. These matches highlight theoccurrences ofthe\npattern intwoofthefugalepisodes, which modulate toFigure8.J.S.Bach, Fugue inCminor ,Well-Tempered\nClavierBook I,bars 1–2.\nFigure9.Bar11ofBach’ sFugue, thesource ofourpat-\ntern, shownatright.\ndifferent keys.\nOurweight function islenient with translated notes that\nfallanoctaveawayfrom notes ofthescore. Consequently ,\nweﬁndmatches ofthepattern where thefugalsubject is\ninthetopvoice, andthecountersubject starts only ami-\nnor6thbelowthesubject inthebottom voice. InFig-\nure10these appear inbar3andinthesecond halvesof\nbars 22and23.Observ ethatthelatter matches arefound\ntwice over,which islogical. (The “phantom” partner of\nthematch inbar3haslowerweight.) Apart from phan-\ntoms, ouralgorithm properly separates thevalidmatches\nfrom theextraneous ones.\n1 2 3 4\nC100.99.399.3\n97.9 95.795.7 94.390.790.790.\n89.3\n87.987.987.184.384.3\n9 10 11 12\nC100.99.399.3\n97.9 95.795.7 94.390.790.790.\n89.3\n87.987.987.1\n21 22 23 24\nC100.99.399.3\n97.9 95.795.7 94.390.790.790.\n89.3\n87.987.987.1\nFigure10.Selected bars showing matches ofthepattern\ninBach’ sFugue.4.1.Discussion\nThese examples showthatouralgorithm canbeuseful in\nﬁnding occurrences ofamotif inpolyphonic music, even\nwhen there aremelodic variations, andevenwhen themo-\ntifispolyphonic. Although ouralgorithm hasthelimita-\ntion that itdoes notpermit changes tothedurations of\nnotes, anexample ofminor rhythmic variations ofashort\nmotif wasstilldetected.\nOntheother hand, ouralgorithm misses matches in\nwhich thenotes ofthepattern havebeen shortened inthe\nscore. Our algorithm also produces false positi veswhen\napattern with averysmall range ofpitches matches into\nlong chord notes ofthescore. Both issues might bead-\ndressed bygiving greater weight tothestarts ofnotes.\nThis would alleviate theﬁrstproblem, since itwould di-\nminish thepenalty when apattern note matches toanote\nofthescore ofshorter duration. Thesecond problem would\nalso bemitig ated, since pattern notes matching into the\nmiddle ofanote ofthescore would recei velessweight.\nFinally ,wemention thatourimplementation wasdone\ninthehigh-le vellanguage Mathematica, soitisnotsensi-\nbletoreport thecomputation time forthese examples.\n5.CONCLUSIONS\nInthispaper wehaveexplored thepossibilities andlimita-\ntions ofanapproach tomusic pattern matching thatuses a\nverynatural geometric representation ofmusic, andturns\ntheproblem into thatofﬁnding a“best” translation ofa\nsmall setoflinesegments intoalargerset.This geometric\napproach hasbeen used before, buthasnotbeen explored\nasthoroughly asstring matching techniques, eventhough\nitsuccessfully deals with polyphon y.\nOurcontrib ution istoshowhowthisapproach canbe\nused together with fairly general weight functions tomea-\nsure thequality ofamatch. This opens upthepossibil-\nityofarich range ofapproximate music pattern match-\ningtechniques. Ourexperiments only scratch thesurface\nofwhat may bepossible. Itiseasy toimagine avari-\netyofenhancements, forexample: incorporating infor -\nmation about thekeyoftonal music; adding information\nabout dynamics andstress; weighting more heavilythose\nmatches thatoccur atthebeginnings ofnotes; allowing\ntheuser tospecify which notes ofthepattern aremore\nimportant, etc.\nOuralgorithm runs intimeO(nm(d+logm))where\ndisthesizeofthepitch set,nisthesizeofthescore, and\nmisthesizeofthepattern. This isﬁneforsmall patterns,\nbuttooexpensi veforlargerones. Wehaveargued that\nthefactorO(nm)islikelytobeveryhard toimpro ve.\nImpro ving thedependence ondisperhaps more tractable,\nandalsomore relevantforreasonably sized patterns.\n6.ACKNOWLEDGEMENTS\nWethank Erna VanDaele formusical discussions andad-\nvice; IanMunro fortheidea ofhowtoﬁnd thebestkmatches; andTherese Biedl, Dan Brown,andAnne-Marie\nDono vanforuseful suggestions.\n7.REFERENCES\n[1]G.Aloupis, T.Fevens, S.Langerman, T.Matsui,\nA.Mesa, D.Rappaport, andG.Toussaint. Comput-\ningthesimilarity oftwomelodies. InProceedings\nofthe15th Canadian Confer ence onComputational\nGeometry ,pages 81–84, 2003.\n[2]G.Barequet andS.Har-Peled. Polygon-containment\nand translational min-Hausdorf f-distance between\nsegment sets are3SUM-hard. International Jour-\nnalofComputational Geometry and Applications ,\n11(4):465–474, 2001.\n[3]A.Brinkman andM.Mesiti. Graphic modeling of\nmusical structure. Computer sinMusic Resear ch,\n3:1–42, 1991.\n[4]D.Byrd andT.Crawford. Problems ofmusic in-\nformation retrie valinthereal world. Information\nProcessing andMana gement ,38:249–272, 2002.\n[5]T.H.Cormen, C.E. Leiserson, R.L. Rivest, and\nC.Stein. Introduction toAlgorithms, 2nd edition .\nMcGra wHill, 2001.\n[6]M.J. Dovey.Atechnique for“regular expression”\nstyle searching inpolyphonic music. InProceedings\nofthe2ndAnnual International Symposium onMu-\nsicInformation Retrie val(ISMIR 2001) ,pages 179–\n185, 2001.\n[7]J.S. Downie. Music information retrie val. An-\nnual ReviewofInformation Science andTechnolo gy,\n37:295–340, 2003.\n[8]J.S.Downie. Towardthescientiﬁc evaluation ofmu-\nsicinformation retrie valsystems. InProceedings of\nthe4thInternational Confer ence onMusic Informa-\ntionRetrie val(ISMIR 2003) ,pages 25–32, 2003.\n[9]S.Downie andM.Nelson. Evaluation ofasimple\nandeffectivemusic information retrie valmethod. In\nProceedings ofthe23rdAnnual International ACM\nSIGIR Confer ence onResear chandDevelopment in\nInformation Retrie val,pages 73–80, 2000.\n[10] J.Erickson. Lowerbounds forlinear satisﬁability\nproblems. Chica goJournal ofTheor etical Computer\nScience ,1999(8), 1999.\n[11] J.Foote. Anovervie wofaudio information retrie val.\nMultimedia Systems ,7:2–11, 1999.\n[12] C.Francu andC.G. Nevill-Manning. Distance met-\nricsandindexing strate gies foradigital library of\npopular music. InProc.IEEE International Confer -\nence onMultimedia andEXPO (II),pages 889–894,\n2000.[13] A.Gajentaan and M.H. Overmars. Onaclass\nofo(n2)problems incomputational geometry .\nComputational Geometry:Theory andApplications ,\n5(3):165–185, 1995.\n[14] W.B.Hewlett. Abase-40 number linerepresentation\nofmusical pitch notation. Musik ometrika ,50:1–14,\n1992.\n[15] K.Lemstrom. String Matc hing Techniques forMusic\nRetrie val.PhD thesis, University ofHelsinki, De-\npartment ofComputer Science, 2000.\n[16] K.Lemstrom andJ.Tarhio. Transposition invari-\nantpattern matching formulti-track strings. Nordic\nJournal ofComputing ,10:185–205, 2003.\n[17] D.´OMaid ´ın.Ageometrical algorithm formelodic\ndifference. InW.B.Hewlett andE.Selfridge-Field,\neditors, Melodic Similarity: Concepts, Procedur es,\nandApplications ,volume 11ofComputing inMusi-\ncology,pages 65–72. MIT Press, 1997–1998.\n[18] S.Malino wski. Music animation machine.\nhttp://www .well.com/user/smalin/mam.html.\n[19] M.Mongeau andD.Sank off.Comparison ofmu-\nsical sequences. Computer sand theHumanities ,\n24:161–175, 1990.\n[20] E.Selfridge-Field. Conceptual andrepresentational\nissues inmelodic comparision. InW.B.Hewlett and\nE.Selfridge-Field, editors, Melodic Similarity: Con-\ncepts, Procedur es,andApplications ,volume 11of\nComputing inMusicolo gy,pages 3–64. MIT Press,\n1997–1998.\n[21] L.Tanur.Ageometric approach topattern match-\ninginpolyphonic music. Master’ sthesis, School of\nComputer Science, University ofWaterloo, 2004. to\nappear .\n[22] R.Typke,P.Giannopoulos, R.C.Veltkamp, F.Wier-\ning,andR.vanOostrum. Using transportation dis-\ntances formeasuring melodic similarity .InPro-\nceedings ofthe4thInternational Confer ence onMu-\nsicInformation Retrie val(ISMIR 2003) ,pages 107–\n114, 2003.\n[23] E.Ukkonen, K.Lemstr ¨om, and V.M¨akinen.\nGeometric algorithms fortransposition invariant\ncontent-based music retrie val.InProceedings ofthe\n4thInternational Confer ence onMusic Information\nRetrie val(ISMIR 2003) ,pages 193–199, 2003.\n[24] G.A.Wiggins, K.Lemstr ¨om, and D.Meredith.\nSIA(M)ESE: Analgorithm fortransposition invari-\nant, polyphonic content-based music retrie val.In\nProceedings ofthe3rdInternational Confer ence on\nMusic Information Retrie val(ISMIR 2002) ,pages\n283–284, 2002."
    },
    {
        "title": "Gaussian Mixture Models For Extraction Of Melodic Lines From Audio Recordings.",
        "author": [
            "Matija Marolt"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416576",
        "url": "https://doi.org/10.5281/zenodo.1416576",
        "ee": "https://zenodo.org/records/1416576/files/Marolt04.pdf",
        "abstract": "The presented study deals with extraction of melodic line(s) from polyphonic audio recordings. We base our work on the use of expectation maximization algorithm, which is employed in a two-step procedure that finds melodic lines in audio signals. In the first step, EM is used to find regions in the signal with strong and stable pitch (melodic fragments). In the second step, these fragments are grouped into clusters according to their properties (pitch, loudness...). The obtained clusters represent distinct melodic lines. Gaussian Mixture Models, trained with EM are used for clustering. The paper presents the entire process in more detail and gives some initial results.",
        "zenodo_id": 1416576,
        "dblp_key": "conf/ismir/Marolt04",
        "keywords": [
            "expectation-maximization algorithm",
            "two-step procedure",
            "melodic fragments",
            "pitch",
            "loudness",
            "Gaussian Mixture Models",
            "clustering",
            "distinct melodic lines",
            "audio signals",
            "polyphonic audio recordings"
        ],
        "content": "GAUSSIAN MIXTURE MODELS FOR EXTRACTION OF \nMELODIC LINES FROM AUDIO RECORDINGS\n Matija Marolt  \n Faculty of Computer and Information Science \nUniversity of Ljubljana, Slovenia \nmatija.marolt@fri.uni-lj.si  \nABSTRACT \nThe presented study deals with extraction of melodic \nline(s) from polyphonic audio recordings. We base our \nwork on the use of expectation maximization algorithm, \nwhich is employed in a two-step procedure that finds \nmelodic lines in audio signals. In the first step, EM is \nused to find regions in the signal with strong and stable \npitch (melodic fragments). In the second step, these \nfragments are grouped into clusters according to their \nproperties (pitch, loudness...). The obtained clusters \nrepresent distinct melodic lines. Gaussian Mixture \nModels, trained with EM are used for clustering. The \npaper presents the entire process in more detail and \ngives some initial results. \n1. INTRODUCTION \nOne of the problems that remain largely unsolved in \ncurrent computer music researches is the extraction of \nperceptually meaningful features from audio signals. By \nperceptually meaningful, we denote features that a \ntypical listener can perceive while listening to a piece of \nmusic, and these may include tempo and rhythm, \nmelody, some form of harmonic structure, as well as the \noverall organisation of a piece.  \nA set of tools that could handle these tasks well \nwould provide good grounds for construction of large \nannotated musical audio databases. The lack of such \ndata currently represents a major drawback for the \ncomputer music community, as it is very difficult to \nmake use of a large variety of machine learning \nalgorithms (requiring large amounts of annotated data) \nor make any kind of large scale evaluations of various \nMIR approaches on real-world data. It would also bridge \nthe gap between a large number of researches made on \nparametric (MIDI) data that amongst other include \nsimilarity measures, estimation of rhythm or GTTM \ndecomposition. Audio analysis, learning and \ncompositional systems could also make use of such \ninformation.  An overview of past researches shows that techniques \nfor tempo tracking in audio signals are quite mature; \nseveral tools (i.e. [1]) are available for use, some of \nthem work in realtime. Rhythmic organisation is already \na harder problem, as it has more to do with higher level \nmusical concepts, which are harder to represent [2]. A \npromising approach to finding harmonic structure in \naudio signals has been presented by Sheh and Ellis [3]. \nOur paper deals with extraction of melodic lines from \naudio recordings. The field has been extensively studied \nfor monophonic signals, where many approaches exist \n(i.e. [4]). For polyphonic signals, the work of several \ngroups is dedicated to complete transcription of audio \nsignals, with the final result being a score that represents \nthe original audio ([5, 6, 7]). Algorithms for simplified \ntranscriptions, like extraction of melody, have been \nstudied by few, with the notable exception of the work \ndone by Goto [8].  \nOur work builds on ideas proposed by Goto with the \ngoal of producing a tool for extraction of melodic lines \nfrom audio recordings. The approach includes extraction \nof sinusoidal components from the original audio signal, \nEM estimation of predominant pitches, their grouping \ninto melodic fragments and final GMM clustering of \nmelodic fragments into melodic lines. The paper briefly \ndescribes each of these stages and presents some \npreliminary results. \n2. FINDING MELODIC FRAGMENTS \nThe extraction of melodic lines begins with discovery of \nfragments that a melodic line is composed of – melodic \nfragments. Melodic fragments are defined as regions of \nthe signal that exhibit strong and stable pitch. Pitch is \nthe main attribute according to which fragments are \ndiscovered; other features, such as loudness or timbre, \nare not taken into consideration.  \n2.1. Spectral Modeling Synthesis \nWe first separate the slowly-varying sinusoidal \ncomponents (partials) of the signal from the rest \n(transients and noise) by the well known spectral \nmodeling synthesis approach (SMS, [9]). SMS analysis \ntransforms the signal into a set of sinusoidal components \nwith time-varying frequencies and amplitudes, and a \nresidual signal, obtained by subtracting the sines from \nthe original signal. We used the publicly available \nSMSTools software (http://www.iua.upf.es/mtg /clam) to Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that \ncopies are not made or distributed for profit or commercial \nadvantage and that copies bear this notice and the full citation on \nthe first page. \n© 2004 Universitat Pompeu Fabra. \n   \n \n analyse our songs with a 100 ms Blackman-Harris \nwindow, 10 ms hop size. Non-harmonic style of analysis \nwas chosen, as our signals are generally polyphonic and \nnot necessary harmonic (drums...). \n2.2. Psychoacoustic masking \nThe obtained sinusoidal components are subjected to a \npsychoacoustic masking model that eliminates the \ncomponents masked by stronger ones. Only \nsimultaneous masking is taken into consideration – \ntemporal masking is ignored. Tonal and noise maskers \nare calculated from the set of sinusoidal components and \nthe residual signal, as described in [10], and components \nthat fall below the global masking threshold removed. \nOn average, the masking procedure halves the total \nnumber of sinusoidal components. \n2.3. Predominant pitch estimation \nAfter the sinusoidal components have been extracted, \nand masking applied, we estimate the predominant \npitch(es) in short (50 ms) segments of the signal. Our \npitch estimating procedure is based on the PreFEst \napproach introduced by Goto [8], with some \nmodifications. The method employs the Expectation-\nMaximisation (EM) algorithm, which treats the set of \nsinusoidal components within a short time window as a \nprobability density function (observed PDF), which is \nconsidered to be generated from a weighted mixture of \ntone models of all possible pitches at this time interval. \nA tone model is defined as a PDF, corresponding to a \ntypical structure of a harmonic tone (fundamental \nfrequency + overtones). The EM algorithm iteratively \nestimates the weights of all possible tone models, while \nsearching for one that maximizes the observed PDF \n(maximizes the set of sinusoidal components within the \nchosen time window). Consequently, each tone model \nweight represents the dominance of the tone model and \nthereby the dominance of the tone model’s pitch in the \nobserved PDF. \n2.4. Melodic fragments \nWeights produced by the EM algorithm indicate the \ndominant pitches in short regions of time across the \nsignal. Melodic fragments are formed by tracking the \ndominant pitches through time and thereby forming \nfragments with continuous pitch contours (loudness or other factors are not taken into consideration). The first \npart of the procedure is similar to pitch salience \ncalculation as described by Goto [8]. For each pitch with \nweight greater than a dynamically adjusted threshold, \nsalience is calculated according to its dominance in a 50 \nms look-ahead window. The procedure tolerates pitch \ndeviations and individual noisy frames that might \ncorrupt pitch tracks by looking at the contents of the \nentire 50 ms window.  \nAfter saliences are calculated, melodic fragments are \nformed by continuously tracking the dominant salient \npeaks and producing fragments along the way. The final \nresult of this simple procedure is a set of melodic \nfragments, which may overlap in time, are at least 50 ms \nlong and may have slowly changing pitches. Parameters \nof each fragment are its start and end time, its time-\nvarying pitch and its time-varying loudness. The \nfragments obtained provide a reasonable segmentation \nof the input signal into regions with stable dominant \npitch. An example is given in Fig. 1, which shows \nsegmentation obtained on a 5.5 seconds excerpt from \nAretha Franklin's interpretation of the song Respect. 25 \nfragments were obtained; six belong to the melody sung \nby the singer, while the majority of others belong to \ndifferent parts of the accompaniment, which become \ndominant when lead vocals are out of the picture. \nAdditionally, three noisy fragments were found, which \nwere either due to consonants or drum parts.  \nWe performed informal listening tests by \nresynthesizing the fragments (on the basis of their pitch \nand amplitude) and comparing these resynthesized \nversions with the original signal. Most of the fragments \nperfectly captured the dominant pitch in the areas, even \nif, while listening to the entire original signal, some of \nthe fragments found were not immediately obvious to \nthe listener (i.e. keyboard parts in the given example). \nWe carried out such tests on a set of excerpts from 10 \ndifferent songs, covering a variety of styles, from jazz, \npop/rock to dance, and the overall performance of the \nalgorithm for finding melodic fragments was found to be \nsatisfying; it discovered a majority of fragments \nbelonging to the main melodic line, which is the main \npoint of interest in this study.  \nMost errors of the fragment finding procedure are \noctave-related, when the pitch of a segment is found to \nbe an octave higher or lower than the perceived pitch. \nAlso, areas in which several competing melodic lines \nFigure 1. Segmentation into melodic fragments of an excerpt from Respect sung by Aretha Franklin. \n  \n \n with similar loudnesses compete for listener's attention \nare problematic, as the EM algorithm tends to switch \nbetween lines thereby producing series of broken \nfragments. Sometimes, such switching also appears \nbetween a line and its octave equivalent, which is highly \nundesirable (i.e. bass line in the right part of Fig. 1).  \n3. FORMING MELODIC LINES \nThe goal of our project is to extract one or more melodic \nlines from an audio recording. How is a melodic line, or \nmelody, defined? There are many definitions; Levitin \ndescribes melody as an auditory object that maintains its \nidentity under certain transformations along the six \ndimensions of pitch, tempo, timbre, loudness, spatial \nlocation, and reverberant environment; sometimes with \nchanges in rhythm; but rarely with changes in contour \n[12]. Not only that melodies maintain their identity \nunder such transformations, or rather because of that, \nmelodies themselves are usually (at least locally in time) \ncomposed of events that themselves are similar in pitch, \ntempo, timbre, loudness, etc.  \nThe fact becomes useful when we need to group \nmelodic fragments, like the ones found by the procedure \ndescribed before, into melodic lines. In fact, the process \nof discovering melodic lines becomes one of grouping \nmelodic fragments through time into melodies. \nFragments are grouped according to their properties. \nIdeally, one would make use of properties which \naccurately describe the six dimensions mentioned \nbefore, especially pitch, timbre, loudness and tempo. \nOut of these, timbre is the most difficult to model; we \nare not aware of studies that would reliably determine \nthe timbre of predominant voices in polyphonic audio \nrecordings. Many studies, however, make use of timbre \nrelated features, when comparing pieces according to \ntheir similarity, classifying music according to genre, \nidentifying the singer, etc. (i.e. [13]). The features used \nin these studies could be applied to our problem, but so \nfar we have not yet made such attempts. To group \nfragments into melodies, we currently make use of only \nfive  features, which represent:  \n- dominance: average weight of the tone model that \noriginated the fragment, as calculated by the EM \nprocedure; \n- pitch: centroid of fragment's frequency with regard to \nits weight; \n- loudness: mean loudness calculated according to \nZwicker's loudness model [11] for partials belonging \nto the fragment; \n- pitch stability: average change of pitch over successive \ntime instances. This could be classified as the only \ntimbral feature used and mostly separates vocal parts \nfrom stable instruments; \n- onset steepness: steepness of overall loudness change \nduring the first 50 ms of the fragment's start. The \nfeature penalizes fragments that come into picture \nwhen a louder sound stops (i.e. fragments belonging to \naccompaniment).  \n To group melodic fragments into melodies, we use a \nmodified Gaussian mixture model estimation procedure, \nwhich makes use of equivalence constraints during the \nEM phase of model estimation [14]. Gaussian Mixture \nModels (GMMs) are one of the more widely used \nmethods for unsupervised clustering of data, where \nclusters are approximated by Gaussian distributions, \nfitted on the provided data. Equivalence constraints are \nprior knowledge concerning pairs of data points, \nindicating if the points arise from the same source \n(belong to the same cluster - positive constraints) or \nfrom different sources (different clusters - negative \nconstraints). They provide additional information to the \nGMM training algorithm, and seem intuitive in our \ndomain. We use GMMs to cluster melodic fragments \ninto melodies according to their properties. Additionally, \nwe make use of two facts to automatically construct \npositive and negative equivalence constraints between \nfragments.  \nFragments may overlap in time, as can be seen in Fig. \n1. We treat melody as a succession of single events \n(pitches). Therefore, we can put negative equivalence \nconstraints on all pairs of fragments that overlap in time. \nThis forbids the training algorithm to put two \noverlapping fragments in the same cluster and thus the \nsame melodic line. We also give special treatment to the \nbass line, which may appear quite often in melodic \nfragments (Fig. 1). To help the training algorithm with \nbass line clustering, we also put positive equivalence \nconstraints on all fragments with pitch lower than 170 \nHz. This does not mean that the training algorithm will \nnot add additional fragments to this cluster; it just causes \nall low pitched fragments to be grouped together.  \nThe clustering procedure currently only works on \nentire song excerpts (or entire songs). We are working \non a version that will work within an approx. 5 second \nsliding window and that will dynamically process new \nfragments and reform existing clusters or form new \nclusters as it progresses through a given piece. Such \nprocedure will more accurately reflect the nature of \nhuman perception of music, mimicking short-term \nmusical memory. \nWe have not yet made any extensive tests of the \naccuracy of our melody extracting procedure. This is \nmainly due to the lack of a larger annotated collection of \nsongs that could be used to automatically measure the \naccuracy of the approach. Results of clustering on a 30 \nsecond excerpt of Otis Redding's song Respect, as sung \nby Aretha Franklin, are given in Table 1.  \n \n lead \nvocal back \nvocals  \nbass  \nguitar  \nbrass  \nkeys  \nnoise \nC1 0.03 0.24 0.03 0 0.1 0.33 0.35 \nC2 0.93 0.29 0 0 0.1 0 0.05 \nC3 0.03 0.38 0 0.33 0.3 0 0.3 \nC4 0 0 0.97 0 0.05 0.33 0.08 \nC5 0 0.1 0 0.67 0.45 0.33 0.22 \nTable 1. GMM clustering of fragments from \"Respect\"   \n \n 152 melodic fragments were found by the fragment \nfinding procedure; all lead vocal and backing vocal parts \nwere correctly discovered. All fragments were hand \nannotated into one of seven categories (lead vocal, \nbacking vocals, bass, guitar, brass, keyboards, noise). \nFragments were then clustered by the GMM algorithm \ninto five clusters, which would ideally represent the \nmelody (lead vocal), bass line, backing vocals, \naccompaniment and noise. Results of the clustering \nprocedure are given in Table 1. It shows percentages of \nfragments belonging to the seven annotated categories in \nthe five clusters. Ideally, lead vocal fragments (melody) \nwould all be grouped into one cluster with no additional \nfragments. Most (93%) were indeed grouped into cluster \n2, but the cluster also contains some other fragments, \nbelonging to backing vocals, brass and a small amount \nof noise. The majority of bass fragments were put into \ncluster 4, together with some low pitched keyboard \nparts, while other clusters contain a mixture of \naccompaniment and backing vocals. As our goal lies \nmainly in the discovery of the (main) melodic line, \nresults are satisfying, especially if we take into \nconsideration that practically no timbre based features \nwere taken into consideration when clustering. Most of \nthe melody is represented by fragments in cluster 2, with \nsome additional backing vocal fragments, which could \nactually also be perceived as part of the melody.  \nThe effect of negative and positive constraints on the \nclustering procedure was also assessed; somewhat \nsurprisingly, constraints did not have a large impact on \nthe clustering procedure. Small improvements were \nachieved mostly in separation of accompaniment from \nlead vocal and bass lines.  \n4. CONCLUSIONS \nThe presented approach to melody extraction is still in \nan initial phase, but we are satisfied with first obtained \nresults. Currently, we are in the process of annotating a \nlarger number of pieces, which will be used for \nimproving the feature set used in GMM training, as so \nfar, we settled for a very small number of parameters, \nmainly because of the small set of examples we worked \nwith. We plan to concentrate on timbral features, which \nare expected to bring improvements, especially with \nmismatches in parts where accompaniment becomes \ndominant. The larger database will also enable us to test \nand compare several different clustering strategies. \n5. REFERENCES \n[1] S. Dixon, “Automatic Extraction of Tempo and \nBeat from Expressive Performances”, Journal \nof New Music Research, 30 (1), pp 39-58, \n2001. \n[2] J. Seppänen, “Tatum grid analysis of musical \nsignals”, in Proc. IEEE Workshop on \nApplications of Signal Processing to Audio and Acoustics, New Paltz, New York, Oct. 21--24, \n2001. \n[3] A. Sheh and D.P.W. Ellis, “Chord \nSegmentation and Recognition using EM-\nTrained Hidden Markov Models”, in \nProceedings of ISMIR 2003, Baltimore, \nMaryland, USA, 2003.  \n[4] T. De Mulder, J.P. Martens, M. Lesaffre, M. \nLeman, B. De Baets, H. De Meyer, “An \nAuditory Model Based Transcriber of Vocal \nQueries”, Proc. of ISMIR 2003, Baltimore, \nMaryland, USA, October 26-30, 2003. \n[5] Klapuri, A, “Automatic transcription of music,” \nin Proceedings Stockholm Music Acoustics \nConference, Stockholm, Sweden, Aug. 6-9, \n2003. \n[6] Marolt M, \"Networks of Adaptive Oscillators \nfor Partial Tracking and Transcription of Music \nRecordings,\" Journal of New Music Research, \n33 (1), 2004. \n[7] J.P. Bello, Towards the Automated Analysis of \nsimple polyphonic music: A knowledge-based \napproach, Ph.D. Thesis, King's College \nLondon - Queen Mary, Univ. of London, 2003. \n[8] M. Goto, “A Predominant-F0 Estimation \nMethod for CD Recordings: MAP Estimation \nusing EM Algorithm for Adaptive Tone \nModels”, in Proc. of 2001 IEEE International \nConference on Acoustics, Speech, and Signal \nProcessing, pp.V-3365-3368, May 2001. \n[9] X. Serra and J. O. Smith, “Spectral Modeling \nSynthesis: A Sound Analysis/Synthesis System \nBased on a Deterministic Plus Stochastic \nDecomposition”, Computer Music Journal \n14(4), pp. 14–24, 1990. \n[10] T. Painter, A. Spanias, “Perceptual Coding of \nDigital Audio”, Proceedings of the IEEE, Vol. \n88 (4), 2000. \n[11] E. Zwicker, H. Fastl, Psychoacoustics: Facts \nand Models, Berlin: Springer Verlag, 1999. \n[12] D.J. Levitin, “Memory for Musical Attributes \nfrom Music”, Cognition, and Computerized \nSound, Perry R. Cook (ed.). Cambridge, MA: \nMIT Press, 1999.  \n[13] T. Zhang, “Automatic singer identification”, \nProceedings of IEEE Conference on \nMultimedia and Expo, vol.1, pp.33-36, \nBaltimore, July 6-9, 2003. \n[14] N. Shental, A.B. Hillel, T. Hertz, D. Weinshall, \n\"Computing Gaussian Mixture Models with \nEM using Side-Information\", in Proc. of Int. \nConference on Machine Learning, ICML-03, \nWashington DC, August 2003."
    },
    {
        "title": "Automatic Genre Classification Using Large High-Level Musical Feature Sets.",
        "author": [
            "Cory McKay",
            "Ichiro Fujinaga"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416158",
        "url": "https://doi.org/10.5281/zenodo.1416158",
        "ee": "https://zenodo.org/records/1416158/files/McKayF04.pdf",
        "abstract": "This paper presents a system that extracts 109 musical features from symbolic recordings (MIDI, in this case) and uses them to classify the recordings by genre. The features used here are based on instrumentation, texture, rhythm, dynamics, pitch statistics, melody and chords. The classification is performed hierarchically using different sets of features at different levels of the hierarchy. Which features are used at each level, and their relative weightings, are determined using genetic algorithms. Classification is performed using a novel ensemble of feedforward neural networks and k-nearest neighbour classifiers. Arguments are presented emphasizing the importance of using high-level musical features, something that has been largely neglected in automatic classification systems to date in favour of low-level features. The effect on classification performance of varying the number of candidate features is examined in order to empirically demonstrate the importance of using a large variety of musically meaningful features. Two differently sized hierarchies are used in order to test the performance of the system under different conditions. Very encouraging classification success rates of 98% for root genres and 90% for leaf genres are obtained for a hierarchical taxonomy consisting of 9 leaf genres.",
        "zenodo_id": 1416158,
        "dblp_key": "conf/ismir/McKayF04",
        "keywords": [
            "MIDI",
            "genres",
            "symbolic recordings",
            "classification",
            "musical features",
            "genetic algorithms",
            "feedforward neural networks",
            "k-nearest neighbour classifiers",
            "high-level musical features",
            "low-level features"
        ],
        "content": "AUTOMATIC GENRE CLASSIFICATION USING LARGE \nHIGH-LEVEL MUSICAL FEATURE SETS\nCory McKay Ichiro Fujinaga \nMcGill University \nFaculty of Music \nMontreal, Quebec, Canada \ncory.mckay@mail.mcgill.ca \n McGill University \nFaculty of Music \nMontreal, Quebec, Canada \nich@music.mcgill.ca \n \nABSTRACT \nThis paper presents a system that extracts 109 musical \nfeatures from symbolic recordings (MIDI, in this case) \nand uses them to classify the recordings by genre. The \nfeatures used here are based on instrumentation, texture, \nrhythm, dynamics, pitch statistics, melody and chords. \nThe classification is performed hierarchically using \ndifferent sets of features at different levels of the \nhierarchy. Which features are used at each level, and \ntheir relative weightings, are determined using genetic \nalgorithms. Classification is performed using a novel \nensemble of feedforward neural networks and k-nearest \nneighbour classifiers. \nArguments are presented emphasizing the importance \nof using high-level musical features, something that has \nbeen largely neglected in automatic classification \nsystems to date in favour of low-level features.  \nThe effect on classification performance of varying \nthe number of candidate features is examined in order to \nempirically demonstrate the importance of using a large \nvariety of musically meaningful features. Two \ndifferently sized hierarchies are used in order to test the \nperformance of the system under different conditions. \nVery encouraging classification success rates of 98% \nfor root genres and 90% for leaf genres are obtained for \na hierarchical taxonomy consisting of 9 leaf genres. \n \nKEYWORDS \nGenre, classification, hierarchical, features, music. \n1. INTRODUCTION \nMusical genre has a particular importance in the field of \nmusic information retrieval. It is used by retailers, \nlibrarians, musicologists and listeners in general as an \nimportant means of organizing music. Anyone who has \nlooked through the discount bins of a music store will \nhave experienced the frustration of searching through \nmusic that is not sorted by genre. Furthermore, the \nimportance of genre in the mind of listeners is \nexemplified by research indicating that the style in which a piece is performed can influence listeners’ \nliking for the piece more than the piece itself [13]. \nThe need for an effective automatic means of \nclassifying music is becoming increasingly pressing as \nthe number of recordings available continues to increase \nat a rapid rate. Software capable of performing \nautomatic classifications would be particularly useful to \nthe administrators of the rapidly growing networked \nmusic archives, as their success is very much linked to \nthe ease with which users can search for types of music \non their sites. These sites currently rely on manual genre \nclassifications, a methodology that is slow and unwieldy. \nAn additional problem with manual classification is that \ndifferent people classify genres differently, leading to \nmany inconsistencies. \nThere has been a significant amount of research into \nusing low-level features to classify audio recordings into \ncategories based on factors such as genre and style (see \nSection 2). Although this research is certainly very \nvaluable, the current lack of reliable polyphonic \ntranscription systems makes it difficult to impossible to \nextract high-level features from audio recordings, as this \nrequires precise knowledge of information such as the \npitch and timing of individual notes. Most research to \ndate has therefore made use of primarily low-level, \nsignal-processing based features. Although there have \nbeen some very interesting efforts to generate features \nwith musical meaning from audio recordings, the \nlimitations of current signal-processing capabilities has \nlimited these endeavours so far. \nThe problem of implementing a reliable genre \nclassifier that deals with realistic taxonomies has yet to \nbe solved. It is therefore appropriate to take advantage \nof whatever resources are available in order to improve \nperformance. There is a large body of existing \nrecordings in symbolic formats. High-level features can \nbe extracted from digital formats such as MIDI, \nMusicXML, Humdrum and GUIDO, and optical music \nrecognition techniques can also be used to process \nscores into digital files from which high-level features \ncan be extracted. It is therefore reasonable to pursue \nresearch in the use of high-level features extracted from \nrecordings in symbolic formats. This largely untapped \nsource of features could be used to supplement low-level \nfeatures extracted from audio recordings. If automated \ntranscription does improve, then audio recordings could \nbe translated into symbolic form, and research in the use \nof high-level features would become even more useful. \nFurthermore, high-level features make it possible to \nclassify scores, be they paper or digital, when audio Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that \ncopies are not made or distributed for profit or commercial \nadvantage and that copies bear this notice and the full citation on \nthe first page. \n© 2004 Universitat Pompeu Fabra. \n   \n \nrecordings are not available. It therefore seems \nappropriate to pursue research into classification with \nhigh-level features in parallel with further research \ninvolving low-level features. \nIn addition to practical applications, a system that can \nautomatically classify recordings by genre has \nsignificant theoretical musicological interest as well. \nThere is currently a relatively limited understanding of \nhow humans construct musical genres, the mechanisms \nthat they use to classify music and the characteristics \nthat are used to perceive the differences between \ndifferent genres. A system that could automatically \nclassify music and reveal what musical dimensions it is \nusing to do so would therefore be of great interest. Low-\nlevel signal processing based features are of little use in \nthis respect, something that further emphasizes the \nimportance of studying the use of high-level features. \nThis kind of research also has applications beyond \nthe scope of genre classification. The techniques \ndeveloped for a genre classification system could be \nadapted for other types of classifications, such as by \ncompositional style or historical period. Once a \nclassification system is implemented, one need only \nmodify the particular training recordings and taxonomy \nthat are used in order to perform arbitrary types of \nclassifications. \nThe key to the success of all of this is the choice of \nfeatures. Although effective classifiers are certainly \nnecessary, the performance of even a perfect classifier is \nlimited by its percepts. A realistic genre taxonomy will \ninclude tens, and possibly hundreds, of categories. \nFurthermore, the categories that are actually used in \npractice are inconsistent and contain a good deal of \noverlap. It is therefore necessary to have a wide range of \nfeatures available in order to effectually segment such a \ndifficult feature space. \nUnfortunately, the use of too many features can \noverload classifiers by providing them with too much \ninformation. A good feature selection system is therefore \nessential so that the most relevant features are taken \nadvantage of and the others are eliminated. The use of a \nhierarchical taxonomy paired with a successful feature \nselection system can improve performance, as one can \nfirst make coarse classifications using certain sets of \nfeatures, and then use different sets of features to make \nincreasingly finer classifications as one descends down \nthe taxonomical hierarchy. \nSection 2 of this paper reviews recent research in \nautomatic genre classification and some musicological \nresearch on features in general. Section 3 provides an \noverview of the features that were used here and the \nphilosophy behind their selection. Section 4 discusses \nthe feature selection and classification techniques that \nwere used and Section 5 explains how these techniques \nwere used to perform hierarchical classifications. \nSection 6 presents the experiments that were performed \nto test the effectiveness of the system, and the results. \nSection 7 provides some final conclusions. 2. RELATED RESEARCH \nThere have been a number of interesting studies on \nautomatic genre classification of audio files. Tzanetakis \net al. [19, 20] have published research that used a variety \nof low-level features to achieve success rates of 61% \nwhen classifying between ten genres. \nAdditional research based on audio recordings has \nbeen performed by Pye, who successfully classified \nmusic into one of six categories 92% of the time [16]. \nDeshpande, Nam and Singh constructed a system that \ncorrectly classified among three categories 75% of the \ntime [3]. Jiang et al. correctly classified 90.8% of \nrecordings into five genres [8]. Kosina achieved a \nsuccess rate of 88% with three genres [9]. Grimaldi, \nKokaram and Cunningham achieved a success rate of \n73.3% when classifying between five categories [6]. Xu \net al. achieved a success rate of 93% with four \ncategories [22]. McKinney and Breebaart achieved a \nsuccess rate of 74% with seven categories [12].  \nThere has been somewhat less research into the \nclassification of symbolic data (e.g. MIDI). Chai and \nVercoe were successful in correctly performing three-\nway classifications 63% of the time [2]. Shan and Kuo \nachieved success rates between 64% and 84% for two-\nway classifications [17]. Although these studies are very \ninteresting, they focus more on pattern classification \ntechniques rather than features. \nThere has also been some important research by \nWhitman and Smaragdis on combining features derived \nfrom audio recordings with “community metadata” that \nwas derived from text data mined from the web [21]. \nAlthough beyond the scope of this paper, this line of \nresearch holds a great deal of potential, despite the \nproblems related to finding, parsing and interpreting the \nmetadata. \nIt should be noted that there are number of \nproblematic issues that have been brought up relating to \nthe formation of genre taxonomies in general [1, 14]. \nAlthough there has been a great deal of work on \nanalyzing and describing particular types of music, there \nhas been relatively little research on deriving features \nfrom music in general. Lomax and his colleagues in the \nCantometrics project [10] have performed the most \nextensive work to date in this direction. They compared \nthousands of songs from hundreds of different cultural \ngroups using 37 features. Although there have been a \nfew other efforts to list categories of features, they have \ntended to be overly broad. Works such as Tag’s \n“checklist of parameters” [18] are still useful as a \ngeneral guide, however. A number of musicologists, \nsuch as Fabri [4], have also done some very interesting \nwork on musical genre theory. \n3. FEATURE EXTRACTION \nIn this study, features were extracted from MIDI files. \nThis format was chosen because a diverse range of files \nare easily available in this format. Although it is true that   \n \ngenre classification of MIDI files in particular is not a \npressing problem from a practical perspective, the \nfeatures discussed here could just as easily be extracted \nfrom other symbolic formats. \nWhen choosing high-level features to use, it was \nnecessary to keep in mind that it is desirable not only to \nhave features that effectively partition recordings into \ndifferent categories, but also to have features that are of \nmusicological interest. As an initial step towards \narriving at such features, one might look to how humans \naccomplish this task, as we are able to successfully \nperform genre classifications, so we do provide one, \nalbeit not the only, viable model. \nOne might imagine that high-level musical structure \nand form play an important role, given that this is an \narea on which much of the theoretical literature has \nconcentrated. This does not appear to be the case, how-\never. Research has found that humans with little to \nmoderate musical training are able to make genre \nclassifications agreeing with those of record companies \n72% of the time (among a total of 10 genres),  based on \nonly 300 milliseconds of audio [15]. This is far too little \ntime to perceive musical form or structure. This suggests \nthat there must be a sufficient amount of information \navailable in very short segments of music to successfully \nperform classifications. This does not mean that one \nshould ignore musical form and structure, as these are \nlikely useful as well, but it does mean that they are not \nstrictly necessary. However, it is probably a better \napproach to extract features based on simple musical \nobservations rather than on sophisticated theoretical \nmodels. Such models tend to have limited applicability \nbeyond the limited spheres which they were designed to \nanalyze, and sophisticated automatic musical analysis \nremains an unsolved problem in many cases. \nIdeally, one would like to use features consisting of \nsimple numbers. This makes storing and processing \nfeatures both simpler and faster. Features that represent \nan overall aspect of a recording are particularly \nappropriate in this respect. Features based on averages \nand standard deviations allow one to see the overall \nbehaviour and characteristics of a particular aspect of a \nrecording, as well as how much it varies. \nA catalogue of 160 features that can be used to \ncharacterize and classify recordings was devised [11], \n109 of which were implemented in the system discussed \nhere. Although too numerous to discuss here in detail, \nthese features belong to the following seven categories: \n \n· Instrumentation (e.g. whether modern instruments are \npresent) \n· Musical Texture (e.g. standard deviation of the average \nmelodic leap of different lines) \n· Rhythm (e.g. average time between attacks) \n· Dynamics (e.g. average note to note change in loudness) \n· Pitch Statistics (e.g. fraction of notes in the bass register) \n· Melody (e.g. fraction of melodic intervals comprising a \ntritone) \n· Chords (e.g. prevalence of most common vertical interval) \n Two types of features were used: one-dimensional \nfeatures and multi-dimensional features. One-\ndimensional features each consist of a single number \nthat represents an aspect of a recording in isolation. \nMulti-dimensional features consist of sets of related \nvalues that have limited significance taken alone, but \ntogether may reveal meaningful patterns. The reason for \nthis differentiation is explained in Section 4.  \n4. CLASSIFICATION METHODOLOGY AND \nFEATURE SELECTION \nThe first stage in selecting a classification method \ninvolves choosing one of the three basic paradigms: \nexpert systems, supervised learning and unsupervised \nlearning. Expert systems involve explicitly implemented \nsets of rules, something that is not appropriate for genre \nclassification, given the complexity of the task and the \nlimited extent to which the content-based differences \nbetween genres are understood. \nUnsupervised learning involves simply allowing a \nsystem to cluster samples together based on similarities \nthat it perceives in the feature space. Although this is \ncertainly useful for certain types of study, such as \ngrouping anonymous recordings in order to gain insights \ninto possible composers, it is of limited applicability to \nlarge-scale genre taxonomies. The genre categories that \nhumans use are often inconsistent and illogical, so the \ngroupings produced by unsupervised learning, although \ninteresting theoretically, would likely bear little \nresemblance to the actual categories used by humans. \nSupervised learning systems appear to be the best \noption, and were used exclusively here. These systems \ninvolve giving classifiers labelled training samples. The \nclassifier then attempts to form (hopefully generalisable) \nrelationships between the features of the training \nsamples and the related categories. \nTwo well-known types of supervised classification \ntechniques were used in the system described here: \nfeedforward neural networks (NN) and k-nearest \nneighbour (KNN). NNs have the advantage of being \nable to simulate logical relationships between features, \nbut can require long training times. KNN classifiers, in \ncontrast, cannot simulate sophisticated logical \nrelationships between features, but require essentially no \ntraining time. The use of both techniques allows one to \nuse NNs where the modelling of more sophisticated \nrelationships between features is likely to be most \nbeneficial, while using KNN classifiers elsewhere in \norder to limit training times.   \nKNN classifiers operate by treating each sample as a \npoint in feature space and finding the distribution of the \ncategories of the k training points closest to each test \nsample. Feedforward NNs operate by constructing a \nnetwork of input units, hidden units and output units \nconnected by weights. Each input to a unit is multiplied \nby its particular weight, and the sum of the results is fed \ninto an activation function (the sigmoidal function, in \nthis case). Training is performed by iteratively   \n \nperforming a gradient descent through error space and \nmodifying the weights. \nThe relative advantages and disadvantages of these \ntwo approaches was the motivation behind the one-\ndimensional and multi-dimensional features discussed in \nSection 3. For example, the bins of a histogram \nconsisting of the relative frequency of different melodic \nintervals were treated as a multi-dimensional feature, but \nthe average duration of melodic arcs was treated as a \none-dimensional feature. Although it is of course true \nthat all features are potentially interrelated logically, \nthose sub-features grouped into multi-dimensional \nfeatures were particularly subject to this \ninterdependence. \nEach multi-dimensional feature was classified by a \nseparate multi-dimensional neural network, thus \nincreasing the likelihood that appropriate relationships \nwould be learned between the components of each \nmulti-dimensional feature. The one-dimensional \nfeatures, in contrast, were all processed by a single KNN \nclassifier. This greatly reduced the training time, as the \nmajority of features were one-dimensional, and training \na neural network or networks to process them would \nhave been too time consuming. \nFeature selection was performed in several stages, all \nof which used genetic algorithms (GAs). GAs have been \nused successfully in the past for musical classification \n[5], and recent research has confirmed their fitness for \nfeature selection and weighting [7]. \nGAs make use of “chromosomes” that contain bit \nstrings that are iteratively evolved. The “fitness” of each \nchromosome is evaluated after each generation, and the \nbest performers combine their bit strings to form the \nnext generation. Techniques such as random “mutation” \nof bits and “cloning” of top performers can also be used. \nThis results in increasingly fit chromosomes whose bit \nstrings represent better solutions to problems. \nGAs offer no guarantee of finding optimal solutions, \nbut they often do provide good solutions in a reasonable \namount of time. Considering that an exhaustive \nexploration of the feature selection problem is too \ncomputationally expensive when dealing with large \nnumbers of features, GAs are a good alternative.  \nThe first stage of feature selection was performed by \nusing GAs to find the features that provided the best \nresults for the KNN classifier. All other features were \nthen ignored by the KNN classifier, and GAs were \napplied again to find the best relative feature weightings. \nThe NNs for each multi-dimensional feature were \nthen trained. The combined classification of the \nKNN/NN ensemble was found by calculating an average \nof the classification scores for each category produced \nby each component of the ensemble (i.e. the KNN and \neach NN). A final feature selection stage was then \nperformed by applying GAs to each of the components \nof the ensemble in order to potentially eliminate some. A \nfinal weighting was evolved using GAs for each of the \nsurviving members of the ensemble. The result of all of this after training was a weighted \nensemble of classifiers consisting of a single KNN \nclassifier using a weighted subset of all possible one-\ndimensional features and a set of NNs representing a \nsubset of all possible multi-dimensional features. Such a \nclassifier ensemble could be seen as a black box that \ntook in the entire feature set of a recording as input, \nignored the features it had selected out, and output a \nclassification score for each candidate category that it \nhad been trained to recognize. A number of these black \nboxes were trained to classify recordings hierarchically, \nas described in Section 5 below. \n5. HIERARCHICAL CLASSIFICATION \nAs mentioned previously, classification was performed \nhierarchically. Recordings were first classified by “root \ngenre” (i.e. the broadest genre categories, such as Jazz, \nClassical or Popular). Classification then proceeded to \nthe next level of the hierarchy, where only the sub-\ncategories of the winners of the previous stage of \nclassification were considered as candidates. This \ncontinued iteratively down the hierarchy of genres until \nonly “leaf genres” (i.e. genres with no sub-categories) \nremained, and these were chosen as the winning genres. \nThe classification at each level of the hierarchy \ninvolved separately trained specialist classifier \nensembles of the type presented in Section 4. Each of \nthese classifier ensembles was trained only on \nrecordings belonging to their candidate categories, and \ntherefore developed feature selections and weightings \nespecially suited to their categories. A Jazz classifier, for \nexample, might be trained only on Swing, Bebop and \nFunky Jazz recordings, whereas a root classifier would \nbe trained on recordings of all genres. A root classifier \nwould therefore likely be good at making coarse \nclassifications, but a Jazz classifier would likely be \nbetter at classifying a recording into specialized sub-\ngenres of jazz once the recording had been labelled as \nJazz by the root classifier. \nHierarchical classification has the potential weakness \nthat a mistake made at a broad level of the hierarchy can \nlead to a decent through an entirely erroneous branch of \nthe hierarchy. Basic flat classification was therefore \nperformed as well as hierarchical classification, in order \nto provide a reference point that would enable one to \ndetermine whether hierarchical classification did indeed \nimprove performance. A more detailed experimental \ncomparison of the application of different classification \nmethodologies to this problem, including hybrid \nmethodologies, will be available in [11]. \n6. THE EXPERIMENT \nA total of 950 MIDI recordings were used for training \nand testing, using a five-fold cross-validation process. \nTwo different taxonomies were used in order to assess \nthe performance of the system under different \nconditions. The number of candidate features available   \n \nfor feature selection was also varied, in order to judge \nthe significance of the number of features available. \nThe first taxonomy that was used consisted of three \nroot genres and nine leaf genres, as shown in Figure 1. \nThis particular taxonomy was chosen because it is \ncomparable in size and diversity to the previous research \ndiscussed in Section 2. \n \nClassical Jazz Popular \n   Baroque    Bebop    Country \n   Modern    Funky Jazz    Punk \n   Romantic    Swing    Rap \nFigure 1. Basic taxonomy used. \nThe results using hierarchical classification were \nexcellent, as can be seen in Figure 2. On average, the \nroot genre was correctly identified 98% of the time and \nthe leaf genre was correctly identified 90% of the time. \nThese rates are not only much higher than those of the \nexisting symbolic data systems discussed in Section 2, \nbut also as good, and in most cases better, than the \nexisting audio systems. Furthermore, the experiment \nperformed here with flat classification resulted in \nreduced success rates of 96% and 86% for root and leaf \ngenres respectively. This decrease in performance \ndemonstrates the utility of hierarchical classification. \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 2. Average classification success rate. The leaf \ngenre bars give the average success rates of the leaf \ngenres belonging to the corresponding root genres. \nIt was found that the particular features selected by \neach of the classifier ensembles varied significantly. \nThis was as expected, as the types of features that would \nbe useful in differentiating between different types of \nClassical music, for example, would reasonably be \ndifferent than those used to differentiate between \nCountry and Rap music. The particular features chosen \nby different classifier ensembles is a source of great \nmusicological interest, and would be a rich topic for a \nfuture paper. Although generalizations are difficult to \nmake due to the variety between classifier ensembles, \none interesting observation that can be made is that \nfeatures related to instrumentation were particularly \nimportant, as they were assigned a collective average of \n47% of the weightings allocated amongst the features \ncomprising the seven feature groups. \nThe differences between the weightings evolved by \nthe different classifier ensembles implies that the development of a large catalogue of features than can \nserve as candidates for feature selection for different \nspecialized classifiers could be of great use. An \nadditional experiment was performed to further \ninvestigate this. The experiment described above was \nrepeated three more times, but only a randomly selected \nsubset of the total available features was made visible to \nthe feature selection systems. The results, displayed in \nFigure 3, demonstrate that performance increased \nsignificantly when more candidate features were \navailable for feature selection. \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 3. Effect of number of features available for \nfeature selection on flat classification success rates. \nA taxonomy with nine leaf categories is of course \nsomewhat limited from the perspective of realistic \nclassification problems. In order to investigate the real-\nworld applicability of the system, an experiment was \nperformed using an expanded multi-level taxonomy \ncomposed of 9 root categories and 38 leaf categories, a \ntaxonomy much larger than has previously been used to \ntest an automatic genre classification system. The root \ngenres and the leaf genres were respectively successfully \nidentified 81% and 57% of the time. Although not \nsufficiently high for practical purposes, these rates were \nsignificantly higher than chance (11% and 3% \nrespectively), and the results show that there is at least \nthe potential to successfully deal with realistically sized \ntaxonomies, something which has not been done \npreviously. \n7. CONCLUSIONS \nVery encouraging success rates of 98% for root genres \nand 90% for leaf genres were achieved for a taxonomy \nconsisting of 3 root genres and 9 leaf genres. These rates \ncompare favourably to results achieved in previous \nresearch. Further experiments demonstrated that \nincreasing the number of features available to a \nhierarchical classification system that uses feature \nselection to train specialist classifiers causes \ncorresponding improvements in performance. A further \nexperiment with a greatly expanded taxonomy showed \nthe potential of the system described here for \nsuccessfully dealing with a realistic taxonomy, \nsomething that is as yet an unsolved problem.  \nThis research demonstrated the effectiveness of large \nsets of high-level musical features for genre Effect of Candidate Features\n50556065707580859095100\n20 56 85 109\nNumber of Candidate FeaturesSuccess Rate (%)Root Genres\nLeaf Genres\nClassification Performance\n0102030405060708090100\nClassical Jazz Pop Average ChanceSuccess Rate (%)Root Genres\nLeaf Genres  \n \nclassification when paired with good feature selection \nmethods and hierarchical classification. Future research \ninto both the use of the hierarchical classification \ntechniques described here and the development of \nfurther high-level features is certainly warranted, and \ncould potentially lead to a viable classifier capable of \ndealing with realistically large genre taxonomies. \nFurther study of which features were selected by which \nspecialist classifier ensembles could also be of great \nmusicological interest. \n8. ACKNOWLEDGEMENTS \nThanks to the Fonds Québécois de la recherche sur la \nsociété et la culture for their generous financial support, \nwhich has helped to make this research possible. \n9. REFERENCES \n[1] Aucouturier, J. J., and F. Pachet. 2003. \nRepresenting musical genre: A state of the art. \nJournal of New Music Research 32 (1): 1–12.  \n[2] Chai, W., and B. Vercoe. 2001. Folk music \nclassification using hidden Markov models. \nProceedings of the International Conference on \nArtificial Intelligence. \n[3] Deshpande, H., U. Nam, and R. Singh. 2001. \nClassification of music signals in the visual \ndomain. Proceedings of the Digital Audio \nEffects Workshop. \n[4] Fabbri, F. 1982. What kind of music? Popular \nMusic 2: 131–43. \n[5] Fujinaga, I. 1996. Exemplar-based learning in \nadaptive optical music recognition system. \nProceedings of the International Computer \nMusic Conference. 55–6. \n[6] Grimaldi, M., A. Kokaram, and P. \nCunningham. 2003. Classifying music by genre \nusing a discrete wavelet transform and a round-\nrobin ensemble. Work Report. Trinity College, \nUniversity of Dublin, Ireland. \n[7] Hussein, F., R. Ward, and N. Kharma. 2001. \nGenetic algorithms for feature selection and \nweighting, a review and study. International \nConference on Document Analysis and \nRecognition. 1240-4. \n[8] Jiang, D. N., L. Lu, H. J. Zhang, J. H. Tao, and \nL. H. Cai. 2002. Music type classification by \nspectral contrast feature. Proceedings of \nIntelligent Computation in Manufacturing \nEngineering. 113-6. \n[9] Kosina, K. 2002. Music genre recognition. \nDiploma thesis. Technical College of \nHagenberg, Austria.  [10] Lomax, A. 1968. Folk song style and culture. \nWashington, D.C.: American Association for \nthe Advancement of Science. \n[11] McKay, C. In press. Automatic genre \nclassification of MIDI recordings. Master’s \nthesis. McGill University, Canada. \n[12] McKinney, M. F., and J. Breebaart. 2003. \nFeatures for audio and music classification. \nProceedings of the International Symposium on \nMusic Information Retrieval. 151–8. \n[13] North, A. C., and D. J. Hargreaves. 1997. \nLiking for musical styles. Music Scientae 1: \n109–28. \n[14] Pachet, F., and D. Cazaly. 2000. A taxonomy \nof musical genres. Proceedings of the Content-\nBased Multimedia Information Access \nConference. \n[15] Perrott, D., and R. O. Gjerdingen. 1999. \nScanning the dial: An exploration of factors in \nthe identification of musical style. Research \nNotes. Department of Music, Northwestern \nUniversity, Illinois, USA. \n[16] Pye, D. 2000. Content-based methods for the \nmanagement of digital music. Proceedings of \nthe International Conference on Acoustics, \nSpeech, and Signal Processing. 2437–2440. \n[17] Shan, M. K., and F. F. Kuo. 2003. Music style \nmining and classification by melody. IEICE \nTransactions on Information and Systems E86-\nD (3): 655–69. \n[18] Tagg, P. 1982. Analysing popular music: \nTheory, method and practice. Popular Music 2: \n37–67. \n[19] Tzanetakis, G., and P. Cook. 2002. Musical \ngenre classification of audio signals. IEEE \nTransactions on Speech and Audio Processing \n10 (5): 293–302. \n[20] Tzanetakis, G., G. Essl, and P. Cook. 2001. \nAutomatic musical genre classification of audio \nsignals. Proceedings of the International \nSymposium on Music Information Retrieval. \n205–10.  \n[21] Whitman, B., and P. Smaragdis. 2002. \nCombining musical and cultural features for \nintelligent style detection. Proceedings of the \nInternational Symposium on Music Information \nRetrieval. 47–52. \n[22] Xu, C., N. C. Maddage, X. Shao, F. Cao, and \nQ. Tian. 2003. Musical genre classification \nusing support vector machines. Proceedings of \nthe International Conference on Acoustics, \nSpeech and Signal Processing. V 429–32."
    },
    {
        "title": "Extracting the perceptual tempo from music.",
        "author": [
            "Martin F. McKinney",
            "Dirk Moelants"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415146",
        "url": "https://doi.org/10.5281/zenodo.1415146",
        "ee": "https://zenodo.org/records/1415146/files/McKinneyM04.pdf",
        "abstract": "The study presented here outlines a procedure for mea- suring and quantitatively representing the perceptual tempo of a musical excerpt. We also present a method for apply- ing such measures of perceptual tempo to the design of automatic tempo-trackers in order to more accurately rep- resent the perceived beat in music. Keywords: Tempo, Perception, Beat-tracking",
        "zenodo_id": 1415146,
        "dblp_key": "conf/ismir/McKinneyM04",
        "keywords": [
            "perceptual tempo",
            "musical excerpt",
            "measuring",
            "quantitative representation",
            "beat-tracking",
            "automatic tempo-trackers",
            "design",
            "representation",
            "perceived beat",
            "keywords"
        ],
        "content": "EXTRACTING THE PERCEPTUAL TEMPO FROM MUSIC\nMartin F. McKinney\nPhilips Research Laboratories\nEindhoven, The NetherlandsDirk Moelants\nIPEM-Department of Musicology\nGhent University, Belgium\nABSTRACT\nThestudypresentedhereoutlinesaprocedureformea-\nsuringandquantitativelyrepresentingtheperceptualtempo\nofamusicalexcerpt. Wealsopresentamethodforapply-\ning such measures of perceptual tempo to the design of\nautomatictempo-trackersinordertomoreaccuratelyrep-\nresent the perceived beat inmusic.\nKeywords: Tempo, Perception, Beat-tracking\n1. INTRODUCTION\nTempoisabasicelementandusefuldescriptiveparameter\nof music and has been the focus of many systems for au-\ntomaticmusicinformationretrieval,i.e.,automatictempo\ntrackers [9]. When describing musical tempo, it is often\nuseful to make a distinction between notatedtempo and\nperceptual tempo. Notated and perceptual tempo can dif-\nfer in that, for a given excerpt of music, there is only a\nsingle notated tempo, while listeners unfamiliar with the\nscore can perceivethe tempo to exist at different metri-\ncal levels [6]. For some pieces of music, the perceptual\ntempo is quite ambiguous, while for others it is not. It\nis often desirable to have a representation of perceptual\ntempo rather than notated tempo, especially in situations\nwhere the notated tempo of an audio track is unknown or\nunavailable.\nAcommonproblemwithsystemsforautomatictempo\nextraction is that they do not distinguish between notated\nandperceptualtempoand,asaresult,cannotreliablyrep-\nresenteitherformoftempo. Afurtherconsequenceofthis\nshortcomingisthatmeaningfulperformanceevaluationof\nsuch systems is difﬁcult because the form of the output is\npoorly deﬁned.\nThis study provides a method for measuring and char-\nacterizing the perceptual tempo of musical excerpts and\nthenapplyingthecharacterizationtothedevelopmentand\ntesting of automatic tempoextractors.\nPreviousstudiesontheperceptionofpulsehaveshown\nthat listeners tend to prefer tempi near a “resonance” of\n∼120 beats per minute [3, 11, 7]. When subjects were\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2004 Universitat Pompeu Fabra.asked to tap to the beat in studies using artiﬁcial tone se-\nquencesandmusicalexcerptsasstimuli,theywouldpref-\nerentially tap at metrical levels whose tempi were in this\nresonantrange. Wehaveshowninasimilarstudythat,for\nindividual musical excerpts, a resonance model can pre-\ndict the distribution of subjects’ tapped tempi for some\nbut not all excerpts [6]. Several factors, including vari-\nous types of rhythmic accents (e.g., dynamic and dura-\ntional), are likely to cause the distribution of perceptual\ntempi for some excerpts to deviate from a simple reso-\nnance representation [8]. A proper system for tempo ex-\ntraction should accurately represent the perceptual tempo\nof music in all cases, including those that are not eas-\nily represented by a simple resonant model and those in\nwhich the perceptual tempo isambiguous.\nThe typical structure of a system for tempo extraction\ncan be divided into two stages: 1) a stage that generates\na representation of temporal dynamics either by extract-\ning it from audio (e.g., taking the derivative of the sig-\nnal energy in a number of frequency bands [9]) or deriv-\ning it from a symbolic representation such as MIDI; and\n2) a secondary stage that tabulates periodic regularities in\nthedrivingsignalproducedintheﬁrststage,forexample,\nthroughtheuseofresonatorﬁlterbanks[9,5],multi-agent\nmethods[4],orprobabilisticmodels[1]. Itiscommonthat\nasingletempovalueoralistofcandidatevaluesisgener-\natedfromthesetabulationsofperiodicitiestorepresentthe\ntempo of a given piece of music. However, a more inter-\nmediaterepresentation,suchasbeathistograms(see[10])\ncanbeamorevaluabledescriptionofthetempowhentry-\ning to relate it to the actual perceived tempo. Here we\nshowhowsuchrepresentationscanbeusedinconjunction\nwith perceptual data to tune systems for tempo extraction\nso that they more accurately represent perceptual tempo.\n2. METHOD\nWeperformedanexperimentinwhichlistenerswereasked\nto tap to the beat of 24 10-second musical excerpts cov-\nering a wide range of musical styles (see Appendix). We\nderived a measure of perceived tempo from the tapping\ntimes using linear regression and generated histograms\nof all subjects’ perceived tempi for each excerpt. These\nhistograms of perceived tempo served as the “group re-\nsponse” for each excerpt and were taken to represent the\noverall perceived tempo for a particular excerpt.\nAnalogsoftheperceived-tempohistogramswereauto-\nmatically generated from the audio waveforms of the ex-50 100 150 20000.20.40.60.81Mozart: K.299 (Classical), Error = 3.74\n50 100 150 20000.20.40.60.81W. Nelson: No Love At All (Country), Error = 1.82\n50 100 150 20000.20.40.60.81Fraction of Subjects (−), Model Output ( ×××)\nTempo (BPM)Scarface: Heaven (Hip Hop), Error = 2.17\n50 100 150 20000.20.40.60.81C. Parker: Au Privave (Jazz), Error = 1.84Figure 1. Perceived-tempo histograms and histograms calculated using the RFB method. Each plot shows, for a single\nmusical excerpt, the perceived-tempo histogram calculated from subjects’ tapping responses (solid line) and a tempo\nhistogramcalculatedfromtheaudiosignalusingtheRFBmethod(dottedline). Theerrorbetweenthehistograms(Eq.1)\nis shown for each plot.\ncerptsusingthreeformsofcommonbeatextractors[9,2]:\n(1)amulti-bandresonatorﬁlterbankafterScheirer[9];(2)\nanenvelopeautocorrelationmethod;and(3)aninter-onset\ninterval (IOI) histogram method. For each method, an\ninternal representation of “periodicity energy” as a func-\ntionoftempowastakentobeananalogousrepresentation\nof perceived tempo and quantitatively compared with the\nperceived-tempo histograms.\n2.1. Resonator ﬁlterbank method\nThe resonator ﬁlterbank (RFB) method of beat extraction\nwas taken from [9] and consists of two stages: 1) a set\nof driving signals are generated from the derivative of the\ntemporalenvelopeinsixfrequencybandsoftheaudiosig-\nnal; 2) each driving signal is ﬁltered by a bank of res-\nonators and then the output of like-frequency resonators\nare summed across frequency bands (See [9] for details).\nTheoutputoftheextractor’sresonatorﬁlterbankswere\nsummed and then plotted against resonant frequency to\nobtain a histogram in the same format as the perceived-\ntempo histogram. Direct comparisons of the extractor-\ngenerated histograms to the perceived-tempo histograms\nweremadeinordertoevaluatetheperformanceoftheex-\ntractor (see Results).2.2. Autocorrelation method\nFor the autocorrelation (AC) method of beat extraction\nwe used the same driving signal as in the RFB method\n(the derivative of the temporal envelope in six frequency\nbands). However, instead driving a bank of resonators,\nwesimplytooktheautocorrelationofthedrivingsignalin\neach band and then summed across bands. The autocor-\nrelation was normalized by the inverse of the lag in order\nto eliminate edge effects due a relatively short signal (10\nseconds). The lag was translated to beats-per-minute to\nallow comparison to the perceived-tempo histograms.\n2.3. IOI Histogram method\nIn the ﬁnal method of beat extraction, onsets of auditory\neventswereestimatedbythresholdingthedrivingfunction\noftheRFBmethod. Onsetsweremarkedateverypointthe\ndriving function crossed the threshold in the positive di-\nrection. Threshold values were empirically chosen. Inter-\nvals between onsets were then calculated and all-order1\ninterval histograms were generated. IOI was then trans-\nlated to beats-per-minute to allow comparison of the IOI\nhistograms to the perceived-tempo histograms.\nAll histograms were normalized to range from 0 to 1.\nThe error, E, between a perceived-tempo histogram, Hp,\n1All-order interval histograms include intervals between non-\nconsecutive onsets as well as those between consecutive onsets.50 100 150 20000.20.40.60.81Mozart: K.299 (Classical), Error = 4.27\n50 100 150 20000.20.40.60.81W. Nelson: No Love At All (Country), Error = 2.20\n50 100 150 20000.20.40.60.81Fraction of Subjects (−), Model Output ( ×××)\nTempo (BPM)Scarface: Heaven (Hip Hop), Error = 0.68\n50 100 150 20000.20.40.60.81C. Parker: Au Privave (Jazz), Error = 3.23Figure 2. Perceived-tempo histograms and histograms calculated using the AC method. Same format as Fig. 1.\nandthetempohistogramextractedautomaticallyfromthe\naudio, He, was calculated as follows:\nE=/radicaltp/radicalvertex/radicalvertex/radicalbt250/summationdisplay\nBPM =50(Hp−He)2·max(Hp,He)2(1)\nwhereboth HpandHearefunctionsoftempo( BPM).\nTheerrorisessentiallyaweightedmean-square-difference\nbetween the histograms over the range of tempi from 50\nto 250 BPM. The weighting term, max(Hp,He)2, gives\nmore weight to those parts of the histograms with large\nvalues, e.g., peaks in the histogram.\n3. RESULTS\nResultsshowthatareasonableﬁttosomeoftheperceived-\ntempohistogramscanbemade. Figure1showsfourmea-\nsured histograms along with tempo histograms calculated\nusingtheRFBmethodforthesamemusicalexcerpts. Fig-\nure2showsthesamefortheAutocorrelationmethod. Al-\nthough none of the panels show a perfect match between\nthetwotypesofhistograms,manyofthemshowthatthey\nshare similar characteristics, for example the place and\nstrength-order of peaks. The peaks of the histograms are\nimportantbecausethetheyreﬂecttempithatareperceptu-\nallysalient(intheperceptual-tempohistograms)andtempi\nat which there is high “periodicity energy” (in the calcu-\nlated histograms).\nThebestﬁtbetweenhistogramsisshowninthebottom-\nleft panel of Fig. 2, where Autocorrelation method pre-dicts the perceived-tempo histogram for an excerpt from\nHeaven.\nThe measure of error ( E) indicates goodness of ﬁt be-\ntweenthetwotypesofhistogramsanditcanbeseenfrom\nthe plots that an error of around 2 or less indicates that\nthe two primary peaks in both histograms occur at the\nsame place and have the same strength-order (right plots\nof Fig. 1; upper-right and lower-left plots of Fig. 2).\n0 5 10 15 20 25012345678\nExcerpt NumberError,  ERFBACIOI\nFigure 3. Error between perceived-tempo and calculated\ntempo histograms. The error (calculated with Eq. 1) is\nshown for each tempo calculation method (RFB: Reso-\nnant Filter Bank; AC: Autocorrelation; IOI: Inter-onset\nInterval) as a function of excerpt number.IndividualhistogramsgeneratedbytheIOImethodare\nnotshown,howeverasummaryoftheerrorsforalltempo\nextraction methods is shown in Fig. 3. On average, the\nRFB method provides the lowest error (2.33) followed by\nthe IOI method (2.75) and the AC method (2.88). There\nis, however, quite a bit of variation and for some excerpts\n(8 and 10), the AC method performs the best.\nAnalysis of excerpts whose tempi distributions are not\nwell ﬁt by the tempo extractor can provide useful cues to\nthe aspects of the signal relevant to the perceived tempo.\nWearecurrentlyanalyzingthoseexcerptsforwhichtempo\nis not easily predicted in an effort to ﬁnd characteristics\nthat may be useful for next-generation tempo trackers.\n4. CONCLUSIONS\nDistinguishingbetweennotatedandperceptualtempohas\nadvantages not only for cognitive models of tempo per-\nception, but also for the development of systems for au-\ntomatic tempo extraction. The histogram representation\nof perceived tempo serves well as a basis for the tuning\nand quantitative assessment of automatic tempo extrac-\ntors. Usingthetechniquespresentedhere,wehaveshown\nthatwecangenerateaccuraterepresentationsofperceived\ntempo, which is an important component of systems for\nmusical information retrieval.\n5. APPENDIX\nExcerpt Composer/\nNumber Performer Title Genre\n1 Fibich Poeme Classical\n2 Mendelssohn Spring Song Classical\n3 Mozart K.299 Classical\n4 Cumberland Cumberland\nHighlanders Mtn. Home Country\n5 The Soggy I Am a Man of\nBottom Boys Const. Sorrow Country\n6 W. Nelson & No Love\nW. Jennings At All Country\n7 Blue Sonix Come On Dance\n8The Youngsters Spanish Harlem Dance\n9 Large Prof. Kool Hip Hop\n10 Scarface Heaven Hip Hop\n11 U God Bizarre Hip Hop\n12 C. Parker Au Privave Jazz\n13 F. Morgan Mood Indigo Jazz\n14 Miles Davis So What Jazz\n15 F. Villalona Musica Latina Latin\n16 M. Bethania & Sonho Meu\nGal Costa Latin\n17 Bob Marley Could You Be Loved Reggae\n18 Bob Marley Jammin’ Reggae\n19 Alannah Miles Just One Kiss Rock\n20 Jimi Hendrix Hey Joe Rock\n21 Lenny Kravitz Come On And\nLove Me Rock\n22 Bill Withers Grandma’s Hands Soul\n23 C. Mayﬁeld Think Soul\n24 D. Hathaway What’s Going On? Soul\nTable 1. Sources of musical excerpts used in this study.6. REFERENCES\n[1] A.T. Cemgil, B. Kappen, P. Desain, and H. Honing.\nOn tempo tracking: Tempogram representation and\nkalman ﬁltering. Journal of New Music Research ,\n29(4):259–274, 2000.\n[2] S. Dixon, E. Pampalk, and G. Widmer. Classiﬁca-\ntion of dance music by periodicity patterns. In Pro-\nceedingsofthe4thInternationalConferenceonMu-\nsic Information Retrieval , Baltimore, MD, October\n2003. Johns Hopkins University.\n[3] P.Fraisse. Rhythmandtempo. InD.Deutsch,editor,\nThePsychologyofMusic ,pages149–180.Academic\nPress, New York, 1982.\n[4] M. Goto and Y. Muraoka. Music understanding at\nthebeatlevel: Real-timebeattrackingforaudiosig-\nnals. In D.F. Rosenthal and H.G.Okuno, editors,\nComputationalAuditorySceneAnalysis ,pages157–\n176. Lawrence Erlbaum Associates, Mahwah, New\nJersey, 1998.\n[5] Edward W. Large and Caroline Palmer. Perceiv-\ningtemporalregularityinmusic. CognitiveScience ,\n26:1–37, 2002.\n[6] Martin F. McKinney and Dirk Moelants. Deviations\nfrom the resonance theory of tempo induction. In\nR.Parncutt,A.Kessler,andF.Zimmer,editors, Con-\nferenceon Interdisciplinary Musicology ,Graz, Aus-\ntria, 2004. Dept. of Musicology,University of Graz.\n[7] Dirk Moelants. Preferred tempo reconsidered. In\nC. Stevens, D. Burnham, G. MacPherson, E. Schu-\nbert, and J. Renwick, editors, Proceedings of the\nInternational Conference on Music Perception and\nCognition , Adelaide, 2002. Causal Productions.\n[8] Richard Parncutt. A perceptual model of pulse\nsalience and metrical accent in musical rhythms.\nMusic Perception , 11(4):409–464, 1994.\n[9] E.D.Scheirer.Tempoandbeatanalysisofacoustical\nmusicalsignals. JournaloftheAcousticalSocietyof\nAmerica, 103:588–601, 1998.\n[10] George Tzanetakis and Perry Cook. Musical genre\nclassiﬁcationofaudiosignals. IEEETransactionson\nSpeech and Audio Processing , 10(5):293–302, July\n2002.\n[11] Leon van Noorden and Dirk Moelants. Resonance\nin the perception of musical pulse. Journal of New\nMusic Research , 28(1):43–66, 1999."
    },
    {
        "title": "Feature Weighting for Segmentation.",
        "author": [
            "R. Mitchell",
            "Irfan A. Essa"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1414976",
        "url": "https://doi.org/10.5281/zenodo.1414976",
        "ee": "https://zenodo.org/records/1414976/files/MitchellE04.pdf",
        "abstract": "This paper proposes the use of feature weights to reveal the hierarchical nature of music audio. Feature weighting has been exploited in machine learning, but has not been applied to music audio segmentation. We describe both a global and a local approach to automatic feature weighting. The global approach assigns a single weighting to all features in a song. The local approach uses the local separability directly. Both approaches reveal structure that is obscured by standard features, and emphasize segments of a particular size.",
        "zenodo_id": 1414976,
        "dblp_key": "conf/ismir/MitchellE04",
        "keywords": [
            "feature weights",
            "hierarchical nature",
            "machine learning",
            "music audio segmentation",
            "automatic feature weighting",
            "global approach",
            "local approach",
            "standard features",
            "structure",
            "segments"
        ],
        "content": "FEATURE WEIGHTING FOR SEGMENTATION\nR. Mitchell Parry Irfan Essa \nGeorgia Institute of Technology \nCollege of Computing / GVU Center \n \nABSTRACT \nThis paper proposes the use of feature weights to reveal \nthe hierarchical nature of music audio. Feature \nweighting has been exploited in machine learning, but \nhas not been applied to music audio segmentation. We \ndescribe both a global and a local approach to automatic \nfeature weighting. The global approach assigns a single \nweighting to all features in a song. The local approach \nuses the local separability directly. Both approaches \nreveal structure that is obscured by standard features, and emphasize segments of a particular size. \n1. INTRODUCTION \nWith vast and growing digital music libraries and \npersonal music collections, manual indexing procedures \nare becoming intractable. Automatic audio segmentation \nalgorithms such as [1], [3], and [5] provide indices and reveal structure. Hierarchical schemes, such as [6], \nrequire a variable scale of analysis. We claim that a \nscale-specific feature weighting enables this process by \nrevealing appropriately sized segments. The \nsignificance of this work is that we apply feature \nweighting to better distinguish adjacent audio segments. \nFeature weighting has been exploited in the machine \nlearning community to improve classification. For \ninstance, features may be weighted by criterion \nfunctions such as a correlation criterion or Fisher’s \ncriterion [4], which we describe briefly. Correlation \ncriteria correlate each input feature with its class label. \nOne correlation criterion that we use is given by: \n \nyyxxyy xxC\n−−−•−=\n ) () (, (1) \nwhere x is a vector of scalar samples, y is the class label \n(±1), and x and y are their means [4]. Fisher’s \ncriterion measures the discriminability between two \nclasses in one dimension and is given by: \n 2\n22\n12\n2 1 ) (\nσσ+−=xxF , (2) \nwhere ix and 2\niσ is the sample mean and variance for \nclass i, respectively [2]. 2. METHODS \nAlthough our general feature weighting method is \napplicable to any feature set, we focus on identifying \nlarge spectral changes as segment boundaries. \nTherefore, we extract principal components of the low-\nfrequency power spectrum as suggested by Foote [3].  \nOur general method is to consider every potential \ntransition (between every adjacent pair of N frames) and \nconstruct a weight vector for each. We use a criterion \nfunction to rate how well each feature discriminates between frames on either side of the boundary in a local \nanalysis window. The size of the window indicates the \nscale of analysis. \n2.1. Features \nWe partition the audio into 50 ms non-overlapping \nframes, apply a Hanning window to each frame and \ncompute the logarithm of the magnitude of its Fast \nFourier Transform. Continuing with Foote’s \nmethodology, the high-frequency portion is ignored \n(corresponding to frequencies greater than ¼ the \nsampling rate). Finally we use Principal Component \nAnalysis (PCA) to find the seven primary directions of \nvariance within the raw feature space. This preserves \nmuch of the variance in the data while reducing the \ndimensionality. Thus, we use a seven-element feature \nvector to represent each frame. \n2.2. Local Weight Vectors \nLet jiX, represent our features where i is the feature \nindex and j is the frame number. We designate \nk k kC\nk C CC W,7 ,2 ,1 ,..., ,=  as the local correlation \ncriterion weight vector for the boundary between frames \nk and k+1 using equation (1): \n ()()\nyy x xyy x xC\nki kiki ki\nki−−−•−=\n , ,, ,\n, , (3) \nwhere wnti ti ti ki X X X x+ ++=, 2, 1, , ,..., , , t = k – nw/2, y is a \nvector of nw/2 ones followed by nw/2 negative ones \n(representing samples before and after the boundary, \nrespectively), and nw is the number of frames in the \nanalysis window. Using equation (2) we designate \nk k kF\nk F FF W,7 ,2 ,1 ,...,,=  as the local Fisher’s criterion \nweight vector: Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that \ncopies are not made or distributed for profit or commercial \nadvantage and that copies bear this notice and the full citation on the first page. \n© 2004 Universitat Pompeu Fabra.   \n \n   ()\n2\n,22\n,12\n,2 ,1\n,\nk kk k\nkix xFσσ+−= , (4) \nwhere 2 , 2, 1, ,1 ,..., ,wnti ti ti k X X X x+ ++=  represents the \nsamples before the boundary, ,12 , ,2 ++=\nwnti k X x  \n,22 ,++wntiXwntiX+, ...,  represents samples after the \nboundary, kix, is the mean, and 2\n,kiσ is the variance of \nxi,k. \n2.3. Global and Local Approach \nWe consider two main approaches to using the N-1 local \nweight vectors C\nkW and F\nkW: summarizing a global \nweight vector for application to the whole song and \nsubstituting the local separability as a transition rating \nitself.  \nThe global approach produces one weight vector for \nthe whole song. One advantage of this is the potential \ngeneralization if songs from the same artist or genre \nrequire similar global weight vectors. Applying a small \nset of likely feature weights avoids the cost of \ncomputing song-specific weight vectors. This approach \nsmoothes local fluctuation and ensures consistency across the song. In addition, the newly weighted \nfeatures may be further analyzed, such as visualizing the \nself-similarity matrix. Given our N-1 weight vectors in a \nsong, we compute a simple average to generate the \nsingle global weighting. \nThe local approach uses the total separability \nbetween samples on either side of a boundary directly as \nthe transition rating. This is analogous to using the local \nweight vectors to modulate features within their local \nanalysis window for a given boundary. We use the sum \nof squared feature weights computed from equation (3) \nor sum of feature weights from equation (4) as an \nindication of the separability between samples. For instance, if all of the features easily discriminate \nbetween each side of a potential boundary we can rate \nthe boundary highly. This method avoids explicit \nfeature weighting and adapts to local changes in \nfeatures, similar to [1]. \nAs a compromise between the two approaches, we \nalso weight each local weight vector by its separability \nscore before summarization. Local weight vectors that \nprovide better separation contribute more to the global \nweight vector. \n3. RESULTS \nWe present the effect of feature weighting in general \nand results of our automatic weighting procedure. For \nanalysis, we extract the first seven principal components \nof the low-frequency power spectrum normalized to \nzero mean as raw features. This constitutes the original \nweighting inherent in the data. Additionally, we \nconstruct a second set of features normalized to zero \nmean and unit variance. These features provide an unbiased representation on which to begin our feature \nweighting. \nSelf-similarity matrices visualize the effect of our \nfeature weighting. Figure 1 depicts the song “Mr. \nJones” by the Counting Crows using raw and \nnormalized features on the full song and the first 40 \nseconds. White indicates similarity, while black \nindicates dissimilarity. Time proceeds down and to the \nright. White squares along the main diagonal reveal \nsegments.  \nThe raw features clearly separate large segments. At \nfirst glance the uniformly weighted features only \nobscure these large segments. Upon closer inspection, they reveal smaller segments. Figure 1c and 1d enlarges \nthe first 40 seconds of each similarity matrix to show \nthe effect of this feature weighting. The raw features \nprovide little insight for small segments, while the \nnormalized features, in Figure 1d, reveal this low level \nrepeating structure. Clearly, feature weighting plays a \nrole in what segments are revealed. The next step is to \nfind a feature weighting automatically. \n \nFigure 1 . Similarity matrices for “Mr. Jones” by \nCounting Crows, using the whole song with raw \nfeatures (a) and normalized f eatures (b); and first forty \nseconds with raw features (c), normalized features (d). \n3.1. Global Approach \nWe begin with the raw features shown in Figure 1c. \nAfter applying our global weighting method with a local \nanalysis window of 3.2 seconds (1.6 seconds before and \nafter a boundary), we would expect to generate an \nimage similar to Figure 1d. Figure 2 compares the result \nof the correlation criterion (2a) and Fisher’s criterion \n(2b) for generating a global weighting. Fisher’s criterion \ngenerates feature weights that correctly discern the \nsegments, while the correlation criterion appears \nmuddled by the ill-suited i nput weights. In addition, \nFisher’s self-similarity matrix better reveals the \nsegments than the normalized features from Figure 1d.   \n \n  Using the normalized features for the first 40 seconds \nof “Porcelain” by Moby, we see that different sized \nanalysis windows reveal different segment sizes. Figure \n3 shows the similarity matrix computed with an analysis \nwindow of 3.2 s (3a) and 12.8 s (3b). The shorter \nanalysis window reveals small measure-length \nsegments. The larger window reveals the transition to a \nmore complex rhythm track at 20 seconds. \n \nFigure 2 . Self-similarity matrix for features weighted \nby correlation criterion (a) and Fisher’s criterion (b). \n \nFigure 3 . Fisher criterion weighted features using \nanalysis window of 3.2 s (a) and 12.8 s (b). \nWe can use selective window sizes to reveal different \ntypes of segments. Figure 4 and 5 compares selected \nwindow sizes using Fisher’s criterion for Schumann’s \n“Kinderszenen (Scenes from Childhood) for piano, Op. 15 No. 10”. We use a window size of 1.6 seconds to \nenhance approximately note-length segments. Figure 4a \nreveals these notes as small white squares along the \nmain diagonal. This piano piece contains alternating \nsoft and loud sections. Using a window size of 12.8 \nseconds reveals these larger sections (4b). The track \nbegins with approximately one second of silence (the \nfirst square on the diagonal). The next relatively quite \nportion begins at approximately 10 seconds. The feature \nweights for Figure 4a and 4b are shown in 4c and 4d, \nrespectively. The dominant feature weight in Figure 4d \nbelongs to the first principal component, which indeed \ncaptures most of the spectral energy. \nFigure 5 shows the self-similarity matrices for the \nwhole piano piece and better depicts the alternation between quiet and loud sections in 5b. The checkerboard \npattern of patches indicates alternation between two \nstates: soft and loud. The transitions do not appear crisp \nbecause the change in loudness occurs gradually. The \nweights are shown in 5c and 5d. They closely resemble \nthose computed on the first 40 seconds shown in Figure \n5, indicating a consistency across the whole song. Anecdotally, this suggests a computational advantage in \ncomputing weights using a small fraction of the song \nand applying them to the whole song. \n \nFigure 4 . The first 40 seconds of “Kinderszenen” \nweighted using 1.6 and 12.8 second analysis window \nin (a) and (b), with weights in (c) and (d), respectively. \n \nFigure 5 . “Kinderszenen” weighted using 1.6 and 12.8 \nsecond analysis window in (a) and (b), with weights in \n(c) and (d), respectively. \n3.2. Local Approach \nOur second approach uses local  separability directly as \na transition rating. Using the raw features from Figure \n1d that are not well-matched to the scale of analysis, we \ncompare the transition rating from Foote’s segmentation \nalgorithm, the sum of the C2 correlations and the sum of \nFisher’s criterion for each potential boundary. Figure 6 shows the sum-normalized plots for the raw features in \nFigure 1c. Fisher’s criterion outperforms both of the \nother transition ratings by pinpointing segment \nboundaries with tall narrow peaks. Because Fisher’s \ncriterion is normalized by variance, feature weights do \nnot affect its measure of discriminability.  \nA transition rating based on Fisher’s criterion may be \nemployed at various scales of analysis. Figure 7 shows \nthe first 40 seconds of “Porcelain”. The 1.6 second \nwindow (top) reveals each note. The 3.2 second window \n(middle) emphasizes two-note segments and ignores the \nintervening transitions. The 12.8 second window \nidentifies the rhythm change at 21 seconds. The false peak at 3 seconds is an artifact of the analysis window \noverlapping with the start of the file.   \n \n  \n \nFigure 6 . Transition rating computed from \nsegmentation algorithm (top), total correlation \n(middle), and Fisher’s criterion (bottom). \n \nFigure 7 . Transition ratings: Fisher’s criterion on \n“Porcelain” using an analysis window of 1.6 seconds \n(top), 3.2 seconds (middle), and 12.8 seconds (bottom). \n \nFigure 8 . “Head Like a Hole” by Nine Inch Nails: (a) \nnormalized features, (b) simple average, (c) weighted \naverage favored, and (d) peaks only weighted average. \nWe also combine the two approaches by favoring \nlocal weights that provide better separation in computing the global weights. We consider weighting \neach local weight vector by its separability score and \nexcluding local weights that do not produce peaks in the \nseparability score. Figure 8 shows self-similarity \nmatrices for the original features (a), simple average (b), \nweighted average (c), and weighted average with peaks only (d). The combined approaches (c and d) provide a \nsubtle improvement in clarity. \n4. CONCLUSION AND FUTURE WORK \nThis paper proposes the use of feature weights to reveal \nthe hierarchical nature of music audio. Clearly, feature \nweighting affects the size and type of segments \nrevealed. We describe two approaches to automatic \nfeature weighting that emphasize features that separate a \nparticular segment size. Regardless of the inherent \nweighting in the data, Fisher’s criterion outperforms \nstandard self-similarity analysis or a correlation \ncriterion for global and local approaches. A \ncombination of the global and local methods provides a \ncompromise between the two. \nWe focus on segmenting audio at points of large \nspectral change. This type of segmentation is suited to \nlocating the point of introduction or exclusion of instruments in a mix because different instruments tend \nto affect different portions of the frequency spectrum. If \nthe goal is to truly distinguish different instruments in \nthe mix, Independent Component Analysis (ICA) may \nbe preferable to the PCA features used here [7]. Future \nwork may explore this possibility. \n5. REFERENCES \n[1] Chen, S. & Gopalakrishnan, P. S. ''Speaker, \nEnvironment and Channel Change Detection \nand Clustering via the Bayesian Information \nCriterion'', DARPA Speech Recognition \nWorkshop , 1998. \n[2] Duda, R., Hart, P., & Stork, D. Pattern \nClassification , John Wiley & Sons, New York, \n2nd Edition, 2001. \n[3] Foote, J. & Cooper, M. ''Media Segmentation \nusing Self-Similarity Decomposition'', \nProceedings of SPIE , 2003. \n[4] Guyon, I, and Elisseeff, A. ''An Introduction to \nVariable and Feature Selection'', JMLR , Vol. 3, \nMarch 2003. \n[5] Tzanetakis, G, and Cook, P. ''Multifeature \nAudio Segmentation for Browsing and \nAnnotation'', Proc. of WASPAA , New Paltz, \nUSA, 1999. \n[6] Slaney, M. and Ponceleon, D. ''Hierarchical \nSegmentation using Latent Semantic Indexing \nin Scale Space'', Proceedings of ICASSP , Salt \nLake City, USA, May 2001. \n[7] Smaragdis, P. ''Redundancy Reduction for \nComputational Audition, a Unifying \nApproach'', Ph.D. Dissertation, Massachusetts \nInstitute of Technology, Media Laboratory, \n2001."
    },
    {
        "title": "Optimizing Measures Of Melodic Similarity For The Exploration Of A Large Folk Song Database.",
        "author": [
            "Daniel Müllensiefen",
            "Klaus Frieler"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418031",
        "url": "https://doi.org/10.5281/zenodo.1418031",
        "ee": "https://zenodo.org/records/1418031/files/MullensiefenF04.pdf",
        "abstract": "This investigation aims at finding an optimal way of measuring the similarity of melodies. The applicability for an automated analysis and classification was tested on a folk song collection from Luxembourg that had been thoroughly analysed by an expert ethnomusicologist. Firstly a systematization of the currently available approaches to similarity measurements of melodies was done. About 50 similarity measures were implemented which differ in the way of transforming musical data and in the computational algorithms. Three listener experiments were conducted to compare the performance of the different measures to human experts’ ratings. Then an optimized model was obtained by using linear regression, which combines the output of several measures representing different musical dimensions. The performance of this optimized measure was compared with the classification work of a human ethnomusicologist on a collection of 577 Luxembourg folksongs.",
        "zenodo_id": 1418031,
        "dblp_key": "conf/ismir/MullensiefenF04",
        "keywords": [
            "investigation",
            "measuring",
            "melodies",
            "automated analysis",
            "classification",
            "expert ethnomusicologist",
            "systematization",
            "similarity measures",
            "listener experiments",
            "optimized model"
        ],
        "content": "1 OPTIMIZING MEASURES OF  MELODIC SIMILARITY \nFOR THE EXPLORATION OF  A LARGE FOLK SONG \nDATABASE \nDaniel Müllensiefen Klaus Frieler \nUniversity of Hamburg \nDepartment of Systematic Musicology  University of Hamburg \nDepartment of Systematic Musicology  \nABSTRACT \nThis investigation aims at finding an optimal way of \nmeasuring the similarity of melodies. The applicability \nfor an automated analysis and classification was tested \non a folk song collection from Luxembourg that had \nbeen thoroughly analysed by an expert \nethnomusicologist. Firstly a systematization of the \ncurrently available approaches to similarity \nmeasurements of melodies was done. About 50 \nsimilarity measures were implemented which differ in \nthe way of transforming musical data and in the \ncomputational algorithms. Three listener experiments \nwere conducted to compare the performance of the \ndifferent measures to human experts’ ratings. Then an \noptimized model was obtained by using linear regression, which combines the output of several \nmeasures representing different musical dimensions.  \nThe performance of this optimized measure was \ncompared with the classification work of a human \nethnomusicologist on a collection of 577 Luxembourg \nfolksongs. \n1. INTRODUCTION \nMelodic similarity is a very valuable concept for \nanalyzing large melody databases. Especially with \ncomprehensive folksong collections, one often wants to \nanswer questions like: Which melodies are variants of \none another? Which melodies are duplicates with minor \ndifferences only? How can melodies be grouped together \naccording to their similarity and do they reflect inherent \nrelationships? \nThe study of folk song collections along this approach \nhas a long tradition in ethnomusicology and goes far back before the computer age [1, 2, 8, 9, 10, 22, 23]. \nNow, with the computer as a convenient tool for the \ntreatment of large data collections and the recent \nadvances in the field of music information retrieval, our \ninterest was, whether a grouping of folk song melodies \naccording to their similarity could be done \nalgorithmically and to which extend the quality \napproaches that of a human expert. \nIn case the results indeed show little difference \ncompared to the work of a human expert such an \nalgorithm could become a useful tool for \nethnomusicologists for gaining an overview on the \nsimilarity relations in a large melody collection.  As reference sample we used a catalogue of 3312 \nphrases taken from 577 Luxembourg folk songs. They \nwere assembled from 5 different sources, which were \nsorted, analyzed, and partially annotated by \nethnomusicologist Damien Sagrillo [20]. His \nclassification work was carried out with great experience \nin ethnomusicological treatments of large melody \ncollections. He gives great emphasis to musically \nrelevant features and details of the melodies and phrases. \nAs we were provided with a digital copy of the melody \ncatalogue in its classified form, we were able to test the performance of our algorithmic measures against \nSagrillo’s classification. \n \nReviewing the literature on similarity measurement for \nmelodies of the last two decades the biggest concern was \nactually not the lack of measurement procedures for \nmelodic similarity but rather their abundance. Several \nvery different techniques have been proposed for \ndefining and computing melodic similarity. They all \ncover distinct aspects or elements of melodies, e.g. \nintervals, contour, rhythm, and tonality, and each with \nseveral ways of transforming the musical information \ninto numerical datasets. The basic techniques for measuring the similarity of this type of datasets are edit \ndistance, n-grams, correlation and difference \ncoefficients, and hi dden Markov models. In the literature \nthere are plenty of examples of successful applications \nof these specific similarity measures: For example \nMcNab et al. [14] and Uitdenbogerd [24] for edit \ndistance and n-grams, also Downie [4] for n-grams, \nSteinbeck [23] and Schmuckler [21] for correlation and \ndifference coefficients, O'Maidin [18] for a complex \ndifference measure and Meek & Birmingham [15] for \nHMMs.  \n \nThis study evaluates for which type of data which \nsimilarity measures are cognitively most adequate. We \nfirst conducted three listener experiments to find an optimized similarity measure out of a set of basic \ntechniques and their variants. The expert ratings \ngenerated in these experiments were compared with the \noutput of the similarity algorithms implemented in the \nsoftware toolkit SIMILE. An optimization was done \nusing a linear regression model to combine measures \nthat reflect melodic information from distinct \ndimensions.   \n \n 2Finally the model was tested on the similarity \nclassification in Sagrillo’s catalogue.  \n \n \n2. APPROACHES TO MEASURING MELODIC \nSIMILARITY \n2.1.  Mathematical Framework \nWe developed a mathematical framework in order to \nhandle the huge amount of different similarity measures \nthat can be found in the literature. This allowed us to \nsystematically classify the similarity measures in a \ncompact and unified way. It also simplified the \ncomparison of different models with one another other \nand with empirical data. Furthermore, it served as kind \nof a construction kit and as a source of inspiration for \nnew similarity measures. Finally, it was very helpful for \nimplementing the algorithms into our software. \nWe define the “melodic space” M as a subset of the \nCartesian product of a (real-valued) time coordinate (representing onsets) and a (integer- or real-valued) \npitch coordinate. \nA similarity measure is then a map \ns : M x M -> [0,1] \nwith the following properties: \n1. Symmetry: s(m,n) = s(n,m) \n2. Self identity: s(m,m) = 1 \n3. Transposition-, Translation- and Dilation \ninvariance. \n‘Transposition’ means translation in the pitch \ncoordinate, translation is time-shift and ‘dilation’ means \ntempo change (time warp). Though these properties \nhold only approximately for human similarity \njudgments, they facilitate implementation and \ncomparison. Similarity measures form a convex set, i.e. \nany linear combination of similarity measures, where the sum of coefficients equals 1, is again a similarity \nmeasure. This property enabled us to calculate \ncombined, optimized measures, by means of linear \nregression. Furthermore, any product of two similarity \nmeasure is again a similarity measure.  \nMost of the similarity measures involved the following \nprocessing stages:  \n1. Basic transformations (Representations) \n2. Main Transformations \n3. Computation \n 2.2.  Transformations \nThe most common basic transformations are projections \nand ‘differentiations’. Projections can act on either the \ntime or pitch coordinate, (with a clear preference for \npitch projections). ‘Differentiation’ means using \ndifferences between coordinates instead of absolute \ncoordinates, i.e. intervals and durations instead of pitch \nand onsets.  \nAmong the main transformations rhythmical weighting, \nfuzzifications (classifications) and contourization are \nthe most important. Rhythmical weighting can be done for quantized melodies, i.e. melodies where the \ndurations are integer multiples of a smallest time unit T. \nThen each pitch of duration nT can be substituted by a \nsequence of n equal tones with duration T. After a pitch \nprojection the weighted sequence will still reflect the \nrhythmical structure. The concept of rhythmical \nweighting has been widely used in other studies e.g. [6, \n9, 23].  \nFuzzifications are based on the notion of fuzzy sets, i.e. \nsets in which an element belongs to it with a certain \ndegree between 0 and 1. However, the fuzzifications \nreduce to classification if the basic set is decomposed \ninto mutually disjunct subsets. This was true for all our \ncases. Other studies exploited this idea in similar ways \ne.g. [19]. \nGaussification is a method to construct an integrable \nfunction from a set of discrete time-points [5]. This \nenables a comparison between two rhythms by means of \nthe scalar product of two functions. This can be viewed as a measure for rhythmic similarity.  \nContourization is based on the idea, that the \nperceptually important notes are the extrema, i.e. the \nturning points of a melody. This extremum is taken (the \nexact choice depends on the model) and the pitches in \nbetween are substituted with interpolated values, e.g., \nfrom a linear interpolation (in fact we used this \nexclusively). The idea of contourization was e,g, \nemployed in the similarity measures by Steinbeck [23] \nand Zhou & Kankanhalli [26]. \nAmong the other core transformations available are the \nranking of pitches and Fourier transformation on \ncontour information (following the approach of \nSchmuckler, [21]) or methods of assigning a ‘harmonic \nvector’ like Krumhansl’s tonality vector [11] to certain \nsubsets (bars) of a melody, just to name a few. \n2.3.  Similarity Computations \nThe next stage of processing is the computation of a \nsimilarity value. The measures we used can roughly be \nclassified in three categories: Vector measures, \nsymbolic measures and musical (mixed) measures, \ndepending on the computational algorithm. The vector \nmeasure treats the transformed melodies as vectors in a suitable real vector space such that methods like scalar \nproducts and other means of correlation can be applied   \n \n 3to. The symbolic measures on the contrary treat the \nmelodies as strings, i.e. sequences of symbols, where \nwell-known measures like edit distance (see e.g. [16]) or \nn-gram-related measures (see e.g. [4]) can be used. The \nmusical or mixed measures typically involve more or \nless specific musical knowledge and the computation \ncan be from either the vector or the symbolical realm. \nThe symbolical measures based on edit distance and n-\ngrams has proven to be the best throughout. We used \nedit distance for many different alphabets (raw pitch, \ninterpolated pitch, intervals, fuzzified intervals, fuzzified durations, implicit tonality) and we employed \nsolely global alignment and the simplest cost function.  \nWe applied three different n-gram approaches (Sum \nCommon, Coordinate Matching, Ukkonen [24]) for \ndifferent alphabets (intervals, interval categories, \ninterval directions, fuzzified rhythms) with a constant n-\ngram length of 3.  \nSome general problems had to be solved for some \nmodels to ensure transposition and tempo invariance \nand to account for melodies having different lengths \n(number of notes). If a measure is not transposition \ninvariant a priori, in principle the maximum over all \nsimilarities of all possible transpositions by an integer \nnumber of semitones within an octave can be taken, like \nO'Maidin [18] proposed. Likewise, for models, which \nrequire the melodies to be of same length, as most of the \ncorrelation measures do, we took the maximum of all similarities of sub-melodies of the longer melody with \nthe same length as the shorter one. This type of shifting \nhas been proposed for example by Leppig [13]. Tempo \ninvariance is generally no problem when using \nquantized melodies.  \nIn summary, the techniques for melodic data \ntransformation and pattern matching/similarity \nmeasurement employed in this study incorporate the \nmajor approaches in this field of the last 15 years. \nAdditionally, systemizing these approaches led to the \nconstruction of several new similarity measures (see [5, \n16] for a detailed description). We implemented in our \nsoftware a total number of 48 different similarity \nmeasures, counting all variants out of which 39 were \nused in the analysis. A complete list with short \ndescriptions of the various measures is found in the \nappendix. We used the same MIDI-files as program input that were used for the experiments. All melodies \nwere quantized.\n \n3. LISTENER EXPERIMENTS \n3.1. Experimental Design \nWe conducted three rating experiments in a test-retest-\ndesign. The subjects of the tests were musicology students with longtime practical musical experience. In \nthe first experiment the subjects had to judge 14 \nmelodies taken from western popular music to six \nsystematically derived variants of each on a 7-point scale. The second and third experiment served as \ncontrol experiments. In the second experiment two \nmelodies from the first experiment were chosen and \npresented along with the original six variants plus six or \nfive variants, which had their origin in completely \ndifferent melodies. The third experiment used the same \ndesign as the first one, but tested a different error \ndistribution for the variants and looked for the effects of \ntransposition of the variants. \nOnly subjects who showed stable and reliable \njudgments were taken into account for further analysis. From 82 participants of the first experiment 23 were \nchosen, which met two stability criteria: They rated the \nsame pairs of reference melody and variant highly \nsimilar in two consecutive weeks, and they gave very \nhigh similarity ratings to identical variants. This type of \nreliability measurement is considered an important \nmethodological improvement compared with earlier \nexperiments involving similarity ratings. For the second \nexperiment 12 out of 16 subjects stayed in the analysis. \n5 out of 10 subjects remained in the data analysis of the \nthird experiment. \nThe inter- and intrapersonal judgments of the selected \nsubjects showed very high correlations on various \nmeasures (e.g. the coefficient Cronbach’s alpha reached \nvalues of 0.962, 0.978 and 0.948 for the three experiments respectively). This supports the assumption \nthat something like a 'true' similarity’ exists, at least for \nthe group of western musical experts. This is of course a \nnecessary prerequisite for the comparison between \nautomated algorithmic and human judgments. \n3.2. Results \nTo get an overview over the performance and the \ndifferentiation of the 39 similarity measures in relation \nto the subjects’ mean ratings (vpn_mean), multi-\ndimensional scaling (MDS; for details on algorithms \nand model options see [3], [12]) was used to display the \nresults graphically. The euclidean distances between all \n39 similarity measures over all melody pairs from \nexperiment 2 (variants from original vs. from different \nmelodies) were computed. 18 measures that showed the \nleast distance to the subjects’ means and that could be \nrepresentative for all the 39 measures were selected for the MDS (13 are displayed with their names on the \nfollowing graph). With these 18 measures (and the \nmean of subjects’ ratings) an MDS model was \ncomputed that used only the ordinal information in the \ndistance data. The usual MDS criteria, RSQ (=portion \nof the variance explained) and stress, were used as \nindicators of fit (stress = 0.075, RSQ = 0.98). A two-\ndimensional solution was chosen that is represented by \nfigure 1. \nA meaningful interpretation of this solution views \ndimension 1 as the degree to which the similarity \nmeasures incorporate rhythmical information: To the \nright a measure from the n-gram approach are located that uses the fuzzified rhythm ( ngukkfr ) values as data.   \n \n 4The other two rhythmic measures rhytgaus  \n(gaussification of onsets) und rhythfuzz  (edit distance of  \nFigure 1 : Multidimensional scaling solution for \nsimilarity measures on data from experiment 2 \nfuzzified duration values) are located as well to the \nextreme right on this axis. \nDimension 2 can be interpreted as global vs. local \ninformation. Below, the n-grams measures are located that reflect only differences in short sequences of notes. Above contour and edit distance measures can be seen that give importance to the coherence of two melodies \nover their full course.  \nAs this study sets out to find an optimized measure from \nthe considered algorithms, we chose five similarity measures from the 18 measures that had entered the MDS to find an optimal weighted combination in a \nlinear regression model. The five measures represented \ndifferent information dimensions or sources according to the outcome of the MDS and they had the least distance to the subjects’ ratings compared with their neighbours from the same information dimension.  \nWe did this selection process separately for the data \nfrom experiment 1 and experiment 2. Since the task of \nthe subjects in experiment 2 was to differentiate between variants from the same melody and from variants with an origin in a different melody, this experiment came closer to the classification work done by Sagrillo. So the results \nreported in the following stem only from the data of \nexperiment 2.  \nThe best five models for experiment 2 were (ordered \naccording to their euclidean distances, minimum first): diffEd  (2.04), nGrUkkon  (2.44), harmCorE  (2.98), \nconSEd (3.57) und rhythFuzz (3.65). \n•  diffEd  (Edit distance of intervals) \n•  nGrUkkon  (Ukkonen measure from 3-\ngrams of intervals)  \n• harmCorE  (Edit distance of harmonic \nsymbols per bar, which were obtained by means of Krumhansl's tonality \nvectors)  \n• conSEd  (Edit distance for contourized \nmelodies, Steinbeck's algorithm) \n• rhythFuzz  (Edit distance of classified \nduration values) \nWith these five measures as input for a linear regression \nanalysis, we determined an optimized measure to explain the human data on the 7-point-scale of the following \nform:  \nopti3  = 3.03* ngrukkon  + 2.5* rhytfuzz  + 1.44* harmcore  \nOpti3  proved to be 33.4% better than the best single \nmeasure for experiment 2. Similarly, opti1 performed \n28.5% better than the best single measure on the data of \nexperiment 1. The superior performance of the optimized hybrid measure o pti3 (experiment 2) can be \nseen from the following diagram: \n \n \n   \n \nFigure 2 : Distance of different similarity measures to \nsubjects’ mean ratings on data from experiment 2 \nThese optimized models fit the human judgements very \nwell. For experiment 1 there was 83 % of the variance \nexplained by the combined measure opti1 , and for \nexperiment 2 ( opti3 ) even 92%. \nIt is noteworthy, that we found different optimized \nsimilarity models for the data of experiment 1 and 2. This can be explained by the fact that the rating contexts and tasks for the subjects differed. Music experts seem \nto change their judgement strategies depending on the \nkind of task and data they are given. Thus, a decision has to be taken on which type melody variants should to be matched against each other before choosing a combined similarity measure for a specific application. Once the \nsituation in which the similarity measure is supposed to \noperate is clearly defined, the measure can work quite efficiently on new data. This is shown in the following section.\n \n4.  FOLK SONG ANALYSIS \nFor the analysis of the folksong collection the measure \noptimized on the data of experiment 2 (opti 3) was taken to work on the melodies from Luxembourg without any further adjustment of parameters or exchange of \ncomponents. \nWith this similarity measure several analysis tasks were \ncarried out: 00,511,522,533,54\nsubj_mean opti3 diffed ngrukkonharmcore consed rhythfuzzDimension 13 2 1 0 -1 -2Dimension 22,0\n1,51,0\n,5\n0,0\n-,5\n-1,0-1,5\n-2,0subjmeanharmcore\nrhytfuzzrhytgaus\nngrukkfrngrukkor\nngrsumcrngrukkon\nngrsumcodiffedconed consed\nrawedw\nrawed  \n \n 5a) The description of the almost normal \ndistribution of the 171,405 similarity values between all melodies in the collection,  \nb) The analysis of ‘interesting cases’ in which the \nsimilarity values of different dimensions \n(melody, harmony, rhythm) differ significantly,  \nc) The spotting out of doublets and variants and  d) The classification of melodies into groups or \nfamilies according to Sagrillo’s catalogue.  \nOnly steps c) and d) will be covered in the two following \nsubsections. \n4.1.  Duplicates and variants \nA crucial test for any similarity measure is the task of \nidentifying identical or almost identical melodies in a database. Unfortunately, we had no complete information about identical melodies, but a suffix ‘V’ in database of the tunes indicates a variant to a specific \ntune. There were 19 of such marked songs in the \nLuxembourg database, which we inspected manually. Apart from the 5 tunes marked with a ‘V’ that had the same lyrics but a different melody, the remaining 14 melodies had similarity values of 0.6 or higher according to the opti3  measure. \nWe also examined all melody pairs in the database with \nsimilarity values above 0.6 (49 melody pairs). These pairs can be roughly classified in  \n1. ‘Duplicates’ (same or near same melody and \nsame or near same title): 37 pairs \n2. ‘Parodies’ (same or near same melody but \ndifferent title and probably different lyrics): 10 pairs \n3. ‘Psalms’: 2 pairs \nThe so-called ‘psalms’ are special types of songs which \nare typically written without meter, consist almost \ncompletely of tone repetitions and have usually small tone range.  Some songs could be found with 3 or more variants. One example is a song called ‘De Malbrough’, which can also be found in a collection from Lorraine. \nInspecting it, it turned out that it is highly similar to the \nwell-known (english) song ‘He’s a jolly good fellow’.  \n \n4.2. Algorithmic and Expert Classification \n The final task was the reconstruction of Sagrillo’s \nclassification of the 3312 phrases from the Luxembourg \nmelodies. Apart from the indication of variants (see above) Sagrillo used two hierarchical levels of similarity grouping. He firstly sorted the phrases numerically according to several gross criteria and then performed a very careful analysis ‘by hand’. We simply used the \ngrouping on one classification level as criterion of a \ngreater similarity (0=not member of the same group, 1=member of the same group). We used logistic regression to model Sagrillo’s classification with our similarity measures and the Area under Curve (Receiver \nOperating Curves) from Signal Detection theory to \nevaluate the solutions. Due to computing limitations, we worked on a sample of 52,724 melody comparisons \ncoming from 438 phrases classified by Sagrillo in 21 groups.  \nWe first tested the performance of our opti3  measure. \nBut it performed quite poorly on short phrases (usually \nonly 1-3 bars) since it had been optimized for longer melodic lines. We received an AUC value of only 0.676. So an optimization for the new empirical melodic entity of phrases seemed necessary. This was done in an \nanalogous manner to the optimization process described \nin 3.2: We calculated the AUC scores for any of the 39 similarity measures and picked the measure for every information dimension that discriminated best. The five best measures for discriminating the phrases were: \n• Pitch/interval: rawEd  (Edit distance of \nraw pitch values) \n• Contour: conSEd  (Edit Distance of \ncontourized pitch values, contourization according to Steinbeck, 1982) \n• Short motives: nGrUkkon  (Ukkonen \nmeasures for 3-grams of intervals) \n• Harmony: harmCorr  (Correlation \nmeasure for tonality values based on Krumhansl’s tonality vector)  \n• Rhythm: rhytFuzz  (Edit distance of \nclassified duration values) \nWe found an optimal model including rawEd , consEd , \nand ngrUkkon,  with rawEd  having the greatest weight in \nthe logistic regression term. This model classified 88.6% of the 52,724 phrase pairs correctly (92.4% of the non-\nclass members and 61.1% of the class members). This \nmodel showed a good overall discrimination power as can be seen by its ROC diagram and its AUC value of 0.845 which can be interpreted as ‘excellent’ according to [7].  \n \nROC Curve\n1 - Specivity1,0 ,8 ,5 ,3 0,0Sensivity1,0\n,8\n,5\n,3\n0,0\n \nFigure 4 : ROC curve of optimized measure for phrase \nclassification \nIt is possible to give more weight to the detection of \nclass-members by choosing a different cut-off value for \nthe logistic regression function (at the cost of assuming a \nhigher percentage of misclassified non-class members).   \n \n 6With a cut-off value of 0.133 we classified 72.3% of the \nclass members correctly (85.1% of the non-class members correct, 83.5% correct overall).  \nHowever, the detection of the class members is still not \nperfect but an inspection of Sagrillo’s groups showed \nthat his similarity classification is rather of a continuous nature than one of actual groups. So especially in large groups the first and the last members possess generally low similarity values in our optimized model. A more \nsophisticated approach would be to use all levels of his \nhierarchical classification or the proximity of the phrases in his ordered catalogue as dependent variable in the regression model. This is planned for the near future. \n5. SUMMARY \nAs this study has focused on the classification and \nexploration of a folksong collection, the chosen \nmethods gave satisfying and promising results with \nstrong implications. The strikingly simple idea of \nevaluating and gauging a large number of melodic \nsimilarity measures from the literature with the help of \nempirical research and mathematical systematization \nenabled us to develop a successful tool. Furthermore, \nour empirical work seems to prove that the concept of \nmelodic similarity is a stable and well-defined notion \nfor human music experts. Due to the high importance of melodic similarity in \nmany areas of music research and engineering, our tool \nis ready and waiting for many other applications, e.g., in \nQuery-by-Humming systems, as a tool for melodic \nmemory research, or as starting point for cognitive \nmodels of human melodic similarity judgments. Some \nof these, as well as refinements, optimization and \nevaluations of the tool in other domains, will be done in \nthe future. \n  \n6. REFERENCES \n[1] Bartók, B. & Lord, A.B.  Serbo-Croatian Folk \nSongs: Texts and Transcriptions of Seventy-\nFive Folk Songs from the Milman Parry \nCollection and a Morphology of Serbo-\nCroatian Folk Melodies . New York: Columbia \nUniversity Press, 1951. \n[2] Bartók, B. \"Why and How Do We Collect Folk \nMusic?\". Béla Bartók Essa ys. Ed. Benjamin \nSuchoff . London: Faber & Faber, 1976, 9-24. \n[3] Borg, I & Lingoes, J.C. Multidimensional \nsimilarity structure analysis . New York: \nSpringer, 1987. \n[4] Downie, J. S. Evaluating a Simple Approach to \nMusical Information retrieval: Conceiving \nMelodic N-grams as Text . PhD thesis, \nUniversity of Western Ontario, 1999 [5] Frieler, K. Mathematische Musikanalyse - \nTheorie und Praxis .  PhD thesis, University of \nHamburg (in preparation), 2004 \n[6] Hofmann-Engl, L. \"Rhythmic Similarity: A \ntheoretical and empirical approach\". \nProceedings of the 7th International \nConference on Music Perception and \nCognition, Sydney 2002.  Ed. C. Stevens, D. \nBurnham, G. McPherson, E. Schubert, J. \nRenwick. Adelaide, Causal Productions, 2002  \n[7] Hosmer, D. W. & Lemeshow, S. Applied \nLogistic Regression.  Wiley, New York, 2000.  \n[8] Jesser, B. Interaktive Melodieanalyse: \nMethodik und Anwendung computergestützter \nAnalyseverfahren in Musikethnologie und Volksliedforschung: typologische \nUntersuchung der Balladensamlung des DVA . \nBern: Peter Lang, 1990. \n[9] Juhasz, Z. “A Model of Variation in the Music \nof a Hungarian Ethnic Group”. Journal of New \nMusic Research , 29, No. 2, 2000, 159-172. \n[10] Kluge, R. Faktorenanalytischen \nTypenbestimmung an Volksliedmelodien . \nLeipzig: VEB Deutscher Verlag für Musik, \n1974.  \n[11] Krumhansl, C. L. Cognitive foundations of \nmusical pitch. New York: Oxford University \nPress, 1990. \n[12] Kruskal, J.B. & Wish, M. Multidimensional \nscaling . Beverly Hills: Sage, 1978. \n[13] Leppig, M. “Musikuntersuchungen in \nRechenautomaten“. Musica  41/2, 1987, p140-\n150. \n[14] McNab, R. J., Smit h, L.A., Witten, I.H., \nHenderson, C.L. & Cunningham, S.J.  \n“Towards the Digital Music Library: Tune \nretrieval from Acoustic Input”. Proceedings \nACM Digital Libraries , 1996. \n[15] Meek, C. & Birmingham, W. \"Johnny Can't \nSing: A Comprehensive Error Model for Sung \nMusic Queries.\" ISMIR 2002 Conference \nProceedings , IRCAM, 2002, p124-132. \n[16] Mongeau, M. & Sankoff, D. “Comparision of \nMusical Sequences”. Computers and the \nHumanities 24, 1990, p161-175. \n[17] Müllensiefen, D. Variabilität und Konstanz \nvon Melodien in der Erinnerung . PhD thesis, \nUniversity of Hamburg (in preparation), 2004 \n[18] O`Maidin, D. \"A Geometrical Algorithm for \nMelodic Difference in Melodic Similarity\". Melodic Similarity: Concepts, Procedures, and \nApplications. Computing in Musicology 11 . Ed.   \n \n 7Walter B. Hewlett & Eleanor Selfridge-Field. \nCambridge: MIT Press, 1998 \n[19] Pauws, S. \"Cuby hum: A Fully Operational \nQuery by Humming System\". ISMIR 2002 \nConference Proceedings , IRCAM, 2002, p187-\n196. \n[20] Sagrillo, D. Melodiegestalten im \nluxemburgischen Volkslied: Zur Anwendung \ncomputergestützter Verfahren bei der \nKlassifikation von Volksliedabschnitten.  Holos, \nBonn, 1999. \n[21] Schmuckler, M. A, “Testing Models of \nMelodic Contour Similarity.\" Music Perception  \nVol. 16, No. 3, 1999, p109-150. \n[22] Seeger, Ch. “Versions and Variants of the \nTunes of ‘Barbara Allen’”. Selected reports in \nethnomusicology  Vol.I, No. 1, 1966. \n[23] Steinbeck, W. Struktur und Ähnlichkeit: \nMethoden automatisierter Melodieanalyse. Kieler Schriften zur Musikwissenschaft XXV . \nKassel, Basel, London: Bärenreiter, 1982 \n[24] Uitdenbogerd, A. L.  Music Information \nRetrieval Technology.  PhD thesis, RMIT \nUniversity Melbourne Victoria, Australia, 2002 \n[25] Zadeh, L. \"Fuzzy sets\". Inf. Control , 1965, \np338-353. \n[26] Zhou, Y. & Kankanhalli, M. S. \"Melody \nalignment and Similarity Metric for Content-\nBased Music Retrieval\". Proceedings of SPIE-\nIS&T Electronic Imaging.  SPIE Vol. 5021, \n2003, p112-121. \n7. APPENDIX: TABLE OF EMPLOYED \nSIMILARITY MEASURES  \n \nAbbreviation Model \n  \nRAWED Raw pitch edit distance  \nRAWEDW Raw pitch edit distance, weighted  \nRAWPCST Raw pitch P-B. corr, 0-1 \nRAWPCWST Raw pitch P-B. Corr., weighted, 0-1 \nCONSED Contour (Steinbeck) edit distance \nCONSPCST  Contour (Steinbeck), P-B. corr., 0-1  \nCONED  Contour edit distance weighted  \nCONPCST Contour, P-B. corr., 0-1 \nFOURRST Fourier (ranks), weighted, 0-1  \nFOURRWST Fourier (ranks), weighted, 0-1  \nFOURRI Fourier (ranks, intervals) \nDIFFED Intervals (Edit distance)  \nDIFF Intervals (Mean difference)  \nDIFFEXP Intervals (Mean difference,  exp.) \nDIFFFUZ Intervals (fuzzy), Edit Distance  \nDIFFFUZC Intervals (fuzzy contour) NGRSUMCO n-grams  Sum Common  \nNGRUKKON n-grams  Ukkonnen  \nNGRCOORD Coordinate Matching (count dictinct) \nNGRSUMCR Sum Common  (interval direction)  \nNGRUKKOR n-grams  Ukkonnen (interval dir.)  \nNGRCOORR n-grams Coord. Match.  (interval dir.) \nNGRSUMCF n-grams  Sum Common  (fuzzy)  \nNGRUKKOF n-grams  Ukkonnen   (fuzzy)  \nNGRCOORF n-grams  Count distinct   (fuzzy)  \nNGRSUMFR n-grams  sum common (fuzzy \nrhythm)  \nNGRUKKFR n-grams  Ukkonnen  (fuzzy rhythm)  \nNGRCOOFR n-grams Coord. Match. (fuzzy \nrhythm) \nRHYTGAUS Rhythm (gaussified onset points)  \nRHYTFUZZ Rhythm (fuzzy), edit distance  \nHARMCORR Harmonic correlation (type I)  \nHARMCORK Harmonic correlation (type II)  \nHARMCORE Harmonic correlation (Edit distance)  \nHARMCORC Harmonic correlation (circle)"
    },
    {
        "title": "Towards an Efficient Algorithm for Automatic Score-to-Audio Synchronization.",
        "author": [
            "Meinard Müller",
            "Frank Kurth",
            "Tido Röder"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416302",
        "url": "https://doi.org/10.5281/zenodo.1416302",
        "ee": "https://zenodo.org/records/1416302/files/MullerKR04.pdf",
        "abstract": "In the last few years, several algorithms for the automatic alignment of audio and score data corresponding to the same piece of music have been proposed. Among the ma- jor drawbacks to these approaches are the long running times as well as the large memory requirements. In this paper we present an algorithm, which solves the synchro- nization problem accurately and efficiently for complex, polyphonic piano music. In a first step, we extract from the audio data stream a set of highly expressive features encoding note onset candidates separately for all pitches. This makes computations efficient since only a small num- ber of such features is sufficient to solve the synchroniza- tion task. Based on a suitable matching model, the best match between the score and the feature parameters is computed by dynamic programming (DP). To further cut down the computational cost in the synchronization pro- cess, we introduce the concept of anchor matches, matches which can be easily established. Then the DP-based tech- nique is locally applied between adjacent anchor matches. Evaluation results have been obtained on complex poly- phonic piano pieces including Chopin’s Etudes Op. 10.",
        "zenodo_id": 1416302,
        "dblp_key": "conf/ismir/MullerKR04",
        "keywords": [
            "audio and score data alignment",
            "polyphonic piano music",
            "highly expressive features",
            "note onset candidates",
            "dynamic programming",
            "anchor matches",
            "complex polyphonic piano pieces",
            "Chopin’s Etudes Op. 10",
            "evaluation results",
            "computational cost reduction"
        ],
        "content": "TOWARDSAN EFFICIENT ALGORITHM FOR AUTOMATIC\nSCORE-TO-AUDIOSYNCHRONIZATION\nMeinardM ¨uller,FrankKurth,TidoR ¨oder\nUniversit¨at Bonn, Institut f ¨ur Informatik III\nR¨omerstr. 164, D-53117 Bonn, Germany\nfmeinard, kurth, roedert g@cs.uni-bonn.de\nABSTRACT\nIn the last few years, several algorithms for the automatic\nalignment of audio and score data corresponding to the\nsamepieceofmusichavebeenproposed. Amongthema-\njor drawbacks to these approaches are the long running\ntimes as well as the large memory requirements. In this\npaper we present an algorithm, which solves the synchro-\nnization problem accurately and efﬁciently for complex,\npolyphonic piano music. In a ﬁrst step, we extract from\nthe audio data stream a set of highly expressive features\nencoding note onset candidates separately for all pitches.\nThismakescomputationsefﬁcientsinceonlyasmallnum-\nber of such features is sufﬁcient to solve the synchroniza-\ntion task. Based on a suitable matching model, the best\nmatch between the score and the feature parameters is\ncomputed by dynamic programming (DP). To further cut\ndown the computational cost in the synchronization pro-\ncess,weintroducetheconceptofanchormatches,matches\nwhich can be easily established. Then the DP-based tech-\nniqueislocallyappliedbetweenadjacentanchormatches.\nEvaluation results have been obtained on complex poly-\nphonic piano pieces including Chopin’sEtudes Op. 10.\n1. INTRODUCTION\nModern digital music libraries consist of large document\ncollections containing music data of diverse characteris-\ntics and formats. For a single piece of music, the library\nmaycontainthemusicalscore,severalcompactdiscrecor-\ndings of a performance, and various MIDI ﬁles. Inhomo-\ngeneity and complexity of such music data make content-\nbased browsing and retrieval in digital music libraries a\ndifﬁcult task with many yet unsolved problems. Here,\nsynchronization algorithms which automatically link data\nstreams of different data formats representing a similar\nkindofinformationareofgreatimportance. Inthispaper,\nwe consider the fundamental case that one data stream,\ngiven as a MIDI ﬁle, represents the score of a piece of\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopiesbear this notice and the full citation on the ﬁrst page.\nc°2004UniversitatPompeu Fabra.musicandtheotherdatastream,givenasaWAVﬁle,rep-\nresents a recorded performance of the same piece of mu-\nsic. The latter data is also simply referred to as audio.\nThesynchronization task then amounts to associating the\nnote events given by the score data stream with their oc-\ncurrences in the audio ﬁle.\nScore and audio data fundamentally differ in their re-\nspectivestructureandcontent,makingscore-to-audiosyn-\nchronization a difﬁcult task. On the one hand, the score\nconsists of note parameters such as pitch, onset times, or\nnote durations, leaving a lot of space for various interpre-\ntations concerning, e.g., the tempo, dynamics, or multi-\nnote executions such as trills. On the other hand, the\nwaveform-based CD recording of some performance en-\ncodes all the information needed to reproduce the acous-\ntic realization — note parameters, however, are not given\nexplicitly. Therefore, all present approaches to score-to-\naudio synchronization (see Section 2) proceed in two sta-\nges: In the ﬁrst stage, suitable parameters are extracted\nfrom the score and audio data streams making them com-\nparable. Inthesecondstage,anoptimalalignmentiscom-\nputed by means of dynamic programming (DP) based on\na suitable local distance measure.\nTheapproachdiscussedinthispaperalsofollowsalong\ntheselines. However,weputspecialemphasisontheefﬁ-\nciency of the involved algorithms — concerning running\ntime as well as memory requirements. In contrast to pre-\nviousapproaches,weuseasparsesetofhighlyexpressive\nfeatures, which can be efﬁciently extracted from the au-\ndio data stream. Due to its expressiveness, this feature set\nallowsforanaccuratesynchronizationwithhightimeres-\nolution(around20ms). Duetoitssparseness,itfacilitates\na time and memory efﬁcient alignment procedure (based\non0to20featurevectorsperseconddependingonthere-\nspective segment of the piece of music). In our research\nwe have concentrated on polyphonic piano music of ar-\nbitrary genre and complexity. This allows us to exploit\ncertaincharacteristicsofthepianosoundtobeusedinthe\nfeatureextraction. Ourunderlyingconcept,however,may\nbetransferredtoothermusicaswellbymodifyingthefea-\nture set.\nInthesecondstage,weusedynamicprogramming(DP)\nto compute the actual score-to-audio alignment. Our sug-\ngested matching model differs from the the classical con-\ncept of dynamic time warping (DTW) employed in thesynchronization algorithms suggested in [6, 7]. Since we\nprefer to have missing matches over having bad or wrong\nmatches, we do not force the alignment of all score notes\nbut rather allow note objects to remain unmatched. Fur-\nthermore, we present efﬁciently computable local score\nfunctions which relate the audio features to the note pa-\nrameters of the score data. Here we are led by the fol-\nlowing simple but far-reaching principle: The score data\nwill guide us in what to look for in the audio data stream.\nIn other words, all information contained in the extracted\naudio features, which is not reﬂected by the score data,\nremains unconsidered by the local score function.\nAs for classical DTW, the running time as well as the\nmemoryrequirementsinthesecondstageareproportional\nto the product of the lengths of the two sequences to be\naligned. In view of efﬁciency it is therefore important to\nhave sparse feature sets leading to short sequences. The\nsynchronizationalgorithmcanbeacceleratedconsiderably\nif one knows matches prior to the actual DP computation.\nTo account for such kind of prior knowledge, we intro-\nduce the notion of anchor conﬁgurations which may be\nthoughtofasnoteobjectshavingsomesalientdynamicor\nspectral properties, e.g., some isolated fortissimo chord\nwith some salient harmonic structure or some long pause.\nThe counterparts of such note objects in the audio data\nstreams can be determined by a linear-time linear-space\nalgorithmwhichefﬁcientlyprovidesuswithso-calledan-\nchor matches. The remaining matches can then be com-\nputed by much shorter, local DP computations between\nthese anchor matches.\nThe rest of this paper is organized as follows. After\na brief overview of related approaches in Section 2, we\ndescribe the two stages: the feature extraction in Sec-\ntion 3 and the alignment procedure in Section 4. Then,\nin Section 5, we describe how to improve the efﬁciency\nof our synchronization by introducing the concept of an-\nchor matches. The synchronization results as well as the\nrunningtimebehaviorofouralgorithmforcomplexpoly-\nphonicpianopiecesincludingChopin’sEtudesOp.10are\npresentedinSection6. Section7concludesthepaperwith\na summary and perspectiveson future work.\n2. RELATEDWORK\nThere are several problems in computational musicology\nwhich are related to the synchronization problem such as\nautomatic score following, automatic music accompani-\nment, performance segmentation or music transcription.\nDuetospacelimitationsthereaderisreferredto[1,6]for\na discussion and links to the literature. There, one also\nﬁnds a description of other conceivable applications of\nscore-to-audio alignment. We now summarize recent ap-\nproachesfromtherelativelynewﬁeldofautomaticscore-\nto-audio synchronization as described in [1, 6, 7].\nAll three approaches proceed in the two stages men-\ntionedabove. Turetskyetal.[7]ﬁrstconvertthescoredata\n(given in MIDI format) into an audio data stream using a\nsynthesizer. Then, the two audio data streams are ana-lyzed by means of a short-time Fourier transform (STFT)\nwhich in turn yields a sequence of suitable feature vec-\ntors. Basedonanadequatelocaldistancemeasurepermit-\ntingpairwisecomparisonofthesefeaturevectors,thebest\nalignment is derivedby means of DTW.\nThe approach of Soulez et al. [6] is similar to [7] with\none fundamental difference: In [7], the score data is ﬁrst\nconvertedintothemuchmorecomplexaudioformat—in\nthe actual synchronization step the explicit knowledge of\nnote parameters is not used. Contrary, Soulez et al. [6]\nexplicitly use note parameters such as onset times and\npitches to generate a sequence of attack, sustain and si-\nlence models which are used in the synchronization pro-\ncess. This results in a more robust algorithm with respect\nto local time deviationsand small spectral variations.\nSince the STFT is used for the analysis of the audio\ndata stream, both approaches have the following draw-\nbacks: Firstly, the STFT computes spectral coefﬁcients\nwhich are linearlyspread over the spectrum resulting in a\nbad low-frequency resolution. Therefore, one has to rely\non the harmonics in the case of low notes. This is prob-\nlematic in polyphonic music where harmonics and funda-\nmental frequencies of different notes often coincide. Sec-\nondly, in order to obtain a sufﬁcient time resolution one\nhas to work with a relatively large number of feature vec-\ntors on the audio side. (For example, even with a rough\ntime resolution of 46ms as suggested in [7] more than\n20feature vectors per second are required.) This leads to\nhuge memory requirements as well as long running times\nin the DTW computation.\nIntheapproachofAriﬁetal.[1],noteparameterssuch\nas onset times and pitches are extracted from the audio\ndata stream (piano music). The alignment process is then\nperformedinthescore-likedomainbymeansofasuitably\ndesigned cost measure on the note level. Due to the ex-\npressivenessofsuchnoteparametersonlyasmallnumber\nof features is sufﬁcient to solve the synchronization task,\nallowing for a more efﬁcient alignment. One major draw-\nback of this approach is that the extraction of score-like\nnote parameters from the audio data — a kind of music\ntranscription—constitutesadifﬁcultandtime-consuming\nproblem,possiblyleadingtomanyfaultilyextractedaudio\nfeatures. Thismakesthesubsequentalignmentstepadel-\nicate task.\n3. COMPUTATIONOF SPARSEFEATURESETS\nBefore we describe the ﬁrst stage, the extraction step, it\nis helpful to recall some facts concerning the music to be\naligned. Asmentionedbefore,weconsiderpolyphonicpi-\nano music of any genre and complexity. This allows us to\nexploit certain characteristics of the piano sound. How-\never, dealing with piano music is still a difﬁcult task due\nto the followingfacts(see, e.g., [2, 3] for more details):\n²Strikingasinglepianokeyalreadygeneratesacom-\nplex sound consisting not only of the fundamental\npitchandseveralharmonicsbutalsocomprisingin-harmonicities caused by the keystroke (mechanical\nnoise) as well as transient and resonance effects.\n²Especially due to the usage of the right (sustain-\ning) pedal, the note lengths in piano performances\nmaydifferconsiderablyfromthenotelengthsspec-\niﬁed by the score. This results in complex sounds\nin polyphonic music which are not reﬂected by the\nscore. Furthermore, pedaling also has a great effect\non the timbre (sound spectrum) of a piano sound.\n²The piano has a large pitch range as well as dy-\nnamic range. The respective sound spectra are not\njusttranslated,scaled,orampliﬁedversionsofeach\nother but differ fundamentally in their respective\nstructure depending on pitch and velocity.\nTo make the alignment robust under such spectral, dy-\nnamicandtemporalvariations,weonlyconsiderpitchand\nonsetinformationforouraudiofeatures. Theextractionof\nsuchfeaturesisbasedonthefollowingfact: Strikingapi-\nanokeyresultsinasuddenenergyincrease(attackphase).\nThisenergyincreasemaynotbesigniﬁcantrelativetothe\nentire energy — in particular if the keystroke is soft and\nthe generated sound is masked by the remaining signal.\nHowever,theenergyincreaserelativetothespectralbands\ncorresponding to the fundamental pitch and harmonics of\nthe respective key may still be substantial. This observa-\ntionsuggeststhefollowinggeneralfeatureextractionpro-\ncedure (cf. [8], for a similar approach):\n²Firstdecomposetheaudiosignalintospectralbands\ncorresponding to the fundamental pitches and har-\nmonics of all possible piano keys.\n²Thencomputethepositionsofsigniﬁcantenergyin-\ncreases for each band. These positions constitute\ncandidates for note onsets.\nNote that, opposed to the approach in [1], we do not try\ntoextractfurthernoteparametersfromtheaudioﬁle. The\nalignment will purely be based on these onset candidates.\nWe now describe our feature extraction in detail. For\nconvenience,weidentifythenotesA0toC8ofastandard\npiano with the MIDI pitches p= 21top= 108. For ex-\nample, we speak of the note A4 (frequency 440Hz) and\nsimply write p= 69. Besides the fundamental pitch of\na note p, we also consider the ﬁrst two harmonics, which\ncanbeapproximatedbythepitches p+12andp+19. The\ngeneralization of our concepts to a constant or variable,\nnote-dependent number of higher harmonics is straight-\nforward(cf. Section 4.2).\n3.1. SubbandDecomposition\nIndecomposing theaudio signalwe usea ﬁlterbank con-\nsisting of 88bands corresponding to the piano notes p=\n21top= 108. Sinceagoodsignalanalysisisthebasisfor\nour further procedure, the imposed ﬁlter requirements are\nstringent: To properly separate adjacent notes, the pass-\nbands of the ﬁlters should be narrow, the cutoffs should\n00.10.20.30.40.50.60.70.80.9−70−60−50−40−30−20−10010\nNormalized Frequency  ( ×π rad/sample)dB \n60      70                80                      88      −      92 Figure 1. Magnitude responses for the elliptic ﬁlters cor-\nresponding to the MIDI notes 60, 70, 80, and 88to92\n(sampling rate 4410Hz).\nbesharp,andtherejectioninthestopbandshouldbehigh.\nIn addition, the ﬁlter orders should be small to allow for\nefﬁcient computation. In order to design a set of ﬁlters\nsatisfying these requirements for all MIDI notes in ques-\ntion, we work with three different sampling rates: 22050\nHz for high frequencies ( p= 93; : : : ; 108),4410Hz for\nmediumfrequencies( p= 57; : : : ; 92),and 882Hzforlow\nfrequencies ( p= 21; : : : ; 56). Each ﬁlter is implemented\nusinganeighth-orderellipticﬁlterwith 1dBpassbandrip-\nple and 50dB rejection in the stopband. To separate the\nnotesweusea Qfactor(ratioofcenterfrequencytoband-\nwidth) of Q= 25and a transition band half the width of\nthe passband. Figure 1 shows the magnitude response of\nsome of these ﬁlters.\nElliptic ﬁlters have excellent cutoff properties as well\nas low ﬁlter orders. However, these properties are at the\nexpenseoflargephasedistortionsandgroupdelays. Since\ninourofﬂinescenariotheaudiosignalsareentirelyknown\nprior to computations, one can apply the following trick:\nAfter ﬁltering in the forward direction, the ﬁltered signal\nis reversed and run back through the ﬁlter. The result-\ning output signal has precisely zero phase distortion and\na magnitude modiﬁed by the square of the ﬁlter’s magni-\ntude response. Further details may be found in standard\ntextbooks on digital signal processing such as [5].\nWe have found this ﬁlter bank to be robust enough to\nworkforareasonablytunedpiano. Forout-of-tunepianos\none may easily adjust the center frequencies and band-\nwidths as suggested in [8].\n3.2. Onset Detection\nAfterﬁlteringtheaudiosignal,wecomputetheshort-time\nroot-mean-square(STRMS)powerforeachofthe 88sub-\nbands. To this end, we convolve each squared subband\nsignal with a Hann window of suitable length. In our\nexperiments, we picked the three different window sizes\nof101,41, and 21samples depending on the sampling\nrates 22050,4410, and 882Hz, respectively. The result-\ning curves are further lowpass-ﬁltered and downsampled\nby factors 50,10, and 10, respectively. Finally, the ﬁrst-\norder difference function is calculated and half-wave rec-\ntiﬁed (i.e., taking only the positive part of the function).\n\nFigure 2 . First four measures of Op. 100, No. 2 by\nFriedrich Burgm ¨uller.\n40000 80000−0.2−0.100.10.2\n(a)\n8000 16000−0.0500.05\n(b)\n800 16000123x 10−3\n(c)\n 1 2  3  4\n 5 6 7\n 8 910\n11\nFigure 3. (a) Audio signal of a performance of the score\nshown in Figure 2. (b) Filtererd audio signal w.r.t. to the\npitch p= 72. (c) Onset signal OS72with detected peaks\nindicated by circles.\nAltogether,weobtainforeachnote parectiﬁeddifference\nsignal, also denoted as onset signal and written as OSp,\nwhich expresses the local energy increase in the subband\ncorresponding to pitch p. The time resolution depends on\nthe sampling rate. In our case, each sample of the onset\ncurvecorrespondsto 50=22050 = 2 :3ms,10=4410 = 2 :3\nms, and 10=882 = 11 :3ms, respectively. As an illustra-\ntion, Figure 3 shows in (a) the waveform of some audio\nsignalrepresentingaperformanceofthescoredepictedin\nFigure2. Theﬁlteredaudiosignalwithrespecttothepitch\np= 72(C5,523Hz) is shown in (b) and the correspond-\ning onset curve OS72in (c).\n3.3. PeakPicking\nThe local maxima or peaksof the onset signal OSpindi-\ncate the positions of locally maximal energy increases in\nthe respective band. Such peaks are good candidates for\nonsetsofpianonotesofpitches p,p¡12,orp¡19. (Re-\ncall that besides the fundamental pitch we consider two\nharmonics.) In theory, this sounds easy. In practice and\nfor complex piano pieces, however, one has to cope with\n“bad”peaksnotgeneratedbyonsets: Resonanceand beat\neffects (caused by the interaction of strings) often lead\nto additional peaks in the onset signals. Furthermore, a\nstrongly played piano note may generate peaks in sub-\nbands that do not correspond to harmonics (e.g., caused\nbymechanicalnoise). Thedistinctionofsuch“bad”peaks\nand peaks coming from onsets is frequently impossible\nandthepeakpickingstrategybecomesadelicateproblem.Since in general the “bad” peaks are less signiﬁcant then\nthe “good” ones, we use local thresholds (local averages)\nto discard the peaks belowthese thresholds.\nIn (c) of Figure 3 the peaks of the onset signal OS72\nare indicated by a number, circles indicating which peaks\nwere chosen by the peak picking strategy. Peak 7and10\ncorrespond to the note C5 ( p= 72) played in the right\nhand. Peaks 2,3,4,8, and 11correspond to the ﬁrst har-\nmonics of the note C4 ( p= 60) played in the left hand.\nIt can be seen that the ﬁrst harmonics of the ﬁrst and ﬁfth\nC4 in the left hand caused the two peaks 1and5, which\nwererejectedbyourlocalthresholdconstraints. Thisalso\nholds for the “bad” peaks 6and9.\nAfter a suitable conversion, we obtain a list of peaks\nfor each piano note p. Each peak is speciﬁed by a triple\n(p; t; s )where pdenotes the pitch corresponding to the\nsubband, tthe time position in the audio ﬁle, and sthe\nsize expressing the signiﬁcance of the peak (or velocity\nofthenote). Forcomputingthescore-to-audioalignment,\nonly these peak sequences are required — the audio ﬁle\nas well as the subbands are no longer needed. This con-\nsiderably reduces the amount of data (e.g., a mono audio\nsignal of sampling rate 22050Hz requires 30–50times\nmore memory than the corresponding peaks).\n4. SYNCHRONIZATIONALGORITHM\nAs a preparation for the actual synchronization step, we\ndivide the notes of the score into score bins , where each\nscore bin consists of a set of notes with the same musical\nonsettime. Forexample,forthescoreinFigure2theﬁrst\nscore bin is S1:=f48;52;55gcontaining the ﬁrst three\nnotes, and so on. Similarly, we divide up the peak lists\nintopeak bins . To this end, we evenly split up the time\naxis into segments of length 50ms. Then we deﬁne peak\nbinsbyassigningeachpeaktothesegmentcorresponding\nto its time position. Finally, we discard all empty peak\nbins. Altogether we obtain a list S= (S1; S2; : : : ; S n)of\nscore bins and a list P= (P1; P2; : : : ; P m)of peak bins\nwhere nandmdenotetherespectivenumberofbins. The\ndivision into peak bins seems to introduce a time resolu-\ntionof 50ms. AswewillseeinSection4.3,thisisnotthe\ncasesincewefurtherprocesstheindividualnotesafterthe\nbin matching procedure.\n4.1. Matching Model\nThenextstepofthesynchronizationalgorithmistomatch\nthesequences Sofscorebinsandthesequence Pofpeak\nbins. Beforedoingso,wehavetospecifyasuitablematch-\ning model.\nDue to note ambiguities in the score such as trills or\narpeggiosaswellasduetomissingandwrongnotesinthe\nperformance, not every note object of the score needs to\nhave a realization in the audio recording. There also may\nbe “bad” peaks extracted from the audio ﬁle. Therefore,\nas opposed to classical DTW, we do not want to force ev-\nerynotebintobematchedwithapeakbinandviceversa.As in our alignment we only consider note onsets, where\nanotegivenbythescoreisassociatedwiththeonsettime\nofthecorrespondingphysicalrealization,eachnoteofthe\nscore should be aligned with at most one time position in\nthe audio data stream. Furthermore, notes with the dif-\nferent musical onset times should be assigned to different\nphysical onset times. These requirements lead to the fol-\nlowingformal notion of a match:\nDeﬁnition: Amatchbetween SandPas deﬁned above\nis a partial map ¹: [1 : n]![1 :m]that is strictly\nmonotonously increasing.\nThefactthatobjectsin SorPmaynothaveacounter-\npart in the other data stream is modeled by deﬁning ¹as\na partial function and not as a total one. The monotony of\n¹reﬂects the requirement of faithful timing: if a note bin\ninSprecedes a second one this should also hold for the\n¹-images of these bins. ¹being a function and strictness\nof¹ensures that each note bin is assigned to at most one\npeak bin and vice versa.\n4.2. ScoreMeasures\nIngeneraltherearemanypossiblematchesbetween Sand\nP. Tocomputethe“best”matchweneedsomemeasureto\nassignaqualitytoamatch. SimilartoDTW,weintroduce\na local score dmeasuring the similarity between a note\nbinSiandapeakbin Pj. Therearemanypossibilitiesfor\nadequate score functions depending upon which aspects\nof the match are to be emphasized. Recall that the note\nbinSiis the set of notes of the same musical onset time,\nwhere each note is given by its pitch p. The peak bin Pj\nconsistsofpeakscorrespondingtoitstimesegment. Each\npeak is speciﬁed by a triple (q; t; s ), where qdenotes the\npitch of the corresponding subband, tthe time position,\nandsthe size of the peak. Then we deﬁne the local score\nd(i; j) :=d(Si; Pj)by\nd(i; j) :=X\np2SiX\n(q;t;s)2Pj(±p;q+±p+12;q+±p+19;q)¢s;\nwhere ±a;bequals one if a=band zero if a6=bfor any\ntwo integers aandb. Note that the sum ±p;q+±p+12;q+\n±p+19;qis either one or zero. It is one if and only if the\npeak (q; t; s )appears in a subband pertaining to the fun-\ndamental pitch or either one of the harmonics of the note\np. In this case, the peak (q; t; s )contributes to the score\nd(i; j)according to its size s. In other words, the local\nscore d(i; j)is high if there are many signiﬁcant peaks in\nPjpertaining to notes of Si. Note that the peaks not cor-\nrespondingtoscorenotesareleftunconsideredby d(i; j),\ni.e., the score data indicates which kind of information\nto look for in the audio signal. This principle makes the\nscorefunctionrobustagainstadditionalorerroneousnotes\nin the performance as well as “bad” peaks. Since the note\nandpeakbinstypicallycontainonlyveryfew(around 1to\n10) elements, d(i; j)can be computed efﬁciently.\nFinally, we want to indicate how to modify the deﬁ-\nnition of dto obtain other local score functions. In anobvious way, one can account for a different number of\nharmonics. Moreover, one can introduce note-dependent\nweights to favor certain harmonics over others. For ex-\nample, the fundamental pitch dominates the piano sound\nspectrum over most of its range, except for the lower two\noctaves where most of the energy is in the ﬁrst or even\nsecondharmonics. Thissuggeststofavorthefundamental\npitchfortheuppernotesandtheﬁrstorsecondharmonics\nforthelowerones. Omittingthefactor sintheabovedeﬁ-\nnitionof d(i; j)leadstoalocalscorefunctionwhich,intu-\nitively spoken, is invariant under dynamics, i.e., strongly\nplayed notes and softly played notes are treated equally.\n4.3. Dynamic Programmingand Synchronization\nBased on the local score function d, the global score of a\nmatch ¹between SandPis givenby the sum\nP\n(i;j):j=¹(i)d(i; j):\nTo compute the score-maximizing match between Sand\nP, we use dynamic programming (DP). To this end, we\nrecursivelydeﬁne the global score matrix D= (Di;j)by\nDi;j:= max fDi;j¡1; Di¡1;j; Di¡1;j¡1+d(i; j)g\nandD0;0:=Di;0:=D0;j:= 0for1·i·nand\n1·j·m. Then the score-maximizing match can be\nconstructed from Dby the followingprocedure:\ni:=n,j:=m,¹defined on ;\nwhile (i >0)and (j >0)do\nifD(i; j) =D(i; j¡1)then j:=j¡1\nelse if D(i; j) =D(i¡1; j)then i:=i¡1\nelse ¹(i) :=j,i:=i¡1,j:=j¡1\nreturn ¹\nNotethatthisprocedureindeeddeﬁnesamatch ¹inthe\nsenseofourmatchingmodeldeﬁnedinSection4.1. After\nmatchingthenotebinswiththepeakbins,weindividually\nalign the notes of Sito time positions in the audio ﬁle,\nimproving the time resolution of 50ms imposed by the\npeak bins: For a note p2Si, consider the subset of all\npeaks (q; t; s )2P¹(i)withq=p,q=p+ 12, orq=\np+19. Ifthissubsetisempty,thenote pisleftunmatched.\nOtherwise, assign the note p2Sito the time position t\nbelonging to the peak (q; t; s )of maximal size swithin\nthis subset. The ﬁnal assignment of the individual notes\nconstitutes the synchronization result.\nAsanexample,Figure4illustratesthesynchronization\nresult of the score data shown in Figure 2 and the audio\ndata shown in part (a) of Figure 3. Observe that notes\nwith the same musical onset time may be aligned to dis-\ntinct physical onset times. (This takes into account that a\npianist may play some notes of a chord a little earlier in\nordertoaccentuatethesenotes.) Finally,wewanttopoint\nout that the assigned time positions generally tend to be\nslightly delayed. The reason is that it takes some time to\nbuild up a sound after a keystroke and that we actually\nmeasure the maximal increase of energy. In general, this\ndelay is largerfor lowerpitches than for higher pitches.20000 48000 60000 80000−0.200.20.4\n(a)\n48000 52000 56000 60000−0.200.20.4\n(b)Figure 4. (a) Synchronization result for the audio ﬁle of\nFigure 3. The matched notes are indicated by the vertical\nlines. (b) Enlargementof a segmentof (a).\n5. EFFICIENCY AND ANCHOR MATCHES\nRecall that running time as well as memory requirements\nof DP are proportional to the productof the number of\nscore and peak bins to be aligned. This makes DP inefﬁ-\ncient for long pieces (cf.Section 6). Classical techniques\nfor speeding-up DP computations are based on introduc-\ning global constraints which, however, does not improve\nthe complexitysubstantially.\nThe best possible complexity for a synchronization al-\ngorithm is proportional to the sumof the number of score\nand peak bins. Such a result may be achieved by using\ntechniquesemployedinareassuchas scorefollowing (see,\ne.g., [4]) which may be regarded as a kind of online syn-\nchronization. Such algorithms, however, are extremely\nsensible towards wrong or missing notes, local time devi-\nations,orerroneouslyextractedfeatures,whichcanresult\nin very poor synchronization results for complex, poly-\nphonic music.\nThe quality of the computed alignment and the robust-\nnessofthe synchronizationalgorithmare offoremost im-\nportance. Consequently,increasingefﬁciencyofthealgo-\nrithm should not degrade the synchronization result. To\nsubstantially increase the efﬁciency, we suggest the fol-\nlowing simple but powerful procedure: First, identify in\nthe score certain conﬁgurations of notes, also referred to\nasanchor conﬁgurations , which possess salient dynamic\nand/or spectral properties. Such a conﬁguration may be\nsomeisolatedfortissimochord,anoteorchordplayedaf-\nter or before some long pause, or a note with a salient\nfundamentalpitch. Duetotheirspecialcharacteristics,an-\nchor conﬁgurations can be efﬁciently detected in the cor-\nresponding audio ﬁle using a linear-time/linear-space al-\ngorithm. From this, compute score-to-audio matches, re-\nferred to as anchor matches , for the notes contained in an\nanchor conﬁguration. Finally, align the remaining notes.\nThis can be done locallyby applying our DP-based syn-\nchronization algorithm on the segments deﬁned by two\nadjacent anchor matches. The acceleration of the over-\nallprocedurewilldependonthedistributionoftheanchor\nmatches. Thebestoverallimprovementsareobtainedwith\nevenly distributed anchor matches. For example, n¡1\nanchor matches dividing the piece into equally long seg-mentsspeedsuptheaccumulatedrunningtimeforalllocal\nDPcomputationsbyafactor n. Thememoryrequirements\nare even cut down by a factor of n2since only the score\nmatrixoftheactivelocalDPcomputationhastobestored.\nOf course, ﬁnding suitable anchor conﬁgurations is a\ndifﬁcult research problem by itself (cf. Section 7). For\nthe moment, we use a semiautomatic ad-hoc approach in\nwhich the user has to specify a small number of suitable\nanchorconﬁgurationsforagivenpieceofmusic. Wehave\nimplementedseveralindependentdetectionalgorithmsfor\ndifferenttypesofanchorconﬁgurationswhichareapplied\nconcurrently in order to decrease the detection error rate.\nPauses in the audio data as well as isolated fortissimo\nchords are detected by suitably thresholding the ratio be-\ntween short-time and long-time signal energy computed\nwith a sliding window. Additionally, since pauses as well\nas long isolated chords correspond to segments with a\nsmall number of note onsets, such events can be detected\ninourpeaklistsfromtheextractionstep(seeSection3.3)\nbymeansofasuitablesparsenesscriterion. Notesofsalient\nfundamental pitch, i.e., notes whose fundamental pitch\ndoesnotclashwithharmonicsofothernoteswithinalarge\ntime interval, may be detected by scanning through the\ncorresponding subband using an energy-based measure.\nTo further enhance detection reliability, we also investi-\ngate the neighborhoods of the detected candidate anchor\nmatchescomparingnotesbeforeandaftertheanchorcon-\nﬁgurationtothecorrespondingsubbandpeakinformation.\nThen we discard candidate anchor matches exhibiting a\ncertain likelihood of confusion with the surrounding note\nobjectsorpeakevents. Theresultinganchormatchesmay\nbe presented to the user for manual veriﬁcation prior to\nthe local DP matching stage.\n6. EXPERIMENTS AND RESULTS\nA prototype of our synchronization algorithm has been\nimplemented in MATLAB. For the evaluation we used\nMIDI ﬁles representing the score data and corresponding\nCD recordings by various interprets representing the au-\ndio data. Our test material consists mainly of classical\npolyphonic piano pieces of various lengths ranging from\nseveral seconds up to 10minutes. In particular, it con-\ntains complex pieces such as Chopin’s Etudes Op. 10 and\nBeethoven’spiano sonatas.\nIt has already been observed in previous work that the\nevaluationofsynchronizationresultsisnotstraightforward\nand requires special care. First, one has to specify the\ngranularity of the alignment, which very much depends\non the particular application. For example, if one is inter-\nested in a system that simultaneously highlights the cur-\nrent measure of the score while playing a corresponding\ninterpretation (as a reading aid for the listener), an align-\nment deviation of a note or even several notes might be\ntolerable. However, for musical studies or when used as\ntraining data for statistical methods a synchronization at\nnote levelor evenonset levelmight be required.\nIntuitive objective measures of synchronization qual-ity are the percentage of note events correctly matched,\nthe percentage of mismatched notes, or the deviation be-\ntween the computed and optimal tempo curve. (The out-\nput of a synchronization algorithm may be regarded as\na tempo deviation or tempo curve between the two in-\nput data streams.) However, such a measure will fail if\nthe note events to be aligned do not exactly correspond\n(suchasfortrills,arpeggios,orwrongnotes). Inthiscase,\nthe measure might give a low grade (bad score), which\nis not due to the quality of the algorithm but due to the\nnature of the input streams. One would then rate a syn-\nchronization as “good” if the musically most important\nnote events are aligned correctly. Unfortunately, such an\nevaluationrequiresmanualinteraction,makingtheproce-\ndure unfeasible for large-scale examinations. Similarly,\nthe measurement of tempo curves requires some ground\ntruth about the desired outcome of the synchronization\nprocedure. The design of suitable objective measures,\nwhichallowasystematicandautomaticassessmentofthe\nsynchronization results, is still an open research problem\nand out of our scope.\nIn this paper, we evaluate our synchronization results\nmainly via soniﬁcation as follows: Recall that the input\nofoursynchronizationalgorithmisaMIDIﬁlerepresent-\ning the score and a WAV ﬁle representing the audio data.\nThealgorithmalignsthemusicalonsettimesgivenbythe\nscore (MIDI ﬁle) with the corresponding physical onset\ntimes extracted from the audio ﬁle. According to this\nalignment,wenowmodifytheMIDIﬁlesuchthatthemu-\nsical onset times correspond to the physical onset times.\nIn doing so we only consider those notes of the score that\nare actually matched and disregard the unmatched notes.\nThen we convert the modiﬁed MIDI ﬁle into an audio ﬁle\nby means of a synthesizer. If the synchronization result is\naccurate,thethussynthesizedaudiodatastreamrunssyn-\nchronously with the original performance. To make this\nresult comprehensible (audible) we produce a stereo au-\ndioﬁlecontaininginonechannelthemonoversionofthe\noriginalperformanceandintheotherchannelamonover-\nsion of the synthesized audio ﬁle. Listening to this stereo\naudio ﬁle will exhibit, due to the sensibility of the human\nauditorysystem,evensmallesttemporaldeviationsofless\nthan50ms between note onsets in the two version. To\ndemonstrateoursynchronizationresultswemadesomeof\nthe material availableat\nwww-mmdb.iai.uni-bonn.de/download/sync/,\nwhere we provide the score data (as a MIDI ﬁle), the au-\ndio data as well as the soniﬁcation of the synchronization\nresult of several classical piano pieces including the 25\nEtudes Op. 100 by Burgm ¨uller, the 12Etudes Op. 10 by\nChopin,andseveralsonatasbyBeethoven. Evenforthese\ncomplex pieces, our synchronization algorithm computes\naccurateglobalalignments,whicharemorethansufﬁcient\nfor applications such as the retrieval scenario, the reading\naid scenario or for musical studies. Moreover, most on-\nsetsofindividualnotesarematchedwithhighaccuracy—\nevenforpassageswithshortnotesinfastsuccessionbeingblurred due to extensive usage of the sustain pedal. (Lis-\nten, e.g., to the synchronization result of the Revolution\nEtude Op. 10, No. 12, by Chopin). Furthermore, aligning\nsudden tempo changes such as ritardandi, accelerandi, or\npauses generally poses no problem to our algorithm.\nOur current algorithm is sensitive towards some spe-\nciﬁc situations, where it may produce some local mis-\nmatches or may not be able to ﬁnd any suitable match.\nFirstly, pianissimo passages are problematic since softly\nplayed notes do not generate signiﬁcant energy increases\nintherespectivesubbands. Therefore,suchonsetsmaybe\nmissedbyourextractionalgorithm. Secondly,arepetition\nofthesamechordinpianoandforteisproblematic. Here,\nthe forte chord may cause “bad” peaks (see Section 3.3),\nwhich can be mixed up with the peaks corresponding to\nthe softly played piano chord. Such problematic situa-\ntionsmaybehandledbymeansofasubsequentalgorithm\nwhich is based on spectral features rather than based on\nonset features. A direct comparison to the approach in\n[1]showedthatouralgorithmisnotonlymorerobustand\nmore efﬁcient — concerning the extraction step as well\nas the synchronization step — but also results in a more\naccurate alignment.\nWe now give some examples to illustrate the running\ntime behavior and the memory requirements of our MAT-\nLAB implementation. Tests were run on an Intel Pentium\nIV, 3 GHz with 1 GByte RAM under Windows 2000. Ta-\nble 1 shows the running times for several pieces where\nthepiecesarespeciﬁedbytheﬁrstcolumn. Here,“Scale”\nconsists of a C-major scale played four times in a row in\ndifferent tempi, “Bu02” is the Etude No. 2, Op. 100, by\nF. Burgm ¨uller (see also Figure 2). “Ch03” and “Ch12”\nare Etude No. 3, Op. 10 (“Tristesse”) and Etude No. 12,\nOp. 10 (“Revolution”) by F. Chopin. Finally, “Be01” and\n“Be04” are the ﬁrst and fourth movement of Beethoven’s\nsonata Op. 2, No. 1. The second column shows the num-\nber of notes in the score of the respective piece and the\nthird column the length in seconds of some performance\nofthatpiece. Inthefourthandﬁfthcolumnsoneﬁndsthe\nnumber of note bins and peak bins (see Section 4). The\nnext column shows that the running time for the peak ex-\ntraction, denoted by t(peak), is about linear in the length\noftheperformance. Finally,thelastcolumnillustratesthat\nthe actual running time t(DP)of the DP algorithm is, as\nexpected,roughlyproportionaltotheproductofthenum-\nber of note bins and peak bins. The running time of the\noverallsynchronizationalgorithmisessentiallythesumof\nt(peak)andt(DP). The soniﬁcations of the correspond-\ningsynchronizationresultscanbefoundonourwebpage\nmentioned above.\nTable 2 shows how running time and memory require-\nmentsoftheDPcomputationsdecreasesigniﬁcantlywhen\nusing suitable anchor conﬁgurations (see Section 5). The\nthird column of Table 2 shows the respective list of an-\nchor matches which were computed prior to DP compu-\ntation. Here an anchor match is indicated by its assigned\ntime position within the audio data stream. The compu-\ntation time of these anchor matches is negligible relativePiece#notes len.#bins #bins t(peak)t(DP)\n(sec)(notes) (peaks) (sec) (sec)\nScale 3220 32 65 30.4\nBu02 480 45 244 615 22 37\nCh03 1877 173 618 1800 114 423\nCh12 2082 172 1318 2664 116 714\nBe01 2173 226 1322 2722 149 716\nBe04 4200 302 2472 3877 2012087\nTable 1. Running time of our synchronization algorithm\nfor variouspiano pieces.\nPiece len. listof anchor matches t(DP)MR\n(sec) (positionsgivenin sec) (sec)(MB)\nCh03 173 -4238.90\n98.5 2223.20\n42.5, 98.5, 146.2 1421.45\n42.5/74.7/98.5/125.3/146.2 870.44\nBe01 226 -71628.79\n106.5 3638.24\n53.1/106.5/146.2/168.8/198.5 1291.54\nBe04 302 -208776.67\n125.8 1042 20.4\n55.9/118.8/196.3/249.5 4335.09\nTable2. Accumulatedrunningtimeandmemoryrequire-\nmentsofthelocalDPcomputationsusinganchormatches.\nto the overall running time. The fourth column shows\nthe accumulated running time for all local DP computa-\ntions. As can be seen, this running time depends heavily\nonthedistributionoftheanchormatches. Forexample,in\nthe “Ch03” piece, one anchor match located in the mid-\ndle of the pieces roughly accelerates the DP computation\nby a factor of two. Also the memory requirements (MR),\nwhich are dominated by the “largest” local DP computa-\ntion, decrease drastically (see the last column of Table2).\n7. CONCLUSIONS\nIn this paper we have presented an algorithm for auto-\nmatic score-to-audio synchronization for polyphonic pi-\nano music. In view of efﬁciency and accuracy, we ex-\ntracted from the audio ﬁles a sparse but expressive set of\nfeaturesencodingcandidatesfornoteonsetsseparatelyfor\nall pitches. Using note and peak bins, we further reduced\nthe number of objects to be matched. The actual align-\nment was computed by dynamic programming based on\na suitable matching model, an efﬁciently computable lo-\ncal score measure, and subsequent individual note treat-\nment. The synchronization results, evaluated via soniﬁ-\ncation, are accurate even for complex piano music. To\nincrease the efﬁciency of the synchronization algorithm\nwithout degrading the alignment quality, we introduced\nthe concept of anchor matches which can be efﬁciently\ncomputed by a semi-automatic approach.\nDespiteconsiderableadvances,therearestillmanyopen\nresearchproblemsinautomaticscore-to-audio alignment.\nOne of our goals is to design robust linear-time/linear-\nspace synchronization algorithms producing high-qualityalignments. To this end, one could try to automatically\nextract anchor conﬁgurations by means of, e.g., statisti-\ncal methods and by using additional dynamics parame-\nters. For relatively short segments one could then try to\nuse linear-timescore-followingtechniques instead of DP.\nOur soniﬁcation only gives an overall feeling of syn-\nchronization quality. For the future, it would be impor-\ntant to design objective quality measures and to build up\na manually annotated evaluation database, allowing the\nmeasurement of technology progress and overall perfor-\nmance.\nAutomatic music processing is extremely difﬁcult due\nto the complexity and diversity of music data. One gen-\nerally has to account for various aspects such as the data\nformat(e.g.,score,MIDI,PCM),thegenre(e.g.,popmu-\nsic, classical music, jazz), the instrumentation (e.g., or-\nchestra, piano, drums, voice), and many other parameters\n(e.g.,dynamics,tempo,ortimbre). Therefore,auniversal\nalgorithm yielding optimal solutions for all kinds of mu-\nsic is unrealistic. For the future it seems to be promising\ntobuildupasystemthatincorporatesdifferent,competing\nstrategiesinsteadofrelyingononesinglestrategyinorder\nto cope with the richness and varietyof music.\n8. REFERENCES\n[1] Ariﬁ, V., Clausen, M., Kurth, F., M ¨uller, M.:\n“Automatic Synchronization of Musical Data:\nA Mathematical Approach”, In W. Hewlett\nand E. Selfridge-Fields, editors, Computing in\nMusicology . MIT Press, in press, 2004.\n[2] Blackham, E.D.: Klaviere. Die Physik der\nMusikinstrumente, 2. Auﬂage, Spectrum,\nAkademischer Verlag,1998.\n[3] Fletcher,N.H.,Rossing,T.D.: ThePhysicsof\nMusical Instruments. Springer-Verlag,1991.\n[4] Orio, N., Lemouton, S., Schwarz, D.: “Score\nFollowing: State of the Art and New Devel-\nopments”, Proc. Conf. of New Interfaces for\nMusical Expression NIME . Montreal, 36–41,\n2003.\n[5] Proakis, J.G., Manolakis D.G.: Digital Signal\nProcesssing .Prentice Hall, 1996.\n[6] Soulez, F., Rodet, X., Schwarz, D: “Improv-\ning polyphonic and poly-instrumental music\nto score alignment”, Proc. ISMIR . Baltimore,\nUSA, 2003.\n[7] Turetsky, R. J., Ellis, D. P., “Force-Aligning\nMIDI Syntheses for Polyphonic Music Tran-\nscription Generation”, Proc. ISMIR . Balti-\nmore, USA, 2003.\n[8] Scheirer, E. D.: “Extracting Expressive Per-\nformanceInformationfromRecordedMusic”,\nM.S. thesis, MIT Media Laboratory,1995."
    },
    {
        "title": "A Drum Pattern Retrieval Method by Voice Percussion.",
        "author": [
            "Tomoyasu Nakano",
            "Jun Ogata",
            "Masataka Goto",
            "Yuzuru Hiraga"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417569",
        "url": "https://doi.org/10.5281/zenodo.1417569",
        "ee": "https://zenodo.org/records/1417569/files/NakanoOGH04.pdf",
        "abstract": "This paper presents a method for voice percussion recog- nition and its application to drum pattern retrieval. Recog- nition of voice percussion (verbalized expression of drum sound by voice) requires an approach that is different from existing methods. Individual differences in both vocal characteristics and the kinds of verbal expressions used add further complication to the task. The approach taken in this study uses onomatopoeia as internal representation of drum sounds, and combines the recognition of voice percussion with the retrieval of intended drum patterns. This scheme is intended to deal with the two types of in- dividual differences mentioned above. In a recognition experiment with 200 utterances of voice percussion, our method achieved a recognition rate of 91.0% for the highest- tuned setting. keywords: voice percussion recognition, drum pattern retrieval, onomatopoeia representation",
        "zenodo_id": 1417569,
        "dblp_key": "conf/ismir/NakanoOGH04",
        "keywords": [
            "voice percussion recognition",
            "drum pattern retrieval",
            "onomatopoeia representation",
            "individual differences",
            "recognition rate",
            "utterances",
            "highest-tuned setting",
            "experiment",
            "approach",
            "task"
        ],
        "content": "A DRUM PATTERN RETRIEVAL METHOD BY VOICE PERCUSSION\nTomoyasu Nakano\n/DDJun Ogata\n/DEMasataka Goto\n/DEYuzuru Hiraga\n/DD/DDGraduate School of Library, Information and Media Studies\nUniversity of Tsukuba, Japan/DENational Institute of Advanced Industrial Science and Technology (AIST), Japan\nABSTRACT\nThis paper presents a method for voice percussion recog-\nnition and its application to drum pattern retrieval. Recog-\nnition of voice percussion (verbalized expression of drumsound by voice) requires an approach that is different from\nexisting methods. Individual differences in both vocal\ncharacteristics and the kinds of verbal expressions used\nadd further complication to the task. The approach taken\nin this study uses onomatopoeia as internal representationof drum sounds, and combines the recognition of voice\npercussion with the retrieval of intended drum patterns.\nThis scheme is intended to deal with the two types of in-dividual differences mentioned above. In a recognition\nexperiment with 200 utterances of voice percussion, our\nmethod achieved a recognition rate of 91.0% for the highest-\ntuned setting.\nkeywords: voice percussion recognition, drum pattern\nretrieval, onomatopoeia representation\n1. INTRODUCTION\nThis paper presents a method for voice percussion recog-\nnition and its application to drum pattern retrieval. Voice\npercussion in our context is the mimicking of drum sounds\nby voice, expressed in verbal form that can be transcribed\ninto phonemic representation, or onomatopoeia (e.g. don-\ndon, ta-ta ). In this sense, voice percussion is not a direct,\nfaithful reproduction of the acoustic properties of actual\ndrum sounds.\nThe analysis and recognition of voice percussion re-\nquires an approach that is different from those in existing\nwork, such as drum sound recognition [1, 4] or query-by-\nhumming [6]. Drum sound recognition looks for acoustic\nproperties that are characteristic of the instrument, but in\nour case, mapping between the input voice and target in-strument sound is only indirect and metaphoric. Query-\nby-humming focuses on pitch detection and melodic fea-\nture extraction, but these have less relevance in voice per-cussion recognition, which is primarily concerned with\nclassiﬁcation of timbre and identiﬁcation of articulation\nmethods. Differences among individuals in both vocal\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copiesare not made or distributed for proﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\nc/AD2004 Universitat Pompeu Fabra.\ncharacteristics and the kinds of verbal expressions used\nadd further complication to the task.\nThe following sections describe our approach and the\nexperimental results of its evaluation. Section 2 presents\nan overview of the proposed method. Section 3 describes\nthe experiments for deriving the model and its evaluation.Section 4 concludes the paper, with discussion on provi-\nsions for future work.\nWe view voice percussion recognition to have promis-\ning applications in widening the scope of music informa-\ntion retri evalmethods, and introducing new possibilities\nfor music transcription, composition and arrangement.\n2. PROPOSED METHOD\nThe proposed method combines voice percussion recog-\nnition with drum pattern retrieval. A drum pattern is a\nsequence of percussion beats typically used in popular\nmusic (see Figure 2 for example drum patterns). In the\ncurrent work, drum patterns consist of two percussion in-struments — bass drum (BD) and snare drum (SD). A\nvoice percussion input is regarded as expressing a par-\nticular drum pattern. So instead of identifying individual\ndrum sounds in isolation, the entire sequence is matched\nwith the entries of a drum pattern database. The recogni-tion is considered to be successful if the intended pattern\nis correctly retrieved. An overview of the method is shown\nin Figure 1.\nReﬂecting the verbal nature of the input, onomatopoeia\nis used as an intermediate level representation of voicepercussion (Fig.1, B2, B3). This plays the pivotal role\nof connecting the recognition and retri evalphases.\nTypical onomatopoeic expressions of single drum sounds\nare stored in a pronunciation dictionary (B2), where the\nentries are derived from the results of existing work on\nonomatopoeia [5], and also from the extraction experi-\nment described in 3.1. Using this dictionary, the entriesof the drum pattern database (D) are expanded into possi-\nble onomatopoeic expressions (B3).\nThe elements of these expressions are mapped into se-\nquences of acoustic features by an acoustic model (B4),\nwhich is a stochastic transitional network using the Hid-den Markov Model (HMM). The base acoustic model is a\nphoneme-level HMM (monophone) provided by CSRC [7],\ntrained by a newspaper dictation corpus (40,000 utterances\nby 270 Japanese speakers).\nFor a given voice percussion utterance (A1), our methodVoice\nPercussionInput\nDrum Pattern\nDatabasePronunciation\nDictionaryOnomatopoeia\nExpressionAcoustic\nModel\nBD  SD  BD  BD  SD\nOutputFeature\nExtraction(MFCC)BD SD BD SD\nBD SD BD BD SDd o N p a q d o d o p a qd o N t a N d o N t a NBD    d o N\nBD     t o N    ......SD     t a NSD    p a q    ......\nNd\noSequence\nDatabase\n 0    1.1    2   2.7   3Sequence\nOnset TimeSimilarity\nComputing(IOI vector)\nViterbi\nAlgorithm\nDrum PatternBD  SD  BD  BD  SD\nOnset Time\nDatabase0    1    2   2.5  3\n0    1  1.5   2    3(B1) (B2) (B3)(A1)\n(A2)\n(A3)\n(A4)(A5)\n(A6)(B4)\n(D) (C)\nFigure 1 . Overview of the proposed method.\nﬁrst extracts its acoustic features in the form of MFCCs\n(Mel Frequency Cepstral Coefﬁcients: A2). This output\nis matched with the acoustic model using the Viterbi al-\ngorithm (A3). The instrument name sequence with the\nhighest likelihood score is selected as the retrieved result\n(A4). This stage of retri eval, where only the instrument\nname sequence is retrieved, is called sequence matching .\nA single instrument name sequence may correspond to\nseveral drum patterns with different rhythm. Drum pat-\ntern matching further matches the IOI (Inter-Onset Inter-\nval) vector with entries in the onset time database (C).\nHere, the IOI vector is deﬁned as the sequence of temporal\ndifferences between adjacent onset times. The similarity\nof the IOI vectors is calculated by the cosine measure , and\nthe drum pattern with the highest score is selected as the\nretrieved result (A6).\nThe acoustic model and the pronunciation dictionary\ncan be switched from their basic versions to more tuned\nversions ( e.g.reﬂecting individual difference), providing\nvarious settings for the recognition experiments.\n3. EXPERIMENTS AND RESULTS\nThe proposed method is implemented and tested in two\nkinds of experiments. Experiment 1 is a preliminary psy-\nchological experiment of expression extraction , which gath-\ners data on how subjects express drum sounds and pat-terns. Experiment 2 is a series of recognition experiments,\nwhich evaluates the performance of the proposed method\nunder various settings.\nThe target drum patterns have the length of 4 beats (one\nmeasure in 4/4 time), and consist of Bass Drum (BD) andSnare Drum (SD) sounds with no simultaneous beats.\n3.1. Experiment 1: Expression Extraction\nThe purpose of this experiment is to identify and extract\ntypical onomatopoeic expressions of drum patterns by hu-\nman subjects. The data obtained in the experiment are\nused for the construction and tuning of the pronunciationdictionary and the acoustic model. The data are also used\nas test data for the recognition experiments described in\n3.2.3.1.1. Experiment Setting and Task\nThe subjects were instructed to listen to ten kinds of drum\npatterns (each in 80 and 120 M.M. tempo), and to freely\nsing out each pattern according to their verbal image. Fig-\nure 2 shows the drum patterns used in the experiment.\nThe lower notes correspond to the BD sounds, and thehigher notes, the SD sounds. The sound stimuli were gen-\nerated by using the BD and SD sounds in the RWC Music\nDatabase (Musical Instrument Sound) [3]\n1.\nThere were 17 subjects (all Japanese speakers) with\nages from 19 to 31. Two had previous experience with\npercussion instruments. The 20 patterns were presented\nto each subject in random order.\nNo.01\nNo.10No.08\nNo.09No.07No.06\nNo.05No.04No.03No.02\nFigure 2 . Drum patterns used in the extraction experi-\nment.\n3.1.2. Results and Discussion\nThe onomatopoeic expressions for individual drum sounds\ncould be classiﬁed into six types — namely “CV”, “CVQ”,\n“CVN”, “CVNQ”, “CVRN” and “CVRNQ”. The symbolsused above stand for:\nC: consonant (t, d, p, ...)\nV: vowel (a, o, e, ...)Q: choked sound, written as q\nN: syllabic nasal ( nsound)\nR: long vowel\n1421BD1N3.WA V , 421SD3N3.WA V (RWC-MDB-I-2001 No.42)For example, with “t” for C and “a” for V , CV , CVQ and\nCVN are “ta”, “taq”, and “tan”, respectively.\nThe leading CV pair was generally signiﬁcant for in-\nstrument identiﬁcation (between BD and SD). Typical CV\npairs used for BD and SD are shown in Table 1, togetherwith the number of subjects that used them (most subjects\nused several CV pairs for the same instrument). The trail-\ning parts (of Q, R, N) had correspondence with the length\nand rhythmic patterns of the beats.\nBD\ntype\n /d o/\n /d u/\n /t o/\n /t u/\n /z u/\n /r e/\nsubjects\n 9\n 8\n 5\n 4\n 3\n 1\nSD\ntype\n /t a/\n /d a/\n /p a/\n /k a/\n /ch a/\n /t e/\n /r a/\nsubjects\n 13\n 3\n 3\n 2\n 1\n 1\n 1\nTable 1 . Examples of CV pairs used for drum sounds.\nAlthough most subjects distinguished timbre by ono-\nmatopoeic expression, two subjects made the BD/SD dis-\ntinction mostly by pitch. These two cases were excluded\nfrom the recognition experiment described below. An-other two subjects (both with percussion instrument ex-\nperience) verbally expressed rest notes, which may reﬂect\ntheir ordinary practice.\nBut in general, the obtained data seemed to ensure the\nvalidity of the proposed method, which is tested in exper-\niment 2.\n3.2. Experiment 2: Recognition Experiments\nThe proposed method was tested in a series of recognition\nexperiments. The drum pattern database used in the exper-iments had 1169 drum patterns (538 instances of instru-\nment sequence patterns), including all ten patterns used in\nthe extraction experiment (Fig.2). 1167 of the drum pat-terns were extracted from the SMF (Standard MIDI File)\ndata of the RWC Music Database (Popular Music) [2], and\nthe remaining two are patterns from the extraction experi-\nment not included in the SMF data (patterns 8 & 10).\nThe recognition experiments used the utterance data of\n15 subjects in the extraction experiment, which are di-vided into two groups — test set (10 subjects, 200 utter-\nances) and training set (5 subjects, 100 utterances).\n3.2.1. Experimental Setting\nFour experiments (A)–(D) were performed, over a com-\nbination of three acoustic models and two pronunciation\ndictionaries. The four conditions are as follows.\n(A) Base acoustic model of Japanese phonemes (model 1),\nbasic pronunciation dictionary of Japanese phonemes.\n(B) Speaker-independent acoustic model (model 2), basic\ndictionary.\n(C) Speaker-speciﬁc acoustic model (model 3), basic dic-\ntionary.(D) Acoustic model 3, pronunciation dictionary with en-\ntries restricted to those used in voice percussion utter-\nances (individual dictionary).\nModels 2 & 3 were tuned by using MLLR-MAP [8] as\nthe adaptation method, which is a combination of MLLR\n(Maximum Likelihood Linear Regression) and MAP (Max-imum A Posteriori Probability). Model 2 is tuned with the\ntraining set, corresponding to a general acoustic model of\nvoice percussion. Model 3 is tuned with the utterance dataof individual subjects in the test set.\n3.2.2. Evaluation Criteria\nThe results were evaluated by three criteria, namely, se-\nquence evaluation, onset evaluation, and drum pattern eval-\nuation.\nSequence evaluation is the recognition rate (percentage\nof test data correctly recognized) of instrument name se-\nquences; where the recognized sequence is the one with\nthe highest likelihood score obtained by the Viterbi algo-rithm. Onset evaluation checks the deviation of extracted\nonsets from the correct onsets (hand-labeled). Drum pat-\ntern evaluation is the recognition rate of drum patterns,\ni.e.combination of instrument name and onset sequences.\nThe onset sequence ( IOI vector ) having the highest sim-\nilarity ( cosine measure ) with the input is selected as the\nrecognized result. Since different drum patterns may have\nthe same instrument name sequence, this is a more strictcriteria than sequence evaluation.\nThe recognition rates for acoustic model 3 are evalu-\nated by using a cross validation method. In each trial, the\nmodel is tuned by 18 of the 20 utterances by a single sub-ject, and is tested by using the remaining two as test data.\nThe overall recognition rate is the result of 10 trials per\nsubject for the 10 subjects in the test set.\n3.2.3. Results and Discussion\nTable 2 shows the recognition rates for sequence evalu-\nation and drum pattern evaluation. The results show an\nincreasing trend with a large improvement (of over 15%)\nbetween experiments (B) and (C) — indicating that incor-\nporating task-speciﬁc and individually tuned data is effec-\ntive for improving system performance.\nFigures 3 and 4 show the sequence evaluation results\nby subject and by drum pattern. The increasing trend isespecially notable for subjects I and IV , where the recogni-\ntion rates improved from 0% in experiment (A) to 70–95%\nRecognition Rate\nCondition\nAcoustic\n Pronunciation\nDrum\nModel\n Dictionary\n Sequence\nPattern\n(A)\n model 1\n basic\n 65.0%\n 62.5%\n(B)\n model 2\n basic\n 70.0%\n 68.5%\n(C)\n model 3\n basic\n 89.0%\n 86.5%\n(D)\n model 3\n individual\n 93.0%\n 91.0%\nTable 2 . Experimental conditions and results.in experiment (D). This is the main source of the overall\nimprovement between (B) and (C).\nFigure 5 shows the onset evaluation results for experi-\nment (D). The overall difference is small (mean=-0.0220sec, standard deviation=0.0231 sec), indicating that onset\nextraction is precise enough in general. Larger deviations\noccurred in cases such as when a silent pause between ad-jacent sounds was not properly detected, two sounds could\nnot be separated, or noise such as breath sound was falsely\ndetected.\nThe precision of onset detection is also reﬂected in the\nrelatively small difference between sequence evaluationand drum pattern evaluation in Table 2. This suggests\nthat the correct recognition of instrument name sequences\ncomes hand-in-hand with the correct recognition of theonset time.\nAlthough recognition failure in experiments (A) and\n(B) are mainly due to lack of task-speciﬁc data and in-\nsufﬁcient tuning, there are still failures in the more precise\nsettings of experiments (C) and (D). These failures are dueto cases such as when rests were spoken out to maintain\nrhythm, or when instrument difference was expressed by\npitch. Dealing with these and other problems are issues offuture work.\n020406080100 Recognition Rate[%]\nII I(A) (B) (D) (C)\nSpeaker No.III IV V VI VII VIII IX X\nFigure 3 . Result of sequence evaluation (by subject).\nDrum Pattern No.020406080100 Recognition Rate[%](A) (B) (D) (C)\n1 234 56 7 8 910\nFigure 4 . Result of sequence evaluation (by drum pat-\ntern).\n4. GENERAL DISCUSSION AND CONCLUSION\nThis paper presented a method for voice percussion recog-\nnition and drum pattern retrieval. The results indicate thegeneral effectiveness of our method, obtaining a recogni-\ntion rate of about 70% as the bottom line. The increase\nof the recognition rate for tuned settings (experiments (C)\nTime Deviation [s]-0.2 -0.05 0.05 0 -0.15 -0.10100200\nFigure 5 . Result of onset evaluation.\nand (D)) suggests that individual difference is wide ranged,\nand from a practical point of view, system performance\ncan be improved dramatically by adjusting to individualusers.\nStill, the current framework is restricted in important\naspects. First, the data used is obtained from Japanese\nspeakers, so the system should be seen to be adjusted to\nJapanese users. Whether the general framework of ourmethod can be applicable to users of different languages\nand background culture needs investigation.\nSecond, the current model handles only two instruments,\nnamely SD and BD. Extending the scope of instruments\n(e.g. including tams, cymbals, etc.), and the complexity\nof drum patterns ( e.g.allowing simultaneous beats) is an\nimmediate issue for future work.\nThere are many prospective applications conceivable\nfor our method. The most direct application is in mu-\nsic information retrieval, where the scope of retrieval will\nbe greatly enhanced by dealing with percussion soundsand rhythmic patterns. Other possible application can be\nfound in composition and performance supporting sys-\ntems. The current work is intended to be the ﬁrst step\ntoward opening such new possibilities.\n5. REFERENCES\n[1] Goto, M. et al. ”A Sound Source Separation System for\nPercussion Instruments”, IEICE Transactions , J77-D-II,\npp.901-911, 1994 ( in Japanese ).\n[2] Goto, M. et al. ”RWC Music Database: Popular, Classical,\nand Jazz Music Databases”, Proc. of ISMIR , pp.287-288,\n2002.\n[3] Goto, M. et al. ”RWC Music Database: Music Genre\nDatabase and Musical Instrument Sound Database”, Proc.\nof ISMIR , pp.229-230, 2003.\n[4] Herrera, P. et al. ”Automatic Classiﬁcation of Drum\nSounds: A Comparison of Feature Selection Methods and\nClassiﬁcation Techniques”, Proc. of ICMAI , LNAI2445,\npp.69-80, 2002.\n[5] Hiyane, K. et al. ”Study of Spectrum Structure of Short-\ntime Sounds and its Onomatopoeia Expression”, Technical\nReport of IEICE. SP97-125 , pp.65-72, 1998 ( in Japanese ).\n[6] Kageyama, T. et al. ”Melody Retrieval with Humming”,\nProc. of ICMC , pp.349-351, 1993.\n[7] Lee, A. et al. ”Continuous Speech Recognition Consortium\n— an open repository for CSR tools and models —”, In\nProc. IEEE Int’l Conf. on Language Resources and Evalu-ation (LREC2002) , pp.1438-1441, 2002.\n[8] Thelen, E. et al. ”Speaker Adaptation in the Philips Sys-\ntem for Large V ocabulary Continuous Speech Recogni-\ntion”, Proc. of ICASSP , pp.1035-1038, 1997."
    },
    {
        "title": "Towards Automatic Transcription of Australian Aboriginal Music.",
        "author": [
            "Andrew Nesbit",
            "Lloyd C. L. Hollenberg",
            "Anthony Senyard"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416874",
        "url": "https://doi.org/10.5281/zenodo.1416874",
        "ee": "https://zenodo.org/records/1416874/files/NesbitHS04.pdf",
        "abstract": "We describe a system designed for automatic extraction and segmentation of didjeridu and clapsticks from cer- tain styles of traditional Aboriginal Australian music. For didjeridu, we locate the start of notes using a complex- domain note onset detection algorithm, and use the de- tected onsets as cues for determining the harmonic series of sinusoids belonging to the didjeridu. The harmonic se- ries is hypothesised, based on prior knowledge of the fun- damental frequency of the didjeridu, and the most likely hypothesis is assumed. For clapsticks, we use indepen- dent subspace analysis to split the signal into harmonic and percussive components, followed by classification of the independent components. Finally, we identify areas in which the system can be enhanced to improve accuracy and also to extract a wider range of musically-relevant features. These include algo- rithms such as high frequency content techniques, and also computing the morphology of the didjeridu.",
        "zenodo_id": 1416874,
        "dblp_key": "conf/ismir/NesbitHS04",
        "keywords": [
            "Automatic extraction",
            "didjeridu detection",
            "clapsticks segmentation",
            "traditional Aboriginal music",
            "note onset detection",
            "harmonic series",
            "independent subspace analysis",
            "classification",
            "musically-relevant features",
            "enhancement algorithms"
        ],
        "content": "TOWARDSAUTOMATICTRANSCRIPTION OFAUSTRALIAN\nABORIGIN ALMUSIC\nAndrewNesbit1\nSchoolofPhysics\nUniversityofMelbourne\nAustraliaLloydHollenber g\nSchoolofPhysics\nUniversityofMelbourne\nAustraliaAnthonySenyard\nDeptofComputer Science\nandSoftwareEngineering\nUniversityofMelbourne\nAustralia\nABSTRACT\nWedescribe asystem designed forautomatic extraction\nandsegmentation ofdidjeridu and clapsticks from cer-\ntainstyles oftraditional Aboriginal Australian music. For\ndidjeridu, welocate thestart ofnotes using acomple x-\ndomain note onset detection algorithm, andusethede-\ntected onsets ascues fordetermining theharmonic series\nofsinusoids belonging tothedidjeridu. Theharmonic se-\nriesishypothesised, based onprior knowledge ofthefun-\ndamental frequenc yofthedidjeridu, andthemost likely\nhypothesis isassumed. Forclapsticks, weuseindepen-\ndent subspace analysis tosplit thesignal into harmonic\nandpercussi vecomponents, follo wed byclassiﬁcation of\ntheindependent components.\nFinally ,weidentify areas inwhich thesystem canbe\nenhanced toimpro veaccurac yandalso toextract awider\nrange ofmusically-rele vantfeatures. These include algo-\nrithms such ashigh frequenc ycontent techniques, andalso\ncomputing themorphology ofthedidjeridu.\n1.INTRODUCTION\nThetraditional music ofIndigenous Australians isﬁrmly\nentrenched inoral tradition. The songs arepassed down\nthrough generations within agroup, without written nota-\ntion, andtypically describe thehistory andculture ofthe\ngroup.\nWehavedesigned andimplemented ourtranscription\nsystem with twostyles ofAustralian music inmind. The\nﬁrstisLirrga,agenre ofmusic from northwest Australia.\nTheperformances westudy arecomposed andperformed\nbyPius Luckan (voice, clapsticks) andClement Tchinb ur-\nrur(didjeridu) [11].Therecordings were made inWadeye\n(Port Keats, northern Australia). IntheMarri Ngarr lan-\nguage (one ofsevenlanguages spok enatWadeye),clap-\nsticks arecalledtitiranddidjeridu iskarnbi .\nPermission tomakedigitalorhardcopiesofallorpartofthisworkfor\npersonalorclassroom useisgrantedwithoutfeeprovidedthatcopies\narenotmadeordistributedforpro®torcommercial advantageandthat\ncopiesbearthisnoticeandthefullcitationonthe®rstpage.\nc\r2004UniversitatPompeuFabra.The second setofrecordings isacollection oftradi-\ntional songs from theGalpu clan ofnortheast Arnhem\nLand. Thesongs arearranged andperformed byGurritjiri\nGurruwiwi (voice), with Djalu Gurruwiwi (didjeridu) [5].\nTheYolngu people who reside here callthedidjeridu yidaki\nandtheclapsticks bilma .\nThe rhythmic structures ofthese musics arecomple x\nandhighly expressi ve.Polyrhythms, changes oftempo\nandchanges ofmetre arecommon andintegraltothemu-\nsic.\nOur initial motivation forcreating thissystem wasto\nconstruct auseful tooltoaidethnomusicologists studying\nAustralian Aboriginal music. Wehadaccess tomanually-\ncreated transcriptions oftheLirrgasongs, andthese served\nasagood model astothelevelofdetail oursystem should\naimtowards. Although oursystem hasbeen designed with\nmore than onestyle ofmusic inmind, forthisreason, and\nalso forthefactthatthetwostyles ofmusic areverydif-\nferent, wehaveexecuted most ofourevaluations onthe\nLirrga.\nThesystem isdesigned todetermine onsets oftheclap-\nsticks, andonsets andfundamental frequencies ofthedid-\njeridu parts. Weassume that there isonly oneofeach\ninstrument playing atanygiventime, andthatthefunda-\nmental frequenc yofthedidjeridu isbelow100Hz,which\nworks well formost ofoursamples. Wedonotattempt to\ntranscribe vocals inthissystem.\nAsfarasweaware,nopublished research onautomatic\ntranscription ofAustralian Aboriginal music exists. How-\never,workhasbeen done instudying themusical acoustics\nofthedidjeridu, andrecent studies may befound in[3],\n[4],[7].\nSeminal studies into thehistory andcultural signiﬁ-\ncance ofthedidjeridu include [14].Also, research into\ndidjeridu notation [13]provides guidelines astothetypes\noffeatures wemay wish toextract.\n1The®rstauthoriscurrently withtheDepartment ofElectronic Engi-\nneering,QueenMary,UniversityofLondon.!\"#$%$\"#$\"&\n'()'%*+$\n*\"*,-'.'/0!1234.5.&*,\n*(#.6\n7.,&$8)*\"9\n:,*''.;-\n+6<%6\"$\"&'!\"#$%$\"#$\"&\n+6<%6\"$\"&'\n=*8<6\".+\n'.5\"*,>$8+(''.?$\n'.5\"*,\n@\"'$&\n#$&$+&.6\"\n:,*%'&.+9'\n&8*\"'+8.%&.6\"@\"'$&\n#$&$+&.6\"\n>.&+A\n$'&.<*&.6\"\n4.#B$8.#(\n&8*\"'+8.%&.6\"1.\"('6.#\n&8*+9.\"5\nFigure1.High-le velovervie wofthesystem.\n2.THESYSTEM\nAsthetechniques used fortranscription ofdidjeridu (de-\nterministic) areverydifferent tothose used fortranscrip-\ntionofclapsticks (statistical), thesystem isessentially split\ninto twodisjoint “halv es”,asindicated inFigure 1.The\nlefthalf indicates thedata transformations thatoccur for\ndidjeridu processing, andtheright halfdescribes theop-\nerations used forextracting clapsticks.\n2.1.Extraction ofdidjeridu\nTheoverall scheme used forthisphase wasbased onasys-\ntemforautomatic transcription ofbass lines [6],butwith a\ndifferent onset detection scheme. Theoriginal signal was\npassed through abank ofbandpass ﬁlters, emitting signals\nintheranges 0–100 Hz,100–200 Hzand200–300 Hz.\nThese ranges were chosen tocapture thefundamental fre-\nquenc yofthedidjeridu (typically below100Hz), andits\nﬁrsttwoupper harmonics intoeach frequenc yband. The\nother instruments carried verylittle energyinthese fre-\nquenc yranges, andso,within theconte xtofthisproject,\nweassume that allmusical information carried inthese\nranges belongs tothedidjeridu.2.1.1.Onsetdetection\nOurinformal experiments revealed thatcomple x-domain\nonset detection [1]works well forlow-frequenc ysignals.\nForeach frequenc yband, comple x-domain onset de-\ntection wasapplied. After thethree frequenc ybands had\nbeen analysed inthisway,theonsets from each ofthe\nbands were combined intoonesequence. Each onset was\nconsidered inturn: ifitwascloser than 50mstoanother\nonset anditsamplitude waslessthan theneighbouring on-\nset,itwasremo ved. Thus, theonsets forthedidjeridu\nwere givenbytheresulting sequence.\n2.1.2.Frequencyestimation\nThenextstage wastoestimate thefundamental frequenc y\nateach onset. Aframe aslong aspossible wasconsid-\nered, starting justafter anonset andending justbefore the\nnextonset. Foreach such frame, asinusoid extraction al-\ngorithm wasapplied. Rather than usethemethod detailed\nin[12]assuggested by[6],weopted forthetriangle win-\ndowmethod [9].Everynote under 100Hzwasconsidered\ntobeacandidate fundamental frequenc y,andwemakethe\nassumption onourdata thatonly onenote isplaying ata\ntime.\nForeach fundamental frequenc ycandidate F0,wepre-\ndictitsharmonic series asfnF0g1\u0014n\u0014N.Theactual har-\nmonic series associated with eachF0isdetermined from\nthisbyconsidering eachnF0for2\u0014n\u0014Ninturn, and\nsearching fortheextracted sinusoid whose frequenc ylies\nwithin 3%ofitspredicted value, andwhose amplitude isa\nmaximum. Wefound thatavalue ofN=9gavegood re-\nsults, although thisprobably could havebeen made much\nsmaller without noticable lossofaccurac yinourresults.\nAtthisstage, wehaveoneormore harmonic series cor-\nresponding toeach onset. Todetermine themost probable\nharmonic series foreach offset, weassign each series a\nconﬁdence measure asdescribed in[6].The series with\nthehighest conﬁdence isdeemed tobethecorrect one,\nandhence, thefundamental frequenc yisdetermined.\nNote thatouralgorithm isasimpliﬁed version ofthe\noneitisbased on.Inparticular ,foreach onset, thealgo-\nrithm described in[6]tracks theharmonic series overtime\ninorder todetermine thenote offsetforthatseries, and\ntodetermine thecorrect series using amore sophisticated\nmeasure. Wechose thissimpler technique because itwas\nnotpractical toachie vethenecessary frequenc yresolu-\ntionforaccurate determination ofsinusoids forsuch low\nfrequencies: theshort frames required foraccurate time\nresolution prohibited this.\n2.2.Extraction ofclapsticks\nToextract theclapsticks, weused themethod ofindepen-\ndent subspace analysis (ISA) described in[15].Thefol-\nlowing discussion isessentially asummary ofthatpaper .\nThis technique isbased onindependent component anal-\nysis (ICA), andweuseittosplit theoriginal signal into!\"#$%&'()*'&%+\n,+-./0.1\"2\".#\n3!4565\"$\"2'&)'%,\".\n7#,+0+#,+#2\n-./0.#+#2\n'#'&81\"1)379:6!;.(2)2\"/+)<.%(\"+(\n2('#1=.(/)3!><>6\n9&'11\"?-'2\".#\n'#,\n(+-.#12(%-2\".#9&'012\"-@1\nA'(/.#\"-)'%,\".\nFigure2.Independent subspace analysis [15]\nharmonic andpercussi vecomponents. Theclassic formu-\nlation ofblind source separation byICA requires atleast\nasmanyobserv edsignals asthere existsources. Inour\ncase, wehaveoneobserv ation (the recording itself) and\nthree sources (didjeridu, clapsticks andvocals). Figure 2\nbrieﬂy indicate thesteps thatISA performs toovercome\nthislimitation. The original (single observ ation) time-\nseries signal istransformed tothefrequenc ydomain by\nshort-time Fourier transform (STFT). Togetreliable sep-\naration, weused long frames (100 ms)with ahalf-frame\noverlap. Singular valuedecomposition (SVD) isperformed\nontheresultant magnitude spectrogram, andamaximum-\nvariance subspace oftheoriginal spectrogram, reduced to\nddimensions, isthen computed. Finally ,amplitude en-\nvelopes andfrequenc yweights ofeach ofthedindepen-\ndent components arecomputed using ICA. (These dam-\nplitude envelopes andfrequenc yweights may beused to\ndetermine theindependent spectrograms. Wedonotmake\nuseofthese, however,sothisisnotdone inoursystem.)\nThrough experimentation, wefound thatsetting 15\u0014\nd\u001420provides excellent separation with anacceptable\ncomputational cost.\nTheremaining taskinthisstage wastoclassify each of\nthedseparated components into either harmonic orper-\ncussi vecategories. [15]describes ﬁvemeasurable features\noftheindependent components, each ofwhich givesanin-\ndication ofthepercussion-lik eness ofeach oftheindepen-\ndent components. Oursystem makesuseoftwoofthese\nfeatures.\nTheﬁrst, percussi veness, isdetermined bycomputing\natrain ofunitimpulses, where each impulse islocated ata\nmaximum oftheamplitude envelope (determined during\ntheICA), andconvolving thisimpulse train with amodel\npercussion template. This percussi veimpulse ismodelled\nbyaninstantaneous onset andlinear decay towards zero\nwithin 200ms. The measure ofpercussi veness isgiven\nbythecorrelation coefﬁcient between theoutput ofthe\nconvolution andtheamplitude envelope.\nThe second feature, noise-lik eness, uses thecompo-\nnent’ svector offrequenc yweights (determined during the\nICA). Similarly tothemethod described above,anim-\npulse train corresponding tothemaxima ofthefrequenc y\nvector isconvolvedwith aGaussian windo w.Thenoise-\nlikeness isgivenbythecorrelation coefﬁcient oftheorig-\ninalfrequenc yvector andtheoutput oftheconvolution.\nFigure3.Kilakanggi:Ourmother excerpt reference tran-\nscription (lirrga1).Track1fromCD[11].Songtextc\rcomposed\nbyClement Tchinburrur,sungbyPiusLuckan,recorded byChester\nStreet,PtKeats,1985.Musicaltranscription c\rLindaBarwick, Univer-\nsityofSydney,2002.MarriNgarrmorphemic analysisandtranslation\nc\rLysbethFord,BIITE,2002.Reproduced withpermission.\nFigure4.Yithayithakangki:Father,ourFather ex-\ncerpt reference transcription (lirrga2).Track2fromCD[11].\nSongtextc\rcomposed byClementTchinburrur,sungbyPiusLuckan,\nrecorded byChesterStreet,PtKeats,1985.Musicaltranscription c\r\nLindaBarwick, UniversityofSydney,2002.MarriNgarrmorphemic\nanalysisandtranslation c\rLysbethFord,BIITE,2002.Reproduced with\npermission.\nThe decision astowhether acomponent waspercus-\nsiveorharmonic wasmade bycomparing thepercussi ve-\nness andnoise-lik eness measures topredetermined thresh-\nolds.\nTodetermine theonsets oftheclapsticks, anote onset\ndetection algorithm, asdescribed inSection 2.1.1 wasap-\nplied tothesum oftheamplitude envelopes determined\nduring theICA process andcorresponding tothepercus-\nsivelyclassiﬁed component.\n3.RESULTS\nTheaccurac yofthesystem, andalso itssensiti vitytoin-\nﬂections, willimpro vewith theincorporation ofmore so-\nphisticated techniques andalgorithms. Wediscuss thisin\nsection 4.\nWeevaluated theperformance ofoursystem byrun-\nning itonshort (approximately 10seconds) excerpts and\ncomparing theresults against thetranscriptions Figures 3–\n6,which were prepared manually byethnomusicologists.\n3.1.Didjeridu transcription\nTomeasure thetranscription accurac y,weemplo yamet-\nricdescribed in[8];thispaper givesmore than onemetric,\nandwechoose themore stringent alternati ve.ItisdeﬁnedFigure5.Yithakanggiwarringgirrmagulil: OurFather\nenterintousexcerpt reference transcription (lirrga3).\nTrack4fromCD[11].Songtextc\rcomposed byClementTchinbur-\nrur,sungbyPiusLuckan,recorded byChesterStreet,PtKeats,1985.\nMusicaltranscription c\rLindaBarwick, UniversityofSydney,2002.\nMarriNgarrmorphemic analysisandtranslation c\rLysbethFord,BI-\nITE,2002.Reproduced withpermission.\nFigure6.FatherDeakin excerpt reference transcription\n(lirrga4).Track5fromCD[11].Songtextc\rcomposed byClement\nTchinburrur,sungbyPiusLuckan,recordedbyChesterStreet,PtKeats,\n1985.Musicaltranscription c\rLindaBarwick, UniversityofSydney,\n2002.MarriNgarrmorphemic analysisandtranslation c\rLysbethFord,\nBIITE,2002.Reproduced withpermission.\nbythefollo wing relation:\nR=no.ofnotes found correctly\nno.ofnotes found intotal+no.ofnotes missed\nwhich, when applied toourresults, givesthese results (Ta-\nble1):\nExcerpt notes found\ncorrectfound\ntotalmissed R\nlirrga136 33 39 3 0.79\nlirrga230 23 37 7 0.53\nlirrga348 44 46 4 0.88\nlirrga421 14 17 7 0.58\nTable1.Results fordidjeridu.\nThe overall accurac y(average R)was70%. Almost\nalloftheerrors were aresult ofthenote onset detec-\ntion, rather than thefrequenc yestimation. One reason for\nthisisthatsome manynote onsets aredifﬁcult todetect\nwith anamplitude envelope type method, because anote\nonset does notnecessarily correspond toalargeincrease\ninamplitude. Another reason isduetohuman subjecti v-\nityinformulating thereference transcriptions, andalsoin\nmatching generated transcriptions totheir references.3.2.Clapsticks transcription\nWithrespect totheseparation ofclapsticks from thehar-\nmonic components, errors didindeed occur intheclassi-\nﬁcation stage. Weused themetric outlined in[15]and\nfound thatoverall, 71% ofpercussi vecomponents were\nfound correctly .Spurious percussi veclassiﬁcations were\nat31%.\nTheaccurac yofnote onset detection forclapsticks ex-\ntraction wasmeasured similarly tothatoftheclapsticks,\nbythepreceeding formula. Forallcorrectly classiﬁed\nclapstick tracks weobtain thefollo wing results (Table 2):\nExcerpt notes found\ncorrectfound\ntotalmissed R\nlirrga120 20 22 0 0.91\nlirrga28 8 8 0 1.0\nlirrga324 24 30 0 0.80\nlirrga47 7 7 0 1.0\nTable2.Results forclapsticks\nThis givesanoverall accurac yof93% forcorrectly\nclassiﬁed clapstick tracks.\n4.CONCLUSIONS ANDFUTURE WORK\nThere aremanywaysinwhich weintend toincrease the\naccurac yandscope ofoursystem. Weidentify several\nmeasures forthis.\nFirst onourlististracking inﬂection changes during\nasingle note played bythedidjeridu. Whilst thecurrent\ndidjeridu extraction method compares favourably with the\nmodel exempliﬁed byourWesternised transcriptions, we\nwish totrack theharmonic series associated with each\nonset. This poses anewproblem: howdoes onemap\nchanges inharmonic series tochanges ininﬂection? As\ndescribed inSection 3.1,itisalsothecase thatchanges in\ninﬂection, rather than largechanges inamplitude, corre-\nspond tonewnote onsets. This would therefore increase\ntheaccurac yratings ofourtranscriptions. Thecomple x-\ndomain onset detection algorithm wehaveused [1]picks\nchanges ininﬂection well, andsotheproblem remains:\nhowtoclassify onsets determined thisway,andhowtouse\nextra information todetermine when inﬂection changes\ncorrespond todeﬁnite onsets.\nFuthermore, clapsticks haveconsiderable energyinthe\nhigh frequenc ysubbands, and sowewould investigate\ntracking oftransient energyinhigh frequenc yranges [2]\ninorder toaugment note onset detection forclapsticks.\nThemorphology ofthedidjeridu often varies depend-\ningonthegeographical location inAustralia from which\nthemusic originates. This hasaneffectontheresonant\nfrequencies oftheinstrument [3].Therefore, byidentify-\ningnotes played athigher resonant frequencies, wecould\ncompute themorphology ofthedidjeridu.5.ACKNOWLEDGEMENTS\nThe authors gratefully ackno wledge thecontrib utions of\nthefollo wing people: Linda Barwick fordiscussions lead-\ningtoagreater understanding oftheethnomusicological\nandcultural aspects oftheproject, andalso forproviding\nthesource music recordings andtheir transcripts; Aaron\nCorn forcareful proofreading ofourpaper andinsightful\nsuggestions; Christian Dittmar fordiscussions leading us\ntoabetter understanding ofindependent subspace analy-\nsis.\nThis project wasfunded primarily bytheSchool of\nPhysics, University ofMelbourne, andalso partly bythe\nDepartment ofElectronic Engineering, Queen Mary ,Uni-\nversity ofLondon.\n6.REFERENCES\n[1]Chris Duxb ury,Juan-P ablo Bello, MikeDavies, and\nMark Sandler .Comple x-domain onset detection for\nmusical signals. InProceedings ofthe6thInterna-\ntionalConferenceonDigitalAudioEffects(DAFx-\n03),London, UK, September 2003.\n[2]Chris Duxb ury,Mark Sandler ,and MikeDavies.\nAhybrid approach tomusical note onset detection.\nInProceedings ofthe5thInternational Conference\nonDigitalAudioEffects(DAFx-02) ,Hamb urg,Ger-\nmany,September 2002.\n[3]Neville Fletcher .Thedidjeridu (didgeridoo). Acous-\nticsAustralia,24:11–15, 1996.\n[4]Neville Fletcher ,LloydHollenber g,John Smith, and\nJoeWolfe. Thedidjeridu andthevocal tract. InPro-\nceedingsoftheInternational Symposium onMusical\nAcoustics ,pages 87–90, Perugia, Italy,2001.\n[5]Gurritjiri Gurruwiwi. Waluka .Yothu YindiFounda-\ntion, 2001. Compact discmusic recording, featuring\nDjalu Gurruwiwi onyidaki.\n[6]Stephen W.Hainsw orth andMalcolm D.Macleod.\nAutomatic bass line transcription from polyphonic\nmusic. InProceedings oftheInternational Com-\nputerMusicConference,Havana, Cuba, September\n2001.\n[7]LloydHollenber g.The didjeridu: Lipmotion and\nlowfrequenc yharmonic generation. Australian\nJournalofPhysics ,53:835–850, 2000.\n[8]Kunio Kashino andHiroshi Murase. Music recog-\nnition using note transition conte xt.InProceedings\noftheIEEEConferenceonAcoustics, Speechand\nSignalProcessing ,volume 6,Seattle, Washington,\nUSA, May 1998.\n[9]Florian Keiler andUdo Z¨olzer .Extracting sinusoids\nfrom harmonic signals.JournalofNewMusicRe-\nsearch,30(3):243–258, 2001.[10] Anssi Klapuri. Sound onset detection byapply-\ningpsychoacoustic knowledge. InProceedings of\ntheIEEEInternational ConferenceonAcoustics,\nSpeechandSignalProcessing ,Phoenix, Arizona,\nUSA, March 1999.\n[11] Pius Luckan andClement Tchinb urrur .MarriNgarr\nchurchLirrgaSongs ,Unpublished. Musical per-\nformance recorded on10March, 1985, byChester\nStreet atWadeye,NT,Australia.\n[12] Malcolm D.Macleod. Nearly fastML estimation\noftheparameters ofrealandcomple xsingle tones\norresolv edmultiple tones.IEEETransactions on\nSignalProcessing ,46(1):141–148, January 1998.\n[13] Alice Moyle.Aboriginal SoundInstruments .Aus-\ntralian Institute ofAboriginal Studies, Canberra,\nAustralia, 1978.\n[14] Alice Moyle. The didjeridu: Alatemusical intru-\nsion.WorldArchaeology,12(3):321–331, 1981.\n[15] Christian Uhle, Christian Dittmar ,and Thomas\nSporer .Extraction ofdrum tracks from polyphonic\nmusic using independent subspace analysis. InPro-\nceedings ofthe4thInternational Symposium onIn-\ndependent Component Analysis andBlindSignal\nSeparation(ICA2003) ,Nara, Japan, April 2003."
    },
    {
        "title": "Indexing and Retrieval of Music Documents through Pattern Analysis and Data Fusion Techniques.",
        "author": [
            "Giovanna Neve",
            "Nicola Orio"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416372",
        "url": "https://doi.org/10.5281/zenodo.1416372",
        "ee": "https://zenodo.org/records/1416372/files/NeveO04.pdf",
        "abstract": "One of the challenges of music information retrieval is the automatic extraction of effective content descriptors of music documents, which can be used at indexing and at retrieval time to match queries with documents. In this pa- per it is proposed to index music documents with frequent musical patterns. A musical pattern is a sequence of fea- tures in the score that is repeated at least twice: features can regard perceptually relevant characteristics, such as rhythm, pitch, or both. Data fusion techniques are applied to merge the results obtained using different features. A set of experimental tests has been carried out on retrieval effectiveness, robustness to query errors, and dependency on query length on a collection of Beatles’ songs using a set of queries. The proposed approach gave good results, both using single features and, in particular, merging the rank lists obtained by different features with a data fusion approach.",
        "zenodo_id": 1416372,
        "dblp_key": "conf/ismir/NeveO04",
        "keywords": [
            "challenge",
            "automatic extraction",
            "content descriptors",
            "music documents",
            "indexing",
            "retrieval time",
            "queries",
            "musical patterns",
            "sequence of features",
            "frequent patterns"
        ],
        "content": "INDEXING ANDRETRIEV ALOFMUSICDOCUMENTS THROUGH\nPATTERNANALYSISANDDATAFUSIONTECHNIQ UES\nGiovannaNeve\nUniversityofPadova\nDepartment ofInformation EngineeringNicolaOrio\nUniversityofPadova\nDepartment ofInformation Engineering\nABSTRACT\nOne ofthechallenges ofmusic information retrie valis\ntheautomatic extraction ofeffectivecontent descriptors\nofmusic documents, which canbeused atindexing andat\nretrie valtime tomatch queries with documents. Inthispa-\nperitisproposed toindexmusic documents with frequent\nmusical patterns. Amusical pattern isasequence offea-\ntures inthescore thatisrepeated atleast twice: features\ncanregard perceptually relevantcharacteristics, such as\nrhythm, pitch, orboth. Data fusion techniques areapplied\ntomergetheresults obtained using different features. A\nsetofexperimental tests hasbeen carried outonretrie val\neffectiveness, robustness toquery errors, anddependenc y\nonquery length onacollection ofBeatles’ songs using a\nsetofqueries. Theproposed approach gavegood results,\nboth using single features and, inparticular ,merging the\nrank listsobtained bydifferent features with adata fusion\napproach.\n1.INTRODUCTION\nTheresearch workdescribed inthispaper focuses onin-\ndexing ofmusic documents aided atefﬁcient Music In-\nformation Retrie val(MIR). The presented methodology\nissimilar toapproaches based ontheretrie valofmusic\ndocuments insymbolic notation –MIDI, GUIDO [5],or\nSMDL –through aquery-by-e xample paradigm. There-\nsearch workonthisarea ofMIR canberoughly divided in\ntwocategories: on-line searching techniques, which com-\npute amatch between arepresentation ofthequery anda\nrepresentation ofthedocuments each time anewquery is\nsubmitted tothesystem; andindexing techniques, which\nextract off-line, from music documents, alltherelevantin-\nformation thatisneeded atretrie valtime andperform the\nmatch between query anddocuments indexes.\nBoth approaches havepositi veandnegativeaspects.\nOntheonehand, on-line search allowsforadirect mod-\neling ofquery errors byusing, forinstance, approximate\npattern matching techniques thatdealwith possible sources\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forproﬁt orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation ontheﬁrstpage.\nc\n\u00002004 Universitat Pompeu Fabra.ofmismatch, e.g.insertion and/or deletion ofnotes. This\nhigh ﬂexibility isbalanced byhigh computational cost, be-\ncause thecomple xityisatleast proportional tothesizeof\nthedocument collection and, depending onthetechnique,\ntothedocuments length. Ontheother hand, indexing\ntechniques aremore scalable tothedocument collection,\nbecause theindexﬁlecanbeefﬁciently accessed through\nhashing andthecomputational comple xitydepends onquery\nlength. Thehigh scalability isbalanced byamore difﬁcult\nextraction ofdocument content, with nontrivialproblems\narising incase ofquery errors thatmay cause acomplete\nmismatch between query anddocument indexes. Both\napproaches havegiveninteresting andpromising results.\nYet,indexing approaches need tobeinvestigated inmore\ndetail because oftheintrinsic higher computational efﬁ-\ncienc y.\nPrevious workonon-line search hasbeen carried out\nfollo wing different strate gies. Theworkpresented in[2]\nisbased ontheuseofpattern disco verytechniques, taken\nfrom computational biology ,tocompute occurrences of\nasimpliﬁed description ofthepitch contour ofthequery\ninside thecollection ofdocuments. Another approach, re-\nported in[7],applies pattern matching techniques todoc-\numents andqueries inGUIDO format, exploiting thead-\nvantages ofthisnotation instructuring information. Ap-\nproximate string matching hasbeen used by[6].Mark ov\nchains havebeen proposed in[3]tomodel asetofthemes\nthathasbeen extracted from music documents, while an\nextension tohidden Mark ovmodels hasbeen presented\nin[16]asatooltomodel possible errors insung queries.\nThe workpresented in[8]reports acomparison ofdif-\nferent approaches toon-line search, with adiscussion on\ncomputational comple xityandscalability offour different\ntechniques based ondynamic time warping.\nAnexample ofresearch workonoff-line document in-\ndexing hasbeen presented in[4].Inthatworkmelodies\nwere indexedthrough theuseofN-grams, each N-gram\nbeing asequence of \u0001pitch interv als. Experimental re-\nsults onacollection offolk songs were presented, testing\ntheeffects ofsystem parameters such asN-gram length,\nshowing good results interms ofretrie valeffectiveness,\nthough theapproach seemed notberobusttodecreases\ninquery length. Another approach todocument indexing\nhasbeen presented in[11],where indexing hasbeen car-\nriedoutbyautomatically highlighting music lexical units,\nthathavebeen called musical phrases. Differently than theprevious approach, thelength ofindexeswasnotﬁxedbut\ndepended onthemusical conte xt.That ismusical phrases\nwere computed exploiting knowledge onmusic percep-\ntion, inorder tohighlight only phrases thathadamusical\nmeaning. Phrases could under goanumber ofdifferent\nnormalization, from thecomplete information ofpitch in-\ntervalsandduration tothesimple melodic proﬁle.\nMost oftheapproaches, andtheonepresented inthis\npaper aswell, focus onthemelody ,while other music di-\nmensions, such asharmon y,timbre, orstructure, arenot\ntakeninto account. This choice may become alimita-\ntiondepending onthewaytheuser isallowed tointeract\nwith thesystem andonhispersonal knowledge onmusic\nlanguage. When thequery-by-e xample paradigm isused,\ntheeffectiveness ofasystem depends onthewayaquery\nismatched with documents: Iftheuser may express his\ninformation need through aquery-by-humming interf ace,\nthemelody isthemost likelydimension thathewilluse.\nMoreo ver,fornonexpert users, melody andrhythm (and\nlyrics) arethemore simple dimensions fordescribing their\ninformation needs.\n2.MELODIC PATTERNS FORINDEXING AND\nRETRIEVING MUSIC\nThebasic idea underlying ourworkisthatamusic docu-\nment canbeeffectivelydescribed byexcerpts ofitsmelodic\nfeatures. Themain goal then becomes theautomatic ex-\ntraction ofrelevantexcerpts from anunstructured ﬂowof\nnotes. Differently from previous workcarried outonmu-\nsical phrases extraction [13],itisproposed thatmelodic\nexcerpts, ormelodic patterns, canbecomputed bythe\nsimple analysis oftherepetitions ofsubstrings ofnote\nfeatures inside themelody; thiswaynoprior knowledge\nonmusic perception ormusic structure isrequired. The\nproposed approach issimilar totheonepresented in[17],\nwhere frequent phrases areused toindexamusic database.\nUnfortunately ,[17]focused oncomputational efﬁcienc y\nanddidnotpresent ananalysis onretrie valeffectiveness,\nandthus results cannot becompared.\n2.1.Terminology\nBefore introducing themethodology ,itcanbeuseful to\ndescribe indetail theterminology used inthefollo wing\nsections.\nAfeatureisoneofthecharacteristics that describe\nsubsequent notes inascore. Anote feature canbe:the\npitch, thepitch interv alwith theprevious note (PIT), a\nquantized PIT,theduration, theinteronset interv alwith\nthesubsequent note (IOI), theratio ofIOIwith theprevi-\nousnote, andsoon.Allthefeatures canbenormalized or\nquantized. Inourapproach, features arerelated topitch\nandrhythm that, though usually correlated, canbetreated\nindependently .Forexample, manysongs canbeguessed\nonly bytapping therhythm ofthemelody while other ones\ncanbeeasily recognized evenifplayed with notempo or\nrubato.Astring isasequence offeatures. Anysequence of\nnotes inamelody canbeconsidered astring. Itcanbe\nnoted thatstrings canbeused asrepresentati veofamelody ,\nwhich istheideaunderlying manyapproaches toMIR, but\ntheeffectiveness bywhich each string represents adocu-\nment may differ.Forinstance, itisnormally accepted that\ntheﬁrstnotes ofamelody play animportant roleinrecog-\nnition, orthatstrings thatarepart ofthemain theme or\nmotif aregood descriptors aswell. String length isan\nimportant issue: Long strings arelikelytobeeffectivede-\nscriptors, yettheymay lead toproblems when theuser is\nrequest toremember long parts ofamelody forquerying\naMIR system. Inourexperiments, strings shorter than\nthree notes havebeen discarded, because theywere not\nconsidered signiﬁcant descriptors.\nApatternisastring thatisrepeated atleast twice in\nthescore. The repetition canbeduetothepresence of\ndifferent choruses inthescore orbytheuseofthesame\nmusic material (e.g., motifs, rhythmical cells) along the\ncomposition. Each pattern isdeﬁned bythestring offea-\ntures, byitslength\u0000andbythenumber oftimes\u0001itis\nrepeated inside thescore. Allpatterns that appear only\ninside longer patterns havebeen discarded. The compu-\ntation ofpatterns canbedone automatically using well\nknownalgorithms forpattern disco very.\nPatterns canbeconsidered ascontent descriptors of\nmusic documents. Depending onthefeatures, patterns\ncarry different information about document content. Itis\npossible totakeadvantage from alternati vedescriptors by\napplying adatafusion approach. This approach isusu-\nallycarried outintheresearch area ofMeta Search En-\ngines [9],where theresults obtained bydifferent indexing\nandretrie valmethodologies arecombined –orfused –to-\ngether according toapredeﬁned weighting scheme.\nInthepresent work,three kinds offeatures areused for\nthepattern selection step –theinteronset interv al(IOI)\nnormalized tothequarter note, thepitch interv al(PIT)\ninsemitones, andboth (BTH) –andtwodata fusion ap-\nproaches areexploited –thecombination ofIOIandPIT\n(Fuse2) andthecombination ofallthefeatures (Fuse3).\n2.2.Document Indexing\nDocument indexing isamandatory step fortextual infor -\nmation retrie val.Through indexing, therelevantinfor -\nmation about acollection ofdocuments iscomputed and\nstored inaformat thatallowseasy andfastaccess atre-\ntrievaltime. Document indexing iscarried outonly when\nthecollection iscreated orupdated, when users arenotyet\naccessing thedocuments, andthen theproblems ofcom-\nputational time andefﬁcienc yareusually lessrestricti ve.\nIndexing speeds upretrie valtime because itisfaster to\nsearch foramatch inside theindexesthan inside thecom-\nplete documents.\nAccording totheterminology introduced intheprevi-\noussection, each document hasanumber ofpatterns of\ndifferent length andwith different multiplicity .Itispro-\nposed thatpatterns areeffectivedescriptors fordocumentindexing, thatisadocument canbeindexedbythepat-\nterns ofitsmelodies. The ﬁrst step ofdocument index-\ningconsists intheautomatic computation ofthepatterns\nofeach document. Aspreviously mentioned, thefeatures\nused tocompute thepatterns areIOI, PIT,andBTH. Pat-\nterncomputation iscarried outwith anad-hoc algorithm\nthatcomputes exhausti velyallthepossible patterns, and\nstores them inahash table.\nAnexhausti vepattern disco veryapproach highlights\nahigh number ofpatterns thathavelittle ornomusical\nmeaning; forinstance, apattern thatisrepeated only two\northree times inadocument islikelytobecomputed\nbychance justbecause thecombination offeatures isre-\npeated insome notes combinations. Moreo ver,some pat-\nterns related toscales, repeated notes, orsimilar musical\ngestures, arelikelytoappear inalmost alldocuments and\nhence tobepoor discriminants among documents. Ingen-\neral, thedegree bywhich apattern isagood indexmay\nvarydepending onthepattern andonthedocument. This\nisatypical situation oftextual information retrie val,where\nwords may describe adocument toadifferent extent. For\nthisreason itisproposed toapply theclassical\n\u0000\u0002\u0001\u0004\u0003 \u0005\u0007\u0006\u0007\u0001mea-\nsure[1].Adocument isdescribed byasparse array ,where\neach element isassociated toadifferent pattern inthecol-\nlection. Thevalue ofeach element isgivenbythe\n\u0000\u0002\u0001\b\u0003\t\u0005\u0007\u0006\u0007\u0001\nvalue thatis,thenumber oftimes apattern appears inthe\ndocument (the\n\u0000\u0002\u0001term) ismultiplied bytheinverseofthe\nfraction ofdocuments thatcontain thatpattern, computed\ninlogscale (the\n\u0005\u0007\u0006\u0007\u0001term).\nTheindexisbuiltasaninverted ﬁle,where each term\nofthevocab ulary isadifferent pattern inagivennotation\n(i.e., atextstring). Each entry intheinverted ﬁlecorre-\nsponds toadifferent pattern, andcanefﬁciently becom-\nputed inanexpected time \n\f\u000b\u0002\r\u000f\u000e with anhashing function.\nGiventhedifferent setsoffeatures, three inverted ﬁles are\nbuilt,respecti velyforfeatures IOI,PIT,andBTH. Inverted\nﬁles canbeefﬁciently stored inmemory ,eventually using\ncompression, andaccessed atretrie valtime [1].Thesize\noftheinverted ﬁleandtheimplementation ofthehashing\nfunction depend onthenumber ofdifferent patterns ofthe\ncomplete collection.\nItmay beuseful toﬁxthemaximum allowable pattern\nlength toimpro veindexing. Infact,itislikelythatvery\nlong patterns areduetorepetitions ofcomplete themes\ninthescore andtaking into account also them will give\naquite sparse inverted ﬁle. Moreo ver,itisunlik elythat\nauser will query thesystem singing acomplete theme.\nThese considerations suggest thatlong patterns could be\ntruncated when theyareoveragiventhreshold. Truncated\npatterns canbesubstituted bythesetoftheir subpatterns.\nThe choice ofthethreshold canbedone experimentally ,\nusing atestcollection aspresented inSection 3.2.\n2.3.QueryProcessing\nForthequery processing step, itisassumed thatusers in-\nteract with thesystem according toaquery-by-e xample\nparadigm. Inparticular ,users should beable todescribe\ntheir information needs bysinging (humming orwhistling),playing, orediting with asimple interf aceashort excerpt\nofthemelody thattheyhaveinmind. The development\nofauser interf aceisoutofthescope ofthepresent paper ,\nbutithasbeen thesubject ofprevious research byoneof\ntheauthors both onaudio query transcription [15]andon\nWebinterf aces forplaying/editing symbolic queries [12].\nFortheaims ofthiswork, itisassumed thatqueries are\ntranslated inasequence offeatures byanexternal prepro-\ncessing module.\nThe string thatrepresents thetranslated query hasto\nbeprocessed. Query processing allowsforextracting a\nnumber ofdescriptors thatcanbeused tomatch thequery\nwith potentially relevantdocuments. Inthiswork,itisas-\nsumed thataquery islikelytocontain strings thatcharac-\nterize thesearched document, either because theyappear\nveryoften inside itstheme orbecause theyarepeculiar of\nthatparticular melody .Inother words, itisassumed thata\nquery contains relevantpatterns ofthesearched document,\nwhich may haveahigh\n\u0000\u0002\u0001–frequent inside thedocument\n–and/or ahigh\n\u0005\u0007\u0006\u0007\u0001–infrequent across thecollection.\nThe automatic detection ofrelevant strings cannot be\ncarried outasfordocument indexing, because normally\nqueries aretooshort tohaverepetitions andhence tocon-\ntainpatterns. Asimple approach toextract relevantstrings,\norpotential patterns, from aquery consists incomputing\nallitspossible substrings. That is,from aquery oflength\u0010notes areautomatically extracted\n\u0010\u0012\u0011\u0014\u0013strings ofthree\nnotes, plus\n\u0010\u0015\u0011\u0017\u0016strings offour notes, andsoonuntil the\nmaximum allowable length forapattern isreached. This\napproach canbeconsidered similar toquery expansion in\ntextual information retrie val,which isknowntoincrease\nrecall attheriskoflowering precision. Ontheother hand,\nitisexpected thatmost ofthearbitrary strings ofaquery\nwillneverform arelevantpattern inside thecollection, and\nthen thenegativeeffects onprecision could bebounded.\n2.4.RankingRelevantDocuments\nAtretrie valtime, thestrings areautomatically extracted\nfrom thequery and matched with thepatterns ofeach\ndocument. The computation ofpotentially relevantdoc-\numents isperformed using theVector Space Model [1],\nwhich allowscomputing thedistance between thevector\nofstrings representing thequery andthevector ofpatterns\nrepresenting each document. Hence, foreach document\naRetrie valStatus Value (RSV) iscalculated, thehigher\ntheRSV,thecloser thedocument with thequery .Arank\nlistofpotentially relevantdocuments iscomputed from\nRSVs, obtaining three parallel rank lists according tothe\nthree features used.\nIngeneral theorderings ofdocuments intherank lists\ndiffer.Differences may beduetomanyfactors, asthedi-\nverseimportance ofrhythm andmelodic proﬁle, theeffect\noferrors inthequery ,thekind ofmelodic excerpt chosen\nforthequery .Itisexpected thatBTH ranking will give\nhigh scoring totherelevantdocuments when thequery is\nsufﬁciently long andcorrectly played, because BTH pat-\nterns areacloser representation oftheoriginal melody .On\ntheother hand, IOIandPITarerobusttoquery errors inquery documents\nextraction transcription\npatterns strings\nIOIPITBTH\nindexessearch engine data fusion\nrank BTH rank IOI\nrank PIT rank Fuse2 rank Fuse3\nFigure1.Thephases oftheproposed methodology: Indexing, retrie val,anddata fusion\nmelodic proﬁle andrhythm, respecti vely.Moreo ver,sim-\nplerepresentations asIOIandPITareexpected tobeless\nsensiti vetoquery length because ofthepossible presence\nofsubpatterns ofrelevantmotifs.\nItispossible totakeadvantage from theexistence of\ndifferent rank lists byfusing together theresults, inorder\ntogivetheuser asingle rank listwhich takesintoaccount\ntheresults ofthethree parallel approaches. Asintroduced\ninSection 2.1,thisisatypical problem ofdata fusion [9],\nwhich isusually faced bymerging therank lists obtained\nbydifferent search engines. Inthiscase, thefusion canbe\ncarried outdirectly using theRSVs, because theyareall\nbased onthesame\n\u0000\u0002\u0001 \u0003 \u0005\u0007\u0006 \u0001scheme. Inparticular ,anew\nRSV iscomputed asaweighted sum ofRSVs ofsingle\nfeatures obtaining anewrank list.Anumber oftests with\ndifferent combinations ofweights havebeen carried outto\nﬁndtheoptimal setting foraMIR system.\nThecomplete methodology isshowninFigure 1,where\nsteps undertak enatindexing time areshownontheleft,\nwhile theoperations thatareperformed atretrie valtime\nareshownontheright. From Figure 1andtheabovedis-\ncussion, itisclear thatthecomputational comple xityde-\npends onthequery length –i.e.,thenumber ofstrings that\narecomputed from thequery –while itisscalable onthe\nnumber ofdocuments. This isanimportant characteris-\ntic,because thetime needed toreply toaquery canbe\nreasonably lowalsoforlargecollections ofdocuments.\n3.EXPERIMENT ALEVALUATION\nThe evaluation hasbeen carried outonasmall testcol-\nlection according totheCranﬁeld model forinformation\nretrie val,which isused attheTextREtrie valConference\n(TREC) [18].Atestcollection consists inasetofdocu-\nments, asetofqueries, andasetofrelevance judgments\nthatmatch documents toqueries. Thecreation ofacom-\nmon background forevaluation isstill anopen issue in\ntheMusic Information Retrie valcommunity [14],hence\nwecreated asmall testcollection from scratch. Anum-\nberofexperimental tests hasbeen carried outtoevaluatetheeffectiveness ofthemethodology andtherobustness to\ncommon problems thatmay affectretrie valeffectiveness,\naserrors inthequeries andveryshort queries.\n3.1.TheTestCollection\nAsmall testcollection ofpopular music hasbeen cre-\nated using\r\n\u0001\u0003\u0002Beatles’ song inMIDI format downloaded\nfrom theWeb.Asforanytestcollection, documents may\ncontain errors. Inapreprocessing step, thechannels con-\ntaining themelody havebeen extracted automatically and\nthenote durations havebeen normalized; incase ofpoly-\nphonic scores, thehighest pitch hasbeen chosen aspart\nofthemelody .After preprocessing, thecollection con-\ntained \r\n\u0001\u0004\u0002complete melodies with anaverage length of\u0013\u0006\u0005\u0007\u0005notes, ranging from \b\u0007\toftheshortest melody to \n\f\u000b\n\u0005\nofthelongest. Evenifanumber ofapproaches forper-\nforming automatic theme extraction hasbeen already pro-\nposed intheliterature [10],ourmethodology relies onin-\ndexing ofcomplete melodies, because repetitions ofcho-\nruses andverses canbetakenintoaccount bythe\n\u0000\u0002\u0001 \u0003 \u0005\u0007\u0006\u0007\u0001\nmeasure.\nAsetof\n\u0005\n\u0001queries hasbeen created byrandomly se-\nlecting\n\u0013\n\u0001themes inthedataset andusing theﬁrstnotes\nofthechorus andoftherefrain. The initial note andthe\nlength ofeach query were chosen tohaverecognizable\nmotifs thatcould beconsidered representati veofrealusers’\nqueries. Thequeries hadanaverage length of\t\u000e\r\n\u0002\nnotes,\nranging from\n\u0005to\n\u0013\rnotes. Only thetheme from which\nthequery wastakenwasconsidered asrelevant.Using this\ninitial setofcorrect queries, analternati vesethasbeen\ncreated byadding errors onpitch, duration, andboth, ob-\ntaining anewsetof \r\n\u0013\n\u0001queries. Asimple error model has\nbeen applied, because errors were uniformly distrib uted\nalong thenotes inthequeries, with aprobability ofabout\r\n\u0016\r\n\u0016\u0003\u000f.Asformanyapproaches toapproximate string\nmatching, anerror canbeconsidered theresult ofadele-\ntionandaninsertion, thus these alternati vesources ofer-\nrors havenotbeen explicitly modeled. Testsonrobust-\nness toquery length were carried outbyautomaticallyshortening theinitial queries byanincreasing percentage,\ndisre garding thefactthat query would notsound musi-\ncal.Inthisway, \r \u000b\n\u0001more queries with decreasing length\nhavebeen automatically generated. Forallthemodiﬁed\nqueries, only thetheme ofinitial query wasconsidered as\nrelevant. Inthefollo wing, wewillrefer totheonly rele-\nvantdocument with thetermr-doc foralltheexperiments.\n3.2.Truncation ofPatterns\nAlltheexperimental analyses, whose results areshownin\nthefollo wing sections, havebeen carried outafter truncat-\ningpatterns longer than agiventhreshold\n\u0000.When apat-\ntern\n\u0001\u0001\u0003\u0002\r \r \r\n\u0001\u0005\u0004\u0007\u0006hadalength of\u0000\t\b\n\u0000,ithasbeen replaced\n(intheindexing step) byallitssubpatterns ofexact length\u0000,thatisthe \u0000\n\u0011\n\u0000\u000b\n\rsubpatterns\n\u0001\u0001\f\u0002\r \r \r\n\u0001\u000e\r\u000f\u0006,\n\u0001\u0001\u0011\u0010\r \r \r\n\u0001\u000e\r\u0013\u0012\u0014\u0002\u0015\u0006,\nandsoonuntil\n\u0001\u0001\u0005\u0004\u0017\u0016\u0018\r\r \r \r\n\u0001\u0005\u0004\u0007\u0006,where some ofthesubpat-\nterns may bealready extracted, because theywere partof\nother motifs.\nWiththeaimofcomputing theoptimal threshold for\nthetestcollection, ﬁvedifferent thresholds havebeen tested,\nrespecti vely \n,\n\u0002, \r\n\u0001, \r \n,and\n\u0013\n\u0001notes. Results interms\nofaverageprecision [1]arereported inFigure 2forthe\nmean ofthethree single features andthetwodata fusion\napproaches, using the\n\u0005\n\u0001correct queries. Theretrie valef-\nfectiveness decreased with high values ofthethreshold,\nmeaning thatacompact representation ofpatterns canbe\nmore effectivethan longer ones. This result isconsistent\nwith theﬁndings reported in[4].The average precision\nwasapproximately constant when thresholds higher than\r \n\u0011 \u0013\n\u0001notes were applied, probably because thenumber\nofdifferent patterns longer than\n\u0013\n\u0001notes islessthan \b\n\u000f\nandwith alowvalue of \u0001.Theuseofshort patterns can\nbeauseful waytocontrol theincrease oftheindexwhen\nnewdocuments areadded tothecollection. Due tosimple\ncombinatorial reasons, thenumber ofdifferent patterns\nisbounded bythepattern length; ontheother hand, the\nuseofshort patterns hasthedrawback ofahigher number\nofpatterns thatareincommon among documents, which\nmay lowerprecision.\nItisinteresting tonote thatdatafusion approaches gave\nconsistently better results than single approaches, howcan\nbeseen both from thecomparison ofabsolute values and\nofcurveslopes inFigure 2.This beha viorhasbeen found\ninallourexperiments, which arepresented inthefollo w-\ningsections, where results areshownonly for\n\u0000\u001a\u0019\n.\n3.3.RetrievalEffectiveness\nTheﬁrstdetailed analysis regarded theretrie valeffective-\nness with thesetof\n\u0005\n\u0001correct queries. Results areshown\ninTable1,where theaverage precision (Av.Prec.), theper-\ncentage queries thatgavether-doc within theﬁrst \u001bpo-\nsitions (with \u001b\u001d\u001c\u001f\u001e \r\f \n\u0016 \n! \r\n\u0001!\"),andtheones thatdidnot\ngivether-doc atall(“not found”), arereported asrepre-\nsentati vemeasures. Asitcanbeseen, IOIgavethepoor-\nestresults, eveniffor \t\n\u0001\u000fofthequeries ther-doc were\namong theﬁrstthree retrie ved.Thehighest average preci-\nsion using asingle feature wasobtained byBTH, with theS=5 S=7 S=10 S=15 S=200.60.650.70.750.80.850.90.951\nsingle features\ndata fusion    \nFigure2.Average precision forthemean ofsingle fea-\ntures (solid line) anddata fusion (dashed line) with differ-\nenttruncation thresholds\ndrawback ofanon-of fbeha vior: either ther-doc istheﬁrst\nretrie vedoritisnotretrie vedatall(\n\u0013\r \n\u000fofthequeries).\nPITgavegood results, with allthequeries thatfound the\nr-doc among theﬁrstthree documents.\nIOI PIT BTH Fuse2 Fuse3\nAv.Prec. 0.74 0.93 0.98 0.96 0.98\u0019\r 57.5 87.5 97.5 92.5 95.0#\u001690.0 100 97.5 100 100#\n 95.0 100 97.5 100 100#\r\n\u000197.5 100 97.5 100 100\nnotfound 0 0 2.5 0 0\nTable1.Retrie valeffectiveness forcorrect queries\nFuse2 gaveanimpro vement inrespect totheseparate\nfeatures –IOIandPIT –with anaverage precision of\u0001\r \t\f\b,hence with values comparable toBTH andwithout\nthedrawback ofnotretrie ving ther-doc (inallcases the\nr-doc isamong theﬁrst three retrie ved). Itcould beex-\npected thatadding BTH inthedata fusion would notgive\nfurther impro vements, since BTH isalready acombina-\ntionoftheﬁrsttwo.ThesetofBTH patterns isasubset\noftheunion ofsetofIOIandPITpatterns, while itcan\nbeshownthat setBTH includes theintersection ofsets\nIOIandPIT,because ofthechoice ofnotconsidering sub-\npatterns that havethesame multiplicity oflonger ones.\nGiventhese considerations, itisclear thatBTH does not\nintroduce newpatterns inrespect toIOIandPIT.Yet,as\ncanbeseen from column labeled with Fuse3 inTable 1,\ntheuseofallthethree features allowed forreducing the\ndrawbacks ofthethree single rankings. This result can\nbeexplained considering thatBTH haddifferent\n\u0000\u0002\u0001 \u0003 \u0005 \u0006\u0007\u0001\nscores, which were someho wmore selecti vethan simple\nIOIandPITandwhich gaveaveryhigh\n\u0000\u0002\u0001 \u0003 \u0005\u0007\u0006 \u0001score to\nther-doc incase ofagood match.\nThe best results forFuse2 andFuse3 havebeen ob-\ntained assigning equal weights tothesingle ranks. When\nthe\n\u0000\u0002\u0001 \u0003 \u0005\u0007\u0006 \u0001scores haddifferent weights animpro vement\nwasstillobserv edinrespect tosingle rankings, though toa\nminor extent. Forthisreason, results forFuse2 andFuse3arepresented only when equal weights areassigned.\n3.4.RobustnesstoErrorsintheQueries\nUsers arelikelytoexpress their information needs inan\nimprecise manner .Thequery-by-e xample paradigm iser-\nrorprone because theexample provided bytheuser isnor-\nmally anapproximation oftherealinformation need. In\nparticular ,when theuser isaskedtosing anexcerpt ofthe\nsearched document, errors canbeduetoimprecise recall\nofthemelody ,problems intuning, tempo ﬂuctuations, and\ningeneral alltheproblems that untrained singers have.\nMoreo ver,transcription algorithms may introduce addi-\ntional errors inpitch detection andinmelody segmenta-\ntion. Therobustness toerrors hasbeen tested onanexper-\nimental setup. Since indexing iscarried outonmelodic\ncontour andonrhythm patterns, theerrors thatmay af-\nfecttheretrie valeffectiveness regard thepresence ofnotes\nwith awrong pitch andawrong duration. Asmentioned\ninSection 3.1,asetofqueries with automatically added\nerrors hasbeen generated inorder totesttherobustness of\ntheapproach inacontrolled environment.\nTheresults obtained forqueries with errors intherhythm\narepresented inTable 2.Asexpected, theperformances\nofIOIdropped, with adecrease oftheaverage precision –\nfrom\n\u0001\r\n\u0002\u0005to\n\u0001\r \n \r–andaconsiderable amount ofqueries\nthatdidnotretrie vether-doc. The same considerations\napply toBTH, with anevenbigger drop intheperfor -\nmances –average precision at\n\u0001\r\n\u0002\u0007\u0002andmore than aﬁfth\nofthequeries thatdidnotretrie vether-doc. Asexpected,\nPITwasinsensible tothiskind oferror .Itisinteresting\ntonote thatdata fusion allowed forcompensating thede-\ncreases inperformances ofsingle ranks, giving forboth\nFuse2 andFuse3 anaverage precision equal totheone\nobtained without errors.\nIOI PIT BTH Fuse2 Fuse3\nAv.Prec. 0.51 0.93 0.77 0.97 0.98\u0019\r 35.0 87.5 75.0 95.0 97.5#\u001665.0 100 77.5 100 100#\n 70.0 100 77.5 100 100#\r\n\u000172.5 100 77.5 100 100\nnotfound 17.5 0 22.5 0 0\nTable2.Results forqueries with errors inrhythm\nTheresults obtained forqueries with errors inpitch are\npresented inTable 3.Dually totheprevious case, IOIis\nnotsensible tothiskind oferrors, while PIT andBTH\nhadadrop inperformances, both interms ofaverage pre-\ncision andinthepercentage ofqueries that didnotre-\ntrievether-doc. Also Fuse2 showed anegativetrend in\nretrie valeffectiveness, with themain characteristics that\nther-doc isretrie vedmore seldom astheﬁrstdocument –\nfrom \t\n\u0013\u0007\u000fto \b\n\u0001\r\n\u0001\u000f–while itisstillretrie vedwithin the\nﬁrst\n\u0016documents inmost ofthecases. Fuse3 showedtobe\nparticularly robustalso tothiskind oferrors, with almostexactly thesame retrie valeffectiveness obtained with cor-\nrectqueries.\nIOI PIT BTH Fuse2 Fuse3\nAv.Prec. 0.74 0.65 0.71 0.88 0.99\u0019\r 57.5 60.0 67.5 80.0 97.5#\u001690.0 67.5 72.5 97.5 100#\n 95.0 70.0 72.5 100 100#\r\n\u000197.5 75.0 72.5 100 100\nnotfound 0 20.0 27.5 0 0\nTable3.Results forqueries with errors inpitch\nTheresults obtained forqueries with errors both inpitch\nandinduration arepresented inTable 4.The twotypes\noferror hadindependent uniform distrib utions overthe\nquery notes. Asexpected, performances ofIOIandPIT\narecompletely comparable with theones presented inTa-\nble2andTable3respecti vely,while performances ofBTH\nhaveaconsiderable drop downto\n\u0001\r\n\u0005\r\n\u000f.Also Fuse2 and\nFuse3 showed adecrease interms ofaverage precision,\nwhich inboth cases isabout\r\n\u0001\u000fhigher than thebestav-\nerage precision ofsingle features. Ithastobenoted that\nthiskind oferrors affects also thepercentage ofqueries\nthatdidnotretrie vether-doc; moreo ver,data fusion ap-\nproaches allowed forobtaining apercentage smaller than\ntheonegivenbythebestofthesingle features.\nIOI PIT BTH Fuse2 Fuse3\nAv.Prec. 0.51 0.65 0.41 0.74 0.75\u0019\r 35.0 60.0 37.5 67.5 67.5#\u001665.0 67.5 42.5 77.5 80.0#\n 70.0 70.0 42.5 82.5 85.0#\r\n\u000172.5 75.0 42.5 85.0 85.0\nnotfound 17.5 20.0 60.0 12.5 12.5\nTable4.Results forqueries with errors inboth pitch and\nduration\nAsitcanbeseen from these results, Fuse3 gaveacon-\nsiderable impro vement inrespect tothesingle rankings\ncontrib ution. Aquery-by-query analysis showed thatthis\nbeha viorisduetothefactthatthesum of\n\u0000\u0002\u0001 \u0003 \u0005 \u0006\u0007\u0001scores of\nthesingle features gavealwaysanewranking where the\nr-doc wasatthesame levelofthebestofthethree separate\nranks; thatis,ifoneofthethree gavether-doc asthemost\nrelevantdocument, also Fuse3 hadther-doc inﬁrstposi-\ntion. Moreo ver,forsome queries, thefused rank gavethe\nr-doc atﬁrstposition evenifnone ofthethree single ranks\nhadther-doc asthemost relevantdocument. These im-\nprovements canbeexplained bytwofactors: First, when\nther-doc wasretrie vedattopposition byoneofthefea-\ntures, ithadaveryhigh\n\u0000\u0002\u0001 \u0003 \u0005\u0007\u0006 \u0001score thatgaveanim-\nportant contrib ution totheﬁnal rank; Second, ther-doc\nwasoften retrie vedwith ahigh rank bytwoorthree ofthe\nfeatures, while ingeneral other documents were notcon-\nsidered asrelevantbymore than onefeature. Similar con-\nsiderations apply ,though ataminor extent, alsotoFuse2.Thesmall increment ofaverage precision forqueries with\nerrors –Fuse2 inTable2 andFuse3 inTable 3–wasdue\ntothechange ofasingle document from thesecond tothe\nﬁrstposition, andcanbeconsidered negligible.\nThese results canbecompared with other approaches\napplied totestcollections ofsimilar size. Forinstance,\n[16]tested aHMM-based indexer,obtaining\n\u0005\r\f\r\n\u0002\u000fof\ntimes ther-doc hadthehighest rank. Inthesame paper ,a\nsimple string matcher isused asabaseline, giving avalue\nof \r \u000b\u000e\r\n\u0002\u000fforthesame experiment. Theapproaches based\nonDynamic TimeWarping [8]gave,inthebest case, the\n\u0005\r\n\u0001\u000foftimes ofther-doc retrie vedattoprank. There-\nsults presented in[3]showedthatin \u000b\n\u0001\r\n\u0001\u000fofthecases the\nr-doc wasthenearest neighbor ofthequery .Incarrying\noutthecomparison with ourapproach, ithastobeconsid-\nered thatallofthisprevious workused realqueries, whose\neffectmay decrease signiﬁcantly system performances.\n3.5.Dependency toQueryLength\nAﬁnal analysis hasbeen carried outontheeffects of\nquery length totheretrie valeffectiveness. Itisknown\nthatusers ofsearch engines donotexpress their informa-\ntion needs using much information. The community of\ninformation retrie valhadtofacetheproblems ofﬁnding\nrelevantinformation also with vague orshort queries. To\nsome extent, asimilar problem applies toMIR because\nusers may notremember long excerpts ofthemusic doc-\numents theyarelooking for.Moreo ver,untrained singers\nmay notliketosing foralong time asong thattheyprob-\nably donotknowverywell. The effects ofquery length\nonaMIR system should then beinvestigated.\nTestsonthedependenc ytoquery length havebeen car-\nried outonasetofqueries thatwere obtained from the\noriginal setofqueries byshortening thenumber ofnotes\nfrom \t\n\u0001\u000fto \u000b\n\u0001\u000foftheir original lengths. The average\nlength, itsstandard deviation (STD), andtheminimum\nandmaximum lengths foreach newgroup ofqueries is\nreported inTable 5.Withthisapproach, queries may be-\ncome veryshort, forinstance aquery oftwonotes cannot\nretrie veanydocument because patterns shorter than three\nnotes arenottakenintoaccount.\nFraction \r\n\u0001\u0007\u0001\u000f\t\n\u0001\u000f\b\n\u0001\u000f\n\u0002 \u0001\u000f\u000b\n\u0001\u000f\nMean 9.8 8.8 7.8 6.9 5.8\nSTD 3.5 3.3 2.8 2.5 2.1\nMin 4 4 3 3 2\nMax 21 19 17 15 13\nTable5.Query lengths when theoriginal queries were re-\nduced toadecreasing percentage oftheir number ofnotes\nResults onretrie valeffectiveness with increasingly shorter\nqueries aredepicted inFigure 3,where theaverage preci-\nsion isreported forthethree single features andthetwo\ndata fusions. Asitcanbeseen from Figure 3,there was\nageneral decrease ofperformances. The drops inaver-\nageprecisions were accompanied byanincreasing per-IOI PIT BTH FUSE2 FUSE30.50.550.60.650.70.750.80.850.90.951\n100%\n90% \n80% \n70% \n60% \nFigure3.Average precision with different query lengths\ncentage ofqueries thatdidnotretrie vether-doc, which\nintheworstcases, BTH andIOIforareduction to \u000b\n\u0001\u000f,\nreached\n\u0013\n\u0002\r \n\u000fand\n\u0013 \u0013\r \n\u000frespecti vely.\nConsistently with previous results, Fuse3 gavethebest\nperformances andshowed ahigher robustness todecrease\ninquery length. Thetrend forFuse3 alone isreported in\nTable 6.Also inthiscase results showed thatthedata fu-\nsion approach wasenough robusttochanges intheinitial\nqueries. Asmentioned inSection 3.1,each initial query\nhasbeen created selecting anumber ofnotes thatallowed\ntorecognize thetheme byahuman listener .Moreo ver,\neach query wasmade byoneormore musical phrases –or\nmusical gestures ormotifs –considering thatauser would\nnotstop singing hisquery atanynote, butwould endthe\nquery inaposition thathavea“musical sense”. Forthis\nreason, tests onquery length cangiveonly ageneral indi-\ncation onpossible changes inretrie valeffectiveness. An\nanalysis oftheeffectofquery length hasbeen proposed\nin[2],where itisestimated thatintheworstcase auser\nshould sing upto \r \bnotes touniquely identify amusic\ndocument, eventhough thisnumber canbereduced when\nonly melodies areindexed.\nFraction \r\n\u0001\u0007\u0001\u000f\t\n\u0001\u000f\b\n\u0001\u000f\n\u0002 \u0001\u000f\u000b\n\u0001\u000f\nAv.Prec. 0.98 0.96 0.93 0.87 0.74\u0019\r 95.0 92.5 90.0 82.5 65.0#\u0016100 97.5 95.0 90.0 80.0#\n 100 100 97.5 92.5 85.0#\r\n\u0001100 100 97.5 95.0 87.5\nnotfound 0 0 2.5 2.5 7.5\nTable6.Retrie valeffectiveness forFuse3 when query\nlengths arereduced bydifferent fractions\n4.CONCLUSIONS\nAmethodology forindexing andretrie ving music docu-\nments insymbolic format hasbeen presented. Themethod-ology isbased ontheautomatic extraction ofpatterns,\ncomputed from information onrhythm, pitch, andthecom-\nbination ofthetwo.Data fusion techniques havebeen\napplied toimpro veretrie valeffectiveness. The approach\ncanbeextended including different features, forinstance\nbased onthequantization ofpitch contour andnote du-\nrations, thatcould bemore robusttoquery errors. Addi-\ntional features canreplace theones thathavebeen tested\ninthepresent work,orcanbeadded tothedata fusion.\nThemethodology hasbeen tested onasmall testcol-\nlection, giving encouraging results, evenifastandard test\ncollection ofdocuments andrealqueries would beadvis-\nable tocompare these results with theones obtained with\nother techniques. Though results need tobeconﬁrmed by\nmore extensi vetestonlargerdatabases, there aresome\ncharacteristics that arelikelytoapply ingeneral. Data\nfusion techniques impro vetheperformance ofamusic in-\nformation retrie valsystem. Forinstance, thefusion oftwo\nseparate features (Fuse2) seemed tobemore robusttoer-\nrorsthan thecombination ofthetwo(BTH). Other results\nthat canbegeneralized regard thehigher performances\ngivenbyshort patterns compared tolonger ones.\nThebestresults havebeen obtained byfusing three dif-\nferent indexing schemes. Yetsimple indexing, likethe\nonepurely based onIOI, showed tobeenough effective\ntodevelop aquery-by-tapping MIR system, inparticu-\nlarformusic genres inwhich therhythm ismore im-\nportant, oreasier toremember ,than melody .Experimen-\ntaltests showed also thatretrie valeffectiveness obtained\nwith pitch information, canbeimpro vedusing rhythmic\ninformation, especially inthecase ofshort orerror prone\nqueries.\n5.REFERENCES\n[1]Baeza-Y ates, R.,Ribeiro-Neto, B.ModernIn-\nformation Retrieval,ACM Press, NewYork,\n1999.\n[2]Bainbridge, D.,Nevill-Manning, C.G.,Wit-\nten,I.H.,Smith, L.A.,McNab, R.“Towards\naDigital Library ofPopular Music”,Proc.of\nthe4thACMConferenceonDigitalLibraries ,\nBerkle y,USA, pp.161–169, 1999.\n[3]Birmingham, W.P.,etal.“MUSAR T:Music\nRetrie valViaAural Queries”, Proc.oftheIS-\nMIR,Bloomington, USA, pp.73–82, 2001.\n[4]Downie, S.,Nelson, M.“Evaluation ofaSim-\npleandEffectiveMusic Information Retrie val\nMethod”, Proc.oftheACM-SIGIR Confer-\nence,Athens, Greece, pp.73–80, 2000.\n[5]GUIDO The GUIDO Music Notation For-\nmat Homepage, http://www.salieri.\norg/guido/ ,visited onJuly 2004.\n[6]Haus, G.,Pollastri, E.“AMultimodal Frame-\nwork forMusic Inputs”, ACMMultimedia2000Conference,Plymouth, USA, pp.282–\n284, 2000.\n[7]Hoos, H.H., Renz, K., G¨org, M.\n“GUIDO/MIR –anExperimental Musi-\ncalInformation Retrie valSystem Based on\nGUIDO Music Notation”, Proc.oftheISMIR ,\nBloomington, USA, pp.41–50, 2001.\n[8]Hu,N.,Dannenber g,R.B. “AComparison of\nMelodic Database Retrie valTechniques Using\nSung Queries”, Proc.oftheACM/IEEE JCDL ,\nPortland, USA, pp.301-307, 2002.\n[9]Lee, J.H. “Analysis ofMultiple Evidence\nCombination”, Proc.ofACM-SIGIR Confer-\nence,Philadelphia, USA, pp.267–275, 1997.\n[10] Meek, C.,Birmingham, W.“Automatic The-\nmatic Extractor”, JournalofIntelligentInfor-\nmationSystems ,Kluwer Academic Publishers,\nVol.21(1), pp.9–33, 2003.\n[11] Melucci, M.,Orio, N.“Musical Information\nRetrie valUsing Melodic Surface”,Proc.of4th\nACMConferenceonDigitalLibraries ,Berk e-\nley,USA, pp.152–160, 1999.\n[12] Melucci, M.,Orio, N.“SMILE: ASystem for\nContent-based Music Information Retrie val\nEnvironments”, Proc.ofInternational Confer-\nenceRIAO,Paris, France, pp.231-246, 2000.\n[13] Melucci, M.,Orio, N.“Evaluating Automatic\nMelody Segmentation Aimed atMusic Infor -\nmation Retrie val”,Proc.oftheACM/IEEE\nJCDL ,Portland, USA, pp.310-311, 2002.\n[14] The MIR/MDL Evaluation Project White Pa-\nperCollection, http://music- ir.org/\nevaluation/wp.html ,visited onJuly\n2004.\n[15] Orio, N.,Sisti Sette, M.“AHMM-Based Pitch\nTrackerforAudio Queries”, Proc.oftheIS-\nMIR,Baltimore, USA, pp.249-250, 2003.\n[16] Shifrin, J.,Pardo, B.,Meek, C.,Birmingham,\nW.“HMM-Based Musical Query Retrie val”,\nProc.oftheACM/IEEE JCDL ,Portland, USA,\npp.295-300, 2002.\n[17] Pienim ¨aki, A.“Inde xing Music Database\nUsing Automatic Extraction ofFrequent\nPhrases”, Proc.oftheISMIR ,Paris, France,\npp.25–30, 2002.\n[18] TREC. Home page ofTextREtrie valConfer -\nence,http://trec.nist.gov/ ,visited\nonJuly 2004."
    },
    {
        "title": "Automatic Detection Of Vocal Segments In Popular Songs.",
        "author": [
            "Tin Lay Nwe",
            "Ye Wang 0007"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417846",
        "url": "https://doi.org/10.5281/zenodo.1417846",
        "ee": "https://zenodo.org/records/1417846/files/NweW04.pdf",
        "abstract": "This paper presents a technique for the automatic classification of vocal and non-vocal regions in an acoustic musical signal. The proposed technique uses acoustic features which are suitable to distinguish vocal and non-vocal signals. We employ the Hidden Markov Model (HMM) classifier for vocal and non-vocal classification. In contrast to conventional HMM training methods which employ one model for each class, we create an HMM model space (multi-model HMMs) for segmentation with improved accuracy. In addition, we employ an automatic bootstrapping process which adapts the test song’s own models for better classification accuracy. Experimental evaluations conducted on a database of 20 popular music songs show the validity of the proposed approach.",
        "zenodo_id": 1417846,
        "dblp_key": "conf/ismir/NweW04",
        "keywords": [
            "Automatic",
            "classification",
            "vocal",
            "regions",
            "acoustic",
            "musical",
            "signal",
            "Hidden",
            "Markov",
            "Model"
        ],
        "content": "AUTOMATIC DETECTION OF VOCAL SEGMENTS IN \nPOPULAR SONGS\nTin Lay Nwe*Ye Wang \nSchool of Computing \nNational University of Singapore \n3 Science Drive 2,Singapore 117543 \ntlnma@i2r.a-star.edu.sg  School of Computing \nNational University of Singapore \n3 Science Drive 2,Singapore 117543 \nwangye@comp.nus.edu.sg  \n \n* Tin Lay Nwe is currently with I2R, Singapore ABSTRACT \nThis paper presents a technique for the automatic \nclassification of vocal and non-vocal regions in an \nacoustic musical signal. The proposed technique uses \nacoustic features which are suitable to distinguish vocal \nand non-vocal signals. We employ the Hidden Markov \nModel (HMM) classifier for vocal and non-vocal classification. In contrast to conventional HMM \ntraining methods which em ploy one model for each \nclass, we create an HMM model space (multi-model HMMs) for segmentation with improved accuracy. In \naddition, we employ an automatic bootstrapping process which adapts the test song’s own models for \nbetter classification accuracy. Experimental evaluations \nconducted on a database of 20 popular music songs show the validity of the proposed approach. \n \n1. INTRODUCTION \nRapid progress in computer and Internet technology \nhas enabled the circulation of large amounts of music \ndata on the Internet. With the immense and growing \nbody of music data, automatic analysis of song content \nis important for music retrieval and many other \napplications. The singing voice (vocal) is one of the most important characteristics of music [1]. It is still a \nchallenge to detect the vocal segments within a song \nautomatically. \n \nThe problem of vocal det ection can be stated as \nfollows: given a song, classify each segment of the \nsong in terms of whether it is pure instrumental \n(referred to as a non-vocal segment in this paper) or a \nmixture of vocals with/without background \ninstrumental (referred to as the vocal segment). \n \nThe basic procedure for any vocals /non-vocals segmentation system includes the extraction of feature \nparameters from audio signals with a time resolution constrained by the analysis window length.  Then, \nsegments of the song are classified as vocal or non-\nvocal using a statistical classifier or a relatively simple threshold method.  \n A large number of features have been proposed to represent audio signals. Some of the methods originate \nfrom the area of speech r ecognition. These include \nMel Frequency Cepstral Coefficients (MFCC) [1, 2], Linear Prediction Coeffici ents (LPC) [1, 2, 3], \nperceptual linear prediction co efficients [4, 5], energy \nfunction [6] and the average zero-crossing rate [6]. In \naddition, features that have  been used in the area of \nmusic analysis are spectral flux [6], relative subband energy [1] and features th at can differentiate the \nharmonic structure of music signals [3, 6, 7, 8]. Zhang [6] mentioned that voice signal tends to have a higher \nrate of change than instrume ntal music, and the start of \nvocals can be indicated by  the appearance of high \npeaks in the spectral flux valu e. Kim [8] stated that the \nstraightforward method to detect vocals is to note the \nenergy within the frequencies bounded by the range of \nvocal energy. Collectively, the studies [3, 6, 7, 8] \nstated that features that  can measure the harmonic \ncontent of the music signal are important for detecting vocals in a song. To measure harmonicity, the \nfrequency range of 200Hz ~2 KHz is considered in [8]. \nThe highest frequency usually considered for the analysis of normal speech is 3 kHz [9]. Due to the fact \nthat the harmonic content of vocals is higher than normal speech [6], the frequencies which are higher \nthan 3 kHz are important to take into consideration for \nvocals analysis. Based on these studies, features that \ncan capture the harmonic cont ent and spectral structure \nof the audio seem to be suitable features for vocal \ndetection. \n \nEarly audio segmentation algorithms such as [16] are specifically designed for speech signals. These \nalgorithms detect the several acoustic events such as \nspeaker changing points in the audio. More recent \nworks often use pattern classifiers such as HMM, Neural Network (NN), Support Vector Machine \n(SVM) for song segmentation [1, 2, 4, 5]. In those \nstudies use pattern classifiers such as HMM, Neural Network (NN), Support Vector Machine (SVM) for \nsong segmentation [1, 2, 4, 5]. In those studies, a \nstatistical model for each of the vocal or non-vocal \nclass was created using entire songs in the training \ndata. However, popular songs usually have a structure comprising of intro, verse, chorus, bridge and outro,   \n \nand di fferent  sect ions are of  different  signal strengt hs \n[10]. The si gnal strengt h of t he chorus section is \nusual ly much higher than the intro or verse sect ion. \nTherefore, st atistical models of vocal  and non-vocal  \nclasses should be built based on the structure of the \nsong.  The m ethod in [5] uses the classifier of a speech \nrecognizer trained on norm al speech to detect speech-\nlike sounds of m usic. However, t here are si gnificant \ndifferences between singing and speech signals. Since \nsinging voice is a rel atively poor m atch to norm al \nspeech, it could be m ore effective if we use singing \nvoice instead of speech for statistical m odelling. \nTzanet akis [1] used a boot strappi ng process for t he \nidentification of vocal  segm ents. A sm all random  \nsampling of the song was annot ated by  the user and \nthese sam ples were used t o train the song-speci fic \nvocal  and non-vocal  models. Thi s approach requi res \nmanual annot ation for every  song i t processes and is \ntherefo re not fully au tomatic. Furt herm ore, si nce onl y \na sm all num ber of sam ples can be annot ated by  user, i t \naffects th e quality o f the train ing data. \n \nTaking the existing research a step further, our focus \nhere is to  construct a statistical classifier with \nparam etric m odels that learn the specific vocal \ncharact eristics of a song wi thout the need for m anual \nannotation. In addition, we employ the multi-m odel \nHMM (MM- HMM)  training approach to tackle the \nintra-song and i nter-song vari ations for improved \nclassification perform ance. Ou r approach consi sts of \ntwo steps. First, MM-HMM is trained usi ng the vocal  \nand non-vocal  segm ents of songs from  a training \ndatabase. Then, the test song i s segm ented and \nclassified  using MM-HMM. Fo llowing that, the first \nclassification resul t of the test song is used to train its \nown vocal  and non-vocal  boot strapped HM M models. \nFinally, the song i s segm ented agai n using its own \nmodels.  \n \nThe rest of the paper i s organi zed as fol lows. The \nprocess of feat ure ext raction from  an audi o signal is \npresented in Sectio n 2. Details o f the MM-HMM \nclassifier and t he boot strappi ng process are given in \nSection 3. The song dat abase used i n the experi ments \nis descri bed i n Sect ion 4. The experi ment set-up and \nresul ts are given in Section 5. Sect ion 6 concl udes t he \npaper. \n2. ACOUSTIC FEATURES \nOur t echni que of feat ure ext raction is based on sub-\nband processi ng that uses t he Log Frequency  Power \nCoefficients (LFPC ) to provi de an i ndication of t he \nenergy  distribution am ong subbands. \n \nA di gital waveform  is convert ed i nto an acoust ic \nfeature vector for classifi cation. For high accuracy in \nvocals detection, the featur es su itable fo r the task  \nshoul d be selected. We assum e that the spect ral \ncharacteristics of different segm ents (pure vocals, vocals with  instrumental and pure instrum ental) are \ndifferent . If vocal s begi n whi le instrumental is going \non, a sudden i ncrease i n the energy  level of the audi o \nsignal is observed [6] . Ther efore, we extract feature \nparam eters based on t he distribution of energy  in \ndifferent  frequency  bands i n the range from  130Hz to \n16 kHz. We use these p arameters to  facilitate th e \nclassification of vocal  and non-vocal  segm ents. \n \nA music signal is divided i nto fram es of 20 m s in \nlength with  a 13ms overlap. Each  fram e is multiplied \nwith a ham ming wi ndow t o minimize signal \ndiscontinuities at the end of each fram e, and then, fast \nFouri er Transform  (FFT) i s com puted. Each audio \nfram e is passed through a bank of 12 bandpass filters \nspaced logarithm ically from  130Hz to 16 kHz. Figure 1 \nis a diagram matic representation of  12 subband filters. \n \n \n130Hz  1.3kHz 4 kHz 16 kHz 8 kHz \n \n               Figu re 1. Subband frequency  divisions\n \nSubband-based Log Frequency  Power C oefficients \n(LFPC ) [11] are t hen com puted usi ng Equat ions (1) \nand (2).  \n()∑+\n−==2\n22)( )(m\nm\nm\nmbf\nbfkt t kX mS ,                 (1) 12,...,2,1=m\n \nwhere,  is th e k )(kXtth spect ral com ponent  of t he \nhamming wi ndowed si gnal,  is the fram e num ber, \nis the out put of the  subband, and   \nand  are t he cent re frequency  and bandwi dth of the \n subband, respect ively. t\n)(mStthmmf\nmb\nthm\n \nThe LFPC  param eters whi ch provi de an i ndication of \nenergy  distribution am ong subbands are calculated as \nfollows: \n \n⎥⎦⎤\n⎢⎣⎡=\nmt\ntNmSm LFPC)(log10)(10                        (2)  \n \nwhere is the num ber of spect ral com ponent s in the \n subband. For each fram e, 12 LFPCs are obtained.  mN\nthm\n \nFigure 2 shows t he energy  distribution of non-vocal  \nand vocal s segm ents over above-defi ned 12 subband \nfilters. Th e seg ments are ex tracted  from six differen t \nsongs. The total length of each vocal/non-vocal \nsegm ent is 90 seconds. The fi gure shows t hat the vocal  \nsegm ents have relatively higher energy  values in the \nhigher frequency  bands i n com parison wi th the non-\nvocal segm ents. Therefore, as  can be seen in Figure 2,   \n \nLFPC is a quite effective feature for the discrim ination \nof vocal  and non-vocal  segm ents.  \n \n \nFigure 2.  Energy  distribution of pure instrumental \nsegments and vocals with instrumental segments \nover 12 subband filters  \n \n3. CLASSIFIER FORMULATION \n3.1. Multi-model HMM classifier \n \nMost studies on vocal s det ection use st atistical pattern \nclassifiers [1, 4, 5] . However, to our knowl edge, none \nof the st udies t akes i nto account  song st ructure \ninform ation in song m odelling. An im portant \nobservat ion i s that vocal  and non-vocal  segm ents \ndisplay intra-song si gnal charact eristics vari ation. For \nexam ple, signal strengt hs in different  sect ions (verse, \nchorus, bri dge and out ro) are usual ly different . In our \nobservat ion, for m ost songs, t he signal strengt h of the \nverse is relativ ely lo w co mpared to that of the chorus. \nChorus sections are usual ly of st ronger si gnal strengt h \nin comparison with verse sections since they m ay have \nbusier drum s, some addi tional percussi on, a ful ler \nstring arrangem ent and an addi tional melody line [10]. \nThe verse section usual ly has l ighter arrangem ent than \nthe chorus section. Sam ple waveform s extracted from  \ndifferent  verse and chorus sections of a popul ar song \nare depicted in Figure 3. \n \n \n \n \nFigure 3.  Waveforms of 10-second segments \nextracted from  (a) the vers e, (b) the chorus  sections  \nof the song ‘25 Minutes’. The horizontal axis \nrepresents tim e in seconds. \n \nTempo and l oudness are i mportant attributes \naccount ing for i nter-song vari ation. Therefore, we \nintegrate the song st ructure, inter-song and intra-song \nvariation into our m odels.  The t raining dat a (vocal  or non-vocal  segm ents) are \nmanually classified based on t he sect ion type (i ntro, \nverse, chorus, bri dge and outro), t empo and l oudness. \nWe assum e the tempo of t he input song t o be \nconst rained bet ween 40~ 185 beat s per minutes (BPM). \nWe divide m usic into high and l ow tempo classes \naccording to a fixed threshol d, which is 70BPM in our \ncurrent  implementation. Similarly, we di vide m usic \ninto loud and soft classes according to a threshold, \nwhich is determ ined by each individual song in the \ntraining dataset. Finally, a m odel is created for each \nclass as shown i n Figure 4.  In our current  \nimplementation, we use 20 m odels for m odelling vocal  \nand non-vocal  segm ents respect ively. \n \nIntro Verse Chorus Bridge Outro Vocal or Non-vocal Class \nMode l C \n(low tempo, \nloudness) Mode l D \n(low tempo, \nsoftness) Mode l B \n(high tempo, \nsoftness) Model A  \n(high tempo, \nloudn ess) \nFigure 4. Creating several variants of the vocal or non-\nvocal HMM model \n \nThis process results in multiple HMM m odels for each \nvocal and non-vocal class. Several models for each \nclass form  an HMM m odel space, to allow m ore \naccurate modelling in com parison to the single-m odel \nbaselin e. \n3.2. Bootstrapping process \n \nIn the above sect ion, we have creat ed a cl assifier that \nlearns the characteristics of the vocal and instrum ental \ncomponent s in the training dat a. We may use this \nclassifier to segm ent songs. However, t he vari ations in \ntempo, timbre propert ies and l oudness of di fferent  \nsongs m ay affect the classification accuracy. By \nemploying a classifier th at can learn the specific \ncharact eristics of the test song, we can ant icipate a \nbetter classificatio n performance. W ith similar \nmotivations, a method of boot strappi ng was proposed \nin whi ch song-speci fic vocal  charact eristics were \nlearned by the classifier to segm ent the song [1] . This \nprocess requi res hum an annot ated vocal  and non-vocal  \nsegm ents (boot strapped samples) of every  test song t o \ntrain their model. In addi tion, t his method depends on \nthe number of bootstrapped sam ples avai lable to learn \nthe vocal  charact eristics of t he song. In our approach, \nwe fi rst segm ent the song i nto vocal  and non-vocal  \nsegm ents using the MM-HMM classifier. W e then use \nthe initial segm entation as bootstrapped samples to   \n \nbuild song-speci fic vocal  and non-vocal  models of the \ntest song wi th a boot strappi ng process. Thi s process \nallows us to use a song’s own m odel for cl assification \nas shown i n Figure 5. Thi s bootstrappi ng process \nmakes the algorithm  adaptive and capabl e of achi eving \nhigher vocals detection accuracy.  \n \nTest \nSong Vocal / Non-\nVocal \nSegments Bootstrapped \nSamples Bootstrapped \nTraining \nBootstrapped \nHMM  Multi-Model \nHMM  Classifier \n \nFigu re 5 : Bootstrapped training and segmentation \nprocess \n4. SONG DATABASE \nIn order t o conduct  the experi ments, we com pile a \nsmall song dat abase whi ch includes 20 popul ar songs. \nThe songs are sel ected to obtain a variety in time \nperiod and artists. On average, the vocal segm ents \noccupy  67% of the total durat ion of a song, and t he \nrest 33% are non-vocal  segm ents. Each song is \nannot ated m anually to obtain the vocal  and non-vocal  \nsegm ents to provi de ground t ruth dat a. Thi s ground \ntruth data is used to evaluate system  perform ance. \nTypical segm ent durat ions range from  0.8 seconds to \n12 seconds. The sam pling frequency  of t he songs is \n44.1 kHz, st ereo channel  and 16 bi t per sam ple. In our \nexperi ments, t he songs wi th original sampling \nfrequency  are used wi thout re-sam pling. Six songs of \ndifferen t artists are allo cated  to the train ing dataset an d \nthe rem aining 14 songs t o the test set. There is no \noverlap between  the two  datasets. \n5. EXPERIMENTS \n5.1. Experimental configuration \n \nSeveral  experi ments are conduct ed to evaluate the \neffect iveness of the proposed approach. W e use t he \ncontinuous densi ty HM M with four st ates and two \nGaussi an m ixtures per st ate for al l HM M models in all \nour experi ments. The st andard procedures for HM M \ntraining and cl assification are wel l docum ented in [9]. \nIn our experi ments, 30% of t he dat abase which \nincludes six songs, sel ected random ly, is used as \ntraining data, and t he system is tested on t he rem aining \n14 songs.  Using this training dat abase, t he MM-HMM \nclassifier is trained t o obtain several  vari ants of t he \nvocal  and non-vocal  HM M models which are shown i n \nFigure 4. \n \nFirst, the test song i s blocked i nto 200m s anal ysis \nfram es, and then, LFPC features are calculated from  \n20ms wi th 13m s overl apping subfram es. Then, t he \nsong i s segm ented into vocal  and non-vocal  fram es of \n200m s in durat ion usi ng the MM-HMM classifier. In the MM-HMM classifier, ever y analysis fram e of the \nsong is matched wi th models of the classifier, and t he \nfram e is assigned to the model havi ng the best  match. \nAs shown in Figure 4, each m odel of the MM-HMM \nclassifier is associ ated wi th a vocal /non-vocal  class, \nsection type (verse, chorus, et c.), tempo and loudness \nlevel. Therefore, in the cl assification process, the MM-\nHMM classifier assi gns a vocal /non-vocal  label as well \nas section type, tempo and loudness l evel labels to a \nclassified analysis fram e. As a result, the fram es of the \nsong cl assified by  the M M-HMM classifier are \nassoci ated wi th song st ructure inform ation. \n \nThis initial segm entation produces a bootstrapped \ndatabase which includes vocal  and non-vocal  segm ents \n(boot strapped sam ples) of t he test song. Then, t he \nbootstrapped sam ples are used t o train the HMM. This \nprocess provi des song-speci fic vocal  and non-vocal  \nmodels of the test song. Si nce boot strapped sam ples are \nassoci ated wi th song st ructure inform ation, the \nbootstrappi ng training process t akes care of song \nstructure inform ation in song m odelling. Finally, the \ntest song is segm ented into vocal  and non-vocal  regions \nusing the song-speci fic vocal  and non-vocal  models. \nThe sam e analysis fram e length used in the MM-HMM \nclassifier is also used here. \n \nTo find the best m atched m odel for each analysis \nfram e, fram e log-likelihoods are cal culated for all \nmodels and t he likelihoods are com pared i n the HMM \nclassifiers. Accum ulating t he fram e log-likelihoods \nover a longer period is more st atistically reliable for \ndecision making [12] . In addi tion, t he feat ure \nparam eters of a rel atively short  fram e length do not  \ncapture inform ation about  melody, rhythm or long-t erm \nsong structure [13]. To observe t he cl assification \naccuracy of using longer analysis fram e lengths, \nadditional experi ments are carri ed out  using analysis \nfram es of 400 m s, 600 m s, 800 m s, 1000 m s, 1200 m s \nand 1400 m s. The experi mental resul ts are present ed in \nTable 1. \n5.2. Results and discussion \n \nTable 1 shows the average detection accuracy of the \nvocal  and non-vocal  segm ents of 14 pop songs by  the \nMM-HMM classifier and t he boot strappi ng method \nusing di fferent  anal ysis fram e lengths. \n \nThe resul ts show t hat long-t erm acoust ic feat ures are \nmore capabl e of di fferent iating vocal  and non-vocal  \nsegm ents. The opt imal fram e length seem s to be around \n1 second. The reason of t he decreased perform ance for \nlonger fram e length is that the assum ption of \nstationari ty in a fram e is not longer val id. With long \nfram e length it is more likely that the vocal  and non-\nvocal segm ents are present in a fram e. \n \n   \n \n \n \nTable 1 : Vocal/ non-vocal segment detection average \naccuracies of different analy sis fram es over 14 songs \nN= Non-vocal s egments, V=Vocal s egments, \nAvg=Average \n \nRelatively high accuracy is obtained using the MM-\nHMM classifier. Aft er em ploying the boot strappi ng \nprocess, the accuracy of the system  is im proved. \nRepeating the boot strappi ng process several  times \nimproves perform ance but  with the penal ty of increased \ncomputational cost . As our prel iminary experi ments \nshow that the perform ance i mprovem ent is marginal. \nTherefore, we appl y boot strappi ng onl y once i n the \nexperi ment.  \n \nTable 2 : Indices and titles of 20 songs in the database \nTraining Dat a \nSong  \nIndex Title \n1 [1978] - Village People - YMCA \n2 [2002]  - Blue - One Love \n3 [1986]  - Chris DeB urgh - Lady  in Red \n4 [1986]  - Roxette -  It must have been l ove \n5 [1984]  - Stevie Wonder - I just  called to \nsay I love y ou \n6 [2000]  - Ronan Keat ing - W hen you say  \nnothing at  all \nTestin g Data \nSong  \nIndex Title \n1 [1993]  - MLTR  - 25 M inutes \n2 [1993]  - MLTR  - Wild Women \n3 [1983]  - The Pol ice - Every  breat h you \ntake \n4 [1993]  - MLTR  - The Act or \n5 [2000]  N'Sync - Thi s I prom ise you \n6 [1993]   - M LTR  - Sleeping C hild \n7 [1980]  - AB BA - Super Trouper \n8 [1999]  - Backst reet Boys - As Long As \nYou Love M e \n9 [2001] - W estlife - W orld Of Our Own 10 [1999]  - Backst reet Boys - Back To Your \nHeart \n11 [1989] - Phil Collins - Another day in \nParadise \n12 [1995]  Take That  - Back for good \n13 [1998]  Eagl e Eye Cherry - Save Toni ght \n14 [2003]  - Dido - W hite Flag Fram e \nsize \n(ms) MM- HMM Bootstrapped HMM \n N V Avg N V Avg \n200 78.8 73.5 76.2 79.2 75.9 77.6 \n400 80.9 78.9 79.9 80.4 82.1 81.3 \n600 80.4 82 81.2 80.8 84.2 82.5 \n800 80.6 84.3 82.5 79.2 87 83.1 \n1000 81.9 84.3 83.1 82 86.6 84.3 \n1200 80.1 85.2 82.7 79.3 87.4 83.4 \n1400 78.1 86.4 82.3 78.3 88.2 83.3  \nFigure 6 shows t he resul ts of t he vocal /non-vocal  \nsegm ent classification for al l the test songs i n our \ndatabase. Indices of t he test songs as wel l as t raining \nsongs and their titles are listed in Table 2. The \nclassification perform ance is not consi stent among t he \nsongs.  This is because the songs in the database are of \ndifferent  charact eristics. For exam ple, som e songs are \nassoci ated wi th high tempo and l oudness whi le som e \nare associated with low tem po and softness. In \naddition, vocal s in som e songs are dom inant in most \npart of t he song whi le others havi ng strong \ninstrum ental accom panim ent throughout the song. \nBased on the charact eristics of t he song, t he system \nachieves accuracies ranging fro m the highest of 91.1% \n(25 M inutes) to the lowest of 77.2% (W hite Flag). In \ngeneral , songs wi th light background instrumental \nobtain higher accuracy th an songs with strong \nbackground i nstrumental. \n \nThe boot strappi ng process depends on the bootstrapped \nsamples. For t he last two songs (Indi ces 13 and 14) of \nFigure 6, the bootstrapped HMM is lower in accuracy \nthan the MM-HMM classifier . The reason is that the \naccuracies of MM-HMM are relatively low for these \nsongs and l arger num bers of bootstrapped samples are \nincorrect ly labelled com pared t o the other songs. \n \nWe give an exam ple of vocal  segm ents detected by the \nbootstrapped HM M together wi th manually annot ated \nvocal  segm ents of t he chorus sect ion of a song are \nshown i n Figure 7. \n \n \n \nFigure 7: (a) The segment (Chorus, 20 sec) of the song \n’25 Minutes’ (b) Manually  annotated vocal segments (c) \nAutom atically  detected vocal s egments \n \nTo com pare the perform ance of LFPC feature with \ntraditional MFCC feat ure, experi ments are conduct ed \nusing M FCC feature for t he fram e size of 1000m s. The \nresults are sum marized in Table 3. LFPC feature \noutperform  the MFCC feature. MFCC featu re is mainly \nused for speech recognition and its capability to capture   \n \nthe spectral characteristics of  the audi o signal seem s to \nbe lower than LFPC feature. \n \nTable 3 : Perform ance com parison between LFPC \nfeature and traditional MFCC feature (Fram e \nsize=1000m s) \nMM- HMM Bootstrapped HMM Featur\ne N V Avg N V Avg \nLFPC  81.9 84.3 LFPC  81.9 84.3 LFPC  \nMFCC 59.6 83.3 MFCC 59.6 83.3 MFCC \nFEA=Feature, N= Non-vocal segm ents, V=Vocal \nsegm ents, Avg=Average \n \nNext, we investigate the e ffectiveness of em ploying \nsong structure inform ation in song modelling. The \nexperi ments are conduct ed usi ng the basel ine HM M \ntraining method in which onl y one m odel is creat ed for \neach vocal and non-vocal class. This approach \ndisregards the structure of the song in song m odelling. \nFirst, the experi ments are conduct ed usi ng the sam e \nnumber of mixtures per st ate (2 m ixtures/ state) for bot h \nMM- HMM an d base lin e HMM. Th e results presented \nin Table 4 show t hat the MM-HMM training m ethod \noutperform s the basel ine HM M training approach for \nthe sam e num ber of m ixtures per st ate. The base line \nHMM h as 20 times less free parameters in compariso n \nwith MM- HMM as MM- HMM h as 40 models in total. \nTo gi ve t he base l ine HMM a fair chance for \ncomparison, we perform  further experi ment using base \nline HMM with 10 m ixtures per state. The results \npresent ed in Tabl e 4 show t hat using more free \nparam eters for base l ine HM M can not improve \nperform ance. The reason is that autom atic data \nclustering is not accurate in comparison with manual \nclustering. \n \nTable 4 : Perform ance com parison between MM-\nHMM training and base l ine HMM training \nmethod  (Fram e size =1000m s) \n N V Avg \nMM- HMM \n(2 mixtures/state) 81.9 84.3 83.1 \nBaselin e HMM  \n(2 mixtures/state) 79.2 83.4 81.3 \nBaselin e HMM \n(10mixtures/state) 57.4 91.8 74.6 \nV=Vocal segm ents, N= Non-vocal segm ents \n \nFigure 8 shows how wel l the test signal matches our \nMM-HMM. It d isplays th e probability distribution of \ncorrect ly matched models as wel l as of wrongl y \nmatched models for the test segm ents in five di fferent  \nsection types. The darkness of the rectangles indicates \nthe matching probability. As ex pected , seg ments fro m \nthe verse section are m ore likely to match the verse \nmodels than ot hers. In t he sam e way , segm ents from  \nthe other sectio ns tend to match their resp ectiv e models \nrather than ot her m odels. However, chorus segm ents tend to match bot h the chorus and out ro models. This is \ndue to the fact that the chor us is repeated in the outro \nsection before t he song fades out  [14] . \n \n \n \n \nFigure 8: Probability  distributions of the num ber of matched \nmodels for the test segm ents in five different sections \n \nBased on our experi mental resul ts, we coul d consi der \nthe following opt ions t o improve sy stem perform ance. \n \nIt is well-known from  the literature that singing vocals \nare h ighly harmonic (en ergy exists at integer multiples \nof the fundam ental frequency ), and ot her high energy  \nsounds – drum s in particular – are not  as harm onic and \ndistribute their energy  more wi dely in frequency  [8]. \nOver 90% of the vocal  signal is harm onic (much m ore \nthan the case of a speech signal)  [6]. It is believed that \na vocal  signal in general  has hi gher val ues of t he \nharm onic com ponent , com pared t o the instrumental \ncomponent  [3]. Unfort unately, the rel atively simple \nfeatures used in our current  system cannot  capi talize \nthe harm onic feat ure of si nging vocal . Nevert heless, \nthe prom ising resul ts from  our current  system lead us t o \nbelieve t hat we can furt her i mprove our system \nperform ance by  incorporat ing the harm onic feat ures. \n \nThe other possibility is to  enable a semi-automatic \nsystem  sim ilar to  that in  [1]. Ho wever, in stead  of \nchoosi ng bootstrappi ng samples random ly, we coul d \nallow the user to check and t o select the boot strapped \nsamples (vocal  and non-vocal  segm ents) manually from  \nthe initial seg mentation performed by the MM-HMM. \nThe accuracy of the initia l MM-HMM classifier is \naround 80%, wi th 20% of t he boot strapped samples \nwrongl y labelled. The m anual sel ection of \nautomatically d etected  resu lts is a less d emanding job \nfor the user i n com parison with the manual annot ation \nof si nging vocal s from  the ori ginal song. By \nincorporat ing human intervent ion in the loop, i t woul d \nimprove the quality of the bootstrapped sam ples \nsignificantly as shown i n Figure 5. As a consequence, \nwe coul d expect  a bet ter system  perform ance. \n   \n \nIf we stick to an automatic system, we can use the \nneighbourhood information in HMM model space [15] \nand Bayes factors as tools to calculate the confidence \nmeasure on the output of the initial MM-HMM classifier. With these additional techniques, segments \nthat have a high probability  of being labelled wrongly \nby the classifier are rejected, and we can train the \nbootstrapped vocal detector using generally correctly \nlabelled samples. \n6. CONCLUSION \nWe have presented an automatic approach for detecting vocal segments in a song. The proposed \napproach combines the multi-model HMM classifier \nand the bootstrapping method. The key points are \nintegration of song structure information and song-\nspecific vocal characteristics in song modelling. The bootstrapping process is used to improve vocals \ndetection accuracy. \n In a test dataset comprising 14 popular songs, our \napproach has achieved an accuracy of 84.3% in \nidentifying vocal segments from non-vocal ones.  \nOne drawback of the proposed approach is that it is \ncomputationally expensive since it entails two training \nsteps: training the MM-HMM classifier and training \nthe bootstrapped classifier. To reduce computational complexity, the number of bootstrapped samples can \nbe reduced. Instead of using all the segments of a song (of an average duration of 3 minutes) for bootstrapped \ntraining, only a certain number of samples with a very \nhigh confidence measure of co rrect classification could \nbe used. This would reduce computation time and further improve system performance. \n \n7. REFERENCES \n \n[1] Tzanetakis, G. “Song-specific bootstrapping of singing voice structure”, IEEE \nInternational Conference On Multimedia And Expo,  2004. \n[2] Maddage, N., Xu, C., and Wang, Y. “An SVM-based classification approach to \nmusical audio”, 4th International Conference \non Music Information Retrieval , Maryland, \nUSA, 2003. \n[3] Chou, W., and Gu, L., “ Robust singing \ndetection in speech/music discriminator design”, \nProceedings of IEEE International \nConference on  Acoustics, Speech, and Signal \nProcessing, vol. 2, pp.865 - 868, 2001. \n[4] Berenzweig, A., Ellis, D. P. W., and \nLawrence, S., “Using voice segments to \nimprove artist classification of music,” AES 22nd International Conference, Espoo , \nFinland, 2002. \n[5] Berenzweig, A.L. and Ellis, D.P.W. \n“Locating singing voice segments within \nmusic signals,” Proceedings of IEEE \nWASPAA’01, pp.119-122, New York, Oct. \n2001. \n[6] Zhang, T, “ System and method for automatic \nsinger  identification”, IEEE International \nConference on Multimedia and Expo , \nBaltimore , MD, 2003. \n[7] Maddage, N., Wan, K.W., Xu, C., and Wang, Y., “Singing Voice Detection Using Twice-\nIterated Composite Fourier Transform ”, IEEE \nInternational Conference On Multimedia And Expo, 2004. \n[8] Kim, Y., and Whitman, B., ”Singer \nidentification in popular music recordings \nusing voice coding features”,  Proceedings of \nInt. Symposium on Music Information Retrieval , 2002. \n[9] Rabiner, L.  R.,  and  Juang, B. H.,  \nFundamentals  of  speech recognition, \nPrentice Hall, Englewood Cliffs, N.J, 1993.  \n[10] Waugh, I. Song Structure. Music tech \nmagazine, October 2003. \n[11] Nwe, T.L, Foo, S.W, and De Silva, L.C.  \n“Stress classification using subband based \nfeatures ”, IEICE Transactions on Information \nand Systems, Special Issue on Speech \nInformation Processing, Vol. E86-D, no.3, \npp. 565-573, March 2003.  \n[12] Tsai, W.H., Wang, H.M., Rodgers, D., Cheng, S.S., and Yu, H.M. ”\nBlind clustering \nof popular music recordings based on singer \nvoice characteristics”,  4th International \nConference on Music Information Retrieval , \nMaryland, USA, 2003. \n[13] Berenzweig, A., Logan, B., Ellis, D.P.W., and \nWhitman, B., “A large-scale evaluation of \nacoustic and subjective music similarity  \nmeasures,” Proc Intl Conf on Music \nInformation Retrieval , Washington DC, 2003. \n[14] Watson, C.J., “The everything song writing \nbook”, Adams Media Corporation, Avon, \nMassachusettes, 2003. \n[15] Jiang, H., and Lee, C.H., “ A new approach to \nutterance verification based on neighborhood information in model space”, \nIEEE \nTransactions on Speech and Audio Processing, vol. 11, pp.425 – 434, 2003. \n[16] Andre-Obrecht, R., “A New Statistical \nApproach for the Automatic Segmentation of   \n \nContinuous Speech Signals”, IEEE \nTransactions on Acoustics, Speech, and \nSignal Processi ng, vol. 36, No. 1, pp. 29-40, \nJanuary  1988.  \n \n707580859095\n12 345 678 9 10 11 12 13 14\nIndex of SongSegmentation \nAccuracy(%)MM-HMM Bootstrapped HMM\nFigu re 6: Vocal/ non-vocal segm ent detection accuracies \nfor individual songs with analy sis frame length of 1000ms"
    },
    {
        "title": "Automatic extraction of music descriptors from acoustic signals.",
        "author": [
            "François Pachet",
            "Aymeric Zils"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416106",
        "url": "https://doi.org/10.5281/zenodo.1416106",
        "ee": "https://zenodo.org/records/1416106/files/PachetZ04.pdf",
        "abstract": "High-Level music descriptors are key ingredients for music information retrieval systems. Although there is a long tradition in extracting information from acoustic signals, the field of music information extraction is largely heuristic in nature. We present here a heuristic-based generic approach for extracting automatically high-level music descriptors from acoustic signals. This approach is based on Genetic Programming, used to build relevant features as functions of mathematical and signal processing operators. The search of relevant features is guided by specialized heuristics that embody knowledge about the signal processing functions built by the system. Signal processing patterns are used in order to control the general processing methods. In addition, rewriting rules are introduced to simplify overly complex expressions, and a caching system further reduces the computing cost of each cycle. Finally, the features build by the system are combined into an optimized machine learning descriptor model, and an executable program is generated to compute the model on any audio signal. In this paper, we describe the overall system and compare its results against traditional approaches in musical feature extraction à la Mpeg7.",
        "zenodo_id": 1416106,
        "dblp_key": "conf/ismir/PachetZ04",
        "keywords": [
            "Genetic Programming",
            "relevant features",
            "acoustic signals",
            "signal processing operators",
            "heuristic-based",
            "machine learning descriptor",
            "caching system",
            "optimized machine learning descriptor model",
            "audio signal",
            "executable program"
        ],
        "content": "AUTOMATIC EXTRACTION OF MUSIC DESCRIPTORS \nFROM ACOUSTIC SIGNALS\nPachet François, Zils Aymeric \nSony CSL Paris \n{pachet, zils}@csl.sony.fr  \nABSTRACT \nHigh-Level music descriptors are key ingredients fo r music \ninformation retrieval systems. Although there is a long \ntradition in extracting information from acoustic s ignals, the \nfield of music information extraction is largely he uristic in \nnature. We present here a heuristic-based generic a pproach \nfor extracting automatically high-level music descr iptors \nfrom acoustic signals. This approach is based on Ge netic \nProgramming, used to build relevant features as fun ctions of \nmathematical and signal processing operators. The s earch of \nrelevant features is guided by specialized heuristi cs that \nembody knowledge about the signal processing functi ons \nbuilt by the system. Signal processing patterns are  used in \norder to control the general processing methods. In  addition, \nrewriting rules are introduced to simplify overly c omplex \nexpressions, and a caching system further reduces t he \ncomputing cost of each cycle. Finally, the features  build by \nthe system are combined into an optimized machine l earning \ndescriptor model, and an executable program is gene rated to \ncompute the model on any audio signal. In this pape r, we \ndescribe the overall system and compare its results  against \ntraditional approaches in musical feature extractio n à la \nMpeg7. \n1.  INTRODUCTION \nThe exploding field of Music Information Retrieval has \nrecently created extra pressure to the community of  audio \nsignal processing, for extracting automatically hig h level \nmusic descriptors. Indeed, current systems propose users i.e. \nthe possibility to access music titles based on the ir actual \ncontent, rather than on file names. Existing system s today are \nmostly based on editorial information (e.g. Kazaa),  or \nmetadata which is with millions of music titles (e. g. the peer-\nto-peer systems such as Kazaa) and query functions limited \nusually to string matching on title names. The natu ral \nextension of these systems is content-based access,  entered \nmanually, either by pools of experts (e.g. All Musi c Guide) \nor in a collaborative manner (e.g. MoodLogic). Beca use \nthese methods are costly and do not allow scale up,  the issue \nof extracting automatically high-level features fro m acoustic \nsignals is key to the success of online music acces s systems.  Extracting automatically content from music titles is a long \nstory. Many attempts have been made to identify dim ensions \nof music that are perceptually relevant and can be extracted \nautomatically. One of the most known is tempo or be at. Beat \nis a very important dimension of music that makes s ense to \nany listener. [1] introduced a beat tracking system  that \nsuccessfully computes the beat of music signals wit h good \naccuracy. There are, however, many other dimensions  of \nmusic that are perceptually relevant, and that coul d be \nextracted from the signal: the presence of voice in  a music \ntitle, the perceived intensity (subjective impressi on of energy \nthat music titles convey: with the same volume, a H ard-rock \nmusic title conveys more energy than an acoustic gu itar \nballad with a soft voice), difference between “live ” and \nstudio recording, recognition of typical musical ge nres, \nevaluation of the danceability of a song, etc. Yet this \ninformation is difficult to extract automatically, because \nmusic signals are usually highly complex, polyphoni c in \nnature, and incorporate characteristics that are st ill poorly \nunderstood and modeled. \n2.  TOWARDS AUTOMATIC EXTRACTION OF \nMUSICAL DESCRIPTORS \n2.1.  The traditional method: combination of generic Low-\nLevel Descriptors \nTypically, the design of a descriptor extractor con sists in \ncombining generic Low-Level Descriptors (LLDs) as \nrelevant characteristics of acoustic signals (featu res), using \nmachine learning algorithms (see, e.g. [2], [3], [4 ]):  \n- Several features are computed. A typical reference for \naudio signal features is the Mpeg7 standardization \nprocess [5] that proposes a battery of LLDs for \ndescribing basic characteristics of audio signals. \n- the most relevant features are selected and \ncombined into machine learning processes, to \nprovide an optimal model for the descriptor.  \nThe traditional method sketched above works well on ly for \nrelatively easy problems; problems for which generi c low \nlevel features are adapted. However, generic featur es can \nonly extract information which is “predominant” in the \nsignal, and are, by definition, unable to focus on specific, \nproblem-dependent properties. The core assumption o f this \npaper is precisely that in order to solve more diff icult \nproblems one needs specific features adapted to the  problem \nat hand. \nOur second assumption is that these specific featur es can be \nextracted automatically as compositions of signal p rocessing \noperations. Permission to make digital or hard copies of all or  part of this work \nfor personal or classroom use is granted without fe e provided that \ncopies are not made or distributed for profit or co mmercial \nadvantage and that copies bear this notice and the full citation on \nthe first page. \n© 2004 Universitat Pompeu Fabra. \n   \n \n 2.2.  Improving generic LLD combination using automatic \noperators composition \nThe design of specific features that are relevant f or a given \ndescription problem is usually done by hand by sign al \nprocessing experts. This section introduces the ide a of \ngenerating automatically such specific features ada pted to a \nparticular problem.  \nAlthough there is no known general paradigm for des igning \ndomain-specific features, their design usually foll ows some \nsort of patterns. One of them consists in filtering  the signal, \nsplitting it into frames, applying specific treatme nts to each \nsegment, then aggregating all these results back to  produce a \nsingle value. This is typically the case of the bea t tracking \nsystem described in [1] that yields eventually a fl oat \nrepresenting the tempo. The same applies to timbre \ndescriptors proposed in the music information retri eval and \nmore generally to most audio descriptors described in the \nliterature ([6]). Of course, this global scheme of \nexpansion/reduction is under specified, and an infi nite \nnumber of such schemes could be envisaged.  \nOur goal is therefore to design a system that is ab le to 1) \nsearch automatically relevant signal processing fea tures, seen \nas compositions of functions and build a model of t he \ndescriptor and 2) reduce the search space significa ntly using \ngeneric knowledge on signal processing operators. \n3.  EDS, AN EXTRACTOR DISCOVERY SYSTEM \nWe describe here the main ingredients of the EDS sy stem: \nthe definition of a description problem, the automa tic \nconstruction of relevant signal processing function s, and their \ncombination into a general descriptor model.  \n3.1.  Definition of a description problem \nThe definition of the description problems handled by the \nsystem has to remain simple to preserve the general ity of the \napproach. One simple way to define a description pr oblem is \nto use the supervised learning approach: a set of l abeled \nsignals, also called learning database , defines the \ndescription problem. These labels are either numeri c values, \nsuch as an evaluation of their “musical energy” (be tween 0 \nand 1), or a class label, such as the “presence of a singing \nvoice or not”, or the genre chosen in a given taxon omy. The \nsystem will then finds the rules of the labeling of  the signals, \ni.e. the model of the descriptor, by designing a fu nction \nwhich produces outputs as close as possible to the learning \ndatabase labels. \n3.2.  General principles of EDS \nOur system EDS composes automatically operators to \ndiscover features as signal processing functions th at are \noptimal for a given descriptor extraction task. Its  global \narchitecture consists in two fully automatic parts:  modeling \nof the descriptor and synthesis of the extractor. \nThe modeling of the descriptor is the main part of EDS. It \nconsists in searching automatically for relevant fe atures and \nthen for the optimal model that combines these feat ures. The \nsearch for specific features is based on genetic pr ogramming, \na well-known technique for exploring search spaces of function compositions (see [7]). The genetic progra mming \nengine composes automatically signal processing ope rators to \nbuild arbitrarily complex functions, and evaluates their \nrelevance to extract a given descriptor on a given learning \ndatabase. The evaluation of a function is very cost ly, as it \ninvolves complex signal processing on whole audio \ndatabases. Therefore, to limit the search, a set of  heuristics \nare introduced to improve the a priori relevance of  the \ncreated functions, as well as rewriting rules to si mplify \nfunctions before their evaluation. Once the system has found \nrelevant features, it combines them to feed them in to various \nmachine learning models, and then optimizes the mod el \nparameters. \nThe synthesis part consists in generating an execut able file to \ncompute the best model on any audio signal. This pr ogram \nallows computing this model on arbitrary audio sign als, to \npredict their value for the modeled descriptor. \n3.3.  Automatic construction of features \nFunctions are represented in EDS as compositions of  basic \noperations applied on an arbitrary input audio sign al. The \nbasic operators can be mathematical, such as taking  the mean \nvalues of a set, or can process a signal, temporall y (such as \ncorrelation), or spectrally (such as a low-pass fil tering). In \naddition, some operations are parameterized using c onstant \nvalues (like cut-off frequencies), or external sign als (for \nexample a correlation with a fixed reference signal ). \nConsequently, the functions can be represented as s ignal \nprocessing operators trees (see Fig.1): \n \nFft(Derivation(InSignal), Max(Correlation  \n(InSignal, Constant_Signal)) \n<==> \nF f t \nD e r iv M a x \nI n p u tS i g n a l C o r r e l a ti o n \nI n S i g n a l C o n s t a n t_ S i g n a l  \nFig. 1: The syntactic tree of a function in EDS  \n \n3.4.  Automatic construction of features \nThe automatic construction of correct functions rel ies on the \ncontrol of the types of data handled by the functio ns, and on \nthe introduction of signal processing expertise as heuristics. \n3.4.1.  Data Types \nThe need for typing is well-known in Genetic Progra mming, \nto ensure that the functions generated are at least  \nsyntactically correct. Different type systems have been \nproposed for GP, such as strong typing ([8]) that m ainly \ndifferentiate between the “programming” types of th e inputs \nand outputs of functions. To control the physical p rocesses in \nEDS, we need to distinguish how the functions built  by the \nsystem handle the data, at the level of their “phys ical \ndimension”. For instance, audio signals and spectru m are \nboth as vectors of floats, but are different in the ir   \n \n dimensions: a signal is a time to amplitude represe ntation, a \nspectrum frequency to amplitude.  \nOur typing system, based on the following construct s, \nrepresents this difference, to ensure that our resu lting \nfunctions make sense. Using only three physical dim ensions \n(time “t”, frequency \"f\", and amplitudes or non-dim ensional \ndata “a”), we are able to represent most of the dat a types \nhandled by the system, by building atomic (single v alue), \nvector (multiple values), and functional (data of a  given type \nevolving in a dimension of another type) types. \n3.4.2.  Operator types \nThe operations in EDS transform physically the data , and can \ntherefore be specified using the typing system. For  each \noperator, we define typing rules that provide the t ype of its \noutput data, depending on the types of its input da ta. The \ntyping rules are usually reduced into a dimensional ity rule \nand a transformation rule. For example, the “Spectr um” \noperation transforms a signal of type “t:a” into a frequency \nspectrum of type “f:a”. \n3.4.3.  Controlling processing methods using patterns \nThe types of data handled by a function are a signa ture of the \ngeneral processing methods used in the function. In  order to \ncontrol globally the processing methods through the  \nsuccessive types of data handled by the functions, we have \nintroduced \"generic operators\" that stand for one o r several \nrandom real operator(s) whose output types are forc ed. EDS \ncan deal with three different generic operators (no tated \"*\", \n\"!\", and \"?\") that have different functionalities. These generic \noperators allow specifying locally the processes to  use in a \nfunction. By composing them in patterns , we describe a \nglobal set of processes to apply on an audio signal  to obtain a \nfinal value. For instance, the simple pattern  \"?_a  (!_Va  (Split \n(*_t:a  (Signal))))\" is a translation of the general extra ction \nscheme presented in 3.1, standing for the process “ Temporal \ndomain – Split – Agregation”. There are various way s to \ninstantiate this pattern, for example “ Sum a (Square Va (Mean Va \n(Split Vt:a (HpFilter t:a (Signal t:a , 1000Hz )))))”. \nPatterns are specified in the EDS algorithm in orde r to guide \nthe search of functions. \n3.4.4.  Heuristics \nIn order to guide the instantiation of the patterns , we need to \nintroduce knowledge in the system, as signal proces sing \nheuristics that represent the know-how of signal pr ocessing \nexperts, about functions seen a priori, both favori ng a priori \ninteresting functions and ruling out obviously non- interesting \nones.  \nA heuristic in EDS associates a score to a potentia l \ncomposition of operators, between 0 (forbidden comp osition) \nand 10 (very recommended composition). These scores  are \nused when EDS builds a new function, to select the \ncandidates between all the possible operations. \nBasically, the heuristics allow controlling the str ucture of the \nfunctions, avoiding bad combination of operations, ranging \nconstant parameters values, and avoiding usually us eless \noperations. 3.4.5.  Automatic construction of functions \nThe automatic synthesis of functions is performed i n bottom-\nup fashion, starting from the input signal, and gra fting \nsequentially the operators one after the other up t o the top of \nthe tree, all the generic operators being instantia ted, i.e. \nreplaced by real operators.  \n3.5.  Search for optimized features \nThe function search part in EDS consists in buildin g signal \nprocessing functions that are increasingly relevant , using an \nalgorithm based on genetic programming, i.e. the ap plication \nof genetic search to the world of functions, as int roduced by \n[9]. The algorithm builds a population of functions  from a \ngiven pattern, and tries to improve them iterativel y by \napplying various genetic transformations on them. R unning \nthis algorithm once provides one optimal function t o be used \nin the final model. Therefore, this algorithm is ru n N times to \nbuild N optimized functions constituting the final feature set \nused in the final model of the descriptor. \n \nDuring the genetic search, each new population is c reated by \napplying genetic transformations on the most releva nt \nfunctions of the current population. 3 main transfo rmations \nare used in EDS: variations of the constant paramet ers \n(keeps the tree structure a function and applies \nvariations on its parameters such as cut-off freque ncies \nor window sizes ), mutation of operations in the function, \nand crossover of two parts of two functions. \n \nEventually, in order to search more efficiently, re writing \nrules (simplifying functions using a fixed point me chanism) \nand a caching mechanism (keeping the most useful re sults in \nmemory, depending on their computation time, utilit y, and \nsize) have been included in the system. \n3.6.  Final model of the descriptor \nAfter running the genetic search, EDS finds relevan t features \nwell adapted to the description problem at hand. Th ese \nfeatures are then combined into an optimized model of the \ndescriptor, using generic machine learning techniqu es (kNN,  \nNeural Nets, Decision Trees, etc…).  \n \nEach of these models carries with it a certain numb er of \nparameters such as the number of neighbours in the k-NN \nmethod, or the number of layers for the Neural Netw orks. \nThe processes of selecting the optimal model with t he \noptimal parameters are entirely automated in EDS. \nThe final descriptor model is the best model found,  defined \nby a set of relevant features and a modelling techn ique with \noptimized parameters, such as for instance:  \n“6-NN (‘Max(Fft(S))’,‘Variance(Autocorrelation(S))’ )”. \n \nThe performance of this model is evaluated on a tes t \ndatabase (different from the learning database) for  assessing \ndefinitively its performance. \nAnd finally, a self-executable extractor is generat ed \nautomatically to compute the model on a .wav signal .   \n \n 4.  PERFORMANCE OF THE SYSTEM \nWe present here the performance on the two steps of  EDS: \nthe relevance of the features and the quality of th e models \nfound by EDS, compared to those made using the Mpeg 7 \nLLDs dataset (called “LLDs”). \n4.1.  Regression problem: Musical energy \nThe problem consists in providing a model of the su bjective \nenergy of musical extracts, based on the results of  perceptive \ntests (see [10]). This descriptor addresses the int uitive \ndifference there is, for example, between a punchy punk-rock \nsong with loud saturated guitars and screaming voic e \nconveys and an acoustic guitar ballad with a soft v oice, at a \nconstant volume level. \n4.1.1.  LLDs \nThe best LLD found was “Mean (SpectralSkewness (Spl it \n(Signal, 250.0)))”, with correlation=0,548 on learn  and 0,658 \non test. A forward features selection (see[12]) on the LLDs \nkept 25 features to build the final model of musica l energy. \nThe best method found was a Model Tree provided a \ncorrelation=0.698 (0.810 on test), which correspond s to an \naverage model error of 12.80% (13,26%). \n4.1.2.  EDS \nThe best function found by EDS is \n“BestEDS(Signal)=Square (Log10 (Mean (Min (Fft (Spl it \n(Testwav, 4009))))))”, with correlation=0,744 on le arn, and \n0,812 on test. A forward selection kept 4 features to build \nthe final model of musical energy. The best method found \nwas a Linear Regression that provided a correlation =0.780 \non learn, and 0.836 on test, which corresponds to a n average \nmodel error of 11.52% (13,06%). \n4.2.  Objective classification problem: Presence of singi ng \nvoice \nThe problem consists in providing a model that allo ws \ndetecting the presence of singing voice in polyphon ic audio \nsignals (see [11]). \n4.2.1.  LLDs \nThe best LLD found was “SpectralSpread (Testwav)” ( Fisher \n= 0,282 on learn, and 0,215 on test). A forward sel ection \nkept 8 features. The best method found is a Naïve B ayes \nclassifier providing a 72% of good classification o n learn and \n69.5% on test. \n4.2.2.  EDS \nThe best function found by EDS is “Log10 (Range \n(Derivation (Sqrt (Blackman (MelBands (Testwav, \n24.0))))))” (Fisher=1,209 on learn, and 0,831 on te st).  \nA forward selection kept 12 features to build the f inal EDS \nmodel of musical energy. The best method found was a kNN \nclassifier providing 86.5% of good classifications on learn, \nand only 78.5% on test. 5.  CONCLUSION \nWe have introduced a new approach for designing \nautomatically efficient extractors for high-level a udio \ndescriptors. Although it uses a limited palette of signal \nprocessing functions, the proposed system, EDS, alr eady \nproduces better results than standard approaches us ing the \nMpeg7 generic features. \nThe generality of the approach allows EDS to addres s the \nwhole class of extraction problems in the large, fr om the \ndetection of “live” recordings or the modeling of m usic \ndanceability or percussivity, etc... \nSubstantial increase in performance is expected by extending \nthe palette of signal operators to more refined ope rators, as \nwell as in adding more refined heuristics and rewri ting rules \nto prune the search space.  \n6.  REFERENCES \n[1] Eric D. Scheirer. Tempo and beat analysis of ac oustic \nmusical signals. J. Acoust. Soc. Am. (JASA) 103:1 ( Jan \n1998), pp 588-601. \n[2] Eric D. Scheirer, and Malcolm Slaney. Construct ion and \nevaluation of a robust multifeature speech/music \ndiscriminator. Proc. ICASSP ’97. \n[3] P. Herrera, A. Yeterian, F. Gouyon. Automatic \nclassification of drum sounds: a comparison of feat ure \nselection methods and classification techniques. Pr oceedings \nof 2nd International Conference on Music and Artifi cial \nIntelligence, Edinburgh, Scotland, 2002. \n[4] Geoffroy Peeters, Xavier Rodet. Automatically s electing \nsignal descriptors for sound classification. Procee dings of the \n2002 ICMC, Goteborg (Sweden), September 2002. \n[5] Perfecto Herrera, Xavier Serra, Geoffroy Peeter s. Audio \ndescriptors and descriptors schemes in the context of MPEG-\n7. Proceedings of the 1999 ICMC, Beijing, China, Oc tober \n1999. \n[6] JJ Aucouturier, François Pachet. Music similari ty \nmeasures: what's the use ? In proceedings of the 3r d \ninternational symposium on music information retrie val \n(ISMIR02), Paris, October 2002. \n[7] John R. Koza. Genetic Programming: on the \nprogramming of computers by means of natural select ion. \nCambridge, MA: The MIT Press. \n[8] David J Montana. Strongly typed genetic program ming. \nIn Evolutionary Computation 3-2, 1995, pp 199-230. \n[9] David E. Goldberg. Genetic algorithms  in searc h, \noptimization and machine learning. Addison-Wesley P ub. \nCo. 1989. ISBN: 0201157675. \n[10] Aymeric Zils, François Pachet. Extracting auto matically \nthe perceived intensity of music titles. Proceeding s of 6th \nInternational Conference on Digital Audio Effects \n(DAFX03), London, UK, September 8-11, 2003. \n[11] A.L. Berenzweig, Dan P. W. Ellis. Locating sin ging \nvoice segments within music signals. IEEE workshop on \napplications of signal processing to acoustics and audio \n(WASPAA01), Mohonk NY, October 2001. \n[12] Fukunaga, K., \"Statistical pattern recognition \", \nAcademic press, 1990."
    },
    {
        "title": "A Matlab Toolbox to Compute Music Similarity from Audio.",
        "author": [
            "Elias Pampalk"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418077",
        "url": "https://doi.org/10.5281/zenodo.1418077",
        "ee": "https://zenodo.org/records/1418077/files/Pampalk04.pdf",
        "abstract": "A Matlab toolbox implementing music similarity mea- sures for audio is presented. The implemented measures focus on aspects related to timbre and periodicities in the signal. This paper gives an overview of the implemented functions. In particular, the basics of the similarity mea- sures are reviewed and some visualizations are discussed.",
        "zenodo_id": 1418077,
        "dblp_key": "conf/ismir/Pampalk04",
        "keywords": [
            "Matlab toolbox",
            "music similarity measures",
            "audio",
            "timbre",
            "periodicities",
            "signal",
            "functions",
            "basics",
            "similarity measures",
            "visualizations"
        ],
        "content": "AMATLAB TOOLBOX TO COMPUTE\nMUSICSIMILARITY FROM AUDIO\nElias Pampalk\nAustrian ResearchInstitute for Artiﬁcial Intelligence\nFreyung 6/6, A-1010 Vienna, Austria\nABSTRACT\nAMatlabtoolboximplementingmusicsimilaritymea-\nsures for audio is presented. The implemented measures\nfocus on aspects related to timbre and periodicities in the\nsignal. This paper gives an overview of the implemented\nfunctions. In particular, the basics of the similarity mea-\nsures arereviewed and somevisualizations arediscussed.\n1. INTRODUCTION\nNew technologies are needed to access large digital mu-\nsic collections (e.g. automatic playlist generation, rec-\nommendation, query-by-example, hierarchical organiza-\ntion and visualization). One important building block is a\ncomputational model of music similarity based on audio\nanalysis.\nAlthough approaches to compute similarity have been\npublished (e.g. [5, 6, 1, 7, 8, 4]) hardly any open source\nimplementations are available. One of the few excep-\ntions is the CLAM framework which offers functions to\ncompute descriptors which can be used for music simi-\nlarity.1Another exception is the Marsyas framework for\ncomputeraudition[12]whichimplementsseveraldescrip-\ntorsfor genre classiﬁcation.\nIn this paper an open source Matlab toolbox is pre-\nsented wich is available on the Internet.2The toolbox\nimplements several similarity measures and functions to\nvisualize intermediate steps in the computations. Further -\nmore,somebasicfunctionalitiesareincludedtocreatethe\nislands of music metaphor where islands represent clus-\nters of similar pieces [7]. Although the toolbox is not\nsuited for large scale processing (i.e. collection sizes be -\nyond 5000 pieces) it can serve as reference implemen-\ntation and as playground to study effects of parameters.\nFirst experiments in this direction using parts of the cur-\nrent toolbox were reported in [9]. In this paper the main\nfunctionsarepresentedandsomedemonstrationsaregiven.\n1http://www.iua.upf.es/mtg/clam\n2http://www.oefai.at/˜elias/ma\nPermissiontomakedigitalorhardcopiesofallorpartofthisw orkfor\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributedforproﬁtorcommercialadvantagean dthat\ncopies bearthis notice andthefull citation ontheﬁrst page .\nc/circlecopyrt2004Universitat PompeuFabra.2. SIMILARITY MEASURES\nThere are four main issues involved in designing similar-\nity measures. First it is necessary to decide which fea-\ntures to extract from the audio signal. This decision de-\npends on the targeted concept (e.g. timbre, rhythm, har-\nmony, melody etc.) and involves signal processing, psy-\nchoacoustics, and music perception. The second issue is\nclosely related to the ﬁrst and deals with how to sum-\nmarize and describe a piece of music based on the ex-\ntracted features. Often pieces are very inhomogeneous\n(e.g.Bohemian Rhapsody byQueen). Simply calculat-\ning the mean of the extracted features is usually not use-\nful. The third issue is how to compare the representation\nof one piece with another. Depending on the previously\nchosen representation this might be difﬁcult. The visual-\nizations in the toolbox support the user in understanding\nthedifferentchoicesforeachdecision. Thefourthissueis\ncomputationalefﬁciency. Itisnotpossibletomodelevery\nnerve cell in the human auditory system when processing\nmusicarchives withterabytes of data.\nIn the following we brieﬂy review the similarity mea-\nsuresand how they areimplemented inthe toolbox.\n2.1. Frame Clustering and ClusterModel Similarity\nThe idea of FC/CMS was ﬁrst published by Logan and\nSalomon [6] and improved and optimized by Aucouturier\nandPachet[1,2,3]. Apieceofmusiciscutintothousands\nof very short frames (of about 20-30ms length). Each of\nthese frames is described in terms of loudness per fre-\nquency band. Usually Mel frequency cepstrum coefﬁ-\ncients(MFCCs)areused,howeveranyloudness/frequency\nmodel could be used (e.g. sone/bark).\nOnce this ﬁrst step is computed there are two steps to\ncompare the similarity between two pieces. First, in the\nframe clustering (FC) step, the frames are clustered into\nusually 3-40 groups (using either k-means or Gaussian\nmixture models (GMM) with expectation maximization)\nand thediagonal covariance of each group iscomputed.\nSecond,intheclustermodelsimilarity(CMS)step,the\nsimilarity of two pieces modeled by clusters is computed.\nTwo different approaches have been used. Logan and Sa-\nlomonproposedtheEarthMover’sDistance(EMD,origi-\nnally developed as distance measure for image retrieval,\nsee [10] for details) in combination with the Kullback-\nLeibler (KL) divergence to compute the distance betweentwoclusters.\nAucouturier and Pachet proposed using Monte Carlo\n(MC) sampling to measure the similarities of the clus-\nter models. In particular, given two pieces A and B a\nlarge sample (2000-5000) is drawn from each of the clus-\nter models. Then the likelihood that the sample drawn\nfrom A was generated by model B and vice versa is com-\nputed and used as distance function. The advantage of\nEMD-KL compared to MC is that it is computationally\nmuchfaster,howeverinanevaluationpresentedin[3]MC\noutperformed EMD-KL.\nThe computation of the frames is either done using\n“mamfcc” or “ma sone”. In both cases the audio signal\nmust be in the memory (usually loaded by “wavread”).\nFC/CMS is implemented by the functions “ma fc” and\n“macms”. In both cases parameters deﬁne which vari-\nation (i.e. k-means or GMM, EMD-KL or MC) to use.\nAny combination is possible. For the GMM and MC the\nNetlab3toolbox and for the EMD the code published by\nYossi Rubner isrequired.\nIn the remainder of this paper “AP 30” is used to de-\nscribe FC with 20 MFCC coefﬁcients (ignoring the 0th\ncoefﬁcient) and a GMM with 30 centers. For CMS the\nMC is used with 2000 samples as suggested by Aucou-\nturierandPachet. “LS30”isusedtodescribethesameas\nAP30butusingk-meansinsteadofaGMMandEMD-KL\ninstead of MCas suggested by Logan and Salomon.\n2.2. Spectrum Histograms\nTheSHs[8]areasimpleapproachtosummarizethespec-\ntral shape. They are based on a sone/bark representation\nof the audio signal. The SHs describe a piece by count-\ning how many times each loudness level was exceeded\nin each frequency band. The distance between two SHs\nis measured with the Euclidean metric. SHs are compu-\ntationally much faster than FC/CMS variations, however\ntheir performance is also signiﬁcantly worse. The func-\ntionsneededtocomputeSHsare“ma sone”and“ma sh”.\n2.3. Periodicity Histograms\nPHswereoriginallypresentedinthecontextofbeattrack-\ning [11]. Details of the differences between the PH simi-\nlaritymeasureandpreviousapproachescanbefoundin[8].\nTheideaistodescribeperiodicallyreoccurringbeats. The\nfeatures are extracted by further processing the sone/bark\nrepresentation. The distance is measured using the Eu-\nclidean metric. Although the PH is not a good similar-\nity measure it is a good starting point to compute higher\nlevel descriptors. The functions needed to compute PHs\nare “masone” and “ma ph”.\n2.4. Fluctuation Pattern\nFPs are an alternative approach to describe periodicities.\nThe main difference between FPs [7] and PHs is that the\n3http://www.ncrg.aston.ac.uk/netlabAP 30 LS 30 SHPHFP\nSone 6m6m6m\nMFCC 7m 7m\nFeatures 132m 15m48s12m 23s\nDistances 74m 4m <1s<1s <1s\nTotal 213m 26m 7m18m6.5m\nR-Precision 0.43 0.380.210.130.25\nTable 1. Small Evaluation. Computation times (in min-\nutes [m]and seconds [s])and R-precision values.\nFPs include information on the energy distribution in the\nfrequency spectrum which the PHs discard. Another dif-\nferences is that FPs use a FFT instead of a comb-ﬁlter to\nﬁnd periodicities in the critical-bands (bark-scale). Fur -\nthermore,whilethePHsusearesonancemodelwhichhas\na maximum at about 120bpm the FPs use a ﬂuctuation\nmodel which has a peak at 4Hz (240bpm). The distance\nis measured using the Euclidean metric. The functions\nneeded tocompute FPs are“ma sone” and “ma fp”.\n3. EXAMPLES\nIn this section some examples of how the toolbox can be\nused for visualizations and evaluations are given. Further\nexamples are included in the code of most functions and\nare executed when the functions are called without input\narguments.\n3.1. Similarity Measures\nFigures1-4showhowtherepresentationsusedbythesim-\nilarity measures reﬂect characteristics of the music. In al l\nﬁgures,darkershadingscorrespondtohighervalues. Each\nsubplot represents a different piece of music, namely (a)\nChopin - Waltz (Op.69 No.1), (b) Bon Jovi - Its My Life,\nand(c)KaiTracid-Tiefenrausch. Thelaterbelongstothe\ngenre dance/electronic and has very strongbeats.\n3.2. Simple Evaluation\nThere are different ways to evaluate similarity measures.\nA true evaluation can only be performed in the context of\na speciﬁc application and usually requires extensive user\nstudies. However, several work arounds to compare dif-\nferent parameter settings have been presented, e.g. in [3]\nthe authors use the R-precision. A simple script which\ndemonstrates how such an evaluation can be performed is\nincluded inthetoolbox (“ma simpleeval”).\nResults from such an evaluation are shown in Table 1\nincluding the computation times. The data set consists\nof 118 pieces classiﬁed into 19 categories. Note that the\ncomputation time for the feature computation for AP 30\ncan be reduced by using less iterations of the expectation\nmaximization algorithm and stopping the iteration when\ntheGaussianmixturemodelconverges. Ascanbeseenthe\nperformance of AP 30 clearly outperforms SH, PH, and\nFP, while it is comparable to LS 30. Preliminary exper-\niments indicated that the main difference in performanceFrequency BandLoudness [dB]\n102030−20−100102030\n102030\nFrequency Band(a)\nFrequency BandLoudness [dB]\n102030−40−2002040\n102030\nFrequency Band(b)\nFrequency BandLoudness [dB]\n102030−2002040\n102030\nFrequency Band(c)\nFigure 1. Frame Clustering (AP 30). On the left, the\nshadings correspond to the combined density distribution\nof the 30 Gaussians. On the right, the lines depict the\ncluster centers (i.e. the means ofthe Gaussians).\nPeriodicity [BPM]Strength\n50100150200\n(a) Periodicity [BPM]Strength\n50100150200\n(b) Periodicity [BPM]Strength\n50100150200\n(c)\nFigure 2. Periodicity Histograms.\nbetween LS and AP is a result of using EMD instead of\nMonte Carlo sampling.\nIn addition to comparing R-precision values it is inter-\nesting to study confusion matrices. We compute the con-\nfusion between classes A (true) and B (estimated) as the\naverage ratio (over all elements in A) of elements from\nB in the set of nnearest neighbors to an element from\nA. Where nis the number of elements in B. As can be\nseen some genres are very clearly distinguished such as\nblues(blu),classicorchestra(clo),classicpiano(clp), and\nspeech (spe). Furthermore, we had 3 classes which only\ncontained pieces from one artist (bar, nma, sub), all of\nwhich are also well distinguishable. Most of the other\nclassesarepoorlydistinguished,includingalternative( alt),\npop,andromanticdinnermusic(rom). Oneexplanationis\nthat the classes are deﬁned subjectively and are not nec-\nBark\nModulation [Hz]2468101020\n013\n(a)Bark\nModulation [Hz]2468101020\n239\n(b)Bark\nModulation [Hz]2468101020\n181\n(c)\nFigure 3. Fluctuation Patterns.\nLoudness [sone]Bark\n5101520\n(a)Loudness [sone]Bark\n5101520\n(b)Loudness [sone]Bark\n5101520\n(c)\nFigure 4. Spectrum Histograms.\nalt\nblu\nclo\nclp\ndan\neur\nhap\nhar\nhip\nmys\npop\npun\nroc\nrnr\nrom\nspe\nbar\nnma\nsub\nalt\nblu\ncloclp\ndan\neur\nhap\nhar\nhip\nmys\npoppun\nroc\nrnr\nrom\nspe\nbar\nnma\nsubLS\nalt\nblu\nclo\nclp\ndan\neur\nhap\nhar\nhip\nmys\npop\npun\nroc\nrnr\nrom\nspe\nbar\nnma\nsub\nalt\nblu\ncloclp\ndan\neur\nhap\nhar\nhip\nmys\npoppun\nroc\nrnr\nrom\nspe\nbar\nnma\nsubAP\nFigure 5. Confusion matrices for LS 30 and AP 30. The\nrowsrefertotheestimatedclassesandcolumnstothetrue\nclasses. Black corresponds to100%, white tozero.\nessarily conﬁned to distinctive timbres. As expected the\nmain characteristics of the AP 30 and LS 30 confusionspe(3)spe(3)spe(3)spe(3)spe(3)spe(3)spe(3)spe(3)spe(3)spe(3)spe(3)spe(3)spe(3)spe(3)spe(3)spe(3)spe(3)spe(3)spe(3)spe(3)spe(3)spe(3)spe(3)spe(3)spe(3)\nmys(2)mys(2)mys(2)mys(2)mys(2)mys(2)mys(2)mys(2)mys(2)mys(2)mys(2)mys(2)mys(2)mys(2)mys(2)mys(2)mys(2)mys(2)mys(2)mys(2)mys(2)mys(2)mys(2)mys(2)mys(2)\nclo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)\nclp(5)clp(5)clp(5)clp(5)clp(5)clp(5)clp(5)clp(5)clp(5)clp(5)clp(5)clp(5)clp(5)clp(5)clp(5)clp(5)clp(5)clp(5)clp(5)clp(5)clp(5)clp(5)clp(5)clp(5)clp(5)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)\nspe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)spe(1)\nmys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)\nclo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)clo(1)\nclp(2)clp(2)clp(2)clp(2)clp(2)clp(2)clp(2)clp(2)clp(2)clp(2)clp(2)clp(2)clp(2)clp(2)clp(2)clp(2)clp(2)clp(2)clp(2)clp(2)clp(2)clp(2)clp(2)clp(2)clp(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)\nclo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)\nblu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)blu(2)\nmys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)\nclo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)clo(2)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)\nmys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)\nrnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)\nrom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)rom(2)\nspe(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)\nblu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)blu(1)\nmys(1)\nmys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)mys(1)\ndan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)dan(1)\npop(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)hip(1)\nsub(1)\nrom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)\nrom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)\ndan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)dan(1)\nhip(1)\npop(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)rnr(1)\ndan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)dan(1)\nsub(1)\nrom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)rom(1)\ndan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)dan(1)\neur(1)\nhap(1)\nhip(1)\nrnr(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)eur(1)\npun(1)\ndan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)dan(1)\nroc(1)\nsub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)\nalt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)alt(1)\neur(1)\nhip(1)\nsub(1)\nsub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)sub(1)\nalt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)alt(1)\nhap(1)\npop(1)\nroc(1)\nrom(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)hip(1)\nalt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)alt(1)\npop(1)\nroc(1)\nsub(1)\npun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)\nalt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)alt(1)\ndan(1)\nhap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)hap(3)\npop(2)\npun(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)alt(1)\nmys(1)\nnma(1)\npun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)pun(1)\nnma(1)\neur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)eur(1)\nnma(1)\nnma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)bar(4)\nblu(1)\nhar(1)\nrnr(1)\neur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)eur(1)\nhar(1)\nroc(1)\nnma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)nma(1)\npun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)pun(3)\nhar(1)\nrnr(1)\nalt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)alt(1)\nhar(1)\nbar(1)\nhap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)hap(1)\nhar(1)\npun(1)\nrnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)rnr(1)\nnma(1)\nFigure 6. Islands of Musiccreated usingsplineinterpolation and SD H with s= 2.\nmatrices are very similar.\n3.3. Islandsof Music\nIncombinationwiththeSOM4andSDH5toolboxesitis\npossible to create the Islands of Music visualization with\nonly fewlines of code.\nAn example is shown in Figure 6, example code can\nbe found in “ma simpleiom”. The distances are com-\nputed using FC/CMS (AP 30) on the same data used for\nthe small evaluation. In the upper left there is an island\nrepresenting speech (spe), in the lower left there is an is-\nlandwithclassicpiano(clp)andclassicorchestra(clo). T o\nthe south-east of the speech island there is an island with\nmainly blues (blu), to the north-east of the blues island\nthere are some hip-hop pieces (hip). However, further to\ntheeastofthemapthedistinctionsarenotasclear. Thisis\nconﬁrmedbytheconfusionmatrices. Forexample,rock’n\nroll (rnr), rock (roc), and punk rock (pun) seem randomly\ndistributed.\n4. ACKNOWLEDGMENTS\nThe Austrian Research Institute for Artiﬁcial Intelligenc e\nis supported by the Austrian Federal Ministry for Edu-\ncation, Science, and Culture and by the Austrian Federal\nMinistry for Transport, Innovation, and Technology. This\nresearch was supported by the EU project FP6-507142\nSIMAC6andthenationalprojectY99-INF,sponsoredby\nthe Austrian Federal Ministry of Education, Science, and\nCulture intheform of aSTART Research Prize.\n5. REFERENCES\n[1] J.-J. Aucouturier and F. Pachet, “Music similarity\nmeasures: What’s theuse?,” in Procof ISMIR , 2002.\n4http://www.cis.hut.ﬁ/projects/somtoolbox\n5http://www.oefai.at/˜elias/sdh\n6http://www.semanticaudio.org[2] J.-J. Aucouturier and F. Pachet, “Finding songs that\nsound the same,” in Proc of IEEE Benelux Work-\nshop on Model Based Processing and Coding of Au-\ndio, 2002.\n[3] J.-J. Aucouturier and F. Pachet, “Improving Timbre\nSimilarity: How high’s the sky?,” in Journal of Neg-\nativeResearchResultsinSpeechandAudioSciences ,\nvol. 1, no. 1, 2004.\n[4] A. Berenzweig, D.P.W. Ellis, and S. Lawrence, “An-\nchor space for classiﬁcation and similarity measure-\nment ofmusic,” in Proc of ICME , 2003.\n[5] J.T.Foote, “Content-basedretrievalofmusicandau-\ndio,” inProcofSPIEMultimediaStorageandArchiv-\ningSystems II ,1997, vol. 3229.\n[6] B. Logan and A. Salomon, “A music similarity func-\ntionbasedonsignalanalysis,” in ProcofICME ,2001.\n[7] E. Pampalk, A. Rauber, and D. Merkl, “Content-\nbased organization and visualization of music\narchives,” in Proc of ACM Multimedia , 2002.\n[8] E. Pampalk, S. Dixon, and G. Widmer, “Exploring\nmusic collections by browsing different views,” in\nProc of ISMIR ,2003.\n[9] E.Pampalk,S.Dixon,andG.Widmer, “Ontheevalu-\nation of perceptual similaritymeasures for music,” in\nProc of DAFx , 2003.\n[10] Y. Rubner, C. Tomasi, and L. J. Guibas, “The earth\nmover’s distance as a metric for image retrieval,” Intl\nJournal of Computer Vision , vol. 40, no. 2, 2000.\n[11] E.D.Scheirer, “Tempoandbeatanalysisofacoustic\nmusical signals,” JASA, vol. 103, no. 1, 1998.\n[12] G.TzanetakisandP.Cook, “Marsyas: Aframework\nfor audio analysis,” Organised Sound , vol. 4, no. 3,\n2000."
    },
    {
        "title": "Tempo Tracking with a Single Oscillator.",
        "author": [
            "Bryan Pardo"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415090",
        "url": "https://doi.org/10.5281/zenodo.1415090",
        "ee": "https://zenodo.org/records/1415090/files/Pardo04.pdf",
        "abstract": "I describe a simple on-line tempo tracker, based on phase and period locking a single oscillator to performance event timings. The tracker parameters are optimized on a corpus of solo piano performances by twelve musicians. The tracker is then tested on a second corpus of performances, played by the same twelve musicians. The performance of this tracker is compared to previously published results for a tempo tracker based on combining a tempogram and Kalman filter.",
        "zenodo_id": 1415090,
        "dblp_key": "conf/ismir/Pardo04",
        "keywords": [
            "on-line",
            "tempo",
            "tracker",
            "phase",
            "period",
            "locking",
            "oscillator",
            "performance",
            "event",
            "timings"
        ],
        "content": "TEMPO TRACKING WITH A SINGLE OSCILLATOR\n Bryan Pardo  \n Northwestern University Department of Computer Science \n1890 Maple Avenue, Evanston, IL 60201-3150 \n+1-847−491−3500  \npardo@northwestern.edu  \nABSTRACT \nI describe a simple on-line tempo tracker, based on \nphase and period locking a single oscillator to performance event timings. Th e tracker parameters are \noptimized on a corpus of solo piano performances by twelve musicians. The tracker is then tested on a second corpus of performances, played by the same twelve musicians. The performance of this tracker is compared \nto previously published results for a tempo tracker based on combining a tempogram and Kalman filter.  \n1. INTRODUCTION \nTempo tracking real music performances by machine \nhas been the subject of mu ch research. With some \nexceptions [1], recent systems divide into those using a \nbank of oscillators (such as the work of Large and Jones [2] and Goto [3]) and those based on probabilistic graphical models (such as the work of Raphael [4] and Cemgil and Kappan [5]). \nI am interested in applying tempo tracking to \naccompaniment of a semi-impr ovisational style, such as \nJazz or Blues. In such mu sic, the exact sequence of \nnotes to be played is unspecified (during an improvised solo, for example). Because of this, one must use approaches that work in real-time, with no score \nknowledge. Of the tempo-tracking approaches referred to in this section, only those based on oscillators and the tempo tracker described in the 2001 paper by Cemgil et al. [6] apply. \nCemgil, et al. modeled tempo as a stochastic \ndynamical system, using a Bayesian framework. Here, tempo is a hidden state variable of the system and its value over time is modelled by a Kalman filter. They improve tracking accuracy by using a wavelet-like multi-scale expansion of the performance and backtracking with smoothing. Excellent results are reported on a corpus of MIDI piano performances, and the authors made this corpus available to other researchers to allow compar ison between systems on the \nsame corpus. \n \n Dixon [7] compared the Cemgil system to an off-line, \ntwo-pass tempo tracking system [8]. His results show the performance of the two-pass system to be statistically indistinguishable from the Cemgil et al. system. \nSince Dixon’s system was designed for off-line use, \nthis leaves open the question of which approach, oscillator or Kalman filter, is more effective for on-line tempo tracking. As a first step towards answering this question, I built a simple, oscillator-based tempo tracker, and tested its performance on the corpus used \nby Cemgil et al . This paper describe the tracker, the test \ncorpus used, the performance measures and parameter optimizations applied, and compares results to those in Cemgil et al. \n2. THE TEMPO TRACKER \nThe tempo tracker created for this paper is based on a \nsingle oscillator, whose period and phase adjust to synchronize with a sequence of events.  \nThe system treats a performance as a time series of \nweights, W, over T time steps (hereafter referred to as \n“ticks”). Here, w\nt is the weight at tick t. For this paper, \nweight is defined as the number of note onsets occurring at tick t. If there are no note onsets at t, w\nt = 0. This \napproach to time is natural in the world of MIDI, where all events occur on clock ticks.  When dealing with audio, one must define a mapping from time-steps into time. In this case, the time step t would typically \ncorrespond to window of analysis t.  \nBefore beginning, the initial beat onset and period \nmust be selected. The method typically used by human musicians is to wait for a minimum of two events to occur. The time between the first two events is the \ninitial estimate of the period and the onset time of the second event is taken as the start of a beat. The approach I take is similar. \nThe first beat, b\n0, is defined as the first tick whose \nweight is non-zero. The second beat, b1, is the second \ntick whose weight is non-zero, subject to the constraint that the time between them is at least the minimal allowed period for a beat, p\nmin. The minimal allowed \nperiod prevents a sloppily played pair of notes, supposed to occur simultaneously, from setting the initial tempo estimate to a very fast value. For this paper, I fix p\nmin = 0.1 seconds (600 beats per minute).   Permission to make digital or hard c opies of all or part of this work \nfor personal or classroom use is granted without fee provided that \ncopies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. \n© 2004 Universitat Pompeu Fabra. \n   \n \nThe initial estimates for the next beat onset, b2, and \nbeat period, p1, work on the assumption the next beat \nperiod will be identical to the initial beat period. Once initialized, the tracker is upda ted at every clock tick, \nusing the steps outlined in Equations 1 through 8. \nEquation 1 finds the distance between the current \ntick, t, and the expected onset of the next beat, b\ni. The \nvalue, d, is measured in units of the current estimated \nbeat period, pi. If d > 0, the current tick is after the \nexpected beat onset.  If d > 0, it is prior to the expected \nbeat onset. \ni\nitbdp−=  (1) \nIf  (| | ) and ( 0)t dwε<>  (2) \n        (1 )ia v gp pk d=+  (3) \n         1 ii ibb p−=+  (4) \nElse if ( )  ( 0)ittb w>∧ = =  (5) \n          1 ii=+  (6) \n         1 iip p−=  (7) \n         1 ii ibb p−=+  (8) \n \nBeat onset and period estimat es are only affected by \nevents that fall within a window range [, ]εε− of the \nexpected beat onset. Window width is measured in \nperiods, not ticks. Thus, as the estimated tempo slows, the window widens, since a period becomes longer.  \nIf Equation 2 is true, Equation 3 updates the period \nestimate. This equation depends on p\navg, a weighted \naverage of the last n beat periods (in this paper, n = 20), \nwhere the weight of each period is exponentially discounted by the memory parameter, m. Increasing m \nhas the effect of smoothing the response of the tracker by increasing the weight applied to past periods. Equation 9 calculates the weighted average.  \n1\n0\n1\n0n\nj\nij\nj\navg n\nj\njpm\np\nm−\n−\n=\n−\n==∑\n∑ (9) \n \nOnce the average period is calculated, Equation 3 \nupdates the current period estimate. Here the correction factor, k, determines how far the estimate is adjusted in \nresponse to an event. The larger k is, the farther the \nperiod and beat estimates are adjusted. Equation 4 \nupdates the estimate of the next beat onset. \nIf Equation 5 evaluates to “true,” the current tick, t, is \npast the window around b\ni where the beat onset may be \naffected. In this case, beat and period estimates are updated by Equations 6 through 8. Thus, during a tacit passage, the beat tracker will continue to report beats using the current estimate of period and phase.  3. ERROR MEASUREMENT \nWhen the tracker processes a performance, it produces a \nsequence of beat onsets, B = {b\n0, b1,…,  bn-1}. Given a \nknown-correct beat sequence for the performance, C = {c\n0, c1,…,  cm-1}, the error in the sequence returned \nby the tracker can be measured.  Define bj as the \nelement of the estimate sequence closest to the ith beat \nonset in the correct sequence. Equation 10 defines phase error for correct beat c\ni. \n1||\n||ij\ni\niicbeccφ\n+−=− (10) \nFigure 1 shows an example calculation of phase error \nfor the second correct beat. Here, the phase error between the second estimated beat and the closest correct beat is 0.3. The phase  error for the third correct \nbeat approaches 0, as the n earest estimated beat is quite \nclose to the correct beat. \nCorrect beat Estimated beat \nTime in ms       0      100             200           300 Phase Error  = 0.3 Period Error = |log2(70/100)|  =  0.52 \n70 \n100 30 \n \nFigure 1. Examples of phase and period error \nEquation 11 definesp\nie, the period error for the ith \ncorrect beat. Here, pj is the period estimate \ncorresponding to estimate beat bj. Figure 1 shows the \nperiod error estimate for the second correct beat.  \n2\n1log  j p\ni\niipecc+= −  (11) \n \nWhen 0p\nie=, the period of the nearest estimated \nbeat is exactly equal to the correct beat. If 2p\nie=, the \ntracker has found a period either ¼ or 4 times that of the \ncorrect beat period. Note that phase error and period error are relatively independent, and it is possible to be in-phase with the correct beat, while having a period of twice the correct value.  \nmax ( )\n(, ) 1 0 0()\n2ji j\niWc b\nBCnmρ−\n=+∑\n (12) \nCemgil et al. created ρ, an error measure (the Cemgil \nscore) that combines elemen ts of period error and phase \nerror. This is defined in Equation 12. Recall that m is \nthe number of beats in the correct sequence, C, and n is \nthe number of beats in the sequence returned by the tracker, B. Here, W() is a Gaussian window function \ndefined in [6].  \nIt can bee seen that \nρ grows from 0 to 100 as phase \nerror and period error decrease to 0, and  ρ = 100  only if \nthe correct beat sequence, C, exactly equals the one \nreturned by the tracker, B.    \n \n4. TRAINING CORPUS TESTING CORPORA \nTo compare results between systems, it is important to \ntest both systems on the same testing corpus, and train on the same training corpus. Thus, I test and train on the same corpora used in Cemgil et al.  \nFor the training corpus, a piano arrangement of \nMichelle  (by the Beatles) was given to twelve piano \nplayers. Four were profe ssional Jazz musicians, five \nwere professional classical musicians, and three were \namateurs. Each subject was asked to perform the piece at three tempi:  “normal,”  “slow, but still musical,” and “fast.” It was left up to the player to determine what “normal,” “slow,” and “fast” meant. Three repetitions were recorded for each tempo and each performer. One amateur was unable to play Michelle, resulting in a corpus of 99 performances (11 subjects, 3 tempi per subject, 3 performances per tempo).  \nA testing corpus was created, using the same twelve \nsubjects and protocol, with an arrangement of the Beatles’ Yesterday . All twelve musicians were able to \nplay the Yesterday  arrangement, resulting 108 \nperformances.  \nPerformances were recorded as MIDI, using a \nYamaha Disklavier C3 Pro grand piano connected to a Macintosh G3 computer running Opcode Vision DSP. Once recorded, the score time for each performance \nnote was written into the file as a MIDI text message, to be used as an answer key. \nMost performances in both the Michelle corpus and \nthe Yesterday corpus vary in a 20 beat-per-minute range. The median tempo of the Michelle corpus is roughly 60 beats per minute, while that of the Yesterday corpus is 90 beats per minute. Space requirements preclude a more detailed discussion of the corpora. For more detail, please consult http://www.nici.kun.nl/ mmm/\n, where both corpora are \navailable for download. \n5. OPTIMIZING PARAMETERS \nThe memory parameter, m and the correction rate \nparameter k adjust the responsiveness of the simple \noscillator tracker to tempo variation. The window size \nparameter, ε, determines how far out of phase a stimulus \nmay be and still affect the tracker’s beat and period estimate.  \nTo explore the space of possible parameter settings, I \nrandomly selected 5,000 combinations of k, m, and\nε, \nchoosing values for all parameters from a uniform \ndistribution over the interval (0,1). For each combination of parameter settings, I ran the tracker on all 99 performances in the Michelle training corpus, \nrecording the mean values for phase error, e\nφ.  \nThe best set of parameters found for the Michelle \ncorpus was 0.65,   0.36,  and 0.43mk ε == = , \nreturning a mean eφ = 0.0244, or 0.024 seconds per beat, \ngiven an average tempo of 60 beats per minute. 6. TESTING \nUsing the parameters that minimize phase error on the \nMichelle corpus, I ran the tracker on all performances of Yesterday in the test corpus. Figure 2 shows a histogram of tracker performance on the Yesterday corpus for phase error, peri od error, and Cemgil score. \nThe vertical dimension indicates the number of performances falling into a given bin.  The horizontal dimension indicates the valu e of an error measure.  \n0 0.1 0.2 0.3 0.4 0.50102030Phase ErrorNumber of performances\n0 0.1 0.2 0.3 0.4 0.5051015Period ErrorNumber of performances\n-20 0 20 40 60 80 100 120051015Cemgil ScoreNumber of performancesmean = 0.07\nstd    = 0.03 \nmean = 0.08\nstd     = 0.03 \nmean = 81.7\nstd    =   8.9\n \nFigure 2.  Phase error ( eφ), period error ( ep), and \nCemgil score ( ρ) on the Yesterday corpus \nWhile the scores for period error would seem to \nindicate that the tracker co rrectly locked on to the \nquarter note level, the tracker  actually locked on to the \neighth note in every case. Since my scores are intended to be comparable to those published in Cemgil et al. [6], I have treated the eighth note as the beat level, as their scores were also adjusted to account for tracking at the eighth note level, rather than the quarter note level.  \nThe phase error values in Figure  2 indicate the \ntracker was, on average, out-of phase by 7% of the value of an eighth-note on a typical performance. The \nmean tempo in the Yesterday corpus is 90 beats per minute, for an average beat onset error of 0.023 seconds per beat.  \nIt is instructive to look at performances with high and \nlow error scores. Figure 3 shows three individual tempo tracks from the Yesterday cor pus. In this figure, the \nvertical dimension indicates the tempo in beats per minute. The horizontal dimension indicates the beat, from the beginning of the performance to the end. A solid line with black points shows the tempo from the answer key. The dashed line with hollow circles \nindicates the output of the tracker. \nThe upper panel of Figure 3 shows the performance \non which the tracker performed the worst. In this case, \nClassical pianist 2 suddenly increased the tempo from 54 to 65 beats per minute, causi ng the tracker to lose the \nbeat. The tracker never recovered. Appropriately, this performance had poor values on all error measures.    \n \nThe middle panel shows a typical tempo track on a \nperformance in the Yesterda y corpus. This performance \nis also by Classical musici an 2, and was selected for \ndisplay because it gives a good idea of the typical performance of the tempo track er. In this performance, \nthe tracker looses the tem po in the third measure of \nYesterday, then recovers by the fourth measure and continues to track the b eat with success for the \nremainder of the performance. \nThe lowest panel shows an above-average tempo \ntrack, as indicated by all performance measures. Here, the system tracked with su ccess, from beginning to end. \n0 10 20 30 40 50 60 70 80 9030405060708090Beats per MinuteWORST:  Yesterday, Classical Pianist 2, Slow Performance, Rep. 2\n0 10 20 30 40 50 60 70 80 9060708090100110120Beats per MinuteMEDIAN: Yesterday, Classical Pianist 2, Fast Performance, Rep. 1\n0 10 20 30 40 50 60 70 80 908090100110120130140\nBeatBeats per MinuteBEST:  Yesterday, Jazz Pianist 3, Fast Performance, Rep. 2 Cemgil score = 43.2 period error = 0.18 phase error = 0.18\nCemgil score = 82.4 period error = 0.07 phase error = 0.06\nCemgil score = 95.7 period error = 0.05 phase error = 0.03\n \nFigure 3.  Performance on individual files \nTable 1 compares the Cemg il scores for the tracker \ndescribed in this paper to those of the tempogram tracker, and those of a tempogram plus a 10\nth order \nKalman filter, described in Cemgil et al [6]. All systems were trained on the Michelle corpus and tested on the Yesterday corpus. Values are rounded to the nearest whole number. Values in parentheses are standard deviations. The top row of this table shows mean values for all performances in the Yesterday corpus. The next \nthree rows show mean values for all Amateur, Jazz, and Classical performers, respectiv ely. The final three rows \nshow values for the corpus, broken down by tempo.  \nGroup  Single \nOscillator Tempogram Tempogram \n+ Kalman  \nAll Perfs. 82 (8) 74 (12) 86 (9) \nAmateur 82 (7) 74 (7) 88 (5) \nClassical 76 (13) 66 (14) 82 (11) \nJazz 88 (5) 81 (7) 92 (4) \nFast 84 (12) 79 (9) 90 (6) \nNormal 83 (5) 74 (9) 88 (6) \nSlow 77 (12) 68 (9) 84 (10) \nTable 1.  Cemgil scores on the Yesterday corpus \nTable 1 shows that the single oscillator tracker \nperforms better than the tempogram, and somewhat worse than the tempogram plus 10\nth order Kalman filter. \nThe mean scores for the single oscillator fall within a single standard deviation of the scores returned by the tempogram plus 10\nth order Kalman filter. This indicates \nthat the performances of the two systems are, statistically speaking, very close. \n7. CONCLUSIONS \nI described a tempo tracker, based on a single oscillator, \nthat is simple to implement for real-time tempo following, requires no score knowledge, and shows performance comparable to a more complex tracker \nbased on a tempogram plus 10\nth order Kalman filter.  \nWhile the results of this experiment do not resolve \nwhether an oscillator approach is preferable to a Kalman filter, the average phase error for the simple oscillator tracker on a corpus of 108 performances of Yesterday played by 12 musicians wa s 23 milliseconds per beat. \nAverage error this small indicates the tracker is good enough for many tasks requiring tempo estimation. \n8. ACKNOWLEDGMENTS \nThanks to Richard Ashley, of Northwestern University, \nfor collecting the training and testing corpora.  \n9. REFERENCES \n[1] Rosenthal, D.F., Machine Rhythm: Computer \nEmulation of Human Rhythm Perception, in Media Arts and Science. 1992, MIT: Cambridge, MA. p. 139. \n[2] Large, E.W. and M.R. Jones, The Dynamics of \nAttending: How People Track Time-Varying Events. Psychological Review, 1999. 106(1): p. 119-159. \n[3] Goto, M. and Y. Muraoka, Real-time beat \ntracking for drumless audio signals: Chord change detection for mu sical decisions. Speech \nCommunication, 1999. 27: p. 311-335. \n[4] Raphael, C., A Hybrid Graphical Model for \nRhythmic Parsing. Artificial Intelligence, 2002. 132(1-2): p. 217-238. \n[5] Cemgil, A.T. and B. Kappan, Monte Carlo \nMethods for Tempo Tracking and Rhythm Quantization. Journal of Artificial Intelligence Research, 2003. 18: p. 45-81. \n[6] Cemgil, A.T., et al., On tempo tracking: \nTempogram representation and Kalman filtering. Journal of New Music Research, 2001. 28(4): p. 259-273. \n[7] Dixon, S. An empirical comparison of tempo \ntrackers. In 8th Brazilian Symposium on Computer Music. 2001. Fortaleza, Brazil. \n[8] Dixon, S. and E. Cambouropoulos. Beat \ntracking with musical knowledge. in The 14th European Conference on Artificial Intelligence. 2000. Amsterdam, Netherlands."
    },
    {
        "title": "Musical key extraction from audio.",
        "author": [
            "Steffen Pauws"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416326",
        "url": "https://doi.org/10.5281/zenodo.1416326",
        "ee": "https://zenodo.org/records/1416326/files/Pauws04.pdf",
        "abstract": "The realisation and evaluation of a musical key extraction algorithm that works directly on raw audio data is pre- sented. Its implementation is based on models of human auditory perception and music cognition. It is straightfor- ward and has minimal computing requirements. First, it computes a chromagram from non-overlapping 100 msecs time frames of audio; a chromagram represents the likeli- hood of the chroma occurrences in the audio. This chro- magram is correlated with Krumhansl’s key profiles that represent the perceived stability of each chroma within the context of a particular musical key. The key profile that has maximum correlation with the computed chroma- gram is taken as the most likely key. An evaluation with 237 CD recordings of classical piano sonatas indicated a classification accuracy of 75.1%. By considering the ex- act, relative, dominant, sub-dominant and parallel keys as similar keys, the accuracy is even 94.1%.",
        "zenodo_id": 1416326,
        "dblp_key": "conf/ismir/Pauws04",
        "keywords": [
            "chromagram",
            "Krumhansls key profiles",
            "classification accuracy",
            "75.1%",
            "evaluation",
            "237 CD recordings",
            "classical piano sonatas",
            "computing requirements",
            "human auditory perception",
            "music cognition"
        ],
        "content": "MUSICAL KEY EXTRACTION FROM AUDIO\nSteffen Pauws\nPhilips Research Laboratories Eindhoven\nProf. Holstlaan 4\n5656 AA Eindhoven, the Netherlands\nsteffen.pauws@philips.com\nABSTRACT\nThe realisation and evaluation of a musical key extraction\nalgorithm that works directly on raw audio data is pre-sented. Its implementation is based on models of human\nauditory perception and music cognition. It is straightfor-\nward and has minimal computing requirements. First, itcomputes a chromagram from non-overlapping 100 msecs\ntime frames of audio; a chromagram represents the likeli-\nhood of the chroma occurrences in the audio. This chro-\nmagram is correlated with Krumhansl’s key proﬁles that\nrepresent the perceived stability of each chroma withinthe context of a particular musical key. The key proﬁle\nthat has maximum correlation with the computed chroma-\ngram is taken as the most likely key. An evaluation with237 CD recordings of classical piano sonatas indicated a\nclassiﬁcation accuracy of 75.1%. By considering the ex-\nact, relative, dominant, sub-dominant and parallel keys as\nsimilar keys, the accuracy is even 94.1%.\n1. INTRODUCTION\nBesides tempo, genre and music mood, musical key is\nan important attribute for Western (tonal) music though\nonly musically well-trained people can identify the key in\na piece of music easily [3]. Knowing the key of a piece of\nmusic is relevant for further music analysis or for musicapplications such as mood induction; the mode of the key\nis deemed to provide a speciﬁc emotional connotation [6].\n1.1. Related work\nThe extraction of key from music audio is not new, but\nnot often reported in literature (see, for instance, Leman’salgorithm [8] for an exception in which human tone center\nrecognition is modeled).\nMany algorithms that are found in literature work on\nsymbolic data only (e.g., MIDI or notated music) by\neliminating keys if the pitches are not contained in thekey scales [9], by looking for key-establishing harmonic\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proﬁ t or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\nc/AD2004 Universitat Pompeu Fabra.\naspects [4] or key-establishing aspects at accent loca-\ntions [1], by using the tonal hierarchy [7] extended with\nthe role of subsidiary pitches and sensory memory [5], by\nsearching for keys in the scalar and chordal domain in par-allel [12], by harmonic analysis [11], by median ﬁltering\nusing an inter-key distance [10], or by computing a inter-\nkey distance using a geometric topology of tonality (i.e.,\nthe spiral array) [2].\n2. METHOD\nOur approach to key extraction starts by computing a\nchromagram over six octaves from A0 (27.5 Hz) to A6\n(1760 Hz) from the raw audio data. This chromagram rep-resentation is used as input to the maximum-key proﬁle\ncorrelation (MKC) algorithm to get the musical key.\n2.1. Chromagram computation\nThe chromagram is deﬁned as the restructuring of a spec-\ntral representation in which the frequencies are mapped\nonto a limited set of 12 chroma values in a many-to-one\nfashion. This is done by assigning frequencies to the ‘bin’\nthat represents the ideal chroma value of the equally tem-pered scale for that frequency. The ‘bins’ correspond to\nthe twelve chromas in an octave.\nTo this end, the spectrum/CB /B4 /CU /B5is modelled as a com-\nbination of the spectral content of the perceptual pitch\nand the musical background, denoted as background level/AH /B4 /CU /B5at frequency /CU,/CB /B4 /CU /B5/BP\n/C6/CG/D2 /BP/BD\n/CW\n/D2 /A0 /BD/CF /B4 /D2/CU /B5 /BT /B4 /D2 /B5/B4 /AH /B4 /CU /B5/B7 Æ /B4 /D2/D4/CX\n/A0 /CU /B5/B5(1)\nwhere the spectral pitch content is modeled as a scaled\nimpulse train reﬂecting the interpretation of pitch as a har-\nmonic series; it contains high energy bursts at integral\nmultiples /D2/D4/CX, for harmonic index /D2.F u r t h e r , /C6denotes\nthe number of harmonics, /CW /B4 /AK /BD/B5denotes the factor con-\ntrolling peak contribution to the pitch percept, /CF /B4 /A1 /B5is an\narc-tangent function representing the transfer function of\nthe auditory sensitivity ﬁlter, and /BT /B4 /D2 /B5is the gain for har-\nmonic /D2.\nThe computation of Equation 1 is done by adding\nharmonically compressed amplitude FFT-based spectrumrepresentations, for which the following properties are im-\nplemented.\n1. Spectral content above 5 kHz is cut off by down-\nsampling the signal. It is assumed that harmonics\nin the higher frequency regions do not contributesigniﬁcantly to the pitches in the lower frequency\nregions.\n2. Only a limited number of harmonically compressed\nspectra are added. We use/C6 /BP/BD /BH .\n3. Spectral components (i.e., the peaks /BT /B4 /D2 /B5 /BP/CB /B4 /D2/D4/CX\n/B5) are enhanced to cancel out spurious peaks\nthat do not contribute to pitches.\n4. Spectral components at higher frequencies con-\ntribute less to pitch than spectral components atlower frequencies. We use/CW /BP/BC /BM /BK/BF.\n5. The frequency abscissa is transformed to a loga-\nrithmic one by means of interpolation, since human\npitch perception follows logarithmic laws.\n6. A weighting function is used to model the human\nauditory sensitivity; the perceived loudness of a\npitch depends on its frequency. We use an arc-\ntangent function.\nFrom an algorithmic point of view, the input signal is\npartitioned in non-overlapping time frames of 100 mil-liseconds. If the signal is in stereo format, a mono version\nis created by averaging both channels ﬁrst.\nSince further processing considers only the musical\npitches from A0 (27.5 Hz) to A6 (1760.0 Hz), the har-\nmonic compression is done over 6 octaves from 25 Hz un-\ntil 5 kHz, also to capture some harmonics of the higher\npitch frequencies. So, spectral content at frequencies\ngreater than 5 kHz are not taken into account. A low-pass ﬁltering of 10 kHz by FIR approximation and a dec-\nimation process bandlimits and downsamples the signal.\nThis down-sampling decreases dramatically the comput-\ning time necessities without affecting results seriously.\nThe ’remaining’ samples in a frame are multiplied by a\nHamming window, zero-padded, and the amplitude spec-\ntrum is calculated from a 1024-point FFT. This spectrum\nconsists of 512 points spaced 4.88 Hz on a linear fre-quency scale. Next, a procedure is applied aiming at en-\nhancing the peaks without seriously affecting frequencies\nor their magnitudes. Only values at and around the spec-tral peaks are taking into account by setting all values at\npoints that are more than two FFT points (9.77 Hz) sepa-\nrated from a relative maximum, equal to 0. The resulting\nspectrum is then smoothed using a Hanning ﬁlter.\nSince a linear resolution of 4.88 Hz is far too limited for\nthe lower pitch regions (the pitch frequency difference be-\ntween C2 and C/CL2 is 3.89 Hz), the values of the spectrum\non a logarithmic frequency scale are calculated for 171(/CS /BD/BC/BE/BG /BP /BI /CT) equidistant points per octave by cubic-spline\ninterpolation. The interpolated spectrum is multiplied by\na raised arc-tangent function, mimicing the sensitivity of\n C D EF G A B1234567(a) C major\npitch classesmean probe tone rating\n C D EF G A B1234567(b) C minor\npitch classesmean probe tone rating\nFigure 1 . Mean probe tone rating (or key proﬁles) in the\ncontext of the key C major (a) and the key C minor (b).Adopted from [7]\nthe human auditory system for frequencies below 1250\nHz. The result is shifted along the logarithmic frequency\nscale, multiplied by a decreasing factor/CWand added for all\nharmonics to be resolved ( /C6 /BP /BD/BH) resulting in the har-\nmonically compressed spectrum deﬁned over at least six\noctaves.\nThe chromagram for each frame is computed by locat-\ning the spectral regions in the harmonically compressed\nspectrum that correspond with each chroma in A-440equal temperament. For the pitch class C, this comes\ndown to the six spectral regions centered around the pitch\nfrequencies for C1 (32.7 Hz), C2 (65.4 Hz), C3 (130.8Hz), C4 (261.6 Hz), C5 (523.3 Hz) and C6 (1046.5 Hz).\nThe width of each spectral region is a half semitone\naround this center. The amplitudes in all four spectral re-\ngions are added and normalized to form one chroma re-\ngion. Adding and normalizing the chromagrams over allframes results in a chromagram for the complete music\nsample.\n2.2. Maximum key-proﬁle correlation\nThe maximum key-proﬁle correlation is an algorithm for\nﬁnding the most prominent key in a music sample [7].\nOriginally, the algorithm was devised for symbolic encod-ings of music (e.g., MIDI). Here, it is used as a back-end\nto a signal processing step that works on raw audio data.\nThe MKC algorithm is based on key proﬁles that\nrepresent the perceived stability of each chroma within\nthe context of a particular musical key. Krumhansl and\nKessler [7] derived the key proﬁle by a probe tone rating\ntask. In this task, subjects were asked to rate, on a scale of\n1 to 7, the suitability of various concluding pitches afterthey had listened to a preceding musical sample that estab-\nlished a particular key. The mean ratings represent the key\nproﬁles and show clear differences in the perceived stabil-ity of the chromas: highest ratings are given to the tonic,\nand the other two pitches of the triad, followed by the rest\nof pitches of the scale to be concluded by the non-scale\npitches (see Figure 1).\nKey proﬁles only depend on t he relationship between a\npitch and a tonal center and not on absolute pitches. Con-\nsequently, proﬁles for different major or minor keys are\nall transpositions of each other.\nThe MKC algorithm is based on the assumption thatthe most stable chromas occur most often in a music sam-\nple. It computes the correlation (i.e., Pearson’s product\nmoment correlation) between the distribution of chroma\noccurrences in the musical sa mple and all 24 key proﬁles.\nRecall the chromagram takes the role of this distribution\nof chroma occurences given as a vector with 12 elements.\nThe key proﬁle that provides the maximum correlation istaken as the most probable key of the musical sample. The\ncorrelation value can be used as the salience of the per-\nceived key or the degree of tonal structure of the musicsample.\nThe strong point of the MKC algorithm is that it uses\na differential weighting of all scale pitches and non-scale\npitches. This means that the tonic, the perfect ﬁfth and the\nthird and all other pitches vary in importance in establish-\ning a key at listeners.\n3. EV ALUATION\nThe evaluation of the algorithm consisted of a perfor-\nmance assessment in ﬁnding the correct key from a set of237 performances of classical piano sonatas on CD. The\ncorrect key was deﬁned as the main key for which the mu-\nsical composition was originally composed. Recall thatmusic composers use various key modulating techniques\nto build up tension and relaxation in the music. However,\nmany compositions start and end with the same key; thesepieces are called monotonal .\nThe following CDs were used. All 237 recordings of\nthe CDs were used in the experiment.\nRosalyn Tureck\nJ.S. Bach, The Well-tempered Clavier Books I & II, 48 Preludes\nand Fugues\nRecording New York 12/1952 - 5/1953Remastering 1999, Deutsche Grammophon, 463 305-2\nJeno Jando\nJ.S. Bach, The Well-tempered Clavier Book I, 24 Preludes andFuguesNaxos Classical, 8.55379697, 1995\nVladimir Askenazy\nD. Shostakovich, 24 Preludes & Fugues, op.87,Decca, 466 066-2, 1999.\nGlenn Gould\nJ. Brahms, The Glenn Gould Edition,Sony Classical, Sony Music Entertainment, 01-052651-10,1993.\nEvgeny Kissin\nF.F. Chopin, 24 Preludes Op. 28, Sonate no. 2, Marche funebre /\nPolonaise op.53Sony Classical, Sony Music Entertainment.\nThe original key of the compositions was compared\nwith the extracted key from the CD-PCM data of the piano\nperformances in fragments of 2.5, 5.0, 10.0, 15.0, 20.0,\n25.0 and 30.0 seconds at the start, at the middle and atthe end of the performances. Lastly, the complete perfor-\nmance was analysed. Note that a simple time measure of 4\nbeats at a tempo of 100 beats per minute takes 2.4 seconds.Also, note that the end of the performances (as found on\nthe CDs) is often played by slowing down and sustaining\nthe closing chord until it ‘dies out’.\nlength (secs.)\n start middle end\n2.5\n 53.6% (127) 21.9% (52) 38.8% (92)\n5.0\n 59.1% (140) 25.6%(61) 44.3% (105)\n10.0\n 61.2% (145) 29.5% (70) 58.2% (138)\n15.0\n 66.2% (157) 30.0% (71) 58.7% (139)\n20.0\n 67.5% (160) 32.9% (78) 64.1% (152)\n25.0\n 71.7% (170) 34.2% (81) 64.6% (153)\n30.0\n 72.2% (171) 37.1% (88) 66.7% (158)\nTable 1 .Classiﬁcation accuracy for ﬁnding the exact main\nkey in 237 piano sonatas for variable-length fragments\nfrom 2.5 to 30 seconds at various locations.\nexact\n 75.1% (178)\nRel.\n 6.8% (16)\nV\n 1.3% (3)\nIV\n 6.3% (15)\nPar.\n 4.6% (11)\ntotal\n 94.1%(223)\nTable 2 .Classiﬁcation accuracy for ﬁnding the exact main\nkey in 237 complete piano sonatas. Confusions with the\nrelative, dominant (V), sub-dominant (IV) and the parallel\nkey are given. In the last row, the classiﬁcation accuracy\nis shown if we consider all these related keys as correct\nkeys.\nIn Tables 1 and 2, the results are shown in terms of per-\ncentage correct. If we consider the algorithm as reliable,\nit is evident that most of the classical compositions can betermed as monotonal. As shown in Table 1, analysing the\nstart and the end of a performance for at least a 25 second\nfragment provides a sensible judgement of the main key\n(i.e., 65-72% correct). The last ﬁve seconds of a classi-\ncal piano performance provides a unreliable judgement ofthe main key, as too little data on harmony are present due\nto the last ‘dying out’ chord of most performances. Also,\nthe middle of a performance should not be used to extractthe main key since the composition might already have\ngone through various key changes. As shown in Table 2,\nto obtain the best possible judgement of the main key of\na performance with an accuracy of 75.1%, the complete\nperformance needs to be analysed.\nThe algorithm makes mistakes by confusing the exact\nmain key with its relative, dominant, sub-dominant or par-\nallel key. As shown in Table 2, this is considerable (about\n4-7%) for the relative, sub-dominant and parallel keys.The cause of these key confusions needs to be sought in\nthe way in which the piano sonatas are composed. How-\never, since these keys are all ‘friendly’ to each other, theycan all be considered as similar in particular music ap-\nplications. Then, the classiﬁcation accuracy amounts to\n94.1%.4. CONCLUSION\nThe present key extraction algorithm starts by computing\nthe chromagram from raw audio data of a musical frag-\nment. To this end, it extracts the likelihood of all possiblepitches in the range from A0 (27.5 Hz) to A6 (1760 Hz)\nby computing harmonically compressed spectra in non-\noverlapping time frames of 100 msecs. The likelihood ofall pitches are collected in a single octave and averaged for\nthe complete musical fragment to arrive at a chromagram.\nThe algorithm needs only minimum amount of computing\nnecessities; it runs in parallel while the system is playing\nout music allowing online tracking of the harmonic pro-gression in the music.\nThis chromagram is used in a correlative comparison\nwith the key proﬁles of all 24 Western musical keys.These key proﬁles express what chromas are most impor-\ntant (i.e., most stable) in a given key on a rating scale from\n0 to 7. The key which proﬁle demonstrates the highest cor-\nrelation with the provided chromagram is taken as the key\nof the musical fragment under study.\nThe algorithm identiﬁes correctly the exact main key\nin 75.1% of the cases by analysing the complete CD\nrecording of piano sonatas. If we assume exact, relative,dominant, sub-dominant and parallel keys as similar, it\nachieves a 94.1% accuracy. We have no data on recordings\nwith other instrumentation or from other musical idioms.\nThe algorithmic performance seems to comply with hu-\nman performance. Note that mu sically trained people can\nidentify the correct key in 75% of the cases, tough after\nlistening only to the ﬁrst measure [3]. However, we do\nnot know to what extent the algorithm and humans makesimilar mistakes. This is concern for further research.\nConcluding, the following weak points of the current\nalgorithm need attention:/AFThe raw audio data are taken as is, whereas a pre-processing stage might reveal fragments in a musi-\ncal performance that contain key-relevant informa-\ntion and fragments that do not. An check on har-monisity and transients, for instance, may clearly\ndiscern fragments with harmonic instruments car-\nrying prime information on musical key from noisy,\npercussive instruments./AFMusic perceptive and cognitive factors that estab-\nlish a musical key at a human listener can be fur-\nther integrated into the algorithm. Temporal, rhyth-\nmic and musical harmonic factors of pitches are not\nmodelled, whereas it is known, for instance, that\nthe temporal order of pitches and the position ofpitches in a metrical organisation (e.g., the ﬁrst beat,\nstrong accents) inﬂuence both the perception of a\ntonal center (i.e., the tonic of the key)./AFMusic theoretical and compositional constructs arenot modelled in the algorithm. Composers use var-ious key modulation techniques in which they sig-\nnify how strong a new key will be established (i.e.,\nSchenkerian analysis). For instance, a cadence inroot position and more than three chords from the\ndiatonic scale establish a strong (new) key in theo-\nretical sense.\n5. REFERENCES\n[1] Chafe, C., Mont-Reynaud, B., Rush, L., To-\nward an intelligent editor of digital audio:Recognition of musical constructs.\nComputer\nMusic Journal, 6 , 30-41, 1982.\n[2] Chew, E., An Algorithm for Determining Key\nBoundaries. In Proceedings of the 2nd Intl\nConference on Music and Artiﬁcial Intelli-\ngence , 2002.\n[3] Cohen, A.J., Tonality and perception: Mu-\nsical scales prompted by excerpts from DasWohl-temperierte Clavier of J.S.Bach.\nPaper\npresented at the Second Workshop on Physical\nand Neuropsychological Foundations of Mu-\nsic, Ossiach, Austria , 1977.\n[4] Holtzman, S.R., A program for key determina-\ntion, Interface, 6 , 29-56, 1977.\n[5] Huron, D., Parncutt, R., An improved model\nof tonality perception incorporating pitch\nsalience and echoic memory, Psychomusicol-\nogy, 12 , 154-171, 1993.\n[6] Kastner, M.P., Crowder, R.G., Perception of\nthe Major/Minor Distinction: OIV . Emotional\nconnations in young children, Music Percep-\ntion, 8, 2 , 189-202.\n[7] Krumhansl, C.L., Cognitive Foundations of\nMusical Pitch , Oxford Psychological Series,\nno. 17, Oxford University Press, New York,\n1990.\n[8] Leman, M., Schema-based tone center recog-\nnition of musical signals, Journal of New Mu-\nsic Research, 23 , 169-204, 1994.\n[9] Longuet-Higgins, H.C., Steedman, M.J., On\ninterpreting Bach, Machine Intelligence, 6 ,\n221-241, 1971.\n[10] Shmulevich, I., Yli-Harja, O. Localized Key-\nFinding: Algorithms and Applications. Music\nPerception, 17, 4 , p. 531-544, 2000.\n[11] Temperly, D., An algorithm for harmonic anal-\nysis. Music Perception, 15, 1 , 31-68, 1997.\n[12] V os, P.G., Van Geenen, E.W. A parallel pro-\ncessing key-ﬁnding model. Music Perception,\n14, 2 , 185-224, 1996."
    },
    {
        "title": "Industrial audio fingerprinting distributed system with CORBA and Web Services.",
        "author": [
            "Jose Pedro",
            "Vadim Tarasov",
            "Eloi Batlle",
            "Enric Guaus",
            "Jaume Masip"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1414792",
        "url": "https://doi.org/10.5281/zenodo.1414792",
        "ee": "https://zenodo.org/records/1414792/files/PedroTBGM04.pdf",
        "abstract": "With digital technologies, music content providers face serious challenges to protect their rights. Due to the wide- spread nature of music sources, it is very difficult to cen- tralize the audio management. Audio fingerprinting al- lows the identification of audio content regardless of the audio format and without the need of additional metadata. Monitoring the audio being broadcasted by the TV and radio stations of a country requires the design and imple- mentation of a scalable, robust and modular framework. We have chosen CORBA as distributed environment. The whole functionality needs to be decoupled from clients. To do so, Web services have been deployed. The audio identification core uses a Hidden Markov Model-based audio fingerprinting technology. The paper discusses the design and implementation issues of a complete distribut- ing system that automatically monitors audio content, spe- cifically music and commercials.Today, a working proto- type of such a system already exists, and is dedicated to monitoring several radio and tv stations in Spain.",
        "zenodo_id": 1414792,
        "dblp_key": "conf/ismir/PedroTBGM04",
        "keywords": [
            "digital technologies",
            "music content providers",
            "audio management",
            "audio fingerprinting",
            "audio identification",
            "corba",
            "web services",
            "hidden markov model",
            "audio content monitoring",
            "audio commercials"
        ],
        "content": "INDUSTRIAL AUDIO FINGERPRINTING DISTRIB UTED SYSTEM WITH\nCORB AAND WEB SER VICES\nJose P.G.Mahedero, Vadim Taraso v,Eloi Batlle, Enric Guaus, Jaume Masip\nAudio visual Institute\nUniversitat Pompeu Fabra\nOcata 1,Barcelona, Spain\nfjpgarcia, vtaraso v,eloi, eguaus, jmasip g@iua.upf.es\nABSTRA CT\nWithdigital technologies, music content providers face\nserious challenges toprotect their rights. Due tothewide-\nspread nature ofmusic sources, itisverydif\u0002cult tocen-\ntralize theaudio management. Audio \u0002ngerprinting al-\nlowstheidenti\u0002cation ofaudio content regardless ofthe\naudio format andwithout theneed ofadditional metadata.\nMonitoring theaudio being broadcasted bytheTVand\nradio stations ofacountry requires thedesign andimple-\nmentation ofascalable, robustandmodular frame work.\nWehavechosen CORB Aasdistrib uted environment. The\nwhole functionality needs tobedecoupled from clients.\nTodoso,Webservices havebeen deplo yed. The audio\nidenti\u0002cation core uses aHidden Mark ovModel-based\naudio \u0002ngerprinting technology .Thepaper discusses the\ndesign andimplementation issues ofacomplete distrib ut-\ningsystem thatautomatically monitors audio content, spe-\nci\u0002cally music andcommercials.T oday ,aworking proto-\ntype ofsuch asystem already exists, andisdedicated to\nmonitoring severalradio andtvstations inSpain.\n1.INTR ODUCTION\nDue totheoutgro wing \u0002eld ofdigital rights management\n(DRM), theneed toautomate thetracking ofdifferent au-\ndiosources arose. Traditionally thistaskwasaccomplished\nbysampling thesources (orjustasking them) andstoring\nwhat hadbeen broadcasted inacertain period oftime. At\nthetime theonly sources ofaudio which companies kept\ntrack ofwere radio stations.\nTimewent byandsome factors showed theneed for\ncompanies toautomate thetracking process. Among these\nfactors wecould mention thegrowthofthemusic rights\nmanagement companies andtheir signi\u0002cance, theincreas-\ningnumber ofradio stations, andoneofthemost impor -\ntant (ifnotthemost) thearrivaloftheInternet andthe\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forpro\u0002t orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation onthe\u0002rstpage.\nc\r2004 Universitat Pompeu Fabra.resulting exponential growthofmusic transfer .\nAsthenumber ofsources andthequantity ofaudio it-\nselfwasreally huge, itbecame clear thatadistrib uted sys-\ntemsupporting manypluggable sources wasanoptimal\nsolution.\nThesystem needed tobepowerful enough toservedif-\nferent clients1each ofthem having itsownrequirements.\nItalso hadtoprovide ef\u0002cient load balancing andfault\ntolerance. Theexperience indistrib uted systems pointed\ntheuseofCORB A(http://www .CORB A.org)asarobust\nplatform tointegrate andcooperate manyheterogeneous\nsystems [5].\n2.MAIN DESCRIPTION\nWehaveimplemented asystem composed byseveralnodes\ninchargeoftheaudio recognition. Theycommunicate in\nahierarchical structure inwich each node tries torecog-\nnize audio. Incase anode isunable toidentify apattern,\nitpasses therequest tothenextnode ofthehierarchy .The\nidenti\u0002cation procedure continues until itreaches thetop\nlevelofthehierarchy .\nNodes actascaches ofthetoplevelnode. Each of\nthem hasalistofpatterns representing aportion ofall\nthemodels present inthesystem, soanode willidentify\naportion ofwhat thewhole thesystem isable to.Ifthe\naudio fragment isnotidenti\u0002ed bythetoplevelnode, the\naudio passes tomanual (human) processing. See\u0002gure 1\nEach identi\u0002cation transaction (either positi veornega-\ntive)islogged inorder toprovide feedback information to\ntheclients. Theycanalso monitorize thebeha vior ofthe\nsystem andadjust global parameters depending ontheir\nneeds.\n1Wewillusetheterm client torefer toaprogram thatwillcommu-\nnicate with thesystem aswell astheenduser ofthesystem. This non\ndistinction ismade because from thepoint ofviewofthesystem these\nterms (program client anduser client) aresimilar andthesystem makes\nnodifference.Figur e1.main description\nFrom alogical point ofview,oursystem iscomposed\nofdifferent layers each ofthem with itsownpurpose:\nIdenti\u0002cation :thecore ofthesystem. This layer isin\nchargeoftheidenti\u0002cation oftheaudio. Ittakesau-\ndiostreams asinput andtries toidentify them using\ntheaudio patterns datase. Itconsists ofC++ audio\nrecognition libraries.\nDistrib ution :groups physical units into virtual nodes\nthat cooperate forthesame purpose, forexample\ncommercials, music orTVvirtual node. Itman-\nages theload ofeach virtual node andisincharge\nofdistrib uting jobs tothenodes depending ontheir\nrespecti veload. This layer sends jobs totheidenti-\n\u0002cation layer andloggs theresults recei vedfrom it.\nThis layer also manages each virtual node' spattern\nlisttoavoidoverloading thetopnode byproviding\nlowerlevelnodes with feedback information from\nhigher levelnodes. This process results inanup-\ndated pattern database foreach node. Thus, alower\nlevelnode willnotfailtoidentify anygivenpattern\nmore than once. Atthesame time with load balanc-\ning,thislayer isresponsible forerror recoveryand\nhandle system failures. CORB Aisused toprovide\nseamless integration between remote components.\nIntelligence :itprovides mechanisms forprioritizing jobs\neither ondemand byclient request ortosatisfy the\ndistrib ution layer needs. Italso executes jobs pe-\nriodically orcanevenestimate processing times to\ngiveclients abetter feedback. Aswell asthedis-\ntribution layer ,thisoneuses CORB Aasaplatform\nforprocess distrib ution andcoordination.\nIntegration :alayer inchargeofmaking thesystem\naccessible tomanydifferent clients each ofthem\nhaving itsownoperating system andrequirements.\nThis isaccomplished byusing web services, which\nprovide afunctional APIofthesystem.User interface :theuseofweb services letsuptoclient\ntheimplementation oftheendinterf ace,sothispart\nisnotcovered inthispaper .\n3.IDENTIFICA TION LAYER\nSystem overvie wThe system presented here goes be-\nyond thetemplate matching paradigm toastatistical\npattern matching paradigm. Thegoals aretoincor -\nporate robustness tothesystem bystatistically mod-\neling theaudio evolution while reducing thesizeof\nthe\u0002ngerprint byconsidering local andglobal per-\nceptual redundancies inacorpus ofmusic data.\nFeature extraction A\u0002lter-bank based analysis approx-\nimates thehuman inner earbeha vior andtheMel-\ncepstrum coef\u0002cients (MFCC) [2]analise themusic\ntimbers [4].\nChannel estimation Ifthedistorting channel H(!)isslowly\nvarying wecandesign a\u0002lter that, applied tothe\ntime sequence ofparameters, isable tominimize\ntheeffects ofthechannel. The \u0002lter wedesigned\nforoursystem isH(z)=0:991\u0000z\u00001\n1\u00000:98z\u00001.\nHidden Mark ovModel Observ ers\nPolyphonic music canbeseen asasequences ofsi-\nmultaneous acoustic events (notes). In[1],wede-\n\u0002ned asetofabstract events that allowamathe-\nmatical description ofcomple xmusic assequences\nofevents. These events arecaptured byHidden\nMark ovModels(HMM)[6 ]thatmodel abstract au-\ndiogenerators. (trained with theExpectation-Maxim-\nization algorithm [3])\nIdenti\u0002cation Algorithm\nEach HMM \u0002ngerprint inthesong database uniquely\nidenti\u0002es each song among theothers. The\u0002nger -\nprint database isgenerated using theViterbi algo-\nrithm [8].Theidenti\u0002cation algorithm matches an\ninput streaming audio against allthe\u0002ngerprints to\ndetermine whene verasong section hasbeen de-\ntected. The Viterbi algorithm isused again with\nthepurpose ofexploiting theobserv ation capabili-\ntiesoftheHMM models contained inthe\u0002ngerprint\nsequences.\n4.DISTRIB UTION LAYER\nThis layer hasmanytasks toaccomplish: group nodes :\nvirtual nodes represent ahardw areabstraction layer that\nincreases system scalability .Assur eagood load balanc-\ning:inadistrib uted system itisessential tomakeagood\nload balancing inorder tohaveanalmost constant through-\nput. This isalso atask ofthislayer .Fault toler ance :a\nnode may bedisconnected during processing without any\nconsequences. Feedbac kclients andnodes :clients need\ntorecei vetheresults oftherecognition processes aswellastomonitorize theglobal beha viour ofthesystem. As\nwementioned intheMain description section (2),nodes\narefedback from thetopnode topreventfuture identi\u0002-\ncation failures.\nThis layer iscomposed ofthreedifferentlevels (see\n\u0002gure2):\n-Physical level:each node isseen asasingle pro-\ncessing unit.\n-Logical unitlevel:hardw areresources aregrouped\ntogether intovirtual nodes. This leveldivides iden-\nti\u0002cation tasks depending onwhich kind ofaudio it\nwillidentify .Forexample wecanhavethree units\nforcommercial audio identi\u0002cation grouped together\nintoonevirtual node. Other example could besev-\nerallogical units hosted onthesame physical ma-\nchine. Thenumber ofphysical units thatform each\nvirtual node canbecon\u0002gured to\u0002tthesystem needs.\n-Process level:thisiscomposed ofvirtual nodes and\nalogger .Each virtual node isbuiltofthreeele-\nments :Data provider s:provide data from hetero-\ngeneous sources inauni\u0002ed manner .Reco gnizer :\nidenti\u0002es theaudio supplied bythedata providers.\nContr oller :handles communication andsynchro-\nnization issues between data providers andrecog-\nnizer .Italso communicates with other controllers\ntoperform load balancing anderror recovery.In\ncase ofidenti\u0002cation failure itischargeofpassing\ntheidenti\u0002cation tasktothetoplevelnode.\nApart from virtual nodes there isaspecial component\nnamed loggerwho isinchargeoflogging alltheeventsre-\nceivedfrom thecontrollers ofthevirtual nodes. Itwrites\nlogs tothedatabases. Theycanbelater consulted by\nclients bycalling theservices provided bytheIntegration\nlayer .Logs re\u0003ect information about therecognition pro-\ncess such astiming, which node processed theaudio, kind\nofaudio processed etc. This information help clients to\ntrack thewhole identi\u0002cation process.\n5.INTELLIGENCE LAYER\nThe Intelligence layer servesfortheprocess optimiza-\ntion.It isseparated intoseveralintelligent agents thatper-\nform automatic stream tagging based onmetadata. The\nsystem preprocesses thestream description, separates the\nfragments with voice from those with music. Italso per-\nforms sourcesignal cachemana gement .Thesource sig-\nnaliscached andarranged inmanageable fragments al-\nlowing tominimize thenumber ofqueries made toexter-\nnaldata storage system, thus minimizing thetraf\u0002c.This\nlayer also provides identi\u0002cation result optimization, au-\ntomatic jobmanagement, automatic pattern listgeneration\nandpattern reinde xation.\nFigur e2.Distrib ution layer\n6.INTEGRA TION LAYER\nThis layer integrates modules provided bythedistrib u-\ntion (see 4Distrib ution) andtheIntelligence (see 5In-\ntelligence) layer with theclients. Atthesame time islets\nserver-client interaction asmuch open aspossible bypro-\nviding afunctional API.\nSOAP(http://www .w3.or g/TR/soap/) isalightweight pro-\ntocol based onXML-RPC calls. Itisdesigned toworkin\nadecentralized environment. Itsitsontopoftheproto-\ncolstack andcanusemanydifferent transport protocols\nsuch asSMTP ,FTP orother .Howeveritsmost widely\nused overHTTP because ofitsfacilities for\u0002rewall/proxy\n\u0002ltering.[7 ]\nFrom ageneral point ofview,working with SOAPcon-\nsists onmaking requests toaWebService (from nowon\nWSorsimply service )inaspeci\u0002c SOAPformat, and\nrecei veresponses from theWS. The Service interf aceis\ndescribed inaWSDL (http://www .w3.or g/TR/wsdl, asu-\nperset ofXML) \u0002leindicating methods calls, objects, and\nexceptions thatwill besent across thenet. AWebSer-\nvice isasetofrelated methods exposed toclients via\nSOAPprotocol. Asalltheexchange ofinformation is\nmade with XML (both data andcontrol messages), SOAP\nmakespossible theinteraction between different platforms\norlanguages such asCOM, CORB A,Perl, Tcl, theJava-\nlanguage, C,C++, Python, orPHP programs running any-\nwhere ontheInternet.\nFeatur esofWebServices :Applications cancommuni-\ncate across theInternet, theyarelanguage andplatform\nindependent, theycanrunovermanyprotocols, andthey\narehuman readable.\nDue totheir features, WSsuitperfectly ourneeds. The\naims ofthelayer areaccomplished byusing WSbecause\ntheyactasabridge between clients andmodules inthe\nunderlying layers which useaCORB Ainfrastructure. Byproviding a\u0002ne grained functional API, WS de\u0002ne the\nwayclients communicate with thesystem, butdon'tim-\npose anyrestriction ontheir implementation. InaMVC\npattern thiswould mean acomplete separation oftheView\nfrom Model andContr oller andforce clients only tofol-\nlowSOAPencoding rules butnottouseaspeci\u0002c lan-\nguage orenduser interf ace.\n6.1. implementation\nFigur e3.Integration layer organization\nThis layer isbuiltondifferent modules (see\u0002gure 3):\n-Axis:Axis (http://ws.apache.or g/axis/) isthename\noftheimplementation oftheSOAPspeci\u0002cation we\nuse. Itsisaservlet that must beplugged intoa\nservlet container .Itsresponsibility istoroute SOAP\nmessages between services andclients.\n-SOAPServer :weuseTomcat Serverasaservlet\ncontainer toroute SOAPrequests andresponse to\nandfrom AXIS servlet.\n-SQL Server :mysql isused asapersistence layer\n(audio model storage, system information, etc.)\n-Contr oller s:there aretwodifferent controllers. Ser-\nvice contr oller sthatmanage database access mod-\nulesandCORB Acontr oller sthathandle interactions\nwith theunderlying layers using CORB A.\n-CORB AComponents :components inchargeofdis-\ntribution, load balancing andaudio processing. Apart\nfrom distrib ution tasks, theyaccept audio recogni-\ntionjobs from their controllers andinvokeIdenti\u0002-\ncation layer components.\n7.CONCLUSIONS\nThe combination ofHidden Mark ovModels applied to\n\u0002ngerprinting, with CORB AandWebServices results in\narobust,scalable andperformant distrib uted frame work.\nHidden Mark ovModels provide anaccurate \u0002ngerprinting\ntechnique which ismade robustandscalable with theuse\nofCORB A.Theintegration provided byWebServices lets\nasmuch open aspossible theinteraction between clients\nandsystem functionalities.8.REFERENCES\n[1]E.Batlle. Automatic Song Identi\u0002cation inBroadcast Au-\ndio. Proc.IASTED Signal andImageProcessing Confer -\nence,2002.\n[2]J.A.F.E.Batlle, C.Nadeu. Feature Decorrelation Meth-\nodsinSpeech Recognition. IntConf onSpok enLangua ge\nProcessing ,1998.\n[3]A.D.etAltri. Maximum Likelihood from Incomplete Data\nviatheEMAlgorithm. Journal oftheRoyal Statistical So-\nciety ,39(1):138, January 1977.\n[4]B.Logan. Mel Frequenc yCepstral Coef \u0002cients forMusic\nModeling. Proc.ISMIR ,2000.\n[5]S.V.Michi Henning. Advanced CORB Aprogramming with\nC++ .Addison Wesley,1999.\n[6]L.R.Rabiner .ATutorial onHMM andSelected Appli-\ncations inSpeech Recognition. Proceedings oftheIEEE ,\n77(2):257286, 1989.\n[7]E.D.Simon StLaurent, JoeJohnston. Programming Web\nServices with XML-RPC .OReilly ,2001.\n[8]A.J.Viterbi. Error Bounds forConvolutional Codes and\nanAsymptotically Optimum Decoding Identi\u0002cation. IEEE\nTrans.Info. Theory ,1967."
    },
    {
        "title": "Clustering Symbolic Music Using Paradigmatic and Surface Level Analyses.",
        "author": [
            "Anna Pienimäki",
            "Kjell Lemström"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417447",
        "url": "https://doi.org/10.5281/zenodo.1417447",
        "ee": "https://zenodo.org/records/1417447/files/PienimakiL04.pdf",
        "abstract": "In this paper, we describe a novel automatic cluster analy- sis method for symbolic music. The method contains both a surface level and a paradigmatic level analysing block and works in two phases. In the first phase, each music document of a collection is analysed separately: They are first divided into phrases that are consequently fed on a harmonic analyser. The paradigmatic structure of a given music document is achieved comparing both the melodic and the harmonic similarities among its phrases. In the second phase, the collection of music documents is clus- tered on the ground of their paradigmatic structures and surface levels. Our experimental results show that the novel method finds some interesting, underlying similari- ties that cannot be found using only surface level analysis.",
        "zenodo_id": 1417447,
        "dblp_key": "conf/ismir/PienimakiL04",
        "keywords": [
            "automatic",
            "symbolic",
            "music",
            "novel",
            "method",
            "surface",
            "paradigmatic",
            "analysis",
            "phrases",
            "harmonic"
        ],
        "content": "CLUSTERING SYMBOLIC MUSICUSINGPARADIGMA TICAND\nSURFACELEVELANALYSES\nAnna Pienim ¨akiandKjell Lemstr ¨om\nUniversityofHelsinki\nDepartment ofComputer Science\u0000Anna.Pienimaki, Kjell.Lemstrom \u0001@cs.Helsinki.FI\nABSTRACT\nInthispaper ,wedescribe anovelautomatic cluster analy-\nsismethod forsymbolic music. Themethod contains both\nasurfacelevelandaparadigmatic levelanalysing block\nandworks intwophases. Intheﬁrst phase, each music\ndocument ofacollection isanalysed separately: Theyare\nﬁrst divided into phrases thatareconsequently fedona\nharmonic analyser .Theparadigmatic structure ofagiven\nmusic document isachie vedcomparing both themelodic\nandtheharmonic similarities among itsphrases. Inthe\nsecond phase, thecollection ofmusic documents isclus-\ntered ontheground oftheir paradigmatic structures and\nsurfacelevels. Our experimental results showthat the\nnovelmethod ﬁnds some interesting, underlying similari-\ntiesthatcannot befound using only surfacelevelanalysis.\n1.INTRODUCTION\nThe area ofMusic Information Retrie val(MIR) hasex-\ntended tocontain severaltracks. One ofthese ismusic\nclassiﬁcation. Here weconsider anunsupervised version\nofclassiﬁcation, cluster analysis. Wewillcallthepieces\nofmusic documents andthesetofallthegivendocuments\ncollection .\nThe aimofthecluster analysis istocluster themost\nsimilar objects close toeach other .Thesimilarity between\nclustered objects ismost commonly measured using dis-\ntance functions, which deﬁne objects tobethemore simi-\nlartoeach other thesmaller thedistance isbetween them.\nInMIR clusters areused, forexample, inplaylist gener -\nation orstyle identiﬁcation tasks. Musicologists, onthe\nother hand, may usetheresults ofcluster analysis when\npreprocessing their analysis material, such asmaterial col-\nlected during aﬁeld study .\nMusic canbeseen asastructural phenomenon consist-\ningofmanylevels[5].The surfacelevel,distinct notes\norsmall combinations ofthem, isquite easily approached\nandtherefore widely studied incomputer -assisted musi-\nPermission tomakedigitalorhardcopiesofallorpartofthisworkfor\npersonalorclassroom useisgrantedwithoutfeeprovidedthatcopies\narenotmadeordistributedforpro®torcommercial advantageandthat\ncopiesbearthisnoticeandthefullcitationonthe®rstpage.\nc\n\u00022004UniversitatPompeuFabra.cology andMIR. Indeed, inmanycases there isnoneed\nfordeeper ,musicological analysis. Forinstance, amateur\nlisteners recognise similarities between pieces ofmusic\nmainly bysurfacelevelcharacteristics such asmelodic\nandrhythmic patterns. Formusicologists, thesurfacelevel\nanalysis isnecessary butbynomeans sufﬁcient. However,\nthere areonly afewstudies (see e.g.[1])thatalgorithmi-\ncally analyse deeper structural levels,such astheparadig-\nmatic level,i.e.,arough segmentation ofthemusic docu-\nment intophrases [6].\nThis paper introduces anewcluster analysis method\nforsymbolic music data based onboth paradigmatic and\nsurfacelevelanalysis. Each document ofthecollection\nisﬁrstanalysed andencoded using itsparadigmatic struc-\nture, melody ,andharmon yasitsparameters. Then the\nwhole collection isclustered using adistance measure that\ncalculates both theparadigmatic andthesurfacelevelsim-\nilarity among themusic documents.\nBackground. Indataclustering themain three problems\narehowtodescribe thedata items comparably ,what kind\nofdistance measure issuitable fordescribing thesimilar -\nities between data items, andhowtoﬁndthemost repre-\nsentati veitem foracluster .Often data items arerepre-\nsented by\u0003-dimensional vectors thatarecompared byus-\ningsome nicely beha ving distance measure (such assome\nmetric ).\nWhen dealing with (symbolic) music, selecting asuit-\nable feature tobeextracted may notbeobvious. Thedata\nitems may alsobecomple xwhich makesitdifﬁcult toﬁnd\nasuitable distance measure. Moreo ver,when dealing with\nnon-numerical values, each representati veofacluster has\ntobeanexisting data item, notanyartifactsuch asthe\nmean value ofthecluster .\nIntheliterature, onecanﬁndtwoapproaches forclus-\ntering music. Theﬁrst, which wecallparadigmatic clus-\ntering ,gets individual documents asinput (intheform of\npre-deﬁned phrases oranalysed document excerpts ofap-\nproximately equal length) andaims atﬁnding some inner\nstructure (seee.g.[1]). Thephrases aredescribed byusing\nseveralfeatures such asthemelodic curve,theinterv als\nofthemelody ,andtherhythmic relationships. Theother\napproach, collection clustering ,aims atmaking clusters\nofdocuments inagivencollection (see e.g.[2]).These\nmethods often workonsecondary data, such asstatisticalcharacteristics ofthemelody .\nOurnovelmethod, tobedescribed inthefollo wing sec-\ntions, combines ideas ofboth ofthese approaches. Itﬁrst\nsegments each document ofthecollection into phrases;\nthephrase similarity isbased both onthemelodic line\nandontheharmonic analysis oftheexcerpt containing\nthephrase athand. Theparadigmatic structure ofthedoc-\numents isobtained bycluster analysis using information\ncollected during theformer phase. Todescribe thedoc-\numents weuseadjacenc ylists; each such listisassoci-\nated with adocument andstores results ofparadigmatic\nandsurfacelevelanalyses ofthecorresponding document.\nThe whole collection canthen beclustered byusing the\nadjacenc ylists.\n2.CLUSTERING PARADIGMA TICLEVEL\nUSINGSURFACELEVELINFORMA TION\nLetusnowdescribe howweanalyse individual docu-\nments. This includes describing thesurfacelevelandthen\nforming paradigmatic clusters based onthesurfacelevel\ndescriptions. Theadjacenc yliststoring these results isde-\nscribed inSection 3.\nTemperle yhasintroduced heuristics forsegmenting\nmonophonic music into phrases [7]. Inorder toapply\nthem here, weneed amelody extraction method because\nourcollection contains mainly polyphonic documents.\nForthemoment, weuseasimplistic approach thatcon-\nsiders notes with thehighest pitch values asmelody .\nHaving applied Temperle y’sphrase segmentation, we\nhavethemelodic phrases.Foreach melodic phrase, we\nalsoneed theassociated polyphonic conte xt,theharmonic\nphrase.Having extracted these outoftheoriginal docu-\nment, weanalyse them using Temperle y’sharmonic anal-\nysis heuristics [7].Because thelength oftheharmonic\nphrases may vary,wecompress them byreplacing runs of\nanysymbol with asingle symbol. Forinstance, string\u0000\u0000\u0002\u0001\u0003\u0001\u0005\u0004\u0006\u0001\u0007\u0004\b\u0000 isreplaced by\u0000\t\u0000\n\u0001\u000b\u0001\u0007\u0004\b\u0000 .\nLet \f\u000e\r\u000f\f\u0011\u0010\u0013\u0012\u0015\u0014\u0017\u0016\u0019\u0018\u001a\u0018\u0019\u0018\u001b\f\u0011\u0010 \u0003\u001c\u0014beastring oflength \u0003onan\nalphabet \u001d.Weusesuch strings torepresent extracted\nphrases: Let \u001ebesuch anextracted phrase. Then \u001e\u0011\u001f \r\u001e!\u001f\"\u0010\u0013\u0012\u0015\u0014#\u0016\u001a\u0018\u0019\u0018\u001a\u0018\u0019\u0016\u0017\u001e\u001c\u001f$\u0010 \u0003\u001c\u0014and \u001e&%'\r(\u001e&%)\u0010*\u0012\u001a\u0014\u0017\u0016+\u0018\u001a\u0018\u001a\u0018\u0019\u0016\u0017\u001e\u001c%\u0007\u0010 \u0003\u001c\u0014represent\nitsmelodic andharmonic parts, respecti vely.Ifthere are,extracted phrases forsome document -,weform asim-\nilarity matrix .0/ofsize\n,21\",,such thatthecell .3/\n465 7,for\u0012\t8:9+\u0016<;=8\n,,isgivenbytheformula:.\n/\n4>5 7\r(?3@\u0006A\n%\u0007B\u001e\n%\n4\u0016\u0017\u001e\n%7=C\u0011DB\u0012FEG?\nC@\u0006A\n\u001f\"B\u001e\n\u001f\n4\u0016\u0017\u001e\n\u001f\n72C\u0016(1)\nwhere A2\u001f and AH% aremelodic andharmonic editdis-\ntances, respecti vely,asdeﬁned below.Factor ?( IJ8K?L8\u0012)adjusts theweights ofthecomponents.\nTheharmonic distance,A2%,ontwostrings\u001eand\u001e\u001cMof\nlengthsN \u001eONP\r \u0003andN \u001e&MQNP\r\u0003R iscalculated bythefollo w-ingrecurrence resembling theconventional editdistance:A\n%\nS\u001bS\r I (2)A\n%\nTVU\r WYX[Z\n\\]]^]]_\nAH%\nTa`!b5\nUD\u0012dcAH%\nT5\nU\u0015`!bD\u0012dcAH%\nTa`!b5\nU\u0015`!bD(if \u001eO\u0010 e#\u0014&\rf\u001e+M#\u0010 gd\u0014then I\nelse h\nBe\u001b\u0016ig\nC) \u0018\nThedistance between strings\u001eand\u001e\u001cMcanthen befound\nreading thevalue oftheentryA2%\njlk.Theadditional part,h\nBe\u001b\u0016ig\nC,takesinto account musicological ﬁndings. Itis\ndeﬁned asfollo ws.h\nBe\u001b\u0016Qg\nC\rWYX[Z\n\\]]]]]]^]]]]]]_\n9\nb\u0016if\u001eO\u0010 e#\u0014#\u0016\u0017\u001e\nM\u0010 gl\u0014areinversions ofsame chord9\u0002ml\u0016if \u001eO\u0010 e#\u0014#\u0016\u0017\u001e&Ma\u0010 gl\u0014aredifferent chords ofsame\nfunction9\u0002nl\u0016if \u001eO\u0010 e#\u0014#\u0016\u0017\u001e&Ma\u0010 gl\u0014aremajor andminor forms of\nsame chord\u0012d\u0016otherwise \u0018\nThe coefﬁcients 9\nb, 9\nm,and 9\u0002n( 9\nb8o9\nm8p9\u0002n )enable\nweighting between functional differences ofchords. The\nfunctions ofthechords aredeﬁned traditionally: thetonic\nfunction includes chords \u0000and \u0001\u0007\u0000,thesubdominant \u0000\u0002\u0001\nand \u0000q\u0000,thedominant \u0001and \u0001\u0007\u0000q\u0000.Thechord \u0000\n\u0000q\u0000belongs\ninclass other s.\nThemelodic editdistance,A2\u001f,inEquation 1iscalcu-\nlated byusing thestraightforw ardtransposition invariant\neditdistance,AYr,asdeﬁned in[4].\nParadigmatic LevelClustering .Inparadigmatic level,\nwecluster together phrases that canbeseen asvariants\nofonecommon phrase. The result isoften represented\nasastring ofparadigm symbols, such asABCCA .Thus,\nparadigmatic levelsuggests alsoarough segmentation for\nthedocument.\nToﬁndthecluster ,weiterate thesimilarity matrix .s/\n4>5 7\n(Equation 1).Between each iteration, thesimilarity ma-\ntrixisupdated using thecomplete linkapproach (see e.g.\n[3])thattends tobuildspherical clusters. Theiteration is\nhalted when thevalues inthesimilarity matrix exceed a\npredeﬁned threshold valuet.Theparadigms arelabeled\ninthealphabetic order starting with A.\n3.CLUSTERING MUSICCOLLECTION\nTheresults oftheanalyses described abovearestored into\nanadjacenc ylist(Fig. 1).The main body ofthisstruc-\nture, which ismerely anarray ofheader cells, isconcep-\ntually divided into twoparts: paradigmatic andsurface\nlevelparts. Theheader celloftheparadigmatic partcon-\ntains alink totheformed paradigm string. The surface\nlevelpart contains varying amounts ofheader cells, one\nforeach paradigm. Foreach such header cell, there isa\nlinktoalistcontaining twostrings thatrepresent theasso-\nciated harmonic andmelodic phrases. When aparadigm\nhasseveralvariations, thephrase with thelowest distance\ntoalltheother variations isopted fortherepresentati ve\nphrase oftheparadigm.Figure1.Adjacenc ylistcorresponding todocument.\nRecall thatwhen wemeasure thesimilarity oftwomu-\nsicdocuments, wewanttocombine thesimilarity oftheir\nparadigmatic structures with thesimilarities ofthecor-\nresponding surfacelevels. Once weareable todothat,\nwecanconstruct asimilarity matrix (analogous tothatof\nEquation 1)forallthedocuments inagivencollection.\nThen wecancluster thewhole collection using thecon-\nstructi vehierarchical algorithm thatwasalready used for\nparadigmatic clustering. The resulting dendrogram will\ngiveusthehierarchical structure ofthecollection.\nTothatend, let-and-\nMbetwoanalysed documents\nandlet\u001e \r\n\u0001\u0003\u0002\u0005\u0004\u0006\u0002,\u001e+M&\r\n\u0001\u0003\u0001\u0007\u0002\b\u0001,and\td\u0016\n\t>Mdenote theas-\nsociated paradigm andsurfacestrings, respecti vely.The\nstraightforw ardwaytocouple-and-\u0002Mtogether would be\ntoalign paradigms sharing thesame label andcalculate\nthecorresponding surfacelevel(phrase) similarities. Nat-\nurally ,thismay notbetheoptimal choice because thela-\nbeling process isconsistent only overasingle document.\nWhen having acloser look attheparadigm strings of \u001e\nand \u001e+Mabove,onecanhypothesise thatcoupling where\n\u0002\nand\n\u0004intheﬁrststring arealigned with\n\u0001and\n\u0002inthe\nsecond, respecti vely,may result inamore appropriate in-\nterpretation. Obviously theoptimal coupling depends on\nthecorresponding surfacelevelstructures\tand\t6M,noton\ntheparadigm labels.\nSince theoptimal coupling iscomputationally veryex-\npensi ve,wehaveimplemented agreedy version thatre-\nsults innearly optimal coupling inmost ofthecases. For\nassessing thesimilarity oftwoparadigm strings, wehave\nimplemented three distance measures based ontheedit\ndistance frame work. The ﬁrst, denoted byA\nb\n,isvery\nstraightforw ard, yetinmanycases effective.Itapplies\nconventional edit distance (with deletion, insertion, and\nsubstitution operations) totheparadigm strings without\nanyrelabeling. The second, A\nm,isbased onHamming\ndistance (the only allowed editing operation issubstitu-\ntion) andbeha vesmore nicely inthecases where thetwo\nstrings clearly share asimilar structure, butonestring is\nmuch longer than theother .Thethird, A\nn,works as A\nm\nbutalso relabels theparadigm string\u001e\u001cMaccordingly with\u001ebyusing thealignment givenbythegreedy coupling.\nLet \u001eand \tdenote theparadigm and surfacelevel\nstrings ofsome document-,and\u0003thenumber ofdocu-\nments incollection\n\u0004.Weform asimilarity matrix.\f\u000bofsize \u0003\n1\u0003,such thatthecell .\r\u000b\nT5\nU\u0016\u0019\u0012Y8Ke\u001b\u0016Qg$8 \u0003,isgiven\nbytheformula:.\n\u000b\nT5\nU\r ?\u0015m\b@\u0006A\u000f\u000e\nB\t\nT\u0016\n\t\nUC DB\u0012 EG?\u0015m\nC@sWYX Z\n\\^_\nA\nbB\u001e\nT\u0016#\u001e\nUCA\nmB\u001e\nT\u0016#\u001e\nUCA\nnB\u001e\nT\u0016#\u001e\nUC\u0016\n(3)\nwhere ?\nm( I$8\u000e?\nm8 \u0012)isaparameter used toadjust the\nrelati veweight between paradigmatic and surfacelevel\nstructures. Let N \t\u0002Nbethenumber ofparadigms indocu-\nment\t,andRo\r W\u0011\u0010\u0013\u0012\nBN \t\u0002N \u00166N \t>MQN\nC.Now,A\u000e\nB\td\u0016\n\t M\nCgivesthe\nsurfacelevelsimilarity andisdeﬁned bytheformula:A\u0014\u000e\nB\td\u0016\n\t\nM\nC\r\n\u0015\u0016\u0016\u0017\nk\u0018T\u001a\u0019 b\nB./ \u001b\u001d\u001c 5 \u001b\u001d\u001e\u001c\nC\nm\u0016 (4)\nwhere.3/isthesurfacedistance deﬁned inEquation 1.\nTheparadigms compared,\t\nTand\t MT,areselected using the\ngreedy coupling. IfN \t\u0002N \u001f \r'N \t>MQNtheparadigms without pairs\narecompared tonullstrings.\nThe minimum operation inEquation 3opts forone\nofthepossibilities forparadigmatic structure similarities.\nWhile theintuition of A\nb\nshould beobvious (seee.g.[4]),\ntheidea behind A\nm(and A\nn)requires further explaining.\nThis isdone inthefollo wing paragraph.\nIgnoringRepetitions. Letushavetwostrings \u001eand \u001e\nM\noflengths N \u001e N \r \u0003and N \u001e&MQN \r R,such that \u0003fE R \r!#\"I.When applying theconventional edit distance,A\nb\n,tothem,A\nbB\u001e \u0016#\u001e+M\nC%$\n!although thestructure of\u001e\nmay justbeaprolonged version of\u001e\u001cM,likeincase:\u001e \r\u0001\u0007\u0002&\u0004\u0006\u0001\u0007\u0002\u0005\u0004\u0006\u0001\u0003\u0002\u0005\u0004and\u001e+M&\r\n\u0001\u0007\u0002&\u0004.\nNow,ifweconsider theshorter string asasearch pat-\nterntobesearched forinthelonger string, wecaneasily\nseethat\u001e\nMoccurs exactly in\u001eatpositions 1,4,and7.This\nsuggests that \u001e&Mand \u001eareparadigmatically much closer\nthan A\nbB\u001e\u0011\u0016\u0017\u001e&M\nC\r(' would hint. Itiswell-kno wnthatthe\neditdistance frame workcanbestraightforw ardly adapted\ntothepattern matching case. Now,instead ofﬁnding the\nminimum value atthebottom rowofthedynamic pro-\ngramming table computing thematching process, wecol-\nlect ) N \u001e N * N \u001e&M\u0017N +minimum values. The mean value ofthis\nsetisgivenasthedistance between thetwostrings.\nInorder toobtain meaningful results indoing so,we\nneed toapply theHamming distance. This isduetothe\nwell-kno wnfactthattheadjacent values inthedynamic\nprogramming table varyatmost byonewhen allowing in-\nsertions, deletions andsubstitutions. Ifdone so,inthe\nresulting difference one perfect occurrence may beac-\ncounted severaltimes, giving asmall score although the\nstructures would havebeen rather dissimilar .\nAsforanexample ofourmeasure A\nm,letusconsider\u001eand \u001e&Masgivenabove.Thebottom rowofthedynamic\nprogramming table would be3,3,0,3,3,0,3,3,0and)-, ./,, .\n\u001e,\n+$\r10 .Because there arethree zeros inthebottom\nrow(three exact occurrences ofthepattern), A\nmB\u001e \u0016#\u001e\nM\nC\rI.Inthiscase, theresult describes intuiti velybetter the\nsimilarity of\u001eand\u001e&MthanA\nbB\u001e \u0016#\u001e+M\nC\r2'.Atthispoint,thefunctioning of A\nn,asexplained above,should have\nbecome obvious, aswell.\n4.EXPERIMENT ALRESULTS\nLetusnowdescribe brieﬂy ourimplementation, thein-\nﬂuence ofcoefﬁcients used, thetestsets, andthemain\nresults obtained. Asmentioned inSection 2,weused seg-\nmentation andharmonic analysis modules based onTem-\nperle y’sheuristics [7]. Implementations ofthem canbe\nfound freely available fornon-commercial use1.Weim-\nplemented themelody extraction, clustering, anddistance\ncalculation algorithms asPerl scripts. The data ﬂowbe-\ntween various scripts wasmanaged byashell script.\nWestudied thebeha viour ofthemethod using twotest\nsetscontaining documents inMIDI format. The ﬁrst set\ncontained 48short fragments ofclassical pieces. This set\nwasused mainly forevaluating thefunctionality ofthe\nmethod. Formore thorough analysis ofthemethod we\nbuiltthesecond testsetbychoosing 145classical pieces\nrandomly from theMutopia2database. The size ofthe\nsecond setwasintentionally keptassmall aspossible,\nbecause ofthehuge amount ofhandiw orkneeded when\nanalysing theresults oftheclustering.\nFirst westudied theparadigmatic clustering phase. We\nexperimented onvarious threshold valuest.Ateach con-\nsidered value, wegenerated alistofadjacenc ylists and\nanalysed thequality ofparadigmatic clustering. Wefound\nthat thethreshold value 20gavethebest paradigmatic\nclustering result inthisdata set.\nWeexperimented also onthecoefﬁcient?ofEqua-\ntion 1.Intheﬁrst experiment thevalue of ?inparadig-\nmatic phase wassetin\nmn.Wefound that theharmonic\nanalysis haddifﬁculties with surfacelevelchromaticism\nand theresults skewed theparadigmatic clustering, as\nwell. Inthesecond experiment wesetthevalue of ?inb\u0000.Inthiscase, theproblems ofthechromaticism didnot\ndominate theparadigmatic clustering asstrongly asinthe\nﬁrstexperiment.\nThe coefﬁcients 9\nb, 9\nm,and 9\u0002nwere setin0.25, 0.5\nand0.75, respecti vely.Infuture, wewillevaluate awider\nrange ofvalues forthem.\nInthecollection clustering phase weexperimented on\ncoefﬁcient value ?\nm.Weused ?\nmintesting theimportance\nofparadigmatic structure setting ?\nmtobe1andthus elimi-\nnating itsinﬂuence. Wefound thatwhen setting thevalue\nof?\nmveryhigh, theresulting hierarchical structure was\nconstructed mostly onthebasis ofoccasional similarities\nonthesurfacelevel.These similarities areofcourse in-\nteresting insuch research problems that aim atﬁnding,\nforinstance, similar melodic patterns from music docu-\nments. Inourcase, however,these similarities, especially\nwhen occurring without wider musicological conte xt,are\nnotveryinteresting.\nWhen thevalue of?\u001amwassetlowthus stressing thein-\nﬂuence oftheparadigmatic levelsimilarities theresulting\n1http://www .link.cs.cmu.edu/music-ana lysis/\n2http://www .mutopiaproject.or g/structure wasaltered. Wefound thatcertain documents\nthatwere located near toeach other when thesurfacelevel\nsimilarity wasstressed, were inthesame subcluster also\nwhen stressing theparadigmatic levelsimilarity .Inthis\ncase wefound both paradigmatic andsurfacelevelsim-\nilarities between thedocuments when examining thead-\njacenc ylists ofthedocuments. Inmanycases, however,\nthesimilarity between certain documents wasnoticeable\ninthehierarchical clustering structure only when stress-\ningtheparadigmatic levelsimilarity .This beha viour was\ncharacteristic ofdocuments containing especially Bach’ s\ncompositions. This kind ofsimilarity could notbefound\nbyexamining only thesurfacelevelofthedocuments.\n5.CONCLUSION\nWedescribed anovelautomatic analysis method based on\nparadigmatic andsurfacelevelsimilarity ofmusic repre-\nsented insymbolic form. Ourexperimental results onthe\nmethod were encouraging, giventhesimplicity ofit.Be-\ncause ofthemodular structure, ourmethod iseasily cus-\ntomised: other surfaceleveldescriptions, such asstatis-\ntical parameters, canbeaccommodated byadding aterm\nintoEquation 1.\n6.REFERENCES\n[1]Cambouropoulos E.and Widmer ,G.,“Au-\ntomatic motivicanalysis viamelodic cluster -\ning”, Journal ofNewMusic Resear ch29(4),\npp.303–317, 2000.\n[2]Eerola, T.,J¨arvinen, T.,Louhi vuori, J.,and\nToiviainen, P.“Statistical features and per-\nceivedsimilarity offolkmelodies”, Music Per-\nception 18,pp.275–296, 2001.\n[3]Hand, D.,Mannila, H.,andSmyth, P.,Princi-\nples ofData Mining .MIT Press, Cambridge,\nMass., 2001.\n[4]Lemstr ¨om,K.andUkkonen, E.,“Including in-\ntervalencoding intoeditdistance based music\ncomparison andretrie val”,Proc.AISB Sympo-\nsium onCreative &Cultur alAspects andAp-\nplications ofAI&Cognitive Science ,Birming-\nham, UK, pp.53–60, 2000.\n[5]Lerdahl, F.and Jackendof f,R.,AGener a-\ntiveTheory ofTonal Music .MIT Press, Cam-\nbridge, Mass., 1983.\n[6]Nattiez, J.-J., Music andDiscour se–Towar d\naSemiolo gyofMusic .Princeton University\nPress, 1990.\n[7]Temperle y,D.,TheCognition ofBasic Musi-\ncalStructur es.MIT Press, Cambridge, Mass.\n2001."
    },
    {
        "title": "Music meter and tempo tracking from raw polyphonic audio.",
        "author": [
            "Aggelos Pikrakis",
            "Iasonas Antonopoulos",
            "Sergios Theodoridis"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416348",
        "url": "https://doi.org/10.5281/zenodo.1416348",
        "ee": "https://zenodo.org/records/1416348/files/PikrakisAT04.pdf",
        "abstract": "This paper presents a method for the extraction of music meter and tempo from raw polyphonic audio recordings, assuming that music meter remains constant throughout the recoding. Although this assumption can be restrictive for certain musical genres, it is acceptable for a large cor- pus of folklore eastern music styles, including Greek tra- ditional dance music. Our approach is based on the self- similarity analysis of the audio recording and does not as- sume the presence of percussive instruments. Its novelty lies in the fact that music meter and tempo are jointly de- termined. The method has been applied to a variety of musical genres, in the context of Greek traditional music where music meter can be 2 4, 3 4, 4 4, 5 4, 7 8, 9 8, 12 8 and tempo ranges from 40bpm to 330bpm. Experiments have, so far, demonstrated the efficiency of our method (music meter and tempo were successfully extracted for over 95% of the recordings). Keywords: music meter tracking, beat tracking, content- based music retrieval",
        "zenodo_id": 1416348,
        "dblp_key": "conf/ismir/PikrakisAT04",
        "keywords": [
            "music meter",
            "tempo",
            "polyphonic audio recordings",
            "self-similarity analysis",
            "percussive instruments",
            "folklore eastern music",
            "Greece traditional dance music",
            "joint determination",
            "music genres",
            "Greek traditional music"
        ],
        "content": "MUSIC METER AND TEMPO TRACKING FROM RAW POLYPHONIC\nAUDIO\nAggelos Pikrakis, Iasonas Antonopoulos and Sergios Theodoridis\nDepartment of Informatics and Telecommunications\nUniversity of Athens, Greece\nABSTRACT\nThis paper presents a method for the extraction of music\nmeter and tempo from raw polyphonic audio recordings,\nassuming that music meter remains constant throughout\nthe recoding. Although this assumption can be restrictivefor certain musical genres, it is acceptable for a large cor-\npus of folklore eastern music styles, including Greek tra-\nditional dance music. Our approach is based on the self-\nsimilarity analysis of the audio recording and does not as-\nsume the presence of percussive instruments. Its noveltylies in the fact that music meter and tempo are jointly de-\ntermined. The method has been applied to a variety of\nmusical genres, in the context of Greek traditional musicwhere music meter can be\n2\n4,3\n4,4\n4,5\n4,7\n8,9\n8,12\n8and tempo\nranges from 40bpm to 330bpm. Experiments have, so far,\ndemonstrated the efﬁciency of our method (music meter\nand tempo were successfully extracted for over 95% of\nthe recordings).\nKeywords: music meter tracking, beat tracking, content-\nbased music retrieval\n1. INTRODUCTION\nContemporary content-based music retrieval applications\nhave highlighted the need to extract rhythmic features from\nraw polyphonic audio recordings, in order to increase theefﬁciency of tools that perform a diversity of tasks, in-\ncluding musical genre classiﬁcation, query-by-humming\nand query-by-rhythm, to name but a few, e.g, [1, 22]. To-\nward this end, several attempts have been made to create\nan algorithmic perception of rhythm. Most research hasfocused on tempo tracking, whereas, on the other hand,\nmusic meter extraction has attracted signiﬁcantly less at-\ntention.\nThe ﬁrst attempts, dating back to the early 90’s, in-\nvolved MIDI signals [2], [3], [4], [5], [6], [7]. However,\nthe need to circumvent the limitations imposed by MIDI\nsignals, led to the development of several tempo-tracking\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copiesare not made or distributed for proﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2004 Universitat Pompeu Fabra.\nmethodologies that were applied on raw polyphonic au-\ndio. Goto & Muraoko [10, 11] focused on real-time beat\ntracking of popular music, assuming a tempo range of\n61-120 bpm and music meter 4/4. Shceirer [12] intro-\nduced a tempo tracking approach that is independent of\nmusical genre and does not demand a constant beat track.Foote ([13, 14, 15, 16]), investigated the properties of the\n“self-similarity matrix” and proposed the generation of\nthe “beat spectrum” from audio recordings. A compar-ative study of tempo trackers was given by Dixon in [8],\nwho also presented a real-time tempo tracker capable of\ndisplaying tempo variations in an animated display [9].\nThis paper\n1presents a method for the extraction of\nmusic meter and tempo from raw polyphonic audio reco-\nrdings, assuming that music meter remains constant throu-ghout the recoding. This assumption is acceptable for a\nlarge corpus of Greek traditional dance music, which has\nbeen in the center of our study. Our approach is based onthe fact that the diagonals of the self-similarity matrix of\nthe audio recording reveal periodicities corresponding to\nmusic meter and beat. By examining such periodicities it\nis possible to jointly estimate the music meter and tempo\nof the recording, as described in section 2. The methodhas been applied to musical genres in the context of Greek\ntraditional music whose music meter can be\n2\n4,3\n4,4\n4,5\n4,7\n8,\n9\n8,12\n8and whose tempo ranges from 40bpm to 330bpm.\nSection 2 describes the algorithmic aspects of our me-\nthod. Section 3 provides implementation details and re-\nsults of the experiments that have been carried out andﬁnally, section 4 highlights our future research priorities.\n2. MUSIC METER AND TEMPO EXTRACTION\nAt a ﬁrst step, each raw audio recording is divided into\nnon-overlapping long-term segments, each of which has a\nduration equal to 10seconds. The choice for the length\nof the long-term segments is justiﬁed in section 3. Mu-sic meter and tempo are then extracted on a segment by\nsegment basis. More speciﬁcally, for each long-term seg-\nment, a short-term moving window generates a sequenceof feature vectors. Approximate values for the length of\nthe short term window and overlap between successive\nwindows are 100ms and 97ms respectively, suggesting\n1Research described by the authors in this paper was funded by the\nGreek Secretariat of Research and Technology, in the framework ofproject POLYMNIA - EPAN 4.5a3ms moving window step. Having experimented with\na variety of feature candidates and their combinations, we\nchose to focus on two variations of the mel-frequency cep-\nstrum coefﬁcients (details are given in section 3.1).\nLet us denote by F={f1,f2,...f N}, the feature se-\nquence of length Nthat is extracted from a long-term seg-\nment. Sequence Fserves as the basis to calculate the Self\nSimilarity Matrix (SSM) of the segment [13, 14, 15, 16],\nusing the Euclidean function as a distance metric (Figure\n1). Since the SSM is symmetric around the main diagonal,in the sequel it sufﬁces to focus on its lower triangle. At\nFeature frames : {f1,f2,… ,fN}Fe atur e fra m e s : {f1,f2,… ,fN}\nj\niSelf Similarity Matrix\nDistance Metric\n(i,j)\nFigure 1 . Self Similarity Matrix\na next step, the mean value of each diagonal of the SSM\nis calculated. If Bkstands for the mean value of the kth\ndiagonal, then:\nBk=1\nN−kN/summationdisplay\nl=k||fl,fl−k|| (1)\nwhere N−kis the length of the kthdiagonal and ||.||is\nthe Euclidean distance function.\nAs can be seen in Figure 2, if Bis treated as a function\nofk, then its plot against kexhibits certain local minima\n(valleys) for a number of k’s. Each valley can be inter-\npreted as corresponding to a periodicity, that is inherent inthe long-term segment being examined. In Figure 2, the\nbeat of the segment appears as a valley around the 70-th\ndiagonal. This segment has been extracted from a Greek\ntraditional dance of music meter\n7\n8. In Figure 3, an overall\nview of the segment periodicities can be seen, where mul-tiples of the beat, including the music meter itself also,\nappear as valleys. In the general case, submultiples of the\nbeat are also likely to appear as local minima. It is worthnoticing that\na) the global minimum of Bdoes not always coincide with\nthe beat or the music meter\nK1\nFigure 2 . Plot of Bkversus kfocusing on the beat range.\nK2\nFigure 3 . Plot of Bkversus kfocusing on the meter range.\nb) the indices of the diagonals corresponding to local min-\nima are in most cases approximate multiples or submulti-\nples of the beat index\nc) function Bdecreases (increases) more rapidly around\ncertain local minima and this appears as sharper valleys\nin the Figures 2 and 3.\nObviously, if diagonal kcorresponds to a local mini-\nmum, then the time equivalent of the respective periodi-cityT\nkisTk=k∗step, where step is the short-term step\nof the moving window ( 100−97 = 3 ms for our study).\nTherefore, small short term steps (i.e., large overlap) in-crease the accuracy of the periodicity estimates, while also\nincreasing the computational cost (due to an increase in\nthe length of the feature sequence F).\nThe aforementioned analysis suggests that, although\nperiodicities corresponding to the music meter and beat\nare likely to appear as local minima (valleys) of B, fur-\nther processing of Bis required, in order to identify which\nvalleys actually refer to music meter and beat. In order to\nproceed, we assume that the tempo of the recording, mea-\nsured in beats per minute (bpm) can vary from 40bpm to330bpm. This beat range is applicable to the corpus of\nGreek traditional music of our study but may require tu-\nning for other musical genres. It also suggests a range of\ndiagonals, i.e., k-values, say [ks,kl], in which the beat of\nthe segment is expected to appear as a local minimum of\nB. Outside this range, i.e., for k>k l, multiples of the\nbeat lag including music meter, are also expected to ap-pear as valley. If k\nmax is the last (downmost) diagonal\nof interest, music meter is likely to appear as a valley in\nthe range of (kl,kmax].kmaxmust be large enough to ac-\ncount for all music meters and tempo ranges in question.\nFor our music corpus, the time equivalent of kmaxwas set\nto3secs (see section 3.2).\nIn the sequel, beat candidates in the range [ks,kl]are\nexamined in pairs with meter candidates in the range(k\nl,kmax]. For each pair of such candidates, let us denote\nbyk1the lag in (ks,kl]and by k2the meter candidate in\n(kl,kmax].I fCbandCmare the numbers of beat and me-\nter candidates respectively, then there exist Cb∗Cmsuch\npairs. In order to proceed, two different decision criteriaare applied on this set of pairs. Each criterion generates\nindependently a meter and beat decision by exploiting a\nsubset of pairs, as is explained below.\n2.1. Meter decision criteria\nCriterion A\nAt ﬁrst, the local minima in the range (k\nl,kmax], for which\nthe two neighboring local minima possess larger values,\nare selected. For example, such is the case with metercandidate marked as k\n2in Figure 3, that corresponds to a\nperiodicity indicating music meter7\n8. In this example, the\nlocal minima pointed by the dotted arrows, correspondingto\n4\n8,6\n8,8\n8,10\n8are ﬁltered out. This initial ﬁltering proce-\ndure can be useful for audio recordings of music meter7\n8,\n9\n8and12\n8stemming from Greek Traditional music.\nAt a second step, each beat candidate is examined in\npair with the remaining meter candidates. For each suchpair, the fraction\nk2\nk1should coincide, within an error mar-\ngine, with one of the fractions related to the music meters\nof our study, i.e.,2\n4,3\n4,4\n4,5\n4,7\n8,9\n8,12\n8. All pairs falling\noutside the error margin are discarded ( eis assumed con-\nstant for all allowable music meters) and was set equal to\n0.3for our experiments. In other words, each music me-\nter is considered to lie in the center of a bin, whose width\nis equal to 2e. If a pair of valleys {k1,k2}is assigned to\na bin, k1is considered to be the beat lag. Furthermore,\nfor each pair assigned to a bin, the quantity C{k1,k2}is\ncalculated:\nC{k1,k2}=Bk1+Bk2 (2)\nIf more than one pair is assigned to the same bin, the pair\ngenerating the lowest C{k1,k2}is considered to be the win-\nning pair for the bin. After all pairs have been processed,\nthe music meter of the segment is determined according\nto the bin with the lowest C{k1,k2}value.\nCriterion B\nThe previous criterion can be modiﬁed by taking into ac-count, for the calculation of the C{k1,k2}value, the slope\n(sharpness) of the valleys of each pair being examined\n(and not just their absolute values). This deals with the\nfact that, a pair of sharp valleys corresponding to the ac-tual music meter and beat does not always coincide with\nthe pair having the lowest sum of absolute values. There-\nfore, C\n{k1,k2}can be calculated as follows:\nC{k1,k2}=slope (Bk1)\nBk1+slope (Bk2)\nBk2(3)\nwhere slope (.)is a measure of the sharpness around each\nvalley of function B. Equation 3 suggests that, if both val-\nleys are sharp and deep, then C{k1,k2}has a large value.\nHaving determined the music meter bin for all pairs, the\npair yielding the maximum value of C{k1,k2}is consid-\nered to be the winner and the bin to which it has been\nassigned stands for the music meter of the segment.\nAlthough in general both criteria result in an acceptable\nperformance, there are cases where one succeeds and theother fails. This is similar to two classiﬁers, where (from\npattern recognition theory [17]), a classiﬁer with a bet-\nter overall error, can fail in cases where others succeed.The remedy is to combine classiﬁers. To comply with\nthis philosophy, two music meter decisions are generated\nfrom each long term segment. This has turned out to in-\ncrease the overall performance signiﬁcantly. If Sis the\nnumber of long term segments, then 2∗Smusic meter\ndecisions are generated. The most frequently encountered\nmusic meter is selected as the meter of the whole audio\nrecording and its frequency of appearance is returned asthe certainty of the overall decision.\n2.2. Tempo Estimation\nThere now remains to determine the tempo of the audio\nrecording. For the music corpus of our study, it can be as-\nsumed that tempo remains approximately constant throu-ghout the audio recording, and is therefore possible to\nreturn an average tempo value. Alternatively, a tempo\nvalue per long term segment can be returned, so as tohighlight tempo ﬂuctuations, or even, return additionally,\nthe time limits of the segments that produced wrong es-\ntimates, since this type of information might be useful to\nalgorithms that extract repeating patterns from audio re-\ncordings.\nAt this point, it has to be noted that certain assump-\ntions have to be adopted concerning the beat range, thatmay need ﬁne-tuning for musical genres outside the con-\ntext Greek traditional dance music. As a ﬁrst assumption,\nthe tempo associated with music meters\n2\n4,3\n4,4\n4and5\n4\ncannot be greater than 180bpm while the tempo of meters\n7\n8,9\n8and12\n8must be over 180bpm and up to 330bpm (as\nmentioned before). This implies that the range of beat\nlags [ks,kl]is divided into two successive sub-regions,\ni.e.,[ks,kq]and[kq,kl], which correspond to1\n8and1\n4pe-\nriodicities respectively and ks,kq, and klare the lags of\n330bpm (fastest1\n8),180bpm (fastest1\n4) and 40bpm (slow-\nest1\n4).0 100 200 300 400 500 600 700 800 90000.10.20.30.40.50.60.70.80.91\nDiagonal IndexK KK\n1q\n2\nFigure 4 . Plot of Bkversus kfor an audio meter of3\n4,\nwhere the eighth note is the dominant beat lag.\nAs it was previously mentioned, each long term seg-\nment produces two music meter decisions, each of whichis associated with a pair of lags (k\n1,k2). In the ideal case,\nfor a decision that coincides with the overall music meter\nestimate, k2should be the meter lag and k1the beat lag.\nHowever in practice, for meters2\n4,3\n4,4\n4and5\n4,k1often\nlies in the range [ks,kq]and refers to the eighth note perio-\ndicity instead of the expected quarter note periodicity. For\nexample, in Figure 4, which refers to a segment from an\naudio recording of music meter3\n4, for both decision cri-\nteria, the dominant pair of lags (marked k1,k2in Figure\n4) corresponds to a beat lag of1\n8and meter lag of6\n8, be-\ncause of a less dominant quarter note periodicity (marked\nkq). As a result, meter6\n8and3\n4can be confused. How-\never, in the context of the Greek traditional dance musicthat we studied, these can be thought to be equivalent and\nit therefore sufﬁces to double the value of the beat lag.\n3. IMPLEMENTATION DETAILS AND RESULTS\nOF EXPERIMENTS\nThe length of the long term segments was set equal to\n10secs with zero overlap between successive segments.\nThis segment length is large enough to capture periodic-\nities of slow tempo values in the range of 40bpm. For\nthe short term analysis, the moving window size was ap-proximately 93ms (4096 samples for sampling frequency\n44.1kHz) and the moving window step was set equal to\n∼=3ms (128samples for sampling frequency 44.1kHz).\nIt has to be noted that the moving window step reﬂects the\nbeat accuracy. Smaller values produce more accurate beatestimates but increase computational complexity signiﬁ-\ncantly. For slow tempo recordings, we also experimented\nwith longer short term windows up to 186ms (8192 sam-\nples for sampling frequency 44.1kHz). However, for fast\ntempo values (as is the case with music meter\n7\n8, etc, in the\ncontext of Greek traditional music), large short term win-dows result in poor valleys in the beat range. Although\nin this case smaller short term windows would be desi-\nrable, it would not be possible to achieve tone resolution in\nthe low frequency range as imposed by the chroma-basedMFCCS (see 3.1).\n3.1. Feature selection details\nFor the short term analysis of each long term audio seg-\nment, we considered both energy and mel frequency cep-\nstral coefﬁcients (MFCCs) [18, 19]. In addition to thestandard MFCCs, which assume equally spaced critical\nband ﬁlters in the mel scale, we also experimented with a\ncritical band ﬁlter bank consisting of overlapping triangu-\nlar ﬁlters, whose center frequencies follow the equation:\nC\nk= 110 ∗2k\n12 (4)\nThat is, the ﬁlter bank is chosen to align with the chro-\nmatic scale of semitones (starting from 110Hz and rea-\nching up to approximately 5KHz). If whole-tone spacing\nis adopted, equation (4) becomes:\nCk= 110 ∗2k\n6 (5)\nOur variation of the mel frequency cepstrum bears certain\nsimilarities with the “chroma vector” [21].\nCompared with energy, the two variants of the MFCCs,\nalthough computationally expensive, yield signiﬁcantly bet-\nter results, and this is mainly because periodicities corre-sponding to beat and meter are emphasized (Figure 5). It\nhas to be noted though, that energy gave good results for a\nsigniﬁcant number of recordings of music meter\n2\n4and3\n4,\nbut failed for most of the recordings with music meter5\n4,\n7\n8,9\n8and12\n8. Depending on the frequency content distri-\nbution of the recording, especially in the case of dominant\nsinging voices, our variant of the mel frequency cepstrum\nled to an improved performance compared to the standardapproach. This was mainly observed in the cases of\n5\n4,\n9\n8and12\n8. The standard MFCC’s were computed using\nSlaney’s auditory toolbox [20].\n3.2. Self Similarity Analysis details\nFor the distance metric, we adopted the standard Euclidean\ndistance function (also used in [21]). The use of the co-\nsine distance ([13, 14, 15, 16]) in our experiments tendedto lead to inferior performance.\nDue to the assumptions adopted in section 2, we only\nneed to focus on a subset of the diagonals of the SSM. Forsampling frequency 44.1KHz and moving window step 3\nms, the range [k\ns,kl]is mapped to diagonals [63,517],kq\n(fastest quarter note) corresponds to the 115-th diagonal\nandkmaxis mapped to the 1159 -th diagonal. For our mu-\nsic corpus, kmaxwas chosen large enough to account for\nmusic meter periodicities of2\n4and tempo values in the\nrange of 40bpm. In general, kmax(as well as ks,kqand\nkl) needs ﬁne tuning depending on the music genre. For\nexample, if4\n4audio recordings of slow tempo also need to\nbe taken into consideration, the kmaxvalue should at least\nbe doubled.100 200 300 400 500 600 700 8000.650.70.750.80.850.90.95\nk (lag)B(k) (normalized)\nenergy\nchroma−based MFCCs1/8 \n7/8 \nFigure 5 . Plot of Bkversus kfor energy and chroma-\nbased MFCCs, for ten second audio extract of music meter\n7\n8.\n3.3. Results of experiments\nThe music corpus of our study consists of 300raw audio\nrecordings of Greek dance folklore music and neighbo-\nring Eastern music traditions. Throughout each recording,music meter remains constant. This corpus was assem-\nbled under musicological advice and focuses on most fre-\nquently encountered folk dances, exhibiting signiﬁcant rhy-thmic variety over beat and music meter (see Table 1).\nmusic meter\n tempo range (bpm)\n # of recordings\n7\n8\n200-280\n 45\n9\n8\n250-330\n 45\n12\n8\n260-330\n 10\n2\n4\n40-160\n 90\n3\n4\n80-170\n 60\n4\n4\n70-140\n 90\n5\n4\n90-120\n 10\nTable 1 . Description of music corpus.\nApproximately one third of the audio corpus consists\nof live performances and digitally remastered recordings.\nFor live performances, certain beat ﬂuctuation was ob-served and for these recordings it makes more sense to\nreturn a beat value per long term segment, instead of an\naverage beat value.\nFor the majority of the recordings (over 95%), the rhy-\nthmic features in question were successfully extracted. Most\nmistaken results were produced by confusion of music\nmeter\n2\n4with4\n4,5\n4with9\n8or4\n4and7\n8with3\n4or4\n4. The\nmain reason for the above cases of confusion, is that the\ndominant periodicities in the beat range, often deviate sig-\nniﬁcately from the desired values and as a result the pair(beat lag, meter lag) is assigned to an incorrect (neighbo-\nring) meter bin. Especially in the case of meter\n2\n4, confu-\nsion with4\n4may also occur because a very strong periodi-city at four quarter-notes is observed.\nIn addition, for certain long term segments, due to the\nnature of the signal, the features that have been employed\nfail to capture any periodicities at all. As a last remark, in\ncertain cases, especially for meter cases of7\n8,9\n8and12\n8,\na dominant quarter note, appears in the beat range instead\nof the expected eighth note, thus leading to an incorrect\nmeter and beat estimate. As a remedy to this situation, it\nis possible to divide by two all valleys in the range [kq,kl]\nand treat these new values as candidate beat lags.All experiments were carried out using the Matlab work-\nbench.\n4. CONCLUSIONS AND FUTURE WORK\nWe have presented a method for the extraction of music\nmeter and tempo from raw audio recordings, assuming\nthat music meter remains constant throughout the reco-\nrding. The method was applied on a music corpus consi-sting of recordings stemming from Greek Traditional Mu-\nsic and neighboring music traditions. In the future, feature\nselection will be expanded to cover more feature candi-dates and their combinations. In addition, we will investi-\ngate ways to pre-process the SSM prior to calculating the\nmean of diagonals, in order to detect subsections of the di-\nagonals that emphasize the inherent periodicities. Toward\nthis end, Dynamic Time Warping techniques are expectedto be employed [22]. Finally, it is our intention to inve-\nstigate the effectiveness of the methodology in the context\nof other music genres.\n5. REFERENCES\n[1] George Tzanetakis and Perry Cook, “Musical Genre\nClassiﬁcation of Audio Signals”, IEEE Transactions\non Speech and Audio Processing, vol. 10, No. 5, July\n2002\n[2] Allen, P. & Dannenberg, R. “Tracking musical beats\nin real time”, Proceedings of the 1990 International\nComputer Music Conference, pages 140143. Inter-\nnational Computer Music Association , San Fran-\ncisco, USA, 1990.\n[3] Large, E. & Kolen, J. “Resonance and the perception\nof musical meter”, Connection Science, 6:177208 ,\n1994\n[4] Large, E. “Beat tracking with a nonlinear oscilla-\ntor” Proceedings of the IJCAI95 Workshop on Ar-\ntiﬁcial Intelligence and Music, pages 2431 Inter-\nnational Joint Conference on Artiﬁcial Intelligence,\n1995\n[5] Large, E. “Modeling beat perception with a nonlin-\near oscillator” In Proceedings of the 18th Annual\nConference of the Cognitive Science Society , 1996\n[6] Dixon, S. “A lightweight multi-agent musical beat\ntracking system” PRICAI 2000: Proceedings of thePaciﬁc Rim International Conference on Artiﬁcial\nIntelligence, pages 778788 , Springer, 2000\n[7] Dixon, S. & Cambouropoulos, E. “Beat tracking\nwith musical knowledge” ECAI 2000: Proceedings\nof the 14th European Conference on Artiﬁcial Intel-ligence, pages 626630 , IOS Press, 2000\n[8] Dixon, S. “An Empirical Comparison of Tempo\nTrackers” Proceedings of 8th Brazilian Symposium\non Computer Music, pp 832-840 Fortaleza, Brazil,\n31 July - 3 August 2001\n[9] Dixon, S. et al, “Real Time Tracking and Visualiza-\ntion of Musical Expression” ICMAI 2002: Proceed-\nings of the 2nd International Conference on Music\nand Artiﬁcial Intelligence, pages 58-69 , LNAI 2445,\nSpringer-Verlag, 2002\n[10] Goto, M. & Muraoka, Y . “A real-time beat tracking\nsystem for audio signals” Proceedings of the Inter-\nnational Computer Music Conference, pages 171174Computer Music Association, San Francisco, USA,\n1995\n[11] Goto, M. & Muraoka, Y . “Real-time beat tracking\nfor drumless audio signals” Speech Communication,\n27(34):331335, 1999\n[12] Scheirer, E. “Tempo and beat analysis of acoustic\nmusical signals” Journal of the Acoustical Society of\nAmerica, 103(1):588601, 1998\n[13] J. Foote “Visualizing Music and Audio using Self-\nSimilarity” Proceedings of ACM Multimedia 99, pp.\n77-80 Orlando, FL, USA, ACM Press, 1999\n[14] J. Foote, and S. Uchihashi “The Beat Spectrum: A\nNew Approach to Rhythm Analysis” Proceedings of\nInternational Conference on Multimedia and Expo\n(ICME) 2001\n[15] Jonathan Foote, Matt Cooper, and Unjung Nam\n“Audio Retrieval by Rhythmic Similarity” Proceed-\nings of Third International Symposium on Musical\nInformation Retrieval (ISMIR), pp. 265-266 Paris,\nFrance, September 2002\n[16] J. Foote “Automatic Audio Segmentation using a\nMeasure of Audio Novelty” Proceedings of IEEE\nInternational Conference on Multimedia and Expo,\nvol. I, pp. 452-455 2000\n[17] Sergios Theodoridis and Konstantinos Koutroum-\nbas, Pattern Recognition (2nd Edition) , Academic\nPress, 2003.\n[18] Lawrence Rabiner and Bing-Hwang Juang, Funda-\nmentals Of Speech Recognition Prentice Hall, New\nJersey, USA, 1993\n[19] John R. Deller Jr, John G.Proakis and John\nH.L.Hansen Discrete-Time Processing Of Speech\nSignals Prentice Hall, New Jersey, USA, 1987[20] Auditory Toolbox Malcom Slaney, Technical Re-\nport #1998-010 , Interval Research Corporation mal-\ncolm@interval.com\n[21] Roger B. Dannenberg & Ning Hu “Discovering Mu-\nsical Structure in Audio Recordings” Proceedings\nof 2nd International Conference on Music and Ar-\ntiﬁcial Intelligence, pp 43-57 , Edinburg, Scotland,\nSeptember 2002\n[22] A. Pikrakis, S. Theodoridis, D. Kamarotos, “Recog-\nnition of isolated musical patterns using context de-\npendent dynamic time warping”, IEEE Transactions\non Speech and Audio Processing, vol. 11(3), May2003"
    },
    {
        "title": "A Hybrid Graphical Model for Aligning Polyphonic Audio with Musical Scores.",
        "author": [
            "Christopher Raphael"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416500",
        "url": "https://doi.org/10.5281/zenodo.1416500",
        "ee": "https://zenodo.org/records/1416500/files/Raphael04.pdf",
        "abstract": "We present a new method for establishing an alignment between a polyphonic musical score and a corresponding sampled audio performance. The method uses a graphi- cal model containing both discrete variables, correspond- ing to score position, as well as a continuous latent tempo process. We use a simple data model based only on the pitch content of the audio signal. The data interpretation is defined to be the most likely configuration of the hidden variables, given the data, and we develop computational methodology for this task using a variant of dynamic pro- gramming involving parametrically represented continu- ous variables. Experiments are presented on a 55-minute hand-marked orchestral test set. Keywords: Polyphonic Score Alignment",
        "zenodo_id": 1416500,
        "dblp_key": "conf/ismir/Raphael04",
        "keywords": [
            "polyphonic musical score",
            "sampled audio performance",
            "graphical model",
            "discrete variables",
            "continuous latent tempo process",
            "data model",
            "pitch content",
            "data interpretation",
            "hidden variables",
            "computational methodology"
        ],
        "content": "AHYBRID GRAPHICAL MODEL FOR ALIGNING POL YPHONIC AUDIO\nWITH MUSICAL SCORES\nChristopher Raphael\nSchool ofInformatics\nIndiana University ,Bloomington\ncraphael@indiana.edu\nABSTRA CT\nWepresent anewmethod forestablishing analignment\nbetween apolyphonic musical score andacorresponding\nsampled audio performance. The method uses agraphi-\ncalmodel containing both discrete variables, correspond-\ningtoscore position, aswell asacontinuous latent tempo\nprocess. Weuseasimple data model based only onthe\npitch content oftheaudio signal. Thedata interpretation\nisde\u0002ned tobethemost likelycon\u0002guration ofthehidden\nvariables, giventhedata, andwedevelop computational\nmethodology forthistaskusing avariant ofdynamic pro-\ngramming involving parametrically represented continu-\nousvariables. Experiments arepresented ona55-minute\nhand-mark edorchestral testset.\nKeywords: Polyphonic Score Alignment\n1.INTR ODUCTION\nWeaddress anaudio recognition problem inwhich acor-\nrespondence isestablished between apolyphonic musical\nscore andanaudio performance ofthatscore. There are\ntwoversions ofthisproblem, often called on-line and\noff-line recognition orparsing.\nOff-line parsing uses thecomplete performance toes-\ntimate theonset time foreach score note, thus theoff-line\nproblem allowsonetolook intothefuture while estab-\nlishing thematch. Partofourinterest inoff-line parsing\nproblem stems from acollaboration with theVariations2\nDigital Music Library Project atIndiana University .One\nofthemanyaims ofthisproject istoallowlisteners, in\nparticular students intheir School ofMusic, newtools for\nlearning andstudying music, interlea ving sound, text,mu-\nsicnotation, andgraphics. One speci\u0002c goal istogivethe\nuser random access toarecording allowing playback to\nbeginatanytime, expressed inmusical units, e.g.thethird\nbeatofmeasure 47.Clearly thisapplication requires either\nhand marking ofaudio oroff-line parsing. Another off-\nlineapplication istheediting andpost-processing ofdig-\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forpro\u0002t orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation onthe\u0002rstpage.\nc\n\u00002004 Universitat Pompeu Fabra.italaudio, inwhich manytasks require theuser tolocate\nandmodify aspeci\u0002c place inaverylargeaudio data\u0002le.\nOur personal interest inoff-line parsing ismotivated by\nyetanother application: ourworkinmusical accompani-\nment systems. Inthiseffortweresynthesize aprerecorded\naudio performance atvariable rate toaccompan yalive\nsoloist. Thesynchronization requires thatwebeginwith a\ncorrespondence between theprerecorded audio andamu-\nsical score.\nOn-line parsing, sometimes called score-following ,pro-\ncesses thedata inreal-time asthesignal isacquired. Thus,\nnolook ahead ispossible, aswell asimposing speed\nconstraints onthereal-time algorithm. The goal ofon-\nlineparsing istoidentify themusical events depicted in\nthescore with little latenc yandhigh accurac y.Musical\naccompaniment systems must perform thistask with the\nlivesoloist' sinput. Other applications include theauto-\nmatic coordination ofaudio-visual equipment with musi-\ncalperformance, such asopera supertitles andreal-time\nscore-based audio enhancement e.g.pitch correction. We\nwill treat theoff-line problem inthiswork, howeverex-\ntensions ofourapproach toon-line parsing arepossible.\nManyresearchers havetreated on-line andoff-line mu-\nsical parsing including [2],[12],[1], [6],[3],[7],[4],\n[5],[9],[11],toname several. See[10]forathorough\nbibliography ofthesubject. While manyvariations exist,\nthepredominant approach seeks abest possible match by\nwarping thescore to\u0002tthedata using some form ofdy-\nnamic programming. Themeasures ofmatch quality are\nquite varied, including edit-lik edistances andprobabilis-\nticmeasures, asinthepopular hidden Mark ovmodel ap-\nproaches. Without doubt, these efforts contain manyno-\ntable successes, however,theproblem stillremains open.\nInourpersonal experience with theHMM approach cited\nabove,results degrade, sometimes dramatically ,asween-\ncounter increasingly dif\u0002cult domains such ascomple x\npolyphon y,varied instrumental texture, fastnotes, reartic-\nulations andoctaveslurs, largetempo changes, unpitched\nsounds, etc.While theliterature contains verylittle inthe\nwayofformal evaluations, other researchers seem toex-\nperience similar problems. The need foramore robust\nandwidely applicable approach isthemotivation forthe\ncurrent work.\nWebelie vetheAchilles' heel ofallpast approaches\nweknow,including ourown, isthemodeling oflengthfortheindividual notes. Iftheissue istreated atall,note\nlengths areeither constrained tosome range ormodeled\nasrandom, with therange ordistrib ution depending ona\nglobal tempo orlearned from past examples. Either im-\nplicitly orexplicitly ,thenote lengths areregarded asinde-\npendent variables. However,note lengths areanything but\nindependent. Ourbelief, bolstered byconventional musi-\ncalwisdom, isthatthelion'sshare ofnote length variation\ncanbeexplained interms ofatime-v arying tempo pro-\ncess. Thefailure tomodel time-v arying tempo shifts more\nburden totheaudio data modeling, requiring themethod\ntofollo wthescore almost exclusi velyusing sound, with-\noutregard foroneofthemost basic aspects ofmusical\ntiming. This workexplicitly models tempo asareal-v alued\nprocess, hoping thatthemore powerful model willbeable\nexplain what thedata model cannot. Ourdata model, in-\ntroduced inSection 2,isindeed simple-minded, focusing\nexclusi velyonpitch content. While weexpect thatim-\nprovements tooursystem will beachie vedbystrength-\nening thismodel, theresults presented inSection 4argue\nthatourfocus onthetempo model iswell-placed.\nThe most straightforw ardapproach totempo model-\ningwould represent thestate oftheperformance asa\nscore position andtempo pairboth discrete variables.\nFrom frame toframe theposition would beupdated using\nthecurrent tempo while thetempo would beallowed to\ngradually vary.Wehaveattempted such anapproach us-\ningaHMM frame work, butfound thatthediscretization\nofposition andtempo needed tobeextremely \u0002nebefore\nuseful results were achie ved.This earlier effortis,byno\nmeans, astra wman created only tomotivatethecur-\nrentapproach. Rather ,ourcurrent approach stems from a\ndeeper understanding ofthecomputational issues learned\nfrom thisprevious effort.\nWe\u0002rst present inSection 2amathematical model\nthatcombines anote-le velmodel forrhythmic interpre-\ntation with aframe-by-frame data model. Thenote-le vel\nmodel explicitly represents tempo variation andnote-by-\nnote deviations. The data model isbased completely on\nthepitch content oftheaudio. The most likelyparse is\nnotcomputable byconventional means, howeverSection\n3introduces amethod bywhich excellent approximations\ntothemost likelyparse canbecomputed. Weattrib utethe\nsuccess oftheapproach tothenear-global optimization\nperformed inthissection. Section 4presents results ona\n55minute widely varied testsetofshort orchestral move-\nments containing examples from Mozart toShostak ovich.\nTheresults demonstrate thatthenote onset estimates are\ngenerally quite accurate, andonly veryrarely does theal-\ngorithm become irreco verably lost. Ourdataset hasbeen\nmade publicly available tofacilitate comparisons.\n2.THE MODEL\nInthecase ofmonophonic music, amusical score canbe\nrepresented asasequence ofscore positions, expressed in\nbeats, with associated pitches. Polyphonic music canbe\nviewed, similarly ,asasequence ofscore positions withassociated chords.Inthepolyphonic case, thescore po-\nsitions would becreated bysorting thecollection ofall\nmusical eventlocations (inbeats) overallparts, anddis-\ncarding duplicate positions. Each score position, inbeats,\nwould then beassociated with thecollection ofpitches\nthat sound until thenextmusical event. Thus oursim-\npli\u0002ed score does notrepresent which notes come from\nwhich instruments ordistinguish between attacking and\nsustaining notes inachord, although these aspects could\nbeincluded inamore sophisticated audio model than we\nuseatpresent. Wenotate thisscore by\u0001\u0003\u0002\u0005\u0004\u0007\u0006\t\b\n\u0004\f\u000b\r\u0006\u000e\u0001\u000f\u0002\u0011\u0010\u0007\u0006\t\b\f\u0010\n\u000b\r\u0006\n\u0012\f\u0012\n\u0012\f\u0006\n\u0001\u0003\u0002\u0014\u0013\u0015\u0006\u0016\b\u0017\u0013\u0018\u000b(1)\nwhere the \u0019theventbegins at\n\u0002\u001b\u001abeats and\n\b\n\u001aisthecol-\nlection ofcurrently-sounding pitches. Byconvention,\n\u0002\u001c\u0004\u001e\u001d\u001fand\n\b\f\u0013isa0-note chord corresponding tothesilence\nattheendoftheaudio.\nLet  \n\u0004!\u0006\n\u0012\f\u0012\f\u0012\f\u0006 \n\u001abethesequence oftimes, inseconds, at\nwhich thechord onsets occur .Typically theonset times\naretheproduct ofnumerous interpretati veissues aswell\nastheinevitable inaccuracies ofhuman performance. We\nmodel here what webelie vetobethemost important fac-\ntorsgoverning musical timing: time-v arying tempo aswell\nasnote-by-note deviations. More precisely ,wemodel a\nrandom process on  \n\u0004\n\u0006\f\u0012\n\u0012\f\u0012\f\u0006 \n\u0013and \"\n\u0004\n\u0006\f\u0012\n\u0012\f\u0012\n\u0006\"\n\u0013through\"\n\u001a\n\u001d\"\n\u001a\u000e#$\u0004&%(')\u001a(2) \n\u001a\n\u001d \n\u001a\u000e#*\u0004+%-,\u000f\u001a\"\n\u001a.%(/\n\u001a(3)\nfor\u0019\n\u001d102\u0006\n\u0012\f\u0012\n\u0012\f\u0006\t3where\n,\u0003\u001aisthelength, inbeats, ofthe\u0019thchord:\n,\u0003\u001a\n\u001d4\u0002\u001a65\n\u0002\u001a\u000e#$\u0004.\"\n\u0004\n\u0006\f\u0012\n\u0012\f\u0012\n\u0006\"\n\u0013described byEqn. 2isourtempo process\nwhere\"\n\u001aisthelocal beat length (secs. perbeat) atthe\u0019thchord. The\n')\u001avariables areassumed tohave0mean\nsothemodel givesthelocal tempo ateach newmusical\neventastheprevious tempo plus asmall error .Inother\nwords, tempo ismodeled asarandom walk.\nAccording tothemodel, each onset time, \n\u001a,isgiven\nastheprevious onset time,  \n\u001a\u000e#*\u0004,plus thecurrent chord\nlength aspredicted bythecurrent tempo (\n,7\u001a\"\n\u001a),plus a\nrandom increment (\n/\u000e\u001a).These lastrandom increments,\nthe\n/\n\u001a,arealso assumed tobe0mean variables sothey\ntend tobesmall. One possible viewofthe\n/\n\u001avariables\nisasagogicaccents atleast when theyarepositi ve\nandhence correspond tonote lengthenings. However,in-\ndependent ofanymusical modeling considerations, these\nvariables givethemodel ameans ofexplaining note length\nvariations assomething other than tempo change, thus sta-\nbilizing themodel.\nThedependenc ystructure ofthe \"and  variables isex-\npressed asadirected acyclic graph inthetopofFigure 1.\nIninterpreting thispicture, thebeha vior ofeach variable\n(node inthegraph) canbedescribed givenonly itsparents\ninthegraph, e.g. \n\u001adepends on \n\u001a\u000e#*\u0004and\"\n\u001a.\nLetting\"\n\u001d8\u0001\"\n\u0004\u0007\u0006\f\u0012\n\u0012\f\u0012\u0017\u0006\"\n\u0013\u0018\u000band \n\u001d8\u0001 \n\u0004\u0007\u0006\f\u0012\n\u0012\f\u0012\u0017\u0006 \n\u0013\u0018\u000b,this\nmodel leads toasimple factorization ofthejoint probabil-0 0 0 0 0 1 1 2 2 1 2 0 x\nyts\n...\n...\nFigur e1.The dependenc ystructure ofthemodel vari-\nables expressed asadirected acyclic graph (DAG).Circles\ndenote continuous variables, while squares denote discrete\nvariables. Thedarkened squares represent observ edvari-\nables thespectrogram frames.\nitydensity , 9\n\u0001\"\n\u0006 \n\u000b,as9\n\u0001\"\n\u0006 \n\u000b:\u001d9\n\u0001\"\n\u0004\n\u000b9\n\u0001 \n\u0004\f\u000b\n\u0013;\u001a\r<*\u0010\n9\n\u0001\"\n\u001a>=\"\n\u001a\u000e#*\u0004\f\u000b9\n\u0001 \n\u001a>= \n\u001a\u000e#*\u0004?\u0006\"\n\u001a?\u000b\nThefactors inthisequation are,more explicitly ,9\n\u0001\"\n\u0004\u0017\u000b@\u001d A(\u0001\"\n\u0004?BDCFEDG?\u0006'\n\u0010E\nG\n\u000b9\n\u0001 \n\u0004\n\u000b@\u001d A(\u0001 \n\u0004\nBDC$H7G?\u0006'\n\u0010HG\n\u000b9\n\u0001\"\n\u001a\n=\"\n\u001a\u000e#*\u0004\n\u000b@\u001d A(\u0001\"\n\u001a\nB\"\n\u001a\u000e#$\u0004\n\u0006'\n\u0010EJI\n\u000b(4)9\n\u0001 \n\u001aK= \n\u001a\u000e#*\u0004!\u0006\"\n\u001a\u0007\u000b@\u001d A(\u0001 \n\u001aLB \n\u001a\u000e#*\u0004%-,\n\u001a\"\n\u001aM\u0006'\n\u0010HI\n\u000b(5)\u0019\n\u001dN0\u001e\u0012\f\u0012\f\u0012\f\u0006\t3,where\nA-\u0001JOP\u0006DCQ\u0006'\n\u0010\u000bdenotes theunivariate\nnormal density function with mean\nCandvariance\n'\n\u0010\n.\nThemodel parameters\nCE\nG?\u0006'\n\u0010E\tG\n\u0006DC$H7G?\u0006'\n\u0010HGand R\n'\n\u0010EJI\n\u0006'\n\u0010HIMS\n\u0013\u001a\r<*\u0010\nareassumed known. Inpractice,\nCE\nGand\n'\n\u0010EDGwould be\nchosen tore\u0003ect ourknowledge about theinitial tempo.\nThe R\n'\n\u0010EJI\n\u0006'\n\u0010HIMS\n\u0013\u001a\r<*\u0010variances should re\u0003ect thatboth tempo\nchanges andnote-by-note changes innote length increase\nwith longer note value. The\n'\n\u0010HGvariance should bees-\nsentially in\u0002nite, corresponding toourlack ofknowledge\nabout where the\u0002rstnote ofthepiece willbegin,thereby\nrendering thechoice of\nC\nHGirrele vant.\nOuraudio datacome from asampled audio signal which\nwepartition intoasequence ofoverlapping frames, TMU\n\u0006\f\u0012\n\u0012\f\u0012TWV\n#$\u0004,\neach corresponding to Xseconds ofsound ( XZY\\[\n\u001fms.\ninourexperiments). Foreach frame, ]\n\u001d\u001f\n\u0006\n\u0012\f\u0012\n\u0012\u0017\u0006\tA54^,\nwelet _)`denote theindexofthechord thatissounding\nfortheframe. Forexample, thesequence ofvalues:_U\n\u0006_\n\u0004\n\u0006_\n\u0010\n\u0012\f\u0012\f\u0012L\u001d\u001fW\u001f\n\u0012\f\u0012\f\u0012\u001fa b\u0017c degf\n^W^\n\u0012\n\u0012\f\u0012^a b\rc de\nG\n0h0i\u0012\f\u0012\n\u0012\t0a b\rc degj\n\u0012\f\u0012\n\u0012(6)\nwould signify thatthe\u0002rstnote begins at k\fU?X seconds and\nlasts fork\n\u0004Xseconds, thesecond note beginsat\n\u0001k\fU\n%k\n\u0004\f\u000bX\nseconds andlasts fork\n\u0010Xseconds, etc.Inourmodel each\nframe ofaudio data depends only onthecurrent chord\nwhich issounding, thus if_\n\u001d_U\n\u0006\n\u0012\f\u0012\f\u0012\n\u0006_V\n#*\u0004andT\n\u001dTU\n\u0006\f\u0012\n\u0012\f\u0012\u0017\u0006TV\n#$\u0004,wehave9\n\u0001T\n=_\n\u000b+\u001d\nV\n#$\u0004;`\n<U\n9\n\u0001TW`\n=_)`\n\u000b0 2 4 6 8 100 5 10 15 20 25\nfreqspectral energy\n0 2 4 6 8 100 10 20 30 40\nxy1 + y2\nFigur e2.Left: Anidealized spectrum forasingle\nnote. Right: Ananalogous spectrum fortwosimulta-\nneous notes created asasuperposition oftwosingle-note\nspectra.\nTheconditional distrib ution of Tgiven _isdepicted inthe\nbottom ofFigure 1.\nOuractual data model isgivenby9\n\u0001T`\n=_`\n\u000b&\u001dl\b\u0007\u0001T`\n\u000b-m\n;n\n<o\u0004MpWq\u000er\n\u00017s$\u000bDtrMu\nn\u000ev\u0003wyx\n(7)\nInEqn. 7,p\nqrisanidealized powerspectrum associated\nwith chord indexedby _z`andisobtained asasuperpo-\nsition oftheidealized individual note spectra and then\nnormalized tosum to1overthefrequenc yvariable,\ns.\nTheleftpanel ofFigure 2showsanexample ofanideal-\nized single-note spectrum, while theright panel givesthe\nidealized spectrum foratwo-note chord. Thus, theright\nspectrum, normalized tosum toonewould play therole\nofpWq\u000erinEqn. 7forthecorresponding two-note chord.\ninEqn. 7, T`istheobserv edspectrum forthe ]thaudio\nframe, and {isascaling constant thatweights thecontri-\nbution ofthedata term totheoverall model. The factor\b\u0007\u0001Th`\n\u000bcanbedisre garded since the TM`variables are\u0002xed,\nthus the\n\bW\u0001Th`\n\u000b'sareconstant. This data model would re-\nsultifwewere toassume thattheobserv edspectrogram\nwasthehistogram ofarandom sample from theprobabil-\nitydistrib ution givenbyp\nqr.However,evenwithout such\njusti\u0002cation, themodel isintuiti velyplausible.\nWeareinterested inspecifying ajoint model onthe\nvariables \"\n\u0006 \n\u0006_\n\u0006T.Thekeyobserv ation here isthat, upto\nadiscretization error ,  and _contain identical informa-\ntion: thepartitioning oftheaudio signal intochords. Thus\nwelosenothing byeliminating andmodeling9\n\u0001\"\n\u0006_\n\u0006T\n\u000b.\nTodothis, note thatanysequence_\n\u001d_zU\n\u0006\n\u0012\f\u0012\f\u0012\f\u0006_|V\n#*\u0004im-\nplicitly \u0002xesthetheactual onset times through \n\u001a2\u0001_\n\u000b+\u001dl}\u0015~PR\u000e]\u0007_)`\n\u001d\u0019\nSX (8)\nsowehave9\n\u0001\"\n\u0006_\n\u000b\u001b\u001d9\n\u0001\"\n\u0006 \n\u0001_\n\u000b\t\u000bwhere thelater proba-\nbility isgivenbythemodel ofEqns. 2and3.Wehave0\n0\n0 1 11\n2\n0 11 1 1 2 2 2 3\nFigur e3.The search treeassociated with optimization.\nThelabel ofatreenode isthescore indexdescribing the\ncurrent chord.\nalready described theconditional distrib ution9\n\u0001T\n=_\n\u000b,so9\n\u0001\"\n\u0006_\n\u0006T\n\u000b@\u001d9\n\u0001\"\n\u0006_\n\u000b9\n\u0001T\n=_\n\u000b(9)\u001d9\n\u0001\"\n\u0004\n\u000b9\n\u0001 \n\u0004\u0017\u000b\n\u0013;\u001a\r<*\u0010\n9\n\u0001\"\n\u001aK=\"\n\u001a\u000e#$\u0004\n\u000b9\n\u0001 \n\u001aK= \n\u001a\u000e#*\u0004?\u0006\"\n\u001a?\u000b(10)\nV\n#*\u0004;`\n<U\n9\n\u0001T`\n=_`\n\u000b\nWhile theright hand sideofEqn. 10appears todepend on ,werecall that \n\u001a\n\u001d \n\u001a\n\u0001_\n\u000b,asinEqn. 8.\nWecannot write anysimple DAGrepresentation ofthe\nprobability distrib ution on \"\n\u0006_\n\u0006T.Theactual DAGwould\nbecompletely connected between the \"and _layers. How-\never,themodel isstill computationally tractable, aswe\nshowinthefollo wing section.\n3.COMPUTING THE MAP ESTIMA TE\nOurgoal isnowphrased asfollo ws:Giventheobserv ed\ndata, T,weseek themost likelycon\u0002guration oftheunob-\nservedvariables,\"and_:\u0001\u0017\"\n\u0006L_\n\u000b+\u001d4\u0007\u0016:}?Eyq\n9\n\u0001\"\n\u0006_\n\u0006T\n\u000b(11)\nIfallvariables were discrete, thisproblem would besolv-\nablethrough traditional dynamic programming techniques,\nhowever,note thatthetempo process,\",iscontinuous. We\ndescribe here amethod forapproximating theglobal solu-\ntiontoEqn. 11using atechnique thatissimilar todynamic\nprogramming, butallowsustotreat continuous variables.\nConsider thetreeofFigure 3,which describes anenu-\nmeration ofallpossible realizations ofthelabeling pro-\ncess _.First, theroot ofthetreeislabeled 0.Then, any\nnode inthetreewith label \u0019\n3willhavetwochildren\nlabeled by \u0019and \u0019\n%4^,while anode labeled\n3willhave\nasingle child labeled\n3.Thelabels\n\u001fand\n3correspond\ntothesilence atthebeginning andendoftheaudio data.\nNote thatanypath from theroot ofthetreetoanode at\ndepth ]traverses asequences oflabels corresponding toa\npossible realization ofthe _)U\n\u0006\n\u0012\f\u0012\f\u0012\f\u0006_|`,andhence analign-\nment ofthe\u0002rst]\n%^frames ofaudio data; clearly all\npossible realizations arecontained inthetree.Traversing apartial path through thetree(\u0002xing _U\n\u0006\n\u0012\f\u0012\f\u0012\n\u0006_`)\nimplies thatthe\u0002rst ]\n%^frames contain \u0019notes where\u0019\n\u001d_)`.Additionally ,the\u0002rstvariables  \n\u0004!\u0006\f\u0012\n\u0012\f\u0012\u0017\u0006 \n\u001aarealso\ndetermined through Eqn. 8.Forthepartial path, _)U\n\u0006\f\u0012\n\u0012\f\u0012\f\u0006_|`,\ntheprobability density forthevariables _\n`U\n\u001d\u0001_|U\n\u0006\n\u0012\f\u0012\f\u0012\f\u0006_|`\n\u000b,T\n`U\n\u001d\u0001T\u0007U\n\u0006\f\u0012\n\u0012\f\u0012\f\u0006TW`\n\u000b, \"\n\u001a\u0004\n\u001d\u0001\"\n\u0004!\u0006\n\u0012\f\u0012\n\u0012\f\u0006\"\n\u001a?\u000bis9\n\u0001\"\n\u001a\u0004\n\u0006_\n`U\n\u0006T\n`U\n\u000b@\u001d9\n\u0001\"\n\u0004\n\u000b9\n\u0001 \n\u0004\n\u000b\n\u001a;\n<*\u0010\n9\n\u0001\"\n\n=\"\n\n#$\u0004\f\u000b9\n\u0001 \n\n= \n\n#$\u0004?\u0006\"\n\n\u000b(12)\n`;\n<U\n9\n\u0001T\n\n=_\n\n\u000b\n(again \n\u001a\n\u001d \n\u001a\n\u0001_\n`U\n\u000b)Foreach partial path_\n`Ude\u0002ne9q\nrf\n\u0001\"\n\u001aW\u000b+\u001d }?E\nG\r E\nIyG9\n\u0001\"\n\u001a\u0004\n\u0006_\n`U\n\u0006T\n`U\n\u000b(13)9q\nrf\n\u0001\"\n\u001a\u0007\u000bgivesthequality ofaparticular path _\n`Uasafunc-\ntionofthecurrent tempo \"\n\u001a,assuming themost favorable\nchoice ofpasttempo variables \"\n\u0004?\u0006\f\u0012\n\u0012\f\u0012\f\u0006\"\n\u001a\u000e#$\u0004.\nWhile thecalculations aresome what involved,\n9q\nrf\n\u0001\"\n\u001a\n\u000b canbeshowntofollo wthesimple parametric form3-\u0001\"\nB\u0016F\u0006D\u0002\u0006D2\u000b:\u001d4|\n#\nGju\nE\u0016#)v\njw\t\n(14)\nwith parameters\n*\u0006\t\u0002\u001c\u0006\t.Infact,asimple recursion can\nbedeveloped forthisfunction asoneproceeds downa\nparticular path inthetreeofFigure 3,asfollo ws. Sup-\npose_\n`\n#$\u0004U\n\u001d\u0001_U\n\u0006\f\u0012\n\u0012\f\u0012\n\u0006_`\n#$\u0004\n\u000bissuch that_`\n#$\u0004\n\u001d\u0019and9q\nr\nGf\n\u0001\"\n\u001a\n\u000b6\u001d3-\u0001\"\n\u001a\nB\u0016F\u0006D\u0002\u0006D2\u000b.There aretwocases. First,\nif _`\n\u001d_`\n#$\u0004(the \u0019thchord persists through the ]th\nframe), then9q\nrf\n\u0001\"\n\u001a\u0007\u000b@\u001d 9q\nr\nGf\n\u0001\"\n\u001aW\u000b9\n\u0001TW`\n=_|`\n\u000b\u001d 3-\u0001\"\n\u001a\nBy9\n\u0001T`\n=_`\n\u000b\r\u0006D\u0002\u0006D2\u000b\nOtherwise,9q\nrf\n\u0001\"\n\u001a\ro\u0004\n\u000b@\u001d }?EJI\n9q\nr\nGf\n\u0001\"\n\u001a\n\u000b9\n\u0001\"\n\u001a\ro\u0004\n=\"\n\u001a\n\u000b9\n\u0001 \n\u001a\ro\u0004h= \n\u001aL\u0006\"\n\u001a\ro\u0004\n\u000b9\n\u0001TW`\n=_|`\n\u000b(15)\u001d 3\u0001\"\n\u001a\ro\u0004\u0007B\u0016|\u0003\u0006\t\u0002\u00147\u0006DL\u000b\nwhere>8\u001d\n9\n\u0001Th`\n=_)`\n\u000b0?'\n\u0010EJIDG\n'\n\u0010H\u0003IDG\n\n#\nGj ¢¡\nI\tG\n¡\nI\nh£IDGJ¤o¥j£jIDG ¢¦\nh§j¨IDG\n¥\nh§j¡\nIDG\u0002\n\u001d\n\u0002'\n\u0010HIDG\n%-,\n\u001a\ro\u0004W\u0001 \n\u001a\ro\u00045 \n\u001a\u0007\u000b\f\u0001\u000f%('\n\u0010EJIDG\n\u000b,\n\u0010\u001a\u0017o\u0004\n\u0001\u0003%-'\n\u0010EJI\tG\n\u000b%-'\n\u0010HIDGM8\u001d\n\u0001\u000f%('\n\u0010E\nIDG\n\u000b'\n\u0010H\u0003I\tG,\n\u0010\u001a\ro\u0004\n\u0001\u000f%('\n\u0010EJIDG\n\u000b%-'\n\u0010HI\tG\nNote that, rather than having asingle score foreach\npartial path, _\n`U,wedescribe thequality ofthepath asa\nfunction ofthecurrenttempo ,\"\n\u001a:\n9q\nrf\n\u0001\"\n\u001a\u0007\u000b:\u001d©3-\u0001\"\n\u001aMBy*\u0006\t\u0002\u001c\u0006\tL\u000b.\nIninterpreting thisrepresentation, thescaling parameter ,Figur e4.Left: The functionsR\n9q\nrf\n\u0001\"\n\u001a\n\u000bSbefore Right:\nThereduced collection offunctions after thinning.,givesanoverall description ofthequality ofthepartial\npath since itisthemaximal probability attainable. That is}?E\n3\u0001\"\nB\u0016F\u0006D\u0002\u0006D2\u000b:\u001d©3-\u0001\u001f\n\u0006\u0016F\u0006D\u0002\u0006D2\u000bQ\u001d4\u0002describes thebestvalueofthecurrent tempo, \"\n\u001a,forthe\npath.\nisameasure ofhowfastthepath quality decreases\naswemoveawayfrom thebesttempo,\n\u0002.\nClearly thenumber oftreenodes isexponential inthe\ntree'sdepth making itimpossible toexplore thetreethor-\noughly without additional insight. The keyobserv ation\nhere isthatsome partial paths canbeeliminated without\nsacri\u0002cing thesearch fortheoptimal path.\nSuppose thattwopartial paths_\n`Uandª _\n`Uaresuch that\nboth beginthe\u0019thnote atthe]thframe:_`\n\u001dª _`\n\u001d\u0019\nand_`\n#$\u0004\n\u001dª _`\n#$\u0004\n\u001d\u0019\n5©^.Ifwealso have\n9q\nrf\n\u0001\"\n\u001a\n\u000b¬«9®­q\nrf\n\u0001\"\n\u001a\n\u000bforallvalues ofthecurrent tempo \"\n\u001a,then no\nmatter howthepaths continue, the _\n`Ubranch willalways\nbeat the ª _\n`Ubranch. So,without anyloss, wecaneliminate\nthelatter branch.\nMore generally ,suppose that kisthecollection ofpaths\nthatbeginthe \u0019thnote onthe ]thframe, i.e.k\n\u001dR\u000e_\n`U\n\u0007_`\n#$\u0004\n\u001d\u0019\n5¯^\n\u0006_`\n\u001d\u0019\nS\nDe\u0002ne theset°i±\n~PF\u0001k\n\u000basthesmallest subset ofksuch\nthat}?q\nrfM²\u0007³|´\fµ ¶u\ne\nv\n9q\nrf\n\u0001\"\n\u001a\n\u000b:\u001d©}?q\nrf\n²\ne\n9q\nrf\n\u0001\"\n\u001a\n\u000bforall \"\n\u001aasinFigure 4.Thus °i±\n~·o\u0001k\n\u000baretheGaus-\nsians thatattain themaximum valueforsome tempo value.\nPathsnotinkarenotoptimal foranyvalue of\"\n\u001a,so,rea-\nsoning asabove,wecaneliminate anysuch path without\nlossofoptimality .Due tothesimple parametric form of\nthe\n9q\nrf\n\u0001\"\n\u001a\n\u000bfunctions, thethinning procedure canbecom-\nputed with acomputational cost thatisquadratic in\n=k\n=.\nThe thinning algorithm canbeperformed onarestricted\nsetofpossible tempo values, say\n\u0001\"!¸\nµ ¶\n\u0006\"\n¸Q¹Dº\n\u000btoachie ve\ngreater reduction ofthepartial paths. Analgorithm forthe\nthinning operation isdiscussed in[8]aswell adiscussion\nofcomputational comple xityandtheoptimality properties\noftherestricted thinning algorithm.\nInourexperiments thenumber ofkernels thatsurvi ve\nthethinning process does notincrease with thenumber\noforiginal kernels. Assuming thatthenumber ofsurvi v-\ningkernels ofeach thinning operation isbounded bysomemaximum, thethinning procedure reduces thenumber of\npartial paths ateach frame toanumber thatis,atworst,\nlinear in\n3thenumber ofchords inthescore. The\npaths survi ving thinning represent atinyfraction ofwhat\nwould existotherwise since, without thinning, thenumber\nofpartial paths growsexponentially in ],thecurrent anal-\nysisframe. Inourapplications,\n3canbeinthethousands,\nsoanalgorithm thatislinear in\n3isstillnottractable and\nmore pruning must bedone. Wediscuss further pruning\ninthefollo wing section.\nThe\u0002nal parse isobtained bytracing back thebestsur-\nviving path atthe\u0002nal frame,\nA.That is,ourparse esti-\nmate is\n_\nVU\n\u001d©\u0007\u0016Q}?q\u000e»\nf\nF\u0001_\nVU\n\u000bwhere9q!»\nf\n\u001d©3-\u0001\"\n\u0013\nB\u0016o\u0001_\nVU\n\u000b\u0017\u0006D\u0002¼\u0001_\nVU\n\u000b\u0017\u0006Dz\u0001_\nVU\n\u000b\t\u000b\nandonly paths thatreach the\u0002nal score position,\n3,in\nthe\nAthframe areconsidered. Itisalso straightforw ard\ntorecoverthehidden tempo variables, although wedonot\ndosointhisparticular application.\n3.1. Further Pruning\nAsobserv edabove,westillneed toprune paths tomake\ntheproposed algorithm feasible. Asimple approach would\nbetosortthecurrent (survi ving) hypotheses onthe\npa-\nrameter andprune thesmallest ofthese. Inexperimenting\nwith thispruning method, wehaveobserv edthatbranches\nalready exceeding areasonable amount oftime forthe\ncurrent chord avoidbeing pruned bydelaying thechord\nchange, thereby delaying incurring theassociated note length\nfactor 9\n\u0001 \n\u001a\ro\u0004\n= \n\u001a\n\u0006\"\n\u001a\n\u000b.This phenomenon isanalogous to\nthehorizon effect ofcomputer chess inwhich hypothe-\nsesrecei vefalsely in\u0003ated scores bypostponing anin-\nevitable endbeyond thesearch horizon.\nAsecond problem isthat, atanyparticular analysis\nframe, ],thevarious partial paths, _\n\u001d\u0001_)U\n\u0006\f\u0012\f\u0012\n\u0012\f\u0006_|`\n\u000b,will\nrepresent different positions inthemusical score. Sup-\npose thataparticular path iscurrently inthe \u0019thnote in\nthescore. Then thelikelihood ofthispartial path willcon-\ntainafactor foreach ofthe \n\u0004!\u0006\n\u0012\f\u0012\n\u0012\f\u0006 \n\u001aasinEqn. 12.Since\u0019varies overthepartial paths, the\n-scores arecomposed\nofdifferent numbers offactors anddirect comparison is\nsuspect.\nWeremedy these problems bysorting thepartial paths\nover½\n\u0001_\n\u000b:\u001d4¾\u001b\u0001_\n\u000b%\n\u0013`VÀ¿\nu\nq\nv\u001aandpruning thepaths hav-\ningthelowest ½\n\u0001_\n\u000bscores, where¾\u001b\u0001_\n\u000bÁ\u001d\n`Â\n<U>ÃPÄ\n9\n\u0001T\n\n=_\n\n\u000bÅ\n\u0001_\n\u000b@\u001d }?E\nG\n E\nIDGy\nH\u0003I\tGyÆ`Ã·Ä\nR\u00169\n\u0001\"\n\u0004\u0017\u000b9\n\u0001 \n\u0004\f\u000b\u001a\rÇ\u0004;\n<*\u0010\n9\n\u0001\"\n\n=\"\n\n#$\u0004\f\u000b9\n\u0001 \n\n= \n\n#$\u0004?\u0006\"\n\n\u000bS\nIntheaboveequation,\n¾\u001b\u0001_\n\u000bissimply thedata loglike-\nlihood ofthepartial path.\nÅ\n\u0001_\n\u000bistheoptimal model\nloglikelihood forthe\u0002rst\u0019\n%©^tempo andposition vari-\nables with \n\u001a\ro\u0004taking some value inthefuture. Since apartial path, _,implicitly \u0002xesthe\u0002rst \u0019position vari-\nables  \n\u0004\n\u0006\f\u0012\n\u0012\f\u0012\f\u0006 \n\u001a,weonly maximize overtheremaining\nvariables. While weomit thecalculation, onecaneasily\ncompute\nÅ\n\u0001_\n\u000brecursi velyas ]increases. Atany\u0002xed\niteration, then wearesorting overthedata loglikelihood\nplus aconstant times theaveragemodel likelihood, there-\nfore notpenalizing thepaths with more notes. However,\nas ]ÉÈ\nAand \u0019¼È\n3thisgradually reduces tothethe\noriginal loglikelihood\n¾\u001b\u0001_\n\u000b%Å\n\u0001_\n\u000basin13.\n4.EXPERIMENTS\nToevaluate ouralgorithm weconstructed atestsetofshort\norchestral movements (and oneopera selection), repre-\nsenting avariety ofmusical styles. Therestriction tothe\norchestral domain does notre\u0003ect aknownlimitation of\nourmethods tothecontrary ,orchestral music isquite\nheterogeneous andcontains manyofthedata types webe-\nlievetobemost problematic forscore matching, such as\ntempo changes, rubato, fastnotes, andvaried instrumen-\ntaltexture. Thechoice ofdata wasin\u0003uenced bypersonal\ntaste.\nRecall that ourmusical score isrepresented asase-\nquence of(musical time, chord) pairs asinEqn. 1.In\nmanycases, thisrepresentation canbeconstructed auto-\nmatically from aMIDI \u0002le. Tothiswecollected around\n20MIDI \u0002les from theClassical Midi Archives .Increat-\ningourscores wereplaced note sequences within asingle\nvoice thatappeared tobetrills ortremolos bysustained\nnotes. Inaddition, twonotes verynearly sharing thesame\nonset time areboth assumed tobeginatthesimpler mu-\nsical time (the onewith thesmaller denominator ,when\nexpressed inbeats). Aside from these special cases, the\nprocessing consists ofastraightforw ardextraction ofdata\nfrom theMIDI \u0002les. Inparticular ,ouralgorithm creates,\nforeach MIDI \u0002le,anote listcontaining themusical onset\ntimes with thecorresponding MIDI note commands, alist\noftempo changes, andlistofmeter changes.\nAbout halfofthese \u0002les were rejected forvarious rea-\nsons, either before orafter thispreprocessing stage: some\n\u0002les were piano reductions, some hadsuspiciously com-\nplexreconstructed rhythm suggesting expressi vetiming,\nsome contained other assorted anomalies. There isnorea-\nsontoassume thatourmatching algorithm would failon\ntheMIDI \u0002les werejected. Infact,aprevious experi-\nment showed excellent results with apiano transcription\noftheSacri\u0002cial Dance from Stravinsk y'sLeSacredu\nPrintemps .However,ourgoal here wastokeeptheex-\nperimental conditions asconstant aspossible overthetest\nset.Despite thisgoal, theaccepted MIDI \u0002les were notof\nuniform quality .Some contain manywrong notes, some\ncontain numerous tempo changes while others only mark\nsudden andsigni\u0002cant tempo changes, andother sources\nofvariability probably exist. Nonetheless, weresisted the\nurgetotweak thescores byhand.\nForeach ofthesurvi ving \u0002leswethen took correspond-\ningaudio data from aCD, downsampled tomono 8KHz.\nTable 1givesthetestsettotaling nearly 55minutes ofmu-Work Orchestra Conductor Year Mins.\nMahler\nSymphon y4Mvmt. 1 Boston Ozawa 1987 5.23\nHolst\nThePlanets Mercury Toronto Davis 1986 4.03\nThePlanets Mars Toronto Davis 1986 6.88\nMozart\nSymph. 41Mvmt 2 Berlin vonKarajan 1978 7.78\nSymph. 41Mvmt 4(1) Berlin vonKarajan 1978 2.20\nSymph. 41Mvmt 4(2) Berlin vonKarajan 1978 3.84\nCosi FanTutte Overture London Haitink 1987 4.54\nibid. Soa veSiailVento London Haitink 1987 3.02\nDebussy\nTroisNocturnes Fetes Cleveland Boulez 1995 6.52\nDvorak\nSymphon y8Allegretto London Leppard 1997 6.05\nShostak ovich\nSymphon y1Mvmt 2 Chicago Berstein 1989 4.88\nTable 1.Thetestsetforthescore matching experiments.\nsic.TheMahler example isonly the\u0002rst\u0002veorsominutes\nofthemovement. Thelastmovement oftheMozart sym-\nphon ywasbrokeninto twosections duetoarepeat that\nappeared intheMIDI \u0002le(and hence ourscore) butnot\ntheperformance acommon problem.\nForeach ofthese examples wecreated ground truth by\nplaying theaudio \u0002lefrom thecomputer andtapping along\nonthekeyboard while recording thetimes ofeach key\npress. Each section ofconstant meter wasgivena\u0002xed\nnumber oftaps permeasure. These \u0002les were then hand-\ncorrected using aninteracti veprogram thatallowstheuser\ntosee,hear,andadjust thetaps which aresuperimposed\nvisually overthetheaudio spectrogram andaurally (as\nclicks) overthesound \u0002le. The tap\u0002les arenotparticu-\nlarly accurate inidentifying individual beat locations, but\nalsodonotaccumulate error overtime. That is,theynever\nget lost.\nOur model iscompletely speci\u0002ed once wedescribe\nthevariances ofEqns. 4and5, R\n'\n\u0010EDI\n\u0006'\n\u0010HIS.Wemodel the\nvariances asparametric functions oftheexpected note du-\nrations, computed using thenote lenths R\n,\n\u001aSandthelo-\ncaltempo prescribed inthescores derivedfrom theMIDI\n\u0002les. Inparticular ,wemodel these variances aslinear\nfunctions oftheexpected note duration. Itwould also be\npossible tomodel thevariances asparametric functions of\ntheexpected duration, derivedfrom theexpected tempo\nassociated with each brach ofthesearch tree. While we\nbelie vethese twoapproaches would givenearly identi-\ncalresults, thelatter introduces amodeling complication,\nsince ourmodel assumes thevariances areknownapri-\nori,rather than functions ofunkwno wntempo variables.\nHowever,thelatter method also uses amore accurate ex-\npected duration inthecomputation ofthemodel variances\nandmight bene\u0002t from this. Thelinear parameters were\nchosen bytrial anderror ,though wewillestimate amore\ncomple xparametric model infuture work.\nWethen processed each ofouraudio \u0002les according to\nouralignment method, asdescribed inthepreceding sec-\ntions. Everytime abranch ofthesearch treebegananote\ncorresponding toatempo change (asgivenbytheMIDI\n\u0002le) Wereset thetempo distrib ution tohavethemean in-\ndicated bytheMIDI \u0002lewith arather largeand\u0002xedvari-Histogram of error[abs(error) < 1]\nerrorsFrequency\n−1.0 −0.5 0.0 0.50 500 1000 1500 2000\nHistogram of error[abs(error) >= 1]\nerrorsFrequency\n0 5 10 15 200 5 10 15 20\nFigur e5.Top: Histogram oferrors lessthan 1sec. in\nabsolute value. Bottom: The remaining errors. Note\nthatsimilar barheights represent about 50times asmany\ncounts ontheleftpanel.\nance. Theresult oftheprocess isacollection ofestimated\nonset times, oneforeach note ofthescore, written outto\na\u0002le. Foreach note thatfallsonabeat, wecompute the\nerror asthedifference between theestimated onset time\nandthecorresponding taptime. Oftheentire collection of\n5038 error estimates, 95% oftheerrors were lessthan .25\nsec. inmagnitude, while 72% were lessthan .125 secs.\nIninterpreting these results, oneshould keepinmind that\nthetapping ground truth isnotespecially accurate, thereby\nmaking themeasured performance ofthealgorithm ap-\npear worse than theyareintruth. Infact,ourlistening\noftheclick \u0002les forboth therecognized data andthe\nground truth suggests thattherecognized results aregen-\nerally more accurate than theground truth. Since display-\ningallerrors inthesame histogram renders therarer large\nerrors invisible, Figure 5givestwohistograms: theleft\npanel showsthedistrib ution oftheerrors thatarelessthan\n1sec., while theright histogram givestheremaining er-\nrors. Note the50-fold difference inscale between thetwo\nhistograms. Wesuspect thatthelefthistogram really says\nmore about theground truth than ourrecognition accu-\nracy.\nThe histograms ofFigure 5showthat ouralgorithm\ngivesaccurate performance when itisnotlost. Figure 6\nshowsadifferent aspect ofthealgorithm' sperformance\nbygiving theerrors, plotted against beat times, foreach\npiece inthecollection. InFigure 6theindividual traces0 2 4 60 5 10 15 20\nminutesseconds\nsoaveshostakovichfetedvorakmarsmercurymahlercosi_overturejupiter_andantejupiter_mvmt4.0jupiter_mvmt4.1\nFigur e6.Error vs.beat time foreach piece inthedata\nset.Thevarious examples arestack edvertically forease\nofcomparsion, soerrros areseen asdeviations from the\nbaseline rather actual heights.arestack edvertically forthesakeofcomparison, sothe\nerrors should beinterpreted asdeviations from thebase-\nline,thus weseeoccasional bursts oferrors ontheorder\nofseveralseconds. Figure 6demonstrates therather sur-\nprising ability ofouralgorithm torecoverafter signi\u0002cant\nerrors. Infact,theonly places inwhich oursystem does\nnotrecoverfrom being lostarenear theveryends ofthe\nexcerpts.\nThepredominant cause ofthesigni\u0002cant errors appear -\ninginFigure 6issections ofmusic with little ornopitch\nvariation. Recall thatourdata model isbased solely on\npitch content sothedata model contrib utesessentially no\ninformation when thepitch content isstatic. Not sur-\nprisingly ,ouralgorithm experienced dif\u0002culty with such\nsections, asintherepeated fffftritone-laden chords inthe\nbrass andstrings ending Mars;thepianissimo open \u0002fths\ninthestrings ending theShostak ovich movement; theharp,\nstring, andtimpani ostinato theprecedes thetrumpet trio\ninFetes;thelong cadence intoGMajor attheendMahler\nexcerpt (bar102); andthe\u0002nal chords oftheMozart over-\nture. This suggests thatrather simple impro vements toour\ndata model, such asmodeling note attacks, might produce\nbetter performance insuch cases.\nThe graphs and analyses provide asense ofoural-\ngorithm' sperformance onthisdif\u0002cult testset,however,\ntheyarenosubstitute foranaudio demonstration. Tothis\nend, wehaveputclick \u0002les ofeach piece ontheweb at\nhttp://fafner .math.umass.edu/ismir04\nThese audio (.wav)\u0002les contain theoriginal performance\nwith clicks superimposed ateverydetected note onset.\nWeencourage theinterested reader tolisten tosome of\nthese examples. Inaddition, tofacilitate comparisons, the\nabovedirectory also contains theoriginal MIDI \u0002les, the\ntap\u0002les, theestimates produced byouralgorithm, aswell\nastheoriginal 8KHz audio \u0002les.\n5.ACKNO WLEDGMENTS\nThis worksupported byNSF grant IIS-0113496. Thanks\ntoDon Byrd andmembers oftheVariations2 Digital Mu-\nsicLibrary Project atIndiana University fordiscussions\nregarding practical aspects ofthiswork.\n6.REFERENCES\n[1]Baird B.,Blevins D.,andZahler N.Arti\u0002-\ncialIntelligence andMusic: Implementing an\nInteracti veComputer Performer, Computer\nMusic Journal vol.17,no.2,1993.\n[2]Dannenber gR.AnOn-Line Algorithm for\nReal-T ime Accompaniment, Proceedings of\ntheInternational Computer Music Confer -\nence,1984 ICMA, Paris, France, 1984.\n[3]Grubb L,and Dannenber gR.AStochastic\nMethod ofTracking aVocal Performer, Pro-ceedings oftheInternational Computer Music\nConfer ence,1997 ICMA, 1997.\n[4]Loscos A.,Cano P.,and Bonada J.Score-\nPerformance Matching using HMMs, Pro-\nceedings oftheInternational Computer Music\nConfer ence,1999 ICMA, 1999.\n[5]Orio, N.,andDechelle, F.Score Following\nUsing Spectral Analysis andHidden Mark ov\nModels, Proceedings oftheInternational\nComputer Music Confer ence,2001 ICMA,\n2001.\n[6]Puck ette, M.Score follo wing using thesung\nvoice, Proceedings oftheInternational Com-\nputer Music Confer ence,1995 ICMA, 1995.\n[7]Raphael, C.Automatic Segmentation of\nAcoustic Musical Signals Using Hidden\nMark ovModels, IEEE Trans. onPAMI vol.\n21,no.4,1999.\n[8]Raphael, C.AHybrid Graphical Model for\nRhythmic Parsing, Arti\u0002cial Intellig ence vol.\n137, no.1,2002.\n[9]Turetsk yR.,andEllis D.Ground-T ruthTran-\nscriptions ofReal Music from Force-Aligned\nMIDI syntheses, Proc.Int.Symp. Music Info.\nRetrie val,2003 Baltimore MD, 2003.\n[10] Schw arz D. Score Follow-\ning Commented Bibliography,\nhttp://www .ircam.fr/equipes/temps-\nreel/sui vi/bibliography .html IRCAM, 2003\n[11] Soulez F.,Rodet X.,andSchw arzD.Impro v-\ningPolyphonic andPoly-Instrumental Music\ntoScore Alignment, Proc.Int.Symp. Music\nInfo. Retrie val,2003 Baltimore MD, 2003.\n[12] Vercoe B.The Synthetic Performer inthe\nConte xtofLivePerformance, Proceedings\noftheInternational Computer Music Confer -\nence,1984 ICMA, Paris, France, 1984."
    },
    {
        "title": "Demonstration of &apos;Music Plus One&apos;--- a System for Orchestral Musical Accompanimen.",
        "author": [
            "Christopher Raphael"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417899",
        "url": "https://doi.org/10.5281/zenodo.1417899",
        "ee": "https://zenodo.org/records/1417899/files/Raphael04a.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1417899,
        "dblp_key": "conf/ismir/Raphael04a",
        "content": "DEMONSTRA TION OFMUSIC PLUS ONE ASYSTEM FOR\nORCHESTRAL MUSICAL ACCOMP ANIMENT\nChristopher Raphael\nIndiana University ,Bloomington\nSchool ofInformatics\ncraphael@indiana.edu\nKeywords: Accompaniment Systems, Graphical Mod-\nels,Score Following.\nPastworkonmusical accompaniment systems [1],[2],\n[5],[3],[4],[6],[7],hasfocused mostly ongenerating\nMIDI orother sparsely-parameterized accompaniments for\nlivemusicians. MIDI isisparticularly well-suited topi-\nanoandpercussion music since, inthese cases, theMIDI\nrepresentation captures much oftheinterpretati vequality\noftheperformance. Most other acoustic instruments are\nconsiderably lessdeterministic andtheir MIDI equivalents\narecurrently much lessconvincing. Wepresent here asys-\ntemthatgenerates acomplete orchestr alaccompaniment\nthatfollows aliveplayer andlearns andassimilates the\nplayer' sinterpretation through aseries ofrehearsals. Un-\nlikeourprevious efforts, thissystem creates theaccompa-\nniment bysynthesizing, inrealtime, anorchestral accom-\npaniment using anactual audio recording.\nOursystem contains three separate modules called Lis-\nten,Predict andSynthesize which perform tasks anal-\nogous tothehuman' shearing ofthesoloist, anticipating\nthefuture trajectory ,andactual playing oftheaccompani-\nment.\nListen isbased onahidden Mark ovmodel thattracks\nthesoloist' sprogress through themusical score. Each\nnote inthescore ismodeled asaseries ofstates, while\nthenote models arechained together inleft-to-right fash-\niontoform thehidden process. Listen canbeused off-\nline,toestimate theonset times ofeach note inthescore\ngiventheentire acoustic data \u0002le,oron-line, bydeliver-\ningreal-time onset time estimates. Intheon-line version\nthese estimates aredelivered with aslittle latenc yasthe\nlocal ambiguity oftheacoustic data willallow.\nPredict models ourprior andlearned notions ofmusical\ninterpretation through acollection ofhundreds ofGaus-\nsian random variables whose dependenc ystructure isex-\npressed byabelief netw ork.Theobserv ablevariables cor-\nrespond tothenote onset times estimated byListen aswell\nastheknownaccompaniment onset times. These variables\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forpro\u0002t orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation onthe\u0002rstpage.\nc\n\u00002004 Universitat Pompeu Fabra.areconnected through alayer ofhidden variables corre-\nsponding tounobserv able quantities such aslocal tempo\nandrhythmic stress. During arehearsal phase welearn\nthemusical interpretations ofsoloist andaccompaniment\nbyestimating parameters forthedistrib utions ofthehid-\ndenvariables using theEMalgorithm. During liveper-\nformance thismodule anticipates thefuture musical evo-\nlution bycomputing themost likelytime ofthenextun-\nplayed accompaniment note givenallcurrently observ ed\nvariables. Thus ourfuture predictions areconstantly be-\ningreassessed asnewinformation isobserv ed.These pre-\ndictions form thebasis onwhich theaccompaniment is\nsynthesized.\nThe Synthesize module begins with adigital record-\ningoftheorchestral accompaniment playing without the\nsoloist. Inourexperiments wehaveused Music Minus\nOne recordings which arecreated forthepurpose ofpro-\nviding canned accompaniment foraliveplayer .Ourplay-\nback ofthisrecording will bewarped inatime-v ariable\nwayconsistent with thereal-time analysis oftheliveplayer' s\nacoustic signal, asestimated through Listen, andthelearned\nmusical interpretation, asrepresented inPredict. This is\naccomplished through aphase vocoder .Thephase vocoder\nsynthesizes atime-v arying trajectory through theorches-\ntralperformance bysimulating thelocal frequenc ycon-\ntent, computed through ashort-time Fourier transform,\nwhile preserving thephase continuity from frame toframe\nforeach frequenc y.\nWewillprovide alivedemonstration ofthissystem.\n1.REFERENCES\n[1]Dannenber gR.AnOn-Line Algorithm for\nReal-T ime Accompaniment, Proceedings of\ntheInternational Computer Music Confer -\nence,1984 ICMA, 1984.\n[2]Dannenber gR.,.Mukaino H.NewTech-\nniques forEnhanced Quality ofComputer\nAccompaniment, Proceedings oftheInter -\nnational Computer Music Confer ence,1988\nICMA 1988.\n[3]Vercoe B.,Puck etteM.Synthetic Rehearsal:\nTraining theSynthetic Performer, Proceed-ings oftheInternational Computer Music\nConfer ence,1985 ICMA 1985.\n[4]Baird B.,Blevins D.,Zahler N.Arti\u0002cial In-\ntelligence andMusic: Implementing anInter -\nactiveComputer Performer, Computer Music\nJournal vol.17,no.2,1993.\n[5]Grubb L,and Dannenber gR.AStochastic\nMethod ofTracking aVocal Performer, Pro-\nceedings oftheInternational Computer Music\nConfer ence,1997 ICMA, 1997.\n[6]Raphael C.AProbabilistic Expert System for\nAutomatic Musical Accompaniment, Jour-\nnalofComputational andGraphical Statistics ,\nvol.10no.3,487512.\n[7]Raphael C.(2002), ABayesian Netw orkfor\nReal-T ime Musical Accompaniment, Neu-\nralInformation Processing Systems (NIPS) 14\n2001."
    },
    {
        "title": "Audio Issues In MIR Evaluation.",
        "author": [
            "Josh Reiss",
            "Mark B. Sandler"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415840",
        "url": "https://doi.org/10.5281/zenodo.1415840",
        "ee": "https://zenodo.org/records/1415840/files/ReissS04.pdf",
        "abstract": "Several projects are underway to create music testbeds to suit the needs of the music analysis and music information retrieval (MIR) communities. There are also plans to unify testbeds into a distributed grid. Thus the issue of audio file formats has come to the forefront. The creators of a music library or MIR testbed are confronted with many questions pertaining to file formats, their quality, metadata, and copyright issues. We discuss the various formats, their advantages and disadvantages, and give a set of guidelines and recommendations. This document is a positional paper. It is intended to foster discussion and not as a definitive statement. Nevertheless, it is hoped that the proposals put forth here may serve as a guideline to use in construction of an MIR evaluation testbed.",
        "zenodo_id": 1415840,
        "dblp_key": "conf/ismir/ReissS04",
        "keywords": [
            "music testbeds",
            "music analysis",
            "music information retrieval",
            "audio file formats",
            "distributed grid",
            "metadata",
            "copyright issues",
            "music library",
            "MIR evaluation testbed",
            "guidelines and recommendations"
        ],
        "content": "AUDIO ISSUES IN MIR EVALUATION  \nJosh Reiss  Mark Sandler  \nDept. of Electronic Engineering  \nQueen Mary, University of London  \nMile End Road, London E14NS  \nUnited Kingdom  \nABSTRACT  \nSeveral projects are underway to create music testbeds to suit \nthe needs of the music analysis and music information retrieval \n(MIR) communities.  There  are also plans to unify testbeds into \na distributed grid. Thus the issue of audio file formats has come \nto the forefront. The creators of a music library or MIR testbed \nare confronted with many questions  pertaining to file formats, \ntheir quality, metadata, and copyright issues. We discuss the \nvarious formats, their advantages and disadvantages, and give a \nset of guidelines and recommendations. This document is a \npositional paper. It is intended to foster discussion and not as a \ndefinitive statement. Nevertheless, it is hoped that the \nproposals put forth here may serve as a guideline to use in \nconstruction of an MIR evaluation testbed.  \n1. OVER VIEW OF AUDIO FORMAT S \nIn order to describe  the a udio formats available, we distinguish \nbetween  raw audio formats, compressed formats, and \nmultimedia interchange wrappers. This classification is \ngeneralized and there are many exceptions. For instance, WAV \nformat supports compression, and AES31 describes file \nformats, compression schemes and wrappers. However, the \nfollowing groups formats based on their complexity, usage and \npurpose, and thus serves as a good guide to the options \navailable for audio formats used in a testbed.  \n1.1. Raw Audio Formats  \nThe most co mmon audio formats for end users are based on \nsimple, open standards that have been designed and promoted \nby companies for certain platforms. These include Apple’s \nAIFF format for the Mac, Sun’s au format for UNIX, and the \nWAV format for Windows, developed  by Microsoft and IBM. \nDespite th is legacy , these formats can be played on almost any \ncomputer using many audio applications, and contain no \nfeatures specific to their original intended platforms.  They are \nintended for storing uncompressed, PCM -encoded, raw audio in \na single binary file. They support a variety of bit rates, sample \nrates and channels, and contain a header containing such \ninformation. Of these formats, WAV and AIFF are by far the \nmost common. Almost all audio workstations support both.  \n1.2. Broadcast WAV  \nBroadcast WAV  was introduced to allow file exchange between \ndigital audio workstations (DAWs) used  in radio and television \nproduction [1] and  is now  the standard for file storage and \nexchange in the audio production  industry. A lmost all master \nrecordings, including those from small stud ios, live recordings \nand remasterings are created using Broadcast WAV s. Even workstations using proprietary software allow import and \nexport in the Broadcast WAV format.  \nAll WAV file players should recognize and play Broadcast \nWAV. The Broadcast WAV forma t is similar to a WAV file \nexcept it contains an additional header with information about \nthe originator, a time stamp and sound sequence description \nmetadata . The basic audio format is 16 -bit linear PCM sampled \nat 48kHz, but additional sample rates and bi t depths may also \nbe used, and MPEG -encoded audio is supported.  Broadcast \nWAV files are often stored as multiple mono files. A multi -\ntrack recording may thus contain a large number of high quality \nfiles and an edit decision list is needed to describe how t hey are \ncombined in the final mix.  \n1.3. Compressed Audio Formats  \nThe choices of compressed audio formats are almost endless. \nThe problem has arisen since many standards bodies and many \ncompanies have released different compressed formats, and \nthese have all fou nd niches where they have become popular \nand entrenched. However, for the purposes of a testbed, only \nthe most relevant ones will be considered. Here, relevance may \nbe defined in terms of quality, popularity and ease -of-use. \n1.3.1.  Lossless Compression Schemes  \nFew lossless compression schemes have seen widespread use \nwith audio. This is because the compression achieved by \nlossless means is usually insufficient to warrant the added \ncomplexity. Also, l ossless schemes do not seem appropriate for \nuse in a testbed. Alt hough they provide no loss in quality, \nmany players would not support them, they provide additional \ncomputational cost in encoding and decoding, and are not \ntypically used for many of the audio formats under \nconsideration. But the overriding reason why the y are not \nnecessary is that if it can be assumed that the testbed has  ample \nstorage space, then the primary purpose for lossless \ncompression becomes  irrelevant. Conversely, if storage capacity \nis significantly limited, then only lossy compression provide s \nenough reduction in file size to warrant its use. \n1.3.2.  Lossy Compression Schemes  \nThe issues concerning lossy compression schemes for encoding \nof audio files in a testbed are far more pertinent. Lossy schemes \nhave widespread use, and so its required that many m usic \nanalysis and processing algorithms are robust against their use. \nConversely, they degrade audio quality  and ensure that the \naudio becomes further removed from the groundtruth. A full \ndiscussion of the pros and cons of the use of lossy compression \nin conjunction with audio is presented  in Section  2.1, along with \nrecommendations concerning their use. In this section, we \ndiscuss the options available for lossy compression.  \nDifferent lossy compression schemes, or  codecs, have seen \nacceptance for use in different settings. Audio encoded for \nnetwork transmission, for use on DVDs and in theatres, for use \nwith telephony and for digital broadcasting, have all seen the Permission to make digital or hard copies of all or part of this \nwork for  personal or classroom use is granted without fee \nprovided that copies  are not made or distributed for profit or \ncommercial advantage and that  copies bear this no tice and the \nfull citation on the first page.  \n© 2004 Universitat Pompeu Fabra.    \n \nacceptance of various compression schemes. These sc hemes \ninclude both proprietary methods proposed by companies and \nopen methods from international standards bodies and from \nindustrial consortiums. Although open compression methods \nare preferred in research settings, many audio collections that \nmight be do nated to a testbed could be encoded using \nproprietary methods, and these methods are popular enough to \nbe considered as tests of robustness.  \nCodecs based on perceptual models gained widespread \nacceptance with the introduction of mp3. Mp3 compression , \nuses information about human hearing to make decisions about \nwhich parts of the sound data are extraneous. These extraneous \nfrequency components are coarsely quantised  with minimal  \neffect on the sound perception. Its open standard and large \ncompression rates has lead to mp3 audio becoming the preferred \nformat for audio distribution online.  \nMany perceptual coders have better performance than mp3 \nand have achieved widespread acceptance. Microsoft’s WMA , \nfor instance,  encodes audio at equivalent quality to mp3 wit h \nonly half the size. It has widespread support due to Microsoft \nWindows’ large user base, but has many Windows -specific \nfeatures. The Advanced Audio Coding (AAC) developed  is a \nhigh-quality alternative . Coding efficiency is similar to that of \nWMA. AAC is supported by a growing number of \nmanufacturers as the logical successor to mp3.  \nDespite mp3’s popularity for audio transmission over the \ninternet, large music libraries, especially from commercial \nenterprises, will often use alternative compression schemes . \nThe choice of compression method, if used, is discussed in a \nlater section. For now, we note that, if the MIR community is \nsoliciting for donations of copyrighted music, it is not sufficient \nto simply demand that all audio be encoded in mp3 format.  \n1.4. Excha nge Formats and Wrappers  \nFor audio editing and mixing, metadata primarily involves edit \ndecision lists. For information retrieval, metadata necessary for \ntext-based searches by composer, performer, year, etc… must \nbe inc luded. Audio transmission over the i nternet requires \ncoders for streaming as well as custom metadata attributes for \nadvanced applications. And copyright information is required in \norder to guarantee protections for creators and license holders.  \nThus a variety of formats have been proposed fo r audio file  \nexchange . These formats typically use a wrapper , which \ncontains the metadata, along with audio and other multimedia \ndata in any of a variety of formats.  \nOMFI (Open Media Framework Interchange ) provided a \npartial solution for interchange  of prod uction -level audio. \nOMFI allows an entire  audio  project, including individual tracks \nand editing , mixing and processing instructions, to be \ninterchanged between different DAW s. Suitability and \ndependability  issues  lead to the introduction of the  Advanced \nAuthoring Format (AAF). AAF incorporates more metadata \ncapabilities, Microsoft’s open container format Structured \nStorage, management of pluggable effects and codecs, and has \nbroad industry support. AAF is extensibl e, royalty -free, and  \nsupp orts digital rights management and interactive content.  \nThe Material eXchange Format (MXF) is designed to \nfacilitate transfer of finished content  between broadcast systems. MXF is derived from the AAF data model and the \ntwo formats  are complementary. Both formats can stand on \ntheir own. A broadcast system may use only MXF and a \npostproduction house, just AAF, but a broadcaster with a post \nfacility may well use both  \nAAF, MXF and OMFI are intended for exchange of audio, \nvideo and othe r media, and were not designed specifically for \naudio and music . The only open exchange format designed by \nand for the audio community is AES31.  \nThe Audio Engineering Society Standards Committee \nWorking Group on Audio -File Transfer and Exchange was \nestabli shed in response to demand for a project interchange \nformat that may be used as a simple alternative to OMF and \nproprietary formats. The result is AES31 , which  provides \ntechnical specifications for transferal and storage of  digital audio \nmedia, metadata , and projects . \n§ AES31 -1 is concerned with file transport  from one system to \nanother by removable media or high -speed network. AES31 -1 \nspecifies a transport compatible with FAT32 structures.  \n§ AES31 -2 focuses on how the data should be arranged on the \nremovable m edia or packaged for network transfer. It \nspecifically recommends the use of Broadcast Wave files for \nstorage of individual audio tracks.  \n§ AES31 -3 describes a project structure  using an Audio \nDecision List, or ADL. The ADL is based  on Edit Decision \nLists, but with sample -accurate precision, parameters for \nmultiple channels, crossfades, level automation and so on.  \n§ AES31 -4 is an extensible object model capable of describing a \nwide range of parameters for advanced applications.  \nAES31 -1 through 3 have been rat ified as standards [2]. AES31 -\n4 is in the development stages, and there is currently liasing \nbetween the AES standards body and the AAF  consortium to \nharmonise the two. Thus, it appears that there will soon be an \ninternationally recognised standard for the interchange of \ncomplete audio projects, based on existing standards.  \n1.4.1.  Metadata for Information Retrieval  \nThe field of wrappers and exch ange formats for audio becomes \nfar more complex when one includes the multitude of standards \nproposed within the Information Retrieval and Library Science \ncommunities [3]. These metadata formats describe how \ndocuments should be linked, metadat a necessary for text -based \nsearches, and copyright information. These formats, and the \nissue s regarding their use, are  virtually unaffected by the choice \nof audio format. They are mentioned here so that one may \nunderstand the full range of options availabl e and whether their  \nuse in any way conflicts affects the choice of audio format.  \nTo resolve audio format  issues , it is not necessary to know \nall the metadata standards and how they operate. There exists  \nappropriate schemes to associate disparate audio files, to \nclassify and retrieve audio in digital libraries, and to add \nadditional metadata suitable for audio present in a music digital \nlibrary (MDL) or testbed. Furthermore, these schemes are \ndistinct from the wrappers such as AES31 and AAF, which are \nmore appropriate for stor ing and exchange of audio projects. \nWrappers such as AES31, or even Windows Media Format 9, \nmay be necessary if one wishes to incorporate audio as close as \npossible to the groundtruth, e.g., the recording masters.    \n \n2. INCLUSION  OF AUDIO  IN AN MIR TESTBED  \nIn th is section, we list a set of guidelines that should be \nfollowed in the choice of file formats for the audio in a MIR  \ntestbed. These suggestions represent the ideals for choice of \nformat, use of compression, use of standards, and file access \nand editing. Actual choices made in the creation of a testbed are \nlimited by the files to which the creators receive access. Thus, \nthese guidelines also serve as a list of requests for files provided \nfor use in the testbed by copyright holders.  \n2.1. Quality Guidelines  \nAudio f iles should be presented in the highest quality  format \npossible, ideally the master recordings. If a compressed format \nis used, it should be used in tandem with the original format.  \nAlthough this may seem  obvious, people have argued against \nthis for severa l reasons.  Uncompressed high quality audio \noccupies a tremendous amount of space, whereas compressed \naudio can be less than one tenth the size yet still of acceptable \nquality for many purposes. Low quality formats are very \npopular, and hence retrieval shou ld focus on those formats. \nRetrieval and analysis methods should be robust against \ncompression schemes and thus compressed audio should be \nused in order to guarantee robustness . However, as shall be \nexplained, use of compressed or low -quality formats sever ely \nlimits the quality of retrieval as well as the scope of analysis \ntasks that are possible. Furthermore, the benefits of compressed \naudio, with the exception of small file size, can be achieved \nmore effectively and simpl y if the original audio is stored.  \n2.1.1.  Compression as Error  \nThe most accurate retrieval is achieved using the highest quality \naudio. Any lossy compression involves  a distortion of the \nsignal. The groundtruth, which represents the actual original \nsignal(s) without errors introduced by acquisiti on, processing or \ncompression, yields the most information which can be used to \naid retrieval. Furthermore, compression often introduces  \nadditional signal processing which  may introduce artifacts and \nhinder the retrieval of relevant documents.  \n2.1.1.1  Preprocessin g \nOften overlooked is the preprocessing that occurs before \ncompression. Before a signal is compressed, there is usually a \npreparation stage whereby it is companded (to modify the \nvolume range), equalized (to normalise the strength over the \nfrequency range)  and/or boosted (increased strength at certain \nfrequencies). Each processing method modifies the signal and \nmakes it further removed from the original. In addition it is \noften cleaned, which may remove important musical \ncomponents as well as background noi se. These processing \nsteps all combine to make MIR  more difficult. And since it is \nusually not known exactly what processing was performed, it \nbecomes difficult to differentiate between retrieval failure (low \nprecision or low recall) due to problems with t he retrieval \nmethod or due to excessive processing on the audio files.  \n2.1.1.2  Repeated Encoding  \nAudio compression can happen at various stages between \ninitial performance and final playback, or deploy ment in an \naudio testbed. Processing and transmission operations, change \nof formats and/or bitrates, all combine to create multiple cycles of decoding, processing, and re -encoding of the audio content. \nThe quantization noise from each cy cle accumulates and leads \nto a progressive drop in audio quality. The resultant distortion \nquickly becomes audible and becomes more and more \nproblematic with each subsequent generation.  Figure  1 \ndemonstrates the effec t of multiple encodings to convert a 2 \nchannel, 48khz, 16 bit, 57 second sample of music to 128kbps \nmp3 format. The Objective Difference Grade [4] is a perceptual \naudio quality measure , which rates the difference between two \nsignals on a scale from 0 (imperceptible) to -4 (very annoying).  \n-4-3-2-10Objective Difference Grade6 5 4 3 2 1 0\nNumber of Encodings \nFigure 1. Effect of multiple encodings on audio quality. The \nObjective Difference Grade measures perceived difference in \nquality between a reference and test signal . \n2.1.2.  Introduction of Artifacts  \nA dangerous aspect of compression is that it can introduce \nartifacts. Not only are the inaudible parts of the signal affected, \nand the audible parts carry less information, but the audible \nsignal may become  drastically modified. In effect, this \nguarantees that even a robust similarity measure may fail if \nartifacts h ave been introduced.  And furthermore, the inaudible \nartifact s may still affect the reliability of analysis algorithms.  \n2.1.2.1  Pre-echo  \nWhen a transient occurs, a perceptual model will  allocate only a \nfew bits to each of the quantizers because a transient signal will \nspread out over many subbands [5]. When the compressed data \nis decoded, the quantizati on noise, which was supposed to be \nfully masked, may now spread over the entire block. Therefore, \nthis noise will also precede the time domain transient. The \nquantization noise announces the transient in advance  (see \nFigure 2), pro ducing a  potentially  audible artifact. It can be \nnoticed prior to the signal attack as a pre -echo.  \n2.1.2.2  Aliasing (low frequenc y sampling ) \nAliasing is well -understood but often overlooked in the coding \nprocess. If too low a sampling rate is used, the signal can \nimpersonate another signal at lower frequency.  Even the use of \nsome anti -aliasing filters may not prevent aliasing, since  a poor \ndesign may permit  some high frequency components. And since \nmany signals are sampled at very close to Nyquist, design of \nsuitable anti -aliasing filters is difficult.  \nAliasing introduces additional problems when used in \nconjunction with compression . Aliasing results in the \nquantization noise introduced into a specific subband creating \nadditional noise at different frequency locations. Thus, \nfrequency components that have negligible effect on audio \nquality become non -negligible when they are aliased down into \nmore audible frequencies. Although there are many ways that \naliasing problems can be avoided, it is not guaranteed that all \npopular audio coders will have implemented these methods.    \n \n-0.50.00.5\n2000 1500 1000 500 0\n-0.50.00.5\n2000 1500 1000 500 0  \nFigure 2. Original signal(top) and pre -echoe s(bottom) from \ncastanets with block processing of 2048 samples . \n2.1.2.3  Birdies (masking)  \nThe most used perceptual measure in audio codi ng is the \nmasking threshold. For low bit rates, s light variations of the \nmasked threshold from frame to frame lead s to very different bit \nassignments. As a result, groups of spectral coefficients may \nappear and disappear , resulting in the appearance of spu rious \naudio objects . These artifacts, known as birdies, have been \nreported both for the tuning of audio codecs and for objective \nperceptual assessment methods.  \n2.1.2.4  Loss of Stereo Image  \nIntensity stereo coding exploits the fact that the perception of \nhigh freq uency sound relies mostly on the envelope rather than \non the waveform itself. Thus, the signal envelope is encoded \nrather than the  waveform. This is done by transmitting a shared \nset of spectral coefficients (carrier signal) instead of separate \nsets for ea ch channel . For transient signals with dissimilar \nenvelopes in different channels, the original distribution of the \nenvelope onsets between the coded channels cannot be \nrecovered. In a stereophonic recording of an applauding \naudience, the individual envelo pes will be very different in the \nright and left channel due to the distinct clapping events \nhappening at different times in both channels.  \nAfter the intensity stereo encoding / decoding process, the \nfine time structure of the signals is mostly the same in  both \nchannels. Perceptually important signal onsets propagate to the \nopposite channel. The spatial impression tends to narrow and \nthe perceived stereo image collapses into the center position. \nFor signals with uncorrelated transient information in each \nchannel, like an applauding audience, the signal may seem to \ndisappear from different locations at different times.  \n2.1.3.  Quality Requirements for Audio Analysis  \nCompression hinder s the ability to analyse audio files correctly. \nEach lossy compression method uses a n algorithm that analyses \nthe signal  and determines which components can be removed \nwithout serious degradation of quality. For instance, WMA is \nbased on the Malvar wavelet . If this wavelet is used as a \nsimilarity measure, then the results will be biased t owards \nranking WMA compressed files as highly similar.  Similarly, \nprocessing methods which encode data in masked frequencies, \nor similarity measures which use high frequency comparison, \nmay fail on mp3s because masking and high frequency removal \nare integr al to mp3 encoding. This ha s disastrous consequences \nfor instrument templates, since mp3 compressed audio is not  an \naccurate representation of the frequency content (including \nharmonics) when a note is played on an instrument.  Production of inaudible artefacts  due to compression is also \nproblematic  since they may  still hinder analysis.  Both p re-echo \nand sampling rate reduction , for instance, increase the \nuncertainty of the time at which an event occurs. Thus accurate \nnote onset  measurement becomes more dif ficult.  \n2.1.4.  Why use the master recordings?  \nThe previous section provides a variety of reasons why a \ncompressed or low -quality audio signal should not be used as \nthe preferred audio format for an MIR  testbed. Although this \njustifies the use of high qual ity audio, it does not make the case \nfor the use of original master recordings. Their use is justified \ndue to their quality, richness, metadata and the fact that they \nprovide capabiliti es for far more analysis, processing, and \ninvestigation of retrieval methods.  \n2.1.4.1  Highest quality  \nThe master recordings are the highest quality digital recording of \na performance that is available. As such, they represent the \nclosest to the groundtruth that ma y be achieved. They provide \ninformation far in excess of CD quality recording s. For instance , \nit is not uncommon for master recordings to incorporate 16 24 -\nbit tracks, sampled at 96kHz or higher. Since it has already been \nestablished that processing and co mpression introduce errors \nwhich may be detrimental to the evaluation of MIR  methods, \nthe use of master recordings guarantees the least chance of these \nproblems occurring. Furthermore, high quality allows for more \naccurate transcriptions, measurements of h armonic contours, \ninstrument recognition, and so on. All of which are tools which \nmay be applied to an MIR system.  \n2.1.4.2  Tests Robustness  \nAn argument given in favour  of low -quality or compressed \naudio is that good MIR systems should be robust to the \ndistortions  produced by compression. However, limiting the \naudio to any one compressed format restricts the ability to test \nfor an algorithm’s robustness against other formats. The best \nway to test for robustness is to commence with the highest \nquality format, and th en see if the same audio is retrieved when \nmp3 encoded, or at low sample rate, or using any format which \nintroduces error. If one starts with the highest quality audio, \none can then find the point at which retrieval is affected. \nRobustness cannot properly be determined if one does not have \naccess to the initial recordings.  \n2.1.4.3  Rich Data  \nEach track in a master recording is typically a high quality \nrecording of a single instrument  from  a single session. Access to \nsuch material provide s researchers with rich data to analyse, and \nretrieval experts with more methods to perform retrieval.  \nMany analysis and processing routines performed on audio \ndata are, in effect, reverse -engineering of master r ecordings. \nSource separation attempts to separate the individual voices. \nInstrument recognition attempts to identify the various \ninstruments used in a recording [6]. Onset detection and note \nrecognition techniques are plagued by complexities due to the \npolyphonic, multi -voice nature of most recordings [7]. Yet on \nmost masters, the instruments are separated on different tracks, \nthe voices are on different tracks, and each track has few   \n \npolyphonies. Transcription becomes  easier since it need only be \napplied to one voice under known conditions.  \nFurthermore, effects are introduced in t he mixing process. \nFading, time stretching and distortion, for instance, may be \nadded in the mix but the original master tracks would remain \nuntouched. Using the master tracks in the testbed would allow \none to retrieve audio based on a more meaningful musi cal \nsimilarity measure, since most of the audio production -based \n(dis)similarity would not be present.  \n2.1.4.4  Metadata  \nFor studio recordings produced on DAW s, which includes the \nvast majority of commercial recordings, each track is labelled \nwith meaningful metad ata, such as timestam p, title, performer \nand instrument . Exchange formats such as AAF and AES31, \nalso include editing and mixing information so that the final \nproduction mix can be recreated from the master tracks.  \nThis, together with the innate richness o f the data, provides \npowerful tools which can be exploited by MIR  systems. One \ncan, for instance, search for all of a performer’s guitar tracks, or \nfor a specific percussive style that may occur on any recording, \nor all uses of a popular sample. Not to use  master recordings \nwould be to throw away, meaningful metadata for which it \nwould be impossible to recreate.  \n2.2. Usability Guidelines  \nResearchers will not use a system which requires them to adapt \nto a new and possibly unproductive environment. Hence a  \ntestbed  should not require the MIR community to adapt to its \npreferred  format, platform or development environment. It \nshould support all popular variations and provide a mechanism \nwhereby users of unusual variants may still access the testbed. \nFurthermore, requi ring many researchers to each support one \nspecific option is duplication of effort.  It is more efficient to \nbuild support for many alternatives directly into the testbed.  \nIn order to achieve this, open standards are required \nwherever possible. Proprietary  formats may not be supported \nby common audio players and streamers, or many development \nenvironments. These open standards are also necessary because \nthe testbed is there to help the community. If a proprietary \nstandard is used and an MIR system fails, it  may not be \npossible to tell why it failed. Without understanding an \nencoding technique, one cannot determine why that encoder \nmay have caused an audio file not to be retrieved.  \nFinally, closed proprietary formats cause lock-in. That is, \nusers will require  the associated  encoders and decoders, as well \nas processing tools. These often all originat e from the same \ncompany. Productivity then necessitates a reliance on this \ncompany, and other alternatives are rejected because they \ncannot be used with the existin g audio files. Rather than building \na testbed for the wider MIR community, this will create a \nsoftware specific testbed of only limited, specialized use.  \n2.3. Complexity Guidelines  \nThe MIR community is incredibly diverse. It is comprised of \nmusicologists, engin eers and library scientists, to name just a \nfew. Thus testbed  users  are often highly specialized. They may \nhave limited knowledge of music theory, programming, \ninformation retrieval or signal processing. A format reliant on an \narcane metadata format will b e impractical for engineers, and audio formats using sophisticated psychoacoustic models will \nseem obtuse to information retrieval experts.  \nThe solution is to implement simple, transparent and well -\nunderstood formats wherever possible. Metadata should be \nencoded in text format so that it can be simply read without \nrequiring advanced programming skills. Audio encoding should \nnot require advanced processing or prior knowledge of acoustics \nor human sound perception. The format should be one that is \neither supp orted by most languages and development \nenvironments, or one where it is easy to construct decoders and \nencoders. Wherever possible, converters should be provided so \nthat the audio file can be played or analysed in all major \nformats. Such a simple scheme a llows for the entire MIR \ncommunity to benefit from the testbed with minimal  time spent \non acquiring irrelevant format -specific skills.  \n3. RECOMMENDATIONS  \nIn this document we have outlined the options available for the \naudio files used in an MIR  testbed and mu sic digital library. We \nhave also provided and justified a set of guidelines for the audio \nfiles, formats and wrappers used in the testbed construction. \nGiven these options and guidelines, it is possible to list a set of \nexplicit recommendations concerning  formats, converters, \nquality and copyright infringement prevention. These \nrecommendations come with the caveat that the primary factor \nin determining the nature of audio files in a testbed is restricted \nby what the copyright holders are willing to provide . \nNevertheless, the testbed creators and MIR/MDL infrastructure \nexperts should be able to effectively argue the case for the \npreferred audio files under the preferred conditions.  \nRecommendation 1:  A popular, simple, well -understood and \nuncompressed format should be used as the primary format for \nencoding audio files.  \nWe noted earlier that WAV and AIFF are the two most \npopular raw audio formats and are supported by almost all \nDAW s and audio players. However, almost all development \nenvironments offer encodin g and decoding of WAV files, \nwhereas AIFF support is not built -in to some development \nenvironments. In addition, standards have formally endorsed \nWAV. For these reasons, we recommend that uncompressed \nWAV files be the main audio format.  \nRecommendation 2:  Whenever possible, the master recordings \nshould be obtained and stored with any metadata and audio \nproduction information.  \nThe reasons for use of master recordings were outlined in \nSection  2.1.4 . They represent th e highest quality audio \navailable, and the closest approximation to the groundtruth. \nAlmost all digital masters are stored as Broadcast WAV. The \nAES31 standard, supported by DAW manufacturers and \nendorsed by the audio production and broadcasting \ncommunitie s, provides a  simple open standard for easily \ntransferable Broadcast WAV encoded audio files and associated \nmetadata. Together with Recommendation 1, this provides a \nringing endorsement for the use of Broadcast WAV format, raw \naudio master recordings as th e testbed essence . \nRecommendation 3: Testbed creators must guarantee that files \ncan be analysed using all popular development environments,   \n \nlistened to with all popular audio players, and on all major \noperating systems.  \nThis necessitates system testing by M DL designers, but \nshould not require effort on the part of MIR system \nresearchers. Popular development environments include the \nanalysis software MATLAB, programming languages Java and \nC/C++, and the scripting language Perl.  Relevant audio players \ninclude Quicktime, Windows Media Player, RealPlayer and \nWinAmp. Since some popular audio formats are not supported \nby all major media players, it may be necessary to provide \nconverters. Again, this should be implemented on the testbed \nside, not by individual MIR r esearchers. The operating systems \nthat should be supported are Mac, Windows and Linux/UNIX. \nSupport should extend to recent versions, not just the current \nversion, e.g., Mac OS 8.x and 9.x as well as OS10.  \nRecommendation 4:  The testbed should allow multipl e \nformats.  Although the first three recommendations suggest a \npreferred audio format and its support, they do not preclude \nthe use of multiple formats in the testbed. Multiple formats \nshould be used for the storage of audio because it allows one to \nskip th e audio conversion step where it would be used, because \naudio files may be provided in different formats, and because it \nprovides researchers with a rich and heterogeneous testbed that \nallows evaluation of diverse retrieval systems.  \nRecommendation 5:  MIR r esearchers must be allowed to \nlisten to the material in the testbed. Any artifact s or distortions \nintroduced to satisfy the demands of copyright holders should \nnot restrict the ability of researchers to analyse their MIR \nsystem and evaluate its performance  on the corpus.  \nThis recommendation  depends on the restrictions imposed \nby copyright holders and the reaction of the MIR community \nto those restrictions. In order to ensure that no high quality \naudio is leaked outside the research community, severe \nlimitat ions will most likely be placed on the ability to listen to \nthe files in the testbed. Nevertheless listening tests are an \nessential part of music -related research.  \nAt a minimum, researchers should be able to listen to a low \nquality popular format version of the audio with embedded \nartifact s. Options include streaming, providing audio in mono, \nin a highly compressed form, embedding artefacts  such as pings \nand drop -outs, thumbnailing and watermarking. Streaming \nseems reasonable although it is possible to rip an audio stream \nand redistribute it as a file. Artefacts  are irritating and detract \nfrom the ability to perform listening test s, as do all audio \nmodifications. Furthermore, they are unsuitable for \ndemonstrations. Thumbnailing is still frontier  research, a nd a \nthumbnail may not contain the most relevant audio material.  \nWatermarking is also problematic because i t entails \nemphasised responsibility a nd enhanced liability. A ny leaked \naudio can be tracked back to audio researchers , or at least  to the \ntestbed us er community. This implies that the maintainers of \nthe testbed can more easily be held liable since it can be shown \nthat leaked material originated from the testbed. Due to these \nissues, watermarking is discouraged.  \nIt is difficult to gage in advance how m uch of an imposition \nany of these limitations will place on research. However, all \nartifact s and distortions can affect the evaluation of relevance. \nFurthermore, artifact s are innately problematic in listening tests since they affect the way audio sounds. Therefore we \nrecommend that they be avoided wherever possible.  \n4. CONCLUSION  \nThe guidelines and recommendations put forth in this document \nare intended for the creation of the most powerful and \naccessible music digital library possible. We assume d that size \nconstraints on the testbed are minimal. This allows us to \nrecommend master recordings in favour of  highly compressed \nformat s. If there are severe size limitations  then all of the above \nrecommendations would need revision. The size of other data in \nthe testbed, such as symbolic music representations, would \nneed to be taken into account. However, it is a reasonable \nassumption that any large -scale testbed intended for use by the \ngreater MIR  community would have ample space for all data.  \nThe image and video retrieval communities have also been \ndealing with format , quality and copyright issues . Quality is not \nas strong an issue for both media, since most  video is of \ncomparably high quality, and high quality images are easily \nfound. The image community uses uncompressed  image formats \nthat are  easily interchangeable , and the video c ommunity has \nyet to settle on a standard, although MPEG -2 is common. But \nboth communities are plagued by copyright issues. They \ncurrently have projects underway to provide large testbeds of \nmaterial with few cop yright access issues.  \nFinally, the question of preferred audio format has been \ntackled in the related discipline of audio restoration and \npreservation. Almost universally, the members of this \ncommunity recommend storage of files in Broadcast WAV \nformat (AE S, EBU, Audio Restoration Services, Library of \nCongress’s  National Digital Library Program , …). Thus, \nadoption of such a format allow s the MIR research community \nto easily collaborate with this and other related disciplines.  \n5. REFERENCES  \n[1] \"Specification of the Broadcast Wave Format, EBU Tech. \nDoc. 3285,\" European Broadcast Union, July 1997  \n[2] \"AES standard for network and file transfer of audio,\" \nAudio Engineering Society 1999,2001   \n[3] Metadata applications and management, Internat ional \nYearbook of Library and Information Management, 2003 -\n2004. London: Facet Publishing, 2004.  \n[4] T. Thiede , et al. , \"PEAQ - The ITU Standard for Objective \nMeasurement of Perceived Audio Quality,\" J. Audio Eng. \nSoc., vol. 48, pp. 3 -29, 2000.  \n[5] M. Erne , \"Pre -Echo Distortion,\" in Audio Coding Artifacts \nEducational CD -Rom, J. Herre and J. D. Johnston, Eds.: \nAudio Engineering Society, 2001.  \n[6] P. Herrera , et al. , \"Towards Instrument Segmentation for \nMusic Content Description: A critical review of instrume nt \nclassification techniques,\" I SMIR , Plymouth, MA, 2000.  \n[7] J. Pickens , et al. , \"Polyphonic Score Retrieval,\" ISMIR , \nParis, France, 2002."
    },
    {
        "title": "Drum sound classification in polyphonic audio recordings using localized sound models.",
        "author": [
            "Vegard Sandvold",
            "Fabien Gouyon",
            "Perfecto Herrera"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415808",
        "url": "https://doi.org/10.5281/zenodo.1415808",
        "ee": "https://zenodo.org/records/1415808/files/SandvoldGH04.pdf",
        "abstract": "This paper deals with automatic percussion classification in polyphonic audio recordings, focusing on kick, snare and cymbal sounds. We present a feature-based sound modeling approach that combines general, prior knowl- edge about the sound characteristics of percussion instru- ment families (general models) with on-the-fly acquired knowledge of recording-specific sounds (localized mod- els). This way, high classification accuracy can be ob- tained with remarkably simple sound models. The accu- racy is on average around 20% higher than with general models alone.",
        "zenodo_id": 1415808,
        "dblp_key": "conf/ismir/SandvoldGH04",
        "keywords": [
            "automatic percussion classification",
            "polyphonic audio recordings",
            "kick",
            "snare",
            "cymbal sounds",
            "feature-based sound modeling",
            "general models",
            "localized models",
            "high classification accuracy",
            "simple sound models"
        ],
        "content": "PERCUSSION CLASSIFICA TION INPOL YPHONIC AUDIO\nRECORDINGS USING LOCALIZED SOUND MODELS\nVegardSandvold\nUniversity ofOslo\nOslo, Norw ayFabien Gouyon\nUniversitat Pompeu Fabra\nBarcelona, SpainPerfecto Herr era\nUniversitat Pompeu Fabra\nBarcelona, Spain\nABSTRA CT\nThis paper deals with automatic percussion classi\u0002cation\ninpolyphonic audio recordings, focusing onkick, snare\nandcymbal sounds. Wepresent afeature-based sound\nmodeling approach that combines general, prior knowl-\nedge about thesound characteristics ofpercussion instru-\nment families (general models) with on-the-\u0003y acquired\nknowledge ofrecording-speci\u0002c sounds (localized mod-\nels). This way,high classi\u0002cation accurac ycanbeob-\ntained with remarkably simple sound models. Theaccu-\nracyisonaverage around 20% higher than with general\nmodels alone.\n1.INTR ODUCTION\nThis paper deals with automatic symbolic transcription of\npercussion mixedinpolyphonic audio signals. That is,\ngivenamulti-timbral audio signal, thegoal istwofold: to\nautomatically classify itspercussion sounds andtoauto-\nmatically determine their positions onthetime axis.\nSnare drum sounds, forinstance, canshowlargevaria-\ntions intimbral characteristics. Inautomatic isolated sound\nclassi\u0002cation [8],thisistypically dealt with from ama-\nchine learning perspecti ve:asound model (roughly ,thresh-\nolds forspeci\u0002c relevantsignal features) isbuiltfrom a\nlarge,diverse collection oflabeled snare drum sounds.\nThis model issubsequently used toassign labels toun-\nknowninstances.\nHowever,inourframe work, thetemporal boundaries\nofthesounds toclassify areunkno wn.Alistofpotential\npercussion sound occurrences must be\u0002rstextracted from\ntheaudio recording. Different rationales havebeen pro-\nposed tosolvethisissue. Forinstance, onemay assume\nthatpercussion sounds arebound tooccur in\u0002xed-length\nregions around speci\u0002c time-points, either sharp onsets\n[3,4]orbeats atthetatum level[5,11,1].\nDealing with polyphonic recordings raises anadditional\nissue: percussion sounds aresuperposed with, andsur-\nrounded by,ahigh levelofnoise, i.e.other instruments\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forpro\u0002t orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation onthe\u0002rstpage.\nc\r2004 Universitat Pompeu Fabra.ase.g.voice, piano, guitars etc.Evenworse, simultaneous\noccurrences ofseveralclasses ofpercussion instruments\n(e.g. kick +hihat orsnare +hihat) may beencountered.\nTodeal with thisissue, existing literature advocates di-\nverse research directions. Some advocate source separa-\ntion techniques asIndependent Subspace Analysis [1,2]\norsignal models asSinusoidal +Residual (assuming that\ndrums areintheresidual component) [7,11].Noise re-\nduction techniques, asRAST A[10],arealso thinkable.\nAnother option istobuildsound models from alargecol-\nlection oflabeled noisy percussion instrument sounds\nextracted from polyphonic audio recordings [9].Themain\nassumption inthismethod isthat, inaverage onthetrain-\ningdatabase, thenoise showsconsiderably higher vari-\nability than thedrum sounds.\nTheapproach by[4,13]also assumes thatpercussion\nsound characteristics showlessvariability than surround-\ningnoise; however,thisassumption isnotmade atthe\nscope ofatraining database, butrather atthesmaller scope\nofindividual audio recordings .Theydesign verysimple,\ngeneral sound templates foreach percussi vesound (ac-\ntual waveform templates [4,13],atthedifference with\nthesound models previously mentioned) and\u0002ndthesev-\neralsub-se gments intheaudio recording athand thatbest\nmatch those templates (bymeans ofacorrelation func-\ntion). This process isiterated several times andsound\ntemplates aregradually re\u0002ned bytime-domain averaging\nofthebest-matching segment intheveryaudio recording\nathand.\nOur approach istocombine general, prior knowledge\nabout thesound characteristics ofpercussion instrument\nfamilies with on-the-\u0003y acquired knowledge ofrecording-\nspeci\u0002c sounds. Instead ofpursuing universally validsound\nmodels andfeatures [8,9],unique, localized sound mod-\nelsarebuiltforeveryrecording using features thatarelo-\ncally noise-independent andgivegood class separation.\nInstead ofactually synthesizing newwaveform templates\nfrom theaudio signal [4,13],wetailor (inagradual fash-\nion)feature spaces tothepercussion sounds ofeach record-\ning.\nTherefore, anonset detector yieldsNpotential drum\nsound occurrences thataresubsequently processed asfol-\nlows:\n1.Classi\u0002cation using general drum sound models\n2.Ranking andselection oftheM<Nmost reliablyclassi\u0002ed instances\n3.Feature selection anddesign oflocalized models us-\ningthoseMinstances\n4.Classi\u0002cation oftheNsegments using thelocalized\nmodels\nAsitturns out, ourattempts attheautomatic rank-\ningandselection hasnotyetprovided satisf actory results.\nTherefore, wecorrected manually theoutput ofsteps 1\nand2andprovided only correct percussion sound instances\ntothefeature selection step. Consequently ,inthispa-\nper,wepresent amore focused evaluation ofthelocal-\nized sound model design, aswell asaproper comparison\nbetween general sound model andlocalized sound model\nperformances. Using thecorrected instance subsets, we\ninvestigate howtheperformance ofthelocalized models\nevolveasincreasingly smaller proportions ofthedata is\nused forfeature selection andtraining.\nAutomatic techniques forinstance ranking arecurrently\nbeing evaluated. Together with theevaluation ofthefully\nautomatic system theyaretheobject ofaforthcoming pa-\nper.\n2.METHOD\n2.1. Data andfeatur es\nThetraining data setforgeneral model building consists\nof1136 instances (100 mslong): 1061 onset regions taken\nfrom 25CD-quality polyphonic audio recordings and75\nisolated drum samples. These were then manually anno-\ntated, assigning category labels forkick,snare,cymbal ,\nkick+cymbal ,snare+cymbal andnot-per cussion .Other\npercussion instruments liketoms andLatin percussions\nwere leftout.Cymbals denote hi-hats, rides andcrashes.\nAnnotated testdataconsists ofseventeen 20-second ex-\ncerpts takenfrom 17different CD-quality audio record-\nings (independent from thetraining data set). The total\nnumber ofmanually annotated onsets inalltheexcerpts is\n1419, average of83perexcerpt.\nTraining andtestdata arecharacterized by115spec-\ntralfeatures (averages andvariances offrame values) and\ntemporal features (computed onthewhole regions), see\n[9]and[12]fordetails.\nTheexperiments described intheremainder ofthispa-\nperwere conducted with Weka.1\n2.2. Classi\u0002cation with general models\nInorder todesign general drum sound models, we\u0002rst\npropose toreduce thedimensionality ofthefeature space\nbyapplying aCorr elation-based Featur eSelection (CFS)\nalgorithm (Section 2.4)onthetraining data. From thetotal\nof115, anaverage of24.67 features areselected foreach\nmodel.\nThis data isthen used toinduce acollection ofC4.5\ndecision trees using theAdaBoost meta-learning scheme.\n1http://www .cs.waikato.ac.nz/ml /weka/Bagging orboosting approaches hasturned outtoyield\nbetter results when compared toother more traditional\nmachine learning algorithms [9].\n2.3. Instance ranking andselection\nThe instances classi\u0002ed bythegeneral models must be\nparsed inorder toderivethemost likelycorrectly classi-\n\u0002edsubset. Severalrationales arepossible. Forinstance,\nwecanuseinstance probability estimates assigned bysome\nmachine learning algorithms asindicators ofcorrect clas-\nsi\u0002cation likelihood. Another option istouseclustering\ntechniques. Instances ofthesame percussion instrument,\nwhich wearelooking for,would form themost populated\nandcompact clusters while other drum sounds andnon-\npercussi veinstances would beoutliers.\nHowever,asmentioned above,wewent forasafe op-\ntion: manually parsing theoutput ofthegeneral classi\u0002ca-\ntionschemes. Using thecorrected output, weinvestigated\nhowtheperformance ofthelocalized models evolvedas\nincreasingly smaller proportions oftheinstances selected\nfrom arecording were used toclassify theremaining sound\ninstances oftherecording. Since dependence between\ntraining andtestdata sets isknowntoyield overly op-\ntimistic results, these testwere performed bydoing ran-\ndomized, mutually exclusi vesplits onthecomplete col-\nlection ofinstances foreach recording.\n2.4. Featur eselection\nAcollection ofcorrectly classi\u0002ed instances from arecord-\ningarethen used tobuildnew,localized sound models.\nRelevantfeatures forthelocalized sound models are\nselected using aCorr elation-based Featur eSelection (CFS)\nalgorithm thatevaluates attrib utesubsets onthebasis of\nboth thepredicti veabilities ofeach feature andfeature\ninter-correlations [6].This method yields asetoffeatures\nwith recording-speci\u0002c good class separability andnoise\nindependence. Thelocalized models may differfrom the\ngeneral models intworespects: theymay bebased on\n1)adifferent feature subset (feature space) and2)differ-\nentthreshold values (decision boundaries) forspeci\u0002c fea-\ntures. Asageneral comment, thefeatures showed signi\u0002-\ncantly better class-homogeneity forlocalized models than\nforthegeneral models.\n2.5. Classi\u0002cation with localized models\nFinally ,theremaining instances must beclassi\u0002ed with\ntherecording-speci\u0002c (localized) models.\nForthis\u0002nal step, wepropose touseinstance-based\nclassi\u0002ers, such as1-NN (k-Nearest Neighbors, withk=\n1).Instance-based classi\u0002ers areusually quite reliable and\ngivegood classi\u0002cation accuracies. However,usual criti-\ncisms arethattheyarenotrobusttonoisy data, theyare\nmemory consuming andtheylack generalization capabil-\nities. Nevertheless, inourframe work,these arenotissues\nweshould beworried about: bytheverynature ofourModel General Localized\n#feat. Accurac y#feat. Accurac y\nKick 19 80.27 5.73 95.06\nSnare 33 70.9 10.41 93.1\nCymbal 22 66.31 10.94 89.17\nTable 1.Average number offeatures used andaccurac y\n(%) forkick, snare and cymbal sound classi\u0002cation in\npolyphonic audio recordings, using both general andlo-\ncalized models.\nmethod, instances arereliable, there arefewofthem and\nweexplicitly seek localized (i.e.notgeneral) models.\n3.EXPERIMENTS, RESUL TSAND DISCUSSION\nTable 1showsacomparison oftheperformance ofthe\ngeneral andlocalized models applied tothepolyphonic\naudio excerpts. The number ofselected features iscon-\nstant forthegeneral models, butindividual toeach record-\ningforthelocalized models. The classi\u0002cation accura-\nciesofthelocalized models aredetermined using 10-fold\ncross-v alidation.\nThenumber offeatures selected forthelocalized mod-\nelsissigni\u0002cantly less than forthegeneral models. At\nthesame time, theperformance oftheformer isclearly\nsuperior .Maybe notsurprisingly ,thisisresulting from\nthelesser variability ofpercussion sounds within aspe-\nci\u0002c recording, which givesclearer decision boundaries\ninthefeature space between instances ofthedifferent cat-\negories.\nDoing feature selection onallsound instances ofarecord-\ning(100%) should givewhat weconsider theideal fea-\nturesubset, which should giveoptimal performance (noise-\nindependence andclass separation) ontheindividual record-\nings. Figure 1showstheaverage classi\u0002cation accurac y\nofthekick, snare andcymbal models, using theoptimal\nfeature subsets foreach localized model. Thetraining-test\ndata splits arerepeated 30times foreach reported training\ndata setpercentage.\nWeseefrom the\u0002gure thattheaccurac yneverdrops\ndownbelowthat ofthegeneral sound models (mark ed\nbydotted lines). Itseems liketheperformance makesa\nsigni\u0002cant drop around 20% 30%, indicating asortof\nthreshold ontheminimum number ofinstances needed\ntopermit successful classi\u0002cation. This proportion cor-\nresponds toabout 1725samples. Further studies have\ntobedone toestablish whether itistherelati vepercentage\northeapproximate number ofsamples thatissigni\u0002cant\nfortheperformance ofthelocalized models.\nInpractice itisnotpossible toknowtheoptimal fea-\nture subsets, asfeature selection must beperformed on\nareduced data set. Table 2showsaverage classi\u0002cation\naccuracies together with theaverage number ofselected\nfeatures forkick, snare andcymbal models, using truly\nlocalized features.\nThere isaslight loss ofperformance from localized0 10 20 30 40 50 60 70 80 9065707580859095100Average classification accuracy [%]\nProportion of complete dataset used for training [%]Performance of localized models\nKick\nSnare\nCymbal\nFigur e1.Accurac yforkick, snare andcymbal sound\nclassi\u0002cation using theoptimal feature subsets and de-\ncreasing proportions ofcorrect instances tocreate thelo-\ncalized models. Thedotted lines mark theaccuracies ob-\ntained with general models.\nmodels with optimal feature subsets (Figure 1).Using\n30% oftheinstances, theaccurac ydecreases 7.3% for\nkicks, 7.57% forsnares and1.17% forcymbals. Weob-\nservethatareduction intheamount oftraining instances\ngreatly effects thefeature selection. Besides ageneral\ndecrease innumber ofselected features, thevariation in\ntypes offeatures selected foreach recording canbehigh.\nWhat isnotevident from thetables, isthevariability\noftheperformance among individual recordings. Atone\nextreme 96.72% accurac yisobtained using only 1feature\nand10% ofthecomplete data set. When comparing to\nclassi\u0002cation with general models, itappears thatrecord-\nings having theleast successful localized models arealso\nleast favorable forclassi\u0002cation with general models.\nAlso, itisimportant tonotice thatrelevantfeatures for\nlocalized models usually differfrom onerecording tothe\nother ,which justi\u0002es theproposed method. Letusfocus,\nforinstance, onsingle-feature based kick models. De-\npending onthespeci\u0002c recording athand, some used e.g.\nthemean oftheframe energyvalues inthe1stBark band,\nothers themean ofthe3rdspectral moment insuccessi ve\nframes, orother features. Snare models used e.g.themean\noftheframe energyvalues inthe11th Bark band orthe\nmean ofthe4thMFCC insuccessi veframes, etc. Cym-\nbalsmodels used e.g.themean of9thMFCC insuccessi ve\nframes orthemean offrame spectral \u0003atness values, etc.\n4.CONCLUSION AND FUTURE WORK\nInthispaper ,wepropose todesign feature-based percus-\nsion instrument sound models specialized onindividual\npolyphonic audio recordings. Initial classi\u0002cation with\ngeneral sound models andparsing oftheir output provides\nareduced setofcorrectly classi\u0002ed sound instances fromPercentage Kick Snare Cymbal\n#features Accurac y#features Accurac y#features Accurac y\n50% 5 89.9 11.86 90.84 6.67 86.73\n40% 5.1 88.42 8.71 87.88 7.13 85.35\n30% 3 86.6 5.71 82.96 3.57 84.72\n20% 2.5 85.51 4 77.22 3.4 79.4\n10% 1 77.92 1.71 73.34 1.27 71.53\nTable 2.Average number offeatures used andaccurac y(%) forkick, snare andcymbal sound classi\u0002cation using\ndecreasing proportions ofcorrect instances toselect relevantfeatures andperform 1-NN classi\u0002cation.\nasingle recording. Byapplying aafeature selection al-\ngorithm tothereduced instance setweobtain thereduced\nfeature sets required todesign recording-speci\u0002c, local-\nized sound models. Thelocalized models achie vedanav-\nerage classi\u0002cation accurac y(and feature dimensionality)\nof95.06% (5.73) forkicks, 93.1% (10.41) forsnares and\n89.17% (10.94) forcymbals, which represents impro ve-\nments ofrespecti vely14.79%, 22.2% and22.86% over\ngeneral model classi\u0002cation accuracies. Wealso showed\nthatthechoice ofrelevantfeatures forpercussion model\ndesign should bedependent, atsome extent, onindividual\naudio recordings.\nPartoffuture workistoimplement asemi-automatic\npercussion transcription tool based ontheapproach pre-\nsented inthispaper .Ourresults areencouraging, butwe\nneed toprocess more andlonger recordings toclaim that\nthemethod isgeneral andscales upwell. More effort\nhastobeputintodetermination ofreliable estimators of\ngeneral model classi\u0002cations. Wemust also consider the\nin\u0003uence ofnoisy data inlocalized model design. An-\nother direction forfuture workistoexplore whether ISA,\nRAST AorSinusoidal +Residual pre-processing canim-\nprovetheclassi\u0002cation performance.\n5.REFERENCES\n[1]Dittmar C.andUhle C.Further Steps towards\nDrum Transcription ofPolyphonic Music\nProc.AES 116th Convention ,Berlin, 2004.\n[2]FitzGerald D., Coyle E.and Lawlor B.\nSub-band Independent Subspace Analysis for\nDrum Transcription Proc.5thInternational\nConfer ence onDigital Audio Effects,Ham-\nburg,2002.\n[3]Goto M., Tabuchi M.and Muraoka Y.An\nAutomatic Transcription System forPercus-\nsion Instruments Proc.46th Annual Conven-\ntionIPSJapan ,1993\n[4]Gouyon F.,Pachet F.andDelerue O.On the\nuseofzero-crossing rateforanapplication of\nclassi\u0002cation ofpercussi vesounds Proc.3d\nInternational Confer ence onDigital Audio Ef-\nfects,Verona, 2000.[5]Gouyon F.and Herrera P.Exploration of\ntechniques forautomatic labelling ofaudio\ndrum tracks instruments Proc.MOSART ,\nBarcelona, 2001.\n[6]Hall M.A.,Correlation-based Feature Se-\nlection forDiscrete andNumeric Class Ma-\nchine Learning, Proc.oftheSeventeenth In-\nternational Confer ence onMachine Learning ,\n2000.\n[7]Heittola T.andA.Klapuri. Locating Segments\nwith Drums inMusic Signals .Technical Re-\nport, Tampere University ofTechnology ,2002.\n[8]Herrera P.,Peeters G.and Dubno vS.Au-\ntomatic Classi\u0002cation ofMusical Instrument\nSounds Journal ofNewMusic Resear ch\nVol.32 .1,2003\n[9]Herrera P.,V.Sandv old and F.Gouyon.\nPercussion-related Semantic Descriptors of\nMusic Audio Files, Proc.AES 25th Interna-\ntional Confer ence,London, 2004.\n[10] Klapuri, Virtanen, Eronen, Seppnen. Auto-\nmatic transcription ofmusical recordings,\nProc.Consistent &Reliable Acoustic Cues\nWorkshop ,Aalbor g,2001.\n[11] Paulus J.andKlapuri A.Model-Based Event\nLabeling intheTranscription ofPercussi ve\nAudio Signals Proc.6thInternational Con-\nference onDigital Audio Effects,London,\n2003.\n[12] Peeters G.,Alargesetofaudio featur esfor\nsound description (similarity and classi\u0002ca-\ntion) intheCUID ADO project .CUID ADO\nI.S.T .Project Report, 2004.\n[13] ZilsA.,Pachet F.,Delerue O.andGouyon F.,\nAutomatic Extraction ofDrum Tracks from\nPolyphonic Music Signals Proc.2ndInterna-\ntional Confer ence onWebDelivering ofMusic ,\nDarmstadt, 2002."
    },
    {
        "title": "The Anatomy of a Bibliographic Search System for Music.",
        "author": [
            "Ryan Scherle",
            "Donald Byrd"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417789",
        "url": "https://doi.org/10.5281/zenodo.1417789",
        "ee": "https://zenodo.org/records/1417789/files/ScherleB04.pdf",
        "abstract": "Traditional library catalog systems have been effective in providing access to collections of books, films, and other material. However, they have many limitations when it comes to finding musical information, which has signifi- cantly different, and in many ways more complex, struc- ture. The Variations2 search system is an alternative de- signed specifically to aid users in searching for music. It leverages a rich set of bibliographic data records, express- ing relationships between creators of music and their cre- ations. These records enable musicians to search for mu- sic using familiar terms and relationships, rather than try- ing to decipher the methods libraries typically use to or- ganize musical items. This paper describes the design and implementation of the system that makes these searches possible.",
        "zenodo_id": 1417789,
        "dblp_key": "conf/ismir/ScherleB04",
        "keywords": [
            "traditional library catalog systems",
            "limitations of traditional systems",
            "finding musical information",
            "significantly different structure",
            "Variations2 search system",
            "alternative for music searching",
            "leveraging bibliographic data",
            "expressing relationships",
            "musicians searching for music",
            "familiar terms and relationships"
        ],
        "content": "THE ANATOMY OFABIBLIOGRAPHIC SEARCH SYSTEM FOR MUSIC\nRyan Scherle andDonald Byrd\nIndiana University\nDigital Library Program\u0000rscherle, donbyrd \u0001@indiana.edu\nABSTRA CT\nTraditional library catalog systems havebeen effectivein\nproviding access tocollections ofbooks, \u0002lms, andother\nmaterial. However,theyhavemanylimitations when it\ncomes to\u0002nding musical information, which hassigni\u0002-\ncantly different, andinmanywaysmore comple x,struc-\nture. TheVariations2 search system isanalternati vede-\nsigned speci\u0002cally toaidusers insearching formusic. It\nleverages arichsetofbibliographic data records, express-\ningrelationships between creators ofmusic andtheir cre-\nations. These records enable musicians tosearch formu-\nsicusing familiar terms andrelationships, rather than try-\ningtodecipher themethods libraries typically usetoor-\nganize musical items. This paper describes thedesign and\nimplementation ofthesystem thatmakesthese searches\npossible.\n1.INTR ODUCTION\nWhile searching collections ofmusical works bycontent\nisaninteresting problem, andhasbeen studied bytheMu-\nsicInformation Retrie valcommunity atlength [9],biblio-\ngraphic searching isequally important. Itisalso, perhaps,\nequally challenging: theexceptional demands ofmusic\narewell-kno wntocatalogers, andtheyhavebeen studied\nindetail bylibrary scientists [15].Searching formusic,\neither bycontent orbymetadata (bibliographic informa-\ntion), isacomple xproblem forseveralreasons:\n1.Multiple versions: Famous books may beavail-\nable inavariety oflanguages, butitisrare fora\nlibrary tocontain multiple versions ofabook that\nwould beofinterest toanyone other than schol-\nars(the Bible being anotable exception). Users\nmay beinterested in\u0002nding aparticular translation\nofCanterb uryTales,butmore often, theysimply\nwanttoread thestory .However,with music, theli-\nbrary ismore likelytohavemultiple versions, and\nusers aremuch more likelytoprefer oneversion\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forpro\u0002t orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation onthe\u0002rstpage.\nc\n\u00022004 Universitat Pompeu Fabra.overanother (e.g., Bing Crosby' sWhite Christ-\nmas, rather than aversion byElvis Presle y,The\nBeach Boys,orGarth Brooks).\n2.Thewhole/part problem: Musical works often ex-\nistinahierarchical structure. Forexample, anopera\nhasaname, anditcontains manynamed arias. A\nuser may beinterested intheworkasawhole, or\nsimply inoneofthearias.\n3.Related works/v ariations: Aportion ofabook or\n\u0002lm may bequoted inanother workofthesame\ntype, buttherelationship isusually verystraightfor -\nward. Music, however,isoften copied, varied, and\nreused inunexpected ways, liketheappearance of\ntheOde toJoytheme from Beetho ven's9thSym-\nphon yasamajor theme inthemovieDie Hard or\ntheexcerpt from LaMarseillaise atthebeginning\noftheBeatles' song AllYouNeed IsLove.The\ncomple xityoftherelationships involvedisamajor\nreason thede\u0002niti vearticle Borro wing [5]issome\n35pages inlength.\n4.Instrumentation: Music isaperforming art,one\nthatisperformed byanenormous variety ofinstru-\nments aswell asvoices, andtheperforming me-\ndiaareoften important toasearcher .Instrumen-\ntation alsogreatly increases theimportance ofmul-\ntiple versions. Pianists play arrangements ofpieces\noriginally written forviolin, violinists play arrange-\nments ofsongs, etc.\n5.Language: Aproblem speci\u0002c tometadata searches\nisthatoflanguage. Someone searching forProust' s\nRemembrance ofThings Pastprobably would not\nbeinterested initems cataloged under theoriginal\ntitle, \u001eAlarecherche dutemps perdu; theyalmost\ncertainly would notbeinterested inallversions of\nthework, regardless oflanguage. However,itis\nverylikelythatanyone who asks fortheRite of\nSpringthe famous ballet byStravinsk ywould be\nequally interested inLeSacre duPrintemps, since\nthetitleofapublication ofmusic says almost noth-\ningabout thecontent.\nDigital library projects thatstore music typically mimic\nstandard library practice, storing asingle record foreach\nitem (e.g., CD, LP,score) inthecollection, with noex-\nplicit relationship between items. Onesuch system isIndi-\nanaUniversity' shighly successful VARIA TIONS project\n[6].VARIA TIONS provides links todigital media directlySearchLease\nManagerUser□Account\nManagerAccess□Control\nMetadata□RepositoryScores\nAudioJava\nRMIHTTP RTSP\nor\nHTTPClient\nServerUser□Tools\nMetadata\nAccessImage\nAccessAudio\nAccess\nFigur e1.System architecture\nfrom records inthelibrary' sonline catalog. Since the\nsearch functionality ishandled bythestandard library cat-\nalog, theproblems described aboveaffectsearch results,\noften making music dif\u0002cult to\u0002nd.\nSmiraglia [13]argues thattocorrect problems likethese,\nmusic retrie valsystems must becentered ontheabstract\nconcept ofthemusical work ,rather than theindividual\nitem inalibrary' scollection. The Functional Require-\nments forBibliographic Records (FRBR) developed by\ntheInternational Federation ofLibrary Associations and\nInstitutions [8]makesasimilar argument forgeneral li-\nbrary materials, andsome FRBR-based systems havebe-\nguntoappear .However,todate, fewmusic projects have\nadopted work-based models. One system thatattempts to\nmanage workinformation independently ofalbuminfor -\nmation istheAllMusic Guide [1].This system provides\nlinks between albums andworks, allowing forsome nav-\nigation between versions ofawork,butthesystem isstill\nlargely item-based.\nVariations2, thesuccessor toVARIA TIONS, isadigi-\ntallibrary system designed toimpro veaccess toanduseof\nmusic. Instead ofthetraditional item-based model, Varia-\ntions2 uses afully work-based model tosupport searching\nandbrowsing. This model collects allversions ofawork\ninresponse tosearches, regardless ofinstrumentation or\nlanguage. Links areprovided from whole works totheir\nparts, andsearches foranamed portion ofaworkwill\n\u0002nd therelevantwhole work. The nextsection givesan\novervie woftheVariations2 architecture, which provides\naframe workforthesubsequent description ofthesearch\nsystem itself.\n2.SYSTEM ARCHITECTURE\nVariations2 allowsusers toaccess music inavariety of\nformats (audio recording, score page images, and, soon,\nencoded scores). The Variations2 system hasamodular\ndesign, separating storage ofmedia \u0002les from storage of\nthebibliographic metadata. Thestructure ofthesystem is\nshowninFigure 1.\nUsers primarily interact with theVariations2 client. The\nclient contains low-levelmodules thataccess metadata and\nplay/display thelibrary' smedia content. User-leveltools,builtontopofthese low-levelmodules, include anau-\ndioplayer andascore viewer,aswell asatool forsyn-\nchronizing playback ofrecordings andscores, andatool\nfordiagramming theform ofamusical piece. Theclient\niswritten inJava,using theSwing user interf acetoolkit,\nwhich makesitportable. Thefullversion oftheclient runs\nonWindowsandMac OSX,while manynon-graphical\nfunctions canalso beperformed onother operating sys-\ntems, including Linux andAIX.\nClients useJava'sprotocol forRemote Method Invo-\ncation (RMI) tocommunicate with theVariations2 ser-\nver.The servermediates access toalllibrary materials,\nincluding audio recordings, scanned score images, andthe\nrepository ofmetadata describing these materials. Allre-\nquests totheservermust pass through anaccess-control\nsystem. Liketheclient, theserveriswritten inJava,and\ncanrunonmultiple operating systems.\nBibliographic records anduseraccount information are\nstored inametadata repository using IBM' srelational data-\nbase system, DB2. Tofacilitate searching oftextual infor -\nmation, thedatabase uses DB2 NetSearch Extender .The\ndatabase design isdescribed further insection 5.2.\nAfter asearch hasidenti\u0002ed bibliographic records for\ndesired media items, users canopen theitems intheap-\npropriate tool. When theclient requests access toanau-\ndiorecording, theserverchecks thattheuser isallowed\ntoaccess therecording, andthen issues theclient alease .\nThe lease contains aURL tomedia onastreaming ser-\nver.Toprotect therights ofcopyright holders, thisURL is\nvalidforalimited time period, forcing theclient torenew\nthelease periodically .Thestreaming serverruns Apple' s\nDarwin Streaming Serverpackage. Using QuickT imefor\nJava,theclient hasfullcontrol overanymedia forwhich\nitholds alease. Audio isstreamed intwoformats: 192kb\nbpsMP3 forhigh-bandwidth connections and28kb bps\nAACformat (within MPEG4) forlow-bandwidth connec-\ntions.\nAsimilar process isfollowed when theclient requests\nascore image. Inthiscase, thelease contains theURL\nofamulti-page image onaWebserver.Score images\narestored intheDjVuformat developed byAT&T Labs\n[4].DjVu'scompression technique, which separates fore-\nground information from background information before\ncompressing, allowsfor\u0002les ofsmall size toretain ex-\ntremely high quality .Other compression formats, which\ntrytocompress foreground andbackground together ,tend\ntolosesigni\u0002cant musical markings (e.g., note stems, dot-\ntednotes) anddistort stafflines. Inparticular ,DjVuim-\nages ofmusic notation look much better than PDF.\n3.THE SEARCH PROCESS\nFrom auser' spoint ofview,searching Variations2 issim-\nilartousing asearch engine ontheWeb.Thebasic search\nscreen, showninFigure 2,allowstheuser toenter acre-\nator/composer ofamusical work, aperformer ,thename\noftheworkitself, thework'smusical key,and/or thetype\nofmedia onwhich theworkappears. Most ofthesearch□\nFigur e2.Search screen\nresults arehyperlinks which leadtomore detail and/or me-\ndia.Users canalsopage forw ards andbackw ards through\npreviously-vie wedresult screens.\nAmusic student searching forworks byJ.S.Bach may\nsimply enter bach into thecreator/composer \u0002eld, as\nshowninFigure 2.Results aredisplayed inthebottom\nhalfofthesearch windo w,indicating thatfour creators in\nthesystem match thename bach.\nInthiscase, theuser isnotimmediately presented with\nalistoflibrary holdings (i.e., sound \u0002les andscores). Pre-\nsenting acomplete listwould cause problems similar to\nthose ofstandard library search systems. Instead, theuser\nisaskedtodisambiguate thequery bychoosing oneof\nthecreators matching bach. Clicking ontheentry for\nBach, Johann Sebastian willretrie vealistoftheworks\ncomposed byhimthatareavailable inthesystem.\nThroughout thedisambiguation process, ifaresult set\ncontains exactly oneitem, thatitem isautomatically se-\nlected, andthequery continues without user interv ention.\nForexample, iftheuser searches forcreator name beeth-\noven, andthere isonly oneresult, theclient willimme-\ndiately issue anewquery togetBeetho ven'sworks, and\nthese willmakeupthe\u0002rstresult screen showntotheuser.\nThis allowsusers toprovide only asmuch information\nasisnecessary toretrie vethedesired content. If,during\nthedisambiguation process, theuser prefers toviewall\nmatching items, theymay click ontheSelect All linkat\nthetopoftheresults (seeFigure 3).\n□\nFigur e3.Search results detail\nThenumber ofdisambiguation screens auser must go\nthrough isdependent onthequery \u0002elds theuser com-\npletes aswell asthecontents ofthelibrary ,butisnever\nmore than four,andistypically only oneortwo.When a\n□\nFigur e4.Search results: library holdings\nuser reaches theendofthedisambiguation process, they\nwill reach ascreen that displays occurrences ofthere-\nquested workinthelibrary' scollection. Figure 4shows\ntheresults ofselecting Bach, Johann Sebastian from\nthecreator listing, andthen selecting Brandenb urgische\nKonzerte. Nr.1from thelistofworks. Variations2 cur-\nrently includes three occurrences ofthiswork,tworecord-\nings andascore. The result screen includes buttons to\nopen each item intheappropriate media player .\nInaddition tothebasic searches described above,users\ncanclick oneofthetabs atthetopofthemain search\nwindo w(see Figure 2)toaccess additional searching op-\ntions. Anadvanced search screen provides more query\n\u0002elds than thebasic screen, including subject headings,\nalbum/score title(asopposed tothetitleofasingle work),\nandcontrib utors other than creators orperformers (de-\nscribed below).Akeywordsearch screen provides searches\nmore typical ofexisting library search systems, allow-\ningsearch overterms appearing anywhere inrelation to\narecording orscore. Abrowsing system allowsusers\ntoquickly learn theextent ofthesystem' scontent. Cat-\nalogers andsystem administrators may useanadditional\nsearch screen thatsupports search overadministrati vedata,\nincluding thecurrent status ofarecord andthetime itwas\ncreated orlastmodi\u0002ed.\n4.THE DATAMODEL\nTosearch information inthisway,themetadata must be\nstructured toaccurately re\u0003ect therelationships between\nmusical entities. The MAchine-Readable Cataloging\n(MARC) [11]records thatarestandard inlibrary catalogs\nhavemanydif\u0002culties capturing these relationships [7].\nTheVariations2 data model, showninFigure 5,isan\nwork-based model thatallowsforrich representation of\ntherelationships between musical works, creators, per-\nformers, andperformances, with aminimum ofredun-\ndant information. This model isverysimilar totheFRBRWORK\r\nINSTANTIATION\r\nCONTAINER\r\nMEDIA OBJECT\rCONTRIBUTOR\ris manifested in\r\nis enclosed in\r\nis represented by\ris created by\ris created by\r\nis created by\rBrandenberg Concerto\r\nNo. 1 by J. S. Bach\r\nPerformance in Dec. 1983 by\r\nAmsterdam Baroque Orchestra\r\nCD set titled\r\nThe Brandenberg Concerti\r\nDigitized sound file\r\nFigur e5.Data model example\nmodel [8].TheVariations2 datamodel includes \u0002verecord\ntypes:\u0003Work: Represents theabstract concept ofamusical\npiece orsetofpieces.\u0003Instantiation: Represents amanifestation ofaWork\nasarecorded performance orascore.\u0003Container: Represents thephysical item (orsetof\nitems) onwhich Instantiations canbefound. There\nmay bemanyInstantiations onasingle Container .\u0003Media Object: Represents apiece ofdigital media\ncontent, such asasound \u0002leorscore image.\u0003Contrib utor: Represents aperson orgroup who\ncontrib uted tothecreation ofmusic.\nTofacilitate searching, Contrib utors arebrokendown\nintothree classes based onthenature oftheir contrib ution:\u0003Creator: AContrib utor attached toaWork. That\nis,aperson/group who contrib uted directly tothe\ncreation ofthemusical piece, butnotnecessarily to\nanyparticular performance orscore. This isusually\nacomposer orlyricist.\u0003Performer: AContrib utorattached toanInstantia-\ntion. This isaperson/group who contrib uted tothe\ncreation ofaparticular performance orpublication\nofaWork. This usually corresponds tothetradi-\ntional de\u0002nition ofaperformer ,butcanalsoencom-\npass roles likeconductors, remix artists, andscore\neditors.\u0003Other Contrib utor: AContrib utor attached toa\nContainer .This isaperson/group who contrib uted\ntothecreation oftheactual albumorscore volume.\nWhile thisdesignation isnotused veryoften, itcan\nbeuseful forfamous producers, orindividuals who\nwrite liner notes.\nEach type ofrecord holds avariety of\u0002elds contain-\ningdescripti ve,administrati ve,andstructural metadata.\nFigure 6showstheVariations2 details screen forJ.S.\nBach, illustrating some ofthedescripti veinformation that\nisstored inContrib utorrecords. Administrati vemetadata\n□\nFigur e6.Details forContrib utorJ.S.Bach\nincludes information about therecord' screation, thenum-\nberoftimes ithasbeen accessed, andinformation about\nrelated records inother library systems (e.g., OCLC or\nIU'smain library catalog). Structural metadata includes\nboth information about relationships between therecords\n(asshowninFigure 5)andstructure within arecord. A\nContainer record representing asetofCDs will havea\nContainer Structure with onenode foreach CD, andsub-\nnodes foreach track onaCD. Likewise, thestructure of\naContainer foranimage score will haveahierarchy of\nnodes, with thelowest levelnodes representing individual\npages. This structure canbeused bythevarious user tools\ntonavigate theContainer .\nUsually ,aContainer record will contain structure ap-\npropriate fornavigation, butinsome cases, thestructure\nofaContainer does notcorrespond toWorks inthesys-\ntem. Auser may beinterested inhearing TheBeatles per-\nform Blue Suede Shoes. However,theonly occurrence\nofthisperformance inthelibrary ispartofamedle y,andit\nbeginsinthemiddle ofaCDtrack. Toovercome thisprob-\nlem, Variations2 uses astructural metadata element called\naBinding, which connects anInstantiation, aWork, and\naMedia Object. The Instantiation forThe Beatles' per-\nformance ofBlue Suede Shoes contains speci\u0002c time\noffsets intotheappropriate Media Object. When theuser\nselects thisInstantiation from thesearch results, theaudio\nplayer uses theBinding structure toopen themedia atthe\ncorrect point. More detailed Bindings canbecreated as\nwell, downtothelevelofoneBinding foreach measure\ninapiece. Theuser tools canusethisstructure tonavigate\naMedia Object with respect tothestructure ofaWork,in-\ndependent ofthestructure represented intheContainer .\nFurther details about theVariations2 data model canbe\nfound elsewhere ([12]and[10]).\n5.DATACREA TION AND STORA GE\nWithahighly-connected datamodel, caremust betakento\ncreate andstore therecords inamanner thatpreserv esthe\nrelationships. Therecords arecreated byimporting data\nfrom other sources, augmented with manual cataloging.Forstorage, thedata records aremapped intothetables of\narelational database.\n5.1. Record creation\nWhen anewitem (i.e., Container) isadded tothesystem,\ninitial metadata isimported from aMARC bibliographic\nrecord inthelibrary' sonline catalog system. Theimport\nsystem identi\u0002es asmuch information aspossible from\ntheMARC record, and\u0002llsthecorresponding \u0002elds ofa\nContainer record. TheMARC record foraContainer of-\ntencontains thenames ofContrib utors orWorks thatap-\npear ontheContainer ,butthese names arenotexplicitly\nlinkedtothestructure oftheContainer .Ahuman cata-\nloger uses thisinformation tocreate newContrib utor or\nWorkrecords, orlinktoexisting ones.\nInaddition totheMARC bibliographic records used\ntopopulate \u0002elds intheContainer ,Variations2 makesuse\nofexisting MARC authority records representing musi-\ncalworks andcontrib utors. These records provide an-\nother source from which information may beimported,\nbutsome datamust stillbeentered manually .Inparticular ,\nnearly allinformation intheInstantiation record (includ-\ningBindings) must begenerated byahuman cataloger ,as\nthere isnocorresponding information intheMARC for-\nmat.\n5.2. Metadata repository\nOnce records havebeen created, theyarestored inarela-\ntional database using IBM' sDB2, version 8.1. One table\nisstored foreach record type, aswell asasingle table to\nhold information common toallrecord types (status, cre-\nation time, etc.). Asetofutility tables stores \u0002elds that\nmay berepeated within arecord, likenotes. Fields such\naspersonal names andalbumtitles thatneed theadvanced\nsearching features provided byDB2 NetSearch Extender\narecopied toaseparate indextable, called TextInde x.\nTable Name Contains\nEntity Fields thatarecommon\ntoallprimary records\nWork Non-repeatable \u0002elds ofa\nInstantiation record, andreferences to\nContainer tables with repeatable \u0002elds\nMedia Object\nContrib utor\nContrib utionSequence Repeatable \u0002elds, which\nNoteSequence may come from anyof\nResourceSequence theprimary record types...\nTextInde x Copies of\u0002elds thatneed\ntobesearchable astext\nUserPro\u0002le Information about user accounts\nGroup andtheir associated privileges\nReserv eList\nTable 1.Partial listing ofdatabase tablesTheContrib utor record forBach, Johann Sebastian,\npartially showninFigure 6,isstored intheContrib utorta-\nble.Each \u0002eld oftherecord corresponds toa\u0002eld ofthis\ntable. Theprimary name iscopied totheTextInde xtable,\nalong with thevariant names. The Worktable contains\narecord forBrandenb urgische Konzerte. Nr.1.This\nWorkrecord references asetofrecords intheContrib u-\ntionSequence table. Inthiscase, thesetcontains asingle\nrecord, which refers totheBach, Johann Sebastian Con-\ntributorrecord.\nWhen arecord isrequested from thedatabase, the\u0002elds\nareretrie vedfrom alltables related totherecord andas-\nsembled into asingle Javaobject. Records may bere-\ntrievedfrom thedatabase individually orassets inre-\nsponse tosearches, asdescribed inthenextsection.\n6.SEARCH SYSTEM INTERN ALS\nTheactual workofsearching isshared bytheclient and\nserver.The client generates queries based ontheuser' s\nrequests, andrenders results inanappropriate manner .\nTheservertakesqueries andcollects thematching records\nfrom thedatabase.\n6.1. Client searchprocess\nWhen theuser clicks theSearch button, theVariations2\nclient packages therequested \u0002elds intoaquery forsub-\nmission totheserver.Thebach query described insec-\ntion3istranslated intoastructure thatlooks likethis:\nConstraints Creator(name) =bach\nResult conte xt Creator\nProjections Creator(dates, names),\nWork(contrib utions)\nTable 2.Initial query forCreator bach\nConstr aints areastraightforw ardencoding ofthesearch\nterms entered bytheuser,labeled with thecorresponding\nrecord types and\u0002elds. The result conte xtindicates the\nprimary type ofrecord being sought atthispoint inthe\nsearch process. This isanoptional portion ofthequery ,\nnecessary foravoiding con\u0003icts incertain cases. Itwill\nbefurther explained inSection 6.2. Asetofprojections\nisautomatically selected bytheclient based onthecon-\nstraints, indicating thetypes ofrecords and\u0002elds thatthe\nservershould return.\nEventhough theprimary purpose ofthisquery isto\nreturn Contrib utor records, theprojection requests Work\nrecords aswell. This isnecessary because theWorkrecord\nstores information about theroleaContrib utor played in\ntheWork'screation, andtheclient willbedisplaying these\nroles toaidtheuser inidentifying thecorrect Contrib utor.\nInFigures 2and3,roles aredisplayed under each Con-\ntributor' sname.\nAfter thequery isprocessed bytheserver,theclient\nrecei vesasetofrecords encoded asJavaobjects. Atthis\npoint, therecord setwillbetransformed intoHTML fordisplay asadisambiguation screen sotheuser canselect\nsomething, orifthere isonly oneCreator intheresult set,\nitwill beautomatically selected. Ineither case, anew\nquery willbesenttotheserver.Table3showsthecontents\nofthenewquery ifJ.S.Bach isselected.\nConstraints Creator(id) =1015\nResult conte xt Work\nProjections Work(all)\nCreator(primary name, dates)\nTable 3.Continued query forBach' sworks\nThis query requests allWorkrecords thathaveaCre-\natorwith ID1015 (Bach' sIDnumber). Aprojection for\nCreator isalso set,since Creator' snames arehelpful in\ntheidenti\u0002cation ofWorks, andarealwaysdisplayed on\naWorkresult screen. When theserverreturns theresult\nsetforthisquery ,theuser will beshowntheWorkdis-\nambiguation screen, andtheprocess begins again, until a\nscreen oflibrary holdings (Instantiations orContainers) is\nreached.\n6.2. Serversearchprocess\nThe Variations2 servertakesqueries posed bytheclient\nintheformat described aboveandreturns theappropriate\nJavaobjects. When theserverrecei vesaquery ,thefol-\nlowing basic process isexecuted:\n1.AnSQL query isexecuted toretrie veIDnumbers\nforrecords oftheresult conte xttype.\n2.Anadditional SQL query isexecuted foreach pro-\njection, retrie ving IDnumbers ofrecords related to\ntheobjects identi\u0002ed instep2.\n3.Foreach oftheIDnumbers identi\u0002ed insteps 1and\n2,aJavaobject isassembled from thedatabase, pro-\njecting toinclude only therequested \u0002elds.\nQueries may contain anycombination ofconstraints,\nresult conte xt,andprojections. Due tothiscomple xity,\nSQL queries toidentify matching records inthemetadata\nrepository arenothard coded. Instead, theservergener -\natesSQL queries automatically .\nAllconstraints areused ateverystep ofthesearch,\nsince theyarenecessary toidentify matching records. For\nexample, iftheusersearched forCreator bach andWork\ntitle west side story, theCreator constraint onitsown\nwould match severalrecords, butnone ofthese would cor-\nrespond with records matching theWorkconstraint.\nToensure queries with multiple constraints correctly\ntraverserelationships between thedatabase tables, theser-\nverreferences adirected graph thatstores thenames of\ntables and\u0002elds thatconnect them. This graph issim-\nilartoFigure 5,with theaddition ofnodes toexpress\nthede\u0002nitions ofthethree Contrib utortypes described in\nSection 4.The serverperforms adepth-\u0002rst traversal of\nthegraph, starting atthenode representing thetargetdata\ntype. When itreaches anode inthegraph correspond-\ningtoaconstraint inthequery ,itadds theconstraint totheSQL statement, alsoadding select statements thatlink\nback tothestarting data type.\nThese SQL queries cangetquite long. Thequery shown\ninFigure 7isoneofthesimplest, requesting Contrib utors\nthatmatch theCreator name bach. The\u0002rsthalfofthe\nquery searches forthethename bach inanyTextInde x\nrecords connected totheContrib utor.name \u0002eld. Thesec-\nond half ofthequery ensures that only Contrib utors of\ntype Creator arefound bylinking Contrib utorrecords with\nrecords intheWorktable.\nSome constraints refer to\u0002elds intheEntity table, and\napply toallrecord types. Forqueries containing these\nconstraints, aninitial SQL query isexecuted using only\ntheEntity constraints, andtheresults areplaced inatem-\nporary database table. Theoriginal Entity constraints are\nthen replaced with aconstraint indicating membership in\nthetemporary table, andthemain query isexecuted. This\nallowsformore ef\u0002cient processing than applying theEn-\ntityconstraints toeach table asitisencountered.\nManyqueries donotrequire aresult conte xt,astheap-\npropriate results could begenerated bysimply retrie ving\nallrecords speci\u0002ed intheprojections thatconform tothe\nconstraints. Aresult conte xtisnecessary ,however,when\nrequesting thelistofWorks performed byaPerformer .In\nthiscase, theconstraints require Contrib utorrecords tobe\ninterpreted asPerformers, butwealso wanttoproject for\nCreators, since theCreators areoften theprimary method\nforidentifying aWork. Ifthesame constraints were ap-\nplied toallprojections, noCreators would match unless\ntheyhappened tobelisted asboth aCreator andPerformer\nofthepiece.\nWhen theserverhascompleted theprocess of\u0002nding\nandbuilding matching records, thesetisreturned tothe\nclient fordisplay totheuser orusewithin oneoftheuser\ntools.\n6.3. Increasing recall\nBecause weareusing adatabase forstorage andretrie val\nofthedata, theretrie valprocess \u0002nds only records that\nmatch thequery exactly; partially-matching records are\nnotfound. Ininformation retrie valterms, thisis100%\nprecision (everydocument found isadesired document),\nbutlessthan 100% recall (not everydesired document is\nretrie ved).\nSince thecontents ofthedatabase arefocused onmu-\nsic,rather than onabroader collection oftopics, precision\nislessofanissue than would normally bethecase inan\ninformation retrie valsystem. Users ofafocused system\nlikethisoften wanthigher recall. Eventhough adatabase\nisbeing used, itispossible totrade precision forgreater\nrecall. Severaltechniques areused toincrease thenumber\nofrecords matching aquery:\u0003Variations incapitalization, diacritics, andpunctua-\ntionareignored. Terms intheTextInde xtable are\nnormalized, with punctuation anddiacritics remo ved,\nandallletters converted tolowercase. The sameSELECTContributor.sql idFROMTextIndex, Contributor WHERE\nContributor.name=TextIndex.sql idANDCONTAINS(TextIndex.text, '\"bach%\"')=1 AND\nContributor.sql idIN(SELECT ContributionSequence.e contributorRef sqlidFROM\nContributionSequence, WorkWHEREWork.contributions=ContributionSequence.s qlid)\nFigur e7.SQL statement toidentify bach asaCreator\nprocess isapplied toanyquery terms thatreference\n\u0002elds intheTextInde x.\u0003Terms arematched asindividual keywords, notas\nphrases, making theorder ofterms within a\u0002eld ir-\nrelevant,unless theuserexplicitly indicates aphrase\nwith quotation marks.\u0003IntheSQL statement, awild card character isap-\npended toeach query term, allowing users totype\nonly the\u0002rst fewletters ofaname ortitle. This\ncharacter appears asapercent sign inFigure 7.\u0003Aswith MARC authority records, thenames ofCon-\ntributors andareoften supplemented with variant\nnames, indicating alternate spellings of,oraliases\nfor,theprimary name. Figure 6indicates some of\nthevariant names stored with theBach record.\nAsaresult ofthese techniques, queries asvaried as\nbach, j.s.,Johann Bach, andIog Seb.Bakh will\nmatch therecord forBach, Johann Sebastian.\n6.4. Performance concer ns\nAlthough search speed isnotamajor focus oftheVaria-\ntions2 project, results must bepresented totheusers quickly\nenough that theydonotbecome toodissatis\u0002ed. Exe-\ncuting comple xSQL statements, building objects from\nthedatabase, transferring objects across anetw orkcon-\nnection, andrendering result screens takestime. Vari-\nations2 makesextensi veuseofcaching toquickly pro-\nvide results forpopular searches. LikeaWebbrowser,\ntheVariations2 client caches theresults ofsearches, so\ntheuser canpage forw ardandbackw ards without wait-\ningfortheservertoanswer aquery .Theservermaintains\ntwoseparate caches. One stores IDnumbers thatmatch\nrecently executed queries, sothatthedatabase will only\nbesearched once when multiple users execute thesame\nquery ,ashappens often forclassroom assignments. The\nsecond servercache stores Javaobjects forrecently re-\nquested IDnumbers, allowing objects tobequickly pro-\njected andreturned, without excess time building the\u0002elds\nofeach object from thedatabase.\nInaddition tocaching results, thesizeofresult setsis\nlimited. Atthetime ofthiswriting, theVariations2 sys-\ntemcontains 1982 Works on389Containers. Eventually ,\nweplan toinclude thenearly 9000 Containers represented\nintheoriginal VARIA TIONS collection [6],andcontinue\ngrowing inresponse totheneeds oftheIUSchool ofMu-\nsic. Non-speci\u0002c queries that result inlargeresult sets\ncancause manytypes ofproblems, including slowing re-\nsponse times, over\u0003owing theJavaRMI connection, or\nexceeding theamount ofmemory allotted totheclient.\nDue tothehighly connected nature ofthedata, splittingresult setsintomultiple pages isproblematic, sothesys-\ntemcurrently places ahard limit onthenumber ofobjects\nintheresult conte xtthatcanbereturned atatime.\n6.5. Keywordsearches\nInaddition tothe\u0002eld-based search described above,Vari-\nations2 provides akeywordsearch system thatallows\nusers tosearch forterms appearing anywhere inrelation\ntoalibrary item (i.e., aContainer). This issimilar tothe\nitem-based keywordsearch users traditionally perform in\nanonline library catalog.\nKeywordsearch works abitdifferently than theba-\nsicsearch, since keywords foragivenContainer must be\ndrawnfrom manytypes ofrecords. Forexample, users\nsearching forkeywordbeatles would expect to\u0002ndmore\nthan justContainers thathavethisterm inoneoftheCon-\ntainer \u0002elds. Rather ,theywould expect to\u0002nd allCon-\ntainers thatcontain Instantiations performed bytheCon-\ntributornamed The Beatles.\nAKeywordInde xtable inthedatabase isused tocollect\nkeywords foreach Container .This table isinitialized with\nContainer titles, sominimal searching canbedone when\naContainer is\u0002rst entered. Periodically ,aprocess runs\ntocompute keywords andaddthem totheKeywordInde x.\nThis process collects allWorks, Instantiations, Perform-\ners,Creators, andOther Contrib utors associated with a\ngivenContainer .All\u0002elds ofthese records areadded to\nthelistofkeywords, except forcertain administrati venote\n\u0002elds.\nDuring search, theKeywordInde xtable functions much\nliketheTextInde xtable. Keywords may beused asacon-\nstraint with other search \u0002elds, andthegenerated SQL\nstatements useDB2 NetSearch Extender toprovide string-\nmatching functionality .Since keywords arebased onthe\ncontents ofaContainer ,there isnoneed forquery dis-\nambiguation. Only Container records aredisplayed inthe\nresult screen forkeywordsearches.\n7.CONCLUSIONS AND FUTURE DIRECTIONS\nEventhough Variations2 isprimarily aresearch system, it\nhasbeen inuseatIndiana University andseveralsatellite\nsites fortwoyears, andhasprocessed over3000 search re-\nquests. Thesearch system hasbeen evaluated inusability\ntests andinactual library use. Users hadlittle dif\u0002culty\nunderstanding thesearch process, andtheywere pleased\nwith both theorganization ofthedataandthesearch inter-\nface[14].\nCataloging items forusewith theVariations2 system\nrequires considerable human effort. This effortisinaddi-tiontotheefforttypically required bythelibrary tocata-\nlogaitem inMARC format. Weareinvestigating meth-\nodsforincreasing theamount ofmetadata thatcanbecol-\nlected automatically .This includes information available\ninMARC records that cannot easily beimported with-\nouthuman interv ention, aswell asinformation from other\nsources. Another solution under investigation iscooper -\nativecataloging, using methods similar toOCLC' sman-\nagement ofcooperati vecataloging forMARC records.\nUltimately ,systems that combine bibliographic and\ncontent-based searching arelikelytobemore effective\nthan either method used alone. Users insearch ofmu-\nsicoften query with acombination ofbibliographic and\ncontent-based information [2].Asa\u0002rsttestofcombined\nsearching, wehaveintegrated Variations2 with theUni-\nversity ofMichigan' sVocalSearch system [3].Inthecom-\nbined system, users areabletosingaquery forarestricted\nsetoflibrary materials (Beatles songs). VocalSearch iden-\nti\u0002es songs with matching themes, andforw ards theID\nnumbers ofthese songs toVariations2. The Variations2\nclient then requests thematching Works from theserver,\nandpresents them totheuser inrankedorder .This sys-\ntem givestheuser theoption tobeginwith acontent-\nbased query andusebibliographic information tore\u0002ne\nresults, retaining theability toseeallavailable Instantia-\ntions (performances orscores) oftheWork. Options for\ncontent-based searching will beexpanded infuture ver-\nsions. Plans areunderw aytoaddmusical themes tothe\nsystem, aswell ascomplete musical scores insymbolic\nform. Both themes andsymbolic scores will besearch-\nable infuture versions ofVariations2.\n8.ACKNO WLEDGMENTS\nWewish tothank themembers oftheVariations2 develop-\nment team, who contrib uted tothedesign andimplemen-\ntation ofthesearching system andprovided feedback on\nearly versions ofthispaper .Theyinclude JonDunn, Mark\nNotess, Geor geYang, JimHalliday ,andRob Pendleton.\nThis material isbased upon worksupported bytheNa-\ntional Science Foundation under grant 9909068. Anyopin-\nions, \u0002ndings, andconclusions orrecommendations ex-\npressed inthismaterial arethose oftheauthors anddo\nnotnecessarily re\u0003ect theviewsoftheNational Science\nFoundation.\n9.REFERENCES\n[1]AllMusic Guide Website.\nhttp://www .allmusic.com.\n[2]D.Bainbridge, S.J.Cunningham, andJ.S.Downie.\nHowpeople describe their music information needs:\nAgrounded theory analysis ofmusic queries. In\nProceedings oftheFourth International Confer ence\nonMusic Information Retrie val(ISMIR 2003) ,Bal-\ntimore, MD, October 2003.[3]W.P.Birmingham, K.O'Malle y,J.W.Dunn, and\nR.Scherle. V2V: Asecond variation onquery-by-\nhumming. InProceedings oftheThirdJoint Con-\nference onDigital Libraries (JCDL2003) ,page 380,\nHouston, TX,May 2003. IEEE Computer Society .\n[4]L.Bottou, P.Haffner,P.Howard,P.Simard, Y.Ben-\ngio, andY.L.Cun. Browsing through high qual-\nitydocument images with DjVu.InProceedings of\nIEEE Advances inDigital Libraries .IEEE, 1998.\n[5]P.Burkholder .Borro wing. InS.Sadie, editor ,\nTheNewGroveDictionary ofMusic andMusicians .\nMacmillan, 2ndedition, 2001.\n[6]J.W.Dunn andC.A.Mayer .VARIA TIONS: adig-\nitalmusic library system atIndiana University .In\nProceedings oftheFourth ACMConfer ence onDig-\nitalLibraries ,Berk eley,CA, 1999.\n[7]H.Hemmasi. Why notMARC? InProceedings\noftheThirdInternational Confer ence onMusic In-\nformation Retrie val(ISMIR 2002) ,pages 242248,\nParis, France, October 2002.\n[8]IFLA Study Group ontheFunctional Requirements\nforBibliographic Records. Functional Requir ements\nforBiblio graphic Recor ds.K.G.Saur,Munich,\n1998.\n[9]ISMIR: TheInternational Conferences onMusic In-\nformation Retrie valandRelated Activities.\nhttp://www .ismir .net/.\n[10] IUDigital Music Library Data Model Speci\u0002cation.\nhttp://v ariations2.indiana.edu/pd f/DML -\nDataModel-V2.pdf.\n[11] Library ofCongress MARC Standards Website.\nhttp://www .loc.go v/marc/.\n[12] N.Minibaye vaandJ.W.Dunn. Adigital library data\nmodel formusic. InProceedings oftheSecond Joint\nConfer ence onDigital Libraries (JCDL2002) ,pages\n154155, Portland, OR, July 2002. IEEE Computer\nSociety .\n[13] R.P.Smiraglia. Musical works asinformation re-\ntrievalentities: Epistemological perspecti ves. In\nProceedings oftheSecond International Confer ence\nonMusic Information Retrie val(ISMIR 2001) ,pages\n8592, Bloomington, IN,October 2001.\n[14] Variations2: IUDigital Music Library Version 2.1\nUsability TestReport ::Searching.\nhttp://v ariations2.indiana.edu/usab ility.html.\n[15] S.Vellucci. Biblio graphic Relationships inMusic\nCatalo gs.Scarecro wPress, Lanham, MD, 1997."
    },
    {
        "title": "Learning to Align Polyphonic Music.",
        "author": [
            "Shai Shalev-Shwartz",
            "Joseph Keshet",
            "Yoram Singer"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416546",
        "url": "https://doi.org/10.5281/zenodo.1416546",
        "ee": "https://zenodo.org/records/1416546/files/Shalev-ShwartzKS04.pdf",
        "abstract": "We describe an efficient learning algorithm for aligning a symbolic representation of a musical piece with its acous- tic counterpart. Our method employs a supervised learn- ing approach by using a training set of aligned sym- bolic and acoustic representations. The alignment func- tion we devise is based on mapping the input acoustic- symbolic representation along with the target alignment for learning support vector machines (SVM), our align- space which separates correct alignments from incorrect ones. We describe a simple iterative algorithm for learn- ing the alignment function and discuss its formal proper- ties. We use our method for aligning MIDI and MP3 rep- resentations of polyphonic recordings of piano music. We also compare our discriminative approach to a generative method based on a generalization of hidden Markov mod- els. In all of our experiments, the discriminative method outperforms the HMM-based method.",
        "zenodo_id": 1416546,
        "dblp_key": "conf/ismir/Shalev-ShwartzKS04",
        "keywords": [
            "supervised learning",
            "acoustic representation",
            "symbolic representation",
            "aligning function",
            "learning algorithm",
            "SVM",
            "alignment function",
            "discriminative approach",
            "generative method",
            "HMM-based method"
        ],
        "content": "LEARNING TOALIGNPOLYPHONIC MUSIC\nShaiShalev-Shwartz JosephKeshetYoramSinger\nfshais,jkeshet,singer g@cs.huji.ac.il\nSchoolofComputer ScienceandEngineering,\nTheHebrewUniversity,Jerusalem, 91904,Israel\nABSTRACT\nWedescribe anefﬁcient learning algorithm foraligning a\nsymbolic representation ofamusical piece with itsacous-\nticcounterpart. Ourmethod emplo ysasupervised learn-\ningapproach byusing atraining setofaligned sym-\nbolic andacoustic representations. The alignment func-\ntion wedevise isbased onmapping theinput acoustic-\nsymbolic representation along with thetargetalignment\nintoanabstract vector -space. Building ontechniques used\nforlearning support vector machines (SVM), ouralign-\nment function distills toaclassiﬁer intheabstract vector -\nspace which separates correct alignments from incorrect\nones. Wedescribe asimple iterati vealgorithm forlearn-\ningthealignment function anddiscuss itsformal proper -\nties. Weuseourmethod foraligning MIDI andMP3 rep-\nresentations ofpolyphonic recordings ofpiano music. We\nalso compare ourdiscriminati veapproach toagenerati ve\nmethod based onageneralization ofhidden Mark ovmod-\nels.Inallofourexperiments, thediscriminati vemethod\noutperforms theHMM-based method.\n1.INTRODUCTION\nThere arenumerous waystorepresent musical recordings.\nTypically ,arepresentation iseither symbolic (e.g. amu-\nsical score orMIDI events) oradigitized audio form such\nasPCM. Symbolic representations entertain quite afew\nadvantages which become handy inapplications such as\ncontent-based retrie val.However,performances ofmusi-\ncalpieces aretypically recorded inoneofthecommon\nforms forcoding ofaudio signals. Score alignment is\nthetaskofassociating each symbolic eventwith itsactual\ntime ofoccurrence intheobserv edaudio signal.\nThere areseveralapproaches tothealignment problem\n(seeforinstance [14,16]andthereferences therein). Most\noftheprevious workonalignment hasfocused ongen-\nerative models andemplo yedparameter estimation tech-\nniques inorder toﬁndamodel thatﬁtsthedata well. In\nthispaper wepropose analternati veapproach forlearning\nPermission tomakedigitalorhardcopiesofallorpartofthisworkfor\npersonalorclassroom useisgrantedwithoutfeeprovidedthatcopies\narenotmadeordistributedforpro®torcommercial advantageandthat\ncopiesbearthisnoticeandthefullcitationonthe®rstpage.\nc\r2004UniversitatPompeuFabra.alignment functions thatbuilds onrecent workondiscrim-\ninativesupervised learning algorithms. Theadvantage of\ndiscriminati velearning algorithms stems from thefactthat\ntheobjecti vefunction used during thelearning phase is\ntightly coupled with thedecision task oneneeds toper-\nform. Inaddition, there isboth theoretical andempiri-\ncalevidence that discriminati velearning algorithms are\nlikelytooutperform generati vemodels forthesame task\n(cf.[4,19]).Tofacilitate supervised learning, weneed\ntohaveaccess toatraining setofaligned data, consisting\nofsymbolic representations along with thedivision ofthe\nperformance intotheactual start times ofnotes.\nThere arenumerous applications where anaccurate and\nfastalignment procedure may become handy .Soulez et\nal.[16] describe fewapplications ofscore alignment such\nascontent-based retrie valand comparisons ofdifferent\nperformances ofthesame musical piece. Inaddition, the\nability toalign between symbolic andacoustic represen-\ntations isanessential ﬁrststep towardapolyphonic note\ndetection system (seealso[18,20,9]).Thegoal ofapoly-\nphonic note detection system istospot notes inanaudio\nsignal. This detection task israther difﬁcult ifnumer -\nousnotes areplayed simultaneously (e.g. inpolyphonic\npieces). There existtheoretical andempirical evidences\nthatsupervised learning iseffectivealso forcomple xde-\ncision problems andisthus likelytobeadequate forpoly-\nphonic note detection. However,supervised learning al-\ngorithms relyontheexistence oflabeled examples. For-\ntunately ,theabundance oflargeacoustic andsymbolic\ndatabases together with anefﬁcient alignment procedure\nenables theconstruction oftraining setforthepolyphonic\nnote detection problem.\nRelatedworkMusic toscore alignment isanimportant\nresearch topic andhasmanyapplications. Most ofthe\nprevious workhasfocused onmonophonic signals. See\nforexample [13,5,7]andthereferences therein. Several\nrecent works [16,14]deal with more comple xpolyphonic\nsignals. Inthispaper ,wesuggest toautomatically learn\nanalignment function from examples using adiscrimina-\ntivelearning setting. Ourlearning algorithm builds upon\nrecent advances inkernel machines andlargemarginclas-\nsiﬁers forsequences [2,1,17]which inturn buildonthe\npioneering workofVapnik andcolleagues [19,4].The\nspeciﬁc form ofthelearning algorithm described inSec. 3\nstems from recent workononline algorithms [8,3].2.PROBLEMSETTING\nInthissection weformally describe thealignment prob-\nlem. Wedenote scalars using lowercase Latin letters (e.g.\nx),andvectors using bold faceletters (e.g.x).Asequence\nofelements isdesignated byabar(\u0016x)anditslength isde-\nnoted byj\u0016xj.\nInthealignment problem, wearegivenadigitized au-\ndiosignal ofamusical piece along with asymbolic rep-\nresentation ofthesame musical piece. Ourgoal istogen-\nerate analignment between thesignal andthesymbolic\nrepresentation. Theaudio signal isﬁrstdivided intoﬁxed\nlength frames (weuse20msinourexperiments) andad\ndimensional feature vector isextracted from each frame\noftheaudio signal. Forbrevitywedenote thedomain of\nthefeature vectors byX\u001aRd.The feature represen-\ntation ofanaudio signal istherefore asequence offea-\nture vectors \u0016x=(x1;:::;xT),wherext2Xforall\n1\u0014t\u0014T.Asymbolic representation ofamusical piece\nisformally deﬁned asasequence ofevents which repre-\nsent astandard waytoperform themusical piece. There\nexistnumerous symbolic representations. Forsimplicity\nandconcreteness wefocus onevents oftype “note-on”.\nFormally ,each “note-on” eventisapair(p;s).Theﬁrst\nelement ofthepair,p2P=f0;1;:::;127gisthenote’ s\npitch value (coded using theMIDI standard). Thesecond\nelement, sisassumed tobeapositi veinteger(s2N)as\nitmeasures thestart time ofthenote inapredeﬁned dis-\ncrete units (weuse20msinourexperiments). Therefore,\nasymbolic representation ofamusical piece consists of\nasequence ofpitch values\u0016p=(p1;:::;pk)andacorre-\nsponding sequence ofstart-times \u0016s=(s1;:::;sk).Note\nthatthenumber ofnotes clearly varies from onemusical\npiece toanother andthuskisnotﬁxed.Wedenote by\nP?(and similarly N?andX?)thesetofallﬁnite-length\nsequences overP.Insummary ,analignment instance is\natriplet (\u0016x;\u0016p;\u0016s)where \u0016xisanacoustic representation of\nthemusical piece and(\u0016p;\u0016s)isasymbolic representation\nofthepiece. The domain ofalignment instances isde-\nnoted byZ=X?\u0002(P\u0002N)?.Analignment between\ntheacoustic andsymbolic representations ofamusical\npiece isformally deﬁned asasequence ofactual start-\ntimes \u0016y=(y1;:::;yk)where yi2Nistheobserv ed\nstart-time ofnoteiintheacoustic signal.\nClearly ,there aredifferent waystoperform thesame\nmusical score. Therefore, theactual (orobserv ed)start\ntimes ofthenotes inthepercei vedaudio signal arevery\nlikelytobedifferent from thesymbolic start-times. Our\ngoal istolearn analignment function that predicts the\nobserv edstart-times from theaudio signal andthesym-\nbolic representation, f:Z!N?.Tofacilitate an\nefﬁcient algorithm weconﬁne ourselv estoarestricted\nclass ofalignment functions. Speciﬁcally ,weassume\ntheexistence ofapredeﬁned setofalignment features,\nf\u001ejgn\nj=1.Each alignment feature isafunction oftheform\n\u001ej:Z\u0002N?!R.That is,each alignment feature\ngets acoustic andsymbolic representations ofamusical\npiecez=(\u0016x;\u0016p;\u0016s),together with acandidate alignment\u0016y,andreturns ascalar which, intuiti vely,represents the\nconﬁdence inthesuggested alignment \u0016y.Wedenote by\n\u001e(z;\u0016y)thevector inRnwhose jthelement is\u001ej(z;\u0016y).\nThealignment functions weuseareoftheform\nf(z)=argmax\n\u0016yw\u0001\u001e(z;\u0016y); (1)\nwherew2Rnisavector ofimportance weight thatwe\nneed tolearn. Inwords,freturns asuggestion foran\nalignment sequence bymaximizing aweighted sum ofthe\nscores returned byeach feature function \u001ej.Note thatthe\nnumber ofpossible alignment sequences isexponentially\nlarge.Nevertheless, asweshowbelow,under mild con-\nditions ontheform ofthefeature functions \u001ej,theopti-\nmization inEq.(1)canbeefﬁciently calculated using a\ndynamic programming procedure.\nAsmentioned above,wewould liketolearn thefunc-\ntionffrom examples. Each example containing analign-\nment iscomposed ofanacoustic andasymbolic represen-\ntation ofamusical piecez=(\u0016x;\u0016p;\u0016s)2Ztogether with\nthetruealignment between them, \u0016y.Let\u0016y0=f(z)bethe\nalignment suggested byf.Wedenote by\r(\u0016y;\u0016y0)thecost\nofpredicting thealignment \u0016y0where thetruealignment is\n\u0016y.Formally ,\r:N?\u0002N?!Risafunction thatgetstwo\nalignments andreturns ascalar which isthecosttopredict\nthesecond input alignment where thetruealignment isthe\nﬁrst. Weassume that\r(\u0016y;\u0016y0)\u00150andthat\r(\u0016y;\u0016y)=0.\nAnexample foracostfunction is,\n\r(\u0016y;\u0016y0)=1\nj\u0016yjj\u0016yjX\ni=1jyi\u0000y0\nij:\nInwords, theabovecost istheaverage oftheabsolute\ndifference between thepredicted alignment andthetrue\nalignment. Inourexperiments, weused avariant ofthe\nabovecost function andreplaced thesummands jyi\u0000y0\nij\nwithmaxf0;jyi\u0000y0\nij\u0000\"g,where \"isapredeﬁned small\nconstant. Theadvantage ofthiscost isthatnolossisin-\ncurred duetotheithnote ifyiandy0\niarewithin adistance\nof\"ofeach other .The goal ofthelearning process is\ntoﬁndanalignment function fthatattains small cost on\nunseen examples. Formally ,letQbeany(unkno wn)dis-\ntribution overthedomain ofalignment examples, Z\u0002N?.\nThe goal ofthelearning process istominimize therisk\nofusing thealignment function, deﬁned astheexpected\nerror offonalignment examples, where theexpectation\nistakenwith respect tothedistrib utionQ,\nrisk(f)=E(z;\u0016y)\u0018Q[\r(\u0016y;f(z))]:\nTodoso,weassume thatwehaveatraining setofalign-\nment examples each ofwhich isidentically andindepen-\ndently distrib uted (i.i.d.) according tothedistrib utionQ.\n(Note thatweonly observ ethetraining examples butwe\ndonotknowthedistrib utionQ.)Inthenextsection we\nshowhowtousethetraining setinorder toﬁndanalign-\nment function fwhich achie vessmall costonthetraining\nsetandthatwith high probability ,achie vessmall average\ncostonunseen examples aswell.The paper isorganized asfollows. InSec. 3wede-\nscribe anefﬁcient algorithm thatlearns analignment func-\ntionffrom examples. The learning algorithm assumes\nthatfisasdescribed inEq.(1).Aspeciﬁc setofacous-\nticfeatures andfeature functions isdiscussed inSec. 4.In\nSec. 5wedescribe adynamic programming procedure that\nefﬁciently calculates f.InSec. 6wedescribe analterna-\ntivemethod foralignment which isbased onagenerati ve\nmodel. InSec. 7wereport experiments onalignment of\npolyphonic piano musical pieces andcompare ourmethod\ntothegenerati vemethod. Finally ,some future directions\narediscussed inSec. 8.\n3.DISCRIMIN ATIVELEARNING ALGORITHM\nRecall that asupervised learning algorithm\nfor alignment recei vesasinput atraining set\nS=f(z1;\u0016y1);:::;(zm;\u0016ym)gand returns aweight\nvectorwdeﬁning analignment function fgiven\ninEq. (1). Inthefollowing wepresent aniterati ve\nalgorithm forlearning theweight vectorw.Wedenote\nbywttheweight vector after iteration tofthealgorithm.\nWestart with thezero vectorw0=0.Oniteration t\nofthealgorithm, weﬁrst recei veatripletz=(\u0016x;\u0016p;\u0016s)\nrepresenting theacoustic and symbolic representations\nofoneofthemusical pieces from ourtraining set.Next,\nweusethecurrent weight vectorwtforpredicting the\nalignment between \u0016xand(\u0016p;\u0016s)asinEq.(1).Let\u0016y0\ntbethe\npredicted alignment. Wethen recei vethetruealignment\n\u0016yfrom thetraining setandsuffercost\r(\u0016y;\u0016y0\nt).Ifthe\ncost iszero wecontinue tothenextiteration andkeep\nwtintact, hencewt+1=wt.Otherwise, weupdate the\nweight vector tobe\nwt+1=wt+p\n\r(\u0016y;\u0016y0\nt)\u0000wt\u0001at\nkatk2at; (2)\nwhereat=\u001e(z;\u0016y)\u0000\u001e(z;\u0016y0\nt).Inwords, weaddtowt\navector which isascaled version ofthedifference be-\ntween thealignment feature vector resulting from thetrue\nalignment \u001e(z;\u0016y)andtheoneobtained bythealignment\nfunction \u001e(z;\u0016y0\nt).Itissimple toshowthatwt+1isthe\nminimizer ofthefollowing projection problem\nmin\nwkw\u0000wtk2s.t. (3)\nw\u0001\u001e(z;\u0016y)\u0015w\u0001\u001e(z;\u0016y0\nt)+p\n\r(\u0016y;\u0016y0\nt)\nTherefore, after updating w,thescore ofthetrue align-\nment\u0016yislargerthan thescore ofthepredicted alignment\n\u0016y0\ntbyatleastp\n\r(\u0016y;\u0016y0\nt).Moreo ver,among allweight vec-\ntorswthatsatisfy theinequality inEq.(3),wt+1isclos-\nesttothevectorwt.After each update ofw,weﬁndthe\nlargestalignment error onthetraining set\n\u000f=maxf\r(\u0016y;f(z)):(z;\u0016y)2Sg:\nIf\u000fislowerthan atermination parameter ,denoted \u000f0,we\nstop andreturn thelastvalue ofw.Apseudo-code ofthe\nlearning algorithm isgiveninFig.1.Input: Atraining setS=f(z1;\u0016y1);:::;(zm;\u0016ym)g;\naccurac yparameter \u000f0\nInitialize: Setw=0;\n(z;\u0016y)=argmaxf\r(\u0016y;f(z)):(z;\u0016y)2Sg;\n\u000f=\r(\u0016y;f(z))\nWhile \u000f\u0015\u000f0do:\nPredict: \u0016y0=f(z)=argmax\n\u0016yw\u0001\u001e(z;\u0016y)\nPayCost: \r(\u0016y;\u0016y0)\nSet:a=\u001e(zi;\u0016yi)\u0000\u001e(zi;\u0016y0)\nUpdate: w w+p\n\r(\u0016yi;\u0016y0)\u0000w\u0001a\nkak2a\nChoosenextexample:\n(z;\u0016y)=argmaxf\r(\u0016y;f(z)):(z;\u0016y)2Sg;\n\u000f=\r(\u0016y;f(z))\nOutput: Final weight vectorw\nFigure1.Thealignment learning algorithm.\nThefollowing theorem bounds thenumber ofiterations\nperformed bytheabovelearning algorithm. Ouranalysis\nassumes thatthere exists aweight vectorw?2Rnsuch\nthatthefollowing inequality holds forallexamples inthe\ntraining set(z;\u0016y)2Sandforall\u0016y0\nw?\u0001\u001e(z;\u0016y)\u0015w?\u0001\u001e(z;\u0016y0)+p\n\r(\u0016y;\u0016y0):(4)\nNote thatifweusew?inEq.(1)then\r(\u0016y;f(z))=0for\nalltheexamples inthetraining set.Amodiﬁcation ofthe\nalgorithm tothecase where such vector does notexistcan\nbederivedusing asimilar technique totheonedescribed\nin[3].\nTheorem1.LetS=f(z1;\u0016y1);:::;(zm;\u0016ym)gbeaset\noftrainingexamples. Assumethatthereexistsaweight\nvectorw?2RnsuchthatEq.(4)holdsforall(zt;\u0016yt)\nand\u0016y0.Inaddition, assumethatforalltandforall\u0016y0we\nhavek\u001e(zt;\u0016y0)k\u00141=2.Letfbethealignment function\nobtained byrunningthealgorithm fromFig.1onSwith\naccuracyparameter \u000fo.Thenthetotalnumberofitera-\ntionsofthealgorithm isatmostkw?k2=\u000f0.\nTheproof ofthetheorem isprovided inalong version\nofthepaper [15]. Thm. 1states thatthenumber ofitera-\ntions ofthealgorithm does notdepend onthenumber of\nexamples. Therefore, only asmall part oftheexamples\ninthetraining setactually effects theresulting alignment\nfunction. Intuiti vely,wecanviewtheexamples which do\nnoteffecttheresulting alignment function asavalidation\nset. Byconstruction, theerror ofthealignment function\nonthisvalidation setissmall andthus itisverylikelythat\nthetrueriskofthealignment function (onunseen data) is\nalso small. The following theorem formalizes thisintu-\nition.\nTheorem2.LetS=f(z1;\u0016y1);:::;(zm;\u0016ym)gbeatrain-\ningsetofexamples identically andindependently dis-\ntributedaccordingtoanunknown distributionQ.As-\nsumethattheassumptions ofThm.1hold.Inaddition,\nassumethat\r(\u0016y;\u0016y0)\u0014Lforallpairs(\u0016y;\u0016y0)andletkbethesmallestintegersuchthatk\u0015kw?k2=\u000f0.Letfbe\nthealignment functionobtainedbyrunningthealgorithm\nfromFig.1onS.Thenforany0\u0014\u000e\u00141thefollowing\nboundholdswithaprobability ofatleast1\u0000\u000e\nrisk(f)\u0014\u000f0+Ls\nkln(em=k)+ln(1=\u000e)\n2(m\u0000k):\nThe proof ofthistheorem isalso provided inalong\nversion ofthepaper [15]. Insummary ,Thm. 2states that\nifwepresent thelearning algorithm with alargenumber of\nexamples, thetrueriskoftheresulting alignment function\nislikelytobesmall.\n4.FEATURES\nInthissection wedescribe thealignment feature functions\nf\u001ejgn\nj=1.Inourexperiments weusedn=10alignment\nfeatures asfollows.\nTheﬁrst9alignment features takethefollowing form,\n\u001ej(\u0016x;\u0016p;\u0016s;\u0016y)=j\u0016pjX\ni=1^\u001ej(xyi;pi);1\u0014j\u00149(5)\nwhere each^\u001ej:X\u0002P!R(1\u0014j\u00149)isasetoflocal\ntemplates foranalignment function. Intuiti vely,^\u001ejisthe\nconﬁdence thatapitch valuepistarts attime indexyiof\nthesignal.\nWenowdescribe thespeciﬁc form ofeach oftheabove\nlocal templates, starting with^\u001e1.Giventhetthacous-\nticfeature vectorxtandapitch valuep2P,thelo-\ncaltemplate ^\u001e1(xt;p)istheenergyoftheacoustic sig-\nnalatthefrequenc ycorresponding tothepitchp.For-\nmally ,letFpdenote aband-pass ﬁlter with acenter fre-\nquenc yattheﬁrstharmon yofthepitchpandcut-of ffre-\nquencies of1=4tone belowandabovep.Concretely ,the\nlowercut-of ffrequenc yofFpis440\u00012p\u000057\u00000:5\n12Hzand\ntheupper cut-of ffrequenc yis440\u00012p\u000057+0 :5\n12Hz,where\np2P=f0;1;:::;127gisthepitch value (coded us-\ningtheMIDI standard) and440\u00012p\u000057\n12isthefrequenc y\nvalue inHzassociated with thecodewordp.Similarly ,\n^\u001e2(xt;p)and^\u001e3(xt;p)aretheoutput energies ofband-\npass ﬁlters centered atthesecond andthird pitch harmon-\nics,respecti vely.Alltheﬁlters were implemented using\nthefastFourier transform.\nTheabove3local templates f^\u001ejg3\nj=1measure energy\nvalues foreach timet.Since weareinterested inidentify-\ningnotes onset times, itisreasonable tocompare energy\nvalues attimetwith energyvalues attimet\u00001.However,\nthe(discrete) ﬁrstorder derivativeoftheenergyishighly\nsensiti vetonoise. Instead, wecalculate thederivativesof\naﬁtted second-order polynomial ofeach oftheabovelo-\ncalfeatures. (This method isalso acommon technique\ninspeech processing systems [11].) Therefore, thenext\n6local templates f^\u001ejg9\nj=4measure theﬁrst andsecond\nderivativesoftheﬁrst3local templates.\nWhile theﬁrst9alignment features measure conﬁ-\ndence ofalignment based onspectral properties oftheInput: Acoustic-symbolic representation z=(\u0016x;\u0016p;\u0016s);\nAnalignment function deﬁned byaweight vectorw\nInitialize:8(1\u0014t\u0014j\u0016xj);D(0;t;1)=0\nRecursion:\nFori=1;:::;j\u0016pj\nFort=0;:::;j\u0016xj\nFor\u00162M\nIf(si\u0000si\u00001>\u001c)\nD(i;t;\u0016)=max\n\u001602MD(i\u00001;t\u0000l;\u00160)+w\u0001^\u001e(xt;pi;\u0016;\u00160),\nwhere l=\u00160(si\u0000si\u00001)\nElse [If(si\u0000si\u00001\u0014\u001c)]\nD(i;t;\u0016)=max\nl2LD(i\u00001;t\u0000l;\u0016)+w\u0001^\u001e(xt;pi;\u0016;\u0016),\nwhere L=f\u0000\u001c;\u0000\u001c+1;:::;\u001c\u00001;\u001cg\nTermination: D?=max\nt;\u0016D(j\u0016pj;t;\u0016)\nFigure2.Theprocedure forcalculating thebestalignment.\nsignal, thelastalignment feature captures thesimilarity\nbetween \u0016sand\u0016y.Formally ,let\n\u0016i=yi\u0000yi\u00001\nsi\u0000si\u00001(6)\nbetheratio between theithinterv alaccording totheobser -\nvation totheinterv alofthecorresponding symbolic rep-\nresentation. Wealsorefer to\u0016iastherelati vetempo. The\nsequence ofrelati vetempo values ispresumably constant\nintime, since\u0016sand\u0016yrepresent twoperformances ofthe\nsame musical piece. However,inpractice thetempo ratios\noften differfrom performance toperformance andwithin\nagivenperformance. Thelocal template ^\u001e10measures the\nlocal change inthetempo,\n^\u001e10(\u0016i;\u0016i\u00001)=(\u0016i\u0000\u0016i\u00001)2;\nand\u001e10issimply thecumulati vesum ofthechanges in\nthetempo,\n\u001e10(\u0016x;\u0016p;\u0016s;\u0016y)=j\u0016sjX\ni=2^\u001e10(\u0016i;\u0016i\u00001): (7)\nThe relati vetempo ofEq.(6)isill-deﬁned whene ver\nsi\u0000si\u00001iszero (orrelati velysmall). Since wedeal\nwith polyphonic musical pieces, veryshort interv alsbe-\ntween notes arerather relevant. Therefore, wedeﬁne the\ntempo \u0016iasinEq.(6)butconﬁne ourselv estoindices i\nforwhich si\u0000si\u00001isgreater than apredeﬁned value\u001c(in\nourexperiments weset\u001c=60ms). Finally ,wedenote by\n^\u001e(xt;p;\u0016;\u00160)thevector inR10oflocal templates, whose\njthelement is^\u001ej(xt;p)if1\u0014j\u00149andwhose 10th\nelement is^\u001e10(\u0016;\u00160).\n5.EFFICIENT CALCULA TIONOFTHE\nALIGNMENT FUNCTION\nSofarwehaveputaside theproblem ofevaluation time of\nthefunction f.Recall thatcalculating frequires solving\nthefollowing optimization problem,\nf(z)=argmax\n\u0016yw\u0001\u001e(z;\u0016y):Adirect search forthemaximizer isnotfeasible since the\nnumber ofpossible alignment sequences \u0016yisexponential\ninthelength ofthesequence. Fortunately ,asweshow\nbelow,byimposing afewmild conditions onthestruc-\ntureofthealignment feature functions, thebestalignment\nsequence canbefound inpolynomial time.\nForsimplicity ,wedescribe anefﬁcient algorithm for\ncalculating thebest alignment using thefeature functions\n\u001ejdescribed inSec. 4.Similar algorithms canbecon-\nstructed foranyfeature functions thatcanbedescribed as\nadynamic Bayesian netw ork(c.f.[6,17]).\nWenowturn tothedescription ofadynamic pro-\ngramming procedure forﬁnding thebest alignment given\nanalignment function deﬁned byaweight vectorw.\nLetMbethesetofpotential tempo ratios oftheform\n(yi\u0000yi\u00001)=(si\u0000si\u00001).Foragivenratio\u00162M,de-\nnote byD(i;t;\u0016)thescore forthepreﬁx ofthenotes\nsequence 1;:::;iassuming that their actual start times\narey1;:::;yi\u00001andforthelastnoteyi=twith\u0016=\n(yi\u0000yi\u00001)=(si\u0000si\u00001).This variable canbecomputed\nefﬁciently inasimilar fashion totheforw ardvariables cal-\nculated bytheViterbi procedure inHMMs (see forin-\nstance [12]). The pseudo code forcomputing D(i;t;\u0016)\nrecursi velyisshowninFig. 2.Thebest sequence ofac-\ntual start times, \u0016y0,isobtained from thealgorithm by\nsaving theintermediate values that maximize each ex-\npression intherecursion step. The comple xity ofthe\nalgorithm isO(j\u0016pjj\u0016xjjMj2),where jMjisthesize of\nthesetM.Note thatjMjistrivially upper bounded by\nj\u0016xj2.However,inpractice, wecandiscretize thesetof\ntempo ratios andobtain agood approximation totheac-\ntual ratios. Inourexperiments wechose thissettobe\nM=f2\u00001;2\u00000:5;1;20:5;21g.\n6.GENERA TIVEMETHOD FORALIGNMENT\nWecompare ourdiscriminati vemethod foralignment to\nagenerati vemethod based ontheGeneralized Hidden\nMark ovModel (GHMM) [10]. Inthegenerati vesetting,\nweassume thattheacoustic signal \u0016xisgenerated from the\nsymbolic representation (\u0016p;\u0016s)asfollows. First, theac-\ntualstart times sequence \u0016yisgenerated from\u0016s.Then, the\nacoustic signal \u0016xisgenerated from thepitch sequence \u0016p\nandtheactual start time sequence \u0016y.Therefore,\nPr[\u0016xj\u0016p;\u0016s]=X\n\u0016yPr[\u0016x;\u0016yj\u0016p;\u0016s]\n=X\n\u0016yPr[\u0016yj\u0016s]Pr[\u0016xj\u0016y;\u0016p]:\nWenowdescribe theparametric form weuseforeach of\ntheterms intheaboveequation. Asin[14], wemodel\ntheprobability oftheactual start-times giventhesym-\nbolic start time byPr[\u0016yj\u0016s]=Qj\u0016yj\ni=1Pr[\u0016ij\u0016i\u00001],where\n\u0016iisasdeﬁned inSec. 4.Inourexperiments, weesti-\nmated theprobability Pr[\u0016ij\u0016i\u00001]from thetraining data.\nTomodel theprobability Pr[\u0016xj\u0016y;\u0016p]weusetwoGaussian\nMixture Models (GMM). The ﬁrst GMM approximatesGHMM-1 GHMM-3 GHMM-5 GHMM-7 Discrim.\n1 10.0 188.9 49.2 69.7 8.9\n2 15.3 159.7 31.2 20.7 9.1\n3 22.5 48.1 29.4 37.4 17.1\n4 12.7 29.9 15.2 17.0 10.0\n5 54.5 82.2 55.9 53.3 41.8\n6 12.8 46.9 26.7 23.5 14.0\n7 336.4 75.8 30.4 43.3 9.9\n8 11.9 24.2 15.8 17.1 11.4\n9 11473 11206 51.6 12927 20.6\n10 16.3 60.4 16.5 20.4 8.1\n11 22.6 39.8 27.5 19.2 12.4\n12 13.4 14.5 13.8 28.1 9.6\nmean 1000.1 998.1 30.3 1106.4 14.4\nstd 3159 3078.3 14.1 3564.1 9.0\nmedian 15.8 54.2 28.5 25.8 10.7\nTable1.Summary oftheLOO loss(inms)fordifferent algo-\nrithms foralignment.\ntheprobability ofxtgiventhat apitchpstarts attime\nt.Wedenote thisprobability function byPr[xtjp].The\nsecond GMM approximates theprobability ofxtgiven\nthat apitchpdoesnotstart attimet.This probabil-\nityisdenoted byPr[xtj:p].Foragiventimet,let\nPt=fp2P:9i;yi=t;pi=pgbethesetofall\npitches ofnotes thatstart ontimet,andletPt=PnPtbe\nthecompletion set.Using theabovedeﬁnitions theprob-\nability oftheacoustic signal \u0016xgiventheactual start time\nsequence \u0016yandthepitch sequence \u0016pcanbewritten as\nPr[\u0016xj\u0016y;\u0016p]=j\u0016xjY\nt=1Y\np:PtPr[xtjp]Y\np:PtPr[xtj:p]:\nWeestimated theparameters oftheGMMs from thetrain-\ningsetusing theExpectation Maximization (EM) algo-\nrithm. Thebestalignment ofanewexample (\u0016x;\u0016p;\u0016s)from\nthetestsetisthealignment sequence \u0016y0thatmaximizes the\nlikelihood of\u0016xaccording tothemodel described above.A\ndetailed description ofthedynamic programming proce-\ndure forﬁnding thebest alignment isprovided inalong\nversion ofthispaper [15].\n7.EXPERIMENT ALRESULTS\nInthis section wedescribe experiments with thealgo-\nrithms described aboveforthetask ofalignment ofpoly-\nphonic piano musical pieces. Speciﬁcally ,wecompare\nour discriminati veand generati vealgorithms. Recall\nthat ouralgorithms useatraining setofalignment ex-\namples fordeducing analignment function. Wedown-\nloaded 12musical pieces fromhttp://www.piano-\nmidi.de/mp3.php where sound and MIDI were\nboth recorded. Here thesound servesastheacous-\ntical signal \u0016xand theMIDI istheactual start times\n\u0016y.Wealso downloaded other MIDI ﬁles ofthe\nsame musical pieces from avariety ofother web-\nsites and used these MIDI ﬁles forcreating these-\nquences \u0016pand\u0016s.The complete dataset weused\nisavailable fromhttp://www.cs.huji.ac.il/ \u0018\nshais/alignment .\nWeused theleave-one-out (LOO) cross-v alidation\nprocedure forevaluating thetestresults. IntheLOOsetup thealgorithms aretrained onallthetraining ex-\namples except one, which isused asatestset. The\nloss between thepredicted andtrue start times iscom-\nputed foreach ofthealgorithms. Wecompared there-\nsults ofthediscriminati velearning algorithm described\ninSec. 3totheresults ofthegenerati velearning algo-\nrithm described inSec. 6.Recall thatthegenerati veal-\ngorithm uses aGMM tomodel some oftheprobabilities.\nGHMM−1 GHMM−3 GHMM−5 GHMM−7 Disc.020406080Loss[ms]\nFigure3.The average loss\nofalltheLOO experiments ex-\ncluding thebest andworst re-\nsults.The number of\nGaussians used\nby the GMM\nneeds to be\ndetermined. We\nused thevalues\nof1,3,5and7\nasthe number\nof Gaussians\nand wedenote\nby GHMM- n\nthe resulting\ngenerati ve\nmodel with n\nGaussians. In\naddition, we\nused theEM algorithm totrain theGMMs. The EM\nalgorithm convergestoalocal maximum, rather than to\ntheglobal maximum. Acommon solution tothisproblem\nistousearandom partition ofthedata toinitialize the\nEM. Inallourexperiments with theGMM weused\n15random partitions ofthedata toinitialize theEM\nand chose theone that leads tothehighest likelihood.\nThe LOO results foreach ofthe12musical pieces are\nsummarized inTable 1.Asseen from thetable, the\ndiscriminati velearning algorithm outperforms allthe\nvariants ofgenerati vealgorithms inalloftheexperiments.\nMoreo ver,inallbuttwooftheexperiments thelossofthe\ndiscriminati vealgorithm islessthan20ms,which isthe\nlength ofanacoustic frame inourexperiments, thus itis\nthebest accurac yonecanhope forthistime resolution.\nItcanbeseen thatthevariance oftheLOO lossobtained\nbythegenerati vealgorithms israther high. This canbe\nattrib uted tothefactthat theEM algorithm converges\ntoalocal maximum which depends oninitialization of\ntheparameters. Therefore, weomitted thehighest and\nlowest lossvalues obtained byeach ofthealgorithms and\nre-calculated theaverage loss overthe12experiments.\nThe resulting mean values along with therange ofthe\nlossvalues aredepicted inFig.3.\n8.FUTURE WORK\nWearecurrently pursuing afewextensions. First, weare\nnowworking onapplying themethods described inthis\npaper toother musical instruments. The main difﬁculty\nhere istoobtain atraining setoflabeled examples. Weare\nexamining semi-supervised methods thatmight overcome\nthelack ofsupervision. Second, weplan toautomatically\ngenerate largedatabases ofaligned acoustic-symbolic rep-resentations ofmusical pieces. These datasets would serve\nasanecessary steptowards theimplementation ofapoly-\nphonic note detection system.\nAcknowledgements Thanks toOfer Dekel,NirKruase, andMo-\nriaShale vforhelpful comments onthemanuscript. This work\nwassupported byEUPASCAL Netw orkOfExcellence.\n9.REFERENCES\n[1]Y.Altun, I.Tsochantaridis, and T.Hofmann. Hidden\nMark ovsupport vector machines. InICML ,2003.\n[2]M.Collins. Discriminati vetraining methods forhidden\nMark ovmodels: Theory andexperiments with perceptron\nalgorithms. InEMNLP ,2002.\n[3]K.Crammer ,O.Dekel,S.Shale v-Shw artz, andY.Singer .\nOnline passi veaggressi vealgorithms. InNIPS ,2003.\n[4]N.Cristianini andJ.Shawe-T aylor .AnIntroductiontoSup-\nportVectorMachines .Cambridge University Press, 2000.\n[5]R.Dannenber g.Anon-line algorithm forreal-time accom-\npaniment. ICMC ,1984.\n[6]T.Dean andK.Kanaza wa.Amodel forreasoning about\npersistent and causation. Computational Intelligence,\n5(3):142–150, 1989.\n[7]A.S.Dure yandM.A.Clements. Melody spotting using\nhidden Mark ovmodels. InISMIR ,2001.\n[8]M.Herbster .Learning additi vemodels online with fast\nevaluating kernels. InCOLT,2001.\n[9]A.Klapuri, T.Virtanen, A.Eronen, andJ.Seppanen. Auto-\nmatic transcription ofmusical recordings. InCRAC,2001.\n[10] L.T.Niles andH.F.Silverman. Combining hidden Mark ov\nmodel andneural netw orkclassiﬁers. InICASSP ,1990.\n[11] L.Rabiner and B.H. Juang.Fundamentals ofSpeech\nRecognition .Prentice Hall, 1993.\n[12] L.R. Rabiner andB.H. Juang. Anintroduction tohidden\nMark ovmodels. IEEEASSPMagazine ,3(1):4–16, Jan.\n1986.\n[13] C.Raphael. Automatic segmentation ofacoustic musical\nsignals using hidden Mark ovmodels.IEEEtrans.onPat-\nternAnalysisandMachineIntelligence,21(4), April 1999.\n[14] S.Shale v-Shw artz, S.Dubno v,N.Friedman, andY.Singer .\nRobusttemporal and spectral modeling forquery by\nmelody .InSIGIR ,2002.\n[15] S.Shale v-Shw artz, J.Keshet, and Y.Singer .Learn-\ning toalign polyphonic music. Long version.\nhttp://www .cs.huji.ac.il/\u0018shais/Shale vKeSi04long.ps .\n[16] F.Soulez, X.Rodet, andD.Schw arz. Impro ving poly-\nphonic andpoly-instrumental music toscore alignment. In\nISMIR ,2003.\n[17] B.Taskar ,C.Guestrin, andD.Koller.Max-mar ginMark ov\nnetw orks. InNIPS ,2003.\n[18] R.Turetsk yandD.Ellis. Ground-truth transcriptions of\nrealmusic from force-aligned MIDI syntheses. InISMIR ,\n2003.\n[19] V.N.Vapnik.Statistical Learning Theory .Wiley,1998.\n[20] P.J.Walmsle y,S.J.Godsill, andP.J.W.Rayner .Polyphonic\npitch tracking using joint Bayesian estimation ofmultiple\nframe parameters. InProc.IeeeWorkshoponApplica-\ntionsofSignalProcessingtoAudioandAcoustics ,October\n1999."
    },
    {
        "title": "Audio Fingerprinting In Peer-to-peer Networks.",
        "author": [
            "Prarthana Shrestha",
            "Ton Kalker"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417457",
        "url": "https://doi.org/10.5281/zenodo.1417457",
        "ee": "https://zenodo.org/records/1417457/files/ShresthaK04.pdf",
        "abstract": "Despite the immense potential of Peer-to-Peer (P2P) net- works in facilitating collaborative applications, they have become largely known as a free haven for pirated music swapping. In this paper, we present an approach wherein the collective computational power of the P2P networks is exploited to combat the problem of unauthorized mu- sic file sharing. We propose a distributed system based on audio fingerprinting,that makes it possible to recognize the music content present in the network. When the con- tents are identified, the network can take special measures against the use or sharing of unauthorized music. This proposed system is self-adapting, and robust. The forego- ing properties make the system particularly suitable for use in dynamic and heterogeneous environment of P2P networks. In order to investigate the behavior of the proposed sys- tem, a system-level model has been created using the Par- allel Object Oriented Specification Language (POOSL). This model was used to investigate an optimal system con- figuration that maximizes the identification of the content.",
        "zenodo_id": 1417457,
        "dblp_key": "conf/ismir/ShresthaK04",
        "keywords": [
            "Peer-to-Peer networks",
            "Unauthorized music file sharing",
            "Distributed system",
            "Audio fingerprinting",
            "Self-adapting",
            "Robust",
            "Dynamic and heterogeneous environment",
            "System-level model",
            "Optimal system configuration",
            "Content identification"
        ],
        "content": "AUDIOFINGERPRINTING IN PEER-TO-PEERNETWORKS\nPrarthanaShrestha,TonKalker\nFacultyof Electrical Engineering, EindhovenUniversityof Technology\nP.O.Box 513, 5600 MB Eindhoven,The Netherlands\nABSTRACT\nDespite the immense potential of Peer-to-Peer (P2P) net-\nworks in facilitating collaborative applications, they have\nbecome largely known as a free haven for pirated music\nswapping. In this paper, we present an approach wherein\nthe collective computational power of the P2P networks\nis exploited to combat the problem of unauthorized mu-\nsic ﬁle sharing. We propose a distributed system based\nonaudioﬁngerprinting,thatmakesitpossibletorecognize\nthe music content present in the network. When the con-\ntentsareidentiﬁed,thenetworkcantakespecialmeasures\nagainst the use or sharing of unauthorized music. This\nproposedsystem is self-adapting,and robust. The forego-\ning properties make the system particularly suitable for\nuse in dynamic and heterogeneous environment of P2P\nnetworks.\nInordertoinvestigatethebehavioroftheproposedsys-\ntem,asystem-levelmodelhasbeencreatedusingthePar-\nallel Object Oriented Speciﬁcation Language (POOSL).\nThismodelwasusedtoinvestigateanoptimalsystemcon-\nﬁgurationthatmaximizestheidentiﬁcationofthecontent.\n1. INTRODUCTION\nPeer-to-Peer(P2P)hasbecomeanestablishednetworkfor\nmusic ﬁle storage and sharing and, yet has remained con-\ntroversialregardingunauthorizedmusicswappingthrough\nthe network. Apart from legal banning, and use of ﬁre-\nwall/ port scanning methods to prevent illegitimate shar-\ning, one of the emerging approaches is the application of\nMusicInformationRetrieval(MIR)overP2Pnetworks[1].\nThe content-based MIR, such as audio-ﬁngerprints and\nwatermarks, can be utilized to identify unauthorized en-\ntries in the network; on the basis of which special mea-\nsures can be taken against their use. In this paper, audio-\nﬁngerprintingintroducedby[2]ismodelledandsimulated\nin P2P network, felicitating distributed computation over\nthe network.\nAccording to the described ﬁngerprinting technique, a\nstream of audio is converted into a stream of 32-bit sub-\nﬁngerprints. A song of 5 minutes produces 25000 sub-\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopiesbear this notice and the full citation on the ﬁrst page.\nc°2004UniversitatPompeu Fabra.ﬁngerprintsandthusamoderatedatabaseof10,000songs\ngives approximately 250 million sub-ﬁngerprints. In or-\ndertoidentifyanunknownaudio,ablockcomprising256\nsubsequentsub-ﬁngerprints isused tosearch fora nearest\nneighbormatch. Thismeans,250millionsearchesforthe\nidentiﬁcation of one song! The process becomes far too\nexpensive to execute in a single PC as the database size\nincreases. Therefore, the immense computational power\navailableinP2Pnetworkpresentsanidealplatformtoim-\nplement distributedaudio-ﬁngerprinting.\nTheﬁngerprintingtechnologynotonlyidentiﬁesunau-\nthorizedﬁlesbutalsoprovidesseveralbeneﬁtstotheusers.\nFirstly, it provides efﬁcient browsing; ﬁnding an exact\nmatch for a search. Secondly, guarantees authenticity on\ndownloads;retrievingexactlyaswhatthenamesays. And\nthirdly,the user can use it to organizepersonal music col-\nlection by correct meta-data labelling. However, in this\npaper we are focused on distributed ﬁngerprinting rather\nthanprovidingtheseabovementionedservicesorintegrat-\ning with anyexistingP2P music sharing networks.\nThe idea of distributed ﬁngerprinting is applied by di-\nvidingthelargeﬁngerprintdatabaseintoseveralsegments\nand distributing them among the peers [3]. While search-\ningforaﬁngerprintmatch,theysimultaneouslycheckinto\ntheir database. However, in case of P2P networks, it is to\nbe taken into account that the characteristic of the peers\nare highly dynamic, i.e. unpredictable about joining and\nleavingthenetworkatanytimeand heterogenous ,i.e. the\nresources possessed by each peer varyin wide range.\nInordertocoordinatethedistributedﬁngerprintingpro-\ncess with minimum administration, a system is proposed\ninwhichtheparticipatingpeersaredividedintohierarchi-\ncal groups according to their resources. In addition, num-\nber of redundant peers are introduced, such that one task\nis assigned to several peers. This ensures that the result\nwillbeavailableevenifsomepeersdisconnectduringthe\njob.\nThe performance of the proposed system was inves-\ntigated by using a system-level model based on Parallel\nObject Oriented Speciﬁcation Language (POOSL). The\nmodel was evaluated in terms of latency, query hits and\npacket loss for various architectures with different peer\nfailure rate. A network with 10 peers including 5 redun-\ndantones,resultedqueryhitsof94%andlatencyof12.3m\nsec. It was also observed that UDP is the suitable trans-\nmissionprotocoltouseinsuchasystemwhileTCPledto\nunresolvabledeadlocks.\u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\t\u0005\n\u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\t\u0005\n\t\u0005\u000b\f\r\u0007\u000e\u000f \n\n\t\u0005\u000b\f\r\u0007\u000e\u0010 \n\t\u0005\u000b\f\r\u0007\u000e\u0011 \n\n\t\u0005\u000b\f\r\u0007\u000e\u0012 \n\n\t\u0005\u000b\f\r\u0007\u000e\u0013 \n\t\u0005\u000b\f\r\u0007\u000e\u0014 \u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\t\u0005\n\u000b\u0005\t\u0002\u0003 \n\u0015\u0016\r\u0017\u0007\u0017\u0016\u000e\u0004\u0018\t\u0017\u0004 Figure 1. Communication between Supervisor group and\nWorkunits.\n2. PEER GROUPSAND FUNCTIONALITIES\nPeer groups are fundamental building blocks of the pro-\nposeddistributedsystem;holdingaspeciﬁcfunction,build-\ning redundancy, and restricting communication messages\nonly to relevant peers. The system model contains the\nthree followinghierarchical peer groups:\nThesupervisor group is a top level group chosen from\nmost resourceful and reliable peers. It co-ordinates the\noverall activity of the group, such as handling requests\nforpeersjoiningthenetwork,ﬁngerprintextraction,query\nformation and submission, and result retrieval. The mem-\nbers of supervisor group communicate with each other\nin certain intervals and share information that how many\npeers are workingunder each of them.\nThemanager groupismoderatelyresourcefulandacts\nasintermediarybetweensupervisorandworkergroup. Each\npeer in the manager group possess at least one redun-\ndant peer, who owns the same job and data. They are\nresponsible for acquiring and dispatching the ﬁngerprint\ndatabase fragments and forward query received from su-\npervisor group to worker groups. The group members\nshare information about their status in frequent intervals\n.\nTheworkergroup is responsible for performing com-\nputations on ﬁngerprint data received from the managers\nand if a match is found send reply to prescribed address.\nEachoftheworkershasoneormoreredundantpeers.The\nworkers are considered as very dynamic and communica-\ntion among the group members is prohibited.\nThepeergroupsconstituteafunctionalunitcalled work\nunit, consisting of managers and workers, as an indepen-\ndent entity running the ﬁngerprinting service. In a typi-\ncal setup, as illustrated in Figure 1, the supervisor sends\nquerytoaworkunitwherea numberofmanagersreceive\nit. The managersforwardthequery toworkers,which are\nequipped with a segment of the database. The workers\nsearch for a match in their database, and a reply is send\nto the given address. The system is characterized, as de-\nscribed in the followingsub-sections.\n\u0001\u0002\u0003\u0002\u0004\u0005\u0006\n\u0001\u0002\u0003\u0002\u0004\u0002\u0005\u0006\u0001\u0002\u0003\u0002\u0004\u0005\u0006\u0007\u0006\b\t\n \u0007\b\u0006\t\u0005\u0006\n\n\u000b\u0003\u0004\u0005\u0006\f\u0006\u000b\u0003\r\u000e\u000f\u0005\u0010\b\u0011\u000b\r\b\u0006\u0012 \n\u0007\b\u0006\t\u0005\u0006\u0007\b\u0006\t\u0005\u0006\u000b\b\u0006\f\u0005\u0006\u0007\b\t\n \n\u0007\b\u0006\t\u0005\u0006Figure 2. Worker node taking over the role of manager\nwhen the latter is disrupted.\n2.1. Scalability\nAsmorepeersjointhenetwork,thesystemshouldbeable\nto accommodate the increased number of peers without\nany performance degradation. When the peers join the\nnetwork ﬁrst time, they start as workers. As the number\nworkers exceeds certain limit, the manager group invites\na resourceful worker to take the manager role. Once the\nnumber of managers and workers exceeds the limit, they\nsplit into two work units. Similarly, when supervisor gets\noverloaded, another peer is requested to take the supervi-\nsor role; both sharing the workload.\n2.2. Adaptability\nWhensomechangesoccurinthesystemsuchaspeerfail-\nures, the peer groups are expected to adapt themselves\nwithout any external intervention and least performance\ndrop. When a worker fails, another redundant worker is\nexpected to complete the job, so no special methods are\nevoked. When a manager fails, its redundant pair informs\nthe manager group and requests a peer from the worker\ngroup, preferably the most resourceful one, to replace the\nlost manager as described in Figure 2. In case of the su-\npervisor group, at least one of the supervisor is always\nkept alive, using dedicated peers. If another supervisor\nis required, an existing supervisor requests the most re-\nsourceful peer in the networkto takeoverthe role.\n2.3. Reliability\nReliability is measured on the basis of correct replies of\nﬁngerprint queries within a prescribed time. In order to\nobtain a reliable system from the dynamic peers, a num-\nber of redundant peers are employed who share same re-\nsponsibility and data.This ensures even if one peer fails,\nthe workis not interrupted.Figure3. POOSL model of distributedﬁngerprinting system\n3. SYSTEM-LEVEL MODELLING OF\nDISTRIBUTEDFINGERPRINTING SYSTEM\nSystem-level modelling allows an abstract representation\nofasystem,withoutentailingabstractionfromimplemen-\ntation details that are less relevant for analyzing the sys-\ntems performance. Therefore the proposed system was\nmodelledusingParallelObject-OrientedLanguage(POOSL)\nand a graphical simulation tool called SHESim. Once the\nmodels were validated in SHESim, they were executed\nin Rotalumus to achieve fast performance measures. The\ntools and their detail descriptions are availableat [6].\nFigure3 depicts a screen-shot of SHESim representing\ndistributed-ﬁngerprinting model. The top window shows\nthe model consisting of two objects, Supervisor and Re-\nsult Analysis, and a Work Unit. Inspection of a WorkUnit\nopens up the bottom window, containing Managers and\nWorkers.\nThis system-levelmodel does not include communica-\ntion between Manager with database repository nor any\ninternal communications within a peer group and there is\nalso no role switching among peers. The following topics\ndescribe the model conﬁgurations used for simulation.\nQuery: Each query is presumed to be an packet size of\n1Kbyte, corresponding to 256x32 sub-ﬁngerprint block.\nThe packetframeworkis illustrated in Figure 4.\nPacketID DestinationID TimeToLive EntranceTime Payload\nFigure4. Query conﬁguration\nTransmission channel : P2P applications are built on top\nofIP(InternetProtocol)andtransportprotocol,mostpop-\nularlyTCP(TransmissionControlProtocol)andUDP.For\ndetails please refer to [5] and [4]. In this model a trans-\nmission channel is assumed to be 10Mbps wide and UDPand TCP protocols are implemented on it. Based on the\nexperimental data, channel transmission delay calculated\nto be 2.6msec [5] and packetloss due to UDP as 1% [4].\nPeer dynamics : In order to represent peer dynamics, a\ncontrolvariableissetinbothManagerandWorkergroups\nthat turns off a prescribed number of peers randomly dur-\ning simulation.\nFingerprint distribution and computation : Each of the\nWorkersisassignedwithﬁngerprintdatabasesizeof5000\nsongs. The model is set with database search time of\n4msec on average and computation error of1%, as pre-\nsented in [2].\n4. SIMULATIONRESULTS\nPerformance of the described model was measured using\nfollowingparameters:\nQueryHits : Thisrepresentsthenumberofcorrectlyiden-\ntiﬁedsongsuponquery. Itsvaluedependsonthepeerfail-\nure,transmissionerrorsandaccuracyofﬁngerprintdatabase\nsearch algorithm.\nPacket Loss : Upon every query, a number of replies are\nexpecteddependingonthenumberofredundancies. Ifthe\nexpectedpacketsfailtoarrive,duetopeerfailureortrans-\nmission error,theyare counted as packetloss.\nLatency: Latencyisthemeasureofaveragetimerequired\nto get a query reply against transmission and processing\ndelays. It is calculated by using the long run average\nmethod available in POOSL, which averages the individ-\nual latencies of all the query replies.\nThe simulations were executed with the conﬁdence level\nof 95% and accuracy of 99%, based on long-run sample\naverage method available in POOSL. The observed aver-\nage latency was about 12.3msec and simulation error was\n1:5¤10¡7.10 20 30 40 50 60 70 80 902030405060708090100\npeers failure(%) query hits(%)Figure5. Query hits as a function of peers failure.\n4.1. Results using UDP\nInordertoinvestigatethesystembehaviorindifferentlev-\nels of redundancy, the model described in Subsection 3\nwas executed with different redundancy in same propor-\ntionofpeerfailure;managers10%andworkers50%. The\nresults are illustrated in Table 1, which were also veriﬁed\nanalytically using probability theories.\ntotal total redundant redundant query\nmanager worker manager worker hits(%)\n1 1 0 0 43\n2 2 1 0 67\n2 4 1 2 87\n3 6 2 3 96\n4 8 3 4 98\nTable 1. Measured query hits for different redundant\nnumber of workersand managers under one supervisor\nFigure 5 and Figure 6 depict the query hits and packet\nloss respectively, resulted in varying percentage of failing\npeers in the system of Figure 3. The simulations were ex-\necuted in two setups. Firstly, the proportion of workers\nfailure was held 40% and the managers failure was var-\nied, represented by the solid line. Secondly, the manager\nfailure proportion was held 20% and worker failure was\nvaried,represented by dotted line.\n4.2. Results using TCP\nTCP was implemented using Multicast protocol. Since\nthis involves high connection overhead and requires ac-\nknowledgementmessageupontransmissionofeachpacket,\neachqueryendedupintime-outoradeadlock. Sonosim-\nulation result could be reproduced with this approach.\n5. CONCLUSION\nInthispaperwepresentedandistributedaudio-ﬁngerprinting\napproach in P2P networks. This enables P2P to become\ncontent-aware, which can be used to ﬁlter out unautho-\nrized contents from the network. A system was proposed\n10 20 30 40 50 60 70 80 9030405060708090100\npeers failure(%)packet loss(%)Figure6. Packetloss as a function of peers failure.\nwith hierarchical peer architecture and studied its perfor-\nmance in redundancy. The system was simulated and an-\nalyzed using a system-level model developed on POOSL.\nThe system behaviorcan be summarized as follows:\nScalability : the system can be extended to accommodate\nlargenumber of peers by adding newworkunits.\nReliability : the system can be made reliable, using ad-\nequate redundancy. When 40% of workers and 10% of\nmanager were turned off randomly during simulation, a\nworkunitof10peersincluding5redundantones,resulted\n96% query hit, while the packetloss was50%.\nAdaptability : thesystemcanworkautonomouslyandad-\njust itself in changing circumstances.\nTransmission : UDP is an efﬁcient transport protocol for\nsuch a system. TCP resulted networkdeadlocks.\nLatency: the average time required to obtain query re-\nsponse is 12.3msec. For a system with more work units,\nthe queries can be processed in parallel.\nThefutureresearchwillfocusonevaluatinginternalcom-\nmunication details and implementation of the model into\na system prototype.\n6. REFERENCES\n[1]Guo J., Tzanetakis G.,Steenkiste P., ”Content-\nbsed retrieval of Music in Scalable P2P Net-\nworks”,ICME,2003.\n[2]Haitsma J., Kalker T., ”A Highly Robust Au-\ndio Fingerprinting System”, ISMIR,2002.\n[3]Verbeke J., Nadgir N., Ruetsch G., Sharapov\nI., ”Framework for Peer-to-Peer Distributed\nComputinginaHeterogeneous,Decentralized\nEnvironment”, Sun Microsystems , July 2002.\n[4]Jacklin A., ”Using UDP to Increase the Scal-\nabilityofPeer-to-Peernetworks”, M.Scthesis,\nUniversityof Shefﬁeld , May 2003.\n[5]Peterson L.L., Davie B.S., Computer net-\nworks: A Systems Approach , Morgan Kauf-\nmann, 2000.\n[6]”POOSL”,www.ics.ele.tue.nl/lvbokhov/poosl/"
    },
    {
        "title": "Creating a nested melodic representation: competition and cooperation among bottom-up and top-down Gestalt principles.",
        "author": [
            "Jane Singer"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417965",
        "url": "https://doi.org/10.5281/zenodo.1417965",
        "ee": "https://zenodo.org/records/1417965/files/Singer04.pdf",
        "abstract": "A set of principles (based on Gestalt theory) governing how we group notes into meaningful groups has been widely accepted in the literature. Based on these principles, many divergent theories of melodic segmentation and representation have been proposed. However, these theories have not succeeded in achieving a comprehensive and verifiable representation of melody. This is largely due to the fact that multiple competing segmenting factors produce, for any single melody, a large number of possible segmentations and therefore representations. Here a model is proposed, which incorporates widely accepted principles of segmentation. These rules govern three types of factors: (1) changes in proximity (for producing disjunctive segmentation), (2) changes in overall contour and intervallic texture and (3) patterns and periodicity that create parallelism among segments. Because of the nature of the segmentation rules, these same rules establish the attributes of the groups they produce. Based on original research in Singer 2004, principles for establishing preferences among competing rules are formulated in order to create a few preferred representations for approximately 1,000 monophonic folksongs.",
        "zenodo_id": 1417965,
        "dblp_key": "conf/ismir/Singer04",
        "keywords": [
            "Gestalt theory",
            "melodic segmentation",
            "representation",
            "multiple competing segmenting factors",
            "comprehensive and verifiable representation",
            "model",
            "segmentation rules",
            "attributes of the groups",
            "preferences among competing rules",
            "monophonic folksongs"
        ],
        "content": "CREATING A NESTED MELODIC REPRESENTATION: \nCOMPETITION AND COOPERATION AMONG BOTTOM-UP AND \nTOP-DOWN GESTALT PRINCIPLES  \nJane Singer \nThe Hebrew University of Jerusalem  \nThe Departm ent of Musicology \njsinger@cc.huji .ac.il \n \nAbstract \nA set of principles  (bas ed on Ges talt theory ) \ngoverning how we group notes into meaningful \ngroups has been widely  accepted in the literature. \nBased on these principles, many  divergent theories of \nmelodic segmentation and representation have been \nproposed. However, these theories have not \nsucceeded in achieving a com prehens ive and \nverifiable representation of m elody. This is largely  \ndue to the fact that m ultiple com peting segm enting \nfactors  produce, for any  single m elody, a large \nnumber of possible segmentations and therefore \nrepresentations. Here a model is proposed, which \nincorporates  widely  accepted principles  of \nsegmentation. These rules govern three types of \nfactors: (1) changes in proximity  (for producing \ndisjunctive segmentation), (2) changes in overall \ncontour and intervallic texture and (3) patterns and \nperiodicity  that create parallelism among segments. \nBecaus e of the nature of the segmentation rules , these \nsame rules establish the attributes of the groups they \nproduce. Based on original research in Singer 2004, \nprinciples for estab lishing preferences among \ncompeting rules  are form ulated in order to create a \nfew preferred representati ons for approximately  1,000 \nmonophonic folksongs.  \n1. Introduction \nGestalt rules of grouping, adapted from the field of \nvisual perception, are widely  cited in order to explain \nhow the listener groups a stream of notes into \nmeaningful groups. Since different rules deal with \ndifferent parameters of varying magnitudes (note \nlength, interval size, re sts) a number of possible \npoints of segmentation become apparent. Rather than \npointing to shortcom ings in the Gestalt theory , the \ncompeting factors  do in fact reflect the ambiguity  that \nexists within the listening experience. In order to \nbuild a comprehensive model of melodic \nsegm entation, this am biguity  needs to be represented. \nOn one hand, this eases the demands made on the \nmodel, since it is not expected to produce a single \n“correct” representation. On the other hand, \nrepresenting such ambiguity  is not achieved by  \nsimply enum erating alternate repres entations : each \nproposed description represents a choice among \ndifferent organizing principl es, adhering to some rules \nwhile violating others. Va rious writings (such as Lerdahl and Jackendoff 1983, Narmour 1990 and \nBregman 1990, Tenney  1980, Temperley  2001) \nlargely  agree on the m ajor role that proxim ity, \nsimilarity  and sy mmetry play  in melodic \nsegmentation and abstraction.  However, the \napplication of these rules has remained problematic. \nMost models  either describe only  specific factors  that \ninfluence segm entation or fail to implem ent the model \non m ore than a few exam ples chosen specifically  for \ntheir adherence to the relevant principles. \nThe proposed model includes both bottom-up rules of \nproximity  and similarity , along with top-down rules \nthat detect periodicity  and patterns (intervallic and \nrhythmic) among groupings.  \nThis paper describes a mode l that was described in \ndetail in Singer 2004. This model is unique in these \naspects : \n(1) It includes conditions for conjunctive, as well as \ndisjunctive segm entation (perceptible changes in \nthe m elodic surface, without an intervening \ndisjunctive interval).  \n(2) It integrates top-down processes (repetition and \nother forms of sy mmetry ) that help reduce the \nnumber of possible parses (from among those \nsuggested by  bottom-up parses, based mostly  on \nproximity ).  \n(3) It includes a full formalization and is \nimplemented on a large body  of real-world data \n(the Essen Folksong Database, henceforth EFD). \nThe purpose of this paper is not to detail the \nindividual rules of the model, but rather to explain the \nlogic of different rule ty pes, and to show how they \nwork together to choose a few preferred \nsegmentations. \nOther models of disjunc tive m elodic segm entation \nhave chosen the EFD data as a source for verify ing \ntheir segmentation algorithms. Bod bases his theory  \non long-term memory  and asserts that these principles \nhave preference over the bottom-up proximity  \nprinciples, such as those proposed in Temperley  and \nTenney . Both Tem perley  and Bod’s  claim s are \nsupported by  quantitative results (that is, their model \noutput matches a high percentage of the EFD \nsegmentations).  \nAll these models make highl y valid contributions to \nthe understanding of the level and ty pe of \nsegmentation represented in the EFD.  Upon \nexam ining the EF D segments, however, it becom es \nclear that changes in the m elodic surfaces suggest \n   intermediate points of segmen tation, and that in order \nto describe the whole s egment, it becom es neces sary \nto break it down into subunits  (such as upbeat figures, \nascending and descending figures, etc.). \nConsequently , in order to create a representation of \nmelodic abstraction, the m elodic ins tances  need to be \nexhaustively  segmented into highly  coherent units \nthat can be accurately  des cribed. Thes e types of \ngroupings cannot alway s be accounted for by  \ndisjunctive segmentation. \nIn order to define changes  in melodic surface, it \nbecom es neces sary to define conjunctive as  well as \ndisjunctive segmentation. The proposed model \nconsiders changes in melodic movement, such as \noverall contour and intervallic texture or m akeup as \nsegment-inducing factors. Conjunctive segmentation \nis neces sary in order to repres ent m elodies , since \nthese attributes (contour and intervallic texture) can \nchange without being reinforced by  disjunctive \nsegmentation. The conditions for perceiving overall \ncontour of a segment were  discussed in Singer 2004. \nChapter 4 of this thesis c onducts a listener experiment \nto show that under certain circumstances listeners are \nwilling to fully  ignore note-to-note m ovem ent and \nchoose overall contour as the most salient feature. \n(Briefly , this is shown to be  true by the fact that the \nlistener identifies “ most sim ilar” pairs as being those \nwith similar contour, not as those with matching note-\nto-note patterns, under the defined conditions.) The \nconstraints im posed on the exam ined m aterial in this \nlistener experiment (the me lodies ) were incorporated \ninto the model’s definition of single-contour \nsegments. \nChapter 5 of the sam e thesis discusses the com petition \nbetween top-down and bottom-up processing. It is \nshown that within the EFD,  segmentation is imposed \non the note s tream  with a preference towards  top-\ndown principles.  Top-down processing benefits from \nthe wisdom of hindsight: it is able to judge the \nrelative saliency  of past events, detect parallelism  \nbetween groups, and create an optimal segmentation \nhierarchy . Although top-down parsing probably  \nalway s takes  place (lis teners  alway s review pas t \nevents, while they  are ta king in the present-sounding \nnote), it revises, rather than nullifies, past \nformulations. Past and present events remain in \ncompetition. \nOne of the most difficult problems of implementing \nthe Gestalt principles within  a m odel is  the fact that \ntoo many  possible points of segmentation are \nsuggested by the rules. Because a change in proxim ity \naccording to the bottom -up principles  is relative, \nalmost every  interval can be a prospective boundary . \nAlthough many  possible points of segmentation can \nbe elim inated as having little segm enting influence, \nmany com peting elem ents emerge as possible points  \nof segmentation. \nThe proposed m odel contends with the m ultiplicity  of \ncontributing factors  according to principles  gleaned \nfrom the EFD, and introduces non-disjunctive \nsegmentation according to changes  in contour and \nintervallic textures (Singer 2004, chapter 4). The purpose of the present paper is to demonstrate \nhow these m ultiple rules work together, specifically  \nhow some rules help elim inate the m ultiplicity  of \nparses produced by  the proximity  principles. \n2. Gr ouping principles \nThe proposed model is based on two ty pes of \nprinciples: top-down and bottom-up. The bottom-\nup/top-down dichotomy  is not a clear one since many  \ntop-down principles operate on a level very close to \nthe melodic s urface, while m any bottom -up proces ses \n(such as  extended paus es), can reinforce the \nseparation between entire movements. The \nsegmentation principles are described here as intra-\ngroup (evaluating a possible note group) and inter-\ngroup (evaluating contiguous proposed groups to \ndetect different ty pes of parallelism). The first group \nis event based, whereas th e second seeks out patterns. \n2.1. Intra-group rules (“bottom-up”) \nUpon hearing a melodic unit (phrase, period or other), \nthe listener may  perceive a number of possible \ngroupings. The level of ambiguity  may  vary  from \nmelody  to melody , from performer to performer and \neven from  listener to listener. The distinctness of a \ngroup is determined by  (1) the cohesiveness of the \ngroup, as well as (2) the contrast between the \nattributes of the group (such as contour) with those of \nthe neighboring groups.  \nThe cohesiveness of any  single grouping is dependent \non a number of factors: \n1. The size of the intervals within the proposed group \n(small intervals). \n2. The durations of the not es within the group (short \nnotes). \n3. The continuity  of overall direction (no peaks). \n4. The level of sim ilarity  of intervals. \nTwo important features of melodic description (nos. 3 \nand 4 above) are overall con tour and intervallic \ntexture ; these consist of abstr actions of intervallic \ninformation that are not linked to a single event, and \ncan only be form ulated from  all intervals within a \nsegment. If the group is not delimited by a disjunctive \ninterval, there is no actual separation (disjunction), \nand the saliency  of such group borders is low. \nBecause the cognition of contour and texture requires \nan abstraction based on information from all the \nintervals within the segm ent, it constitutes a top-down \nprocess, although it operates at a level close to the \nmelodic s urface. Figu re 1  dem onstrates  identical \ncontours, each with different intervallic m akeup. \nWithin the proposed model, figures that adhere to a \nsingle contour (with no salient peaks), maintain \nconsistent melodic makeup and have no intervening \ndisjunctions are referred to as Primitives . Thes e \ngroupings constitute the m ost coherent and basic \nmelodic units. Consecutive conjunctive Prim itives \nthat maintain a single direction (without intervening \npeaks ) are recom bined into Contours .  Contour pairs \nthat cons ist of an anacrus is followed by  a trochaic are \njoined into Constr ucts.  \n \n2 \n a\n  \nb\n  \nc\n  \nd\n  \nFigure 1:   Identical contour, differing \nintervallic makeup (for the ascending figure): \n(a) thirds, (b) ascending seconds, (c) zigzag, \n(d) jump. \nConsecutive Contours can be either conjunctive or \ndisjunctive. A Disjunc tion cons ists of a s pecialized \ninterval that is  perceived as  a separation between \nsegments, rather than bel onging to a segment. Within \nthe model, the rules governing the identification of \nDisjunctions are largely  based on Gestalt principles. \nIn addition a num ber of m etric qualities that are \ntypical to dis junctive intervals  are cons idered as  \nfactors .1 \n2.2. Inter-group rules \nThe level of segm entation identified in the EFD can \nbe accounted for by  parallelism-identify ing principles.  \nAt this level, one of the m ost im portant principles is\nthat of equal-length segments. It was found (Singer \n2004, chapter 5) that within the EFD, of the 10,000 \nmelodies  that were checked (the firs t two s egments of \neach), 9410 of these had segm ents of equal measure \ncount. Since most segments do not begin on a \ndownbeat (meaning that segments are not made up of \nwhole m easures), the number of measures is \nexpressed in downbeat count, rather than beat count. \nSince evoking this method isolates the measure \n(referred to as the m edian m easure) where the \ndisjunction probably  occurs (and not the exact event), \nother factors needed to be considered. Proximity  \n(specifically  the relative s ize of the IOI2) was one \nfactor that helped to is olate the exact pos ition of the \nboundary  within the median measure, even at this \nhigher level.  \nA group of principles were gleaned from the EDF that \naccounted for over 90% of the segmentations \nexamined (9545 of 10395). These \na. The longest note (IOI) in the median \nmeasure \n                                                           \n1 In Singer 2004 (chapter 5), it is shown that \ndisjunctive intervals show a strong tendency  to be \nintervals that begin on the beat, and a lesser tendency  \nto end off the beat. Therefore, m ost EDF  segments \nbegan with upbeat figures. \n2 Inter-ons et-interval, the tim e span between the onset \nof a note and the onset of the next note. b. The larges t interval in the m edian m easure \nc. The point in the m edian m easure that \nmatches the m etric position of the first note \nof the incipit \n \na (reinforced by  rhythmic im itation) \n \nb and c \n \nb and  c \nc (reinforced by  rhythmic and intervallic im itation) \n \nFigure 2:  Some exam ples of parallelis m-creating \nboundaries. \n2.3. Competition and coop eration among the rules \nIn order to incorporate m ultiple rules within the \nmodel, it becom es neces sary to define a s ystem for \nweighting the principles  and factors  that is  capable of \nchoosing the preferred segmentations.  \n2.3.1 . The problem of over-segmentation \nFigure 3 demonstrates the recombination of \nPrim itives into Contours (Contours often contain only  \na single Prim itive), and Contours into Constructs. In \naddition, a top-down segmentation is included \n(specifically  the segm entation represented in the \nEFD). Here, the number of measures (4) divides \nevenly  into two segm ents (2+2), and the top-down \n(EFD) segmentation conforms to rules a, b, and c in \n2.2, as well as being reinforced by  a repeating \nrhythmic pattern. The EFD segmentation concurs \nwith the bottom-up parsing, since the last note of the \nsegment is longer than any  previous note (reinforcing \nthe proxim ity principle).  Nonetheles s, in the cas e \nwhere little or no am biguity  is generated by \ncompeting rules , even a s ingle rule can trigger over-\nsegmentation. According to the proximity  principle, \nwhen applied to the time ax is (duration), every  long-\nshort note pair can be a possible point of \nsegmentation (Disjunction), and every  short-long note \npair becom es a possible segm ent (Prim itive).  The \nrules for creating single-direction groups (Contours) \nhelp overcome this over-segmentation by  preferring \nlonger groups (greater than two notes) of a single \nContour. Therefore in F igure 3, we see two alternate \ninterpretations at the Contour level. One inserts a \ndisjunction and upbeat; the other prefers including the \nentire first measure within a s ingle Contour.  The \npreference for longer groups, together with metric \n 3 \n restrictions im posed on the Prim itive (that forbid a \nPrim itive from  extending beyond the first note of the \nnext measure3), restricts the length and number of \nPrim itives that are included within the preferred set of \nsegmentations. \n \n \n2.3.2 . The problem of top-down/bottom-up rule \ncompetition \nIn order to replicate the ty pe of segmentation found \nwithin the EFD, it is often necessary  to override \nsalient expressions of the proximity  rule. The top-\ndown rules often break up groupings that are strongly  \nsuggested by long notes and large intervals. The \nproximity  rule, however, is not totally  ignored; many  \npoints of segmentation are located at the longest note, \nwithin the median measure. F igure 4 (an exam ple of \nthe actual model output) depicts a melody  that shows \na clear parallel division into two parts. As in Figure 3, \nthe two-measure se gments are strongly  reinforced by  \nthe repetition of a rhy thmic pattern. Therefore, \nalthough the second measure is  strongly  suggested as \nthe m easure of s egmentati on, the exact point of \nsegmentation is not apparent. The point chosen in the \nEFD can only  be accounted for by the matching \n                                                           \n3  Additional m etric constraints are applied to the \nsegments at all levels. Briefly , the strong beats, like \npeaks, are salient events that interfere with the \ncoherency  of the groupings at different levels; \ntherefore they  induce some degree of segmentation. upbeat metric positions and durations (upbeat to m. 1 \nand m. 3). This disregards the first note of m. 2 as a \npoint of segmentation (the longest note in the \nproposed segment). Within the model output (Figure \n4), this second point of segmentation is suggested as \nan alternative. \n \nFigure 3:  Intra-group (bottom-up) and inter-group (bottom-down) segmentations of a melody . \n3. Conclusions \nThe proposed model is to identify  and represent the \nmost salient features  of the EF D m elodies . The \ngraphic output of the model (see Figure 4) includes \na few (1-3) alternate parsings for all levels of the \nmodel. Over 90% of the melodies in the output \ninclude a segm entation that m atches the original \nEFD segmentation. \nThe full set of graphic representations of the model \noutput (along with the input data and XML output) \ncan be found at: \nhttp://shum .huji.ac.il/~jsi nger/thesis/files.htm . \nReferen ces \nBod, R. “Memory -based models of melodic \nanaly sis: challenging the Gestalt principles, ” \nJournal of New Music Research  30, 3 (2001): 27-37 \nBregm an, A.S. Auditory  Scene Analy sis: the \nPreceptual Organization of Sound. (Cam bridge,  \nMassachusetts, 1990). \nLerdahl, F. and R. J ackendoff, A Gener ative \nTheory of Tonal Music . Cambridge, MA, 1983. \nNarmour, E., The Analysis and Cognition of Basic \nMelodic Structures . Chicago, 1990. \nSchaffrath, H. The Essen Folksong Database , 1990 \n(including later data encoded by  Ewa Dahlig, \nDam ien Sagrillo and David Halperin). \nSinger, J. A Model of Melodic Representation: \nTowards an Implementation of Music Information \nRetrieval (PhD Thes is). The Hebrew Univers ity of \nJerusalem, 2004. \nTemperley , D. The Cognition of Basic Musical \nStructures. Cambridge, MA, 2001. \nTenney , J. and L. Polansky , \"Temporal Gestalt \nPerceptions  in M usic,\" Journal of Music Theory  24 \n(2000): 205-241. \n \n4"
    },
    {
        "title": "The International Music Information Retrieval Systems Evaluation Laboratory: Governance, Access and Security.",
        "author": [
            "J. Stephen Downie",
            "Joe Futrelle",
            "David K. Tcheng"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415120",
        "url": "https://doi.org/10.5281/zenodo.1415120",
        "ee": "https://zenodo.org/records/1415120/files/StephenFT04.pdf",
        "abstract": "The IMIRSEL (International Music Information Retrieval Systems Evaluation Laboratory) project provides an unprecedented platform for evaluating Music Information Retrieval (MIR) and Music Digital Library (MDL) techniques, by bringing together large corpora and significant computational resources with the necessary rights management and technical infrastructure to support a variety of MIR/MDL research areas. The standardized research collection being deployed represents a large and diverse corpus of musical examples, which we are hosting in our secure environment for use in evaluating MIR/MDL algorithms. Grid services and NCSA's D2K machine learning environment provide a powerful, high- performance, and secure framework for designing, optimising, and executing complex MIR/MDL evaluation applications. IMIRSEL provides a community resource for researchers who would otherwise not be able to afford the content rights and computational resources to carry out large-scale MIR/MDL evaluations. Keywords: evaluation, system modelling, Grid computing",
        "zenodo_id": 1415120,
        "dblp_key": "conf/ismir/StephenFT04",
        "keywords": [
            "Music Information Retrieval (MIR)",
            "Music Digital Library (MDL)",
            "right management",
            "computational resources",
            "Grid services",
            "NCSAs D2K machine learning environment",
            "community resource",
            "large corpus",
            "evaluation applications",
            "research collection"
        ],
        "content": "THE INTERNATIONAL MUSIC INFORMATION \nRETRIEVAL SYSTEMS EVALUATION LABORATORY: \nGOVERNANCE, ACCESS AND SECURITY \nJ. Stephen Downie Joe Futrelle David Tcheng \nGraduate School of Library \nand Information Science \nUniversity of Illinois at \nUrbana-Champaign  National Center for \nSupercomputing Applications \nUniversity of Illinois at \nUrbana-Champaign  National Center for \nSupercomputing Applications \nUniversity of Illinois at \nUrbana-Champaign \nABSTRACT \nThe IMIRSEL (International Music Information \nRetrieval Systems Evaluation Laboratory) project provides an unprecedented pl atform for evaluating \nMusic Information Retrieval (MIR) and Music Digital Library (MDL) techniques, by bringing together large corpora and significant computational resources with the necessary rights management and technical infrastructure to support a variety of MIR/MDL research areas. The standard ized research collection \nbeing deployed represents a large and diverse corpus of musical examples, which we are hosting in our secure environment for use in evaluating MIR/MDL algorithms. Grid services and NCSA's D2K machine learning environment provide a powerful, high-performance, and secure framework for designing, \noptimising, and executing complex MIR/MDL evaluation applications. IMIRSEL provides a community resource for researchers who would otherwise not be able to afford the content rights and computational resources to carry out large-scale MIR/MDL evaluations. \nKeywords: evaluation, system modelling, Grid \ncomputing \n1. INTRODUCTION \nThe International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL) is being constructed at the University of Illinois at Urbana-Champaign. IMIRSEL is an integral sub-component of the “Music Information Retrieval (MIR) / Music Digital Library (MDL) Evaluation Project.” The principal goal of the project is the creation and refinement of secure yet robust access mechanisms that will allow the \nmanipulation of a unique, terabyte-scale standard corpus of multimodal music materials (e.g., audio, text, symbolic, and metadata). These materials are being put together for the research and evaluation use of the international MIR/MDL res earch community. Because of space limitations, readers ar e directed to [3] and [4] \nfor background information and detailed explications of project motivations, goals and components. Figure 1 illustrates the basic IMIRSEL framework.  \nIn this paper, we introduce and outline the three key \nfeatures of IMIRSEL that will directly affect members of the MIR/MDL research community. Section 2 introduces the governance structure of IMIRSEL along with the sets of first prinicples guiding its operation and management. Section 3 presents information on the Virtual Research Lab (VRL) architecture that we are developing to provide seamless access to the test collections and to the supercomputing resources of the National Center for Supercomputing Applications (NCSA). Section 4 discusses the security framework being laid out to protect the test collections from illicit distribution and cyber-vandalism. Section 5 presents the summary and plans for future work. \n2. GOVERNANCE \n2.1. Introduction \nIt is important that IMIRSEL outlive its initial four-year funding period. We see the initial four-year time span as only the first phase in the construction of a permanent, evolving and vibrant research resource for the MIR/MDL community. This is particularly true if IMIRSEL is to play its intended role as a continuing primary locus of the proposed annual TREC-like evaluation events. To ensure the long-term sustainability \nand impact of IMIRSEL, a two-level governance and advisory structure is bei ng put into place. At the \nUniversity level, a five-member Board of Governors (BOG) is being struck. The BOG is the entity that has official responsibility for all the legal aspects of IMIRSEL including contracts and user-agreements. The International Advisory Board (IAB) forms the second level of governance. The memb ership of the IAB will be \ndrawn from a wide variety of constituencies and will play a crucial role in ensuring that IMIRSEL fulfils its mandate of service to the MIR/MDL research community. \nPermission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made  or distributed for profit or \ncommercial advantage and that copies bear this notice and the full \ncitation on the first page. \n© 2004 Universitat Pompeu Fabra.   \n \n2.2. Board of Governors \nMembership in the Board of Governors consists of the \nfollowing officers from the University of Illinois at Urbana-Champaign: \n 1. IMIRSEL Director /Principal Investigator \n2. Representative appointed by the Office of the Vice-Chancellor Research \n3. Representative appointed by the Office of the University Librarian \n4. Representative appointed by the Office of the Director of the National Center for Supercomputing Applications \n5. Representative appointed by the Office of the Dean of the Graduate School of Library and Information Science \nIt is the mandate of the BOG to provide general \nproject oversight. The BOG is charged with the development and vetting of the necessary terms-of-use agreements with the research teams involved in using the test collections. The BOG is also charged with the \ndevelopment and vetting of all contractual matters with present and future contributors of database content.  \nWith regard to the creation of the terms-of-use \nagreements, and the conten t-provision agreements, the \nfive basic principles guiding all decisions are: \n1. Recognition of the supreme importance of protecting the valuable intellectual property of the content providers. \n2. Recognition of the important responsibility each participating team has of protecting the valuable intellectual property of the content providers. \n3. Recognition that the materials provided are for the scientific research and evaluation purposes of the international MIR/MDL community. \n4. Recognition of the importance of dissemination of development and evaluation results to the broader scientific community via publication of findings. 5. Recognition in publications and presentations of the support being provided by the University, project funders, the content providers, and so on. \n2.3. International Advisory Board \nAn International Advisory Board (IAB) is also being struck. Representatives are being drawn from the following classes of individuals: \n 1. The content-provider community \n2. The user-community (i.e., users of the test-collection database) \n3. The International Conferences on Music Information Retrieval steering committee \n4. The general MIR and MDL communities \n5. The music industry \n6. The music library community \n7. The traditional IR and TREC communities \nIt is the mandate of the IAB to report to the BOG on \nan advisory basis concerning such matters as progress being made, access issues, design and implementation \nof the TREC-like evaluati on experiments, opportunities \nfor future funding and collaborative research, and so on.  \nCommunications with the IAB will reside primarily \nin the electronic domain with http://music-ir.org playing a central role. We plan on meeting with available members at least one a year, most likely in conjunction with the Internationa l Conferences on Music \nInformation Retrieval (ISMIR). At least one on-site meeting (i.e., at UIUC) is also planned. Meeting onsite is intended to deliver a clearer understanding to the  IAB of the physical and organizational infrastructure involved in the project and would thus allow the IAB to provide more informed feedback and recommendations. We also see onsite meeting(s) as an opportunity to solicit the support of potential funding agencies and content-providers (i.e., demonstrate to them the project’s important features including security, community support, and collaborative research opportunities involving the va rious parties, etc.). \nFigure 1  General schematic of IMIRSEL as presented in [3] and [4].  \n  \n \n3. VIRTUAL RESEARCH LAB ACHITECTURE \n3.1. NCSA and The “Grid” \nThe National Center for Supercomputing Applications \n(NCSA) is part of the University of Illinois. Its mandate is to enable academic research ers to get the most out of \nits vast computing resources. NCSA grid computing resources include five world-class supercomputers with a total of approximately 5000 processors and 40 TB of RAM. NCSA’s total compute power is in the 30 Teraflop range [5]. In addition, NCSA maintains high-capacity, high-performance storage resources such as Unitree, a tape-based tertia ry storage system with \nessentially unbounded capacity and 6TB of high-speed disk cache [6].   \nWithin NCSA is the Automated Learning Group \n(ALG) which specializes in data mining and cyber infrastructure development. Over the years, ALG has embedded all of its computing tools in a software system called “D2K” which stands for “Data to Knowledge” [9]. D2K can be viewed as two things. First, it is a new parallel programming language; second, D2K is a body of reusable software components designed for the quick modeling and deployment of new problem-solving programs. \n3.2. D2K \nAt a fundamental level, D2K is a data flow \nprogramming language, along with a development environment called the “D2K Toolkit” that allows users to visualize data flow programs and interactively develop new D2K applications to meet their needs.  \n3.2.1.  Modules and Pipes \nD2K’s atomic computational unit is called a \n“module”—a black box with a number of inputs and outputs. Modules can be developed using a number of programming languages (e.g., Java, C, C++, Matlab, Perl, etc.) allowing D2K to in tegrate diverse codes. The \nD2KToolkit IDE is written in 100% java for maximum portability. To allow for the incorporation of codes written in languages other than  Java, D2K has tools for \n“wrapping” modules writte n in other languages—\nprovided they follow standardized API for the language.  \nWhen a module executes, it “pulls” data objects \nfrom one or more of its inputs, does some computation, and “pushes” the resulting data objects into one or more of its output pipes. A single module execution can result in the pulling or pushing of more than one data object per pipe. Therefore a module is viewed as computing processes rather than mathematical functions.  \nConnections between modules inputs and outputs \nare called “pipes”. Each pipe has a buffer and can hold more than one data object at a time. As described below, buffers allow for a useful type of parallelism automatically employed when pipes start to fill up. 3.2.2.  Itineraries \nModules are composed together to form a directed graph (i.e., a flow chart) linking module inputs to module outputs. The directed graph defines a D2K’s data flow program (DFP) also known as an “itinerary”.  \nThe execution of an itinerary begins by executing \nthe “head” modules (modules with no inputs). After this initial action, all other module executions are triggered by changes to the state module input pipes or by modules executing themselves.  \n3.2.3.  Parallelism \nOne key advantage of D2K is that it allows for the easy \ndevelopment of parallel codes. On multi-processor \nsystems or a network of machines, more than one module can be executing at any point in time. Using this form of parallelism, the maximum number of modules that can execute simultaneously is equal to the number of modules in the itinerary. The other form of D2K parallelism is “pipe-based” parallelism and is invoked when a pipe begins to fill with data. \n \n \n \nFigure 2 : A simple itinerary capable of pipe \nbased parallel processing. \n Figure 2 shows a simple itinerary in which module \nA feeds module B which feeds module C. Module A, pushes a stream of audio file names to module B, one at a time. Module B reads a single file name, extracts features from the audio, and then outputs an example to Module C. Module C, collects all examples created and when the stream ends, writes the set of examples to a \nfile. Modules A, B, and C all fire repeatedly, but only  modules A and C need state memory to operate correctly. Module A uses state memory to keep track of which audio file name in the directory to output next,  and Module C has state memory to accumulate the complete example set. Module B, however, is stateless, meaning its behavior is independent of previous executions. Since D2K knows that Module B is stateless, it monitors its input pipe and when Module B’s input pipe has more than one element, D2K will create multiple copies of the Module B (if there are idle \nCPUs) routing specific inputs to each copy. All copies \nof Module B execute in parallel and their results are   \n \ncollated and pushed into the pipe connecting B and C. On a 256 processor cluster, th is itinerary could cause all \n256 processors of the cluster to be simultaneously running copies of Module B, reading files, and extracting audio features. D2K users can limit the total number of processors used to any number less than 256. \n3.3. D2K and the Virtual Research Labs \nNCSA’s computing Grid resources are vast, but using \nthem can be cumbersome. Currently the Grid supports only batch processing. Jobs are submitted. They wait in a queue for an unknown amount of time (depending on problem size and system load). They begin execution by loading necessary data from mass storage and, after execution completes, the final results are saved to mass storage. This batch processing paradigm, while efficient from the perspective of overall Grid throughput, does not support interactive design and testing of D2K itineraries which is necessary for VRL users.  \nTo address this problem, we envisage a two stage \nprocess for creating new applications: development and production.  \nIn the development phase, the users execute D2K \nlocally on their desktop computers or network of computers. Using local resour ces allows for interaction \nwith the D2K Toolkit for designing and testing itineraries. Once the user is satisfied that the D2K itinerary runs correctly, the user would proceed to the \nproduction phase. \nIn the production phase, the D2K itinerary and \nnecessary data files will be submitted to the NCSA Grid as a batch process using standard grid protocols. Once the batch process executes the results will be retrieved from the Grid and presented to the user. \nUsers will be able to \"toggle\" between local and Grid modes. In local mode, itinerary response will be instantaneous but users will be constrained by their own, probably limited, computational resources. In Grid mode, batch processes will be queued, perhaps for some time, however, once the processing begins, users will benefit from the fact that their full-scale itineraries will be drawing upon the supercomputing resources of NCSA. \n3.3. A Virtual Research Lab Setup: An Example \nFigure 3 illustrates a D2K itinerary for evaluating a \nMIR contest solution for genre classification given a predefined set of training and testing audio files and their associate genres.  \nAudio file names and their genres (as well as other \nmetadata not related to this problem) are stored in an Oracle database. Access to the database is enabled with the modules “Get Genre Classification Training Files From DB” and “Get Genre Classification Test Files \nFrom DB”. These modules output a stream of audio file names and a stream of associated genres.  \nThese modules are considered input  modules \nbecause they are the source of  the data driving the D2K \nitinerary. Input modules icons are recognized by the downward pointing arrow in their icon. \nThe module “Create Audio Example” reads a file \nname, analyzes the audio file extracting a set of numeric audio features, and creates an example by pairing the \naudio features with the genre the database (the ground truth). The two instances of the \"Create Audio Example\" module in this itiner ary are identical copies of \neach other. Given the data flow topology, these modules \ncan operate in parallel. Becau se they are both stateless \nmodules, multiple copies of each can be created, completely saturating all available computing resources. \nFigure 3 : A D2K itinerary for evaluating the performance of a genre classification system.   \n \nStateless modules that can be copied are recognized by the three looping arrows in their icons.  \nThe “Learn Classifier from Examples” module \nanalyzes a set of supervis ed learning examples and \noutputs a predictive model that assigns genre classifications to new unseen examples.  \nThe “Report Model” module simply displays the \nmodel in text or graphics to the user. This module is considered an output module and output modules can be recognized by the upwards pointing arrow in the icon. \n“Measure Accuracy” takes a model, and a set of \ntesting examples, applies the model the testing examples, computes the error of each prediction, and passes accuracy statistics to  the “Report Accuracy” \nmodule. \nD2K comes with many general purpose modules for \nlearning from examples and since they all conform to the same API, users can easily replace one modeling strategy for another by simply swapping this module out for different one. Given this working example, new users can modify it by replacing the example and/or the model creation modules with their own and avoid re-implementing the supporting modules. \n4. SECURITY ARCHITECTURE \n4.1. Requirements \nProtecting the IP rights of content providers (e.g., Naxos) is one of the most critical challenges faced by IMIRSEL. The primary security goal is to prevent the transmission of copyrighted material from the music collection to any third party, who might then knowingly or unwittingly distribute it. The secondary goal is to \nprovide seamless access to the copyrighted material by \nresearch codes, so that MI R algorithms can be evaluated \nagainst the collection. Achieving these two conflicting goals requires a non-trivial security architecture. \n4.2. Architectural Strategy \nThere are several components to the IMIRSEL security \narchitecture. The first component is a set of legal agreements between IMIRSEL and content providers allowing for the transfer of  copyrighted material to \nIMIRSEL and the use of copyrighted material by MIR research codes. These agreements are designed to specifically allow only these uses, and retain all other rights, and serve to constrain any other agreements or policies that IMIRSEL will make with researchers using the facility. \nThe second component of the IMIRSEL security \narchitecture is operating system and network hardening. These standard practices involve the use of firewalls and application proxies, disabling non-essential services, auditing, and disallowing external access to data services. Figure 4 shows the proposed network architecture. Data services are protected by a two-level \nfirewall that only allows traffic from the Grid services host, which acts as an application proxy for all requests that might result in access to the data. \n \nFigure 4 : Proposed IMIRSEL network configuration. A \ntwo-level firewall with an application proxy (the Grid \nservices host) protects data from external access while providing trusted code on the compute nodes access to the data.\n \nThe final component of the IMIRSEL security \narchitecture is the Grid Secur ity Infrastructure (GSI) [2]. \nGSI provides public-key authentication, message \nintegrity, session encryption, and credential delegation for all services. GSI is a best -of-breed family of security \ntechnologies that can be used to protect against unauthorized access, eav esdropping, and code \ntampering. GSI protects user, host, and service credentials with public-key en cryption, and uses a proxy \ncredential strategy to ensure that any creden tial that is \ntransmitted from one service to another is safe from tampering and cannot be used beyond its limited lifetime. GSI certificates are issued and cryptographically signed by trusted certificate authorities (CA), whose private keys are carefully guarded secrets; \nIMIRSEL will act as a certificate authority for its users, and will maintain the integrity of the CA’s key. No user whose identity has not been verified by IMIRSEL will be given a certificate. \n4.3. Trusted Code Model \nSince the only entities that re quire access to copyrighted \ndata are the MIR research codes, IMIRSEL will provide \nan environment in which only those codes have access to the data, and cannot use that access to transfer the data outside of the IMIRSEL system. This is extends the idea of trusting users to trusting users’ code.     The trustworthiness of code will be assured using several mechanisms:  \n1. Digital signature . Researchers will need to \ncryptographically sign their codes, so that   \n \nIMIRSEL will be able to verify at any time that \na researcher’s code has not been modified by a \nthird party. By signing their code, researchers will be asserting that it is not designed to attempt to circumvent any of IMIRSEL’s security protocols. \n2. Auditing . Researchers’ codes will be monitored \nas they run and any attempts at unauthorized access will be brought to the attention of both system administrators and code authors. \n3. Proxy authentication . Codes will access data \nfrom data server(s) using Grid services. Each code author will delegate his/her credentials to his/her codes, which will act on the author’s behalf. In effect, IMIRSEL will trust researchers’ code as much as it trusts them. It will be able to manage this trust relationship dynamically [8]. \n Data will be protected from unauthorized access by Grid \naccess control mechanisms [7] and the firewall architecture described above. Codes will be able to access copyrighted material, but will be prevented from \ntransmitting it outside of IMIRSEL’s private network. Researchers will be able to access results of codes (e.g., \nMIR query results, statistical measures of MIR algorithm performance) using several secure mechanisms, such as Grid-authenticated D2K API calls and GridFTP [1] access to a limited, scratch file area behind the firewall which will be closely monitored so to make sure no copyrighted material is placed there by codes. \n5. \nSUMMARY AND FUTURE WORK \nIMIRSEL provides tools, data, computational resources, software support, and security for MIR researchers. It also represents a new, collaborative, Grid-computing based way of doing MIR/MDL research. By supporting a variety of MIR/MDL research in the IMIRSEL environment, IMIRSEL will facilitate the development of shared, reusable technologies and methodologies. \nTo realize this goal, we plan two major kinds of \nactivities. First, we will provide user support to assist MIR researchers in using the IMIRSEL facility. This involves both training users in how to adapt their codes to run in the IMIRSEL environment (e.g., how to develop D2K applications and interface existing codes \nsuch as Matlab codes into D2K), as well as providing system administration and account management. Initially, we will work closely with a small set of “early \nadopters”, then develop FAQ’s, tutorials, sample applications, and other training materials. Users will be encouraged to share thei r collective work using \nIMIRSEL-provided tools such as discussion groups and shared repositories.   \nSecond, we will extend the D2K framework to \naccommodate both batch and interactive modes of execution, so that res earchers can develop an \napplication using local comput ational resources and data \nand then seamlessly deploy it in IMIRSEL’s secure environment, or deploy applications in distributed, hybrid environments were computation is shared  \nbetween local resources and Grid resources. As the project matures, the deployment of hybrid environments will be the grounding upon which we hope to facilitate interactive evaluation modalities (i.e.,  real-time system \nand interface evaluations). \n6. ACKNOWLEDGEMENTS \nThe Andrew W. Mellon Foundation is thanked for \ntheir financial support. This project is also supported by the National Science Foundation (NSF) under Grant Nos. NSF IIS-0340597 and NSF IIS-0327371. We also thank the content providers and the MIR/MDL community for their support. \n7. REFERENCES \n[1] Allcock, B., et al. “Data Management and Transfer in High Performance Computational Grid Environments”, Parallel Computing \nJournal , 28(5): pp. 749-771, 2002. \n[2] Butler, R., et al. “A national-scale authentication infrastructure”, IEEE Computer\n33(12):  pp. 60-66, 2000. \n[3] Downie, J. S. “The scientific evaluation of music information retrieval systems: Foundations and future”, Computer Music \nJournal  28 (2): pp. 12-23, 2004. \n[4] Downie, J. S. “Toward the scientific evaluation of music information retrieval systems”, Proceedings of the Fourth International Conference on Music Information Retrieval , \nBaltimore, MD, pp. 25-32, 2003. \n[5] “NCSA: Hardware”, http://www.ncsa.uiuc.edu/AboutUs/Hardware/, 2004. \n[6] “NCSA’s Mass Storage System”, http://www.ncsa.uiuc.e du/Divisions/CC/HPD\nM/unitree/, 2004. \n[7] Pearlman, L., et al. “A Community Authorization Service for Group Collaboration”, IEEE 3rd International \nWorkshop on Policies for Distributed Systems and Networks , 2002. \n[8] Welch, V., et al. “X.509 Proxy Certificates for Dynamic Delegation”, 3rd Annual PKI R&D \nWorkshop , 2004. \n[9] Welge, M., et al.  “Data to Knowledge (D2K) An Automated Learning Group Report”, National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, 2003."
    },
    {
        "title": "Well-Tempered Spelling: A Key Invariant Pitch Spelling Algorithm.",
        "author": [
            "Josh Stoddard",
            "Christopher Raphael",
            "Paul E. Utgoff"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1414762",
        "url": "https://doi.org/10.5281/zenodo.1414762",
        "ee": "https://zenodo.org/records/1414762/files/StoddardRU04.pdf",
        "abstract": "In this paper is described a data-driven algorithm for the functionally correct spelling of MIDI pitch values in terms of Western musical notation.  Input is in the form of MIDI files containing accurate pitch and rhythmic information with corresponding ground-truth spelling information for training and evaluation.  The algorithm recovers harmonic information from the MIDI data and spells pitches according to their relation to the local tonic.  The algorithm achieved 94.98% accuracy on the pitches that required accidentals in the local key and 99.686% overall.  Voice-leading resolution was found to be the best feature of those used to infer the correct spelling.  Also, this paper outlines great potential for improvement under this model.",
        "zenodo_id": 1414762,
        "dblp_key": "conf/ismir/StoddardRU04",
        "keywords": [
            "data-driven",
            "algorithm",
            "MIDI pitch",
            "spelling",
            "Western musical notation",
            "harmonic information",
            "spelling pitches",
            "accidentals",
            "local tonic",
            "voice-leading resolution"
        ],
        "content": "WELL-TEMPERED SPELLING: \nA KEY-INVARIANT PITCH SPELLING ALGORITHM \nJoshua Stoddard \nU. Mass., Amherst \nMath & Stat Department \n   Christopher Raphael \nU. Mass., Amherst \nMath & Stat Department \n  \n Paul E. Utgoff \nU. Mass., Amherst \nComp. Sci. Department \n \nABSTRACT \nIn this paper is described a data-driven algorithm for the \nfunctionally correct spelling of MIDI pitch values in \nterms of Western musical notation.  Input is in the form \nof MIDI files containing accurate pitch and rhythmic information with corresponding ground-truth spelling \ninformation for training and evaluation.  The algorithm \nrecovers harmonic information from the MIDI data and spells pitches according to their relation to the local \ntonic.  The algorithm achie ved 94.98% accuracy on the \npitches that required accidentals in the local key and 99.686% overall.  Voice-leading resolution was found to \nbe the best feature of those used to infer the correct \nspelling.  Also, this paper outlines great potential for improvement under this model.      \n1. INTRODUCTION \nIn MIDI, pitch information is encoded as an integer \npitch-level value.  The pitch-level, however, does not \nuniquely determine the spelling in Western music \nnotation [4].  Different spellings are called enharmonically equivalent   if they map to the same \npitch-level.  Pitch spelling  is the process of retrieving \nthe spelling information lost in the pitch-level \nrepresentation of pitch.        \n One obvious application of pitch spelling is in the translation from MIDI to  Western music notation.  \nCurrently, most music notation software can perform \nrudimentary pitch spelling on MIDI data, but the results are often prone to enharmonic errors.  The spelling of a \npitch is strongly influenced by its harmonic and melodic \ncontext.  This higher-level contextual information often \ncan be retrieved reliably from the pitch-level \ninformation in MIDI data, though the problem is highly \nnon-trivial [6].  Thus, a pitch spelling algorithm that can retrieve and make use of more contextual melodic and \nharmonic information may produce more accurate \nresults. \n  This paper presents a data driven algorithm for \npitch spelling in a harmonic context.  The algorithm \nassigns spellings to pitches in polyphonic, rhythmically \naccurate MIDI data according to a harmonic parse \ngenerated by existing harmonic analysis software [6] and a decision tree structur e generated automatically \nfrom training data using Breiman et. al.’s CART [1].  \nOn the test data, it showed a success rate of 94.98% (misspelled 70 of 1395 notes) on the cases in which the \nspelling was not completely determined by the harmonic \nparse, and 99.686% overall (misspelled 71  of 22,593 \ntotal notes).  Later, 26 of the 71 ‘misspelled’ cases were \ndiscovered to be errors in the ‘ground-truth’ spelling \ndata with which they were  compared.  The overall \naccuracy of the algorithm is limited by the accuracy of \nthe harmonic parse, which is generated independently of \nthe rest of the algorithm.  However, the fact that the \nalgorithm was able to captur e errors in the ground-truth \ndata speaks to its robustness under imperfect \ncircumstances      The algorithm presented here differs from \nexisting pitch spelling algorithms such as those of \nCambouropoulos [2], Meredith [5] and Chew & Chen \n[3] in that it views pitch spelling as independent, key-\ninvariant Boolean classifica tion problems on the pitch \nlevels falling outside the local key.  This means that the \npitch-levels that appear in the key signature of the local \nkey are spelled accordingly, and the remaining pitches are spelled relative to these.  Key-invariance  is the \nassumption that the spelling of a pitch-level given its \nposition relative to the local tonic is independent of the local key.  This is in keeping with the fact that in well-\ntempered tuning, all keys of the same mode (major, for \nexample) have the same harmonic structure.   \n2. PITCH SPELLING \nPitch spelling serves two functions: to make printed \nmusic harmonically consistent, and to make it easy for a musician to read.  For discrete-pitch instruments (eg. \npiano), enharmonic discrepancies in spelling have no \neffect on intonation, but on a continuous pitch instrument (eg. violin) there is a subtle but audible \ndifference between enharmonically equivalent pitches.  \nThis means enharmonic errors can affect the intonation \nin machine-generated music,  but most human musicians \nwill automatically play the most harmonically appropriate enharmonic equivalent to what is printed.   \nPermission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that \ncopies are not made or distributed for profit or commercial \nadvantage and that copies bear this notice and the full citation on \nthe first page. \n© 2004 Universitat Pompeu Fabra.  \nFigure 1.  Possible spellings for different pitch-levels  \n \nThus, enharmonic discrepancies are rarely audible in \nhuman performance, but enharm onic errors detract from \nthe readability and correctne ss of a printed score. \nA musician needs to be able to read, digest, and \nanticipate musical ideas from a score in real time, so \nease in reading is very significant.  Ease in reading can \nmean different things in different contexts and to \ndifferent musicians, but in an overwhelming majority of cases there is a unique best spelling for each pitch.   \nSome of the most important considerations for \nthe spelling of a piece are harmonic consistency, voice-leading consistency, and notational parsimony.  \nHarmonic consistency  refers to the idea that chords have \nwell-defined functions and spellings.  For example, a C \nmajor triad consists of the pitches \nc, e, and g, and any \nother enharmonic spelling of this set of pitches would \nrefer to a functionally different chord.  The harmonic function of a chord is determined by its harmonic \ncontext.  In tonal music, chords have specific ways in \nwhich they are expected to appear and to resolve, and these expectations are held subconsciously by the \nlistener.  Thus, if the harmonic parse is known, the \nfunction and hence the spelling of each chord is uniquely determined (with a few exceptions, which will \nbe touched on later).  Harmonic consistency is generally \nthe most important consideration in pitch spelling as practiced by humans, and hence enharmonic \nequivalency literally means ‘equivalent up to harmonic \nfunctionality.’  Harmonic consistency is clearly only \napplicable to music with a traditional sense of tonality.   \nVoice-leading consistency  refers to the melodic \nfunctionality of each note within a single voice.  \nMelodic, or voice-leading functionality is a property of \nindividual pitches in relation to their immediate neighbors.  Contextual implications in voice-leading are \nnot as specific as they are for chords, so retrieving \nvoice-leading information from the melodic context is not as accurate as retrieving harmonic data from a \nharmonic parse.  Fortunately, voice-leading \nfunctionality is consistent with harmonic functionality, so harmonically consistent spellings automatically \nexhibit voice-leading consistency.  There are a few \nexceptions to this property, but in those cases the harmonic consistency prevails.  Thus, in conjunction \nwith harmonic consistency, voice-leading consistency \nneed only be concerned with the pitches not captured in the harmonic parse.  These cases are called non-chord \ntones  (NCTs), and their contextual implications are \nmore specific in general. \n  \nNotational parsimony  requires the spelling of a \npiece with a minimum of printed accidentals.  In theory, if harmonic information is available, this is \naccomplished by parsing the piece by tonal center (or \ntonic) such that a maximum of pitches can be spelled in the key-signature of the loca l tonic without accidentals.  \nIn general, this is not equivalent to spelling everything \nwith a natural (\n) whenever possible.  In practice, \nprinting a score directly from such a parse (ie. with an \nabsolute minimum of printe d accidentals) is likely to \nchange key signatures freque ntly enough to be awkward \nand obscure some of the gl obal structure of the piece, \nbut that issue lies outside the scope of this paper.           \nFor a given pitch-level there are several \npossible enharmonically equivalent spellings, so by \nnecessity, keys of the same mode built on enharmonically equivalent spellings are themselves \nenharmonically equivalent.  Since harmonic function is \ndefined relative to the tonic, spellings based on enharmonically equivalent keys are harmonically \nidentical, effectively nega ting concern over harmonic \nconsistency.  Thus, in the interest of readability and notational parsimony, it is reasonable to assume that any \nkey whose key signature exceed s 7 sharps or flats will \nbe spelled as an enharmonic equivalent with fewer \nsharps/flats.  For example, G\n major (8 sharps) would be \nspelled as A\n  major (4 flats).  \nFor a given key and for all pitch-levels that \nrequire accidentals when notated in that key (ie. pitch-\nlevels for which no spelling falls within the key signature) there are pitches one half-step in either \ndirection that do fall within the key signature.  Thus, it \nis reasonable to expect that  in the context of a key, \nevery pitch-level requiring an  accidental will be spelled \nas either its lower neighbor raised by a half-step or its \nupper neighbor lowered by a half-step.  Under these two \nassumptions, the space of possible spellings is limited to \nthose in Figure 1.   \nIf the local tonic is known, the space of \npossible spellings can be reduced further.  Given the \nlocal tonic, it is harmonically consistent to spell each pitch-level without accidental s (ie. according to the key \nsignature) whenever possible.  For example, in a G \nmajor passage MIDI pitch-level 66 would always be \nspelled as \nf\n instead of g\n, and in C major all pitch-\nlevels corresponding to white keys on the piano would \nbe given their natural (\n ) spellings, as in Figure 2. \n   \nCD\nC\nDE\n \nD\nEFG\n \nF\n G A\n \nG\n AB\nA\nB\n \nFigure 2.   Possible spellings in C major Pitch-Level: 60 61 62 63 64 65 66 67 68 69 70 71 \nSpellings: D\n \nC \nB\n D\n \nC\n E\n \nD \nC\n E\n \nD\n F\n \nE \nD\n G\n \nF \nE\n G\n \nF\n A\n \nG \nF\n A\n \nG\n B\n \nA \nG\n B\n \nA\n C\n \nB \nA\n Thus, given a harmonic parse, the space of \npossible spellings at any point in a piece is a subset of those in Figure 1, determined by the local tonic.  This \nmeans that with an accurate harmonic parse and under \nthe assumption of key-invariance, pitch spelling is reduced to a Boolean classification problem on the five \npitch-levels that cannot be spelled without accidentals.  \nIn C, they are the black keys on the piano.  These cases have fundamentally different functions relative to the \ntonic, and to capture that, the algorithm treats them as \nindependent classification problems.  Thus, it treats \nspelling \nc\n vs. d\n in C as fundamentally the same as \nspelling f\n vs. g\n in F, but different than spelling f\n vs. \ng\n in C. \nIt is important to notice that the harmonic \nfeature of interest here is the key signature  of the local \ntonic, not the local tonic itself, and not necessarily the \ncomposer’s key signature.  Thus relative keys (eg. C major and A minor) would fall under the same heading.  \nFor the remainder of the paper, local key  refers to the \nfamily of keys that share the same key signature.   \n3. ALGORITHM \nThe following algorithm is designed to determine which \nfeatures in the harmonic and/or melodic context of a note \nare helpful in recovering the accurate spelling information.   The algorithm takes MIDI data as input \nand generates a corresponding harmonic parse by \napplying the method described in [5].  To generate an accurate harmonic parse, this model requires that the \nMIDI data contain accurate rhythmic information.  A \nlocal key parse is then extracted from the harmonic parse.  For each note n with pitch-level p that must be \nspelled with an accidental in the local key, several \nfeatures are calculated.  La ter, these features will be \nevaluated to determine which are most informative.  \n \n1)  History Vector  (H[12]): the distribution of the \n12 MIDI pitch-values modulo the octave in a window of a pre-specified length immediately \npreceding n relative to the local key at  n.\n 1    \n \n2)  Future Vector  (F[12]): the distribution of the 12 \nMIDI pitch-values modulo the octave in a window of a pre-specified length immediately \nfollowing n relative to the local key at  n. \n   \nThese features are designed to capture general \ninformation about the harmonic context before and after \nn, relative to the local key at n.  The motivation here is \nthat the two spellings considered for pitch-level p are \nrelated harmonically to k via some number of steps along \nthe circle of fifths in opposite directions.  A pitch s \nrelates to a key harmonically in terms of the distance (a whole number) and direction (a Boolean value) stepwise \n                                                 \n1 More precisely, the tonic of the major key corresponding to the local \nkey signature at n \n along the circle of fifths from the local key to the closest \nkey for whom s is a member (ie. Spelled without an \naccidental).  The two spellings considered for p are \nraised and lowered versions of pitches in k, so they refer \nto sharpening (adding sharps or removing flats from k) \nand flattening (adding flats or removing sharps from k) \nmotion along the circle of fifths  respectively.   Thus, if a \nparticular pitch s is the correct spelling of p, it may be \nreasonable to see energy in  H and/or F corresponding to \npitches that lie between k and s on the circle of fifths.  \nFor example, a\n  is 3 steps in the flat direction from C \nmajor and g\n  is 3 steps in the sharp direction, so if H and \nF contain significant energy at pitch-levels corresponding to b\n (1 step flat) and e\n  (2 steps flat) but \nnot those corresponding to f\n  (1 step sharp) or c\n  (2 \nsteps sharp), this may imply a\n  is preferable to g\n .    \n \n3)  History Key Gradient  (∆KH): the average \ndifference and direction along the circle of \nfifths (expressed as a single floating point \nvalue) between the local key at n and the local \nkeys in a window immediately preceding n, \nweighted by rhythmic proximity to n.   \n \n4)  Future Key Gradient  (∆KF): the average \ndifference and direction between the local key at n and the local keys in a window \nimmediately following n, weighted by rhythmic \nproximity to n.   \n \nThese features are designed to capture \ninformation about the rate of change in local key upon \narriving at n.  The function of these features is similar to \nthat of H and F except that th ey are computed in terms of \nthe harmonic parse rather than pitch-level information. \n    \n5)  Resolution  (R): a ternary feature that attempts \nto capture voice-leading information about the \nresolution of n.  It scans the piece after n for the \nfirst appearance of pitch p+1 or p-1.  \nWhichever appears first determines R as +1 or -\n1 respectively.  If neither appears, or if they \nappear simultaneously, R is 0.   \n \nThis is by no means an exhaustive model for \nresolution in general, but it captures the majority of \ncases.  In particular, it is sufficient to capture resolution information in chromatic NCTs. \nGround-truth files containing the ‘true’ \nspellings are then given to decision tree software, which automatically generates the spelling algorithm using \ndifferent sets of the above features.  The ground-truth \nand MIDI data are both generated from MusicXML data.  MusicXML is a format designed as a link between \ndifferent formats for high-level  representation of music.  \nIn particular, it encode s the necessary spelling \ninformation and can be easily translated into MIDI.  \nUnfortunately, MusicXML is not yet widely used, and \nthere is currently not much data available in this format.    4. RESULTS \n4.1. Data, Priors and Results  \nThe algorithm was run on a set of 31 movements from \n15 chamber music pieces by various composers, divided \ninto training and test sets as per Table 2.  All the data is from Project Gutenberg [\nwww.gutenberg.net/music ], \ncurrently the only online archive of public domain sheet \nmusic in MusicXML format  to our knowledge.  \nThe reduction of pitch spelling to a key-\ninvariant Boolean classifica tion problem relies heavily \non having an accurate harmoni c parse.  The training \ndata was pruned to eliminate pathological cases \nresulting from an imperfect harmonic parse.  The test data was not pruned. \n Table 1 shows the prior distributions\n2 of all \npossible spellings in the training and test data respectively on each of the B oolean cases (ie. requiring \naccidentals).  Each of these cases is classified according \nto its possible spellings in C major, though it is important to remember that these cases do not have the \nsame spellings when they appear in other keys. \n   \nTrain c\n/d\n d\n /e\n f\n /g\n g\n /a\n a\n /b\n \nRaised 200 95 309 779 14 \nLowered 7 57 0 77 254 \nTotal 207 152 309 856 268 \nPrior 96.6% 62.5% 100% 91.0% 94.8%\n \nTable 1a.   Distribution of spellings on the cases requiring \naccidentals in the training data  \n \nTest c\n/d\n d\n /e\n f\n /g\n g\n /a\n a\n /b\n \nRaised 139 83 291 568 14 \nLowered 12 71 0 34 183 \nTotal 151 154 291 602 197 \nPrior 92.1% 53.9% 100% 94.4% 92.9%\n \nTable 1b.   Distribution of spelli ngs on the cases requiring \naccidentals in the test data \n \nUnfortunately, f\n/g\n is a degenerate case in \nthis data as it is always spelled as the raised fourth scale \ndegree (rather than the lowered fifth).  The raised fourth \nscale degree is more closely related to the home key than the lowered fifth via the circle of fifths (1 step \nsharp vs. 5 steps flat), so a bi ased prior was expected.  It \nis unlikely, though, that the lowered fifth scale degree never appears in practice. \n \n  \n                                                 \n2 The prior distribution  on a scale degree is the relative frequency of \none spelling over the other in the data set. Composer Piece Training Test \nBach, J. S. BWV 1047 \nBWV 1050 I – II \n- - \nII \nBeethoven  Op. 18  No. 1 \nOp. 59  No. 2 Op. 59  No. 3 III \n- - - \nIV \nIII \nBrahms Op. 51  No. 1 II - \nHaydn Op. 1  No. 1 \nOp. 74  No. 1 \nOp. 74  No. 2 II – III \nI & III \n- V \n- \nII \nMozart, W. A. K. 80 \nK. 155 \nK. 156 \nK. 458 I – II \nI – II \nI – II \nII III \nIII \nIII – IV \nIII – IV \nSchubert  Op. 125  No. 1 I & III IV \nSchumann, R. Op. 41  No. 1 II – III - \n \nTable 2.   Training and Test data \n \n Of the four non-degenerate cases, Table 3 \nshows the accuracy of the algorithm on the test data \nunder several sets of features.  For the first three , the \nresolution feature was by far the most informative.  For \na\n/b\n, resolution was not informative, but the other \nfeatures produced a slight win over the prior.   \n \nFeature set c\n /d\n d\n /e\n g\n /a\n a\n /b\n \nAll Features 92.1% 70.8% 94.4% 93.4% \n∆KF, ∆KF, R  92.1% 81.2% 94.4% 94.4% \nR only 92.7% 82.5% 96.0% 92.9% \nPriors 92.1% 53.9% 94.4% 92.9% \n \nTable 3.   Accuracy of the algorithm under several feature sets   \n4.2. Special Cases that Result in Misspellings \nOf the cases that were missed, many resulted from a few \nspecific phenomena.  This al gorithm is not currently \ncapable of recognizing these cases, but they are few and \nspecific enough that this type of algorithm could likely be developed to handle most of them.  Table 4 shows a \nbreakdown of misspellings under the most accurate \nfeature set for each scale degree.  Non-chord tones (NCTs), augmented sixth chords (6+), and fully-\ndiminished seventh chords (o 7) are specific, mutually \nexclusive cases, each with a well-defined melodic or \nharmonic function.   \n \nMisspellings c\n/d\n d\n /e\n g\n /a\n a\n /b\n \nTotal  11 27 21 11 \nNCT 0 10 1 11 \nAug. Sixth 0 7 0 0 \nFully-Dim. 7 0 10 14 0 \n \nTable 4.   Total misspelled cases and the subsets thereof \nresulting from specific functionalities     4.2.1.  Non-Chord Tones \nNon-chord tone refers to one of a family of cases in \nwhich a note is considered to have no harmonic function \n(ie. it is not considered part of the concurrent chord), but \nhas a specific melodic function and spelling.  Figure 4 is an example of NCTs from Mozart K.458.  NCTs can be \neither diatonic (falling within the local key) or \nchromatic (falling outside the local key).  Clearly, the cases of interest here are chromatic NCTs.  The spelling \ncaptured by the resolution feature (R) is consistent with \nthe melodic function of chromatic NCTs, though it is \nnot a perfect predictor for this data. \n \n \n \nFigure 4.  NCTs in Mozart, K. 458 mvt. IV Allegro Assai \n4.2.2.  Augmented Sixth Chords   \nThere are several varieties of augmented sixth chords, \nbut they all contain the 1st, lowered 6th and raised 4th \nscale degrees ( c, f\n, and a\n in C major).  They get their \nname from the augmented sixth interval formed between \nthe lowered 6th and raised 4th scale degrees.  Augmented \nsixth chords tend to resolve (harmonically) to the dominant ( V) chord with the lowered 6\nth and raised 4th \nresolving (melodically) outward by a half-step, each to \nthe 5th as implied by the spelling, and the 1st resolving \ndown by half-step to the 7th.  In practice, however, \naugmented sixth commonly resolve to other dominant \nfunction chords like the cadential 6-4 chord or the dominant seventh ( V\n7) as in Figure 5.  In these cases, \none or more of the tones in the augmented sixth chord \ndo not resolve as expected.  These are examples of cases in which the determining feature for the spelling of a \npitch is a resolution that is expected , but not achieved.  \n \n \n \nFigure 5.  Common resolutions of a German augmented sixth \nchord \n \n \n In addition to its voice-leading functionality, \nthe augmented sixth in the spelling of an augmented sixth chord also serves to eliminate ambiguity regarding \nits harmonic function.  Using an enharmonically \nequivalent spelling, an augmented sixth chord can look like a dominant seventh function chord (in a different \nkey) as shown in Figure 6.  Thus, it is very important to \nthe harmonic legibility of a piece that augmented sixth \nchords are spelled correctly. \n \n \n \nFigure 6.   Enharmonically equivalent spellings \n4.2.3.  Fully-Diminished Seventh Chords \nFully-diminished seventh chords, in contrast to the vast \nmajority of functional chords, are not always well-\ndefined in terms of spelling.  For most chords, the root \nstructure is orientable in any inversion.  For example, in a major triad, the root is the same pitch in every \ninversion.  For fully-diminished seventh chords, on the \nother hand, inversions are indistinguishable from each \nother, and the root is not uniquely identifiable, as shown \nin Figure 7.  Technically, a unique best spelling for a \nfully-diminished seventh chord can be determined by the voice-leading of the individual parts, but in practice, \ncomposers often spell fully diminished sevenths \narbitrarily.   \n \n \n \nFigure 7.  Enharmonically equivalent  fully-diminished seventh \nchords \n \nFor c\n/d\n, d\n/e\n and g\n/a\n, the best results \nwere achieved by spelling directly according to the \nresolution feature (R).  Thus, all of the errors made by \nthe algorithm in these cases  resulted from misleading \nvalues of the resolution feature (R).  These cases are \nbroken down in Table 5.  \n \nMisleading R  c\n/d\n d\n /e\n g\n /a\n Totals \nTotal  11 27 21 59 \nExpected Res. 1 15 7 23 \nOctave Problem 2 1 5 8 \nUncapturable 8 11 9 28 \n \nTable 5.   Misspellings due to misleading R values      \n \n 4.2.4.  Expected Resolution \nIn some cases, the determin ing feature for the spelling \nof a pitch is its expected resolution, which may never \nactually be achieved.  If this is the case, it is a result of \nthe harmonic progression and not melodically motivated.  Some common examples of this \nphenomenon involving augmented sixth chords are \nshown in Figure 5.  Theoretically, these cases should be able to be captured by the harmonic parse. \n4.2.5.  Octave Problems \nThe way the R is calculated, the resolution of a pitch p \nis only captured if it occurs in the same octave as p.  In \nsome cases, a tendency tone is passed between different octaves before it is resolved, and the resolution only \noccurs once.  Thus, some cases were missed because the \nresolution fell in the wrong octave.  For example, in \nFigure 8, the \ng\n in the Viola part in measure 46 is \nresolved to a\n in the Violin II part, but the resolution \nfeature captures the g\n in measure 48. \n \n \n \nFigure 8.   Excerpt from Beethoven, Op. 59 “Razumovsky” \nNo. 2  mvt. IV Presto    \n4.2.6.  Uncapturable Cases \nAll of the special cases mentioned above are capturable \nunder the assumptions of this algorithm.  The last category in table 5 refers to all remaining cases in which \nthe spelling is inconsistent with the assumptions of this \nalgorithm.  This includes error caused by ambiguity of spelling in fully diminished seventh chords, among \nother things.  Interestingly, upon comparing these cases \nagainst published scores, 26 of the 28 cases in this \ncategory were discovered to be errors in the MusicXML \nspelling data.           \n  \n 5. CONCLUSIONS    \nOverall, this algorithm accurately spelled 94.98% \n(misspelled 70 of 1395) of the cases requiring accidentals in the local key and 99.686% (misspelled 71  \nof 22,593) total on all notes in the test data.  The results \nspeak for a strong dependence of spelling information on voice-leading resolution, although the quality of the \noutcome was limited by scarce and imperfect ground-\ntruth data.  Also, the vast majority of misspellings generated here can be accounted for in terms of a few \ntractable cases, so the level of accuracy achieved by this \ntype of algorithm has room to improve dramatically.   \nACKNOWLEDGEMENTS \nThis work is supported by NSF grant ISS-0113496. \nREFERENCES \n[1] Breiman, L., Friedman, J., Olshen, R. and \nStone, C. Classification and Regression Trees.  \nWadsworth, 1984. \n[2] Cambouropoulos, E. “Automatic Pitch \nSpelling: From Numbers to Sharps and Flats”, \nProceedings of the VIII Brazilian Symposium \non Computer Music, Fortaleza, Brazil, 2001.  \n[3] Chew, E., Chen, Y. “Determinimg Context-\nDefining Windows: Pitch Spelling using the Spiral Array”, Proceedings of the Fourth \nInternational Conference on Music \nInformation Retrieval,  Baltimore, Maryland, \nUSA, 2003. \n[4] Hewlett, W. “A Base-40 Representation of \nMusical Pitch Notation”, CCARH Publications, Stanford, CA, 1986. \n[5] Meredith, D. “Pitch Spelling Algorithms”, \nProceedings of the 5\nth Triennial ESCOM \nConference, Hannover, Germany, 2003. \n[6] Raphael, C., Stoddard, J. “Harmonic Analysis \nwith Probablilistic Graphical Models”, Proceedings of the Fourth International \nConference on Music In formation Retrieval,  \nBaltimore, Maryland, USA, 2003."
    },
    {
        "title": "Search Effectiveness Measures for Symbolic Music Queries in Very Large Databases.",
        "author": [
            "Craig Stuart Sapp",
            "Yi-Wen Liu",
            "Eleanor Selfridge-Field"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417913",
        "url": "https://doi.org/10.5281/zenodo.1417913",
        "ee": "https://zenodo.org/records/1417913/files/StuartLS04.pdf",
        "abstract": "In the interest of establishing robust benchmarks for search efficiency, we conducted a series of tests on sym- bolic databases of musical incipits and themes taken from several diverse repertories. The results we report differ from existing studies in four respects: (1) the data quantity is much larger (c. 100,000 entries); (2) the levels of melo- dic and rhythmic precision are more refined; (3) anchored and unanchored searches were differentiated; and (4) re- sults from joint pitch-and-rhythmsearches were compared with those for pitch-only searches. The search results were evaluated using a theoretical approach which seeks to rank the number of symbols re- quired to achieve “sufficient uniqueness”. How far into a melody must a search go in order to find an item which is unmatched by any other of the available items? How much does the answer depend on the specificity of the query? How much does anchoring the query matter? How much does the result depend on the nature of the reper- tory? We offer experimental results for these questions.",
        "zenodo_id": 1417913,
        "dblp_key": "conf/ismir/StuartLS04",
        "keywords": [
            "search efficiency",
            "sym-bolic databases",
            "musical incipits",
            "themes",
            "melodic and rhythmic precision",
            "anchored and unanchored searches",
            "joint pitch-and-rhythm searches",
            "pitch-only searches",
            "theoretical approach",
            "sufficient uniqueness"
        ],
        "content": "SEARCH-EFFECTIVENESS MEASURES FORSYMBOLIC MUSIC\nQUERIESINVERYLARGEDATABASES\nCraigStuartSapp\nCCRMA andCCARH\nDept.ofMusic\nStanfordUniversity\nStanford, CA94305-3076, USAYi-WenLiu\nCCRMA\nDept.ofElectrical Engineering\nStanfordUniversity\nStanford, CA94305,USAEleanorSelfridge-Field\nCCARH\nDepts.ofMusic,Symbolic Systems\nStanfordUniversity\nStanford, CA94305-3076, USA\nABSTRA CT\nIntheinterest ofestablishing robustbenchmarks for\nsearch efﬁcienc y,weconducted aseries oftests onsym-\nbolic databases ofmusical incipits andthemes takenfrom\nseveraldiverse repertories. The results wereport differ\nfrom existing studies infourrespects: (1)thedataquantity\nismuch larger(c.100,000 entries); (2)thelevelsofmelo-\ndicandrhythmic precision aremore reﬁned; (3)anchored\nandunanchored searches were differentiated; and(4)re-\nsults from joint pitch-and-rhythm searches were compared\nwith those forpitch-only searches.\nThe search results were evaluated using atheoretical\napproach which seeks torank thenumber ofsymbols re-\nquired toachie ve“sufﬁcient uniqueness”. Howfarintoa\nmelody must asearch goinorder toﬁnd anitem which\nisunmatched byanyother oftheavailable items? How\nmuch does theanswer depend onthespeciﬁcity ofthe\nquery? Howmuch does anchoring thequery matter? How\nmuch does theresult depend onthenature ofthereper -\ntory? Weofferexperimental results forthese questions.\n1.REPER TORY\nThe musical data used foranalyses inthispaper arede-\nrivedfrom Themeﬁnder1which contains afamily ofdata-\nbases encoded intheHumdrum format.2Unlik eMIDI\ndatainwhich some pitch-data compared tographical nota-\ntionisambiguous, thisformat provides explicit pitch and\nrhythm descriptions forallnotes. This enables evaluation\noftheimportance ofdistinguishing between enharmonic\nspellings (e.g.,G\n\u0000vs.A \u0001)aswell astheimportance ofa\nnote’ smetric position.\nThemain purpose oftheThemeﬁnder website istoen-\nable trained musicians toidentify works bytheir melodies\n1http://www .themeﬁ nder.org\n2http://dactyl.som.ohio-stat e.edu/Hu mdrum\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forproﬁ torcommercial advantage andthat\ncopies bear thisnotice andthefullcitation ontheﬁrstpage.\nc\n\u00022004 Universitat Pompeu Fabra.Dataset GenreOrig.Code#Incipits\u0003USRISM\nA/IIInstrumental,\nVocal (17th–\n18th cents.)Plaine\n&Easie55,490\u0003Renaissance\n(Italy)*Motets\n(16th cent.)DARMS 18,946\u0003Classical* Instr.,Vocal MIDI 10,718\u0003Essen\nEuropean*Folksongs EsAC 6,232\u0003Polish\nreligious\nmonophon yDevotional\nsongs, 16th,\n19th cents.EsAC 6,060\u0003Essen\nAsian*Folksongs\n(China)EsAC 2,241\u0003Luxembour g*Folksongs EsAC 612\nTotal 100,299\nTable 1.Constituent databases used foranalysis. Starred\nitems arepublicly searchable.\nasremembered. Users areassumed tobenotationally lit-\nerate, since theyaremost likelytoseek awork-title. That\nis,theyareseeking textual metadata from asymbolic-data\nsearch. Results areviewable innotation andplayable as\ncorresponding MIDI ﬁles. The length ofthequeries is\natthediscretion oftheuser.The constituent collections\n(some publicly searchable, others limited tolicensed use)\neach represent adifferent kind ofmusic (Table 1).\nThe repertories varysubstantially bymusical mode.\nSome collections aretonal, some modal, andoneispen-\ntatonic. Within thetonal collections, signiﬁcant range can\nbefound with respect todiatonic, chromatic, and(occa-\nsionally) enharmonic usage. The Essen-European, Clas-\nsical, andRISM datasets areoverwhelmingly tonal. The\nRenaissance Italian database emplo ysmodes ofthepe-\nriod; thePolish data contains twosubsets andisalmost\nevenly divided between modal andtonal monophon y.The\nEssen-Asian dataset ispentatonic. Pentatonicism (theuse\nofﬁvetones peroctave)makesscale-de gree searches am-\nbiguous, since mapping ﬁve-tone proﬁles onto seven-tone\ngrids yields inevitable differences inscale-de gree usage.\nSince wewere interested incomparing procedures and\ntheir effectiveness indifferent repertories, wedidnotat-\ntempt tocorrect forthisdistortion. TheRenaissance reper -tories subscribe todifferent systems ofrhythmic organi-\nzation than what isconventional incommon music nota-\ntion. This complicates theinvestigation ofmultiple tiers\nofprecision inrhythmic deﬁnition. Theclassical dataset\nisoverwhelmingly instrumental, while thePolish data is\nexclusi velyvocal.\nNone ofthecomponent datasets inThemeﬁnder orig-\ninated asHumdrum data, which represents pitch, dura-\ntion, barring, andtheglobal variables ofnotated music\n(meter signature, keysignature, clef, etc.).However,All\ntherepertories were originally encoded atalevelofdetail\nsufﬁcient tosupport translation into Humdrum, orwere\nsigniﬁcantly manually edited inthecase ofMIDI transla-\ntions. Thetotal number ofrecords used inthisstudy was\n100,299.\nThe useofmusical incipits raises fundamental ques-\ntions ofmusical identity .Allincipits aremonophonic as\nused inThemeﬁnder ,butsome oftheunderlying reperto-\nriesarepolyphonic. Inrelation topolyphonic music, Lin-\ncoln [6]givesoneincipit foreach voice (typically ﬁve).\nRISM givestheincipit (usually from thehighest-pitched\ninstrument orvoice, e.g.,Violin 1orSoprano) butisgen-\nerous ingiving separate incipits forlinkedportions ofa\nsingle movement (e.g.,instrumental ritornello andaria).\nAqualitati vedifference distinguishes incipits from\nthemes. Incipits introduce asong, work, ormovement.\nTheyservewell forshort works thatareuncomplicated.\nThemes represent apiece ofmusic (usually alonger ,more\ncomple xone) insome more essentially cogniti veway.\nThe mental extrapolation ofthemes isahuman task and\ntherefore vulnerable tosubjecti vevariation. Asbest we\nknow,there isnostudy ofthevariation thatmight befound\nbymultiple subjects intheidentiﬁcation ofthematic ma-\nterial.\nInconstructing this analysis, wegaveconsideration\ntotherelationship (orlack thereof) between incipits (in\nmonophonic conte xts)andoverall pieces (wedidnotdis-\ntinguish between incipits andthemes perse).Wesub-\nsequently compared search results forincipit data com-\npared tofull-w orkdata fortheEssen folksong collection\n(inwhich central European music predominates).\n2.EXISTING STUDIES\nInsearching forrelated literature, wefound fewstudies\nwhich were systematic innature andwhich addressed sub-\nstantial quantities ofdata, although manystudies touched\nonsome aspect ofthisgeneral area ofinquiry .McNab et\nal.[7],Dannenber getal.[2],andRand andBirming-\nham [8]explored similar procedures butinrelation toa\nquery-by-singing situation.\nIn[8]188MIDI ﬁles were used asabasis forproﬁl-\ningdurational change, pitch change, and“note-drop” in\nanefforttosimulate thekinds ofuser errors anticipated\ninsung input. Theynoted thatcorrelation coefﬁcients and\ncount correlations performed equally well inprocessing\nandcombined them inamodiﬁed scoring metric. In[2]\nonedatabase of2,844 items wasgenerated from aMIDIcollection ofBeatles songs andasecond, of8,926 themes\n(averaging 41notes), wasbased onanencoded collection\nofsongs. Here theycompared theresults ofmetrics de-\nrivedfrom (1)pitch plus inter-onset interv als(IOI), (2)\nmelodic contour matches, and(3)Hidden Mark ovMod-\nels.In[7]asimilar study toours using theEssen database\nwasconducted aspartofresearch forsymbolic-music re-\ntrievalviasinging.\nTheclosest parallels with ourownworkinpurely sym-\nbolic searching arefound in[4],[9],[10],and[5].The\nﬁrsttwowere concerned with sorting records stored inre-\nlated symbolic databases intoamusical equivalent ofal-\nphabetical order forbibliographical purposes (e.g.,ﬁnding\nconcordances forworks which areanon ymous orwhich\nareattrib uted tomultiple composers). The authors of\n[10]sought todetermine thefeasibility ofthequery-by-\nhumming approach bysimulating some oftheknownde-\nﬁciencies inuser input. Theyconsidered melodic rep-\nresentation atﬁvelevelsofpitch-resolution, although it\nisunclear exactly how“interv als” were deﬁned atsome\nlevels(inthecategories 3,5,7,9,12)ofpitch resolu-\ntion. Theyattempted tosimulate different levelsofinac-\ncurac yinsung queries. Theyreported results fordifferent\ndatabase sizes (of0.6,1.2,2.5,and3.6million notes) and\nsearch-k eylengths (4–8 tokens). Theyfound thatathree-\ninterv alcontour required a1.7longer query-length than a\nsemitone resolution. Theyreported that5-state, 7-state,\nand9-state representations oftheunderlying melodies led\ntosimilar results butinallcases produced impro vement\noverthree-state representations. Theauthors of[5]exam-\nined theeffectiveness ofusing rhythm andpitch ofnotes\ninparallel forsearch queries.\n3.DATAFEA TURES\nCurrently Themeﬁnder allowssearching formusic exam-\nples byseverallevelsofprecision, going from veryspe-\nciﬁc (exact pitch) toverygeneral (gross contour). How-\never,forthisstudy thedescripti vefeatures were expanded\ntoinclude rhythm aswell toexamine which rhythmic fea-\ntures may beofbeneﬁt when searching inThemeﬁnder .\nFourteen symbolic features ofmusic were examined in\nthisstudy—se venforpitch andsevenforrhythm. Here\nisalistofthesevenpitch features extracted from themu-\nsical data which areordered from speciﬁc togeneral:\np1Enharmonic pitchclass :Pitches arenamed bydia-\ntonic letter (A..G) andinﬂection: natural ( \u0000),sharp\n(\n\u0000),ﬂat( \u0001),double-sharp (x),ordouble-ﬂat ( \u0001 \u0001).\np2Musical interval :Thedistance between twopitches\nspeciﬁed bymelodic direction, diatonic size (3rd,\n5th,etc.), and“quality”: perfect, major ,minor ,aug-\nmented, ordiminished. Table 2givesalistofthe35\nbasic interv alsperoctaveconsidered.\np3Twelve-tone pitchclass :12pitches asemitone apart\nperoctavewhich map one-to-one onthemusical\nkeyboard.p4Twelve-tone interval :The distance between two\nsuccessi vetwelv e-tone pitches interms ofsemi-\ntones.\np5Scale degree:Pitches aredescribed bytheir diatonic\nposition ina7-tone scale (e.g.,major orminor).\np6Grosscontour :Interv alsareassigned tooneofthree\ncategories bymelodic direction (up, down,orthe\nsame).\np7Reﬁned contour :Interv alsareassigned tooneof\nﬁvecategories: 2“steps” (pitch changes ofanytype\nofsecond interv al)upordown, 2“leaps” (pitch\nchanges ofminor thirds orgreater) upordown,or\nunchanged pitch.\nnamecodenamecode\nC \u0000 0\nC \u0001 1 C \u0002\u0003\u0002 39\nCx 2 C\u000238\n. 3 Bx 37\nD \u0002\u0003\u0002 4 B \u000136\nD \u0002 5 B \u000035\nD \u0000 6 B \u000234\nD\u0001 7 B\u0002\u0003\u0002 33\nDx 8 . 32\n. 9 Ax 31\nE \u0002\u0003\u0002 10 A \u000130\nE \u000211 A \u000029\nE\u000012 A\u000228\nE \u000113 A \u0002\u0003\u0002 27\nEx 14 . 26\nF \u0002\u0003\u0002 15 Gx 25\nF \u000216 G \u000124\nF\u000017 G\u000023\nF \u000118 G \u000222\nFx 19 G \u0002\u0003\u0002 21namecodenamecode\nper1 0 per8 40\naug1 1 dim8 39\naaug1 2 ddim8 38\n(ddim2) 3 aaug7 37\ndim2 4 aug7 36\nmin2 5 maj7 35\nmaj2 6 min7 34\naug2 7 dim7 33\naaug2 8 (ddim7) 32\n(ddim3) 9 aaug6 31\ndim3 10 aug6 30\nmin3 11 maj6 29\nmaj3 12 min6 28\naug3 13 dim6 27\naaug3 14 (ddim6) 26\nddim4 15 aaug5 25\ndim4 16 aug5 24\nper4 17 per5 23\naug4 18 dim5 22\naaug4 19 ddim5 21\nTable 2.The35pitch-names peroctaveandmusical inter-\nvalsused inp1(pch)&p2(mi)Codes giveninabase-40\nenumeration [3].Interv alsinparentheses notconsidered.\nWehaveobserv edthat exact-pitch searches penalize\nfaulty recollection andthatgross-contour searches often\nproduce anexcessi venumber ofprospecti vematches. We\nhavecome tobelie vethatscale-de gree searches areover-\nallthemost robustforsearching fortonal music incipits.\nScale-de gree searches tieforthemost-used search type on\nThemeﬁnder ,along with theexact-pitch search follo wed\ninthird place bygross contour .3\nTwo-tothree-letter abbre viations used inthispaper for\neach pitch andrhythm feature aregiveninTable 3.Inad-\ndition tothesevenpitch features described above,seven\nrhythm features were alsoextracted from themusical data\nforthisstudy .Rhythm canbecategorized intotwobasic\ncomponents ofduration andmeter .Three levelsofdu-\nration andfour levelsofmetric features aregiveninthe\nfollo wing list:\nr1Duration :theduration ofnotes inthemusical score\n(notthesame asperformance duration). Also called\ninter-onset interv alinperceptual studies.\n3http://ismir2003.ismir .net/t utorials/Sa ppﬁchiers/slide0025.htmAbbr.Searchtype #states\np1pch enharmonic pitch class 35\np2mi musical interv al (35)\np312p 12-tone pitch class 12\np412i 12-tone pitch interv al (12)\np5sd scale-de gree (diatonic pitch class) 7\np6pgc pitch gross contour 3\np7prc pitch reﬁnedcontour 5\nr1dur duration ?\nr2dgc duration gross contour 3\nr3drc duration reﬁnedcontour 5\nr4blv beat level 2\nr5mlv metric level ?\nr6mgc metric gross contour 3\nr7mrc metric reﬁnedcontour \u0004\nTable 3.Symbolic music features. Numbers inparenthe-\nsesnotlimited tooneoctaveinthestudy .States listed\nwith “?”arenotenumerable without aspeciﬁc database\n(seeTable 4).\nr2Duration grosscontour :Thefollo wing note ischar-\nacterized asbeing (1)longer ,(2)shorter or(3)equal\ninduration tothecurrent note.\nr3Duration reﬁned contour :Areﬁned version ofthe\nrhythmic gross contour where longerandshorter\narefurther split into twosub-cate gories: (1)next\nnote is\u0005\u0007\u0006\t\b aslong, (2)nextnote\n\u0007\u0006\u000b\b aslong,\n(3)nextnote issame duration, (4)nextnote is\n\f\u0006\u000b\b\nasshort, and(5)nextnote is\u0005\r\u0006\t\b asshort.\nr4Beat level:Agross metric description ofnotes be-\ningeither (1)onthebeat, or(2)offthebeat.\nr5Metric level:Areﬁned metric description ofthe\nmetric position (inaparticular meter). Forexam-\nple,beats could beonthequarter -note level,8th-\nnote off-beats would bethe8th-note level.\nr6Metric grosscontour :qualitati vedifferences be-\ntween metric levelsofadjacent notes: (1)nextnote\nisonastronger ,(2)weak er,or(3)equivalent metri-\ncalposition.\nr7Metric reﬁned contour :reﬁned qualitati vediffer-\nences between metric levelsofadjacent notes. Met-\nricchanges placed inﬁvecategories, where thenext\nnote isinametric position which is:(1)weak er,(2)\nmuch weak er,(3)stronger ,(4)much stronger ,or(5)\nthesame asthecurrent note.\nTheThemeﬁnder search-engine alsoallowsusers toﬁl-\nterbymeter -signatures, butitcurrently offersnosearches\nofrhythmic features onanitem-per -item basis aslisted\nabove.Figure 1demonstrate theextraction ofvarious fea-\nturedata from amusical incipit.\nInactual datasets, notallpossible feature states are\nused. Weﬁnd that thenumber ofstates encountered at\nhigher -orders ofpitch precision varies from repertory to\nrepertory .Also, interv allic searches athigher levelsof\nprecision arecomputed with discrete octaves(i.e.,thein-\ntervalofaperfect twelfth isnotequated with that ofa\nperfect ﬁfth), sotherange ofactual feature states variespch\nmi\n12p\nsd12i\npgc\nprc\ndur\ndgc\nblv\nmlv\nmgc\nmrcdrcF\n5\n1\nE\n1\n0+M3\n+4\nWU\nEU\nwE\n-1A\n9\nE\n03+m3\n+3\nU\nU\nL\nH\nHlC\n0\n5\nQ\n1\n2P1\nW0\nS\nS\nE\nwEC\n0\n5\nQ\n1\n0P1\n0\nS\nS\nE\nHE\nhC\n0\n5\nQ\n1\n1+M2\n+2\nWU\nu\nE\nwED\n2\n6\nQ\n1\n0-M2\n-2\nH\nHD\nd\nL\nlC\n0\n5\nQ.\n1\n2-m3\n-3\nW\nWD\nD\nS\nS\n-1A\n9\n3\nE\n0-M3\n-4\nD\nD\nL\nH\nHlF\n5\n1\nQ\n1\n1+M3\n+4\nU\nU\nE\nWE\nwA\n9\n3\nQ\n1\n0-M2\n-2\nD\nd\nL\nH\nHlG\n7\n2\nH\n1\n2\nFigur e1.Example music along with extracted pitch and\nrhythm features.\nSearchTypeClassical PolishAll\n12i 70 40 88\npch 29 26 32\nmi 95 52 109\ndur 109 29 122\nmlv 10 12 14\nTable 4.Numbers ofstates forvariable features.\nwith thedatabase andthesymbol extraction method. Ta-\nble4showsvarious experimental state counts forselected\nrepertories.\n3.1. Sear ching Procedur es\nWeused theThemeﬁnder data collections directly toavoid\nrestrictions oftheweb interf aceandtoprocess data more\nquickly with aspecialized searching program written in\nCwhich stored thedata inmemory between searches.\nAnchored searches ofthedatabase (searches thatmatch\nfrom thestart ofanincipit only) could bedone ata\nrate of500/second with 100,000 incipits ona1.5GHz\ncomputer .Fully examining aparticular musical feature’ s\nsearch charactistics ontheentire database requires about\nonemillion database queries, taking about 1/2ofanhour\ntocomplete.\nUnanchored searches (searching starting atanyposi-\ntionintheincipit aswell asthebeginning) required longer\nsearch times ofabout 5searches/second on1.8million\nnotes inthecombined database, with search statistics tak-\ningalmost three days perfeature tocollect. Anchored\nsearches aresuitable forincipits. Unanchored searches\nmay bemore useful forﬁnding themes incomplete works.\nWeransearch tests oneach ofthevarious pitch and\nrhythm feature setslisted inTable 3.Withthose ﬁgures in\nhand, wethen ranasetofjoint pitch-rhythm feature search\ntests touseforcomparison. Wealso tested theresults for\nincipits against theresults forfull-score searches inthe\nEssen dataset. Our purpose wastodetermine theextent\ntowhich thetests weranonincipits would bevalidfor\ncomplete works (Table 6).4.EFFECTIVENESS MEASURES\nThree related concepts were developed foranalyzing\nsearch characteristics: (1)Match Count Proﬁles, (2)Time-\nto-Uniqueness andtoSufﬁcienc y,and(3)various entrop y\nmeasurements. These concepts aredescribed indetail in\ntheeach ofthefollo wing subsections.\n4.1. Match Count Proﬁles\nPerforming asearch with ashort query sequence yields\nmore matches inthedatabase than longer query se-\nquences. Therefore, aMatc hCount Proﬁle wasdeveloped\ntoexamine thebeha vior ofthematch counts asthequery\nlength isincreased. Forexample, Table 5showsthean-\nchored andunanchored Match Count Proﬁles forthemu-\nsicinFigure 1using scale-de gree features.\nQuery Length 1 2 34567891011\nAnchored 2493 566 288 126 641812 5421\nUnanchored 8399 4834 1772 626 259 563610621\nTheoretical 1643 304 64 14 31\nTable 5.Match Count Proﬁles formusic inFigure 1.\nInTable 5,thequery length indicates thenumber of\nelements used inthesearch starting from theﬁrstnote in\nthemusic. Forexample, thelength-four query is“135\n5”.This query resulted in126total songs starting with the\nexact same pattern, and626songs which contain theexact\nsame pattern anywhere inthesong.\nThe “theoretical” line atthebottom ofthetable in-\ndicates aprediction ofhowmanyanchored matches the\nquery isexpected togenerate giventheprobability distri-\nbution ofsymbols inthedatabase which will beexam-\nined intheentrop ysection infurther detail. Table 5also\ndemonstrates thatthequery length needed toﬁndaunique\nmatch isthesame whether thesearch wasanchored atthe\nstart ofthemusic orallowed tostart anywhere inashort\nmonophonic song.\nAfter generating individual feature proﬁles forall\nthemes inthedatabase, thesimilar -feature proﬁles were\naveraged andcanbeseen inFigure 2.This ﬁgure shows\nproﬁles foreach ofthe14pitch andrhythm features ex-\ntracted from music inthedatabase. Succinct feature sets\ndisplay steeper slopes onthecurves.Allpitch features\nﬁnd unique matches inthedatabase byabout 12-length\nqueries; howeverrhythm features require much longer se-\nquences touniquely ﬁndmatches inthedatabase.\nThetworhymic features, blvandmgc areexception-\nallybadatﬁnding unique matches. Themgc feature has\nanunusually gradual slope when compared tomrcbut\nitstillseems tobecorrect after careful double-checking\nofthedata andmay justbeapeculiarity ofthefolksong\ndataset.\n4.2. TTU andTTS\nWenowdeﬁne twoexperimental measures ofimportance\nintheMatch Count Proﬁles:PSfrag replacements\n12\n34\n56\n78\n910\n1112\n1314151617181920\nsearch length2\n24\n46\n68\n810\n1012\n12match count (\n\u0000\n\u0001\u0002\n\u0003)Match Count Proﬁles\nEssen dataset (8,473 songs)\nTTS\u0004\u0006\u0005blvmgc\ndgc\nmlvdurmrcdrc pgcprcsdpch=12pmi=12i\nFigur e2.Proﬁles forthecomplete Essen dataset forall\n14pitch andrhythm features. Gray =duration features;\ndashed =metric features; thinsolid =pitch features.\nIncipit only Fullwork\nAnchored TTU (mi) 6.87619 8.74826\nseach Failure rate 0.669% 0.0354%\nUnanchored TTU (mi) 7.926 10.2858\nsearch Failure rate 1.07% 0.0472%\nTable 6.Comparison ofTTU forincipits andforthefull\nworks from which theyoriginate (based onEssen dataset).\u0007Time-to-uniqueness (TTU): thequery length re-\nquired toﬁndaunique match inthedatabase.\u0007Time-to-suf ﬁcienc y(TTS): thequery length re-\nquired toﬁnd anupper limit tothenumber ofre-\nsulting matches.\nFigure 2marks theTTS levelfortheupper -limit of\nmatch counts when itissetto10matches, aswedoso\narbitrarily throughout thisstudy .Asitturns out, TTS is\npreferable toTTU when calculating theentrop yinthefol-\nlowing section where theinitial slope oftheproﬁle isused\ntoestimate theentrop yrate. Toverify thisobserv ation, ex-\namine thepitch feature curvesinFigure 2.Thestart ofthe\ncurvesarenearly linear onalogarithmic scale, butwhen\napproaching theTTU point theysigniﬁcantly ﬂatten out.\nTheconvergence pattern showsinFigure 2isrepresen-\ntativeofwhat wefound inallofthedatabases, although\nthere aresmall differences inthedetails from repertory\ntorepertory .The average TTS, giventhegreatest preci-\nsioninpitch resolution andananchored search, wasfound\ntobelessthan 6[data tokens], buttheperformance dif-\nferences between thefour most precise levelswere min-\nimal. This conﬁrms andreﬁnes previous results reported\nbySchlichte [9]andHoward[4].\nNoTTU could beretrie vedforhighly generic themes,\nbutthedetails varybydataset size andfeature set. (For\nexample, forpgcintheclassical set,11% didnotachie ve\nTTU within thefullincipit size.) Because certain incipits\ndidnothaveaunique match, their TTUs were excluded\nfrom averages. Figure 3givestheTTU andTTS failurerates fortheentire dataset forvarious musical features.\nLess speciﬁc features yield higher failure rates. Itisim-\nportant tonote that TTU wasgivenaspecial deﬁnition\nforthisstudy topreventfalse failures: ifasingle match\nwasnotfound bysearching with anentire incipit, then the\nsearch wasstilldetermined toreach aunique endifthe\nmatch count wasthesame fortheentire incipit minus the\nlastnote.PSfrag replacements\n01020\n304050failure rate\nTTU3.4\n3.43.41.1\n5.9\n1.4\n5.9 5.9 1.4 1.4 5.31.5\n142.8316.5463.4\n3.4\n1.1\n5.9\n1.4\n5.9\n1.4\n5.3\n1.5\n14\n2.8\n31\n6.5\n46\nTTSG\nTTS.53\n.53.53 .111.5\n.151.5 1.51.5\n.161.3.215.8.382020\n1.11.1\n35pch12p12p-rgc\n12i12i-rgc\nmimi-rgc\nsdsd-rgc\nprcprc-rgc\npgcpgc-rgc rgcrgc rgc rgc rgc rgc rgc\npch\nFigur e3.Search failure rates foralldatasets according\ntosearch feature. Forexample, when searching bypch\n3.4% oftheincipits arenotfound “uniquely” evenbythe\nendoftheincipit (with anaverage length of18notes).\nSimilarly ,0.53% ofincipits cannot befound with 10or\nfewer matches bysearching fortheentire length ofthe\nincipit.\n4.3. Entr opy\nEntrop yisameasure ofdiversity used incomparisons of\ninformation. Inmathematical terms, entrop y,\n\b\n\t\f\u000b\u000e\r,eval-\nuates therandomness ofavariable interms ofhowwidely\nspread theprobability distrib ution, \u000f\n\t\u0006\u000b\u000e\r,is.Mathemati-\ncally ,itcanbeshownthat \u0006\u0011\u0010\u0013\u0012\u0015\u0014\u0017\u0016 isneverlargerthan the\nactual number ofstates.\n\b\n\t\u0006\u000b\u000e\rismeasured inbitsand\ncanbecalled theﬁrst-order entrop y.Equation 1givesthe\ndeﬁnition ofﬁrst-order entrop y[1],\b\n\t\f\u000b\u000e\r\u0019\u0018\u001b\u001a\u001d\u001c\n\u001e \u001f \"!\n\u000f\n\u001f$#&%\u0011'\u0011(\u000f\n\u001f\n(1)\nwhere \u000fisthenormalized probability distrib ution (the\nsum ofall \u000f\n\u001f\nis1)oftherandom variable\n\u000b,and )is\nthenumber ofstates which\n\u000bmay posses. Asanexam-\nple, Figure 4showstheprobability distrib ution forpch\nand12p features inalldatasets which yields ﬁrst-order\nentropies of3.40 and3.33 bits/symbol respecti vely.\nOntheother hand, theentrop yrate,*\n\t\f\u000b,+-\r,describes\ntheunpredictability ofarandom process\n\u000b,+.Arandom\nprocess,\n\u000b\u000e+,with alargeﬁrst-order entrop ydoes notnec-\nessarily havealargeentrop yrate since thesequence of\nelements intheprocess may notbepurely random. Math-\nematically ,itcanbeshownthat *\n\t\u0006\u000b,+-\r\n\b.\t\u0006\u000b\u000e\r. *\n\t\f\u000b/+-\r\nismeasured inbits/symbol.\nIf\n\u000bvaries intime andforms an 0-length sequence,\nthen itsentrop yrate, *\n\t\f\u000b\u000e+-\r,isdeﬁned as*\n\t\u0006\u000b\n+\r1\u0018\n\b\n\t\f\u000b/+-\r0(2)PSfrag replacements\na\u0000\ng\u0001e\u0000\nd\u0001b\u0000\na\u0001f\ne\u0001c\nb\u0001gg\nd\ne\u0002\u0003\u0002a\nb\u0002\u0003\u0002e\nf\u0002b\nc \u0002f \u0004\ng\u0002c\u0004\nd\u0002twelv e-tone pitch class\n\u0005\u0007\u0006 \u0005\b\u0005\n\u0005\u0007\u0006 \u0005\b\t\n\u0005\u0007\u0006 \u0005\u000b\n\u0005\u0007\u0006!\n(\u0005\u0007\u0006!\r\fdensity12pProbability Distrib ution\n PSfrag replacements\u000e \u000e\nF\n\u000e \u000e\nC\n\u000e \u000e\nG\n\u000e \u000e\nD\n\u000e \u000e\nA\n\u000e \u000e\nE\n\u000e \u000e\nB\n\u000e\nF\n\u000e\nC\n\u000e\nG\n\u000e\nD\n\u000e\nA\n\u000e\nE\n\u000e\nB\n\u000f\nF\n\u000f\nC\n\u000f\nG\n\u000f\nD\n\u000f\nA\n\u000f\nE\n\u000f\nB\n\u0010\nF\n\u0010\nC\n\u0010\nG\n\u0010\nD\n\u0010\nA\n\u0010\nE\n\u0010\nBxFxCx\nGxDxAxExB\nenharmonic pitch class\n\u0011\n\f\n\u0011\u0013\u0012\n\u0011\n\t\n\u0011\u0013\u0014\u0011\n(\u0011\n!\n\u0005density (\n\u0015 \u0016\u0017\n\u0018\u0019)pchProbability Distrib ution\nFigur e4.Example probability distrib utions forpchand\n12p features (alldatasets). The pitch E \u0000occurs inpch\nabout once ineverytennotes, while A \u0001 \u0001occurs about once\ninamillion notes.\nwhere\n\u000b\u000e+denotes therandom vector (\n\u000b!,\n\u000b\n(\n, \u001a\u0007\u001a\u001b\u001a,\n\u000b+),\nanditsentrop ycanbecalculated bysumming overallpos-\nsible sequence probabilities:\b\n\t\u0006\u000b\n+\r\u0011\u0018\u001b\u001a\n\u001c\n\u001e \u001f\u001c\n\u001c\n\u001e \u001f\u001d\n\u001e\u0007\u001e\u001b\u001e\n\u001c\n\u001e\u0006\u001f\u001f\n\u000f\n\u001f\u001c! \n\u001f\u001d\" \n\u0006#\u0006#\u0006 \n\u001f\u001f\n#&%\u0011'\u0011(\u000f\n\u001f\u001c! \n\u001f\u001d\" \n\u0006#\u0006#\u0006 \n\u001f\u001f(3)\nThe total number ofterms inEquation 3is )\n+,which is\nthetotal number ofpossible 0-length sequences\n\u000b\n+may\nrealize.\n\b\n\t\u0006\u000b\u000e+ \rcanbecalled the 0-thorder entrop y.In\nother words, theentrop yrateistheaverage number ofbits\npersymbol needed toencode acertain setof\n\u000b,+,while\nthe0-thorder entrop yisthetotal number ofbitsrequired\ntoencode thesame setof\n\u000b\u000e+.\nCalculating\n\b\n\t\u0006\u000b\u000e+ \riscomplicated andnotuseful for\nourpurposes, especially since incipit lengths vary,sowe\nuseTTS toestimate theentrop yrate,*:*\n\u0018\n#&%\u0011'($ %\nTTS\n%(4)\nwhere &isthetotal number ofunique incipits ina\ndataset, Kdeﬁnes thecutof fnumber ofmatch forsufﬁ-\ncienc y( '\n\u0018)(+*arbitrarily forourpurposes), andTTS is\ntheaverage time-to-suf ﬁcienc yofallanchored searches.\nTTS determines entrop yratemore accurately than TTU\ndoes, provided thatthesearch-decay rateremains expo-\nnential uptotheTTS point.\nWhen combining twofeatures inasearch, severalother\ninformation-theory concepts areuseful. Joint entrop y,\b.\t-,\u0013.\u000b/ \r,refers tothecombined entrop yofthefeatures.\nThejoint entrop yisalwayslessthan orequal tothesum\noftheindividual features. Mutual information,)\n\t-,\u00130\u000b/ \r,is\nthatportion ofthejoint entrop ywhich isshared byboth\nfeatures (seeFigure 5).\nWeattempted todetermine how“comple x”each ofthe\nrepertories is.Figure 6compares theﬁrst-order entrop yPSfrag replacements\b\n\t1/ \r\n\b\n\t-, \rJoint entrop y2436587:9<;\nConditional entrop y24365>=\u001b9<;Mutual information?83658@:9!;Conditional entrop y Conditional entrop y\n24369A=\u001b5B;\nFigur e5.Venndiagram showing conceptual relationships\nbetween entrop y,joint entrop y,conditional entrop y,and\nmutual information.\nwith theexperimentally measured entrop yrateforthevar-\nious datasets. Theﬁrst-order entrop yindicates themaxi-\nmum expected comple xityofthemusic, andtheentrop y\nrate quantiﬁes theactual comple xity.Figure 6demon-\nstrates thattheclassical-theme dataset isthemost “com-\nplex”from theperspecti veofinformational feature (here\n12p)variety ,while thePolish religious-song corpus isthe\nleast “comple x.”The Renaissance dataset hasthelow-\nestﬁrst-order entrop y.However,itsentrop y-rate ishigher\nwith respect toitsentrop ythan anyother dataset. This\nmeans thateventhough theRenaissance incipits usefewer\ntwelv e-tone pitches than theclassical theme set,theincip-\nitsarejustas“comple x.”\n00.511.522.533.54\nPSfrag replacements12p: First-Order Entr opyandEntr opyRate\nClassical US-RISMAllDatasetsEssenAsia\nEssenEurope PolishRenaissancebits/symbol\nFirst-Order EntropyEntropyRate\nFigur e6.Twelv e-tone pitch (12p)entrop yperrepertory ,\nsorted byﬁrst-order entrop y.Light bars showﬁrst-order\nentrop y.Dark bars showentrop yrate.\nNote thattheentrop yrates inFigure 6arealwaysless\nthan theentrop yforeach dataset. This indicates thatmelo-\ndicsequences arenotpurely random based onthefeature\nprobability distrib utions andexplains why thetheoretical\nmatch counts inTable 5aretoolow.\n5.JOINT SEARCH FEA TURES\nWiththemeasuring techniques described intheprevious\nsections, itisuseful tonowconsider theeffects ofcombin-\ningpitch andrhythm features intoasingle search. Entrop yEntrop yType Deﬁnition Value\npgcﬁrst-order entrop y\n243pgc\n;1.5325\ndgcﬁrst-order entrop y\n243dgc\n;1.4643\nJoint ﬁrst-order entrop y\n243pgc\n7dgc\n;2.9900\nConditional pitch\nentrop y(givenrhythm)\n243pgc\n=dgc\n;1.5256\nConditional rhythm\nentrop y(givenpitch)\n243dgc\n=pgc\n;1.4575\nMutual information\n243pgc\n;\u0001\u0000243pgc\n=dgc\n;0.0068\nTable 7.Calculation ofmutual information from ﬁrst-\norder pitch andrhythm entropies (values based onEssen\ndatasets).\nSearch Features TTS TTU\npgc 7.4 12.1\ndgc 11.2 17.0\npgc+dgc 4.6 8.4\nTable 8.TTU andTTS values forEssen data forseparate\nandjoint prcanddgcfeatures.\ncalculations ofmutual information showthat pitch and\nrhythm features areveryindependent (Table 7,sosearch-\ningwith both atthesame time isnotawasteful endea vor.\nToimplement joint searches inpractical terms, we\nenumerated both thepitch andrhythm features inseries\nonto thesequence ofprime numbers, and then multi-\nplied thepitch andrhythm features together togetaone-\ndimensional search sequence which could besearched in\nthesame manner asthepitch orrhythm features alone.\nIfnecessary ,thissetcanthen bedecomposed again into\ntheoriginal twofeature setswithout anadditional lookup\ntable.\nForexample, thecombined pgc-dgcsearch emplo ys9\nstates. pgcstates canbeassigned totheprimes 2,3,and5.\ndgcstates canthen beassigned tothenextthree primes:\n7,11,13. Then thepossible states forthejoint search\npgc-dgcwould be:14,21,22,26,33,35,39,55,and65.\nSearches combining pitch andrhythmic features signif-\nicantly reduced theTTS andTTU. Forexample, Table 8\nshowsthatboth TTS andTTU query lengths were reduced\nabout 30% forpgc-dgcjoint-feature searches.\nComparing allpitch features jointly searched with dgc,\nasimilar reduction inTTS length isnoticed inallreper -\ntories (Figure 7).These tests gaveuseful insights intothe\nresults ofthemore extensi veseries ofmeasures wede-\nrivedfrom coupling dgcwith each levelofpitch precision\n(Figure 2).\nFigure 8showsMatch Count Proﬁles fortwopitch fea-\ntures andtworhythmic features along with their joint-\nfeature proﬁles. Twointeresting things areshowninthis\nﬁgure: (1)thelow-detail pgcfeature plus arhythmic fea-\ntureperforms searches asgood asorbetter than 12ifea-\ntures alone, and(2)more speciﬁc rhythmic features gen-\nerate lowerTTS values than lessspeciﬁc features. How-\never,chosing different rhythmic features tocombine with\naparticular pitch feature does notgreatly impro vethePSfrag replacements\nEssen Asian (2,241 incipits)\n02468101214\ndgc pgc prc sd 12i mi 12p pchaverage TTSalonejointPSfrag replacements\nClassical (10,724 incipits)\n02468101214\ndgc pgc prc sd 12i mi 12p pchaverage TTSalone joint\nFigur e7.Pitch-only compared tojoint searches with dgc\nfortwodifferent datasets. Light bars arepitch features\nsearch alone. Dark bars showjoint TTS values when a\npitch feature iscombined with dgc.\nTTS values asopposed tochoosing adifferent rhythm fea-\nture. Note inparticular inFigure 8thatthe6-state pgc-blv\njoint feature isalmost identical inentrop y-rate to12ialone\nwhich hasabout 20states.\n \n \nPSfrag replacements\n12\n34\n56\n78\n910\nsearch length2\n24\n46\n68\n810\n1012match count (\n\u0002\n\u0003\u0004\n\u0005)Pitch/Rhythm Joint Match Count Proﬁles\nEssen dataset (8,743 songs)\nTTS\u0004\f\u0005blv\ndgc\npgc\n12ipgc-blv\n12i-blvpgc-dgc\n12i-dgc\nFigur e8.Effects ofcombining pitch andrhythm searches\nonmatch count proﬁles.\n6.CONCLUSIONS\nThe three search-ef fectiveness measuring techniques de-\nscribed inthispaper areuseful inquantitati velyevaulat-\ningthesearch properties ofthemultitude offeatures that\ncanbeextracted from musical data. Inparticular ,applying\nthese metrics toananalysis ofjoint pitch-rhythm features\nclearly sumarizes theimpro vements insearch times which\narepossible under realistic search conditions.Animportant veriﬁcation inthissetoftests wasthatthe\nmost signiﬁcant increases insearch-ef fectiveness come\nfrom theprogressions inquantization precision from\npitch/gross-contour (pgc)toprc,andfrom scale degree\n(sd)to12-tone pitch (12p).However,theimpro vement\ninperformance ofsd(7states) overprc(5states) is\nslight, aﬁnding which isgenerally similar to[9].Also,\ntwelv e-tone searches arepreferable toenharmonic pitch-\nclass searches because pch/12pandmi/12imatch proﬁles\nwere coincident toabout onepartin10,000. Since the12p\nstates arefewerthan pch’sandtheir search characteristics\narenearly identical, twelv e-tone searches arepreferrable\nforminimizing query errors.\nAnother useful result istherelati veordering ofeffec-\ntiveness forvarious rhythmic features canbemade byex-\namining Figure 2.Allrhythmic features arelesseffective\nforsearching than themost general pitch feature ofpgc.\nFrom most speciﬁc tomost general, therhythm features\nare: mlv,dur,mrc,drc,dgc,blv,andmgc.However,\nthese features arenarro wlyclustered intheir efﬁcienc y,so\ndifferent repertories may reorder them slightly .\nCombining dgc and pgc isaparticularly effective\nsearch strate gysince there islittle mutual information be-\ntween thetwofeatures. Both features arevague bythem-\nselves,buttogether theyarejustasormore effectivethan\npitch-only scale-de gree searches. Interestingly ,incorpo-\nrating dgcwith other more speciﬁc pitch features yields\nonly marginally better TTS/TTU times (about 1.5fewer\nnotes required inthesearch query ,ascompared to4.5\nfewernotes with pgconaverage).\nOctaveinformation does notsigniﬁcantly impro ve\nsearch results, since thepch/12p and mi/12isets had\nnearly identical entrop yrates. The pch/12p setdidnot\nencode octave,while themi/12isetpreserv edoctavein-\nformation. Also, enharmonic spellings arenotmore ef-\nfectivethan twelv e-tone search types, since 12pandpch\nhavesimilar entrop yandverysimilar entrop y-rates.\nFurther analytical materials, updates and results\ncan berequested and areposted ontheinternet at\nhttp://themeﬁnder .org/analysis.\n7.ACKNO WLEDGMENTS\nThenucleus oftheThemeﬁnder database wasdesigned by\nDavidHuron. Itsoriginal implementation ontheweb was\ndesigned byAndreas Kornst ¨adt. Much ofitssubsequent\ndevelopment, including data translations, wasundertak en\nbyCraig Sapp, who usually maintains it.The bulkof\ntheEssen data wasmade available originally byHelmut\nSchaf frath andsubsequently byEwaDahlig. The Essen\nChinese data wasencoded byBaoqiang Han. The Lux-\nembour gian addition wasencoded andmade available by\nDamien Sagrillo. The Italian Latin motet data waspro-\nvided byHarry B.Lincoln [6].RISM data wasmade\navailable bytheUSRISM Committee centered atHarv ard\nUniversity .The “classical” data wasprovided inlarge\npart byDavidHuron, andwasrevised andexpanded by\nCCARH staffandStanford University students. Theau-thors extend their cordial thanks tothese contrib utors as\nwell astothose notmentioned.\n8.REFERENCES\n[1]Cover,Thomas, andJoyA.Thomas, Elements of\nInformation Theory ,NewYork: Wiley,1991.\n[2]Dannenber g,Roger B.,W.P.Birmingham, G.\nTaznetakis, C.Meek, N.Hu,B.Pardo, “The\nMuseArt Testbed forQuery-by-Humming\nEvaluation, ”Proc.ofthe4thAnnual International\nSymposium onMusic Information Retrie val,\nBaltimore, Maryland, 2003.\n[3]Hewlett, Walter B.“ABase-40 Numberline\nRepresentation ofMusical Pitch Notation, ”\nMusik ometrika 4(1992), 1-14.4\n[4]Howard,John. “Strate giesforSorting Musical\nIncipits, ”inMelodic Similarity: Concepts,\nProcedur es,andApplications (Computing in\nMusicolo gy11),ed.Walter B.Hewlett andE.\nSelfridge-Field. Cambridge, Mass.: TheMIT Press,\n(1998), 119-128.\n[5]Kim, Youngmoo E.,W.Chai, R.Garcia, andB.\nVercoe, “Analysis ofaCounter -Based\nRepresentation forMelody ,”Proc.ofthe1st\nInternational Sympoisum onMusic Information\nRetrie val;Plymouth, Mass., 2000.5\n[6]Lincoln, Harry .TheLatin Motet: IndexestoPrinted\nCollections, 1500-1600 .Institute ofMediae val\nMusic; Ottawa,1993.\n[7]McNab, Robert, L.A. Smith, I.H.Witten, C.L.\nHenderson andS.J.Cunningham. “Towards the\nDigital Music Library: TuneRetrie valfrom\nAcoustic Input”. Proc.ofthe1stACMInternational\nConfer ence onDigital libraries ,Bethesda,\nMaryland; 1996.6\n[8]Rand, William, andWilliam Birmingham,\n“Statistical Analysis inMusic Information\nRetrie val,”Proc.ofthe2ndAnnual International\nSymposium onMusic Information Retrie val,\nBloomington, Indiana, 2001.\n[9]Schlichte, Joachim. “Der automatische Vergleich\nvon83,243 Musikincipits ausderRISM-Datenbank:\nErgebnisse -Nutzen -Perspekti ven,”Fontes artis\nmusicae 37(1990), 35-46.\n[10] Sorsa, Timo, andJyriHuopaniemi, “Melodic\nResolution inMusic Retrie val,”Proc.ofthe2nd\nAnnual International Symposium onMusic\nInformation Retrie val,Bloomington, Indiana,\n2001.7\n4http://www .ccarh.or g/publicat ions/re prints/base40\n5http://web .media.it.edu/˜c haiwei/papers/Music IRmelody .pdf\n6http://portal.acm.or g/citation.cf m?id= 226934\n7http://music-ir .org/gsdl/ismir2 001/poste rs/sorsa .pdf"
    },
    {
        "title": "Exploring Microtonal Matching.",
        "author": [
            "Iman S. H. Suyoto",
            "Alexandra L. Uitdenbogerd"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416656",
        "url": "https://doi.org/10.5281/zenodo.1416656",
        "ee": "https://zenodo.org/records/1416656/files/SuyotoU04.pdf",
        "abstract": "Most research into music information retrieval thus far has only examined music from the western tradition. How- ever, music of other origins often conforms to different tuning systems. Therefore there are problems both in rep- resenting this music as well as finding matches to queries from these diverse tuning systems. We discuss the issues associated with microtonal music retrieval and present some preliminary results from an experiment in applying scoring matrices to microtonal matching.",
        "zenodo_id": 1416656,
        "dblp_key": "conf/ismir/SuyotoU04",
        "keywords": [
            "research",
            "music",
            "information",
            "retrieval",
            "western",
            "tradition",
            "tuning",
            "systems",
            "problems",
            "microtonal"
        ],
        "content": "EXPLORING MICROTONAL MATCHING\nIman S. H.Suyotoand AlexandraL. Uitdenbogerd\nSchoolofComputerScienceand InformationTechnology\nRMIT University\nABSTRACT\nMostresearchintomusicinformationretrievalthusfarhas\nonly examined music from the western tradition. How-\never, music of other origins often conforms to different\ntuningsystems. Thereforethereareproblemsbothinrep-\nresentingthis music as well as ﬁndingmatchesto queries\nfrom these diverse tuning systems. We discuss the issues\nassociated with microtonal music retrieval and present\nsome preliminary results from an experiment in applying\nscoringmatricestomicrotonalmatching.\n1. INTRODUCTION\nIntheﬁeldofmusicretrievalresearchthereisatemptation\nto work only with western music, as this is readily avail-\nableinavarietyofelectronicformats. Musicofothercul-\nturespresentsmanydifﬁculties. Muchisnotrecorded,and\nis orally transmitted. Recordings are not found in quan-\ntity in the local record store. The unusual tunings of mu-\nsicfromsomeculturesmeansthatitcannotbeadequately\nrenderedinstandardmusicformatssuchasMIDI.Despite\nthese difﬁculties, there are collections of music gathered\nby ethnomusicologists that may be available with some\neffortintranscription.\nTherehasbeensomeworkontherepresentationofmu-\nsicwithnon-westerntuning. Mostproposalsinvolvemap-\npingexistingnotestoslightlydifferenttunings(discuss ed\nin Section 3). Additionally there has been comparative\nanalysis of collections of folk music from different cul-\ntures. For example, Schaffrath analysed a collection of\nChinese folk tunes represented using scale numbers. He\nused a similar representation for music of German ori-\ngin and compared the properties of each collection. This\ncross-comparisonignoredthedisparatetuningsofthetwo\ntypes of music, but allowed for statistical comparison of\nmelodies, revealing telling differences between Chinese\nandGermanmelodictraditions[28].\nAtthisstageitisnotclearwhetherseparatetechniques\nare required for matching microtonally, or whether there\nis a need for such an application. This work assumes that\nPermission tomakedigital orhard copies ofallorpartofthi swork for\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributed forproﬁtorcommercialadvantag e andthat\ncopies bear this notice and the full citation on the ﬁrstpage .\nc/circlecopyrt2004 Universitat Pompeu Fabra.matching with exact tuning is required, and therefore as-\nsumes that the queries will be well-formed, possibly by\na musicologist, or that an example recorded piece of mu-\nsic wouldbe usedasthe sourceandpitchesautomatically\nextractedfromit.\nInthispaperwedescribesomeexampletuningsystems\nwhich are not perfectly represented by standard western\nrepresentations. We discussvariousrepresentationsavai l-\nable,thenshowa simplerepresentationoftuningsystems\nthat was used in our work. The experiments show the\neffect of applying various dynamic programming scor-\ning matrices to the problem of matching. The matching\ntechniques generally work well within the experimental\nframework, but the test collection is very small due to a\nlackofavailabilityofmicrotonalmusicinelectronicform .\n2. TUNING SYSTEMS\nManytraditionaltuningsystemsexist. Inparticular,East -\nern music has many tuning systems. Some instances are\nSundanese (West Java, Indonesia) slendro(ﬁve-tone sys-\ntem), Javanese (Central and East Java, Indonesia) pelog\n(seven-tone system), and the Chinese ﬁve-tone system.\nFor illustration, we present Javanese pelog here. The\npitches sound close to the Phyrgian mode, which corre-\nsponds to E-F-G-A-B-C-D and its transpositions, in the\nWestern twelve-tonesystem [18].\nNot all tuning systems have corresponding pitches in\nother tuning system(s). For example, in Sundanese slen-\ndro padantara tuning, the interval between two pitches is\n240cents [32]. If we try to ﬁt this into equal-tempered\ntwelve-tone system, we get a pitch in Sundanese slendro\npadantara that lies almost in the middle of two pitches in\nequal-temperedtwelve-tonesystem( 240between 200and\n300). Although 240is closer to 200than to 300, it is\nnot close enough to any. This is because the difference\nis larger than just noticeable difference (see Section 4.1).\nThisphenomenonisdescribedas microtonality .\n3. COMPUTER REPRESENTATIONS FOR\nMICROTONALMUSIC\nFor a music representation to accommodate alternative\ntunings and microtonalism, it needs to provide the facil-\nity for the pitches of the tuning system to be deﬁned. We\nassume here that there is consistency in the tuning across\na piece of music or at least a section of the music, so that\nonetuningdeﬁnitioncanbeappliedto themusic.Below we discuss several methods that can be applied\nto microtonal representation, and we follow this with the\nsimpleapproachusedforourexperimentaldata.\nMIDIMIDI (Musical Instrument Digital Interface), in\nterms of music format, is a representation that supports\npolyphonicmusicandmultitrackrecordingandplayback.\nItencodesnotesasintegers.\nTheStandard MIDI File (SMF) format and General\nMIDI(GM) deﬁnes how MIDI data should be stored. It\nhasnosupportfornon-twelve-tonesystems. Accordingto\nCorreia and Selfridge-Field, Scholz has proposed an ex-\ntensiontoMIDIconcerningtuningsystems[4]. However,\nit is not a strict standard. MIDI Manufacturer Associa-\ntionhaspublished MIDITUNINGExtensions [22]. Thisis\nhowever still limited to twelve-tone systems. What users\ncan do is alter the tuning of each pitch. For example, we\ncandetuneCby −20cents,C ♯by15cents,Dby 50cents,\nand so on. With standard MIDI this can be achieved by\npitchbendevents.\nESACESAC has support for non-twelve-tone systems.\nFor example, Schaffrath deﬁned the tuning of heptatonic\nscales to allow the encoding of music from many cul-\ntures[28].\nIn ESAC, a pitch is represented as a number. This is\napplicable to scales originating from most cultures. En-\ncodingisinvariantwithrespecttoscaleortuningsystems.\nThe tonic (ﬁrst note) is always represented by “ 1”. The\nnote encoding is similar to solfege [29]. Note durations\nare stored as relative durations. Besides pitches and note\ndurations, the title, source, and social function of a tune\narealso stored[29].\nWith regard to melody retrieval, matching on an\nESAC-encoded tune can involve pitch components only,\nnote duration component only, or a combination of both.\nESAC has been used to identify that the ﬁrst ﬁve notes\nof the clarinet part of Mozart’s second trio from Clar-\ninet Quintetresemblestwo Germanfolksongs,“HoertIhr\nHerrn und lasst euch sagen” and “Trauer, Trauer, ¨ uber\nTrauer”[29].\nDespite being effective for melody retrieval, this rep-\nresentation has a basic limitation: it has no support for\npolyphonicmusic.\nHumdrum Huron has created a set of utilities called\nHumdrum, which is useful for facilitating the posing and\nansweringofmusicresearchquestions[19]. Humdrumby\nitself is not useful for representing any music, because it\nisasyntax[19]. Humdrumisnotlimitedtoanyparticular\ntuning system. Users may deﬁne their own representa-\ntions. Fortwelve-tone-systemrepresentation,aHumdrum\nrepresentationcalled kernhasbeensuggested.\nOne of the strengths of Humdrum is its support for\npolyphonic music which is one of its design consider-\nations. It can represent “sequential and/or concurrent\ntime-dependent discrete symbolic data” [19]. Notes that\nsound at the same time are called concurrent attributes ,while notes that sound sequentially are called sequential\nevents[19].\nXMLXML is a rapidly developing technology. There\nis an abundance of tools supporting the meta-language,\nsuch as authoring tools, development libraries, viewers,\nand converters. It can also be extended by users to ﬁt\ntheir own needs. Data deﬁnition is made possible by the\nuse ofa DocumentTypeDescription(DTD)and anXML\nSchema [7]. As XML documents can contain semistruc-\ntured data [7] they can be used to store music. Using the\nmethod suggested by Roland [27], notes are described as\nshownintheexamplebelow.\n<note pitch=\"C4\" dur=\"1\" />\n<note pitch=\"D4\" dur=\"0.5\" />\n<note pitch=\"E4\" dur=\"0.5\" />\nAddingmicrotonalinformationwouldbeasimplemat-\nterofdeﬁningfurtherXMLtagsthatcontaintuninginfor-\nmationforthescale.\nXML is effective,but inefﬁcient in its raw state. How-\never, despite its verbose nature, XML generally com-\npresses very well. Its efﬁciency can be increased further\nbyindexing.\nMTRITheMicro-TonalRepresentationforInformation\nRetrieval (MTRI) was designedas a music representation\nmethod for our microtonal experiments. It consists of\nthe essential elements required for melody, harmony and\nrhythm representation, but ignores other musical aspects\nsuchasloudness. However,therepresentationissufﬁcient\nto capture the recognisable elements of a piece of music.\nForthesakeofbrevitywemainlydescribethetuningsys-\ntemrepresentationhere,asthatistheprincipleconcernof\nthispieceofresearch.\nTo encode a tune in MTRI, two ﬁles are used: MTP\n(MTRI pitch speciﬁcation ﬁle) and MTS (MTRI score\nﬁle). AnMTPstoresinformationaboutpitchfrequencies,\nwhilean MTSstoresinformationaboutnoteevents.\nIn an MTP, the parameter Nis used to describe the\nnumber of notes in the tuning system. As an example,\nfor the equal-temperedtwelve-tone system, N= 12(see\nFigure1). Apitchisstoredinoneline. Whenstoringpitch\nnames that have the same frequency, all the pitch names\nmustappearbeforethefrequency.\nAn MTS begins with the directive :Use, which spec-\niﬁes the MTP to be associated with the tune. For modern\ncompositions with tuning systems that change mid-work,\nit would be simple to extend the representation by allow-\ning the use of the directivewherevera new tuningsystem\nis required. The note names are not limited to the letters\nAtoG,somicrotonalmelodiesthatdividetheoctaveinto\nmorethan12notesarealso abletobestored.\n4. RETRIEVAL\nAninformationretrievalsystemisonlyusefulwhenitcan\nanswer queries effectively. Information retrieval system sN = 12\nC B+ 261.63\nC+ D- 277.18\nD 293.66\nD+ E- 311.13\nE F- 329.63\nF E+ 349.23\nF+ G- 369.99\nG 392.00\nG+ A- 415.30\nA 440.00\nA+ B- 466.16\nB C- 493.88\nFigure1. MTP forequal-temperedtwelve-tonesystem.\nthat support ranked queries measure how similar a query\nis to items in the database according to some meaning of\nrelevance[41]. Mostmusicretrievalsystemsincludingthe\nonewereportheresupportrankedqueries. Inourcase,the\nqueriesare tunes.\nVarious matching techniques have been developed\nto anticipate query vagueness. Dynamic programming\nwas examined by Uitdenbogerd and Zobel [35], a tech-\nnique suggested by Mongeau and Sankoff [23]. The\nﬁrst published use of n-grams for melody matching was\nby Downie [10]. The concept was further examined\nby Uitdenbogerd [35], Pickens [25] and Doraisamy and\nR¨ uger [9]. The comparison of both approaches has been\nshown by Uitdenbogerd and Zobel [36], which indicated\nthatn-grams can be used as a fast alternative to dynamic\nprogramming approaches to melody matching without\nsigniﬁcant loss of effectiveness. An alternative approach\nis the indexing of notes and applying a look-up of each\nnote in multiple musical keys, with the Chinese remain-\nder theorem for transposition-invariant retrieval [3]. Re -\ncent work by Birmingham, Meek, O’Malley, Pardo, and\nShifrin[1] usesstochasticmodels.\nDannenberg,Birmingham,Tzanetakis, Meek, Hu, and\nPardo[6]alsousedHMM(HiddenMarkovModels)along\nwith dynamic programming in conjunction with directed\nmodulo-12 standardisation [36] and Inter Onset Interval\nratiovalues. They also tested melodic contour match-\ning. Effectiveness was reported as MRR (Mean Recip-\nrocal Rank), the percentageof answersranked as the ﬁrst\nanswer,inthetoptwo,andinthetopthree. Closelyrelated\nto thisworkare thosebyMeek andBirmingham[21] and\nShifrin and Birmingham [31], both of which use HMM\nforsearchingandMRRto reportitseffectiveness.\nKageyama, Mochizuki, and Takashima [20] used dy-\nnamicprogrammingfortheirquery-by-hummingretrieval\nsystem. Their system also made use of note duration in-\nformation for melody matching. The query melody and\nthe melodies in the database were transposed for match-\ning. Note duration was used as the weight for matching\nscore. The effectiveness is reported using the number of\nmelodicsamples (outof 100)retrievedas the ﬁrst answer\nandinthetopten.\nTo support comparison of different renditions of thesamepieceofmusic,melodystandardisationisused[35].\nHere, a pitch is not representedexactlyas it sounds. This\nis to support approximate matching. This is analogous\ntoa techniqueintextretrievalsystemscalledcase folding\n(convertingallcharacterstothesamecase[41]). Thereare\nmany possble melody standardisations, but we will only\ncovertheonerelevanttothisexperiment exactmicrotonal\ninterval. We also considerincorporatingnotedurationin-\nformationformatching.\nWe use approximate string matching [24]. The algo-\nrithm to be used is edit distance , also known as Leven-\nshtein distance . We do not use the simplest form of this\nalgorithm, as described by Crochemore and Rytter [5].\nHowever,we use its variationcalled alphabet-weightedit\ndistance[12]. Matching is done in conjunctionwith con-\ntour, directedmodulo,andexact microtonalintervalstan-\ndardisations. ThisisdiscussedfurtherinSection4.4.\n4.1. PitchStandardisation\nUitdenbogerd’s doctoral thesis [34] discusses various re-\ntrievalstandardisationssomeofwhicharethebasisforour\nmicrotone-enabled techniques. Besides contour and ex-\nactintervalstandardisations,thethesisalsofocuseson di-\nrectedmodulo-12 fortheunderlyingexperiments. Thedi-\nrected modulo-12standardisation represents each note as\na numericvaluewhichistheintervalinsemitones(scaled\ntoamaximumofoneoctave)relativetothepreviousnote.\nThevalueisexpressedas:\nρ12≡/braceleftbigg\n0 ; d= 0\nd(1 + (( I−1) mod 12)) ; d/negationslash= 0(1)\nwhere Iistheintervalbetweenanoteanditspreviousnote\n(absolute value) and dis1if the previous note is lower\nthanthecurrentnote, −1ifhigher,and 0ifotherwise[33,\n34]. This is however limited to twelve-tone systems. For\nnon-twelve-tone systems, the formula can be generalised\nso thata noteis expressedas:\nρt≡/braceleftbigg\n0 ; d= 0\nd(1 + (( I−1) mod t)) ;d/negationslash= 0(2)\nwhere tis the number of tones in the tuning system [33].\nThisismayonlyworkwellforequal-temperedtuningsys-\ntems and a special scoring technique may need to be de-\nveloped for matching two tunes having different number\noftonesin theirtuningsystems.\nExact Microtonal Interval standardisation is an exten-\nsion of exact interval standardisation as described in Uit-\ndenbogerd and Zobel [36]. In the exact interval stan-\ndardisation, a note is represented using the number of\nsemitones between itself and its previous note [36]. In\ncontrast, for microtone-enabled matching purposes, we\nexpress intervals in cents. As an example, “Melbourne\nStillShines”(Figure2)isrepresentedas“ 700 400 100\n-500 -500 200 300 -200 -100 -200 ”(seeTa-\nble 1). Two notesthat differare perceivedas “fairlysimi-\nlar”whenthefrequencydifferenceislessthanjustnotice-\nabledifference(JNDF)[8, 26]. JNDF isnota linearmea-\nsure. At 100 Hz, JNDF is 3 Hz, while at 2000 Hz, JNDFTable 1. Exact microtonal interval standardisation exam-\nple.\nTransition ιc\nC4-G4 700\nG4-B4 400\nB4-C5 100\nC5-G4 −500\nG4-D4 −500\nD4-E4 200\nE4-G4 300\nG4-F4 −200\nF4-E4 −100\nE4-D4 −200\nFigure2. MelbourneStill Shinesbyadeishs.\nis 10 Hz [26]. However, Zwicker and Fastl [42] suggest\nusingalinearapproximationofJNDF functionof:\n∆(ν) = 0.007ν (3)\nwhere νis frequency in Hz and ∆is the linear JNDF\nfunction. Theirsuggestionisbasedonmusicaltonescom-\nmonlyconsistingofhigherfrequencyharmonics. Incents,\nJNDF is approximately 12 cents ( 1200 log2|ν+0.007ν\nν| ≈\n1200 log2|ν−0.007ν\nν|= 1200(0 .01)).\n4.2. DurationStandardisation\nKageyama, Mochizuki, and Takasima [20] suggested the\nuse of dynamic programmingfor note duration similarity\nevaluation. They used a weighted sum representing the\nduration and pitch similarity of melodies. Similarly we\nusenotedurationcontourstandardisationtorepresentnot e\ndurations. Inthisstandardisation,anoteisrepresentedb y\nits durationrelative to its previousnote [34]. The follow-\ning symbols are used: “R” for same, “L” for longer, and\n“S” for shorter. For example, “Melbourne Still Shines”\n(seeFigure2)isrepresentedas“ LSRLSRLRRR ”.\nTo incorporate note duration similarity into overall\nsimilarity,wemodelpitchanddurationsimilaritiesastwo\northogonalvectors. Therefore,theoverallsimilarityis:\nΣ≡ςπˆπ+ςδˆδ (4)\nwhereΣis the resultant similarity vector, ςπis the pitch\nsimilarity, ςδis the duration similarity, and ˆπandˆδare\nrespectively pitch and duration unit vectors. Ranking\nis based on the magnitude of resultant similarity vec-\ntor,|Σ|=/radicalbig\nς2π+ς2\nδ. If two tunes have the same |Σ|, the\nonewithhigher ςπisrankedhigher.4.3. PolyphonicMusic\nMost music ispolyphonicin thesense that morethanone\nnotesoundssimultaneously. Thisaddsextracomplexityto\nthe matching process. In our work we treat each track or\npartofapolyphonicpieceasaseparatesequenceofnotes\nfor matching. For example, if a piece consisted of violin,\ncelloandpianoparts,thequerywouldbematchedagainst\neach of these separately. This results in a similarity score\nforeachpart. Thebest oneischosenastherepresentative\nscore for the piece. Matching against all tracks in this\nmanner was shown to be an effective approach in earlier\nwork [36]. Where there is polyphony within a part no\nnotes are discarded, and the sequence as deﬁned in the\noriginal ﬁle is retained. While this may be an issue for\nmatching real queries it does not affect the experiments\nreportedhereastheyinvolveknown-itemsearchesandthe\nqueryandpotentialanswersareprocessedidentically.\n4.4. ApproximateMatching\nIn this work we use a variation of edit distance called\nalphabet-weight edit distance. In the “ordinary” edit dis-\ntance, a penalty score is given for every character differ-\nence oredit operation . There are three edit operations:\nmismatch ,insertion, anddeletion[5, 12, 24]. Each oper-\nation has a penalty score of 1. The number of operations\nmust be as minimal as possible. For example, we have\ntwo strings: “SHRIMP” and “TRUMPET”. The minimal\nnon-matchoperationsare 2 mismatches,2 insertions, and\n1 deletion. Therefore, the distance between “SHRIMP”\nand“TRUMPET”is 2 + 2 + 1 = 5 .\nInalphabet-weight edit distance, a scoring matrix is\nused containing values that should be assigned as costs\nto various operations [12]. This is commonly applied to\ngenomics, for example, in Henikoff and Henikoff [14].\nWe testedmatchingusingvariousscoringmatrices. Local\nalignment isatechniquetoﬁndasubstringpossessingthe\nhighest similarity. This is more useful than global align-\nment(including edit distance), where the overall similar-\nity between two strings is calculated, because it allows a\nshortquerytobematchedwithalongpieceofmusic[35].\nTherefore,we use localalignment(alsoknownas Smith–\nWaterman alignment [12]) for our experiments. In local\nalignment, there is no negative similarity, and the maxi-\nmumscoreisreturnedasthelocalalignmentscore.\nFor our experiments, we designed several scoring\nschemes. The scoring schemes are based on the as-\nsumption that similar intervals should be penalised less\nthan those that differ greatly. Exact matches should be\nrewarded highly, and severe mismatches should be pe-\nnalised highly. For missing notes (insertion/deletion op-\nerations), we applieda range of penalties, includingzero.\nWe included zero penalty because tunes contain notes\nthat are not signiﬁcant (such as grace notes), or repeti-\ntive notes, of which some can easily be missed or added\n(forexample,“One Note Samba”by Jobim). At the same\ntime, we also consider larger penalties to allow a tighter\nmatching process, which is expected to reduce the num-Table 2. Scoringschemesforexactmicrotonalinterval.\nName χInsertion/deletion penalty\n050.5 0\n05-2 0.5 −50\n1 1 0\n1-2 1 −50\n151.5 0\n15-2 1.5 −50\n2 2 0\n2-2 2 −50\n100 100 -180\n0.00 0 .00 0 .00 0 .00\n100 0.00 25 .00 25 .00 1 .81\n130 0.00 22 .51 47 .51 22 .51\n200 0.00 16 .72 39 .24 41 .05\n-50 0.00 12 .58 14 .2453.47\nFigure 3 . Similarity between “ 100 130 200 -50 ”\nand “100 100 -180 ” calculated using exact micro-\ntonalintervalscoringscheme 1-1.\nTable 3. Scoring schemesfor note durationcontourstan-\ndardisation.\nName Insertion/deletionpenalty\n1 0\n2 −1\n3 −2\nberoffalsematches.\nForexactmicrotonalintervalstandardisation,wemake\nuseoftheJNDFvalueincentsasdiscussedinSection4.1.\nWe use thisformulatocalculatereward/penaltyscores:\nω=/braceleftBigg\n−T ;|ιc|\n∆c≥2/parenleftbig\nT1/χ/parenrightbig\nT− ⌊/parenleftBig\n|ιc|\n∆c/parenrightBigχ\n⌋;|ιc|\n∆c<2/parenleftbig\nT1/χ/parenrightbig(5)\nwhere ωis the reward/penalty score, ιcis the interval in\ncents, χ > 0is the reward/penalty order, and ∆c=\n1200 log21.007. Weuse T= 25forourexperiments. The\nscoring schemes we use for our experiments are shown\nin Table 2. As an example, suppose we have two mi-\ncrotonal melodic sequences “ 100 130 200 -50 ” and\n“100 100 -180 ”andwearetomatchthemwith χ= 1\nand insertion/deletion score of −25. Figure 3 shows the\nlocalalignmentmatrixforthematchingprocess.\nThescoringmatrixweusedformatchingwithnotedu-\nration contour standardisation is given in Figure 4. We\nusedthreeinsertion/deletionpenalties(see Table3).L R S\nL 1 0 −1\nR 0 2 0\nS−1 0 1\nFigure 4. Scoring matrix for note duration contour stan-\ndardisation. “ L”, “R”, and “ S” respectively indicate a\n“longer”,a“same”,anda“shorter”.\n5. RETRIEVAL PERFORMANCEEVALUATION\nWe need a measure to evaluate the effectivenessof an in-\nformation retrieval system. We test the effectiveness of\nour system using known-item searches. This means, for\neachquery,wealreadyknowwhichspeciﬁcitemwewant\nreturnedastheanswer. Aknown-itemsearchis similarto\nthehomepageﬁndingtaskinHawkingandCraswell[13],\nwe chose to apply the measures used there, which are\nmean reciprocal of rank (MRR) and the probability that\nananswerisrankedtop10( P10).\nMRR is also commonly used in question-answering\nsystems(whereacorrectanswerforaquestionisknown),\nfor examples, in Wang, Xu, Yang, Liu, Cheng, Bu, and\nBai [40],Voorhees[37],andVoorheesandTice [39]. Do-\nraisamy and R¨ uger [9] also used this measure for their\nmusic information retrieval experiments. Downie [10]\nused the term “modiﬁed precision” to describe “recipro-\ncalofrank”,thusMRRwasalsousedthere. MRRhasbe-\ncomeoneofthedefactostandardmeasuresforevaluating\nthe performance of music information retrieval systems.\nThis is shown by recent papers by Shifrin and Birming-\nham [31], Dannenberg, Birmingham, Tzanetakis, Meek,\nHu,andPardo[6],andMeekandBirmingham[21].\nMRR can be deﬁned mathematically as/angbracketleftbig1\nr/angbracketrightbig\n. For\nexample, if three queries produce answers ranked ﬁrst,\nﬁfteenth, and second, the mean reciprocal of rank is\n1\n3/parenleftbig1\n1+1\n15+1\n2/parenrightbig\n= 0.52. Usingthe same example, P10=\n1\n3(1 + 0 + 1) = 67% . Higher values of/angbracketleftbig1\nr/angbracketrightbig\nandP10in-\ndicatemoreeffectiveretrieval.\nMRRand P10mightbesufﬁcienttomeasuretheability\nof retrieval systems in returning correct answers. How-\never, it is better if a system is able to judge the level of\ncorrectnessofanswers. Totestthisability,weuse highest\nfalse match (HFM). HFM is the similarity of the highest-\nrankedincorrectanswerwithrespecttothatofthecorrect\nanswer [15, 16]. It is typically expressed as percentage.\nFor example, if the similarity score of the correct answer\nis 40, and the highest ranked incorrect answer is 32, the\nHFM is32\n40= 80%. HFMisusefultodeterminehowwell\ncorrect answers are separated from incorrect ones. A re-\ntrieval system is better than the others when it produces\nthelowestmeanHFM(MHFM)amongall. HoadandZo-\nbel also suggested the use of separation andseparation-\nto-HMF ratio in their papers[15, 16]. However,we think\nthatitisnotnecessaryforourexperiment,sinceinourex-\nperiments, only one answer is considered correct. Thosemeasureswouldbeusefulforsystemsreturningmorethan\nonerelevantanswer.\n6. EXPERIMENTS\nAhighlyeffectivetechniqueshouldhaveanMRRof 1.00.\nThismeansthat sucha techniquealwaysreturnsa correct\nanswer in the ﬁrst place. When two techniques each pro-\nduce an MRR of 1.00and a P10of100.00%, we need\nMHFM to distinguishthe two. A techniquethat produces\nlowerMHFMhasabetteraverageabilitytoseparateacor-\nrect answer from incorrect ones. Similarly, when a tech-\nniquedoesnotperformreasonablywell,thatis,MRRless\nthan1.00andP10less than 100.00%, MHFM is also ex-\npected to be as low as possible. This means that the ﬁrst\nanswer returned (which is incorrect) does not mismatch\ntoofar.\n6.1. CollectionandQuerySet\nOur experiment used as its source a subset of the MIDI\nﬁle collection used for earlier melody retrieval experi-\nments [36]. As these all use standard western tuning, a\nset of 3 polyphonic microtonally-tuned pieces were tran-\nscribed from recordings, giving a total collection size of\n2390.\nFromthiscollectionwerandomlyselected22pieces,in\nadditiontothe3microtonalones. Fromeachtune,weex-\ntractedtwo randomexcerpts. Thesewereselectedbyran-\ndomlyselectingatrack,thenrandomlychoosingastarting\npointwithin thetrack. Polyphonictrackswerehandledas\ndescribed in Section 4.3. Queries were randomly given\na length between 12 and 21 notes. Therefore, out of 50\nqueries,therewere6microtonalqueries.\nThequeryset andthecollectioncontainedmelodicse-\nquences in equal-tempered twelve-tone, Sundanese de-\ngung, and Sundanese madenda tuning systems. Search-\nablepitchandnotedurationsequenceswerederivedfrom\nMTRIencoding.\n6.2. Method\nAll queries were matched against all of the tunes in\nour collection. This is regardless of their tuning sys-\ntems, meaningthat microtonalquerieswere also matched\nagainstnon-microtonaltunesandviceversa.\nIn our experiment, we used the scoring schemes de-\nscribed in Section 4.4 for exact microtonal interval pitch\nstandardisationsanddurationcontourstandardisation.\nWithin our experimental framework exact microtonal\nintervalstandardisationsresultintoMRR of 1.00andP10\nof100.00%. Therefore,itisworthlookingintothevalues\nof MHFM. Using exact microtonal interval standardisa-\ntionalwaysdiscriminatescorrectanswersfromfalseones.\nDifferent scoring schemes usually produce different\nHFM’s. We observe that the most contributing factor\nin lowering the value of HFM is the insertion/deletion\npenaltyscore. Thiscanbeobservedbycontrasting:Table 4. Results ofqueryevaluationforexact microtonal\ninterval standardisation with duration similarity ignore d\n(ςδ= 0). MHFMand P10areshownaspercentagevalues.\nScoringscheme MRR MHFM P10\n05 1.00 99 .77 100 .00\n05-2 1.00 91 .93 100 .00\n1 1.00 99 .51 100 .00\n1-2 1.00 65 .75 100 .00\n2 1.00 98 .89 100 .00\n2-2 1.00 55 .21 100 .00\n0.5 1.0 1.5 2.0\nReward/penalty order0.020.040.060.080.0100.0Mean highest false match (%)insertion/deletion penalty = 0\ninsertion/deletion penalty = -50\nFigure 5. MHFM versus reward/penalty order ( χ) with\ndurationsimilarityignored( ςδ= 0).\n•exact microtonal interval standardisation scoring\nschemes 05,and05-2.\n•exact microtonal interval standardisation scoring\nschemes 1,and1-2.\n•exact microtonal interval standardisation scoring\nschemes 2,and2-2.\nThere is one exception in our results, however. Us-\ning exact microtonal interval standardisation, scoring\nschemes 05-1and05-2producethesameresult.\nForexactmicrotonalintervalstandardisation,thevalue\nofχhas contribution in discriminating answers. The\ndifference does not seem to be signiﬁcant in scoring\nschemes involving zero insertion/deletion penalty. How-\never, combined with large insertion/deletion penalty, the\ncontribution of χbecomes obvious. By comparing\nscoring schemes 05-2,1-2, and2-2(all employ-\ning insertion/deletionpenaltyof −50)with the respective\nMHFM’s 91.93%,65.75%,and55.21%,weseetheeffect\nofincreasing χtoreduceMHFM(seeFigure5). Wemake\na further hypothesis that such effect is asymptotic, and it\nmaybeinvestigatedinthe future.\nHigh measures are shown in the results. This may be\ncaused by the size of our collection, which is small. We\nmay investigate the methods with a bigger collection in\nthe future. The high measures can also be due to known-\nitem search queries in a collection that contains tunes of\nwhichmelodicpatternsarediverse.\nExperiments on exact microtonal interval standardisa-\ntion also conﬁrm the usefulness of duration informationTable 5. Results of queryevaluationfor exact microtonal\ninterval standardisation with duration similarity incorp o-\nrated. MHFM is shown as percentage values. P10was\nalways100%andMRRwasalways1.\nScoringscheme Duration scoringscheme MHFM\n05 1 99.66\n05 2 99.66\n05 3 99.66\n05-2 1 91.83\n05-2 2 91.83\n05-2 3 91.83\n1 1 99.41\n1 2 99.41\n1 3 99.41\n1-2 1 66.71\n1-2 2 66.68\n1-2 3 66.68\n2 1 98.78\n2 2 98.78\n2 3 98.78\n2-2 1 55.26\n2-2 2 55.20\n2-2 3 55.19\n0_5 0_5-2 1 1-2 2 2-2\nScoring scheme0.020.040.060.080.0100.0Mean highest false match (%)Note duration ignored\nNote duration incorporated\nFigure 6. MHFM’s for exact microtonal interval stan-\ndardisation. The bar showing the incorporation of note\ndurationis the best result (lowest MHFM) fromthree du-\nrationcontourscoringschemes.\nwith greater magnitude of insertion/deletion penalty to\nslightly improveretrievaleffectiveness. However,the im -\nprovementis insigniﬁcant comparedto the extra process-\ningrequired.\n7. CONCLUSIONS\nOur results demonstrate the applicability of microtone-\naware matching techniques to music of various tuning\nsystems. Microtone-aware matching techniques applied\nin our experiments were non-microtone-aware matching\ntechniques extended for ﬁner frequency spectrum of mu-\nsic.\nTheresultsofourexperimentsshowthat:\n1. Exact microtonal interval standardisation in con-\njunctionwithamicrotone-awarescoringiseffective\nformicrotonalmusic informationretrieval.2. Themostcontributingvalueinloweringmeanhigh-\nest false match of pitch similarity is the inser-\ntion/deletionpenaltyscore.\n3. In matching with exact microtonal interval stan-\ndardisation, larger reward/penalty order can cause\nlowermeanhighestfalsematch,particularlyincon-\njunctionwithlargeinsertion/deletionpenalty.\n4. Note duration information may improve retrieval\neffectiveness by extending the discrimination be-\ntween correct/relevant and incorrect/irrelevant an-\nswersslightly.\nHowever, we recognise the limitations of the collec-\ntion and query set used in this experiment. The next step\nin work on microtonal matching needs to be the procure-\nment of a sufﬁciently large collection to allow reliable\nexperimentation. Once this is achieved, experiments that\ndemonstratewhetherﬁne-grainedtechniquesare required\nwill bemoreconvincing.\nAcknowledgements\nWethankJustinZobelforhisfeedbackandAdindaMarita\nforhelpwithmusictranscription.\n8. REFERENCES\n[1] W. Birmingham, C. Meek, K. O’Malley, B. Pardo, and\nJ.Shifrin. Musicinformationretrievalsystems. Dr.Dobb's\nJournal, pages 50–53, September 2003.\n[2] D. Byrd, J. S. Downie, T. Crawford, W. B. Croft, and\nC. Nevill-Manning, editors. International Symposium on\nMusic Information Retrieval , volume 1, Plymouth, Mas-\nsachusetts, USA,October 2000.\n[3] M. Clausen, R. Engelbrecht, D. Meyer, and J. Schmitz.\nPROMS: A web-based tool for searching in polyphonic\nmusic. InByrdet al.[2].\n[4] E. Correia, Jr, E. Selfridge-Field, et al. Glossary. In\nSelfridge-Field[30],pages 581–610.\n[5] M. Crochemore and W. Rytter. Text Algorithms . Oxford\nUniversityPress,New York, USA,1994.\n[6] R. B. Dannenberg, W. P. Birmingham, G. Tzanetakis,\nC. Meek, N. Hu, and B. Pardo. The MUSART testbed for\nquery-by-humming evaluation. In Hoos and Bainbridge\n[17],pages 41–47.\n[7] C.J.Date. AnIntroductiontoDatabaseSystems . Addison-\nWesley, Boston, USA,eighthedition, 2003.\n[8] C. Dodge and T. A. Jerse. Computer Music: Synthe-\nsis, Composition, and Performance . Wadsworth, Belmont,\nUSA,second edition, 1997.\n[9] S. Doraisamy and S. M. R¨ uger. An approach towards a\npolyphonic music retrieval system. In Downie and Bain-\nbridge [11].\n[10] J. S. Downie. The Musiﬁnd music information retrieval\nproject,phaseIII:Evaluationofindexingoptions. In Cana-\ndian Assoc.for Inf. Sci.Proc.the 23rd Annual Conf.,Con-\nnectedness: Information, Systems, People, Organizations ,\npages 135–146. CAIS,1995.[11] J. S. Downie and D. Bainbridge, editors. International\nSymposium on Music Information Retrieval , volume 2,\nBloomington, Indiana, USA,October 2001.\n[12] D.Gusﬁeld. Algorithms on Strings, Trees,and Sequences:\nComputerScienceandComputationalBiology . Cambridge\nUniversityPress,Cambridge, UK,1997.\n[13] D.HawkingandN.Craswell. OverviewoftheTREC-2001\nWebTrack. InVoorhees and Harman[38],pages 61–68.\n[14] S. Henikoff and J. G. Henikoff. Amino acid substitution\nmatrices from protein blocks. In Proc. Natl. Acad. Sci.\nUSA,volume 89, pages 10915–10919, November 1992.\n[15] T.C.HoadandJ.Zobel. Methodsforidentifyingversion ed\nand plagiarized documents. J. Am. Soc. Inf. Sci. Technol. ,\n54(3):203–215, February 2003.\n[16] T. C. Hoad and J. Zobel. Video similarity detection for\ndigitalrights management. InM. Oudshoorn, editor, Proc.\nAustralasian Computer Sci. Conf. , pages 237–245, Ade-\nlaide,Australia, February2003.\n[17] H.H.HoosandD.Bainbridge, editors. International Sym-\nposium on Music Information Retrieval , volume 4, Balti-\nmore, Maryland, USA,October 2003.\n[18] B. Hugh. Claude Debussy and the Javanese Game-\nlan. http://cctr.umkc.edu/userx/bhugh/\nrecit/debnotes/gamelan.html .Accessed16De-\ncember 2003.\n[19] D.Huron. Humdrumandkern: Selectivefeatureencoding .\nInSelfridge-Field[30],pages 375–401.\n[20] T. Kageyama, K. Mochizuki, and Y. Takashima. Melody\nretrieval with humming. In Proc. Int. Computer Music\nConf.,pages 349–351, 1993.\n[21] C. Meek and W. P. Birmingham. The dangers of parsi-\nmony in query-by-humming applications. In Hoos and\nBainbridge [17],pages 51–56.\n[22] MIDI Manufacturer Association. MIDI Tuning Bank\nand Dump Extensions. http://www.midi.org/\nabout-midi/tuning_extens.shtml . Accessed\n16December 2003.\n[23] M. Mongeau and D. Sankoff. Comparison of musical se-\nquences. In Computers and the Humanities , volume 24,\npages 161–175. Kluwer,1990.\n[24] G. Navarro and M. Rafﬁnot. Flexible Pattern Matching\nin Strings: Practical On-line Search Algorithms for Texts\nand Biological Sequences . Cambridge University Press,\nCambridge, UK, 2002.\n[25] J. Pickens. A comparison of language modeling and prob-\nabilistic text information retrieval approaches to mono-\nphonic music retrieval. InByrdet al.[2].\n[26] J. G. Roederer. Introduction to the Physics and Psy-\nchophysics of Music . Springer-Verlag, New York, USA,\nsecond edition, 1975.\n[27] P. Roland. XML4MIR: Extensible markup language for\nmusic information retrieval. InByrdet al.[2].\n[28] H. Schaffrath. The retrieval of monophonic melodies an d\ntheir variants: Concepts and strategies for computer-aide d\nanalysis. In A. Marsden and A. Pople, editors, Computer\nRepresentations andModelsinMusic ,pages95–109. Aca-\ndemic Press,London, UK,1992.[29] H. Schaffrath. The Essen Associative Code: A code for\nfolksong analysis. InSelfridge-Field[30],pages 343–361 .\n[30] E.Selfridge-Field,editor. BeyondMIDI:TheHandbook of\nMusical Codes . MIT Press,Cambridge, USA,1997.\n[31] J. Shifrin and W. P. Birmingham. Effectiveness of HMM-\nbasedretrievalonlargedatabases. InHoosandBainbridge\n[17],pages 33–39.\n[32] A. Supandi, U. Ngalagena, I. Djunaedi, D. Sain, and R. S.\nRiswara. Teori Dasar Karawitan . Pelita Masa, Bandung,\nIndonesia, thirdedition, 1976.\n[33] I.S.H.Suyoto,S.Uitdenbogerd, andJ.Zobel. Microton al\nmusicinformationretrieval.Researchpapersubmissionfo r\nRMITSchool ofComputer ScienceandInformationTech-\nnology “Research Methods” course (unpublished), 2003.\n[34] A.L.Uitdenbogerd. MusicInformationRetrievalTechnol-\nogy. PhD thesis, School of Computer Science and Infor-\nmationTechnology, RMIT,Melbourne, Australia,2002.\n[35] A. L.Uitdenbogerd and J.Zobel. Matching techniques fo r\nlarge music databases. In D. Bulterman, K. Jeffay, and\nH. J. Zhang, editors, Proc. ACM Multimedia Conf. , pages\n57–66, Orlando, USA,November 1999.\n[36] A. L. Uitdenbogerd and J. Zobel. Music ranking tech-\nniques evaluated. In M. Oudshoorn, editor, Proc. Aus-\ntralasianComputerSci.Conf. ,pages275–283,Melbourne,\nAustralia, January 2002.\n[37] E. M. Voorhees. Overview of the TREC-9 question an-\nswering track. In E. M. Voorhees and D. K. Harman,\neditors,Proc. Ninth Text REtrieval Conf. , pages 71–79,\nGaithersburg, USA, November 2000. National Institute of\nStandards andTechnology.\n[38] E. M. Voorhees and D. K. Harman, editors. Proc. Tenth\nTextREtrievalConf. ,Gaithersburg,USA,November 2001.\nNational Institute of Standards and Technology.\n[39] E. M. Voorhees and D. M. Tice. The TREC-8 question\nanswering track evaluation. In E. M. Voorhees and D. K.\nHarman, editors, Proc. Eighth Text REtrieval Conf. , pages\n83–105, Gaithersburg,USA,November1999. NationalIn-\nstitute of Standards andTechnology.\n[40] B. Wang, H. Xu, Z. Yang, Y. Liu, X. Cheng, D. Bu, and\nS. Bai. TREC-10experiments at CAS-ICT:Filtering,web\nand QA. InVoorhees andHarman [38],pages 109–121.\n[41] I. H. Witten, A. Moffat, and T. C. Bell. Managing Giga-\nbytes: Compressing and Indexing Documents and Images .\nMorgan Kaufmann Publishing, San Fransisco, USA, sec-\nond edition, 1999.\n[42] E.ZwickerandH.Fastl. Psychoacoustics: FactsandMod-\nels. Springer-Verlag, Berlin, Germany, second edition,\n1999."
    },
    {
        "title": "Music Information Retrieval systems: why do individuals use them and what are their needs?.",
        "author": [
            "Sara Taheri-Panah",
            "Andrew MacFarlane 0001"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416334",
        "url": "https://doi.org/10.5281/zenodo.1416334",
        "ee": "https://zenodo.org/records/1416334/files/Taheri-PanahM04.pdf",
        "abstract": "conducted on the behaviour of music information retrieval (MIR) users, in spite of the immense popularity of free music retrieval systems available on the Internet. In this study we examine the issue of music seeking behaviour through the examination of users life style effect of three different age groups using questionnaires. It was found that lifestyles had a significant impact on users need for music and hence their music seeking behaviour. The importance of social networks in music information seeking was reinforced in this study. An experiment was conducted with three different types of search on the Kazaa MIR system and the participants interviewed in order to collect data. Users found the Kazaa system intuitive and easy to use. Searchers used both song titles and lyrics for finding relevant music items. The insights provided by this study can be of assistance in the development of user focused Internet MIR systems. Keywords: user music seeking behaviour retrieval",
        "zenodo_id": 1416334,
        "dblp_key": "conf/ismir/Taheri-PanahM04",
        "keywords": [
            "music information retrieval",
            "free music retrieval systems",
            "internet music seeking",
            "user behaviour",
            "age groups",
            "questionnaires",
            "lifestyles",
            "music seeking behaviour",
            "social networks",
            "Kazaa MIR system"
        ],
        "content": "Music Information Retrieval systems: why do individuals use them and\nwhat are their needs?\nS. Taheri-Panah A. MacFarlane\nCentre for Interactive Systems\nResearch\nDepartment of Information Science\nCity University, LondonCentre for Interactive Systems\nResearch\nDepartment of Information Science\nCity University, London\nEmail: andym@soi.city.ac.uk*\nABSTRACT\nAbstract: To date there has been very little research\nconducted on the behaviour of music information\nretrieval (MIR) users, in spite of the immense popularity\nof free music retrieval systems available on the Internet.\nIn this study we examine the issue of music seeking\nbehaviour through the examination of users life style\neffect of three different age groups using questionnaires.\nIt was found that lifestyles had a significant impact on\nusers need for music and hence their music seeking\nbehaviour. The importance of social networks in music\ninformation seeking was reinforced in this study. An\nexperiment was conducted with three different types of\nsearch on the Kazaa MIR system and the participants\ninterviewed in order to collect data. Users found the\nKazaa system intuitive and easy to use. Searchers used\nboth song titles and lyrics for finding relevant music\nitems. The insights provided by this study can be of\nassistance in the development of user focused Internet\nMIR systems.\nKeywords: user music seeking behaviour retrieval\n1. INTRODUCTION\nMore than ever music has become an important and\nprominent part of our culture with each individual having\ntheir own preference for the type of music they listen to,\nhow they retrieve it and where they listen to it. There are\nmany ways in which individuals listen to music; some of\nthese are socially in a bar with friends, whilst exercising\nat the gym, alone at home etc.  The introduction of\nInternet services such as Napster [1] and Kazaa [2] has\ngiven users the ability to gain access to large amounts of\ndigital music previously inaccessible other than by\npurchase in Music stores/shops. The free availability of\nmusic recently means that the music industry has lost its\ncontrol to some extent over the release and distribution\nof music. Some of the causes for this growth are evident\nand self-explanatory e.g. financial reasons.  We can\nidentify the availability of free music as one of the main\nreasons for their success, but there are many more.\nAdvances in the underlying technology and theinfrastructure of both the Internet and computer\nmachinery has made the availability of multi media as\nprevalent as text documents [3].  The scale of the effect\nthese factors are having on music seeking behaviour is\nillustrated by the Pew Internet & American Life Project\n[4].  Through their research they reported that from the\ninternet activities offered to internet users, music\nretrieval has by far excelled other activities in both its\nuptake and in its usage e.g. it was reported that in 2000\njust over 1.1 million people had used Napster [1],\nincreasing by 506.8% to 6.7 million just six months later.\nIt is this dramatic change in so many people’s music\nretrieval habits which became the driving force for this\nstudy, with the focus on the music seeker needs and\nopinions.\nSome studies have already been conducted on\nmusic information retrieval on the Internet, but the\nmajority of these have only considered the systems\nperspective.  Although Music Information Retrieval\n(MIR) have developed considerably over the past  few\nyears, there are many aspects of them that require further\ninvestigation [5].  An important aspect of this whole\nphenomenon which has been somewhat neglected is the\nnormal MIR user.  These are individuals who are not\nmusic experts or professionals: we concentrate on this\nuser group in this paper. The paper has the following\nstructure. Section 2 describes the research aims and\nobjectives. In section 3 we outline the Kazaa system used\nfor data collection purposes. Section 4 describes the\nresearch design, while the results from the evaluation are\ndiscussed in section 5. A conclusion is given at the end.\n2. RESEARCH AIMS AND OBJECTIVES\nThere is a paucity of research in the study of MIR\nsystems from a user’s perspective.  With the availability\nof new music formats and retrieval methods on the\nInternet, music seekers have many more options\navailable to them.  A study addressing the needs of\nmusic seekers behaviour is therefore essential.  The\nfocus of this study is to identify and understand the\nneeds of the music seeker, through a series of\nquestionnaires and  interviews. The aim is to give a\nbetter understanding of music seekers and their music\nretrieval needs. In this study there are two central issues\nthat will be addressed.  The first is how, when and why\npeople use music.  The second is how well are the needs\nof music seekers met by these Internet MIR systems.Permission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided that\ncopies are not made or distributed for profit or commercial\nadvantage and that copies bear this notice and the full citation on\nthe first page.\n© 2004 Universitat Pompeu Fabra.The aim of this study is to learn what music seekers are\nlooking for and how they perform search.\n3. THE KAZAA SYSTEM\nFigure 1: The Kazaa system Architecture [6]\nWe use the Kazaa System [2] for our data collection\npurposes.  Kazaa is a file sharing peer to peer network\nsystem which is deployed for MIR. A Kazaa server has\nits own set of files and search facility.  The users of\nKazaa have access to many types of files including music\nfiles, pictures, videos, software and even text documents,\nthus Kazaa is a multi media retrieval system [2]. Each\nKazaa user’s computer itself acts as a search engine and\nlocates the file in the computer of other Kazaa users.\nThe system designates some users who have more\npowerful computers as a supernode.  Once a Kazaa user\nenters a query the request is forwarded to a supernode\nwhich firstly conducts an internal search for the\nrequested file in its own archives.  If the file is located in\nthe supernode the seeking computer is informed and the\nitem retrieved.  If the supernode does not have the file\nthe query is forwarded to other computers connected to\nthe Kazaa system.  Once the file has been found the\nseeking computer is informed through the string of\ncomputers the request passed through. The architecture\nof this system is shown in figure 1. We chose to use this\nsystem as Kazaa Media Desktop has been reported to be\none of the most popular MIR since Napster was shut\ndown.\n4. EXPERIMENTAL DESIGN\nWe acquired primary data for this study by using\nquestionnaires and interviews. The following gives an\noverview of the design of these elements for the\nexperiment.\n4.1. Questionnaires\nThe purpose of the questionnaire was to acquire more\ninformation about music and associated issues.  Therewere four main issues that were addressed by the\nquestions. These were:\n• The methods of music retrieval and collection\npeople use.\n• Music usage and its use in conjunction with\nother activities.\n• Music retrieval system (MIR) usage.\n• The needs of the users\nTo ensure the questionnaires were efficient and\ninformative, a pilot questionnaire was conducted on 10\nindividuals.  These questionnaires were given to\nindividuals randomly in a variety of locations.  The\nresults from the pilot questionnaires were used to\nimprove and produce a final version of the questionnaire\nto use in the study. This research was conducted in July\nand August of 2003. The revised questionnaires were\ngiven to 45 individuals to complete in a number of\nlocations including universities, libraries and coffee\nshops.  To produce a fair and accurate study the numbers\nin each group and the ratio of males to females was kept\nas balanced as possible.  A summary of the individuals\nwho participated in this study is given in table 1.\nGroup Age\nRangeTotal\nNoNo of\nfemalesNo of\nmales\nA19 and under17 8 9\nB20 – 39 15 8 7\nC40 and over13 7 6\nTable 1: The age groups used in the questionnaire and\nthe number of individuals in each group.\n4.2. Interviews\nPre interview searches were conducted in the researchers\nworking environment, with access to Kazaa.  The\nresearcher remained present to assist the user and help\nthe participants with Kazaa.  This research was\nconducted to provide a more qualitative set of\ninformation for the research. In the experiment, 10\nparticipants were asked to carry out three tasks on Kazaa.\nEach participant was given a worksheet to record their\nresults.  Three tasks were given to the user:\n• Known item (or song) searches.\n• Search of new and old songs from the same\nmusic genre.\n• Search of songs from different genres.\nThe researcher went through each task on the\nworksheet before the observation session started.  The\nparticipants were given the option to look for artist’s\nnames and song titles using Internet search engines if\nthey could not think of any themselves. Once the search\ntasks were complete the participants were interviewed\nabout their experience with the Kazaa MIR.5. EXPERIMENTAL RESULTS\n5.1. Analysis of the user group\nThe data in table 1 above has been combined with the\ninformation regarding the amount of free time available\nfor listening to music (see figure 2).\n0%10%20%30%40%50%60%70%80%90%100%\n19 and under 20-3940 and over\nAge groupsFfree time (%)81-100%\n61-80%\n41-60%\n21-40%\n1-20%\n0%\nFigure 2: Relationship between an individual’s age and\ntheir free time listening to music.\nA number of interesting observations about this group\ncan be made from this data. Firstly as age increases less\ntime is spent listening to music.  Interestingly in the 19\nand under age group we see a wide spread of data.  This\nsuggests that the individuals within the older age groups\nhave similar habits, particularly with regards to music.\nLooking more generally at the music usage across all\nages we can infer that the younger music users have more\nflexibility with their time.  For the older participants on\nthe other hand, time is restricted because of the extra\nresponsibilities they have.  This shows that the younger\nparticipants in our study have more time available to\nthem and can therefore spend more of it on activities\nsuch as listening to music.  Listening to music as an\nactivity was also compared to other activities. Here it\nwas found that in the 19 and under age group, listening to\nmusic was preferred in all of the other categories except\nfor watching TV and socialising with friends.  This\nchanged in the 20-39 age group, where the only preferred\ncategory to music was watching TV.\nFor the 40 and over group we see a slight shift\nto watching TV and socialising with friends being\npreferred over music.  There is also a fondness and\npreference for reading in this group: music was\nconsistently rated at the third or fourth most popular\nInternet activity across all the age groups.  Interestingly\nthe 19 and under and the 40 and over group both rated\ncommunications as their top Internet activity.  In contrast\npurchasing goods or services was the most popular\nInternet activity for the 20-39 age group.  From this we\ncan only speculate that at this point in life, the\ndeveloping careers and lifestyle of this age group\ncontributes to a more hectic schedule than the other two\nage groups.  It is possible that more of these individuals\nare likely to purchase goods using the Internet since they\nmay not have a lot of spare time.Considering the life styles of the other two\ngroups, we would expect them to be more settled.\nConsequently they have more time to dedicate to less\nimportant Internet activities as in this case for\ncommunication purposes. The Internet activities of these\nindividuals may perhaps be connected to the level of\ncomputer knowledge and experience of each age group\n(see figure 3).  This clearly shows how the participants of\nthe age groups are distributed in accordance of their IT\nliteracy.  The majority of the beginners were the 19 and\nunder group followed by the 40 and over.  This is\nplausible since many 20-39 year olds use computers on a\ndaily basis for work.  This would give them the ability to\nperform more complex activities using their computers\nand the Internet.\nComputer knowledge of each age group\n0%20%40%60%80%100%\n19 and under20-3940 and over\nThe age groupsExpert\nIntermediate\nbeginner\nNever used\nFigure 3: The level of computer knowledge across the\nage groups.\n5.2. Analysis of the participant’s musical\nrequirements\nOur results illustrated the relevance and importance of\nmusic in all age groups.  We found that a high percentage\nof the participants in this study pay particular attention to\nthe lyrical contents of songs. The majority of people\nacross the age groups used music to fulfil two common\ncategories; music that makes you feel happy and music\nthat masks problems.  However the majority of the\nyounger individuals, including the 19 and under and the\n20-39 age groups also listened to music to make them\nfeel energetic whilst their older counterparts used it for\nrelaxation.  This is credible if we consider the difference\nin the social activities of the two younger groups: the\nmajority of these participants said that they mainly listen\nto music outside of their homes.  The explanation\nsuggested for this lies with the social activities of these\nage groups, where going to the gym, bars and clubs is\nmore common.\nIt was found that an influential factor on\npeople’s music choice is in the form of TV and radio.\nFriends were also found to be an influence since music is\nused for many social activities for both young and old:\nthe majority of our participants said that they listen to\nmusic in the company of others. The influence friends\nhave on each others music taste is considerable:\noverwhelming 96% of the participants said that they\ndiscuss music with their friends. With these findings wecan assert that people inform each other about music that\nthey purchase, listen to and possibly the music retrieval\nmethods they use.  In order to examine this issue in more\ndetail, the participants were asked to estimate the number\nof their friends who they share their music taste with.\nThe majority of the participants across all of the age\ngroups, in particular the 19 and under group, shared their\nmusic taste with most of their friends.\n5.3. Analysis of the experiment and interviews\nThe majority of the individuals who participated in the\nobservations and completed the mini-questionnaire had\nused MIR before.  Altogether around 80% of these\nindividuals claimed to have used MIRs, with Kazaa\nbeing the most popular.  This is important since the\neffective use of these systems depends on who the\nexpected users are and their level of knowledge [7].\nAfter having used the Kazaa MIR in this observation\nmost people said they would speak of it favourably,\nirrespective of prior usage.  Users were able to use Kazaa\nwith little or no training. The new users of Kazaa were\ninquisitive and did enquire about some of the more\nunique features offered by this system, such as the theatre\nfunction. Although when the user becomes aware of the\navailability of images and videos, this turns out to be\nself-explanatory. The participants were asked to rate\nsome of the features of the system from very important to\nvery unimportant (with the values of 5 to 1 respectively).\nFrom this a score was then calculated in accordance to\nhow all of the individuals rated the features.  With this\nscore it is easier to determine the most important feature\nfor all of the participants.  The evidence from calculated\nscores showed that the speed of MIR along with the\nquality of the music are the most important features to\nthe participants.  Unfortunately neither one of these\nfeatures is controlled by the Kazaa system.  The speed\ndepends primarily on the type of Internet connection the\nKazaa users are using.  The quality of music depends on,\nwhat type of files the users share on the system through\ntheir computer. A good search facility was also\nconsidered to be important by the participants; this is\nlogical since a good search engine is necessary in\nlocating files on the system.  This is a feature that can be\ncontrolled by the system administrators and they can\nmake it as flexible or as precise as they want.  There are\nbenefits to both of these; a more precise search engine\nwill return fewer but more accurate files.  In comparison\na more flexible search engine will be able to locate\nslightly different variations of the search term, giving the\nMIR user the opportunity to find items that have been\nspelt incorrectly.  In all these instances a rigid search\nengines would intentionally block out  items user may be\ninterested in.\nIt was found that the most preferred and used type of\nquery by these individuals was by using the titles of\nsongs, which was followed by using the artist’s name.\nSurprisingly some individuals said that they used lyrics\nfrom songs to find a specific item. This is important toexplore in further studies since occasionally individuals\nmay not know or remember the details of a particular\nsong [8].  This suggests that those who share items and\nthose who look for items, pick up on and enter the same\nparts of a song instead of the title. A particular feature\nthat was favoured by the participants was that this\nsystem gave its users the ability to look through the\nitems of the users connected to the network. The\nexplanation for this is that if an individual has one music\nitem that a music seeker likes and wants, then it is likely\nthat they have a few more (that are not being directly\nsearched for by the music seeker).  This brings back the\nsocial issues in music sharing. Most of the participants\nmentioned that they thought the results of the advanced\nsearch were not as good or as accurate as they had\nexpected.  The normal quick search was found to be\nmore efficient.  When asked about the provision of\nadditional features, the participants did not think that\nsupplying information on these systems is necessary for\nthe most part.  However, the participants did suggest that\ninformation on artists would be useful.\n6. CONCLUSIONS AND FURTHER RESEARCH\nBy identifying the importance of music to users, the\nissues surrounding music retrieval have been explored.\nFrom this research, it was found that different people\nused it for different reasons: for personal enjoyment in a\nvariety of circumstances and with friends. Through\nanalysing the interviews it was found that these music\nretrieval systems have been designed with a variety of\nmusic seekers in mind.  That is they are aimed at users\nwith novice to expert knowledge of computers and the\nInternet. The system used text queries to locate and\nretrieve items, which is suitable for almost all music\nseekers. Although the more experienced music seekers\nand the music professionals would benefit from\nconducting their searches more accurately using music\npitch and rhythm, the ordinary user would find it difficult\nto use such evidence in their searches [11].  However,\nfrom this research it appears that for the most part, the\nneeds of music seekers are currently being met by MIR\nsystems. It should be noted that this issue is causing\nsignificant copyright problems [12,13], which could\npotentially have a significant impact on the usage of MIR\nsystems such as Kazaa.\nFurther research in this area is essential,\nparticularly as the music seeking behaviour of users will\nchange as they become older – for example it is clear\nthat the behaviour of users in the over 40 group will\nchange when those with more advanced IT and Internet\nskills move into that group. Therefore while we are sure\nthat our sample is representative in the current time\nframe, it may not be in the next five or ten years due to\nchanges in the makeup of the groups. It is therefore\nimportant to keep monitoring the behaviour of users to\ntrack any changes as they occur. A further issue is\nunderstanding precisely what the ‘information problem’\nis in music retrieval. Textual information retrieval has\nsome understanding of this issue [14], but it is unclearwhat such a concept means in music retrieval, if indeed\nit has any meaning at all. User studies are useful, but\nsome kind of cognitive framework is required if we are\nto better understand the music seeking behaviour of MIR\nusers.\n7. ACKNOWLEDGEMENTS\nWe are grateful to Tim Crawford for his advise in\npreparing this paper.\n8. REFERENCES\n[1] Smith, T. (2000) Napster has huge number of\nusers shock! [online] Available on:\nhttp://www.theregister.co.uk/content/6/13789.h\ntml  [visited  22nd April 2004]\n[2] Kazaa web site, http://www.kazaa.com [visited\n22nd April 2004]\n[3] Tang, N. (1999) Multimedia Information Retrieval\nSystems: An Overview, [online] Available on:\nhttp://www.cs.ucla.edu/~tang/papers/multimedi\na-IR.pdf [visited 22nd April 2004]\n[4] Graziano, M. and Rainie, L. (2001) The music\ndownloading deluge: 37 million American adults\nand youths have retrieved music files on the\nInternet. Available on:\nwww.pewinternet.org/reports/pdfs/PIP_More_\nMusic_Report.pdf  [visited 22nd April 2004]\n[5] Byrd, D. and Crawford, T. (2002) Problems of\nmusic information retrieval in the real world,\nInformation processing and management,38\n(2), pp 249-272.\n[6] Veltman, M. (2002) The Dutch solution to\npeer-to-peer software: the latest ruling in the\nKazaa-case, [online] Available on:\nhttp://www.kent.ac.uk/law/undergraduate/modu\nles/ip/handouts/2002_3/Kazaa_essay.doc\n[visited 16 June 2003]\n[7] Lippincott,A.( 2001) Issues in content-based\nmusic information retrieval, Journal of\nInformation Science, 28 (2) 2002, pp. 137–142.[8] Cunningham, S. (2002) User studies: A First\nStep in Designing an MIR Testbed [online]\nAvailable on: http://music-\nir.org/evaluation/wp1/wp1_cunningham.pdf\n[visited 22nd April 2004]\n[9] Futrelle, J. (2002) Three Criteria for the\nEvaluation of Music Information, Retrieval\nTechniques against Collections of Musical\nMaterial [online] Available on: http://music-\nir.org/evaluation/wp1/wp1_futrelle.pdf  [visited\n22nd April 2004]\n[10] Chevallet, J. and Nigay, L. Characteristics of users'\nneeds and activities: A design space for interactive\ninformation retrieval systems. Available on:\nhttp://iihm.imag.fr/publs/1996/HCIIRChavellet\nNigay96.pdf [visited 22nd April 2004]\n[11] Uitdenbogerd, A. and Zobel, J. (2000) Music\nRanking Techniques Evaluated, In International\nSymposium on Music Information Retrieval,\n[online] Available on:\nhttp://goanna.cs.rmit.edu.au/~jz/fulltext/acsc02\nuz.pdf [visited 22nd April 2004]\n[12] Beverley, P. (1999).  Protecting Copyright,\nIncreasing Choice and Generating Revenue\nfrom the Internet, in Fox, M\n(2002)Technological and social drivers of\nchange in the online music industry, First\nMonday, [online]  7 (2) Available on:\nhttp://www.firstmonday.dk/issues/issue7_2/fox/\nindex.html [visited 22nd April 2004]\n[13] Austin, S. (2004). Web warning to pop pirates,\nMetro, Friday 26th March, p1.\n[14] Ingwersen, P. (1992). Information Retrieval\nInteraction, Taylor Graham, Available on:\nwww.db.dk/pi/iri [visited 22nd April 2004]"
    },
    {
        "title": "Rhythm and Tempo Recognition of Music Performance from a Probabilistic Approach.",
        "author": [
            "Haruto Takeda",
            "Takuya Nishimoto",
            "Shigeki Sagayama"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418209",
        "url": "https://doi.org/10.5281/zenodo.1418209",
        "ee": "https://zenodo.org/records/1418209/files/TakedaNS04.pdf",
        "abstract": "This paper concerns both rhythm recognition and tempo analysis of expressive music performance based on a probabilistic approach. In rhythm recognition, the mod- ern continuous speech recognition technique is applied to find the most likely intended note sequence from the given sequence of fluctuating note durations in the per- formance. Combining stochastic models of note dura- tions deviating from the nominal lengths and a probabilis- tic grammar representing possible sequences of notes, the problem is formulated as a maximum a posteriori estima- tion that can be implemented using efficient search based on the Viterbi algorithm. With this, significant improve- ments compared with conventional “quantization” tech- niques were found. Tempo analysis is performed by fit- ting the observed tempo with parametric tempo curves in order to extract tempo dynamics and characteristics of performance to use. Tempo-change timings and param- eter values in tempo curve models are estimated through the segmental k-means algorithm. Experimental results of rhythm recognition and tempo analysis applied to classical and popular music performances are also demonstrated. keywords: rhythm recognition, hidden Markov models, tempo analysis, segmental k-means algorithm, continuous speech recognition framework, n-gram grammar",
        "zenodo_id": 1418209,
        "dblp_key": "conf/ismir/TakedaNS04",
        "keywords": [
            "probabilistic approach",
            "modern continuous speech recognition",
            "note sequence",
            "fluctuating note durations",
            "maximum a posteriori estimation",
            "Viterbi algorithm",
            "stochastic models",
            "tempo analysis",
            "parametric tempo curves",
            "segmental k-means algorithm"
        ],
        "content": "RHYTHMAND TEMPO RECOGNITION OF MUSIC PERFORMANCE\nFROMA PROBABILISTICAPPROACH\nHaruto Takeda TakuyaNishimoto ShigekiSagayama\nGraduateSchool of Information Science and Technology\nTheUniversityof Tokyo\nABSTRACT\nThis paper concerns both rhythm recognition and tempo\nanalysis of expressive music performance based on a\nprobabilistic approach. In rhythm recognition, the mod-\nern continuous speech recognition technique is applied\nto ﬁnd the most likely intended note sequence from the\ngiven sequence of ﬂuctuating note durations in the per-\nformance. Combining stochastic models of note dura-\ntionsdeviatingfromthenominallengthsandaprobabilis-\nticgrammarrepresenting possiblesequences ofnotes, the\nproblemisformulatedasamaximum aposteriori estima-\ntion that can be implemented using efﬁcient search based\non the Viterbi algorithm. With this, signiﬁcant improve-\nments compared with conventional “quantization” tech-\nniques were found. Tempo analysis is performed by ﬁt-\nting the observed tempo with parametric tempo curves\nin order to extract tempo dynamics and characteristics of\nperformance to use. Tempo-change timings and param-\neter values in tempo curve models are estimated through\nthesegmental k-meansalgorithm. Experimentalresultsof\nrhythmrecognitionandtempoanalysisappliedtoclassical\nand popular music performances are also demonstrated.\nkeywords: rhythm recognition, hidden Markov models,\ntempoanalysis,segmental k-meansalgorithm,continuous\nspeech recognition framework, n-gram grammar\n1. INTRODUCTION\nTechniques for restoring music score information from\nmusical performances are useful in content-based mu-\nsic information retrieval (MIR). This paper concerns a\nmethodforestimatingthetemporalfactorsofascorefrom\ngivenmusicalperformancedatausingrhythmrecognition\nand tempo analysis.\nMusic score information plays an important role in\nMIR because of its ﬂexibility and its compactness com-\npared to audio signals. A fast query search by melody\nor rhythm pattern is possible using the score data stored\nin a database. In addition, score data is ﬂexible against\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopiesbear this notice and the full citation on the ﬁrst page.\nc°2004UniversitatPompeu Fabra.musical conversion like transposition (key changes). Uti-\nlizing these features, similarity, for instance, can be ef-\nﬁciently calculated between the search query and music\ncontents. Large music databases of audio contents, how-\never, are typically not associated with score information\ncorresponding to the contents. Thus needs for technique\nto obtain or restore score information from the audio sig-\nnals. The technique can also be applied to feature extrac-\ntion for tagging the meta data in MPEG4 contents.\nCurrently,mostmethodsforrestoringsheetmusicscore\nfrom music audio signals consists of two processing\nstages. First,spectrumanalysisofaudiosignalsisdoneto\ndetectpitchfrequencyandonsettimingofeachnoteevent\nin the audio signals. The result can be shown in a piano-\nroll display and can usually be recorded in the standard\nMIDI (Musical Instrument Digital Interface) ﬁle (SMF).\nIn the next step, score information notated by symbols, is\nrestored from the SMF data obtained from the ﬁrst pro-\ncessing stage. Though the audio signal analysis process\nis not a trivial problem, excellent performance is attained\nby several recent efforts, such as “specmurt analysis” [1]\nwhich converts spectrogram into a piano-roll-like display\nnearly equivalent to MIDI data. Alternatively, music can\nbe played with MIDI instruments such as electronic pi-\nano that directly produces MIDI signals, the audio signal\nprocessing step can be skipped.\nNow,thepaperwillfocusonthelatterprocess,assum-\ning that the music performance data is given as a MIDI\nsignal. The methods described in this paper can be ap-\nplied to any performance data which contain note onset\ntiming information.\nQuantization, the conventional method for rhythm ex-\ntraction from MIDI performance, does not work well\nfor expressive music as shown in Fig. 2. Since human\nperformers changes tempo and note lengths both inten-\ntionally and unintentionally to make their performances\nmore expressive, the note lengths deviates so much from\nthe nominal note lengths intended by the performer, that\nsimple quantization of note lengths can not restore the\nintended note length and often results in an undesired\n(funny)score.\nOn the other hand, when human listen to the music,\nthey can usually perceive its rhythmic structure and clap\ntheir hands to the beat of the music. If they have acquired\nmusical knowledge through their musical training, they\ncan evengiveareasonable interpretation of the rhythmasFigure 1. A piano-roll-like result of “specmurt anasilys”\n(top) applied to a real music signal of “J. S. Bach: Ricer-\ncare`a6ausdasMusikalischeOpfer,BWV1079”(scoreat\nbottom) performed by a ﬂute and strings, excerpted from\nthe RWCmusic database [11].\nCC\n\u0000\n\u0000CC\nFigure 2. The result of quantization of a MIDI signal\nby commercial software (lower) compared to the original\nscore (upper) of “Tr ¨aumerei” played on an electronic pi-\nano.\na note sequence since they know which rhythm patterns\nare more likely to appear among all possible rhythms.\nThey do not quantize note lengths they hear, but instead,\nrecognize a sequence of the performed note lengths as a\nrhythm pattern. In summary, the rhythm is something\nnot to quantize but to recognize. Therefore, to estimate\nrhythm patterns from performed note lengths, we focus\nonanalgorithmtorecognizetherhythmpatternsfromthe\nviewpoint of speech recognition.\nWe proposed a new rhythm recognition approach[3, 4]\nin 1999 utilizing probabilistic modeling which is of-\nten employed in modern continuous speech recognition\n(CSR) technology from our viewpoint of strong anal-\nogy between rhythm recognition and speech recognition.\nSpeech recognition[2] takes a feature vector sequence as\ninput and outputs the recognized word sequence, while\nrhythm recognition takes the note length sequence as in-\nput and outputs the rhythm patterns. In the proposed\nmodel, both appearance of rhythm patterns and deviation\nof note length are associated with probability to evalu-\natehowlikelyhypothesizedrhythmpatternsarereallyin-\n    MIDI\nperformance\nrhythmartistic\nexpressionscore\nnote lengths tempo\nTemporal Informationtonset times\ndurations\nIOIs\noffset timesnote values\ntime signiture\nmeasurestempo curves\ntempo dynamicsFigure3. Temporalinformationofperformancedatacon-\nsistsofscoreinformation(rhythm)andartisticexpression\n(tempo).\ntendedinthegivenperformance. Inthisapproach,wede-\nﬁned a probabilistic vocabulary of rhythm words trained\nwith a music database. The rhythm recognition prob-\nlem was formulated as a connected rhythm word recog-\nnition and solved by a continuous speech recognition al-\ngorithm. Thisframeworksimultaneouslyenabledbar line\nallocationbyadding“up-beatrhythm”words,beatrecog-\nnition by preparing two-beat vocabulary and three-beat\nvocabulary connected in parallel, and tempo estimation\nboth for changing tempo and unknown tempo. In this\napproach, the model parameter values can be optimized\nthrough stochastic training, and rhythm recognition can\nbe performed with an efﬁcientsearch algorithm.\nThere have also been several efforts for rhythm recog-\nnition based on probabilistic modeling [5, 6] to estimate\nnote values or beats although the time signature has to\nbe given before recognition, and a prioriprobabilities of\nrhythm pattern is not taken into account. We discuss our\napproach to rhythmrecognition in Section 2.\nIn addition to rhythm, tempo is another important fac-\ntorforMIR.Thoughtheyarebothrelatedtotemporalfac-\ntors in music, rhythm is primarily related to microscopic\nchanges in consecutive note lengths and tempo is more\nrelated to macroscopic and slow changes. As shown in\nFig. 3, these two factors are coupled to yield each of ob-\nserved note durations. Tempo sometimes changes rapidly\nlikeAdagio toAllegro. The local tempo ﬂuctuations\nwithin phrase depend on music genre, style and perform-\ners. Tempo often expresses artistic characteristics of the\nperformance, while rhythm expresses the intended score.\nIf these factors are separately extracted from music per-\nformance, they may be effective for content-based music\nsearch like “music that has overture and allegro”, or “per-\nformance playing that phrase veryslowly”.\nThere are some researches that dealt with performed\ntempo for analyzing the performance characteristics.\nWhile previous works of tempo analysis includes visual-\nization of performance [7] and comparison of perform-\ners background (jazz and classical) by periodic statistics\nof tempo [8], our objective is to extract information that\ncharacterize the performances including tempo changes\nandtempohandlinginphrases. Weproposeatempoanal-\nysis method by estimating partly smooth and continuous\n“tempo curves.” It will be discussed in Section 3.2. RHYTHM RECOGNITION\n2.1. RhythmVocabulary\nExtending the analogy between rhythm recognition and\nspeech recognition, we introduce a “rhythm vocabulary”\nin order to construct a probabilistic model for rhythm\nrecognition. Comparing human knowledge about rhythm\npatterns to a stochastic language model in modern CSR\ntechnology, rhythm patterns can be modeled as a stochas-\nticnotegeneratingprocess. Thismodelgeneratesthenote\nsequence of a rhythm pattern associated with a probabil-\nity that varies on music genres, styles, and composers of\na“rhythmvocabulary”. The“rhythmvocabulary”consists\nofunits(thistime,onemeasure)called“rhythmword”. A\nrhythmvocabularyandagrammarofrhythmwordscanbe\nobtained through stochastic training using existing music\nscores.\nOne advantage of using rhythm words for modeling\nrhythmpatternsisthatmeterinformationcanbeestimated\nsimultaneously along with notes. Thus, the locations of\nbarlinesinascorecorrespondwiththepositionofbound-\naries in a rhythm word sequence. Time signature is also\ndetermined by investigating sum of note values in esti-\nmated rhythmwords.\n2.2. ProbabilisticGrammar forRhythmVocabulary\nSimilar to language model of CSR, n-gram model of\nrhythm words is used for a grammar of rhythm vocabu-\nlary. That is, the probability of a rhythm word sequence\nW=fwmgM\nm=1is approximated by cutting out the his-\ntory of rhythmwordappearance,\nP(W) =P(w1;¢¢¢; wn¡1)\n¢MY\nm=nP(wtjwm¡1;¢¢¢; wm¡n+1) (1)\nConditionalprobabilitiescanbeobtainedthroughstatisti-\ncal training using previouslycomposed music scores.\nThen-grammodelreﬂectsthelocalfeaturesofthemu-\nsic passage, but does not the global structure including\nrepetition of rhythm patterns. As is often the case with\nCSR, unknown rhythm patterns in the vocabulary is sub-\nstituted with similar existing patterns. To obtain more re-\nliable values for model parameters, linear interpolation or\nother techniques commonly used for language model can\nbe applied.\n2.3. Nominal Relation of TemporalInformation\nThe observed duration (IOI, Inter-Onset Interval) x[sec]\nof note in the performance is related both to the note\nvalue1(time values) q[beats] in score and the tempo ¿\n[BPM] (beats per minute) as follows:\nx[sec] =60[sec/min]\n¿[beats/min]£q[beats] (2)\n1“Note values” are nominal length of notes. For example, if a note\nvalue of quarter note is deﬁned as 1[beat], that of half note is 2[beats]\nand that of eighth note is 1=2[beat].Table 1. Rhythm word examples and their probabilities\nobtained thorough stochastic training.\nrhythmwords wP(w)\n0.1725\n0.1056\n0.0805\n0.0690\n¢¢¢¢¢¢ ¢¢¢¢¢¢\nHidden Markov Modelx1x2x3x4x1\nx2x3x4\n1q2q31 s2 s3 s\nFigure4. ObservedIOIsandrhythmwordsareassociated\nin the frameworkof Hidden MarkovModels (HMMs).\n2.4. Modeling RhythmWordsusing HMMs\nA rhythm word and a sequence of deviating IOIs are\nprobabilistically related using a Hidden Markov Model\n(HMM) [9].\nSuppose that consecutive nIOIs, xk;¢¢¢; xk+l, and a\nrhythm word, wi=fq1;¢¢¢; qSig, are given, where Si\ndenotesthenumberofnotescontainedintherhythmword\nwi. When several notes are intended to be played simul-\ntaneouslyinpolyphonicmusic,shorttimeIOIs(ideally0)\nare observed, such as x1in Fig. 6. These IOIs correspond\nto the same note value qin a rhythm word wi. We model\nthis situation by using the HMM and associate note val-\nues and observed IOIs. As shown in Fig. 6, HMM states\ncorrespond to note values in a rhythm word, and IOIs are\noutput valuefrom state transitions.\nIntheHMMs,probabilitiesaregiventoeachstatetran-\nsition and transition output. Probability of observing x\nis modeled with a zero-mean normal distribution at auto-\ntransition of state s.as(k)s(k+1)denotes a probability to\nchange from state s(k)to state s(k+ 1). Self-transition\nprobability as;scorresponds to the times of stay in state\ns, that is, the number of notes simultaneously played in\nthe state, whose expectation is given by1\n1¡as;s. Values of\nas;sare automatically determined with statistics of score,\nas shown in Fig. 5. Variation of IOIs that corresponds\nto note values qis assumed to distribute normally with\nmeans60\n¯¿¢qs[sec] and variance ¾2, where ¯¿is the aver-\nage tempo of the previous rhythm word described in 2.5.\nThis corresponds to the output probability of state transi-\ntionbs;s+1(x).\nTherefore,theprobabilitythatarhythmword wiisper-\nformed as a sequence of IOIs fxk0gk+l\nk0=kis givenby\nP(xk;¢¢¢; xk+ljwi)\n=lY\nk0=kas(k0)s(k0+1)bs(k0)s(k0+1)(xk0)(3)0.6413 0.0704 0.5074 0.4211 0.6185 0.0434 0.4107 0.2667self-transition probabilityFigure 5.An example of stochastic training of state transition\nprobabilities: States of strong beats have higher probability of\nself-transition than states of weak beats.\nτττ τ 12\n3 4w1w2w3w4\ntempoa rhythm word sequence\np( 1τ)τ2τ3p( 2τ) τ4p( 3τ) τ5p( 4τ)\nFigure 6. Tempo tracking in each rhythm word using\nprobability of tempo variations.\n2.5. Probabilityof TempoVariations\nTheﬂuctuationoftheperformedtempoisalsotreatedwith\nprobabilities. Since we do not have it a priori knowl-\nedge about the tempo variation speciﬁc to the given per-\nformance,wesimplyassumethatatempoofameasureis\nclose to that of the previous measure. The average tempo\n¯¿in a measure with rhythm word wiis calculated using\nEq. (2) by\n¯¿=lX\nk0=kxk,SiX\ns=1qs\nWe give conditional probability for consecutive aver-\nage tempo P(¯¿mj¯¿m¡1)by assuming that the difference\nlog ¯¿m¡log ¯¿m¡1in log scale distributes normally with\nmean 0.\nThen, the probability that an IOI sequence Xis ob-\nservedforagivenwordrhythmsequence W,P(XjW)is\nobtained from the product of Eq. (3) and the probability\nof tempo variations\nP(XjW) =MY\nm=1P(xl(m);¢¢¢; xl(m+1)¡1jwm)P(¯¿m+1j¯¿m)\n(4)\nwhere xl(m)denotestheﬁrstIOIinthe m-thrhythmword.\n2.6. MAP Estimation forRhythmRecognition\nBy integrating these probabilistic models, rhythm recog-\nnition can be formulated as a MAP estimation problem.\nUsingarhythmvocabulary,rhythmrecognitioncanbede-\nﬁned to ﬁnd the “most likely rhythm patterns” ˆWfor a\ngivenIOI sequence X. According to the Bayes theorem,\nˆW= argmax\nfwmgM\nm=1P(WjX) = argmax\nfwmgM\nm=1P(XjW)P(W)\n(5)\nwherethenumberofrhythmwords, M,isalsovariablein\nthe search. In our model, Eqs. (1) and (4) are substituted\nwith Eq. eq:argmaxW.\nperformanceHMMs (rhythm words)w1w2w3w4w5scorem\nkIOIsx1x2x3x4x5 hidden states (note values)\nthe most likely rhythm word\nin each level\nw1w2w3the most likely state transition\nin each rhythm word\nthe most likely rhythm pattern  (a sequence of rhythm words)Level building algorithmViterbi search algorithm\nδ(\n\u0000 \u0001\u0003\u0002\u0005\u0004 \u0006\b\u0007\n\t\f\u000b\u000e\r\nδ(\n\u0000\u0001\n\u0006\u000f\u0007\u0010\rFigure 7. Network search to ﬁnd the optimal rhythm-\nword sequence and the optimal state sequence using the\nViterbisearch algorithm.\n2.7. SearchAlgorithm\nFinding the most likely rhythm word sequence in Eq. (5)\nis a search process in a network of HMMs that, in turn,\neach consist of state transition networks. Several search\nalgorithm developed for CSR can be applied for this pur-\npose,sincemodelsofbothrecognitionssharethecommon\nhierarchal networkstructure.\nThis time, we implemented the search using the Level\nBuilding algorithm [10]. In the following algorithm,\n±(t; m)stands for the highest cumulative likelihood for\nthetth IOIs with mrhythm words. The Viterbi algorithm\nis used for calculating ±(t; mjw).\n——————————————————\nform=1to maximum numberofbarlines\nfor every w0in rhythm vocabulary\nfort=1to numofnotes\n±(t; mjw1;¢¢¢; wm¡1; w0)\n= max\nt0±(t0; m) +d(t0; tjw0)\nfor every t=1toT\n±(t; m) = max\nw±(t; mjw)\nˆW= argmax\nm±(T; m)\n——————————————————\n2.8. Experimental Evaluation\nThe proposed method was evaluated with performance\ndataplayedbyhumanwithelectronicpiano2andrecorded\nin SMF as listed in Table 2. Data M1 consists of rela-\ntively simple rhythm patterns and was played with nearly\n2YAMAHAClavinova.Table2. Testdata for rhythmrecognition experiments.\ndataID musicpiece\nM1J.S. Bach: Fugain c-moll, BWV847.\nfromDas wohltemperierteKlavier,Teil1.\nM2R.Schumann: “Tr ¨aumerei”\nfrom“Kinderszenen,”Op. 15, No. 7.\nM3L.v.Beethoven: 1st Movementof\nPianoSonata, Op. 49-2.\nM4W.R. Wagner: “Brautchor”\nfrom“Lohengrin”\nM5TheBeatles: “Yesterday”\nM6TheBeatles: “Michelle”\nTable3. 3 conditions of constructing rhythmvocabulary.\ncondition training data #rhythmwords\nclosed1 eachof testing data (M1 »M6)14,10,12,16,9,8\nclosed2 22pieces including testing data 162\nopen16pieces excludingtesting data 139\nconstant tempo. On the other hand, the tempo of M2\n(Tr¨aumerei) changed much in the performance accord-\ning to the tempo indication of ritand the performers’\nindividual expression. M3 tends to be played with con-\nstanttempo,butrhythmpatternsincludeeighthandtriplet\neighth notes.\nTo construct of rhythm vocabulary, a bigram model\n(n= 2in Eq. (1)) was trained under 3 conditions listed\nin Table 3. The ﬁrst condition “closed 1” is the most spe-\nciﬁc condition of the three, where the rhythm vocabulary\nhas been extracted from the testing music material. The\nsecond condition “closed2” shares the same rhythm vo-\ncabulary extracted from all testing materials. Under 3rd\ncondition “open”, the model has been acquired from 16\nmusicpiecesdifferentfromtestingmaterials. Inthiscase,\nsomerhythmpatternsinthetestingmusicmaybemissing\nin the trained vocabulary.\nAccuracyofnotevalues qforeachIOI xwasevaluated\nbyN¡S\nN, where Nis the number of IOIs and Sdenotes\nthe number of misrecognized IOIs. Also, accuracy both\nof rhythm-words in each measure and of locations of bar\nlines were evaluatedby:\nAcc=N¡D¡I¡S\nN\nwhere I,S,Ddenote insertion, substitution and deletion\nerrors, respectively, and Nis the number of measures in\nthe original score.\nTables 4, 5 and 6 show results of rhythm recognition\nsigniﬁcantly superior to the note value accuracy obtained\nby the quantization method: 14.4–18.8%. A typical mis-\nrecognitionisduetofailuretotracktempoinseveralparts\nwhere the tempo changes much within a measure as a re-\nsultoftheindicationof rit.orperformerexpression. Since\nwe modeled tempo as constant within a rhythm word, the\nHMM could not adapt to such a rapid tempo change. An-\nother typical misrecognition was that eighth notes wereTable 4. Accuracy of note value qof IOI xin the perfor-\nmance [%].\nmodel M1M2M3M4M5M6ave.\nclosed 1 99.899.710010010010099.9\nclosed 2 98.595.710010093.794.296.4\nopen89.862.380.748.390.090.676.9\nTable5. Accuracyofrhythmword winrhythmscore[%].\nmodel M1M2M3M4M5M6ave.\nclosed 1 10095.810010010010099.3\nclosed 2 93.388.010010070.896.591.4\nopen60.046.068.418.845.886.254.1\nTable6. Accuracyof bar line allocations [%].\nmodel M1M2M3M4M5M6ave.\nclosed 1 10099.710010010010099.9\nclosed 2 10083.310010087.510093.7\nopen46.650.078.960.454.110065.0\nFigure 8.Tempo [BPM] for IOIs in piano performance of\n“Michelle” by The Beatles.\nsometimes misrecognized as triplets. Recognition perfor-\nmance degraded for “open-data” training cases most pos-\nsibly due to insufﬁcienttraining data.\n3. TEMPO ANALYSIS\n3.1. MultilayerTempoCharacteristics\nAfterrhythmrecognitionoftheperformedmusicdata,in-\nstantaneous local tempo ¿k=xk\nqkcan be calculated from\nthe observed IOI xand estimated note value qaccording\nto Eq. (2). As the estimated instantaneous local tempo,\nhowever, ﬂuctuats almost randomly as shown in Fig. 8,\ntempo analysis is necessary to extract the “true” tempo\nunderlying behind the observedtempo.\nWe assume that musical performances contain hier-\narchical (multilayer) tempo-related factors with differenttime scales. For example, each measure contains rhyth-\nmiccharacteristicsbasedontraditionalmusicstyles,such\nas Wiener Waltz, Polonaise, etc. The melody phrase may\nbecharacterizedbytheperformers’articulationsortempo\ncontrolstylesaccordingtotheirartisticexpression. Music\nworks are often composed of several parts, each with its\nowndifferenttempoindication,andincludedrastictempo\nchanges in the music pieces.\nOur strategy for obtaining tempo characteristics of\neach hierarchical structure is to ﬁt the performed tempo\nwithin time segments to a tempo pattern by optimizing\nthe model parameters, and also to cluster several con-\nsecutive measures in order to form tempo curves. In\nthe proposed model, slow changes in tempo are modeled\nas a tempo curve in each segment, while drastic tempo\nchanges are dealt as boundaries between different seg-\nments. The rhythm recognition discussed in Section 2\nprovides a method to estimate the note sequence given\na sequence of IOIs in Eq. (2). In this section, we pro-\nvide a method for tempo analysis by detecting timings of\ntempochangesandbyﬁttingatempocurvetopartialmu-\nsic phrases.\n3.2. Formulatingthe TempoCurve\nSincethesequenceoflocaltempos f¿kgN\nk=1includesﬂuc-\ntuations and deviations in the performance, we model\nthe performed tempo with multiple concatenated smooth\ntempo curves where a tempo curve ¿(tjµ)is a continuous\nfunctionoftime t[sec]withparameters µandmodeledby\npolynomial function in the logarithmicscale, i.e.,\nlog¿(tjµ) =a0+a1t+a2t2+¢¢¢+aPtP(6)\nwith parameters µ=fa0;¢¢¢; aPg.\nNow, we assume that the difference between the ob-\nservedtempo ¿kandthemodeledtempo ¿(tnjµ)atthe n-\nthonsettimeonthetempocurve,i.e., ²k= log( ¿(tkjµ))¡\nlog(¿k), can be regarded as a probabilistic deviation from\nanormaldistributionwithmean0andvariance ¾2. There-\nfore, the simultaneous probability of deviations of all\nnotes is givenby:\np(²1;¢¢¢; ²N)\n=NY\nk=11p\n2¼¾2expµ\n¡(log¿k¡log¿(tkjµ))2\n2¾2¶\n(7)\n3.3. Probabilityof TempoChanges\nIn this paper, we assume that tempo is nearly constant\nwithin segments and sometimes changes drastically be-\ntweenthem. Wemodeltheprobabilityofchangingtempo\nbetween consecutivesegmentsby:\nP(¯¿k;¯¿k+1) = 1¡expµ\n¡(¯¿k¡¯¿k+1)2\n2¾2¶\n(8)\nwhere ¯¿kistheaveragetempowithinasegmentinthe k-th\ntempo model,¯¿k=Sr+1¡1X\nk=Sr¿kxk,Sr+1¡1X\nk=Srxk\nandSkindicatestheindexoftheﬁrstnoteofthe k-thseg-\nment. Eq. (8) yields a probability of 0when tempo stays\nthe same value ¯¿k= ¯¿k+1.\n3.4. MAP Estimation of TempoAnalysis\nWe use the maximum a posteriori probability as a cri-\nterion for optimizing the model in order to ﬁnd the best\nﬁtting tempo patterns and to detect the timings of tempo\nchanges. In other words, given the sequence of onset tim-\nings of a performance and the corresponding note val-\nues, the most likely tempo curves are estimated. With the\nBayes theorem, the tempo analysis can be written as:\nˆT= argmax\nTP(TjX; Q) = argmax\nTP(X; QjT)P(T)\n(9)\nwhere Tdenotes the tempo curve, Xthe performance,\nandQthe score information. This time, P(X; QjT)\nis given in Eq. (7), and P(T)in Eq. (8), and by\ntaking logarithm of them, Eq. (9) is found equivalent\nand can be used in ﬁnding concatenated tempo curves\n¿(tjˆµ1;¢¢¢;ˆµ1;ˆS1;¢¢¢;ˆSR¡1)with the parameters esti-\nmated by:\nfˆµ1;¢¢¢;ˆµR;ˆS1;¢¢¢;ˆSR¡1g\n= argmax\nfµgK\nk=1;fSrgR¡1\nr=1RX\nr=1(¡d(mr; mr+1jµr) +a(¯¿r;¯¿r+1))\n(10)\nwhere\nd(Sr; Sr+1jµr)\n=1\n2Sr+1¡1X\nk=Sr·\nlog(2¼¾2) +(log¿k¡log¿(skjµr))2\n¾2¸\na(¯¿r;¯¿r+1) = logµ\n1¡exp(¡(¯¿r¡¯¿r+1)2\n2¾2)¶\nandRis the number of sudden tempo alternations and is\nalsothevariableusedinestimation. The r-thtempocurve\n¿(tjµr)isdeﬁned only in the range of tSr·t < t Sr+1.\n3.5. Optimization Algorithm of TempoModel\nOptimization of the model expressed by Eq. (10) can be\nachieved using the segmental k-means algorithm [2]. Af-\nter the initial boundary is given, this algorithm is per-\nformed by iterating 2 steps: optimization and segmenta-\ntion (see Fig. 9).\nOptimization Step\nParametersofeachrhythmpatterncanbeoptimizedby\nminimizing d(mr; mr+1¡1). Since this function is con-\nvexforthefunction ¿(t),minimizationcanbeformulatedtime tempo [bpm]segmentation to set boundaries for tempo curvesoptimization of model parameters\nss ss11 22θ1θ1 θ2θ2θ3θ3σ2σ2τ(t|θ1) θ τ(t|θ3)\n\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\f\u000b\u000e\r\u0010\u000f\u0011\u0001 \u0000\u0012\u0001\u0013\u0003\u0006\u0005\b\u0007\n\t\f\u000b\u000e\r\u0010\u000f\u0011\u0001 \u0000\u0012\u0001\u0013\u0003\u0006\u0005\b\u0007\n\t\f\u000b\u000e\r\u0010\u000f\u0011\u0001τ(t|2)\ntime tempo [bpm]\nobserved tempoFigure 9. Iteration of segmentation and curve ﬁtting in\nthe segmental k-meansalgorithm. (conceptual diagram)\nbased on variable principle and carried out by setting the\nfunctional derivative ±d(mr; mr+1¡1)tobe 0.\nSr+1¡1X\nk=Sr(log¿k¡log¿(tkjµr))¢±log¿(tk) = 0\nFrom this, the optimal parameters of the model polyno-\nmial (Eq. (8)) are found by solving the following P+ 1\nequations:\nSr+1¡1X\nk=Srlog¿k¢tp0\nk¡Sr+1¡1X\nk=SrPX\np=0t(p+p0)\nk¢ap= 0\nwhere p0= 0;1;¢¢¢; P.\nVariance ¾2inEq. (7)isalsooptimizedforallsamples\nin the observedlocal tempo data with\nˆ¾2=1\nNRX\nr=1Sr+1¡1X\nk=Sr³\nlog¿k¡log¿(tkjˆµr)´2\nIn this optimization step, the parameters are updated for\neach of tempo curves fµrgR\nr=1and the variance of the\ntempo deviation ¾2.\nSegmentation Step\nBoundariesofthesegmentedregionofthetempocurve\ncan be found efﬁciently using DP (Dynamic Program-\nming) algorithm to maximize the objective function. We\ndenote the cumulative log likelihood of m-th measure in\nther-th tempo curve by ±r(m), the number of measures\nbyM,andtheorderofeachmeasureby m. Thealgorithm\nis:\n——————————————————————\nr=1\nform=1toM\n±0(m) =d(0; m)\nforr=2toR\n\u0000\u0001\u0003\u0002\u0005\u0004\u0007\u0006\t\b\u000b\n\f\u0002\r\u0006 \u000e\u0001 \u0001\u0003\u000f\n\u000e \u0010\u0011\u0013\u0012\u0000\n\u0011\u0013\u0012\u000b\u0014\nδ2(3)δ2(m)\nδ2(m+1)δ2(M)δ(m)δ(m+1) δ(M)\u0001 \u0001\u0003\u000f\n\u000e \u0010δ1(2)3 3 3\u0011\u0013\u0012\u000e d(1,2)d(3,m)d(m+1,M)\n\u0014\n,τ\ntime\ntempo curve 1 tempo curve 2 tempo curve 3time point  of tempo changets1ts2a(τ)32\na(τ,τ)21\u0000\u000e\n\u0006\u0015\u0002\r\u0016\u0017\u0001\u0003\u0002\r\u0018\u000b\u0019\u001a\u0006Figure 10 . Dynamic Programming (DP) to detect bar\nlines at tempo changes.\nform=k+ 1toM\n±r(m) =\nmax\nm02(k;¢¢¢;m¡1)[±r¡1(m0) +d(m0; m) +a( ¯¿r¡1;¯¿r)]\n——————————————————————\nHere,thelastnode ±p(M)givesthelogarithmoftheMAP\nprobability, and the optimal path is obtained by trace-\nback. The most likely boundary is given by the path in\nthe node trellis as shownin Fig. 10.\nThe number of tempo changes Ris estimated with the\nMAP estimator of Eq. (10) by comparing the MAP prob-\nabilities for R= 1;2; :::.\n3.6. Experimental Example\nA musical performance with an electronic piano recorded\nin SMF was modeled by tempo curves using the pro-\nposed model. To demonstrate the algorithm, we used\n“F¨urchtenmachen”3as an example with suddenly alter-\ning tempo between “Schneller(faster)” and original slow\ntempo severaltimes within the piece.\nTwo kinds of tempo curves were tested on the per-\nformance of “F ¨uchtenmachen.” First, using a quadratic\ntempo curve model: log¿(t) = a0+a1t+a2t2, the\ntimings of tempo change were correctly estimated as\nshown in Fig. 11. Next, by ﬁtting linear tempo curves:\nlog¿=a0+a1t, detailed tempo behavior was ex-\ntracted. In the MIDI recording of piano performance of\n“F¨urchtenmachen,” the number of tempo changing time\npoints and the locations of changing bar lines are esti-\nmated correctly.\nTheproposedmethodwasalsoevaluatedinestimation\nofthenumberoftempochangesandthebar-linelocations\nattempo-changingtimings. Theresultswereveriﬁedwith\nMIDI data associated with the RWC music database of\nclassical music [11] which had been manually prepared\nto approximately label the audio recording. Other exper-\nimental evaluation were also successful in RWC-MDB-\nC-2001, No. 1, Haydn’s “Symphony No. 94 in G major,\nHob.I-94‘TheSurprise’,1stmvmt.”,andRWC-MDB-C-\n2001No.13,Mozart’s”StringQuartet”No.19inCmajor,\nK.465, 1st mvmt.\n3A piano piece from “Kinderszenen”, Op. 15, No.11, composed by\nRobert Schumann.\u0000\u0002\u0001\n\u0003\u0002\u0001\n\u0004\u0002\u0001\n\u0005\u0006\u0001\u0002\u0001\n\u0005\u0006\u0007\u0002\u0001\n\u0005\u0006\u0000\u0002\u0001\n\u0005\u0006\u0003\u0002\u0001\n0 5 10 15 20 25 30 35observed tempo\ntempo curve\b\n\t\n\u000b\f\n\r\n\u000e\u000f\n\u0010\u0011\n\u0012\u0013\u0015\u0014\u0017\u0016\u0019\u0018\u001b\u001a\u001d\u001c\u001e\u0018 \u001f\"!Figure 11 . Example of quadratic tempo model ( log¿=\na0+a1t+a2t2)ﬁttorealperformance: tempo-changing\ntimings are detected correctly.\nFigure12 . Exampleoflineartempomodel( log¿=a0+\na1t) ﬁt to real performance: intra-phrase tempo changes\nare observed.\n4. CONCLUSION\nWe have discussed rhythm recognition and tempo analy-\nsisofexpressivemusicalperformances,basedonaproba-\nbilistic approach. Given a sequence of note durations de-\nviated from nominal note lengths in the score, the most\nlikely note values intended by the performer are found\nwith the same framework as continuous speech recogni-\ntion. This framework consists of stochastically deviat-\ning note durations modeled by HMMs and a stochastic\ngrammarof“rhythmvocabulary”expressedwith N-gram\ngrammar. The maximum a posteriori note sequence is\nobtained by an efﬁcient search using the Viterbi and level\nbuilding algorithms. Signiﬁcant improvements have been\ndemonstratedcomparedwithconventional“quantization”\ntechniques. Tempoanalysisisperformedbyﬁttingapara-\nmetric tempo curves to the observed local tempos for the\npurpose of extracting tempo dynamics and characteristics\nof the performance. Timings of tempo changes and opti-\nmaltempocurveparametersaresimultaneouslyestimated\nusing segmental k-meansalgorithm.\nFutureworkincludesintegratingdirectmodelingpoly-\nrhythm patterns, which includes synchronized multi-\nrhythm patterns, to give a direct relation between prob-abilistic models and score data. Validity of the model\nshould also be examined using audio recordings of pro-\nfessional instrumental players.\n5. ACKNOWLEDGEMENTS\nWe thank Chandra Kant Raut for his valuable comments\non English expressionsin this paper.\n6. REFERENCES\n[1] S. Sagayama, K. Takahashi, H. Kameoka, T. Nishimoto,\n“SpecmurtAnasylis: “APiano-Roll-VisualizationofPoly-\nphonic Music Signal by Deconvolution of Log-Frequency\nSpectrum,”Proc. ISCA. SAPA,2004, to appear.\n[2] L. Rabiner, B.-H. Juang: Fundamentals of Speech Recog-\nnition,Prentice-Hall, 1993.\n[3] N. Saitou, M. Nakai, H. Shimodaira, S. Sagayama, “Hid-\nden Markov Model for Restoration of Musical Note Se-\nquence from the Performance,” Proc. of Joint Conf of\nHokuriku Chapters of Institutes of Electrical Engineers,\nJapan,1999, F-62, p.362, Oct 1999. (in Japanese)\n[4] N. Saitou, M. Nakai, H. Shimodaira, S. Sagayama: “Hid-\nden Markov Model for Restoration of Musical Note Se-\nquence from the Performance,” Technical Reports of Spe-\ncialInterestGrouponMusicandComputer,IPSJ,pp.27–\n32,1999. (in Japanese)\n[5] A. Cemgil, B. Kappen, P. Desain, H. Honing: “On tempo\ntracking: Tempogram Representation and Kalman ﬁlter-\ning,”J. NewMusic Research, vol.29, no. 4, 2000.\n[6] C.Raphael: “AutomatedRhythmTranscription,”Proc.IS-\nMIR,pp. 99–107, 2001.\n[7] P. Trilsbeek, P. Desain, H. Honing: “Spectral Analysis\nof Timing Proﬁles of Piano Performances,” Proc. ICMC,\n2001.\n[8] S. Dixon, W. Goebl and G. Widmer The Performance\nWorm: “Real Time Visualisation of Expression Based\non Langner’s Tempo-Loudness Animation,” Proc. ICMC,\npp361-364. 2002.\n[9] L. R. Rabiner, B. H. Juang: “An Introduction to Hidden\nMarkovModels,”IEEE ASSP magazine,pp. 4–16, 1986.\n[10] C. Myers, L. R. Rabiner: “Connected Digit Recogni-\ntionUsingLevel-BuildingDTWAlgorithm,”IEEETrans.\nASSP,Vol.29, pp. 351–363, 1981.\n[11] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka:\n“RWC Music Database: Popular, Classical, and Jazz Mu-\nsicDatabases,”Proc. ISMIR, pp.287-288, 2002."
    },
    {
        "title": "Pregroup Grammars for Chords.",
        "author": [
            "Richard Terrat"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416702",
        "url": "https://doi.org/10.5281/zenodo.1416702",
        "ee": "https://zenodo.org/records/1416702/files/Terrat04.pdf",
        "abstract": "Pregroups had been conceived as an algebraic tool to recognize grammatically well-formed sentences in natural languages [3]. Here we wish to use pregroups to recognize well-formed chords of pitches, for a given definition of those chords. We show how a judicious choice of basic and simple types allows a context-free grammatical description. Then we use the robustness property to extend the set of well-formed chords in a simple way. Finally we  argue in favor of an utilization of pregroups grammars for the recognition and classification of chord sequences.",
        "zenodo_id": 1416702,
        "dblp_key": "conf/ismir/Terrat04",
        "keywords": [
            "Pregroups",
            "natural languages",
            "well-formed sentences",
            "chords of pitches",
            "context-free grammatical description",
            "robustness property",
            "well-formed chords",
            "classification of chord sequences",
            "algebraic tool",
            "robustness"
        ],
        "content": "PREGROUP GRAMMARS FOR CHORDSLIRMM/CNRS161, rue ADA34000 MontpellierFranceterrat@lirmm.frRichard G. TERRAT&IRCAM1,Place Igor Stravinsky75004 ParisFranceterrat@ircam.frABSTRACTPregroups had been conceived as an algebraic tool torecognize grammatically well-formed sentences innatural languages [3].Here we wish to use pregroups to recognizewell-formed chords of pitches, for a given definition ofthose chords. We show how a judicious choice of basicand simple types allows a context-free grammaticaldescription.Then we use the robustness property to extendthe set of well-formed chords in a simple way.Finally we  argue in favor of an utilization ofpregroups grammars for the recognition andclassification of chord sequences.1. INTRODUCTIONFollowing the seminal work of Noam CHOMSKY [5]attempting to provide a formal description of thesyntax of natural languages, many researchers haveprovided various such formalisms in the musical field.Formal descriptions are proved to be usefultools either for a better comprehension of musicalstructures (with recognition grammars), or with the aimof automatic composition or improvisations (withgenerative grammars).Nevertheless either these grammars are easy toimplement (e.g. context-free) but insufficient toaccount for complex musical structures, or heavilydependent of the context and thus difficult to describeand process.Thanks to recent works we can hope to get outof this dilemma. For recognition grammars, pregroupgrammars play an important role in that work.In this paper we will first summarize in §2 themain definitions and properties of pregroups, then §3will present the concept of type, §4 the one of typingused for the syntactic units of the language while §5specifies the method of recognition ; §6 will show howpregroups can be used for simple examples of chordgrammars and §7 concludes with current prospects.2. PREGROUPS2.1. DefinitionA pregroup is a partially ordered monoid in which eachelement a has both a left adjoint  al and a right adjointar satisfying :Contractional . a  → 1(1)a . ar → 1 Expansion(Generally not needed for syntax verification)1 → a . al(2)1 → ar . awhere “→” denotes the partial order relation, “.” themultiplication and “1” the unit of the monoid.(The multiplication sign will be omitted in the sequel).2.2. Propertiesa 1 = a = 1  a(3)1 is the unit of the monoid(a b) c = a (b c)(4)multiplication is associativea → b  and  c → d  implies  a c → b d(5)order is compatible with multiplicationa b → 1 → b a  implies  a = bl  and  b = ar(6)adjoints are unique(a b)l = bl al        (a b)r = br ar(7)adjunction is quasi-distributivea → b  implies  bl → al  and  br → ar (8)adjunction reverses the orderalr = a = arl(9)no mixed adjointsPermission to make digital or hard copies of all or part of thiswork for personal or classroom use is granted without feeprovided that copies are not made or distributed for profit orcommercial advantage and that copies bear this notice and the fullcitation on the first page.© 2004 Universitat Pompeu Fabra.Properties (3) to (5) are part of the definitionof a monoid ; properties (6) to (9) can be derived fromdefinitions and their proofs can be found in [3].3. TYPES3.1. Basic typesAs linguists, we will work with the free pregroupgenerated by a partially ordered set of so called basictypes which are elements of an enumerable set.Bold face symbols : a, b, c, … will range over basictypes.The order between basic types is declared assuch, and is stored in a table.B = { a, b, c, ….} 3.2. Simple typesSimple types are formed from basic types and theiradjoints. Thus they form an infinite enumerable set :∑ = { … all,al,a,ar,arr , … bll,bl,b,br,brr , … }Simple types inherit the order from the basictypes as follows :if  a→b  then  bl→al, br→ar, all→bll, arr→brr,Hence to check whether a → b , it issufficient to check whether a and b have the sameexponent, i.e. a = as  ,  b = bs  where a and b are basictypes and s consists of a finite number n of repetitionsof the same suffix, either l or r.If a and b have the same exponent, then a →b if either n is even and a→ b, or n is odd and b → a.Otherwise, neither a reduces to b nor b to a.Contractions of simple types can beunderstood as rules such as :all al  → 1 ,  al a → 1 ,  a ar → 1 ,  ar arr →1 3.3. TypesStrings of simple types will be called compoundedtypes or more simply : types.The order → can be read as \"reduces to\".4. TYPING4.1. AssignmentAs a first step, the language to be analyzed has to bedescribed in some grammatical terms. The smallestsyntactic units (words) are supposed to be in adictionary.The next step is to define the set of basictypes and its ordering. Then, to every word, one, ormore generally several types have to be assigned.The typing assignment must be done in such away that the sequence of words constitutes a well-formed construct if and only if the corresponding stringof its types reduces to the basic type corresponding tothe construct. If more than one type is assigned to aword, it is sufficient that one of these types yields astring reducing to the final type.Every typing respecting this condition is saidto be correct, i.e. it allows to recognize only well-formed constructs and complete, i.e. it recognizes allwell-formed constructs.4.2. Extension and robustnessAn important property of typing is the possibility toextend the constructs in a monotonic way, i.e. withoutchanging the properties of the previous string : this iscalled the robustness.These extensions can be done in two ways :4.2.1. Assigning new typesIf a new type y is assigned to a word w, withoutchanging the previous assigned types, and x is one ofthe previous assigned types such that y → x then everystring of words recognized as well-formed using thetype assignment x for w is also well-formed using y.4.2.2. Extension by new basic typesLet B be a given set of basic types. It is possible toextend this set by declaring new basic types and theirorder relations, obtaining thus a larger set of basic typesB' ⊃ B. Then the free pregroup P' generated by B'includes the free pregroup P generated by B.If a and b belong to P and a → b can bederived in P, it can also be derived in P'.4.3. ConservativityIf a → b can be derived in some pregroup P' and botha and b belong to a smaller pregroup P ⊂ P', then thewhole reduction can be done in P.The proof of this non trivial property can befound in [3].5. LANGUAGE ANALYSIS5.1. Step by stepThe robustness and conservativity properties allow toproceed step by step to analyze a language. One cantake a subset of the language and show its correctnessand completeness. Then one can extend the fragmentseither by assigning new types to words or by addingnew basic types and verifying that the typing involvingthe new types is also correct and complete for the newconstruct.5.2. Type checking algorithmsA type checking algorithm is an algorithm decidingwhether a → b for arbitrary types a and b. In fact, it issufficient to prove that bl a → 1 since the followingproperty :a → b  if and only if   bl a → 1(10)It has been shown that such an algorithmexists and that its complexity is at most O(n3)6. A SIMPLE CHORD GRAMMAR6.1. Rules for \"non dissonant\" chordsLet us call \"non dissonant\" a chord which has noharmonics in \"critical bands\" [4]. A non dissonantchord may be \"non consonant\" since to be consonantthe pitches of the chord must have several commonharmonics or at least have close harmonics thatproduces only vibratos.  Thus a chord may be neitherdissonant nor consonant. So are most chords in theequal temperament.Many harmony handbooks give rules forgeneration or recognition of these chords [2]. Theserules are always expressed in a natural language.On the other hand, classical grammars [1] areused for chord sequences descriptions [6] especially inJazz music. Unfortunately these grammars are contextsensitive and some of these rules must be completedby natural language sentences.More recently, categorical grammars have beenused to cope with these difficulties [7] [8].6.2. A first simple approach6.2.1. DefinitionLet us call chord a non dissonant chord. Let us givethe following simple definition :a chord is a sequence of at least 3 pitchessuch that the distance between two successive pitchesof the chord is at least 3 semi-tones.This is a simple way to prevent closeharmonics from appearing in critical bands.This musical definition can be turned into anarithmetic one :0 is assigned to the first pitch of the chordcalled the rootthe other pitches of the chord are made of anincreasing  sequence of at least 2 integers ≥3 such asthe difference between 2 consecutive elements of thesequence is ≥ 3.Each of these integers represents the chromaticdistance of a pitch to the root.Let us now define a grammar where words arethe pitches and correct sentences are the chords.6.2.2. Basic typesWe define the following set of basic typesChfor the chordP (i)for the last pitchP (i,j)for the other pitchesi is related to the chromatic distance of a pitch to itsroot ; j is related to the rank of a pitchand the following order :P (i,j) → P (k,j)  iff   k > i(11)P (i,j) → P (i)(12)6.2.3. TypingEach pitch of the chord is represented by an integermeasuring its chromatic distance to the root : forexample a m7b9 chord is represented by the followingsequence :3  7  1013To each of these integers, say n, we assign one of the 3following types  :P (n-3)r Chn ≥ 6for the pitch in the last positionP (n,1)n ≥ 3for the pitch in the first positionP (n-3,k)r P(n,k+1)n ≥ 3(k+1)for the pitches in an intermediateposition kFor the above example, the typing is :3 P (3,1)7P (4,1)r P(7,2)10P (7,2)r P (10,3)13P (10)r Chleading to the following type of the whole sequence :P (3,1) P (4,1)r P (7,2) P (7,2)r P (10,3) P (10)r ChAs P (3,1) → P (4,1) according to (11)and P(10,3) → P (10) according to (12)the previous type reduces to Ch proving the correctnessof the chordLet us now take an other example, such as a m6 chord,represented by the following sequence :  3  7  9and its type :  P (3,1) P (4,1)r P (7,2) P (6)r Chthat reduces to  :  P (7,2) P(6)r Chwhich is irreducible.Such a chord is, according to our definition, incorrect.6.3. An extensionThe above definition for a chord appears to be toorestrictive. According to the property of robustness, wecan extend the assignment of types without changingthe previous language.For example, if we accept chords with aminimum chromatic distance of 2 semi-tones betweenits pitches, the following assignments have to be added:P (n-2)r Chn ≥ 4for the pitch in the last positionP (n,1)n ≥ 2for the pitch in the first positionP (n-2,k)r P(n,k+1)n ≥ 2(k+1)for the pitches in an intermediateposition kSo, the above example of a m6 chord : 3 7 9can be assigned to the following type :P (3,1) P (5,1)r P (7,2) P (7)r Chthat reduces to Ch and becomes correct6.4. Chord specificationsIn the previous simple examples, one single basic typeCh has been assigned to a valid chord. It is possible torefine the definitions in order to specify more preciselythe kinds of chords.For example, if we want to distinguish majorand minor chords, different specific assignments of thefirst pitch, i.e. the third degree, must be done. Theywill be associated to 2 different assignments of thechord, namely the major and minor ones.New basic types:mfor minorMfor majorChmfor a minor chordChMfor a major chordNew typing :m P(3,1)  for the first pitch iff its value is 3M P(4,1)  for the first pitch iff its value is 4P (n-2)r mr Chmn  ≥ 4P (n-2)r Mr ChMn  ≥ 4for the pitch in the last positionA simple minor chord will be represented by thesequence :  3 7and its type by :  m P(3,1) P(5)r mr Chmthat reduces to : ChmIn the same way, we can specify 7th chords,and so on. This may lead to 4 classic chord types as :ChM, Chm, Ch7, Chm7.6.5. Chord sequencesFurthermore, we can add the relative root of the chordin its definition in order to specify its degree relative toa given tone. Thus we can distinguish, for instance, aVm7 from a IIm7 in a chord sequence. Each of thesechords will thus be assigned to a proper single type ora sequence of such types.At this point, the analysis of a chord sequencebecomes possible. A new grammar at a higher levelwill now consider chords as clauses and chordsequences as sentences [8]. This is a classical way ofanalyzing a complex language level by level.To each basic type(s) representing a specificchord we will now assign new types.7. CONCLUSIONWe have shown that pregroup grammars can be used todescribe the context-sensitive syntax of chords. Furtherworks involving sequences of chords are underinvestigation. For example, the six rules of Steedman[6] concerning various chord sequences of blues aredescribed by a context-sensitive generative grammarcontaining some assertions expressed in a naturallanguage. Steedman himself is now using combinatorycategorical recognition grammars [7] to cope withcontext sensitivity. In that direction pregroupgrammars may be  possible for context-free formalrecognition (rather that generation).Furthermore, as pregroup grammars are able torecognize specific sentence constructions in naturallanguages, they may also be able to recognize specificchord sequences allowing their classification in someparticular context ; for example in jazz music : blues,rag, anatole (chord sequence of \"I got rhythm\" – G.Gershwin -) etc .. Such classifications would be auseful help for indexing pieces in large databases,especially on the WWW, either by extracting chordfiles from \"real books\" or possibly with the help ofsoftware extracting chord sequences from scores or evenaudio files [5]8. REFERENCES[1] CHOMSKY Noam (1979) – Structuressyntaxiques – Editions du Seuil[2] EBERHARD Daniel Mark (2001) - DasWichtigste in der Musik steht nicht in denNoten –  Skript zum (Jazz-) ImprovisationsWorkshop[3] LAMBEK Joachim (2000) - An algebraicapproach to English sentence - unpublishedlecture notes, McGill University, QC, Canada[4] PIERCE John (1984) – Le son musical – Ed.Pour la Science[5] SHEH Alexander, ELLIS Daniel P. W. (2003)– Chord Segmentation and Recognition usingEM-Trained Hidden Markov Models – ISMIR2003[6] STEEDMAN Mark (1984) – A GenerativeGrammar for Jazz Chord Sequences – MusicPerception 2, 52-77 1984[7] STEEDMAN Mark (2003) – FormalGrammars for Computational MusicAnalysis : The Blues and the Abstract Truth –INFORMS Atlanta October 2003[8] TERRAT Richard (2004) – A Pregroupgrammar for chord Sequences – SMC 04 –Paris October 2004"
    },
    {
        "title": "Retrieval of percussion gestures using timbre classification techniques.",
        "author": [
            "Adam R. Tindale",
            "Ajay Kapur",
            "George Tzanetakis",
            "Ichiro Fujinaga"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416612",
        "url": "https://doi.org/10.5281/zenodo.1416612",
        "ee": "https://zenodo.org/records/1416612/files/TindaleKTF04.pdf",
        "abstract": "Musicians are able to recognise the subtle differences in timbre produced by different playing techniques on an in- strument, yet there has been little research into achiev- ing this with a computer. This paper will demonstrate an automatic system that can successfully recognise differ- ent timbres produced by different performance techniques and classify them using signal processing and classifica- tion tools. Success rates over 90% are achieved when clas- sifying snare drum timbres produced by different playing techniques.",
        "zenodo_id": 1416612,
        "dblp_key": "conf/ismir/TindaleKTF04",
        "keywords": [
            "automatic system",
            "timbre recognition",
            "playing techniques",
            "signal processing",
            "classification tools",
            "different performance techniques",
            "classification",
            "snare drum timbres",
            "classification success rate",
            "90%"
        ],
        "content": "RETRIEVALOF PERCUSSION GESTURESUSING TIMBRE\nCLASSIFICATIONTECHNIQUES\nAdamTindale\nMusicTechnology\nMcGillUniversityAjayKapur\nElectricalEngineering\nUniversityofVictoriaGeorgeTzanetakis\nComputerScience\nUniversityofVictoriaIchiroFujinaga\nMusicTechnology\nMcGillUniversity\nABSTRACT\nMusicians are able to recognise the subtle differences in\ntimbreproducedbydifferentplayingtechniquesonanin-\nstrument, yet there has been little research into achiev-\ning this with a computer. This paper will demonstrate an\nautomatic system that can successfully recognise differ-\nenttimbresproducedbydifferentperformancetechniques\nand classify them using signal processing and classiﬁca-\ntiontools. Successratesover90%areachievedwhenclas-\nsifying snare drum timbres producedby differentplaying\ntechniques.\n1. INTRODUCTION\nOne major goal of music information retrieval is auto-\nmatic music transcription. There are two main problems\ntosolveinthesesystems: instrumentrecognitionandtrans -\nlation into a symbolic musical format (e.g., MIDI). Al-\nthoughtheinstrumentlabellingtypicallyreturnedbysuch\nsystems is adequate in most cases, the inclusion of dif-\nferent timbre produced by a single instrument would be\nusefulformanydifferentapplicationsandstudies.\nCurrentlytherearemanysystemsthatcansuccessfully\nclassifysoundsintoinstrumentgroupingsbutnoneofthem\nexaminethetimbrespacewithinthesegroupings[10]. The\ngoal of this project is to develop a system that can iden-\ntifythesubtledifferencesintimbreproducedbyaninstru-\nment and classify these differences (see Figure 1). The\nsubtle timbre recognition has the potential to aid in other\ntasks: drummer recognition, gestural control of music,\ngenreclassiﬁcationofmusic,etc.\nThe snare drum was chosen as the instrument for this\nstudy because it can create many different subtle timbres\nproducedbycontrolledandquantiﬁableperformancetech-\nniques; althoughsubtle timbres can be producedby other\ninstrumentsit isoftendifﬁculttocontroltheirproductio n.\nOurpreviousstudypresentedresultswitha limited\namount of data and only time-domain features [16]. The\ncurrent study includes a much larger amount of test and\nPermission tomakedigital orhard copies ofallorpartofthi swork for\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributed forproﬁtorcommercialadvantag e andthat\ncopies bear this notice and the full citation on the ﬁrstpage .\n©2004 Universitat Pompeu Fabra.\nS o u n dI\nn s t r u m e n t 1I\nn s t r u m e n t 2T i\nm b r e AT i\nm b r e BT i\nm b r e CT i\nm b r e DT i\nm b r e ET i\nm b r e F\nB o u n d a r y o fCu r r e n t S y s t e m s\nS c o p e o f C u r r e n t P r o j e c t\nFigure1. Systemoutline.\ntraining data for the classiﬁer, spectral features, and the\nexplorationofdifferentwindowingtechniques.\n2. SNAREDRUM\nA snare drum is made up of ﬁve main components: the\nshell, lugs, heads, rims, and the snares. The shell is gen-\nerally a piece of wood that is bent so that it is a cylinder.\nMetal ﬁttings, calledlugs, areattachedto theshell forthe\npurpose of holding all of the componentsof the drum to-\ngether. Therearetwo headsona moderndrum: the batter\nhead,thatisthetopheadthatisusuallystruck,andtheres-\nonanthead,the bottomhead. The rimsare solid piecesof\nmetalthatholdtheheadstothedrumbyapplyingpressure\ntotheheadfromtheouteredge. Therimscanvarytheten-\nsionontheheadsothatitmaybetuned.Thecharacteristic\nofthesnaredrumthatdifferentiatesitfromotherdrumsis\nthe snares. The snares are usually made of metal and are\nstrungacrossthebottomheadofthedrum. Thesnaresvi-\nbrateinresonancewhenthedrumisstruckaddinganoise\ncomponenttothe soundproducedbythedrum[14].\nSeeFigure2forrepresentationsofsnaredrumsignals.\n3. RELATED RESEARCH\nWhile therehasbeensome researchonsnare drums,very\nlittle of it deals directly with timbre production. The ma-\njority of the research falls into two areas: acoustics and\nautomatic transcription. The ﬁrst major published study\non the snare drum was mostly concerned with amplitude\nand durations [7]. The study scientiﬁcally introduced the\nidea of a stroke height, the height that the stick starts its\nstrike from,as beingthe major factor in the resulting am-plitudeof the strike (see section 5). Complexinteractions\nbetweenthemodesofvibrationoftheheadshavebeenob-\nservedanddiscussed[22],whichisusefulevaluatingwhat\ntype of features to look for when trying to classify tim-\nbre. An empirical study [11] showed that different types\nof snare drumheadson the same drumcan producevary-\ning timbres. Another study of this nature showed spectra\nfrom a snare drum with its snares engaged and not en-\ngaged[20] thatdemonstratedalargedifferencein timbre.\nDahl [1] has doneinvestigationsin determiningthe strike\nforce by analysis of the drumsticks using video capture\ntechniques.\nThere has been a signiﬁcant amount of research in au-\ntomaticmusictranscriptionthathasdealtspeciﬁcallywit h\ndrums. Most of this research deals with elements of the\ndrumset and related percussion instruments in pop mu-\nsic. The earliest major study was conducted by Schloss\nin his doctoral dissertation where he was able to recog-\nnise timbres produced by conga drums and then produce\na transcription[15]. Herrera et al. have conductedexten-\nsivestudiesthatrecognisepercussivesoundsproducedby\na drumsetinstrument[9,6].\nAlso studiesonextractingtimbreinformationfromin-\nstruments to be used for real-time control of computers\nfor musical applications exist. The extraction of control\nfeatures from the timbre space of the clarinet is explored\nin [3]. Deriving gesture data from acoustic analysis of a\nguitarperformanceisexploredin[17].\n4. IMPLEMENTATION\nTwo different software packages are employed for this\nproject: Weka [21] and Matlab [13]. Matlab was used to\nperformthe feature extraction and the results are put into\na data ﬁle for Weka which was used for the classiﬁcation\nexperiments.\n4.1. FeatureExtraction\nFeature extractionwas accomplishedwith Matlab by cre-\nating functions that operate on the audio signals and pro-\nduceresultsthatarestoredasafeaturematrixintheWeka\nﬁle format.\nTwomaintypesoffeaturesareextracted: time-domain\nandspectraldomainfeatures.\nThetime-domainfeaturesincluded: TemporalCentroid,\nAttackTime,RMS,Zero-CrossingRate[5],SubbandAnal-\nysis (RMS in four bands:0–200Hz, 200–1000Hz, 1000–\n3000Hz,3000–20000Hz).\nThe spectral domain features included: Spectral Flux,\nSpectralRolloff,SpectralCentroid,SpectralKurtosis,S pec-\ntralSkewness,Mel-FrequencyCepstrumCoefﬁcients[12],\nLinearPredictiveCodingCoefﬁcients,Energyin nine\nwavelet bands, Variance from the mean in each wavelet\nband[18,19].0 23 45 6800.51Amplitude\n0 23 45 6800.51Amplitude\n0 23 45 6800.51Amplitude\n0 23 45 6800.51Amplitude\n0 23 45 6800.51\nTime (mS)Amplitude128025603840512034\n6800.5Amplitude\n128025603840512034\n6800.5Amplitude\n128025603840512034\n6800.5Amplitude\n128025603840512034\n6800.5Amplitude\n128025603840512034\n6800.5\nTime (mS)\nFrequency (Hz)AmplitudeBrush Strike \nRimshot Strike \nCenter Strike \nHalfway Strike \nEdge Strike \nFigure2. Representationsofsnaredrumsounds.\n4.2. Classiﬁcation\nArtiﬁcialneuralnetworksareusedtoclassifythedata[2].\nThe network is comprised of six hidden nodes and one\noutput nodefor each class in the test, which variedas ex-\nplainedbelow. Thenetwastrainedwith1000epochs.\nDifferentcombinationsoffeaturesandclassesareused\nto train the classiﬁer in order to evaluate the system per-\nformance. This study uses seven different types of snare\ndrum strokes in order to create different timbres on the\nsnare drum: rimshot, brush stroke, center, near-center,\nhalfway,near-edgeandedge. Arimshotiswhentheplayer\nstrikestherimandheadofthedrumatthesametime(see\nFigure3). Abrushstrokeiswhentheplayerhitsthedrum\nwith a brushinstead of a stick. The playersare instructed\ntohitthecenteroftheheadwhenperformingbothofthese\nstrokes. The rest of the strokes are different positions on\nthe batter head. The snare drums are marked so that the\nplayers would strike the same place when performingthe\nstrokes. Ten-fold cross-validation was used to randomly\nselect samples from the data set as training and testing\ndata[2].\nFour different groups of classes are used to train the\nclassiﬁer:\n1. All Classes(All)\n2. Center,Near-Center,Halfway,Near-Edge,Edge\n(Only5)(see Figure4fortheirlocation)\n3. Rimshot,Brush,Edge(RBE)\n4. Center,Halfway,Edge(CHE)\n5. EXPERIMENT\nThree expert players each played three different drums,\nstriking each of the seven different stroke types twenty\ntimes. This resulted in 1260 individual samples that are\nused as testing and training data for the classiﬁer. All\nplayers used the same wooden stick (Vic Firth Concert)Figure3. Rimshot\nFigure4. Positions\nwith a stroke height of six inches [7]. The brush used\nwas a Vic Firth standard brush fully extended. The three\nsnare drums are: Yamaha Concert 14 \"x 6.5\", Ludwig\nConcert 14 \"x 6.5\", and a Gretsch Standard 14 \"x 4.5\".\nThe drums use standard plastic heads on both the batter\nand snare head. The heads are manufactured by Remo\nontheLudwigandGretschdrums,andbyYamahaonthe\nYamahadrum.\nSampleswererecordedwithaShureSM57microphone\ninto a Mark of the Unicorn 896 at CD quality (16-bits /\n44100Hz). The microphone was placed one inch above\nthe edge of the drum angled down at approximately 30°.\nThe recordings were conducted in the Electronic Music\nStudioatMcGillUniversitythathassomesound-prooﬁng.\n5.1. Preprocessing\nAllofthesampleswererecordedseparatelyandthennor-\nmalized for input into the system. A gating function was\nusedtodeterminethepositionoftheonset. Thegatefunc-\ntion was set to a threshold of -60 dB. The onset is deter-\nmined as the ﬁrst zero-crossing previous to the index re-\nturnedbythegatefunction.\nThe features are calculated on four different window\nlengths: Attack section (see below), 512, 1024, and 2048\nsamplesfromtheonset. The“attack”sectionisdeﬁnedas\nthesignalfromtheonsetofthesound,asgivenbythegate\nfunction, to point of maximum amplitude. The average\nlength of this window is 610 samples with a standard de-\nviation of 65. The ﬁxed-length windows were calculated\nbeginningfromtheonsetdeterminedbythegatefunction.6. RESULTS\nAll of the combinations of classes and feature sets were\ncollectedintoseparateWekaﬁlesandthenrunthroughthe\nclassiﬁer. High success rates were achieved for all tests.\nThe classiﬁer was able to accurately (greater than 95%)\nclassify the timbres for the tests with three classes (RBE\n&CHE).Thetestswithlargernumberofclassesalsoper-\nformed with great accuracy (see Table 1 and 2). Many of\nthe misclassiﬁcations in these tests were classiﬁed as the\nnextnearesttimbre.\nBy using Weka, k-nearest neighbour(kNN)(see Table\n3 and 4) and Support Vector Machines (SVM) (see Table\n5 and6)classiﬁerswereexamined.\nTimbres AllOnly5 RBE CHE\nAttack 89.3% 88.3% 99.8% 98.1%\n512 85.0% 86.1% 99.4% 96.4%\n1024 85.6% 86.6% 99.4% 98.3%\n2048 85.3% 85.6% 99.4% 98.9%\nTable 1. Results using all features with different number\nofclassesanddifferentwindowlengthsusingNeuralNet-\nworkclassiﬁer.\nTimbres AllOnly5 RBE CHE\nAttack 73.1% 71.1% 96.8% 91.3%\n512 79.9% 77.0% 98.1% 95.9%\n1024 77.1% 79.2% 98.1% 97.2%\n2048 79.1% 79.0% 99.3% 98.1%\nTable2. Resultsusingonlythetime-domainfeatureswith\ndifferent number of classes and different window lengths\nusingNeuralNetworkclassiﬁer.\nTimbres AllOnly5 RBE CHE\nAttack 94.9% 95.3% 98.1% 99.3%\n512 93.0% 89.6% 98.9% 96.1%\n1024 94.4% 94.1% 99.4% 98.7%\n2048 92.6% 91.1% 99.3% 96.9%\nTable 3. Results using all features with different number\nofclassesanddifferentwindowlengthsusingkNNclassi-\nﬁer.\nThe kNN classiﬁer was the most consistent classiﬁer,\nnearly all of its results are above 90%. The SVM classi-\nﬁer performedvery well with the three classes but poorly\nwiththelargersetsofclassesusingtime-domainfeatures.\nTheneuralnetworkhadthehighestresult(99.8%with all\nfeaturesontheRBE classset).\nThedifferentsetsoffeaturesyieldedinterestingresults .\nThetime-domainfeaturesperformednearlyaswellasthe\nfullfeaturesetwhenclassifyingthegroupswithonlythree\nclasses but signiﬁcantly less effective when classifyingTimbres AllOnly5 RBE CHE\nAttack 90.8% 90.9% 96.1% 95.7%\n512 90.9% 88.8% 98.9% 97.2%\n1024 91.2% 87.6% 99.1% 96.9%\n2048 92.0% 90.0% 98.9% 97.2%\nTable4. Resultsusingonlythetime-domainfeatureswith\ndifferent number of classes and different window lengths\nusingkNNclassiﬁer.\nTimbres AllOnly5 RBE CHE\nAttack 86.6% 86.8% 99.3% 97.4%\n512 83.4% 79.7% 98.7% 92.6%\n1024 82.1% 82.3% 98.1% 97.4%\n2048 83.5% 82.6% 99.6% 96.7%\nTable 5. Results using all features with different number\nof classes and different window lengths using SVM clas-\nsiﬁer.\nTimbres AllOnly5 RBE CHE\nAttack 57.1% 55.1% 94.3% 85.4%\n512 65.4% 59.0% 97.0% 89.1%\n1024 68.1% 61.4% 98.0% 91.1%\n2048 68.8% 62.1% 98.1% 91.9%\nTable6. Resultsusingonlythetime-domainfeatureswith\ndifferent number of classes and different window lengths\nusingSVMclassiﬁer.\nmultiple classes. These observations suggest that it may\nbe possible to implement an accurate recognition system\nwithonlytime-domainfeatures,whichwouldallowshort-\ntimeoperationsforreal-timerecognition.\nThe spectral features were very useful for differentiat-\ning the different positions along the radius of the drum.\nOverall, the classiﬁers performed the “Only 5” and the\n“CHE” tests an average of 7.8% better with the spectral\nfeaturesthanwiththe onlythetime-domainfeatures.\nThedifferentwindowsizeswerenotmajorcontributers\nto the overall recognition rate. Further investigation int o\nappropriate windowing techniques will be performed in\norder to maximise the results. It was interesting that the\nsmallestwindow(512samples)wasstillabletoclassifyas\naccurately as the long window and that the ﬁtted window\n(“Attack”window)seemedto havelittle beneﬁt.\n7. CONCLUSION\nWe have presented a system that can successfully recog-\nnisethesubtledifferencesintimbreproducedwhenasnare\ndrumis struck in differentlocationsalong the batter head\nas well as other performance techniques. The present re-\nsearch into the identiﬁcation of subtle timbres produced\nby an instrument is a ﬁrst step towards a comprehensive\nsystem that can transcribemusic and provideinformationat thetimbral level. Futureresearchwill involveapplying\nthis system to other instruments in different contexts and\nthen integrating it into an automatic transcription system\nas well as investigating other features and classiﬁcation\ntechniques. The system will also be implemented for a\nreal-timesystemandreevaluated.\nData collected in this study will be made available to\ninterestedpartiesuponcontactingthe authors.\n8. ACKNOWLEDGEMENTS\nThankstoManjBenningforinvaluablehelpwiththeMat-\nlabcoding. ThankstoD’ArcyPhillipGray,KristieIbrahim\nand Sarah Mullins for taking the time to play the snare\ndrumsforthisexperiment.\n9. REFERENCES\n[1] Dahl,S.2001. Armmotionandstrikingforceindrum-\nming.InternationalSymposiumon Musical Acoustics.\n1: 293–6.\n[2] Duda, R., P. Hart, and D. Stork. 2000. Pattern classi-\nﬁcation.NewYork: JohnWiley&Sons.\n[3] Egozy, E. 1995. Deriving musical control features\nfrom a real-time timbre analysis of the clarinet. Mas-\nters Thesis, Department of Electrical Engineering and\nComputer Science. Massachussetts Institute of Tech-\nnology.\n[4] Fujinaga, I., and K. MacMillan. 2000. Realtime\nrecognition of orchestral instruments. Proceedings of\ntheInternationalComputerMusic Conference. 141–3.\n[5] Gouyon, F., F. Pachet, and O. Delerue. 2000. On the\nuse of zero-crossing rate for an application of classiﬁ-\ncation of percussive sounds. Proceeding of the Work-\nshoponDigitalAudioEffects.\n[6] Gouyon,F.,andP.Herrera.2001.Explorationoftech-\nniques for automatic labeling of audio drum tracks’\ninstruments. Proceedings of MOSART: Workshop on\nCurrent DirectionsinComputerMusic.\n[7] Henzie, C. 1960. Amplitude and duration character-\nistics of snare drum tones. Ed.D. Dissertation. Indiana\nUniversity.\n[8] Herrera P., X. Amatriain, E. Batlle, and X. Serra.\n2000.Towardsinstrumentsegmentationformusiccon-\ntentdescription: Acritical reviewofinstrumentclassi-\nﬁcationtechniques. InternationalSymposiumonMusic\nInformationRetrieval.\n[9] Herrera, P., A. Yeterian, and F. Gouyon. 2002. Au-\ntomatic classiﬁcation of drum sounds: A comparison\nof feature selection and classiﬁcation techniques. Pro-\nceedingsofSecondInternationalConferenceonMusic\nandArtiﬁcialIntelligence. 79–91.[10] Herrera, P., G. Peeters, and S. Dubonov. 2003. Au-\ntomatic classiﬁcation of musical instrument sounds.\nJournalofNewMusic Research 32(1): 3–21.\n[11] Lewis, R., and J. Beckford. 2000. Measuring tonal\ncharacteristics of snare drum batter heads. Percussive\nNotes38(3): 69–71.\n[12] Logan,B. 2000.Mel-frequencycepstral coefﬁcients\nformusicmodeling. ProceedingsofInternationalSym-\nposiumonMusic InformationRetrieval.\n[13]MATLAB reference guide. 1992. Natick, MA: The\nMathworks,Inc.\n[14] Rossing, T. 2002. The science of sound. San Fran-\ncisco: Addison-WesleyPublicationsCompany.\n[15] Schloss, A. 1985. On the Automatic transcriptionof\npercussive music - From acoustic signal to high-level\nanalysis. Ph.D. Dissertation. CCRMA, Stanford Uni-\nversity.\n[16] Tindale, A., A. Kapur, and I. Fujinaga. 2004. To-\nwardstimbrerecognitionofpercussivesounds. Submit-\ntedtoICMC 2004.[17] Traube,C., P. Depalle, and M. Wanderley.2003.In-\ndirectacquisitionofinstrumentalgesturebasedonsig-\nnal, physical and perceptual information. Conference\nonNewInterfacesforMusical Expression.\n[18] Tzanetakis, G., G. Essl, and P. Cook. 2001. Audio\nanalysisusingthediscretewavelettransform. Proceed-\ningsof ConferenceinMusic TheoryApplications.\n[19] Tzanetakis, G., and P. Cook. 2002. Musical genre\nclassiﬁcation of audio signals. IEEE Transactions on\nSpeechandAudioProcessing 10(5): 293–302.\n[20] Wheeler, D. 1989.Focus on research: Some experi-\nmentsconcerningtheeffectofsnaresonthesnaredrum\nsound.Percussive Notes 27(4): 48–52.\n[21] Witten, I., E. Frank, and M. Kaufmann. 2000. Data\nmining: Practicalmachinelearningtoolswithjavaim-\nplementations. San Fransico: Addison-Wesley Publi-\ncationsCompany.\n[22] Zhao,H.1990. Acousticsofsnare drums: Anexper-\nimentalstudyofthemodesofvibration,modecoupling\nand sound radiation patterns. M.S. Thesis. Northern\nIllinoisUniversity."
    },
    {
        "title": "Visualizing and Exploring Personal Music Libraries.",
        "author": [
            "Marc Torrens",
            "Patrick Hertzog",
            "Josep Lluís Arcos"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1414746",
        "url": "https://doi.org/10.5281/zenodo.1414746",
        "ee": "https://zenodo.org/records/1414746/files/TorrensHA04.pdf",
        "abstract": "Nowadays, music fans are beginning to massively use mobile digital music players and dedicated software to or- ganize and play large collections of music. In this con- text, users deal with huge music libraries containing thou- sands of tracks. Such a huge volume of music easily over- whelms users when selecting the music to listen or when organizing their collections. Music player software with visualizations based on tex- tual lists and organizing features such as smart playlists are not really enough for helping users to efficiently man- age their libraries. Thus, we propose new graphical vi- sualizations and their associated features to allow users to better organize their personal music libraries and therefore also to ease selection later on.",
        "zenodo_id": 1414746,
        "dblp_key": "conf/ismir/TorrensHA04",
        "keywords": [
            "mobile digital music players",
            "dedicated software",
            "organize and play",
            "huge music libraries",
            "thousands of tracks",
            "overwhelms users",
            "selecting music",
            "organizing collections",
            "efficiently manage",
            "personal music libraries"
        ],
        "content": "VISUALIZING AND EXPLORING PERSONAL MUSIC LIBRARIES\nMarc Torrens\nMusicStrands Inc.\nCorvallis, OR, USA\ntorrens@MusicStrands.comPatrick Hertzog\nAI Lab., EPFL\nLausanne, Switzerand\nPatrick.Hertzog@epﬂ.chJosep-Llu ´ıs Arcos\nIIIA, CSIC\nBellaterra, Spain\narcos@iiia.csic.es\nABSTRACT\nNowadays, music fans are beginning to massively use\nmobiledigitalmusicplayersanddedicatedsoftwaretoor-\nganize and play large collections of music. In this con-\ntext,usersdealwithhugemusiclibrariescontainingthou-\nsandsoftracks. Suchahugevolumeofmusiceasilyover-\nwhelms users when selecting the music to listen or when\norganizing their collections.\nMusicplayersoftwarewithvisualizationsbasedontex-\ntual lists and organizing features such as smart playlists\narenotreallyenoughfor helpinguserstoefﬁcientlyman-\nage their libraries. Thus, we propose new graphical vi-\nsualizationsandtheirassociatedfeaturestoallowusersto\nbetterorganizetheirpersonalmusiclibrariesandtherefore\nalso to ease selection later on.\n1. INTRODUCTION\nNew technologies combining portable digital music play-\nerswithdedicatedsoftware(suchasiPod1withiTunes2),\ntogetherwithnewmusicdistributionchannelsthroughIn-\nternet are quickly changing the way people organize and\nplaymusic. Thus,anewcommunityofdigitalmusicusers\nisemerging. Theseusersdealwithmusicdifferentlycom-\npared to the traditional way. Instead of dealing with al-\nbums or CDs, they basically face their music at the track\nlevel by :\n•acquiring track by track, and\n•creating and playing personalized playlists.\nIn such contexts, users have to deal with huge libraries\nofmusicintheircomputersandmobileplayers. Musicli-\nbrariescaneasilycontainthousandsoftracks(correspond-\ning to hundreds of CDs). Such a huge volume clearly\noverwhelms users when choosing the music to listen at a\ncertainmoment. Therefore,thissituationposesseveralIT\n1http://www.apple.com/iPod .\n2http://www.apple.com/iTunes .\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2004 Universitat Pompeu Fabra.challenges regarding how to offer adequate tools to users\nin order to support them organizing their collection and\nin their decision making process of selecting, and playing\nmusic.\n1.1. The challenge\nMusicplayersoftwarewithvisualizationsbasedontextual\nlists and organizing features such as smart playlists are\nnot enough for helping users to efﬁciently manage their\nlibraries which may easily contain thousands of tracks.\nThus, in order to avoid the current situation where users\nare clearly overwhelmed with the problem of selecting\ntracks,weproposenewvisualizationsandtheirassociated\nmanagingfeatures. Inthefollowing,wedescribeourpro-\nposals and then we compare and evaluate them.\nWe have basically explored three3different visualiza-\ntions which allow users to have a better overview of the\ncontents of their music libraries and therefore to ease its\norganization. On the other hand, the two ﬁrst visualiza-\ntions are shown to be very useful helping users to build\nplaylists graphically instead of having to express ﬁltering\ncriteria which may may be confusing to users.\n2. VISUALIZATION TECHNIQUES\nThis section presents three different ways of graphically\nvisualizing music libraries considering ﬁve criteria (thus,\nﬁvedimensions)whicharegenre,artist,yearandaquanti-\ntativecriteriontobechosenbytheusersuchasplaycount,\nrating,addedorlastplayeddate. Thegoalofthesevisual-\nizationsistwo-fold: a)togiveanoverviewofthecontents\nof a music library, and b) to visualize playlists and give\nsome support to manage and organize them. Depending\non the visualization model users get different advantages\nsince they have a different geometric expressiveness. All\nthe explored techniques give a topologic overview of a\nmusic library regarding its tracks.\n2.1. Disc Visualization\nThisapproach, called discvisualization,is based onwell-\nknown graphical charts in form of discs. Users are used\nto manage such kind of visualizations which give good\npercentage and proportional overviews.\n3As we will see, the two ﬁrst visualizations, using discs and rectan-\ngles, can be considered as variants of the same basic concept.2.1.1. Description\nAs shown in Figure 1, the disc is divided in different sec-\ntors that represent each of the genres of the library4. The\nsize of a sector is proportional to the number of tracks\nof the associated genre with respect to the whole library.\nTherefore, the size of a sector is directly proportional to\nthe importance of the corresponding genre within the li-\nbrary. At the same time, sectors are split in sub-sectors\nrepresentingtheartistsoftheassociatedgenre. Again,the\nsize of sub-sectors is proportional to the number of tracks\nof the artist. The radius of the disc, from the center to the\nperimeter, can be seen as the time axis: the center rep-\nresents the year of the oldest track of the library and over\ntheperimeterthemostrecenttracksarepositioned. Tracks\narethendepictedaspointsoverthediscaccordingtotheir\nattributes, i.e.,genre, artist, year. Tracks belonging to the\nsamealbumarepositionedcontiguously,thusithastheef-\nfect of producing like arcs of points representing albums.\nThe order in which the albums are depicted is alphanu-\nmeric, and the order for the tracks of the same album is\nthe original order in the album. The quantitative attribute\nto be chosen by the user (for instance playcount, rating,\nlast played date, added date, etc) is depicted according to\ndifferent color tonalities. Colors are used to express the\nexactvalueforonetrackinitsassociatedpoint. Themean\nvalue of all the tracks for one genre is also used to color\nthe corresponding sector.\nFigure 1 illustrates how playlists and smart playlists\ncanbeshownusingthediscvisualization. Tracksofplay-\nlists without any grouping logic can be depicted by using\ngeometric forms different than regular points which are\nused in general for the rest of the songs. For instance,\nin Figure 1, the playlist called “Jogging playlist” (num-\nber 4) is displayed by using bigger points in red. The\nother example in the ﬁgure for playlists which do not fol-\nlow any geometric logic is the “25 last played” (number\n3) whose tracks are represented as little red crosses. The\nother playlists (numbers 1, 2, and 5) are shown as red re-\ngionssincetheyfollowaregulargeometricform. Therest\noftheplaylists(numbers6and7)arenothighlightedsince\ntheuserhasnotactivatedtheircorrespondingcheckboxes.\nInsuchavisualization,thetrackcurrentlybeingplayed\ncould be highlighted and a path grouping the tracks to be\nplayed next could also be displayed. In this way, the user\nwill get an idea of what regions of his library are going to\nbe used in the current musicsequence.\n2.1.2. Interaction Principles\nIn this section, we describe how users interact with the\ndiscvisualization. Basically,thefollowingprincipleshave\nbeen identiﬁed:\nNavigation. The attributes of any track of the library\ncan be visualized in textual form by just positioning the\ncursor over its point. When moving the cursor over the\ndisc, the track details are displayed and the artist of the\n4Theﬁguresillustratingthediscandrectangleapproachesweregen-\nerated with a real music library of about 2.500 tracks.corresponding sub-sector is highlighted. In a similar way,\nthe year is indicated with a circle.\nZoom.When a user is interested in getting a more\ndetailed view of his library, he can zoom over any sec-\ntor of the disc. This zoom will then generate a disc with\nthe same visualization and interaction principles but ap-\npliedtojustthegenreoftheselectedsector. Therefore,in\nthis ﬁrst zoom level, sectors representing genres become\nsectors representing artists with sub-sectors representing\nalbums. All the other dimensions and general principles\nremain the same. Finally, the latest zoom level when se-\nlectingasectorrepresentinganartistwouldproduceadisc\nwhere sectors are the albums of the selected artist, with-\nout any sub-sector. Thus, in this latest zoom level, users\nobtain a graphical representation of the tracks for a given\nartist.\nPlaylist management. As explained in Section 2.1.1\nandshowninFigure1,thediscvisualizationcanbenicely\nused to graphically display playlists and smart playlists.\nMoreover, this visualization can be used to edit or cre-\nate new playlists with useful graphical help. The mecha-\nnismisbasedonconsideringplaylistsassetsandthenbe-\ning able to construct set operations to form new playlists.\nMultiple playlists can be selected at the same time and\nthen apply operations such as union, intersection, differ-\nence,andsoon. Sinceplaylistsaregraphicallyvisualized\nas sets, it is convenient and useful to apply set operations\noverthem. Theresultingplaylistsarealsographicallydis-\nplayed.\nWhen creating (or editing) playlists with tools like the\nones provided by iTunes(either directly selecting songs,\nor by constructing a set of logic rules for smart playlists),\nthediscvisualizationisusefulforshowingtheplaylistbe-\ning created step by step. So, at any moment of the cre-\nation (or edition) of a playlist, the user can immediately\nsee how the new playlist changes, its approximate size\nand its topology. Such procedures help users to have a\nbetter idea of which zones of the library are overused or\nFigure 1. Visualizing playlists and smart playlists in mu-\nsic libraries by using discs.underused, or the zones implied in each playlist.\nStandard search procedures. When using standard\nﬁltering procedures based on keyword search, the disc vi-\nsualization can also be of a great help by highlighting the\nﬁltered songs dynamically. In the same way that iTunes\ndynamically changes the list of tracks in the results win-\ndow, the visualization highlights the tracks graphically.\n2.2. Rectangle Visualization\nThis visualization is similar to the disc visualization but\nusingrectanglesinsteadofdiscs. Inthediscvisualization,\nthetimeaxiswasrepresentedalongtheradiusofthedisc,\nandintherectanglevisualizationthetimeaxisgoesalong\nthe vertical axis. Similarly, for this visualization, the at-\ntribute genre goes along the horizontal axis. The result of\nthis visualization is shownin Figure 2.\nFigure2. Visualizingmusiclibrariesbyusingrectangles.\nEven if both visualizations have similar features, they\nmay give different user experiences with their advantages\nand downsides as describedin Section 3.\n2.2.1. Interaction Principles\nThe main principles described for the disc visualization\napply to this visualization, however the zoom functional-\nity may be differently applied.\nZoom.In this visualization, zooms can be done in the\nsame way as for disc visualizations. When zooming over\na genre (which is a sub-rectangle), the horizontal axis be-\ncomestheartistdimension. Similarly,whenzoomingover\nan artist, the horizontal axis becomes the album dimen-\nsion.\nAnother way of applying zooms in the rectangle visu-\nalization is to just consider that all the tracks in the li-\nbrary are always shown, but the scale of the horizontal\naxis changes. Therefore, using this approach, the user ex-\nplores the whole library just by using a scroll bar for pan-\nning over an speciﬁc zone. In this case, when zooming\nin, the horizontal axis still represents the genres, and the\nartists within each genre. With a second level of zoom, inaddition to genres and artists, the axis also represents the\nalbums for each artist. The horizontal axis and its scroll\nbarareaccordinglyadapteddependingonthezoomlevel.\n2.3. Tree-Map Visualization\nThis visualization is using Tree-Maps in a similar way as\ndescribed in [1] but for visualizing music libraries.\nInthisvisualization5,thesizeofrectanglesarealways\nproportionaltothenumberoftracksintheattributerepre-\nsented by the rectangle. At the same time, rectangles are\nrecursively split in sub-rectangles showing other propor-\ntions. For example, in Figure 3 rectangles are recursively\nsplit three times: the whole library (the parent rectangle)\nis split into genres, each genre is split in its sub-genres,\nand ﬁnally each sub-genre is split in its artists.\nThe color of each rectangle indicates a quantitative at-\ntribute to be chosen by the user, similarly as the previous\nvisualizations, e.g.,playcount, last played date, ratings,\nand so on. However, in this visualization, since tracks are\nnot depicted, only mean values are represented by differ-\nent color tonalities.\nTheinteractionmechanismforthevisualizationisvery\nstraightforward for zooming: the user selects a rectangle,\nand the parent rectangle showsthen the selected attribute.\nFigure 3. Tree-Map visualization for a whole music li-\nbrary.\n3. COMPARISON\nThevisualizationsusingdiscsandrectanglesbasicallyof-\nfer the same functionalities, while the Tree-Map visual-\nization is more likely to be used just for giving a better\noverview of the contents of music libraries. This is be-\ncausethediscandtherectangleapproachesarecapableto\nshow information at the track detail whereas it is unclear\n5ForbettershowingtheconceptofTree-Mapsvisualizationsformu-\nsic libraries, we assume that a sub-category of genre, called sub-genre,\nis available for each track.how to represent tracks using Tree-Maps. A comparison\namongthedifferentpresentedvisualizationsandtheirfea-\ntures is summarized as follows:\n•Visualizations based on discs and rectangles offer\nsimilar functionalities, but also different pros and\ncons due to their different geometric forms:\n–Discs give a better visual idea about the pro-\nportions of sectors and sub-sectors compared\nto rectangles and sub-rectangles.\n–Trackpointsaredifferentlydistributedindiscs\nand rectangles. For libraries with more re-\ncent tracks than old ones, the points are better\nplaced in the disc visualization. On the other\nhand, libraries which are more homogeneous\nwithrespecttotheyearoftheirtracksarebet-\nter suited for the rectangle approach.\n–Thezoomingfeatureismoreuseablewithrect-\nangles since the whole library space can be\nrepresentedwiththehelpofscrollbars. Zoom-\ninginthediscvisualizationimpliestofocusto\na smaller portion of the library.\n–Intherectangleversion,criteriaforbothcoor-\ndinatesisexplicitlyshown(genre/artist/albums\nandyear)thusthepositioningoftracksiseas-\nilyunderstoodbyusers. Inthediscrepresenta-\ntion,theyearcoordinategoesalongtheradius\nof the disc so possibly more efforts could be\nrequired by users to quickly understand it.\n•Tree-Map visualizations are more adequate to give\nan overview with respect to the number of tracks\nbelonging to each attribute represented by the size\nof its rectangles.\n•The Tree-Map approach is not very well-suited for\ndisplaying information about tracks or playlists.\n•Discs and rectangles can be used to visualize, and\nmore importantly to create and edit playlists. The\nTree-Map representation does not offer this possi-\nbility because tracks are not shown.\nAll the approaches presented in this paper (and also\ntextuallists)shouldberegardedascomplementarybycon-\nsideringtheabovearguments. Inthisway,acompletemu-\nsic player software may allow users to choose among the\ndifferent approaches. Also, it is feasible to automatically\ndecide which approach has to be used depending on the\ntopology of the library of the user and the action the user\nisconsidering,resultinginareally smartmusicorganizer.\n4. CONCLUSIONS\nTextual list-based visualizations and organizing features\nsuch as smart playlists are not enough to really support\nusers who deal with music collections of thousands of\ntracks. Inordertoassistmusicfanstobettermanagehugedigital music libraries, we have proposed new visualiza-\ntionsandtheirassociatedfeatures. However,theproposed\napproachesshouldberegardedascomplementarytomore\nconventional tools like textuallists.\nWe believe that advanced but yet simple visualizations\nare critical for supporting the process of exploring and\ntherefore re-discovering personal music collections. Ac-\ntually,itseemsreasonabletobelievethatmanytimesusers\nmaybeinterestedinrediscoveringtheirownmusicinstead\nof thinking about enlarging their collections. Moreover,\nusers may be interested in exploring their music collec-\ntiontoactuallydecidewhattoacquireorlistennext. Cur-\nrently, this rediscovering process can be tedious by using\ntextuallists,whilethepresentednewapproachesfacilitate\nsuch task.\n4.1. User Studies\nStronger and ﬁnal arguments for validating the suggested\napproachesshouldbegivenbyrigoroususerstudies. These\nuserstudieswillbedevelopedwithdifferenttypeofusers\nconsidering at least factors like technology matureness,\nage,educationalbackground,andtopologyoftheirlibraries\n(size, recency of tracks, homogeneity).\nAcknowledgments\nWe would like to thank Michel Speiser for his great im-\nplementation of the ideas shown in this paper during a\nsemesterprojectattheEPFL.WealsothankMusicStrands\nS.A. for providing XML parsers to read real iTunesli-\nbraries.\n5. REFERENCES\n[1] Ben Shneiderman. Tree Visualization with Tree-\nMaps: 2-d Space-Filling Approach. ACM Transac-\ntions on Graphics , 11(1):92–99, January 1992."
    },
    {
        "title": "A Comparison of Rhythmic Similarity Measures.",
        "author": [
            "Godfried T. Toussaint"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416812",
        "url": "https://doi.org/10.5281/zenodo.1416812",
        "ee": "https://zenodo.org/records/1416812/files/Toussaint04.pdf",
        "abstract": "Traditionally, rhythmic similarity measures are compared according to how well rhythms may be recognized with them, how efficiently they can be retrieved from a data base, or how well they model human perception and cog- nition. In contrast, here similarity measures are compared on the basis of how much insight they provide about the structural inter-relationships that exist within families of rhythms, when phylogenetic trees and graphs are com- puted from the distance matrices determined by these sim- ilarity measures. Phylogenetic analyses yield insight into the evolution of rhythms and may uncover interesting an- cestral rhythms.",
        "zenodo_id": 1416812,
        "dblp_key": "conf/ismir/Toussaint04",
        "keywords": [
            "Rhythmic similarity measures",
            "Phylogenetic analysis",
            "Rhythm evolution",
            "Distance matrices",
            "Music information retrieval",
            "Copyright infringement resolution",
            "Rhythm families",
            "Bell-patterns",
            "Clave patterns",
            "Computational music theory"
        ],
        "content": "ACOMPARISONOFRHYTHMIC SIMILARITY MEASURES\nGodfried T.Toussaint\nMcGillUniversity\nSchoolofComputer Science\nABSTRA CT\nTraditionally ,rhythmic similarity measures arecompared\naccording tohowwell rhythms may berecognized with\nthem, howefﬁciently theycanberetrie vedfrom adata\nbase, orhowwell theymodel human perception andcog-\nnition. Incontrast, here similarity measures arecompared\nonthebasis ofhowmuch insight theyprovide about the\nstructural inter-relationships thatexistwithin families of\nrhythms, when phylogenetic trees andgraphs arecom-\nputed from thedistance matrices determined bythese sim-\nilarity measures. Phylogenetic analyses yield insight into\ntheevolution ofrhythms andmay unco verinteresting an-\ncestr alrhythms.\n1.INTR ODUCTION\nMeasuring thesimilarity between rhythms isafundamen-\ntalproblem incomputational music theory ,with manyap-\nplications such asmusic information retrie valandcopy-\nright infringement resolution. Ithasbeen argued thatthe\ntype ofsimilarity measure used should depend onthein-\ntended application [8].Wedescribe theresults ofapilot\nstudy carried outtocompare avariety ofrhythm similar -\nitymeasures with respect tohowuseful theyareforthe\nphylogenetic analysis offamilies ofrhythms. Itshould be\nemphasized thatthefocus ofthispaper isnotthemeasure-\nment ofpsychological similarity (based onperception).\nItisalso assumed that thedata isobtained from musi-\ncalscores rather than audio, andthus tempo problems are\nignored. Twofamilies ofrhythms were selected forthis\nstudy ,thefundamental rhythm timelines in4/4and12/8\ntime, used intraditional West-African andAfro-American\nmusic [12],[14].These timelines arealso called bell-\npatterns orclave patterns, ofwhich themost famous on\ntheworldmusic scene istheclave Son.Although thedata\nused inthisstudy consist ofonly twosmall families of\nrhythms, these were chosen fortworeasons: (1)theyare\nimportant andwell knowninworldmusic, and(2)theyare\nsimple andwell understood from themusicological point\nofview,thus simplifying thevalidation process oftheir\nphylogenetic analyses.\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forproﬁt orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation ontheﬁrstpage.\nc\r2004 Universitat Pompeu Fabra..4\n4.\n..C\n....4\n4....4\n4\nx . . x . . x . . . x . x . . .\n1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0\n3 3 4 2 41.\n2.\n3.\n4.\n5.\n6.\n7.\n8.\nFigur e1.Eight common waysofrepresenting theclave\nSonrhythm.\nTheclave Sonisusually notated using standard music\nnotation which affords manywaysofexpressing arhythm.\nFoursuch examples aregiveninthetopfour lines ofFig-\nure1.Western music notation isnotideally suited torep-\nresent African music [1].Theremaining lines showways\nofrepresenting rhythms thatavoidWestern notation.\nThere exists awide variety ofmethods formeasuring\nthesimilarity oftworhythms represented bystrings of\nsymbols [13].Traditionally similarity between twopat-\nterns ismeasured byasimple template matching opera-\ntion. More recently similarity hasbeen measured with\nmore powerful andcomple xfunctions such astheearth\nmover’sdistance [2].\nInthissection wedescribe theﬁve,easy tocompute,\npopular andrepresentati vemeasures ofrhythm similar -\nity(ordissimilarity) thatwere compared inthisstudy .It\nshould benoted thatalthough therhythms considered here\nhave,within each family ,thesame number ofonsets, the\nHamming, swap,andchronotonic distance measures do\nnotassume thisrestriction andworkjustaswell forcom-\nparing rhythms containing unequal numbers ofonsets.\nThe Hamming distance :The Hamming distance isthe\nnumber ofplaces inthebinary n-bitstring representation\noftherhythm where bitsdonotmatch. Itiseasily com-\nputed inO(n)time.\nThe Euclidean inter valvector distance :Some rhythm\ndetection algorithms andsystems formachine recognition\nofmusic patterns [3]useinter-onset intervals asthebasicfeatures formeasuring similarity .These aretheinterv als\noftime between consecuti venote onsets inarhythm. The\ndissimilarity between tworhythms, each consisting ofn\ntime interv als,may becomputed inO(n)time using the\nEuclidean distance between thetwointerv alvectors.\nThe inter val-ratio distance :CoyleandShmule vich [3]\nrepresent amusic pattern bywhat theycalladifference-\nof-rhythm vector .IfT=(t1;t2;:::;tn)isavector of\ninter-onset time interv alsthen theydeﬁne thedifference-\nof-rhythm vector asX=(x1;x2;:::;xn\u00001),where xi=\nti+1=ti.Theypropose adistance measure based onthis\nvector which may also beeasily computed inO(n)time.\nThis measure iscalled here theinterval-r atiodistance .\nThe swap distance :Ageneralization oftheHamming\ndistance istheedit distance which allowsforinsertions\nanddeletions ofnotes [9].Using dynamic programming\ntheeditdistance may becomputed inO(n2)time.\nThe problem ofcomparing twobinary strings ofthe\nsame length with thesame number ofone’ssuggests an\nextremely simple editoperation called aswap .Aswapis\naninterchange ofaoneandazero thatareadjacent toeach\nother .Theswapdistance istheminimum number ofswaps\nrequired toconvertonerhythm toanother .Note thatthis\ndistance measure extends naturally tohandle rhythms with\nadifferent number ofonsets. Assume thatrhythm Ahas\nmore onsets than rhythm B.Then theswapdistance may\nbedeﬁned astheminimum number ofswapsrequired to\ntakeallonsets ofAtotheonsets ofB,with therestriction\nthateach onset ofBmust recei veatleast oneonset ofA,\naswasdone in[4]fortheanalysis ofFlamenco rhythms.\nThe swapdistance may becomputed byactually per-\nforming theswaps,butthisisinefﬁcient because aquadratic\nnumber ofswapsmay berequired intheworstcase. Ifwe\ncompare theinterv aldistances instead, theswapdistance\nmay becomputed inO(n)time when therhythms have\nthesame number ofonsets, andinsome other situations\naswell, resulting inalargegain overusing linear ordy-\nnamic programming.\nThe chronotonic distance :In1987 Kjell Gustafson, at\nthePhonetics Laboratory oftheUniversity ofOxford, pro-\nposed anoriginal method torepresent arhythm asatwo-\ndimensional graph [5].Hisidea isbest explained with an\nexample. Consider ﬁrst theclaveSonpattern shownon\nlineﬁveofFigure 1.Although thisrepresentation isreal-\nistic interms ofthetime atwhich beats occur ,therel-\nativedurations oftheinterv alsarenoteasily observ ed.\nInahisto gramapproach torhythm visualization thein-\ntervalsareplotted along they-axis [5],resulting inthe\nadjacent-interval-spectrum oftherhythm inwhich therel-\nativelengths oftheinterv alsareclearly visible butthe\ntemporal information along thex-axis islost. Toobtain\nagraphical representation thatposesses theadvantages of\nboth ofthese methods, Gustafson simply combines them.\nThe result ofthisunion isillustrated with theclaveSon\ninFigure 2.Each temporal element (interv al)isnowa\nbox andboth thexandyaxesrepresent thelength of\ntime oftheinterv al.Gustafson refers tosuch adisplay as\nTED AS(Temporal Elements Displayed AsSquares). This0123456789101112131415\nClave Son16\nFigur e2.Thechronotonic (TED AS)representation ofthe\nclaveSon.\nidea wasredisco vered byHofmann-Engl [6].Wewillre-\nfertothisasthechronotonic representation ofarhythm.\nViewing thechronotonic representation asapiece-wise\nlinear function opens thedoor toalargefamily ofpossi-\nbledistance functions with along history intheﬁelds of\nstatistics andpattern recognition [10].Giventwoproba-\nbility density functions f1(x)andf2(x),there aremany\nmeasures ofthedistance (orseparation) between them.\nOneofthemost well knownistheKolmo gorovvariational\ndistance [11]givenby:\nK=Z\njf1(x)\u0000f2(x)jdx: (1)\nHere themeasure Kisused tocompare rhythms using\nthechronotonic representation proposed byGustafson [5]\n(eventhough thefunctions arenotprobability density func-\ntions). Inthisdiscrete setting itisclear thatKmay be\ncomputed easily inO(n)time.\n2.COMP ARING RHYTHMIC SIMILARITY\nMEASURES\nThe measures discussed inthepreceeding section were\ncompared using twofamilies ofrhythms: thefundamen-\ntalbinary andternary timelines used inWest-African and\nAfro-American traditional music. Theﬁrstfamily consists\nofthesixﬁve-beat clave-bell patterns in4/4time [12],and\nthesecond iscomprised ofthetenseven-beat bellpatterns\nin12/8 time [14].These twofamilies aredepicted inthe\nchronotonic notation ofGustafson inFigures 3and4,re-\nspecti vely.One ofthemain goals ofthisstudy istoob-\ntaininsight into theevolution ofrhythms aswell asun-\ncoverinteresting “ancestral” rhythms. Hence, rather than\nuseanyofthemyriad traditional cluster analysis methods,\nhere phylogenetic analysis tools from bioinformatics are\nused. After all,atapurely mathematical level,rhythms\nandDNAmolecules areboth sequences ofsymbols.\nSeveral techniques exist forgenerating phylogenetic\ntrees from distance matrices [7].Some ofthese methods\nhavethedesirable property thattheyproduce graphs that\narenottrees, when theunderlying structure isinherently\nnottree-lik e.One notable example isSplitsT ree[7].Like\nthemore traditional phylogenetic trees, SplitsT reecom-\nputes aplane graph embedding with theproperty thatthe\ndistance inthedrawing between anytwonodes reﬂects the\ntrue distance between thecorresponding rhythms inthe\ndistance matrix. However,ifthetreestructure does not012345678910111213141516\n012345678910111213141516\n012345678910111213141516\n012345678910111213141516\n012345678910111213141516SonShiko\nSoukous\nRumba\nBossa\nGahu012345678910111213141516\nFigur e3.Thechronotonic representation ofthe4/4time\nclave patterns.\nmatch thedata perfectly then newnodes inthegraph may\nbeintroduced with thegoal ofobtaining abetter ﬁt.Such\nnodes may suggest implied “ancestral” rhythms from which\ntheir “offspring” may bederived.Inaddition, edges may\nbesplit toform parallelograms, such asinFigure 5.The\nrelati vesizes ofthese parallelograms areproportional to\nanisolation indexthatindicates howsigniﬁcant theclus-\ntering relationships inherent inthedistance matrix are.\nSplitsT reealso computes thesplitability index,ameasure\nofthegoodness-of-ﬁt oftheentire splits graph. This ﬁt\nisobtained bydividing thesum ofalltheapproximate\ndistances inthesplits graph bythesum ofalltheorigi-\nnaldistances inthedistance matrix [7].Forthese reasons\nSplitsT reewasthemethod ofchoice inthisstudy .\n3.RESUL TS\nSpace limitations donotpermit adiscussion ofallthe\nresults. Sufﬁceittosaythat thequantitati vephyloge-\nnetic analyses support thetenets previously established\nforthese rhythms viatraditional qualitati vemusicological\nmethods. Here only some oftheresults obtained with the\nbestmeasure, thechronotonic distance, aresummarised.\nForthe4/4time rhythms thesplits graph showninFig-\nure5isatree-lik egraph with aperfect ﬁtof100% This\ndistance measure istheonly onethat exhibits astrong\nclustering inthisfamily ofrhythms. Inparticular ,thelong\nparallelogram clearly separates Bossa-No vaandGahu from\ntherest. Anexamination oftheir chronotonic functions of\ntime (see Figure 3)yields aclearly distinguishable dis-\ncriminating feature. TheBossa-No vaandGahu functions\nhaveonly onelocal maximum, whereas each ofthefour\nother rhythms hastwolocal maxima.\nForthe12/8 time rhythms thesplits graph shownin\nFigure 6exhibits arichstructure with aperfect ﬁtof100%.\nThere aretwohighly clustered groups. The ﬁrst group0123456789101112\n0123456789101112\n0123456789101112\n0123456789101112\n0123456789101112\n0123456789101112\n0123456789101112\n0123456789101112\n0123456789101112\n0123456789101112Soli\nTambú\nBembé\nBembé-2\nYoruba\nTonada\nAsaadua\nSorsonet\nBemba\nAshanti\nFigur e4.Thechronotonic representation ofthe12/8 time\nbellpatterns.\nRumba\nSoukousSonShiko\nBossa-NovaGahuFit=100.0%\nFigur e5.The4/4time rhythms.\nconsists ofSoli,Asaadua ,Tamb´u,Yoruba ,andBemb ´e.\nThesecond group iscomprised ofTonada ,Ashanti ,Bemb ´e-\n2,andSorsonet .Furthermore, Bemba lieshalf-w aybe-\ntween thetwogroups.\nThesplits graph hasnine potential ancestral nodes. One\nofthese isdistinguished from therest, notonly because\nofitshigh degree, butbecause itislocated inthecen-\nteroftheentire collection, andisatadistance oftwo\nfrom alltenrhythms. Forthisreason itislabelled the2-\ncenter oftheset.Areconstruction ofthisancestral rhythm\nyields thepattern [x.x.x.x.x.x.],which isasteady ,\nisochronous “heart-beat. ”\nSeveraldesirable criteria may beused forcomparing\nphylogenetic trees offamilies ofrhythms. Such criteria\ninclude (1)simplicity ,(2)goodness ofﬁt,(3)indication\nofclustering, and(4)generation ofinteresting “ancestral”\nrhythms.Tonada\nAshanti Bembé-2SorsonetBembaSoliAsaaduaTambú\nYoruba\nBembé\n2-centerFit=100.0%\nFigur e6.The12/8 time rhythms.\nSimplicity :Thesimplest splits graphs were obtained with\ntheswapdistance.\nGoodness ofﬁt:Theonly distance measures thatyielded\na100% ﬁtforboth binary andternary rhythms were the\nswapandchronotonic distances. TheHamming distance\nalsoproduced a100% ﬁtforthebinary rhythms.\nClustering :Themost impressi veclustering wasobtained\nwith thechronotonic distance. Forthebinary rhythms,\nGahu andBossa-No vaareclearly differentiated from the\nrest. Fortheternary rhythms thesplits graph yields two\nwell separated clusters.\nAncestral rhythm generation :Themost noteworthy “an-\ncestral” rhythms were produced with thechronotonic dis-\ntance forternary rhythms andwith theHamming distance\nforthebinary rhythms.\nInconclusion, acomparison ofthesigniﬁcance ofthe\nrole played bytheﬁvedistance measures according to\neach ofthefour criteria outlined inthepreceeding, sug-\ngests thatthebest rhythmic dissimilarity measure isthe\nchronotonic distance, follo wed bytheswap distance in\nclose second place. Generalizing these conclusions toother\nlargerfamilies ofrhythms ispremature. However,these\nresults constitute apositi veﬁrststep, andencourages the\ncontinuation ofthislineofresearch ingreater depth.\n4.REFERENCES\n[1]Simha Arom. African Polyphony andPolyrhythm .\nCambridge University Press, Cambridge, England,\n1991.\n[2]Sung-Hyuk Cha andSargurN.Srihari. Onmeasur -\ningthedistance between histograms. Pattern Reco g-\nnition ,35:1355–1370, 2002.\n[3]E.J.CoyleandI.Shmule vich. Asystem forma-\nchine recognition ofmusic patterns. InProceedings\noftheIEEE International Confer ence onAcoustics,\nSpeec h,andSignal Processing ,Seattle, Washington,\n1998.[4]Miguel D´ıaz-Ba ˜nez, Giovanna Farigu, Francisco\nG´omez, DavidRappaport, and Godfried T.Tous-\nsaint. Elcomp ´asﬂamenco: aphylogenetic anal-\nysis. InProceedings ofBRIDGES: Mathematical\nConnections inArt,Music andScience ,Southwest-\nernColle ge,Winﬁeld, Kansas, July 30-August 1\n2004.\n[5]Kjell Gustafson. The graphical representation of\nrhythm. In(PROPH) ProgressReports fromOxfor d\nPhonetics ,volume 3,pages 6–26, University ofOx-\nford, 1988.\n[6]Ludger Hofmann-Engl. Rhythmic similarity: A\ntheoretical andempirical approach. InC.Stevens,\nD.Burnham, G.McPherson, E.Schubert, and\nJ.Renwick, editors, Proceedings oftheSeventh In-\nternational Confer ence onMusic Perception and\nCognition ,pages 564–567, Sidne y,Australia, 2002.\n[7]Daniel H.Huson. SplitsT ree: Analyzing andvisu-\nalizing evolutionary data. Bioinformatics ,14:68–73,\n1998.\n[8]Benoit Meudic. Musical similarity inapolyphonic\nconte xt:amodel outside time. InProceedings ofthe\nXIV Colloquium onMusical Informatics ,Firenze,\nItaly,May 8-10 2003.\n[9]KeithS.Orpen andDavidHuron. Measurement of\nsimilarity inmusic: Aquantitati veapproach fornon-\nparametric representations. InComputer sinMusic\nResear ch,volume 4,pages 1–44. 1992.\n[10] Godfried T.Toussaint. Onthedivergence between\ntwodistrib utions andtheprobability ofmisclassiﬁ-\ncation ofseveraldecision rules. InProceedings of\ntheSecond International Joint Confer ence onPat-\ntern Reco gnition ,pages 27–35, Copenhagen, Den-\nmark, August 13-15 1974.\n[11] Godfried T.Toussaint. Sharper lowerbounds fordis-\ncrimination information interms ofvariation. IEEE\nTransactions onInformation Theory ,pages 99–100,\nJanuary 1975.\n[12] Godfried T.Toussaint. Amathematical analysis of\nAfrican, Brazilian, and Cuban clave rhythms. In\nProceedings ofBRIDGES: Mathematical Connec-\ntions inArt, Music and Science ,pages 157–168,\nTowson University ,Towson, MD, July 27-29 2002.\n[13] Godfried T.Toussaint. Algorithmic, geometric, and\ncombinatorial problems incomputational music the-\nory.InProceedings ofXEncuentr osdeGeome-\ntriaComputacional ,pages 101–107, University of\nSevilla, Sevilla, Spain, June 16-17 2003.\n[14] Godfried T.Toussaint. Classiﬁcation andphyloge-\nnetic analysis ofAfrican ternary rhythm timelines.\nInProceedings ofBRIDGES: Mathematical Con-\nnections inArt, Music and Science ,pages 25–36,\nGranada, Spain, July 23-27 2003."
    },
    {
        "title": "Utility System For Constructing Database Of Performance Deviations.",
        "author": [
            "Ken&apos;ichi Toyoda",
            "Kenzi Noike",
            "Haruhiro Katayose"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417953",
        "url": "https://doi.org/10.5281/zenodo.1417953",
        "ee": "https://zenodo.org/records/1417953/files/ToyodaNK04.pdf",
        "abstract": "Demand for music databases is increasing for the stud- ies of musicology and music informatics. Our goal is to construct databases that contain deviations of tempo, and dynamics, start-timing, and duration of each note. This paper describes a procedure based on hybrid use of DP Matching and HMM that efficiently extracts deviations from MIDI-formatted expressive human performances. The algorithm of quantizing the start-timing of the notes has been successfully tested on a database of ten expressive piano performances. It gives an accuracy of 92.9% when one note per bar is given as the guide. This paper also in- troduces tools provided so that the public can make use of our database on the web. Keywords Database, HMM, DP matching",
        "zenodo_id": 1417953,
        "dblp_key": "conf/ismir/ToyodaNK04",
        "keywords": [
            "database",
            "tempo deviations",
            "dynamics",
            "start-timing",
            "duration",
            "MIDI-formatted",
            "expressive human performances",
            "quantizing notes",
            "accuracy",
            "public access"
        ],
        "content": "UTILITY SYSTEM FOR CONSTRUCTING DATABASE OF\nPERFORMANCE DEVIATIONS\nKen’ichi Toyoda, Kenzi Noike, Haruhiro Katayose\nKwansei Gakuin University\nGakuen, Sanda, 669-1337 JAPAN\n{toyoda, noike, katayose }@ksc.kwansei.ac.jp\nABSTRACT\nDemand for music databases is increasing for the stud-\nies of musicology and music informatics. Our goal is to\nconstruct databases that contain deviations of tempo, and\ndynamics, start-timing, and duration of each note. Thispaper describes a procedure based on hybrid use of DP\nMatching and HMM that ef ﬁciently extracts deviations\nfrom MIDI-formatted expressive human performances. The\nalgorithm of quantizing the start-timing of the notes has\nbeen successfully tested on a database of ten expressivepiano performances. It gives an accuracy of 92.9% when\none note per bar is given as the guide. This paper also in-\ntroduces tools provided so that the public can make use ofour database on the web.\nKeywords Database, HMM, DP matching\n1. INTRODUCTION\nMusic databases can contribute to studies of musicology\nand music informatics. The information that music data-bases should provide is diverse: from the acoustics sounds\nof musical instruments to SMF ﬁles, etc. As for the acous-\ntics and sounds of musical instruments, a copyright-freedatabase has been provided for academic use by Goto et\nal. [1].\nThis paper deals with a database of musical deviation\nthat characterizes music “expressiveness.” In ethnomusi-\ncology, there are reports that focus on musical deviations.\nFor example, Bell collected musical deviations of Indianmusic (RAGA) [2]. As for western tonal music, for in-\nstance, CCARH at Stanford University is engaged in the\ndevelopment of large databases of musical and textual ma-terials [3]. However, there are few databases of deviations\navailable for academic use. Widmer’s project is one of the\nfew that have obtained musical deviations in western mu-\nsic [4]. The researchers have collected the deviations from\nHorowitz’ performances and have been trying to classifythe performances into certain styles. The derived devia-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copiesare not made or distributed for pro ﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2004 Universitat Pompeu Fabra.\ntions are limited to transitions of tempi and general dy-\nnamics, and the database is yet to be published.\nOur goal is to construct databases that contain devia-\ntions regarding tempo and dynamics, start-timing, and du-\nration of each note. This paper describes, as the ﬁrst step\nin constructing databases, a method to obtain deviationdata from MIDI-formatted music and tools for testing the\nutility of the database.\nTheﬁrst thing that we have to do to obtain deviations\nis quantization. Quantization is one of the principal com-petences in transcribing music. In the 1980’s, effective\nquantization methods were proposed [5][6][7]. Recently,\nit has been reported that approaches based on statistics arehighly effective [8][9][10]. However, it is still dif ﬁcult to\nsecure accuracies of more than 90% in quantizing expres-\nsive music, and such high accuracy is needed to reveal the\ndeviations. Moreover, manual correction to cover the er-\nrors is more troublesome.\nThe other practical way of obtaining error-free data is\nto utilize score guides. Katayose et al. derived devia-tions of each note, using dynamic programming matching\n(DP) between the musical sound and its score [11]. This\nmethod is reliable enough to obtain error-free data. How-\never, the input of score data is itself time-consuming.\nThis paper proposes an ef ﬁcient procedure for compil-\ning a deviation database, based on hybrid use of a match-\ning procedure and a quantization procedure. A coarseDP matching between the performance and sparse guides\ngiven by the user contribute to limit the freedom of tempo\nchanges. A Hidden Markov Model (HMM) is used for\nassigning the time values to the notes between the spans\nﬁxed by the matching procedure.\nThe next section describes the design of the data for-\nmat. We then describe the procedures to obtain expressiontemplates and show some experimental results. Finally,\nwe discuss the effectiveness of the database together with\nsome tools for utilizing it.\n2. MUSICAL EXPRESSION AND DATA FORMAT\n2.1. How to describe musical expressions\nThere are various control channels and methods for play-\ning musical instruments. Enumeration of controllers of\nthe delicate nuance of acoustic sound, itself, may be one.....\n.....2.00  BEATTIME  475.436  4\n2.00  ( 0.00  E3  78  3.00  -0.11 )\n=21.00  BEATTIME  468.384  4\n1.00  ( 0.00  C#4  76  0.75  -0.09 )  (0.04  E1  60  1.00  -0.13 )\n1.75  ( 0.10  D4  77  0.25  -0.14 )\n2.00  BEATTIME  461.538  4\n2.00  ( 0.00  B3  75  1.00  -0.03 )  (0.00  G#3  56  1.00  0.03)\n3.00  BEATTIME  469.851  4\n3.00  ( 0.00  B3  72  1.00  0.00)  (0.09  G#3  56  1.00  -0.12 )  (0.14  D3  57  1.00  -0.21 )\n=3\n1.00  BEATTIME  470.219  4\n1.00  ( 0.00  B3  77  2.00  -0.05 )  (0.00  G#3  47  2.00  -0.05 )  (-0.06  D4  57  2.00  -0.32 )\n2.00  BEATTIME  470.219  4\n3.00  BEATTIME  462.606  4\n3.00  ( 0.00  F#4  75  1.00  -0.15 )  (\n0.00  D4   54  1.00  0.03)\n=4\n1.00  BEATTIME  469.851   4\n1.00  ( 0.00  D#4  73  0.75  -0.38 )  (0.02  C4  65  0.75  -0.08 )\n..........\nFigure 1 . Data Format of Deviation Database\nof the important study targets. Limited to keyboard in-\nstruments represented by piano, the controllers to play the\nmusic are start timing, duration, and dynamics (velocity inMIDI) of each note and pedal operation. We designed the\nNOTE format, in which the deviations from the canonical\ndescription were separately described for each event.\n2.2. Format\nFigure 1 shows a part of the deviation data in the database\n[12]. The left column represents the start timing of the\nevents. Information about each note, except for the start\ntiming, is placed in brackets. Each bracketed term, in or-der, represents, the deviation of start time, note name, ve-\nlocity, duration, and the deviation of duration. In this for-\nmat, the tempo is described using the descriptor beattime .\nThe description is followed by the note length in millisec-\nonds and its note value.\n3. SYSTEM OVERVIEW\nThe approach to extracting deviations is as follows (see\nFigure 2):\n1. Prepare performance data recorded in MIDI format\nand its guide data described in NOTE format. The\nguide data is translated from SMF (standard MIDI\nﬁle) or MusicXML data by using the application\n‘SMF2note’[13].\n2. Match performance notes and guide notes by using\nDP Matching. In this process, the canonical onset\ntiming of the performance’s notes and local tempi\nare partly given.\n3. Quantize the onset timing of notes between the spans\nﬁxed by the matching procedure by using a Hidden\nMarkov Model (HMM) of which hidden states cor-\nrespond to the canonical onset timing and the out-\nputs represent the ﬂuctuating onset timing.\n4. Calculate local tempi\n1deviations of onset timing\n1The positions where calculated tempi (for example, per downbeat or\nper bar) can be given by the user.Performance Data\n  0.000 ý(F4 1.000)\n  4.000 ý(G4 3.000)\n  8.000 ý(C4 2.000)\n12.000    (C#5 0.500) ýĠĠ\nĠĠ\n=11.000 BEATTIME 650.244  41.000 (+0.000  C3  50  0.500  -0.028) 1.500 (- 0.096  F3  54  0.250  +0.110)  ĠĠPerformance Deviation DataGuide Data\nD.P.Matching\nQuantization of Notes\n      using  HMMQKD\nGuidePerformance\nFigure 2 . System Overview : Extraction of deviation data\nwith hybrid use of DP matching and HMM quantization\nand those of the durations and assign velocities to\nthe identi ﬁed notes.\n5. Output the deviation data\n4. MATCHING PERFORMANCE NOTES AND\nGUIDE NOTES\nThis section describes the method to match the perfor-\nmance notes and guide notes by using DP [14]. This pro-\ncedure reduces the errors of the following quantization\nprocedure.\n4.1. DP Matching\nEach node P(k,l)of the trellis represented in the mid-\ndle of Figure 2 denotes the correspondence of the perfor-\nmance note and guide note. P(k,l)has a value S(k,l)\nwhich denotes the similarity of the correspondence de-\npending on the route from the origin node to P(k,l).S(k,l)\nis calculated as:\nS(0,0) = 0\nS(k,l)=max(sim(i,j,k,l )+S(i,j)) (1)\nk−m≤i<k ,l −n,≤j<l\nwhere m,nare constants and sim(i,j,k,l )denotes the\nsimilarity of the correspondence which depends on the\nroute from P(i,j), the parent node of P(k,l),t oP(k,l).P(k,l)’s parent node which maximizes S(k,l)is also de-\ntermined by equation (1). In this regard, however, a node\ndenoting the correspondence of two notes having differ-\nent pitches is not assumed to be the candidate of P(k,l)’s\nparent. The most appropriate correspondence of perfor-\nmance and guide notes for the given song is obtained by\ntracking back parent nodes from the terminal node to theorigin node.\n4.2. Similarity of correspondence\nsim(i,j,k,l )in the equation (1) is calculated as:\nsim(i,j,k,l )= 3/summationdisplay\nn=1wn·sn(i,j,k,l ),\nwhere s1,s2,s3mean as follows:\n•s1denotes the similarity between the local tempo\ndetermined by guide notes and the average tempoof a whole song. s\n1reﬂects the assumption that,\nthough local tempi does not ﬂuctuate much com-\npared with the ﬂuctuation of the whole tempo tran-\nsition.\n•s2denotes the similarity between the onset time of\na guide note and that of a performance note. Here,\nboth are compared on the millisecond time scale.\nThe onset times of guide notes are converted fromthe beat scale to the millisecond time scale by using\nthe average tempo of the whole song.\n•s\n3denotes the likelihood of the velocity of the per-\nformance note in terms of the canonical onset tim-\ning of the guide note. When the velocity takes on a\nlarge value at the downbeat, s3also takes on a large\nvalue, and vice versa.\nwndenotes the weighting factor of sn. We determined sn\naccording to pilot studies.\n5. QUANTIZATION USING HMM\nThis section describes the quantization procedure of on-\nset timing based on the HMM. The proposed method fol-lows Otsuki’s approach in part [8]. Otsuki’s algorithm\nestimates the canonical onset timing of chords (merging\nchords) as a preprocessing. However, with Otsuki’s algo-rithm, there may be an initial error in the whole quanti-\nzation procedure. In contrast, our quantization algorithm\nfor chords does not require the preprocessing for merg-\ning chords. Our approach represents simultaneous onset\ntimings as auto-regressions of a state in the HMM, andtheir detection is embedded in the quantizing process. We\ncanﬁnd the same idea in the recent work of Takeda et al.\n[10]. The difference between our algorithm and Takeda’sis that ours embeds in the HMM the factors regarding not\nonly IOIs (Inter Onset Intervals) but also their ratios and\nvelocities.0.00 1.00 1.50 0.50 1.00timevelocity: a sequence of performance notes\n: a sequence of quantized onset timesЇ\nϺ\nFigure 3 . A hidden Markov model which has canonical\nonset timing as hidden states and ﬂuctuated onset timing\nand velocities as outputs\n5.1. Formulation of Quantization using HMM\nLetΦ=( φ1,φ2,...,φ T)denote a sequence of feature\nvectors whose elements are onset timing and velocities of\nperformance notes. Θ=( θ1,θ2,...,θ T)2denotes a se-\nquence of canonical onset timings. Here, estimating the\nintended Θfrom the observed Φis assumed to be the same\nasﬁnding the Θ∗that maximizes the a posteriori probabil-\nity,P(Θ|Φ). According to Bayes theorem, Θ∗also maxi-\nmizes P(Φ|Θ)P(Θ). The above description is formulated\nas follows:\nΘ∗=a r gm a x\nΘP(Φ|Θ)P(Θ) (2)\nWe assume further that the probability of θtappearing\ndepends only on the preceding θt−1and that Θcould be\ndescribed as a chain of units called “Rhythm Words[8]”\nwhich means small units having states corresponding tothe canonical onset timing. Then, Θ\n∗can be rewritten as:\nΘ∗=a r g m a x\nΘ∈GWT/productdisplay\nt=1aθt−1θtbθt(φt) (3)\nwhere Gwdenotes the whole set whose elements are chains\nof Rhythm Words obtained from 77 musical scores made\nin advance, aθt−1θtdenotes the transition probability from\nθt−1toθtandbθt(φt)denotes the probability that φtemerges\nfromθt. The sequence of intended onsets Θ∗is estimated\nbased on equation (3) , where, giving credit to the preced-\ning matching process, any Θconﬂicting with the guide’s\nonset timing is excluded from the candidates.\n5.2. Factors regarding performance ﬂuctuations\nThe IOI, ratio of IOIs, and velocity were characterized as\nfollows:\n•IOI\nbqtdenotes the probability that a canonical IOI qθt(=\nθt+1−θt) is observed as an IOI qφt(=φt+1−φt).\n2In what follows, θtdenotes both the value and the corresponding\nstate.Table 1 . Results of quantizing onset timing : The top row represents the amount of guide notes. For example, ’per 1 bar’\nrepresents that the note whose onset timing is the earliest in each bar is given as a guide.\nstart and end notes\n 1 note per 2 bars\n 1 note per 1 bar\n 1 note per 1 beat\n 1 note per 1 chord\n\u0000itle\n threshold HMM\n threshold HMM\n threshold HMM\n threshold HMM\n threshold HMM\nMinuet\n 8.1 99.0\n 84.4 99.8\n 93.8 99.8\n 99.0 99.8\n 99.5 99.8\nPrelude\n 26.4 96.9\n 97.4 96.9\n 97.8 96.9\n 99.1 98.7\n 100 99.3\nGavotte\n 11.7 91.5\n 92.9 93.9\n 99.4 99.4\n 99.4 99.7\n 99.6 99.7\nWaltz No.7\n 8.3 86.2\n 57.1 96.8\n 74.0 96.8\n 97.8 96.8\n 100 100\nCuckoo\n 19.4 34.6\n 88.3 85.7\n 98.0 94.0\n 98.0 97.7\n 99.6 99.4\nLittle serenade\n 10.4 57.2\n 58.4 86.6\n 95.7 100\n 99.7 100\n 99.7 100\nf¨ur Elise\n 11.6 23.1\n 65.6 77.8\n 78.1 81.7\n 96.3 97.8\n 99.7 100\nTroimerai\n 7.8 55.6\n 63.8 80.3\n 76.8 95.3\n 98.0 98.4\n 98.9 100\nK.331\n 7.6 43.5\n 46.4 78.5\n 67.1 95.4\n 98.3 98.7\n 99.6 99.6\nRaindrop\n 19.5 27.4\n 38.1 48.5\n 54.5 69.5\n 85.7 86.0\n 99.6 99.9\naverage\n 13.1 61.7\n 69.2 84.5\n 83.5 92.9\n 97.1 97.4\n 99.6 99.8\nIn this case, bqis assumed to follow a Gaussian dis-\ntribution whose mean is qθt.\n•ratio of IOIs\nLetrθtdenote the “canonical” ratio of adjacent two\nIOIs, that is qθttoqθt−1.brtdenotes the probabil-\nity that a canonical rθtis observed as an rφt.I n\nthis case, brtis also assumed to follow a Gaussian\ndistribution whose mean is rθt.\n•velocity\nbvtdenotes the probability that the tthnote’s veloc-\nity is observed as vθt.vmaxdenotes the maximum\nvelocity in the several notes in the vicinity of thet\nthnote. If the tthnote corresponds to the down-\nbeat,bvtis assumed to follow a Gaussian distribu-\ntion whose mean is vmax\nThe variances in the distributions of the above probabil-\nities were statistically determined by using 45 deviation\ndata made in advance. Here, we assumed that bθt(φt)\napproximates the multiplication of these probabilities, as\nfollows:\nbθt(φt)≈bqt·brt·bvt\n6. EXPERIMENTAL EVALUATION\nThe algorithm for quantizing the start-timing has been test-\ned on a database of ten MIDI-formatted expressive piano\nperformances. These test samples contain both rhythmi-\ncally complex pieces (e.g. ‘Troimerai’) and simple pieces\n(e.g. “Menuett G dur”).\n6.1. Experiments of Quantizing onset timing\nLet the percentage of correct answers Cor onbe:\nCoron=N−nsub\nN×100\nwhere N,nsubare de ﬁned as follows:timepitch\nC5\nG4\n&œœ? ??performance\nguide\nFigure 4 . An example of mistakable cases in DP matching\n•N: the number of performance notes that exist in\nthe answer data3\n•nsub: the number of notes in Nfor which the onset\ntimings are not equal to the answer.\nThe experimental results are shown in Table 1. Each\nnumber in the table represents the average recognition rates\nfor 10 sample pieces. The results gotten by the proposed\nalgorithm are in the “HMM” columns. For comparison,the results of experiments where the quantization resolu-\ntion is ﬁxed to a sixteenth note are also shown.\nThe results show that the proposed algorithm is bet-\nter than a simple threshold-based quantization processing.\nWhen 1 note per bar is given as the guide, the averagerecognition rate reached 92.9%\n4. This result could be said\nto meet the purpose of making deviation data ef ﬁciently\nand precisely. The proposed method will especially beuseful to those untrained in making score data, who use\nnotation software such as MakeMusic!’s FINALE.\nThe errors in the quantization are classi ﬁed into\n•ones caused by mismatching performance notes to\nguide notes (see Figure 4)\n•ones splitting the notes’ intended simultaneous on-\nset timings (see Figure 5)\n3The answer data don’t contain grace notes.\n4Except for one piece whose recognition rate was the worst in the\ndataset, the average recognition rate reached 95.5%&\n?#\n#C\nCœœœœœ\nœœ˙˙˙œœœœ\nœœœœœœœ\nœœœœœœœœ\nœœœœœœœœ\n&\n?#\n#C\nCœœœœœ\nœœ˙˙˙œœœ\n3œœœ\nœœœœœœ\n3≈≈œœœœœœœœ\n‰œœœœœœœcorrect\nincorrectsplit\nFigure 5 . Splitting the notes intended simultaneous onset\ntiming\nWe may resolve the former errors by giving guide notes\nwhose pitches are not identical when the same notes arerepeated. The latter error can be reduced in part by ad-\njusting the probabilities concerning the auto-regressions\nof the state in the HMM, to the style of the music to be\nanalyzed (see 5.2).\n6.2. Experiments on quantizing durations\nHere, we mention brie ﬂy the tendencies of durations in\nmusical scores:\n•The durations of performance notes do not always\nreﬂect the canonical note values described in scores.\nGranted that no expression mark is written in thescores, players can consciously or unconsciously\nchange canonical note values into various durations\nin their performances.\n•When the pedal is used to lengthen the note dura-\ntion, the observed durations, the intervals between\nonset timing and offset timings in MIDI data, tend\nto be shorter than intended in the score.\nThese facts make it very hard to obtain canonical note\nvalues from the durations of performance notes.\nAgainst such a background, we tried to quantize the du-\nrations of performance notes by which the canonical note\nvalues in the musical scores are the criterion for quantiza-\ntion. We used a method re ﬂecting the following custom of\nmusical notation:\n•In musical scores, the offset timing of a note tends\nto be described as the onset timing of subsequentnotes\n•In musical scores, the note durations tend to be longer\nthan the intervals between the onset and offset tim-\nings in MIDI data.\nAfter quantizing the onset timing by using the HMM-\nbased method, the durations were quantized according to\nthe following rules (see Figure 6):timeʏlenlen        :   :   ʐlenlen\nquantized onset timesobserved duration\nquantized duration\nthresholdthreshold\nFigure 6 . Quantizing durations\n•When the offset timing of a note is earlier than its\nnext onset timing, the offset timing is identi ﬁed with\nthe next onset timing.\n•When the offset timing of a note is later than its next\nonset timing, the offset timing is quantized accord-\ning to the threshold that divides the area whose endsare the nearest neighbour onset timings of the offset\ntiming.\nα\nlenandβlen(αlen<βlen) in Figure 6 were determined\nfrom pilot studies.\nLet the recognition rate Cor on,len be:\nCoron,len =N−n/prime\nsub\nN×100\nwhere N,n/prime\nsubare de ﬁned as follows:\n•N: the numbers of performance notes that exist in\nthe answer data5\n•n/prime\nsub: the number of notes in N for which the onset\ntiming or durations are not equal to the answer.\nThe experimental results are shown in Table 2. Each\nnumber in the table represents the average recognition ratesfor ten sample pieces, which are the same as used in the\nexperiment on quantizing onset timing. The results gotten\nby the proposed method re ﬂecting the custom in scoring\nare shown in Table 2 in the “Custom” column. For com-\nparison, the results of an experiment where the quantiza-\ntion resolution is ﬁxed to a sixteenth note are also shown.\nThe results show that the proposed algorithm is better\nthan a simple threshold-based quantization processing. Itseems reasonable to conclude that the proposed algorithm\nreﬂects the customary form of a score, to some extent.\nHowever, the obtained recognition rates themselves are\nnot satisfactory. The dif ﬁculties mentioned above, which\nmay be the cause of the errors, have not been fully exam-ined. We are going to discuss them again at the end of this\nsection.\n5The answer data don’t contain grace notes.Table 2 . Results of quantizing durations\nGuide\n start and end\n per 2 bars\n per 1 bar\n per downbeat\n per chord\nThreshold\n 28.2\n 41.5\n 44.1\n 61.0\n 68.8\nCustom\n 41.6\n 57.8\n 67.7\n 75.7\n 84.9\nTable 3 . The amount of time required to make guide data. productivity is given by the 2nd. column value divided by the\n1st. column value\nwhole notes\n 1 note per a chord\n productivity\nexpert of Finale\n 10min. 18sec.\n 2min. 16sec.\n 22.0%\nbeginner of Finale\n 1hour 9min. 49sec.\n 12min. 20sec.\n 17.7%\n6.3. Measurement of productivity\nThe primary goal of the system described so far is to re-\nduce the load when trying to obtain deviation data fromMIDI-formatted music. To assess the utility of the sys-\ntem, we compared the times for inputting the score data\nby using FINALE .\nTable 3 compares the times to input Gavotte composed\nby Chopin. The data show that we can save around 80%\nof the workload to input the score while obtaining a 99.8%accuracy (onset timing) deviation database. It thus seems\nthat the proposed method is effective, especially for those\nwho are interested in music expression abut are not accus-tomed to using notation software.\n6.4. Discussion\nLet us summarize the points that have been made in this\nsection:\n1. The proposed method exhibits a high capability in\nquantizing the onset timing.\n2. There is still room for improving the quantizing du-\nrations, ie., giving a note value to a note.\n3. The proposed method is an effective tool for making\na deviation database.\nAlthough we might conclude that the proposed method\nsatisﬁes the primary goal, some people who regard the\ncustomary form of a score as crucial to making the music\ndatabase may think the second point could be a fatal short-coming. We con ﬁrmed the effectiveness of the follow-\ning heuristics to solve this problem: Rests seldom appear\nat the process of phrase. Applying this heuristic limited\nthe quantizing duration to that of the human MIDI perfor-\nmances, When a pianist presses the pedal down beyond\na certain depth and if the note is a one whose position is\nfar from other notes, the note duration is interpreted to be\nlonger than the duration indicated in the MIDI data.\nNote values of a score do not always represent the acous-\ntically “correct” feature. The score’s note values are expe-\ndient in transcribing music. Quantizing duration seems to\nbe less important than quantization of onset timing froma cognitive viewpoint. A suggestion to cope with this sit-\nuation is to gather many performances and to make aver-\nage deviation data from their individual deviations. Theproposed method, at least, can extract the average of the\nperformances. It is also able to segregate the canonical\npart from the deviations of a performance, most of whichcontain mis-touches (extra notes or missing notes).\n7. DATABASE AND TOOLS\nThe database is to be released on the web [13].\n7.1. Database\nThe sources of the deviation data were pieces recorded in\nSMF which were played by pianists or MIDI ﬁles which\nwere in the public domain. The database contains about\n40 pieces in “Sonatinen Album” and 20 well-known clas-\nsical pieces, for example, “f¨ ur Elise”, “Turkish March”,\nand so on. One set of deviation data was made for each\nof the above pieces. Additionally, we prepared several de-\nviation data for Mozart’s “K.331” and Chopin’s “EtudeOp.10, No.3” and “Walzer Op.64, No.2”, for the studies\non performance rendering.\nAs for above pieces, not re ﬂecting the purpose to make\nefﬁcient deviation data, we also made deviation data whose\ndurations were accurately canonicalized to the durations\nrepresented in the scores by using guide scores containingall notes of the pieces.\n7.2. Tools\nWe here introduce some tools for utilizing the database.\n7.2.1. Sequencer\nflower\nflower is a sequencer to play deviation data. The user is\nable to assign the degree of deviation in the onset timing,duration and local tempi. For example, if all degrees are\n0%, the consequent performance will have no deviation.\nIn turn, if all of the degrees are assigned 100%, flower\nreproduces the performance as it is. That is, it enables\ncomparison between a mechanical performance and an ex-\npressive performance. Of course, any degree between 0%Figure 7 . K.331 Henle Edition\nFigure 8 . K.331 Peters Edition\nand 100% can be assigned. Moreover, flower can ac-\ncept the degrees more than 100% in order to emphasize\ndeviations.\nAlso, flower has functions as follows:\n•Play only the speci ﬁed voice\n•Play according to the default velocity\n7.2.2. Visualizing Tool nov\nnov provides a piano-roll-type representation of the de-\nviation data. Figure 7 and 8 indicate the deviations of “K.331” Henle edition and those of “K.331” Peters edition.\nnov enables easy visual comparison of several deviation\ndata.\n7.2.3. Interactive Performance System\niFP\nIt would be useful to exploit a deviation database for en-\ntertainment purposes rather than just research purposes.\nFigure 9 . A Conducting Interface using capacity trans-\nducer\nVariance of\nExpressionwithin a Beat\nTempoDynamics\nFigure 10 . Example of real-time visualization of a perfor-\nmance. K.331 played by S. Bunin.\niFP [15] is an interactive performance system that en-\nables one to play expressive music with a conducting in-\nterface as shown in Figure 9. iFP uses the deviation data\nas a performance template. By using the template as a\nskills-complement, a player can play music expressively\nover and under beat level. The scheduler of iFP allows\nthe player to mix her/his own intention and the expressive-\nness written in the deviation data. It also provides real-\ntime visualization (feedback) of expressiveness as shownin Figure 10.\n7.2.4. Converter\nnote2xml\nPresently, we are preparing note2xml which converts\nthe deviation data into MusicXML(4R) format. This for-\nmat, developed by Rencon WG, is based on Recordare\nMusicXML and has the purposes of distributing datasetsfor training and evaluating performance rendering systems\n[16]. MusicXML(4R) is able to preserve deviation data\nwritten in NOTE format.\n8. CONCLUSIONS\nThis paper proposed a method to extract performance de-\nviation data ef ﬁciently from MIDI piano performances,\nbased on DP Matching and an HMM. We gave a brief\noverview of the database and related tools. The exper-\nimental results showed that we can ef ﬁciently construct\ndatabases of performance deviations. We are going to ex-\npand the database by using the proposed method.\nDatabases of performance deviations should help to fos-\nter studies in music informatics and musicology. The data-\nbases could become indispensable, especially because they\ncan accelerate studies on performance rendering. Educa-\ntion and entertainment ﬁelds could also make good use of\nsuch databases.\nWe would like to improve the algorithm, so it can ex-\ntract deviations from acoustic sounds. We also would like\nto construct a computational tapping model, which maygive the rational position of tempo change terms beat-\ntime, as a future work.\n9. ACKNOWLEDGEMENT\nThe authors would like to thank Ms. Mitsuyo Hashida,\nand Mr. Keita Okudaira for their contributions to the study.Prof. Hiroshi Hoshina and Mr. Yoshihiro Takeuchi made\nvaluable comments, and they were very helpful. This re-\nsearch was support-ed by PRESTO, JST, JAPAN.\n10. REFERENCES\n[1] Goto, M., Hashiguchi, H., Nishimura, T.\nand Oka, R. “RWC Music Database: Mu-\nsic Genre Database and Musical Instrument\nSound Database”, Proceedings of the 4th In-\nternational Conference on Music Information\nRetrieval , pp.229-230, 2003.\n[2] Bell, B. “Raga: approches conceptuelles et\nexperimentales”, MIM-CRSM-Sud Musiques -\nCCSTI Marseille , 1988.\nhttp://www.lpl.univ-aix.fr/lpl/presentation/\npublications/docs/bel/raga.pdf\n[3] CCARH Homepage:\nhttp://www.ccarh.org/\n[4] Widmer, G., Dixon, S., Goebl, W., Pam-\npalk, E. and Tobudic, A. “In Research of the\nHorowitz Factor”, AI Magazine , FALL 2003,\npp.111-130, 2003.\n[5] Schloss, W. Andrew. On the Automatic Tran-\nscription of Percussive Music – from Acousti-\ncal Signal to High-Level Analysis. Ph.D. the-\nsis, CCRMA - Stanford University, 1985.[6] Desain, P. and Honing, H. “The Quantiza-\ntion of Musical Time: A Connectionnist Ap-\nproach”, MIT press, Computer Music Journal ,\nV ol. 13, No. 3, pp. 56–66, 1989.\n[7] Katayose, H., Imai, M. and Inokuchi, S.\n“Sentiment Extraction in Music”, Proceed-\nings of the International Conference on Pat-\ntern Recognition , pp.1083-1087, 1988.\n[8] Otsuki, T., Saito, N., Nakai, M., Shimodaira,\nH. and Sagayama, S. “Musical Rhythm\nRecognition Using Hidden Markov Model”,\nTransaction of Information Processing Societyof Japan , V ol.43, No.2, pp. 245–255, 2002 (in\nJapanese).\n[9] Hamanaka, M., Goto, M., Asoh, H. and Otsu,\nN. “Learning-Based Quantization: Estima-tion of onset timing in a Musical Score”,\nProceedings of the World Multiconference\non Systemics, Cybernetics and Informatics\n(SCI2001) , V ol.X, pp. 374–379, 2001.\n[10] Takeda, H., Nishimoto, T. and Sagayama, S.\n“Estimation of Tempo and Rhythm from MIDIPerformance Data based on Rhythm V ocabu-\nlary HMMs”, IPSJ SIGNotes 2004-MUS-54 ,\npp. 51–56, 2004 (in Japanese).\n[11] Katayose, H., Fukuoka, T., Takami, K. and\nInokuchi, S. “Expression extraction in vir-\ntuoso music performances”, Proceedings of\nthe Tenth International Conference on PatternRecognition , pp.780-784, 1989.\n[12] NOTE format Speci\nﬁcation:\nhttp://ist.ksc.kwansei.ac.jp/ katayose/Downlo\nad/Document/\n[13] Katayose Lab. Download Page:\nhttp://ist.ksc.kwansei.ac.jp/ katayose/Downlo\nad/frameset.html\n[14] Bellman, R. Dynamic Programming , Prince-\nton University Press, Princeton, 1957.\n[15] Katayose, H. and Okudaira, K. “Using an Ex-\npressive Performance Template in Music Con-\nducting Interface”, Proceeding of the New In-\nterfaces for Musical Expression (NIME04) ,\npp.124-129, 2004.\n[16] Hirata, K., Noike K. and Katayose, H. “Pro-\nposal for a Performance Data Format”, Work-\ning Notes of IJCAI-03 Workshop on methods\nfor automatic music performance and their ap-\nplications in a public rendering contest , 2003."
    },
    {
        "title": "Towards Automatic Identification Of Singing Language In Popular Music Recordings.",
        "author": [
            "Wei-Ho Tsai",
            "Hsin-Min Wang"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417511",
        "url": "https://doi.org/10.5281/zenodo.1417511",
        "ee": "https://zenodo.org/records/1417511/files/TsaiW04.pdf",
        "abstract": "The automatic analysis of singing from music is an important and challenging issue within the research target of content-based retrieval of music information. As part of this research target, this study presents a first attempt to automatically identify the language sung in a music recording. It is assumed that each language has its own set of constraints that specify which of the basic linguistic events present in a singing process are allowed to follow another. The acoustic structure of individual languages may, thus, be characterized by statistically modeling those constraints. To this end, the proposed method employs vector clustering to convert a singing signal from its spectrum-based feature representation into a sequence of smaller basic phonological units. The dynamic characteristics of the sequence are then analyzed by using bigram language models. Since the vector clustering is performed in an unsupervised manner, the resulting system does not use sophisticated linguistic knowledge and, thus, is easily portable to new language sets. In addition, to eliminate the interference of",
        "zenodo_id": 1417511,
        "dblp_key": "conf/ismir/TsaiW04",
        "keywords": [
            "automatic analysis",
            "music retrieval",
            "language identification",
            "singing process",
            "linguistic events",
            "basic phonological units",
            "vector clustering",
            "bigram language models",
            "unsupervised manner",
            "new language sets"
        ],
        "content": "Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted wit hout fee provided that \ncopies are not made or distributed for profit or commercial \nadvantage and that copies bear this notice and the full citation on \nthe first page. \n© 2004 Universitat Pompeu Fabra. \n TOWARDS AUTOMATIC IDENTIFICATION OF SINGING \nLANGUAGE IN POPULAR MUSIC RECORDINGS\nWei-Ho Tsai and Hsin-Min Wang \nInstitute of Information Science, Academia Sinica \nNankang, 115, Taipei, Taiw an, Republic of China \n{wesley,whm}@iis.sinica.edu.tw \nABSTRACT \nThe automatic analysis of singing from music is an \nimportant and challenging issue within the research target of content-based retrieval of music information. As part of this research target, this study presents a first attempt to automatically identify the language sung in a music recording. It is assu med that each language has its \nown set of constraints that specify which of the basic linguistic events present in a singing process are allowed to follow another. The acoustic structure of individual languages may, thus, be characterized by statistically modeling those constraints. To this end, the proposed method employs vector clustering to convert a singing signal from its spectrum-based feature representation into a sequence of smaller basic phonological units. The dynamic characteristics of the sequence are then analyzed by using bigram language models. Since the vector clustering is performed in an unsupervised manner, the resulting system does not use sophisticated linguistic knowledge and, thus, is easily portable to new language sets. In addition, to eliminate the interference of background music, we leverage the statistical estimation of a piece’s music background so that the vector clustering is relevant to the solo singing voices in the accompanied signals. \n1. INTRODUCTION \nRecent advances in digital signal processing tech-\nnologies, coupled with what are essentially unlimited data storage and transmission capabilities, have created an unprecedented growth of music material being produced, distributed, and made available universally. On the other hand, our ever-increasing appetite for music has provided a major impetus for the development of various new technologies. However, as the amount of music-related data and information continues to grow, finding the desired item from the innumerable options can, ironically, become more and more difficult. This problem has consequently motivated research into developing techniques for automatically extracting \ninformation from music. Specific topics such as melody spotting [1], instrument recognition [5], music score transcription [11], and genre classification [19], are being extensively studied within the overall context of content-based retrieval of music information. More recently, research in this area has made a foray into the problem of extracting singing information from music, such as lyric recognition [20] – decoding what is sung; and singer identification [10] – determining who is singing. In tandem with the above research, this study presents a first attempt to identify the singing language of a song. Specifically, it aims to determine which among a set of candidate languages is sung in a given music recording.  \nSinging Language IDentification (singing LID) is \nuseful for organizing multilingual music collections that are unlabeled or insufficiently labeled. For instance, a song titled in English, but not sung in English, is commonplace in popular music, and very often it is not easy to infer the language of a song simply from its title. In such a case, singing LID can be deployed to categorize music recordings by language, without needing to refer to the lyrics or other information attached textually to the recordings. This function could support preference-based searches for music and may also be useful for assisting other techniques for classifying music, such as genre classification. Singing LID can also be used to distinguish between songs that have the same tune, but different languages. Such a case exists commonly in cover versions of songs, in which a singer performs a song written or made famous by a different artist. Since popular songs are often translated from one language to another and the titles are changed accordingly, singing LID could aid a melody-based music retrieval system for better handling of multilingual music documents.  \nRelatedly, copious amounts of research have been \nperformed on spoken language identification (spoken LID) [12,16], which aims to identify the language being spoken from a sample of speech by an unknown speaker. \nSpurred by the market trend and the need to provide services to a wide public, spoken LID has been gaining in importance as a key step toward multilingual automatic systems such as multilingual speech recognition, information retrieval, and spoken language translation. Various methods [7,8,21] have been proposed in attempts to mimic the ability of humans to   \n \n distinguish between languages. From a linguistic \nstandpoint, spoken la nguages can be distinguished from \none another by the following traits. \n• Phonology. Phonetic inventories are different from \none language to another. Even when languages have nearly identical phones, the frequency of the occurrence of phones and the combinations of phones differ significantly across languages.  \n• Prosody. Significant differences exist in the duration \nof phones, speech rate and the intonation across different languages. \n• Vocabulary. Each language has its own word roots \nand lexicons, and the process of word formation is also different from one language to another.  \n• Grammar. The syntactic and semantic rules which \ngovern the concatenation of words into spoken utterances can vary greatly from language to language. \nAlthough humans identify the language of a speech \nutterance by using one or a multiple of the traits described above, spoken-LID research to date has not exhaustively exploited all of these traits. Instead, it has developed methods which are reliable, computationally efficient, and easily portable to new language sets. In particular, phonological and prosodic information are the most prevalent cues exploited for spoken LID, since they are easily extracted from the acoustic signal without requiring too much language-specific knowledge. More particularly, a very promising and feasible way for spoken LID [8,14] is the stochastic modeling of the so-called phonotactics , i.e., the dependencies inherent in the \nphonetic elements of utterances. A spoken-LID system based on phonotactics commonly consists of a phonetic element recognizer, followed by a set of n-gram -based \nlanguage models. There are also various modifications thereof [6,14,21]. Other combinations that use other language-discriminating information [2,4,7], and do not involve complex linguistic knowledge, are also being studied to improve spoken-LID performance. \nIntuitively, singing LID might be performed using \nthe methods for spoken LID. However, singing differs from speech in many ways, including various phonological modifications employed by singers, prosodic shaping to fit the overall melody, and the peculiar wordings used in lyrics. Moreover, interference caused by the background music is often inevitable in most popular songs. As a result, porting a well-developed technique of spoken LID to the singing LID may present its own set of problems. For example, in using phonotactic information for singing LID, it is rather difficult and cost prohibitive to build a phone recognizer capable of handling accompanied singing signals with satisfactory accuracy and reliability. In addition, existing spoken-LID methods based on prosodic information might fail in the singing-LID task, since the original prosodic structures of spoken language are largely submerged by the overall melody. Therefore, to better handle the singing-LID problem, this study attempts to develop a data-driven method for singing LID, which does not involve the cumbersome task of phone \nrecognition and can be robust against the interference of the background music. \nThe rest of this paper is organized as follows. The \noverview of the proposed method is introduced in Section 2. The details of the singing-LID components, including vocal/non-vocal segmentation, language characteristic modeling, and stochastic matching, are presented in Sections 3, 4, and 5, respectively. Finally, the experimental results are discussed in Section 6, and conclusions are drawn in Section 7. \n2. METHOD OVERVIEW \nA singing-LID system takes as input a test music \nrecording and produces as output the identity of the language sung in that music recording. Since the vast majority of music is a mixture of assorted sound sources, a prerequisite for designing a successful singing-LID system is to extract, model, and compare the characteristic features of language acoustics without interference from non-language features. Toward this end, a singing-LID process as shown in Figure 1 is proposed. It consists of two phases: training and testing.   \nTraining\nDataManual\nVocal/Non-vocal\nSegmentationLanguage\nCharacteristic\nModelingLanguage-specific\nModels\n(Training Phase)\n(Testing Phase)\nTest\nDataAutomatic\nVocal/Non-vocal\nSegmentationStochastic\nMatching\n& DecisionHypothesized\nLanguageVocal/Non-vocal\nCharacteristic\nModelingVocal &\nNon-vocal\nModels\n \nFigure 1.  Illustration of the singing-LID process. \n \nIn the training phase, a music database containing all \nthe languages of interest sung by plenty of singers must be acquired beforehand. The database is used to establish a characteristic representation of individual languages. Since singing language is irre levant to accompaniment, \nthe training procedure begins with a segmentation of each music recording into vocal and non-vocal regions, where a vocal region consists of concurrent singing and accompaniment, whereas non-vocal regions consist of accompaniment only. In our implementation, the vocal/non-vocal segmentation of the training data is performed manually. Then, the acoustic characteristics of the vocal and non-vocal regions are stochastically modeled in order to automate the segmentation procedure in the testing phase. On the other hand, a stochastic modeling technique is performed in an attempt   \n \n to extract the underlying characteristics of singing \nlanguage in the vocal segments by specifically suppressing the characteristics of the background. After that, each language is repres ented by a la nguage-specific \nparametric model.  \nDuring testing, the vocal and non-vocal segments of \nan unknown music recording are automatically located and marked as such. The vocal segments are then examined using each of the language-specifi c parametric \nmodels. Finally, the language of the model deemed best matching the observed vocal segments is taken as the language of that test recording.\n \n3. VOCAL/NON-VOCAL CHARACTERISTIC \nMODELING AND SEGMENTATION \nThe basic strategy applied here follows our previous \nwork [18], in which a stochastic classifier is constructed for distinguishing vocal from non-vocal regions. This classifier consists of a front-end signal processor that converts digital waveforms to spectrum-based feature vectors, e.g., cepstral coefficients, followed by a backend statistical processor that performs modeling and matching.  \nIn modeling the acoustic characteristics of the vocal \nand non-vocal classes, a set of Gaussian mixture models (GMMs) is used. For each of the languages of interest, a \nGMM is created using the feature vectors of the manually-segmented vocal parts of music data sung in \nthat language. Thus, L vocal GMMs Λ\n1, Λ2 ,…, ΛL are \nformed for L languages. On the other hand, a non-vocal \nGMM ΛN is created using the feature vectors of all the \nmanually-segmented non-vocal parts of music data. Parameters of the GMMs are initialized via k-means \nclustering and iteratively adjusted via expectation-maximization (EM) [3]. When an unknown music recording is present, the classifier takes as input the T-\nlength feature vectors  X = {x\n1, x2, ..., xT} extracted from \nthat recording, and produces as outputs the frame \nlikelihoods p(xt|ΛN) and  p(xt|Λl), 1 ≤ l ≤ L, 1 ≤ t ≤ T. \nSince singing tends to be continuous, classification can be made in a segment-by-segment manner. Specifically, a W-length segment is hypothesized as either vocal or non-\nvocal using \n,) | ( log )| ( log max\n1 11\nvocal-nonvocal\n∑ ∑\n=+\n=+≤≤Λ ⎟\n⎠⎞⎜\n⎝⎛Λ≤>W\niN i kWW\nil i kWLlp p x x   (1) \nwhere k is the segment index. \n4. LANGUAGE CHARACTERISTIC MODELING \nThis section presents a stochastic method for \nrepresenting the characteristics of singing languages. The method can be implemented without involving complicated linguistic rules and pre-prepared phonetic transcriptions. 4.1. Vector Tokenization Followed by Grammatical \nModeling \nOur basic idea is to explore the phonotactics-related \ninformation of individual languages by examining the statistical dependencies of sound events present in a singing signal. In contrast to the conventional phonotactic modeling approach, which relies on phone recognition as a front-end operation, we use an unsupervised classification method to derive the basic phonological units inherently in a singing process. This allows us to circumvent the cumbersome task of segmenting singing into linguistically meaningful elements \nGiven a set of training data consisting of spectrum-\nbased feature vectors computed from the manually-segmented vocal parts of music, the procedure for language characteristic modeling comprises two stages as shown in Figure 2. In the first stage, vector clustering is employed on all feature vectors pertaining to a particular language, and a language-specific code book, \nconsisting of several codewords for characterizing the individual clusters, is formed. Each of the feature vectors is then assigned a codeword index of its associated cluster. It is assumed that each of the clusters represents a certain vocal tract configuration corresponding to a fragment of a broad phonetic class, such as vowels, fricatives, or nasals. The concatenation of different codeword indices in a singing signal may follow some language-specific rules resembling phonotactics, and hence the characteristics of the singing languages may be extracted by analyzing the generated codeword index sequences.  \nTraining data (Feature Vectors)\nVector Clustering\nGrammatical ModelingCodebook\nBigram Language ModelSmoothing & Merging\nBasic Phonological Unit SequencesCodeword Index\nSequencesVector\nTokenization\n \nFigure 2 .  Language characteristic modeling. \n \nTo reflect the fact that a vocal tract configuration \ncannot change suddenly, the generated codeword index sequences are smoothed in the time domain. For smoothing, an index sequence is first divided into a series of consecutive, non-overlapping, fixed-length segments, and each segment is assigned the majority   \n \n index of its constituent vectors. After that, adjacent \nsegments are further merged as a homogeneous segment if they have the same codeword index. Each homogeneous segment is regarded as a basic phonological unit. Accordingly,  a vocal part of music is \ntokenized into a sequence of basic phonological units.  \nIn the second stage, a grammatical model is used to \ncharacterize the dynamics of the generated basic phonological unit sequences. There are many choices to do this. In our implementation, bigram language models [9] are used. Parameters of a bigram language model, consisting of interpolated bigram probabilities, are estimated using the relative frequency method: \n, ) 1( ) | (\n1 11∑ ∑= =− −+ == =K\nk ki\nK\nk ikij\nt tnn\nnni wj wp α α (2) \nwhere  w t and  w t-1 denote two successive basic \nphonological units, α is an interpolating factor subject to \n0 ≤ α ≤ 1, K is the codebook size, ni is the number of \nbasic phonological units assigned as codeword i, and nij \nis the number of two successive basic phonological units assigned as codewords i and j, respectively. Note that \nthe transition between two separate vocal regions in a music recording is not ta ken into account in the \ncomputation of bigram probabilities. In summary, a language-specific model consists of a code book and a \nbigram language model. \n4.2. Solo Voice Codebook Generation \nThe effectiveness of the language characteristic \nmodeling described above crucially depends on whether the vector tokenization truly relates to the notion of phonology. Since the vast majority of popular music contains background accompaniment during most or all vocal passages, directly using conventional vector clustering methods, such as k-means algorithm on the \naccompanied singing signals, may cause that the generated clusters are not only related to the vocal tract configurations, but also to the instrumental types. To alleviate this problem, we develop a codebook generation method for vector clustering based on an estimation of the stochastic characteristics of the underlying solo voices from accompanied singing signals.  \nLet X = {x\n1, x2,..., xT} denote all the feature \nvectors computed from the vocal regions of music \nrecordings. Due to the existence of accompaniment, X \ncan be considered as a mixture of a solo voice S = {s1, \ns2, ..., sT} and a background music B = {b1, b2, ..., bT}. \nMore specifically, S and B are added in the time domain \nor linear spectrum domain, but both of them are unobservable. Our aim is to create a codebook for representing the generic characteristics of the solo voice \nsignal S, such that the vector tokenization can be \nperformed on the basis of this codebook. Under the vector clustering framework, we assume that the solo voice signal and background music are, respectively, \ncharacterized by two independent codebooks C\ns = {cs,1, cs,2,…, cs,Ks} and Cb = {cb,1, cb,2,…, cb,Kb}, where  cs,i, 1 ≤ \ni ≤ Ks, and  cb,j, 1 ≤ j ≤ Kb, are the codewords. To better \nrepresent the acoustic feature space, each cluster is modeled by a Gaussian density function. Therefore, a codeword consists of a mean vector and a covariance \nmatrix, i.e., c\ns,i = {µs,i, Σs,i} and  cb,j = {µb,j, Σb,j}, where \nµs,i and µb,j are mean vectors, and Σs,i and Σb,j are \ncovariance matrices. The vector clustering can be \nformulated as a problem of how to best represent X by \nchoosing and combining the codewords from Cs and Cb. \nTo measure how well the vector clustering is performed, we compute the following conditional probability: \n{} ,   ),|( max ),|(\n1, , ,∏\n==T\ntjbis tjib s p p ccx CCX       (3) \nwhere p(xt |cs,i, cb,j) accounts for one of the possible \ncombination of the solo voice and background music \nwhich can form an instant accompanied voice xt. If the \naccompanied signal is formed from a generative function \nxt = f (st, bt), 1 ≤ t ≤ T, the probability p(xt | cs,i, cb,j) can \nbe computed by \n, ) , ;( ) ,;( ),|(\n),(, , , , , ,  ∫∫\n==\ntt tfjb jb is is jbis t dd p G G\nbs xbs b s ccx Σ Σ µ µ  \n       (4) \nwhere G(⋅) denotes a multi-variant Gaussian density \nfunction. In using such a measurement, vector clustering \nis considered as effective if the probability p(X|Cs,Cb) \ncan be as large as possible. \nIn most popular music, substantial similarities exist \nbetween the non-vocal regions and the accompaniment of the vocal regions. Therefore, although the background \nmusic  B is unobservable, its stochastic characteristics \nmay be approximated from the non-vocal regions. This assumption enables us to es timate the background music \ncodebook C\nb directly, using the k-means clustering \nalgorithm on the feature vectors from the non-vocal regions. Accordingly, from the available background \nmusic codebook C\nb and the observable accompanied \nvoice X, it is sufficient to derive the solo voice codebook \nCs via a maximum likelihood estimation as follows: \n).,|( maxargb s s p\nsCCX C\nC=∗        (5) \nEquation (5) can be solved using the EM algorithm, \nwhich starts with an initial codebook Cs and iteratively \nestimates a new codebook sCˆ such that p(X|b sCC,ˆ ) ≥ \np(X|Cs,Cb). It can be shown that the need of increasing \nthe probability p(X|b sCC,ˆ ) can be satisfied by \nmaximizing the auxiliary function \n,),|( log) , ( )ˆ (\n111, ,* *∑∑∑\n==== = =  ,T\ntK\niK\njjb is t s ssb\np jjii Q ccx δ CC  (6) \nwhere δ (⋅) denotes a Kronecker delta function, and  \n).,|( maxarg),(, ,\n,* *\njbis t\njip ji ccx =    (7)   \n \n Letting ∇Q )ˆ (s sCC , = 0 with respect to each parameter to \nbe re-estimated, we have \n{}\n,\n),|() , (,,| ),|() , (\nˆ\n11, ,* *11, , , ,* *\n,\n∑∑∑∑\n====\n= =⋅ = =\n=T\ntN\njjb is tT\ntN\njjb is t t jb is t\nis\npjjiiE pjjii\nccxccxs ccx\nδδ\nµ\n (8) \n{}\n,            ),|() , (,,| ),|() , (\nˆ\n, ,11, ,* *11, , , ,* *\n,\nisisT\ntJ\njjb is tT\ntJ\njjb is t tt jb is t\nis\npjjiiE pjjii\nµµ ′ −= =′ ⋅ = =\n=\n∑∑∑∑\n====\nccxccxss ccx\nδδ\nΣ\n(9) \nwhere the prime operator ( ′) denotes vector transpose, \nand E{⋅} denotes expectation. The details of the \nEquations (8) and (9) required for implementation can be found in [13,17,18]. Figure 3 summarizes the procedure for the solo voice codebook generation.  \nCodebook\nGenerationMusic Recordings\nVocal/Non-vocal\nSegmentationNon-vocal\nSignal  B\nVocal Signal X Background\nMusic\nCodebook Cb\nSolo Voice Codebook Cs( max p( B | Cb ) )\nCodebook\nGeneration\n( max p( X | Cs ,Cb ) )\n \nFigure 3 . Procedure for a solo voice codebook \ngeneration. \n5. STOCHASTIC MATCHING AND DECISION \nThis section is concerned with the testing phase of the \nproposed singing-LID system. As shown in Figure 4, a test music recording is first segmented into vocal and non-vocal regions, and the feature vectors from the non-\nvocal regions are used to form a codebook C\nb, which \nsimulates the characteristics of the background accom-paniment in the vocal regions. For each of the L \ncandidate languages, the associated solo voice code book  \nC\ns,l, 1 ≤ l ≤ L, along with the background music \ncodebook Cb, are used to tokenize the feature vectors of \nthe vocal regions { x1, x2,..., xT} into a codeword index \nsequence V (l) = {v1(l), v2(l),…, vT(l)}, where T is the total \nlength of the vocal regions, and vt(l), 1 ≤ t ≤ T, is \ndetermined by \n.  ),|( maxarg,)(\n,,)(\n⎥⎦⎤\n⎢⎣⎡=jbl\nis tjiil\nt xp v cc   (10) \nEach of the codeword index sequences V (l), 1 ≤ l ≤ L, is \nthen converted into a basic phonological unit sequence W (l)  = { w1(l), w2(l),…, )(\n)(l\nNlw} by smoothing and merging \nthe adjacent identical indices. \nFor each language l, the dynamics of the basic \nphonological unit sequence W(l) are examined using a \nbigram language model λ(l), in which a log-likelihood \nlog p(W(l)|λ(l)), that W(l) tests against λ(l), is computed \nusing \n.) | ( log1)| ( log)(\n1)(\n1)(\n)()( )(∑\n=− = λlN\ntl\ntl\nt ll lw wpNWp  (11) \nNote again that the transitions between vocal regions are \nnot taken into account when  computing Equation (11). \nAccording to the maximum likelihood decision rule, the identifier should decide in favor of a language satisfying \n).| ( log maxarg)( )( * l l\nlWp l λ =   (12) \n \nCodebook\nGenerationA Test Music Recording\nVocal/Non-vocal\nSegmentationNon-vocal\nRegions\nVocal Regions\nBackground\nMusic\nCodebook\nLanguage 1\nSolo Voice\nCodebook\nVector\nTokenizationLanguage 2\nSolo Voice\nCodebook\nVector\nTokenizationLanguage L\nSolo Voice\nCodebook\nVector\nTokenization\nBigram\nLikelihood\nComputationLanguage 1\nBigram\nModel\nBigram\nLikelihood\nComputationLanguage 2\nBigram\nModel\nBigram\nLikelihood\nComputationLanguage L\nBigram\nModel\nMaximum Likelihood Decision\nHypothesized LanguageBasic Phonological Unit Sequences\nLog-likelihoods\n \n Figure 4 . Procedure for hypothesizing the language of \nan unknown test music recording. \n6. EXPERIMENTS \nTo test the validity of the proposed singing-LID method, \ncomputer simulations must be conducted with music data covering various languages, music styles, singers, and so on. However, during the initial development stage, the performance of our singing-LID system was only   \n \n evaluated using the task of distinguishing between \nEnglish and Mandarin pop songs, due to the difficulty of collecting and annotating multilingual music data. In our experiments, emphasis was put on examining if the characteristics of individual languages can be extracted from the singing in a music recording. \n6.1 Music Database \nOur music database consisted of 224 tracks (112 per \nlanguage) from pop music CDs. The average length of \ntracks was around three minutes. All the tracks were manually labeled with the language identity and the vocal/non-vocal boundaries. Among the 224 tracks, there were 32 pairs of tracks involving cover/original versions of songs, each pair of which contained two same-tune songs, one in English and one in Mandarin. These 32 pairs of tracks, denoted as a subset DB-C, were used for evaluating the performance of the proposed singing-LID system. Genders of the singers in DB-C were almost balanced, and 15 out of 32 pairs of tracks were performed by 15 bilingual singers, i.e., each of the 15 singers performed two same-tune songs, one in English and one in Mandarin. As DB-C was composed, we attempted to avoid the bias arising from tunes, singers, or music styles which may affect the objectivity of assessing a singing-LID system. \nAside from DB-C, the remaining 160 tracks (80 per \nlanguage) in our database were mutually distinct in terms of tunes, lyrics, and singers. These 160 tracks were further divided into two subsets, respectively, denoted as DB-T and DB-R. The DB-T, containing 60 tracks per language, was used as training data for creating vocal/non-vocal models, language-specific code books, \nand bigram language models, while the DB-R, containing the rest 20 tracks per language, was used as another testing data besides DB-C. None of the singers in one of the three subsets appeared in another. All music data were down-sampled from the CD sampling rate of 44.1 kHz to 22.05 kHz, to exclude the high frequency components beyond the range of normal singing voices. Feature vectors, each consisting of 20 Mel-scale frequency cepstral coefficients, were computed using a 32-ms Hamming-windowed frame with 10-ms frame shifts. \n6.2 Experimental Results \nOur experiments began with an evaluation for the \nvocal/non-vocal segmentation of the music data in DB-C and DB-R. Segmentation performance was characterized by a frame-based accuracy computed as the percentage of correctly-hypothesized frames over the total number of test frames. In view of the limited precision with which the human ear detects vocal/non-vocal changes, all frames that occurred within 0.5 seconds of a perceived switch-point were ignored in the accuracy computation. Using 64 mixture components per GMM along with 60-frame analysis segments (empirically the most accurate configurations), the segmentation accuracies resulted on DB-R and DB-C were 78.1% and 79.8%, respectively. Then, the singing-LID performance was evaluated \nwith respect to different length of test recording. Each of the tracks in DB-R and DB-C was divided into several overlapping clips of T feature vectors. A 10-sec clip \ncorresponds to 1000 feature vectors, and the overlap of two consecutive clips was 500 feature vectors. Each clip was treated as a separate music recording. The singing-LID experiments were conducted in a clip-by-clip manner, and the singing-LID accuracy was computed as the percentage of correctly-identified clips over the total number of test clips. In the training phase, the number of codewords used in each language-specific solo voice codebook and the background music codebook were empirically determined to be 32 and 16, respectively. In the testing phase, an online-created background music codebook was empirically set to have 4 codewords, if the number of the non-vocal frames exceeds 200; otherwise, no background music codebook was used. The segment length for smoothing the generated codeword index sequences was empirically set to be 5, and the \ninterpolating factor \nα in Equation (2) was set to be 0.1. \nFor performance comparison, we also performed singing LID without using any background music codebook. \nFigure 5 shows the singing-LID results with respect \nto T = 3000 (30 sec), 6000 (60 sec), 9000 (90 sec), and \nentire track, in which the T-length clips that fully labeled \nas non-vocal were not used for testing. Here, the singing-LID performance achieved with the manual vocal/non-vocal segmentation may serve as an upper bound for that obtained using automatic segmentation. We can see that as expected, the accuracy gains as the clip length increases. It is also clear that the performance of the singing LID based on the usage of solo voice codebooks is noticeably better than that of the method without taking background music into account. Such a performance gap is particularly visible when the test music recordings are long, mainly because more information about the background music can be exploited for assisting the construction of a reliable solo voice codebook. Using the automatic vocal/non-vocal segmentation, the best singing-LID accuracies of 80.0% and 70.0% were achieved when testing the entire tracks in DB-R and DB-C, respectively. The results indicate that the task of identifying the languages of the songs made originally in another language is more difficult. \nTable 1 shows the confusion probability matrix from \nthe best results of the singing LID based on the automatic vocal/non-vocal segmentation. The rows of the matrix correspond to the ground-truth of the tracks while the columns indicate the hypotheses. It can be found that the majority of errors are misidentifications of English songs. We speculate that such errors might be attributed to the louder background music usually existing in English pop songs, compared to Mandarin music, which often mix vocals louder to ensure that Mandarin syllables can be heard and understood semantically with the lack of tone information. The lower vocal-to-background ratio may cause the English model to be relatively ill-generated, and therefore, to poorly match the associated test music   \n \n recording. Another reason for the bias towards Mandarin \nin identifying the tracks in DB-C is likely because a large proportion of the singers in DB-C are Chinese. The accents of those Chinese singers might be different significantly from those of the singers in DB-T, who are mainly American, and hence the resulting discrepancy in phonological realizations may also lead the English model to match the test music recording poorly. One way to solve such problems is to use a wider variety of music data for training language-specific models, but this is not yet investigated at the initial stage of this study.  \n  \n3000 6000 9000\nTest Recording Length (# frames)30.035.040.045.050.055.060.065.070.075.080.085.090.095.0Singing-LID Accuracy (%) Using Solo Voice Codebooks; Manual Seg.\nUsing Solo Voice Codebooks; Automatic Seg.\nWithout Background Music Codebooks; Manual Seg.\nWithout Background Music Codebooks; Automatic Seg.\nEntire\n \n(a) Experiments on DB-R \n \n3000 6000 9000\nTest Recording Length (# frames)30.035.040.045.050.055.060.065.070.075.080.0Singing-LID Accuracy (%) Using Solo Voice Codebooks; Manual Seg.\nUsing Solo Voice Codebooks; Automatic Seg.\nWithout Background Music Codebooks; Manual Seg.\nWithout Background Music Codebooks; Automatic Seg.\nEntire\n \n(b) Experiments on DB-C \nFigure 5 . Singing-LID results. The above experimental results indicate that though \nthe singing-ID performance achieved with our proposed method still leaves much room for improving, a successful automatic singing-LID system should be feasible based on some possible extensions of the framework developed in this study. \n \nHypothesized \nActual English Mandarin \nEnglish 0.75 0.25 \nMandarin 0.15 0.85 \n(a) Experiments on DB-R \n \nHypothesized \nActual English Mandarin \nEnglish 0.63 0.37 \nMandarin 0.22 0.78 \n(b) Experiments on DB-C \nTable 1.  Confusion probability matrix of the \ndiscrimination of Mandarin and English songs. \n7. CONCLUSIONS \nThis study has examined the feasibility of automatically \nidentifying the singing language in a popular music \nrecording. It has been shown that the acoustic characteristics of a language can be extracted from singing signals via grammatical modeling of the basic phonological unit sequences output from the vector tokenization of spectrum-based features. To eliminate the interference of background music, we have proposed a reliable codebook generation method for vector clustering based on an estimation of the solo voice characteristics.  \nThough this study showed that the language sung in \na music recording could be distinguished from one another, the proposed method and the conducted experiments can only be regarded as a very preliminary investigation in the singing-LID problem. To further explore this problem, the essential work is to scale up the music database, which covers a large number of languages, singers with a wider variety of accents, and rich music styles or genres. \n8. ACKNOWLEDGEMENT \nThis work was partially supported by National Science \nCouncil, Taiwan, under Grants NSC92-2422-H-001-093 and NSC93-2422-H-001-0004. \n9. REFERENCES \n[1] Akeroyd, M. A., Moore, B. C. J., and Moore, G. \nA. “Melody recognition using three types of   \n \n dichotic-pitch stimulus”, Journal of the \nAcoustical Society of America , 110(3): 1498-\n1504, 2001. \n[2] Cummins, F., Gers, F., and Schmidhuber, J. \n“Language identification from prosody without explicit features”, Proceedings of the European \nConference on Speech Communication and Technology , Budapest, Hungary, 1999. \n[3] Dempster, A., Laird, N., and Rubin, D. \n“Maximum likelihood from incomplete data via the EM algorithm”, Journal of the Royal \nStatistical Society , 39: 1-38, 1977. \n[4] DuPreez, J. A., and We ber, D. M. “Language \nidentification incorporating lexical information”, Proceedings of the International \nConference on Acoustics, Speech, and Signal Processing , Phoenix, USA, 1999. \n[5] Eronen, A. “Musical instrument recognition \nusing ICA-based transform of features and discriminatively trained HMMs,” Proceedings \nof the International Symposium on Signal Processing and Its Applications , Paris, France, \n2003. \n[6] Harbeck, S., and Ohler, U. “Multigrams for \nlanguage identification”, Proceedings of the \nEuropean Conference on Speech Communication and Technology , Budapest, \nHungary, 1999. \n[7] Hazen, T. J. and Zue, V. W. “Segment-based \nautomatic language identification”, Journal of \nthe Acoustical Society of America , 101(4): \n2323-2331, 1997. \n[8] House, A. S. and Neuburg, E. P. “Toward \nautomatic identification of the language of an utterance. I. Preliminary methodological considerations”, Journal of the Acoustical \nSociety of America , 62(3): 708-713, 1977. \n[9] Jelinek, F. “Self-organized language modeling \nfor speech recognition”, Readings in Speech \nRecognition , Palo Alto, CA: Morgan Kaufmann, \n1990, chap 8, pp. 450-506. \n[10] Kim, Y. E. and Whitman, B. “Singer \nidentification in popular music recordings using voice coding features”, Proceedings of the \nInternational Conference on Music Information Retrieval , Paris, France, 2002. \n[11] Medina, R. A., Smith, L. A., and Wagner, D. R. \n“Content-based indexing of musical scores”, Proceedings of the Join t Conference on Digital \nLibraries , Texas, USA, 2003. \n  [12] Muthusamy, Y. K., Barnard, E., and Cole, R. A. \n“Reviewing automatic language identification”, IEEE Signal Processing Magazine , 4: 33-41, \n1994. \n[13] Nadas, A., Nahamoo, D., and Picheny, M. A., \n“Speech recognition using noise-adaptive prototypes”, IEEE Transactions on Acoustics, \nSpeech, and Signal Processing , 37(10): 1495-\n1503, 1989. \n[14] Nakagawa, S., Ueda, Y., and Seino. “Speaker-\nindependent, text-independent language identification by HMM”, Proceedings of the \nInternational Conference on Spoken Language Processing , Alberta, Canada, 1992. \n[15] Navratil, J. and Zuhlke, W. “An efficient \nphonotactic-acoustic system for language identification”, Proceedings of the \nInternational Conference on Acoustics, Speech, and Signal Processing , Seattle, USA, 1998. \n[16] Navratil, J. “Spoken language recognition – A \nstep toward multilinguality in speech processing”, IEEE Transactions on Speech and \nAudio Processing , 9(6): 678-685, 2001. \n[17] Rose, R. C., Hofstetter, E. M., and Reynolds, D. \nA. “Integrated models of signal and background with application to speaker identification in noise”, IEEE Transactions on Speech and \nAudio Processing , 2(2): 245-257, 1994. \n[18] Tsai, W. H., Wang, H. M., Rodgers, D., Cheng, \nS. S., and Yu, H. M. “Blind clustering of popular music recordings based on singer voice characteristics”, Proceedings of the \nInternational Conference on Music Information Retrieval , Baltimore, USA, 2003. \n[19] Tzanetakis, G. and Cook, P. “Musical genre \nclassification of audio signals”, IEEE \nTransactions on Speech and Audio Processing , \n10(5): 293-302, 2002. \n[20] Wang, C. K., Lyu, R. Y., and Chiang, Y. C. \n“An automatic singing transcription system with multilingual singing lyric recognizer and robust melody tracker”, Proceedings of the \nEuropean Conference on Speech Communication and Technology , Geneva, \nSwitzerland, 2003. \n[21] Zissman, M. A. “Comparison of four \napproaches to automatic language identification of telephone speech”, IEEE Transactions on \nSpeech and Audio Processing , 4(1): 31-44, \n1995."
    },
    {
        "title": "A search method for notated polyphonic music with pitch and tempo fluctuations.",
        "author": [
            "Rainer Typke",
            "Frans Wiering",
            "Remco C. Veltkamp"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417947",
        "url": "https://doi.org/10.5281/zenodo.1417947",
        "ee": "https://zenodo.org/records/1417947/files/TypkeWV04.pdf",
        "abstract": "We compare two methods of measuring melodic sim- ilarity for symbolically represented polyphonic music. Both exploit advantages of transportation distances such as continuity and partial matching in the pitch dimension. By segmenting queries and database documents, one of them also offers partial matching in the time dimension. This method can find short queries in long database docu- ments and is more robust against pitch and tempo fluc- tuations in the queries or database documents than the method that uses transportation distances alone. We com- pare the use of transportation distances with and without segmentation for the RISM A/II collection and find that segmentation improves recall and precision. With every- thing else being equal, the segmented search found 80 out of 114 relevant documents, while the method relying solely on transportation distances found only 60.",
        "zenodo_id": 1417947,
        "dblp_key": "conf/ismir/TypkeWV04",
        "keywords": [
            "Melodic similarity",
            "Polyphonic music",
            "Transportation distances",
            "Pitch fluctuations",
            "Tempo fluctuations",
            "Partial matching",
            "Segmentation",
            "Music search",
            "Database documents",
            "RISM A/II collection"
        ],
        "content": "A SEARCH METHOD FOR NOTATED POLYPHONIC MUSIC WITH\nPITCH AND TEMPO FLUCTUATIONS\nRainer Typke, Frans Wiering, Remco C. Veltkamp\nUtrecht University\nInstitute of Information and Computing Sciences\nABSTRACT\nWe compare two methods of measuring melodic sim-\nilarity for symbolically represented polyphonic music.\nBoth exploit advantages of transportation distances such\nas continuity and partial matching in the pitch dimension.\nBy segmenting queries and database documents, one of\nthem also offers partial matching in the time dimension.\nThismethodcanﬁndshortqueriesinlongdatabasedocu-\nments and is more robust against pitch and tempo ﬂuc-\ntuations in the queries or database documents than the\nmethod that uses transportation distances alone. We com-\npare the use of transportation distances with and without\nsegmentation for the RISM A/II collection and ﬁnd that\nsegmentation improves recall and precision. With every-\nthing else being equal, the segmented search found 80\nout of 114 relevant documents, while the method relying\nsolely on transportation distances found only 60.\n1. INTRODUCTION\nOur goal is a search engine for notated polyphonic music\nthat would allow musicologists to search large databases\nof notated music, to trace musical themes as they spread\nfrom composer to composer and as they develop over the\ncourse of music history. Generally, once the “holy grail”\nofmusicinformationretrieval,automaticpolyphonictran-\nscription from audio, is achieved, there will be an in-\ncreased need for an efﬁcient and effective method for\nsearching notated music. Such a method should be able\nto deal with variations in tempo and pitch as they occur\nwith human performers. This would enable a search en-\ngine to deal with queries entered by humans or to search\ndatabases of transcribed performances by humans.\nRelated Work. Byrd and Crawford [ 2] provide an\noverviewofthechallengesofmusicinformationretrieval.\nThey discuss symbolic retrieval and audio retrieval, and\nthey show that polyphonic matching is challenging. Most\nmethods for comparing monophonic sequences of notes,\nforexamplestringmatching,cannotbeeasilymodiﬁedso\nthat they become also useful for polyphonic music.\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc \r2004 Universitat Pompeu Fabra.A natural way of searching polyphonic music for the\noccurrence of a polyphonic pattern is to view the sym-\nbolicallyrepresentedmusicassetsofnotes,characterized\nby onset time, pitch, and duration, and search for pieces\nthat are supersets of the query. This idea and some vari-\nations were explored by Michael Clausen et al. with the\nPROMS/notify system [ 3], [4] and Lemstr ¨om et al. with\nthe C-Brahms system [ 8], [9], [15]. For example, they\nlooked for supersets of (possibly fuzzy) queries, maxi-\nmized the overlap of set elements, or searched for occur-\nrences of monophonic patterns represented by a string in\npolyphonic pieces represented by some parallel strings.\nMost of these methods put some constraints on the data\nthatcanbesearched,suchastherequirementthatthemea-\nsure structure be known or the note durations and onset\ntimes be quantized.\nInourpreviouswork[ 13],wedescribehowtransporta-\ntion distances such as the Earth Mover’s Distance (EMD)\ncanbeusedformeasuringmelodicsimilarity. Transporta-\ntiondistanceshavetheadvantageofnotrequiringaquan-\ntization or the knowledge of the measure structure; how-\never, they work well only for comparing segments of mu-\nsic of corresponding length. Finding an occurrence of a\nshortqueryinalongpiececannotbedonewithtransporta-\ntion distances alone.\nOur Contribution. Our new, segmented search\nmethod, still uses the advantages of transportation dis-\ntances. It overcomes many restrictions of Clausen’s and\nLemstr¨om’s methods [ 14]. By segmenting the music be-\nfore applying a transportation distance, we are able to\nmatchpiecesofmusicofdifferinglength,andthesegmen-\ntation also makes our method robust against tempo and\npitchﬂuctuations. Ourcomparisonofthenewsegmenting\nmethodwiththeoldtransportationdistancemethodshows\nan improved performance for theRISM A/II collection.\nWe exploit the following advantages of transportation\ndistances:\n\u000fContinuity: If differences between queries and\ndatabase documents are small, transportation dis-\ntances deliver accordingly small values. When a\nqueryisdistorted,thereisnopointatwhichthedis-\ntance would suddenly becomelarger.\n\u000fSupportformanydistortions: Manykindsofdif-\nferences such as grace notes, differences in pitch,\nnote durations, and rhythm are taken into accountFigure 1. A polyphonic query for Bach’s Brandenburg concerto No. 5 (violin part plus the left hand of the cembalo).\nAlthough the rhythm ﬂuctuates, a segmented EMD search for this query brought up the monophonic incipit of the Bran-\ndenburg concerto shown in Figure 2as the ﬁrst match.\nby transportation distances without the need for\ntheir explicit anticipation.\n\u000fPartial Matching for any combination of poly-\nphonic and monophonic music: With some trans-\nportation distances, examples of which include the\nEMD, any combination of monophonic and poly-\nphonic music can be matched.\n\u000fFlexibility: Transportation distances can be ﬁne-\ntunedtogenresandhumanperceptionbymodifying\nthe weighting scheme and ground distance.\nWe would like to beneﬁt from these properties of trans-\nportationdistancesandimproveontheminrobustnessand\npartial matching in the time dimension. In particular, we\nwishtobeabletoﬁndoccurrencesofshortqueriesinlarge\npiecesofmusicandmakeourmethodrobustagainstpitch\nand tempo ﬂuctuations, like those in Figure 1, without re-\nquiring explicit tempo tracking. We ﬁnd that segmenting\nboth queries and database documents into short, overlap-\nping groups does indeed improve the results.\n\u000fRobustness against pitch and tempo ﬂuctua-\ntions:If queries are entered by humans, the pitch\nand/or tempo frequently ﬂuctuate. While such ﬂuc-\ntuations can greatly distort a query, they either do\nnot have a large impact on short segments, or only\non a few of them.\n\u000fPartial matching in the time dimension: Trans-\nportationdistancesdonotgivemeaningfulresultsif\nthe durations of the compared pieces of music vary\ntoo much. By matching segments of similar dura-\ntions,weovercomethisproblemandareabletoﬁnd\nshort queries in long pieces.\nNone of the previously known distance measures for no-\ntated music combine all of these properties, and most\nare discrete in some way. Our contribution is a continu-\nousdistancemeasurethatcombinesthedesiredproperties\nmentioned above.\n2. MEASURING MELODIC SIMILARITY WITH\nTRANSPORTATION DISTANCES\n2.1. Representing notes asweighted point set\nTo be able to use transportation distances, we represent\nnotated music as weighted point sets. Every note is rep-resented as one point whose coordinates are given by the\nonset time and pitch. The weight represents the note du-\nration. Depending on the information available, it is pos-\nsible to make the weights depend on other features, such\nastheinter-onsetintervals,metricstress,melodiccontour,\nposition within a measure, piece, or chord, accents, or a\ncombination of these and possibly other features. How-\never, for this paper, we only make the weights depend on\nnote durations.\nFigure2shows an example of notated music and its\nassociated point set. Rests are represented only implic-\nitly with the surrounding notes’ coordinates and weights.\nTherefore,thepointsetinFigure 2onlycontainsonepoint\nforeachnote,butnonethatwouldrepresenttherest. Note\nthat the horizontal distance between the last two notes is\ntwice that between any other pair of notes.\nAs pitch coordinates, we use Hewlett’s base-40 rep-\nresentation [ 7], a number-line representation that distin-\nguishes between notes with the same pitch but different\nnotation, such as an a ]versus a b [. For the time coordi-\nnates, we arbitrarily assign 1 to the duration of a quarter\nnote. Because of the transformations described in Sec-\ntion2.4(scaling and translation) which we apply to point\nsets before calculating a transportation distance, it does\nnotmatterwhichnumberisassociatedwiththedurationof\naquarternote,aslongastherangeofnumbersinthepitch\ndimensionissimilarenoughtothatinthetimedimension.\nThis is important because it affects the way notes in one\npoint set are matched with notes in the other. If the range\noftimecoordinatesistoosmallincomparisonwiththatof\nthepitchcoordinates,notestendtobematchedwithnotes\nthat occur much later or earlier, but have similar pitches.\n2.2. Transportation Distances\nWe work with two transportation distances, the Earth\nMover’s Distance and the Proportional Transportation\nDistance.\n2.2.1. The Earth Mover’s Distance\nThe Earth Mover’s Distance (EMD) measures the mini-\nmum amount of work needed to transform one weighted\npoint set into another by moving weight. Intuitively\nspeaking, a weighted point can be seen as an amount of\nearth or mass; alternatively it can be taken as an empty\nhole with a certain capacity. We can arbitrarily assignthe role of the supplier to one set and that of the re-\nceiver/demander to the other one, setting, in that way, the\ndirection of weight movement. The EMD then measures\ntheminimumamountofworkneededtoﬁlltheholeswith\nearth (measured in weight units multiplied with the cov-\nered ground distance). See Cohen’s Ph.D. thesis [ 5] for a\nmore detailed descriptionof the EMD.\nFigure 2. The beginning of the violin 1 part of Bach’s\nBrandenburg concerto No. 5, in common music nota-\ntion (top) and as a set of weighted points in the two-\ndimensional space of pitch and onset time (bottom).\nWeightshererepresentnotedurations. Inthisandallother\ndiagrams, the weight is shown as the surface covered by\nthedisksthatrepresentpoints. Hereweassignaweightof\n0.25 to a sixteenth note and 0.5 to an eighth note.\nDeﬁnition Let A = f a 1 ; a 2 ; ::; a m gbe a weighted\npoint set such that a i = f ( x i ; w i ) g ; i = 1 ; ::; m;where\nx i 2 Rk, with w i 2 R+[ f 0 gbeing its corresponding\nweight. Let W=Pm\nj =1w ibe the total weight of set A.\nThe EMD can be formulated as a linear programming\nproblem. Given two weighted point sets A; Band a\ngrounddistance d,wedenoteas f ijtheelementaryﬂowof\nweightfrom x ito y joverthedistance d ij. If W ; Uarethe\ntotal weights of A; Brespectively, the set of all possible\nﬂows F = [ f ij ]is deﬁned by the following constraints:\n1. f ij \u0015 0 ; i = 1 ; :::; m; j = 1 ; :::; n\n2.Pn\nj =1f ij \u0014 w i ; i = 1 ; :::; m\n3.Pm\ni =1f ij \u0014 u j ; j = 1 ; :::; n\n4.Pm\ni =1Pn\nj =1f ij = min ( W ; U )\nThese constraints say that each particular ﬂow is non-\nnegative, no point from the “supplier” set emits more\nweight than it has, and no point from the “receiver” gets\nmore weight than it needs. Finally, the total transported\nweightistheminimumofthetotalweightsofthetwosets.\nThe ﬂow of weight f ijover a distance d ijis penalized\nby its product with this distance. The sum of all these in-\ndividual products is the total cost for transforming Ainto\nB. The EMD( A, B) is deﬁned as the minimum total cost\nover F,normalizedbytheweightofthelighterset;aunit\nof cost or work corresponds to transporting one unit of\nweight over one unit of ground distance. That is:\nEMD( A; B ) =min F 2FPm\ni =1Pn\nj =1f ij d ij\nmin ( W ; U )\nSee Figure 3for an illustration of an optimal ﬂow and the\nmatching of notes.\nFigure 3. An example of a ﬂow. This illustrates the dis-\ntance calculation between the ﬁrst segments of the poly-\nphonic query shown in Figure 4and the monophonic\ndatabase entry shown in Figure 2using the EMD. The\npoint set representing the monophonic segment is shifted\nupwards in this picture to make the ﬂow more visible. Its\nsix points are actually aligned with the six correspond-\ning points of the polyphonic segment so that most of the\nﬂowcomponents(andallﬂowcomponentsinvolvinglarge\namounts of weight) have a grounddistance close to zero.\nProperties and Computation\nThemostimportantpropertiesoftheEMDcanbesum-\nmarized as follows:\n1.The EMD is a metric if the ground distance dis a\nmetric and if the EMD is applied on the space of\nequal total weight sets.\n2.It is continuous. In other words, arbitrarily small\nchangesinpositionand/orweightofexistingpoints\ncause only arbitrarily small changes in its value.\nMoreover,theadditionofapointwithanarbitrarily\nsmall weight leads to an arbitrarily small change in\nthe EMD’s value.\n3.It does not obey the positivity property if the sums\nof the weights of the two sets are not equal. In that\ncase, some of the weight of the heavier distribution\nremainsunmatched. Therefore,theEMDallowsfor\npartial matching. As a result, there are cases where\nit does not distinguish between two non-identical\nsets. This can be useful, for example when match-\ning a monophonic melody to a piece that contains\nthe same melody, but with anaccompaniment.\n4.Inthe caseofunequal totalweights,the trianglein-\nequality does not hold.\nThe triangle inequality is relevant for the indexing\nmethod described in Section 4.2.\nThe EMD can be computed efﬁciently by solving the\ncorresponding linear programming problem, for exam-\nple by using a streamlined version of the simplex algo-rithm for the transportation problem (Hillier and Lieber-\nman 1990). We used Rubner’s EMD function [ 12], which\nimplementsHillier’sandLieberman’salgorithm. Itispos-\nsible that the simplex algorithm performs an exponen-\ntial number of steps. One could use polynomial algo-\nrithmslikeaninteriorpointalgorithm,butinpracticethat\nwould outperform the simplex algorithm only for very\nlarge problem sizes. Since the transportation problem is\na special case of the minimum cost ﬂow problem in net-\nworks,apolynomialtimealgorithmforthatcouldbeused\nas well.\n2.2.2. The Proportional Transportation Distance\nGiannopoulos and Veltkamp [ 6] proposed a modiﬁcation\nof the EMD in order to get a similarity measure based\non weight transportation such that the surplus of weight\nbetweentwopointsetsistakenintoaccountandthetrian-\ngle inequality still holds, which is useful for the indexing\nmethod described in Section 4.2. They call this modiﬁed\nEMD the “Proportional Transportation Distance” (PTD)\nbecause any surplus or shortage of weight is removed in\na way that the proportions are preserved before the EMD\nis calculated. The PTD is calculated by ﬁrst dividing, for\nboth point sets, every point’s weight by its point set’s to-\ntalweight,andthencalculatingtheEMDfortheresulting\npoint sets.\nThe PTD is a pseudo-metric. In particular, it obeys\nthe triangle inequality. It still does not have the positivity\nproperty since the distance between positionally coincid-\ning sets with the same percentages of weights at the same\npositions is zero. However, this is the only case in which\nthe distance between two non-identical point sets is zero.\nThe PTD will distinguish between two sets which differ\ninonlyonepoint. IthasallotherpropertiesthattheEMD\nhas for equal total weight sets.\n2.3. Ground Distance\nForallresultsinthispaper,weusetheEuclideandistance\nas ground distance. Thus, the distance between two notes\nwith the coordinates ( t i ; p i) and ( t j ; p j) is\nd ij =q\n( t i \u0000 t j )2 + ( p i \u0000 p j )2 :\nA variation possibly interesting for polyphonic music\nwould be to make the distance in the pitch dimension de-\npendonharmonyinsteadofjustcalculatingthedifference\nbetween pitches.\n2.4. Transformationsusedforachievingtransposition\nand tempo invariance\n2.4.1. Transposition invariance\nIn order to achieve transposition invariance, we calculate\nthe minimum distance for a range of transpositions. Be-\ncause we store pitch as discrete values, there are only\nﬁnitely many transpositions with a constant upper bound\nthat we need to try.This translation in the pitch dimension such that the\ndistance is minimized does not invalidate the triangle in-\nequality.\n2.4.2. Tempo invariance\nOursegmentingmethod(seeSection 3)aimsatcuttingthe\nmusic into segments of corresponding duration. There-\nfore,wecantranslateandscaleallpointsetstoaconstant\nrange of time coordinates before comparing them by us-\ningatransportationdistance. Aftersegmentingmusic,the\ntime coordinate of the last note within a segment depends\non the tempo. By scaling segments so that the maximum\ntime coordinate is always the same constant number, we\neliminate this dependence ontempo.\nNote that this scaling does not invalidate the triangle\ninequality.\n3. SEGMENTING\nThe aims of segmenting are to improve partial match-\ning in the time dimension, to increase robustness against\npitch and/or tempo ﬂuctuations, and to ensure that the\ntransportationdistancesareapplied tocomparablegroups\nof notes. With “comparable”, we mean that the groups\nof notes should contain similar numbers of consecutive\nnotes, and not too many.\nWe are not necessarily concerned with segments that\nmake musical sense. For our experiments, we worked\nwith segment lengths in the range from 6 to 9 consecu-\ntive notes. Segments of this length are usually distinc-\ntiveenoughsothatwedidnotgettoomanymatchesfrom\npieces that are not really similar, but still short enough\nfor getting the desired robustness against tempo and pitch\nﬂuctuations.\nOur segmenting algorithm must fulﬁll certain condi-\ntionsforourmethodtoworkproperly. Wewouldliketobe\nabletoprocessmanuallyrecordedMIDIquerieswithfree,\npossibly ﬂuctuating tempo and unknown measure struc-\nture. Also, we want the segmenting results to be largely\nindependent of how many voices are present at the same\ntime. Therefore, we cannot just take a certain number of\nnotes and declare them a segment. Rather, we must look\nat a certain number of consecutive notes.\nWe work with overlapping segments to reduce the in-\nﬂuence of the position of a query within a piece, and we\ncreate multiple segments with different lengths, but the\nsame starting point, in order to be able to match single\nlong notes with correspondingmultiple shorter notes.\nFor our experiments, we segmented queries and\ndatabase documents as follows:\nFirst,wesetapointertotheonsettimeoftheﬁrstnote\nthat is to become part of the next segment. This is the\nbeginning time of a new segment.\nThen, we move the pointer to the next end of any note\nwhose onset time lies within the current new segment,\nthen to the next beginning of a note. We do this untilwe have the desired number of consecutive notes in the\nsegment.\nWe include all notes with an onset time within the\nclosed interval from the beginning of the segment to the\ncurrent pointer position in the next segment.\nFor example, segment number 1 in Figure 4is found\nlikethis: First,wemoveapointertotheonsettimeofnote\nnumber1,theﬁrstnotewewanttoincludeinthesegment.\nThen, we move the pointer to the end of the longest note\nin the beginning chord (the lowest note), because that is\nthe next ending of any note whose onset time lies in the\ncurrent segment. Now we move the pointer to the begin-\nning of note 2 since this is the next onset time after the\npointer. This way, we have included the whole chord at\nthebeginninginthenewsegment,butcountitasonlyone\nout of six steps. The next ﬁve steps work the same way\n(go to the next end of any note after the pointer, then to\nthe next onset time). As a result, we identify the ﬁrst seg-\nment as shown in Figure 4with its 9 notes as a segment\nwith 6 consecutive notes.\nFigure 4. The ﬁrst ten segments of the polyphonic query\nshown in Figure 1.\nAsillustratedinFigure 4,wegenerateoverlappingseg-\nmentsthatarethreenotesapart,andateverystartingpoint,\nwe create segments of length 6, 7, 8, and 9. All of those\narescaledtothesamedistancebetweentheonsettimesof\ntheﬁrstandlastnote,asdescribedinSection 2.4.2,before\ntransportationdistancesareapplied. Becauseeverynoteis\nthe last note of some segment, there are no leftover notes\nat the end that would not be part of any segment.\nInordertocorrectlyrecognizeconsecutivelegatonotes\nin MIDI queries as consecutive (for getting a legato ef-\nfect, the player releases piano keys only after the follow-\ning note has started), it was sufﬁcient to treat all notes as\nif they were only 80% of their length for the purpose of\nsegmenting.\nBy segmenting queries and database documents, we\nincrease the number of comparisons of point sets that\nare necessary for answering a query. On the other hand,the individual comparisons become simpler since smaller\npoint sets need to be compared, and the size of point sets\nis bounded. The number of comparisons grows linearly.\nIf we segment as illustrated in Figure 4, the number of\nsegments is always less than or equal to the number of\nconsecutive notes times 4/3 (every 3 notes, there are up\nto 4 beginnings of a segment). The actual search time\ngrows only logarithmically if one uses the vantage index-\ning method described in Section 4.2.\n4. SEARCHING\nOur database contains pre-calculated segments of all\npieces. To answer a query, we segment it, then for each\nquery segment, we search the set of all segments for the\nmost similar ones, and ﬁnally combine the results of the\nsegment searches.\nEachsegmentsearchyieldsalistofpiecesthatcontain\nat least one matching segment. The overall result should\nbe a list of pieces with many closely matching segments.\nForthis,weneedtocomputeadistanceforeachpiecethat\noccurs in at least one segment search result. To do this,\nwe ﬁrst determine the maximum distance M that occurs\nin anysegment search result. For each segmentsearch re-\nsult in which a piece P occurs, we add the distance of the\nhighestrankedsegmentofPtotheoverallscoreforP.For\neach segment search result in which P does not occur, we\ndo not know the distance of the corresponding segments\nbecauseitwashighenoughforthesegmentofPtonotoc-\ncurinthisresultlist. Therefore,itisatleastthemaximum\ndistanceinthisresultlist,butprobablyclearlyhigher. We\ngetgoodresultsifweaddtwiceMtotheoverallscorefor\nP in such cases. For each segment search result without a\nsegmentfromPthatisbothprecededandfollowedbyseg-\nmentsearchresultswithsegmentsfromP,weadd4times\nM to the overall score for P. If the query is really a subset\nof the database document P, there should not be a section\nwithin P that does not match, therefore there should be a\nhigherpenaltyformissingsegmentswithinthequerythan\nfor missing segments at the beginning or end of a query.\nThe resulting overall score is a distance measure. It\nis zero if for every segment of the query a matching seg-\nment with distance zero was found in the same database\ndocument P. The distance measure grows with the indi-\nvidual distances of segments and with the number of seg-\nments for which no matches were found. While the un-\nderlying transportation distance is symmetric, the result-\ning distance measure is not. The triangle inequality does\nnot hold, and it is not always positive for unequal pieces\nof music. Therefore, it is not ametric.\n4.1. Adjusting the search radius for different seg-\nments\nFor each segment, we perform an nnearest-neighbours\nsearch up to a given maximumsearch radius m.\nWhen using the vantage indexing method (see Section\n4.2), we cannot directly search for nnearest neighbours,but need to work with a search radius. This radius has to\nbe different for different segments if we want to retrieve\nsimilar numbers of neighbours. For typical musical pat-\nterns, like many repeated notes within one segment, there\ntend to be many more neighbours within a small radius\naround the segment than for very distinctive patterns of\nnotes.\nWe do not want to impose the task of selecting an ap-\npropriate search radius for each segment on the user, who\nshould not need to be aware of the segmenting in the ﬁrst\nplace. Our search engine, therefore, adjusts the search ra-\ndius during the search as follows: The search starts with\na given low initial value which is unlikely to be too large\nfor any segment. If during the search we ﬁnd more than\nnneighbours with distance zero, the segment is not dis-\ntinctive enough to be considered at all, and this segment\nsearch can be stopped immediately. There are segments\nthatdonotcontainenoughcharacteristicmusicalmaterial\nfor being helpful. If at the end of the search, not enough\nmatches (less than the nnearest neighbours we are look-\ning for) were found within the search radius, we increase\nthe radius and search again. In this case, it is sufﬁcient\nto search the area outside the original search radius, but\nwithin the new, enlarged one. We do this only while the\nsearch radius is less than the given maximum search ra-\ndius m.\n4.2. Nearest neighbour searches with the vantage in-\ndexing method\nSince it would be prohibitively time consuming to com-\npute a transportation distance to a query point set for all\npoint sets in the database, we use the vantage indexing\nmethod described by Vleugels and Veltkamp [ 16]. If the\ntriangle inequality holds for the transportation distance,\nthis method allows us to rule out almost all database\nobjects without having to calculate the time consuming\ntransportationdistance. Wecanruleoutallobjectswhose\ndistancetoanyofthevantageobjectsdiffersbymorethan\nour search radius from the distance of the query object to\nthe same vantage object.\nBefore searching, we pick some vantage objects, for\nexample vrandomlyselectedpointsetsthatarealreadyin\nthe database. Then, for each point set in the database, we\ncalculatethetransportationdistancetoeachofthevantage\nobjects.\nFor the search, we ﬁrst determine the distance of the\nquery object to each vantage object. If the query object\nis in the database, these distances are already calculated.\nOtherwise, we calculate them now. Then, we retrieve\nall database objects whose distance to the query object,\nmeasured with the L 1norm in the v-dimensional space\nof distances to vantage objects, is less than or equal to\nthe search radius. This can be done with an approximate\nnearest-neighbour search with O ( k log n )L 1norm cal-\nculations [ 1] plus kexpensive transportation distance cal-\nculations, where kis the number of reported point sets.\nIf one prefers an exact nearest neighbour search, one can\nquery a v-dimensional kd-tree using O ( n1 \u00001\nv + k )L 1norm calculations, which is more expensive for sensible\nnumbers of vantage objects (a larger vwill allow us to\nrule out more database objects).\nOnly for the objects that could not be ruled out based\nonthetriangleinequalitydowehavetocomputethetrans-\nportation distance. With our constant segment length, the\ncomplexity is O ( k ), where kis the number of reported\nobjects.\nIn practice, searching the RISM A/II collection usu-\nally takes a few minutes on a PC with 1 GB of main\nmemory. The most important factors determining how\nlong the search really takes are the number of segments\nin the query and whether previous queries were similar.\nIn that case, the contents of the cache containing part of\nthe database indices are useful, and the search takes only\na few seconds. The large impact of caching effects makes\nit seem that with enough main memory for holding the\ndatabase indices, response times of a few seconds would\nbepossibleforallqueries. Weusetwotables,onecontain-\ningthedistancestovantageobjectsforeverysegmentand\none containing the weighted point sets for all segments.\nTogether, these tables including the MySQL indices take\nup about 1.7 GB of space, so with about 2 GB of main\nmemory,therewouldbeagoodchanceofattainingsearch\ntimes of a few seconds.\nBy using the vantage indexing method, we do not\nchange the search result, we just calculate it faster. When\nworkingwithatransportationdistanceforwhichthetrian-\ngle inequality holds, e. g. the PTD, calculating the trans-\nportation distances only for the candidates with similar\ndistances to vantage objects yields the same result as an\nexhaustive database search. One might argue that human\nsimilarity measures are not even symmetric and usually\nalso do not obey the triangle inequality, therefore using\na method that relies on the triangle inequality for index-\ning seems suspect. However, our distance measure as de-\nscribedinthissectiondoesnotobeythetriangleinequality\nalthoughitisbasedonthePTD.Itperformswellinexper-\niments where human experts judge its results [ 14]. We do\nnot reduce the quality of our results by exploiting the fact\nthat for the underlying transportation distance, the PTD,\nthe triangle inequality holds.\nFortheEMD,notevenaweaktriangleinequalitysuch\nasEMD(A,B) \u0014 k(EMD(A,C)+EMD(C,B))holds(with\nk \u0015 0). Counterexamples exist where EMD(A,B) >0,\nEMD(A,C)=0, and EMD(B,C)=0; see Figure 5.\nHowever, our experiments show that with the RISM\nA/II[11] collection, when using the vantage indexing\nmethod with the EMD, usually all matches within a\nthird of the search radius are retrieved. Hence if the\nsearch radius is increased accordingly, the vantage index-\ning method can still be used for polyphonic searches with\ntheEMD,albeitwithoutaguaranteeforthecompleteness\nof the matches.Figure 5 . For the EMD, not even the weak triangle\ninequality holds. In this example, EMD(A,B) > k\n(EMD(A,C) + EMD(C,B)) for all k \u0015 0.\n5. COMPARISON\nTo see how well the segmented search method works, we\nmanually entered rhythmically distorted queries using a\nMIDI keyboard. For example, a segmented EMD search\nﬁndsthemonophonicincipitshowninFigure 2astheﬁrst\nmatch for the query shown in Figure 1when we search\ntheRISMA/IIcollectionwithabouthalfamillionofmu-\nsical incipits (incipits are the beginnings of pieces, typi-\ncally about 20 notes long).\nWe also compared the original method described by\nTypke et al. in [ 13] to our improved method using the\nRISM A/II collection. To avoid any bias, we randomly\nselected 16 incipits out of the database as queries. For\neach of them, we used both methods for retrieving the 25\nmost similar incipits. As transportation distance, we used\nthe PTD since the collection is mostly monophonic, and\nthe EMD’s partial matching in the pitch dimension is not\nneeded. This gave us a total of 800 matches (with some\noverlap), for each of which we decided whether it was\nmelodically similar to the query and therefore relevant.\nFigure 6. Interpolated recall-precision averages. Since\neveryquerywascontainedinthedatabase,andbothmeth-\nodscorrectlyrecognizeidentity,thedifferenceissmallfor\nvery similar documents. The advantages of segmenting\nbecome apparent for documents that are less similar, but\nstill similar enough to be considered relevant.\nWe decided about the relevance in a way that mini-\nmized the inﬂuence of any bias towards one method. For\neach query, we created one combined result list that con-\ntained all documents which were returned by any of the\ntwo search methods. Those that were returned by bothwere listed only once. These lists were not sorted by\nmethod or by the ranks of documents, but by the library\nholding the source manuscripts. Therefore, for every rel-\nevancedecisionitwasveryhardtotellwhichmethodhad\nretrieved the document in question. The relevance deci-\nsions were taken by two people, each of whom covered\nhalf the queries. As M ¨ullensiefen et al. point out [ 10],\n“subjects with stable similarity judgements seem to have\nthesamenotionofmelodicsimilarity”. Thus,ahighnum-\nber of human experts making the similarity judgements is\nnot always necessary.\nSince for our comparison, we searched the RISM A/II\ncollection for pieces similar to queries taken from the\nsamecollection,therewerenopitchorrhythmdistortions.\nTherefore, this comparison does not show all strengths of\nthe segmented search method. But the segmented search\nstill performs better than thenon-segmented one.\nSee Figure 6for a recall-precision graph. For the pur-\nposeofthisgraph,weassumedthatallrelevantdocuments\nwereretrievedbyoneofthetwomethods. Amongalldoc-\numents that any method retrieved, 114 were judged to be\nrelevant. The comparison of whole incipits produced 60\nrelevantdocuments,whilethesegmentedsearchfound80.\nThe increased retrieval performance of the segmented\nsearch is largely due to the improved partial matching in\nthe time dimension. Figure 7shows an example where\nthis matters.\nFigure 7. Query (top): John Dowland, “If ﬂuds of tears\ncould clense my follies past”. A segmented search ﬁnds\nthematch(bottom)byJosephusFodor,aviolinduet. This\nmatchisnotfoundbythesearchmethodrelyingpurelyon\nthetransportationdistancebecausealthoughthemelodies\naresimilar,thedurationsoftheincipitsdonotcorrespond.\n6. CONCLUSIONS\nOur comparison of segmented and non-segmented\nsearches using the RISM A/II collection showed that the\nimproved partial matching in the time dimension, which\nis achieved bysegmenting, improvesprecision andrecall.\nWe have also used segmented searches with transporta-\ntion distances for matching polyphonic queries with ﬂuc-\ntuating tempo with similar monophonic incipits from the\nRISM A/II collection with constant tempo as illustrated\nwithFigures 1and2. Thismethodsupportsanycombina-\ntionofmonophonicandpolyphonicnotatedmusicwithor\nwithout pitch and tempo ﬂuctuations.\nThe same indexing methods can be used for searches\nwith or without segmenting. For segmented searches, in-\ndexing is very important since without it, the number of\ntransportation distance calculations gets unbearably high.\nFor example, with segment lengths of 6, 7, 8 and 9 and adistance of 3 notes between beginnings of segments, we\nneed about 4.5 million segments for covering approx. 0.5\nmillion incipits in the RISM A/II collection. A typical\nquery is cut into 20 segments. Without indexing, this\nwould mean almost 100 million transportation distance\ncalculations just for answering one query.\nPossible improvements\nThere are some ways in which our method could still be\nimproved:\nIn order to improve the ranking of the retrieved can-\ndidates for matches, we could add a second ranking step\nafter deciding which documents should be listed at all. In\nthis second step, we could also take those segments into\naccount that did not lead to the candidates’ inclusion be-\ncause they were either not similar enough to any segment\nin the query or not distinctive enough to be considered.\nAlso,wecouldworkwithaﬁneroverlap(startanewseg-\nment at every note instead of just every three notes) and\nmore segment lengths for the ﬁnal step, where the added\neffort would not be very noticeable.\nFigure3showsthattransportationdistancessometimes\nmatch notes with multiple other notes, some of which\ncan be quite far away. It is conceivable that a trans-\nportation distance that would only take the ﬂow compo-\nnent to the closest point in the receiving point set into\naccount would perform better. Such a transportation dis-\ntance,however,wouldintroducediscontinuitieswhenever\npoints are added or removed. It would still be continuous\nif only weights and positions of points are modiﬁed.\n7. REFERENCES\n[1]S. Arya, D. M. Mount, N. S. Netanyahu, R. Silver-\nman, and A. Wu (1994). An optimum algorithm for\napproximate nearest neighbor searching. Proceed-\ningsoftheFifthACM-SIAMSymposiumonDiscrete\nAlgorithms , pp. 573–582.\n[2]D.ByrdandT.Crawford(2002).Problemsofmusic\ninformation retrieval in the real world. Information\nProcessing and Management 38, pp. 249–272.\n[3]M. Clausen, R. Engelbrecht, D. Meyer, and J.\nSchmitz (2000). PROMS: A Web-based Tool for\nSearching in Polyphonic Music. Proceedings of the\n1st International Symposium on Music Information\nRetrieval (ISMIR 2000)\n[4]M. Clausen and F. Kurth (2003). A Uniﬁed Ap-\nproach to Content-Based and Fault-Tolerant Music\nRecognition. IEEE Transactions on Multimedia . To\nappear.\n[5]S. Cohen (1999). Finding Color and Shape Patterns\ninImages. Ph.D.thesis,StanfordUniversity,Depart-\nment of Computer Science.\n[6]P. Giannopoulos and R. C. Veltkamp (2002). A\nPseudo-Metric for Weighted Point Sets. In Heyden,A.,Sparr,G.,Nielsen,M.&Johansen,P.(Ed.), Pro-\nceedings of the 7th European Conference on Com-\nputer Vision (ECCV), pp. 715–730. Copenhagen,\nDenmark: Springer-Verlag.\n[7]W. B. Hewlett (1992). A Base-40 Number-\nline Representation of Musical Pitch Notation.\nMusikometrika , 4, 1–14. Retrieved April 1, 2003,\nfrom http://www.ccarh.org/publications/\nreprints/base40/\n[8]K. Lemstr ¨om, V. M ¨akinen, A. Pienim ¨aki, M. Trkia,\nand E. Ukkonen: The C-BRAHMS Project. Pro-\nceedings of the 4th Internationoal Conference on\nMusicInformationRetrieval(ISMIR2003) ,pp.237–\n238, Johns Hopkins University, Baltimore (MD),\nUSA, 2003.\n[9]K. Lemstr ¨om and J. Tarhio: Transposition Invari-\nantPatternMatchingforMulti-TrackStrings. Nordic\nJournal of Computing, 10 (3) 2003 (to appear). Re-\ntrieved from http://www.cs.helsinki.fi/group/\ncbrahms/publications/lemstrom_tarhio.pdf\n[10]D. M¨ullensiefen and K. Frieler: Measuring melodic\nsimilarity: Human vs. algorithmic Judgments. R.\nParncutt, A. Kessler & F. Zimmer (Eds.) Proceed-\ningsoftheConferenceonInterdisciplinaryMusicol-\nogy (CIM04) Graz/Austria, 15-18 April, 2004\n[11]R´epertoire International des Sources Musicales\n(RISM).SerieA/II,manuscritsmusicauxapr `es1600.\n(2002) K. G. Saur Verlag, M ¨unchen, Germany.\nhttp://rism.stub.uni-frankfurt.de\n[12]Rubner, Y. Source code for the Earth Mover’s\nDistance software . (1998). Retrieved April 1, 2003,\nfrom http://robotics.stanford.edu/˜rubner/\nemd/default.htm\n[13]R.Typke,P.Giannopoulos,R.C.Veltkamp,F.Wier-\ning and R. van Oostrum (2003). Using Transporta-\ntion Distances for Measuring Melodic Similarity.\nISMIR 2003: Proceedings of the Fourth Interna-\ntional Conference on Music Information Retrieval ,\npp. 107–114.\n[14]R. Typke, R. C. Veltkamp, F. Wiering (2004).\nSearching notated polyphonic music using trans-\nportation distances. Proceedings of the ACM Multi-\nmedia Conference 2004, New York.\n[15]E. Ukkonen, K. Lemstr ¨om, and V. M ¨akinen (2003).\nGeometric Algorithms for Transposition Invariant\nContent-Based Music Retrieval. ISMIR 2003: Pro-\nceedings of the Fourth International Conference on\nMusic Information Retrieval , pp. 193–199.\n[16]Vleugels, J. and Veltkamp, R. C. (2002) Efﬁcient\nImage Retrieval through Vantage Objects. Pattern\nRecognition , 35(1) pp. 69–80."
    },
    {
        "title": "Query-by-Beat-Boxing: Music Retrieval For The DJ.",
        "author": [
            "George Tzanetakis",
            "Ajay Kapur",
            "Manj Benning"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418033",
        "url": "https://doi.org/10.5281/zenodo.1418033",
        "ee": "https://zenodo.org/records/1418033/files/TzanetakisKB04.pdf",
        "abstract": "BeatBoxing is a type of vocal percussion, where musicians use their lips, cheeks, and throat to create different beats. It is commonly used by hiphop and rap artists. In this pa- per, we explore the use of BeatBoxing as a query mecha- nism for music information retrieval and more speci£cally the retrieval of drum loops. A classi£cation system that automatically identi£es the individual beat boxing sounds and can map them to corresponding drum sounds has been developed. In addition, the tempo of BeatBoxing is au- tomatically detected and used to dynamically browse a database of music. We also describe some experiments in extracting structural representations of rhythm and their use for style classi£cation of drum loops.",
        "zenodo_id": 1418033,
        "dblp_key": "conf/ismir/TzanetakisKB04",
        "keywords": [
            "BeatBoxing",
            "vocal percussion",
            "hiphop",
            "rap artists",
            "music information retrieval",
            "drum loops",
            "classification system",
            "individual beat boxing sounds",
            "corresponding drum sounds",
            "tempo detection"
        ],
        "content": "QUERY-BY-BEAT-BOXING:MUSICRETRIEV ALFORTHEDJ\nAjayKapur\nElectrical andComputer\nEngineering\nUniversityofVictoriaManjBenning\nElectrical andComputer\nEngineering\nUniversityofVictoriaGeorgeTzanetakis\ngtzan@cs.uvic.ca\nComputer Science\nUniversityofVIctoria\nABSTRACT\nBeatBoxing isatype ofvocalpercussion, where musicians\nusetheir lips, cheeks, andthroat tocreate different beats.\nItiscommonly used byhiphop andrapartists. Inthispa-\nper,weexplore theuseofBeatBoxing asaquery mecha-\nnism formusic information retrie valandmore speci£cally\ntheretrie valofdrum loops. Aclassi£cation system that\nautomatically identi£es theindividual beat boxing sounds\nandcanmap them tocorresponding drum sounds hasbeen\ndeveloped. Inaddition, thetempo ofBeatBoxing isau-\ntomatically detected andused todynamically browse a\ndatabase ofmusic. Wealsodescribe some experiments in\nextracting structural representations ofrhythm andtheir\nuseforstyle classi£cation ofdrum loops.\n1.INTRODUCTION\nDisc jockey(DJ) mixing, which £rstemer gedintheearly\n1950’ sinJamaica, isoneoftheearliest examples ofmu-\nsicinformation retrie val(MIR), where aDJretrie vespre-\nrecorded music from asetofrecords based onthemood\nandatmosphere ofanight club andaudience energy.Tra-\nditionally ,aDJuses asetofturntables inconjunction with\namixerto£lter appropriate music forthemoment. Inthis\npaper ,wepresent newtools forthemodern DJ,enabling\nthem toretrie vemusic with amicrophone byBeatBoxing .\nBeatBoxing isatype ofvocal percussion, where mu-\nsicians usetheir lips, cheeks, andthroat tocreate differ-\nentbeats. Itoriginated asanurban artform. Thehip-hop\nculture oftheearly 1980’ scould seldom afford beat ma-\nchines, samplers, orsound synthesizers. Without machine\nsupplied beats torapover,anewdrum wascreated -the\nmouth. Generally ,themusician isimitating thesound ofa\nrealdrumset orother percussion instrument, butthere are\nnolimits totheactual sounds thatcanbeproduced with\ntheir mouth. AsshowninFigure 1,themusician often\ncovershismouth with onehand tocreate louder ,deeper\nsounds. Awide variety ofsounds canbecreated with this\ntechnique enabling individualBeatBoxer stohavediffer-\nentrepertoires ofsounds.\nPermission tomakedigitalorhardcopiesofallorpartofthisworkfor\npersonalorclassroom useisgrantedwithoutfeeprovidedthatcopies\narenotmadeordistributedforpro£torcommercial advantageandthat\ncopiesbearthisnoticeandthefullcitationonthe£rstpage.\nc°2004UniversitatPompeuFabra.\nFigure1.BeatBoxing\nMost ofexisting workinMIR haseither concentrated\nonmelodic andpitch information insymbolic MIR orfo-\ncused ontimbral information inthecase ofaudio MIR.\nRhythmic information, although animportant aspect of\nmusic, isfrequently neglected. Inthispaper wefocus on\ntheretrie valandbrowsing ofelectronic dance music such\nasDrum &Bass, House, Rhythm &Blues etc.Webelie ve\nthese musical styles provide unique challenges andoppor -\ntunities because their rhythmic characteristics aremore\nimportant than their melodic andtimbral aspects. Inad-\ndition, theproposed techniques andapplications canbe\nused byexperienced retrie valusers thatareeager totry\nnewtechnologies, namely DJs. Furthermore, wewantour\ndeveloped MIR systems tobeused inactiveliveperfor -\nmance formixing andbrowsing inaddition tothetradi-\ntional query/search model.\nBased onthese observ ations, themain goal ofthiswork\nistoexplore theuseofBeatBoxing asaquery mechanism\nforboth retrie valandbrowsing. Thepaper isorganized as\nfollows:Insection 2webrie¤y describe existing digital\ntools fortheDJandrelated workinMIR. Insection 3we\ndescribe howtheindividualBeatBoxing sounds canbeau-\ntomatically identi£ed, ourmethod oftempo extraction and\nexplore theuseofstructured beat representations forstyle\nclassi£cation. Insection 4data collection andvarious ex-\nperiments inclassi£cation andretrie valaredescribed. In\nsection 5theimplementation ofthealgorithms andtwo\nnoveluser interf aces forbrowsing music andprocessing\nBeatBoxing sounds aredescribed. Finally ,insection 6\nwediscuss conclusions, challenges anddirections forfu-\ntureresearch.2.RELATEDWORK\nResearch inbuilding noveldigital systems forDJ’sisa\ngrowing area. There areanumber ofcommercial products\nsuch asFinalScratch1byStanton, which isaturntable\ncontroller thatuses special records tosend position sensor\ndata tothecomputer .Tracktor2byNativeInstruments is\napowerful softw arethatincludes graphical waveform dis-\nplays, tempo recognition, automatic synronization, real-\ntime time stretching, andtencuepoints forlivemixing of\nMP3, WAV,AIFF ,andaudio CDformats.\nAcademic research onbuilding tools fortheDJisalso\nbecoming more commonplace. AudioPad[1]andBlock\nJam[2]areboth performance tools forcontrolling play-\nback ofmusic onsample based sequencers. Mixxx [3]is\nsoftw areused both inrealistic performance setting andas\nameans tostudy DJinterf aceinteraction.\nAnother important areaofin¤uence isautomatic rhythm\nanalysis. Initial workinthisarea such as[4,5]concen-\ntrated ontheextraction oftempo butmore recent work\nhaslookedintoextracting more detailed information. The\nclassi£cation ofballroom dance music based onrhythmic\nfeatures isexplored in[6].Theextraction andsimilarity of\nrhythmic patterns independently oftheactual sounds used\ntoproduce them isexplored in[7]using aDynamic Pro-\ngramming approach. The classi£cation ofdifferent per-\ncussi vesounds using theZeroCrossing Rate isdescribed\nin[8].Theidea ofusing thevoice asaquery mechanism\nisexplored inthedifferent conte xtofIndian tabla music\nin[9].Finally ,ourapproach toQuery-by-Beat-Boxing al-\nthough based onrhythm rather than melodic information\nshares some similarities with query-by-humming systems\nsuch as[10,11].\nOntheapplication side, animportant in¤uence hasbeen\ntheidea ofamusic browsing space where thevisual infor -\nmation iscorrelated with music similarity andrelations.\nExamples include theexploration ofmusic collections by\nusing visual representations ofSelf-Or ganizing Maps [12],\nusing afastversion ofmultidimensional scaling (MDS)\ncalled FastMap in[13]andtheuseofdirect soni£cation\nintheSonic Browser [14].\n3.AUDIOANALYSISANDCLASSIFICA TION\nThe £rststep inQuery-by-BeatBoxing istoidentify the\nindividual vocal percussion sounds. This stage roughly\ncorresponds tothepitch detection-se gmentation stage in\nQuery-by-Humming. Audio drum loops aresigni£cantly\ndifferent forvocalBeatBoxing loops andtherefore require\ndifferent analysis methods. Because ourgoal istobeable\ntoretrie vefrom databases ofdrum loops, weneed tobe\nable toconvertaudio drum loops intosome representation\nthatcanbeused forsimilarity matching between those dif-\nferent types ofsignals.\n1http://www.finalscratch.com (April2004)\n2http://www.native- instruments.com (April2004)\nFigure2.Graphs showing time andfrequenc ydomain of\nvocal bass drum, snare drum andhigh hat\n3.1.BeatBoxing soundidenti£cation\nMost commonly BeatBoxing techniques include sounds\nwhich imitate areal drumset such asbass drum, snare\ndrum, andhigh-hat. However,advanced vocal percus-\nsion hasnolimits tothesounds thatcanbeproduced, in-\ncluding noises such assimulated turntable scratches and\nhumming-along thebeat. Inourexperiments, three gen-\neral types ofofbeat boxing sounds were analyzed and\nclassi£ed. The£rstisabass drum vocal hitthatischarac-\nterized bylowerfrequenc ycoming from thechest ofthe\nperformer .The second isasnare drum vocal hitthatis\ncreated bythequick pass ofairthrough theteeth. The\nthird isahigh-hat vocal hit,which ischaracterized bya\n’S’sibilance sound, created bythetongue arching upward\ntotheroof ofthemouth. Figure 2showsgraphs ofthe\ntime andfrequenc ydomain plots forthese three types of\nvocal hits.\nOne important observ ation isthatthespectral anddy-\nnamic characteristics ofthevocal drum sounds arenotdi-\nrectly similar tothecorresponding realdrum sounds so\nanaudio feature extraction andclassi£cation stage isre-\nquired toidentify thesounds. The produced vocal per-\ncussi vesounds haveshort duration (average 0.25 seconds)\nandtherefore asingle feature vector iscomputed forthe\nduration ofthesound.\nForthefeature extraction weexperimented with avari-\netyoffeature setsproposed intheliterature. Thefollow-\ningfeatures were considered:\n²TimeDomain features: ZeroCrossings, Root-Mean-\nSquarred Energy(RMS) andRamp Time\n²Spectral Domain features: Centroid, Rollof f,and\nFlux\n²Mel-Frequenc yCepstral Coef £cients (MFCC) [15]\n²Linear Predicti veCoef £cients (LPC) [16]−0.18 −0.16 −0.14 −0.12 −0.1 −0.08 −0.06−0.2−0.15−0.1−0.0500.050.10.15\nFirst Principal ComponentSecond Principal ComponentPrincipal Component Analysis Plot of MFCC feature\n−0.24 −0.22 −0.2 −0.18 −0.16 −0.14 −0.12 −0.1 −0.08 −0.06 −0.04−0.25−0.2−0.15−0.1−0.0500.050.10.150.2\nFirst Principal ComponentSecond Principal ComponentPrincipal Component Analysis Plot of LPC feature\nFigure3.Scatter plots forfeature analysis showing three\nclusters (bass (star), snare (circle), high hat(rectangle)\n²Wavelet-based features: Means andstandard devia-\ntions ofwaveletcoef£cients ineach subband [17]\nAnanalysis oftheclassi£cation ability ofeach feature\nsetwasperformed bytraining machine learning classi£ers\naswell asexamining scatter plots ofthecorresponding\ndata. The best single dimensional features were Zero-\nCrossing, Spectral Centroid andRollof f.LPC andMFCC\ncoef£cients performed better than thewavelet-based fea-\ntures. Figure 3showstwo-dimensional scatter plots ofthe\ntwohighest principal components oftheLPC andMFCC\nmulti-dimensional features. The three classes ofinterest\nareclearly separated visually .Classi£cation results are\nprovided insection 4.\n3.2.RhythmAnalysis\nAudio drum loops aresigni£cantly different from vocal\nBeatBoxing sounds. Although amethod based onindi-\nvidual percussion sound identi£cation such astheonede-\nscribed insubsection 3.1could also beutilized foraudio\ndrum loop analysis; ourinitial experiments inthatdirec-\ntionshowed thatthisisnotthecase.Themain reasons are:1)audio drum loops, unlik evo-\ncalpercussion, contain alargevariety ofdifferent sound\nsamples, and2)there issigni£cant overlap intime be-\ntween theindividual drum sounds. Therefore, adiffer-\nentapproach wasfollowed intheanalysis ofdrum loop\nsounds.\nEnvelope \nExtractionEnvelope \nExtractionEnvelope \nExtractionBEAT HISTOGRAM CALCULATION FLOW DIAGRAM\nFull Wave Rectification\nLow Pass Filtering\nDownsampling\nMean Removal\nAutocorrelation\nMultiple Peak Picking\nBeat HistogramDiscrete Wavelet Transform Octave Frequency Bands\nEnvelope Extraction\nFigure4.Beat Histogram Calculation Diagram\nFigure5.Beat Histogram\nInorder toanalyze thedrum loops, thesignal issepa-\nrated intodifferent frequenc ybands using aDiscrete Wavelet\nTransform (DWT). The envelope ofeach band iscalcu-\nlated using Full WaveRecti£cation, LowPassFiltering\nandNormalization. This front-end isbased onthemethod\nforthecalculation ofBeat Histograms described in[18].\nTheBeat Histogram (BH) showsthedistrib ution ofvari-\nousbeat periodicities ofthesignal. Forexample apiece\nwith tempo 60Beats-per -Minute (BPM) would exhibit BH\npeaks at60and120 BPM (quarter andeight notes re-\nspecti vely). Figure 4showsaschematic diagram ofthe\nthiscalculation. Figure 5showsaBHforapiece of\nRhythm andBlues music (notice thepeaks at96BPM\n(main tempo) and192BPM).Figure6.Original signal followed bylowand high\nwavelet bands showing theseparation ofbass drum\nsounds from high-hat sounds\nThemain peak oftheBH(subject tosome heuristics)\nisselected asthetempo ofthebeat boxing signal orthe\ndrum loop thatisprocessed. This automatically detected\ntempo information isused inMusescape forlivebrows-\ningofdrum loops anddance music asdescribed insec-\ntion 5.Inaddition, severalfeatures characterizing theBH\ncanbecomputed andused forsubsequent analysis such\nassimilarity retrie valandclassi£cation. TheBHfeatures\ndescribed in[18] areutilized inthispaper .\nInaddition tocalculating theBH, each subband ofthe\nDWT canbeprocessed separately toidentify thetracks\nofindividual drum sounds. Figure 6showsthree wave-\nforms displays ofaaudio drum loop. Thetopwaveform\nistheoriginal signal. The second waveform contains a\nlowfrequenc ysubband ofthewaveletdecomposition and\nthethird waveform contains ahigh frequenc ysubband. It\neasy tosee(and hear) thatthelowfrequenc yband contains\nmostly thebass drum sounds andthehigh frequenc yband\ncontains mostly thehigh-hat sounds. The advantages of\nusing thesubband approach fordetecting thedrum tracks\ninclude: handling ofsound overlap (ahigh-hat sound that\nisplayed atthesame time asthebass drum sound isstill\nidenti£ed) andthatnoclassi£cation model based onspe-\nci£c sounds isutilized.\nOneobvious question iswhether asimilar waveletanal-\nysiscould beapplied totheBeatBoxing signals. Indeed it\ncanbeused butthemain reason wechoose nottodosois\nthatthewaveletanalysis approach ismore computation-\nallyintensi veanddoesn’ tgiveanybetter results than the\nindividual sound identi£cation method. Inasimilar fash-\niontothequery-by-humming approach, theprocessing of\nthequery hastobefastbutthetargets (inthiscase the\ndrum loops) canbepreprocessed beforehand. Therefore\nquery processing time isanimportant concern buttarget\nprocessing time isnotasimportant.3.3.Structural Representations -Matching\nOnce thevocal percussion signals anddrum loops have\nbeen analyzed then wewould liketodevelop methods for\ncontent-based similarity retrie valandclassi£cation. Inor-\ndertoexperiment with various algorithms thefollowing\nthree tasks were chosen: 1)retrie valusing asquery adrum\nloop atadifferent tempo from theonecontained inthetar-\ngetdatabase, 2)retrie valusing asquery avocal rendition\nofaparticular drum loop from atargetdatabase ofau-\ndiodrum loops, and3)classi£cation ofdrum loops into4\nstyles (described insection 4).\nOur £rst attempt inthat direction wasusing features\ncomputed using theBHrepresentation proposed in[18].\nAlthough, thisapproach works formusic retrie valandmu-\nsical genre classi£cation, theresults were notparticularly\ngood forourtask. Webelie vethisisduetothefactthat\ndrum loop classi£cation requires more detailed informa-\ntionthan theBHs provide. BHs aregood attelling apart\nHipHop from Rock music butdon’tcontain thedetailed\ninformation required toidentify orclassify aparticular\ndrum pattern. Some results ofstyle classi£cation ofdrum\nloops using features based ontheBHarepresented insec-\ntion 4.The results aresigni£cantly better than random\nbutthere isroom forimpro vement.\nAnother approach thathasbeen proposed inthelitera-\nture[7]istheuseofdynamic programming totime-align\ntrajectories offeature vectors todetect similar drum pat-\nterns. Ourinitial experiments with thisapproach were not\nencouraging. Webelie vethemain reason isthatthespec-\ntralcharacteristics ofvocalpercussion sounds areverydif-\nferent from thecharacteristics ofactual drum loop sounds.\nInaddition, thisapproach suffersfrom thedrawback ofnot\ndirectly handling theoverlap ofpercussi vesounds.\nWearecurrently exploring theseparate extraction of\nfeatures oneach band forclassi£cation andsimilarity re-\ntrieval.Preliminary results areencouraging butafullscale\nevaluation hasn’ tyetbeen conducted.\n4.EXPERIMENTS\n4.1.Datacollection\nForBeatBoxing vocalhitidenti£cation, atotal of75sound-\n£les were recorded bytwodifferent “beatbox ers”: 25vo-\ncalbass drums, 25vocal snare drums and25vocal high\nhats. Forretrie valexperiments, wecreated adatabase of\n200 sound£les offour genres ofdance music, typically\nplayed byDJ’s:Drum &Bass (DnB), House, Rhythm &\nBlues (RnB), andReggae(Dub). These sound£les were\nobtained using pre-recorded loops fromDr.RexDrumSe-\nquencer inReason3.Each ofthe100loops chosen for\ntheexperiments hasadefaulttempo atwhich itisnormally\nplayed. Foreach ofthefour genre’ s,25samples ofloops\natthedefaulttempo were recorded, aswell as25samples\nofatime stretched orshrunk version at120BPM touse\nfortempo-in variant recognition analysis. These £leswere\nalsorecorded at44100 Hz.\n3http://www.propellerheads.se (March2004)zcr spc spr lpc mfcc\nVocal Bass Drum 100 100 92 100 88\nVocal Snare Drum 100 96 92 100 88\nVocal High Hat 92 88 96 88 92\nOverall 97.3 94.7 93.3 96 89.3\nTable1.Percentages ofclassi£cation accurac yforBeat-\nBoxing sounds (zcr=ZeroCrossing, spc,r =Spectral Cen-\ntroid,Rollof f)\nFurthermore, twoprofessional BeatBoxer sperformed\n12selected beats (3foreach genre). Twoversions for\neach beat were recorded: (1)listening with headphones\ntothecorresponding Dr.Rexloop atdefaulttempo and\nrecording theperformance, and(2)performing amemo-\nrized beat without anymetronome.\nAllthevoice recordings were recorded using anAKG\nC1000 microphone intoaProtoolsDIGI002sequencer at\nasampling rateof44100 Hz. All£les were normalized\nbefore analysis andexperimentation.\n4.2.Classi£cation\nTable 1showssome representati veclassi£cation percent-\nageaccurac yresults fortheidenti£cation ofindividual vo-\ncalBeatBoxing sounds. These results arecalculated using\nbackpropag ation Arti£cial Neural Netw ork(ANN) using\nleave-one-out cross-v alidation. The best single dimen-\nsional feature wasnumber ofZeroCrossings (zcr) andthe\nbestmulti-dimensional feature setwere theLinear Predic-\ntionCoef £cients (lpc). Thefactthatasingle feature isso\ngood atdiscriminating these sounds enables ef£cient real-\ntime implementation fortheapplications described insec-\ntion 5.Wealsoexperimented with avariety ofother fea-\ntures andparameters buttheresults arenotsigni£cantly\ndifferent.\nOne ofthecommon waystotesttheeffectiveness of\nafeature setfordescribing musical content isstyle/genre\nclassi£cation experiments. Although, ultimately ourgoal\nistohaveafeature representation thatisuseful fordrum\nloop retrie val,evaluating such afeature setdirectly re-\nquires extensi veuser studies toobtain relevance values.\nOntheother hand ground truth forstyle classi£cation (al-\nthough fuzzy evenforhumans) canbeobtained easily .To\nmakesure thattheresults arebased onbeat patterns rather\nthan tempo information allthedrum loops were generated\nat120beats-per -minute (bpm). Although thisconstraint\nprobably underestimates thetrue classi£cation accurac y\nastempo information canbeanimportant cue, wewanted\ntomakesure theresults were purely based onthedrum\npattern characteristics.\nTable 2showstheclassi£cation accurac ypercentage\nforstyle identi£cation using drum loops atthesame tempo.\nFourstyles were considered: Dub,Drum&Bass,House,\nandRhythm&Blues .Thefollowing classi£ers were com-\npared: aNaiveBayes classi£er (BAYES), abackpropag a-\ntion Arti£cial Neural Netw ork(ANN), aSupport VectorRND BAYES ANN SMO NN HUM\n4st 25 44 49 55 44 70\n3st 33 65 71 71 65 -\nTable2.Percentages ofstyle classi£cation accurac y\nfordrum loops (Dub, Drum &Bass, House, Rhythm &\nBlues), stisstyles\nDUB DNB HSE RNB\nDUB 21 2 2 0\nDNB 3 20 0 2\nHSE 7 5 13 0\nRNB 8 14 2 1\nTable3.Confusion matrix forSMO classi£er\nMachine (SMO) andanearest neighbor classi£er (NN).\nMore details about these classi£ers canbefound in[19,\n20].Alltheresults were calculated using 10-fold cross-\nvalidation toensure thattheaccurac yisnotin¤uenced by\nanyparticular partitioning ofthelabeled data intotraining\nandtesting.\nInorder toputthese results into conte xtaninformal\nuserstudy onstyle classi£cation wasconducted. Twosub-\njects listened torandomly chosen drum loops andhadto\nidentify thestyle. Both subjects were musically trained\nandonehadmore experience with dance music anddrum\nloops. Both subjects achie ved70% classi£cation accu-\nracy.Ascanbeseen theautomatic results aresigni£cantly\nbetter than random classi£cation butstillfallshort ofthe\nhuman classi£cation sothere isroom forimpro vement.\nItwasobserv edthatmost errors forboth human and\ncomputer were related toRhythm&Blues drum loops.\nThis canalso beobserv edintheconfusion matrix shown\nonTable 3.Thediagonal oftheconfusion matrix shows\nthecorrect style identi£cation. Forexample theinterpre-\ntation ofthe£rst rowisthat 21outof25Dub (DUB)\ndrum loops were correctly classi£ed, 2were misclassi-\n£edasDrum&Bass (DNB) and2were misclassi£ed as\nHouse (HSE). Therefore onTable 2wealso showthe\nresults ofremo vingRhythm&Blues (RNB) drum loops\nfrom thedataset (3st). Both theautomatic andinformal\nuser study results were done using drum loops atthesame\ntempo (120 BPM).\nTempo information turns outtobeanimportant prob-\nlemintheclassi£cation ofdrum loops. Ontheonehand,\nanalysis algorithms havetobetempo invariant, onthe\nother hand themain identifying characteristic ofcertain\nstyles istheir difference inaverage tempo. Forexample,\nDub drum loops arebelow100bpm whereas Dnb loops\narefaster (140-150 bpm). Webelie vethataddressing this\ntradeof fiscritical butwehaven’tyetfound asatisf actory\nwaydoso.Inorder tohavetempo invariance andalso\ninclude tempo information theonly waywehavetried is\ntoinclude thetempo inthefeature set.Unfortunately this\napproach doesn’ tworkaswell aswewould like.Another problem thatthedesigner ofaudio analysis al-\ngorithms forBeatBoxing anddrum loops hastodeal with\nisthedif£culty ofevaluation. Forexample inorder to\nevaluateBeatBoxing transcription ordrum loop analysis\nextensi veuser annotations need tobeprovided asground\ntruth. Insome cases these annotations canbeextremely\ntime consuming andtherefore itisfaster andmore useful\ntojustusetheearforqualitati veevaluations. Inthiswork\nwechoose acombination ofboth approaches: whene ver\nitwaspossible weconducted experiments andgenerated\nnumbers butinmanycases extensi veparameter tuning\nandinvestig ation ofdifferent features wasdone experi-\nmentally andsubjecti vely.\n5.IMPLEMENT ATION-APPLICA TIONS\nAlargevariety ofgreat softw aretools were used forthis\nwork. Thefeature extraction andclassi£cation were per-\nformed using Marsyas4afree softw areframe workfor\naudio analysis aswell asMatlab .TheAudacity5audio\neditor wasalso used. Forsome oftheclassi£cation ex-\nperiments theWeka[20]machine learning toolbox was\nutilized.\nInaddition, twoprototype applications were developed\ntodemonstrate thepotential ofQuery-by-BeatBoxing. The\nBionicBeatBoxing VoiceProcessor isthefront-end torecord-\ningandanalyzing vocal percussion. Theanalyzed signal\ncanthen beused toinitialize Musescape which isadirect\nsoni£cation toolforbrowsing music.\n5.1.BionicBeatBoxing VoiceProcessor\nTheBionicBeatboxVoiceProcessor (BBVP) isacus-\ntombuiltGUI interf aceinMATLAB (showninFigure 7)\nwhich allowsauser toBeatBox intoamicrophone anduse\ntheinterf acetotransform thevoiced beat into aprofes-\nsional high quality drum loop using existing prerecorded\naudio samples. The voiced beat isparsed into individ-\nualvocal hitsandcompared toauser-speci£c training set\nofdata. Each vocal burstisclassi£ed andtheappropriate\nrealdrum sound istransplanted intotheloop. Theuserhas\ntheability tomap anyvocal sound toanyWAV£lesam-\nplewhich enables avariety ofcreati vepossibilities. This\nwaywecanalternate betweenBeatBoxing anddrum loops\neasily .Inaddition, theinterf acecanbeused toevaluate\ntheperformance ofdifferent features forclassi£cation in\naqualitati verather than quantitati veway.\nWhen ’record’ isclick ed,thesoftw arestarts acquiring\ntheaudio input from thesoundcard. Thesampling rateof\nthedata acquisition is£xedat44100 Hz.Tohelp theuser\nstayintempo, aclick track canbegenerated.\nThe ’Process Beat’ button trigger thetransformation\nofthevoice input into arealdrum loop. First thetime-\ndomain signals areanalyzed to£ndthestart andendpoints\nofeach individualbeatbox sound burst. These points are\nused later todetermine where toplace thedrum samples.\n4http://marsyas.sourceforge.net\n5http://audacity.sourceforge.net\nFigure7.Bionic BeatBox Voice Processor Matlab GUI\ninterf ace\nThesound £leisde-noised andthresholding isused tolo-\ncate thevoice bursts. The threshold canbeadjusted us-\ningthe“Sensiti vity” slider toaccommodate differences\ninbackground noise and magnitude oftheBeatBoxing\nsounds.\nOnce thebeat isparsed intobursts, aclassi£cation al-\ngorithm isused inorder toidentify each type ofvocal hit.\nAback-propag ation neural netw orkbased onaZeroCross-\nings feature isused. Thechoice ofthisfeature wasbased\nontheexperiments described insection 4.Using asingle\nfeature allowsquick results forthisrealtime application.\nThe user must “train” theneural netwith 4sounds for\neach type ofvocal hit.Each sound must beperformed £ve\ntimes, creating thenecessary training data. After thevo-\ncalhitsareidenti£ed, appropriate mappings canbemade\nbased onselected sound £les containing individual drum\nsamples.\nAfter thebeat isprocessed, andtheappropriate identi-\n£edbeats aremapped accordingly ,thenewenhanced beat\nisready tobeplayed. Theuser hasadry/wet mixoption\ntohear theprocessed loop. Iftheslider isallthewaydry\nwhen the’Play’ button ispressed, only theoriginal voiced\nbeatbox will beheard. Iftheslider isallthewaywet,\nonly thetransformed beat will beplayed. The playback\ncanalso bein£nitely looped with the’loop’ button. The\nanalyzed information (tempo, features, individual drum\nsounds) canbesavedforlater usewith other applications,\nsuch asMuseScape andthetransformed query with the\n“real” drum sounds canbesavedasanewaudio £le.Figure8.Musescape drum loop browser\n5.2.Musescape\nMusescape isadirect soni£cation interf aceforbrowsing\nlargecollections ofmusic. Inmost existing retrie valsoft-\nwaretheusers £rstadjust theparameters oftheir query ,\nthen click a“Submit” button andaplaylist ofrelevantre-\nsults isreturned. Incontrast, themain idea inMusescape\nistoprovide continuous aural feedback thatcorresponds\ndirectly totheactions oftheuser (direct soni£cation). For\nexample, when theuser setsthetempo to120beats-per -\nminute (bpm) andselects theDub style there isimmediate\nfeedback about what these values represent byhearing a\ncorresponding drum loop. Sound isalwaysplaying andno\n“Submit/Search” button isused. Figure 8showsascreen-\nshot ofMusescape used forthebrowsing ofdrum loops\nandBeatBoxing loops. Amixing slider canbeused to\ncross-f adebetween different loops inasimilar fashion to\naDJmixing console. Theuser canrecord aBeatBoxing\nloop using theBionicBeatBoxing VoiceProcessor which\nissubsequently analyzed fortempo andstyle information\nasdescribed above.The extracted tempo/style informa-\ntion isthen utilized toinitialize Musescape toaparticu-\nlarregion ofthedrum loop collection. More information\naboutMusescape andanmusic browsing evaluation user\nstudy canbefound in[21]. Aposition paper arguing for\ntheuseofalternati veinterf aces tothetypical Query-by-\nExample paradigm forMIR is[22].\n6.DISCUSSION\nInthispaper ,thedesign anddevelopment ofaQuery-by-\nBeatBoxing system waspresented. More speci£cally we\ndescribe techniques forsolving thefollowing subtasks:\nvocal percussion sound identi£cation, drum loop analy-\nsisandstyle classi£cation. Experimental results showing\nthepotential oftheproposed algorithms areprovided. In\naddition, twouser interf aces forexperimentation andpro-\ntotyping were developed. TheBionicBeatBoxing Voice\nProcessor isused toanalyze vocal percussion signals and\nmap them toaudio drum sounds inorder tocreate drum\nloops. Itcanalso beused asafront-end toMusescape\nwhich isadirect soni£cation audio browsing environment.\nWebelie veourwork, demonstrates thegreat potential of\nusing rhythm andinparticular BeatBoxing formusic in-\nformation retrie val.DJsareaparticularly good targetuser group asthey\nareveryknowledgeable about music andareinterested in\ntheuseofnewtechnologies. Insome ways, evenbefore\nthiswork, theyareprime examples ofmusic information\nretrie valusers. Inouropinion, research inmusic informa-\ntion retrie valhasuntil recently emphasized melodic and\ntimbral aspects. Wehope thatthispaper willinspire more\nworkinexploring rhythm asaretrie valmechanism.\nThere arenumerous directions forfuture research. One\ndirection iscollecting more data from multipleBeatBox-\nersperforming more than the4styles weexplored. Such\nastudy would aidinvalidating ourexisting results. User\nstudies ofDJsusing thesystem inliveperformance situa-\ntions areplanned forthefuture. Theinitial response ofa\nfewDJswehaveshownthesystem hasbeen positi ve.\nWebelie vethatsimilar techniques canbeused forbeat\nretrie valofIndian music, especially tablatheka's(cycles)\n[9]andweareplanning toexplore thatdirection. Ingen-\neral, theuseofMIR techniques inliveperformance isof\nparticular interest. The development ofdomain speci£c\nquery methods andretrie valsystems isanother goal for\nthefuture ofMIR which until nowhasmainly concen-\ntrated onwestern artandpopular music.\nOne ofthemost unexplored andchallenging aspects of\nthisworkisthesimilarity ofbeat patterns byhumans. Al-\nthough wehavesome intuiti veunderstanding ofthepro-\ncess, more detailed experimentation with human subjects\nisrequired. The tradeof fofusing tempo information or\nnotisatypical example were ourknowledge ofhowhu-\nman perception works isincomplete. Musescape isaper-\nfect tool tocollect relevance andsimilarity information\njustbylogging user interactions with thesystem. Forex-\nample itiseasy toexplore howlong subjects remember a\nparticular rhythm andwhich rhythms aresimilar .\nAnother important direction istheexploration feature\nextraction based oneach seperate subband ofthewavelet\nanalysis. Webelie vethathigh levelstructural represen-\ntations ofrhythm patterns areessential forthistask and\nthere isalotoffuture worktobedone inthisarea. There\nisalargelegacyinrhythm analysis andrepresentations for\ntheanalysis ofsymbolic data [23]which wewould liketo\nconnect with automatic audio analysis method such asthe\nones described inthispaper .\nWehope, thatonedayMIR techniques willbeasindis-\npensable toDJsasrecords andturntables aretoday .\nAcknowledgments\nWewould liketothank Joel Fieber forhishelp inimple-\nmenting theBionicBeatBoxVoiceProcessor andAndre ye\nErmolinsk yiforhishelp withMusescape .Manythanks to\nAdam Tindale forhishelp inimplementing Neural Net-\nworks andfeature extraction inMatlab .Thanks toAndre w\nSchloss, Peter Driessen andWu-Sheng Lufortechnical\nsupport anddiscussions.7.REFERENCES\n[1]Patten J.,B.Recht, andH.Ishii, “Audiopad: Atag-\nbased interf aceformusical performance, ”Proc.New\nInterfaces forMusicalExpression(NIME) ,Dublin,\nIreland, 2002.\n[2]Newton-Dunn H.,H.Nakono, andJ.Gibson, “Block\njam,”(abstract)SIGGRAPH ,Tuscon, Arizona,\n2002.\n[3]Anderson T.H., “Mixxx: towards novelDJinter-\nfaces, ”Proc.NewInterfaces forMusicalExpression\n(NIME) ,Montreal, Canada, 2003.\n[4]Goto M.andY.Muraoka, “Real-time rhythm track-\ningfordrumless audio signals -chord change detec-\ntionformusical decisions, ”Proc.Int.Joint.Conf.in\nArti£cial Intelligence:WorkshoponComputational\nAuditorySceneAnalysis ,1997.\n[5]Scheirer E.,“Tempo andbeat analysis ofacoustic\nmusical signals, ”Journalofthe.Acoustical Society\nofAmerica ,vol.103, no.1,pp.588,601, Jan.1998.\n[6]Dixon S.,E.Pampalk, andG.Widmer ,“Classi£ca-\ntionofdance music byperiodicity patterns, ”Proc.\nInt.Conf.onMusicInformaiton Retrieval(ISMIR) ,\nBaltimore, USA, 2003, pp.159–167.\n[7]Paulus J.andA.Klapuri, “Measuring theSimilarity\nofRhythmic Patterns, ”Proc.Int.Conf.onMusic\nInformation Rertieval(ISMIR) ,Paris, France, 2002,\npp.150–157.\n[8]Gouyon F.,F.Pachet, andO.Delerue, “On theuse\nofzero-crossing rateforanapplication ofclassi£ca-\ntion ofpercussi vesounds, ”Proc.COST-G6 Conf.\nonDigitalAudioEffects(DAFX) ,Verona, Italy,Dec.\n2000.\n[9]Gillet O.andG.Richard, “Automatic labelling of\ntabla symbols, ”Proc.Int.Conf.onMusicInforma-\ntionRetrieval(ISMIR) ,Baltimore, USA, 2003, pp.\n117–125.\n[10] PawsS., “CubyHum: Afully operational QBH\nsystem, ”Proc.Int.Conf.onMusicInformation\nRertieval(ISMIR) ,Baltimore, USA, 2003, pp.180–\n197.\n[11] Dannenber gR.andetal., “The MUSAR Ttestbed\nforquery-by-humming evaluation, ”Proc.Int.Conf.\nonMusicInformation Rertieval(ISMIR) ,Baltimore,\nUSA, 2003, pp.41–51.\n[12] Rauber A.,E.Pampalk, and D.Merkl, “Using\nPsycho-Acoustic Models andSelf-Or ganizing Maps\ntoCreate aHierarchical Structure ofMusic bySound\nSimilarity ,”Proc.Int.Conf.MusicInformation Re-\ntrieval(ISMIR) ,Paris, France, Oct. 2002, pp.71–80.[13] Cano P.,M.Kaltenbrunner ,F.Gouyon, andE.Batlle,\n“On theuseofFastMap foraudio retrie valand\nbrowsing, ”Proc.Int.Conf.MusicInformation Re-\ntrieval(ISMIR) ,Paris, France, 2002, pp.275–276.\n[14] Fernstrom M.and E.Brazil, “Sonic Browsing:\nanauditory toolformultimedia asset management, ”\nProc.Int.Conf.onAuditoryDisplay(ICAD) ,Espoo,\nFinland, July 2001.\n[15] DavisS.and P.Mermelstein, “Experiments in\nsyllable-based recognition ofcontinuous speech, ”\nIEEETranscactions onAcoustics, SpeechandSig-\nnalProcessing ,vol.28,pp.357–366, Aug. 1980.\n[16] Makhoul J., “Linear prediction: Atutorial\novervie w,”Proceedings oftheIEEE ,vol.63,pp.\n561–580, Apr.1975.\n[17] Guohui L.andA.Khokar ,“Content-based indexing\nandretrie valofaudio datausing wavelets, ”Int.Conf.\nonMultimedia andExpo(II).IEEE, 2000, pp.885–\n888.\n[18] Tzanetakis G.andP.Cook, “Musical Genre Clas-\nsi£cation ofAudio Signals, ”IEEETransactions on\nSpeechandAudioProcessing ,vol.10,no.5,July\n2002.\n[19] Duda R.,P.Hart, andD.Stork,Patternclassi£ca-\ntion,John Wiley&Sons, NewYork,2000.\n[20] Witten I.HandE.Frank,DataMining: Practical\nmachinelearningtoolsandtechniqueswithJavaIm-\nplementations ,MorganKaufmann, 1999.\n[21] Tzanetakis G., “MUSESCAPE: Aninteracti ve\ncontent-a waremusic browser, ”Proc.Conferenceon\nDigitalAudioEffects(DAFX) ,London, UK, 2003.\n[22] Tzanetakis G.,A.Ermolinsk yi,andP.Cook, “Be-\nyond theQuery-by-Example Paradigm: NewQuery\nInterf aces forMusic Information Retrie val,”Proc.\nInt.Computer MusicConference(ICMC) .Gothen-\nburg,Sweden, Sept. 2002.\n[23] Honing H.,“From time totime: therepresentation\noftiming andtempo, ”Computer MusicJournal ,vol.\n35,no.3,pp.50–61, 2001."
    },
    {
        "title": "Digital Music Interaction Concepts: A User Study.",
        "author": [
            "Fabio Vignoli"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1414994",
        "url": "https://doi.org/10.5281/zenodo.1414994",
        "ee": "https://zenodo.org/records/1414994/files/Vignoli04.pdf",
        "abstract": "The popularity of digital music has recently rapidly increased. The widespread use on computers and portable players and its availability through the Internet have modified the interaction issues from availability towards choice. The user is confronted daily with an enormous amount of music. This situation shapes the need for the development of new user interfaces to access and retrieve music that takes full advantage of the music being digital. This paper reports the results of various user tests aimed at investigating how music listeners organize and access their digital music collection. The aim of the study is to investigate novel interaction concepts to access and retrieve music from large personal collections. The outcome of these studies was an interaction concept based on the notion of similarity of music items (artists and songs). This concept was further refined and developed into a demonstrator eventually tested with users.",
        "zenodo_id": 1414994,
        "dblp_key": "conf/ismir/Vignoli04",
        "keywords": [
            "digital music",
            "popularity",
            "availability",
            "Internet",
            "user interfaces",
            "choice",
            "music collection",
            "personal collections",
            "interaction concepts",
            "similarity of music items"
        ],
        "content": "DIGITAL MUSIC INTERACTION CONCEPTS: A USER \nSTUDY\n Fabio Vignoli  \n Philips Research Laboratories, \nProf. Holstlaan 4, 5656 AA, Eindhoven (NL) \nFabio.Vignoli@philips.com  \nABSTRACT \nThe popularity of digital music has recently rapidly \nincreased. The widespread use on computers and portable players and its availability through the Internet have modified the interaction issues from availability towards choice. The user is confronted daily with an enormous amount of music. This situation shapes the need for the development of new user interfaces to access and retrieve music that takes full advantage of the music being digital.  This paper reports the results of various user tests aimed at investigating how music listeners organize and access their digital music collection. The aim of the study is to investigate novel interaction concepts to access and retrieve music from large personal collections. The outcome of these studies was an interaction concept based on the notion of similarity of music items (artists and songs). This concept was further refined and developed into a demonstrator eventually tested with users. \n1. INTRODUCTION \nThe paper focuses on retrieval, navigation and \norganization of music from large personal user collections. While the focus in literature is on the very important aspect of playlist generation such as [1] or music recommendation, not many have described interactive user interfaces to navigate through music collections. An interesting attempt is found in [2] where features directly extracted from the audio are used to build a map of the music collection. This work, however, addresses collections of only few hundreds of songs (359) and is mainly concerned with PC based interfaces. Another music browser is described in [3], although it seems designed for professional use and not for non-professional music lovers.   The author does not know a deep study on this topic. Vaessens [4] studied how people express and describe their preferences for music they want to hear but it does not address navigation nor organization issues. Another study, reported in [5] focuses on music listening behavior for CD collections. The authors interviewed 6 music lovers who owned a large music collection (350 - 1400 albums). The main questions were: (i) How do users retrieve content out of their collection? (ii) What attributes do they use as cues?  \n/g38/g50/g49 /g38/g40 /g51 /g55/g38/g50/g49 /g38/g40/g51 /g55/g44/g81 /g87 /g72 /g85/g89 /g76/g72 /g90 /g86\n/g40 /g91 /g76 /g86/g87/g76 /g81/g74 /g3 /g86/g92/g86/g87/g72/g80 /g3\n/g68/g81 /g68/g79 /g92 /g86 /g76 /g86/g5 /g48 /g82/g82/g71/g3 /g47/g82/g74/g76 /g70 /g5\n/g40/g91/g83 /g72/g85 /g76 /g80 /g72 /g81/g87\n/g58 /g72/g69 /g3 /g69 /g68/g86 /g72 /g71/g3 /g84/g88 /g72/g86 /g87 /g76 /g82/g81 /g81/g68 /g76 /g85 /g72\n/g38/g50/g49 /g38/g40 /g51 /g55/g38/g50/g49 /g38/g40/g51 /g55/g44/g81 /g87 /g72 /g85/g89 /g76/g72 /g90 /g86\n/g40 /g91 /g76 /g86/g87/g76 /g81/g74 /g3 /g86/g92/g86/g87/g72/g80 /g3\n/g68/g81 /g68/g79 /g92 /g86 /g76 /g86/g5 /g48 /g82/g82/g71/g3 /g47/g82/g74/g76 /g70 /g5\n/g40/g91/g83 /g72/g85 /g76 /g80 /g72 /g81/g87\n/g58 /g72/g69 /g3 /g69 /g68/g86 /g72 /g71/g3 /g84/g88 /g72/g86 /g87 /g76 /g82/g81 /g81/g68 /g76 /g85 /g72\nFigure 1 : Scheme and flow of the user study: interviews \nand analysis of existing system plus a small field test lead to the hypothesis definition which were verified with a larger scale web questionnaire \nCommon ways of organizing a music collection were \nidentified, among them: alphabetical, non-alphabetical and hot rotation (of recent/favourite CDs). All of them have in common the physical format of CDs:  As a \nresult of the physicality of current collections, attributes are mainly visual and spatial attributes and the history enriched information that is added to the collection through the interaction of the user during use. [5].  Unfortunately these results can only partially be applied to digital music collections because these collections have a different physical format. Moreover just relying on the disk metaphor would not make the users aware (and the interaction richer) of the enormous potential offered by digital music.  In our study we are focusing on those characteristics or attributes of the songs that can be obtained (e.g. through content analysis or web-mining) when the music is stored in a digital format on DVDs or hard disks on PCs or CE devices. Some of these attributes (catalogue metadata), such as artist-name, album-name ,song-name \nare well known and widely used. Others, less common, \nare related to intrinsic characteristics of songs such as tempo, rhythm, and timbre. Others, such as listening \nfrequencies and preferences, are dependent on users behavior. The purpose of this study is to identify what are the most important attributes as well as the best way to combine them into a meaningful interaction concept to ease organization, navigation and browse through music collections. \nPermission to make digital or hard copies of all or part of this wo rk \nfor personal or classroom use is granted without fee provided that \ncopies are not made or distributed for profit or commercial \nadvantage and that copies bear this notice and the full citation on \nthe first page. \n© 2004 Universitat Pompeu Fabra.   \n \nThe paper is organized as follows: a first section \ndescribes the objectives and the logical schema of the study, the following sections describe in details the experiments and the results obtained. A section follows that wraps up the requirements for the design of the demonstrator. The last section presents some recommendations and the conclusions\n.\n2. OBJECTIVES OF THE STUDY \nThe main concern of this study is to investigate what the \nusers know, how they listen to and takes care of their music and how they organize and use their collections. This study is made up of smaller user studies logically connected as shown in Figure 1. In particular it consists of interviews with potential users, small field tests of existing software and a web-based questionnaire.  In order to get a clear and unambiguous understanding of the research goal and research questions some frequently used concepts will be defined in the following: \n•Digital music collection : a set of digital music files \nstored in different digital formats on hard disks or memory or optical disks, audio CDs  are not \nconsidered to be digital music collection in this context. \n•Popular music: a group of different music styles \nfound to be popular by people in one period of time (read today). \nThe research questions addressed in this study are the following:  \n•How do users organize their collections? \n•What are the most important attributes and what \nrole they play for organization and usage of a digital music collection?  \n•Are there different attributes for playlists creation, \nbrowsing and navigation? \n•What kinds of additional information do user \ndesires? \nThe study does not address the problem of distributed and/or not always on-line collections of music. The objective of the interviews was to collect qualitative information about users' behavior and needs. On the basis of these information and the results of the analysis of existing systems, a small field-test was designed to test how the availability of new features music influences the retrieval preferences of the users. These two steps lead to the formulation of some hypothesis about users' behavior and preferences. A web-based questionnaire was used to obtain quantitative data to verify these hypotheses. \n3. STUDY 1: INTERVIEWS \nThe semi structured interview technique was chosen as \nthe main tool to obtain a first set of answers. It allows not only answers to direct questions but also to go deeply inside the motivation. The interview consisted of forty questions divided into seven sections. The first section contains questions about the organization of the users personal collection. The second section contains questions about the perceived importance of some \nattributes for search, navigation and browsing. The other sections were dedicated to obtain information about the use of statistics, the process of retrieving songs, play list generation and browsing. Before the interviews, a pilot study with one participant was done. His comments were taken into account and the questionnaire was changed accordingly.   Given the scope of the study, collections of jazz and classical music are excluded. It is generally known that to organize these collections people use specific attributes that matter only for this specific music, but not for the rest. For example the composer of a piece of music is commonly used in jazz and classic music but not for popular music.  Seven subjects were interviewed. Six were males, one female. All subjects had high vocational education, six technical and one psychological. The age varied from 23 to 29 years. All of them were experienced users of PC and Internet and owned a large collection of digital music (> 1000 songs). Their collections contain mainly popular music, but there were also other genres. Before each interview the goal of the study was explained to the participant who agreed to have the interview recorded. Each session took approximately 40 minutes. \n3.1. Results of the interviews \nAll participants use hierarchical structure with folders \nand folders of folders (digital music is mostly played on PCs). Some participants create consistent structures with logical levels of hierarchy, but some have unstructured organization. Four participants out of seven use a hierarchical structure based on artist-name/album-\nname/song-name  with small variations, two based their \nclassification on genre-subgenre  and only one uses \npopularity for his collection (he created his collection on the basis of the top 2000 songs of one radio station). Almost all subjects use additional folders to store music that does not fit the structure of the collection. One participant uses language  as the highest level of the \nclassification. \nAll participants own a CD collection as well. The way \nthey organized their CD collection influences the organization of their digital music collections, basically they keep the same structure. For this reason the concept of album  is very strong. CD covers were described in [5] \nas the most powerful search cues for CD collections and despite the fact that in digital collections they are mostly not available, the participants expressed high interest. It was harder to find women that own large digital music collection than men. Whether there is any correlation between gender and willingness to collect music is unclear. Schuesslers study [6] showed that women were more interested in music than men, as well as having different tastes. Therefore according to this study gender is not the main issue. Perhaps other factors influence the situation indirectly, such as knowledge and experience of technology. It seems that women in general less likely have technical background and experience    \n \n/g51 /g68 /g85 /g68 /g80/g72 /g87/g72 /g85/g86 /g3/g73/g82 /g85 /g3/g85/g72 /g87/g85/g76 /g72 /g89 /g76 /g81 /g74\n/g19/g20/g21/g22/g23/g24/g25/g26/g27\n/g36/g85 /g87 /g76 /g86/g87 /g3 /g81/g68/g80/g72\n/g54/g82/g81/g74/g3 /g87 /g76 /g87 /g79 /g72\n/g41/g76 /g79 /g72/g3 /g81/g68/g80/g72\n/g42/g72/g81/g85 /g72\n/g36/g79 /g69/g88/g80/g3 /g81/g68/g80/g72\n/g60/g72/g68/g85\n/g47/g92/g85 /g76 /g70/g86\n/g48/g88/g86/g76 /g70/g76 /g68/g81/g86\n/g53/g72/g70/g82/g85 /g71/g3 /g79 /g68/g69/g72/g79\n/g47/g92/g85 /g76 /g70/g86/g3 /g90/g85 /g76 /g87 /g72/g85\n/g47/g76 /g89/g72/g3 /g85 /g72/g70/g82/g85 /g71/g76 /g81/g74\n/g47/g68/g81/g74/g88/g68/g74/g72\n/g53/g75/g92/g87 /g75/g80\n/g55/g72/g80/g83/g82\n/g48/g72/g79 /g82/g71/g92\n/g47/g82/g88/g71/g81/g72/g86/g86\n/g47/g72/g81/g74/g87 /g75\n/g51 /g68/g85/g68/g80/g72 /g87 /g72/g85/g86/g49/g88/g80/g69/g72/g85/g3/g82/g73/g3/g83/g68/g85/g87/g76/g70/g76/g83/g68/g81/g87/g86\n/g58 /g76 /g87/g75 /g82 /g88/g87/g3/g87/g76 /g83 /g3/g85 /g72 /g87/g85/g76 /g72 /g89 /g76 /g81/g74 /g58 /g76 /g87/g75 /g3/g87/g76 /g83 /g3/g85/g72 /g87/g85/g76 /g72 /g89 /g76 /g81 /g74\n \nFigure 2:  Frequency of the attributes to retrieve songs \nmentioned during the interviews. Left bar is without tip, right bar after the tip.  \nwith new technologies than men. For this reason \ntechnology could be the barrier because, the main source of digital music consists today of web download and sharing tools. The sizes the collections vary between 1200 and 3500 music files (average about 2300).  \nThe main findings are the following: All participants \norganize their collection and they do not use any specific tool for this purpose. Figure 2 shows the distribution among the participants of the attributes mentioned to retrieve music from their collection. During the interviews it became clear that the participants could not think of attributes other than the usual ones (catalogue metadata) so other attributes where mentioned and subjects were asked to agree or disagree (labeled tip retrieving in Figure 2). \n3.2. Additional remarks \nGenre as organization and retrieval method was \nmentioned quite frequently. In general music listeners feel comfortable to use genre  classification when \nsearching on web sites and in music stores. In music information retrieval (MIR) literature much emphasis is also given to genre classification methods based on feature extraction (see these paper for a deeper review [7;8]). However genre  appears not to be a consistent, \nnor an objective classification scheme, see also [9]. AllMusicGuide [10] has 531 genres and styles, Amazon [11] 719 genres, CDDB [12] has 255 genres and subgenres. From our analysis of the interviews it seems that Genre is more appropriate for classification of \nmusic not known to the users. In this case it gives information about the kind of music they could expect. For personal collections the subjects apparently do not need the genre  because the name of the artists convey \nmore information.  For browsing and selection the interviewd subjects prefer to use attributes such as mood ,situation  or \nactivity. Almost all of them mentioned situation as an important attribute: music for reading, programming, driving, working, morning music, evening music etc.  \nFigure 3: Frequency of declared usage of the features for \nthe MoodLogic group (selection of music by mood or tempo or year). \n4. STUDY 2: ANALYSIS OF EXISTING PRODUCTS \nAn analysis of currently available software players (a.o. \nWinamp, RealOne, Windows Media Player, MusicMatch, Apple iTunes, MoodLogic ) was carried on. There are two main browsing concepts adopted by the tested systems (i) view-based, (ii) association-based.  \nThe view-based concept is the most popular because it \nprovides a wide range of possibilities. It is based on the idea of selecting a particular set of songs according to one or more parameters. A user can define one parameter, for example artist-name and the system \nshows all songs of this artist. It is also possible to set more than one parameter to retrieve the desired music. Usually parameters such as artist-name, album-name,and release-year  are used.  \n The association-based  concept relies on the \" Give me \nsome music similar to this particular song(s) \" idea. For \nthe user this option is very convenient because it requires a low effort level. It is also a nice solution for playlist generation because users have difficulties to express their preferences in a formal way (e.g. by specifying attributes for the playlist) [4]. The drawback of this solution is that there is not much control on the output.   Most of the analyzed software products present the music collection in a hierarchical way based on the following structure: genre, artist, album, and song .\nSome products omit one level, such as MoodLogic, \nwhich omits the album  level. \n5.  STUDY 3: THE MOODLOGIC EXPERIMENT \nDuring the analysis of the existing systems it came out \nthe only software that enable new ways to access to music (thus not based on simple hierarchy) is Moodlogic . This software allows users to choose the \nmusic according to the mood ,t o  t h e  tempo  and to the \nyear-of-release of the songs. Mood classification is based on information obtained from a community of users Moodlogic  offers also the possibility to create one-  Frequency of mentioned retrieval method   \n \n-2-1.5-1-0.500.511.52\nPossibility to\nchoose music\naccording to your\nmoodPossibility to\nchoose music with\nparticular tempoPossibility to\nchoose music,\nwhich was\ncreated in certain\nperiod of timePossibility to\nchoose songs\nsimilar to the\nselected songPossibility to\nchoose songs of\nsimilar artist to\nthe selected artistMoodLogic Group Control Group\nFigure 4: Averaged results for the alternative selection \nmethods question for the moodlogic  and the control  \ngroups (scale from 2: dont like to +2: like a lot) \nclick playlists of songs similar to a seed song selected \nby the user. A specific field test was needed to investigate what users think about these new functionalities.   Twelve participants participated to this test: six of them in the MoodLogic  group, the others in the control \ngroup. The Moodlogic group  used the software for five \ndays. They had access to a collection of 6000 songs (not their personal music collection) of different genres. The participants were asked to try all features and then to use the product the way they like. The one-click playlist  \nfeature did not work properly so a demonstration was showed to the participants. At the end of the experiments the participants were interviewed and a filled out a questionnaire. The answers to this questionnaire are given on a scale from -2 to 2 where -2 is  I dont need \nit, and 2  I want to have it. \n5.1. Results of the MoodLogic Experiment \nFigure 3 shows the frequency of answers to the question \nabout how often the new MoodLogic features were used. It can be seen that the participants did not use these feature often. Figure 4 shows the average results to the question on what features the users like most, respectively for the MoodLogic group and for the \nControl group . There is some visible difference in the \nperception of some features (e.g. mood based selection) for the participants who did experience the software, with respect to the control group. A statistical analysis is not performed due to the limited number of participants. What is apparent from the results is that the both similarity features: music similar to the songs selected and music from similar artists score quite high with respect to the other features. \n6. STUDY 4: THE WEB QUESTIONNAIRE \nThe main goal of web-based questionnaire was to obtain \nquantitative data for the questions asked during the previous two user studies. The questionnaire consists of 14 questions that covered the following topics: personal information, collection and organization, most frequent  /g16 /g22 /g16/g21 /g16/g20 /g19 /g20 /g21 /g22/g44 /g3/g86 /g72 /g68 /g85 /g70/g75 /g3 /g73 /g82 /g85 /g3/g68 /g3/g86/g76/g81/g74 /g79/g72 /g3/g86/g82 /g81/g74/g44 /g3 /g75 /g68/g89 /g72/g3 /g68/g3 /g86 /g87 /g68/g81 /g71/g68/g85/g71/g3 /g83/g79 /g68/g92 /g79 /g76 /g86 /g87/g44 /g3/g79/g76/g86 /g87/g72 /g81 /g3/g87/g82 /g3/g72 /g91 /g76/g86/g87/g76/g81 /g74 /g3/g83 /g79/g68 /g92 /g79/g76/g86 /g87/g86/g44 /g3/g70 /g82 /g80 /g83 /g76/g79/g72 /g3/g68 /g3/g83 /g79/g68 /g92 /g79/g76/g86/g87/g3/g73 /g85 /g82 /g80 /g3/g71 /g76/g73 /g73 /g72 /g85 /g72 /g81 /g87/g3/g86/g82 /g81 /g74 /g86/g44 /g3/g70 /g75 /g82 /g82 /g86/g72 /g3 /g68 /g3/g86/g83 /g72 /g70 /g76/g73 /g76/g70/g3/g74 /g72 /g81/g85 /g72/g44 /g3/g70/g75 /g82 /g82 /g86/g72 /g3/g82 /g81 /g72 /g3/g82 /g85 /g3/g80 /g82 /g85 /g72 /g3/g68 /g85 /g87/g76/g86 /g87/g86/g44 /g3/g70/g75 /g82 /g82 /g86/g72 /g3/g82 /g81 /g72 /g3/g82 /g85 /g3/g80 /g82 /g85 /g72 /g3/g68 /g79/g69 /g88 /g80 /g86/g44 /g3/g86/g75 /g88/g73 /g73 /g79/g72 /g3/g87/g75 /g72 /g3 /g90 /g75 /g82 /g79/g72 /g3/g70 /g82 /g79/g79/g72 /g70/g87/g76/g82 /g81\nFigure 5: Averaged user's behavior (scale from 3: never  \nto +3 very often )\n/g16/g22 /g16/g21 /g16/g20 /g19 /g20 /g21 /g22/g54 /g82 /g81/g74/g3 /g47/g92 /g85/g76 /g70 /g86/g36 /g85 /g87/g76 /g86 /g87/g3/g9 /g3/g68 /g79 /g69 /g88 /g80 /g3/g68 /g85 /g87/g76 /g70/g79 /g72 /g86 /g3/g9 /g3/g69 /g82 /g82 /g78/g86/g36 /g85 /g87/g76 /g86/g87/g3/g83 /g76 /g70 /g87/g88 /g85 /g72/g36 /g85 /g87/g76 /g86/g87/g3/g9 /g3/g80/g88 /g86/g76 /g70/g3 /g90 /g72 /g69 /g16 /g79 /g76 /g81 /g78/g86/g55 /g57 /g3 /g83/g85/g82/g74/g85/g68 /g80 /g86/g38 /g82 /g81/g70 /g72/g85/g87 /g3 /g36 /g74/g72/g81 /g71/g68/g86/g36/g79 /g69 /g88 /g80/g3 /g70 /g82 /g89 /g72 /g85 /g86/g36 /g85/g87 /g76 /g86 /g87 /g3 /g39 /g76 /g86 /g70 /g82/g74/g85/g68 /g83/g75/g92/g36/g85 /g87/g76 /g86/g87/g3/g37/g76 /g82 /g74 /g85 /g68 /g83 /g75 /g92/g57 /g76 /g71/g72/g82/g3 /g70 /g79 /g76 /g83/g86\nFigure 6 : Averaged ratings of desired additional \ninformation/services for a music player (scale from  3: \ndont like  to +3 like a lot )\ntasks, new features and services and finally play lists \ncreation. Eleven questions out of fourteen are multiple choice and the options come from the interviews.  The questionnaire was separated into 6 pages to keep the attention of the participants focused. On the first page a short explanation of the study was presented. The questionnaire was distributed to a number of subjects interested in music, obtained from previous studies [13].  \n \n6.1. Results of the Web Questionnaire \n130 participants responded to the questionnaire. The \nresponses were considered valid if the number of songs in the collection was bigger than 500 and if pop and rock genres contribute to the majority of the collection. According to these criteria only 86 responses were further analyzed. Among the participants 67% were between 20 to 30 years, 22% between 30 to 40 years and the remaining 11% older than 40 years. The population was almost exclusively male: 93% against a 7% female. The participants have their own collections organized along some sort of structure in 87% of the cases.\n never  very often  \ndont like  like a lot  dont like  like a lot    \n \n/g16/g22 /g16/g21 /g16/g20 /g19 /g20 /g21 /g22/g54 /g82 /g81/g74/g3 /g47/g92 /g85/g76 /g70 /g86/g36 /g85 /g87/g76 /g86 /g87/g3/g9 /g3/g68 /g79 /g69 /g88 /g80 /g3/g68 /g85 /g87/g76 /g70/g79 /g72 /g86 /g3/g9 /g3/g69 /g82 /g82 /g78/g86/g36 /g85 /g87/g76 /g86/g87/g3/g83 /g76 /g70 /g87/g88 /g85 /g72/g36 /g85 /g87/g76 /g86/g87/g3/g9 /g3/g80/g88 /g86/g76 /g70/g3 /g90 /g72 /g69 /g16 /g79 /g76 /g81 /g78/g86/g55 /g57 /g3 /g83/g85/g82/g74/g85/g68 /g80 /g86/g38 /g82 /g81/g70 /g72/g85/g87 /g3 /g36 /g74/g72/g81 /g71/g68/g86/g36/g79 /g69 /g88 /g80/g3 /g70 /g82 /g89 /g72 /g85 /g86/g36 /g85/g87 /g76 /g86 /g87 /g3 /g39 /g76 /g86 /g70 /g82/g74/g85/g68 /g83/g75/g92/g36/g85 /g87/g76 /g86/g87/g3/g37/g76 /g82 /g74 /g85 /g68 /g83 /g75 /g92/g57 /g76 /g71/g72/g82/g3 /g70 /g79 /g76 /g83/g86\nFigure 7: Averaged ratings of desired additional \ninformation/services for a music player (scale from  3: \ndont like  to +3 like a lot )\nFigure 8: Requirements for the interaction. The notion of \n\"similarity\" (of artist and songs) was chosen as the basic interaction around which the interface was designed. \nThe most common structure used by the participants is \nhierarchical with artist-name as the first level in about \n60% of the cases and genre  or album  in about 15%.  \nFigure 8 shows the averaged ratings for some alternative selection methods (not based on hierarchy).   It can be seen that similar artists and similar songs scores quite high to confirm the results obtained during the interviews. In Figure 6 the results of the questionnaire on additional information or services are summarized. It is visible that song lyrics and album covers are very appreciated together with discography, biography and video clips. \n \n7. USER REQUIREMENTS \nThe results of the user studies gave us some ideas for \nalternative ways to navigate music collections, which were developed further into a user interface concept. Moreover the following requirements have been identified: (i) the music collection should be presented with the structure\n Artist-Album-Song . (ii) Genre can be \npresented as an option. (iii) The system should allow the selection music according to period of time and mood .\n(iv) The system should offer the opportunity to select songs similar to those already selected and songs of \nartists similar to the already selected artist(s). (v) Statistical data about the usage of the collection should be recorded to enable functionalities such as: play last \nacquired songs , play frequently played songs  and play \nlast songs played . (vi) the following additional music \nrelated information should be presented or access to this information should be provided: song lyrics, album \ncovers and artists discography . (vii) Easy and effective \nway of creating play lists should be provided. Figure 8 shows a schematic view of the more important identified requirements.\n \n8. DEMONSTRATOR AND EVALUATION \nA preliminary demonstrator was developed and \nevaluated with users (a screenshot is shown in Figure 9). The users can browse the collection in the  traditional hierarchical way to select artists or songs and use the similar window to obtain similar items to those selected. When satisfied with the selection of a song they can displace some adapters on the target (top left of the screenshot) to adapt the similarity to their desires (e.g. song similar by tempo and by sound etc). If an adapter is close to the center it is weighted high otherwise it is weighted low. The adapters used for this experiment are: timbre, tempo, year, mood and genre. Mood, genre and tempo are manually labeled for a database of around 2000 songs.  The demonstrator was compared against MoodLogic. The participants to the \"MoodLogic\" experiment were offered to evaluate the two concepts according to following criteria: overall impression, efficiency, \noriginality, understandability, enjoyability, usefulness, on a scale from 2: very poor to +2: very high .The \nresults are available in Figure 10. The demonstrator scored better with respect to Moodlogic  in every aspect. \nThis user test was conducted with only six subjects so it is not much representative, however it shows a positive attitude of the participants towards alternative navigation methods.  \n9. CONCLUSIONS \nThe paper presents a study about navigation and \norganization related issues around digital music. First a series of interviews with potential users gave insight about users behavior, habits and preferences. The most interesting finding confirmed the results in [4] about the use of vague preferences and personal expressions to describe the music they desires.   To evaluate the appeal of being able to choose music according to alternative attribute such as tempo, mood and year, a small field test, which involved the use of the \nMoodLogic  software, was conducted. Finally (to obtain \nquantitative data) a web-based questionnaire based on the results of the interviews and the field test was designed and distributed. The major finding was, not surprisingly, that the most popular form of organization  dont like  like a lot    \n \n/g54 /g87/g92 /g79/g72\n /g54 /g87/g92 /g79/g72\n \nFigure 9:  A very preliminary demonstrator of the \nsimilarity-based concept \n/g16 /g22 /g16/g21 /g16/g20 /g19 /g20 /g21 /g22/g50 /g89 /g72 /g85/g68/g79 /g79 /g3 /g76 /g80 /g83/g85/g72 /g86 /g86 /g76 /g82/g81/g56 /g86 /g72/g73 /g88 /g79 /g81/g72 /g86 /g86/g56 /g81 /g71/g72/g85 /g86 /g87 /g68 /g81/g71 /g68/g69/g76 /g79 /g76 /g87 /g92/g40 /g73 /g73 /g76/g70 /g76/g72 /g81/g70 /g92/g40 /g81/g77/g82 /g92 /g68 /g69 /g76 /g79/g76/g87 /g92/g50/g85 /g76/g74/g76/g81 /g68 /g79 /g76/g87 /g92\n/g54 /g76 /g80 /g76 /g79 /g68/g85/g76 /g87 /g92 /g3 /g38/g82/g81 /g70 /g72/g83 /g87 /g48 /g82/g82/g71 /g47/g82 /g74/g76 /g70\nFigure 10: Results of the user evaluation for MoodLogic \nand the Similarity-based concept \nis hierarchical and based on the artist/album/song  \nstructure. Also not surprisingly we found that the participants want to choose music on the basis of similarity. The features  select songs similar to \n \nthe chosen song a n d   select songs of artists similar to \nthe already selected artist(s) were highly rated by the participants. It is relevant to point out that nobody among the participants mentioned melody as an important retrieval method (however this might be due to the way the issue was addressed).\n \nFollowing these user tests a novel concept to browse a \nmusic collection was proposed. The main requirements were set with respect to the user study: the concept should provide the possibility to browse through the collection in the classical way based on artist/album/song  and at the same time the concept \nshould offer new ways of browsing based on the similarity. These two navigation strategies result to be complementary. To illustrate and test the concept a demonstrator was realized. Preliminary user tests were conducted according to usefulness and usability issues.  A more advanced demonstrator robust enough to be tested in real life experiments is in development. 10. REFERENCES \n [1]  Pauws, S. and Eggen, B., \"PATS: realization and \nevaluation of an automatic playlist generator,\" Proc.of 2001 International Simposium on Music Information Retrieval , 2003. \n [2]  Rauber, A., Pampalk, E., and Merkl, D., \"Using \nPsycho-Acoustic Models and self-organizing maps to Create a hierarchical structuring of Music by sound similarity,\" Proc.of 2001 International \nSimposium on Music Information Retrieval , pp. \n71-80, 2003. \n [3]  Pachet, F., Laburthe, A., Zils, A., and \nAucouturier, J.-J., \"Popular Music Access: The Sony Music Browser,\" Journal of the American \nSociety for Information (JASIS) , 2004. \n [4]  Vaessens, Bart, \"Expression of music \npreferences: how do people describe popular music that the want to hear.\" Master thesis,\nTilburg university, 2002. \n [5]  Ruyter, B. d., Sinke, H., Pauws, S., and Salpietro, \nR. Easy Access to Networked ICE: User issues in accessing large collections of audio content. Philips TN 2000/145. 2000.  \n [6]  Schuessler, K. F., \"Musical taste and socio-\neconomic background.\" Ph.D. Thesis, Indiana University, 1980. \n [7]  Tzanetakis, G. and Cook, P., \"Musical Genre \nClassification of Audio Signals,\" IEEE \nTransactions on Speech and Audio Processing ,\nvol. 10, no. 5, pp. 293-302, July 2002. \n [8]  McKinney, M. and Breebart, J., \"Features for \nAudio Music Classification,\" Proceedings of 4rd \nInternational Conference on Music Information Retrieval, ISMIR 2003 , pp. 151-158, 2003. \n [9]  Aucouturier, J.-J. and Pachet, F., \"Representimg \nMusical Genre: A State of The Art,\" Journal of \nNew Music Research, 2002. \n [10]  All Music Guide. http://www.allmusic.com,\n2003.  \n [11]  Amazon.com.  http://www.amazon.com, 2003.  \n [12]  Gracenote. Gracenote CDDB. \nhttp://www.gracenote.com, 2003.  \n [13]  Vignoli, F. and Kravtsova, N., \"Music \nrecommender\" Proc.of Philips UI Conference \n2002."
    },
    {
        "title": "Mapping Music In The Palm Of Your Hand, Explore And Discover Your Collection.",
        "author": [
            "Fabio Vignoli",
            "Rob van Gulik",
            "Huub van de Wetering"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416960",
        "url": "https://doi.org/10.5281/zenodo.1416960",
        "ee": "https://zenodo.org/records/1416960/files/VignoliGW04.pdf",
        "abstract": "The trends of miniaturization and increasing storage capabilities for portable music players made it possible to carry increasingly more music on small portable devices, but it also introduced negative consequences for the user interface and navigation. Finding music in large collections can be hard if one does not know exactly what to look for. In this paper a novel user interface to browse and navigate through music on small devices is proposed, together with the enabling algorithms. The goal of this interface is to enable the users to explore and discover their entire collection and to support non- specific searches. To this end, a new way to visualize and navigate through music is introduced: the artist map. The artist map is designed to provide an overview of an entire music collection, or a subset thereof, by clearly visualizing the similarity between artists, computed from the music itself. Contextual information (e.g. mood, genre) is added by coloring and by attribute magnets. The artist map is implemented by a graph-drawing algorithm, which uses an improved energy model. The proposed algorithm and interface have been implemented in a prototype and will be tested with ‘real’ users. Keywords: Music, graphical user interface, similarity, navigation, non-specific search, music metadata",
        "zenodo_id": 1416960,
        "dblp_key": "conf/ismir/VignoliGW04",
        "keywords": [
            "miniaturization",
            "increasing storage capabilities",
            "portable music players",
            "user interface",
            "navigation",
            "negative consequences",
            "finding music",
            "large collections",
            "novel user interface",
            "artist map"
        ],
        "content": "MAPPING MUSIC IN THE PALM OF YOUR HAND, \nEXPLORE AND DISCOVER YOUR COLLECTION \nRob van Gulik Fabio Vignoli Huub van de Wetering \nTechnische Universiteit Eindhoven \nDen Dolech 2, 5600 MB, Eindhoven \nR.v.Gulik@student.tue.nl  Philips Research Laboratories, \nProf. Holstlaan 4, 5656 AA, \nEindhoven (NL) \nFabio.Vignoli@philips.com \n(corresponding author)  Technische Universiteit Eindhoven \nDen Dolech 2, 5600 MB, \nEindhoven \nH.v.d.Wetering@tue.nl  \nABSTRACT \nThe trends of miniaturization and increasing storage \ncapabilities for portable music players made it possible to carry increasingly more music on small portable devices, but it also introduced negative consequences for the user interface and navigation. Finding music in large collections can be hard if one does not know exactly what to look for. In this paper a novel user interface to browse and navigate through music on small devices is proposed, together with the enabling algorithms. The goal of this interface is to enable the users to explore and discover their entire collection and to support non-specific searches. To this end, a new way to visualize and navigate through music is introduced: the artist map. The artist map is designed to provide an overview of an entire music collection, or a subset thereof, by clearly visualizing the similarity between artists, computed from the music itself. Contextual information (e.g. mood,\ngenre) is added by coloring and by attribute magnets. The artist map is implemented by a graph-drawing algorithm, which uses an improved energy model. The proposed algorithm and interface have been implemented in a prototype and will be tested with real users. \n Keywords : Music, graphical user interface, similarity, \nnavigation, non-specific search, music metadata \n1. INTRODUCTION \nAdvances in digital media and consumer electronics have \nmade it possible to carry more and more music on small portable devices. From the introduction of the Walkman twenty-five years ago to the current portable hard disk players such as the Philips HDD100 or the Apple iPod, much progress has been made in the areas of sound quality, device functionality and the amount of storage available for your music. Although some of the first hard disk players were too large to fit in your pocket, the current generation of music players with a hard disk is even smaller than a Walkman. The trends of miniaturization and increasing storage capacity have, however, also negative consequences in particular for user interfaces (UI).  Current user interfaces often have folder-based or \nhierarchical structures. Such structures limit the user to find specific items: an album, song or artist for example, provided that the hierarchy is known. When the user is confronted with a huge amount of digital music, finding the right music for e.g. an occasion or mood can be hard. Product innovations such as the iPod touch-wheel, or the super-scroll on the Philips HDD100 are focused on improving the access speed for list-based interfaces. Without doubt these solutions improve the ease of use, but unfortunately do not offer an overview of the music currently loaded on the device. When the users do not know, recall or recognize the name of a specific artist or song, it is difficult to find   or to decide whether or not to play the unknown item. And as collections grow, the amount of unknown or forgotten music increases. As of consequence the users end up enjoying only a fraction of the music in their collection. \n In this paper a novel user interface to browse and \nnavigate through music on small displays (about 10 cm diagonally) is proposed. The proposed solution attempts to tackle the issues raised above by going beyond the traditional directory structure. Our goal is to enable the users to explore and discover their entire music collection by providing an interactive map of the music to be used for browsing and navigation purposes. The paper is organized as follows: Section 2 gives a short overview of the related work in this field: we discuss how people organize their music and how they describe what they want to listen to, and we detail the requirements for our solution. In section 3, we describe the interface concept and in Section 4 our implementation. In Section 5, the proposed user interface is described in more detail. Section 6 contains the conclusions as well as some directions for future research.  \n2. RELATED WORK \nThe way people organize their digital music collection is \noften influenced by the way they organize their CD collection. In general people use catalogue metadata: artist-name, song-name and album-name, when \navailable. Although these metadata are often the only ones used in music players such as Windows MediaPlayer [1] or MusicMatch [2], people tend to use entirely different terms and expressions to describe the music they want to hear [3]. These descriptions are Permission to make digital or hard copies of a ll or part of this work \nfor personal or classroom use is granted without fee provided that \ncopies are not made or distributed for profit or commercial \nadvantage and that copies bear this notice and the full citation on \nthe first page. \n© 2004 Universitat Pompeu Fabra.   \n \nmostly related to the style, the mood and the genre of the music, or the situation at hand. Some examples of such music descriptions are Calm music, Happy music, Music for a romantic evening.  Other user studies [4]   \nshow that people need and like the possibility of using other ways of organizing and browsing their music collections. When confronted with a whole range of features and concepts for music organization and browsing, the participants expressed their interest about the concepts of similarity between songs and between artists.  \n Navigation through music collections is traditionally \naccomplished by using folder-based hierarchical structures based on genre/artist/album. Maintaining such a structure is difficult, because for every item that is added to the device, either the user has to specify where to place it, or catalogue metadata is required (tagged to the music file for example) to determine the position automatically. Also, a folder-based structure is only efficient when users want to search for specific artists or songs but does not support them very well when the desires are not specific. The concept of music similarity as well as the personal and vague descriptions about the style, mood and tempo are thus not well suited to use in a list-based user interface. As the size of the average music collection increases, new ways to find desired music in a less specific way are needed. \n Few attempts to provide visual landscape of music \ncollection are known in literature. Pampalk and others [5] describe an approach to explore music collections based on a modified self-organizing map trained to cluster similar music, Tzanetakis and Cook [6] propose an audio browser-editor for large wall displays, Brazil and Fernström describe the Sonic Browser [7] which uses sonic spatialization to navigate music collections, and finally Cano and others [8] describe a multidimensional scaling algorithm to visualize songs on the basis of their similarity. While many of these systems make use of large screens, the literature is mostly concerned with text and images for small displays and not much has been published on music navigation and browsing. In this paper we propose an interface tailored to small displays that supports non-specific searches, or browsing. \n3. THE ARTIST MAP \nTo be able to use similarity and music attributes as an \nintegral part of a graphical user interface, we introduce the artist map . The artist map is a visualization of a \nmusic collection to be used on a small screen, and is designed to allow and support non-specific music searches. The artist map makes use of both metadata and \n Attributes Values \nmoods Upbeat, Happy, Romantic, \nMellow, Sentimental, Sad,     \nBrooding, aggressive \ngenres Popular, Rock, Americana, \nAlternative, Soul/r+b/rap,  \nDance/lounge,  Dutch Music\nTable 1 : moods and genres used for the experiments \n(moods are manually labeled using information obtained from MoodLogic, genres are manually labeled) \nfeatures. The metadata such as artist, album  and song \nnames  and publication year  can be obtained from web-\nservices and features, such as tempo  and texture (spectral \ninformation) of songs can be computed directly from the music itself. In our experiments we used also attributes such as mood and genre that can be obtained from music classification as described in [9] and [10;11] or from services such as MoodLogic [12]. The specific moods and genres used for the experiments are reported in Table 1. The choice of these moods and genres is arbitrary but does not influence the conclusions, any other choice is equally right or wrong. \n The artist map aims at visualizing a music collection in \nsuch a way that: \n•A clear overview of an entire music collection \nor a subset thereof can be given; \n•Similarity between artists is used and clearly \ndepicted; \n•The attributes mood, genre, year and tempo  \nlabel important positions on the map in order to provide context; \n•Navigation of a, possibly unknown, music \ncollection is supported by non-specific or fuzzy criteria \n Figure 1 shows a sketch of the interface concept. In \nthe map, artists are clustered together if they are similar, while colors and higher level positioning (which is labeled) show attribute information for the type of music they make. In this case, year  and tempo are chosen to be \nthe relevant attributes, and labels are used to show five different year ranges and three different tempi.   \n The artist map is defined on top of the artist similarity. \nWe define the similarity of two artists based on the features extracted from their songs. We used the following method, analogous to that described in [13]. The similarity is defined for each couple of artists. Each song in the collection is processed to obtain feature vectors v\nc. In our experiments we used features computed \nas described in [11] which show good discrimination properties, although standard Mel-Frequency Cepstrum Coefficients (MFCCs) could also have been \nemployed. The v\ncare whitened and used to train a P ×Q\nSelf-Organizing map (SOM) [14].   \n \nFigure 1 : Concept of the artists map: small dots \nrepresent artists in the 2-D space defined by the labels, in this case year of release and tempo .\nFor each artist Aibelonging to the collection, a 2-D \nhistogram RQPH/g76→×: is computed by accumulating the \nresponse of the SOM to the feature vectors vcof the \nsongs performed by that artist. In figure 2 the histogram computed for two different artists is shown. For each artist, the histogram can be regarded as a probability distribution of the songs in the feature space. We can define the similarity Sim(A\ni,Aj)between two artists Ai and \nAj as: \n)),( ),,( min( ),(\n/g19/g19∑∑\n===/g51\n/g75/g52\n/g78/g77 /g76 /g77 /g76 khHkhH AA Sim (1) \n The main reason for adopting this similarity measure \nis because it is computationally cheap with respect to alternative measures such as the Earth Mover Distance as described in [13]. However note that the framework and the algorithm proposed do not impose any constraint on the choice of the similarity measure. \n4. THE MODIFIED SPRING-\nEMBEDDER ALGORITHM \nThe artist map is implemented as a two-dimensional \ndrawing of a graph, in which the vertices represent artists in a music collection. Two vertices are connected if the corresponding artists similarity is above a certain threshold, in which case the artists are said to be similar. We would like this drawing to be nice, which we define by enforcing the following properties: (i) similar artists are placed close together, (ii) clusters of similar artists can be easily identified and (iii) vertices should be distinguishable. Therefore the desired drawing has small edge lengths (minimized or of a uniform small length), but the vertices should not be too close to each other either: simply placing each vertex at the same position  \na) \nb) \nFigure 2 : Distribution of the songs in the feature \nspace for a) Pearl Jam  and b) Abba . It is visible how \nthey are different. \nminimizes edge length to 0, but the resulting picture \nneither bears any information on clusters nor shows a relation between distance and similarity. \n To create such a layout, we adapted a force-directed \ngraph-drawing algorithm, to be able to satisfy the above-mentioned properties. This algorithm uses a physical analogy to compute the layout of a graph, where the graph is seen as a system of bodies with forces acting between them. The algorithm can be regarded as an optimization process that seeks a configuration of the bodies with locally minimal energy. Such an equilibrium configuration, in which the sum of the forces on each body is zero, hopefully corresponds to a nice drawing. \n The Spring Embedder method [15] is among the first \napplications of force-directed methods on graph drawing and evolved from the VLSI technique of force-directed placement. In the spring embedder algorithm, the physical analogy used is as follows: (i) vertices are replaced by charged particles that repel each other, (ii) edges are replaced by springs that connect the particles. Fruchterman and Reingold [16] proposed a modified version of this original spring embedder algorithm, which more closely models the two properties we are interested in. Figure 3 gives a conceptual idea of how the spring embedder algorithm works. Given a graph and its initial drawing (upper left), we assign the attractive and repulsive forces, let the system go to find a low energy state, and end up with a drawing of the same graph that looks clearer (bottom left). The configuration found in a low energy state depends heavily on the exact force model used.   \n \nFigure 3 : Spring-Embedder algorithm example, after \n[12]. The final layout looks clearer than the initial drawing. \n Applying the simple Fruchterman-Reingold model \ndirectly on the artist graph results in a graph drawing that looks nice at the first sight, but closer investigation shows two problems: \n1. The vertices are too evenly spread; the \nclustering is not obvious \n2. The position of clusters is not the same for \nsubsequent runs of the algorithm \n To improve the clustering properties of the layout, we \nchanged the force model based on [17], in which an energy model is introduced that produces a more obvious clustering: the LinLog model. We have tested and \ncompared the results of LinLog to those of other models \nusing a test collection of graphs obtained from [17] and extended for our purposes. Some of our results are shown in Figure 4. Although the graph used in Figure 4 a) and 4 b) is exactly the same, the LinLog drawing a) shows the clusters clearer than drawing b) that was produced by a non-clustering force-directed method. \n Given a graph G(V,E) where Vis a set of nodes and E a\nset of edges, define p:\n/g21RV→ as the 2-D drawing of G.\nThe LinLog energy model U LinLog (p) is defined as: \n) ln( )(\n/g21/g96 /g15 /g94 /g96 /g15 /g94/g89 /g88\n/g57 /g89 /g88 /g40 /g89 /g88/g89 /g88 /g47/g76/g81/g47/g82/g74 p p p p p U −− − = ∑ ∑\n∈ ∈(2) \n Where puand pvare the positions in the drawing of \nnodes uand v respectively. The first term represents \nattraction between connected vertices, while the second term (repulsion) is introduced to avoid overlapping vertices. \n The second problem we identified is not always a \nproblem for graph drawings in general: as long as the clusters are obvious, the actual positions of the clusters \ndo not always matter. \na) b) \nc) d)  \nFigure 4 : Results of clustering tests: a) and c) LinLog \nvs. b) and d) non-clustering force-directed graph layout methods. In c) and d) attribute magnets are introduced.  \nHowever in the case of  music visualization, we desire \ngeometrical dimensions with a clear meaning, as shown in Figure 1 (where the x-axis and y-axis represent year \nand tempo  respectively). Therefore for various runs of \nthe graph layout algorithm, clusters of similar artists (certain kinds of music) should not change position much. For example, if aggressive music ends up at the bottom left of the layout after one run, but at the top right after another run, the users will not be able to find their music, let alone memorize the gist of the complete layout.  \n To obtain this result, we extended the algorithm with \nattribute magnets for each of the attribute types: mood,\ngenre, year and tempo . The magnets are used to roughly \ndefine a priori the desired position in the layout for certain kinds of music. In our implementation, dummy attractors represent these magnets. Therefore in the LinLog model extended with magnets, the energy U\nMLinLog (p) of a drawing p:/g21R MV→∪  is defined as: \n∑\n×∈− + =\n/g48 /g57 /g80 /g89/g80 /g89 /g47/g76/g81/g47/g82/g74 /g48/g47/g76/g81/g47/g82/g74 p pfmvg p Up U\n/g96 /g15 /g94) (),( )( )( (3) \n The second term represents the attraction of the artists \n(vertices of the graph) to the magnets. M is the set of \ncurrently active magnets; f is proportional to the distance    \n \nδbetween an artist vand a magnet m  where distance \nincorporates the radii of vertices  and is defined as \n2*)1()( cc f−=δδ ,where c1 and c2 are constants \nrepresenting the preferred spring length and the spring \nstiffness respectively. And g:V ×M→[0,1] can be \nregarded as the affinity of the artist with the music \nrepresented by the magnet. For example, artists who perform only happy music have affinity equal to 1 for the happy  magnet, while the same artists have affinity equal \nto 0 for all other moods. For each artist the sum of affinities for a given magnet type is equal to 1. An example of the introduction of magnets in the LinLog \nmethod is shown in Figure 4 c), compared to another method with magnets shown in Figure 4 d). It can be seen that the clustering is clearer when the LinLog \nmethod is used. \n5. THE USER INTERFACE \nThe artist map provides a clear overview of the music \ncollection that can be used to provide fast and easy navigation through the music. Besides non-specific searches, we want it to support specific searches as well, such as in traditional hierarchical interfaces. Therefore we defined an integrated interface in which the users can seamlessly navigate through their music using the method they prefer. \nIn Figure 5 three screenshots of the developed \ninterface, used on a music collection of 200 artists and about 2000 songs, are shown. Figure 5 a) is the standard hierarchical interface based on catalogue metadata or  folder structure, while Figure 5 b) and 5 c) show the artist map. In the artist map the users are able to \ndetermine the clustering and coloring properties they desire, for example they could select a mood-map  where \ncoloring depends on the tempo  of the songs as shown in \nFigure 5 b) or they could choose a year-tempo  map as \nshown in Figure 5 c) with a different coloring, namely on the year attribute. In Figure 5 c) the clustering is based \non two types of magnets. The year of release magnets attract artists in the horizontal direction only, while the tempo  magnets operate in the vertical direction only. \nSuch attraction rules and magnet placements make efficient use of the available space. The users have complete control over the map and can alter it by explicitly changing the position (which is not modified by the visualization algorithm) and the type of the magnets. The map changes interactively to adapt to user needs. Zooming in on the map is also provided to show more details about the cluster of artists currently selected. Once the music of interest has been identified, the user can play it directly from the map, or go back to the standard hierarchical visualization to select specific albums and/or songs. \n6. CONCLUSIONS AND FUTURE WORK \nThis paper describes an interface and the enabling \nalgorithms to map a music collection on small displays. The application seeks to support non-specific searches, and proposes the use of the artists map. The artist map provides an overview of an entire music collection or a subset thereof, by clearly visualizing the similarity between artists, computed from the music itself. Other metadata (e.g. from services) such as mood, genre and  \n \na) \n \nb) \n \nc) \nFigure 5 : Screenshots of the interface, applied to a music collection of 200 artists and 2000 songs: a) standard  navigation \nbased on hierarchy, b )amood-map  where clustering is based on mood  (spreading the magnets to make efficient use of \nspace) and coloring is done on tempo  c) a year-tempo  map where clustering is based on year of release along the horizontal \ndirection and tempo  along the vertical direction, and coloring is done on year .  \n \nyear and/or from content analysis such as tempo  are \nadded to provide contextual information.  The users can select their preferred visualization by selecting a combination of coloring and attribute magnets (e.g. mood and tempo ), which provides them with complete control \non the visualization and navigation of their collection. A new graph-rendering algorithm that uses an adapted energy model has been used. This energy model, in which magnets are introduced, helps to better shape the resulting visualization. The proposed algorithm seems to performs better in terms of clustering (the visibility of clusters is used here as reference) and to offer a better overview of the music collection with respect to other algorithms in literature [8;17]. The application is currently being tested with real users to verify its usability in terms of efficacy, effectiveness and enjoyability. The test is made of two parts: first the participants make a list of songs they would like to hear (a playlist) by using only the standard interface, later they perform the same task by using also the artist map. The objective measures are: the time and the number of actions to perform the task and the number of unknown or disliked artists in the resulting playlists. A questionnaire about the interface is also proposed to the participants. \n7. \nREFERENCES  \n[1]  Windows Media Player. \nwww.microsoft.com/windowsmedia, 2004.  \n [2]  Music Match. www.musicmatch.com, 2004.  \n [3]  Vaessens, Bart, \"Expression of music \npreferences: how do people describe popular music that the want to hear.\" Master thesis ,\nTilburg university, 2002. \n [4]  Vignoli, F., \"Digital Music Interaction concepts: \na user study,\" Proceedings of International \nConference on Music Information Retrieval, ISMIR 2004, vol. accepted for publication Oct.2004. \n [5]  Pampalk, E., Dixons, S., and Widmer, G., \n\"Exploring music collections by browsing different views,\" Proceedings of 4rd \nInternational Conference on Music Information Retrieval, ISMIR 2003, pp. 201-208, 2003. \n [6]  Tzanetakis, G. and Cook, P., \"MARSYA3D: A \nPrototype Audio Browser-Editor Using a Large Scale Immersive Visual and Audio Display,\" Proc.of Int, Conf.on Auditory Display (ICAD), 2001. \n [7]  Brazil, E. and Fernstrom M., \"Audio Information \nBrowsing With The Sonic Browser,\" Proc of IEEE Int.Conf.on Coordinated and Multiple Views In Exploratory Visualization (CMV'03),\npp. 26, 2003. \n [8]  Cano, P., Kaltenbrunner, M., Gouyon, F., and \nBattle, E., \"On the use of FastMap for audio Retrieval and Browsing,\" Proceedings of 3rd International Conference on Music Information Retrieval , pp. 275-276, 2002. \n [9]  Liu, D., Lu, L., and Zhang, H.-J., \"Automatic \nmood detection from acoustic music data,\" Proceedings of 4rd International Conference on Music Information Retrieval, ISMIR 2003, pp. 81-87, 2004. \n [10]  Tzanetakis, G. and Cook, P., \"Musical Genre \nClassification of Audio Signals,\" IEEE \nTransactions on Speech and Audio Processing,vol. 10, no. 5, pp. 293-302, July2002. \n [11]  McKinney, M. and Breebart, J., \"Features for \nAudio Music Classification,\" Proceedings of 4rd \nInternational Conference on Music Information Retrieval, ISMIR 2003, pp. 151-158, 2003. \n [12]  MoodLogic. http://www.moodlogic.com,\n2003.  \n [13]  Berenzweig, A., Logan, B., Ellis, D. P. W., and \nWhitman, B., \"A Large-Scale Evaluation of Acoustic and Subjective Music Similarity Measures,\" Proceedings of 4th International \nConference on Music Information Retrieval, ISMIR 2003, pp. 99-105, 2004. \n [14]  kohonen, T., Self-Organizing Maps  Berlin: \nSpringer, 1995. \n [15]  Eades, P., \"A heuristic for graph drawing,\" \nProc.of Congressus Numerantium , pp. 149-160, \n1984. \n [16]  Fruchterman, T. M. J., Reingold, and E.M., \n\"Graph Drawing by Force-directed Placement,\" Software - practice and experience, vol. 21 pp. 1129-1164, 1991. \n [17]  Noack, A., \"An Energy Model for Visual Graph \nClustering,\" Proc of 11th International \nSymposium on Graph Drawing, pp. 425-436, 2003."
    },
    {
        "title": "Instrument identification in solo and ensemble music using Independent Subspace Analysis.",
        "author": [
            "Emmanuel Vincent 0001",
            "Xavier Rodet"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416524",
        "url": "https://doi.org/10.5281/zenodo.1416524",
        "ee": "https://zenodo.org/records/1416524/files/VincentR04.pdf",
        "abstract": "We investigate the use of Independent Subspace Analy- sis (ISA) for instrument identification in musical record- ings. We represent short-term log-power spectra of pos- sibly polyphonic music as weighted non-linear combina- tions of typical note spectra plus background noise. These typical note spectra are learnt either on databases contain- ing isolated notes or on solo recordings from different in- struments. We show that this model has some theoreti- cal advantages over methods based on Gaussian Mixture Models (GMM) or on linear ISA. Preliminary experiments with five instruments and test excerpts taken from com- mercial CDs give promising results. The performance on clean solo excerpts is comparable with existing methods and shows limited degradation under reverberant condi- tions. Applied to a difficult duo excerpt, the model is also able to identify the right pair of instruments and to provide an approximate transcription of the notes played by each instrument.",
        "zenodo_id": 1416524,
        "dblp_key": "conf/ismir/VincentR04",
        "keywords": [
            "Independent Subspace Analysis",
            "Instrument identification",
            "Polyphonic music",
            "Weighted non-linear combinations",
            "Typical note spectra",
            "Background noise",
            "Gaussian Mixture Models",
            "Linear Independent Subspace Analysis",
            "Preliminary experiments",
            "Clean solo excerpts"
        ],
        "content": "INSTRUMENTIDENTIFICA TIONINSOLOANDENSEMBLE MUSIC\nUSINGINDEPENDENT SUBSPACEANALYSIS\nEmmanuel VincentandXavierRodet\nIRCAM, Analysis-Synthesis Group\n1,place Igor Stravinsk y–F-75004 PARIS –FRANCE\n{vincent,rod}@ircam.fr\nABSTRA CT\nWeinvestigate theuseofIndependent Subspace Analy-\nsis(ISA) forinstrument identiﬁcation inmusical record-\nings. Werepresent short-term log-po werspectra ofpos-\nsibly polyphonic music asweighted non-linear combina-\ntions oftypical note spectra plus background noise. These\ntypical note spectra arelearnt either ondatabases contain-\ningisolated notes oronsolo recordings from different in-\nstruments. Weshowthat thismodel hassome theoreti-\ncaladvantages overmethods based onGaussian Mixture\nModels (GMM) oronlinear ISA. Preliminary experiments\nwith ﬁveinstruments andtestexcerpts takenfrom com-\nmercial CDs givepromising results. Theperformance on\nclean solo excerpts iscomparable with existing methods\nandshowslimited degradation under reverberant condi-\ntions. Applied toadifﬁcult duoexcerpt, themodel isalso\nable toidentify theright pairofinstruments andtoprovide\nanapproximate transcription ofthenotes played byeach\ninstrument.\n1.INTR ODUCTION\nThe aim ofinstrument identiﬁcation istodetermine the\nnumber andthenames oftheinstruments present inagiven\nmusical excerpt. Inthecase ofensemble music, instru-\nment identiﬁcation isoften thought asaby-product ofpoly-\nphonic transcription, which describes sound asacollec-\ntionofnote streams played bydifferent instruments. Both\nproblems arefundamental issues forautomatic indexing\nofmusical data.\nEarly methods forinstrument identiﬁcation havefo-\ncused onisolated notes, forwhich features describing tim-\nbreareeasily computed. Spectral features such aspitch,\nspectral centroid (asafunction ofpitch), energyratios of\ntheﬁrst harmonics andtemporal features such asattack\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies\narenotmade ordistrib uted forproﬁt orcommercial advantage andthat\ncopies bear thisnotice andthefullcitation ontheﬁrstpage.\nc\r2004 Universitat Pompeu Fabra.duration, tremolo andvibratoamplitude haveprovedtobe\nuseful fordiscrimination [1].\nThese methods havebeen extended tosolo andensem-\nblemusic using theComputational Auditory Scene Anal-\nysis (CASA) frame work[1,2,3,4].The principle of\nCASA istogenerate inside ablackboard architecture note\nhypotheses based onharmonicity andcommon onset and\nstream hypotheses based ontimbre, pitch proximity and\nspatial direction. Hypotheses arevalidated orrejected ac-\ncording toprior knowledge andcomple xprecedence rules.\nThebesthypothesis isselected forﬁnal explanation.\nFeature matching methods [3,4]usethesame timbre\nfeatures asinisolated notes. Features computed inzones\nwhere severalnotes overlap aremodiﬁed ordiscarded be-\nfore stream validation depending ontheir type. Template\nmatching methods [2]compare theobserv edwaveform lo-\ncally with sums oftemplate waveforms, that arephase-\naligned, scaled andﬁltered adapti vely.\nAlimitation ofsuch methods isthatoften timbre fea-\ntures ortemplates areused only forstream validation and\nnotfornote validation (except in[3]).This may result in\nsome badly estimated notes, anditisnotclear hownote\nerrors affectinstrument identiﬁcation. Forexample abass\nnote andamelody note forming atwo-octa veinterv almay\nbedescribed asasingle bass note with a“strange” spec-\ntralenvelope. This kind orerror could beavoided using\nthefeatures ortemplates ofeach instrument inthenote\nestimation stage.\nTimbre features forisolated notes havealso been used\nonsolo music with statistical models which donotrequire\nnote transcription. Forexample in[5,6]cepstral coef-\nﬁcients arecomputed andmodeled byGaussian Mixture\nModels (GMM) orSupport Vector Machines (SVM).\nInorder forthecepstral coefﬁcients tomakesense,\nthese methods suppose implicitly thatasingle note ispre-\nsentateach time (orthatthechords inthetestexcerpt are\nalso present inthelearning excerpts). Thus theyarenot\napplicable toensemble music ortoreverberant recordings\nandnotrobusttowards background noise changes. More-\novertheydonotmodel therelationship between pitch and\nspectral envelope, which isanimportant cue.Inthisarticle weinvestigate theuseforinstrument iden-\ntiﬁcation ofanother well-kno wnstatistical model: Inde-\npendent Subspace Analysis (ISA). Linear ISAtranscribes\ntheshort-time spectrum ofamusical excerpt asaweighted\nsum oftypical spectra, either adapted from thedata or\nlearnt inaprevious step. Thus itperforms template match-\ninginthespectrum domain. Linear ISA ofpowerspec-\ntrum hasbeen applied topolyphonic transcription ofdrum\ntracks [7,8]andofsynthesized solo harpsichord [9].But\nitsability todiscriminate musical instruments seems lim-\nited, evenonartiﬁcial data [10].Linear ISA ofcepstrum\nandlog-po werspectrum hasbeen used forinstrument iden-\ntiﬁcation onisolated notes [11]andgeneral sound classi-\nﬁcation inMPEG-7 [12].But, astheGMM andSVM\nmethods mentioned above,itisrestricted tosingle class\ndata andsensiti vetobackground noise changes.\nHere weshowthatlinear ISAisnotadapted forinstru-\nment identiﬁcation inpolyphonic music. Wederiveanew\nISA model with ﬁxednonlinearities andwestudy itsper-\nformance onrealrecordings takenfrom commercial CDs.\nThestructure ofthearticle isasfollo ws.InSection 2\nwedescribe agenerati vemodel forpolyphonic music based\nonISA. InSection 3weexplain howtouseitforin-\nstrument identiﬁcation. InSection 4westudy theper-\nformance ofthismodel onsolo music anditsrobustness\nagainst noise andreverberation. InSection 5weshowa\npreliminary experiment with adifﬁcult duoexcerpt. We\nconclude bydiscussing possible impro vements.\n2.INDEPENDENT SUBSP ACEANALYSIS\n2.1. Need foranonlinear spectrum model\nLinear ISA ofpower spectrum explains aseries ofob-\nservedpolyphonic powerspectra (xt)bycombining aset\nofnormalized typical spectra (\bh)with time-v arying pow-\ners(eht).Forsimplicity ,thiscombination isusually mod-\neled asasum. This givesthegenerati vemodel xt=PH\nh=1eht\bh+\u000ftwhere each note from each instrument\nmay correspond toseveraltypical spectra (\bh),andwhere\n(\u000ft)isaGaussian noise [9].Asageneral notation inthe\nfollo wing weusebold letters forvectors, regular letters\nforscalars andparentheses forsequences.\nThis linear model suffersfrom twolimitations.\nAﬁrstlimitation isthatthemodeling error isbadly rep-\nresented asanadditi venoise term\u000ft.Experiments show\nthattheabsolute value of\u000ftisusually correlated withxt,\nandthatthemodeling error may rather beconsidered as\nmultiplicati venoise (orasadditi venoise inthelog-po wer\ndomain). This isconﬁrmed byinstrument identiﬁcation\nexperiments, which usecepstral coefﬁcients (orequiva-\nlently log-po werspectral envelopes) asfeatures, instead\nofpower spectral envelopes [5,6,11].This limitation\nseems crucial regarding theinstrument identiﬁcation per-formance ofthemodel.\nAsecond limitation isthatsumming powerspectra is\nnotanefﬁcient wayofrepresenting thevariations ofthe\nspectrum ofagivennote between different time frames.\nManytypical spectra areneeded torepresent small f0vari-\nations invibrato,wide-band noise during attacks orpower\nriseofhigher harmonics inforte .Summation oflog-po wer\nspectra ismore efﬁcient. Forinstance itispossible torep-\nresent small f0variations byadding toagivenspectrum\nitsderivativeversus frequenc ywith appropriate weights.\nItcaneasily beobserv edthatthisﬁrstorder linear approx-\nimation isvalidforalargerf0variation range considering\nlog-po werspectra instead ofpowerspectra.\nWepropose tosolvethese limitations using nonlinear\nISA with ﬁxedlog(:)andexp(:)nonlinearities thattrans-\nform powerspectra intolog-po werspectra andvice-versa.\nTherestofthisSection deﬁnes thismodel precisely .\n2.2. Deﬁnition ofthemodel\nLet(xt)betheshort-time log-po werspectra ofagiven\nmusical excerpt containing ninstruments. Asusual for\nwestern music instruments, wesuppose thateach instru-\nmentj,1\u0014j\u0014n,canplay aﬁnite number ofnotes\nh,1\u0014h\u0014Hj,lying onasemitone scale (howeverthe\nmodel could alsobeused todescribe percussions).\nDenoting mjtthepowerspectrum ofinstrument jat\ntimetand\b0jhtthelog-po werspectrum ofnotehfrom\ninstrument jattimet,weassume\nxt=log2\n4nX\nj=1mjt+n3\n5+\u000ft; (1)\nmjt=HjX\nh=1exp(\b0jht)exp(ejht); (2)\n\b0jht=\bjh+KX\nk=1vk\njhtUk\njh; (3)\nwhere exp(:)andlog(:)aretheexponential andlogarithm\nfunctions applied toeach coordinate. The vector\bjhis\ntheunit-po wermean log-po werspectrum ofnotehfrom\ninstrument jand(Uk\njh)areL2-normalized “variation spec-\ntra” that model variations ofthespectrum ofthis note\naround \bjh.The scalar ejhtisthelog-po wer ofnote\nhfrom instrument jattimetand(vk\njht)are“variation\nscalars” associated with the“variation spectra”. Thevec-\ntornisthepowerspectrum ofthestationary background\nnoise. The modeling error vector\u000ftissupposed tobea\nwhite Gaussian noise.\nNote thatexplicit modeling ofthebackground noise is\nneeded inorder topreventitbeing considered asafeature\noftheinstruments present intheexcerpt.\nThis nonlinear model could beapproximated bythe\nsimpler onext=max jh\u0002\n\b0jht+(ejht;:::;ejht)T\u0003\n+\u000ft.Indeed thelog-po werspectrum canbeconsidered as\na“preferential feature” asdeﬁned in[3],meaning thatthe\nobserv edfeature isclose tothemaximum oftheunderly-\ningsingle instrument features.\nEq.(1-3) arecompleted with probabilistic priors forthe\nscalar variables. Weassociate toeach note ateach time a\ndiscrete stateEjht2f0;1gdenoting absence orpresence.\nWesuppose thatthese state variables areindependent and\nfollo waBernoulli lawwith constant sparsity factorPZ=\nP(Ejht=0).Finally weassume thatgivenEjht=0\nejhtisconstrained to\u00001 andvk\njhtto0,andthatgiven\nEjht=1ejhtandvk\njhtfollo windependent Gaussian laws.\n2.3. Computation ofacoustic featur es\nThe choice ofthetime-frequenc ydistrib ution for(xt)is\nnotimposed bythemodel. Howevercomparison ofspec-\ntralenvelopes onauditory-moti vated frequenc yscales or\nlogarithmic scales hasusually lead tobetter performance\nthan linear scales forinstrument identiﬁcation [5].Thus\nprecision inupper frequenc ybands isnotneeded andcould\nlead toover-learning. Themodeling off0variations with\nEq.(3)alsoadvocates foralogarithmic frequenc yscale at\nupper frequencies, since f0variations havetoinduce small\nspectral variations forthelinear approximation tobevalid.\nInthefollo wing weuseabank ofﬁlters linearly spaced\nontheERB scalefERB=9:26log(0:00437 fHz+1)be-\ntween 30Hzand11KHz. The width ofthemain lobes\nissettofour times theﬁlter spacing. Wecompute log-\npowers on11msframes (alowerthreshold issettoavoid\ndrop-do wnto\u00001 insilent zones).\n3.APPLICA TION TOINSTR UMENT\nIDENTIFICA TION\nForeach instrument j,wedeﬁne theinstrument model\nMjasthecollection oftheﬁxedISAparameters describ-\ninginstrument speciﬁc properties: thespectra (\bjh)and\n(Uk\njh)andthemeans andvariances oftheGaussian vari-\nablesejhtand(vk\njht)whenEjht=1.Wecallorchestra\nO=(Mj)alistofinstrument models.\nThe idea forinstrument identiﬁcation isnowtolearn\ninstrument models forseveralinstruments inaﬁrst step,\nandinasecond step toselect theorchestra thatbest ex-\nplains agiventestexcerpt. These twosteps called learning\nandinference arediscussed inthisSection.\n3.1. Infer ence\nTheprobability ofanorchestra isgivenbytheBayes law\nP(Oj(xt))/P((xt)jO)P(O).The determination of\nP((xt)jO)involvesanintegration overthestate andscalar\nvariables which isintractable. Weuseinstead thejoint\nposterior Ptrans=P(O;(Ejht);(pjht)j(xt))withpjht=\n(ejht;v1\njht;:::;vK\njht).Maximizing Ptrans means ﬁndingthebestorchestra Oexplaining (xt),butalsothebeststate\nvariables (Ejht),which provide anapproximate polyphonic\ntranscription of(xt).Here again instrument identiﬁcation\nandpolyphonic transcription areintimately related.\nPtrans isdeveloped astheweighted Bayes law\nPtrans/(Pspec)wspec(Pdesc)wdescPstatePorch; (4)\ninvolving thefour probability terms Pspec=Q\ntP(\u000ft),\nPdesc=Q\njhtP(pjhtjEjht;Mj),Pstate=Q\njhtP(Ejht)\nandPorch=P(O)andcorrecting exponents wspecand\nwdesc.Experimentally thewhite noise model for\u000ftis\nnotperfectly valid, since values of\u000ftatadjacent time-\nfrequenc ypoints areabitcorrelated. Weighting bywspec\nwith0<wspec<1isawayoftaking intoaccount these\ncorrelations [13].\nMaximization ofPtrans with respect totheorchestra O\niscarried outbytesting allpossibilities andselecting the\nbestone. ForeachO,thenote states(Ejht)areestimated\niterati velywith ajump procedure. Atstart allstates are\nsetto0,then ateach iteration atmost onenote isadded\norsubtracted ateach timettoimpro vePtrans value. The\noptimal number ofsimultaneous notes ateach time isnot\nﬁxedapriori .Thescalar variables (pjht)arere-estimated\nateach iteration with anapproximate second order New-\ntonmethod.\nThestationary background noise powerspectrum nis\nalso considered asavariable, initialized asmintxtand\nre-estimated ateach iteration inorder tomaximize Ptrans.\nThevariance of\u000ftandthesparsity factorPZaresetby\nhand based onafewmeasures ontestdata. Thecorrecting\nexponents wspecandwdescarealsosetbyhand depending\nontheredundanc yofthedata (largervalues areused for\nensemble music than forsolos).\nSetting arelevantpriorP(O)onorchestras would need\naverylargedatabase ofmusical recordings todetermine\nthenumber ofexcerpts available foreach instrumental en-\nsemble andeach excerpt duration. Here forsimplicity we\nuseP(O)=P\u0000T(H1+\u0001\u0001\u0001+Hn)\nZwhere Tisthenumber of\ntime frames of(xt).This givesthesame posterior prob-\nability toallorchestras onsilent excerpts (i.e.when all\nstates(Ejht)areequal to0).\nObviously thisprior tends tofavorexplanations with\nalargenumber ofinstruments, andthus cannot beused\ntodetermine thenumber ofinstruments inarelevantway.\nExperiments inthefollo wing aremade knowing thenum-\nberofinstruments apriori .\nNote thateveniftheprior wasmore carefully designed,\nthemodel would notbeable todiscriminate aviolin solo\nfrom aviolin duo. Indeed theselection ofthegood or-\nchestra would only bebased onthevalue ofP(O),in-\ndependently ofthemonophonic orpolyphonic character\noftheexcerpt. Toavoidthis, theBernoulli prior forstate\nvariables should bereplaced byamore comple xprior con-straining instruments toplay onenote atatime (plus re-\nverberation oftheprevious notes).\n3.2. About “missing data”\nWementioned abovethatlog-po werspectra are“prefer -\nential features” asdeﬁned in[3].Itisinteresting tonote\nthatinference with ISA treats “missing data” inthesame\nwaythatpreferential features aretreated in[3].Indeed the\ngradients ofPtrans versusejhtandvk\njhtinvolvethequan-\ntity\n\u0019jhtf=exp(\b0\njhtf)exp(ejht)\nPHj\nh0=1exp(\b0\njh0tf)exp(ejh0t)(5)\nwhich isthepowerproportion ofnotehfrom instrument\njintothemodel spectrum attime-frequenc ypoint(t;f).\nWhen thisnote ismask edbyother notes, \u0019jhtf\u00190and\nthevalue oftheobserv edspectrum xtfisnottakeninto\naccount tocompute ejht,(vk\njht)andEjht.Onthecontrary\nwhen thisnote ispreponderant \u0019jhtf\u00191andthevalue of\nxtfistakenintoaccount.\nThis method for“missing data” inference may useavail-\nableinformation more efﬁciently than thebounded margin-\nalization procedure in[4].When severalnotes overlap ina\ngiventime-frequenc ypoint, theobserv edlog-po werinthis\npoint isconsidered tobenearly equal tothelog-po werof\nthepreponderant note, instead ofbeing simply considered\nasanupper bound tothelog-po wers ofallnotes.\n3.3. Lear ning\nInstrument models canbelearnt from alargevariety of\nlearning excerpts, ranging from isolated notes toensem-\nblemusic. The learning procedure ﬁnds inaniterati ve\nwaythemodel parameters thatmaximize Ptrans onthese\nexcerpts. Each iteration consists intranscribing thelearn-\ningexcerpts asdiscussed aboveandthen updating thein-\nstrument models inaccordance.\nThe size ofthemodel andtheinitial parameters are\nﬁxedbyhand. Inourexperiments wesetK=2forall\ninstruments. Themean spectra (\bjh)were initialized as\nharmonic spectra with a-12dBperoctaveshape. The\n“variation spectra” (U1\njh)and(U2\njh)initially represented\nwide-band noise andfrequenc yvariations respecti vely.\nExperiments showed thatlearning onisolated notes is\nmore robustsince thewhole playing range ofeach in-\nstrument isavailable andthestate sequences areknown\napriori .Weobtained lowerrecognition rates with instru-\nment models learnt onsolo excerpts only than with mod-\nelslearnt onisolated notes only (and thelearning duration\nwasalsoconsiderably longer).\nThe learning setused intherest ofthearticle con-\nsists inisolated notes from theRWC Database [14].To\nmakecomparisons with existing methods easier ,wecon-\nsider thesame ﬁveinstruments asin[4]:ﬂute, clarinet,\noboe, bowedviolin andbowedcello, abbre viated asFl,Cl,Ob,VnandVc.Allinstruments arerecorded inthesame\nroom, andforeach oneweselect only theﬁrstperformer\nandthemost usual playing styles. Thus thelearning setis\nquite small.\n4.PERFORMANCE ONSOLO MUSIC\n4.1. Clean conditions\nTheperformance oftheproposed method wasﬁrsttested\nonclean solo music. Foreach instrument, wecollected 10\nsolo recordings from 10different commercial CDs. Then\nweconstructed thetestsetbyextracting 2excerpts of5\nseconds outofeach recording, avoiding silent zones and\nrepeated excerpts.\nResults areshowninTable 1.The average recogni-\ntion rateis90% forinstruments and97% forinstrument\nfamilies (woodwinds orbowed strings). This issimilar to\nthe88% rateobtained in[4].The main source oferror\nisduetocello phrases containing only high pitch notes\nbeing easily confused with violin. Howevercello phrases\ncontaining both high pitch notes andlowpitch notes are\ncorrectly classiﬁed. Ambiguous features ofsome notes in-\nsideaphrase arecompensated bynonambiguous features\nofother notes.\nToassess therelati veimportance ofpitch cues andspec-\ntralshape cues, thesame experiment wasdone with the\ndefaultinstrument models used forlearning initialization,\nwhich allhave-12dBperoctavespectra. Theaverage in-\nstrument andfamily recognition rates dropped to32% and\n56% respecti vely,which isclose torandom guess (20%\nand50%). Only cello hadagood recognition rate(80%).\nThis provesthattheISAmodel actually captures thespec-\ntralshape characteristics oftheinstruments anduses them\ninarelevantwayforinstrument discrimination.\nIdentiﬁed instrument\nFl Cl Ob Vn VcTestexcerptFl 100%\nCl 5% 85% 5% 5%\nOb 95% 5%\nVn 5% 95%\nVc 25% 75%\nTable 1.Confusion matrix forinstrument recognition of\nclean ﬁvesecond solo excerpts from commercial CDs\n4.2. Noisy orreverberant conditions\nWealsotested therobustness ofthemethod against noisy\norreverberant conditions.\nWesimulated reverberation byconvolving theclean\nrecordings with aroom impulse response recorded atIR-\nCAM (1sreverberation time) having anonﬂatfrequen-\ntialresponse. Theaverage instrument recognition ratede-\ncreased to85%. Confusion wasmainly augmented be-tween close instruments (such ashigh pitch cello andlow\npitch violin).\nThen weadded white Gaussian noise totheclean record-\nings with various Signal toNoise Ratios (SNR). Theav-\nerage instrument recognition ratedecreased to83% at20\ndBSNR and71% at0dBSNR when thenoise spectrum\nnwasprovidedapriori ,andto85% and59% when itwas\nestimated without constraints. Thus useful spectral infor -\nmation forinstrument identiﬁcation isstillpresent inlow\nSNR recordings andcanbeused efﬁciently .Howeverthe\nnoise spectrum estimation procedure weproposed works\natmedium SNR butfailsatlowSNR. Aﬁrst reason for\nthisisthatthehyper -parameters (variance of\u000ft,PZ,wspec\nandwdesc)were giventhesame values foralltestcondi-\ntions, whereas theoptimal values should depend onthe\ndata (forexample thevariance of\u000ftshould besmaller at\nlowSNR). Asecond reason isthattheshape oftheposte-\nriorisquite comple xandthatthesimple jump procedure\nweproposed toestimate thenote states becomes sensiti ve\ntonoise initialization atlowSNR. Small impro vements\n(+2% at20and0dBSNR) were observ edwhen initializ-\ningnapriori .Other Bayesian inference procedures such\nasGibbs Sampling may help solvethisproblem.\n5.PERFORMANCE ONENSEMBLE MUSIC\nFinally theperformance ofthemethod wastested onen-\nsemble music. Since weencountered difﬁculties incol-\nlecting asigniﬁcant amount oftestrecordings, weshow\nhere only thepreliminary results obtained onanexcerpt\nfrom Pachelbel’ scanon inDarranged forﬂute andcello.\nThis isadifﬁcult example because 10ﬂute notes outof\n12areharmonics ofsimultaneous cello notes, andmelody\n(ﬂute) notes belong totheplaying range ofboth instru-\nments, ascanbeseen inFig1.\nThe results ofinstrument identiﬁcation areshownin\nFig2.Using thenumber ofinstruments asapriori knowl-\nedge, themodel isabletoidentify theright orchestra. Note\nthatthere isalargelikelihood gapbetween orchestras con-\ntaining cello andothers. Orchestras containing only high-\npitched instruments cannot model thepresence oflow-\npitch notes, which isacoarse error .Orchestras containing\ncello butnotﬂute canmodel allthenotes, butnotwith\ntheright spectral envelope, which isamore subtle kind of\nerror .\nThenote states E1;htandE2;htinferred with theright\norchestra areshowninFig1.Allthenotes arecorrectly\nidentiﬁed andattrib uted totheright instrument, evenwhen\ncello andﬂute play harmonic interv alssuch astwooctaves\noroneoctaveandaﬁfth. There aresome falsealert notes,\nmostly with with short duration. Ifaprecise polyphonic\ntranscription isneeded, these errors could beremo vedus-\ningtime integration inside themodel topromote long du-\nration notes. Forexample theBernoulli prior forstate\nvariables could bereplaced with aHidden Mark ovModel\n(HMM) [15],orevenwith amore comple xmodel in-20406080\nxft\nt (s)f (Hz)\ndB102103104\n0 2 4 6 8\nTrue score (top: flute, bottom: cello)h (MIDI)\n4050607080708090\nE1,ht and E2,ht (top: flute, bottom: cello)\nt (s)h (MIDI)\n0 2 4 6 84050607080708090\nFigur e1.Spectrogram ofaﬂute andcello excerpt and\napproximate transcription (with theright orchestra) com-\npared with thetruescore.\nvolving rhythm, forcing instruments toplay monophonic\nphrases ortaking intoaccount musical knowledge [2].\n6.CONCLUSION\nInthisarticle weproposed amethod forinstrument iden-\ntiﬁcation based onISA. Weshowed that thelinear ISA\nframe workisnotsuited forthis task andweproposed\nanewISA model containing ﬁxednonlinearities. This\nmodel provided good recognition rates onsolo excerpts\nandwasshowntoberobusttoreverberation. Itwasalso\nable todetermine theright pairofinstruments inadifﬁcult\nduoexcerpt andtotranscribe itapproximati vely.\nCompared toother statistical models such asGMM and\nSVM, ISA hastheadvantage ofbeing directly applicable\ntopolyphonic music without needing aprior note tran-−2−1.9−1.8x 105\nFlClObVnVcFl\nCl\nOb\nVn\nVc\nFigur e2.Log-lik elihoods oftheduoorchestras onthe\nduoexcerpt ofFig.1\nscription step. Instrument identiﬁcation andpolyphonic\ntranscription areembedded inasingle optimization pro-\ncedure. This procedure uses learnt note spectra foreach\ninstrument, which makesitsuccessful forboth tasks even\nindifﬁcult cases involving harmonic notes.\nHoweverafewproblems stillhavetobeﬁxed,forin-\nstance better estimating thebackground noise byselecting\nautomatically thevalues ofthehyper -parameters depend-\ningonthedata, determining thenumber ofinstruments\nwith abetter orchestra prior ,andseparating streams us-\ningmusical knowledge when oneinstrument plays several\nstreams. Thecomputational load may also beaproblem\nforlargeorchestras, andcould bereduced using prior in-\nformation from aconventional multiple f0track er.Weare\ncurrently studying some ofthese questions.\nAninteresting waytoimpro vetherecognition perfor -\nmance would betoaddaprior onthetime evolution of\nthestate variables Ejhtorofthescalar variables ejhtand\nvk\njht.Forexample in[8]time-continuity ofthescalar vari-\nables isexploited. In[11]aHMM isused tosegment iso-\nlated notes intoattack/sustain/decay portions anddifferent\nstatistical models areused toevaluate thefeatures oneach\nportion. This uses thefactthatmanycues forinstrument\nidentiﬁcation arepresent intheattack portion [1].This\nsingle note HMM could beextended tomultiple notes and\ninstruments supposing thatallnotes evolveindependently\norintroducing acoupling between notes andinstruments.\nBesides itsuseforinstrument identiﬁcation andpoly-\nphonic transcription, theISAmodel could alsobeused as\nastructured source prior forsource separation indifﬁcult\ncases. Forexample in[15]wecouple instrument mod-\nelsandspatial cues fortheseparation ofunderdetermined\ninstantaneous mixtures.\n7.REFERENCES\n[1]K.D. Martin,Sound-sour cerecognition:Atheory\nandcomputationnal model ,Ph.D. thesis, MIT,1999.[2]K.Kashino andH.Murase, “Asound source identiﬁ-\ncation system forensemble music based ontemplate\nadaptation and music stream extraction, ”Speech\nCommunication ,vol.27,pp.337–349, 1999.\n[3]T.Kinoshita, S.Sakai, andH.Tanaka, “Musical\nsound source identiﬁcation based onfrequenc ycom-\nponent adaptation, ”inProc.IJCAIWorshopon\nCASA ,1999, pp.18–24.\n[4]J.Eggink andG.J. Brown, “Application ofmissing\nfeature theory totherecognition ofmusical instru-\nments inpolyphonic audio, ”inProc.ISMIR ,2003.\n[5]J.Marques and P.J.Moreno, “Astudy ofmusi-\ncalinstrument classiﬁcation using Gaussian Mixture\nModels andSupport Vector Machines, ”Tech. Rep.,\nCompaq Cambridge Research Lab, june 1999.\n[6]J.C. Brown,O.Houix, andS.McAdams, “Feature\ndependence intheautomatic identiﬁcation ofmusi-\ncalwoodwind instruments, ”JournaloftheASA,vol.\n109, no.3,pp.1064–1072, 2001.\n[7]D.Fitzgerald, B.Lawlor,andE.Coyle, “Prior sub-\nspace analysis fordrum transcription, ”inProc.AES\n114thConvention ,2003.\n[8]T.Virtanen, “Sound source separation using sparse\ncoding with temporal continuity objecti ve,”inProc.\nICMC ,2003.\n[9]S.A. Abdallah andM.D. Plumble y,“AnICA ap-\nproach toautomatic music transcription, ”inProc.\nAES114thConvention ,2003.\n[10] J.Klingseisen andM.D. Plumble y,“Towards musi-\ncalinstrument separation using multiple-cause neu-\nralnetw orks, ”inProc.ICA,2000, pp.447–452.\n[11] A.Eronen, “Musical instrument recognition us-\ningICA-based transform offeatures anddiscrimi-\nnativelytrained HMMs, ”inProc.ISSPA,2003.\n[12] M.A. Case y,“Generalized sound classiﬁcation and\nsimilarity inMPEG-7, ”OrganizedSound ,vol.6,no.\n2,2002.\n[13] D.J. Hand andK.Yu,“Idiot’ sbayes -notsostupid\nafter all?,”Int.Statist.Rev.,vol.69,no.3,pp.385–\n398, 2001.\n[14] M.Goto, H.Hashiguchi, T.Nishimura, andR.Oka,\n“RWC Music Database: database ofcopyright-\ncleared musical pieces andinstrument sounds forre-\nsearch purposes, ”Trans.ofInformation Processing\nSocietyofJapan ,vol.45,no.3,pp.728–738, 2004.\n[15] E.Vincent andX.Rodet, “Underdetermined source\nseparation with structured source priors, ”inProc.\nICA,2004."
    },
    {
        "title": "Features and classifiers for the automatic classification of musical audio signals.",
        "author": [
            "Kristopher West",
            "Stephen Cox"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418025",
        "url": "https://doi.org/10.5281/zenodo.1418025",
        "ee": "https://zenodo.org/records/1418025/files/WestC04.pdf",
        "abstract": "Several factors affecting the automatic classification of musical audio signals are examined. Classification is per- formed on short audio frames and results are reported as “bag of frames” accuracies, where the audio is segmented into 23ms analysis frames and a majority vote is taken to decide the final classification. The effect of different pa- rameterisations of the audio signal is examined. The effect of the inclusion of information on the temporal variation of these features is examined and finally, the performance of several different classifiers trained on the data is com- pared. A new classifier is introduced, based on the un- supervised construction of decision trees and either linear discriminant analysis or a pair of single Gaussian clas- sifiers. The classification results show that the topology of the new classifier gives it a significant advantage over other classifiers, by allowing the classifier to model much more complex distributions within the data than Gaussian schemes do.",
        "zenodo_id": 1418025,
        "dblp_key": "conf/ismir/WestC04",
        "keywords": [
            "automatic classification",
            "musical audio signals",
            "bag of frames",
            "audio frames",
            "temporal variation",
            "different parameterisations",
            "classifiers",
            "decision trees",
            "un-supervised construction",
            "Gaussian schemes"
        ],
        "content": "FEATURES AND CLASSIFIERS FOR THE AUTOMATIC\nCLASSIFICATION OF MUSICAL AUDIO SIGNALS\nKris West\nSchool of Computing Sciences\nUniversity of East Anglia\nkristopher.west@uea.ac.ukStephen Cox\nSchool of Computing Sciences\nUniversity of East Anglia,\nsjc@cmp.uea.ac.uk\nABSTRACT\nSeveral factors affecting the automatic classiﬁcation of\nmusical audio signals are examined. Classiﬁcation is per-\nformed on short audio frames and results are reported as\n“bag of frames” accuracies, where the audio is segmented\ninto 23ms analysis frames and a majority vote is taken to\ndecide the ﬁnal classiﬁcation. The effect of different pa-\nrameterisations of the audio signal is examined. The effect\nof the inclusion of information on the temporal variation\nof these features is examined and ﬁnally, the performance\nof several different classiﬁers trained on the data is com-\npared. A new classiﬁer is introduced, based on the un-\nsupervised construction of decision trees and either linear\ndiscriminant analysis or a pair of single Gaussian clas-\nsiﬁers. The classiﬁcation results show that the topology\nof the new classiﬁer gives it a signiﬁcant advantage over\nother classiﬁers, by allowing the classiﬁer to model much\nmore complex distributions within the data than Gaussian\nschemes do.\n1. INTRODUCTION\nAs personal computing power increases, so do both the\ndemand for and the feasibility of automatic music analy-\nsis systems. Soon content discovery and indexing appli-\ncations will require the ability to automatically analyse,\nclassify and index musical audio, according to perceptual\ncharacteristics such as genre or mood.\nIn the ﬁeld of automatic genre classiﬁcation of musical\naudio signals, classiﬁcation is often performed on spec-\ntral features that have been averaged over a large num-\nber of audio frames. Many different classiﬁcation strate-\ngies have been employed, including multivariate single\nGaussian models [1], Gaussian mixture models [2], self-\norganising maps [3], neural networks [4], support vec-\ntor machines [5], k-means clustering, k-nearest neighbour\nschemes [1], Hidden Markov Models [6] and supervised\nhierarchical implementations of the aforementioned clas-\nsiﬁers. It has been observed that in several cases, varying\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for proﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2004 Universitat Pompeu Fabra.the speciﬁc classiﬁer used did not affect the classiﬁcation\naccuracy. However, varying the feature sets used for clas-\nsiﬁcation had a far more pronounced effect on the classi-\nﬁcation accuracy[7].\nIn this paper, classiﬁcation is performed on a large num-\nber of short audio frames calculated from a sample with\nthe ﬁnal classiﬁcation being decided by a majority vote.\nWe explore the features calculated from the audio signal,\nthe temporal modelling of those features, and the classify-\ning schemes that have been trained on the resulting data.\nClassiﬁcation results are reported as “bag of frames” ac-\ncuracies, where the audio is segmented into 23ms analysis\nframes and a majority vote is taken to decide the ﬁnal clas-\nsiﬁcation. Finally we introduce new classiﬁers based on\nthe un-supervised construction of a binary decision tree,\nas described in [8], and either linear discriminant analysis\nor a pair of single Gaussians [9] at each node of the tree.\nThe un-supervised construction of a very large ( >5000\nleaf nodes) decision trees for the classiﬁcation of frames,\nfrom musical audio signals, is a new approach, which al-\nlows the classiﬁer to learn and identify diverse groups of\nsounds that only occur in certain types of music. The re-\nsults achieved by these classiﬁers represent a signiﬁcant\nincrease in the classiﬁcation accuracy of musical audio\nsignals.\nIn section 3 we describe the evaluation of different pa-\nrameterisations of the audio signals and the transforma-\ntions used on them. In section 4 the classiﬁers trained\non this data are detailed and two new classiﬁers are intro-\nduced. In section 5 the test data used in the evaluation ex-\nperiments is described, results achieved are discussed. In\nthe ﬁnal sections we detail the conclusions we have drawn\nfrom these results and detail potential areas for further re-\nsearch.\n2. IMPLEMENTATION\nAll of the experiments detailed in this paper were imple-\nmented within the Marsyas-0.1 framework [10].\n3. PARAMETERISATION OF AUDIO SIGNALS\nPrior to classiﬁcation the audio must be segmented and pa-\nrameterised. We have evaluated the classiﬁcation perfor-\nmance of two different measures of spectral shape used\nto parameterise the audio signals, Mel-frequency ﬁlters(used to produce Mel-Frequency Cepstral Coefﬁcients or\nMFCCs) and Spectral Contrast feature. For comparison\ntheGenre feature extractor for Marsyas-0.1 [10], which\ncalculates a single feature vector per piece, is also in-\ncluded in this evaluation.\n3.1. Segmentation\nAudio is sampled at 22050 Hz and the channels averaged\nto produce a monaural signal. Each analysis frame is com-\nposed of 512 individual audio frames, with no overlap,\nrepresenting approximately 23 ms of audio. Therefore the\nlowest frequency that can be represented in an analysis\nframe is approximately 45 Hz which is close to the lower\nthreshold of human pitch perception. No overlap is used\nas additional experiments have shown no gain in accuracy\nfor a 50% overlap, despite doubling the data processing\nload, in an already data intensive task.\n3.2. Mel-Frequency ﬁlters and Cepstral Coefﬁcients\nMel-frequency Cepstral Coefﬁcients (MFCCs) are percep-\ntually motivated features originally developed for the clas-\nsiﬁcation of speech [11]. MFCCs have been used for the\nclassiﬁcation of music in [1], [12] and [10]. MFCCs are\ncalculated by taking the outputs of up to 40 overlapping\ntriangular ﬁlters, placed according to the Mel frequency\nscale, in a manner which is intended to approximately du-\nplicate the human perception of sound through the cochlea.\nThe magnitude of the fast Fourier transform is calculated\nfor the ﬁltered signal and the spectra summed for each ﬁl-\nter, so that a single value is output. This duplicates the out-\nput of the cochlea which is known to integrate the power\nof spectra within critical bands, allowing us to perceive a\ncourse estimate of spectral envelope shape. The Log of\nthese values is then taken, as it is known that perception\nof spectral power is based on a Log scale. These values\nform the ﬁnal parameterisation of the signal but must be\ntransformed by the Discrete Cosine transform [13], in or-\nder to eliminate covariance between dimensions in order\nto produce Mel-Frequency Cepstral Coefﬁcients.\n3.3. Octave-scale Spectral Contrast feature\nIn [14] an Octave-based Spectral Contrast feature is pro-\nposed, which is designed to provide better discrimination\namong musical genres than MFCCs. When calculating\nspectral envelopes, spectra in each sub-band are averaged.\nTherefore only information about the average spectral char-\nacteristics can be gained. However, there is no represen-\ntation of relative spectral characteristics in each sub-band,\nwhich [14] suggests is more important for the discrimina-\ntion of different types of music.\nIn order to provide a better music representation than\nMFCCs, Octave-based Spectral Contrast Feature consid-\ners the strength of spectral peaks and valleys in each sub-\nband separately, so that both relative spectral characteris-\ntics, in the sub-band, and the distribution of harmonic and\nnon-harmonic components are encoded in the feature. In\nmost music the strong spectral peaks tend to correspondwith harmonic components, whilst non-harmonic compo-\nnents (stochastic noise sounds) often appear in spectral\nvalleys [14], which reﬂects the dominance of pitched sounds\nin western music. Whilst it is considered that two spectra\nthat have different spectral distributions may have similar\naverage spectral characteristics, it should be obvious that\naverage spectral distributions are insufﬁcient to differen-\ntiate between the spectral characteristics of these signals,\nwhich can be highly important to the perception of music.\nThe procedure for calculating the Spectral Contrast fea-\nture is very similar to the process used to calculate MFCCs.\nFirst an FFT of the signal is performed to obtain the spec-\ntrum. The spectral content of the signal is then divided\ninto a small number of sub-bands by Octave scale ﬁlters,\nas apposed to the Mel scale ﬁlters used to calculate MFCCs.\nIn the calculation of MFCCs, the next stage is to sum the\nFFT amplitudes in the sub-band, whereas in the calcula-\ntion of spectral contrast, the spectra are sorted into de-\nscending order of strength and then the strength of the\nspectra representing both the spectral peaks and valleys\nof the sub-band signal are recorded. In order to ensure the\nstability of the feature, spectral peaks and valleys are esti-\nmated by the average of a small neighbourhood (given by\nα) around the maximum and minimum of the sub-band.\nFinally, the raw feature vector is converted to the log do-\nmain.\nThe exact deﬁnition of the feature extraction process is\nas follows: The FFT of the k-th sub-band of the audio sig-\nnal is returned as vector of the form {xk,1, xk,2, . . . , x k,N}\nand is sorted into descending order of magnitude , such\nthatxk,1> x k,2> . . . > x k,N. The equations for calcu-\nlating the spectral contrast feature from this sorted vector\nare as follows:\nPeak k= log/parenleftBigg\n1\nαNαN/summationdisplay\ni=1xk,i/parenrightBigg\n(1)\nV alley k= log/parenleftBigg\n1\nαNαN/summationdisplay\ni=1xk,N−i+1/parenrightBigg\n(2)\nand their difference is given by:\nSCk=Peak k−V alley k (3)\nwhere Nis the total number of FFT bins in the k-th sub-\nband. αis set to a value between 0.02 and 0.2, but does not\nsigniﬁcantly affect performance. The raw Spectral con-\ntrast feature is returned as 12-dimensional vector of the\nform {SCk, V alley k}where k∈[1,6]. Although this\nfeature is termed spectral contrast, suggesting that it is\nonly the difference of the peaks and valleys, the ampli-\ntude of the spectral valleys are also returned to preserve\nmore spectral information.\nA signal that returns a high spectral contrast value will\nhave high peaks and low valleys and is likely to represent a\nsignal with a high degree of localised harmonic content. A\nsignal that returns a low spectral contrast will have a lower\nratio of peak to valley strength and will likely represent a\nsignal with a lower degree of harmonic content and greater\ndegree of noise components.3.4. Marsyas-0.1 single vector Genre feature set\nThis dataset has also been classiﬁed by the Genre feature\nset included in Marsyas-0.1 [10], which estimates a sin-\ngle feature vector to represent a complete piece instead\nof a vector for each 23 ms of audio. This feature set in-\ncludes beat, multi-pitch and timbral features in addition\nto MFCCs. The accurate comparison of algorithms in\nthis type of research is difﬁcult as there are currently no\nestablished test and query sets, however the inclusion of\ntheGenre feature set allows comparison between “bag of\nframes” classiﬁers and classiﬁers which average spectral\ncharacteristics across a whole piece.\n3.5. Reducing covariance in calculated features\nThe ﬁnal step in the calculation of a feature set for classi-\nﬁcation is to reduce the covariance among the different\ndimensions of the feature vector. In the calculation of\nMFCCs this is performed by a Discrete Cosine Transform\n(DCT) [13]. However in [14] the calculation of spectral\ncontrast feature makes use of the Karhunen-Loeve Trans-\nform (KLT), which is guaranteed to provide the optimal\nde-correlation of features. Both [14] and [15] suggest that\nthe DCT is roughly equivalent to the KLT in terms of\neliminating covariance in highly correlated signals. The\nde-correlated data from both transformations is output as\na set of coefﬁcients organised into descending order of\nvariance, allowing us to easily select a subset of the co-\nefﬁcients for modelling, which include the majority of the\nvariance in the data. This is known as the energy com-\npaction property of the transformations.\n3.6. Modelling temporal variation\nSimple modelling of the temporal variation of features can\nbe performed by calculating short time means and vari-\nances of each dimension of the calculated features at every\nframe, with a sliding window of 1 second. These means\nand variances are returned instead of the raw feature vec-\ntor and encode a greater portion of the timbral information\nwithin the music. It is thought that this additional informa-\ntion will allow a classiﬁer to successfully separate some\nstyles of music which have similar spectral characteristics,\nbut which vary them differently. This temporal smearing\nof the calculated features also spreads the meaningful data\nin some analysis frames across multiple frames, reducing\nthe number of frames which do not encode any useful in-\nformation for classiﬁcation.\n4. CLASSIFICATION SCHEME\nIn this evaluation musical audio signals were classiﬁed in\nto one of six genres, from which all of the test samples\nwere drawn. The audio signals were converted into fea-\nture vectors, representing the content of the signal, which\nwere then used to train and evaluate a number of differ-\nent classiﬁers. The classiﬁers evaluated were single Gaus-\nsian models (with Mahalanobis distance measurements),\n3 component Gaussian mixture models, Fisher’s CriterionLinear Discriminant Analysis and new classiﬁers based on\nthe un-supervised construction of a binary decision tree\nclassiﬁer, as described in [8], with either a linear discrim-\ninant analysis [9] or a pair of single Gaussians with Ma-\nhalanobis distance measurements used to split each node\nin the tree. We have only evaluated the performance of\n3 component Gaussian mixture models because our ini-\ntial results showed little improvement when the number\nof components was increased to 6, however the amount of\ntime required to train the models increased signiﬁcantly.\n4.1. Classiﬁcation and Regression Trees\nIn [8] maximal binary classiﬁcation trees are built by form-\ning a root node containing all the training data and then\nsplitting that data into two child nodes by the thresholding\nof a single variable, a linear combination of variables or\nthe value of a categorical variable. In this evaluation we\nhave replaced the splitting process, which must form and\nevaluate a very large set of possible single variable splits,\nwith either a linear discriminant analysis or a single Gaus-\nsian classiﬁer with Mahalanobis distance measurements.\nWhen using either linear discriminant analysis or a sin-\ngle Gaussian, to split a node in the tree, the set of possi-\nble splits of data is either the set of linear discrimination\nfunctions or the set of pairs of single Gaussians calculated\nfrom the set of possible combinations of classes. There-\nfore in this implementation, when a node in the classiﬁca-\ntion tree is split, all the possible combinations of classes\nare formed and either the projections and discriminating\npoints calculated or a single Gaussian is calculated for the\ntwo groups. Finally, the success of each potential split is\nevaluated and the combination of classes yielding the best\nsplit is chosen.\n4.1.1. Selecting the best split\nThere are a number of different criterion available for eval-\nuating the success of a split. In this evaluation we have\nused the Gini index of Diversity, which is given by:\ni(t) = 2 p(i|t)p(j|t) (4)\nwhere t is the current node, p(j|t)andp(i|t)are the prior\nprobabilities of the positive and negative classes. The best\nsplit is the split that maximises the change in impurity.\nThe change in impurity yielded by a split s of node t (\n∆i(s, t)) is given by:\n∆i(s, t) =i(t)−PLi(tL)−PRi(tR) (5)\nwhere PLandPRare the proportion of examples in the\nchild nodes tLandtRrespectively. The Gini criterion will\ninitially group together classes that are similar in some\ncharacteristic, but near the bottom of the tree, will prefer\nsplits that isolate a single class from the rest of the data.\nWe have also examined the performance of the Two-\ning criterion [8] for evaluating the success of a split. Our\nresults show that the performance of this criterion was\nnearly identical to that of the Gini criterion, which [8] sug-\ngests is because the performance of a classiﬁcation tree islargely independent of the splitting criterion used to build\nit. In our initial experiments the performance of the Gini\nsplitting criterion was often very slightly higher than that\nof the Two-ing criterion, hence the Gini criterion has been\nused in all subsequent evaluations.\n4.1.2. Building right sized trees and pruning\nIn [8] it is shown that deﬁning a rule to stop splitting nodes\nin the tree, when it is large enough, is less successful than\nbuilding a maximal tree, which will over-ﬁt the training\nset, and then pruning the tree back to a more sensible size.\nThe maximal tree is pruned by selecting the weakest non-\nterminal node in the tree and removing its subtrees. The\nweakest link in the tree is selected by calculating a func-\ntionGfor each non-terminal node in the tree. G is formu-\nlated as follows:\nG(t) =R(t)−R(Tt)\n|˜Tt| −1, t /∈˜T (6)\nwhere R(t)is the re-substitution estimate of node t as a\nleaf node, which is the misclassiﬁcation cost of the train-\ning data if classiﬁed by majority vote at node t,R(Tt)is\nthe re-substitution estimate of the tree rooted at node t, ˜Tt\nis the set of all terminal or leaf nodes in the tree Ttand\n|˜Tt|is the number of leaf nodes in the tree Tt. The node\nthat produces the lowest value of Gin the tree is identiﬁed\nas the weakest link, the whole tree is duplicated and the\nchild nodes of the weakest node are removed. This pro-\ncess is continued until the root node is reached, yielding a\nﬁnite, nested sequence of pruned trees, ranging from the\nmaximal tree to the tree containing only the root node.\nOnce a ﬁnite, nested sequence of pruned trees has been\nproduced, each tree is evaluated against an independent\ntest sample, drawn from the same distribution as the train-\ning data. This allows us to identify trees that over-ﬁt their\ntraining data, as they should return a higher miss-classiﬁcation\nrate than the right-sized tree. Initially the tree with the\nlowest test sample estimate is selected. In order to reduce\ninstability in the selection of the right sized tree, from a\nseries of trees that may have very similar test sample esti-\nmates, the standard error (SE) of the test sample estimate\nis calculated and the simplest tree (smallest number of leaf\nnodes) within 1 standard error of the lowest scoring tree is\nselected as the output tree. The standard error is calculated\nas follows:\nSE=Rts(T) (1−Rts(T))\nN(7)\nwhere Rts(T)is the independent test sample estimate of\nthe misclassiﬁcation cost of tree TandNis the number\nof examples in the test set.\n5. TEST DATA AND CLASSIFICATION RESULTS\nIn this evaluation, we have used six classes of audio, each\nrepresented by 150 samples, which were a 30 second seg-\nment chosen at random from a song, also chosen at ran-\ndom from a database composed of audio identiﬁed by the\nFigure 1 . Bag of frames classiﬁcation accuracies\nauthors as being from that genre of music. The ﬁrst 10\nseconds of each piece is ignored as this sometimes con-\ntains little data for classiﬁcation. The genres selected were\nRock, Classical, Heavy Metal, Drum and Bass, Reggae\nand Jungle music. Parameterisation of this data set yields\napproximately 1.2 million analysis frames for training and\nevaluation. Each experiment was performed 25 times and\nat each iteration, 50% of the data was chosen at random to\nbe used for testing, whilst the other 50% of the data was\nused for training.\nThe styles of music used in this evaluation have been\ndeliberately chosen to produce a reasonably challenging\ndataset for evaluation. Jungle music is considered to be a\nsub-genre of Drum and Bass and is therefore quite similar\nto it and Heavy Metal is often considered to be a sub-genre\nof Rock music and so we should expect to see some con-\nfusion between these two genres. Heavy Metal can also be\nconsidered to be spectrally similar to Drum and Bass, as\nthey have similar ratios of harmonic to non-harmonic con-\ntent and percussive styles. Reggae can often be spectrally\nsimilar to Rock music, however the genres are melodically\nand rhythmically very different. It should also be noted\nthat samples from Reggae music are often used in Jungle\nrecords, and that both the pace and style of the vocal parts\nin the two genres is almost identical, however the tempos\nof the drum tracks in Jungle music are 2 - 4 times as fast\nas those in Reggae.\nIn the ﬁgures, results labelled as GS correspond to the\nsingle Gaussian models, GMM to Gaussian mixture mod-\nels, LDA to Fisher Criterion Linear Discriminant Anal-\nysis, LDA-CART to Classiﬁcation trees with linear dis-\ncriminant analysis and GAUSS-CART to Classiﬁcation\ntrees with single Gaussians and Mahalanobis distance mea-\nsurements.\nThe “bag of frames” classiﬁcation results in ﬁgure 1\nshow that there is little accuracy bonus to be gained through\nthe use of Spectral Contrast feature instead of Mel Fre-\nquency based features. However, when used in conjunc-\ntion with the decision tree classiﬁer, the increase in classi-\nﬁcation accuracy over the Mel-frequency features is highly\nsigniﬁcant (8% for both the raw feature vectors and the\ntemporally modelled feature vectors).Temporal modelling of features increases the classiﬁ-\ncation accuracy of MFCCs by 2 - 6% for ﬂat classiﬁcation\nschemes and 6 - 7% for the decision tree classiﬁers. The\naccuracy increase achieved for Spectral contrast features\nwas 0 - 4% for ﬂat classiﬁcation schemes and 5 - 8% for\nthe decision tree classiﬁers\nIn almost every case the decision tree classiﬁer has\nachieved the greatest increases and has performed better\nthan other models in accuracy, achieving increases of upto\n12% and 21% for the raw MFCCs and temporally mod-\nelled MFCCs respectively, over Gaussian Mixture models.\nThe increases achieved for raw Spectral Contrast feature\nand the temporally modelled version are 20% and 21%\nrespectively.\nThe decision tree classiﬁer based on single Gaussians\nhas consistently performed better than the Linear Discrim-\ninant Analysis based classiﬁer. However, it is interesting\nto note that in our initial experiments the individual frame\nclassiﬁcation accuracy is actually higher for the Linear\nDiscriminant analysis based classiﬁer in almost every case.\nTherefore, confusion must be better spread from the Gaus-\nsian based classiﬁer in order to yield the greater “bag of\nframes” classiﬁcation result.\nWhen the results for “bag of frames” classiﬁcation are\ncompared to the single vector Genre feature extractor in-\ncluded in Marsyas-0.1 [10], it is clear that when using\nﬂat classifying schemes, accuracy with the Genre feature\nset is roughly equal to the accuracy achieved by Spectral\ncontrast feature with temporal modelling. The decision\ntree classiﬁers yield a 4% improvement to the Genre fea-\nture set’s accuracy, however Spectral Contrast feature with\ntemporal modelling and a decision tree classiﬁers beats\nthis by over 16% at 82.79% classiﬁcation accuracy.\n6. CONCLUSIONS\nThe separation of Reggae and Rock music was a particu-\nlar problem for the feature extraction schemes evaluated\nhere, perhaps because they not only share similar spec-\ntral characteristics but also similar ratios of harmonic to\nnon-harmonic content, resulting in virtually no increase in\naccuracy for Spectral Contrast feature. The calculation of\nmeans and variances of the features helped to alleviate this\nconfusion, perhaps by capturing some small amount of\nrhythmic variation in the one second temporal modelling\nwindow. Rock is a form of popular music with a heav-\nily accented beat1whilst Reggae is a style of music with\na strongly accented subsidiary beat2, therefore, in order\nto completely separate Rock and Reggae music we would\nneed to identify and separate the main and subsidiary (On\nand Off) beats, which would require a greater level of\nrhythmic modelling than is performed here, however this\nmaybe approximated by the simple temporal modelling.\nSimilar trends are evident in classiﬁcation of “Drum\nand Bass” music and “Jungle” music. Jungle music is\nclosely related to Drum and Bass Music and is considered\n1http://xgmidi.wtal.de/glossary.html\n2http://simplythebest.net/music/glossaryto be a sub-genre of Drum and Bass music as it has sim-\nilar instrumentation and conforms to the same basic set\nof rhythmic rules, but imposes certain additional rhythmic\nrestrictions. Temporal modelling of these genres achieves\nan increase in group classiﬁcation accuracy but no increase\nin the separation of the two classes. This may be due to the\nabsence of rhythmic modelling, as the two classes are of-\nten only differentiated by the length, complexity and rep-\netition of the clearly deﬁned rhythmic structures.\nThe large increases in accuracy achieved by the Clas-\nsiﬁcation and regression tree classiﬁers may be due to\ntheir ability to represent much more complex distributions\nwithin the data. Because the audio frames in this evalua-\ntion are quite short (23ms in length, which is close to the\nthreshold of pitch/pulse perception) and the data is drawn\nfrom a complex, culturally based distributions, the dis-\ntribution of each class in the feature space maybe very\ncomplex and interwoven with the other classes. The deci-\nsion tree classiﬁer allows the recursive division of the fea-\nture space into an unspeciﬁed number of tightly deﬁned\ngroups of sounds, which better represent the multi-modal\ndistributions within the data. Effective classiﬁcation is\nachieved by identifying groups of sounds which only oc-\ncur in a certain class of music.\nGaussian models with a limited number of components\nare unable to model multi-modal distributions in the data.\nIncreasing the separation of classes within the data by\ntransformation can only be attempted once, and easily sep-\narable or outlier classes can cause other classes to be less\nwell separated. By contrast, a decision tree classiﬁer can\nperform different transformations at each level of the tree\nand is not limited by a ﬁxed number of components.\n6.1. McNemar’s test\nMcNemar’s test [16] is used to decide whether any appar-\nent difference in error-rates between two algorithms, A1&\nA2, tested on the same dataset is statistically signiﬁcant.\nMcNemar’s test is performed by summarising the classi-\nﬁcation results of the two algorithms tested in the form\nof a two by two matrix containing the number of exam-\nples correctly classiﬁed by both algorithms( N00), neither\nalgorithm ( N11) and those only classiﬁed correctly by one\nof the algorithms ( N10&N01). As there is no informa-\ntion about the relative performance of the two algorithms\nwhen they agree, these last two values are the only ones\nused in McNemar’s test. Let H0be the hypothesis that\nthe underlying error-rates are the same. Then under H0\nan error is as likely to be made by A1asA2and the dis-\ntribution of N10&N01is the distribution obtained when\ntossing a fair coin and tails ( N10) is obtained. This is a\nbinomial distribution and the P-values are easily obtained\nfrom tables.\nMcNemar’s test has been applied to one iteration of\neach classiﬁcation algorithm, with the same data and test\nsets. The results are summarised in ﬁgure 2. Results that\nhave a P-value greater than 0.05 are not statistically sig-\nniﬁcant and are shown in white, results with a P-value of\n0.01 to 0.05 are shown in grey and statistically signiﬁcantFigure 2 . Statistical signiﬁcance of classiﬁcation results\nfrom McNemar’s test\nresults, with a P-value of less than 0.01 are shown in black.\nThe algorithms in this ﬁgure have been grouped ac-\ncording to the classiﬁer used. This shows a clear pattern in\nthe results, the accuracy improvements made by the deci-\nsion tree classiﬁer are always statistically signiﬁcant. Ar-\nranging the algorithms according to the feature set used or\nwhether temporal modelling was used, produces no dis-\ncernable pattern, other than a fragmented version of that\nproduced by the classiﬁers. Clearly this indicates that the\nuse of a decision tree classiﬁer has had the most statisti-\ncally signiﬁcant effect on classiﬁcation performance.\n7. FURTHER WORK\nWork in the future will concentrate on investigating meth-\nods of increasing the accuracy of these classiﬁers, includ-\ning: calculating a conﬁdence score for each classiﬁed frame\nand weighting the contribution to ﬁnal classiﬁcation by\nthat score, selecting frames for classiﬁcation, using vari-\nable frame rate or segmentation of the audio signal through\nonset detection and either including rhythmic analysis to\nthe feature set or by adding categorical, rhythmic variable\nsplits to the classiﬁcation trees.\n8. REFERENCES\n[1] George Tzanetakis and Perry Cook, “Musical genre\nclassiﬁcation of audio signals”, IEEE Transactions\non Speech and Audio Processing , 2002.\n[2] George Tzanetakis, Georg Essl, and Perry Cook,\n“Automatic musical genre classiﬁcation of audio sig-\nnals”, in Proceedings of ISMIR 2001: The Inter-\nnational Conference on Music Information Retrieval\nand Related Activities .\n[3] Pedro J Ponce de Le ´on and Jos ´e M Iesta, “Feature-\ndriven recognition of music styles”, Tech. Rep., De-\npartamento de Lenguajes y Sistemas Inform ´aticos,Universidad de Alicante, Ap. 99, E-03080 Alicante,\nSpain, 2003.\n[4] Paul Scott, “Music classiﬁcation using neural net-\nworks”, Tech. Rep., Stanford University, Stanford,\nCA 94305, 2001.\n[5] Changsheng Xu, Namunu C Maddage, Xi Shao,\nFang Cao, and Qi Tian, “Musical genre classiﬁca-\ntion using support vector machines”, Tech. Rep.,\nLaboratories for Information Technology, 21 Heng\nMui Keng Terrace, Singapore 119613, 2003.\n[6] Igor Karpov, “Hidden Markov classiﬁcation for mu-\nsical genres”, Tech. Rep., Rice University, 2002.\n[7] Martin F McKinney and Jeroen Breebaart, “Features\nfor audio and music classiﬁcation”, in Proceedings\nof the Fourth International Conference on Music In-\nformation Retrieval (ISMIR) 2003 .\n[8] Leo Breiman, Jerome H Friedman, Richard A Ol-\nshen, and Charles J Stone, Classiﬁcation and Re-\ngression Trees , Wadsworth and Brooks/Cole Ad-\nvanced books and Software, 1984.\n[9] Andrew Webb, Statistical Pattern Recognition , John\nWiley and Sons, Ltd, 2002.\n[10] George Tzanetakis, “Marsyas: a software frame-\nwork for computer audition”, Web page, October\n2003, http://marsyas.sourceforge.net/.\n[11] S B Davis and P Mermelstein, “Comparison of para-\nmetric representations for monosyllabic word recog-\nnition in continuously spoken sentences”, IEEE\nTransactions on Acoustics, Speech and Signal Pro-\ncessing , 1980.\n[12] Alan P Schmidt and Trevor K M Stone, “Music clas-\nsiﬁcation and identiﬁcation system”, Tech. Rep., De-\npartment of Computer Science, University of Col-\norado, Boulder, 2002.\n[13] N Ahmed, T Natarajan, and K Rao, “Discrete cosine\ntransform”, IEEE Transactions on Computers , 1974.\n[14] Dan-Ning Jiang, Lie Lu, Hong-Jiang Zhang, Jian-\nHua Tao, and Lian-Hong Cai, “Music type classi-\nﬁcation by spectral contrast feature”, Tech. Rep.,\nDepartment of Computer Science and Technology,\nTsinghua University, China and Microsoft Research,\nAsia, 2002.\n[15] Miodrag Potkonjak, Kyosun Kim, and Ramesh\nKarri, “Methodology for behavioral synthesis-based\nalgorithm-level design space exploration: DCT case\nstudy”, in Design Automation Conference , 1997, pp.\n252–257.\n[16] L Gillick and Stephen Cox, “Some statistical is-\nsues in the comparison of speech recognition algo-\nrithms”, in IEEE Conference on Accoustics, Speech\nand Signal Processing , 1989, pp. 532–535."
    },
    {
        "title": "The Influence of Pitch on Melodic Segmentation.",
        "author": [
            "Tillman Weyde"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415556",
        "url": "https://doi.org/10.5281/zenodo.1415556",
        "ee": "https://zenodo.org/records/1415556/files/Weyde04.pdf",
        "abstract": "Melodic segmentation is an important topic for music in- formation retrieval, because it divides melodies into musi- cally relevant units. Most influential theories on melodic segmentation of the last decades have stressed the role of pitch for melodic segmentation. The general assump- tion was, that relatively large changes or distances in any musical parameter like pitch, time, dynamics, or melodic movement mark segment boundaries. This has generally been accepted despite the lack of empirical studies. Here an empirical study is presented, that investigates the influ- ence of inter-onset-intervals (IOI), intensity accents, pitch intervals, and pitch interval direction changes. The results show a significant influence only for IOIs and intensity, but neither for pitch intervals nor for changes in interval direction. The validity of the results and possible explana- tions are discussed and directions of further investigations are outlined. 1",
        "zenodo_id": 1415556,
        "dblp_key": "conf/ismir/Weyde04",
        "keywords": [
            "melodic segmentation",
            "music information retrieval",
            "musically relevant units",
            "pitch",
            "melodic movement",
            "empirical study",
            "influences IOIs",
            "intensity accents",
            "pitch intervals",
            "changes in interval direction"
        ],
        "content": "THEINFLUENCE OF PITCH ON MELODIC SEGMENTATION\nTillmanWeyde\nUniversityof Osnabr ¨uck\nResearchDepartment of Music and Media Technology\ntweyde@uos.de\nABSTRACT\nMelodic segmentation is an important topic for music in-\nformationretrieval,becauseitdividesmelodiesintomusi-\ncally relevant units. Most inﬂuential theories on melodic\nsegmentation of the last decades have stressed the role\nof pitch for melodic segmentation. The general assump-\ntion was, that relatively large changes or distances in any\nmusical parameter like pitch, time, dynamics, or melodic\nmovement mark segment boundaries. This has generally\nbeen accepted despite the lack of empirical studies. Here\nanempiricalstudyispresented,thatinvestigatestheinﬂu-\nenceofinter-onset-intervals(IOI),intensityaccents,pitch\nintervals,andpitchintervaldirectionchanges. Theresults\nshow a signiﬁcant inﬂuence only for IOIs and intensity,\nbut neither for pitch intervals nor for changes in interval\ndirection. Thevalidityoftheresultsandpossibleexplana-\ntionsarediscussedanddirectionsoffurtherinvestigations\nare outlined.\n1 Introduction\nThe segmentation of melodies is an essential part of\nmelody perception and cognition. Melodic segments\nor motifs form the basic elements of melodic struc-\nture, like words are the basic meaningful elements of\nspeech. The importance of segmentation has long been\ndiscussed by theorists (e.g. Riemann,1884), and it has\nbeen the subject of theories ( Lerdahl and Jackendoff ,\n1983,Cooper and Meyer ,1960) and computer models\n(Tenney and Polansky ,1980,Cambouropoulos ,2001,\nTemperley ,2001). As melody comprises pitch, rhythm,\ndynamics, and implicitly harmony, most approaches tried\nto identify inﬂuential factors in all of these domains. The\ngeneralassumptionwasthattheGestaltrulesofproximity\nand similarity are the basis for the introduction of bound-\naries between musical phrases. There are of course other\nfactors, like the recognition of known patterns, closure or\nPr¨agnanz, but their inﬂuence has less often been investi-\ngated (see Bod,2001,Weyde,2002). The idea of treating\nthe different dimensions elegantly by applying the same\nprinciple has lead to some questionable hypotheses.\nThere have been very few empirical studies, mainly to\nevaluate existing segmentation algorithms against the ex-\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopiesbear this notice and the full citation on the ﬁrst page.\nc°2004UniversitatPompeu Fabra.amplesofmusicalsegmentations(see Spevaketal. ,2002).\nThebasicideaofthisworkistosystematicallytesttheef-\nfect of different musical parameters on the perceived seg-\nmentation. For these tests synthetic melodies have been\ngenerated, which vary musical parameters independently\nand systematically.\n2 Experimental Design\nThe experiment has a forced choice design, asking\nsubjects for the length of segments when listening to\na melody. Subjects were presented short melodic\nsequences, which were designed to be completely\nisochronous and uniform, apart from two conﬂicting seg-\nmentation cues, of which one indicated a segmentation\nintogroupsoftwonotesandtheotherintogroupsofthree.\nFour factors were tested: inter-onset-intervals, loud-\nness accents, pitch intervals, and changes in pitch di-\nrection. Some authors suggest to use the intensity\ndifferences instead of accents Lerdahl and Jackendoff\n(1983),butthisapproachwasnotpursuedbecausealready\nWoodrow (1909) had found in his experiments, that rel-\natively loud notes tend to mark the beginning of a new\ngroup, while soft ones do not.\nEachofthefactorswasvariedinﬁvesteps,resultingin\n25 listening samples per pair of factors, and all six possi-\nble pairs of factors were tested resulting in 150 listening\nexamples. This approach of testing pairs of factors was\nchosentoreducethenumberofsamplesintheexperiment,\nwhich would have been 650 for all possible combinations\nof the four factorsin ﬁvesteps.\nThe experiments were conducted in two passes, each\ncoveringthreeofthesixpairs. Thestimuliwerepresented\nviaMIDIwithapianosoundonapersonalcomputerwith\na program, that allowed the subjects to repeat the play-\nback and to make a choice of preferred segmentation into\ngroups or 2 or 3. The melodies comprised twelve notes\nand were presented at a comfortable volume level. The\nsubjectscouldlistenasoftenandlongastheywanted,but\nthey had to make a choice. After the user made a choice,\na break of 3 seconds was introduced to avoid integrated\nperception of the melodies. Within each factor-pair the\nstimuli were presented in random order. In the ﬁrst pass\neightsubjectsparticipated,andnineinthesecondset. All\nsubjects were music students between 21 and 26 years of\nage.\n3 Results\nIn the following, the stimuli and results and a regression\nanalysis of the six factor-pairsare presented.Figure 1.Piano-Roll of an Ioi/Velocity stimulus. Darker\nrectanglesdepictloudernotes. Thebackgroundlatticehas\none horizontal line per semitone and one vertical line per\nsecond.\n 2 2.2 2.4 2.6 2.8 3\n 30\n 60\n 90\n 120\n 150IOI 10 20 30 40 50\nVelocity 2 2.2 2.4 2.6 2.8 3\nFigure 2.Average choice of segment length in the IOI-\nVelocityexperiment.\n3.1 Ioi/Velocity\nIn this set of stimuli, intensity accents were applied ev-\nery three notes. Intensity was realized by using MIDI-\nVelocity, the unaccented value was 70 and the accent\nrange was from 10 to 50. By subjective examination the\nsoundgeneratorrespondedapproximatelylineartotheve-\nlocity values in terms of perceived loudness. Longer IOIs\nwereinsertedbeforeeverythirdnoteintherangefrom30\nto150milliseconds. Avisualizationisshowninﬁgure 1.\nThe average response of the subjects is shown in ﬁg-\nure2. The responses were as could be expected for a cat-\negorization task, showing sensitivity to both factors and\nsaturation effectsfor high velocityand lowIOIs.\nAlogisticregressionwasperformedontheexperimen-\ntal data with the following results shown in table 1. Both\nIOI and velocity have signiﬁcant inﬂuence on the choice\nand the logistic model accounts for 33% of the variance.\n3.2 Ioi/Pitch\nHere pitch changes were inserted every three notes, alter-\nnating upward and downwards. The intervals were in the\nrange form 1 to 5 starting with middle c. The IOIs were\nvariedlikeinthelastexperiment. Avisualizationisshown\nin ﬁgure3.\nThe average response of the subjects is shown in ﬁg-\nure4. It shows clearly that the effect of IOIs is much\nstronger than that of pitch. This is also conﬁrmed by the\nModel Log Likelihood -74.557\nIntercept Log Likelihood -111.355\nR2.330\nCoefStd. Err Coef/SE Â2P-Value\nconst. 1.540 .642 2.399 5.755 .0164\nioi ¡:031 .006 ¡5:17626.790 < :0001\nvelo .102 .018 5.50930.349 < :0001\nTable1.Logisticregressionof IOI/Velocity\nFigure3.Piano-Roll of an Ioi/Pitch stimulus.\n 2 2.2 2.4 2.6 2.8 3\n 30\n 60\n 90\n 120\n 150IOI 1\n 2\n 3\n 4\n 5Interval 2 2.2 2.4 2.6 2.8 3\nFigure 4.Average choice of segment length in the IOI-\nPitch experiment.\nlogistic regression results, showing that IOIs have signiﬁ-\ncant inﬂuence (see table 2).\n3.3 Ioi/Direction\nIn these stimuli there was a pitch change after every note\nwhichchangeddirectioneverythreenotes,alternatingup-\nward and downwards. The intervals were in the range\nform 1 to 5 starting with middle c. The IOIs were varied\nlikein the last set. A visualization is shownin ﬁgure 5.\nTheaverageresponse(ﬁgure 6)showspredominantef-\nfect of IOIs. For an interval of 2 semitones, the direction\nchange has no effect at all. The logistic regression (ta-\nble3) showsthat againonly IOIs havesigniﬁcant effect.\n3.4 Pitch/Direction\nThese stimuli combine the regular intervals changing di-\nrection every three notes with additional intervals every\ntwo notes. Both the regular and the additional intervals\nwere variedfrom 1 to 5 semitones (see ﬁgure 7).\nThe average response (ﬁgure 8) shows no clear ten-\ndency. In the logistic regression (table 4) the inﬂuence\nof the additional intervals is stronger than that of the di-\nrection change, butneither has signiﬁcant effect.\n3.5 Pitch/Velocity\nThese stimuli combine the regular intervals changing ve-\nlocityeverythreenoteswithadditionalintervalseverytwo\nnotes. Both the regular and the additional intervals were\nvariedfrom 1 to 5 semitones (see ﬁgure 9).\nModel Log Likelihood -112.705\nIntercept Log Likelihood -138.469\nR2.186\nCoef.Std. Err Coef/SE Â2P-Value\nconst.2.584 .549 4.71122.190 < :0001\nioi -.027 .004 -6.343 40.231 < :0001\npitch -.013 .115 -.115 .013 .9086\nTable2.LogisticregressionIOI/PitchFigure5.Piano-Rollof an Ioi/Direction stimulus.\n 2 2.2 2.4 2.6 2.8 3\n 1 2 3 4 5\nDir. Int. 30\n 60\n 90\n 120\n 150IOI 2 2.2 2.4 2.6 2.8 3\nFigure 6.Average choice of segment length in the IOI-\nDirection experiment.\nThe average response (ﬁgure 10) shows no clear ten-\ndency. In the logistic regression (table 5) the inﬂuence of\ntheadditionalintervalsisstrongerthanthatofthevelocity\nchange, butneither has signiﬁcant effect.\n3.6 Direction/Velocity\nThese stimuli combine the regular intervals changing ve-\nlocity every three notes with intensity accents every two\nnotes as in the previous examples. A sample is shown in\nﬁgure11.\nThe average response (ﬁgure 12) shows no clear ten-\ndency. In the logistic regression (table 6) the inﬂuence of\ntheadditionalintervalsisstrongerthanthatofthevelocity\nchange, butneither has signiﬁcant effect.\n4 Discussion\nThe results of the pairwise tests show signiﬁcant effects\nfor velocity and IOIs, but not for pitch or change of pitch\ninterval direction. This result is consistent over all pairs\nof factors. This contrasts to most currently accepted the-\nories on melodic perception. There are several possible\nexplanationsfor this effect,with differentconsequences.\n4.1 Experimental Design Considerations\nIn the results the level of variance accounted for by the\nfactors in the model is not very high. So there may be\nother factors, that inﬂuence the decision. One possibil-\nity is that the pauses between the examples did not suf-\nﬁce to prevent the establishing of a metrical structure in\nthelistenerthatcouldbiastheperceptionofthenextsam-\nple. The perception of melodic segmentation can to some\nModel Log Likelihood -120.305\nIntercept Log Likelihood -132.459\nR2.092\nCoef.Std. Err Coef/SE Â2P-Value\nconstant .842 .483 1.742 3.036 .0814\nDir -.100 .112 -.889 .791 .3737\nIOI -.018 .004 -4.549 20.697 < :0001\nTable3.Logistic regressionIOI/Direction\nFigure7.Piano-Rollof a Pitch/Direction stimulus.\n 2 2.2 2.4 2.6 2.8 3\n 1 2 3 4 5\nDir. Int.\n 1 2 3 4 5\nAdd. Int. 2 2.2 2.4 2.6 2.8 3\nFigure 8 .Average choice of segment length in the\nPitch/Direction experiment.\ndegree be controlled by the listener, therefore personal or\nmomentarypreferencesmayaccountforapartofthevari-\nance. There are obviously other factors like harmony and\ntempo which can have an inﬂuence on the segmentation.\nMostimportanttomentionhereistherecognitionofhigh-\nlevel structures (e.g. repetitions, harmonic progression)\nandtherecognitionofpatternsknownformothercontexts.\nAnother argument against the validity of the experi-\nmental results could be, that the stimuli were synthetic,\nand that perception may be different in the context of real\nmusic. Evaluatingsegmentationinthecontextofrealmu-\nsic is difﬁcult, because of the interaction of multiple fac-\ntors and measuring the similarity of segmentations (see\nSpevak et al. ,2002). On the other hand, if the rules like\ntheGroupingPreferenceRules ofLerdahlandJackendoff\n(1983) were general laws of perception, they should have\neffectin synthetic settings, too.\n4.2 The Role of Pitch in Melodic Segmenta-\ntion\nAfter all, the results are consistent throughout the tests,\nshowing signiﬁcant inﬂuence of pitch and velocity. It\nseems therefore justiﬁed to conclude that rhythm and dy-\nnamicshaveaconsiderablystrongerinﬂuenceonsegmen-\ntation than pitch interval size and direction changes. At\nleastconsideringlinearandloglineareffectsintheranges\ntested here.\nIt seems plausible that pitch does have a signiﬁcant ef-\nfect on melodic segmentation, since it plays such an im-\nportantroleinmelodyandmusicingeneral. Itispossible\nthat only large intervals have an effect and that the range\nModel Log Likelihood -133.649\nIntercept Log Likelihood -135.725\nR2.015\nCoef.Std. Err Coef/SE Â2P-Value\nconstant .226 .455 .497.247 .6194\nint .167 .103 1.6232.635 .1046\ndir -.126 .103 -1.223 1.496 .2213\nTable4.Logistic regressionPitch/DirectionFigure9.Piano-Roll of a Pitch/Velocitystimulus.\n 2 2.2 2.4 2.6 2.8 3\n 1 2 3 4 5\nInterval 10\n 20\n 30\n 40\n 50Velocity 2 2.2 2.4 2.6 2.8 3\nFigure 10 .Average choice of segment length in the\nPitch/Velocityexperiment.\noftheintervalsusedweretoosmall. Anotherpossibilityis\nthattherearemorecomplexeffectswhicharenotcaptured\nby the regressionanalyses.\nTobringmorecertaintyintothisissue,itwouldbeuse-\nful, to conduct further experiments. The current experi-\nmentdoeshavesomeshortcoming,asitwasnotdesigned\nto prove the general signiﬁcance of pitch to melodic seg-\nmentation. To make sure that there is no effect from one\npresentation to other, one could play some noise inbe-\ntween. It is also necessary to have examples which have\nonly one segmentation factor, and to vary the length of\nthe segmentation. These extension of the experiment will\nrequire a larger number of tests, with larger numbers of\nsubjects.\n5 Conclusion\nThe results of the experiment described here indicate that\nthe size of pitch intervals have little inﬂuence on melodic\nsegmentation compared to timing and dynamics. It this\nwere conﬁrmed by more data, it would have important\nconsequencesformusicinformationretrieval. Inthatcase,\nsegmentation algorithms could largely ignore pitch inter-\nval size. Instead other models on how pitch inﬂuences\nmelodicsegmentationwouldhavetobeevaluated. Fores-\ntablishing certainty on this question, further experiments\nare needed.\nReferences\nBod, R. (2001). Memory-based models of melodic analysis:\nChallenging the gestalt principles. Journal of New Music Re-\nModel Log Likelihood -115.836\nIntercept Log Likelihood -148.659\nR2.221\nCoef.Std. Err Coef/SE Â2P-Value\nconstant 2,247 ,522 4,30718,551 <;0001\nPitch -,065 ,114 -,570 ,325 ,5689\nVelo -,092 ,013 -6,910 47,749 <;0001\nTable5.LogisticregressionPitch/Velocity\nFigure11 .Piano-Rollof a Direction/Velocitystimulus.\n 2 2.2 2.4 2.6 2.8 3\n 1 2 3 4 5\nDir. Int. 10\n 20\n 30\n 40\n 50Velocity 2 2.2 2.4 2.6 2.8 3\nFigure 12 .Average choice of segment length in the Di-\nrection/Velocityexperiment.\nsearch,30(3).\nCambouropoulos, E. (2001). The local boundary detection\nmodel (lbdm) and its application in the study of expressive\ntiming. In Proceedings of the International Computer Music\nConference2001 , pages 290–293, Havanna,Cuba.\nCooper,G.W.andMeyer,L.B.(1960). TheRhythmicStructure\nof Music. Universityof Chicago.\nLerdahl, F. and Jackendoff, R. (1983). A Generative Theory of\nTonalMusic . The MIT Press, Cambridge, Mass.\nRiemann, H. (1884). Musikalische Dynamik und Agogik . Ham-\nburg,Leipzig, St. Petersburg.\nSpevak, C., Thom, B., and H ¨othker, K. (2002). Evaluating\nmelodicsegmentation. InAnagnostopoulou,C.,Ferrand,M.,\nandSmaill,A.,editors, MusicandArtiﬁcialIntelligence.Pro-\nceedings of the Second International Conference on Music\nand AI, volume 2445 of Lecture Notes on Artiﬁcial Inetelli-\ngence,pages 168–182. Springer-Verlag.\nTemperley, D. (2001). The Cognition of Basic Musical Struc-\ntures. The MIT Press, Cambridge, Mass.\nTenney, J. and Polansky, L. (1980). Temporal gestalt perception\nin music. Journalof Music Theory , 24(2):205–241.\nWeyde, T. (2002). Integrating segmentation and similarity in\nmelodic analysis. In Stevens, K., Burnham, D., McPherson,\nG., Schubert, E., and Renwick, J., editors, Proceedings of the\nInternationalConferenceonMusicPerceptionandCognition\n2002, pages 240–243, Sydney, Australia. University of New\nSouth Wales.\nWoodrow, H. (1909). A quantitative study of rhythm. Archives\nof Psychology ,14:1–66.\nModel Log Likelihood -144.789\nIntercept Log Likelihood -151.824\nR2.046\nCoef.Std. Err Coef/SE Â2P-Value\nconstant .215 .435 .493 .244 .6217\nDir .137 .100 1.378 1.899 .1681\nVelo -.035 .010 -3.413 11.650 .0006\nTable6.LogisticregressionDirection/Velocity"
    },
    {
        "title": "Automatic Record Reviews.",
        "author": [
            "Brian Whitman",
            "Dan Ellis"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416646",
        "url": "https://doi.org/10.5281/zenodo.1416646",
        "ee": "https://zenodo.org/records/1416646/files/WhitmanE04.pdf",
        "abstract": "Record reviews provide a unique and focused source of linguistic data that can be related to musical recordings, to provide a basis for computational music understanding systems with applications in similarity, recommendation and classification. We analyze a large testbed of music and a corpus of reviews for each work to uncover pat- terns and develop mechanisms for removing reviewer bias and extraneous non-musical discussion. By building upon work in grounding free text against audio signals we in- vent an “automatic record review” system that labels new music audio with maximal semantic value for future re- trieval tasks. In effect, we grow an unbiased music editor trained from the consensus of the online reviews we have gathered. Keywords: cultural factors, language, machine learn- ing, audio features, reviews",
        "zenodo_id": 1416646,
        "dblp_key": "conf/ismir/WhitmanE04",
        "keywords": [
            "language",
            "machine learning",
            "audio features",
            "reviews",
            "cultural factors",
            "audio signals",
            "free text",
            "unbiased music editor",
            "grounding",
            "semantic value"
        ],
        "content": "AUTOMATIC RECORD REVIEWS\nBrian Whitman\nMIT Media Lab\nMusic Mind and Machine GroupDaniel P.W. Ellis\nLabROSA\nColumbia University Electrical Engineering\nABSTRACT\nRecord reviews provide a unique and focused source of\nlinguistic data that can be related to musical recordings,\nto provide a basis for computational music understanding\nsystems with applications in similarity, recommendation\nand classiﬁcation. We analyze a large testbed of music\nand a corpus of reviews for each work to uncover pat-\nternsanddevelopmechanismsforremovingreviewerbias\nandextraneousnon-musicaldiscussion. Bybuildingupon\nwork in grounding free text against audio signals we in-\nvent an “automatic record review” system that labels new\nmusic audio with maximal semantic value for future re-\ntrieval tasks. In effect, we grow an unbiased music editor\ntrained from the consensus of the online reviews we have\ngathered.\nKeywords: cultural factors, language, machine learn-\ning, audio features, reviews\n1. INTRODUCTION\nSpreadthroughoutthemusicreviewpagesofnewspapers,\nmagazines and the internet lie the answers to music re-\ntrieval’s hardest problems of audio understanding: thou-\nsands of trained musical experts, known otherwise as re-\nviewers, distill the hundreds of megabytes of audio data\nfrom each album into a few kilobytes of semantic classi-\nﬁcation. Instead of the crude and suspect genre tags and\nartist names that so often serve as semantic ground truth,\nwe can get detailed descriptions of the audio content (in-\nstrumentation, beat, song structure), cultural position (re-\nlationships to other groups, buzz, history) and individual\npreference (the author’s opinion of the work). There is\ntremendous value waiting to be extracted from this data,\nas the ostensible purpose of a record review is to provide\nall the necessary categorical and descriptive information\nfor a human judge to ‘understand’ the recording without\nhearing it. If we would like to build music intelligences\nthat automatically classify, recommend and even synthe-\nsize music for listeners, we could start by analyzing the\nconnection between music (or music-derived audio fea-\ntures) and a listener’s reaction as detailed in a review.\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopies bear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2004 Universitat Pompeu Fabra.\nAMG RatingsPitchfork Ratings\n246820406080100\nRandomly selected AMG RatingsPitchfork Ratings\n246820406080100\nAMG RatingsAudio−derived Ratings\n24682468\nPitchfork RatingsAudio−derived Ratings\n204060801002040608010024681012\n246810\n123456\n20406080100120Figure 1. Predicting and analyzing ratings. Top left: cor-\nrelationofAMG(1-9scale)toPitchfork(0-100scale)rat-\nings, correlation coefﬁcient r= 0.264. Top right: Pitch-\nforkratingstorandomlyselectedAMGratings, r= 0.017\nfor this instance. Bottom left: predicting AMG ratings\nfrom audio features, r= 0.147. Bottom right, predicting\nPitchfork ratings from audio, r= 0.127.\nA system for “review understanding” is useful even to\ntext-only retrieval systems: Consider a site that encour-\nages on-line reviews of its stock; user-submitted text can\nbe used in place of a sales-based collaborative ﬁltering\nrecommendation agent, and such systems prove to work\nwell as “buzz” or opinion tracking models[1]. However,\nin our case we are fortunate to have the subject of the re-\nviews – the music audio itself – simultaneously available,\nandourworkconcentratesonthelinkbetweendescription\nand perception. We believe an audio model of ‘romantic\ninterludes’ can be far more expressive, informative, and\nstatistically valid than a model of ‘Rock’ – and given the\nprospect of scaling our models to hundreds of thousands\nof terms and phrases applicable to every kind of music,\nweenvisionabias-freecomputationalmodelofmusicde-\nscription that has learned everything it knows by reading\nreviews and listening to the targets.\nOf course, reviews have their problems. By their na-\nture they are hardly objective – the author’s own back-\nground and musical knowledge color each review. As\nFigure 2 illustrates, music reviews can often be cluttered\nwith ‘outside-world’ information, such as personal rela-\ntionships and celebrity trivia. While these non-musical\ntidbits are entertaining for the reader and sometimes (ifFor the majority of Americans, it's a given: summer is the best season of the year. Or so you'd think, judging from the anonymous TV ad men and women who proclaim, \"Summer is here! Get your [insert iced drink here] now!\"-- whereas in the winter, they regret to inform us that it's time to brace ourselves with a new Burlington coat. And TV is just an exaggerated reflection of ourselves; the hordes of convertibles making the weekend pilgrimage to the nearest beachare proof enough. Vitamin D overdoses abound. If my tone isn't suggestive enough, then I'll say it flat out: I hate theBeginning with \"Caring Is Creepy,\" which opens this album with a psychedelic flourish that would not be out of place on a late-1960s Moody Blues, Beach Boys, or Love release, the Shins present a collection of retro pop nuggets that distill the finer aspects of classic acid rock with surrealistic lyrics, independently melodic bass lines, jangly guitars, echo laden vocals, minimalist keyboard motifs, and a myriad of cosmic sound effects. With only two of the cuts clocking in at over four minutes, Oh Inverted World avoids the penchant for self-indulgence that befalls most outfits who worship at the Figure 2. The ﬁrst few lines of two separate reviews of the same album (The Shins’ “Oh Inverted Word.”) Top: Ryan\nKearney, Pitchforkmedia.com. Bottom: Tom Semioli, All Music Guide.\nobliquely) give a larger picture of the music in question,\nourcurrentpurposewouldbebestservedbymoreconcise\nreviews that concentrated on the contents of the album so\nthatourmodelsofmusicunderstandingandsimilarityare\ndealing with purely content-related features.\nIn this paper we study a large corpus of music audio\nand corresponding reviews as an exploratory work into\nthe utility of music reviews for retrieval tasks. We are\nspeciﬁcally interested in the problems of similarity and\nrecommendation, and view the review parsing and term\ngrounding work in this paper as a necessary step to gath-\nering the knowledge required to approximate human mu-\nsical intelligence. For example, by limiting reviews to\n‘musically salient’ terms grounded by our learning sys-\ntem,acommunity-opinionmodelofsimilarity,basedonly\non text, can be built with highaccuracy.\nWeﬁrstpresentacomputationalrepresentationofpars-\ning for descriptive text and an audio representation that\ncaptures different levels of musical structure. We then\nshow methods for linking the two together, ﬁrst to create\nmodelsforeachtermthatcanbeevaluated,butalsotocull\nnon-musical and biased information from reviews. We\nalso show results in classifying the author’s overall opin-\nion of the work, as expressed in symbolic “star-rating”\nattributesprovidedbythereview,bylearningtherelation-\nshipbetweenthemusicanditsﬁtnessscore. Puttingthese\napproaches together opens the door to an on-line “auto-\nmaticrecordreview”thatcanclassifynewmusicwithnu-\nmeroushuman-readableandunderstandablelabels. These\nlabelscanbeuseddirectlyinaninterfaceorusedasinputs\ntosubsequentsimilarity,classiﬁcationorrecommendation\nsystems.\n2. BACKGROUND\nOur work has concentrated on extracting meaning from\nmusic, using language processing and data mining tech-\nniquestouncoverconnectionsbetweentheperception(au-\ndiostream)anddescription. Manyinterestingresultshave\narisen from this work, including models of metadata de-\nrived from musical communities [2], a “query by descrip-\ntion”systemthatallowsusersanaturalinterfaceformusicretrieval[3],andanewmethodof semanticrankreduction\nwhere the observations are de-correlated based on mean-\ningratherthanpurelystatistics[4]. Byassociatinglistener\nreactions to music (observed through many mechanisms\nfromplayerlogsthroughtopublishedreviews)withanal-\nyses of the audio signal, we can automatically infer novel\nrelations on new, “unheard” music. This paper ties some\nof these threads together for a an approach to extracting\nreliable, consensus information from disparate online re-\nviews.\n2.1. Related Work\n2.1.1. Music Analysis\nSystems can understand music enough to classify it by\ngenre, style, or nationality, as long as the systems are\ntrained with hand-labeled data e.g. [5, 6]. The link be-\ntweenmusicalcontentandgeneralizeddescriptivelanguage\nis not as prominent, although [7] shows that certain style-\nrelated terms such as ‘lyrical’ or ‘frantic’ can be learned\nfrom the score level.\n2.1.2. Grounding\nIn the domain of general audio, recent work has linked\nsound samples to description using the labeled descrip-\ntions on the sample sets [8]. In the visual domain, some\nwork has been undertaken attempting to learn a link be-\ntweenlanguageandmultimedia. Thelexicon-learningas-\npectsin[9]studyasetofﬁxedwordsappliedtoanimage\ndatabase and use a method similar to EM (expectation-\nmaximization) to discover where in the image the terms\n(nouns) appear; [10] outlines similar work. Regier has\nstudied the visual grounding of spatial terms across lan-\nguages, ﬁnding subtle effects that depend on the relative\nshape, size, and orientation of objects [11]. In [15], as-\npects of learning shape and color terms were explored\nalongwithsomeoftheﬁrststepsinperceptually-grounded\ngrammar acquisition.3. THE “MIT AUDIO+AUDIENCE” TESTBED\nThesetofmusicusedinthisarticleandelsewhereisbased\non the Minnowmatch testbed [17] extended with a larger\nvariety of music (instead of just pop) by removing the\npopularity constraint. (Minnowmatch’s music was culled\nfrom the top 1,000 albums on a peer-to-peer network.)\nWe have also added a regularized set of cultural meta-\ndata for each artist, album, and song. In this paper we\nreport results on a set of 600 albums from roughly 500\nartists. Each artist has concordant community metadata\nvectors [2] and each album has at least two reviews, one\nfrom the metadata provider All Music Guide [18] (AMG)\nandonefromthepopularrecordreviewandmusicculture\nweb site Pitchfork Media [19] (Pitchfork). Most records\nalso have tagged community reviews from other sources,\nsuchason-linerecordstores. Othersourcesofcommunity\ninformation in this testbed include usage data and artist\nsimilarity results from the Musicseer [20] survey.\n4. READING THE REVIEWS\nThere are innumerable ways of representing textual in-\nformation for machine understanding, and in our work\nwe choose the simplest and most proven method of fre-\nquency counting. Reviews are in general short (one to\nthree paragraphs), are always connected to the topic (al-\nthough not always directly) and do not require special\nparsing or domain-speciﬁc tools to encode. In our re-\ncent work we used a very general model of community\nmetadata [2]whichcreatesamachineunderstandablerep-\nresentation of artist description by searching the Internet\nfor the artist name and performing natural language pro-\ncessing on the retrieved pages. Since those results were\nnaturally noisier (all text on a web page vs. a succinct\nset of three paragraphs) we needed various post-crawling\nprocessing tricks to clean up the data. In this experiment\nwe borrow tools and ideas from the community metadata\ncrawler but mostly rely on simple information retrieval\ntechniques.\nThereviewsweredownloadedusingaspecializedcrawler\nand added to the Audio+Audience testbed. We chose 600\nalbums, two reviews for each (AMG and Pitchfork) to\nuse later in interrater studies and as an agreement mea-\nsure. All markup was removed and each review is split\ninto plaintext sentences. We decompose the reviews into\nn-grams (terms of word length n), adjective sets (using\na part-of-speech tagger [21]) and noun phrases (using a\nlexical chunker [22]). We compute the term frequency of\neach term as it occurs in a review i.e. if there were 50 ad-\njectives in a review of an album, and loudappeared ﬁve\ntimes,loud’stfis 0.1.We then compute global document\nfrequencies (if loudoccurred in 30 of the 600 reviews, its\nd fwould be 0.05).\nEach pair {review, term }retrieved is given an asso-\nciatedsalienceweight,whichindicatestherelativeimpor-\ntanceof termasassociatedtothe review. Thesesaliences\nare computed using the TF-IDF measure; simply tf/d f.The intuition behind TF-IDF is to reward words that oc-\ncur frequently in a topic but not overall. For example, the\ntermguitarsmight have a high tffor a rock review but\nalso has a high d fin general; this downweights it. But\nelectric banjo has a high tffor particular reviews and a\nlowd f, which causes it to have a high salience weight.\nWe limit the {review, term }pairs to terms that occur in\nat least three reviews so that our machine learning task is\nnot overwhelmed with negative bias. See Table 1 for ex-\nample top-scoring salience terms. We make use of these\nTF-IDF salience scores as a metric to only allow certain\nterms to be considered by the machine learning systems\nthat learn the relationship between terms and audio. We\nlimit terms by their d foverall and then limit ‘learnable’\ntermsbytheirspeciﬁcTF-IDFperalbumreview. Previous\nwork [3] directly used the TF-IDF scores as a regression\ntarget in the learning system; we found this to lessen ac-\ncuracyasTF-IDFdoesnothavegoodnormalizingmetric.\nWe also parse the explicit ratings of each album in our\ncollection. Pitchfork rates each record on a (0..10) scale\nwithdecimals(for100steps),whileAMGusesastarsys-\ntem that has 9 distinct granulations.\nOurchoiceofAMGandPitchforkasourreviewsources\nwasnotaccidental: weselectedthemastwooppositepoles\nin the music criticism space. AMG is a heavily edited\nmetadata source whose reviews are consistently concise,\nshort and informational. Pitchfork’s content is geared to-\nwards a younger audience and more ‘buzz-friendly’ mu-\nsic, acting as more of a news site than a review library.\nThetoneofthelatterisveryinformalandnotveryconsis-\ntent. This makes Pitchfork our self-selected ‘worst case\nscenario’ for ground truth as our results later show– the\nratings and reviews have little representation in the audio\nitself. Likewise,AMGactsasabestcaseandoursystems\nhave an easier time linking their descriptions and ratings\nto music. Nonetheless the two sources serve to comple-\nment each other. There is much to music outside the sig-\nnal, and the culture and buzz extracted from Pitchfork’s\nreviews could be extracted and represented for other pur-\nposes.\n5. LISTENING TO THE MUSIC\n5.1. The “Penny” Cepstral Features\nA number of subproblems arise when attempting to dis-\ncover arbitrary lexical relations between words and mu-\nsic. Theforemostproblemisoneofscale: anylexicalunit\nattached to music can agree with the entire artist (long-\nterm scale), just an album, just a song or piece, or per-\nhaps a small part of the song. Even lower-level are re-\nlations between descriptions and instruments, or ﬁlters or\ntones(“Thissoundisdark,”or“theseguitarsaregrating.”)\nTheproblemsarefurtherexacerbatedwhenmostmachine\nlearning systems treat observations as unordered frames.\nWe are looking for a model of auditory perception that\nattempts to simultaneously capture many levels of struc-\nturewithinamusicalsegment,butdoessowithoutexperi-\nmenterbiasorsupervisionguidance. AcommondownfallHrvatski noun phrases adjectives Richard Davies noun phrases adjectives Hefner noun phrases adjectives\nswarm & dither processed telegraph creatively hefner disenchanted\nhrvatksi feedback richard davies unsuccessful the indie rock trio contemptuous\nsynth patch composed eric matthews and cardinal instrumentalist such a distant memory fashionable\nbaroque symphonies glitch the moles australian an emo band beloved\nold-fashioned human emotion psychotic poetic lyrics quieter guitars and pianos puzzling\npolyrhythmic pandemonium cheerful the kinks surface terriﬁc some humor nasal\nhis breaks fascination crazed his most impressive record reﬂective singer darren hayman ugly\nTable 1. Selected top-scoring nounphrase and adjective terms (TFIDF) from three combinedrecord reviews.\n2\n4\n6\n2\n4\n6\n2\n4\n6\n2\n4\n6\n2\n4\n6\nFrames100 200 300 400 500 600 700 800 9002\n4\n625-50 Hz12-25 Hz6-12 Hz3-6 Hz1.5-3 Hz0-1.5 HzModulation\nrange\nFigure 3. The “Penny” cepstral features for generalized\nsemantic analysis of audio. Six levels of structure are de-\ncoded for this song (“A Journey to Reedham” by Square-\npusher), corresponding to different ranges of modulation\nfrequencies.\nof many heuristically musical feature encodings is their\nreliance on the observation being “cleanly musical” – for\nexample,apitchandbeatbasedfeatureencodingdoesnot\ngeneralizewelltonon-tonalmusicorfreeformpieces. We\nwouldalsolikeourlearningalgorithmtobeabletohandle\ngeneralized sound.\nOur previous work [3] uses a very low-level feature\nof audio, the power spectral density (PSD) at 512 points.\nRoughly, a PSD is the mean of STFT bins over some pe-\nriod of time (we used 4 seconds in our work). While our\nresults were encouraging, we ran up against problems of\nscale in trying to increase our generalization power. As\nwell, we were not capturing time-dependent information\nsuch as “faster” or “driving.” We also attempted to use\ntheMPEG-7time-awarestatepathrepresentationofaudio\nproposed in [23] which gave us perceptibly more “musi-\ncal” results but still did not allow for varying levels of\nmusical structure.\nOur new feature space, nicknamed “Penny” is based\non the well known Mel-frequency Cepstral Coefﬁcients\n(MFCCs) from speech recognition. We take MFCCs at\na 100 Hz sample rate, returning a vector of 13 bins per\naudio frame. We then stack successive time samples for\neach MFCC bin into 64 point vectors and take a second\nFourier transform on these per-dimension temporal en-\nergy envelopes. We aggregate these results into 6 octavewide bins to create a “modulation spectrum” showing the\ndominantscalesofenergyvariationforeachcepstralcom-\nponent over a range of 1.5 Hz to 50 Hz. The result is\nsix matrices (one for each modulation spectrum octave)\neach containing 13 bins of cepstral information, sampled\nat, for instance, 10 Hz (to give roughly 70% overlap be-\ntween successive modulation spectral frames). The ﬁrst\nmatrixgivesinformationaboutslowvariationsinthecep-\nstral magnitudes, indicating things like song structure or\nlarge changes in the piece, and each subsequent matrix\nconcentratesonhigherfrequenciesofmodulationforeach\ncepstral coefﬁcient. An example set of six matrices from\nthe Penny analysis can be seenin Figure 3.\n6. LEARNING THE LANGUAGE\nIn this section we discuss the machinery to learn the rela-\ntion between the audio features and review text. The ap-\nproach we use is related to our previous work, where we\npose the problem as a multi-class classiﬁcation problem.\nIn training, each audio feature is associated with some\nsalience weight of each of the 5,000 possible terms that\nour review crawler discovered. Many of these classes are\nunimportant (as in the case of terms such as ‘talented’ or\n‘cool’– meaningless to the audio domain). We next show\nourattemptatsolvingthesesortsofproblemsusingaclas-\nsiﬁer technique based on support vector machines [24].\n6.1. Regularized Least-Squares Classiﬁcation\nRegularizedLeast-SquaresClassiﬁcation[25]requiressolv-\ningasinglesystemoflinearequationsafterembeddingthe\ndata in a kernel space. Recent work [26, 25] has shown\nthat the accuracy of RLSC is essentially identical to that\nof the closely related support vector machine, but at a\nfraction of the computational cost. We arrange our au-\ndio observations in a kernel-space gram matrix K, where\nKij≡Kf(xi, xj), a generalized dot product between xi\nandxj. Thus, if the generalized dot product is considered\nasimilarityfunction,thegrammatrixcompareseachpoint\nagainst every other in the example space. We usually use\nthe Gaussian kernel,\nKf(x1, x2) =e−(|x1−x2|)2\nσ2 (1)\nwhere |x−y|is the conventional Euclidean distance be-\ntween two points, and σis a parameter we keep at 0.5.Training an RLSC system consists of solving the sys-\ntem of linear equations\n(K+I\nC)c=y, (2)\nwhere Kisthekernelmatrix, cisaclassiﬁer‘machine,’ y\nis the truth value, and Cis a user-supplied regularization\nconstantwhich we keep at 10.1The crucial property of\nRLSC for this task is that if we store the inverse matrix\n(K+I\nC)−1, then for a new right-hand side y(i.e. a new\nset of truth term values we are trying to predict), we can\ncomputethenewclassiﬁer cviaasimplematrixmultipli-\ncation. Thus,RLSCisverywell-suitedtoproblemsofthis\nscale with a ﬁxed set of training observations and a large\nnumber of target classes, some of which might be deﬁned\nafter the initial analysisof the training points.\nTo compute a set of term classiﬁers for audio observa-\ntions (i.e. given an audio frame, which terms are associ-\nated and with what magnitude?) we form a kernel-space\ngram matrix from our Penny features, add the regulariza-\ntion constant, and invert. We then multiply the resultant\nmatrix by a set of ‘term truth vectors’ derived from the\ntraining data. These are vectors with one value for each\noftheexamplesinthetrainingkernelmatrix,representing\nthe salience (from the TF-IDF computation) of that term\nto that audio frame.2This multiplication creates a ‘ma-\nchine’ cwhich can then be applied to the test examples\nfor evaluation.\n7. EXPERIMENTS\nWeconductedasetofexperiments,ﬁrsttestingourfeature\nextraction and learning algorithms’ capability to general-\nize a review for a new piece of music, then using the pre-\ncisionofeachtermmodeltocullnon-musical(unground-\nable) phrases and sentences from reviews, and lastly try-\ningtolearntherelationshipbetweenaudioandreviewrat-\ning. Each task runs up against the problem of ground\ntruth: our models are trained to predict very subjective\ninformation described only through our own data. We\ndiscuss each experiment below with directions into future\nwork.\n7.1. Learning Results\nTo generate reviews automatically from audio we must\nﬁrst learn a model of the audio-to-term relations. We ex-\ntract textual features from reviews for noun phrase and\nadjective types as above and then compute the Penny fea-\nture space on our set of 600 albums, choosing four songs\nat random from each. (We start with MP3 audio and con-\nvert to mono and downsample to 11 kHz.) We use the\n1We arrived at 0.5 for σand 10 for Cafter experimenting with the\nPenny features’ performance on an artist identiﬁcation task, a similar\nmusic-IR problem with better ground truth.\n2We treat all audio frames derived from an album the same in this\nmanner. If a review claims that “The third track is slow and plodding”\nthis causes every frame of audio derived from that album to be consid-\nered slow and plodding.lowest two modulation frequency bins of the Penny fea-\nture across all cepstra for a feature dimension of 26. We\nusea10Hzfeatureframeratethatisthendownsampledto\n1 Hz. We split the albums into testing and training, with\nhalf of the albums in each. Using the RLSC method de-\nscribedabovewecomputethegrammatrixonthetraining\ndataandtheninvert,creatinganew cforeachterminour\nreview corpus.\n7.2. Evaluation of Predicted Terms\nTo evaluate the models on new albums we compute the\ntestinggrammatrixandcheckeachlearned cagainsteach\naudio frame in the test set.\nWe used two separate evaluation techniques to show\nthestrengthofourtermpredictions. Onemetricistomea-\nsure classiﬁer performance with the recall product P(a):\nifP(ap)is the overall positive accuracy (i.e. given an\naudio frame, the probability that a positive association\nto a term is predicted) and P(an)indicates overall nega-\ntive accuracy, P(a)is deﬁned as P(ap)P(an). This mea-\nsure gives us a tangible feeling for how our term mod-\nels are working against the held out test set and is use-\nful for grounded term prediction and the review trimming\nexperiment below. However, to rigorously evaluate our\ntermmodel’sperformanceinareviewgenerationtask,we\nnote that this value has an undesirable dependence on the\nprior probability of each label and rewards term classi-\nﬁerswithaveryhighnatural d f,oftenbychance. Instead,\nfor this task we use a model of relative entropy, using the\nKullback-Leibler (K-L) distance to a random-guess prob-\nability distribution.\nWe use the K-L distance in a two-class problem de-\nscribed by the four trial counts in a confusion matrix:\n“funky” “not funky”\nfunky a b\nnot funky c d\naindicates the number of frames in which a term classi-\nﬁer positively agrees with the truth value (both classiﬁer\nand truth say a frame is ‘funky,’ for example). bindicates\nthe number of frames in which the term classiﬁer indi-\ncates a negative term association but the truth value indi-\ncates a positive association (the classiﬁer says a frame is\nnot‘funky,’buttruthsaysitis). Thevalue cistheamount\nofframesthetermclassiﬁerpredictsapositiveassociation\nbutthetruthisnegative,andthevalueof distheamountof\nframes the term classiﬁer and truth agree to be a negative\nassociation. Wewishtomaximize aanddascorrectclas-\nsiﬁcations; by contrast, random guessing by the classiﬁer\nwouldgivethesameratioofclassiﬁerlabelsregardlessof\ngroundtruthi.e. a/b≈c/d. With N=a+b+c+d,the\nK-L distance between the observed distribution and suchrandom guessing is:\nKL=a\nNlog/parenleftbiggN a\n(a+b) (a+c)/parenrightbigg\n+b\nNlog/parenleftbiggN b\n(a+b) (b+d)/parenrightbigg\n+c\nNlog/parenleftbiggN c\n(a+c) (c+d)/parenrightbigg\n+d\nNlog/parenleftbiggN d\n(b+d) (c+d)/parenrightbigg\n(3)\nThis measures the distance of the classiﬁer away from a\ndegenerate distribution; we note that it is also the mu-\ntual information (in bits, if the logs are taken in base 2)\nbetween the classiﬁer outputs and the ground truth labels\nthey attempt to predict.\nTable 2 gives a selected list of well-performing term\nmodels. Given the difﬁculty of the task we are encour-\naged by the results. Not only do the results give us term\nmodels for audio, they also give us insight into which\nterms and description work better for music understand-\ning. These terms give us high semantic leverage without\nexperimenter bias: the terms and performance were cho-\nsen automatically insteadof from a list of genres.\n7.3. Automatic review generation\nThe multiplication of the term model cagainst the testing\ngram matrix returns a single value indicating that term’s\nrelevance to each time frame. This can be used in re-\nview generation as a conﬁdence metric, perhaps setting a\nthresholdtoonlyallowhighconﬁdenceterms. Thevector\noftermandconﬁdencevaluesforapieceofaudiocanalso\nbe fed into other similarity and learning tasks, or even a\nnatural language generation system: one unexplored pos-\nsibility for review generation is to borrow fully-formed\nsentences from actual reviews that use some amount of\nterms predicted by the term models and form coherent\nparagraphsofreviewsfromthisgenericsourcedata. Work\nin language generation and summarization is outside the\nscope of this article but the results for the term prediction\ntaskandthebelowreviewtrimmingtaskarepromisingfor\nthese future directions.\nOne major caveat of our review learning model is its\ntimeinsensitivity. Althoughthefeaturespaceembedstime\natdifferentlevels,thereisnomodelofintra-songchanges\noftermdescription(aloudsonggettingsoft,forexample)\nand each frame in an album is labeled the same during\ntraining. We are currently working on better models of\ntime representation in the learning task. Unfortunately,\nthe ground truth in the task is only at the album level and\nwe are also considering techniques to learn ﬁner-grained\nmodels from a large set of broad ones.adj Term K-L bits np Term K-L bits\naggressive 0.0034 reverb 0.0064\nsofter 0.0030 the noise 0.0051\nsynthetic 0.0029 new wave 0.0039\npunk 0.0024 elvis costello 0.0036\nsleepy 0.0022 the mud 0.0032\nfunky 0.0020 his guitar 0.0029\nnoisy 0.0020 guitar bass and drums 0.0027\nangular 0.0016 instrumentals 0.0021\nacoustic 0.0015 melancholy 0.0020\nromantic 0.0014 three chords 0.0019\nTable2. Selectedtop-performingmodelsofadjectiveand\nnoun phrase terms used to predict new reviews of music\nwiththeircorrespondingbitsofinformationfromtheK-L\ndistance measure.\n7.4. Review Regularization\nMany problems of non-musical text and opinion or per-\nsonaltermsgetinthewayoffullreviewunderstanding. A\nsimilaritymeasuretrainedonthefrequenciesoftermsina\nuser-submittedreviewwouldlikelybetrippedupbyobvi-\nouslybiasedstatementslike“Thisrecordisawful”or“My\nmother loves this album.” We look to the success of our\ngrounded term models for insights into the musicality of\ndescription and develop a ‘review trimming’ system that\nsummarizes reviews and retains only the most descriptive\ncontent. The trimmed reviews can then be fed into fur-\nther textual understanding systems or read directly by the\nlistener.\nTotrimareviewwecreateagroundingsumtermoper-\nated on a sentence sof word length n,\ng(s) =/summationtextn\ni=0P(ai)\nn(4)\nwhereaperfectlygroundedsentence(inwhichthepredic-\ntive qualities of each term on new music has 100% preci-\nsion)is100%. Thisupperboundisvirtuallyimpossiblein\nagrammaticallycorrectsentence,andweusuallysee g(s)\nof{0.1% .. 10% }. The user sets a threshold and the sys-\ntem simply removes sentences under the threshold. See\nTable 3 for example sentences and their g(s). We see that\ntherateofsentencerecall(howmuchofthereviewiskept)\nvarieswidelybetweenthetworeviewsources;AMG’sre-\nviews have naturally more musical content. See Figure 4\nfor recall rates at different thresholds of g(s).\n7.5. Rating Regression\nLastly we consider the explicit rating categories provided\nin the review to see if they can be related directly to the\naudio, or indeed to each other. Our ﬁrst intuition is that\nlearninganumericalratingfromaudioisafruitlesstaskas\nthe ratings frequently reﬂect more information from out-\nsidethesignalthananythingobservableinthewaveforms.\nThepublic’sperceptionofmusicwillchange,andasare-\nsult reviews of a record made only a few months apart\nmight wildly differ. In Figure 1 we see that correlation\nof ratings between AMG and Pitchfork is generally lowSentence g(s)\nThe drums that kick in midway are also decidedly more similar to Air’s previous work. 3.170%\nBut at ﬁrst, it’s all Beck: a harmonica solo, folky acoustic strumming, Beck’s distinctive, marble-mouthed vocals, and tolls ringing in\nthe background.2.257%\nBut with lines such as, ”We need to use envelope ﬁlters/ To say how we feel,” the track is also an oddly beautiful lament. 2.186%\nThe beat, meanwhile, is cut from the exact same mold as The Virgin Suicides– from the dark, ambling pace all the way down to the\nangelic voices coalescing in the background.1.361%\nAfter listing off his feelings, the male computerized voice receives an abrupt retort from a female computerized voice: ”Well, I really\nthink you should quit smoking.”0.584%\nI wouldn’t say she was a lost cause, but my girlfriend needed a music doctor like I needed, well, a girlfriend. 0.449%\nShe’s taken to the Pixies, and I’ve taken to, um, lots of sex. 0.304%\nNeedless to say, we became well acquainted with the album, which both of us were already fond of to begin with. 0.298%\nTable 3. Selected sentences and their g(s)in a review trimming experiment. From Pitchfork’s review of Air’s “10,000\nHz Legend.”\n2.0 1.8 1.5 1.2 1.0 0.8 0.5 0.22030405060708090100\ng(s) threshold% of review keptPitchfork\nAMG\nFigure 4. Review recall rates at different g(s)thresholds.\nwith a correlation coefﬁcient of r= 0.264(where a ran-\ndom pairing of ratings over multiple simulations gives us\na coefﬁcient of 0.071with 95% conﬁdence.)\nAlthough we assume there is no single overall set of\nrecordratingsthatwouldsatisfybothcommunities,wedo\nbelieve AMG and Pitchfork represent two distinct sets of\n“collective opinion” that might be successfully modeled\none at a time. A user model might indicate which com-\nmunity they ‘trust’ more, and signiﬁcance could then be\nextractedonlyfromthatcommunity. Theexperimentthen\nbecomes a test to learn each reviewing community’s rat-\nings, and to see if each site maintains consistency in their\nscores.\nWe use our Penny features again computed on frames\nof audio derived from the albums in the same manner as\nour review learning experiment. We treat the problem as\na multi-dimensional regression model, and we use a sup-\nport vector machine classiﬁer to perform the regression.\nWe use the same album split for testing and training as\nabove, and train each frame of audio against the rating\n(scaled to 0..1). We then evaluate the model against the\ntestsetandcomputethecorrelationcoefﬁcientagainstthe\nactual rating. The AMG model did well with a correla-\ntion coefﬁcient of r= 0.147. Through empirical simu-lation we established that a random association of these\ntwo datasets gives a correlation coefﬁcient of magnitude\nsmallerthan r= 0.080with95%conﬁdence. Thus,these\nresults indicate a very signiﬁcant correlation between the\nautomatic and ground-truth ratings.\nThe Pitchfork model did not fare as well with r=\n0.127(baseline of r= 0.082with 95% conﬁdence.) Fig-\nure 1 shows the scatter plot/histograms for each experi-\nment;weseethattheaudiopredictionsaremainlybunched\naround the mean of the ground truth ratings and have a\nmuch smaller variance. Visually, it is hard to judge how\nwell the review information has been captured. However,\nthecorrelationvaluesdemonstratethattheautomaticanal-\nysis is indeed ﬁnding and exploiting informative features.\nWhile our results in the rating regression experiment\nwere less than excellent we consider better community\nmodeling part of future work. Within a community of\nmusic listeners the correlation of opinions of albums will\nbe higher and we could identify and tune models to each\ncommunity.\n8. CONCLUSIONS\nWe are using reviews and general text descriptions, much\nas human listeners do, to move beyond the impoverished\nlabels of genres and styles which are ill-deﬁned and not\ngeneralizable. Human description is a far richer source of\ntarget classes and clusters than marketing tags which can\nhave almost no relationship to audio content. By identi-\nfying communities of music preference and then learning\nthe language of music we hope to build scalable models\nof music understanding. Review analysis represents one\nsource of information for such systems, and in this article\nwe have shown analysis frameworks and results on learn-\ningthecrucialrelationbetweenreviewtextsandthemusic\nthey describe.\n9. ACKNOWLEDGEMENTS\nWeareverygratefulforthehelpofRyanMcKinley(Com-\nputing Culture Group, MIT Media Lab) in arranging the\nmusic and metadata testbed.10. REFERENCES\n[1]K. Dave, S. Lawrence, and D. Pennock, “Mining\nthe peanut gallery: Opinion extraction and semantic\nclassiﬁcation of product reviews,” in International\nWorld Wide Web Conference , Budapest, Hungary,\nMay 20–24 2003, pp. 519–528.\n[2]B. Whitman and S. Lawrence, “Inferring descrip-\ntions and similarity for music from community\nmetadata,”in Proc.Int.ComputerMusicConference\n2002 (ICMC) , September 2002, pp. 591–598.\n[3]B. Whitman and R. Rifkin, “Musical query-by-\ndescription as a multi-class learning problem,” in\nProc. IEEE Multimedia Signal Processing Confer-\nence (MMSP) , December 2002.\n[4]B. Whitman, “Semantic rank reduction of music au-\ndio,”inProc.IEEEWorksh.onApps.ofSig.Proc.to\nAcous. and Audio , 2003.\n[5]G. Tzanetakis, G. Essl, and P. Cook, “Au-\ntomatic musical genre classiﬁcation of au-\ndio signals,” 2001. [Online]. Available: cite-\nseer.nj.nec.com/tzanetakis01automatic.html\n[6]W. Chai and B. Vercoe, “Folk music classiﬁcation\nusinghiddenmarkovmodels,”in Proc.International\nConference on Artiﬁcial Intelligence , 2001.\n[7]R. B. Dannenberg, B. Thom, and D. Watson, “A\nmachinelearningapproachtomusicalstylerecogni-\ntion,” inIn Proc. 1997 International Computer Mu-\nsicConference . InternationalComputerMusicAs-\nsociation., 1997, pp. 344–347. [Online]. Available:\nciteseer.nj.nec.com/dannenberg97machine.html\n[8]M.Slaney,“Semantic-audioretrieval,”in Proc.2002\nIEEEInternationalConferenceonAcoustics,Speech\nand Signal Processing , May 2002.\n[9]P. Duygulu, K. Barnard, J. D. Freitas, and\nD. Forsyth, “Object recognition as machine trans-\nlation: Learning a lexicon for a ﬁxed image\nvocabulary,” 2002. [Online]. Available: cite-\nseer.nj.nec.com/duygulu02object.html\n[10]K. Barnard and D. Forsyth, “Learning the semantics\nof words and pictures,” 2000. [Online]. Available:\nciteseer.nj.nec.com/barnard00learning.html\n[11]T. Regier, The human semantic potential . Cam-\nbridge, MA: MIT Press, 1996.\n[12]D.Bailey,“Whenpushcomestoshove: Acomputa-\ntionalmodeloftheroleofmotorcontrolintheacqui-\nsitionofactionverbs,”Ph.D.dissertation,University\nof California at Berkeley, 1997.\n[13]S.Narayanan,“Knowledge-basedactionrepresenta-\ntions for metaphor and aspect (karma),” Ph.D. dis-\nsertation,UniversityofCaliforniaatBerkeley,1997.[14]J. Siskind, “Grounding the Lexical Semantics of\nVerbs in Visual Perception using Force Dynamics\nand Event Logic,” Journal of Artiﬁcial Intelligence\nResearch, vol. 15, pp. 31–90, 2001.\n[15]D. Roy, “Learning words from sights and sounds:\nA computational model,” Ph.D. dissertation, Mas-\nsachusetts Institute of Technology, 1999.\n[16]D.Cruse, LexicalSemantics . CambridgeUniversity\nPress, 1986.\n[17]B. Whitman, G. Flake, and S. Lawrence, “Artist de-\ntection in music with minnowmatch,” in Proc. 2001\nIEEEWorkshoponNeuralNetworksforSignalPro-\ncessing,Falmouth,Massachusetts,September10–12\n2001, pp. 559–568.\n[18]“All music guide.” [Online]. Available:\nhttp://www.allmusic.com\n[19]“Pitchfork media.” [Online]. Available:\nhttp://www.pitchforkmedia.com\n[20]D. Ellis, B. Whitman, A. Berenzweig, and\nS. Lawrence, “The quest for ground truth in musical\nartist similarity,” in Proc. International Symposium\non Music Information Retrieval ISMIR-2002 , 2002.\n[21]E. Brill, “A simple rule-based part-of-speech\ntagger,” in Proc. ANLP-92, 3rd Conference on\nApplied Natural Language Processing , Trento, IT,\n1992, pp. 152–155. [Online]. Available: cite-\nseer.nj.nec.com/article/brill92simple.html\n[22]L. Ramshaw and M. Marcus, “Text chunking us-\ning transformation-based learning,” in Proc. Third\nWorkshop on Very Large Corpora , D. Yarovsky\nand K. Church, Eds. Somerset, New Jer-\nsey: Association for Computational Linguistics,\n1995, pp. 82–94. [Online]. Available: cite-\nseer.nj.nec.com/ramshaw95text.html\n[23]M.Casey,“Generalsoundrecognitionandsimilarity\ntools,” in MPEG-7 Audio Workshop W-6 at the AES\n110th Convention , May 2001.\n[24]V. N. Vapnik, Statistical Learning Theory . John\nWiley & Sons, 1998.\n[25]R. M. Rifkin, “Everything old is new again: A fresh\nlook at historical approaches to machine learning,”\nPh.D. dissertation, Massachusetts Institute of Tech-\nnology, 2002.\n[26]G. Fung and O. L. Mangasarian, “Proximal support\nvector classiﬁers,” in Proc. Seventh ACM SIGKDD\nInternational Conference on Knowledge Discovery\nandDataMining ,ProvostandSrikant,Eds. ACM,\n2001, pp. 77–86."
    },
    {
        "title": "A Case Study of Distributed Music Audio Analysis Using the Geddei Processing Framework.",
        "author": [
            "Gavin Wood",
            "Simon O&apos;Keefe"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1414810",
        "url": "https://doi.org/10.5281/zenodo.1414810",
        "ee": "https://zenodo.org/records/1414810/files/WoodO04.pdf",
        "abstract": "Audio signal processing and refinement is an impor- tant part of a content-based music information retrieval system. As the our repertoire of techniques becomes more varied, there are greater requirements of computation power. Distributed storage techniques have become widespread and almost invisible with the advent of file-sharing sys- tems, on-line digital music stores and on line storage ser- vices. Even discounting data with potential copyright en- tanglements, there is a vast amount that is ripe for analy- sis, and thus parallelised and distributed processing tech- niques seem increasingly appropriate. Existing frameworks are already capable of a signifi- cant amount of audio analysis for music information re- trieval. However they are by and large ignorant of distri- bution and parallelisation. There are middleware libraries to help with aspects of distributed computing, but combin- ing the two can be cumbersome and inefficient. This paper provides a brief description of a software framework that can process audio in a scalable and dis- tributed manner: Geddei. The paper then takes an in- teresting and relevant signal analysis task often used for music information retrieval and implements it under the Geddei framework. The ease of use is discussed and vari- ous measurements taken of Geddei, both in comparison to itself under different circumstances, and ‘reference code’ that was used in a previous study. We discuss the prob- lems with the distribution of the task with Geddei and of- fer some possible solutions.",
        "zenodo_id": 1414810,
        "dblp_key": "conf/ismir/WoodO04",
        "keywords": [
            "audio signal processing",
            "content-based music information retrieval",
            "computation power",
            "distributed storage techniques",
            "file-sharing systems",
            "on-line digital music stores",
            "online storage services",
            "parallelised and distributed processing techniques",
            "existing frameworks",
            "music information retrieval"
        ],
        "content": "A CASE STUDYOF DISTRIBUTEDMUSICAL AUDIOANALYSISUSING\nTHEGEDDEI PROCESSINGFRAMEWORK\nGavinWood\nDepartmentof Computer ScienceSimon O’Keefe\nUniversityof York,YorkYO105DD, UK\nABSTRACT\nAudio signal processing and reﬁnement is an impor-\ntant part of a content-based music information retrieval\nsystem. Astheourrepertoireoftechniquesbecomesmore\nvaried,therearegreaterrequirementsofcomputationpower.\nDistributed storage techniques have become widespread\nand almost invisible with the advent of ﬁle-sharing sys-\ntems, on-line digital music stores and on line storage ser-\nvices. Even discounting data with potential copyright en-\ntanglements, there is a vast amount that is ripe for analy-\nsis, and thus parallelised and distributed processing tech-\nniques seem increasingly appropriate.\nExisting frameworks are already capable of a signiﬁ-\ncant amount of audio analysis for music information re-\ntrieval. However they are by and large ignorant of distri-\nbution and parallelisation. There are middleware libraries\ntohelpwithaspectsofdistributedcomputing,butcombin-\ning the twocan be cumbersome and inefﬁcient.\nThis paper provides a brief description of a software\nframework that can process audio in a scalable and dis-\ntributed manner: Geddei. The paper then takes an in-\nteresting and relevant signal analysis task often used for\nmusic information retrieval and implements it under the\nGeddeiframework. The ease of useis discussed and vari-\nousmeasurementstakenofGeddei,bothincomparisonto\nitself under different circumstances, and ‘reference code’\nthat was used in a previous study. We discuss the prob-\nlems with the distribution of the task with Geddei and of-\nfer some possible solutions.\n1. INTRODUCTION\nAlmost all methods of content-based analysis of musical\naudio for information retrieval (IR) rely to some degree\nupon a signal (pre-)processing technique, and this tech-\nnique can often be at the heart of the method itself. There\nexist several (open) systems for audio signal processing\nwith respect to music IR, not least Tzanetakis’s Marsyas\nlibrary framework[1, 2].\nWith music IR and audio signal processing in general,\nwe ﬁnd that the amount of data to be processed is large.\nThereissigniﬁcantimpetustodeﬁneourproblemsinmore\nindependent,declarativetermsasdistributionbecomesmore\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopiesbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2004UniversitatPompeu Fabra.common in both software (e.g. threading) and hardware\n(e.g. grid technology). Distributive processing allows us\nto combine multiple individual CPUs (processing chips)\nto perform one task as a whole.\nThe problem of distribution may be phrased as one of\norganisation—we have a large task at hand, and we must\nbreak it down into smaller tasks that can be done concur-\nrentlyandindependently . With a large amount of data,\ntheoverheadoftransport,andthusdistribution,islarge—\nmostnotablywhencomparedtotheprocessingtasks. This\ncan be a problem when trying to ﬁnd an optimal distribu-\ntionmethod,sincetransportingdatatoanotherprocessing\nhostmakeslittlesenseiftheoverheadoftransportiscom-\nparable to the cost of processing. Thus the smaller tasks\nmustbeorganisedinsuchawaythatmaximisesuseofthe\nresources availableand minimises unnecessary transport.\n1.1. Applications\nAs techniques become more complex, more varied and\nmore adaptive, music IR methods require more comput-\ning power to (pre-)process audio signal data into forms\nthatareusefulforanalysistechniques. Weseeongoingef-\nforts[3]tocreatealargeandprotectedmusicdatabasefor\nbenchmarking music IR techniques. A distributed gener-\nalisedsystemcouldhelpinprovidingacommonplatform\non which large scale processing benchmarks can be exe-\ncuted. As we see more large computing grids emerging,\nthemusicIRcommunityneedstobeabletousesuchtech-\nnology effectively and easily. A simple and efﬁcient dis-\ntributionframeworkdesignedformusicsignalprocessing\ngoes some wayto solving this problem.\n1.2. Related Work\nSeveral toolkits already exist for the processing and anal-\nysis of audio signals. Libraries such as the Simple Util-\nity Classes (SUCs)[4] provide basic programming com-\nponents for signal-analysis, though distribution and par-\nallelism in general is not addressed. Marsyas provides\na broad programming interface for implementing many\nideas found in music information retrieval, and addresses\nbothtraditional‘bottom-up’designs,aswellasprediction-\ndriven architectures. Its dataﬂow mechanisms are robust\nbutpotentiallyrestrictive1. Again,noimplicitparallelism\nis availablenatively.\n1Marsyas supports processing atoms that are able to be given only\na ﬁxed-amount of data from one atom and provide some other ﬁxed-\namount of data to another atomFigure1. Anactivityﬂowchartofthebeatspectrumanal-\nysis technique.\n2. ANALYSISTECHNIQUE\nForthecasestudy,ananalysistechniquehadtobechosen\nthat provided a useful datagram for music feature extrac-\ntion, yet provided some method of testing against a real-\nworldlegacyversion.\nThereexistseveralsimilarmechanismsforﬁndingrhyth-\nmic information from musical data, be it either in sym-\nbolic form or as an audio signal. Tzanetakis, Essl and\nCook[7]introducethebeathistogram,formedbyautocor-\nrelationofwavelettransforms. Thereisalsothetechnique\nused by Rauber and Fruhwirth [8] which is essentially a\nFFTonselectedLagrangetransformspectralbands. Each\nofthesepresentdatareﬁnedtodescribetherhythmicprop-\nerties of the incoming music. Tzanetakis et al. especially\nshowsthattherhythmmetricisanimportanttoolforgenre\nclassiﬁcation problems.\nThe beat spectrum reports a spectrum of frequencies\nindicating rhythmic similarity; peaks at a frequency sug-\ngestastrongrhythmatthatfrequencyintheincomingau-\ndio. The discussed technique was used in Foote, Cooper\nand Nam [5] for measuring musical recognition though\nsimilarity. ThetechniqueisalsousedinWood&O’Keefe\n[6]formeasuringmusicsimilarity. Bothpapersshowthat\nthebeatspectrumcanprovideausefuldatagramformusic\nsimilarity and analysis in general.\nThebeatspectrumwaschosenasitfulﬁlsbothrequire-\nmentswell. Asitwasimplementedforapreviousstudyit\ngivesa useful reference point for comparison.\n2.1. Method\nWe take 30-second windows of the incoming audio data,\neach overlapped by 15 seconds. An array of short-time\nFourier transforms is then calculated over this 30-second\nwindow using a window size of 2048-samples and a 50%\noverlap.\nThestandardbeat-spectrummethodwasalteredslightly\ntouseapsychoacoustictransformationintheformofBark\ncriticalbanding[9]. Thisreducesapotentiallylargespec-\ntrumintoacompact24‘critical’bandrepresentation,based\nupon empirical studies of the ear and how we perceive\npitch. Experiments from Wood & O’Keefe [6] suggest\nthatthisdecreasescomputationtimesigniﬁcantlywithlit-tle or no effect on the ﬁdelity of the results (from a music\nrecognition point of view,at least).\nThe beat spectrum is formed from the summation of\nsuper-diagonalsonaself-similaritymatrix. Theself-similarity\nmatrix is formed by measuring the ‘similarity’ between\ntwo points in time of some incoming music. For an in-\ncoming audio signal, a useful similarity measure can be\ncomputedfromthecosinedistancebetweenthetwospec-\ntra which represent the frequency components of the mu-\nsicaudioatthosepointsintime;weusethismeasurement\nhere.\n3. GEDDEI’S ARCHITECTURE\nGeddei is an acronym for General Environment for Dis-\ntributed Dataﬂow Experimentation and Investigation. It\nprovides a simple, transparent, declarative style interface\nfor tasks that can be arranged as a data ﬂow-orientated\nproblem. It is highly scalable, equally suited for small-\nscale investigation and batch processing. It is highly ef-\nﬁcient, using mechanisms such as cyclic shared buffers\nto maximise signal-data throughput. A plugin design al-\nlowssimpleadditionofthird-partyprocessingmodules. It\nisusefultoconsiderthetwoprimitivesthatareusedwhen\nconstructingdataﬂownetworksforaudioanalysisinGed-\ndei:\nProcessor : Processorobjectsarethefundamentalcom-\nponent of all Geddei processing modules. They represent\nanatomicandindependenttaskofthewholeproblem,and\nas such represent the granularity of the distribution. Al-\nmostanysinglethinginatypicaldataﬂownetworkcanbe\nlikenedtoaprocessorobject. ThismightincludetheFFT,\nthe similarity matriﬁcation or a downsampler. Even au-\ndio ﬁle players are modelled as processors. This level of\ngeneralisation provides the basis for automation and dis-\ntributability.\nConnection : Connectionobjectsprovideamechanism\nof data transfer between two Processor objects. Connec-\ntion objects take care of getting signal data between pro-\ncessorsregardlessoftheiractual‘positions’inthesystem.\nAconnectionlinkingprocessorsresidingondifferentma-\nchines will be different to (and slower than) one linking\nprocessorsresidinginthesameareaofcomputermemory.\nHoweverthelevelofabstractionmeansthattheprocessors\nare unawareof this.\n4. EVALUATION\nWithGeddei,distributionovertwohostswasassimpleas\nhaving all processing objects reside on one host. No spe-\nciﬁc optimisations needed to be added nor did the code\nneed to be changed in any way for use on different archi-\ntectures, leading to a verylowprototyping time.\nThereferencecodeusedwasthatdevelopedforanear-\nlier study [6]2. It uses the Simple Utility Class library;\nno special optimisations were made at the time, however\nthe simplicity allows the code to run with some degree of\n2The code can be found at www-users.cs.york.ac.uk/\n˜gav/ref.tar.gzFigure 2. Geddei Nite, the Geddei network editor, with\nthe beat spectrum analysis network, watching the output\nof the self-similarity matrix.\nFigure 3. As ﬁgure 2, but changing the focus to the beat\nspectrum’soutput.\nefﬁciency. Dependingontheprogrammer’sabilityandthe\nframeworkused,itmaybelikenedtoanaverage,sequentially-\nbased implementation of some audio preprocessing.\n4.1. User Interface\nNetworks are created and edited with Geddei-Nite. It is\nsimple to inspect the data travelling through any channel\ninrealtimewithoutdisruptingtheactualdata,muchinthe\nsame way a wiretapper would evesdrop on a phone call\nunnoticed. This can be used to investigate some (hypo-\nthetical) unexpectedor otherwise interesting result.\nGeddei’s strict and extensive signal-typing mechanism\nallows Geddei-Nite to probe the signal type and attach\nthe relevant viewer, allowing an informative graph to be\ndrawn. Figures 2 and 3 show the same network with dif-\nfering views.\n4.2. Performance\nTheperformanceevaluationofGeddeiissplitinto3parts:\nWecomparetheGeddei andreferenceimplementation on\nasingleCPUworkstation. WethentestGeddei’sdistribu-\ntion capacity by comparing non-distributed times to dis-\ntributed. Finally we compare the effects of changing the\namount of data to be processed between the Geddei and\nreference implementation. In all experiments, the length\nof the music track to be analysed was279 seconds.\nThe FFT stage was given three variants: It was ei-\ntherleftuntouched,alow-passﬁlterwasappliedat8KHz,\nFigure 4. Comparison of the two systems’ performances\non an undistributed single CPU system. Speciﬁcation:\nAthlon XP 2100; Linux 2.6.3; gcc 3.3.2.\nor Bark-critical banding was applied. A further analysis\nwasmadethatattained boththelow-passﬁlterresultsand\nthe Bark-based results. These change the dimensionality\nof the spectra signiﬁcantly (between 1024, 186 and 24),\nmovingthepotentialbottlenecksofthesystemtoandfrom\nthe self-similarity matrix.\nComparisonsbetweenthetwoimplementationsarerel-\natively easy; Geddei has a built-in monitor processor that\ncollects the output data and measures the time taken for\nprocessing. For the reference software, the GNU ‘time’\ncommand was used to report the total time the CPU spent\nexecutingtheprogram3. Thiscouldpotentiallybefavourable\nto the reference software as the Geddei timing method\nmeasuresrealtimetakenratherthanCPUtimetaken,how-\neveronanotherwiseunusedsystemtheyaresimilarenough\nfor the purpose of this report.\nThis ﬁrst test results are shown in ﬁgure 4. Geddei\ntakes on average 38% less time on each task, a signiﬁcant\nimprovement. As would be expected, both Geddei- and\nreference-based implementations take less time to calcu-\nlated both types of beat spectrum in one run than in two.\nThescalabilityofGeddeiwastestedbycomparingitto\nitself under while running under differing situations. Two\nhosts were used for this, as is described in ﬁgure 5. A re-\nstrictionwasplacedontheanalysisnetworkthatthemusic\nmuststart,andtheanalysedoutputend,ontheworkstation\nmachine (host 1). Figure 5 shows the results. A theoreti-\ncal maximum was calculated using the ideal parallelised-\nlinear-resistanceformula:\nPcombination = (P−1\nhost1+P−1\nhost2)−1(1)\nThis formula is simplistic and gives only a basic ide-\nalistic marker. Problems such as different CPU architec-\nturesgivingdifferentperformancesdependingontaskand\nthe software’s running time only vaguely approximating\nas linear make this a rough guide at best; its inclusion\nhere is to help visualise a potential limit of computation\ntime, given that the two hosts are of differing speeds it is\notherwise difﬁcultto do.\nSeveral distribution conﬁgurations were tried and the\noptimum selected. Those conﬁgurations are detailed be-\nlow. Note that the superscript gives the host number they\nwere distributed onto. They appear in the same order as\nthey are shown in ﬁgure 5. The audio source and data\nreception processors were both on host 1, the designated\nworkstation.\n3thisdoes not include anytime the system spent executingOS tasksFigure5. ComparisonofGeddei’sperformancewhendis-\ntributed over two single CPU hosts. Speciﬁcations: Host\n1: Athlon XP 2100; Linux 2.6.3; gcc 3.3.2. Host 2: Pen-\ntium 4 2.66GHz; Linux 2.4.19; gcc 2.2.4.\nFFT1→SSM2→Diag.Sum.2\nFFT1→BandPass1→SSM2→Diag.Sum.2\nFFT1→Bark1→SSM2→Diag.Sum.2\nFFT1→BandPass1/arrownortheast\n/arrowsoutheastBark1→SSM1→Diag.Sum.1\nSSM2→Diag.Sum.2\nFrom ﬁgure 5 we can see that host 1 is much slower\nthan host 2. This is a particularly difﬁcult problem to dis-\ntribute;therearerelativelyfewprocessingatomsandonly\nthe similarity matrix has a signiﬁcant processing require-\nment, thus we can see Geddei’s distributive capabilities\nbeing taxed.\nThe very-high-bandwidth dataﬂow restricts Geddei’s\nabilitytodistributetheproblemwellbetweenthetwohosts,\nand for the three basic tasks, it is only a little faster than\nrunning on host 2 alone. The difﬁculties are overcome\nwellwhenboththeBark-basedbeatspectrumandthelow-\npass-ﬁlter-based beat spectrum are calculated, attaining a\nslightly better time than the idealistic marker.\nFinally the completion times were recorded for dif-\nfering sizes of beat spectrum and low-pass ﬁlter cutoffs.\nChanging the low-pass ﬁlter cut-off alters the dimension-\nality of the vectors for calculating the cosine distances in\ntheself-similaritymatrix. Ithastheeffectofreducingdata\n(andthuscomputation time)at thecostof reducingthe ﬁ-\ndelity of the results (though most people agree that any-\nthingoveraround4-8KHzisprobablyuselessforthetask\nof music IR).\nChangingthematrixwindowsizewillchangethescope\nof the beat spectrum; we keep the window hop distance\nﬁxedat 50%. Figure 6 showsthe results of these tests.\nBoth graphs are linear conﬁrming that Geddei is per-\nforming akin to the reference design. The lines of best\nﬁt suggest that Geddei is approximately twice as fast as\nthereferencedesign,presumablyheraldingitsparallelised\nlayout, high-speed buffers and optimisation-friendly de-\nsign.\n5. CONCLUSION\nWe have introduced the Geddei framework and provided\nan example of its use for music information retrieval. We\nhave shown that Geddei is well-suited to the design and\ndevelopment of music signal processing tasks and that it\nFigure 6. Comparison between the reference and Ged-\ndei implementations performance when either the matrix\nwindow size (left) or the low-pass ﬁlter cutoff (right) is\nchanged. Both are on the same host as ﬁgure 4.\nrunsatleastasfast,andoftensigniﬁcantlyfasterthanan—\nalbeit simple—existing implementation. We have shown\nthatithassomepotentialfordistributionofmusicIRpro-\ncessingandcandistributewellundergoodconditions,though\nthere is clearly room for improvement in the worst-case\nscenario.\nImmediate future work will involve increasing Ged-\ndei’s repertoire of component techniques and extending\nits distribution capacity to help with the problems of dis-\ntribution granularity. We also want to extend it to pro-\nvide better support for techniques such as band-pass ﬁl-\nters that require very little processing for their data-ﬂow\noverheads. Investigationintoanautomaticmechanismfor\ndistributingtheworkloadwouldalsobeaninterestingand\nlikelyrewardingendeavour.\n6. REFERENCES\n[1] G. Tzanetakis and P. Cook. Audio information re-\ntrieval(air) tools. In Proc.ISMIR ,2000.\n[2] G. Tzanetakis and P. Cook. Marsyas: A framework\nforaudio analysis. OrganisedSound ,4:30–??, 2000.\n[3] J. Stephen Downie. The music information re-\ntrieval(mir)andmusicdigitallibrary(mdl)evaluation\nproject,http://music-ir.org/evaluation/.\n[4] Simple Utility Classes. sucs.sf.net/, 2004.\n[5] J. Foote, M. Cooper, and U. Nam. Audio retrieval by\nrhythmicsimilarity. In In Proc.ISMIR ,2002.\n[6] G. Wood and S. E. O’Keefe. Quantitative compar-\nisons into content-based music recognition with the\nself-organisingmap. In InProc.ISMIR ,2003.\n[7] G. Tzanetakis, G. Essl, and P. Cook. Automatic mu-\nsical genre classiﬁcation of audio signals. In In Proc.\nISMIR, 2001.\n[8] Andreas Rauber and Markus Fr ¨uhwirth. Automati-\ncally analyzing and organizing music archives. Lec-\ntureNotes in Computer Science ,2163:402–??, 2001.\n[9] ed. P. R. Cook. Music, Cognition And Computerized\nSound: AnIntroductionToPsychoacoustics . London,\netc.,MIT,1999."
    },
    {
        "title": "An MPEG-7 Database System and Application for Content-Based Management and Retrieval of Music.",
        "author": [
            "Otto Wüst",
            "Òscar Celma"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418375",
        "url": "https://doi.org/10.5281/zenodo.1418375",
        "ee": "https://zenodo.org/records/1418375/files/WustC04.pdf",
        "abstract": "Computer users are gaining access to and are starting to accumulate moderately large collections of multimedia files, in particular of audio content, and therefore demand new applications and systems capable of effectively retriev- ing and manipulating these multimedia objects. Content- based retrieval of multimedia files is typically based on searching within a feature space, defined as a collection of parameters that have been extracted from the content and which describe it in a relevant way for that particular retrieval application. The MPEG-7 standard offers tools to model these metadata in an interoperable and extensi- ble way, and can therefore be considered as a framework for building content-based audio retrieval systems. This paper highlights the most relevant aspects consid- ered during the design and implementation of a DBMS- driven MPEG-7 layer on top of which a content-based mu- sic retrieval system has been built. A particular focus is set on the data modeling and database architechture issues.",
        "zenodo_id": 1418375,
        "dblp_key": "conf/ismir/WustC04",
        "keywords": [
            "multimedia files",
            "audio content",
            "content-based retrieval",
            "feature space",
            "MPEG-7 standard",
            "metadata modeling",
            "interoperable",
            "extensible",
            "database management system",
            "music retrieval system"
        ],
        "content": "ANMPEG-7DATABASESYSTEM ANDAPPLICA TIONFOR\nCONTENT -BASEDMANAGEMENT ANDRETRIEV ALOFMUSIC\nOtto Wust\nMusicTechnology Group\nUniversitatPompeuFabra`Oscar Celma\nMusicTechnology Group\nUniversitatPompeuFabra\nABSTRACT\nComputer users aregaining access toandarestarting to\naccumulate moderately largecollections ofmultimedia ﬁles,\ninparticular ofaudio content, andtherefore demand new\napplications andsystems capable ofeffectivelyretrie v-\ningandmanipulating these multimedia objects. Content-\nbased retrie valofmultimedia ﬁles istypically based on\nsearching within afeature space, deﬁned asacollection\nofparameters thathavebeen extracted from thecontent\nandwhich describe itinarelevantwayforthatparticular\nretrie valapplication. The MPEG-7 standard offerstools\ntomodel these metadata inaninteroperable andextensi-\nbleway,andcantherefore beconsidered asaframe work\nforbuilding content-based audio retrie valsystems.\nThis paper highlights themost relevantaspects consid-\nered during thedesign andimplementation ofaDBMS-\ndrivenMPEG-7 layer ontopofwhich acontent-based mu-\nsicretrie valsystem hasbeen built. Aparticular focus isset\nonthedata modeling anddatabase architechture issues.\n1.INTRODUCTION\nLargeamounts ofdigital audio arenowadays widely avail-\nable. This canrange from musical pieces orsongs for\ndownload overtheinternet byacasual computer user,to\nhighly specialized sound libraries delivered onDVDtar-\ngeted fortheprofessional musician.\nAsthenumber ofavailable audio media increases, so\ndoes theneed forauser tolocate audio clips inanefﬁ-\ncient way.Theﬁeld ofinformation retrie valdeals with the\nmodeling, indexing andaccessing ofinformation mainly\nwithin digital libraries. Ithasbeen extensi velystudied for\nmanyyears [1],mostly focusing onretrie ving textual in-\nformation using text-based methods.\nDatabase management systems (DBMS) havebeen widely\nused toimplement efﬁcient textbased information retrie val\nsystems, solving manyoftheproblems encountered inthat\nﬁeld. However,inthearea ofmultimedia, andinparticu-\nlarinaudio andmusic information retrie val,there isstill\nPermission tomakedigitalorhardcopiesofallorpartofthisworkfor\npersonalorclassroom useisgrantedwithoutfeeprovidedthatcopies\narenotmadeordistributedforpro®torcommercial advantageandthat\ncopiesbearthisnoticeandthefullcitationonthe®rstpage.\nc°2004UniversitatPompeuFabra.alotofongoing research andopen questions concerning\nimplementation andarchitectural aspects such asscalabil-\nityofthesystems, aswell asfunctional issues related to\ntheusability .\nThis paper addresses thepossibility ofusing available\nDBMS incombination with MPEG-7 standard [2]asthe\nkeybuilding block formusic retrie valapplications thatcan\nsatisfy theneeds ofauser.First, theMPEG-7 standard is\nbrieﬂy reviewed with special focus onaspects thatmust\nbetakeninto consideration when designing andimple-\nmenting itwithin adatabase management system. Then, a\nparticular MPEG-7 database implementation isdescribed.\nThis wasdeveloped throughout theCUID ADO project1,\nandistargeted forcontent-based processing ofmusic MPEG-\n7descriptions. Some oftherelevantfeatures areillus-\ntrated inthesection thatfollowsalong acontent-based au-\ndioeditor ,manipulation, andauthoring application, which\nisbuitontopofthedescribed database system. Finally\nconclusions aredrawn.\n2.MPEG-7ASADATAMODEL\nMPEG-7 hasbeen promoted asastandard with theobjec-\ntivetoprovide acommon interf aceforaudio visual content\ndescription inmultimedia environments. This should al-\nlowthatdifferent MPEG-7 systems ormodules caneasily\ninteroperate.\nThestandard isbased onthenotion ofDescriptors (D)\nandDescription Schemes (DS). The former represent a\nmodel forspeciﬁc high orlowlevelfeatures thatcanbe\nannotated foragivenmedia object. Thelatter justrepre-\nsent agrouping ofaseries ofDescriptors orfurther De-\nscription Schemes inaparticular functional area.\nThe deﬁnition oftheMPEG-7 standard relies onfur-\nther standards oftheMPEG family andheavily onthe\nXML language andXML-Schema which areused inits\nrepresentation anditsdeﬁnition. MPEG-7 itself ispro-\nvided intheform ofanextensible XML-Schema deﬁning\nanobject oriented type hierarch ywhich deliversasetof\npredeﬁned descriptors grouped intoitsfunctional descrip-\ntionschemes.\nAsamatter ofexample, weconsider theAgent DSde-\nﬁned inthestandard, which allowstorepresent data for\npersons, groups ororganizations. Thefollowing example\n1http://www .cuidado.muinXML showsanagent descriptor which deﬁnes apartic-\nularperson:\n<Agentxsi:type=\"PersonType\">\n<Namexml:lang=\"en\">\n<GivenName xml:lang=\"en\">\nJohn\n</GivenName>\n<FamilyName xml:lang=\"en\">\nSmith\n</FamilyName>\n</Name>\n</Agent>\nWerecall theproperties oftheXML language asanad-\nequate means forinterchanging structured self-described\ninformation, andtheXML-Schema language asapower-\nfultool(which overcomes manylimitations oftheDocu-\nment TypeDeﬁnitions) fordescribing thevalidation rules\nofagivenXML document.\nSome ofthemain difﬁculties insupporting MPEG-7\natdatabase levelareaconsequence oftheobject-oriented\ndeﬁnition, theactual size andcomple xity ofthecurrent\nMPEG-7 schema, andatafurther level,duetotheﬂexibil-\nityinthestructure oftheXML documents theschema ad-\nmits asvalid. This canlead tofunctional difﬁculties when\nconsidering anMPEG-7 database implementation, since\ndifferent MPEG-7 applications could useand/or generate\nsemantically equivalent descriptions instructurally andsyn-\ntactically different yetMPEG-7 validdocuments. Although\nitshould beconsidered anapplication issue, ithasanim-\npact onthegenerality ofdatabase implementation, asit\ncanlead toapplication speciﬁc ﬁne-tuning requirements.\nEvenifwerestrict ourselv estothecase ofasingle pre-\ncisely deﬁned application, afurther issue inanMPEG-7\ndatabase implementation isthatthestandard does notde-\nﬁnehowthesearching orindexing hastobemade onthe\ndescribed data. Italso does notmakeanyassumptions\nabout theinternal storage format.\nThere areother factors which arenotinherently related\ntotheproblem ofmodeling formultimedia, butwhich\nhavetobetakenintoaccount when designing arealsys-\ntem formusic information retrie val.The most relevant\nconsideration inthisarea isprobably thefactthatmany\ndescriptors willcontain textual labels, which may need to\nbepresented inseveraluser languages andmore impor -\ntant searched inaconsistent user language independent\nway.MPEG-7 provides thestructure, butdoes notspecify\nanyrules onhowasearch overmulti-language data hasto\nbedone.\n3.DATABASEIMPLEMENT ATION\nThe database implementation described here wasdevel-\noped within theCUID ADO project foritsusebySound\nPalette ,amusic authoring andpost production applica-\ntionwhich targets exhausti vecontent based retrie valand\nprocessing functionalities, asdescribed within thenextsection andin[3]. Fortheactual database implementa-\ntion, anOracle 9iRelease2 object-relational DBMS was\nchosen since itincorporates theXDB component which\ntreats XML inanefﬁcient andnativeway.Ontopofitan\nMPEG-7 speciﬁc layer wasbuilt. Thesystem involves:\n²Arepository ofXML-Schemas which areregis-\ntered within theDB;inparicular ,theMPEG-7 XML-\nSchema andtheschema extensions speciﬁcally de-\nveloped within theCUID ADO project. IntheXML-\nSchema registration process anumber ofunderlying\ndata structures areinferred from thedata deﬁnitions\nencoutered intheXML-Schema andmaintained by\ntheDBMS forstoring theactual content inastruc-\ntured manner .Amapping ofMPEG-7 schema el-\nements totheunderlying internal data structures is\ntherefore established within XDB.\n²Arepository ofdescriptions ;theactual content\ngenerated andmanipulated bytheapplication’ susers\nisinserted intoacombination oftables speciﬁcally\ndeﬁned fortheSoundPalette application andthe\nunderlying object-relational data structures created\nfrom theschema deﬁnitions during theschema reg-\nistration process.\n²Asetofindexes which serveforspeeding upqueries\nandforhelping tomaintain referential integrity within\nparticular elements orattrib utesintheXML descrip-\ntions.\n3.1.Accesstocontent\nTwointerf aces existtointeract with theMPEG-7 database\nlayer .Alow-levelSQL-based interf aceprovides direct ac-\ncess totables. IntheSoundPalette database severalcon-\ntenttables store theXML descriptions inasmanyrowsas\nrequired, generally wehaveonesingle record perMPEG-\n7document which isstored inoneoranother table de-\npending onitsfunctional conte xt.These tables include a\ncolumn ofOracle’ sXML TYPE data type which islinked\ntotheCUID ADO-e xtended MPEG-7 XML-Schema reg-\nistered with theDBMS. The SQL-based interf ace—that\nalsoincorporates functions tosupport theXPathstandard—\nprovides access andlocation totheelements andattrib utes\nwithin theXML descriptions. This low-levelinterf acehas\nalsoaverylowapplication dependenc y.\nThefollowing example showsthesyntax ofaquery to\nobtain thenames ofterms inanMPEG-7 classiﬁcation\nscheme forthose terms with atermID attrib ute”Piano”.\nXPathsyntax isused within thestatement fortwodiffer-\nentpurposes: ﬁrst intheﬁlter section (WHERE clause)\ntorestrict thenumber ofcandidate descriptions, then on\nthereturned documents toextract particular nodes, inthis\ncase theName elements.\nSELECT\nextract( x.content,'/child::node()/Name',\n'xmnls=\"urn:mpeg:mpeg7:schema:2001\"')\n.getStringVal()FROMCUI_CLASSIFICATION_SCHEMES x\nWHEREexistsnode(x.content,\n'//Term[@termID=\"Piano\"]') >0\nInthiscase, forsimplicity ,access istoasingle table\n(CUI CLASSIFICA TION SCHEMES) only,butastate-\nment canequally joinother tables thatstore either MPEG-\n7XML orotherwise simple object relational content.\nThesecond interf acethatwasbuiltisahigh levelAPI\nproviding application-speciﬁc functions, inthiscase very\nspeciﬁc tothefunctionality required bytheCUID ADO\nproject. Itservestohide thetables andthecomple xityof\ntheSQL statements totheapplications.\n3.2.Storage\nThestorage ishidden byboth oftheinterf aces. Itisper-\nformed inunderlying object relational datastrucutres, which\nareautomatically created bytheDBMS during XML-Schema\nregistration. Atinsertion time, theincoming description in\nXML format isparsed andstored distrib uted across allthe\nunderlying database objects which map theMPEG-7 data\nstructures deﬁned intheXML-Schemas registered.\nThis structured storage strate gyresults inaretrie val\nperformance directly comparable tothat ofarelational\ndatabase. Since data isstored within nativedata types,\ncommon B-Treeindexescanbeused toefﬁciently access\nthetargetdocuments without theneccesity toparse each\nofthem. Asadrawback, wehavetonote thesmall over-\nhead required forreconstructing theXML from theun-\nderlying data strucutres. This canhoweverbeneglected\ninmost ofthetypical queries which imply asearch overa\nlargenumber ofdocuments toreturn justafraction ofthe\ntotal.\nFurthermore, thestructured storage automatically avoids\nthedifﬁculties that areencountered when numerical or\ntemporal data isstored astextwithin thetextual XML for-\nmat, andrange queries canbeperformed naturally .\nAnadditional feature isthecompression effectobtained.\nXML tagscontained inthedescription documents, canac-\ncount forahigh percentage oftheoverall document vol-\nume. Tags,howeverdonotneed tobestored astheinter-\nnalobject types inherently mantain theMPEG-7 structure\nofthedescriptions.\nAfurther indexing structure hasbeen created tofacili-\ntatepreserv ation ofthereferential integrity indescriptions\nacross different description documents, andsomeho wex-\ntending thenotion ofIDandIDREF .Forinstance, the\nMPEG-7 layer guaranties thatanelement within aclassi-\nﬁcation scheme cannot bedeleted ormodiﬁed ifitisbeing\nused (referenced) inadescription.\n3.3.Searchbysimilarity\nSearches bysimilarity dousually notconstitute aproblem\ninterms ofdatabase model perse,since theycanbere-\nduced totheproblem ofevaluating adistance function on\nasetofrelevantfeatures. There arehoweverother types ofdatabase problems related tothese types ofsearches, typi-\ncally related tothelowscalability ofthesystems. Inorder\ntodoanexhausti vesearch bysimilarity ,thereference ob-\njectmust theoretically becompared toalltheobjects in\nthedatabase, byevaluating thedistance function, andfre-\nquently thecost ofcomputation ofthatdistance ishigh,\nmaking such anapproach notfeasible when thedatabase\nislarge.\nAsimple search bytimbre similarity function wasim-\nplemented according to[4]which scales well overacor-\npusof100000 simulated descriptions, which were created\nartiﬁcially with random values inthefeatures used bythe\nfunction.\n3.4.Extensions\nAlthough thisparticular database implementation hasbeen\nmade using only aspeciﬁc subset oftheMPEG-7 descrip-\ntors, those foraudio content management, webelie vethat\nitcanbeconsidered generic from thepoint ofviewof\nmetadata management andretrie valbased onmetadata in-\nformation, since atdatabase levelitsolely relies onXML\nandXML-Schema functionality ,andtherefore thisapproach\nshould beable tobeapplied totheﬁeld ofcontent-based\nmodeling andretrie valofvideo applications aswell, with\nverylittle adaptation efforts.\nAfurther feature ofthepresented MPEG-7 database\nlayer isthatitsupports extensibility ,inconformance to\ntheextensible design oftheMPEG-7 XML-Schema. In\nthecase ofthe(SoundPalette )application, asetofnew\ndescriptors formelodic andrhythm description notorigi-\nnally available inthestandard, hadtobeintroduced. The\nextension ismade inform ofanadditional application\nspeciﬁc XML-Schema document which imports thestan-\ndard MPEG-7 XML-Schema document andcreates ase-\nriesofspecialized descriptors such astheScale, theMeter ,\ntheKey,theMelodyContour intheform ofextensions of\ntypes already supplied bythestandard [5].\n4.APPLICA TIONEXAMPLE\nTheSoundPalette isanapplication forcontent based pro-\ncessing andauthoring ofmusic andiscompatible with\nMPEG-7 standard descriptions ofaudio. Ithasbeen de-\nsigned forusers who ownlargelibraries ofsounds and\nloops andoffersnovelwaystointeract andworkwith au-\ndio.Figure 1showsascreenshot oftheapplication’ scon-\ntentbased features. Adrum loop hasbeen analyzed and\nsegmented. This process hasgenerated anumber ofde-\nscriptors which havebeen inserted intothedatabase. The\nwaveform forthedrum loop isdisplayed intheupper re-\ngion (2channels since itisastereo ﬁle), andthetemporal\nsegments havebeen separated with vertical lines.\nInthiscase, thesegmentation hasalso been made for\nthedifferent instruments thatbuildupthedrum loop. This\nisvisible inthelower region, which showsthetwodi-\nmensions ofthesegmentation. Thex-axis isstillthetime\nbase, while they-axis carries thedifferent instruments en-Figure1.Screenshot ofSoundPalette\ncountered asaresult from thesegmentation. The beats\ncorresponding tofour different percussion instruments are\nvisible onthelowerregion.\nThis information, stored inMPEG-7 format, canbe\nthen used tolocate appropriate substitution sounds inthe\naudio database. Intheretrie val,manydifferent MPEG-7\ndescriptors canbeconsidered, such asrelated tophysical\nproperties (sampling rate, ﬁleformat), related toowner -\nship orrights holders, ormore interesting parameters di-\nrectly related totheactual content, such asSoundP alette\nthetimbre, theenergy,thepitch, etc.Inaddition, theuser\nhasthechoice tosettolerances foranyofthenumerical\ndescriptors, inorder tomodify theretrie valaccurac yof\nthesystem.\nFurthermore, SoundPalette andtheunderlying database\nallowtheusertoorganize thecontent collection with MPEG-\n7classiﬁcation schemes. These allowtheuser tobrowse\nthrough ahierarch yofcategories, asopposed tojustre-\ntrieving byissuing queries. Inorder toenhance thebrows-\ningexperience, relations within descriptions aresupported\natdatabase level.\nAllthismetadata information isstored inthedatabase\npresented aboveinMPEG-7 compliant format, andcanbe\nused inorder toeither setfurther ﬁlters when searching\nforaparticular content, ortodirectly retrie vethemedia\nfrom thevirtual containers thattheterms within theclas-\nsiﬁcation scheme constitute.\n5.CONCLUSIONS\nApplications based ontheMPEG-7 standard areemer ging\nintheareas ofmultimedia archi ve,digital broadcasting,\ndigital library ,etc. MPEG-7 provides description mech-\nanisms formultimedia content; however,applications are\nstillimmature andarenotreally explored inconcrete ﬁelds.\nInthisarticle wehavehighlighted thedatabase aspects of\nasystem forcontent based processing andauthoring of\nmusic inwhich MPEG-7 hasbeen successfully used.\nThe MPEG-7 standard hasoffered adequate tools tomodel features emplo yedtodescribe musical content, all\ninasatisf actory wayfrom theuser requirements perspec-\ntive.Furthermore, theXML based technologies thatthe\nMPEG-7 standard emplo ys,haveminimized thedevel-\nopment effortbyallowing theuseofawidely available\ndatabase management system capable ofefﬁciently man-\naging XML. TheMPEG-7 database layer developed pro-\nvides features notencountered inageneral XML database\nandalso hides totheapplication much ofthecomple x-\nityinvolvedinpersistently storing andmanipulating XML\ndescriptions.\nThose descriptors required bytheapplication butnot\ndeﬁned bytheMPEG-7 standard havebeen alsointegrated\nwithout major development effort, which leads tothecon-\nclusion thattheproposed approach isgenerally viable.\n6.ACKNOWLEDGEMENTS\nPartofthis workhasbeen supported bytheEuropean\nCommission, under theCUID ADO project (IST-1999-20194).\n7.REFERENCES\n[1]Frakes,w.,Baeza-Y ates, R.Information Re-\ntrieval:datastructuresandalgorithms Pren-\nticeHall, NewJerse y,1992\n[2]Manjunath, B.S.,Salembier ,P.andSikora,T.\n”Introduction toMPEG 7:Multimedia Con-\ntentDescription Language”. Ed.Wiley,2002.\n[3]Celma, O.,et.al.,”Tools forContent-Based\nRetrie valandTransformation ofAudio Using\nMPEG-7: The SPOf ﬂine andtheMDT ools”\nProceedings of25thInternational AESCon-\nference.London, UK, 2004.\n[4]Peeters, G.McAdams, S.Herrera, P.”Instru-\nment Description intheConte xtofMPEG-7”\nProceedings ofInternational Computer Music\nConference.Berlin, German y,2000.\n[5]G´omez, E.Klapuri, A.Meudic, B.”Melody\nDescription andExtraction intheConte xtof\nMusic Content Processing” JournalofNew\nMusicResearchVol.32.1 ,2003."
    },
    {
        "title": "Disambiguating Music Emotion Using Software Agents.",
        "author": [
            "Dan Yang 0002",
            "Won-Sook Lee"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415272",
        "url": "https://doi.org/10.5281/zenodo.1415272",
        "ee": "https://zenodo.org/records/1415272/files/YangL04.pdf",
        "abstract": "Annotating music poses a cognitive load on listeners and this potentially interferes with the emotions being reported. One solution is to let software agents learn to make the annotator’s task easier and more efficient. Emo is a music annotation prototype that combines inputs from both human and software agents to better study human listening. A compositional theory of musical meaning provides the overall heuristics for the annotation process, with the listener drawing upon different influences such as acoustics, lyrics and cultural metadata to focus on a specific musical mood. Software agents track the way these choices are made from the influences available. A functional theory of human emotion provides the basis for introducing necessary bias into the machine learning agents. Conflicting positive and negative emotions can be separated on the basis of their different function (reward-approach and threat-avoidance) or dysfunction (psychotic). Negative emotions have strong ambiguity and these are the focus of the experiment. The results of mining psychological features of lyrics are promising, recognisable in terms of common sense ideas of emotion and in terms of accuracy. Further ideas for deploying agents in this model of music annotation are presented.",
        "zenodo_id": 1415272,
        "dblp_key": "conf/ismir/YangL04",
        "keywords": [
            "Annotating music",
            "cognitive load",
            "emotional reporting",
            "software agents",
            "Emo prototype",
            "music annotation",
            "compositional theory",
            "functional theory",
            "bias introduction",
            "negative emotions"
        ],
        "content": "DISAMBIGUATING MUSIC EMOTION USING SOFTWARE AGENTS\nDan Yang WonSook Lee \nSystem s Science \nUniversity o f Ottawa \nOttawa C anada K1N 6N5 \ndyang006@uot tawa.ca School  of Inform ation Technol ogy and Engi neeri ng \nUniversity o f Ottawa \nOttawa C anada K1N 6N5 \nwslee@uottawa.ca \n \nABSTRACT \nAnnotating m usic poses a cognitive load on listeners \nand this potentially in terferes with  the em otions being \nreported. One solution is to  let so ftware ag ents learn  to \nmake the annot ator’s t ask easi er and m ore effi cient. \nEmo is a m usic annot ation prot otype that combines \ninputs from  both hum an and soft ware agent s to better \nstudy hum an l istening. A compositional theory of \nmusical meaning provi des t he overal l heuri stics for t he \nannot ation process, with the listener drawi ng upon \ndifferent influences such as acoustics, lyrics an d cultural \nmetadata to focus on a speci fic musical mood. Soft ware \nagents track the way these choices are m ade from  the \ninfluences available. A funct ional theory of hum an \nemotion provides the basis for introducing necessary  \nbias into the m achine l earni ng agent s. Conflicting \npositive and negative em otions can be separated  on the \nbasis of t heir different  funct ion (reward-approach and \nthreat-avoi dance) or dy sfunct ion (psy chotic). Negat ive \nemotions have st rong am biguity and these are the focus \nof the experi ment. The resul ts of m ining psy chological \nfeatures of lyrics are prom ising, recogni sable in terms of \ncommon sense i deas of em otion and i n terms of \naccuracy. Further ideas for de ploying agents in this \nmodel of m usic annot ation are present ed. \n \nKeywords:  annot ation, em otion, am biguity, agent s \n1. INTRODUCTION \nMusic retriev al system s typ ically lack  query-by-\nemotion, leaving it up t o the user t o know whi ch art ist, \nalbum, and genre names correl ate with the desi red \nmusical emotion. Progress i n this area i s hindered by  a \n“seri ous lack of annot ated dat abases t hat allow the \ndevel opment of bot tom-up dat a-driven tools for musical \ncontent extraction”  [8].  In t his paper we show t hat \nmachine learni ng t echni ques can be em bedded i n \nannot ation tools using software agent s, in support  of \nhigh-level design goal s.  \nOne high-level desi gn choi ce is to model musical meaning as com positional, and Em o allows different  \nusers to focus on di fferent  aspect s of t he stimulus \nmaterial to  specify its m usical meaning and em otion.  A \ndiverse range of st imulus m aterial is offered i n this \nenvironment, so  that su bjects are v ery willin g to \nelaborat e furt her about  their mental state, and t he \nunderl ying influences they are focusi ng on can be \ntraced. Such an integrated environm ent overcom es the \nproblem of cognitive overload of other environments \nwhere t he user i s burdened wi th dissimilar tasks of \nemoting and reporting. The use of multiple stim ulus \nmaterials (audi o, graphi cs, t ext lyrics, t ext revi ews) \nassists user need prom pting t o better art iculate and \nexternalise their feelings.  \nThe problem of the reliab ility o f emotion ratin gs is one \nof the m ain motivations for t he desi gn of Em o. \nConsistency of response i s underst ood relative to the \ncombination of st imuli leading up t o the response, and \nemotion is not taken as an absol ute propert y of any \nsingle media file by itself. While variations in response \nare allowed, there are also unwant ed vari ations such as \ndrift due t o the influence of ext reme songs, and \ndeterioration in the quality of response due to fatigue.  \nOutliers su ch as th ese can  be detected  by software \nagents that check for c onsistency bet ween some \ncombination of i nput stimuli and past  ratings. \n2. RELATED WORK \n2.1. Psychological perspectives \nThe structure of em otion st udied by  psy chologists \nincludes core affect  [14], em otion, m ood, attitude  [13], \nand temperam ent  [21]. Psy chological perspect ives on  \nmusic and em otion have t ended t o focus on peopl e’s \nverbal  report s of feel ing st ates and on whet her t hese \nemotion words can be structured according to how \nmany and whi ch dimensions  [14] [15] [19].  \n \nDimensional ratings are qui ck si ngle-item test scales for \neliciting em otions  [15], sui table for repeat ed use in \napplications such as annot ating short  musical segm ents. \nThe ai m is to not confuse t he user with the need to \ndiscrim inate b etween  sim ilar emotions, so related  \nemotions are placed close together on the dim ensional \nscale. This makes the single-item  test rating a useful \nentry point for el iciting em otion. Som e studies di rectly \nrelate dimensional scale ratings to m usical features such \nas tempo, or percei ved energy   [12]. Ot hers have Permission to m ake digital or  hard copies of all or  part of this work \nfor personal or  classr oom use is granted without fee pr ovided that \ncopies ar e not m ade or  distr ibuted for profit or commercial \nadvantage and that copies bear  this notice and the full citation on \nthe first page. \n© 2004 Univer sitat Pom peu Fabr a.   \n \n depicted the relationship between dimensional scales \nand musical features with great er complexity  [9].  Music \nemotion is generally seen as irreducible to simply one or two dimension ratings. For example, the online All \nMusic Guide  [4] uses over 160 different discrete \nemotion categories (e.g. trippy, quirky) to describe artists, by using up to 20 emotion words to describe the \ntone of their career’s work . Baumann’s Beagle system \n [1] mines text documents to collect all the emotion-related words (e.g. ooh, marry, love, wait, dance, fault), \nused in lyrics or online reviews  \n \nPsychological research has explored ways of unifying the dimensional and discrete approaches to emotion \nratings.  Sloboda and Juslin  [15] note that dimensional \nand discrete models can be complementary to each \nother.  One accessible approach is the PANAS-X test \nscale  [22] which has two dimensional ratings called Positive Affect (PA) and Negative Affect (NA). The \ndimensional ratings function as entry points to more \ndetailed ratings of discrete emotions under each axis \n(e.g. Fear under NA). The two PANAS-X dimensions \ncan be mathematically related to Russell’s circumplex model [14]. Russell’s Arousal is the sum of PA and NA, \nwhile Russell’s Valence is the difference (PA – NA).  \nTellegen, Watson and Clark  [19] use the Valence \ndimension (pleasant-unpleasant) as the top-level entry \npoint of a 3-layer model. This unified model offers the \nbenefits of dimensional ratings, plus a theoretical basis \nthat links the entry-point of the hierarchy to the discrete \nemotion categories at the base (as shown in Figure 1).  \n \n2.2. Systems for music annotation \nA number of online systems exist for annotating popular music emotion. The All Mu sic Guide collects user \nresponses from the web in dimensional form (e.g. exciting/relaxing, dynamic/calm). Moodlogic.com \ncollects user emotion ratings from the web in bipolar \ndimensional form (general mood positive or negative), in multivalent dimensional form (e.g. brooding, quirky) \nas well as discrete terms (e.g. love, longing). \nMoodlogic.com allows query-by-emotion using 6 \ndiscrete emotion categories (aggressive, upbeat, happy, \nromantic, mellow and sad). Songs are regularly labelled by two or more emotions. A query for two emotions \ntogether, ‘both happy and upbeat’, retrieved about half \nthe songs in the database. There was no way to \ndisambiguate the results into happy, upbeat as separate \nemotions.  \n \nMicrosoft MSN.com has 115 Mood/Theme discrete category names. No theory of emotion is used in this \ntaxonomy, which is based on a mix of artist names, \nemotion names, country names, and names of parts of \nthe daily routine such as workout, dinner, etc.  Musicat \n [3] also used names of ev eryday contexts such as \nbedtime as well as moods (happy, romantic) to label listening habits. The system learns which songs are played in which listening habit. \n \nThe above taxonomies tended to be ad hoc lists mixing together words for feelings, thoughts and everyday \nactivities instead of systematically examining these \naffective, cognitive and behavioural aspects of emotion. \n2.3. Systems for music data mining \nHuman listening is very effective at organizing the stream of auditory impulses into a coherent auditory \nimage. If digital signal processing primitives can be \nused to discern features of interest to a human listener, \nthen these are useful to add to the music emotion \nannotation environment. The evaluation of the best \nfeatures is hindered by a lack of standardized databases \n [5].  Current feature extraction tools are very low-level, \nsuch as MPEG-7 Low Level Descriptors  [6].  \n \nRecently, wavelet techniques ha ve been developed that \ntile the acoustic landscape into smaller features that \ncorrespond to musical elements such as octaves  [20].  \nAnother innovation is the automatic discovery of feature \nextractors. Sony’s Extractor Discovery System (EDS) \n [12], uses genetic programming to construct trees of DSP operators that are highly correlated to human-\nperceived qualities of music.  \n \nBaumann’s Beagle system  [1] demonstrates the \nrelevance of mining music reviews and lyrics for \nemotion words co-occurring with artist names and song \nnames. State-of-the-art  performance on extracting \nindividual names from text is about 90%, but accuracy falls below 70% when compound relations such as (artist, song, emotion) because errors multiply.  \n3. EMO SYSTEM \n3.1. Motivation \n \nInitially we were looking for online datasets of music already annotated by discrete emotion labels, and what \nwe found was an ad hoc mix of discrete terms, including \ncognitive, behavioural and affective words. Single-item \ndimensional ratings did not separate like emotions, such \nas happy/upbeat or anger/fear. Hierarchical models of music emotion recognition have been reported by Liu \n [10], and we decided to  extend the hierarchical \napproach further to the level of discrete emotion categories. The key problem we found annotating music \nin greater detail was the c ognitive load on the annotator \nin both listening and reporting in detail. The solution we \nare developing uses software agents that learn to make \nthe annotation task more efficient.     \n \n3.2. Emotion model \nIn this paper we focus on Negative Affect as these \nemotions are less distinguishable from  each other than \npositive em otions are from  each other  [21]. This finding \nis shown in the way em otions such as fear and anger are \nhighly correl ated in the dimensional model. In real \nterms, this can be seen  in the way th at related  episodes \nof negative em otion such as h ostility, p aranoia and \nsadness occur in depressiv e illness.  \n \nThe W atson m odel  [19] [21] expl ains negat ive emotion \nas the threat-avoidance func tion i n the structure of \nemotions. Taking fear and anger as an exam ple, these \ncategori es are both hi gh i n negat ive affect  on a \ndimensional scale (high Negat ive Affect  in Figure 1 ), but  \nthey can be di stinguished apart  based on their pattern of \nresponse t o the threat. Fear ant icipates a threat and \ntriggers flight, while anger can i nvolve a fi ght agai nst \nthe threat. By finding m ore inform ation about  the \npattern of response t o a threat, using inform ation from  \nboth the lyrics and t he music, negat ive em otions can be \ndistinguished from  each other into discrete classes. \n \nTaking anger and guilt as an  example, th ese are both \nhigh in negat ive affect  on a di mensional scal e (high \nNegative Affect in Figure 1 ) and pract ically uncorrel ated \nwith Positive Affect. Gu ilt is related  to feelin gs that persist a little time after so me event, wh ile Ho stility is \ndirectly involved wi th som e threat event . \n \nSadness an d guilt sh ow some separation at the middle \nlevel of the em otion m odel, because sadness is slightly \ncorrelated  with Positive Affect wh ile g uilt is n ot. This \nimplies that sadness and guilt can  be better \ndifferen tiated  by assessin g th e Po sitive Affect \ncomponent, given lack of separability in term s of \nNegative Affect.  \n \nThe Tel legen-W atson-C lark m odel discussed in Section \n2.1 is useful  in linking the dimensional and di scret e \nlevels of em otion. The experi ments and resul ts in \nsection 4 are based on t his model shown i n Figure 1. \n  \n4. EXPERIMENTS \n4.1. Music emotion intensity prediction \nThis first experi ment was desi gned t o implement a \nclassifier for m usic em otion intensity, underst ood  i n \nterms of the psy chological models of R ussell  [14] and \nTellegen-W atson-C lark  [19], where intensity represent s \nthe sum  of t he PA and NA di mensions (i n Figure 1).\nThe em otional intensity ratin g scale was calib rated  to \nMicrosoft ’s em otion annot ation pat ent  [17] , usi ng the \nsame method to train a volunteer. The initial database consi sted of 500 random ly-chosen rock song \nsegm ents of 20 seconds each taken beginning a third \nof the way  into the song.  High PA\nPleasantness\nLow NA \nLow PAUnpleasantness High NA delighted \nalert \nexcited \nfocused\nsleepy \ntired \nsluggish at rest \ncalm  \nrelaxedangry  \ndistressed\nfearful \nafraid \nscared \nasham edHigh NA happy  \njoyful \ndiscouraged \nsad \ndownheart ed quiet \nstill amazed \nsurprised \nastonished\n \nFigure 1.  Elements of the Tel legen-W atson-C lark em otion m odel  [19]  [21]. Dot ted lines are t op-\nlevel dimensions. The Positive Affect (PA) an d Neg ative Affect (NA) d imensions shown as \nsolid lines form  the middle of t he hierarchy , and provi de heuri stics needed to discern the \nspeci fic discret e emotion words based on funct ion. Di scret e emotions t hat are cl ose t o an axi s \nare hi ghly correlated  with  that dimension, e.g. sad is sli ghtly correlated  with  positive affect. \n   \n \n \nAcoust ic feature extraction used a num ber of t ools to \ngive a broad mix from  whi ch to select the best  \nfeatures.  \nWavelet tools  [20] were used t o subdi vide the signal \ninto bands approxi mating oct ave boundari es, and then \nenergy extraction and autocorrelation were used to \nestim ate Beats per Minute (BPM ). Other acoustic \nattributes i ncluded l ow-level standard descri ptors \nfrom  the M PEG-7 audi o standard (12 at tributes). \nTimbral features included spectral centroid, spectral \nrolloff, spect ral flux, and spect ral kurt osis. Anot her 12 \nattributes were generat ed by a genet ic algorithm using \nthe Sony  Extractor Di scovery  System (EDS)  [12] with \nsimple regressi on as t he popul ation fi tness cri teria. \nLabels of intensity fro m 0 to 9 were ap plied to \ninstances by a hum an listener report ing the subject ive \nemotional intensity, fo llowing exactly th e human \nlistening training m ethod i n the Microsoft  patent  [17]. \n \nThe W EKA package  [23] was used for machine \nlearni ng. The resul ts below were calculated using \nSupport  Vect or M achine (SVM ) regressi on.  \n4.1.1.  Resul ts and anal ysis for emot ion intensity \nThis experi ment confi rmed the resul ts of Li u  [10] \nwhich found t hat em otional intensity was hi ghly \ncorrel ated wi th rhy thm and t imbre feat ures. We \nachieved alm ost 0.90 correlation (mean absolute error \n0.09), t he best  features bei ng B PM, Sum  of Absol ute \nValues of Norm ed Fast  Fouri er Transform  (FFT), and \nSpectral Kurtosis  [7].  \n4.2. Disambi guation of emoti on usi ng text mi ning \nIn our com positional model of m usical emotion, non-\nacoustic features such as lyrics words or social \ncontextual cont ent do pl ay a role of focusing specific \nemotions and hel p account  for t he range of emotional \nresponses t o the sam e song. One probl em with this \napproach is the size of the vocabulary used in \nexpressi ve l yrics, whi ch coul d be over 40,000 \ndifferent  words for songs i n Engl ish. Various feature-\nreduction strategies are used in classic m achine \nlearni ng, but  it is not certain how wel l these apply to \nemotion detection. W e chose an est ablished approach, \nthe General  Inqui rer [18],  to begi n to explore the \navailable techni ques for verbal  emotion identification. \nThis system was chosen for i ts good coverage of most \nEngl ish words, and i ts com pactness of represent ation, \nwith 182 psy chological features.   \n \nOf 152 30-second clips of Al ternative Rock songs \nlabelled with emotion cat egori es by a vol unteer, onl y \n145 songs had l yrics. The em otion cat egori es of the \nPANAS-X schedule  [22] were used. Lyrics text files \nwere t ransform ed into 182-feat ure vect ors usi ng the \nGeneral  Inqui rer package, and t he W EKA machine learning package was used. The original tex t files \nwere al so exam ined usi ng the Rainbow text mining \npackage [11]. \n4.2.1.  Resul ts and anal ysis for text disambi guation \nThis ex periment tested  the id ea th at general \npsychological feat ures dri ving em otions, such as \nseeki ng and at taining goal s or react ing to threats, can \nbe convey ed speci fically in text and can add focus to \nthe way m usic is in terpreted .  \n \nHostility expletives, n ot, get, got, wan t, never, \ndon’t , go, no, m y, oh, fi ght, burn, show, \nhad, y ou \nSadness love, l ife, time, say , slowly, hol d, feel , \nsaid, say, go, if \nGuilt one, lost, heart, face, alone, sleep, \nmistake, m emory, lies, eyes, die, silence, \nremember \nTable 1. Lyric words that distinguish ly rics by  \nnegative emotion i.e. have high information gain. \n \nHostility Words about no gain from  love or \nfriendshi p,  \nNot being a particip ant in  love or \nfriendshi p, W ords about  not  \nunderst anding,  \nWords expressi ng a need or i ntent \nSadness Loss of wel l-being,  \nWords about  a gai n of wel l-being wi thout \ncolor or rel ationshi p \nGuilt Sayin g-type words,  \nTalking about  gai ns from  love or \nfriendshi p, Passi ve-type words  \nTable 2. General Inquirer[18]  psychological features \nof ly rics text that most distinguish lyrics by negative \nemotion in the W EKA C4.5 decis ion tree. \nHostility, Sa dness a nd Guilt  \nThe negat ive affect  behavi ours are rel ated to threat \navoidance, so words st rongl y related to distinguishing \neach negative emotion from  other negative em otions \nwere ranked usi ng their inform ation gain  [23]. The \ndata sh own in Tab le 1 includes forms of hostile \ndisplay shows i n the form  of threatening expletives, \nand sounds showi ng lack of const raint such as ah, oh. \nOther words t ended t o connot e com mands or t hreats, \nsuch as no, don’t  etc. There were references to \n‘weapons’ and dest ruction such as fi re, burni ng etc. \nWords that favoured guilt over hostility are related to \nwaki ng/sleeping, m istakes, and refl ection. The \nreferen ces to  low en ergy activ ities in guilt are \ninterestin g, considering that th e music is as arousing \nas hostility. \n \n   \n \nSadness was al so interesting in strongl y referri ng to \npositive words such as lo ve, life, feel etc. Th is higher \ncorrelatio n of sadness to positive affect is predicted  in \nthe em otion m odel. \n \nTable 2 shows t he psy chological features mined from  \nlyrics usi ng W EKA’s i mplementation of C4.5. \nInform ally, these features appear to m ake sense, and \nthe machine learni ng has m ined verbal  patterns t hat \none woul d expect  to correspond wi th these negat ive \nemotions, such as needing-ty pe words associated with \nanger. \n Love, Ex citement, Prid e \n \nLove Not knowi ng-type words,  \nNot political,  \nNot loss of well-b eing,  \nNot negative, \nNot failin g, \nGains from  love and fri endshi p,  \nPassive-ty pe word,  \nNot saying-type word \nExcitement Gains of wel l-being from  relationshi p, \nAnim al-type words \nPride Political wo rds,  \nRespect,  \nInitiate ch ange,  \nKnowi ng-type words \nAtten tive Knowing-type words,  \nColor words \nReflectiv e Passiv e type words \nCalm  Completion of a goal-typ e words \nTable 3. General Inquirer[18]  features that most \ndistinguished lyrics by  positive em otion in the \nWEKA C4.5 decis ion tree. \n \nTable 3 shows t he psy chological features that WEKA \nmined from  song lyrics associated with positive \nemotions. Inform ally, these resul ts are recogni zable in \nterms our com mon-sense underst anding of em otions.  \n4.3. Disambi guation of emoti on usi ng data fusi on \nThis experi ment fused t ogether bot h acoust ic and text \nfeatures to m aximise the classification accuracy.  \n \nThere was an increase in  accuracy of successful \nclassification from  80.7% to 82.8%, a decrease in \nmean error from  0.033 t o 0.0252. The decrease i n \nroot relative squared error was from  30.62% t o \n25.04%. \n  \nThese resul ts do not  distinguish very much between \nthe two procedures on such a sm all training set  \nwithout testing, but the num bers do not  cont radict the \ninform al discussion in the preceding section. A larger \nstudy woul d have m ore scope for exam ining the different ty pes of errors in classification from  the \nacoust ic and t ext features. \n5. CONCLUSION AND FUTURE WORK \nThis paper eval uated a st ructured em otion rat ing \nmodel for em bodiment in soft ware agent s to assist \nhuman annot ators i n the music annot ation sy stem \nEmo. In experi ments we found t he structured emotion \nmodel useful  in the cont ext of a com positional model \nof musical m eaning and em otion, where text features \nfocused at tention on m ore speci fic music emotions. \nExperi ments were designed to expl ore t his model, \nand we focused on negat ive em otions where t here is \nthe greatest am biguity. \n \nResu lts were given for a sin gle-attrib ute test to  rate \nemotion intensity (the sum of positive and negative \nenergy  in the model), based on 500 songs. About  \n90% accuracy was achieved using both tim bral and \nrhythmic feat ures. For l earni ng to distinguish like-\nvalenced emotions, a sam ple of 145 ful l-text lyrics \nshowed prom ising resul ts. Inform ally, the verbal  \nemotion features based on General  Inqui rer appeared \nto correlate with  significan t emotion experiences \nreported by listen ers. Th e sm all sam ple size \nprecl uded robust  testing at  this exploratory stage. The \nallmusic.com [4] song browser, wi th 1000s of songs \nclassified by  mood, coul d be one way  to increase the \nsample size significantly. \n \nFuture wo rk will in vestigate th e way in  which a \ncompositional model of m usical meaning and \nemotion can be depl oyed usi ng graphi cal user \ninterface devices for the us er. The system  tracks the \nfocus of attention as each em otion is experienced by \nthe user, and t he resul ting annot ation trees can be \nmined t o hel p confi rm the theory of music as \ncompositional. Su btle sh ifts in  cognitive focus can \ncorrespond with shifts in musical meaning and \nemotion. Di fferent  methods of verbal  emotion \nidentificatio n will be investigated, as th is is a n ew \nand rapi dly growi ng area of m achine learni ng \nresearch. The existing boot strap dat abase appears \nadequate, an d with  further use there will b e more \nsongs added t o the dat abase, com plete wi th \nappropri ate stimulus m aterial such as l yrics and \ncultural d ata. Th e em erging Sem antic Web also \nprovides further opportun ities to find m usical \nstimulus material by means of a shared m usic \nontology  [1]. Graphi cal media types are also relevant \nas stimulus m aterial for  popul ar music such as music \nvideos. Vi sual features c ould be extracted from  music \nvideos as M PEG-7 vi deo descri ptors, and related to \nthe function of each em otion. Som e researchers \nbelieve that a more prom ising approach t o rating \nemotion is to use di rect physiological means  [2]. This \ntype of input coul d be added t o the resource hi erarchy  \nof Em o.   \n   \n \n6. REFERENCES \n[1] Baumann, S, & Kl üter, A., “Super-\nconveni ence for Non-m usicians: Query ing \nMP3 and t he Sem antic Web”, Proceedings \nof the Int ernational Symposi um on Musi c \nInformation Retrieva l, Paris, France, 2002.  \n[2] Cacioppo, J.T., Gardner, W .L. & Berntson, \nG.G., “The Affect System  Has Parallel and \nIntegrative Processi ng C omponent s: Form  \nFollows Funct ion”, Journal  of Personal  and \nSocial Psychol ogy, Vol. 26, No. 5, 1999, \n839-55. \n[3] Chai, W ., “Using User Models in Music \nInformation Retriev al System s”, \nProceedings of the International Symposium \non Music In formation Retrieva l, Plymouth, \nMA, USA, 2000.  \n[4] Datta, D., “Managing m etadata”, \nProceedings of the International Symposium \non Mu sic In formation Retrieva l, Paris, \nFrance, 2002.  \n[5] Downie, J.S., “To wards the Scien tific \nEvaluation of M usic Inform ation R etrieval \nSystem s”, Proceedings of the International \nSymposi um on Musi c Inf ormat ion Ret rieval, \nBaltim ore, MD, USA, 2003. \n[6] ISO/ IEC TC 1/SC29/WG11, “ISO/ IEC \n15938-6 Inform ation Technol ogy – \nMultim edia Content Description Interface \n(MPEG-7) – Part 6: Reference Software”, \nGeneva, Swi tzerland, 2000. \n[7] Kenney , J. F., Keepi ng, E. S., Sect ion 7.12 \nin “Math ematics o f Statistics” Pt.1 , 3rd ed. \nPrinceton, NJ; Van Nost rand, pp.102-103, \n1962. \n[8] Leman, M ., “GOASEM A – Semantic \ndescri ption of m usical audio”. Retrieved \nfrom  http://www.ipem .ugent.be/. \n[9] Leman, M ., Verm eulen, V., De Voogdt , L., \nTaelm an, J., M oelants, D. & Lesaffre, M.,  \n“Correl ation of Gestural Music Audi o Cues \nand Perceiv ed Ex pressiv e Qu alities”, \nLectu re Notes in  Artificia l Intellig ence,  Vol. \n2915, 40-54, Spri nger Verl ag, Hei delberg, \nGerm any, 2004. \n[10] Liu, D., Lu, L. & Zhang, H.J., “Aut omatic \nMood Det ection from  Acoust ic Music \nData”, Proceedings of the International \nSymposi um on Musi c Inf ormat ion Ret rieval, \nBaltim ore, MD, USA, 2003. \n[11] McCallu m, A., “Bo w: A To olkit fo r \nStatistical Language Modelling, Text Retrieval, Classification and Clustering”. \nFrom  www-2.cs.cm u.edu/ ~m ccallum /bow. \n[12] Pachet , F. & Zi ls, A., “Evol ving \nAutomatically Hig h-Level Music \nDescri ptors from  Acoust ic Signals”, Lecture \nNotes In C omput er Sci ence, Vol . 2771, 42-\n53 Spri nger Verl ag, He idelberg Germ any, \n2004. \n[13] Russell, J.A., W eiss, A. & M endelsohn, \nG.A., “Affect Grid: A Single-Item  Scale of \nPleasure and Arousal”, Journal  of  \nPersonality and Social Psychology , 57, 3, \n495-502, 1989. \n[14] Russell, J.A., “Core affect and the \npsychological const ruction of em otion”, \nPsychological Review  Vol. 110, No. 1, 145-\n172, Jan 2003. \n[15] Scherer, K.R., “Toward a dy namic theory of \nemotion”, Geneva Studies in Emot ion, No. \n1, 1-96 Geneva Swi tzerland, 1987.  \n[16] Sloboda, J.A. and Juslin, P.N. \n“Psy chological perspect ive on em otion”, in \nJuslin, P.N. and Sl oboda, J.A.(eds.), Music \nand Emot ion, Oxford University  Press, New \nYork, NY, USA, 2001. \n[17] Stanfield, G.R ., “Sy stem and m ethods for \ntraining a t rainee t o classify fundam ental \nproperties of media en tities”, US Paten t \nAppl ication No. 20030041066, Feb 27, \n2003. \n[18] Stone, P. J., The general  inquirer  a \ncomput er approach t o cont ent anal ysis. MIT \nPress, C ambridge M A USA, 1966. \n[19] Tellegen, A., W atson, D. & C lark, L.A., “On \nthe dimensional and hi erarchical structure of \naffect”, Psychol ogical Science, Vol. 10, No. \n4, Jul y 1999. \n[20] Tzanetakis, G., “Manipul ation, Anal ysis and \nRetrieval Systems for Audi o Signals”, PhD \nThesis, Prin ceton University, Princeton, NJ, \nUSA, 2002. \n[21] Watson, D., Mood and Temperament , \nGuilford Press, New York, NY, USA, 2000. \n[22] Watson, D. & Clark, L.A, “The PANAS-X \nManual for the Positive and Neg ative Affect \nSchedul e – Expanded Form ”. Retrieved \nfrom  http://www.psy chology .uiowa.edu/  \n[23] Witten, I.H., & Frank, E., Data Mining: \nPract ical machi ne learning t ools and \ntechni ques w ith Java implement ations, \nMorgan Kaufm ann, San Franci sco, C A, \nUSA, 2000 ."
    },
    {
        "title": "Automatic Drum Sound Description for Real-World Music Using Template Adaptation and Matching Methods.",
        "author": [
            "Kazuyoshi Yoshii",
            "Masataka Goto",
            "Hiroshi G. Okuno"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415958",
        "url": "https://doi.org/10.5281/zenodo.1415958",
        "ee": "https://zenodo.org/records/1415958/files/YoshiiGO04.pdf",
        "abstract": "This paper presents an automatic description system of drum sounds for real-world musical audio signals. Our system can represent onset times and names of drums by means of drum descriptors defined in the context of MPEG-7. For their automatic description, drum sounds must be identified in such polyphonic signals. The prob- lem is that acoustic features of drum sounds vary with each musical piece and precise templates for them can- not be prepared in advance. To solve this problem, we propose new template-adaptation and template-matching",
        "zenodo_id": 1415958,
        "dblp_key": "conf/ismir/YoshiiGO04",
        "keywords": [
            "automatic description system",
            "drum sounds",
            "real-world musical audio signals",
            "onset times",
            "drum descriptors",
            "MPEG-7",
            "polyphonic signals",
            "acoustic features",
            "drum identification",
            "template-adaptation"
        ],
        "content": "AUTOMATIC DRUM SOUND DESCRIPTION FOR REAL-WORLD MUSIC\nUSING TEMPLATE ADAPTATION AND MATCHING METHODS\nKazuyoshi Yoshii†Masataka Goto‡Hiroshi G. Okuno†\n†Department of Intelligence Science and Technology\nGraduate School of Informatics, Kyoto University, Japan\n‡National Institute of Advanced Industrial Science and Tech nology (AIST), Japan\nABSTRACT\nThis paper presents an automatic description system of\ndrum sounds for real-world musical audio signals. Our\nsystem can represent onset times and names of drums\nby means of drum descriptors deﬁned in the context of\nMPEG-7. For their automatic description, drum sounds\nmust be identiﬁed in such polyphonic signals. The prob-\nlem is that acoustic features of drum sounds vary with\neach musical piece and precise templates for them can-\nnot be prepared in advance. To solve this problem, we\npropose new template-adaptation and template-matching\nmethods. The former method adapts a single seed tem-\nplate prepared for each kind of drums to the corresponding\ndrum sound appearing in an actual musical piece. The lat-\nter method then can detect all the onsets of each drum by\nusing the corresponding adapted template. The onsets of\nbass and snare drums in any piece can thus be identiﬁed.\nExperimental results showed that the accuracy of identi-\nfying bass and snare drums in popular music was about\n90%. Finally, we deﬁne drum descriptors in the MPEG-7\nformat and demonstrate an example of the automatic drum\nsound description for a piece of popular music.\nkeywords : automatic description, polyphonic music,\ndrum sounds, template-adaptation, template-matching\n1. INTRODUCTION\nThe automatic description of contents of music is an im-\nportant subject to realize more convenient music infor-\nmation retrieval. Today, audio editing, music composing\nand digital distribution of music are very popular because\ntechnological advances with respect to computers and the\nInternet are remarkable. However, we have a few efﬁcient\nways to retrieve our favorite musical pieces from huge\nmusic databases (i.e., exploration is limited to artist-ba sed\nor title-based queries). In these backgrounds, many stud-\nies have addressed the content-based music information\nretrieval by describing music contents [4, 12, 18].\nIn this paper, we discuss an automatic description sys-\ntem of drum sounds. We aim at symbolically represent-\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom use is granted without fee provided th at copies\nare not made or distributed for proﬁt or commercial advantag e and that\ncopies bear this notice and the full citation on the ﬁrst page .\nc/circlecop†rt2004 Universitat Pompeu Fabra.ing onset times and names of drums by means of drum\ndescriptors deﬁned in the context of MPEG-7. MPEG-\n7 is a standardization to describe contents of multimedia.\nG´ omez et al. [4] and Peeters et al. [18] designed instru-\nment descriptors in the MPEG-7 format and claimed their\nimportance in music information retrieval. Kitahara et\nal.[12] discussed the identiﬁcation of harmonic sounds to\nautomatically describe names of instruments by using in-\nstrument descriptors. However, no research has addressed\nthe automatic drum sound description.\nBecause drums play an important role in contempo-\nrary music, the drum sound description is necessary to ac-\ncurately extract various features of music that are useful\nfor music information retrieval (e.g., rhythm, tempo, beat ,\nmeter and periodicity). Previous researches, however, ex-\ntracted those features by numerical analysis, not consid-\nering symbolic information with respect to drum perfor-\nmances [9, 15, 16, 20]. Some researches, for example,\naddressed a genre classiﬁcation problem [1, 21]. Charac-\nteristic or typical drum patterns are different among gen-\nres (e.g, rock-style, jazz-style or techno-style). Theref ore,\nsymbolic information of drum sounds provides good clues\nfor the genre classiﬁcation. In addition, it distributes to\nmusic information retrieval which considers users’ prefer -\nences to music because drum patterns are closely related\nto a mood of a musical piece [13].\nIt is required for the automatic drum sound description\nto identify drum sounds in real-world CD recordings. To\nidentify instrument sounds with the harmonic structure,\nseveral methods have been proposed [2, 14]. Those meth-\nods assuming the harmonic structure, however, cannot be\napplied to drum sounds. Some researches addressed the\ndrum sound identiﬁcation for solo tones [8, 10, 11] or syn-\nthesized signals by MIDI [3, 5, 17]. Others discussed the\nextraction of drum tracks, but did not mention the identi-\nﬁcation [22]. The accurate drum sound identiﬁcation for\nreal-world polyphonic music is still difﬁcult problem be-\ncause it is impossible to prepare, in advance, all kinds of\ndrum sounds appearing in various musical pieces.\nTo identify drum sounds, we propose new template\nadaptation and matching methods:\n•The template-adaptation method uses template\nmodels of the power spectrum of drum sounds. The\nadvantage of our method is that only one template\nmodel called “ seed template ” is necessary for each… … … … …\n…\nseed template adapted template spectrum \nexcerpts musical \naudio signal\npower frequency \nST\n0T2TAT1P19 P30 P31 P47 P62 P85 PNP\n…1T…\niterative adaptation Template-Refinement median median Rough-Onset-Detection \nExcerpt-Selection\nFigure 1 . Overview of template-adaptation method: The template is t he power spectrum in the time-frequency domain.\nThis method adapts the single seed template to the correspon ding drum sounds appearing in an actual musical piece. The\nmethod is based on an iterative adaptation algorithm, which successively applies two stages — the Excerpt-Selection stage\nand the Template-Reﬁnement stage — to obtain the adapted template.\nkind of drums: the method does not require a large\ndatabase of drum sounds. To identify bass and snare\ndrums, for example, we should prepare just two\nseed-templates (i.e., prepare a single example for\neach drum sound).\n•The template-matching method is developed to\nidentify all the onset times of drum sound after this\nadaptation. It uses a new distance measure that can\nﬁnd all the drum sounds in the piece by using the\nadapted templates.\nThe rest of this paper is organized as follows. First,\nSection 2 and 3 describe the template-adaptation and\ntemplate-matching methods respectively to identify bass\nand snare drum sounds. Next, Section 4 shows experi-\nmental results of evaluating those methods. In addition,\nit demonstrates an example of the drum sound description\nby using drum descriptors deﬁned in the standard MPEG-\n7 format. Finally, Section 5 summarizes this paper.\n2. TEMPLATE ADAPTATION METHOD\nIn this paper, templates of drum sounds are the power\nspectrum in the time-frequency domain. The promisingadaptation method of Zils et al. [23] worked only in the\ntime domain because they deﬁned templates consisting of\naudio signals. Extending their idea, we deﬁne templates in\nthe time-frequency domain because non-harmonic sounds\nlike drum sounds are well characterized by the shapes of\npower spectrum. Our template-adaptation method uses a\nsingle base template called “ seed template ” for each kind\nof drums. To identify bass and snare drums, for exam-\nple, we require just two seed templates, each of which is\nindividually adapted by the method.\nOur method is based on an iterative adaptation algo-\nrithm. An overview of the method is depicted in Fig-\nure 1. First, the Rough-Onset-Detection stage roughly\ndetects onset candidates in the audio signal of a musical\npiece. Starting from each of them, a spectrum excerpt is\nextracted from the power spectrum. Then, by using all\nthe spectrum excerpts and the seed template of each drum\nsound, the iterative algorithm successively applies two\nstages — the Excerpt-Selection andTemplate-Reﬁnement\nstages — to obtain the adapted template.\n1. The Excerpt-Selection stage calculates the distance\nbetween the template (either the seed template or\nthe intermediate template that is in the middle ofadaptation) and each of the spectrum excerpts by\nusing a specially-designed distance measure. The\nspectrum excerpts of a certain ﬁxed ratio to the\nwhole are selected by ascending order with respect\nto the distances.\n2. The Template-Reﬁnement stage then updates the\ntemplate by replacing it with the median of the se-\nlected excerpts. The template is thus adapted to the\ncurrent piece and used for the next iteration.\nEach iteration consists of these two stages and the iteratio n\nis repeated until the adapted template converges.\n2.1. Rough Onset Detection\nThe Rough-Onset-Detection stage is necessary to reduce\nthe computational cost of the two stages in the iteration. It\nmakes it possible to extract a spectrum excerpt that starts\nfrom not every frame but every onset time. The detected\nrough onset times do not necessarily correspond to the ac-\ntual onsets of drum sounds: they just indicate that some\nsounds might occur at those times.\nWhen the power increase is high enough, the method\njudges that there is an onset time. Let P(t, f)denote the\npower spectrum at frame tand frequency fandQ(t, f)\nbe the its time differential. At every frame (441 points),\nP(t, f)is calculated by applying the STFT with Hanning\nwindows (4096 points) to the input signal sampled at 44.1\nkHz. The rough onset times are then detected as follows:\n1. If∂P(t, f)/∂t > 0is satisﬁed for three consecutive\nframes ( t=a−1, a, a + 1),Q(a, f)is deﬁned as\nQ(a, f) =∂P(t, f)\n∂t/vextendsingle/vextendsingle/vextendsingle/vextendsingle\nt=a. (1)\nOtherwise, Q(a, f) = 0 .\n2. At every frame t, the weighted summation S(t)of\nQ(t, f)is calculated by\nS(t) =2048/summationdisplay\nf=1F(f)Q(t, f), (2)\nwhere F(f)is a function of lowpass ﬁlter that is de-\ntermined as shown in Figure 2 according to the fre-\nquency characteristics of typical bass or snare drum\nsounds.\n3. Each onset time is given by the peak time found by\npeak-picking in S(t).S(t)is smoothed by the Sav-\nitzky and Golay’s smoothing method [19] before its\npeak time is calculated.\n2.2. Seed Template and Spectrum Excerpt\nPreparation\nSeed template TS, which is a spectrum excerpt prepared\nfor each of bass and snare drums, is created from audio\nsignal of an example of that drum sound, which must be\nmonophonic (solo tone). By applying the same method100 200 frequency bin 0pass ratio \n1.0 )(fF\nf\nFigure 2 . Function of the lowpass ﬁlter according to the\nfrequency characteristics of typical bass and snare drums.\nwith the Rough-Onset-Detection stage, an onset time in\nthe audio signal is detected. Starting from the onset time,\nTSis extracted from the STFT power spectrum of the sig-\nnal.TSis represented as a time-frequency matrix whose\nelement is denoted as TS(t, f)(1≤t≤15[frames] ,1≤\nf≤2048 [bins]). In the iterative adaptation algorithm, a\ntemplate being adapted after g-th iterations is denoted as\nTg. Because TSis the ﬁrst template, T0is set to TS.\nOn the other hand, spectrum excerpt Piis extracted\nstarting from each detected onset time oi(i= 1,· · ·, N)\n[ms] in the current musical piece. Nis the number of the\ndetected onsets in the piece. The spectrum excerpt Piis\nalso represented as a time-frequency matrix whose size is\nsame with the template Tg.\nWe also obtain ´Tgand´Pifrom the power spectrum\nweighted by the lowpass ﬁlter F(f):\n´Tg(t, f) = F(f)Tg(t, f), (3)\n´Pi(t, f) = F(f)Pi(t, f). (4)\nBecause the time resolution of the onset times roughly\nestimated is 10 [ms] (441 points), it is not enough to obtain\nhigh-quality adapted templates. We therefore adjust each\nrough onset time oi[ms] to obtain more accurate spectrum\nexcerpt Piextracted from adjusted onset time o′\ni[ms]. If\nthe spectrum excerpt from oi−5[ms] or oi+ 5[ms] is\nbetter than that from oi[ms],o′\ni[ms] is set to the time\nproviding the better spectrum excerpt as follows:\n1. The following is calculated for j=−5,0,5.\n(a) Let Pi,jbe a spectrum excerpt extracted from\noi+j[ms]. Note that the STFT power spec-\ntrum should be calculated again for oi+j\n[ms].\n(b) The correlation Corr(j)between the template\nTgand the excerpt Pi,jis calculated as\nCorr(j) =15/summationdisplay\nt=12048/summationdisplay\nf=1´Tg(t, f)´Pi,j(t, f),(5)\nwhere ´Pi,j(t, f) =F(f)Pi,j(t, f).\n2. The best index Jis determined as index jthat max-\nimizes Corr(j).\nJ= argmax\njCorr(j). (6)\n3.Piis determined as Pi,J.seed template spectrum excerpt \nincluding target drum sound Large Distance \nsplit to blocks \nSmall Distance power power power frequency \nfrequency \nfrequency small \nsmall \nlarge \nsmall \npower summation in block \nFigure 3 . Our improved log-spectral distance measure to\ncalculate the appropriate distance (quantization at a lowe r\nfrequency resolution).\n15frames \n2048bins 5bins \n2frames …power summation \nin unit \nframe frequency \n0\nFigure 4 . Our implementation of the quantization at a\nlower time-frequency resolution for our improved log-\nspectral distance measure.\n2.3. Excerpt Selection\nTo select a set of spectrum excerpts that are similar to the\nintermediate template Tg, we propose an improved log-\nspectral distance measure as shown in Figure 3. The spec-\ntrum excerpts whose distance from the template is smaller\nthan a threshold are selected. The threshold is determined\nso that the ratio of the number of selected excerpts to the\ntotal number is a certain value. We cannot use a normal\nlog-spectral distance measure because it is too sensitive t o\nthe difference of spectral peak positions. Our improved\nlog-spectral distance measure uses two kinds of the dis-\ntanceDi—Difor the ﬁrst iteration ( g= 0) andDifor\nthe other iterations ( g≥1) — to robustly calculate the\nappropriate distance even if frequency components of the\nsame drum may vary during a piece.\nThe distance Difor the ﬁrst iteration are calculated af-\nter quantizing TgandPiat a lower time-frequency reso-\nlution. As is shown in Figure 4, the time and frequency\nresolution after the quantization is 2 [frames] (20 [ms])\nfrequency \npower + + = +…\nFigure 5 . Updating the template by calculating the me-\ndian of selected spectrum excerpts.\nand 5 [bins] (54 [Hz]), respectively. The distance Dibe-\ntween Tg(TS)andPiis deﬁned as\nDi=/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbt15/2/summationdisplay\nˆt=12048/5/summationdisplay\nˆf=1/parenleftBig\nˆTg(ˆt,ˆf)−ˆPi(ˆt,ˆf)/parenrightBig2\n(g= 0),(7)\nwhere the quantized (smoothed) spectrum ˆTg(ˆt,ˆf)and\nˆPi(ˆt,ˆf)are deﬁned as\nˆTg(ˆt,ˆf) =2ˆt/summationdisplay\nt=2ˆt−15ˆf/summationdisplay\nf=5ˆf−4´Tg(t, f), (8)\nˆPi(ˆt,ˆf) =2ˆt/summationdisplay\nt=2ˆt−15ˆf/summationdisplay\nf=5ˆf−4´Pi(t, f). (9)\nOn the other hand, the distance Difor the iterations af-\nter the ﬁrst iteration is calculated by the following normal\nlog-spectral distance measure:\nDi=/radicaltp/radicalvertex/radicalvertex/radicalbt15/summationdisplay\nt=12048/summationdisplay\nf=1/parenleftBig\n´Tg(t, f)−´Pi(t, f)/parenrightBig2\n(g≥1).(10)\n2.4. Template Reﬁnement\nAs is shown in Figure 5, the median of all the selected\nspectrum excerpts is calculated and the updated (reﬁned)\ntemplate Tg+1is obtained by\nTg+1(t, f) = median\nsPs(t, f), (11)\nwhere Ps(s= 1,· · ·, M)are spectrum excerpts selected\nin the Excerpt-Selection stage.\nWe use the median operation because it can suppress\nfrequency components that do not belong to drum sounds.\nSince major original frequency components of a target\ndrum sound can be expected to appear at the same po-\nsitions in most selected spectrum excerpts, they are pre-\nserved after the median operation. On the other hand, fre-\nquency components of other musical instrument sounds\ndo not always appear at similar positions in the selected\nspectrum excerpts. When the median is calculated at t\nandf, those unnecessary frequency components become\noutliers and can be suppressed. We can thus obtain the\ndrum-sound template adapted to the current musical piece\neven if it contains simultaneous sounds of various instru-\nments.adapted template yes no yes yes yes yes yes no \nDoes each excerpt \ninclude the template ? \nexcerpt that includes template \nexcerpt that does not include template excerpt \nexcerpt template characteristic \npoints AT… … … … …spectrum\nexcerptsfrequency 1P19 P30 P31 P47 P62 P85 PNP\n…\npower \n47 P\n62 P ATtemplate ATDistance-Calculation \nWeight-Function-Generation Loudness-Adjustment \nFigure 6 . Overview of template-matching method: This method matche s the adapted template with all spectrum excerpts\nby using the improved Goto’s distance measure to detect all t he actual onset times. Our distance measure can judge\nwhether the adapted template is included in spectrum excerp ts even if there are other simultaneous sounds.\n3. TEMPLATE MATCHING METHOD\nBy using the template adapted to the current musical\npiece, this method ﬁnds all temporal locations where a\ntargeted drum occurs in the piece: it tries to exhaus-\ntively ﬁnd all onset times of the target drum sound. This\ntemplate-matching problem is difﬁcult because sounds of\nother musical instruments often overlap the drum sounds\ncorresponding to the adapted template. Even if the tar-\nget drum sound is included in a spectrum excerpt, the dis-\ntance between the adapted template and the excerpt be-\ncomes large when using most typical distance measures.\nTo solve this problem, we propose a new distance mea-\nsure that is based on the distance measure proposed by\nGoto and Muraoka [5]. Our distance measure can judge\nwhether the adapted template is included in spectrum ex-\ncerpts even if there are other simultaneous sounds. This\njudgment is based on characteristic points of the adapted\ntemplate in the time-frequency domain.\nAn overview of our method is depicted in Figure 6.\nFirst, the Weight-Function-Generation stage prepares a\nweight function which represents spectral characteristic\npoints of the adapted template. Next, the Loudness-\nAdjustment stage calculates the loudness difference be-\ntween the template and each spectrum excerpt by using the\nweight function. If the loudness difference is larger than a\nthreshold, it judges that the target drum sound does not ap-\npear in that excerpt, and does not execute the subsequent\nprocessing. If the difference is not too large, the loudness\nof each spectrum excerpt is adjusted to compensate forthe loudness difference. Finally, the Distance-Calculation\nstage calculates the distance between the adapted tem-\nplate and each adjusted spectrum excerpt. If the distance\nis smaller than a threshold, it judges that that excerpt in-\ncludes the target drum sound.\n3.1. Weight Function Generation\nA weight function represents the magnitude of spectral\ncharacteristic at each frame tand frequency fin the\nadapted template. The weight function wis deﬁned as\nw(t, f) =F(f)TA(t, f), (12)\nwhere TAis the adapted template and F(f)is the low-\npass ﬁlter function depicted in Figure 2.\n3.2. Loudness Adjustment\nThe loudness of each spectrum excerpt is adjusted to that\nof the adapted template TA. This is required by our\ntemplate-matching method: if the loudness is different,\nour method cannot estimate the appropriate distance be-\ntween a spectrum excerpt and the adapted template be-\ncause it cannot judge whether the spectrum excerpt in-\ncludes the adapted template.\nTo calculate the loudness difference between the spec-\ntrum excerpt Piand the template TA, we focus on spec-\ntral characteristic points of TAin the time-frequency do-\nmain. First, spectral characteristic points (frequencies ) at\neach frame are determined by using the weight function w,\nand the power difference ηiat each spectral characteristic…\n…\n) , (1 ,ti ftη\n)(tiδ) , (5 ,ti ftη ) , (12 ,ti ftη L L L L\nthe power differences at characteristic points t\nthe power difference at frame tthe first quantile ATiPAT\nfrequency frame power \nFigure 7 . Calculating the power difference δi(t)at each\nframe t, determined as the ﬁrst quantile of ηi(t, ft,k).\npoint is calculated. Next, the power difference δiat each\nframe is calculated by using ηiat that frame, as is shown in\nFigure 7. If the power of Piis too much smaller than that\nofTA, the method judges that Pidoes not include TA, and\ndoes not proceed with the following processing. Finally,\nthe total power difference ∆iis calculated by integrating\nδi. The algorithm is described as follows:\n1. Let ft,k(k= 1,· · ·,15)be the characteristic points\nof the adapted template. ft,krepresents a frequency\nwhere w(t, ft,k)is thek-th largest at frame t. The\npower difference ηi(t, ft,k)is calculated as\nηi(t, ft,k) =Pi(t, ft,k)−TA(t, ft,k). (13)\n2. The power difference δi(t)at frame tis determined\nas the ﬁrst quantile of ηi(t, ft,k).\nδi(t) = ﬁrst-quantile\nkηi(t, ft,k), (14)\nKi(t) = arg ﬁrst-quantile\nkηi(t, ft,k).(15)\nIf the number of frames where δi(t)≤Ψis satisﬁed\nis larger than threshold Rδ, we judge that TAis not\nincluded in Pi(Ψis a negative constant).\n3. The total power difference ∆iis calculated as\n∆i=/summationtext\n{t|δi(t)>Ψ}δi(t)w(t, ft,Ki(t))\n/summationtext\n{t|δi(t)>Ψ}w(t, ft,Ki(t)). (16)\nIf∆i≤Θ∆is satisﬁed, we judge that TAis not\nincluded in Pi(Θ∆is a threshold). Let P′\nibe an\nadjusted spectrum excerpt after the loudness adjust-\nment, determined as\nP′\ni(t, f) =Pi(t, f)−∆i. (17)3.3. Distance Calculation\nThe distance between the adapted template TAand the\nadjusted spectrum excerpt P′\niis calculated by using an\nextended version of the Goto’s distance measure [5]. If\nP′\ni(t, f)is larger than TA(t, f)— i.e., P′\ni(t, f)includes\nTA(t, f),P′\ni(t, f)can be considered a mixture of fre-\nquency components of not only the targeted drum but also\nother musical instruments. We thus deﬁne the distance\nmeasure as\nγi(t, f) =/braceleftBig0 (P′\ni(t, f)−TA(t, f)≥Ψ),\n1otherwise ,(18)\nwhere γi(t, f)is the local distance between TAandP′\ni\nattandf. The negative constant Ψmakes this distance\nmeasure robust for the small variation of frequency com-\nponents. If P′\ni(t, f)is larger than about TA(t, f),γi(t, f)\nbecomes zero.\nThe total distance Γiis calculated by integrating γiin\nthe time-frequency domain, weighted by the function w:\nΓi=15/summationdisplay\nt=12048/summationdisplay\nf=1w(t, f)γi(t, f). (19)\nTo determine whether the targeted drum played at P′\ni,\ndistance Γiis compared with threshold ΘΓ. IfΓi<ΘΓis\nsatisﬁed, we judge that the targeted drum played.\n4. EXPERIMENTS AND RESULTS\nDrum sound identiﬁcation for polyphonic musical audio\nsignals was performed to evaluate the accuracy of identi-\nfying bass and snare drums by our proposed method. In\naddition, we demonstrate an example of the drum sound\ndescription by means of drum descriptors in MPEG-7.\n4.1. Experimental Conditions\nWe tested our method on excerpts of ten songs included\nin the popular music database RWC-MDB-P-2001 de-\nveloped by Goto et al. [6]. Each excerpt was taken\nfrom the ﬁrst minute of a song. The songs we used in-\ncluded sounds of vocals and various instruments as songs\nin commercial CDs do. Seed templates were created\nfrom solo tones included in the musical instrument sound\ndatabase RWC-MDB-I-2001 [7]: the seed templates of\nbass and snare drums are created from sound ﬁles named\n421BD1N3.WA V and 422SD5N3.WA V respectively. All\ndata were sampled at 44.1 kHz with 16 bits.\nWe evaluated the experimental results by the recall rate,\nthe precision rate and the F-measure:\nrecall rate =the number of correctly detected onsets\nthe number of actual onsets,\nprecision rate =the number of correctly detected onsets\nthe number of onsets detected by matching,\nF-measure =2·recall rate ·precision rate\nrecall rate +precision rate.\nTo prepare actual onset times (correct answers), we ex-\ntracted onset times of bass and snare drums from the stan-\ndard MIDI ﬁle of each piece, and adjusted them to the\npiece by hands.piece bass drum snare drum\nnumber method recall rate precision rate F-measure recall rate precision rate F-measure\nNo. 6 base 26 % (28/110) 68 % (28/41) 0.37 83 % (52/63) 83 % (52/61) 0.83\nadapt 57 % (63/110) 84 % (63/75) 0.68 100 % (63/63) 97 % (63/65) 0.98\nNo. 11 base 54 % (28/52) 100 % (28/28) 0.70 27 % (10/37) 71 % (10/14) 0.33\nadapt 100 % (52/52) 100 % (52/52) 1.00 95 % (35/37) 92 % (35/38) 0.93\nNo. 18 base 26 % (35/134) 100 % (35/35) 0.41 91 % (122/134) 82 % (122/148) 0.86\nadapt 97 % (130/134) 71 % (130/183) 0.82 76 % (102/134) 94 % (102/109) 0.84\nNo. 20 base 95 % (60/63) 100 % (60/60) 0.98 24 % (15/63) 94 % (15/16) 0.38\nadapt 94 % (59/63) 100 % (59/59) 0.97 78 % (49/63) 91 % (49/54) 0.84\nNo. 30 base 19 % (25/130) 89 % (25/28) 0.31 27 % (19/70) 90 % (19/21) 0.42\nadapt 93 % (121/130) 94 % (121/129) 0.93 100 % (70/70) 96 % (70/73) 0.98\nNo. 44 base 6 % (6/99) 100 % (6/6) 0.11 9 % (7/80) 88 % (7/8) 0.16\nadapt 93 % (92/99) 100 % (92/92) 0.96 68 % (54/80) 89 % (54/61) 0.77\nNo. 47 base 77 % (46/60) 98 % (46/47) 0.86 41 % (21/51) 70 % (21/30) 0.52\nadapt 93 % (56/60) 98 % (56/57) 0.96 88 % (45/51) 75 % (45/60) 0.81\nNo. 50 base 92 % (61/66) 94 % (61/65) 0.93 94 % (102/108) 89 % (102/114) 0.92\nadapt 97 % (64/66) 88 % (64/73) 0.92 67 % (72/108) 96 % (72/77) 0.78\nNo. 52 base 86 % (113/131) 96 % (113/118) 0.90 97 % (76/78) 94 % (76/81) 0.96\nadapt 94 % (123/131) 90 % (123/136) 0.92 90 % (70/78) 97 % (70/72) 0.93\nNo. 61 base 96 % (73/76) 100 % (73/73) 0.98 99 % (66/67) 80 % (66/83) 0.88\nadapt 93 % (71/76) 100 % (71/71) 0.97 99 % (66/67) 100 % (66/66) 0.99\naverage base 51.6 % (475/951) 94.8 % (475/501) 0.67 65.2 % (490/751) 84.6 % (490/579) 0.74\nadapt 90.2 % (831/921) 90.0 % (831/927) 0.90 83.4 % (626/751) 92.7 % (626/675) 0.88\nTable 1 . Experimental results of drum sound identiﬁcation for ten m usical pieces in RWC-MDB-P-2001.\nidentiﬁed drum Rδ Ψ Θ∆ΘΓ\n(method ) [frames] [dB] [dB]\nbass drum ( base) 7 -10 1 5000\nbass drum ( adapt ) 7 -10 -10 5000\nsnare drum ( base) 7 -10 -5 5000\nsnare drum ( adapt ) 7 -10 -7 5000\nTable 2 . Thresholds used in four experimental settings.\n4.2. Results of Drum Sound Identiﬁcation\nTable 1 shows the experimental results of comparing our\ntemplate-adaptation-and-matching methods (called adapt\nmethod ) with a method in which the template-adaptation\nmethod was disabled (called base method ); the base\nmethod used a seed template instead of the adapted one for\nthe template matching. In other words, we conducted four\nexperiments in different settings; the identiﬁcation of ba ss\ndrum by the base oradapt method and that of snare drum\nby the base oradapt method. We used different thresh-\nolds shown in Table 2 among four experimental cases to\nproduct the best results in respective case.\nThese results showed the effectiveness of the adapt\nmethod: the template-adaptation method improved the F-\nmeasure of identifying bass drum from 0.67 to 0.90 and\nthat of identifying snare drum from 0.74 to 0.88 on av-\nerage of the ten pieces. In fact, in our observation, the\ntemplate-adaptation method absorbed the difference of the\ntimber by correctly adapting seed templates to actual drum\nsounds appearing in a piece.\nIn many musical pieces, the recall rate was signiﬁ-\ncantly improved in the adapt method. The base method\noften detected a few onsets in some piece (e.g., No. 11 andNo. 30) because the distance between an unadapted seed\ntemplate and spectrum excerpts were not appropriate; the\ndistance became too large because of the difference of the\ntimber. On the other hand, the template-matching method\nof the adapt method worked effectively; all the rates in\nNo. 11 and No. 30, for example, were over 90% in the\nadapt method. If the difference of the timber is small, the\nbase method produced the high recall and precision rates\n(e.g., No. 52 and No. 61).\nAlthough our adapt method is effective in general, it\ncaused a low recall rate in a few cases. The recall rate\nof identifying the snare drum in No. 50, for example,\nwas degraded, while the precision rate was improved. In\nthis piece, the template-matching method was not able to\njudge that the template was correctly included in spectrum\nexcerpts because frequency components of the bass guitar\noften overlapped spectral characteristic points of the bas s\ndrum in those excerpts.\n4.3. Demonstration of Drum Sound Description\nIn this section, we demonstrate an example of the auto-\nmatic drum sound description by using drum descriptors .\nOur proposed template-adaptation and template-matching\nmethods can detect onset times of bass and snare drums\nrespectively. To symbolically represent these informa-\ntion in the context of MPEG-7, drum descriptors and their\nschemes must be deﬁned in the MPEG-7 format.\nFirst, we deﬁne drum descriptors and drum descriptor\nschemes. To describe onset times and names of drums,\nwe use the mpeg7:MediaTimePoint data type and the Enu-\nmeration facet respectively:<simpleType name=\"InstrumentNameType\">\n<restriction base=\"string\">\n<enumeration value=\"BassDrum\"/>\n<enumeration value=\"SnareDrum\"/>\n...\n</restriction>\n</simpleType>\n<complexType name=\"InstrumentOnsetType\">\n<sequence>\n<element name=\"MediaTimePoint\"\ntype=\"mpeg7:MediaTimePointType\"/>\n<element name=\"InstrumentName\"\ntype=\"InstrumentNameType\"/>\n</sequence>\n</complexType>\n<complexType name=\"InstrumentStreamType\">\n<sequence>\n<element name=\"InstrumentOnset\"\nminOccurs=\"0\" maxOccurs=\"unbounded\"/>\n</sequence>\n</complexType>\nwhere the InstrumentOnsetType data type indicates infor-\nmation of a time and a name which corresponds to a onset\nin a musical piece. The InstrumentStreamType data type\nis a set of multiple InstrumentOnsetType elements.\nNext, we describe onset times and names of drums in\na musical piece by means of drum descriptors deﬁned\nabove. We demonstrate an example of the drum sound\ndescription for No. 52 by using our proposed methods.\n<element name=\"DrumStream\" type=\"InstrumentStreamType \"/>\n<DrumStream>\n<InstrumentOnset>\n<MediaTimePoint>T00:00:36382F44100</MediaTimePoint>\n<InstrumentName>BassDrum</InstrumentName>\n</InstrumentOnset>\n<InstrumentOnset>\n<MediaTimePoint>T00:00:54684F44100</MediaTimePoint>\n<InstrumentName>SnareDrum</InstrumentName>\n</InstrumentOnset>\n<InstrumentOnset>\n<MediaTimePoint>T00:01:22506F44100</MediaTimePoint>\n<InstrumentName>BassDrum</InstrumentName>\n</InstrumentOnset>\n...\n</DrumStream>\n5. CONCLUSION\nIn this paper, we have presented an automatic description\nsystem that can describe onset times and names of drums\nby means of drum descriptors . Our system used two meth-\nods to identify all the onset times of bass and snare drums\nrespectively in real-world CD recordings. Even if drum\nsounds prepared as seed templates are different from ones\nused in a musical piece, our template-adaption method can\nadapt the templates to the piece. By using the adapted\ntemplates, our template-matching method then detects all\nthe onset times. Our experimental results have shown that\nthe adaptation method largely improved the F-measure of\nidentifying bass and snare drums. In addition, we deﬁned\ndrum descriptors in the context of MPEG-7 and demon-\nstrated the automatic drum sound description for a real-\nworld musical piece. In the future, we plan to use multi-\nple seed templates for each kind of drums and extend our\nmethod to identify other drum sounds.\nAcknowledgments\nThis research was partially supported by the Ministry of Edu ca-\ntion, Culture, Sports, Science and Technology (MEXT), Gran t-\nin-Aid for Scientiﬁc Research (A), No.15200015, and COE pro -\ngram of MEXT, Japan.6. REFERENCES\n[1] Dixon, S., Pampalk, E., and G. Widmer, G., “Classiﬁcatio n\nof Dance Music by Periodicity Patterns,” Proc. of ISMIR ,\n159–165, 2003.\n[2] Eronen, A. and Klapuri, A., “Musical Instrument Recogni -\ntion Using Cepstral Coefﬁcients and Temporal Features,”\nProc. of ICASSP , 753–756, 2000.\n[3] FitzGerald, D., Coyle, E., and Lawlor, B., “Sub-band In-\ndependent Subspace Analysis for Drum Transcription,”\nProc. of DAFX , 65–69, 2002.\n[4] G´ omez, E., Gouyon, F., Herrera, P., and Amatriain, X.,\n“Using and enhancing the current MPEG-7 standard for a\nmusic content processing tool,” Proc. of AES , 2003.\n[5] Goto, M. and Muraoka, Y ., “A Sound Source Separation\nSystem for Percussion Instruments,” IEICE Transactions ,\nJ77-D-II, 5, 901–911, 1994 (in Japanese) .\n[6] Goto, M., Hashiguchi, H., Nishimura, T., and Oka, R.,\n“RWC Music Database: Popular, Classical, and Jazz Mu-\nsic Databases,” Proc. of ISMIR , 287–288, 2002.\n[7] Goto, M., Hashiguchi, H., Nishimura, T., and Oka, R.,\n“RWC Music Database: Music Genre Database and Musi-\ncal Instrument Sound Database,” Proc. of ISMIR , 229–230,\n2003.\n[8] Gouyon, F. and Herrera, P., “Exploration of techniques\nfor automatic labeling of audio drum tracks instruments,”\nProc. of AES , 2001.\n[9] Gouyon, F. and Herrera, P., “Determination of the meter\nof musical audio signals: Seeking recurrences in beat seg-\nment descriptors,” Proc. of AES , 2003.\n[10] Herrera, P., Yeterian, A., and Gouyon, F., “Automatic\nClassiﬁcation of Drum Sounds: A Comparison of Fea-\nture Selection Methods and Classiﬁcation Techniques,”\nProc. of ICMAI , LNAI2445, 69–80, 2002.\n[11] Herrera, P., Dehamel, A., and Gouyon, F., “Automatic\nlabeling of unpitched percussion sounds,” Proc. of AES ,\n2003.\n[12] Kitahara, T., Goto, M., and Okuno, H.G., ”Category-\nlevel Identiﬁcation of Non-registered Musical Instrument\nSounds,” Proc. of ICASSP , 2004 (in press) .\n[13] Liu, D., Lu, L., and Zhang, H.J., “Automatic Mood Detec-\ntion from Acoustic Music Data,” Proc. of ISMIR , 2003.\n[14] Martin, K.D., “Musical Instrumental Identiﬁcation: A\nPattern-Recognition Approach,” 136th meeting of Ameri-\ncan Statistical Association , 1998.\n[15] Pampalk, E., Dixon, S., and Widmer, G., “Exploring Musi c\nCollections by Browsing Different Views,” Proc. of ISMIR ,\n201–208, 2003.\n[16] Paulus, J. and Klapuri, A., “Measuring the Similarity o f\nRhythmic Patterns,” Proc. of ISMIR , 150–156, 2002.\n[17] Paulus, J. and Klapuri, A., “Model-based Event Label-\ning in the Transcription of Percussive Audio Signals,”\nProc. of DAFX , 1–5, 2003.\n[18] Peeters, G., McAdams, S., and Herrera, P., “Instru-\nment Sound Description in the Context of MPEG-7,”\nProc. of ICMC , 2000.\n[19] Savitzky, A. and Golay, M., “Smoothing and Differentia -\ntion of Data by Simpliﬁed Least Squares Procedures,” J. of\nAnalytical Chemistry , 36, 8, 1627–1639, 1964.\n[20] Scheirer, E.D., “Tempo and Beat Analysis of Acoustic Mu -\nsical Signals,” J. of Acoustical Society of America , 103, 1,\n588–601, 1998.\n[21] Tzanetakis, G. and Cook, P., “Musical Genre Classiﬁcat ion\nof Audio Signals,” IEEE Transactions on Speech and Au-\ndio Processing , 10, 5, 2002.\n[22] Uhle, C., Dittmar, C., and Sporer, T., “Extraction of Dr um\nTracks from Polyphonic Music Using Independent Sub-\nspace Analysis,” Proc. of ICA , 843–848, 2003.\n[23] Zils, A., Pachet, F., Delerue, O., and Gouyon, F., “Auto -\nmatic Extraction of Drum Tracks from Polyphonic Music\nSignals,” Proc. of WEDELMUSIC , 179–183, 2002."
    },
    {
        "title": "Automatic Chord Transcription with Concurrent Recognition of Chord Symbols and Boundaries.",
        "author": [
            "Takuya Yoshioka",
            "Tetsuro Kitahara",
            "Kazunori Komatani",
            "Tetsuya Ogata",
            "Hiroshi G. Okuno"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415068",
        "url": "https://doi.org/10.5281/zenodo.1415068",
        "ee": "https://zenodo.org/records/1415068/files/YoshiokaKKOO04.pdf",
        "abstract": "This paper describes a method that recognizes musical chords from real-world audio signals in compact-disc recordings. The automatic recognition of musical chords is necessary for music information retrieval (MIR) sys- tems, since the chord sequences of musical pieces cap- ture the characteristics of their accompaniments. None of the previous methods can accurately recognize musi- cal chords from complex audio signals that contain vocal and drum sounds. The main problem is that the chord- boundary-detection and chord-symbol-identification pro- cesses are inseparable because of their mutual depen- dency. In order to solve this mutual dependency problem, our method generates hypotheses about tuples of chord symbols and chord boundaries, and outputs the most plau- sible one as the recognition result. The certainty of a hy- pothesis is evaluated based on three cues: acoustic fea- tures, chord progression patterns, and bass sounds. Ex- perimental results show that our method successfully rec- ognized chords in seven popular music songs; the average accuracy of the results was around 77%. Keywords: audio signal, musical key, musical chord, hy- pothesis search",
        "zenodo_id": 1415068,
        "dblp_key": "conf/ismir/YoshiokaKKOO04",
        "keywords": [
            "audio signals",
            "musical chords",
            "music information retrieval",
            "chord boundary-detection",
            "chord-symbol-identification",
            "mutual dependency problem",
            "hypotheses about tuples",
            "most plausible one",
            "recognition result",
            "certainty of a hypothesis"
        ],
        "content": "AUTOMATICCHORD TRANSCRIPTION WITH CONCURRENT\nRECOGNITION OF CHORD SYMBOLS AND BOUNDARIES\nTakuyaYoshioka,TetsuroKitahara,Kazunori Komatani,TetsuyaOgata, and HiroshiG. Okuno\nGraduate School of Informatics, KyotoUniversity\nYoshida-hommachi,Sakyo-ku,Kyoto606-8501, Japan\nABSTRACT\nThis paper describes a method that recognizes musical\nchords from real-world audio signals in compact-disc\nrecordings. The automatic recognition of musical chords\nis necessary for music information retrieval (MIR) sys-\ntems, since the chord sequences of musical pieces cap-\nture the characteristics of their accompaniments. None\nof the previous methods can accurately recognize musi-\ncal chords from complex audio signals that contain vocal\nand drum sounds. The main problem is that the chord-\nboundary-detection and chord-symbol-identiﬁcation pro-\ncesses are inseparable because of their mutual depen-\ndency. In order to solve this mutual dependency problem,\nour method generates hypotheses about tuples of chord\nsymbolsandchordboundaries,andoutputsthemostplau-\nsible one as the recognition result. The certainty of a hy-\npothesis is evaluated based on three cues: acoustic fea-\ntures, chord progression patterns, and bass sounds. Ex-\nperimental results show that our method successfully rec-\nognizedchordsinsevenpopularmusicsongs;theaverage\naccuracyof the results wasaround 77%.\nKeywords: audio signal, musical key, musical chord, hy-\npothesis search\n1. INTRODUCTION\nThe recent rapid spread of online music distribution ser-\nvicesdemandsefﬁcientmusicinformationretrieval(MIR)\ntechnologies. Annotating musical contents in a universal\nformat is one of the most effective ways to fulﬁll this de-\nmand. Although the new ISO standard MPEG-7 [8] pro-\nvides a framework for designing such formats, it does not\ndeﬁnethemethodstoobtainmusicalelementsfromaudio\nsignals. Manualannotationrequiresatremendousamount\nofhumanwork,whichmakesitdifﬁculttomaintainacon-\nsistent annotation quality among human annotators. Au-\ntomatic transcription technologies for musical elements\nare hence needed to avoid these problems. However, they\nhavenot been realized yet.\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkfor\npersonal or classroom use is granted without fee provided that copies\narenotmadeordistributedforproﬁtorcommercialadvantageandthat\ncopiesbear this notice and the full citation on the ﬁrst page.\nc°2004UniversitatPompeu Fabra.We focus on musical chord sequences as one of the\ndescriptors of musical elements. A chord sequence is a\nseries of chord symbols with boundaries that are deﬁned\nas the times when chords change. Descriptors of musi-\ncal chords will play an important role in realizing effec-\ntiveMIR,sincethechordsequencesofmusicalpiecesare\nsimple but powerful descriptions that capture the charac-\nteristics of their accompaniments. They are also the main\nfactors of determining moods of the pieces, especially in\npopular music. Therefore, we address the issue of auto-\nmatic chord transcription.\nThe main problem in automatic chord transcription is\nthe mutual dependency of chord-boundary detection and\nchord-symbol identiﬁcation. It is difﬁcult to detect the\nchord boundaries correctly prior to chord-symbol iden-\ntiﬁcation. If the chord boundaries could be determined\nbefore chord-symbol identiﬁcation, automatic chord tran-\nscriptioncouldbeachievedbyidentifyingthechordsym-\nbols in each chord span, which is deﬁned as the time pe-\nriod between the adjacent boundaries. Although chord-\nboundary-detection methods based on the magnitude of\nlocal spectral changes are reported [2, 4], they are not ac-\nceptable solutions, because they often mistakenly detect\nthe onset times of non-chord tones or drum sounds when\nthese sounds cause prominent spectral changes.\nNoneofthepreviousmethods[1,2,7,9,11,12]hasad-\ndressed this mutual dependency problem. Aono et al.[1]\nand Nawab et al.[9] treated not audio signals from ac-\ntual musical pieces but chord sounds from a single mu-\nsical instrument. Kashino et al.[7] and Su et al.[12]\nassumed that the chord boundaries were given before-\nhand. Fujishima [2] developed a method of detecting the\nchord boundaries based on the magnitude of the spectral\nchanges. However, he treated only musical audio sig-\nnals that do not contain vocal and drum sounds. Sheh\net al.[11] developed a method that identiﬁes chord sym-\nbols in each 100-ms span without detecting chord bound-\naries. However, this method cannot correctly identify\nchordsymbols,becausetheacousticfeaturesinsuchshort\nspansareliabletobeaffectedbyarpeggiosoundsandnon-\nchord tones.\nTo solve this mutual dependency problem, we pro-\nposeamethodthatrecognizeschordboundariesandchord\nsymbols concurrently. Our method generates hypotheses\nabout tuples of chord boundaries and chord symbols, and\nevaluates their certainties. It ﬁnally selects the most plau-sible one as the recognition result. As cues for evaluating\nthe certainties of hypotheses, our method uses chord pro-\ngression patterns ( i.e.concatenations of chord symbols\nthatarefrequentlyusedinactualmusicalpieces)andbass\nsounds as well as acoustic features. To use the chord pro-\ngression patterns appropriately, musical keys are needed.\nOur method hence also identiﬁes the keys from input au-\ndio signals.\nTherestofthispaperisorganizedasfollows: Section2\ndescribes the problems in realizing automatic chord tran-\nscription and our approach to solve them. Section 3 ex-\nplains our method in detail. Section 4 reports the experi-\nmental results that show the effectiveness of our method.\nSection 5 concludes this paper.\n2. AUTOMATICCHORD TRANSCRIPTION\n2.1. Speciﬁcation of AutomaticChord Transcription\nIn this paper, we deﬁne automatic chord transcription as\nthe process of obtaining chord sequence c1c2¢¢¢cnand\nkeykfrommusicalaudiosignals. Wetreatmusicalpieces\nthat satisfy the followingassumptions:\n(A1)The keydoes not modulate.\n(A2)The keyis a major key.\nChord ciisdeﬁned as follows:\nci= (cs; b; e ); (1)\nwhere csdenotes the chord symbol, and bandedenotes\nthe beginning and end times of chord cirespectively. We\ncall duration [b; e]as the chord span of ci. Chord symbol\ncsisdeﬁned as follows:\ncs= (root; style ) (2)\nroot2 fC;C#;¢¢¢;Bg (3)\nstyle2 fmajor,minor,augmented, diminished g;(4)\nwhere rootdenotes the root tone and styledenotes the\nchord style. This deﬁnition of chord styles, for example,\ncategorizes both the major triad and major 7th chords as\nmajor. We think chord styles in such level of detail will\nbe useful in many MIR methods because they capture the\nmoods of musical pieces adequately. Key kis deﬁned as\nthe tuple of its tonic tone ( tonic) and mode ( mode):\nk= (tonic; mode ) (5)\ntonic2 fC;C#;¢¢¢;Bg (6)\nmode =major (7)\n2.2. Problems: Mutual Dependency in Automatic\nChord Transcription\nThemaindifﬁcultyinautomaticchordtranscriptionliesin\nthe following mutual dependency of three processes that\nconstitute automatic chord transcription: chord-boundary\ndetection,chord-symbolidentiﬁcation,andkeyidentiﬁca-\ntion. Because of the mutual dependency, these processes\nare inseparable.\nThe key of C major 0.6\nG Dm EmThe key of G major 0.8\nG D Em\nH1\nH2\nHypothesis H1 is selectedMusical audio signalsFigure 1. Concurrent recognition of chord boundaries,\nchord symbols, and keys\n1.The mutual dependency of chord-symbol identiﬁca-\ntion and chord-boundarydetection\nChord-symbol identiﬁcation requires a target span\nfor the identiﬁcation in advance. However, it is dif-\nﬁcult to determine the chord spans correctly prior\nto chord-symbol identiﬁcation. In order to realize\nhighly accurate chord-boundary detection, the cer-\ntainties of chord boundaries should be evaluated,\nbased on the results of chord-symbol identiﬁcation.\nChord-symbol identiﬁcation is therefore indispens-\nable for chord-boundary detection.\n2.The mutual dependency of chord-symbol identiﬁca-\ntion and keyidentiﬁcation\nChord progression patterns are important cues for\nidentifyingchordsymbols. Applyingthechordpro-\ngression patterns requires musical keys, because\nwhich patterns to apply is dependent on keys. On\nthe other hand, key identiﬁcation usually requires\nchord symbols.\n2.3. Our Solution: Concurrent Recognition of Chord\nBoundaries, Chord Symbols, and Keys\nIn order to cope with the mutual dependency, we devel-\nopedamethodthatconcurrentlyrecognizeschordbound-\naries,chordsymbols,andkeys. Ourmethodgenerateshy-\npotheses about tuples of a chord sequence and a key with\ntheir evaluation values that represent the certainties of the\nhypotheses, and selects the hypothesis with the largest\nevaluationvalueas the recognition result (Figure 1).\nThefollowingthreekindsofmusicalelementsareused\nas cues for calculating the evaluation values of hypothe-\nses:\n1.Acoustic features\nFor acoustic features, we use 12-dimensionalMusical audio signals\nEighth-note\nlevel beat timesBeat tracking\nsystem\nHypothesis searcherChord progression\npatterns\nHypothesis\nevaluatorChroma\nvectorizerBass sound\ndetector\nG D EmThe key of G majorRecognition resultHypotheses\nEvaluation\nvaluesFigure 2. Overview of the automatic chord transcription\nsystem\nDiatonicchord progression\nG!CDm!GG!AmC!F\nNon-diatonicchord progression\nAm!D!G G!A[dim!Am\nTable 1. Examples of the chord progression patterns in\nthe keyof C major\nchromavectors [3],whichroughlyrepresentthein-\ntensities of the 12 semitone pitch classes. Each ele-\nment of a chroma vector corresponds to one of the\n12 pitch classes, and it is the sum of power at fre-\nquencies of its pitches over six octaves. The acous-\ntic features are essential cues because chord sym-\nbols are deﬁned as collections of the 12 semitone\npitch classes.\n2.Chordprogressionpatterns\nChord progression patterns are concatenations of\nchord symbols that are frequently used in musical\npieces (Table 1) . Using chord progression pat-\nterns facilitates reducing the ambiguities of chord-\nsymbol-identiﬁcation results, which are caused by\ntheabsenceofchordtonesandthepresenceofnon-\nchord tones.\n3.Bass sounds\nBass sounds are the most predominant tones in a\nlow frequency region. Using bass sounds improves\nthe performance of automatic chord transcription,\nbecause bass sounds are closely related to musical\nchords, especially in popular music.Initialization:\nforeach s2Sdo\ncalculate f(s)\nTÃT[ fsg\nend\nthe front time Ã0\nHypothesis search:\nwhilethe nexttime exists do\nthe front time Ãthenexttime\nforeach h2Tdo\nExpansion block:\nforeach h´2V(h;thefront time )do\ncalculate f(h´ )\nT´ÃT´[ fh´g\nend\nifhisnot completely expanded do\nU´ÃU´[ fhg\nend\nend\nforeach h2Udo\ndoExpansionblock\nend\nTÃthebest BS hypothesesin T´\nUÃU´\nend\nreturn arg max h2Tf(h)\nFigure 3. Hypothesis-search algorithm. Sis a set of ini-\ntial hypotheses. Tis a set of hypotheses whose chord\nsequences reach the front time. Uis a set of hypothe-\nses whose chord sequences do not reach the front time.\nV(h; t)isasetofchildhypothesesofhypotheses hattime\nt.f(h)is an evaluation function that gives the evaluation\nvalueof hypothesis h.\n3. HYPOTHESIS-SEARCH-BASEDAUTOMATIC\nCHORDTRANSCRIPTION\nOur method is based on hypothesis search, which obtains\nthe most plausible hypothesis of all the possible hypothe-\nsesthatsatisfyagivengoalstatement. Inautomaticchord\ntranscription,thegoalstatementisthatthechordsequence\nofahypothesisrangesfromthebeginningtotheendofan\ninput.\nFigure 2 shows an overview of our automatic chord\ntranscription system. First, the beat tracking system de-\ntects the eighth-note level beat times of an input musical\npiece using the method developed by Goto [4]. Then, the\nhypothesis searcher searches the most plausible hypoth-\nesis about a chord sequence and a key. The search pro-\ngresses every eighth-note level beat time from the begin-\nning of the input. Finally, the searcher outputs the ob-\ntained most plausible hypothesis.\nThe overall process of the hypothesis search is brieﬂy\ndescribed as follows. At the beginning, initial hypothesestimet1 t2 t3 t4 t5 t6 t7timet1 t2 t3 t4 t5 t6 t7the front time the front time\n0.8\n0.80.6\n0.3\n0.4G D\nG DG D E\nD DmD Dm EmT T\nU UG major\nD majorG major\nD major\nG majorthe front time proceeds\nto the next beat time\nThis hypothesis is deleted\n(Pruning)t6\nt5t7\nt7\nt6Eighth-note level\nbeat times\nGroup 1\nGroup 2\nThese hypotheses are forgottenGroup 3Group 1: \n  Hypotheses generated by the expansions\nGroup 2: \n  Hypotheses with their expansions unfinished\nGroup 3: \n  Completely expanded hypothesesFigure4. Twosets of hypothesesfor reasonable pruning\nare given to the hypothesis searcher. Whenever the front\ntime(i.e.thetimetowhichthesearchhasprogressed)pro-\nceedstothenexteighth-notelevelbeattime,thehypothe-\nsis searcher expands all hypotheses at that time into ones\nwhosechordsequencesrangetothefronttime,andthehy-\npothesis evaluator then calculates the evaluation value of\nthem. When the front time ﬁnally reaches the end of the\ninput, the hypothesis that has the largest evaluation value\nis adopted.\n3.1. Hypothesis-searchAlgorithm\nIn order to avoid a combinatorial explosion of the num-\nber of hypotheses, a search algorithm must contain oper-\nations for pruning, which prohibits the expansion of hy-\npotheses with small evaluation values. The pruning must\nbe performed from hypotheses whose chord sequences\nend at the same time, because pruning from hypotheses\nwhose chord sequences end at different times can incor-\nrectly delete hopeful hypotheses.\nOur hypothesis-search algorithm is shown in Figure 3.\nThekeyideaofourpruningmethodistomanagetwosets\nof hypotheses: one is a set of hypotheses with end times\nthat are equal to the front time. The other is a set of hy-\npotheseswithendtimesthatarenotequaltoit. Theprun-\ning is performed from the hypotheses in the former set\n(Figure 4). Therefore, this algorithm reduces the risks of\nwrong pruning.\nTheprogressofthisalgorithmisstraightforward. Ital-\nways needs audio signals only around the front time. The\ntime complexity of this algorithm for an n-length input\nisO(n)when the hypothesis-expansion algorithm takes\ntimeO(1). Since our hypothesis-expansion algorithm is\nof order O(1), our method is able to operate in real time\nwith a largeamount of computational power.\nImplementing this algorithm requires deﬁnition of the\nfollowingsix elements:1.Input-scanning times\nInput scanning times are time points at which hy-\npotheses are expanded. The input-scanning times\nin our system are deﬁned as the eighth-note-level\nbeat times of an input musical piece.\n2.Data structureof a hypothesis\nWe deﬁne hypothesis hof our system as a tuple of\nchord sequence c1c2¢¢¢cnand key k:\nh= (c1c2¢¢¢cn; k): (8)\n3.Set of initial hypotheses\nOursystem’sset( S)ofinitialhypothesesisdeﬁned\nas follows:\nS=f(\"; ki)gNK\ni=0; (9)\nwhere \"denotes the empty chord sequence, and ki\ndenotes a key. In our system, NK = 11based on\nassumption A2;k0denotes the key of C major, k1\ndenotes the key of D [major, ¢¢¢, and k11denotes\nthe keyof B major.\n4.Hypothesis-expansionalgorithm\nHypothesis-expansion algorithm, which is denoted\nbyV(h; t)inFigure3,deﬁnesthechildhypotheses\nof hypothesis hat front time t. Its deﬁnition in our\nsystem is givenin section 3.2.\n5.Criterion for determining the end of expansion\nOur system determines that a hypothesis has com-\npletely expanded when the interval between the\nfront time and the end time of the chord sequence\nofthehypothesisexceedsthemeasure-level-beatin-\ntervalof an input musical pieces.\n6.Evaluation function\nEvaluationfunction f(h)givestheevaluationvalueofhypothesis h. Itsdeﬁnitioninoursystemisgiven\nin section 3.3.\n3.2. Hypothesis-Expansion Algorithm\nOur system’s hypothesis-expansion algorithm expands\nhypothesis h= (c1c2¢¢¢cn; k)into NC hypotheses\nh(i)= (c1c2¢¢¢cnc(i)\nn+1; k)(1·i·NC), and calculates\nscore sc(i)\nn+1, which indicates the certainty of c(i)\nn+1based\nonacousticfeatures. c(i)\nn+1isachordthatbeginsattheend\ntime of chord cnand ends at front time t. This algorithm\nignoresthepossibilityofmodulationbasedonassumption\nA1.\nTheprocedurefordetermining c(i)\nn+1andtheirscoresis\nas follows:\n1. Extract a chroma vector from the spectrum excerpt\nfrom the span that begins at end time ( e) ofcnand\nends at front time t.\n2. CalculatetheMahalanobisdistancebetweentheex-\ntracted chroma vector and the mean chroma vector\nfrom the training audio signals for each chord.\n3. Select NC chord symbols cs(i)\nn+1(1·i·NC),\nwhose distances are smaller than the others. Then,\nc(i)\nn+1is represented as (cs(i)\nn+1; e; t), and sc(i)\nn+1is\ndeﬁned as the normalized value of the reciprocal of\nthe distance of c(i)\nn+1.\n3.3. EvaluationFunction\nGiven hypothesis h= (c1c2¢¢¢cn; k), evaluation func-\ntionf(h)calculates the evaluation value of h. To cal-\nculate the evaluation values of hypotheses, our method\nevaluates the acoustic-feature-based, chord-progression-\npattern-based,andbass-sound-basedcertaintiesofthehy-\npotheses. The acoustic-feature-based certainty of a hy-\npothesis indicates the degree of similarity between the\nchroma vectors from its chord spans and training chroma\nvectors for each chord. The chord-progression-pattern-\nbased certainty indicates the number of chord-symbol\nconcatenations of the hypothesis corresponding to one of\nthechordprogressionpatterns. Thebass-sound-basedcer-\ntainty indicates the degree of predominance of its chord\ntones in a lowfrequencyregion.\nEvaluation function f(h)in our system is deﬁned as\nfollows:\nf(h) = log ac(h) +WPR£logpr(h)\n+WBA£logba(h); (10)\nwhere ac(h)denotes the acoustic-feature-based certainty,\npr(h)denotes the chord-progression-pattern-based cer-\ntainty, ba(h)denotes the bass-sound-based certainty,\nWPR denotes the weight of the chord-progression-\npattern-based certainty, and WBA denotes the weight of\nthe bass-sound-based certainty.3.3.1. Acoustic-feature-basedcertainty\nAcoustic-feature-based certainty ac(h)is deﬁned as fol-\nlows:\nac(h) =nY\ni=1(sci£EPli¡1); (11)\nwhere scidenotes the score of chord ci,lidenotes the\nnumber of intervals of the eighth-note level beats con-\ntainedinthespanof ci,andEPdenotesthespan-extending\npenalty. Deﬁning acoustic-feature-based certainty as the\nproduct of sciwould cause many deletion errors, because\nthe numbers (n) of chords are not equal among different\nhypotheses. Multiplying the span-extending penalty is an\neffectivewayto avoiddeletion errors.\n3.3.2. Chord-progression-pattern-basedcertainty\nChord-progression-pattern-based certainty pr(h)is de-\nﬁned as follows:\npr(h) =PPRm(12)\nm=n¡num(i;9p; q s:t: p ·i·q; c p¢¢¢cq2P)\nfor1·i·n; (13)\nwhere Pdenotes the set of chord progression patterns for\nkeyk, PPR denotes the penalty for mismatched progres-\nsions, and num(i;cond(i))denotes the number of values\nithatsatisfycondition cond(i). Toobtainthesetofchord\nprogression patterns for each key, we stored 71 concate-\nnationsofchordfunctions,accordingtothetheoryofhar-\nmonics (e.g.V!I). Given a key, our method yields the\nsetofchordprogressionpatternsforthekeyfromthepre-\nstored chord-function concatenations. For example, ap-\nplying the key of C major to V !I yields chord progres-\nsion pattern G !C.\n3.3.3. Bass-sound-based certainty\nLetpidenote the most predominant pitch class in a low\nfrequencyregionofthespanofchord ci,andpred idenote\nthe degree of its predominance. Then, bass-sound-based\ncertainty ba(h)isdeﬁned as follows:\nba(h) =nY\ni=1htpi (14)\nhtpi=(\npred i(ifpiisa chord tone of ci)\nPBA (otherwise) ;(15)\nwhere PBA denotes the penalty for the absence of the\nchord tones in the low frequency region. To obtain the\ndegrees of predominance of pitch classes in the low fre-\nquency region, our method forms the pitch probabilistic\ndensity function after applying the band pass ﬁlter for the\nbasslineusingSakuraba’s[10]automaticmusictranscrip-\ntion system implementing Goto’s method [5]. Then, the\ndegree of predominance of each pitch class is deﬁned as\nthe sum of the valuesof the function at its pitches.Piece Short Acoust Our method\nnumber span corr acc corr accKey\nNo.14 42%86%74%89%84% °\nNo.17 57%90%64%91%76% °\nNo.40 38%89%76%85%80% °\nNo.44 34%90%46%88%67% °\nNo.45 53%90%68%86%74% °\nNo.46 57%95%69%93%80% °\nNo.74 45%90%71%92%80% °\n°: Correctly identiﬁed\nTable2. Experimental results\n4. EXPERIMENTALRESULTS\nOursystemwastestedonone-minuteexcerptsfromseven\nsongsofRWC-MDB-P-2001 [6]: No.14,17,40,44,45,46,\nand 74. The current implementation uses the following\nparameters: BS = 20,NC= 7,WPR = 1:0,WBA = 5:0,\nEP= 0:25, PPR = 0:8, and PBA = 0:5. For the training\ndata of chroma vectors, we used 2592 excerpts of audio\nsignals of each chord played on a MIDI tone generator\nand audio signals of the six songs except an input one.\nTo evaluate the effectiveness of concurrent recognition of\nchord boundaries and chord symbols, we implemented a\nsystem that identiﬁes chord symbols in every short span\ncorrespondingtotheeighth-notelevelbeatinterval(called\na short span method). We also implemented a system that\ncalculates the evaluation values of hypotheses based on\nonly acoustic features (called an acoust-method).\nFor evaluating the outputs, we used two criteria: cor-\nrectness corrand accuracy acc, which is deﬁned as fol-\nlows:\ncorr = 1¡#(substitution and deletion errors)\n#(output chords)(16)\nacc= 1¡#(substitution, deletion, and insertion errors)\n# (output chords)\n(17)\nThe correct chord sequences are hand-labeled.\nTheresultsarelistedinTable2(forshortspanmethod,\nonly accuracies are shown). Our system’s average accu-\nracy was 77%. This result shows that our method can\ncorrectly recognize chord sequences from complex mu-\nsical audio signals that contain vocal and drum sounds.\nTheperformanceoftheshortspanmethodwaspoor. This\nis because the short span method often confused major\nchords and their minor versions, since there were many\nspans where the third tones of chords did not appear.\nThe accuracy of the acoust-method was very smaller than\nthat of our method in spite of the high correctness, since\nthe acoust-method made many insertion errors. This is\nbecause the acoustic-feature-based certainties in correct\nchordspanswereliabletobesmallerthanthoseinshorter\nspans due to the spectral changes caused by arpeggio\nsounds. These results show that our concurrent recog-\nnition method of chord boundaries and chord symbolsachieves high improvement of chord-recognition perfor-\nmance,andthatusingchordprogressionpatternsandbass\nsounds also improvesthe performance.\n5. CONCLUSION\nWe have described a method that recognizes musical\nchords and keys from audio signals. To cope with the\nmutual dependency of chord-boundary detection, chord-\nsymbol identiﬁcation, and key identiﬁcation, our method\nruns these processes concurrently, which is achieved by\nsearchingthemostplausiblehypothesisaboutatupleofa\nchord progression and a key. This method operates with-\nout any prior information about the input songs. The ex-\nperimental results show that our method is robust enough\nto achieve 77% accuracy of chord recognition on seven\npopular music songs that contain vocaland drum sounds.\nAcknowledgments: This research was partially supported\nby the Ministry of Education, Culture, Sports, Science and\nTechnology (MEXT), Grant-in-Aid for Scientiﬁc Research (A),\nNo.15200015, the Sound Technology Promotion Foundation,\nand Informatics Research Center for Development of Knowl-\nedge Society Infrastructure (COE program of MEXT, Japan).\nWe thank Mr. Yohei Sakuraba for his permission to use his pro-\ngram.\n6. REFERENCES\n[1] Aono, Y., Katayose, H., and Inokuchi, S. “A Real-time\nSession Composer with Acoustic Polyphonic Instruments”,\nProc.ICMC , pp.236–239, 1998.\n[2] Fujishima, T. “Realtime Chord Recognition of Musical\nSound: a System Using Common Lisp Music”, Proc.\nICMC,pp.464–467, 1999.\n[3] Goto,M.“AChorus-SectionDetectingMethodforMusical\nAudioSignals”, Proc.ICASSP ,V,pp.437–440, 2003.\n[4] Goto, M. “An Audio-based Real-time Beat Tracking\nSystem for Music With or Without Drum-sounds”, Journal\nofNewMusic Research , Vol.30,No.2, pp.159–171, 2001.\n[5] Goto, M. “A Robust Predominant-F0 Estimation Method\nfor Real-time Detection of Melody and Bass Lines in CD\nRecordings”, Proc.ICASSP ,II, pp.757–760, 2000.\n[6] Goto, M., Hashiguchi, H., Nishimura, T., and Oka, R.\n“RWCMusicDatabase: Popular,Classical,andJazzMusic\nDatabases”, Proc.ISMIR ,pp.287–288, 2002.\n[7] Kashino, K., Nakadai, K., Kinoshita T., and Tanaka,\nH. “Application of the Bayesian Probability Network to\nMusic Scene Analysis”, Rosenthal, D.H. and Okuno,\nH.G.Computational Auditory Scene Analysis , Lawrence\nErlbaumAssociates, Pulishers, pp.115–137, 1998.\n[8] Manjunath, B.S., Salembier, P., and Sikora, T. Introduction\ntoMPEG-7 , John Wiley& Sons Ltd., 2002.\n[9] Nawab,S.H.,Ayyash,S.A.,andWotiz,R.“Identiﬁcationof\nMusical Chords using Constant-Q Spectra”, Proc. ICASSP ,\nV,pp.3373–3376, 2001.\n[10] Sakuraba, Y., Kitahara, T., and Okuno, H.G. “Comparing\nFeatures for Forming Music Streams in Automatic Music\nTranscription”, Proc.ICASSP ,IV,pp.273–276, 2004.\n[11] Sheh, A. and Ellis, D.P.W. “Chord Segmentation and\nRecognition Using EM-Trained Hidden Markov Models”,\nProc.ISMIR ,2003.\n[12] Su,B.andJeng,S.“Multi-timberChordClassiﬁcationUs-\ningWaveletTransformandSelf-organizedMapNeuralNet-\nworks”,Proc.ICASSP ,V,pp.3377–3380, 2001."
    },
    {
        "title": "Web Services for Music Information Retrieval.",
        "author": [
            "Mark Zadel",
            "Ichiro Fujinaga"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417069",
        "url": "https://doi.org/10.5281/zenodo.1417069",
        "ee": "https://zenodo.org/records/1417069/files/ZadelF04.pdf",
        "abstract": "In the emerging world of networked and distributed digital libraries, the Web services framework will be a key to facilitating simple inter-application communication be- tween them. Yet, despite the popularity of Web services in the business sector and their seemingly obvious appli- cability to the digital library domain, and to MIR in par- ticular, the adoption of these new protocols has not been widespread. To demonstrate the tremendous potential of Web ser- vices for MIR, this paper presents an application using the Google and Amazon.com databases to generate clus- ters of related musical artists based on cultural metadata. The use of cultural metadata to determine artist related- ness is valuable and interesting because it captures emer- gent popular opinion about music. Starting from an initial seed artist, Amazon Listmania! lists are traversed to find potentially related artists. Google is used to determine which of these candidates are in fact related by assess- ing the co-occurrence of the two artists’ names on Internet web pages. A list of artists related to the seed is returned once a given number of artists is found. The positive results generated by the system illustrate the use of Web services for exploiting the vast amount of untapped data that are available today and highlight their importance for the future, when even more musical data will become available.",
        "zenodo_id": 1417069,
        "dblp_key": "conf/ismir/ZadelF04",
        "keywords": [
            "networked",
            "distributed",
            "digital libraries",
            "Web services framework",
            "simple inter-application communication",
            "business sector",
            "digital library domain",
            "MIR",
            "cultural metadata",
            "Google and Amazon.com databases"
        ],
        "content": "WEBSERVICES FORMUSIC INFORMATIONRETRIEVAL\nMarkZadeland IchiroFujinaga\nFacultyofMusic\nMcGillUniversity\nMontr´ eal,QCH3A 1E3\n{zadel,ich }@music.mcgill.ca\nABSTRACT\nIn the emerging world of networked and distributed\ndigitallibraries,theWebservicesframeworkwillbeakey\ntofacilitatingsimpleinter-applicationcommunicationb e-\ntween them. Yet, despite the popularity of Web services\nin the business sector and their seemingly obvious appli-\ncability to the digital library domain, and to MIR in par-\nticular, the adoption of these new protocols has not been\nwidespread.\nTo demonstrate the tremendous potential of Web ser-\nvices for MIR, this paper presents an application using\nthe Google and Amazon.com databases to generate clus-\nters of related musical artists based on cultural metadata.\nThe use of cultural metadata to determine artist related-\nness is valuable and interesting because it captures emer-\ngentpopularopinionaboutmusic. Startingfromaninitial\nseed artist, Amazon Listmania! lists are traversed to ﬁnd\npotentially related artists. Google is used to determine\nwhich of these candidates are in fact related by assess-\ningtheco-occurrenceofthetwoartists’namesonInternet\nweb pages. A list of artists related to the seed is returned\noncea givennumberofartistsisfound.\nThe positive results generated by the system illustrate\nthe use of Web services for exploiting the vast amount of\nuntapped data that are available today and highlight their\nimportance for the future, when even more musical data\nwill becomeavailable.\n1. INTRODUCTION\nOverthelastfewmillennia,humanshaveamassedanenor-\nmousamountofinformationandmaterialthatisscattered\naround the world. It is becoming abundantly clear that\nthe optimal path for creating useful sources of informa-\ntion is to distribute the task of digitizing the wealth of\nhistoricaland culturalheritagematerialthat existsin an a-\nlogue formats. These may include books, manuscripts,\nmusic scores, maps, photographs,videos, analoguetapes,\nPermission tomakedigital orhard copies ofallorpartofthi swork for\npersonal or classroom use is granted without fee provided th at copies\narenotmadeordistributed forproﬁtorcommercialadvantag e andthat\ncopies bear this notice and the full citation on the ﬁrstpage .\nc/circlecopyrt2004 Universitat Pompeu Fabra.and phonograph records. In order to achieve this goal,\nlibraries, museums, and archives throughout the world,\nwhether large or small, need well-researched policies,\nproper guidance, and efﬁcient tools to digitize their col-\nlectionsandtomakethemavailableeconomically.\nThe topic of this paper is to suggest an answer to the\nquestion of how to access and retrieve the data once they\nare stored. This problem arises even for new digitally-\nbornmaterials.\nThousands of libraries worldwide cannot be expected\nto agree on the same database or query systems to access\ntheir data. This paper investigatesthe possibility of usin g\nWeb services, an emergingtechnologywhich is designed\ntoexchangeinformationbetweendifferentsystems,toad-\ndressthisissue.\nTheuseofWebservicesisbecomingincreasinglypop-\nular in business environments. Web services allow easy\ninteroperability between disparate computer systems,\nwhich has historically been difﬁcult. This recent popu-\nlarity has prompted many businesses to add public Web\nservicesinterfacesto their databases, allowing directpr o-\ngrammatic access to them. The alternative for accessing\nthesedataistoextracttheinformationfrombrowserpages\nintended for human viewing, known as “web scraping.”\nThis approach is brittle and error-proneas small changes\nin pageformattingcanbreaktheextractionalgorithm.\nCulturalmetadataisinformationdescribingpublicopin-\nion and cultural trends, distilled from large amounts of\nunstructured text produced by the public. This text is\ntypically drawn from the web. Recent MIR research has\nused cultural metadata to assess similarity between musi-\ncal artists[3][14]. Thisapproachhasthe advantageofus-\ning currentculturalinformationto makejudgmentsabout\ngenreandsimilarity,anddoesnotrelyoncentralised,and\npotentiallybiased, systemsof classiﬁcation. Community-\nbased,collaborativesystemsforclassiﬁcationandﬁlteri ng\nhavebeenexploredpreviously[6][11][12]. Thesesystems\nmakerecommendationstoauserbasedontheopinionsof\notherswhohavedemonstratedsimilartastes. Theyrequire\nthat users explicitly evaluate material and use this infor-\nmation to try to predict the material’s relevance to other\nlike-mindedusers. Using culturalmetadata to capturethe\nsame information eliminates the explicit extra effort re-\nquiredbythesesystems.\nWhilethewebcanbeseenasadirectreﬂectionofpub-lic opinion, its size makes it difﬁcult to harness and ex-\nploit. This issue has received signiﬁcant attention, and\nnowwebsearchingsystemshavebeensuccessfullyscaled\nand improved such that they are able to cope with the\nlargeamountsof data available. An importantexampleis\nGoogle,averypowerfulandpopularsearchtoolwhichef-\nfectively manages this mass of content. Previously, these\nsystems were only accessible via browser interfaces, but\nnow Web services allow them to be used directly. Sud-\ndenly, the unstructured cultural information contained in\nthesedatabasesiseasilyandreliablyaccessibletoextern al\ncomputerprogramsand,speciﬁcally,to MIRprograms.\nThis paper will demonstrate the potential of Web ser-\nvices through an application that exploits their recent de-\nvelopment and highlights the relevance of Web services\nand Internet resources to MIR. The application addresses\ntheproblemofassessing artist relatednessusingonlycul-\ntural metadata. This approachshould yield more relevant\nclassiﬁcationsofartiststhanpreviouslypossiblesincei tis\nbased on large amounts of current public opinion. Here,\ntwoartistsareconsidered“related”iftheirnamesco-occu r\nonthesamewebpage.\nThe application makes use of the Amazon.com and\nGoogle databases in tandem, and assesses artist related-\nness by measuring the co-occurrence of artist names on\nweb pages. Co-occurrence analysis for music classiﬁca-\ntion has been explored in [1][7]. These papers use web\ncrawlingto retrieverelevant web pages, which are parsed\nand analysed. The application presented here differs in\nthat it examines all web pages, not just a predeﬁned sub-\nset of them, and it does so directly through Web services\ninterfaces. The rest of the paper is organized as follows.\nIn Section 2 we introduceWeb services and their compo-\nnents, includingthe services implementedby Googleand\nAmazon.com. InSection3 we describea sampleapplica-\ntionthatusesthesetwodatabases,andtheresultsofsome\nexperimentsusingtheapplicationareshowninSection4.\nIn Section 5 we discuss and analyze the experiment, and\nweconcludeinSection6.\n2. WEB SERVICES\nWeb servicesallow inter-applicationcommunicationover\na network. They adhere to a well-documented standard\n[4] and are strongly supported by industry and the pri-\nmary web standards organization, W3C. The standard is\ndesigned to be lightweight and platform agnostic, allow-\ningcommunicationbetweenbroadclassesofdevices(e.g.,\ncellphonesandSunworkstations). Webservicesarebased\noncommon,acceptedstandards;thisultimatelyfacilitate s\ntheirimplementationandadoption. Theyareimplemented\nin all major languages commonly used for network pro-\ngramming (an example in Python is given in Figure 1).\nThe technologies on which the Web services architecture\nisbasedinclude: SimpleObjectAccessProtocol(SOAP);\nWeb services Description Language (WSDL); and Uni-\nversalDescription,Discovery,andIntegration(UDDI).\nThe core technology in Web services is XML, a com-mon standard for data representation. It is simple and\nhuman-readable,and is ubiquitousin contemporarycom-\nputing. A large software infrastructureexists for working\nwithXMLdata. SOAP,basedonXML,isusedasthestan-\ndard message-encoding format. Messages between com-\nputer applications are encoded as SOAP messages and\nsent via HTTP (see Figure 2). Responses are similarly\nencodedandarereturnedtotherequestor.\nAlso based on the XML format, UDDI is used to reg-\nister each institution’s services (functioning as a virtua l\nyellow pages directory). WSDL is used to describe the\ntypeofservice,itsaccessprotocol,anditslocation. Thus ,\noneusesUDDItolookforservicesordata,andWSDLto\nﬁnd out how and where to use the service, all via SOAP\nmessages.\nAsWebservicesaregenerallyapplicabletosystemin-\nteroperability and access, they have obvious relevance to\nMIR research. The general unstructured information on\nthe web can be harnessed (as is demonstrated in this pa-\nper),andMIRdatabasescanbemadetocommunicateand\nintegrateeach other’sresources,as well as a host of other\napplications.\nTheattractivenessofthistechnologyforapplicationin\ndistributeddigitallibrariesisthatitassumesthateachs ys-\ntem(library)willbedifferent. Webservicesprovideusers\nwith the “what, where, and how” required to access in-\nformation from heterogeneous systems. These types of\nuniversal methods for ﬁnding out how to access various\narchivesandcollections,eachwithdifferentdatabaselan -\nguagesanddifferentkindsofservices,arenotonlyuseful\nbut will become more and more essential as archives and\nlibraries around the world begin to convert their collec-\ntionsintodigitallyaccessibleformats.\nDespite their tremendous potential and signiﬁcant in-\ndustry activity, Web services have not been used widely\nin the digital library domain. The open-source digital li-\nbrary management system Fedora is one of the few ex-\nceptions. It promotesthe distributed digital library arch i-\ntecturethroughinteroperableaccesstodigital objectsan d\ncommunicationacrosstheInternetbasedonWebservices\n[8].\nAs mentioned, industry interest has resulted in a great\ndeal of development of Web services and related stan-\ndards. Although in its infancy, some early adopters are\nsuccessfully using the technology, demonstrating its po-\ntential. TwosuchexamplesareGoogleandAmazon.com.\n2.1. GoogleWeb Services\nGoogleisbyfar themost popularsearch enginecurrently\nonthe web. It maintainsa massivedatabasewith a poten-\ntial wealth of information. Google provides access to its\ndatabase via its Web services API (application program-\nming interface) [5]. The interface allows programmatic\naccesstostandardGooglesearches,cachedpageretrieval,\nand spelling suggestion searches. All of the information\nreturned in a typical Google query done by hand is en-\ncodedintheresponse: searchresults,pagesnippets,resul timport SOAP\nserver = SOAP.SOAPProxy(\"http://services.xmethods.net /soap/servlet/rpcrouter\")\nprint server._ns(\"urn:xmethods-Temperature\").getTemp (zipcode=\"90210\")\nFigure1. AcompletePythonprogramforgettingthe currenttemperat ureina U.S.zipcoderegionusingSOAP\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<SOAP-ENV:Envelope\nSOAP-ENV:encodingStyle=\"http://schemas.xmlsoap.org/ soap/encoding/\"\nxmlns:SOAP-ENC=\"http://schemas.xmlsoap.org/soap/enc oding/\"\nxmlns:xsi=\"http://www.w3.org/1999/XMLSchema-instanc e\"\nxmlns:SOAP-ENV=\"http://schemas.xmlsoap.org/soap/env elope/\"\nxmlns:xsd=\"http://www.w3.org/1999/XMLSchema\">\n<SOAP-ENV:Body>\n<ns1:getTemp xmlns:ns1=\"urn:xmethods-Temperature\" SOA P-ENC:root=\"1\">\n<zipcode xsi:type=\"xsd:string\">90210</zipcode>\n</ns1:getTemp>\n</SOAP-ENV:Body>\n</SOAP-ENV:Envelope>\nFigure2. TherawXMLSOAP requestproducedbytheexampleinFigure1\ncounts, etc. This allows Google data to be programmati-\ncally queried and the results aggregated to identify pat-\nterns and trends. (See Figure 3 for an example and see\nSection3foritsexplanation.)\nA free license key must be obtained to use the ser-\nvice,whichmust beincludedin eachrequest. Up to1000\nqueriesmaybemadeperdaywith agivenkey.\n2.2. AmazonWeb Services\nAmazonprovidesprogrammaticaccesstoitsdatabasevia\nWeb services as well [2]. A wide variety of queries are\naccepted: product number (ASIN), musical artist, author,\nﬁlm director, manufacturer, etc. Queries return detailed\nlists of products offered for sale, which include product\nname,author,availability,manufacturer,price,release date,\nISBN, etc. Thedatabaseislargeandwell organized.\nAmazon provides various ways for customers to rate\nandreviewsales items. Onemethodis Listmania! ,allow-\ning Amazon users create lists of their favourite or related\nitems. Aproductpagewillincludelinkstoliststhatrefer-\nence it to help consumers ﬁnd other products they might\nbe interested in. These lists can be queried via Amazon\nWeb servicesaswell.\nLike the Google Web API, Amazon Web services re-\nquireafreelicensekeywhichmustbeincludedwitheach\nrequest. Amazonasksthatclientsgenerateonlyonequery\npersecond,andthat searchresultsarecachedlocally.\n3. A SAMPLE APPLICATION\nAsan exampleofthe potentialapplicationofWeb ser-\nvices to music information retrieval, a programwas writ-\nten that uses the Google and Amazon Web APIs to ac-\ncess information that would otherwise be difﬁcult to ob-\ntain. The following problem was posed: given an initial\nseed artist, generate a list of related musical artists base d\non cultural metadata. The main advantage in using thisapproach to determine relatedness is that it relies on cur-\nrent trends in public opinion, and not on a centralized,\ntop-down approach. These lists of related artists could\nbe used for recommendations,or to track genre evolution\novertime.\nThe Google and Amazon databases contain large\namounts of information contributed by the general pub-\nlic. Google indexes the World Wide Web and Amazon’s\nListmania! lists are contributed and edited by Amazon\nusers. The example application accesses this information\nviaWebservicesandusesittoaddresstheaboveproblem.\nThe following assumptions are made: artists who appear\nonthesame Listmania! listarelikelytoberelated,andre-\nlated artists’ namesaremorelikely toappearonthe same\nweb page than if they were not related. Only information\ngenerated by the general public was used in this experi-\nment.\nThe program works as follows. Starting from a seed\nartist, a list of potentiallyrelated artists is generatedf rom\nAmazon data. The pool of potentially related artists is\ngrown recursively from this initial list, ﬁnding artists re -\nlated tothoseartists, etc. This tree is pruned using a re-\nlatedness metric based on Google data. Once some given\nnumber of related artists has been reached, the program\nterminates.\nAmazonListmania! lists are used to generate poolsof\npotentially related artists from an initial one. First, the\ninitial artist’s releases are each queried in the Amazon\ndatabase. For each of these releases, a set of Listmania!\nlists are returned which include the release. These lists\nare each queried, and the artists included in each list are\nreturned. Thus, given an initial artist, we can generate a\npoolofpotentiallyrelatedones.\nGoogle results counts are used to assess the actual re-\nlatedness of two artists. Three queries are done, and the\nresults count is retained from each: “artist1”,“artist2”,\nand“artist1” “artist2” (whereartist1andartist2are re-\nplacedwiththenamesoftheartistsbeingcompared). Each# return a scalar that measures the relatedness between\n# a pair of artists (artist1,artist2)\ndef google_relatedness(artist1,artist2):\n# enclose artist names in quotes\nartist1 = '\"' + artist1 + '\"'\nartist2 = '\"' + artist2 + '\"'\n# find the ratio of the intersection to the smaller of the two s ets\nartist1count = get_google_results_count(artist1)\nartist2count = get_google_results_count(artist2)\ncombinedcount = get_google_results_count(artist1 + ' ' + a rtist2)\nrelatednessmeasure = float(combinedcount)/min(artist1 count,artist2count)\nreturn relatednessmeasure\nFigure3. SamplePythoncodeto queryGooglefortwo artists\nnameisenclosedindoublequotestoensurethatit iscon-\nsidered atomically. A single scalar value is computedfor\nrelatednessaccordingto:\nresults( combined )\nmin(results( artist1 ),results( artist2 )).(1)\nThus,therelatednessisconsideredtobethepercentageof\npagesforagivenartistthatincludetheotherartist’sname\n(seeFigure3). Theminimumisusedtocorrectforthedif-\nference in popularity of the artists. This metric, although\nsimple,providedreasonableresults.\nThus, we generate a list of potentially related artists\nand eliminate the ones that are deemed unrelated accord-\ning to the above Google relatedness metric. The remain-\ning (related) artists are queried recursively. Relatednes s\nisalwaysmeasuredwith respectto theoriginalseedartist\nspeciﬁedintheﬁrst iteration.\n3.1. ImplementationDetails\nThe application was implementedin Python. Python was\nwell suitedtothisexperimentsince it featuresa wealthof\nhigh-levelmodulesand lends itself to rapid development.\nInparticular,modulesspeciﬁcallydesignedforinterfaci ng\nwith the Google Web API and Amazon Web services are\navailable[9][10]. Theirexistencespeakstotheubiquityo f\nthe technologies Web services are built on; Python mod-\nules handling XML are included in the standard distribu-\ntion, and modules handling SOAP messaging are readily\navailable[13].\nTheapplicationworksbytrackingthe set ofartistsleft\nto examine. Initially, the set only contains the seed artist .\nOne artist is taken fromthe set, and a set of artists poten-\ntiallyrelatedtothatartistaregeneratedbyqueryingAma-\nzondataasdescribedabove. Artistsareremovedfromthis\nset that are deemed unrelated to the seed artist according\nto the Google relatednessmetric. A relatednessthreshold\nof0.05wassetempirically,abovewhichartistswerecon-\nsideredrelated. Theremainingartistsareretainedasbein g\nrelated to the seed artist for ﬁnal output, and are added to\nthe set of artists left to examine. This process is repeated\nuntiltheset ofrelatedartistsreachesa givensize.The Google results counts are cached locally in a\nPythondictvariable which is saved between application\ninvocations. This reduces the number of calls into the\nGoogle database, reducing network trafﬁc and saving\nqueriesassociated with the license key. This is especially\nimportantsincevariousqueriesarere-executedoften(e.g .,\nsearchesfortheseedartist,multipleexecutionsofthepro -\ngram). A wrapper function ( get_google_results_\ncount) is used which ﬁrst checks the local cache before\ncalling into the Google database. Similarly, sets of artist s\nandListmania! lists already checked are maintained, and\nno artist or list is checked in the Amazon database twice.\nThese caches can be safely assumed to be valid over one\nortwo days.\nUsing Python proved to be a sensible decision for this\nproject for the reasons mentioned above. The implemen-\ntation is clean and lightweight. Elegance of implementa-\ntion was prioritised over computational efﬁciency as the\nscript runtime was dominated by the time spent waiting\nforqueryresponses\n4. RESULTS\nThe program was tested starting from a variety of seed\nartists, withvaryingstylesandpopularity. Theseedsused\nwereFourTet(electronic),Slayer(heavymetal),Christin a\nAguilera(dancepop),KennyG(adultcontemporary),and\nJohn Coltrane (jazz). In each case, the program success-\nfully returneda list of artiststhat seem reasonablyrelate d\ntotheseedartist. Foreachartisttheentireprocesstakesa n\naverage of about two minutes. The artists were typically\norderedbygenre,but somecrossoveroccurred. Thisis to\nbe expected, as cultural metadata encapsulates more than\nsimple genre. Example results for Christina Aguilera are\nshown in Tables 1 and 2, which show 40 of a total of 168\nartists returned. The results for John Coltrane are shown\ninTables3and4. Overall,theresultsarepositive: solofe-\nmalepopartistsandmainstreamjazzartistsandgroupsare\nranked highly, and there are no glaring aberrations. The\nlast twentyentriesaresmaller artistsor arefromdifferen tArtist Google Relatedness\nChristina Aguilera/Lil Kim/Mya/Pink 1\nAguilera 0.905683192\nBritney Spears 0.721886336\nSisqo 0.691678035\nSpears 0.684401451\nToniBraxton 0.658564815\nWilla Ford 0.632508834\nEmmaBunton 0.498381877\nSusan Tedeschi 0.480686695\nPatricia Manterola 0.461621622\nMelanie C 0.425213675\nBBMak 0.419417476\nJennifer Lopez 0.396614268\nLil’ Bow Wow 0.395973154\nBilly Gilman 0.391975309\nFaith Hill 0.374774775\nMichelle Branch 0.353571429\nJustin Timberlake 0.350148368\nO-Town 0.347962382\nMadonna 0.330108827\nTable 1. ToptwentyresultsforChristinaAguilera\nArtist Google Relatedness\nRosemary Clooney 0.033737864\nPhoebe Snow 0.032386364\nAmel Larrieux 0.03168\nTheGo-Go’s 0.030064935\nRowland 0.029382304\nMuse 0.027569528\nTimPierce 0.022851920\nLynda 0.021138211\nHeitor Pereira 0.018274112\nMartin Tillman 0.017647059\nVarious Artists 0.017533253\nJohnny Mori 0.016697588\nAnahi 0.015185950\nJulia Migenes 0.010829493\nElastica 0.008759036\nInterpol 0.007124464\nVarious Artists 0.006873239\nBruce Fowler 0.004733728\nCraig Eastman 0.002016129\nMichael Fisher 0.001507937\nTable 2. Last twenty results (out of 168) for Christina\nAguilera\ngenres. Even with such a simple relatedness calculation,\nthesystemperformsverywell.\nA numberof problemsare apparent,however,and this\nexample illustrates some of these issues. Some of the\n“artist names” do not correspond to actual artists (e.g.,\n“Various Artists” and “Christina Aguilera/Pink/Lil Kim/\nMya”). Names in Amazon’s database are case sensitive,\nand are sometimes duplicated with different capitaliza-\ntion. Also, artists appear multiple times with different\nnames (e.g., “Britney Spears” and “Spears”). These are\nall issueswithinconsistencyintheAmazondatabase.\n5. DISCUSSION\nArtist relatedness is doubly enforced in this experiment,\nthrough proximity in both the Amazon and Google\ndatabases. ItisnotclearwhethereithermethodonitsownArtist Google Relatedness\nMiles Davis 0.413888889\nMcCoy Tyner 0.393333333\nMiles Davis Quintet 0.391872279\nSonny Rollins 0.373853211\nThelonious Monk 0.371115174\nDexter Gordon 0.365187713\nCannonball Adderley 0.361276596\nHorace Silver 0.358108108\nHank Mobley 0.352231604\nCharles Mingus 0.350877193\nOrnette Coleman 0.333221477\nArtBlakey 0.311858974\nDizzy Gillespie Quintet 0.286738351\nRahsaan Roland Kirk 0.283450704\nWayneShorter 0.280924855\nKenny Burrell 0.277922078\nSonny Clark 0.276315789\nPeter Brotzmann Octet 0.276119403\nJohnny Grifﬁn 0.266968326\nBrown 0.251851852\nTable 3. ToptwentyresultsforJohnColtrane\nArtist Google Relatedness\nRyan Adams 0.014444444\nCompay Segundo 0.014344262\nPrince &the Revolution 0.014089347\nPhilip Glass 0.013927227\nQueens of theStone Age 0.012962963\nFugazi 0.012472648\nHelmet 0.012222222\nFiona Apple 0.011153342\nJoseph Arthur 0.010578947\nDoves 0.010555556\nGregorian Chant 0.009694444\nMorello 0.009017857\nJohn Adams 0.007777778\nHowie Day 0.006848249\nHerring 0.006805556\nDamien Rice 0.006228669\nInterpol 0.006120370\nIbrahim Ferrar 0.005325444\nFranky Perez 0.002735562\nSpottiswoode 0.001424051\nTable4. Lasttwentyresults(outof139)forJohnColtrane\nwould producegood results. Amazon Listmania! lists by\ndeﬁnitiongrouprelateditems,butnotnecessarilyinways\nthat one would expect (e.g., “Really terrible music from\nthe eighties,” “Artists born on a Tuesday”). Similarly, the\nGoogle relatedness metric performs well when used on a\npool of potentiallyrelated artists as givenby the Amazon\nsearch. It is not clear how it would perform if used on a\nrandomlyselectedpoolofartists.\nThe Google metric is simple and should be subjected\nto more experimentation. One observation was that the\nthresholdused(0.05)seemedtoolowforalargeartistlike\nChristina Aguilera. It stands to reason that very popular\nartists will be talked about more, and perhaps the size of\ntheresultssetsforbothartistsshouldbeincorporatedint o\nthe relatedness calculation. Also, more queries or better\nquery strings could be used with the Google database to\nhelp focusthe results. Forexample,searchescouldbere-stricted to a particular range of dates, or to a particular\ncountry. The results counts returned by Google are of-\nten only estimates; an investigationshouldbe done to see\nwhetherthishasanadverseeffectontherelatednessmea-\nsurement.\nSomeartistsusenamesthatarecommonpartsofspeech\nor are associated with other things (e.g., “Muse,”\n“Brown”). Thiswillcausetheseartiststoberankedlower\nthan they would have been otherwise since the extra re-\nsults will count toward the total. This effect is less likely\nto distort the co-occurrence count since if the two names\nappear on the same page, they are both likely to refer to\nmusicalartists. DuetothesizeofthewebandtheGoogle\ndatabase,theeffectsoftheseexceptionswillbeattenuate d\ninthelargeoverallnumberofpages.\nTheautomatedaccesstothisvastamountofdataisthe\nkey to the use of Web services. Here we only used two\nsources, but theoretically one could automate access to\nhundredsof distributed databases over the Internet. Even\nwiththissimpleandmodestexampleusingtwodatabases,\nrichsourceofrelationshipsbetweenartistsandtheirwork s\ncanberetrieved. Theimportantpointinthisexperimentis\nthatWebservicesallowustoconvenientlyaccessthesere-\nsources,andthat theycontainlargeamountsof extremely\nvaluableinformation.\n6. CONCLUSION\nWebservicescarrytremendouspotentialforMIRresearch.\nCoupled with business initiatives on the Internet, they al-\nloweasyaccesstothemassofbothunstructuredandstruc-\ntured information on the Web. The sample application\npresented here takes advantage of these publicly accessi-\nbleresourcestogenerategroupsofrelatedartistsbasedon\na reﬂection of general public opinion. It quantiﬁes relat-\nednessas a functionof the frequencyof co-occurrenceof\nthe artists’ nameson webpages, whichwould bedifﬁcult\nwithoutWebservicesaccesstotheGoogledatabase. This\napproach is valuable since it encapsulates current public\nopinion. The results were generally promising, correctly\nclusteringrelatedartists. Theypromptfurtherexplorati on,\nandsomepotentialreﬁnementshavebeensuggestedhere.\nThe application serves as an example of the promise of\nWeb services for MIR, and suggests further research into\napplicationsofthesenewlyaccessibleresources.\n7. REFERENCES\n[1] Aucouturier,J.-J.,andF.Pachet.2003.Representing\nmusical genre: A state of the art. Journal of New\nMusicResearch 32(1): 83–93.\n[2] Bauche,P.2003. Amazonhacks .O’ReillyandAsso-\nciates.\n[3] Baumann,S., and O. Hummel.2003.Using cultural\nmetadataforartistrecommendations. Proceedingsof\nthe InternationalConference on WEB Delivering of\nMusic,138–41.[4] Booth, D., H. Haas, F. McCabe, E. Newcomer, M.\nChampion,C.Ferris,andD.Orchard.2004. Webser-\nvicesarchitecture ,W3CWorkingGroupNote,avail-\nable online: http://www.w3.org/TR/2004/\nNOTE-ws-arch-20040211\n[5] Calishain, T. and R. Dornfest. 2003. Google hacks .\nO’ReillyandAssociates.\n[6] Hill, W., L. Stead, M. Rosenstein, and G. Fur-\nnas. 1995. Recommending and evaluating choices\nin a virtual community of use. Proceedings of the\nSIGCHI Conference on Human Factors in Comput-\ningSystems ,194–201.\n[7] Pachet, F., G. Westermann, and D. Laigre. 2001.\nMusical data mining for electronic music distribu-\ntion.ProceedingsoftheInternationalConferenceon\nWEBDeliveringof Music ,101–6.\n[8] Payette, S., and T. Staples. 2002. “The Mellon Fe-\ndoraproject: DigitallibraryarchitecturemeetsXML\nand Web Services.” Sixth European Conference on\nResearch and Advanced Technology for Digital Li-\nbraries.Lecture Notes in Computer Science , Vol.\n2459.Berlin: Springer-Verlag,406–21.\n[9] Pilgrim, M. 2004. PyAmazon. Available on-\nline:http://josephson.org/projects/\npyamazon\n[10] Pilgrim, M. 2004. PyGoogle. Available online:\nhttp://pygoogle.sourceforge.net\n[11] Resnick, P., N. Iacovou, M. Suchak, P. Bergstrom,\nandJ.Riedl.1994.GroupLens: anopenarchitecture\nforcollaborativeﬁlteringofnetnews. Proceedingsof\ntheACMConferenceonComputerSupportedCoop-\nerativeWork , 175–86.\n[12] Shardanand,U., and P. Maes. 1995.Social informa-\ntion ﬁltering: algorithms for automating “word of\nmouth”.Proceedings of the SIGCHI Conference on\nHumanFactorsin ComputingSystems , 210–7.\n[13] Ullman, C., and B. Matthews. SOAPpy, 2004.\nAvailable online: http://pywebsvcs.\nsourceforge.net\n[14] Whitman, B., and S. Lawrence. 2002. Inferring de-\nscriptions and similarity for music from community\nmetadata. Proceedings of the International Com-\nputerMusic Conference ,591–8."
    },
    {
        "title": "ISMIR 2004, 5th International Conference on Music Information Retrieval, Barcelona, Spain, October 10-14, 2004, Proceedings",
        "author": [],
        "year": "2004",
        "doi": "10.5281/zenodo.1285647",
        "url": "https://doi.org/10.5281/zenodo.1285647",
        "ee": null,
        "abstract": "The Annotated Jingju Arias Dataset is a collection of 34 jingju arias manually segmented in various levels using the software Praat v5.3.53. The selected arias contain samples of the two main shengqiang in jingju, name xipi and erhuang, and the five main role types in terms of singing, namely, dan, jing, laodan, laosheng and xiaosheng.\n\nThe dataset includes a Praat TextGrid file for each aria with the following tiers (all the annotations are in Chinese):\n\n\n\taria: name of the work (one segment for the whole aria)\n\tMBID: MusicBrainz ID of the audioi recording(one segment for the whole aria)\n\tartist: name of the singing performer (one segment for the whole aria)\n\tschool: related performing school (one segment for the whole aria)\n\trole-type: role type of the singing character(one segment for the whole aria)\n\tshengqiang:boundaries and label of theshengqiangperformed in the aria (including accompaniment)\n\tbanshi: boundaries and label of the banshi performed in the aria (including accompaniment)\n\tlyrics-lines: boundaries and annotation of each line of lyrics\n\tlyrics-syllables: boundaries and annotation of each syllable\n\tluogu: boundaries and label of each of the performed percussion patterns in the aria\n\n\nThe ariasInfo.txt file contains a summary of the contents per aira of the whole dataset.\n\nA subset of this dataset comprising 20 arias has been used for the study of the relationship between linguistic tones and melody in the following papers:\n\n\nShuoZhang, Rafael Caro Repetto, and Xavier Serra (2014) Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing. In Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014), Taipei, Taiwan, October 2731, pp. 343348.\n\n\n\n______ (2015) Predicting Pairwise Pitch Contour Relations Based on Linguistic Tone Information in Beijing Opera Singing. In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015), Mlaga, Spain, October 2630, pp. 107113.\n\n\nHere is the list of the arias from the dataset used in these papers.\n\nThe whole dataset has been used for the automatic analysis of the structure of jingju arias and their automatic segmentation in the following master&#39;s thesis:\n\n\nYile Yang(2016) Structure Analysis of Beijing Opera Arias. Masters thesis, Universitat Pompeu Fabra, Barcelona.\n\n\nUsing this dataset\n\nIf you use this dataset in a publication, please cite the above publications.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nThe audio recordings used for these annotations are available for research purposes. Please contact Rafael Caro Repetto\n\nrafael.caro@upf.edu\n\n\n\nhttp://compmusic.upf.edu/node/349",
        "zenodo_id": 1285647,
        "dblp_key": "conf/ismir/2004"
    }
]