[
    {
        "title": "Polyphonic transcription by non-negative sparse coding of power spectra.",
        "author": [
            "Samer M. Abdallah",
            "Mark D. Plumbley"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415072",
        "url": "https://doi.org/10.5281/zenodo.1415072",
        "ee": "https://zenodo.org/records/1415072/files/AbdallahP04.pdf",
        "abstract": "We present a system for adaptive spectral basis decompo- sition that learns to identify independent spectral features given a sequence of short-term Fourier spectra. When ap- plied to recordings of polyphonic piano music, the indi- vidual notes are identified as salient features, and hence each short-term spectrum is decomposed into a sum of note spectra; the resulting encoding can be used as a ba- sis for polyphonic transcription. The system is based on a probabilistic model equivalent to a form of noisy inde- pendent component analysis (ICA) or sparse coding with non-negativity constraints. We introduce a novel mod- ification to this model that recognises that a short-term Fourier spectrum can be thought of as a noisy realisation of the power spectral density of an underlying Gaussian process, where the noise is essentially multiplicative and non-Gaussian. Results are presented for an analysis of a live recording of polyphonic piano music.",
        "zenodo_id": 1415072,
        "dblp_key": "conf/ismir/AbdallahP04",
        "keywords": [
            "adaptive spectral basis decomposition",
            "identifies independent spectral features",
            "polyphonic piano music",
            "salient features",
            "probabilistic model",
            "noisy independent component analysis",
            "sparse coding",
            "non-negativity constraints",
            "Gaussian process",
            "multiplicative noise"
        ]
    },
    {
        "title": "Time Series Alignment for Music Information Retrieval.",
        "author": [
            "Norman H. Adams",
            "Mark A. Bartsch",
            "Jonah Shifrin",
            "Gregory H. Wakefield"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415694",
        "url": "https://doi.org/10.5281/zenodo.1415694",
        "ee": "https://zenodo.org/records/1415694/files/AdamsBSW04.pdf",
        "abstract": "Time series representations are common in MIR appli- cations such as query-by-humming, where a sung query might be represented by a series of \u2018notes\u2019 for database retrieval. While such a transcription into a sequence of (pitch, duration) pairs is convenient and musically intu- itive, there is no evidence that it is an optimal represen- tation. The present work explores three time series repre- sentations for sung queries: a sequence of notes, a \u2018smooth\u2019 pitch contour, and a novel sequence of pitch histograms. Dynamic alignment procedures are described for the three representations. Multiple continuity constraints are ex- plored and a modified dynamic alignment procedure is de- scribed for the histogram representation. We measure the performance of the three representations using a collection of naturally sung queries applied to a target database of varying size. The results show that the note representation lends itself to rapid retrieval whereas the contour represen- tation lends itself to robust performance. The histogram representation yields performance nearly as robust as the contour representation, but with computational complex- ity similar to the note representation.",
        "zenodo_id": 1415694,
        "dblp_key": "conf/ismir/AdamsBSW04",
        "keywords": [
            "time series representations",
            "MIR applications",
            "query-by-humming",
            "sung queries",
            "sequence of notes",
            "pitch contour",
            "pitch histograms",
            "dynamic alignment procedures",
            "continuity constraints",
            "performance measurement"
        ]
    },
    {
        "title": "Whose future is it?",
        "author": [
            "Philippe Aigrain"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416404",
        "url": "https://doi.org/10.5281/zenodo.1416404",
        "ee": "https://zenodo.org/records/1416404/files/Aigrain04.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1416404,
        "dblp_key": "conf/ismir/Aigrain04"
    },
    {
        "title": "Tempo And Beat Estimation Of Musical Signals.",
        "author": [
            "Miguel A. Alonso 0002",
            "Ga\u00ebl Richard",
            "Bertrand David 0002"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415784",
        "url": "https://doi.org/10.5281/zenodo.1415784",
        "ee": "https://zenodo.org/records/1415784/files/AlonsoRD04.pdf",
        "abstract": "Tempo estimation is fundamental in automatic music processing and in many multimedia applications. This paper presents an automatic tempo tracking system that processes audio recordings and determines the beats per minute and temporal beat location. The concept of spec- tral energy flux is defined and leads to an efficient note on- set detector. The algorithm involves three stages: a front- end analysis that efficiently extracts onsets, a periodicity detection block and the temporal estimation of beat loca- tions. The performance of the proposed method is evalu- ated using a large database of 489 excerpts from several musical genres. The global recognition rate is 89.7 %. Results are discussed and compared to other tempo esti- mation systems. Keywords: beat, tempo, onset detection.",
        "zenodo_id": 1415784,
        "dblp_key": "conf/ismir/AlonsoRD04",
        "keywords": [
            "automatic tempo tracking system",
            "audio recordings",
            "beats per minute",
            "temporal beat location",
            "spectral energy flux",
            "note onset detector",
            "front-end analysis",
            "periodicity detection block",
            "temporal estimation",
            "global recognition rate"
        ]
    },
    {
        "title": "Tools and Architecture for the Evaluation of Similarity Measures : Case Study of Timbre Similarity.",
        "author": [
            "Jean-Julien Aucouturier",
            "Fran\u00e7ois Pachet"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416562",
        "url": "https://doi.org/10.5281/zenodo.1416562",
        "ee": "https://zenodo.org/records/1416562/files/AucouturierP04.pdf",
        "abstract": "The systematic testing of the very many parameters and algorithmic variants involved in the design of high-level music descriptors at large, and similarity measure in par- ticular, is a daunting task, which requires the building of a general architecture which is nearly as complex as a full- fledge Music Browsing system. In this paper, we report on experiments done in an attempt to improve the perfor- mance of the music similarity measure described in [2], using the Cuidado Music Browser ([8]). We do not prin- cipally report on the actual results of the evaluation, but rather on the methodology and the various tools that were built to support such a task. We show that many non- technical browsing features are useful at various stages of the evaluation process, and in turn that some of the tools developed for the expert user can be reinjected into the Music Browser, and benefit the non-technical user.",
        "zenodo_id": 1416562,
        "dblp_key": "conf/ismir/AucouturierP04",
        "keywords": [
            "systematic testing",
            "high-level music descriptors",
            "algorithmic variants",
            "Music Browsing system",
            "improving performance",
            "Music Browser",
            "evaluation process",
            "non-technical browsing features",
            "expert user tools",
            "reinjected into Music Browser"
        ]
    },
    {
        "title": "From Sound Sampling To Song Sampling.",
        "author": [
            "Jean-Julien Aucouturier",
            "Fran\u00e7ois Pachet",
            "Peter Hanappe"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416028",
        "url": "https://doi.org/10.5281/zenodo.1416028",
        "ee": "https://zenodo.org/records/1416028/files/AucouturierPH04.pdf",
        "abstract": "This paper proposes to use the techniques of Music Information Retrieval in the context of Music Interaction. We describe a system, the SongSampler, inspired by the technology of audio sampling, which automatically samples a song to produce an instrument (typically using a MIDI keyboard) that plays sounds found in the original audio file. Playing with such an instrument creates an original situation in which listeners play their own music with the sounds of their favourite tunes, in a constant interaction with a music database.  The paper describes the main technical issues at stake concerning the integration of music information retrieval in an interactive instrument, and reports on preliminary experiments.",
        "zenodo_id": 1416028,
        "dblp_key": "conf/ismir/AucouturierPH04",
        "keywords": [
            "Music Information Retrieval",
            "Music Interaction",
            "SongSampler",
            "Automatic Sampling",
            "Instrumentation",
            "MIDI Keyboard",
            "Original Situation",
            "Listeners",
            "Favourite Tunes",
            "Music Database"
        ]
    },
    {
        "title": "GREENSTONE as a Music Digital Library Toolkit.",
        "author": [
            "David Bainbridge 0001",
            "Sally Jo Cunningham",
            "J. Stephen Downie"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417573",
        "url": "https://doi.org/10.5281/zenodo.1417573",
        "ee": "https://zenodo.org/records/1417573/files/BainbridgeCD04.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1417573,
        "dblp_key": "conf/ismir/BainbridgeCD04"
    },
    {
        "title": "Visual Collaging Of Music In A Digital Library.",
        "author": [
            "David Bainbridge 0001",
            "Sally Jo Cunningham",
            "J. Stephen Downie"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415832",
        "url": "https://doi.org/10.5281/zenodo.1415832",
        "ee": "https://zenodo.org/records/1415832/files/BainbridgeCD04a.pdf",
        "abstract": "This article explores the role visual browsing can play within a digital music library. The context to the work is provided through a review of related techniques drawn from the fields of digital libraries and human computer in- teraction. Implemented within the open source digital li- brary toolkit Greenstone, a prototype system is described that combines images located through textual metadata with a visualisation technique known as collaging to pro- vide a leisurely, undirected interaction with a music col- lection. Emphasis in the article is given to the augmenta- tions of the basic technique to work in the musical domain.",
        "zenodo_id": 1415832,
        "dblp_key": "conf/ismir/BainbridgeCD04a",
        "keywords": [
            "digital music library",
            "visual browsing",
            "digital libraries",
            "human computer interaction",
            "Greenstone",
            "collaging",
            "visualisation technique",
            "leisurely interaction",
            "musical domain",
            "prototype system"
        ]
    },
    {
        "title": "Classification of musical genre: a machine learning approach.",
        "author": [
            "Roberto Basili 0001",
            "Alfredo Serafini",
            "Armando Stellato"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.6365709",
        "url": "https://doi.org/10.5281/zenodo.6365709",
        "ee": "http://ismir2004.ismir.net/proceedings/p092-page-505-paper239.pdf",
        "abstract": "This record contains raw data related to article  Distinct responses of newly identified monocyte subsets to advanced gastrointestinal cancer and COVID-19\n\nMonocytes are critical cells of the immune system but their role as effectors is relatively poorly understood, as they have long been considered only as precursors of tissue macrophages or dendritic cells. Moreover, it is known that this cell type is heterogeneous, but our understanding of this aspect is limited to the broad classification in classical/intermediate/non-classical monocytes, commonly based on their expression of only two markers, i.e. CD14 and CD16. We deeply dissected the heterogeneity of human circulating monocytes in healthy donors by transcriptomic analysis at single-cell level and identified 9 distinct monocyte populations characterized each by a profile suggestive of specialized functions. The classical monocyte subset in fact included five distinct populations, each enriched for transcriptomic gene sets related to either inflammatory, neutrophil-like, interferon-related, and platelet-related pathways. Non-classical monocytes included two distinct populations, one of which marked specifically by elevated expression levels of complement components. Intermediate monocytes were not further divided in our analysis and were characterized by high levels of human leukocyte antigen (HLA) genes. Finally, we identified one cluster included in both classical and non-classical monocytes, characterized by a strong cytotoxic signature. These findings provided the rationale to exploit the relevance of newly identified monocyte populations in disease evolution. A machine learning approach was developed and applied to two single-cell transcriptome public datasets, from gastrointestinal cancer and Coronavirus disease 2019 (COVID-19) patients. The dissection of these datasets through our classification revealed that patients with advanced cancers showed a selective increase in monocytes enriched in platelet-related pathways. Of note, the signature associated with this population correlated with worse prognosis in gastric cancer patients. Conversely, after immunotherapy, the most activated population was composed of interferon-related monocytes, consistent with an upregulation in interferon-related genes in responder patients compared to non-responders. In COVID-19 patients we confirmed a global activated phenotype of the entire monocyte compartment, but our classification revealed that only cytotoxic monocytes are expanded during the disease progression. Collectively, this study unravels an unexpected complexity among human circulating monocytes and highlights the existence of specialized populations differently engaged depending on the pathological context.",
        "zenodo_id": 6365709,
        "dblp_key": "conf/ismir/BasiliSS04",
        "keywords": [
            "monocytes",
            "immune system",
            "precursors",
            "tissue macrophages",
            "dendritic cells",
            "heterogeneity",
            "transcriptomic analysis",
            "single-cell level",
            "distinct monocyte populations",
            "classical monocytes"
        ]
    },
    {
        "title": "Towards a Socio-cultural Compatibility of MIR Systems.",
        "author": [
            "Stephan Baumann 0001",
            "Tim Pohle",
            "Shankar Vembu"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417733",
        "url": "https://doi.org/10.5281/zenodo.1417733",
        "ee": "https://zenodo.org/records/1417733/files/BaumannPV04.pdf",
        "abstract": "Future MIR systems will be of great use and pleasure for potential users. If researchers have a clear picture about their \u201ccustomers\u201d in mind they can aim at building and evaluating their systems exactly inside the different socio-cultural environments of such music listeners. Since music is in most cases embedded into a socio-cultural process we propose especially to evaluate MIR applications outside the lab during daily activities. For this purpose we designed a mobile music recommendation system relying on a trimodal music similarity metric, which allows for subjective on-the-fly adjustments of recommendations. It offers online access to large-scale metadata repositories as well as an audio database containing 1000 songs. We did first small- scale evaluations of this approach and came to interesting results regarding the perception of song similarity concerning the relations between sound, cultural issues and lyrics. Our paper will also give insights to the three different underlying approaches for song similarity computation (sound, cultural issues, lyrics), focusing in detail on a novel clustering of album reviews as found at online music retailers. Keywords: Socio-cultural issues in MIR, multimodal song similarity, ecological validation.",
        "zenodo_id": 1417733,
        "dblp_key": "conf/ismir/BaumannPV04",
        "keywords": [
            "Future MIR systems",
            "music listeners",
            "socio-cultural environments",
            "mobile music recommendation",
            "trimodal music similarity",
            "daily activities",
            "large-scale metadata repositories",
            "audio database",
            "online access",
            "song similarity computation"
        ]
    },
    {
        "title": "Fast labelling of notes in music signals.",
        "author": [
            "Paul Brossier",
            "Juan Pablo Bello",
            "Mark D. Plumbley"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416132",
        "url": "https://doi.org/10.5281/zenodo.1416132",
        "ee": "https://zenodo.org/records/1416132/files/BrossierBP04.pdf",
        "abstract": "We present a new system for the estimation of note at- tributes from a live monophonic music source, within a short time delay and without any previous knowledge of the signal. The labelling is based on the temporal segmen- tation and the successive estimation of the fundamental frequency of the current note object. The setup, imple- mented around a small C library, is directed at the robust note segmentation of a variety of audio signals. A system for evaluation of performances is also presented. The fur- ther extension to polyphonic signals is considered, as well as design concerns such as portability and integration in other software environments.",
        "zenodo_id": 1416132,
        "dblp_key": "conf/ismir/BrossierBP04",
        "keywords": [
            "note attributes",
            "live monophonic music source",
            "short time delay",
            "fundamental frequency estimation",
            "temporal segmentation",
            "robust note segmentation",
            "audio signal evaluation",
            "polyphonic signals",
            "portability",
            "integration"
        ]
    },
    {
        "title": "The emergence of complex network patterns in music networks.",
        "author": [
            "Pedro Cano",
            "Markus Koppenberger"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417663",
        "url": "https://doi.org/10.5281/zenodo.1417663",
        "ee": "https://zenodo.org/records/1417663/files/CanoK04.pdf",
        "abstract": "Viewing biological, social or technological systems as net- works formed by nodes and connections between them can help better understand them. We study the topol- ogy of several music networks, namely citation in allmu- sic.com and co-occurrence of artists in playlists. The anal- ysis uncovers the emergence of complex network phe- nomena in music information networks built considering artists as nodes and its relations as links. The proper- ties provide some hints on searchability and possible op- timizations in the design of music recommendation sys- tems. It may also provide a deeper understanding on the similarity measures that can be derived from existing mu- sic knowledge sources.",
        "zenodo_id": 1417663,
        "dblp_key": "conf/ismir/CanoK04",
        "keywords": [
            "networks",
            "nodes",
            "connections",
            "emergence",
            "complexity",
            "searchability",
            "optimizations",
            "design",
            "music recommendation",
            "similarity measures"
        ]
    },
    {
        "title": "Automatic Location And Measurement Of Score-based Gestures In Audio Recordings.",
        "author": [
            "Michael A. Casey",
            "Tim Crawford"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415184",
        "url": "https://doi.org/10.5281/zenodo.1415184",
        "ee": "https://zenodo.org/records/1415184/files/CaseyC04.pdf",
        "abstract": "This paper reports on our first experiments in using the feature extraction tools of the MPEG-7 international stan- dard for multimedia content description on a novel prob- lem, the automatic identification and analysis of score- based performance features in audio recordings of mu- sic. Our test material consists of recordings of two pieces of 17th- and 18th-century lute music in which our aim is to recognise and isolate performance features such as trills and chord-spreadings. Using the audio tools from the MPEG-7 standard facilitates interoperability and al- lows us to share both score and audio metadata. As well as using low-level audio MIR techniques within this MPEG- 7 context, the work has potential importance as an \u2019or- namentation filter\u2019 for MIR systems. It may also form a useful component in methods for instrumental performer identification.",
        "zenodo_id": 1415184,
        "dblp_key": "conf/ismir/CaseyC04",
        "keywords": [
            "MPEG-7",
            "automatic identification",
            "score-based performance features",
            "audio recordings",
            "music",
            "performance features",
            "trills",
            "chord-spreadings",
            "interoperability",
            "low-level audio MIR techniques"
        ]
    },
    {
        "title": "Architecture for an MPEG-7 Web Browser.",
        "author": [
            "\u00d2scar Celma"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417229",
        "url": "https://doi.org/10.5281/zenodo.1417229",
        "ee": "https://zenodo.org/records/1417229/files/Celma04.pdf",
        "abstract": "The MPEG-7 standard provides description mechanisms and taxonomy management for multimedia documents. There are several approaches to design a multimedia database system using MPEG-7 descriptors. We discuss two of them: relational databases and native XML databases. We have implemented a search and retrieval application for MPEG-7 descriptions based on the latter.",
        "zenodo_id": 1417229,
        "dblp_key": "conf/ismir/Celma04",
        "keywords": [
            "MPEG-7 standard",
            "description mechanisms",
            "taxonomy management",
            "multimedia database system",
            "approaches",
            "relational databases",
            "native XML databases",
            "search and retrieval application",
            "MPEG-7 descriptors",
            "XML databases"
        ]
    },
    {
        "title": "A Brazilian Popular Music Oriented Digital Library For Musical Harmony E-Learning.",
        "author": [
            "Fernando William Cruz",
            "Edilson Ferneda",
            "M\u00e1rcio da Costa P. Brand\u00e3o",
            "Evandro de Barros Costa",
            "Hyggo Oliveira de Almeida",
            "Murilo Bastos da Cunha",
            "Rafael de Sousa",
            "Jo\u00e3o Denicol",
            "Carlos da Silva"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417042",
        "url": "https://doi.org/10.5281/zenodo.1417042",
        "ee": "https://zenodo.org/records/1417042/files/CruzFBCACSDS04.pdf",
        "abstract": "This poster presents a digital library proposal conceived for people interested in acquiring knowledge about Brazilian popular music harmony, particularly in Choro. This Brazilian musical style is a complex popular music form based on improvisation, although it contains classical music elements such as the counterpoint. We are proposing two ways of accessing the music virtual library content: a guided navigation mode, in which users interact with a cooperative Web-based learning system; and a free navigation mode, in which users can make their own queries, both through browsers or client applications.",
        "zenodo_id": 1417042,
        "dblp_key": "conf/ismir/CruzFBCACSDS04",
        "keywords": [
            "digital library",
            "knowledge acquisition",
            "Brazilian popular music",
            "Choro",
            "improvisation",
            "classical music",
            "Web-based learning system",
            "free navigation",
            "queries",
            "client applications"
        ]
    },
    {
        "title": "Understanding Search Performance in Query-by-Humming Systems.",
        "author": [
            "Roger B. Dannenberg",
            "Ning Hu"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416900",
        "url": "https://doi.org/10.5281/zenodo.1416900",
        "ee": "https://zenodo.org/records/1416900/files/DannenbergH04.pdf",
        "abstract": "Previous work in Query-by-Humming systems has left open many questions. Although a variety of techniques have been explored, there has been relatively little work to compare them under controlled conditions, especially with \u201creal\u201d audio queries from human subjects. Previous work comparing note-interval matching, melodic con- tour matching, and HMM-based matching is extended with comparisons to the Phillips CubyHum algorithm and various n-gram search algorithms. We also explore the sensitivity of note-interval dynamic programming searches to different parameters and consider two-stage searches combining a fast n-gram search with a more precise but slower dynamic programming algorithm. Keywords: Query-by-Humming, N-gram, Dynamic Programming, Evaluation",
        "zenodo_id": 1416900,
        "dblp_key": "conf/ismir/DannenbergH04",
        "keywords": [
            "Query-by-Humming",
            "note-interval matching",
            "melodic contour matching",
            "HMM-based matching",
            "Phillips CubyHum algorithm",
            "n-gram search algorithms",
            "dynamic programming searches",
            "parameters",
            "two-stage searches",
            "evaluation"
        ]
    },
    {
        "title": "Methodology and Tools for the evaluation of automatic onset detection algorithms in music.",
        "author": [
            "Laurent Daudet",
            "Ga\u00ebl Richard",
            "Pierre Leveau"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417247",
        "url": "https://doi.org/10.5281/zenodo.1417247",
        "ee": "https://zenodo.org/records/1417247/files/DaudetRL04.pdf",
        "abstract": "This paper addresses the problem of the performance eval- uation of algorithms for the automatic detection of note onsets in music signals. Our experiments show that cre- ating a database of reference files with reliable human- annotated onset times is a complex task, since its sub- jective part cannot be neglected. This work provides a methodology to construct such a database. With the use of a carefully designed software tool, called SOL (Sound Onset Labellizer), we can obtain a set of reference onset times that are cross-validated amongst different expert lis- teners. We show that the mean error of annotated times across test subjects is very much signal-dependent. This value can be used, when evaluating automatic labelling, as an indication of the relevant tolerance window. The SOL annotation software is to be released freely for research purposes. Our test library, 17 short sequences contain- ing about 750 onsets, comes from copyright-free music or from the public RWC database. The corresponding vali- dated onset labels are also freely distributed, and are in- tended to form the starting point for the definition of a reliable benchmark.",
        "zenodo_id": 1417247,
        "dblp_key": "conf/ismir/DaudetRL04",
        "keywords": [
            "performance evaluation",
            "automatic detection",
            "note onsets",
            "database creation",
            "subjective part",
            "cross-validation",
            "software tool",
            "SOL annotation",
            "freely distributed",
            "benchmark"
        ]
    },
    {
        "title": "Causal Tempo Tracking of Audio.",
        "author": [
            "Matthew E. P. Davies",
            "Mark D. Plumbley"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417873",
        "url": "https://doi.org/10.5281/zenodo.1417873",
        "ee": "https://zenodo.org/records/1417873/files/DaviesP04.pdf",
        "abstract": "We introduce a causal approach to tempo tracking for mu- sical audio signals. Our system is designed towards an eventual real-time implementation; requiring minimal high- level knowledge of the musical audio. The tempo track- ing system is divided into two sections: an onset analysis stage, used to derive a rhythmically meaningful represen- tation from the input audio, followed by a beat match- ing algorithm using auto- and cross-correlative methods to generate short term predictions of future beats in the audio. The algorithm is evaluated over a range of musical styles by comparing the predicted output to beats tapped by a musician. An investigation is also presented into three rhythmicallycomplex beat tracking problems, where the tempo is not constant. Preliminary results demonstrate good accuracy for this type of system. Keywords \u2013 Tempo tracking, beat analysis, onset detec- tion, rhythmic analysis",
        "zenodo_id": 1417873,
        "dblp_key": "conf/ismir/DaviesP04",
        "keywords": [
            "causal approach",
            "tempo tracking",
            "musical audio signals",
            "real-time implementation",
            "minimal high-level knowledge",
            "onset analysis stage",
            "beat matching algorithm",
            "tempo constant",
            "rhythmically complex beat tracking",
            "tempo accuracy"
        ]
    },
    {
        "title": "Towards Characterisation of Music via Rhythmic Patterns.",
        "author": [
            "Simon Dixon",
            "Fabien Gouyon",
            "Gerhard Widmer"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416220",
        "url": "https://doi.org/10.5281/zenodo.1416220",
        "ee": "https://zenodo.org/records/1416220/files/DixonGW04.pdf",
        "abstract": "A central problem in music information retrieval is finding suitable representations which enable efficient and accu- rate computation of musical similarity and identity. Low level audio features are ideal for calculating identity, but are of limited use for similarity measures, as many aspects of music can only be captured by considering high level features. We present a new method of characterising mu- sic by typical bar-length rhythmic patterns which are au- tomatically extracted from the audio signal, and demon- strate the usefulness of this representation by its applica- tion in a genre classification task. Recent work has shown the importance of tempo and periodicity features for genre recognition, and we extend this research by employing the extracted temporal patterns as features. Standard classifi- cation algorithms are utilised to discriminate 8 classes of Standard and Latin ballroom dance music (698 pieces). Although pattern extraction is error-prone, and patterns are not always unique to a genre, classification by rhyth- mic pattern alone achieves up to 50% correctness (base- line 16%), and by combining with other features, a classi- fication rate of 96% is obtained.",
        "zenodo_id": 1416220,
        "dblp_key": "conf/ismir/DixonGW04",
        "keywords": [
            "music information retrieval",
            "musical similarity",
            "music identity",
            "low level audio features",
            "high level features",
            "typical bar-length rhythmic patterns",
            "genre classification",
            "tempo and periodicity features",
            "dance music",
            "classification algorithms"
        ]
    },
    {
        "title": "Stochastic Model of a Robust Audio Fingerprinting System.",
        "author": [
            "Peter Jan O. Doets",
            "Reginald L. Lagendijk"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416490",
        "url": "https://doi.org/10.5281/zenodo.1416490",
        "ee": "https://zenodo.org/records/1416490/files/DoetsL04.pdf",
        "abstract": "An audio fingerprint is a compact representation of the perceptually relevant parts of audio content. A suitable audio fingerprint can be used to identify audio files, even if they are severely degraded due to compression or other types of signal processing operations. When degraded, the fingerprint closely resembles the fingerprint of the origi- nal, but is not identical. We plan to use a fingerprint not only to identify the song but also to assess the perceptual quality of the compressed content. In order to develop such a fingerprinting scheme, a model is needed to assess the behavior of a fingerprint subject to compression. In this paper we present the initial outlines of a model for an existing robust fingerprinting system to develop a more theoretical foundation. The model describes the stochastic behavior of the system when the input signal is a station- ary (stochastic) signal. In this paper the input is assumed to be white noise. Initial theoretical results are reported and validated with experimental data.",
        "zenodo_id": 1416490,
        "dblp_key": "conf/ismir/DoetsL04",
        "keywords": [
            "audio fingerprint",
            "perceptually relevant parts",
            "audio content",
            "identifying audio files",
            "degraded audio",
            "compression",
            "signal processing",
            "fingerprinting scheme",
            "perceptual quality",
            "stochastic behavior"
        ]
    },
    {
        "title": "A Polyphonic Music Retrieval System Using N-Grams.",
        "author": [
            "Shyamala Doraisamy",
            "Stefan M. R\u00fcger"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417961",
        "url": "https://doi.org/10.5281/zenodo.1417961",
        "ee": "https://zenodo.org/records/1417961/files/DoraisamyR04.pdf",
        "abstract": "This paper describes the development of a polyphonic mu- sic retrieval system with the n-gram approach. Musical n-grams are constructed from polyphonic musical perfor- mances in MIDI using the pitch and rhythm dimensions of music. These are encoded using text characters enabling the musical words generated to be indexed with existing text search engines. The Lemur Toolkit was adapted for the development of a demonstrator system on a collection of around 10,000 polyphonic MIDI performances. The in- dexing, search and retrieval with musical n-grams and this toolkit have been extensively evaluated through a series of experimental work over the past three years, published elsewhere. We discuss how the system works internally and describe our proposal for enhancements to Lemur to- wards the indexing of \u2018overlaying\u2019 as opposed to index- ing a \u2018bag of terms\u2019. This includes enhancements to the parser for a \u2018polyphonicmusical word indexer\u2019 to incorpo- rate within document position information when indexing adjacent and concurrent musical words. For retrieval of these \u2018overlaying\u2019 musical words, a new proximity-based operator and a ranking function is proposed.",
        "zenodo_id": 1417961,
        "dblp_key": "conf/ismir/DoraisamyR04",
        "keywords": [
            "polyphonic music retrieval system",
            "n-gram approach",
            "MIDI",
            "pitch and rhythm dimensions",
            "text characters",
            "Lemur Toolkit",
            "musical words",
            "text search engines",
            "experimental work",
            "enhancements to Lemur"
        ]
    },
    {
        "title": "Micro-level groundtruthing environment for OMR.",
        "author": [
            "Michael Droettboom",
            "Ichiro Fujinaga"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417217",
        "url": "https://doi.org/10.5281/zenodo.1417217",
        "ee": "https://zenodo.org/records/1417217/files/DroettboomF04.pdf",
        "abstract": "A simple framework for evaluating OMR at the symbol level is presented. While a true evaluation of an OMR system requires a high-level analysis, the automation of which is a largely unsolved problem, many high-level er- rors are correlated to these more tractably-analyzed lower- level errors.",
        "zenodo_id": 1417217,
        "dblp_key": "conf/ismir/DroettboomF04",
        "keywords": [
            "OMR",
            "symbol level",
            "evaluation",
            "automation",
            "high-level analysis",
            "tractably-analyzed",
            "lower-level errors",
            "high-level errors",
            "truly",
            "system"
        ]
    },
    {
        "title": "MIR In Matlab: The MIDI Toolbox.",
        "author": [
            "Tuomas Eerola",
            "Petri Toiviainen"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416234",
        "url": "https://doi.org/10.5281/zenodo.1416234",
        "ee": "https://zenodo.org/records/1416234/files/EerolaT04.pdf",
        "abstract": "The MIDI Toolbox is a compilation of functions for analyzing and visualizing MIDI files in the Matlab computing environment. In this article, the basic issues of the Toolbox are summarized and demonstrated with examples ranging from melodic contour, similarity, key- finding, meter-finding to segmentation. The Toolbox is based on symbolic musical data but signal processing",
        "zenodo_id": 1416234,
        "dblp_key": "conf/ismir/EerolaT04",
        "keywords": [
            "MIDI Toolbox",
            "Matlab computing environment",
            "analyzing",
            "visualizing",
            "melodic contour",
            "similarity",
            "key finding",
            "meter finding",
            "segmentation",
            "symbolic musical data"
        ]
    },
    {
        "title": "Extracting Melody Lines From Complex Audio.",
        "author": [
            "Jana Eggink",
            "Guy J. Brown"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418003",
        "url": "https://doi.org/10.5281/zenodo.1418003",
        "ee": "https://zenodo.org/records/1418003/files/EgginkB04.pdf",
        "abstract": "We propose a system which extracts the melody line played by a solo instrument from complex audio. At every time frame multiple fundamental frequency (F0) hypotheses are generated, and later processing uses various knowledge sources to choose the most likely succession of F0s. Knowledge sources include an instrument recognition module and temporal knowledge about tone durations and interval transitions, which are integrated in a probabilistic search. The proposed system improved the number of frames with correct F0 estimates by 14% compared to a baseline system which simply uses the strongest F0 at every point in time. The number of spurious tones was reduced to nearly a third compared to the baseline system, resulting in significantly smoother melody lines.",
        "zenodo_id": 1418003,
        "dblp_key": "conf/ismir/EgginkB04",
        "keywords": [
            "proposed system",
            "extracts melody line",
            "complex audio",
            "multiple fundamental frequency hypotheses",
            "probabilistic search",
            "improved F0 estimates",
            "reduced spurious tones",
            "smooth melody lines",
            "instrument recognition module",
            "temporal knowledge"
        ]
    },
    {
        "title": "Eigenrhythms: Drum pattern basis sets for classification and generation.",
        "author": [
            "Dan Ellis",
            "John Arroyo"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415948",
        "url": "https://doi.org/10.5281/zenodo.1415948",
        "ee": "https://zenodo.org/records/1415948/files/EllisA04.pdf",
        "abstract": "We took a collection of 100 drum beats from popular music tracks and estimated the measure length and down- beat position of each one. Using these values, we normal- ized each pattern to form an ensemble of aligned drum patterns. Principal Component Analysis on this data set results in a set of basis \u2018patterns\u2019 that can be combined to give approximations and interpolations of all the ex- amples. We use this low-dimension representation of the drum patterns as a space for classification and visualiza- tion, and discuss its application to generating continua of rhythms. Our classification results were very modest \u2013 about 20% correct on a 10-way genre classification task \u2013 but we show that the projection into principal compo- nent space reveals aspects of the rhythm that are largely orthogonal to genre but are still perceptually relevant. Keywords: rhythm, genre, classification, principal compo- nents",
        "zenodo_id": 1415948,
        "dblp_key": "conf/ismir/EllisA04",
        "keywords": [
            "drum beats",
            "measure length",
            "down-beat position",
            "normalized patterns",
            "ensemble of aligned",
            "principal component analysis",
            "basis patterns",
            "low-dimensional representation",
            "genre classification",
            "perceptually relevant"
        ]
    },
    {
        "title": "Musical instrument recognition based on class pairwise feature selection.",
        "author": [
            "Slim Essid",
            "Ga\u00ebl Richard",
            "Bertrand David 0002"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418253",
        "url": "https://doi.org/10.5281/zenodo.1418253",
        "ee": "https://zenodo.org/records/1418253/files/EssidRD04.pdf",
        "abstract": "In this work, musical instrument recognition is consid- ered on solo music from real world performance. A large sound database is used that consists of musical phrases ex- cerpted from commercial recordings with different instru- ment instances, different players, and varying recording conditions. The proposed recognition scheme exploits class pairwise feature selection based on inertia ratio maximization. More- over, new signal processing features based on octave band energy measures are introduced that prove to be useful. Classification is performed using Gaussian Mixture Mod- els in a one vs one fashion in association with a data rescal- ing procedure as pre-processing. Experimental results show that substantial improvement in recognition success is thus achieved.",
        "zenodo_id": 1418253,
        "dblp_key": "conf/ismir/EssidRD04",
        "keywords": [
            "musical instrument recognition",
            "solo music",
            "real world performance",
            "large sound database",
            "class pairwise feature selection",
            "inertia ratio maximization",
            "new signal processing features",
            "octave band energy measures",
            "Gaussian Mixture Models",
            "data rescaling procedure"
        ]
    },
    {
        "title": "Beat and meter extraction using gaussified onsets.",
        "author": [
            "Klaus Frieler"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417851",
        "url": "https://doi.org/10.5281/zenodo.1417851",
        "ee": "https://zenodo.org/records/1417851/files/Frieler04.pdf",
        "abstract": "Rhythm, beat and meter are key concepts of music in gen- eral. Many efforts had been made in the last years to au- tomatically extract beat and meter from a piece of music given either in audio or symbolical representation (see e.g. [11] for an overview). In this paper we propose a new method for extracting beat, meter and phase information from a list of unquantized onset times. The procedure re- lies on a novel method called \u2019Gaussification\u2019 and adopts correlation techniques combined with findings from mu- sic psychology for parameter settings.",
        "zenodo_id": 1417851,
        "dblp_key": "conf/ismir/Frieler04",
        "keywords": [
            "Rhythm",
            "beat",
            "meter",
            "music",
            "automatic",
            "extract",
            "audio",
            "symbolical",
            "representation",
            "unquantized"
        ]
    },
    {
        "title": "Estimating The Tonality Of Polyphonic Audio Files: Cognitive Versus Machine Learning Modelling Strategies.",
        "author": [
            "Emilia G\u00f3mez",
            "Perfecto Herrera"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418007",
        "url": "https://doi.org/10.5281/zenodo.1418007",
        "ee": "https://zenodo.org/records/1418007/files/GomezH04.pdf",
        "abstract": "In this paper we evaluate two methods for key estimation from polyphonic audio recordings. Our goal is to compare between a strategy using a cognition-inspired model and several machine learning techniques to find a model for tonality (mode and key note) determination of polyphonic music from audio files. Both approaches have as an input a vector of values related to the intensity of each of the pitch classes of a chromatic scale. In this study, both methods are explained and evaluated in a large database of audio recordings of classical pieces.",
        "zenodo_id": 1418007,
        "dblp_key": "conf/ismir/GomezH04",
        "keywords": [
            "key estimation",
            "polyphonic audio recordings",
            "cognition-inspired model",
            "machine learning techniques",
            "tonality determination",
            "polyphonic music",
            "chromatic scale",
            "large database",
            "audio recordings",
            "classical pieces"
        ]
    },
    {
        "title": "Speech-Recognition Interfaces for Music Information Retrieval: &apos;Speech Completion&apos; and &apos;Speech Spotter&apos;.",
        "author": [
            "Masataka Goto",
            "Katunobu Itou",
            "Koji Kitayama",
            "Tetsunori Kobayashi"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417339",
        "url": "https://doi.org/10.5281/zenodo.1417339",
        "ee": "https://zenodo.org/records/1417339/files/GotoIKK04.pdf",
        "abstract": "This paper describes music information retrieval (MIR) systems featuring automatic speech recognition. Al- though various interfaces for MIR have been proposed, speech-recognition interfaces suitable for retrieving musi- cal pieces have not been studied. We propose two differ- ent speech-recognition interfaces for MIR, speech com- pletion and speech spotter, and describe two MIR-based hands-free jukebox systems that enable a user to retrieve and play back a musical piece by saying its title or the artist\u2019s name. The first is a music-retrieval system with the speech-completion interface that is suitable for mu- sic stores and car-driving situations. When a user can re- member only part of the name of a musical piece or an artist and utters only a remembered fragment, the system helps the user recall and enter the name by completing the fragment. The second is a background-music play- back system with the speech-spotter interface that can en- rich human-human conversation. When a user is talk- ing to another person, the system allows the user to enter voice commands for music-playback control by spotting a special voice-command utterance in face-to-face or tele- phone conversations. Our experimental results from use of these systems have demonstrated the effectiveness of the speech-completion and speech-spotter interfaces. Keywords: speech recognition, MIR interface, hands-free MIR, jukebox, title and artist search Video demonstration: http://staff.aist.go.jp/m.goto/ISMIR2004/",
        "zenodo_id": 1417339,
        "dblp_key": "conf/ismir/GotoIKK04",
        "keywords": [
            "speech recognition",
            "music information retrieval",
            "automatic speech recognition",
            "speech completion",
            "speech spotter",
            "hands-free jukebox",
            "music retrieval system",
            "background music playback",
            "voice commands",
            "human-human conversation"
        ]
    },
    {
        "title": "Dance music classification: A tempo-based approach.",
        "author": [
            "Fabien Gouyon",
            "Simon Dixon"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416636",
        "url": "https://doi.org/10.5281/zenodo.1416636",
        "ee": "https://zenodo.org/records/1416636/files/GouyonD04.pdf",
        "abstract": "Recent research has studied the relevance of various features for automatic genre classification, showing the particular importance of tempo in dance music classifica- tion. We complement this work by considering a domain- specific learning methodology,where the computed tempo is used to select an expert classifier which has been spe- cialised on its own tempo range. This enables the all-class learning task to be reduced to a set of two- and three-class learning tasks. Current results are around 70% classifi- cation accuracy (8 ballroom dance music classes, 698 in- stances, baseline 15.9%).",
        "zenodo_id": 1416636,
        "dblp_key": "conf/ismir/GouyonD04",
        "keywords": [
            "tempo",
            "dance music",
            "genre classification",
            "domain-specific learning",
            "expert classifier",
            "tempo range",
            "all-class learning",
            "two-class learning",
            "three-class learning",
            "classification accuracy"
        ]
    },
    {
        "title": "Melodic Similarity: Looking for a Good Abstraction Level.",
        "author": [
            "Maarten Grachten",
            "Josep Llu\u00eds Arcos",
            "Ram\u00f3n L\u00f3pez de M\u00e1ntaras"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417403",
        "url": "https://doi.org/10.5281/zenodo.1417403",
        "ee": "https://zenodo.org/records/1417403/files/GrachtenAM04.pdf",
        "abstract": "Computing melodic similarity is a very general problem with diverse musical applications ranging from music anal- ysis to content-based retrieval. Choosing the appropriate level of representation is a crucial issue and depends on the type of application. Our research interest concerns the development of a CBR system for expressive music pro- cessing. In that context, a well chosen distance measure for melodies is a crucial issue. In this paper we propose a new melodic similarity measure based on the I/R model for melodic structure and compare it with other existing measures. The experimentation shows that the proposed measure provides a good compromise between discrim- inatory power and ability to recognize phrases from the same song.",
        "zenodo_id": 1417403,
        "dblp_key": "conf/ismir/GrachtenAM04",
        "keywords": [
            "melodic similarity",
            "musical applications",
            "representation choice",
            "CBR system",
            "expressive music processing",
            "distance measure",
            "melodic structure",
            "I/R model",
            "comparison with existing measures",
            "good compromise"
        ]
    },
    {
        "title": "Extraction of Drum Patterns and their Description within the MPEG-7 High-Level-Framework.",
        "author": [
            "Matthias Gruhne",
            "Christian Uhle",
            "Christian Dittmar",
            "Markus Cremer"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1414888",
        "url": "https://doi.org/10.5281/zenodo.1414888",
        "ee": "https://zenodo.org/records/1414888/files/GruhneUDC04.pdf",
        "abstract": "A number of metadata standards have been published in recent years due to the increasing availability of multimedia content and the resulting issue of sorting and retrieving this content. One of the most recent efforts for a well defined metadata description is the ISO/IEC MPEG-7 standard, which takes a very broad approach towards the definition of metadata. Herein, not merely hand annotated textual information can be transported and stored, but also more signal specific data that can in most cases be automatically retrieved from the multimedia content itself. In this publication an algorithm for the automated transcription of rhythmic (percussive) accompaniment in modern day popular music is described. However, the emphasis here is not a precise transcription, but on capturing the \u201crhythmic gist\u201d of the piece of music in pieces by their dominant rhythmic patterns. A small-scale evaluation of the algorithm is presented along with an example representation of the thus gained semantically meaningful metadata using description methods currently discussed within MPEG-7.",
        "zenodo_id": 1414888,
        "dblp_key": "conf/ismir/GruhneUDC04",
        "keywords": [
            "metadata standards",
            "multimedia content",
            "ISO/IEC MPEG-7",
            "automated transcription",
            "rhythmic accompaniment",
            "popular music",
            "semantically meaningful metadata",
            "description methods",
            "evaluation",
            "example representation"
        ]
    },
    {
        "title": "Time-Warped Longest Common Subsequence Algorithm for Music Retrieval.",
        "author": [
            "AnYuan Guo",
            "Hava T. Siegelmann"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417165",
        "url": "https://doi.org/10.5281/zenodo.1417165",
        "ee": "https://zenodo.org/records/1417165/files/GuoS04.pdf",
        "abstract": "Recent advances in music information retrieval have en- abled users to query a database by singing or humming into a microphone. The queries are often inaccurate ver- sions of the original songs due to singing errors and errors introduced in the music transcription process. In this pa- per, we present the Time-Warped Longest Common Sub- sequence algorithm (T-WLCS), which deals with singing errors involving rhythmic distortions. The algorithm is employed on song retrieval tasks, where its performance is compared to the longest common subsequence algorithm.",
        "zenodo_id": 1417165,
        "dblp_key": "conf/ismir/GuoS04",
        "keywords": [
            "Music Information Retrieval",
            "Singing or humming queries",
            "Database accuracy",
            "Rhythmic distortions",
            "Time-Warped Longest Common Sub-sequence algorithm",
            "Song retrieval tasks",
            "Performance comparison",
            "Longest common subsequence algorithm",
            "Singing errors",
            "Music transcription process"
        ]
    },
    {
        "title": "Survey Of Music Information Needs, Uses, And Seeking Behaviours: Preliminary Findings.",
        "author": [
            "Jin Ha Lee 0001",
            "J. Stephen Downie"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417637",
        "url": "https://doi.org/10.5281/zenodo.1417637",
        "ee": "https://zenodo.org/records/1417637/files/HaD04.pdf",
        "abstract": "User studies focusing upon real-life music information needs, uses and seeking behaviours are still very scarce in the music information retrieval (MIR) and music digital library (MDL) fields. We are conducting a multi- group survey in an attempt to acquire information that can help eradicate false assumptions in designing MIR systems. Our goal is to provide an empirical basis for MIR/MDL system development. In this paper, we present our preliminary findings and analyses based on the 427 user responses we have received to date. Two major themes have been uncovered thus far that could have a significant influence the future development of successful MIR/MDL systems. First, people display \u201cpublic information-seeking\u201d behaviours by making use of collective knowledge and/or opinions of others about music such as reviews, ratings, recommendations, etc. in their music information-seeking. Second, respondents expressed needs for contextual metadata in addition to traditional bibliographic metadata. Keywords: context metadata, relational metadata, associative metadata, public information-seeking",
        "zenodo_id": 1417637,
        "dblp_key": "conf/ismir/HaD04",
        "keywords": [
            "user studies",
            "music information needs",
            "music information retrieval",
            "music digital library",
            "survey",
            "empirical basis",
            "MIR/MDL system development",
            "public information-seeking",
            "contextual metadata",
            "traditional bibliographic metadata"
        ]
    },
    {
        "title": "Audio Features for Noisy Sound Segmentation.",
        "author": [
            "Pierre Hanna",
            "Nicolas Louis",
            "Myriam Desainte-Catherine",
            "Jenny Benois-Pineau"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415214",
        "url": "https://doi.org/10.5281/zenodo.1415214",
        "ee": "https://zenodo.org/records/1415214/files/HannaLDB04.pdf",
        "abstract": "Automatic audio classification usually considers sounds as music, speech, silence or noise, but works about the noise class are rare. Audio features are generally specific to speech or music signals. In this paper, we present a new audio feature sets that lead to the definition of four classes: colored, pseudo-periodic, impulsive and sinusoids within noises. This classification relies on works about the per- ception of noises. This audio feature set is experimented for noisy sound segmentation. Noise-to-noise transitions are characterized by means of statistical decision model based on Bayesian framework. This statistical method has been trained and experimented both on synthetic and real audio corpus. Using proposed feature set increases the discriminant power of Bayesian decision approach com- pared to a usual feature set.",
        "zenodo_id": 1415214,
        "dblp_key": "conf/ismir/HannaLDB04",
        "keywords": [
            "audio",
            "classification",
            "sounds",
            "music",
            "speech",
            "silence",
            "noise",
            "perception",
            "noises",
            "segmentation"
        ]
    },
    {
        "title": "An Analytical Methodology for Acousmatic Music.",
        "author": [
            "David Hirst"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415656",
        "url": "https://doi.org/10.5281/zenodo.1415656",
        "ee": "https://zenodo.org/records/1415656/files/Hirst04.pdf",
        "abstract": "This paper presents a procedure for the analysis of acousmatic music which was derived from the synthesis of top-down (knowledge driven) and bottom-up (data- driven) cognitive psychological views. The procedure is also a synthesis of research on primitive auditory scene analysis, combined with the research on acoustic, semantic, and syntactic factors in the perception of everyday environmental sounds. The procedure can be summarized as consisting of a number of steps: Segregation of sonic objects; Horizontal integration and/or segregation; Vertical integration and/or segregation; Assimilation and meaning.",
        "zenodo_id": 1415656,
        "dblp_key": "conf/ismir/Hirst04",
        "keywords": [
            "acousmatic music",
            "analysis procedure",
            "top-down",
            "bottom-up",
            "cognitive psychological views",
            "primitive auditory scene analysis",
            "acoustic",
            "semantic",
            "syntactic factors",
            "perception of everyday environmental sounds"
        ]
    },
    {
        "title": "MusicAustralia: towards a national music information infrastructure.",
        "author": [
            "Robyn Holmes",
            "Marie-Louise Ayres"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418097",
        "url": "https://doi.org/10.5281/zenodo.1418097",
        "ee": "https://zenodo.org/records/1418097/files/HolmesA04.pdf",
        "abstract": "MusicAustralia is a national music discovery service, developed by the National Library of Australia and ScreenSound Australia, National Film and Sound Archive. The service aims to provide seamless access to music and music information resources, in multiple formats, from custodians across all cultural sectors. This paper describes the development of the service, including its architecture, and content base. Service development to date has concentrated on metadata contribution and discovery strategies, together with development of the national digital music collection. In the future, digital content developed to populate the service could be subjected to Music Information Retrieval applications, to further enrich understanding of Australian music. The paper finishes by examining the challenges of achieving these advanced services in an environment where MIR research is relatively undeveloped.",
        "zenodo_id": 1418097,
        "dblp_key": "conf/ismir/HolmesA04",
        "keywords": [
            "MusicAustralia",
            "National Library of Australia",
            "ScreenSound Australia",
            "National Film and Sound Archive",
            "seamless access",
            "music and music information resources",
            "multiple formats",
            "custodians across all cultural sectors",
            "metadata contribution",
            "discovery strategies"
        ]
    },
    {
        "title": "Finding Approximate Repeating Patterns from Sequence Data.",
        "author": [
            "Jia-Lien Hsu",
            "Arbee L. P. Chen",
            "Hung-Chen Chen"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415530",
        "url": "https://doi.org/10.5281/zenodo.1415530",
        "ee": "https://zenodo.org/records/1415530/files/HsuCC04.pdf",
        "abstract": "In this paper, an application of feature extraction from music data is first introduced to motivate our research of finding approximate repeating patterns from sequence data. An approximate repeating pattern is defined as a sequence of symbols which appears more than once under certain approximation types in a data sequence. By using the \u2018cut\u2019 and \u2018pattern_join\u2019 operators, we develop a level-wise approach to solve the problem of finding approximate repeating patterns.",
        "zenodo_id": 1415530,
        "dblp_key": "conf/ismir/HsuCC04",
        "keywords": [
            "feature extraction",
            "music data",
            "approximate repeating patterns",
            "sequence data",
            "cut operator",
            "pattern_join operator",
            "level-wise approach",
            "approximation types",
            "data sequence",
            "approximate repeating pattern"
        ]
    },
    {
        "title": "Comparison Of Features For DP-Matching Based Query-by-Humming System.",
        "author": [
            "Akinori Ito",
            "Sung-Phil Heo",
            "Motoyuki Suzuki",
            "Shozo Makino"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416178",
        "url": "https://doi.org/10.5281/zenodo.1416178",
        "ee": "https://zenodo.org/records/1416178/files/ItoHSM04.pdf",
        "abstract": "In this paper, we compared three kinds of similarity measures for DP-matching based query-by-humming music retrieval experiments. First, a DP matching-based algorithm is formulated using the similarity between a deltaPitch of an input humming and that of a song in the database. Then the three similarities are introduced: distance-based similarity, quantization-based similarity and fuzzy quantization-based similarity. The three similarities are compared by experiments. From the experimental results, the distance-based one gave the best recall rate. In addition, we examined the combination of distance-based and fuzzy-quantization-based similarities. The experimental result showed that the recall rate was improved by the combination.",
        "zenodo_id": 1416178,
        "dblp_key": "conf/ismir/ItoHSM04",
        "keywords": [
            "DP matching",
            "query-by-humming",
            "music retrieval",
            "deltaPitch",
            "database",
            "similarity measures",
            "recall rate",
            "distance-based similarity",
            "quantization-based similarity",
            "fuzzy quantization-based similarity"
        ]
    },
    {
        "title": "Perceptual Segment Clustering For Music Description And Time-axis Redundancy Cancellation.",
        "author": [
            "Tristan Jehan"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416854",
        "url": "https://doi.org/10.5281/zenodo.1416854",
        "ee": "https://zenodo.org/records/1416854/files/Jehan04.pdf",
        "abstract": "Repeating sounds and patterns are widely exploited throughout music. However, although analysis and mu- sic information retrieval applications are often concerned with processing speed and music description, they typi- cally discard the benefits of sound redundancy cancella- tion. We propose a perceptually grounded model for de- scribing music as a sequence of labeled sound segments, for reducing data complexity, and for compressing audio.",
        "zenodo_id": 1416854,
        "dblp_key": "conf/ismir/Jehan04",
        "keywords": [
            "repeating sounds",
            "patterns",
            "music analysis",
            "music information retrieval",
            "processing speed",
            "music description",
            "sound redundancy cancellation",
            "perceptually grounded model",
            "sound segments",
            "data complexity"
        ]
    },
    {
        "title": "Organizing digital music for use: an examination of personal music collections.",
        "author": [
            "Steve Jones 0002",
            "Sally Jo Cunningham",
            "Matt Jones 0001"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416298",
        "url": "https://doi.org/10.5281/zenodo.1416298",
        "ee": "https://zenodo.org/records/1416298/files/JonesCJ04.pdf",
        "abstract": "Current research on music information retrieval and music digital libraries focuses on providing access to huge, public music collections. In this paper we consider a different, but related, problem:  supporting an individual in maintaining and using a personal music collection. We analyze organization and access techniques used to manage personal music collections (primarily CDs and MP3 files), and from these behaviors, to suggest user behaviors that should be supported in a personal music digital library (that is, a digital library of an individual\u2019s personal music collection).",
        "zenodo_id": 1416298,
        "dblp_key": "conf/ismir/JonesCJ04",
        "keywords": [
            "music information retrieval",
            "personal music collection",
            "digital libraries",
            "access techniques",
            "organizing",
            "supporting users",
            "individual behavior",
            "public music collections",
            "MP3 files",
            "CDs"
        ]
    },
    {
        "title": "MusicBLAST - Gapped Sequence Alignment for MIR.",
        "author": [
            "J\u00fcrgen Kilian",
            "Holger H. Hoos"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416984",
        "url": "https://doi.org/10.5281/zenodo.1416984",
        "ee": "https://zenodo.org/records/1416984/files/KilianH04.pdf",
        "abstract": "We propose an algorithm, MusicBLAST, for approximate pattern search/matching on symbolic musical data. Mu- sicBLAST is based on the BLAST algorithm, one of the most commonly used algorithms for similarity search on biological sequence data [1, 2]. MusicBLAST can be used in combination with an arbitrary similarity measure (e.g., melodic, rhythmic or combined) and retrieves multiple oc- currences of a given search pattern and its variations. Dif- ferent from many other pattern matching techniques, it can find incomplete and imperfect occurrences of a given pat- tern, and produces a significance measure for the accuracy and quality of its results. Like BLAST \u2014 and different from many musical pattern matching approaches \u2014 Mu- sicBLAST retrieves heuristically optimised bi-directional alignments searching iteratively in forward and backward direction by starting at a dedicated seed note position of a performance. Keywords: Similarity, pattern matching, retrieval.",
        "zenodo_id": 1416984,
        "dblp_key": "conf/ismir/KilianH04",
        "keywords": [
            "MusicBLAST",
            "approximate pattern search/matching",
            "symbolic musical data",
            "BLAST algorithm",
            "similarity search",
            "arbitrary similarity measure",
            "multiple occurrences",
            "incomplete and imperfect occurrences",
            "significance measure",
            "heuristically optimised bi-directional alignments"
        ]
    },
    {
        "title": "Artist Classification with Web-Based Data.",
        "author": [
            "Peter Knees",
            "Elias Pampalk",
            "Gerhard Widmer"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417189",
        "url": "https://doi.org/10.5281/zenodo.1417189",
        "ee": "https://zenodo.org/records/1417189/files/KneesPW04.pdf",
        "abstract": "Manifold approaches exist for organization of music by genre and/or style. In this paper we propose the use of text categorization techniques to classify artists present on the Internet. In particular, we retrieve and analyze webpages ranked by search engines to describe artists in terms of word occurrences on related pages. To classify artists we primarily use support vector machines. We present 3 experiments in which we address the fol- lowing issues. First, we study the performance of our ap- proach compared to previous work. Second, we investi- gate how daily fluctuations in the Internet affect our ap- proach. Third, on a set of 224 artists from 14 genres we study (a) how many artists are necessary to define the con- cept of a genre, (b) which search engines perform best, (c) how to formulate search queries best, (d) which overall performance we can expect for classification, and finally (e) how our approach is suited as a similarity measure for artists. Keywords: genre classification, community metadata, cul- tural features",
        "zenodo_id": 1417189,
        "dblp_key": "conf/ismir/KneesPW04",
        "keywords": [
            "genre classification",
            "community metadata",
            "cultural features",
            "support vector machines",
            "webpages",
            "search engines",
            "artists",
            "support vector machines",
            "experiments",
            "genres"
        ]
    },
    {
        "title": "Sound, Music and Textual Associations on the World Wide Web.",
        "author": [
            "Ian Knopke"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416144",
        "url": "https://doi.org/10.5281/zenodo.1416144",
        "ee": "https://zenodo.org/records/1416144/files/Knopke04.pdf",
        "abstract": "Sound files on the World Wide Web are accessed from web pages. To date, this relationship has not been ex- plored extensively in the MIR literature. This paper details a series of experiments designed to measure the similar- ity between the public text visible on a web page and the linked sound files, the name of which is normally unseen by the user. A collection of web pages was retrieved from the web using a specially-constructed crawler. Sound file information and associated text were parsed from the pages and analyzed for similarity using common IR techniques such as TFIDF cosine measures. The results are intended to be used in the improvement of a web crawler for audio and music, as well as for MIR purposes in general.",
        "zenodo_id": 1416144,
        "dblp_key": "conf/ismir/Knopke04",
        "keywords": [
            "Sound files",
            "Web pages",
            "Similarity measures",
            "TFIDF cosine",
            "Crawler",
            "MIR literature",
            "IR techniques",
            "Public text",
            "Audio and music",
            "Web access"
        ]
    },
    {
        "title": "Melodic Atoms for Transcribing Carnatic Music.",
        "author": [
            "Arvindh Krishnaswamy"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417859",
        "url": "https://doi.org/10.5281/zenodo.1417859",
        "ee": "https://zenodo.org/records/1417859/files/Krishnaswamy04.pdf",
        "abstract": "We had introduced a set of 2D melodic units to transcribe Carnatic music previously, and we now provide some il- lustrative examples using real pitch tracks to further our discussion on this topic.",
        "zenodo_id": 1417859,
        "dblp_key": "conf/ismir/Krishnaswamy04",
        "keywords": [ 
            "Carnatic music", 
            "Melodic atoms", 
            "Pitch tracks", 
            "Transcription", 
            "2D melodic units", 
            "Musical entities", 
            "Interval tuning", 
            "Intonation", 
            "Melodic units extraction", 
            "Ragams classification" 
        ]
    },
    {
        "title": "CsoundXML: a meta-language in XML for sound synthesis.",
        "author": [
            "Pedro Kr\u00f6ger"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415652",
        "url": "https://doi.org/10.5281/zenodo.1415652",
        "ee": "https://zenodo.org/records/1415652/files/Kroger04.pdf",
        "abstract": "The software sound synthesis is closely related to the Mu- sic N programs started with Music I in 1957. Although Music N has many advantages such as unit generators and a flexible score language, it presents a few problems like limitations on instrument reuse, inflexibility of use of pa- rameters, lack of a built-in graphical interface, and usually only one paradigm for scores. Some solutions concen- trate in new from-scratch Music N implementations, while others focus in building user tools like pre-processors and graphical utilities. Nevertheless, new implementations in general focus in specific groups of problems leaving oth- ers unsolved. The user tools solve only one problem with no connection with others. In this paper we investigate the problem of creating a meta-language for sound synthe- sis. This constitutes an elegant solution for the above cited problems, without the need of a yet new acoustic compiler implementation, allowing a tight integration which is dif- ficult to obtain with the present user tools.",
        "zenodo_id": 1415652,
        "dblp_key": "conf/ismir/Kroger04",
        "keywords": [
            "Music N",
            "unit generators",
            "flexible score language",
            "limitations on instrument reuse",
            "inflexibility of parameter use",
            "built-in graphical interface",
            "single paradigm for scores",
            "new from-scratch Music N implementations",
            "user tools",
            "pre-processors and graphical utilities"
        ]
    },
    {
        "title": "A Prototypical Service for Real-Time Access to Local Context-Based Music Information.",
        "author": [
            "Frank Kurth",
            "Meinard M\u00fcller",
            "Andreas Ribbrock",
            "Tido R\u00f6der",
            "David Damm",
            "Christian Fremerey"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1414926",
        "url": "https://doi.org/10.5281/zenodo.1414926",
        "ee": "https://zenodo.org/records/1414926/files/KurthMRRDF04.pdf",
        "abstract": "In this contribution we propose a generic service for real- time access to context-based music information such as lyrics or score data. In our web-based client-server sce- nario, a client application plays back a particular (wave- form) audio recording. During playback, the client con- nects to a server which in turn identifies the particular piece of audio as well as the current playback position. Subsequently, the server delivers local, i.e., position spe- cific, context-based information on the audio piece to the client. The client then synchronously displays the received information during acoustic playback. We demonstrate how such a service can be established using recent MIR (Music Information Retrieval) techniques such as audio identification and synchronization and present two partic- ular application scenarios. Keywords: Music services, context-based information, fingerprinting, synchronization.",
        "zenodo_id": 1414926,
        "dblp_key": "conf/ismir/KurthMRRDF04",
        "keywords": [
            "Music services",
            "context-based information",
            "fingerprinting",
            "synchronization",
            "audio identification",
            "Music Information Retrieval (MIR)",
            "web-based client-server scenario",
            "real-time access",
            "client application",
            "server"
        ]
    },
    {
        "title": "Expressive Notation Package - an Overview.",
        "author": [
            "Mika Kuuskankare",
            "Mikael Laurson"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415084",
        "url": "https://doi.org/10.5281/zenodo.1415084",
        "ee": "https://zenodo.org/records/1415084/files/KuuskankareL04.pdf",
        "abstract": "The purpose of this paper is to give the reader a concise overview of Expressive Notation Package 2.0 (hencefor- ward ENP). ENP is music notation program that belongs to a family of music and sound related software packages developed at Sibelius Academy in Finland. ENP has been used in various research projects during the past several years.",
        "zenodo_id": 1415084,
        "dblp_key": "conf/ismir/KuuskankareL04",
        "keywords": [ 
            "Expressive Notation Package", 
            "Music notation program", 
            "Computer-aided composition", 
            "Music analysis", 
            "Virtual instrument control", 
            "Western musical notation", 
            "Typesetting", 
            "LispWorks", 
            "OpenGL", 
            "User interface" 
        ]
    },
    {
        "title": "A multi-parametric and redundancy-filtering approach to pattern identification.",
        "author": [
            "Olivier Lartillot"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416426",
        "url": "https://doi.org/10.5281/zenodo.1416426",
        "ee": "https://zenodo.org/records/1416426/files/Lartillot04.pdf",
        "abstract": "This paper presents the principles of a new approach aimed at automatically discovering motivic patterns in monodies. It is shown that, for the results to agree with the listener\u2019s understanding, computer modelling needs to follow as closely as possible the strategies undertaken during the listening process. Motivic patterns, which may progressively follow different musical dimensions, are discovered through an adaptive incremental identification in a multi-dimensional parametric space. The combinatorial redundancy that would logically result from the model is carefully limited with the help of particular heuristics. In particular, a notion of specificity relation between pattern descriptions is defined, unifying suffix relation \u2013 between patterns \u2013 and inclusion relation \u2013 between the multi-parametric descriptions of patterns. This enables to discard redundant patterns, whose descriptions are less specific than other patterns and whose occurrences are included in the occurrences of the more specific patterns. Resulting analyzes come close to the structures actually perceived by the listener. Keywords: motivic analysis, pattern discovery, melodic identification, redundancy filtering, music cognition.",
        "zenodo_id": 1416426,
        "dblp_key": "conf/ismir/Lartillot04",
        "keywords": [
            "motivic patterns",
            "monodies",
            "computer modelling",
            "adaptive incremental identification",
            "multi-dimensional parametric space",
            "specificity relation",
            "combinatorial redundancy",
            "melodic identification",
            "music cognition",
            "listeners understanding"
        ]
    },
    {
        "title": "Methodological Considerations Concerning Manual Annotation Of Musical Audio In Function Of Algorithm Development.",
        "author": [
            "Micheline Lesaffre",
            "Marc Leman",
            "Bernard De Baets",
            "Jean-Pierre Martens"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1414874",
        "url": "https://doi.org/10.5281/zenodo.1414874",
        "ee": "https://zenodo.org/records/1414874/files/LesaffreLBM04.pdf",
        "abstract": "In research on musical audio-mining, annotated music databases are needed which allow the development of computational tools that extract from the musical audio- stream the kind of high-level content that users can deal with in Music Information Retrieval (MIR) contexts. The notion of musical content, and therefore the notion of annotation, is ill-defined, however, both in the syntactic and semantic sense. As a consequence, annotation has been approached from a variety of perspectives (but mainly linguistic-symbolic oriented), and a general methodology is lacking. This paper is a step towards the definition of a general framework for manual annotation of musical audio in function of a computational approach to musical audio-mining that is based on algorithms that learn from annotated data.",
        "zenodo_id": 1414874,
        "dblp_key": "conf/ismir/LesaffreLBM04",
        "keywords": [
            "annotated music databases",
            "computational tools",
            "Music Information Retrieval (MIR)",
            "high-level content",
            "musical audio-stream",
            "musical content",
            "semantic sense",
            "syntactic sense",
            "general framework",
            "manual annotation"
        ]
    },
    {
        "title": "Improving Melody Classification by Discriminant Feature Extraction and Fusion.",
        "author": [
            "Ming Li",
            "Ronan Sleep"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416728",
        "url": "https://doi.org/10.5281/zenodo.1416728",
        "ee": "https://zenodo.org/records/1416728/files/LiS04.pdf",
        "abstract": "We propose a general approach to discriminant feature extraction and fusion, built on an optimal feature transformation for discriminant analysis [6]. Our experiments indicate that our approach can dramatically reduce the dimensionality of original feature space whilst improving its discriminant power. Our feature fusion method can be carried out in the reduced lower- dimensional subspace, resulting in a further improvement in accuracy. Our experiments concern the classification of music styles based only on the pitch sequence derived from monophonic melodies.",
        "zenodo_id": 1416728,
        "dblp_key": "conf/ismir/LiS04",
        "keywords": [
            "discriminant feature extraction",
            "discriminant analysis",
            "optimal feature transformation",
            "dimensionality reduction",
            "discriminant power",
            "feature fusion method",
            "reduced lower-dimensional subspace",
            "accuracy improvement",
            "classification of music styles",
            "pitch sequence"
        ]
    },
    {
        "title": "Music Recommendation from Song Sets.",
        "author": [
            "Beth Logan"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416658",
        "url": "https://doi.org/10.5281/zenodo.1416658",
        "ee": "https://zenodo.org/records/1416658/files/Logan04.pdf",
        "abstract": "We motivate the problem of music recommendation based solely on acoustics from groups of related songs or \u2018song sets\u2019. We propose four solutions which can be used with any acoustic-based similarity measure. The first builds a model for each song set and recommends new songs according to their distance from this model. The next three approaches recommend songs according to the av- erage, median and minimum distance to songs in the song set. For a similarity measure based on K-means models of MFCC features, experiments on a database of 18647 songs indicated that the minimum distance technique is the most effective, returning a valid recommendation as one of the top 5 32.5% of the time. The approach based on the median distance was the next best, returning a valid recommendation as one of the top 5 29.5% of the time.",
        "zenodo_id": 1416658,
        "dblp_key": "conf/ismir/Logan04",
        "keywords": [
            "music recommendation",
            "acoustics",
            "song sets",
            "acoustic-based similarity",
            "four solutions",
            "K-means models",
            "MFCC features",
            "experiments",
            "database",
            "18647 songs"
        ]
    },
    {
        "title": "Timbre Classification Of A Single Musical Instrument.",
        "author": [
            "Mauricio Alves Loureiro",
            "Hugo Bastos de Paula",
            "Hani C. Yehia"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416320",
        "url": "https://doi.org/10.5281/zenodo.1416320",
        "ee": "https://zenodo.org/records/1416320/files/LoureiroPY04.pdf",
        "abstract": "In order to map the spectral characteristics of the large variety of sounds a musical instrument may produce, different notes were performed and sampled in several intensity levels across the whole extension of a clarinet. Amplitude and frequency time-varying curves of partials were measured by Discrete Fourier Transform. A limited set of orthogonal spectral bases was derived by Principal Component Analysis techniques. These bases defined spectral sub-spaces capable of representing all tested sounds and of grouping them according to the distance metrics of the representation. A clustering algorithm was used to infer timbre classes. Preliminary tests with resynthesized sounds with normalized pitch showed a strong relation between the perceived timbre and the cluster label to which the notes were assigned. Self-Organizing Maps lead to results similar to those obtained by PCA representation and K- means clustering algorithm.",
        "zenodo_id": 1416320,
        "dblp_key": "conf/ismir/LoureiroPY04",
        "keywords": [
            "Discrete Fourier Transform",
            "Principal Component Analysis",
            "timbre classes",
            "pitch",
            "resynthesized sounds",
            "self-Organizing Maps",
            "distance metrics",
            "sound",
            "notes",
            "sound"
        ]
    },
    {
        "title": "Pattern Matching in Polyphonic Music as a Weighted Geometric Translation Problem.",
        "author": [
            "Anna Lubiw",
            "Luke Tanur"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417969",
        "url": "https://doi.org/10.5281/zenodo.1417969",
        "ee": "https://zenodo.org/records/1417969/files/LubiwT04.pdf",
        "abstract": "We consider the music pattern matching problem\u2014to find occurrences of a small fragment of music called the \u201cpat- tern\u201d in a larger body of music called the \u201cscore\u201d\u2014as a problem of translating a set of horizontal line segments in the plane to find the best match in a larger set of horizontal line segments. Our contribution is that we use fairly gen- eral weight functions to measure the quality of a match, thus enabling approximate pattern matching. We give an algorithm with running time O(nm log m), where n is the size of the score and m is the size of the pattern. We show that the problem, in this geometric formulation, is unlikely to have a significantly faster algorithm because it is at least as hard as a basic problem called 3-SUM that is conjec- tured to have no subquadratic algorithm. We present some examples to show the potential of this method for finding minor variations of a theme, and for finding polyphonic musical patterns in a polyphonic score.",
        "zenodo_id": 1417969,
        "dblp_key": "conf/ismir/LubiwT04",
        "keywords": [
            "music pattern matching",
            "finding occurrences",
            "small fragment",
            "larger body",
            "horizontal line segments",
            "quality of a match",
            "algorithm",
            "running time",
            "basic problem",
            "polyphonic musical patterns"
        ]
    },
    {
        "title": "Gaussian Mixture Models For Extraction Of Melodic Lines From Audio Recordings.",
        "author": [
            "Matija Marolt"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416576",
        "url": "https://doi.org/10.5281/zenodo.1416576",
        "ee": "https://zenodo.org/records/1416576/files/Marolt04.pdf",
        "abstract": "The presented study deals with extraction of melodic line(s) from polyphonic audio recordings. We base our work on the use of expectation maximization algorithm, which is employed in a two-step procedure that finds melodic lines in audio signals. In the first step, EM is used to find regions in the signal with strong and stable pitch (melodic fragments). In the second step, these fragments are grouped into clusters according to their properties (pitch, loudness...). The obtained clusters represent distinct melodic lines. Gaussian Mixture Models, trained with EM are used for clustering. The paper presents the entire process in more detail and gives some initial results.",
        "zenodo_id": 1416576,
        "dblp_key": "conf/ismir/Marolt04",
        "keywords": [
            "expectation-maximization algorithm",
            "two-step procedure",
            "melodic fragments",
            "pitch",
            "loudness",
            "Gaussian Mixture Models",
            "clustering",
            "distinct melodic lines",
            "audio signals",
            "polyphonic audio recordings"
        ]
    },
    {
        "title": "Automatic Genre Classification Using Large High-Level Musical Feature Sets.",
        "author": [
            "Cory McKay",
            "Ichiro Fujinaga"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416158",
        "url": "https://doi.org/10.5281/zenodo.1416158",
        "ee": "https://zenodo.org/records/1416158/files/McKayF04.pdf",
        "abstract": "This paper presents a system that extracts 109 musical features from symbolic recordings (MIDI, in this case) and uses them to classify the recordings by genre. The features used here are based on instrumentation, texture, rhythm, dynamics, pitch statistics, melody and chords. The classification is performed hierarchically using different sets of features at different levels of the hierarchy. Which features are used at each level, and their relative weightings, are determined using genetic algorithms. Classification is performed using a novel ensemble of feedforward neural networks and k-nearest neighbour classifiers. Arguments are presented emphasizing the importance of using high-level musical features, something that has been largely neglected in automatic classification systems to date in favour of low-level features. The effect on classification performance of varying the number of candidate features is examined in order to empirically demonstrate the importance of using a large variety of musically meaningful features. Two differently sized hierarchies are used in order to test the performance of the system under different conditions. Very encouraging classification success rates of 98% for root genres and 90% for leaf genres are obtained for a hierarchical taxonomy consisting of 9 leaf genres.",
        "zenodo_id": 1416158,
        "dblp_key": "conf/ismir/McKayF04",
        "keywords": [
            "MIDI",
            "genres",
            "symbolic recordings",
            "classification",
            "musical features",
            "genetic algorithms",
            "feedforward neural networks",
            "k-nearest neighbour classifiers",
            "high-level musical features",
            "low-level features"
        ]
    },
    {
        "title": "Extracting the perceptual tempo from music.",
        "author": [
            "Martin F. McKinney",
            "Dirk Moelants"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415146",
        "url": "https://doi.org/10.5281/zenodo.1415146",
        "ee": "https://zenodo.org/records/1415146/files/McKinneyM04.pdf",
        "abstract": "The study presented here outlines a procedure for mea- suring and quantitatively representing the perceptual tempo of a musical excerpt. We also present a method for apply- ing such measures of perceptual tempo to the design of automatic tempo-trackers in order to more accurately rep- resent the perceived beat in music. Keywords: Tempo, Perception, Beat-tracking",
        "zenodo_id": 1415146,
        "dblp_key": "conf/ismir/McKinneyM04",
        "keywords": [
            "perceptual tempo",
            "musical excerpt",
            "measuring",
            "quantitative representation",
            "beat-tracking",
            "automatic tempo-trackers",
            "design",
            "representation",
            "perceived beat",
            "keywords"
        ]
    },
    {
        "title": "Feature Weighting for Segmentation.",
        "author": [
            "R. Mitchell",
            "Irfan A. Essa"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1414976",
        "url": "https://doi.org/10.5281/zenodo.1414976",
        "ee": "https://zenodo.org/records/1414976/files/MitchellE04.pdf",
        "abstract": "This paper proposes the use of feature weights to reveal the hierarchical nature of music audio. Feature weighting has been exploited in machine learning, but has not been applied to music audio segmentation. We describe both a global and a local approach to automatic feature weighting. The global approach assigns a single weighting to all features in a song. The local approach uses the local separability directly. Both approaches reveal structure that is obscured by standard features, and emphasize segments of a particular size.",
        "zenodo_id": 1414976,
        "dblp_key": "conf/ismir/MitchellE04",
        "keywords": [
            "feature weights",
            "hierarchical nature",
            "machine learning",
            "music audio segmentation",
            "automatic feature weighting",
            "global approach",
            "local approach",
            "standard features",
            "structure",
            "segments"
        ]
    },
    {
        "title": "Optimizing Measures Of Melodic Similarity For The Exploration Of A Large Folk Song Database.",
        "author": [
            "Daniel M\u00fcllensiefen",
            "Klaus Frieler"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418031",
        "url": "https://doi.org/10.5281/zenodo.1418031",
        "ee": "https://zenodo.org/records/1418031/files/MullensiefenF04.pdf",
        "abstract": "This investigation aims at finding an optimal way of measuring the similarity of melodies. The applicability for an automated analysis and classification was tested on a folk song collection from Luxembourg that had been thoroughly analysed by an expert ethnomusicologist. Firstly a systematization of the currently available approaches to similarity measurements of melodies was done. About 50 similarity measures were implemented which differ in the way of transforming musical data and in the computational algorithms. Three listener experiments were conducted to compare the performance of the different measures to human experts\u2019 ratings. Then an optimized model was obtained by using linear regression, which combines the output of several measures representing different musical dimensions. The performance of this optimized measure was compared with the classification work of a human ethnomusicologist on a collection of 577 Luxembourg folksongs.",
        "zenodo_id": 1418031,
        "dblp_key": "conf/ismir/MullensiefenF04",
        "keywords": [
            "investigation",
            "measuring",
            "melodies",
            "automated analysis",
            "classification",
            "expert ethnomusicologist",
            "systematization",
            "similarity measures",
            "listener experiments",
            "optimized model"
        ]
    },
    {
        "title": "Towards an Efficient Algorithm for Automatic Score-to-Audio Synchronization.",
        "author": [
            "Meinard M\u00fcller",
            "Frank Kurth",
            "Tido R\u00f6der"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416302",
        "url": "https://doi.org/10.5281/zenodo.1416302",
        "ee": "https://zenodo.org/records/1416302/files/MullerKR04.pdf",
        "abstract": "In the last few years, several algorithms for the automatic alignment of audio and score data corresponding to the same piece of music have been proposed. Among the ma- jor drawbacks to these approaches are the long running times as well as the large memory requirements. In this paper we present an algorithm, which solves the synchro- nization problem accurately and efficiently for complex, polyphonic piano music. In a first step, we extract from the audio data stream a set of highly expressive features encoding note onset candidates separately for all pitches. This makes computations efficient since only a small num- ber of such features is sufficient to solve the synchroniza- tion task. Based on a suitable matching model, the best match between the score and the feature parameters is computed by dynamic programming (DP). To further cut down the computational cost in the synchronization pro- cess, we introduce the concept of anchor matches, matches which can be easily established. Then the DP-based tech- nique is locally applied between adjacent anchor matches. Evaluation results have been obtained on complex poly- phonic piano pieces including Chopin\u2019s Etudes Op. 10.",
        "zenodo_id": 1416302,
        "dblp_key": "conf/ismir/MullerKR04",
        "keywords": [
            "audio and score data alignment",
            "polyphonic piano music",
            "highly expressive features",
            "note onset candidates",
            "dynamic programming",
            "anchor matches",
            "complex polyphonic piano pieces",
            "Chopin\u2019s Etudes Op. 10",
            "evaluation results",
            "computational cost reduction"
        ]
    },
    {
        "title": "A Drum Pattern Retrieval Method by Voice Percussion.",
        "author": [
            "Tomoyasu Nakano",
            "Jun Ogata",
            "Masataka Goto",
            "Yuzuru Hiraga"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417569",
        "url": "https://doi.org/10.5281/zenodo.1417569",
        "ee": "https://zenodo.org/records/1417569/files/NakanoOGH04.pdf",
        "abstract": "This paper presents a method for voice percussion recog- nition and its application to drum pattern retrieval. Recog- nition of voice percussion (verbalized expression of drum sound by voice) requires an approach that is different from existing methods. Individual differences in both vocal characteristics and the kinds of verbal expressions used add further complication to the task. The approach taken in this study uses onomatopoeia as internal representation of drum sounds, and combines the recognition of voice percussion with the retrieval of intended drum patterns. This scheme is intended to deal with the two types of in- dividual differences mentioned above. In a recognition experiment with 200 utterances of voice percussion, our method achieved a recognition rate of 91.0% for the highest- tuned setting. keywords: voice percussion recognition, drum pattern retrieval, onomatopoeia representation",
        "zenodo_id": 1417569,
        "dblp_key": "conf/ismir/NakanoOGH04",
        "keywords": [
            "voice percussion recognition",
            "drum pattern retrieval",
            "onomatopoeia representation",
            "individual differences",
            "recognition rate",
            "utterances",
            "highest-tuned setting",
            "experiment",
            "approach",
            "task"
        ]
    },
    {
        "title": "Towards Automatic Transcription of Australian Aboriginal Music.",
        "author": [
            "Andrew Nesbit",
            "Lloyd C. L. Hollenberg",
            "Anthony Senyard"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416874",
        "url": "https://doi.org/10.5281/zenodo.1416874",
        "ee": "https://zenodo.org/records/1416874/files/NesbitHS04.pdf",
        "abstract": "We describe a system designed for automatic extraction and segmentation of didjeridu and clapsticks from cer- tain styles of traditional Aboriginal Australian music. For didjeridu, we locate the start of notes using a complex- domain note onset detection algorithm, and use the de- tected onsets as cues for determining the harmonic series of sinusoids belonging to the didjeridu. The harmonic se- ries is hypothesised, based on prior knowledge of the fun- damental frequency of the didjeridu, and the most likely hypothesis is assumed. For clapsticks, we use indepen- dent subspace analysis to split the signal into harmonic and percussive components, followed by classification of the independent components. Finally, we identify areas in which the system can be enhanced to improve accuracy and also to extract a wider range of musically-relevant features. These include algo- rithms such as high frequency content techniques, and also computing the morphology of the didjeridu.",
        "zenodo_id": 1416874,
        "dblp_key": "conf/ismir/NesbitHS04",
        "keywords": [
            "Automatic extraction",
            "didjeridu detection",
            "clapsticks segmentation",
            "traditional Aboriginal music",
            "note onset detection",
            "harmonic series",
            "independent subspace analysis",
            "classification",
            "musically-relevant features",
            "enhancement algorithms"
        ]
    },
    {
        "title": "Indexing and Retrieval of Music Documents through Pattern Analysis and Data Fusion Techniques.",
        "author": [
            "Giovanna Neve",
            "Nicola Orio"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416372",
        "url": "https://doi.org/10.5281/zenodo.1416372",
        "ee": "https://zenodo.org/records/1416372/files/NeveO04.pdf",
        "abstract": "One of the challenges of music information retrieval is the automatic extraction of effective content descriptors of music documents, which can be used at indexing and at retrieval time to match queries with documents. In this pa- per it is proposed to index music documents with frequent musical patterns. A musical pattern is a sequence of fea- tures in the score that is repeated at least twice: features can regard perceptually relevant characteristics, such as rhythm, pitch, or both. Data fusion techniques are applied to merge the results obtained using different features. A set of experimental tests has been carried out on retrieval effectiveness, robustness to query errors, and dependency on query length on a collection of Beatles\u2019 songs using a set of queries. The proposed approach gave good results, both using single features and, in particular, merging the rank lists obtained by different features with a data fusion approach.",
        "zenodo_id": 1416372,
        "dblp_key": "conf/ismir/NeveO04",
        "keywords": [
            "challenge",
            "automatic extraction",
            "content descriptors",
            "music documents",
            "indexing",
            "retrieval time",
            "queries",
            "musical patterns",
            "sequence of features",
            "frequent patterns"
        ]
    },
    {
        "title": "Automatic Detection Of Vocal Segments In Popular Songs.",
        "author": [
            "Tin Lay Nwe",
            "Ye Wang 0007"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417846",
        "url": "https://doi.org/10.5281/zenodo.1417846",
        "ee": "https://zenodo.org/records/1417846/files/NweW04.pdf",
        "abstract": "This paper presents a technique for the automatic classification of vocal and non-vocal regions in an acoustic musical signal. The proposed technique uses acoustic features which are suitable to distinguish vocal and non-vocal signals. We employ the Hidden Markov Model (HMM) classifier for vocal and non-vocal classification. In contrast to conventional HMM training methods which employ one model for each class, we create an HMM model space (multi-model HMMs) for segmentation with improved accuracy. In addition, we employ an automatic bootstrapping process which adapts the test song\u2019s own models for better classification accuracy. Experimental evaluations conducted on a database of 20 popular music songs show the validity of the proposed approach.",
        "zenodo_id": 1417846,
        "dblp_key": "conf/ismir/NweW04",
        "keywords": [
            "Automatic",
            "classification",
            "vocal",
            "regions",
            "acoustic",
            "musical",
            "signal",
            "Hidden",
            "Markov",
            "Model"
        ]
    },
    {
        "title": "Automatic extraction of music descriptors from acoustic signals.",
        "author": [
            "Fran\u00e7ois Pachet",
            "Aymeric Zils"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416106",
        "url": "https://doi.org/10.5281/zenodo.1416106",
        "ee": "https://zenodo.org/records/1416106/files/PachetZ04.pdf",
        "abstract": "High-Level music descriptors are key ingredients for music information retrieval systems. Although there is a long tradition in extracting information from acoustic signals, the field of music information extraction is largely heuristic in nature. We present here a heuristic-based generic approach for extracting automatically high-level music descriptors from acoustic signals. This approach is based on Genetic Programming, used to build relevant features as functions of mathematical and signal processing operators. The search of relevant features is guided by specialized heuristics that embody knowledge about the signal processing functions built by the system. Signal processing patterns are used in order to control the general processing methods. In addition, rewriting rules are introduced to simplify overly complex expressions, and a caching system further reduces the computing cost of each cycle. Finally, the features build by the system are combined into an optimized machine learning descriptor model, and an executable program is generated to compute the model on any audio signal. In this paper, we describe the overall system and compare its results against traditional approaches in musical feature extraction \u00e0 la Mpeg7.",
        "zenodo_id": 1416106,
        "dblp_key": "conf/ismir/PachetZ04",
        "keywords": [
            "Genetic Programming",
            "relevant features",
            "acoustic signals",
            "signal processing operators",
            "heuristic-based",
            "machine learning descriptor",
            "caching system",
            "optimized machine learning descriptor model",
            "audio signal",
            "executable program"
        ]
    },
    {
        "title": "A Matlab Toolbox to Compute Music Similarity from Audio.",
        "author": [
            "Elias Pampalk"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418077",
        "url": "https://doi.org/10.5281/zenodo.1418077",
        "ee": "https://zenodo.org/records/1418077/files/Pampalk04.pdf",
        "abstract": "A Matlab toolbox implementing music similarity mea- sures for audio is presented. The implemented measures focus on aspects related to timbre and periodicities in the signal. This paper gives an overview of the implemented functions. In particular, the basics of the similarity mea- sures are reviewed and some visualizations are discussed.",
        "zenodo_id": 1418077,
        "dblp_key": "conf/ismir/Pampalk04",
        "keywords": [
            "Matlab toolbox",
            "music similarity measures",
            "audio",
            "timbre",
            "periodicities",
            "signal",
            "functions",
            "basics",
            "similarity measures",
            "visualizations"
        ]
    },
    {
        "title": "Tempo Tracking with a Single Oscillator.",
        "author": [
            "Bryan Pardo"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415090",
        "url": "https://doi.org/10.5281/zenodo.1415090",
        "ee": "https://zenodo.org/records/1415090/files/Pardo04.pdf",
        "abstract": "I describe a simple on-line tempo tracker, based on phase and period locking a single oscillator to performance event timings. The tracker parameters are optimized on a corpus of solo piano performances by twelve musicians. The tracker is then tested on a second corpus of performances, played by the same twelve musicians. The performance of this tracker is compared to previously published results for a tempo tracker based on combining a tempogram and Kalman filter.",
        "zenodo_id": 1415090,
        "dblp_key": "conf/ismir/Pardo04",
        "keywords": [
            "on-line",
            "tempo",
            "tracker",
            "phase",
            "period",
            "locking",
            "oscillator",
            "performance",
            "event",
            "timings"
        ]
    },
    {
        "title": "Musical key extraction from audio.",
        "author": [
            "Steffen Pauws"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416326",
        "url": "https://doi.org/10.5281/zenodo.1416326",
        "ee": "https://zenodo.org/records/1416326/files/Pauws04.pdf",
        "abstract": "The realisation and evaluation of a musical key extraction algorithm that works directly on raw audio data is pre- sented. Its implementation is based on models of human auditory perception and music cognition. It is straightfor- ward and has minimal computing requirements. First, it computes a chromagram from non-overlapping 100 msecs time frames of audio; a chromagram represents the likeli- hood of the chroma occurrences in the audio. This chro- magram is correlated with Krumhansl\u2019s key profiles that represent the perceived stability of each chroma within the context of a particular musical key. The key profile that has maximum correlation with the computed chroma- gram is taken as the most likely key. An evaluation with 237 CD recordings of classical piano sonatas indicated a classification accuracy of 75.1%. By considering the ex- act, relative, dominant, sub-dominant and parallel keys as similar keys, the accuracy is even 94.1%.",
        "zenodo_id": 1416326,
        "dblp_key": "conf/ismir/Pauws04",
        "keywords": [
            "chromagram",
            "Krumhansls key profiles",
            "classification accuracy",
            "75.1%",
            "evaluation",
            "237 CD recordings",
            "classical piano sonatas",
            "computing requirements",
            "human auditory perception",
            "music cognition"
        ]
    },
    {
        "title": "Industrial audio fingerprinting distributed system with CORBA and Web Services.",
        "author": [
            "Jose Pedro",
            "Vadim Tarasov",
            "Eloi Batlle",
            "Enric Guaus",
            "Jaume Masip"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1414792",
        "url": "https://doi.org/10.5281/zenodo.1414792",
        "ee": "https://zenodo.org/records/1414792/files/PedroTBGM04.pdf",
        "abstract": "With digital technologies, music content providers face serious challenges to protect their rights. Due to the wide- spread nature of music sources, it is very difficult to cen- tralize the audio management. Audio fingerprinting al- lows the identification of audio content regardless of the audio format and without the need of additional metadata. Monitoring the audio being broadcasted by the TV and radio stations of a country requires the design and imple- mentation of a scalable, robust and modular framework. We have chosen CORBA as distributed environment. The whole functionality needs to be decoupled from clients. To do so, Web services have been deployed. The audio identification core uses a Hidden Markov Model-based audio fingerprinting technology. The paper discusses the design and implementation issues of a complete distribut- ing system that automatically monitors audio content, spe- cifically music and commercials.Today, a working proto- type of such a system already exists, and is dedicated to monitoring several radio and tv stations in Spain.",
        "zenodo_id": 1414792,
        "dblp_key": "conf/ismir/PedroTBGM04",
        "keywords": [
            "digital technologies",
            "music content providers",
            "audio management",
            "audio fingerprinting",
            "audio identification",
            "corba",
            "web services",
            "hidden markov model",
            "audio content monitoring",
            "audio commercials"
        ]
    },
    {
        "title": "Clustering Symbolic Music Using Paradigmatic and Surface Level Analyses.",
        "author": [
            "Anna Pienim\u00e4ki",
            "Kjell Lemstr\u00f6m"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417447",
        "url": "https://doi.org/10.5281/zenodo.1417447",
        "ee": "https://zenodo.org/records/1417447/files/PienimakiL04.pdf",
        "abstract": "In this paper, we describe a novel automatic cluster analy- sis method for symbolic music. The method contains both a surface level and a paradigmatic level analysing block and works in two phases. In the first phase, each music document of a collection is analysed separately: They are first divided into phrases that are consequently fed on a harmonic analyser. The paradigmatic structure of a given music document is achieved comparing both the melodic and the harmonic similarities among its phrases. In the second phase, the collection of music documents is clus- tered on the ground of their paradigmatic structures and surface levels. Our experimental results show that the novel method finds some interesting, underlying similari- ties that cannot be found using only surface level analysis.",
        "zenodo_id": 1417447,
        "dblp_key": "conf/ismir/PienimakiL04",
        "keywords": [
            "automatic",
            "symbolic",
            "music",
            "novel",
            "method",
            "surface",
            "paradigmatic",
            "analysis",
            "phrases",
            "harmonic"
        ]
    },
    {
        "title": "Music meter and tempo tracking from raw polyphonic audio.",
        "author": [
            "Aggelos Pikrakis",
            "Iasonas Antonopoulos",
            "Sergios Theodoridis"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416348",
        "url": "https://doi.org/10.5281/zenodo.1416348",
        "ee": "https://zenodo.org/records/1416348/files/PikrakisAT04.pdf",
        "abstract": "This paper presents a method for the extraction of music meter and tempo from raw polyphonic audio recordings, assuming that music meter remains constant throughout the recoding. Although this assumption can be restrictive for certain musical genres, it is acceptable for a large cor- pus of folklore eastern music styles, including Greek tra- ditional dance music. Our approach is based on the self- similarity analysis of the audio recording and does not as- sume the presence of percussive instruments. Its novelty lies in the fact that music meter and tempo are jointly de- termined. The method has been applied to a variety of musical genres, in the context of Greek traditional music where music meter can be 2 4, 3 4, 4 4, 5 4, 7 8, 9 8, 12 8 and tempo ranges from 40bpm to 330bpm. Experiments have, so far, demonstrated the efficiency of our method (music meter and tempo were successfully extracted for over 95% of the recordings). Keywords: music meter tracking, beat tracking, content- based music retrieval",
        "zenodo_id": 1416348,
        "dblp_key": "conf/ismir/PikrakisAT04",
        "keywords": [
            "music meter",
            "tempo",
            "polyphonic audio recordings",
            "self-similarity analysis",
            "percussive instruments",
            "folklore eastern music",
            "Greece traditional dance music",
            "joint determination",
            "music genres",
            "Greek traditional music"
        ]
    },
    {
        "title": "A Hybrid Graphical Model for Aligning Polyphonic Audio with Musical Scores.",
        "author": [
            "Christopher Raphael"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416500",
        "url": "https://doi.org/10.5281/zenodo.1416500",
        "ee": "https://zenodo.org/records/1416500/files/Raphael04.pdf",
        "abstract": "We present a new method for establishing an alignment between a polyphonic musical score and a corresponding sampled audio performance. The method uses a graphi- cal model containing both discrete variables, correspond- ing to score position, as well as a continuous latent tempo process. We use a simple data model based only on the pitch content of the audio signal. The data interpretation is defined to be the most likely configuration of the hidden variables, given the data, and we develop computational methodology for this task using a variant of dynamic pro- gramming involving parametrically represented continu- ous variables. Experiments are presented on a 55-minute hand-marked orchestral test set. Keywords: Polyphonic Score Alignment",
        "zenodo_id": 1416500,
        "dblp_key": "conf/ismir/Raphael04",
        "keywords": [
            "polyphonic musical score",
            "sampled audio performance",
            "graphical model",
            "discrete variables",
            "continuous latent tempo process",
            "data model",
            "pitch content",
            "data interpretation",
            "hidden variables",
            "computational methodology"
        ]
    },
    {
        "title": "Demonstration of &apos;Music Plus One&apos;--- a System for Orchestral Musical Accompanimen.",
        "author": [
            "Christopher Raphael"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417899",
        "url": "https://doi.org/10.5281/zenodo.1417899",
        "ee": "https://zenodo.org/records/1417899/files/Raphael04a.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1417899,
        "dblp_key": "conf/ismir/Raphael04a"
    },
    {
        "title": "Audio Issues In MIR Evaluation.",
        "author": [
            "Josh Reiss",
            "Mark B. Sandler"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415840",
        "url": "https://doi.org/10.5281/zenodo.1415840",
        "ee": "https://zenodo.org/records/1415840/files/ReissS04.pdf",
        "abstract": "Several projects are underway to create music testbeds to suit the needs of the music analysis and music information retrieval (MIR) communities. There are also plans to unify testbeds into a distributed grid. Thus the issue of audio file formats has come to the forefront. The creators of a music library or MIR testbed are confronted with many questions pertaining to file formats, their quality, metadata, and copyright issues. We discuss the various formats, their advantages and disadvantages, and give a set of guidelines and recommendations. This document is a positional paper. It is intended to foster discussion and not as a definitive statement. Nevertheless, it is hoped that the proposals put forth here may serve as a guideline to use in construction of an MIR evaluation testbed.",
        "zenodo_id": 1415840,
        "dblp_key": "conf/ismir/ReissS04",
        "keywords": [
            "music testbeds",
            "music analysis",
            "music information retrieval",
            "audio file formats",
            "distributed grid",
            "metadata",
            "copyright issues",
            "music library",
            "MIR evaluation testbed",
            "guidelines and recommendations"
        ]
    },
    {
        "title": "Drum sound classification in polyphonic audio recordings using localized sound models.",
        "author": [
            "Vegard Sandvold",
            "Fabien Gouyon",
            "Perfecto Herrera"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415808",
        "url": "https://doi.org/10.5281/zenodo.1415808",
        "ee": "https://zenodo.org/records/1415808/files/SandvoldGH04.pdf",
        "abstract": "This paper deals with automatic percussion classification in polyphonic audio recordings, focusing on kick, snare and cymbal sounds. We present a feature-based sound modeling approach that combines general, prior knowl- edge about the sound characteristics of percussion instru- ment families (general models) with on-the-fly acquired knowledge of recording-specific sounds (localized mod- els). This way, high classification accuracy can be ob- tained with remarkably simple sound models. The accu- racy is on average around 20% higher than with general models alone.",
        "zenodo_id": 1415808,
        "dblp_key": "conf/ismir/SandvoldGH04",
        "keywords": [
            "automatic percussion classification",
            "polyphonic audio recordings",
            "kick",
            "snare",
            "cymbal sounds",
            "feature-based sound modeling",
            "general models",
            "localized models",
            "high classification accuracy",
            "simple sound models"
        ]
    },
    {
        "title": "The Anatomy of a Bibliographic Search System for Music.",
        "author": [
            "Ryan Scherle",
            "Donald Byrd"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417789",
        "url": "https://doi.org/10.5281/zenodo.1417789",
        "ee": "https://zenodo.org/records/1417789/files/ScherleB04.pdf",
        "abstract": "Traditional library catalog systems have been effective in providing access to collections of books, films, and other material. However, they have many limitations when it comes to finding musical information, which has signifi- cantly different, and in many ways more complex, struc- ture. The Variations2 search system is an alternative de- signed specifically to aid users in searching for music. It leverages a rich set of bibliographic data records, express- ing relationships between creators of music and their cre- ations. These records enable musicians to search for mu- sic using familiar terms and relationships, rather than try- ing to decipher the methods libraries typically use to or- ganize musical items. This paper describes the design and implementation of the system that makes these searches possible.",
        "zenodo_id": 1417789,
        "dblp_key": "conf/ismir/ScherleB04",
        "keywords": [
            "traditional library catalog systems",
            "limitations of traditional systems",
            "finding musical information",
            "significantly different structure",
            "Variations2 search system",
            "alternative for music searching",
            "leveraging bibliographic data",
            "expressing relationships",
            "musicians searching for music",
            "familiar terms and relationships"
        ]
    },
    {
        "title": "Learning to Align Polyphonic Music.",
        "author": [
            "Shai Shalev-Shwartz",
            "Joseph Keshet",
            "Yoram Singer"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416546",
        "url": "https://doi.org/10.5281/zenodo.1416546",
        "ee": "https://zenodo.org/records/1416546/files/Shalev-ShwartzKS04.pdf",
        "abstract": "We describe an efficient learning algorithm for aligning a symbolic representation of a musical piece with its acous- tic counterpart. Our method employs a supervised learn- ing approach by using a training set of aligned sym- bolic and acoustic representations. The alignment func- tion we devise is based on mapping the input acoustic- symbolic representation along with the target alignment for learning support vector machines (SVM), our align- space which separates correct alignments from incorrect ones. We describe a simple iterative algorithm for learn- ing the alignment function and discuss its formal proper- ties. We use our method for aligning MIDI and MP3 rep- resentations of polyphonic recordings of piano music. We also compare our discriminative approach to a generative method based on a generalization of hidden Markov mod- els. In all of our experiments, the discriminative method outperforms the HMM-based method.",
        "zenodo_id": 1416546,
        "dblp_key": "conf/ismir/Shalev-ShwartzKS04",
        "keywords": [
            "supervised learning",
            "acoustic representation",
            "symbolic representation",
            "aligning function",
            "learning algorithm",
            "SVM",
            "alignment function",
            "discriminative approach",
            "generative method",
            "HMM-based method"
        ]
    },
    {
        "title": "Audio Fingerprinting In Peer-to-peer Networks.",
        "author": [
            "Prarthana Shrestha",
            "Ton Kalker"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417457",
        "url": "https://doi.org/10.5281/zenodo.1417457",
        "ee": "https://zenodo.org/records/1417457/files/ShresthaK04.pdf",
        "abstract": "Despite the immense potential of Peer-to-Peer (P2P) net- works in facilitating collaborative applications, they have become largely known as a free haven for pirated music swapping. In this paper, we present an approach wherein the collective computational power of the P2P networks is exploited to combat the problem of unauthorized mu- sic file sharing. We propose a distributed system based on audio fingerprinting,that makes it possible to recognize the music content present in the network. When the con- tents are identified, the network can take special measures against the use or sharing of unauthorized music. This proposed system is self-adapting, and robust. The forego- ing properties make the system particularly suitable for use in dynamic and heterogeneous environment of P2P networks. In order to investigate the behavior of the proposed sys- tem, a system-level model has been created using the Par- allel Object Oriented Specification Language (POOSL). This model was used to investigate an optimal system con- figuration that maximizes the identification of the content.",
        "zenodo_id": 1417457,
        "dblp_key": "conf/ismir/ShresthaK04",
        "keywords": [
            "Peer-to-Peer networks",
            "Unauthorized music file sharing",
            "Distributed system",
            "Audio fingerprinting",
            "Self-adapting",
            "Robust",
            "Dynamic and heterogeneous environment",
            "System-level model",
            "Optimal system configuration",
            "Content identification"
        ]
    },
    {
        "title": "Creating a nested melodic representation: competition and cooperation among bottom-up and top-down Gestalt principles.",
        "author": [
            "Jane Singer"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417965",
        "url": "https://doi.org/10.5281/zenodo.1417965",
        "ee": "https://zenodo.org/records/1417965/files/Singer04.pdf",
        "abstract": "A set of principles (based on Gestalt theory) governing how we group notes into meaningful groups has been widely accepted in the literature. Based on these principles, many divergent theories of melodic segmentation and representation have been proposed. However, these theories have not succeeded in achieving a comprehensive and verifiable representation of melody. This is largely due to the fact that multiple competing segmenting factors produce, for any single melody, a large number of possible segmentations and therefore representations. Here a model is proposed, which incorporates widely accepted principles of segmentation. These rules govern three types of factors: (1) changes in proximity (for producing disjunctive segmentation), (2) changes in overall contour and intervallic texture and (3) patterns and periodicity that create parallelism among segments. Because of the nature of the segmentation rules, these same rules establish the attributes of the groups they produce. Based on original research in Singer 2004, principles for establishing preferences among competing rules are formulated in order to create a few preferred representations for approximately 1,000 monophonic folksongs.",
        "zenodo_id": 1417965,
        "dblp_key": "conf/ismir/Singer04",
        "keywords": [
            "Gestalt theory",
            "melodic segmentation",
            "representation",
            "multiple competing segmenting factors",
            "comprehensive and verifiable representation",
            "model",
            "segmentation rules",
            "attributes of the groups",
            "preferences among competing rules",
            "monophonic folksongs"
        ]
    },
    {
        "title": "The International Music Information Retrieval Systems Evaluation Laboratory: Governance, Access and Security.",
        "author": [
            "J. Stephen Downie",
            "Joe Futrelle",
            "David K. Tcheng"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415120",
        "url": "https://doi.org/10.5281/zenodo.1415120",
        "ee": "https://zenodo.org/records/1415120/files/StephenFT04.pdf",
        "abstract": "The IMIRSEL (International Music Information Retrieval Systems Evaluation Laboratory) project provides an unprecedented platform for evaluating Music Information Retrieval (MIR) and Music Digital Library (MDL) techniques, by bringing together large corpora and significant computational resources with the necessary rights management and technical infrastructure to support a variety of MIR/MDL research areas. The standardized research collection being deployed represents a large and diverse corpus of musical examples, which we are hosting in our secure environment for use in evaluating MIR/MDL algorithms. Grid services and NCSA's D2K machine learning environment provide a powerful, high- performance, and secure framework for designing, optimising, and executing complex MIR/MDL evaluation applications. IMIRSEL provides a community resource for researchers who would otherwise not be able to afford the content rights and computational resources to carry out large-scale MIR/MDL evaluations. Keywords: evaluation, system modelling, Grid computing",
        "zenodo_id": 1415120,
        "dblp_key": "conf/ismir/StephenFT04",
        "keywords": [
            "Music Information Retrieval (MIR)",
            "Music Digital Library (MDL)",
            "right management",
            "computational resources",
            "Grid services",
            "NCSAs D2K machine learning environment",
            "community resource",
            "large corpus",
            "evaluation applications",
            "research collection"
        ]
    },
    {
        "title": "Well-Tempered Spelling: A Key Invariant Pitch Spelling Algorithm.",
        "author": [
            "Josh Stoddard",
            "Christopher Raphael",
            "Paul E. Utgoff"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1414762",
        "url": "https://doi.org/10.5281/zenodo.1414762",
        "ee": "https://zenodo.org/records/1414762/files/StoddardRU04.pdf",
        "abstract": "In this paper is described a data-driven algorithm for the functionally correct spelling of MIDI pitch values in terms of Western musical notation.  Input is in the form of MIDI files containing accurate pitch and rhythmic information with corresponding ground-truth spelling information for training and evaluation.  The algorithm recovers harmonic information from the MIDI data and spells pitches according to their relation to the local tonic.  The algorithm achieved 94.98% accuracy on the pitches that required accidentals in the local key and 99.686% overall.  Voice-leading resolution was found to be the best feature of those used to infer the correct spelling.  Also, this paper outlines great potential for improvement under this model.",
        "zenodo_id": 1414762,
        "dblp_key": "conf/ismir/StoddardRU04",
        "keywords": [
            "data-driven",
            "algorithm",
            "MIDI pitch",
            "spelling",
            "Western musical notation",
            "harmonic information",
            "spelling pitches",
            "accidentals",
            "local tonic",
            "voice-leading resolution"
        ]
    },
    {
        "title": "Search Effectiveness Measures for Symbolic Music Queries in Very Large Databases.",
        "author": [
            "Craig Stuart Sapp",
            "Yi-Wen Liu",
            "Eleanor Selfridge-Field"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417913",
        "url": "https://doi.org/10.5281/zenodo.1417913",
        "ee": "https://zenodo.org/records/1417913/files/StuartLS04.pdf",
        "abstract": "In the interest of establishing robust benchmarks for search efficiency, we conducted a series of tests on sym- bolic databases of musical incipits and themes taken from several diverse repertories. The results we report differ from existing studies in four respects: (1) the data quantity is much larger (c. 100,000 entries); (2) the levels of melo- dic and rhythmic precision are more refined; (3) anchored and unanchored searches were differentiated; and (4) re- sults from joint pitch-and-rhythmsearches were compared with those for pitch-only searches. The search results were evaluated using a theoretical approach which seeks to rank the number of symbols re- quired to achieve \u201csufficient uniqueness\u201d. How far into a melody must a search go in order to find an item which is unmatched by any other of the available items? How much does the answer depend on the specificity of the query? How much does anchoring the query matter? How much does the result depend on the nature of the reper- tory? We offer experimental results for these questions.",
        "zenodo_id": 1417913,
        "dblp_key": "conf/ismir/StuartLS04",
        "keywords": [
            "search efficiency",
            "sym-bolic databases",
            "musical incipits",
            "themes",
            "melodic and rhythmic precision",
            "anchored and unanchored searches",
            "joint pitch-and-rhythm searches",
            "pitch-only searches",
            "theoretical approach",
            "sufficient uniqueness"
        ]
    },
    {
        "title": "Exploring Microtonal Matching.",
        "author": [
            "Iman S. H. Suyoto",
            "Alexandra L. Uitdenbogerd"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416656",
        "url": "https://doi.org/10.5281/zenodo.1416656",
        "ee": "https://zenodo.org/records/1416656/files/SuyotoU04.pdf",
        "abstract": "Most research into music information retrieval thus far has only examined music from the western tradition. How- ever, music of other origins often conforms to different tuning systems. Therefore there are problems both in rep- resenting this music as well as finding matches to queries from these diverse tuning systems. We discuss the issues associated with microtonal music retrieval and present some preliminary results from an experiment in applying scoring matrices to microtonal matching.",
        "zenodo_id": 1416656,
        "dblp_key": "conf/ismir/SuyotoU04",
        "keywords": [
            "research",
            "music",
            "information",
            "retrieval",
            "western",
            "tradition",
            "tuning",
            "systems",
            "problems",
            "microtonal"
        ]
    },
    {
        "title": "Music Information Retrieval systems: why do individuals use them and what are their needs?.",
        "author": [
            "Sara Taheri-Panah",
            "Andrew MacFarlane 0001"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416334",
        "url": "https://doi.org/10.5281/zenodo.1416334",
        "ee": "https://zenodo.org/records/1416334/files/Taheri-PanahM04.pdf",
        "abstract": "conducted on the behaviour of music information retrieval (MIR) users, in spite of the immense popularity of free music retrieval systems available on the Internet. In this study we examine the issue of music seeking behaviour through the examination of users life style effect of three different age groups using questionnaires. It was found that lifestyles had a significant impact on users need for music and hence their music seeking behaviour. The importance of social networks in music information seeking was reinforced in this study. An experiment was conducted with three different types of search on the Kazaa MIR system and the participants interviewed in order to collect data. Users found the Kazaa system intuitive and easy to use. Searchers used both song titles and lyrics for finding relevant music items. The insights provided by this study can be of assistance in the development of user focused Internet MIR systems. Keywords: user music seeking behaviour retrieval",
        "zenodo_id": 1416334,
        "dblp_key": "conf/ismir/Taheri-PanahM04",
        "keywords": [
            "music information retrieval",
            "free music retrieval systems",
            "internet music seeking",
            "user behaviour",
            "age groups",
            "questionnaires",
            "lifestyles",
            "music seeking behaviour",
            "social networks",
            "Kazaa MIR system"
        ]
    },
    {
        "title": "Rhythm and Tempo Recognition of Music Performance from a Probabilistic Approach.",
        "author": [
            "Haruto Takeda",
            "Takuya Nishimoto",
            "Shigeki Sagayama"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418209",
        "url": "https://doi.org/10.5281/zenodo.1418209",
        "ee": "https://zenodo.org/records/1418209/files/TakedaNS04.pdf",
        "abstract": "This paper concerns both rhythm recognition and tempo analysis of expressive music performance based on a probabilistic approach. In rhythm recognition, the mod- ern continuous speech recognition technique is applied to find the most likely intended note sequence from the given sequence of fluctuating note durations in the per- formance. Combining stochastic models of note dura- tions deviating from the nominal lengths and a probabilis- tic grammar representing possible sequences of notes, the problem is formulated as a maximum a posteriori estima- tion that can be implemented using efficient search based on the Viterbi algorithm. With this, significant improve- ments compared with conventional \u201cquantization\u201d tech- niques were found. Tempo analysis is performed by fit- ting the observed tempo with parametric tempo curves in order to extract tempo dynamics and characteristics of performance to use. Tempo-change timings and param- eter values in tempo curve models are estimated through the segmental k-means algorithm. Experimental results of rhythm recognition and tempo analysis applied to classical and popular music performances are also demonstrated. keywords: rhythm recognition, hidden Markov models, tempo analysis, segmental k-means algorithm, continuous speech recognition framework, n-gram grammar",
        "zenodo_id": 1418209,
        "dblp_key": "conf/ismir/TakedaNS04",
        "keywords": [
            "probabilistic approach",
            "modern continuous speech recognition",
            "note sequence",
            "fluctuating note durations",
            "maximum a posteriori estimation",
            "Viterbi algorithm",
            "stochastic models",
            "tempo analysis",
            "parametric tempo curves",
            "segmental k-means algorithm"
        ]
    },
    {
        "title": "Pregroup Grammars for Chords.",
        "author": [
            "Richard Terrat"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416702",
        "url": "https://doi.org/10.5281/zenodo.1416702",
        "ee": "https://zenodo.org/records/1416702/files/Terrat04.pdf",
        "abstract": "Pregroups had been conceived as an algebraic tool to recognize grammatically well-formed sentences in natural languages [3]. Here we wish to use pregroups to recognize well-formed chords of pitches, for a given definition of those chords. We show how a judicious choice of basic and simple types allows a context-free grammatical description. Then we use the robustness property to extend the set of well-formed chords in a simple way. Finally we  argue in favor of an utilization of pregroups grammars for the recognition and classification of chord sequences.",
        "zenodo_id": 1416702,
        "dblp_key": "conf/ismir/Terrat04",
        "keywords": [
            "Pregroups",
            "natural languages",
            "well-formed sentences",
            "chords of pitches",
            "context-free grammatical description",
            "robustness property",
            "well-formed chords",
            "classification of chord sequences",
            "algebraic tool",
            "robustness"
        ]
    },
    {
        "title": "Retrieval of percussion gestures using timbre classification techniques.",
        "author": [
            "Adam R. Tindale",
            "Ajay Kapur",
            "George Tzanetakis",
            "Ichiro Fujinaga"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416612",
        "url": "https://doi.org/10.5281/zenodo.1416612",
        "ee": "https://zenodo.org/records/1416612/files/TindaleKTF04.pdf",
        "abstract": "Musicians are able to recognise the subtle differences in timbre produced by different playing techniques on an in- strument, yet there has been little research into achiev- ing this with a computer. This paper will demonstrate an automatic system that can successfully recognise differ- ent timbres produced by different performance techniques and classify them using signal processing and classifica- tion tools. Success rates over 90% are achieved when clas- sifying snare drum timbres produced by different playing techniques.",
        "zenodo_id": 1416612,
        "dblp_key": "conf/ismir/TindaleKTF04",
        "keywords": [
            "automatic system",
            "timbre recognition",
            "playing techniques",
            "signal processing",
            "classification tools",
            "different performance techniques",
            "classification",
            "snare drum timbres",
            "classification success rate",
            "90%"
        ]
    },
    {
        "title": "Visualizing and Exploring Personal Music Libraries.",
        "author": [
            "Marc Torrens",
            "Patrick Hertzog",
            "Josep Llu\u00eds Arcos"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1414746",
        "url": "https://doi.org/10.5281/zenodo.1414746",
        "ee": "https://zenodo.org/records/1414746/files/TorrensHA04.pdf",
        "abstract": "Nowadays, music fans are beginning to massively use mobile digital music players and dedicated software to or- ganize and play large collections of music. In this con- text, users deal with huge music libraries containing thou- sands of tracks. Such a huge volume of music easily over- whelms users when selecting the music to listen or when organizing their collections. Music player software with visualizations based on tex- tual lists and organizing features such as smart playlists are not really enough for helping users to efficiently man- age their libraries. Thus, we propose new graphical vi- sualizations and their associated features to allow users to better organize their personal music libraries and therefore also to ease selection later on.",
        "zenodo_id": 1414746,
        "dblp_key": "conf/ismir/TorrensHA04",
        "keywords": [
            "mobile digital music players",
            "dedicated software",
            "organize and play",
            "huge music libraries",
            "thousands of tracks",
            "overwhelms users",
            "selecting music",
            "organizing collections",
            "efficiently manage",
            "personal music libraries"
        ]
    },
    {
        "title": "A Comparison of Rhythmic Similarity Measures.",
        "author": [
            "Godfried T. Toussaint"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416812",
        "url": "https://doi.org/10.5281/zenodo.1416812",
        "ee": "https://zenodo.org/records/1416812/files/Toussaint04.pdf",
        "abstract": "Traditionally, rhythmic similarity measures are compared according to how well rhythms may be recognized with them, how efficiently they can be retrieved from a data base, or how well they model human perception and cog- nition. In contrast, here similarity measures are compared on the basis of how much insight they provide about the structural inter-relationships that exist within families of rhythms, when phylogenetic trees and graphs are com- puted from the distance matrices determined by these sim- ilarity measures. Phylogenetic analyses yield insight into the evolution of rhythms and may uncover interesting an- cestral rhythms.",
        "zenodo_id": 1416812,
        "dblp_key": "conf/ismir/Toussaint04",
        "keywords": [ 
            "Rhythmic similarity measures", 
            "Phylogenetic analysis", 
            "Rhythm evolution", 
            "Distance matrices", 
            "Music information retrieval", 
            "Copyright infringement resolution", 
            "Rhythm families", 
            "Bell-patterns", 
            "Clave patterns", 
            "Computational music theory" 
        ]
    },
    {
        "title": "Utility System For Constructing Database Of Performance Deviations.",
        "author": [
            "Ken&apos;ichi Toyoda",
            "Kenzi Noike",
            "Haruhiro Katayose"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417953",
        "url": "https://doi.org/10.5281/zenodo.1417953",
        "ee": "https://zenodo.org/records/1417953/files/ToyodaNK04.pdf",
        "abstract": "Demand for music databases is increasing for the stud- ies of musicology and music informatics. Our goal is to construct databases that contain deviations of tempo, and dynamics, start-timing, and duration of each note. This paper describes a procedure based on hybrid use of DP Matching and HMM that efficiently extracts deviations from MIDI-formatted expressive human performances. The algorithm of quantizing the start-timing of the notes has been successfully tested on a database of ten expressive piano performances. It gives an accuracy of 92.9% when one note per bar is given as the guide. This paper also in- troduces tools provided so that the public can make use of our database on the web. Keywords Database, HMM, DP matching",
        "zenodo_id": 1417953,
        "dblp_key": "conf/ismir/ToyodaNK04",
        "keywords": [
            "database",
            "tempo deviations",
            "dynamics",
            "start-timing",
            "duration",
            "MIDI-formatted",
            "expressive human performances",
            "quantizing notes",
            "accuracy",
            "public access"
        ]
    },
    {
        "title": "Towards Automatic Identification Of Singing Language In Popular Music Recordings.",
        "author": [
            "Wei-Ho Tsai",
            "Hsin-Min Wang"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417511",
        "url": "https://doi.org/10.5281/zenodo.1417511",
        "ee": "https://zenodo.org/records/1417511/files/TsaiW04.pdf",
        "abstract": "The automatic analysis of singing from music is an important and challenging issue within the research target of content-based retrieval of music information. As part of this research target, this study presents a first attempt to automatically identify the language sung in a music recording. It is assumed that each language has its own set of constraints that specify which of the basic linguistic events present in a singing process are allowed to follow another. The acoustic structure of individual languages may, thus, be characterized by statistically modeling those constraints. To this end, the proposed method employs vector clustering to convert a singing signal from its spectrum-based feature representation into a sequence of smaller basic phonological units. The dynamic characteristics of the sequence are then analyzed by using bigram language models. Since the vector clustering is performed in an unsupervised manner, the resulting system does not use sophisticated linguistic knowledge and, thus, is easily portable to new language sets. In addition, to eliminate the interference of",
        "zenodo_id": 1417511,
        "dblp_key": "conf/ismir/TsaiW04",
        "keywords": [
            "automatic analysis",
            "music retrieval",
            "language identification",
            "singing process",
            "linguistic events",
            "basic phonological units",
            "vector clustering",
            "bigram language models",
            "unsupervised manner",
            "new language sets"
        ]
    },
    {
        "title": "A search method for notated polyphonic music with pitch and tempo fluctuations.",
        "author": [
            "Rainer Typke",
            "Frans Wiering",
            "Remco C. Veltkamp"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417947",
        "url": "https://doi.org/10.5281/zenodo.1417947",
        "ee": "https://zenodo.org/records/1417947/files/TypkeWV04.pdf",
        "abstract": "We compare two methods of measuring melodic sim- ilarity for symbolically represented polyphonic music. Both exploit advantages of transportation distances such as continuity and partial matching in the pitch dimension. By segmenting queries and database documents, one of them also offers partial matching in the time dimension. This method can find short queries in long database docu- ments and is more robust against pitch and tempo fluc- tuations in the queries or database documents than the method that uses transportation distances alone. We com- pare the use of transportation distances with and without segmentation for the RISM A/II collection and find that segmentation improves recall and precision. With every- thing else being equal, the segmented search found 80 out of 114 relevant documents, while the method relying solely on transportation distances found only 60.",
        "zenodo_id": 1417947,
        "dblp_key": "conf/ismir/TypkeWV04",
        "keywords": [ 
            "Melodic similarity", 
            "Polyphonic music", 
            "Transportation distances", 
            "Pitch fluctuations", 
            "Tempo fluctuations", 
            "Partial matching", 
            "Segmentation", 
            "Music search", 
            "Database documents", 
            "RISM A/II collection" 
        ]
    },
    {
        "title": "Query-by-Beat-Boxing: Music Retrieval For The DJ.",
        "author": [
            "George Tzanetakis",
            "Ajay Kapur",
            "Manj Benning"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418033",
        "url": "https://doi.org/10.5281/zenodo.1418033",
        "ee": "https://zenodo.org/records/1418033/files/TzanetakisKB04.pdf",
        "abstract": "BeatBoxing is a type of vocal percussion, where musicians use their lips, cheeks, and throat to create different beats. It is commonly used by hiphop and rap artists. In this pa- per, we explore the use of BeatBoxing as a query mecha- nism for music information retrieval and more speci\u00a3cally the retrieval of drum loops. A classi\u00a3cation system that automatically identi\u00a3es the individual beat boxing sounds and can map them to corresponding drum sounds has been developed. In addition, the tempo of BeatBoxing is au- tomatically detected and used to dynamically browse a database of music. We also describe some experiments in extracting structural representations of rhythm and their use for style classi\u00a3cation of drum loops.",
        "zenodo_id": 1418033,
        "dblp_key": "conf/ismir/TzanetakisKB04",
        "keywords": [
            "BeatBoxing",
            "vocal percussion",
            "hiphop",
            "rap artists",
            "music information retrieval",
            "drum loops",
            "classification system",
            "individual beat boxing sounds",
            "corresponding drum sounds",
            "tempo detection"
        ]
    },
    {
        "title": "Digital Music Interaction Concepts: A User Study.",
        "author": [
            "Fabio Vignoli"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1414994",
        "url": "https://doi.org/10.5281/zenodo.1414994",
        "ee": "https://zenodo.org/records/1414994/files/Vignoli04.pdf",
        "abstract": "The popularity of digital music has recently rapidly increased. The widespread use on computers and portable players and its availability through the Internet have modified the interaction issues from availability towards choice. The user is confronted daily with an enormous amount of music. This situation shapes the need for the development of new user interfaces to access and retrieve music that takes full advantage of the music being digital. This paper reports the results of various user tests aimed at investigating how music listeners organize and access their digital music collection. The aim of the study is to investigate novel interaction concepts to access and retrieve music from large personal collections. The outcome of these studies was an interaction concept based on the notion of similarity of music items (artists and songs). This concept was further refined and developed into a demonstrator eventually tested with users.",
        "zenodo_id": 1414994,
        "dblp_key": "conf/ismir/Vignoli04",
        "keywords": [
            "digital music",
            "popularity",
            "availability",
            "Internet",
            "user interfaces",
            "choice",
            "music collection",
            "personal collections",
            "interaction concepts",
            "similarity of music items"
        ]
    },
    {
        "title": "Mapping Music In The Palm Of Your Hand, Explore And Discover Your Collection.",
        "author": [
            "Fabio Vignoli",
            "Rob van Gulik",
            "Huub van de Wetering"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416960",
        "url": "https://doi.org/10.5281/zenodo.1416960",
        "ee": "https://zenodo.org/records/1416960/files/VignoliGW04.pdf",
        "abstract": "The trends of miniaturization and increasing storage capabilities for portable music players made it possible to carry increasingly more music on small portable devices, but it also introduced negative consequences for the user interface and navigation. Finding music in large collections can be hard if one does not know exactly what to look for. In this paper a novel user interface to browse and navigate through music on small devices is proposed, together with the enabling algorithms. The goal of this interface is to enable the users to explore and discover their entire collection and to support non- specific searches. To this end, a new way to visualize and navigate through music is introduced: the artist map. The artist map is designed to provide an overview of an entire music collection, or a subset thereof, by clearly visualizing the similarity between artists, computed from the music itself. Contextual information (e.g. mood, genre) is added by coloring and by attribute magnets. The artist map is implemented by a graph-drawing algorithm, which uses an improved energy model. The proposed algorithm and interface have been implemented in a prototype and will be tested with \u2018real\u2019 users. Keywords: Music, graphical user interface, similarity, navigation, non-specific search, music metadata",
        "zenodo_id": 1416960,
        "dblp_key": "conf/ismir/VignoliGW04",
        "keywords": [
            "miniaturization",
            "increasing storage capabilities",
            "portable music players",
            "user interface",
            "navigation",
            "negative consequences",
            "finding music",
            "large collections",
            "novel user interface",
            "artist map"
        ]
    },
    {
        "title": "Instrument identification in solo and ensemble music using Independent Subspace Analysis.",
        "author": [
            "Emmanuel Vincent 0001",
            "Xavier Rodet"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416524",
        "url": "https://doi.org/10.5281/zenodo.1416524",
        "ee": "https://zenodo.org/records/1416524/files/VincentR04.pdf",
        "abstract": "We investigate the use of Independent Subspace Analy- sis (ISA) for instrument identification in musical record- ings. We represent short-term log-power spectra of pos- sibly polyphonic music as weighted non-linear combina- tions of typical note spectra plus background noise. These typical note spectra are learnt either on databases contain- ing isolated notes or on solo recordings from different in- struments. We show that this model has some theoreti- cal advantages over methods based on Gaussian Mixture Models (GMM) or on linear ISA. Preliminary experiments with five instruments and test excerpts taken from com- mercial CDs give promising results. The performance on clean solo excerpts is comparable with existing methods and shows limited degradation under reverberant condi- tions. Applied to a difficult duo excerpt, the model is also able to identify the right pair of instruments and to provide an approximate transcription of the notes played by each instrument.",
        "zenodo_id": 1416524,
        "dblp_key": "conf/ismir/VincentR04",
        "keywords": [
            "Independent Subspace Analysis",
            "Instrument identification",
            "Polyphonic music",
            "Weighted non-linear combinations",
            "Typical note spectra",
            "Background noise",
            "Gaussian Mixture Models",
            "Linear Independent Subspace Analysis",
            "Preliminary experiments",
            "Clean solo excerpts"
        ]
    },
    {
        "title": "Features and classifiers for the automatic classification of musical audio signals.",
        "author": [
            "Kristopher West",
            "Stephen Cox"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418025",
        "url": "https://doi.org/10.5281/zenodo.1418025",
        "ee": "https://zenodo.org/records/1418025/files/WestC04.pdf",
        "abstract": "Several factors affecting the automatic classification of musical audio signals are examined. Classification is per- formed on short audio frames and results are reported as \u201cbag of frames\u201d accuracies, where the audio is segmented into 23ms analysis frames and a majority vote is taken to decide the final classification. The effect of different pa- rameterisations of the audio signal is examined. The effect of the inclusion of information on the temporal variation of these features is examined and finally, the performance of several different classifiers trained on the data is com- pared. A new classifier is introduced, based on the un- supervised construction of decision trees and either linear discriminant analysis or a pair of single Gaussian clas- sifiers. The classification results show that the topology of the new classifier gives it a significant advantage over other classifiers, by allowing the classifier to model much more complex distributions within the data than Gaussian schemes do.",
        "zenodo_id": 1418025,
        "dblp_key": "conf/ismir/WestC04",
        "keywords": [
            "automatic classification",
            "musical audio signals",
            "bag of frames",
            "audio frames",
            "temporal variation",
            "different parameterisations",
            "classifiers",
            "decision trees",
            "un-supervised construction",
            "Gaussian schemes"
        ]
    },
    {
        "title": "The Influence of Pitch on Melodic Segmentation.",
        "author": [
            "Tillman Weyde"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415556",
        "url": "https://doi.org/10.5281/zenodo.1415556",
        "ee": "https://zenodo.org/records/1415556/files/Weyde04.pdf",
        "abstract": "Melodic segmentation is an important topic for music in- formation retrieval, because it divides melodies into musi- cally relevant units. Most influential theories on melodic segmentation of the last decades have stressed the role of pitch for melodic segmentation. The general assump- tion was, that relatively large changes or distances in any musical parameter like pitch, time, dynamics, or melodic movement mark segment boundaries. This has generally been accepted despite the lack of empirical studies. Here an empirical study is presented, that investigates the influ- ence of inter-onset-intervals (IOI), intensity accents, pitch intervals, and pitch interval direction changes. The results show a significant influence only for IOIs and intensity, but neither for pitch intervals nor for changes in interval direction. The validity of the results and possible explana- tions are discussed and directions of further investigations are outlined. 1",
        "zenodo_id": 1415556,
        "dblp_key": "conf/ismir/Weyde04",
        "keywords": [
            "melodic segmentation",
            "music information retrieval",
            "musically relevant units",
            "pitch",
            "melodic movement",
            "empirical study",
            "influences IOIs",
            "intensity accents",
            "pitch intervals",
            "changes in interval direction"
        ]
    },
    {
        "title": "Automatic Record Reviews.",
        "author": [
            "Brian Whitman",
            "Dan Ellis"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1416646",
        "url": "https://doi.org/10.5281/zenodo.1416646",
        "ee": "https://zenodo.org/records/1416646/files/WhitmanE04.pdf",
        "abstract": "Record reviews provide a unique and focused source of linguistic data that can be related to musical recordings, to provide a basis for computational music understanding systems with applications in similarity, recommendation and classification. We analyze a large testbed of music and a corpus of reviews for each work to uncover pat- terns and develop mechanisms for removing reviewer bias and extraneous non-musical discussion. By building upon work in grounding free text against audio signals we in- vent an \u201cautomatic record review\u201d system that labels new music audio with maximal semantic value for future re- trieval tasks. In effect, we grow an unbiased music editor trained from the consensus of the online reviews we have gathered. Keywords: cultural factors, language, machine learn- ing, audio features, reviews",
        "zenodo_id": 1416646,
        "dblp_key": "conf/ismir/WhitmanE04",
        "keywords": [
            "language",
            "machine learning",
            "audio features",
            "reviews",
            "cultural factors",
            "audio signals",
            "free text",
            "unbiased music editor",
            "grounding",
            "semantic value"
        ]
    },
    {
        "title": "A Case Study of Distributed Music Audio Analysis Using the Geddei Processing Framework.",
        "author": [
            "Gavin Wood",
            "Simon O&apos;Keefe"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1414810",
        "url": "https://doi.org/10.5281/zenodo.1414810",
        "ee": "https://zenodo.org/records/1414810/files/WoodO04.pdf",
        "abstract": "Audio signal processing and refinement is an impor- tant part of a content-based music information retrieval system. As the our repertoire of techniques becomes more varied, there are greater requirements of computation power. Distributed storage techniques have become widespread and almost invisible with the advent of file-sharing sys- tems, on-line digital music stores and on line storage ser- vices. Even discounting data with potential copyright en- tanglements, there is a vast amount that is ripe for analy- sis, and thus parallelised and distributed processing tech- niques seem increasingly appropriate. Existing frameworks are already capable of a signifi- cant amount of audio analysis for music information re- trieval. However they are by and large ignorant of distri- bution and parallelisation. There are middleware libraries to help with aspects of distributed computing, but combin- ing the two can be cumbersome and inefficient. This paper provides a brief description of a software framework that can process audio in a scalable and dis- tributed manner: Geddei. The paper then takes an in- teresting and relevant signal analysis task often used for music information retrieval and implements it under the Geddei framework. The ease of use is discussed and vari- ous measurements taken of Geddei, both in comparison to itself under different circumstances, and \u2018reference code\u2019 that was used in a previous study. We discuss the prob- lems with the distribution of the task with Geddei and of- fer some possible solutions.",
        "zenodo_id": 1414810,
        "dblp_key": "conf/ismir/WoodO04",
        "keywords": [
            "audio signal processing",
            "content-based music information retrieval",
            "computation power",
            "distributed storage techniques",
            "file-sharing systems",
            "on-line digital music stores",
            "online storage services",
            "parallelised and distributed processing techniques",
            "existing frameworks",
            "music information retrieval"
        ]
    },
    {
        "title": "An MPEG-7 Database System and Application for Content-Based Management and Retrieval of Music.",
        "author": [
            "Otto W\u00fcst",
            "\u00d2scar Celma"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1418375",
        "url": "https://doi.org/10.5281/zenodo.1418375",
        "ee": "https://zenodo.org/records/1418375/files/WustC04.pdf",
        "abstract": "Computer users are gaining access to and are starting to accumulate moderately large collections of multimedia files, in particular of audio content, and therefore demand new applications and systems capable of effectively retriev- ing and manipulating these multimedia objects. Content- based retrieval of multimedia files is typically based on searching within a feature space, defined as a collection of parameters that have been extracted from the content and which describe it in a relevant way for that particular retrieval application. The MPEG-7 standard offers tools to model these metadata in an interoperable and extensi- ble way, and can therefore be considered as a framework for building content-based audio retrieval systems. This paper highlights the most relevant aspects consid- ered during the design and implementation of a DBMS- driven MPEG-7 layer on top of which a content-based mu- sic retrieval system has been built. A particular focus is set on the data modeling and database architechture issues.",
        "zenodo_id": 1418375,
        "dblp_key": "conf/ismir/WustC04",
        "keywords": [
            "multimedia files",
            "audio content",
            "content-based retrieval",
            "feature space",
            "MPEG-7 standard",
            "metadata modeling",
            "interoperable",
            "extensible",
            "database management system",
            "music retrieval system"
        ]
    },
    {
        "title": "Disambiguating Music Emotion Using Software Agents.",
        "author": [
            "Dan Yang 0002",
            "Won-Sook Lee"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415272",
        "url": "https://doi.org/10.5281/zenodo.1415272",
        "ee": "https://zenodo.org/records/1415272/files/YangL04.pdf",
        "abstract": "Annotating music poses a cognitive load on listeners and this potentially interferes with the emotions being reported. One solution is to let software agents learn to make the annotator\u2019s task easier and more efficient. Emo is a music annotation prototype that combines inputs from both human and software agents to better study human listening. A compositional theory of musical meaning provides the overall heuristics for the annotation process, with the listener drawing upon different influences such as acoustics, lyrics and cultural metadata to focus on a specific musical mood. Software agents track the way these choices are made from the influences available. A functional theory of human emotion provides the basis for introducing necessary bias into the machine learning agents. Conflicting positive and negative emotions can be separated on the basis of their different function (reward-approach and threat-avoidance) or dysfunction (psychotic). Negative emotions have strong ambiguity and these are the focus of the experiment. The results of mining psychological features of lyrics are promising, recognisable in terms of common sense ideas of emotion and in terms of accuracy. Further ideas for deploying agents in this model of music annotation are presented.",
        "zenodo_id": 1415272,
        "dblp_key": "conf/ismir/YangL04",
        "keywords": [
            "Annotating music",
            "cognitive load",
            "emotional reporting",
            "software agents",
            "Emo prototype",
            "music annotation",
            "compositional theory",
            "functional theory",
            "bias introduction",
            "negative emotions"
        ]
    },
    {
        "title": "Automatic Drum Sound Description for Real-World Music Using Template Adaptation and Matching Methods.",
        "author": [
            "Kazuyoshi Yoshii",
            "Masataka Goto",
            "Hiroshi G. Okuno"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415958",
        "url": "https://doi.org/10.5281/zenodo.1415958",
        "ee": "https://zenodo.org/records/1415958/files/YoshiiGO04.pdf",
        "abstract": "This paper presents an automatic description system of drum sounds for real-world musical audio signals. Our system can represent onset times and names of drums by means of drum descriptors defined in the context of MPEG-7. For their automatic description, drum sounds must be identified in such polyphonic signals. The prob- lem is that acoustic features of drum sounds vary with each musical piece and precise templates for them can- not be prepared in advance. To solve this problem, we propose new template-adaptation and template-matching",
        "zenodo_id": 1415958,
        "dblp_key": "conf/ismir/YoshiiGO04",
        "keywords": [
            "automatic description system",
            "drum sounds",
            "real-world musical audio signals",
            "onset times",
            "drum descriptors",
            "MPEG-7",
            "polyphonic signals",
            "acoustic features",
            "drum identification",
            "template-adaptation"
        ]
    },
    {
        "title": "Automatic Chord Transcription with Concurrent Recognition of Chord Symbols and Boundaries.",
        "author": [
            "Takuya Yoshioka",
            "Tetsuro Kitahara",
            "Kazunori Komatani",
            "Tetsuya Ogata",
            "Hiroshi G. Okuno"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1415068",
        "url": "https://doi.org/10.5281/zenodo.1415068",
        "ee": "https://zenodo.org/records/1415068/files/YoshiokaKKOO04.pdf",
        "abstract": "This paper describes a method that recognizes musical chords from real-world audio signals in compact-disc recordings. The automatic recognition of musical chords is necessary for music information retrieval (MIR) sys- tems, since the chord sequences of musical pieces cap- ture the characteristics of their accompaniments. None of the previous methods can accurately recognize musi- cal chords from complex audio signals that contain vocal and drum sounds. The main problem is that the chord- boundary-detection and chord-symbol-identification pro- cesses are inseparable because of their mutual depen- dency. In order to solve this mutual dependency problem, our method generates hypotheses about tuples of chord symbols and chord boundaries, and outputs the most plau- sible one as the recognition result. The certainty of a hy- pothesis is evaluated based on three cues: acoustic fea- tures, chord progression patterns, and bass sounds. Ex- perimental results show that our method successfully rec- ognized chords in seven popular music songs; the average accuracy of the results was around 77%. Keywords: audio signal, musical key, musical chord, hy- pothesis search",
        "zenodo_id": 1415068,
        "dblp_key": "conf/ismir/YoshiokaKKOO04",
        "keywords": [
            "audio signals",
            "musical chords",
            "music information retrieval",
            "chord boundary-detection",
            "chord-symbol-identification",
            "mutual dependency problem",
            "hypotheses about tuples",
            "most plausible one",
            "recognition result",
            "certainty of a hypothesis"
        ]
    },
    {
        "title": "Web Services for Music Information Retrieval.",
        "author": [
            "Mark Zadel",
            "Ichiro Fujinaga"
        ],
        "year": "2004",
        "doi": "10.5281/zenodo.1417069",
        "url": "https://doi.org/10.5281/zenodo.1417069",
        "ee": "https://zenodo.org/records/1417069/files/ZadelF04.pdf",
        "abstract": "In the emerging world of networked and distributed digital libraries, the Web services framework will be a key to facilitating simple inter-application communication be- tween them. Yet, despite the popularity of Web services in the business sector and their seemingly obvious appli- cability to the digital library domain, and to MIR in par- ticular, the adoption of these new protocols has not been widespread. To demonstrate the tremendous potential of Web ser- vices for MIR, this paper presents an application using the Google and Amazon.com databases to generate clus- ters of related musical artists based on cultural metadata. The use of cultural metadata to determine artist related- ness is valuable and interesting because it captures emer- gent popular opinion about music. Starting from an initial seed artist, Amazon Listmania! lists are traversed to find potentially related artists. Google is used to determine which of these candidates are in fact related by assess- ing the co-occurrence of the two artists\u2019 names on Internet web pages. A list of artists related to the seed is returned once a given number of artists is found. The positive results generated by the system illustrate the use of Web services for exploiting the vast amount of untapped data that are available today and highlight their importance for the future, when even more musical data will become available.",
        "zenodo_id": 1417069,
        "dblp_key": "conf/ismir/ZadelF04",
        "keywords": [
            "networked",
            "distributed",
            "digital libraries",
            "Web services framework",
            "simple inter-application communication",
            "business sector",
            "digital library domain",
            "MIR",
            "cultural metadata",
            "Google and Amazon.com databases"
        ]
    },
    {
        "title": "ISMIR 2004, 5th International Conference on Music Information Retrieval, Barcelona, Spain, October 10-14, 2004, Proceedings",
        "author": [],
        "year": "2004",
        "doi": "10.5281/zenodo.1285647",
        "url": "https://doi.org/10.5281/zenodo.1285647",
        "ee": null,
        "abstract": "The Annotated Jingju Arias Dataset is a collection of 34 jingju arias manually segmented in various levels using the software Praat v5.3.53. The selected arias contain samples of the two main shengqiang in jingju, name xipi and erhuang, and the five main role types in terms of singing, namely, dan, jing, laodan, laosheng and xiaosheng.\n\nThe dataset includes a Praat TextGrid file for each aria with the following tiers (all the annotations are in Chinese):\n\n\n\taria: name of the work (one segment for the whole aria)\n\tMBID: MusicBrainz ID of the audioi recording(one segment for the whole aria)\n\tartist: name of the singing performer (one segment for the whole aria)\n\tschool: related performing school (one segment for the whole aria)\n\trole-type: role type of the singing character(one segment for the whole aria)\n\tshengqiang:boundaries and label of theshengqiangperformed in the aria (including accompaniment)\n\tbanshi: boundaries and label of the banshi performed in the aria (including accompaniment)\n\tlyrics-lines: boundaries and annotation of each line of lyrics\n\tlyrics-syllables: boundaries and annotation of each syllable\n\tluogu: boundaries and label of each of the performed percussion patterns in the aria\n\n\nThe ariasInfo.txt file contains a summary of the contents per aira of the whole dataset.\n\nA subset of this dataset comprising 20 arias has been used for the study of the relationship between linguistic tones and melody in the following papers:\n\n\nShuoZhang, Rafael Caro Repetto, and Xavier Serra (2014) Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing. In Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014), Taipei, Taiwan, October 2731, pp. 343348.\n\n\n\n______ (2015) Predicting Pairwise Pitch Contour Relations Based on Linguistic Tone Information in Beijing Opera Singing. In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015), Mlaga, Spain, October 2630, pp. 107113.\n\n\nHere is the list of the arias from the dataset used in these papers.\n\nThe whole dataset has been used for the automatic analysis of the structure of jingju arias and their automatic segmentation in the following master&#39;s thesis:\n\n\nYile Yang(2016) Structure Analysis of Beijing Opera Arias. Masters thesis, Universitat Pompeu Fabra, Barcelona.\n\n\nUsing this dataset\n\nIf you use this dataset in a publication, please cite the above publications.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nThe audio recordings used for these annotations are available for research purposes. Please contact Rafael Caro Repetto\n\nrafael.caro@upf.edu\n\n\n\nhttp://compmusic.upf.edu/node/349",
        "zenodo_id": 1285647,
        "dblp_key": "conf/ismir/2004"
    }
]