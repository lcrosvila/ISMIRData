[
    {
        "title": "An Attack/Decay Model for Piano Transcription.",
        "author": [
            "Tian Cheng 0001",
            "Matthias Mauch",
            "Emmanouil Benetos",
            "Simon Dixon"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1418153",
        "url": "https://doi.org/10.5281/zenodo.1418153",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/085_Paper.pdf",
        "abstract": "We demonstrate that piano transcription performance for a known piano can be improved by explicitly modelling piano acoustical features. The proposed method is based on non-negative matrix factorisation, with the following three refinements: (1) introduction of attack and harmonic decay components; (2) use of a spike-shaped note activa- tion that is shared by these components; (3) modelling the harmonic decay with an exponential function. Transcrip- tion is performed in a supervised way, with the training and test datasets produced by the same piano. First we train pa- rameters for the attack and decay components on isolated notes, then update only the note activations for transcrip- tion. Experiments show that the proposed model achieves 82% on note-wise and 79% on frame-wise F-measures on the ‘ENSTDkCl’ subset of the MAPS database, outper- forming the current published state of the art.",
        "zenodo_id": 1418153,
        "dblp_key": "conf/ismir/0001MBD16",
        "content": "AN ATTACK/DECAY MODEL FOR PIANO TRANSCRIPTION\nTian Cheng, Matthias Mauch, Emmanouil Benetos and Simon Dixon\nCentre for Digital Music, Queen Mary University of London\n{t.cheng, m.mauch, emmanouil.benetos, s.e.dixon }@qmul.ac.uk\nABSTRACT\nWe demonstrate that piano transcription performance\nfor a known piano can be improved by explicitly modelling\npiano acoustical features. The proposed method is based\non non-negative matrix factorisation, with the following\nthree reﬁnements: (1) introduction of attack and harmonic\ndecay components; (2) use of a spike-shaped note activa-\ntion that is shared by these components; (3) modelling the\nharmonic decay with an exponential function. Transcrip-\ntion is performed in a supervised way, with the training and\ntest datasets produced by the same piano. First we train pa-\nrameters for the attack and decay components on isolated\nnotes, then update only the note activations for transcrip-\ntion. Experiments show that the proposed model achieves\n82% on note-wise and 79% on frame-wise F-measures on\nthe ‘ENSTDkCl’ subset of the MAPS database, outper-\nforming the current published state of the art.\n1. INTRODUCTION\nAutomatic music transcription (AMT) converts a musi-\ncal recording into a symbolic representation, i.e. a set of\nnote events, each consisting of pitch, onset time and du-\nration. Non-negative matrix factorisation (NMF) is com-\nmonly used in the AMT area for over a decade since [1].\nIt factorises a spectrogram (or other time-frequency rep-\nresentation, e.g. Constant-Q transform) of a music signal\ninto non-negative spectral bases and corresponding acti-\nvations. With constraints such as sparsity [2], temporal\ncontinuity [3] and harmonicity [4], NMF provides a mean-\ningful mid-level representation (the activation matrix) for\ntranscription. A basic NMF is performed column by col-\numn, so NMF-based transcription systems usually pro-\nvide frame-wise representations with note transcription as\na post-processing step [5].\nOne direction of AMT is to focus on instrument-speciﬁc\nmusic, in order to make use of more information from in-\nstrumental physics and acoustics [5]. For piano sounds,\nseveral acoustics-associated features, such as inharmonic-\nity, time-varying timbre and decaying energy, are exam-\nined for their utilities in transcription. Rigaud et al. show\nc/circlecopyrtTian Cheng, Matthias Mauch, Emmanouil Benetos and\nSimon Dixon. Licensed under a Creative Commons Attribution 4.0 In-\nternational License (CC BY 4.0). Attribution: Tian Cheng, Matthias\nMauch, Emmanouil Benetos and Simon Dixon. “An Attack/Decay Model\nfor Piano Transcription”, 17th International Society for Music Informa-\ntion Retrieval Conference, 2016.\nAttack activations\n2 4 6 8 10\nTime [s]30405060708090100MIDI indexDecay activations\n2 4 6 8 10\nTime [s]30405060708090100MIDI index\nDetected notes (after offset detection)\n2 4 6 8 10\nTime [s]30405060708090100MIDI indexGround truth\n2 4 6 8 10\nTime [s]30405060708090100MIDI indexFigure 1 : An example of output from the proposed model.\nthat an explicit inharmonicity model leads to improvement\nin piano transcription [6], while a note-dependent inhar-\nmonicity parameter is needed for initialisation. Modelling\ntime-varying timbre not only provides a better reconstruc-\ntion of the spectrogram, but also improves note tracking re-\nsults by imposing constraints between note stages (attack,\nsustain and decay) [7, 8]. For decaying energy, Chen et\nal.’s preliminary work uses an exponential model for en-\nergy evolution of notes [9]. Berg-Kirkpatrick et al. rep-\nresent the energy evolution of a piano note by a trained\nenvelope [10]. Cogliati and Duan use a sum of two de-\ncaying exponentials to approximate decays of piano par-\ntials [11]. Ewert et al. represent both time-varying timbre\nand temporal evolution of piano notes by time-frequency\npatches [12]. Temporal evolution modelling allows a note\nevent to be represented by a single amplitude parameter for\nits whole duration, enabling the development of note-level\nsystems with promising transcription results [9, 10, 12].\nThe proposed method is also motivated by piano acous-\ntics. Based on our previous studies on piano decay, we\nknow that exponential decay explains the major energy\nevolution for each partial in spite of various decay pat-\nterns [13]. Here, we further simplify the decay stage us-\ning an exponential decay function and a harmonic template\nper pitch. We separately represent the attack stage for the\npercussive onset of piano sounds. These two stages are\ncoupled by shared note activations. A supervised NMF\nframework is used to estimate note activations, and hence\nactivations of the attack and decay stages (see Figure 1).\nWe detect note onsets by peak-picking on attack activa-\ntions, then offsets for each pitch individually. Experiments584show that the proposed method signiﬁcantly improves su-\npervised piano transcription, and compares favourably to\nother state-of-the-art techniques.\nThe proposed method is explained in Section 2. The\ntranscription and comparison experiments are described in\nSection 3. Conclusions and discussions are drawn in Sec-\ntion 4.\n2. METHOD\nIn this section we ﬁrst introduce the attack and decay\nmodel for piano sounds. Parameters are estimated using\na sparse NMF. Then we explain onset and offset detection\nmethods, respectively.\n2.1 A model of attack and decay\nA piano sound is produced by a hammer hitting the\nstring(s) of a key. It starts with a large energy, then decays\ntill the end of the note. At the attack stage, the strike of the\nhammer produces a percussive sound. It evolves quickly to\nan almost harmonic pitched sound, and then immediately\nenters the decay stage. Considering the different spectral\nand temporal features, we reconstruct these two phases in-\ndividually. The attack sound is generated by:\nVa\nft=K/summationdisplay\nk=1Wa\nfkHa\nkt, (1)\nwhereVais the reconstructed spectrogram of the attack\nphase, as shown in Figure 2(d), and Wais the percussive\ntemplate (Figure 2(e)). f∈[1,F]is the frequency bin, t∈\n[1,T]indicates the time frame, and k∈[1,K]is the pitch\nindex. Attack activations Ha(Figure 2(c)) are formulated\nby the convolution as follows:\nHa\nkt=t+Tt/summationdisplay\nτ=t−TtHkτP(t−τ), (2)\nwhereHare spike-shaped note activations, shown in Fig-\nure 2(b). Pis the transient pattern, and its typical shape is\nshown in Figure 5. The range of the transient pattern is de-\ntermined by the overlap in the spectrogram, with Ttequal\nto the ratio of the window size and frame hop size.\nFor the decay part we assume that piano notes decay\napproximately exponentially [13,14]. The harmonic decay\nis generated by\nVd\nft=K/summationdisplay\nk=1Wd\nfkHd\nkt, (3)\nwhereVdis the reconstructed spectrogram of the decay\nphase (Figure 2(g)), and Wdis the harmonic template\n(Figure 2(h)). Decay activations Hdin Figure 2(f) are gen-\nerated by convolving activations with an exponentially de-\ncaying function:\nHd\nkt=t/summationdisplay\nτ=1Hkτe−(t−τ)αk, (4)\nwhereαkare decay factors, and eαkindicates the decay\nrate per frame for pitch k. Offsets are not modelled; instead\n(a) Spectrogram\n02468Frequency [kHz]\n00.51Amplitude(b) Note activations\n00.51Amplitude(c) Attack activations\n(d) Reconstruction for Attack\n02468Frequency [kHz]\n0246810Frequency [kHz]\n-40 -20 0 20\nAmplitude [dB](e) Percussive template\n00.51Amplitude(f) Decay activations\n(g) Reconstruction for Decay\n02468Frequency [kHz]\n0246810Frequency [kHz]\n-40 -20 0 20\nAmplitude [dB](h) Harmonic template\n(i) Reconstruction\n0.5 1 1.5 2 2.5 3 3.5 4\nTime [s]02468Frequency [kHz]Figure 2 : An illustration of the proposed model (note D3\nwith the MIDI index of 50).\nit is assumed that the energy of a note decays forever. Then\nthe complete model is formulated as follows:\nVft=Va\nft+Vd\nft\n=K/summationdisplay\nk=1Wa\nfkt+Tt/summationdisplay\nτ=t−TtHkτP(t−τ)\n+K/summationdisplay\nk=1Wd\nfkt/summationdisplay\nτ=1Hkτe−(t−τ)αk,(5)\nwhereVis the reconstruction of the whole note, as shown\nin Figure 2(i).\nParametersθ∈{Wa,Wd,H,P,α}are estimated by\nminimising the difference between the spectrogram Xand\nthe reconstruction Vby multiplicative update rules [15].\nThe derivative of the cost function Dwith respect to θis\nwritten as a difference of two non-negative functions:\n∇θD(θ) =∇+\nθD(θ)−∇−\nθD(θ). (6)\nThe multiplicative algorithm is given by\nθ←θ.∇−\nθD(θ)./∇+\nθD(θ). (7)\nWe employ the β-divergence as the cost function. The full\nupdate equations are provided online.1\n1https://code.soundsoftware.ac.uk/projects/\ndecay-model-for-piano-transcription .Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 5857 8 9 10 11 12 13 14\nTime [s]00.020.040.060.080.1Amplitude(a) Note activations\n7 8 9 10 11 12 13 14\nTime [s]00.10.20.3Amplitude(b) Attack activations\nSmoothed attack activaitons\nSmoothed attack activations + threshold\nOnset candidates\n7 8 9 10 11 12 13 14\nTime [s]00.10.20.3Amplitude(c) Detected onsetsFigure 3 : Example of onset detection showing how activa-\ntions are processed.\n2.2 Sparsity\nTo ensure spike-shaped note activations, we simply impose\nsparsity on activations Husing element-wise exponentia-\ntion after each iteration:\nH=Hγ, (8)\nwhereγis the sparsity factor, usually larger than 1. The\nlarger the factor is, the sparser the activations are.\nA preliminary test conﬁrmed that the number of peaks\nin activations decreases as the degree of sparsity increases.\nWe also apply an annealing sparsity factor [16], which\nmeans a continuously changing factor. In this paper, we set\nγto increase from 1 to γa∈[1.01,1.05]gradually within\nthe iterations.\n2.3 Onset detection\nDifferent playing styles and overlapping between notes\nmay cause a mismatch between the observed attack energy\nand the trained transient pattern. This results in multiple\npeaks around onsets in the activations. Figures 3(a) and\n(b) show note activations and attack activations of pitch\nG2 in a music excerpt, respectively. Attack activations in-\ndicate the actual transient patterns of notes obtained by the\nproposed model. Therefore, we detect onsets from attack\nactivations by peak-picking. First, we compute smoothed\nattack activations for each pitch, using a moving average\nﬁlter with a window of 20 bins. Only peaks which exceed\nsmoothed attack activations by a threshold will be detected\nas onset candidates, as shown in Figure 3(b). The threshold\nis adapted to each piece with the parameter δ:\nThre =δmax\nk,tHa\nk,t. (9)\nWe test various δ∈{− 21dB,−22dB,...,−40dB}in this\npaper.\nWe ﬁnd that there are still double peaks around onsets\nafter thresholding. In order to deal with this problem, we\n0 5 10 15 20 25 3005001000(a) Costs of two states\nCosts for state OFF\nCosts for state ON\n0 5 10 15 20 25 3000.51(b) Normalised costs\n0 5 10 15 20 25 30\nTime [s]00.511.5(c) Segments\nDetected onsetsFigure 4 : Costs and segments for pitch F3 (MIDI index\n53).\nsimply merge pairs of peaks which are too close to each\nother. We set the minimal interval between two successive\nnotes of the same pitch to be 0.1 second. If the interval\nbetween two peaks is smaller than the minimal interval,\nwe generate a new peak. The index of the new peak is a\nweighted average of the indices of the two peaks, while its\namplitude is the sum of that of the two peaks. Figure 3(c)\nshows detected onsets after merging double peaks. We ap-\nply the above process again to get rid of triple peaks.\n2.4 Offset detection\nWe adapt the method of [12] to detect the offsets by dy-\nnamic programming. For each pitch, there are two states\ns∈{0,1}, denoting state ‘off’ and ‘on’ respectively. The\ncosts are deﬁned below:\nCk(s,t) =/braceleftBigg/summationtextF\nf=1DKL(Xft,Vft−Vk\nft), s= 0/summationtextF\nf=1DKL(Xft,Vft), s = 1(10)\nwhereVkis the reconstruction of pitch k, andV−Vkis\nthe reconstruction excluding pitch k.DKL(a,b)denotes\nthe KL-divergence between aandb. Then we normalise\nthe costs per pitch to sum to 1 in all frames: /tildewiderCk(s,t) =\nCk(s,t)//summationtext\n˜sCk(˜s,t). Figures 4 (a) and (b) show the costs\nand normalised costs for pitch F3 in a music piece, respec-\ntively.\nWe can ﬁnd the optimal state sequence by applying dy-\nnamic programming on the normalised costs. To do this,\nwe need an accumulated cost matrix and a step matrix to\nstore the smallest accumulated costs and previous states.\nThe accumulated cost matrix Dkis recursively deﬁned as\nDk(s,t) =\n/braceleftBigg\nmin ˜s∈{0,1}(Dk(˜s,t−1) +/tildewiderCk(s,t)w(˜s,s)), t> 1\n/tildewiderCk(s,t), t = 1(11)\nwherewis the weight matrix, which favours self-\ntransitions, in order to obtain a smoother sequence. In this\npaper, the weights are [0.5,0.55; 0.55,0.5]. The step ma-\ntrix E is deﬁned as follows:\nEk(s,t) = arg min\n˜s∈{0,1}(Dk(˜s,t−1) +/tildewiderCk(s,t)w(˜s,s)),t> 1\n(12)586 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016The states are given by\nSk(t) =/braceleftBigg\narg min ˜s∈{0,1}Dk(˜s,t), t=T\nEk(Sk(t+ 1),t+ 1), t∈[1,T−1](13)\nWe ﬁnd that when the activation of the pitch is 0 or very\nsmall, the costs of two states are the same or very close,\nand no state transition occurs. In these parts, the pitch\nstate is off, while dynamic programming can not jump out\nfrom the previous state. In order to deal with this problem\nwe need to exclude these parts before applying dynamic\nprogramming. Figure 4(c) shows the segmentation by de-\ntected onsets and the costs. Each segment starts at a de-\ntected onset and ends when the difference of the smoothed\nnormalised costs is less than a set threshold. We track the\nstates of the pitch for each segment individually.\n3. EXPERIMENTS\nIn the experiments we ﬁrst analyse the proposed model’s\nperformance on music pieces produced by a real piano\nfrom the MAPS database [17]. Then we compare to three\nstate-of-the-art transcription methods on this dataset and\ntwo other synthetic datasets.\nTo compute the spectrogram, frames are segmented\nby a 4096-sample Hamming window with a hop-size of\n882.2A discrete Fourier Transform is performed on each\nframe with 2-fold zero-padding. Sample frequency fsis\n44100Hz. To lessen the inﬂuence of beats in the decay\nstage [13], we smooth the spectrogram with a median ﬁlter\ncovering 100ms. During parameter estimation, we use the\nKL-divergence ( β= 1) as the cost function. The proposed\nmodel is iterated for 50 times in all experiments to achieve\nconvergence.\nSystems are evaluated by precision ( P), recall (R) and\nF-measure (F), deﬁned as:\nP=Ntp\nNtp+Nfp, R=Ntp\nNtp+Nfn, F= 2×P×R\nP+R,\nwhereNtp, Nfp, Nfnare the numbers of true positives,\nfalse positives and false negatives, respectively. In addi-\ntion, we use the accuracy in [18] to indicate the overall\naccuracy:A=Ntp\nNtp+Nfp+Nfn.We employ both frame-wise\nand note-wise evaluation [19], denoted by subscript ‘ f’ and\n‘on’, respectively.\n3.1 Transcription experiment\nThe main transcription experiment is performed on the\n‘ENSTDkCL’ subset of the MAPS database [17]. The pi-\nano sounds of this subset are recorded on a Disklavier pi-\nano. We train percussive and harmonic templates, decay\nrates and the transient pattern on the isolated notes pro-\nduced by the same piano. The transcription experiment is\nrun on the music pieces using the ﬁrst 30s of each piece.3\n2A 20ms hop size is used to reduce computation time. For frame-wise\nevaluation, transcription results are represented with a hop size of 10ms\nby duplicating every frame.\n3The proposed model runs at about 3 ×real-time using MATLAB on\na MacBook Pro laptop (I7, 2.2GHz, 16GB).\n-4 -2 0 2 4\nFrame index00.20.40.60.81AmplitudeTransient patterns for all notes\n-4 -2 0 2 4\nFrame index00.20.40.60.81AmplitudeAverage transient patternFigure 5 : Transient patterns.\nTable 1 : Note tracking results with different ﬁxed sparsity\nfactors (above) and annealing sparsity factors (below).\nγ P onRonFonAonδ(dB)\n1.00 88.52 77.70 82.24 70.54 -29\n1.01 87.70 78.18 82.23 70.53 -30\n1.02 87.67 77.36 81.80 69.87 -30\n1.03 87.22 77.31 81.62 69.66 -31\n1.04 86.95 76.84 81.26 69.17 -32\n1.05 86.38 75.99 80.51 68.14 -33\n1→1.01 87.77 78.08 82.18 70.49 -30\n1→1.02 88.49 77.79 82.36 70.73 -30\n1→1.03 88.22 77.78 82.27 70.60 -31\n1→1.04 87.86 77.66 82.09 70.35 -32\n1→1.05 86.83 77.83 81.76 69.84 -34\n3.1.1 The training stage\nThe training stage includes two rounds. In the ﬁrst round,\nwe ﬁrst ﬁx note activations ( H) for each isolated note ac-\ncording to the ground truth, then update all other param-\neters (Wa,Wd,Pandα). The transient patterns are nor-\nmalised to maximum of 1 after each iteration. In theory,\nthe transient patterns follow a certain shape and could be\nshared by all pitches. So we use the average of the trained\ntransient patterns to reduce the number of parameters and\nto avoid potential overﬁtting. The trained transient patterns\nand the average transient pattern are shown in Figure 5.\nIn the second round, we ﬁx the note activations ( H) and\nthe transient pattern ( P), then update all other parameters\n(Wa,Wdandα).\n3.1.2 Transcription results\nFor transcription, we update note activations H, keeping\nparameters ( Wa,Wd,Pandα) ﬁxed from the training\nstage. Table 1 shows note tracking results (presented as\npercentage) using different sparsity factors. The optimal\nthresholds are shown in the last column. The top part\nof Table 1 are results using ﬁxed sparsity factors. The\nbest results are achieved without the sparsity constraint\n(γ= 1.00), with an F-measure of 82.24%. The perfor-\nmance decreases with increasing sparsity factor. The sec-\nond part of the experiment gives results for using anneal-\ning sparsity. The best F-measure is 82.36% with the set-\nting ( 1.00→1.02). The difference between the best and\nthe worst F-measure is only 0.6 percentage points. In gen-\neral, all results with different sparsity constraints are con-\nsiderably good with optimal thresholds, and the optimal\nthreshold decreases when sparsity gets higher. However,\nF-measures considering both onsets and offsets are quite\nlow, around 40%.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 587In the proposed model, the activation of each note de-\ncays after its onset, as shown in the decay activations of\nFigure 1. Given a note is played, we consider two situa-\ntions. In the ﬁrst case, there is another note of the same\npitch being played later. We know that in this case the ﬁrst\nnote should be ended then. If the activation of the ﬁrst note\nhas already decreased to a low level, there is little inﬂu-\nence on detecting the second note. However, if these two\nnotes are very close, detection of the second note might\nbe missed because of the remaining activation of the ﬁrst\nnote. In the second case, there is another note of a different\npitch being played. The activation of the ﬁrst note won’t\nbe changed by the attack of the second note in our model,\nwhile for standard NMF, there is always some interference\nwith the ﬁrst note’s activation.\nWe compute the performance using thresholds ranging\nfrom−40to−21dB to study performance variations as a\nfunction of the threshold. Figure 6(a) shows the results for\ndifferent ﬁxed sparsity factors. It is clear that precision de-\ncreases with the increase of the threshold, while recall in-\ncreases. The higher the sparsity factor is, the more robust\nthe results are on threshold changes. This is because small\npeaks in activations are already discounted when impos-\ning sparsity, as shown in Figure 7. Lowering the threshold\ndoes not bring many false positives. Results with higher\nsparsity are less sensitive to the decrease of the threshold.\nHowever, when the threshold becomes larger, the results\nwith low sparsity still outperform those with high sparsity.\nWith a larger threshold, the number of true positives de-\ncreases. There are more peaks in activations when using\nlower sparsity, so more true positives remain. This favours\nthe assumption that the true positives have larger ampli-\ntudes. Figure 6(b) shows the robustness of using annealing\nsparsity factors. The transcription results are close to each\nother. With annealing sparsity, the results are better and\nmore tolerant to threshold changes.\n3.2 Comparison with state-of-the-art methods\nWe apply a comparison experiment on three datasets,\npieces from a real piano (‘ENSTDkCl’) and a synthetic\npiano (‘AkPnCGdD’) in the MAPS database [17], and an-\nother 10 synthetic piano pieces (denoted as ‘PianoE’) used\nin [12]. All experiments are performed on the ﬁrst 30s\nof each piece. We compare to two top transcription meth-\nods. Vincent et al. ’s method applies adaptive spectral bases\ngenerated by linear combinations of narrow-band spec-\ntra, so the spectral bases have a harmonic structure and\nthe ﬂexibility to adapt to different sounds [20]. Benetos\nand Weyde’s method employs 3 templates per pitch, and\nthe sequence of templates is constrained by a probabilis-\ntic model [21]. In the PianoE dataset, we also compare to\nanother state-of-the art method of Ewert et al. [12]. This\nmethod identiﬁes frames in NMD patterns with states in a\ndynamical system. Note events are detected with constant\namplitudes but various durations. In the comparison exper-\niment, the proposed system is also trained on isolated notes\nfrom the AkPnCGdD and PianoE pianos. Vincent et al. ’s\nmethod is performed in an unsupervised way, to indicate\n65 70 75 80 85 90 95 100\nPrecision60657075808590Recall1.00\n1.01\n1.02\n1.03\n1.04\n1.05F-measure = 85\nF-measure = 75(a) Results with ﬁxed sparsity factors\n65 70 75 80 85 90 95 100\nPrecision60657075808590Recall1->1.01\n1->1.02\n1->1.03\n1->1.04\n1->1.05F-measure = 85\nF-measure = 75\n(b) Results with annealing sparsity factors\nFigure 6 : Performance (presented percentage) using dif-\nferent sparsity factors and thresholds. The sparsity factors\nare indicated by different shapes, as shown in the top-right\nbox. Lines connecting different shapes are results achieved\nvia the same threshold. The threshold of the top set is\n−40dB , and the bottom set is −21dB . The dashed lines\nshow F-measure contours, with the values dropping from\ntop-right to bottom-left.\nwhat can be achieved without training datasets. We use the\nversion of Benetos and Weyde’s method from the MIREX\ncompetition [22]. We have access to the code and train this\nmodel on isolated notes of the corresponding pianos. For\nEwert’s method we only have access to the published data\nin [12]. These two methods are performed in a supervised\nway.\nBased on previous analysis, we employ the following\nparameters for the proposed model in comparison exper-\niments. The sparsity factor is γ= 1→1.04by bal-\nancing among note tracking results and the robustness to\ndifferent thresholds. Onsets are detected with threshold\nδ=−30dB . In the ﬁrst dataset (‘ENSTDkCl’), results\nof other methods are also reported with optimal thresholds\nwith best note-wise F-measures. Then the same thresholds\nare used for two synthetic piano datasets.\nResults on piano pieces from the ‘ENSTDkCl’ sub-\nset are shown in Table 2(a). The proposed model has a\nnote tracking F-measure of 81.80% and a frame-wise F-\nmeasure of 79.01%, outperforming Vincent et al. ’s unsu-\npervised method by around 10 and 20 percentage points,588 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20161 2 3 4 5 6 7 8 9 10\nTime [s]00.511.52Amplitude(a) Attack activations (sparsity = 1.01)\nAttack activations\nGround Truth\nDetected onsets\n1 2 3 4 5 6 7 8 9 10\nTime [s]00.511.52Amplitude(b) Attack activations (sparsity = 1.05)\nAttack activations\nGround Truth\nDetected onsetsFigure 7 : Detected onsets with different sparsity for pitch\nG4 (MIDI index 67).\nrespectively. Results of Benetos and Weyde’s method are\nin between.\nResults on the synthetic piano ‘AkPnCGdD’ are shown\nin Table 2(b). In general, all methods perform better on\nthis dataset than on the ‘ENSTDkCl’ dataset, especially on\nnote tracking results. The proposed model has the best re-\nsults ( 84.63% on note tracking F-measure and 80.81% on\nframe-wise F-measure), outperforming all other methods\nby at least 5 percentage points.\nResults on the other synthetic dataset ‘PianoE’ are\nshown in Table 2(c). Note tracking results of all meth-\nods are good but frame-wise results are poor. Ewert et\nal.’s method performs the best on note tracking ( 88% on F-\nmeasure), and Benetos and Weyde’s method is the second\n(83.80% on F-measure). The proposed model only outper-\nforms Vincent et al. ’s method, with F-measures of 81.28%\nand79.41% for these two methods respectively. However,\nthe proposed model remains the best on the frame-wise F-\nmeasure ( 66.77%). Pieces in this dataset are from a piano\ncompetition. Many notes have very short durations. The\nremaining energies of a short note in the proposed model\nmay interfere with later notes, causing false negatives.\nA supervised neural network model also works on the\nMAPS database for piano transcription [23]. Besides an\nacoustic model, the method employs a music language\nmodel to capture the temporal structure of music. Al-\nthough the method is not directly comparable, it is no-\nticeable that our method exceeds its results by at least 5\npercentage points on F-measures. When tested on the real\nrecordings using templates trained in the synthetic piano\nnotes, the proposed method has both F-measures of around\n65%, outperforming the method of [23] by 10 percentage\npoints on note-wise F-measure in a similar experiment.\n4. DISCUSSION AND CONCLUSION\nIn this paper we propose a piano-speciﬁc transcription sys-\ntem. We model a piano note as a percussive attack stage\nand a harmonic decay stage, and the decay stage is ex-\nplicitly modelled as an exponential decay. Parameters are\nlearned in a sparse NMF, and transcription is performed inTable 2 : The comparison experiment\n(a) Transcription results on ‘ENSTDkCl’\nMethod FonAonFfAf\nDecay 81.80 69.94 79.01 65.89\nVincent [20] 72.15 57.45 58.84 42.71\nBenetos [21] 73.61 59.73 67.79 52.15\n(b) Transcription results on ‘AkPnCGdD’\nMethod FonAonFfAf\nDecay 84.63 74.03 80.81 68.39\nVincent [20] 79.86 67.32 69.76 55.17\nBenetos [21] 74.05 59.57 53.94 38.65\n(c) Transcription results on ‘PianoE’\nMethod FonAonFfAf\nDecay 81.28 69.12 66.77 51.63\nVincent [20] 79.41 66.39 58.59 42.45\nBenetos [21] 83.80 72.82 60.69 44.24\nEwert [12] 88 - - -\na supervised way. The proposed model provides promising\ntranscription results, with around 82% and79% for note\ntracking and frame-wise F-measures in music pieces from\na real piano in the ‘ENSTDkCl’ dataset. The annealing\nsparsity factor improves both performance and the robust-\nness of the proposed model. The comparison experiment\nshows that the proposed model outperforms two state-of-\nthe-art methods by a large margin on real and synthetic\npianos in the MAPS database. On a different synthetic\ndataset, the other methods performs relatively better, espe-\ncially on note tracking, while the proposed method remains\nbest on frame-wise metrics.\nThe proposed model can also be understood as a decon-\nvolution method in which a patch is parameterised by two\nsets of templates and activations. One advantage of the\nproposed model is that we can build a note-level system by\ndeconvolution, which has provided good transcription re-\nsults [9, 10, 12]. The other is that using parametric patches\nreduces the number of parameters. The model also pro-\nvides us with a way to analyse piano decay rates.\nIn the future, we would like to represent a note’s de-\ncay stage by a decay ﬁlter instead of a decay rate, which\nis more in line with studies on piano decay [13]. Sec-\nondly, the good performance on piano music transcription\nis partly due to the availability of the training datasets. We\nwould like to build an adaptive model, which could work\nin a more general scenario, hence more automatically. Fi-\nnally, we are keen to ﬁnd a way to estimate note offsets\nmore accurately in the proposed model.\n5. ACKNOWLEDGEMENT\nTC is supported by a China Scholarship Council/Queen\nMary Joint PhD Scholarship. EB is supported by a Royal\nAcademy of Engineering Research Fellowship (grant no.\nRF/128). We would like to thank Dr. Sebastian Ewert for\nhis comments on this paper.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 5896. REFERENCES\n[1] P. Smaragdis and J. C. Brown. Non-negative matrix\nfactorization for polyphonic music transcription. In\nProc. IEEE WASPAA , pages 177–180, 2003.\n[2] A. Cont. Realtime multiple pitch observation using\nsparse non-negative constraints. In Proc. ISMIR , pages\n206–211, 2006.\n[3] T. Virtanen. Monaural sound source separation by non-\nnegative matrix factorization with temporal continuity\nand sparseness criteria. IEEE Trans. on Audio, Speech\nand Language Processing , 15(3):1066–1074, 2007.\n[4] N. Bertin, R. Badeau, and E. Vincent. Enforcing har-\nmonicity and smoothness in bayesian non-negative ma-\ntrix factorization applied to polyphonic music tran-\nscription. IEEE Trans. on Audio, Speech and Language\nProcessing , 18(3):538–549, 2010.\n[5] E. Benetos, S. Dixon, D. Giannoulis, H. Kirchhoff, and\nA. Klapuri. Automatic music transcription: challenges\nand future directions. Journal of Intelligent Informa-\ntion Systems , 41(3):407–434, 2013.\n[6] F. Rigaud, A. Falaize, B. David, and L. Daudet. Does\ninharmonicity improve an NMF-based piano transcrip-\ntion model? In Proc. ICASSP , pages 11–15, 2013.\n[7] E. Benetos and S. Dixon. Multiple-instrument poly-\nphonic music transcription using a temporally con-\nstrained shift-invariant model. The Journal of the\nAcoustical Society of America , 133(3):1727–1741,\n2013.\n[8] T. Cheng, S. Dixon, and M. Mauch. Improving piano\nnote tracking by HMM smoothing. In Proc. EUSIPCO ,\npages 2009–2013, 2015.\n[9] Z. Chen, G. Grindlay, and D. Ellis. Transcribing\nmulti-instrument polyphonic music with transformed\neigeninstrument whole-note templates. In MIREX ,\n2012.\n[10] T. Berg-Kirkpatrick, J. Andreas, and D. Klein. Un-\nsupervised transcription of piano music. In Advances\nin Neural Information Processing Systems 27 , pages\n1538–1546. 2014.\n[11] A. Cogliati and Z. Duan. Piano music transcription\nmodeling note temporal evolution. In Proc. ICASSP ,\npages 429–433, 2015.\n[12] S. Ewert, M. D. Plumbley, and M. Sandler. A dynamic\nprogramming variant of non-negative matrix deconvo-\nlution for the transcription of struck string instruments.\nInProc. ICASSP , pages 569–573, 2015.\n[13] T. Cheng, S. Dixon, and M. Mauch. Modelling the de-\ncay of piano sounds. In Proc. ICASSP , pages 594–598,\n2015.[14] G. Weinreich. Coupled piano strings. The Journal of\nthe Acoustical Society of America , 62(6):1474–1484,\n1977.\n[15] D. D. Lee and H. S. Seung. Algorithms for non-\nnegative matrix factorization. In Proc. Advances in\nNeural Information Processing Systems 13 , pages 556–\n562, 2000.\n[16] T. Cheng, S. Dixon, and M. Mauch. A deterministic\nannealing EM algorithm for automatic music transcrip-\ntion. In Proc. ISMIR , pages 475–480, 2013.\n[17] V . Emiya, R. Badeau, and B. David. Multipitch estima-\ntion of piano sounds using a new probabilistic spectral\nsmoothness principle. IEEE Trans. on Audio, Speech,\nand Language Processing , 18(6):1643–1654, 2010.\n[18] S. Dixon. On the computer recognition of solo piano\nmusic. In Proc. Australasian Computer Music Confer-\nence, pages 31–37, 2000.\n[19] M. Bay, A. F. Ehmann, and J. S. Downie. Evaluation of\nmultiple-F0 estimation and tracking systems. In Proc.\nISMIR , pages 315–320, 2009.\n[20] E. Vincent, N. Bertin, and R. Badeau. Adaptive har-\nmonic spectral decomposition for multiple pitch esti-\nmation. IEEE Trans. on Audio, Speech and Language\nProcessing , 18(3):528–537, 2010.\n[21] E. Benetos and T. Weyde. An efﬁcient temporally-\nconstrained probabilistic model for multiple-\ninstrument music transcription. In Proc. ISMIR ,\npages 701–707, 2015.\n[22] E. Benetos and T. Weyde. Multiple-F0 estimation and\nnote tracking for MIREX 2015 using a sound state-\nbased spectrogram factorization model. In MIREX ,\n2015.\n[23] S. Sigtia, E. Benetos, and S. Dixon. An end-to-end neu-\nral network for polyphonic piano music transcription.\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing , 24(5):927–939, 2016.590 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "An Ontology for Audio Features.",
        "author": [
            "Alo Allik",
            "György Fazekas",
            "Mark B. Sandler"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416226",
        "url": "https://doi.org/10.5281/zenodo.1416226",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/077_Paper.pdf",
        "abstract": "A plurality of audio feature extraction toolsets and feature datasets are used by the MIR community. Their differ- ent conceptual organisation of features and output formats however present difficulties in exchanging or comparing data, while very limited means are provided to link features with content and provenance. These issues are hindering research reproducibility and the use of multiple tools in combination. We propose novel Semantic Web ontologies (1) to provide a common structure for feature data formats and (2) to represent computational workflows of audio fea- tures facilitating their comparison. The Audio Feature On- tology provides a descriptive framework for expressing dif- ferent conceptualisations of and designing linked data for- mats for content-based audio features. To accommodate different views in organising features, the ontology does not impose a strict hierarchical structure, leaving this open to task and tool specific ontologies that derive from a com- mon vocabulary. The ontologies are based on the analy- sis of existing feature extraction tools and the MIR litera- ture, which was instrumental in guiding the design process. They are harmonised into a library of modular interlinked ontologies that describe the different entities and activities involved in music creation, production and consumption.",
        "zenodo_id": 1416226,
        "dblp_key": "conf/ismir/AllikFS16",
        "content": "AN ONTOLOGY FOR AUDIO FEATURES\nAlo Allik, Gy ¨orgy Fazekas, Mark Sandler\nQueen Mary University of London\na.allik, g.fazekas, mark.sandler@qmul.ac.uk\nABSTRACT\nA plurality of audio feature extraction toolsets and feature\ndatasets are used by the MIR community. Their differ-\nent conceptual organisation of features and output formats\nhowever present difﬁculties in exchanging or comparing\ndata, while very limited means are provided to link features\nwith content and provenance. These issues are hindering\nresearch reproducibility and the use of multiple tools in\ncombination. We propose novel Semantic Web ontologies\n(1) to provide a common structure for feature data formats\nand (2) to represent computational workﬂows of audio fea-\ntures facilitating their comparison. The Audio Feature On-\ntology provides a descriptive framework for expressing dif-\nferent conceptualisations of and designing linked data for-\nmats for content-based audio features. To accommodate\ndifferent views in organising features, the ontology does\nnot impose a strict hierarchical structure, leaving this open\nto task and tool speciﬁc ontologies that derive from a com-\nmon vocabulary. The ontologies are based on the analy-\nsis of existing feature extraction tools and the MIR litera-\nture, which was instrumental in guiding the design process.\nThey are harmonised into a library of modular interlinked\nontologies that describe the different entities and activities\ninvolved in music creation, production and consumption.\n1. INTRODUCTION\nSeveral content based audio feature extraction frameworks\nand toolsets have been developed over the past decades of\nMIR research aiming to provide a platform for distributing,\nsharing or deploying algorithms. While most tools have the\npotential to become widely adopted common platforms, it\nis most likely that a plurality of them will continue to be\nused by the community as well as adopters of MIR technol-\nogy. However, diverging conceptual organisation of fea-\ntures and different output formats present difﬁculties when\nit comes to exchanging and comparing data, or producing\nannotated datasets. These issues are hindering research re-\nproducibility as well as the use of multiple data or tool sets\nin a single application or experiment.\nA growing demand for shared representations of com-\nputational extraction workﬂows and interoperable data for-\nmats is signiﬁed by several proposed formats, some of\nc/circlecopyrtAlo Allik, Gy ¨orgy Fazekas, Mark Sandler. Licensed un-\nder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Alo Allik, Gy ¨orgy Fazekas, Mark Sandler. “ An\nontology for audio features ”, 17th International Society for Music Infor-\nmation Retrieval Conference, 2016.which are associated with feature extractor tools or ser-\nvices [3, 4, 8, 13, 19]. While existing formats for structur-\ning and exchanging content-based audio feature data may\nsatisfy tool or task speciﬁc requirements, there are still sig-\nniﬁcant limitations in linking features produced in different\ndata sources, as well as in providing generalised descrip-\ntions of audio features that would allow easier identiﬁca-\ntion and comparison of algorithms that produce the data.\nSemantic Web technologies facilitate formal descrip-\ntions of concepts, terms and relationships that enable im-\nplementations of automated reasoning and data aggre-\ngation systems to manage large amounts of information\nwithin a knowledge domain. Both in research and com-\nmercial use cases, it is becoming increasingly important\nto fuse cultural, contextual and content-based information.\nThis may be achieved by leveraging Linked Data enabled\nby the use of shared ontologies and unique identiﬁcation of\nentities. This not only offers the potential to simplify ex-\nperiments and increase productivity in research activities\ntraditionally relying on Web scraping, proprietary applica-\ntion programming interfaces or manual data collection, but\nalso enables incorporation of increasingly larger and more\ncomplex datasets into research workﬂows.\nWe propose a modular approach towards ontological\nrepresentation of audio features. Since there are many\ndifferent ways to structure features depending on a spe-\nciﬁc task or theoretically motivated organising principle, a\ncommon representation would have to account for multi-\nple conceptualisations of the domain and facilitate diverg-\ning representations of common features. This may be due\nto the “semantic gap” between low-level computational\nrepresentations of audio signals and theoretical represen-\ntations founded in acoustics or musicology. This semantic\ngap could potentially be bridged using Semantic Web tech-\nnologies if high-level feature identiﬁcation can be inferred\nfrom computational signatures. However, this function-\nality is currently beyond the scope of existing technolo-\ngies. For example, Mel Frequency Cepstral Coefﬁcients\n(MFCC), which are widely calculated in many tools and\nworkﬂows, can be categorised as a “timbral” feature in\nthe psychoacoustic or musicological sense, while from the\ncomputational point of view, MFCC could be labelled as\na “cepstral” or “spectral” representation. The complexity\narising from this makes music somewhat unique calling for\na robust ontological treatment, although ontological repre-\nsentation of content-based features have been proposed in\nother domains including image processing [23].\nOur framework consists of two separate components to73distinguish between the abstract concepts describing the\naudio feature domain and more concrete classes that repre-\nsent speciﬁc audio features and their computational signa-\ntures. The Audio Feature Ontology (AFO) is the more ab-\nstract component representing entities in the feature extrac-\ntion process on different levels of abstraction. It describes\nthe structure of processes in feature extraction workﬂow\nthrough phases of conceptualisation, modelling and imple-\nmentation. The Audio Feature V ocabulary (AFV) then lists\nexisting audio features providing the terms for tool and\ntask speciﬁc ontologies without attempting to organise the\nfeatures into a taxonomy.\n2. BACKGROUND\nMany recent developments in audio feature data for-\nmats employ JavaScript Object Notation (JSON), which\nis rapidly becoming a ubiquitous data interchange mecha-\nnism in a wide range of systems regardless of domain. Par-\nticularly, the JSON Annotated Music Speciﬁcation (JAMS)\n[8] is a notable attempt to provide meaningful structure to\naudio feature data while maintaining simplicity and sus-\ntainability in representations, which the authors deem as\nthe most crucial factors for wider adoption in the MIR\ncommunity. A different JSON format for features has been\ndeveloped in the AcousticBrainz project [19] with the in-\ntention of making available low and high level audio fea-\ntures for millions of music tracks. This resource provides\nthe largest feature dataset to date while exposing the ex-\ntraction algorithms in an open source environment.\nIt is evident that the simplicity of JSON combined\nwith its structuring capabilities make it an attractive op-\ntion, particularly compared to preceding alternatives in-\ncluding YAML, XML, Weka Attribute-Relation File For-\nmat (ARFF), the Sound Description Interchange Format\n(SDIF), and various delimited formats. All these formats\nenable communication between various workﬂow compo-\nnents and offer varying degrees of ﬂexibility and expres-\nsivity. However, even the most recent JSON methods only\nprovide a structured representation of feature data with-\nout the facility of linking these concepts semantically to\nother music related entities or data sources. For example,\nthe JAMS deﬁnition does not address methodology that\nwould enable detailed description and comparison of au-\ndio features nor does it provide a structured way of linking\nthe feature data to the rest of the available metadata for\na particular music track. Admittedly, the AcousticBrainz\ndataset does provide links to the MusicBrainz1database\nby global identiﬁers, but there is no mechanism to identify\nthe features or compare them to those available in other\nextraction libraries. The Essentia library [3] that is used in\nthe AcousticBrainz infrastructure for feature extraction is\nopen source, thus providing access to the algorithms, but\nthere is no formalised description of audio features beyond\nsource code and documentation yet. Other feature extrac-\ntion frameworks provide data exchange formats designed\nfor particular workﬂows or speciﬁc tools. However, there\nis no common format shared by all the different tools and\n1http://musicbrainz.orglibraries. The motley of output formats is well demon-\nstrated in the representations category of a recent evalu-\nation of feature extraction toolboxes [15]. For example,\nthe popular MATLAB MIR Toolbox export function out-\nputs delimited ﬁles as well as ARFF, while Essentia pro-\nvides YAML and JSON and the YAAFE library outputs\nCSV and HDF5. The MPEG-7 standard, used as bench-\nmarks for other extraction tools provides an XML schema\nfor a set of low-level descriptors, but the deﬁciencies high-\nlighted above also apply in this case.\nSemantic Web technologies provide domain modelling\nand linking methods considerably beyond the expressiv-\nity and interoperability of any of the solutions described\nabove. The OWL family of ontology languages is designed\nto be ﬂexible enough to deal with heterogeneous Web-\nbased data sources. It is also built on strong logical founda-\ntions. It implies a conceptual difference between develop-\ning data formats and ontological modelling. The authors\nof [8] mention a common criticism that RDF-based MIR\nformats similarly to XML suffer from being non-obvious,\nverbose or confusing. However, the potential of meaning-\nful representation of audio features and linking ability to\nother music related information outweighs these concerns.\nOntological representation and linking of divergent do-\nmains is a difﬁcult task, but should not be discarded lightly\nin favour of simplicity. The beneﬁts of even relatively lim-\nited Semantic Web technologies for MIR research have\nbeen demonstrated on a number of occasions. For exam-\nple, the proof-of-concept system described in [17] enables\nincreased automation and simpliﬁcation of research work-\nﬂows and encourages resource reuse and validation by\ncombining several existing ontologies and Semantic Web\nresources, including the Music Ontology2, GeoNames3,\nDBTune4, and the Open Archives Initiative Object Reuse\nand Exchange (OAI-ORE). A system for MIR workﬂow\npreservation has been proposed in [12], which emphasises\nthe importance of representing and preserving the context\nof entire research processes and describes a Context Model\nof a typical MIR workﬂow as a Semantic Web ontology.\nThe original version of the Audio Feature Ontology\nwas created within a framework of a harmonised library\nof modular music-related Semantic Web ontologies [4],\nbuilt around the core Music Ontology [20]. This library\nrelies on widely adopted Semantic Web ontologies such as\nthe Friend of a Friend (FOAF) vocabulary, as well as do-\nmain speciﬁc ontologies for describing intellectual works\n(FRBR) and complex associations of domain objects with\ntime-based events (Event and Timeline ontologies). The\nlibrary also provides a set of extensions describing mu-\nsic speciﬁc concepts including music similarity [9] and the\nproduction of musical works in the recording studio [5].\nSince its publication, it has been integrated in several re-\nsearch projects, including the Networked Environment for\nMusic Analysis (NEMA) [24], the Computational Anal-\nysis of the Live Music Archive (CALMA) [2] as well as\ncommercial applications, e.g. the BBC and its music Web-\n2http://musicontology.com/\n3http://www.geonames.org/\n4http://dbtune.org/74 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016site5. This ontology provides a model for structuring\nand publishing content-derived information about audio\nrecordings and allows linking this information to concepts\nin the Music Ontology framework. However, it does not\nprovide a comprehensive vocabulary of audio features or\ncomputational feature extraction workﬂows. It also lacks\nconcepts to support development of more speciﬁc feature\nextraction ontologies. Structurally, it conﬂates musicolog-\nical and computational concepts to an extent that makes it\ninﬂexible for certain modelling requirements as suggested\nin [7]. In order to address these issues, the updated Au-\ndio Feature Ontology separates abstract ontological con-\ncepts from more speciﬁc vocabulary terminology, supplies\nmethodology for extraction workﬂow descriptions, and in-\ncreases ﬂexibility for modelling of task and tool speciﬁc\nontologies.\n3. ONTOLOGY ENGINEERING\nIn order to gain a better understanding of the MIR do-\nmain and user needs, a catalogue of audio features was\nﬁrst compiled based on a review of relevant literature, ex-\nisting feature extraction tools and research workﬂows. The\nﬁrst phase of this process involved extracting information\nabout features from journal articles, source code and ex-\nisting structured data sources. This information was sub-\nsequently collated into a linked data resource to serve as\na foundation for the ontology engineering process. There\nwas no attempt at explicit classiﬁcation of features into\na hierarchical taxonomy. Source code was parsed from\na number of open source feature extraction packages in-\ncluding CLAM [1], CoMIRV A [21] jMIR (jAudio) [13],\nLibXtract6, Marsyas7, Essentia [3] and YAAFE [11]. Ex-\nisting linked resource representations of the Vamp plug-\nins provided easy access to all the features available for\ndownload on the Vamp plugins Website8. Manual extrac-\ntion was used for the packages which did not provide suit-\nable access for automatic parsing or which were reviewed\nin journal articles, including the Matlab toolboxes (MIR\nToolbox and Timbre Toolbox), Aubio9, Cuidado [18],\nPsySound310, sMIRk [6], SuperCollider SCMIR toolkit,\nand some of the more recent MIREX submissions. A sim-\nple automatic matching procedure was employed to iden-\ntify synonymous features using a Levenshtein distance al-\ngorithm, which aided the compilation of a feature synonym\ndictionary.\nThe catalogue was created in linked data format us-\ning the Python RDFLib library11, which enables quick\nand easy serialisation of linked data into various formats.\nThe catalogue lists feature objects and their attributes and\nserves as the foundation for a hybrid ontology engineer-\ning process combining manual and semi-automatic ap-\nproaches. The catalogue identiﬁes approximately 400 dis-\n5http://bbc.co.uk/music/\n6http://libxtract.sourceforge.net\n7http://marsyas.info\n8http://www.vamp-plugins.org/\n9http://aubio.org/\n10http://psysound.wikidot.com/\n11https://github.com/RDFLib/rdflibtinct features and thereby signiﬁcantly increases the scope\nof the original ontology, which supports identifying about\n30 entities. The catalogue has been published online12and\nallows querying subsets of popular features computed in\nfeature extraction tools to help deﬁne the scope and domain\nboundaries of the ontology. It also sheds light on the range\nof classiﬁcations of features inherent in different software\ntools and libraries, as well as conceptualisations of the do-\nmain in journal articles. Figure 1 shows three divergent\norganisations of features from very different sources.\nFeature Extractor\nDynamics\nPitch\nRhythm\nTimbre\nT onality\nMIR descriptor\nSFX\nT onal\nTime-domain\nRhythm\nSpectral\nFeature\nEigen-domain\nModulationFrequency\nFrequency\nPhysical\nPerceptual\nPhase Space\nCepstral\nT emporalabc\nFigure 1 . Three different taxonomies of audio features ex-\ntracted from (a) MIR Toolbox, (b) Essentia, and (c) Mitro-\nvic et al. [14]\nThe catalogue exempliﬁes the diversity of viewpoints\non classiﬁcation of features within the community. It is\nclear that in some cases audio features are categorised ac-\ncording to musicological concepts, such as pitch, rhythm\nand timbre, while in others, the classiﬁcation is based on\nthe computational workﬂows used in calculating the fea-\ntures or a combination of different domains depending\non the task. Consequently, there is no need to impose a\ndeeply taxonomical structure on the collected audio fea-\ntures, rather the resulting ontology should be focused on\nfacilitating structured feature data representation that is\nﬂexible enough to accommodate all these diverging organ-\nisational principles.\n4. CORE ONTOLOGY MODEL\nThe most signiﬁcant updates to the original ontology\nmodel are designed to address a number of requirements\ndetermined during the engineering process. The proposed\nupdates are intended to:\n•provide comprehensive vocabulary of audio features\n•deﬁne terms for capturing computational feature ex-\ntraction workﬂows\n•support development of domain and task speciﬁc on-\ntologies for existing extraction tools\n•restructure concept inheritance for more ﬂexible and\nsustainable feature data representation\n12http://sovarr.c4dm.eecs.qmul.ac.uk/af/\ncatalog/1.0#Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 75•facilitate design of linked data formats that combine\nstrong logical foundations of ontological structuring\nwith simplicity of representations.\nThe fundamental structure of the ontology has changed in a\ncouple of key aspects. The core structure of the framework\nseparates the underlying classes that represent abstract\nconcepts in the domain from speciﬁc named entities. This\nresults in the two main components of the framework de-\nﬁned as Audio Feature Ontology (AFO, http://w3id.\norg/afo/onto/1.1# ) and Audio Feature V ocabulary\n(AFV , http://w3id.org/afo/vocab/1.1# ). The\nmain differences also include introducing levels of abstrac-\ntion to the class structure and reorganising the inheritance\nmodel. The different layers of abstraction represent the\nfeature design process from conceptualisation of a fea-\nture, through modelling a computational workﬂow, to im-\nplementation and instantiation in a speciﬁc computational\ncontext. For example, the abstract concept of Chroma-\ngram is separate from its model which involves a sequence\nof computational operations like cutting an audio signal\ninto frames, calculating the Discrete Fourier Transform for\neach frame, etc. (see Section 5.1 for a more detailed exam-\nple, and [16] for different methods for extracting Chroma-\nbased features). The abstract workﬂow model can be im-\nplemented using various programming languages as com-\nponents of different feature extraction software applica-\ntions or libraries. Thus, this layer enables distinguishing\na Chromagram implementation as a Vamp plugin from a\nChromagram extractor in MIR Toolbox. The most con-\ncrete layer represents the feature extraction instance, for\nexample, to reﬂect the differences of operating systems or\nhardware on which the extraction occurred. The layered\nmodel is shown in Figure 2.\nAudio Feature\nModel\nFeatureExtractormodelsimplements\nInstanceinstantiates\nFigure 2 . The Audio Feature Ontology core model with\nfour levels of abstraction\nThe core model of the ontology retains original at-\ntributes to distinguish audio features by temporal charac-\nteristics and data density. It relies on the Event and Time-\nline ontologies to provide the primary structuring concepts\nfor feature data representation. Temporal characteristics\nclassify feature data either into instantaneous points in time\n- e.g. event onsets or tonal change moments - or events\nwith known time duration. Data density attributes allow\ndescribing how a feature relates to the extent of an audio\nﬁle: whether it is scattered and occurs irregularly over the\ncourse of the audio signal, or the feature is calculated at\nregular intervals and ﬁxed duration. The change in the\ninheritance model removes the music-speciﬁc subclassing\nofafo:Point ,afo:Segment , and afo:Signal classes which\nwas claimed to make feature representation less ﬂexible in\ncertain use cases [7]. The Segment Ontology was proposed\nas a solution to get around these limitations [7], in which\nthe Segment class functions as a music-generic dimensionbetween explicitly temporal and implicitly temporal con-\ncepts, thus enabling multiple concurrent domain-speciﬁc\nconcepts to be represented. An alternative solution is to\nsubclass afo:Point ,afo:Segment , and afo:Signal directly\nfrom afo:AudioFeature , which, in turn, is a subclass of\nevent:Event . In this case, the feature extraction data can\nbe directly linked to the corresponding term in AFV with-\nout being constrained by domain or task speciﬁc class def-\ninitions. This way, it is not necessary to add the Segment\nOntology concepts to feature representations, thereby sim-\nplifying the descriptions.\ntl:Timeline\ntl:Interval\nmo:Signalmo:time\ntl:Instanttl:timeline\nafo:Signal\nafo:Pointtl:timelinetl:timeline\nafo:AudioFeature\nmusic metadataon the Web\ntl:TimeLineMaptl:domainTimeLine\ntl:Timelinetl:domainTimeLine\ntl:Interval\ntl:Intervaltl:timeline\nafo:Segmentevent:timeevent:timeevent:timerdfs:subClassOfrdfs:subClassOfrdfs:subClassOf\nFigure 3 . Framework model showing how feature data\nrepresentation is linked with music metadata resources on\nthe Web using temporal entities deﬁned in the Timeline\nontology\nAudio features collated from literature and extraction\nsoftware are deﬁned as subclasses in the AFV . An illustra-\ntive Turtle-syntax representation that shows the basic prin-\nciple of how subclassing afo:AudioFeature functions in\nthe context of annotating both sparse and dense features\nis provided in Section 5.2. The other purpose of the vo-\ncabulary is to deﬁne computational extraction workﬂow\ndescriptions, so that features can be more easily identi-\nﬁed and compared by their respective computational signa-\ntures. The following section delves into this in more detail.\n5. CASE STUDIES AND EV ALUATION\n5.1 Representing computational workﬂows\nAFV deﬁnes terms for the tool and task speciﬁc ontolo-\ngies and implements the model layer of the ontology\nframework. It is a clean version of the catalogue which\nonly lists the features without any of their properties with\nmany duplications of terms consolidated. This enables the\ndeﬁnition of tool and task speciﬁc feature implementations\nand leaves any categorisation or taxonomic organisation to\nbe speciﬁed in the implementation layer.\nThe vocabulary also speciﬁes computational workﬂow76 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016models for some of the features which can be linked to\nfrom lower level ontologies. The computational workﬂow\nmodels are based on feature signatures as described\nin [14]. The signatures represent mathematical operations\nemployed in the feature extraction process with each\noperation assigned a lexical symbol. It offers a compact\ndescription of each feature and enables an easier way of\ncomparing features according to their extraction work-\nﬂows. Converting the signatures into a linked data format\nto include them in the vocabulary involves deﬁning a\nset of OWL classes that handle the representation and\nsequential nature of the calculations. The operations\nare implemented as sub-classes of three general classes:\ntransformations, ﬁlters and aggregations. For each abstract\nfeature, we deﬁne a model property. The OWL range\nof the model property is a ComputationalModel class in\nthe Audio Feature Ontology namespace. The operation\nsequence can be deﬁned through this object’s operation\nsequence property. For example, the signature of the\nChromagram feature deﬁned in [14] as “f F l Σ”, which\ndesignates a sequence of (1) windowing (f), (2) Discrete\nFourier Transform (F), (3) logarithm (l) and (4) sum ( Σ) is\nexpressed as a sequence of RDF statements in Listing 1.\nafv:Chromagram a owl:Class ;\nafo:model afv:ChromagramModel ;\nrdfs:subClassOf afo:AudioFeature .\nafv:ChromagramModel a afo:Model;\nafo:sequence afv:Chromagram_operation_sequence_1 .\nafv:Chromagram_operation_sequence_1 a afv:Windowing;\nafo:next_operation\nafv:Chromagram_operation_sequence_2 .\nafv:Chromagram_operation_sequence_2 a\nafv:DiscreteFourierTransform;\nafo:next_operation\nafv:Chromagram_operation_sequence_3 .\nafv:Chromagram_operation_sequence_3 a afv:Logarithm;\nafo:next_operation\nafv:Chromagram_operation_sequence_4 .\nafv:Chromagram_operation_sequence_4 a\nafo:LastOperation, afv:Sum .\nListing 1. Description of a chromagram computation.\nSELECT DISTINCT ?feature\nWHERE {\n?opid a afv:DiscreteCosineTransform .\n?seqid afo:first_operation ?fopid .\n?fopid afo:next_operation+ ?opid .\nOPTIONAL {\n?model afo:operation_sequence ?seqid .\n?feature afo:model ?model .\n}\n}\nListing 2. Retrieving feature types involving the DCT.\nThis structure enables building SPARQL queries to\nretrieve comparative information on features from the\nvocabulary. For example, we can inquire which features\nin the vocabulary employ the Discrete Cosine Transform\ncalculation by executing the query of Listing 2. The query\nwill produce the following result:afv:AutocorrelationMFCCs\nafv:BarkscaleFrequencyCepstralCoefficients\nafv:MelscaleFrequencyCepstralCoefficients\nafv:ModifiedGroupDelay\nafv:ModulationHarmonicCoefficients\nafv:NoiseRobustAuditoryFeature\nafv:PerceptualLinearPrediction\nafv:RelativeSpectralPLP\n5.2 Audio content description\nIn order to determine how well the AFO framework repre-\nsents the audio feature extraction domain, we need to test\nits suitability for representing audio features in the context\nof particular use cases. We employ a task-based methodol-\nogy to focus on evaluating the suitability of AFO in a fea-\nture extraction workﬂow. Task-based evaluation is based\non having a set of pre-deﬁned requirements and it may of-\nfer a measure of practical aspects, such as the human abil-\nity to formulate queries using an ontology, or the accuracy\nof responses provided by the system’s inferential compo-\nnent. In order to qualitatively evaluate the AFO frame-\nwork, we need to deﬁne a set of requirements from the\nperspective of music information retrieval workﬂows. Re-\nviewing common research workﬂows, the following main\nrequirements for audio feature annotations have been dis-\ncovered:\n•identify an extracted audio feature by linking it to a\ncorresponding term in the Audio Feature V ocabulary\n•identify the computational steps involved in the pro-\ncess\n•describe the temporal structure and density of output\n•associate audio features with the audio signal time-\nline\n•identify the feature extraction software tools used in\nthe extraction process\nSparse point-like and dense signal-like features of an\naudio ﬁle - such as onsets or MFCC - can be linked directly\nto their respective classes in AFV in the feature extraction\nprocess as shown in Listing 3.\nThe Turtle representation is but one of the possible\nmeans of serialisation. AFO can facilitate development of\nother data formats that are aligned with linked data prin-\nciples, including binary RDF representations. One of the\ngoals of the development process has been to look for alter-\nnative formats that could be used in different contexts. Due\nto the wide appeal of JSON, the ontology also enables pub-\nlishing feature data in its linked data version. JSON-LD is\nan extension to the standard JSON format that provides an\nentity-centric representation of RDF/OWL semantics and\na means to deﬁne a linked data context with URI connec-\ntions to external ontologies and resources [10]. It has the\npotential to simplify feature representations while main-\ntaining ontological structuring of the data. The format en-\nables establishing links to ontologies where the structure\nof the data is deﬁned by using the key word ”@context”.\nOWL class types are annotated with ”@type” and uniqueProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 77identiﬁers are with ”@id”. The latter functions as a link-\ning mechanism between nodes when an RDF graph is con-\nverted into a JSON tree structure. The JSON-LD represen-\ntation of audio features has been tested in the context of an\nadaptive music player.\n@prefix afv: <http://w3id.org/afo/vocab/1.1#> .\n@prefix mo: <http://purl.org/ontology/mo/> .\n@prefix tl:\n<http://purl.org/c4dm/timeline.owl#> .\n@prefix vamp: <http://purl.org/ontology/vamp/> .\n:signal_f6261475 a mo:Signal ;\nmo:time [\na tl:Interval ;\ntl:onTimeLine :timeline_aec1cb82\n] .\n:timeline_aec1cb82 a tl:Timeline .\n:transform_onsets a vamp:Transform ;\nvamp:plugin plugbase:qm-onsetdetector ;\nvamp:output\nplugbase:qm-onsetdetector_output_onsets .\n:transform_mfcc a vamp:Transform ;\nvamp:plugin plugbase:qm-mfcc ;\nvamp:output\nplugbase:qm-mfcc_output_coefficients .\n:event_1 a afv:Onset ;\nevent:time [\na tl:Instant ;\ntl:onTimeLine :timeline_aec1cb82 ;\ntl:at \"PT1.98S\"ˆˆxsd:duration ;\n] ;\nvamp:computed_by :transform_onsets .\n:feature_1 a afv:MFCC ;\nmo:time [\na tl:Interval ;\ntl:onTimeLine :timeline_aec1cb82 ;\n] ;\nvamp:computed_by :transform_mfcc ;\nafo:value ( -26.9344 0.188319 0.106938 ..) .\nListing 3. An abbreviated example of linking onsets and\nMFCC features to AFV and the Music Ontology\n5.3 Case study: adaptive music player\nBeyond representing audio feature data in research work-\nﬂows, there are many other practical applications for the\nontology framework. One of the test cases is providing\ndata services for an adaptive music player that uses au-\ndio features to enrich user experience and enables novel\nways to search or browse large music collections. Feature\ndata of the music tracks available in the player is stored\nin a CouchDB13instance in JSON-LD. The data is used\nby Semantic Web entities called Dynamic Music Objects\n(dymos) [22] that control the audio mixing functionality\nof the player. Dymos make song selections and determine\ntempo alignment for cross-fading based on features. List-\ning 4 shows an example of JSON-LD representation of a\ntrack used in the system linked to feature annotations.\n6. CONCLUSIONS\nThe Audio Feature Ontology and V ocabulary provide a\nframework for representing audio features using Seman-\ntic Web methods and linked data technologies. It pro-\nvides terminology to facilitate task and tool speciﬁc on-\ntology development and serves as a descriptive framework\n13http://couchdb.apache.org/{\n\"@context\": {\n\"foaf\": \"http://xmlns.com/foaf/0.1/\",\n\"afo\": \"http://w3id.org/afo/onto/1.1#\",\n\"afv\": \"http://w3id.org/afo/vocab/1.1#\",\n\"mo\": \"http://purl.org/ontology/mo/\",\n\"dc\": \"http://purl.org/dc/elements/1.1/\",\n\"tl\": \"http://purl.org/NET/c4dm/timeline.owl#\",\n\"vamp\": \"http://purl.org/ontology/vamp/\"\n},\n\"@type\": \"mo:Track\",\n\"dc:title\": \"Open My Eyes\",\n\"mo:artist\": {\n\"@type\": \"mo:MusicArtist\",\n\"foaf:name\": \"The Nazz\"\n},\n\"mo:available_as\": \"/home/snd/250062-15.01.wav\",\n\"mo:encodes\": {\n\"@type\": \"mo:Signal\",\n\"mo:time\": {\n\"@type\": \"tl:Interval\",\n\"tl:duration\": \"PT163S\",\n\"tl:timeline\": {\n\"@type\": \"tl:Timeline\",\n\"@id\": \"98cfa995..\"\n}}},\n\"afo:features\": [\n{\n\"@type\": \"afv:Key\",\n\"vamp:computed_by\": {\n\"@type\": \"vamp:Transform\",\n\"vamp:plugin_id\":\n\"vamp:qm-vamp-plugins:qm-keydetector\"\n},\n\"afo:values\": [\n{ \"tl:at\": 1.4 , \"rdfs:label\": \"C# minor\",\n\"tl:timeline\": \"98cfa995..\"\n},\n{ \"tl:at\": 5.9 , \"rdfs:label\": \"D minor\",\n\"tl:timeline\": \"98cfa995..\"\n}\n]\n}]}\nListing 4. JSON-LD representation of an audio feature\nlinked with track metadata\nfor audio feature extraction. The updates to the original\nontology for audio features strive to simplify feature rep-\nresentations and make them more ﬂexible while maintain-\ning ontological structuring and linking capabilities. JSON-\nLD has been shown to function as a linked data format\nthat enables converting RDF graph structures to key-value\nrepresentation. This could also apply for other similar\ndata formats and NoSQL database systems. The ontol-\nogy engineering process has produced example ontolo-\ngies for existing tools including MIR Toolbox, Essentia,\nMarsyas and others available from the ontology Website\nhttp://w3id.org/afo/onto/1.1# .\n7. ACKNOWLEDGEMENTS\nThis work was part funded by the FAST IMPACt EP-\nSRC Grant EP/L019981/1 and the European Commis-\nsion H2020 research and innovation grant AudioCommons\n(688382). Sandler acknowledges the support of the Royal\nSociety as a recipient of a Wolfson Research Merit Award.\n8. REFERENCES\n[1] X. Amatriain, P. Arumi, and D. Garcia. Clam: A frame-\nwork for efﬁcient and rapid development of cross-78 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016platform audio applications. In proc. 14th ACM Inter-\nnational Conference on Multimedia , pages 951–954,\nNew York, USA, 2006.\n[2] S. Bechhofer, S. Dixon, G. Fazekas, T. Wilmering,\nand K. Page. Computational analysis of the live mu-\nsic archive. In proc. 15th International Conference on\nMusic Information Retrieval (ISMIR) , 2014.\n[3] D. Bogdanov, N. Wack, E. G ´omez, S. Gulati, P. Her-\nrera, O. Mayor, G. Roma, J. Salamon, J. Zapata, and\nX. Serra. Essentia: an audio analysis library for music\ninformation retrieval. In proc. 14th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR’13) , pages 493–498, Curitiba, Brazil, 2013.\n[4] G. Fazekas, Y . Raimond, K. Jakobson, and M. San-\ndler. An overview of semantic web activities in the\nOMRAS2 project. Journal of New Music Research\n(JNMR) , 39(4), 2010.\n[5] G. Fazekas and M. Sandler. The studio ontology frame-\nwork. In proc. 12th International Society for Music\nInformation Retrieval (ISMIR’11) conference, Miami,\nUSA, 24-28 Oct. , pages 24–28, 2011.\n[6] R. Fiebrink, G. Wang, and Perry R. Cook. Support\nfor MIR prototyping and real-time applications in the\nchuck programming language. In proc. 9th Interna-\ntional Conference on Music Information Retrieval,\nPhiladelphia, PA, USA, September 14-18 , 2008.\n[7] B. Fields, K. Page, D. De Roure, and T. Crawford.\nThe segment ontology: Bridging music-generic and\ndomain-speciﬁc. In proc. IEEE International Confer-\nence on Multimedia and Expo, Barcelona, Spain, 11-\n15 July , 2011.\n[8] E. J. Humphrey, J. Salamon, O. Nieto, J. Forsyth,\nR. Bittner, and J. P. Bello. JAMS: A JSON annotated\nmusic speciﬁcation for reproducible MIR research. In\n15th Int. Soc. for Music Info. Retrieval Conf. , pages\n591–596, Taipei, Taiwan, Oct. 2014.\n[9] K. Jacobson, Y . Raimond, and M. Sandler. An ecosys-\ntem for transparent music similarity in an open world.\nInProceedings of the 10th International Society for\nMusic Information Retrieval Conference, ISMIR 2009,\nKobe, Japan, October 26-30 , 2009.\n[10] M. Lanthaler and C. G ¨utl. On using JSON-LD to create\nevolvable restful services. In proc. 3rd International\nWorkshop on RESTful Design at WWW’12 , 2012.\n[11] B. Mathieu, B. Essid, T. Fillon, J. Prado, and\nG. Richard. YAAFE, an easy to use and efﬁcient audio\nfeature extraction software. In proc. 11th International\nConference on Music Information Retrieval (ISMIR),\nUtrecht, Netherlands, August 9-13 , 2010.\n[12] R. Mayer and A. Rauber. Towards time-resilient mir\nprocesses. In Proceedings of the 13th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR), Porto, Portugal, October 8-12 , 2012.[13] C. McKay and I. Fujinaga. Improving automatic mu-\nsic classiﬁcation performance by extracting features\nfrom different types of data. In proc. of the Inter-\nnational Conference on Multimedia Information Re-\ntrieval , pages 257–266, New York, USA, 2010.\n[14] D. Mitrovic, M. Zeppelzauer, and C. Breiteneder. Fea-\ntures for content-based audio retrieval. Advances in\nComputers , 78:71–150, 2010.\n[15] D Moffat, D Ronan, and J. D. Reiss. An evaluation of\naudio feature extraction toolboxes. In Proc. of the 18th\nInt. Conference on Digital Audio Effects (DAFx-15) ,\nTrondheim, Norway, 2015.\n[16] Meinard M ¨uller and Sebastian Ewert. Chroma Tool-\nbox: MATLAB implementations for extracting vari-\nants of chroma-based audio features. In proc. 12th\nInternational Society for Music Information Retrieval\n(ISMIR’11) conference, Miami, USA , 2011.\n[17] K. Page, B. Fields, B. Nagel, G. O’Neill, D. De Roure,\nand T. Crawford. Semantics for music analysis through\nlinked data: How country is my country? In Proceed-\nings of the 6th International Conference on e-Science,\nBrisbane, Australia, 7-10 Dec. , 2010.\n[18] G. Peeters. A large set of audio features for\nsound description (similarity and classiﬁcation) in the\nCUIDADO project. Tech. rep., IRCAM, 2004.\n[19] A. Porter, D. Bogdanov, R. Kaye, R. Tsukanov, and\nX. Serra. Acousticbrainz: a community platform for\ngathering music information obtained from audio. In\nproc. 16th International Society for Music Information\nRetrieval (ISMIR) Conference , 2015.\n[20] Y Raimond, S. Abdallah, M. Sandler, and F. Giasson.\nThe music ontology. In Proceedings of the 8th Inter-\nnational Conference on Music Information Retrieval,\nISMIR 2007, Vienna, Austria, September 23-27 , 2007.\n[21] Markus Schedl. The CoMIRV A Toolkit for Visualizing\nMusic-Related Data. Technical report, Dept. of Com-\nputational Perception, Johannes Kepler Univ., 2006.\n[22] Florian Thalmann, Alfonso Perez Carillo, Gy ¨orgy\nFazekas, Geraint A. Wiggins, and Mark Sandler. The\nmobile audio ontology: Experiencing dynamic music\nobjects on mobile devices. In proc. 10th IEEE Inter-\nnational Conference on Semantic Computing, Laguna\nHills, CA, USA , 2016.\n[23] M. Vacura, V . Sv ´atek, C. Saathoff, T. Ranz, and\nR. Troncy. Describing low-level image features using\nthe COMM ontology. In proc. 15th International Con-\nference on Image Processing (ICIP), San Diego, Cali-\nfornia, USA , pages 49–52, 2008.\n[24] K. West, A. Kumar, A. Shirk, G. Zhu, J. S. Downie,\nA. F. Ehmann, and M. Bay. The networked environ-\nment for music analysis (NEMA). In proc. 6th World\nCongress on Services, Miami, USA, July 5-10 , 2010.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 79"
    },
    {
        "title": "Conversations with Expert Users in Music Retrieval and Research Challenges for Creative MIR.",
        "author": [
            "Kristina Andersen",
            "Peter Knees"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1418323",
        "url": "https://doi.org/10.5281/zenodo.1418323",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/246_Paper.pdf",
        "abstract": "Sample retrieval remains a central problem in the cre- ative process of making electronic dance music. This pa- per describes the findings from a series of interview ses- sions involving users working creatively with electronic music. We conducted in-depth interviews with expert users on location at the Red Bull Music Academies in 2014 and 2015. When asked about their wishes and expectations for future technological developments in interfaces, most par- ticipants mentioned very practical requirements of storing and retrieving files. A central aspect of the desired systems is the need to provide increased flow and unbroken periods of concentration and creativity. From the interviews, it becomes clear that for Creative MIR, and in particular, for music interfaces for creative ex- pression, traditional requirements and paradigms for music and audio retrieval differ to those from consumer-centered MIR tasks such as playlist generation and recommendation and that new paradigms need to be considered. Despite all technical aspects being controllable by the experts them- selves, searching for sounds to use in composition remains a largely semantic process. From the outcomes of the in- terviews, we outline a series of possible conclusions and areas and pose two research challenges for future develop- ments of sample retrieval interfaces in the creative domain.",
        "zenodo_id": 1418323,
        "dblp_key": "conf/ismir/AndersenK16",
        "content": "CONVERSATIONS WITH EXPERT USERS IN MUSIC RETRIEVAL AND\nRESEARCH CHALLENGES FOR CREATIVE MIR\nKristina Andersen\nStudio for Electro Instrumental Music (STEIM)\nAmsterdam, the Netherlands\nkristina@steim.nlPeter Knees\nDept. of Computational Perception\nJohannes Kepler University Linz, Austria\npeter.knees@jku.at\nABSTRACT\nSample retrieval remains a central problem in the cre-\native process of making electronic dance music. This pa-\nper describes the ﬁndings from a series of interview ses-\nsions involving users working creatively with electronic\nmusic. We conducted in-depth interviews with expert users\non location at the Red Bull Music Academies in 2014 and\n2015. When asked about their wishes and expectations for\nfuture technological developments in interfaces, most par-\nticipants mentioned very practical requirements of storing\nand retrieving ﬁles. A central aspect of the desired systems\nis the need to provide increased ﬂow and unbroken periods\nof concentration and creativity.\nFrom the interviews, it becomes clear that for Creative\nMIR, and in particular, for music interfaces for creative ex-\npression, traditional requirements and paradigms for music\nand audio retrieval differ to those from consumer-centered\nMIR tasks such as playlist generation and recommendation\nand that new paradigms need to be considered. Despite all\ntechnical aspects being controllable by the experts them-\nselves, searching for sounds to use in composition remains\na largely semantic process. From the outcomes of the in-\nterviews, we outline a series of possible conclusions and\nareas and pose two research challenges for future develop-\nments of sample retrieval interfaces in the creative domain.\n1. MOTIVATION AND CONTEXT\nConsiderable effort has been put into analysing user be-\nhaviour in the context of music retrieval in the past two\ndecades [35]. This includes studies on music information\nseeking behaviour [14,17], organisation strategies [15], us-\nage of commercial listening services [36], the needs or\nmotivations of particular users, such as kids [28], adoles-\ncents [34], or musicologists [29], and behaviour analysis\nfor speciﬁc tasks, e.g., playlist and mix generation [13], or\nin speciﬁc settings, e.g., riding together in a car [16] or in\nmusic lessons in secondary schools [49].\nc/circlecopyrtKristina Andersen, Peter Knees. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: Kristina Andersen, Peter Knees. “Conversations with Expert\nUsers in Music Retrieval and Research Challenges for Creative MIR”,\n17th International Society for Music Information Retrieval Conference,\n2016.\nFigure 1 . Live electronic music performance at the Red\nBull Music Academy 2014\nIn this paper, we want to address music retrieval from\nthe perspective of music producers, thus investigate the\nuser behaviour of a group that deals with audio retrieval\nprofessionally on a daily basis, but has received compara-\ntively less attention in MIR research so far—as have other\nquestions from the area of Creative MIR [27].\nThe majority of today’s electronic music is created from\npre-recorded or live-generated sound material. This pro-\ncess often combines sound loops and samples with syn-\nthesized and processed elements using a so-called digital\naudio workstation (DAW), an electronic device or com-\nputer application for recording, editing and producing au-\ndio ﬁles. In these systems, Music Information Retrieval\n(MIR) methods, e.g., for content analysis, gain importance.\nIn essence, future tools and applications need to be aware\nof the nature and content of the music material, in order to\neffectively support the musician in the creative process.\nHowever, user studies on retrieval for musicians and\nproducers are scarce. Cartwright et al. [6] investigate po-\ntential alternatives to existing audio production user inter-\nfaces in a study with 24 participants. In another example,\nBainbridge et al. [4] explore and test a personal digital li-\nbrary environment for musicians, where based on a spatial\nparadigm musicians should be able to capture, annotate,\nand retrieve their ideas, e.g., using query-by-humming. In\nthis paper, our approach is not to test an existing system,\nbut to gain an understanding of the processes involved for\nmusic producers, who are used to working with existing\nmusic software suites.122This work is organized a follows. In section 2, we iden-\ntify and brieﬂy discuss existing MIR approaches in the\ncontext of music production. In section 3, we describe our\nmotivation for engagement with expert users, our approach\nof conducting semi-structured interviews, and information\non the interview background. The main part of the paper\nis presented in sections 4 and 5, where we highlight inter-\nesting outcomes of the interview sessions and distill cen-\ntral topics. Corresponding to this, we conclude this work\nby posing two research challenges for Creative MIR (sec-\ntion 6).\n2. MIR RESEARCH IN MUSIC MAKING\nExisting MIR research targeted at composition support and\nmusic production deals with browsing interfaces to facili-\ntate access to large collections of potentially homogeneous\nmaterial, such as drum samples [41]. Exploration of sam-\nple libraries, e.g., [8, 50], is often supported or driven by\nmethods to automatically extract music loops from mu-\nsic ﬁles. Given the prevalent techniques of sampling and\nremixing in today’s music production practise, such meth-\nods are useful to identify reusable materials and can lead\nto inspirations for new compositions [40, 51] and new in-\nterfaces for remixing [23] In terms of retrieval in the cre-\native domain, and in contrast to consumer-based infor-\nmation systems, the query-by-example paradigm, imple-\nmented as query-by-humming or through other vocal in-\nputs [5, 25, 31], is still an active research ﬁeld.\nOther MIR systems facilitate musical creation through\nautomatic composition systems [10] or mosaicing systems\nthat “reconstruct” the sound of a target piece by concate-\nnating slices of other recordings [38, 45, 54]. This princi-\nple of concatenative synthesis can also be found in interac-\ntive systems for automatic accompaniment or improvisa-\ntion such as the OMax system by Assayag et al. [3], Audio\nOracle by Dubnov et al. [19], or Cont’s Antescofo [11].\nOther MIR systems emphasize the embodiment of cre-\nativity expression. For instance, Schnell et al. [44] pro-\npose a system that combines real-time audio processing,\nretrieval, and playback with gestural control for re-embodi-\nment of recorded sound and music. The Wekinator [22] by\nFiebrink is a real-time, interactive machine learning toolkit\nthat can be used in the processes of music composition and\nperformance, as well as to build new musical interfaces and\nhas also shown to support the musical expression of people\nwith disabilities [32].\n3. WORKING WITH EXPERT USERS\nStandards for user involvement in the ﬁeld of Human Com-\nputer Interaction (HCI) have evolved from a traditional ap-\nproach of metric user-testing of already designed systems,\nto understanding of users and their context through ethno-\ngraphic methods and scenarios, towards an emerging fo-\ncus on developing empathy with the user’s experience of\nlife. Wright and McCarthy state that “‘knowing the user’\nin their lived and felt life involves understanding what itfeels like to be that person, what their situation is like from\ntheir own perspective.” [52]\nThis is especially important in the case of the creative\nexpert users, who are not just looking to complete a series\nof tasks, but rather are engaging with the technology in or-\nder to express themselves through it. As such they can be\nseen to be not only using the technology, but rather collab-\norating with it as described by Tom Jenkinson (aka Square-\npusher): “Through his work, a human operator brings as\nmuch about the machine to light as he does about himself\n... The machine has begun to participate.” [30]\nThis paper describes some of our efforts at building\nsuch an understanding. Eventually, our work will aim to\ncreate musical tools that provide new interfaces to the se-\nlection of sounds and musical data through music anal-\nysis algorithms. The underlying concern will be to not\njust improve existing user interfaces for the creation of\nelectronic music through increases in efﬁciency, but fa-\ncilitate increased ﬂow and unbroken periods of concentra-\ntion and creativity. To do so, we are engaging with expert\nusers throughout the entire project, allowing them a strong\npeer position in the conceptualisation and evaluation of any\nideas.\nOur main users are the participants at the Red Bull Mu-\nsic Academy (RBMA), cf. ﬁg. 1, an event held yearly with\na carefully selected group of professional electronic dance\nmusic makers on the point of breaking though.1\nOur sustained involvement with this group of expert\nusers is key to our strategy of building detailed understand-\nings of current forms of electronic music making, cf. [1].\nWe hope that this will allow us to go beyond user testing,\nand instead aim for a coherent impression of how an inter-\nface may beneﬁt the real-life creative process of the users.\nTo this end, we are committed to conducting interviews in\na fashion that ﬁts within the work-ﬂow and interpersonal\ncommunication style of these music professionals, we ulti-\nmately aim to support creatively with the outcomes of the\nproject. What we need to understand is: How do they\norganise their work, what are their needs, and ultimately\nwhat are their mental models of their music?\nWe conducted 33 in-depth interviews with expert users\non location at the Red Bull Music Academy in Tokyo\n(2014) and Paris (2015). The aim of the 2014 sessions\nwas to establish understandings of existing work practices\namong users, and the 2015 sessions were set up to inves-\ntigate a number of emergent themes in more detail. Our\ninterviews were executed in an open conversational struc-\nture, engaging the interviewees directly as peers, while\naiming to support them to go beyond evaluation of current\ninterfaces and into the imagination of new and unknown\ninterfaces for their own creative practice.\nThe interviews were audio recorded and fully tran-\nscribed. Three independent HCI researchers analysed the\n1http://redbullmusicacademy.com ; From the web page:\n“The Red Bull Music Academy is a world-travelling series of music\nworkshops and festivals [in which] selected participants – producers,\nvocalists, DJs, instrumentalists and all-round musical mavericks from\naround the world – come together in a different city each year. For two\nweeks, each group will hear lectures by musical luminaries, work together\non tracks and perform in the city’s best clubs and music halls.”Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 123transcripts for content-rich quotes: short sentences or para-\ngraphs describing a particular idea or concern. The key-\nwords from these quotes were extracted and used for la-\nbelling the quote, e.g., “search”, “ﬁnding”, “colour”, or\n“pace”. Following this, keywords were manually clustered\ninto bigger concepts or themes. From the material col-\nlected, we identiﬁed the themes of Search ,Categories ,Vi-\nsualisation ,Colour ,Organisation ,Assistance ,Workﬂow ,\nConnections ,Correction ,Suggestions ,Obstructions ,De-\nliberate error ,Tweaks ,Interfaces , and Live. The material\nwe address in this paper belongs to the themes of Search ,\nCategories ,Colour ,Visualisation ,Organisation ,Sugges-\ntions , and Obstructions .\n4. INTERVIEW QUOTES AND FINDINGS\nWhen asked about their wishes and expectations for future\ntechnological developments, most participants mentioned\nvery practical requirements for storing and retrieving ﬁles.\nSounds are usually stored as short audio ﬁles (samples) or\npresets, which can be loaded into playback devices in com-\nposition software.\n“Because we usually have to browse really\nhuge libraries [...] that most of the time are\nnot really well organized. ” (TOK003)\n“If you have like a sample library with\n500,000 different chords it can take a while\nto actually ﬁnd one because there are so many\npossibilities. ” (TOK015)\n“Like, two hundred gigabytes of [samples].\nI try to keep some kind of organisation. ”\n(TOK006)\n“I easily get lost... I always have to scroll back\nand forth and it ruins the ﬂow when you’re\nplaying” (PA011)\n“...what takes me really long time is organis-\ning my music library for DJing. [...] Yes, it\ncould be something like Google image search\nfor example. You input a batch of noise, and\nyou wait for it to return a sound. ” (TOK011)\nEven from this small selection of statements it becomes\nclear that organisation of audio libraries, indexing, and ef-\nﬁcient retrieval plays a central role in the practice of music\ncreators and producers. However, in the search tools pro-\nvided by existing DAWs, this aspect seems addressed in-\nsufﬁciently. When asked directly: “How do you ﬁnd what\nyou are looking for?” answers indicated a number of per-\nsonal strategies that either worked with, or sometimes in\nopposition to, the existing software design.\n“You just click randomly and just scrolling, it\ntakes for ever!” (TOK009)\n“Sometimes, when you don’t know what you\nare looking for, and you’re just going ran-\ndomly through your samples, that might be\nFigure 2 . User sample ﬁle collection, photographed from\nlaptop screen of expert music producer at RBMA 2014.\nhelpful, but most of the time I have something\nin mind that I am looking for, and I am just\ngoing through all these sound ﬁles, and I am\njust waiting for the sound which I had in mind\nto suddenly appear. Or what comes the clos-\nest to what I had in mind. So I think that most\nof the time, I know what I am looking for, and\nthen it is just a matter of time before I ﬁnd it. ”\n(TOK002)\n“Part of making music is about being lost a\nlittle bit and accidentally stumbling upon stuff\nthat you didn’t think would work. ” (TOK007)\nThis highlights a key element of much creative work,\nthe element of the accidental, sometimes caused by the\npositive and negative effects of malfunctioning of sound-\nediting software. Our ﬁnding 1 is that serendipity is highly\nimportant to support creative work and that when existing\nsoftware is not providing this desired functionality, worka-\nrounds will be created.\nOften, users are constructing their own personal sys-\ntems for searching, sometimes working with the structures\navailable to them, but often developing idiosyncratic and\npersonal strategies of misuse or even randomness. We also\nsee users painstakingly creating hierarchical categorisation\nschemes manually in order to stay in control of their own\nsound collections as seen in Figure 2.\nSearching for sounds to use in composition remains a\nbroadly semantic process, where the user has a certain\nstructure of meaning in mind when querying the collection.\nCurrent solutions in this direction rely on databases with\nsets of meta-data (tags) for the available sounds. However,\nall available solutions that come with pre-tagged sounds\nuse a static semantic structure, which cannot adapt to the\nusers individual understanding of the sounds. This is espe-\ncially problematic when the user has a speciﬁc target sound\nin mind, but does not know how this would be described in\nthe pre-tagged semantic structure of the database. In short,\nas our ﬁnding 2 we see that our users have mental images\nof sound that they translate to verbal expressions.124 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016“So it would be really useful to for exam-\nple have some kind of sorting system for\ndrums, for example, where I could for exam-\nple choose: ’bass drum’, and here it is: ’bass’\nand ’bright’, and I would like it to have maybe\nbass drum ’round’ and ’dry’, and you can\nchoose both, the more I choose, of course, the\nless results I will have [...] So it is ﬁltering it\ndown, that is really helpful, if it works well of\ncourse. ” (TOK002)\n“It would be even more useful to be able to\nsearch for a particular snare, but I can’t really\nimagine how, I need something short, low in\npitch, dark or bright in tone and then it ﬁnds\nit... ” (TOK003)\n“There are a lot of adjectives for sound, but\nfor me, if you want a ‘bright’ sound for ex-\nample it actually means a sound with a lot of\ntreble, if you say you want a ‘warm’ sound,\nyou put a round bass, well, round is another\nadjective. ” (TOK009)\nWe also see that the semantic descriptions used in\nthese descriptions are very individually connoted and of-\nten stemming from non-auditory domains, such as haptic,\nor most prominently, the visual domain (e.g., round, dark,\nbright). Thus, the mental images expressed verbally are\noften actually rooted in another domain.\nIn fact, one solution to the problem of indexing sug-\ngested by our participants is to organize sounds by sources,\nby projects in which they have been used, or to colour-code\nthem.\n“If I have hundreds of tracks, I have to colour\ncode everything, and name properly every-\nthing. It’s a kind of system, and also kind of\nI feel the colours with the sounds, or maybe\na rose, if kind of more orange, and brownish\nand maybe... I use that kind of colour coding. ”\n(TOK006)\nFinding 3 is that we see a need for semantic representa-\ntions of sounds, but it’s not only a matter of just tags and\nwords, but rather an ability to stay much closer to the vo-\ncabulary and mental representations of sound of each user.\nAdditionally, the question arises whether the intervie-\nwees really want a direct representation of their mental\nmap in the data structure, or if indeed they rather expect\nsomething more akin to a machine collaborator, that could\ncome up with its own recommendations and suggest a\nstructure based on the personal requirements of the indi-\nvidual user.\n“I’d like it to do the opposite actually, be-\ncause the point is to get a possibility, I mean I\ncan already make it sound like me, it’s easy. ”\n(TOK001)“What I would probably rather want it would\ndo is make it complex in a way that I appreci-\nate, like I would be more interested in some-\nthing that made me sound like the opposite of\nme, but within the boundaries of what I like,\nbecause that’s useful. Cause I can’t do that\non my own, it’s like having a band mate basi-\ncally. ” (TOK007)\nAgain we see the computer as a potential collaborator,\none that might even be granted some level of autonomy:\n“Yeah, yeah, well I like to be completely in\ncharge myself, but I like to... I don’t like other\nhumans sitting the chair, but I would like the\nmachine to sit in the chair, as long as I get to\ndecide when it gets out. ” (TOK014)\nFinding 4 is that instead of computer-generated and si-\nmilarity-based recommendations, desired features are sur-\nprise, opposition, individuality, and control over the pro-\ncess (with the possibility to give up control when needed).\n5. INTERVIEW CONCLUSIONS\nFrom the interviews outlined in the previous section, we\nsee that central concerns in everyday work in music pro-\nduction are core topics of MIR: indexing, retrieval, brows-\ning, recommendation, and intuitive (visual) interfaces.\nMore than in music retrieval systems built for consumer\nor entertainment needs, the expert user in a music produc-\ntion environment will actually evaluate a large part of the\nreturned items to ﬁnd the best—a process that is integral to\nsome as a way to get inspired.\nIn order to facilitate this process and to enable creative\nwork to make better use of MIR tools, we identiﬁed four\ncrucial ﬁndings:\n1. Surprise and serendipity in recommendation and re-\ntrieval are important to support creative work\n2. Users have personal mental images of sound\n3. There is a need for semantic representations of\nsounds for retrieval, which are not just tags and\nwords but rather reﬂect those mental images (which\ncan be visual or haptic)\n4. Instead of “more of the same” recommendations, de-\nsired features are surprise, opposition, individuality,\nand control over the recommendation process\nThe desires for systems that respond to personal vocab-\nularies and individual mental images of sound alongside\nthe desire to have a controllable element of otherness and\ndifference, constitute both a challenge and an opportunity.\nHowever, this also goes someway towards illustrating how\nindividualized the creative expert user needs may turn out\nto be. While we can try to ﬁnd common concerns, it is\nclear that no system will ﬁt the requirements of all users. In\nthe creative domain even more than in consumer-orientedProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 125MIR, allowing personalisation of the systems is a central\nrequirement, cf. [4]. This is reﬂected in the two following\nareas, which take the ﬁndings into a bigger context and out-\nline two conceptual ideas for future Creative MIR research,\nnamely “The Collaborative Machine,” building upon ﬁnd-\nings 1 and 4 to go beyond the idea of a traditional recom-\nmender system, and “Synaesthetic Sound Retrieval,” based\nupon ﬁndings 2 and 3, as an approach beyond tag-based\n“semantic retrieval”.\n5.1 The Collaborative Machine\nThe collaborative machine can be imagined as a virtual\nbandmate who assesses, critiques, takes over, and occa-\nsionally opposes.\nIt appears that in creative work as well as in the con-\nsumer world, successful imitation is not enough for the\nmachine to be recognized as “intelligent” anymore. While\nthis is a ﬁrst and necessary step in creative and intelligent\nbehavior, a machine requires more multi-faceted and com-\nplex behavior in order to be considered a useful advice-\ngiver or even collaborator. However, no matter how well\ngrounded or wise they can be, artiﬁcial knowledge and ex-\npert agent-based advice might be completely useless or,\neven worse, annoying and even odious. Aspects of such\nhuman behavior, as well as of surprise, opposition, and ob-\nstruction, should contribute to making the interaction with\nthe machine more interesting and engaging.\nCan we imagine an intelligent machine providing the\nuser with creative obstructions in the place of helpful sug-\ngestions?\nA creative obstruction is based on the artistic technique\nof “defamiliarisation” as deﬁned by Shklovsky [47]—a ba-\nsic artistic strategy central to both Surrealism and Dada.\nIt is based on the idea that the act of experiencing some-\nthing occurs inside the moment of perceiving it and that\nthe further you confuse or otherwise prolong the moment\nof arriving at an understanding, the deeper or more detailed\nthat understanding will be. This technique and the ﬁndings\nfrom the interviews can be directly translated into new re-\nquirements for recommendation engines in music making.\nThis need for opposition goes far beyond the com-\nmonly known and often addressed needs for diversity, nov-\nelty, and serendipity in recommendation system research,\nwhich has identiﬁed purely similarity-based recommenda-\ntion as a shortcoming that leads to decreased user satis-\nfaction and monotony [7, 48, 53]. This phenomenon spans\nmultiple domains: from news articles [37] to photos [42]\nto movies [18]. One idea proposed to increase diversity\nis to subvert the basic idea of collaborative ﬁltering sys-\ntems of recommending what people with similar inter-\nests found interesting (“people with similar interests also\nlike...”) by recommending the opposite of what the least\nsimilar users (the k-furthest neighbors) want [43]. Indeed\nit could be shown that this technique allows to increase di-\nversity among relevant suggestions.\nIn the context of experimental music creation, Collins\nhas addressed the question of opposition in the Contrary\nMotion system [9] using a low-dimensional representationof rhythm. The system opposes a piano player’s rhythm in\nreal time by constructing a structure located in the space\nof actions “where the human is likely not to be” [9]. The\nhypothesis underlying the system is that being confronted\nwith an oppositional music style can be stimulating for a\nmusician. Experiments where the opposing structure is\nsoniﬁed using a different instrument have indeed shown\nthat musicians start to experiment and play with the op-\nposing agent. For future work, it would be interesting to\nsee whether computer-system-created music (or a system\nthat suggests fragments) will be accepted by experts or de-\nclined, cf. [39].\n5.2 Synaesthetic Sound Retrieval\nMultiple search avenues allow the user to use many differ-\nent ways to describe the searched-for sound. This includes\nacoustic sketching, e.g., [5, 25, 31], as well as graphical\nrepresentations.\nIn a number of quotes in section 4, sounds are described\nby shapes (round), brightness (bright, dark) and textures\n(soft, dry). While these might be regarded as unusual de-\nscriptors of sound, there is some evidence that many hu-\nmans make to some degree use of synaesthetic connections\nbetween visual perceptions and sound. In the Creative MIR\nscenario, we make use of a weak deﬁnition of synaesthesia\nas cross-modal associations, cf. [20, 26], and, in the con-\ntext of computer science, “the more general fact that digital\ntechnologies offer, if not a union of the senses, then some-\nthing akin: the inter-translatability of media, the ability to\nrender sound as image, and vice versa.” [12]\nFocusing on the visual domain, through the interviews,\na number of ideas and notions came up in addition to the\nimportance of brightness, shape, and texture for sound re-\ntrieval. More precisely, colour plays a central role: “I see\nthe music sometimes as more aesthetic and something that\nI can see more than something that I can hear” (PA013),\n“When I listen to music I see colours. [...] I remember\ncolours. ” (PA011), “Different sounds to me have speciﬁc\ncolours. ... [For ﬁnding ﬁles,] I don’t have the actual abil-\nity to use images [now], so I just use colour. ” (PA009).\nSuch a colour-coding, for instance, takes the role of “se-\nmantic tagging”. The fact that a system needs time to learn\na user’s associations ﬁrst, i.e., that it might not work per-\nfectly out of the box but learn their associations over time\n(personalisation), is understood and accepted:\n“You could imagine that your computer gets\nused to you, it learns what you mean by\ngrainy, because it could be different from what\nthat guy means by grainy. ” (PA008)\nThe models learned from personal categories and (visual)\ntagging could then be applied on new collections for per-\nsonal indexing.\nFor browsing sound, the idea of tapping into the visual\ndomain is well-established. Most proposed sound brows-\ning systems are based on 2D arrangements of sounds [8,\n21, 41, 46]—even including personalised adaptation of the\narrangement [24]. In these systems, the visual aspect is the126 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016spatial arrangement of sounds, however, this does not re-\nﬂect the mental models but rather requires the user to learn\nthe mapping provided by the system. Grill and Flexer [26]\nget closer to a synaesthetic representation by visualizing\nperceptual qualities of sound textures through symbols on\na grid.2To this end, they map bipolar qualities of sound\nthat describe spectral and temporal aspects of sound to vi-\nsual properties. The spectral qualities of pitch (high vs.\nlow) and tonality (tonal vs. noisy) are mapped to bright-\nness and hue, and saturation, respectively. The temporal\n(or structural) qualities of smoothness vs. coarseness, or-\nder vs. chaos, and homogeneity vs. heterogeneity are as-\nsociated with the jaggedness of an element’s outline, the\nregularity of elements on the grid, and a variation in colour\nparameters, respectively.\nTo the best of our knowledge, there is no system that\nmatches a visual query representing a mental image of\nsound to a sound collection for retrieval. Developing such\na system would, however, pose an interesting challenge.\n6. POSED CHALLENGES\nResults that enable the two conceptual ideas discussed\nabove can not be trivially achieved. Therefore, to con-\nclude this paper, we want to pose two topics as challenges\nfor future work in Creative MIR to the wider community.\nBoth of these topics should allow for alternative retrieval\nparadigms particularly relevant in creative work. As dis-\ncussed before, they require high levels of personalisation\nin order to facilitate “semantic” retrieval.\nChallenge 1: developing models for exploring dis-\nsimilarity in search\nTo arrive at an artiﬁcial collaborator capable of inspiring\nby opposing, the concept of opposition needs to be ex-\nplored ﬁrst, cf. [2]. Music similarity is a multi-dimensional\nconcept and while proximity can be easily, “semantically”\ndeﬁned through minimizing distance measures, the con-\ncept of dissimilarity is by far more difﬁcult to capture as\nit “spreads out” to different directions and dimensions of\nsound. Finding dissimilar sounding audio from a given\nquery is therefore more challenging and requires individual\nuser models of music perception as well as a solid under-\nstanding of usage context in order to derive an understand-\ning of sounding “different”.\nChallenge 2: developing retrieval methods for visual\nqueries\nThis challenge is to develop a software interface for sound\nsearch based on queries consisting of sketches of mental\nimages, cf. [33]. A central requirement for such an in-\nterface is that it needs to be able to deal with different\nsound properties and different types of sounds, such as ef-\nfects, samples, ambient, tonal, or textured recordings, and\ntherefore comprise different simultaneous representational\nmodels for indexing. For instance, while tonal aspects\nmight be best represented using symbolic music notation,\nnoise sounds should be modeled primarily via their textural\nproperties. It is expected that modeling and indexing will\n2http://grrrr.org/test/texvis/map.htmlheavily draw from audio content processing and analysis\nmethods—again—in order to cover a wide range of sound\nproperty dimensions.\nWe hope that these challenges will drive the discussion\non Creative MIR and its applications in music production\nand help reﬂecting upon and advancing the ﬁeld of music\nretrieval also beyond the speciﬁc area of study of this work.\n7. ACKNOWLEDGEMENTS\nThis work is supported by the European Union’s seventh\nFramework Programme FP7 / 2007–2013 for research,\ntechnological development and demonstration under grant\nagreement no. 610591 (GiantSteps). Figure 1 courtesy of\nRed Bull Music Academy.\n8. REFERENCES\n[1] K. Andersen and F. Grote. “GiantSteps: Semi-structured con-\nversations with musicians”. In Proc CHI EA , 2015.\n[2] K. Andersen and P. Knees. “The dial: Exploring computa-\ntional strangeness”. In Proc CHI EA , 2016.\n[3] G. Assayag, G. Bloch, M. Chemillier, A. Cont, and S. Dub-\nnov. “OMAX Brothers: A dynamic topology of agents for\nimprovisation learning”. In Proc ACM MM Workshop on Au-\ndio and Music Computing for Multimedia , 2006.\n[4] D. Bainbridge, B.J. Novak, and S.J. Cunningham. “A user-\ncentered design of a personal digital library for music explo-\nration”. In Proc JCDL , 2010.\n[5] M. Cartwright and B. Pardo. “Synthassist: Querying an audio\nsynthesizer by vocal imitation”. In Proc NIME , 2014.\n[6] M. Cartwright, B. Pardo, and J. Reiss. “Mixploration: Re-\nthinking the audio mixer interface”. In Proc IUI , 2014.\n[7] `O. Celma and P. Herrera. “A new approach to evaluating\nnovel recommendations”. In Proc RecSys , 2008.\n[8] G. Coleman. “Mused: Navigating the personal sample li-\nbrary”. In Proc ICMC , 2007.\n[9] N. Collins. “Contrary motion : An oppositional interactive\nmusic system”. In Proc NIME , 2010.\n[10] N. Collins. “Automatic composition of electroacoustic art\nmusic utilizing machine listening”. CMJ , 36(3):8–23, 2012.\n[11] A. Cont. “”Antescofo: Anticipatory synchronization and con-\ntrol of interactive parameters in computer music”. In Proc\nICMC , 2008.\n[12] C. Cox. “Lost in translation: Sound in the discourse of\nsynaesthesia”. Artforum International , 44(2):236–241, 2005.\n[13] S.J. Cunningham, D. Bainbridge, and A. Falconer. “‘More of\nan art than a science’: Supporting the creation of playlists and\nmixes”. In Proc ISMIR , 2006.\n[14] S.J. Cunningham, J.S. Downie, and D. Bainbridge. “‘The\npain, the pain’: Modelling music information behavior and\nthe songs we hate”. In Proc ISMIR , 2005.\n[15] S.J. Cunningham, M. Jones, and S. Jones. “Organizing digi-\ntal music for use: An examination of personal music collec-\ntions”. In Proc ISMIR , 2004.\n[16] S.J. Cunningham, D.M. Nichols, D. Bainbridge, and H. Ali.\n“Social music in cars”. In Proc ISMIR , 2014.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 127[17] S.J. Cunningham, N. Reeves, and M. Britland. “An ethno-\ngraphic study of music information seeking: Implications for\nthe design of a music digital library”. In Proc JCDL , 2003.\n[18] T. Di Noia, V .C. Ostuni, J. Rosati, P. Tomeo, and E. Di Sci-\nascio. “An analysis of users’ propensity toward diversity in\nrecommendations”. In Proc RecSys , 2014.\n[19] S. Dubnov, G. Assayag, and A. Cont. “Audio oracle: A\nnew algorithm for fast learning of audio structures”. In Proc\nICMC , 2007.\n[20] K.K. Evans and A. Treisman. “Natural cross-modal mappings\nbetween visual and auditory features”. JOV, 10(1):6, 2010.\n[21] M. Fernstr ¨om and E. Brazil. “Sonic browsing: An auditory\ntool for multimedia asset management”. In Proc ICAD , 2001.\n[22] R. Fiebrink. Real-time Human Interaction with Supervised\nLearning Algorithms for Music Composition and Perfor-\nmance . PhD thesis, Princeton University, NJ, USA, Jan 2011.\n[23] J. Forsyth, A. Glennon, and J.P. Bello. “Random access\nremixing on the iPad”. In Proc NIME , 2011.\n[24] O. Fried, Z. Jin, R. Oda, and A. Finkelstein. “AudioQuilt:\n2D arrangements of audio samples using metric learning and\nkernelized sorting”. In Proc NIME , 2014.\n[25] O. Gillet and G. Richard. “Drum loops retrieval from spoken\nqueries”. JIIS, 24(2-3):159–177, 2005.\n[26] T. Grill and A. Flexer. “Visualization of perceptual qualities\nin textural sounds”. In Proc ICMC , 2012.\n[27] E.J. Humphrey, D. Turnbull, and T. Collins. “A brief review\nof Creative MIR”. In ISMIR Late-Breaking News and Demos ,\n2013.\n[28] M. Hutter and S.J. Cunningham. “Towards the design of a\nkids’ music organizer”. In Proc CHINZ , 2008.\n[29] C. Inskip and F. Wiering. “In their own words: Using text\nanalysis to identify musicologists’ attitudes towards technol-\nogy”. In Proc ISMIR’15 , 2015.\n[30] T. Jenkinson. “Collaborating with machines”. Flux Magazin\n(www.ﬂuxmagazine.com) , March 2004.\n[31] A. Kapur, M. Benning, and G. Tzanetakis. “Query-by-beat-\nboxing: Music retrieval for the DJ”. In Proc ISMIR , 2004.\n[32] S. Katan, M. Grierson, and R. Fiebrink. “Using interactive\nmachine learning to support interface development through\nworkshops with disabled people”. In Proc CHI , 2015.\n[33] P. Knees and K. Andersen. “Searching for audio by sketching\nmental images of sound – A brave new idea for audio retrieval\nin creative music production”. In Proc ICMR , 2016.\n[34] A. Laplante and J.S. Downie. “Everyday life music\ninformation-seeking behaviour of young adults”. In Proc IS-\nMIR, 2006.\n[35] J.H. Lee and S.J. Cunningham. “Toward an understanding of\nthe history and impact of user studies in music information\nretrieval”. JIIS, 41(3):499–521, 2013.\n[36] J.H. Lee and R. Price. “Understanding users of commercial\nmusic services through personas: Design implications”. In\nProc ISMIR , 2015.\n[37] L. Li, D. Wang, T. Li, D. Knox, and B. Padmanabhan.\n“Scene: A scalable two-stage personalized news recommen-\ndation system”. In Proc SIGIR , 2011.[38] E. Maestre, R. Ram ´ırez, S. Kersten, and X. Serra. “Expressive\nconcatenative synthesis by reusing samples from real perfor-\nmance recordings”. CMJ , 33(4):23–42, Dec 2009.\n[39] D.C. Moffat and M. Kelly. “An investigation into people’s\nbias against computational creativity in music composition”.\nAssessment , 13(11), 2006.\n[40] B.S. Ong and S. Streich. “Music loop extraction from digital\naudio signals”. In Proc ICME , 2008.\n[41] E. Pampalk, P. Hlavac, and P. Herrera. “Hierarchical organi-\nzation and visualization of drum sample libraries”. In Proc\nDAFx , 2004.\n[42] A.-L. Radu, B. Ionescu, M. Men ´endez, J. St ¨ottinger,\nF .Giunchiglia, and A. De Angeli. “A hybrid machine-crowd\napproach to photo retrieval result diversiﬁcation”. Proc MMM\n2014.\n[43] A. Said, B. Fields, B.J. Jain, and S. Albayrak. “User-centric\nevaluation of a k-furthest neighbor collaborative ﬁltering rec-\nommender algorithm”. In Proc CSCW , 2013.\n[44] N. Schnell, F. Bevilacqua, N. Rasamimanana, J. Blois,\nF. Gu ´edy, and E. Fl ´ety. “Playing the ‘MO’ – Gestural Con-\ntrol and Re-Embodiment of Recorded Sound and Music”. In\nProc NIME , 2011.\n[45] D. Schwarz. “Current research in concatenative sound syn-\nthesis”. In Proc ICMC , 2005.\n[46] D. Schwarz and N. Schnell. “Sound search by content-based\nnavigation in large databases”. In Proc SMC , 2009.\n[47] V . Shklovsky. Art as Technique. In: Russian Formalist Crit-\nicism: Four Essays, 1965. Trans. L.T. Lemon and M.J. Reis.\nUniversity of Nebraska Press, Lincoln, USA, 1917.\n[48] B. Smyth and P. McClave. “Similarity vs. diversity”. In Proc\nICCBR , 2001.\n[49] D. Stowell and S. Dixon. “MIR in school? Lessons from\nethnographic observation of secondary school music classes”.\nInProc ISMIR , 2011.\n[50] S. Streich and B.S. Ong. “A music loop explorer system”. In\nProc ICMC , 2008.\n[51] H.L. Tan, Y . Zhu, S. Rahardja, and L. Chaisorn. “Rhythm\nanalysis for personal and social music applications using\ndrum loop patterns”. In Proc ICME , 2009.\n[52] P. Wright and J. McCarthy. “Empathy and experience in\nHCI”. In Proc CHI , 2008.\n[53] Y .C. Zhang, D. ´O S´eaghdha, D. Quercia, and T. Jambor. “Au-\nralist: Introducing serendipity into music recommendation”.\nInProc WSDM , 2012.\n[54] A. Zils and F. Pachet. “Musical mosaicing”. In Proc DAFx ,\n2001.128 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Exploring the Latent Structure of Collaborations in Music Recordings: A Case Study in Jazz.",
        "author": [
            "Nazareno Andrade",
            "Flavio V. D. de Figueiredo"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416176",
        "url": "https://doi.org/10.5281/zenodo.1416176",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/251_Paper.pdf",
        "abstract": "Music records are largely a byproduct of collaborative ef- forts. Understanding how musicians collaborate to create records provides a step to understand the social produc- tion of music. This work leverages recent methods from trajectory mining to investigate how musicians have col- laborated over time to record albums. Our case study an- alyzes data from the Discogs.com database from the Jazz domain. Our analysis examines how to explore the latent structure of collaboration between leading artists or bands and instrumentists over time. Moreover, we leverage the latent structure of our dataset to perform large-scale quan- titative analyses of typical collaboration dynamics in dif- ferent artist communities.",
        "zenodo_id": 1416176,
        "dblp_key": "conf/ismir/AndradeF16",
        "content": "EXPLORING THE LATENT STRUCTURE OF COLLABORATIONS IN\nMUSIC RECORDINGS: A CASE STUDY IN JAZZ\nNazareno Andrade\nUniversidade Federal de Campina GrandeFlavio Figueiredo\nIBM Research - Brazil\nABSTRACT\nMusic records are largely a byproduct of collaborative ef-\nforts. Understanding how musicians collaborate to create\nrecords provides a step to understand the social produc-\ntion of music. This work leverages recent methods from\ntrajectory mining to investigate how musicians have col-\nlaborated over time to record albums. Our case study an-\nalyzes data from the Discogs.com database from the Jazz\ndomain. Our analysis examines how to explore the latent\nstructure of collaboration between leading artists or bands\nand instrumentists over time. Moreover, we leverage the\nlatent structure of our dataset to perform large-scale quan-\ntitative analyses of typical collaboration dynamics in dif-\nferent artist communities.\n1. INTRODUCTION\nCollaboration is a major component of musical creation.\nExamining who has collaborated in a record is a common\nmethod to understand their style, content, and process of\ncreation. Collaborators leave a mark in the music, and may\naffect the style of the leading artists themselves. For ex-\nample, the fact that Miles Davis collaborated with Charlie\nParker in the beginning of his career can be seen as an im-\nportant inﬂuence in the development of his style.\nLooking at a larger picture, understanding the string of\ncollaborators of a musician over his or her career is also\nproliﬁc source of information to understand the career it-\nself. Reusing the same example, it is possible to partly de-\nscribe changes in Miles Davis’ style in the 70s by describ-\ning how he changed the musicians recording with him. At\nthe same time, similarities in the sequence of collaborators\nfor two artists may denote similarities in the artists them-\nselves. Complementarily, identifying common sequences\nof leading artists with which different instrumentists have\nperformed also helps understanding how styles and com-\nmunities of musical creation evolve.\nFrom a quantative standpoint, collaboration patterns\nhave often been studied through the use of methods\nfrom graph analysis to large-scale collaboration networks\n(e.g. [1, 9, 16, 18, 21]). However, although these methods\nc/circlecopyrtNazareno Andrade, Flavio Figueiredo. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Nazareno Andrade, Flavio Figueiredo. “Exploring The\nLatent Structure of Collaborations in Music Recordings: A Case Study\nin Jazz”, 17th International Society for Music Information Retrieval Con-\nference, 2016.provide valuable insights, they fail to focus on longitudi-\nnal views of collaboration. This way, they do not allow for\nexamining patterns in collaboration trajectories.\nThis work leverages recent methods proposed for min-\ning trajectories in object consumption to study collabora-\ntions trajectories among musicians. We use TribeFlow [7],\na method recently shown to accurately and expressively\ndiscover latent spaces of consumption sequences in the\nWeb domain [7]. Our work explores how this model can\nalso be used to discover latent structures in the trajectories\nof musicians as they collaborate with the leading artists or\nbands in records. This exploration is done through a case\nstudy with Jazz records. Collaborators and musicians as\nextracted from the Discogs.com collaborative database of\ndiscographic information.\nThe rest of this paper is organized as follows. In the\nnext section we discuss related work. An overview of the\nTribeFlow model is described in Section 3. This is fol-\nlowed by a description of our datasets in Section 4. Our\nmain results are discussed in Sections 5 to 7. Section 5\ndiscusses the latent trajectories (collaboration spaces) ex-\ntracted with TribeFlow. Here, we discuss how the method\nextracts a semantically meaningful latent representation of\nour datasets. In Section 6 we compare the collaboration\nspaces of different artists. Section 7 discusses how artists\nmove between collaboration spaces over time. Finally, in\nSection 8 we conclude the paper.\n2. RELATED WORK\nOur cultural products, music being no exception, are\nstrongly tied of our social interactions, and in particular\nto the dynamics of such interactions. Realizing the im-\nportance of understanding networks of interacting collab-\norators, various research efforts have looked into large-\nscale creation, dissemination and curation of information\nby groups of individuals [2,3,5,10–12,15,17,20,21]. Some\nefforts have also speciﬁcally focused on understanding mu-\nsical recordings as a collaborative effort [1,8,9,16,18,19].\nNevertheless, much less attention has been given to the dy-\nnamics of collaborations trajectories as we do.\nWith regards to musical production, very recently Bae\net. al. [1] looked into the network properties and commu-\nnity structure of the ArkivMusic1database. This database,\ncontains meta-data on classical music records. The authors\nlooked into complex network properties such as power-\nlaw distributions and the small world effect [5] that exist\n1http://www.arkivmusic.com633in such networks. Similar studies based on complex net-\nworks has also been performed for Brazilian music [9,18],\nas well as contemporary popular music [16].\nRegarding the community structure of collaboration\nnetworks, again the work of Bae et. al. [1] performed an\nanalysis of the ego-network of contributors. Their analysis\nuncovered strength of social connections from performers\nof classical musical. In a similar note, Gleiser and Danon\nalso looked into communities of jazz musicians [8].\nOur approach in this paper complements the above by\nviewing collaborations as dynamic trajectories, not static\nnetworks as was done previously. This novel way of look-\ning into collaboration employs recent advances in trajec-\ntory mining [7]. Viewing collaborations as trajectories can\naid practitioners in understanding latent structures that re-\nﬂect on the evolving career of a musician.\nFinally, we point out that various previous efforts\nlooked into the listening behavior of users as trajecto-\nries [7, 13, 14]. To perform our study, we employ the\nTribeFlow method [7]. This method has been shown to be\nmore accurate and interpretable than state-of-the-art base-\nlines in user trajectory mining [7]. We describe the method\nin the next section.\n3. MINING COLLABORATION TRAJECTORIES\nWITH TRIBEFLOW\nMusic records (albums) represent a collaborative effort\nfrom different individuals such as band members, produc-\ners, hired instrumentists, and even graphical artists respon-\nsible for the artwork. Whenever individuals collaborate\nthey leave a trail in their career. For example, in 2005 Lu-\ncas Dos Prazeres collaborated with Nan´a Vasconcelos to\ncreate the album Chegada . Our goal in this study is to un-\nderstand the collaboration trajectory of individuals. In this\ncontext, a trajectory is an ordered sequence of collabora-\ntions by a collaborator.\nFor the sake of clarity, we differentiate between the\nartist orband leading a record and the collaborators who\nparticipate in this album. These collaborators can them-\nselves be the leading artists in other records. Paul, John,\nRingo and George are all viewed as collaborators in an al-\nbum by the artist The Beatles.\nMore formally, each trajectory deﬁnes the sequence of\nartists (ordered by time) that the collaborator contributed\nwith. Let us deﬁne an album as a timestamp, artist/band,\nand a set of collaborators. That is, r= (tr, nr, ar,Lr),\nwhere ris a record or album, tris a timestamp, nris the\nname of the album, aris the artist/band and Lris the set\nof collaborators which contributed to r. The subscript r\nidentiﬁes the release for each element of the tuple. Let R\nbe the set of records. Also, let us deﬁne that records are\nidentiﬁed by integers [1,|R|], as well as that for any pair\nof records ti≤ti+1. That is, records are ordered by the\nrelease timestamp and their ids correspond to the position\nof the record on the deﬁned ordering.\nWith the deﬁnitions above, the trajectory of a collabora-\ntorcis deﬁned as Tc=< ..., a i, ai+j, ... > , where for any\npairai,ai+jwithj≥1,ti≤ti+j(by deﬁnition, albumsids are deﬁned by their ordered timestamps). Also, c∈Li\nas well as c∈Li+j. More importantly, we focus our study\non the changes in collaborations over time. That is, we en-\nforce ai/negationslash=ai+j. With this choice, trajectories represent\nthe changes in artists chosen by a collaborator over time.\nTo exemplify a trajectory, let’s us look into John\nColtrane as a collaborator. In 1955 to 1956, Coltrane col-\nlaborated on various recordings by Miles Davis. Later,\nin 1957 Coltrane collaborated with Thelonious Monk,\nagain, in various recordings. Afterwards, John Coltrane\nreturned to collaborate with Miles Davis in 1958. Tak-\ning this small slice of time as an example, the tra-\njectory of John Coltrane would be represented as: <\nMiles Davis ,Thelonious Monk ,Miles Davis >. Notice\nthat, regardless of partaking in many records with Miles\nDavis in 1955 and 1956, the trajectory only captures the\nchange in collaboration from Davis to Monk.\nIt is important to notice that there exists a variety of la-\ntent factors that lead to a collaboration. That is, while some\ncollaborations will emerge due to geographical constraints,\nothers may exist because of musical genres, temporal inﬂu-\nence, or social network factors. The trajectory, as deﬁned\nabove, will essentially exist because of choices by the col-\nlaborator to collaborate with the artist motivated by these\nfactors. Thus, the end result, regardless the of the underly-\ning factors, is always the same a trajectory of trails/choices\n(artists) that as collaborator has worked with.\n3.1 The TribeFlow Model\nTo extract the latent structure of trajectories, we employ\nTribeFlow [7], a recent method proposed to mine sequen-\ntial data. TribeFlow has recently been shown to discover\na meaningful latent structure in a variety of different set-\ntings. Here, we apply the method to understand musical\ncollaborations based on the trajectories Tc.\nIn our setting, TribeFlow models collaborations as ran-\ndom choices over random environments by a collaborator.\nOne example of a random environment can be: Jazz Artists\nfrom New Orleans in the 1960s. Due to various constrain-\ning factors, as explained above, a collaborator will choose\nto play with an artist from this environment over a set of al-\nbums. After recording these albums, the collaborator will\nagain choose an environment (in some cases, the same as\nbefore) and move on to record more albums with different\nartists. Thus, trajectories are captured as random choices\n(or random walks) over random environments. Each envi-\nronment captures a latent factor that leads to collaborations\nbetween collaborators and artists.\nTribeFlow works using as input a set of trajectories.\nGiven the set of collaborators c∈ C, the set of artists\na∈A , as well as the set of records r∈R , TribeFlow\nwill explore as input the total set of trajectories Tc∈T.r\nwas deﬁned above. aandccan be deﬁned as the names of\nartists and collaborators, respectively. A single parameter\nis required to execute the model. This parameter k=|Z|\ncaptures the number of latent environments z∈Z.\nTribeFlow deﬁnes a Bayesian graphical model (om-\nmited due to space, see [7]) that learns by performing634 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Gibbs sampling for each entry ( ai+j) in every trajectory\nTfrom the following posterior:\nP[z|c, ai+j, ai]∝P[z|c]P[ai|z]P[ai+j|z]\n1−P[ai|z](1)\nP[a|z],P[z|c]andP[z]are all probability distributions\nestimated with TribeFlow. After the model is trained,\nthe above probabilities can be exploited to answer various\nqueries, as we now describe.\n3.2 Answering Questions with TribeFlow\nFirst, we point out TribeFlow’s model is highly inter-\npretable, since it represent probabilities over C,A, andZ.\nBy employing the graphical model described in [7], we\ncan re-arrange the probability equations to answer various\nquestions using the TribeFlow model. More speciﬁcally:\nWhat is the probability that a collaborator goes from\non to collaborate with artist aafter choosing envi-\nronment z?This initial likelihood is captured by the\nprobability P[a|z], learned by the model. It captures the\nimportance of artists in a given environment, that is, artists\nwith high P[a|z]will likely attract more collaborators\nwithin that environment.\nWhat is the probability that a collaborator goes from\ncollaborating with artist ato artist a/prime?Given that a col-\nlaborator will collaborate with various artists over a trajec-\ntory, this ﬁrst question captures the importance of artists as\nlinks to other artists. That is, how likely is a collaborator\nto follow-up on his/her career with a/primeafter playing with a.\nThis question is assessed by:\nP[a/prime|a] =/summationdisplay\nz∈ZP[a/prime|z]P[a|z]P[z] (2)\nWhat is the probability of collaborating with environ-\nment z/primeafter collaborating with environment z?This\nquestion is similar to the previous one. However, it\ncaptures the notion of transitions between environments.\nFor instance, how likely is it that a collaborator will go\nfrom playing with Dixieland artists to playing with Bebop\nartists. P[z/prime|z]is thus deﬁned as:\nP[z/prime|z] =/summationdisplay\na∈AP[a|z/prime]P[z/prime]P[a|z] (3)\nWhat is the probability that an environment zcaused\ncollaborator to go from playing with ato playing with\na/prime?This ﬁnal question can be used to explain trajecto-\nries. It capture’s how likely is an environment zto cause a\nchange in collaboration from atoa/prime. This ﬁnal question is\nanswered with the posterior equation above (Eq. 1).\n3.3 Learning the Model\nFinally, we point out that our results were achieved by ex-\necuting the method with the same parameters as discussedTable 1 . Summary of the dataset used.\nReleases # 54,466\nArtists # 23,890\nin\ndegreemedian 5\nmean 14.8\nmax 3,052\nCollaborators # 70,320\nout\ndegreemedian 1\nmean 5\nmax 830\nCollaborations # 352,932\nby the authors in [7]. More importantly, we made use of the\nTribeFlow without employing the inter-event time heuris-\ntics discussed in [7]. We found that employing such heuris-\ntics had little to no effect on our results. This effect most\nlikely happens because timestamps tare usually expressed\nin years (e.g., 2005) on the Discogs dataset. For this rea-\nson, on the data we analyzed from 15% to 30% of col-\nlaborations happen within a single year, making the time\nbetween collaborations useless in such cases (they are all\nzero). Also, frequent collaborations can happen both over\nshort and large periods of times, such as individuals that\ntake hiatuses on their careers.\nGiven the exploratory nature of our work, for the sake\nof interpretability, all analysis in this work use |Z|= 30 .\nThis number of latent environments provides an expres-\nsive range of latent factors for our purposes while keeping\nsensemaking easily tractable. To understand how to ﬁne\ntune|Z|for other tasks (e.g., prediction) see [6, 7].\n4. DATA USED\nTo investigate collaboration among musicians, we lever-\nage the Discogs.com database. Discogs is a collaborative\nsite to register and annotate discographies which makes\nits database freely available. At the time of writing, the\ndatabase registers approximately six million record re-\nleases, including multiple releases of a same record (eg.\nCD and LP or LPs releases in different countries). Part of\nthis data is annotated with genre and more speciﬁc style\ntags, and with information about which collaborators par-\nticipated in a record and in which capability. For example,\nit is registered in the database that Ron Carter played the\nbass in Charles Mingus’s record Three of Four Shades of\nBlues, initially released in 1977. As this data is collabora-\ntively created, it is likely biased towards the interests of its\ncontributors, and it is naturally incomplete. Nevertheless,\ndue to its sheer volume and to the community veriﬁcation\nof its information, this database provides a promising data\nsource for investigating how collaboration patterns can be\nunderstood through trajectory mining.\nFor this purpose, we use a dataset extracted from the\nDiscogs database as a case study in collaboration. The\ndataset is comprised of all records tagged with the Jazz\ngenre and which possess metadata about instrumentists inProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 635Table 2 . Six exemplary latent environments found learned through TribeFlow from the data. For each environment, we\npresent the ﬁrst collaborators and artist most strongly associated with the environment. Collaborators are listed ﬁrst; artists\nare in italics. All labels were given by the authors based on the artists and collaborators listed.\n(a) Bebop (b) Bebop 2 (c) Free Jazz\nDizzy Gillespie, Miles Davis,\nJ.J. Johnson, Charlie Rouse, Lucky\nThompson, Charlie ParkerJohn Coltrane, Freddie Hubbard,\nDonald Byrd, Hank Mobley, Lee\nMorganSteve Lacy, Don Cherry,\nArchie Shepp, Roswell Rudd\nLester Bowie\nDizzy Gillespie, Charlie Parker,\nMiles Davis, Thelonious Monk,\nDizzy Gillespie And His OrchestraArt Blakey & The Jazz M., Miles\nDavis, John Coltrane, Art Blakey,\nCharles Mingus, Max RoachArchie Shepp, The Sun Ra Arkestra,\nCecil Taylor, Steve Lacy\nAnthony Braxton, Ornette Coleman\n(d) Improvisation UK Big Bands (e) Italian (f) Fusion\nPaul Rutherford, Evan Parker,\nJohn Edwards, Johannes Bauer,\nPaul Rogers, Malcolm GrifﬁthsGiovanni Maier, Gianni Basso,\nLauro Rossi, Dino Piana,\nGiancarlo SchiafﬁniDon Alias, David Liebman,\nDave Holland, Joe Lovano,\nBob Berg, Mino Cinelu\nLondon Improvisers Orchestra,\nChris McGregor’s Brotherhood Of Breath,\nGlobe Unity OrchestraGiorgio Gaslini, Italian Instabile\nOrchestra, Enrico Rava, Nexus,\nChet Baker Nicola ConteMiles Davis, Jaco Pastorius,\nJohn Scoﬁeld, Chick Corea,\nMike Stern, Herbie Hancock\nthe Oct 2015 database dump. After removing duplicated\nreleases and releases with no collaboration metadata, the\nrelease name, year, and collaboration data were extracted\nfrom each release. Furthermore, we focus on instrumen-\ntists in which the metadata associated with his/her role in\nthe record contained one of the following words: bass,\nguitar, drum, vocal, voic, percuss, keyboard, trumpet, sax,\nsaxophon, trombon, ﬂute, synthes .\nThe data resulting from this process is summarized in\nTable 1. Considering a collaboration as a (collaborator,\ntime, leading artist, record) tuple, the in degree of an artist\nadenotes the number of distinct tuples where ais present\nin the data. Similarly, the out degree of a collaborators c\ndenotes the number of distinct tuples in which cis present.\nRon Carter and Miles Davis are the collaborators and artist\nwith the largest degree in the data. Inspecting the releases,\nit is possible to note that the numbers of popular artists are\nslightly inﬂated due to compilation releases.\n5. LATENT TRAJECTORY SPACES\nEach latent environment found by TribeFlow can be seen\nas the result of a set of latent factors that inﬂuence the\nmovement of collaborators between artists. Inspecting the\ncollaborators and artists most associated with each envi-\nronment thus sheds light on what are the relevant factors\naffecting collaboration sequences in our dataset.\nThe latent environments found in our case study reveal\nclear loadings on the environments of stylistic, geographi-\ncal and chronological latent constraints that shape collab-\noration trajectories. Table 2 shows six exemplary latent\nenvironments as described by the collaborators and artists\n(in italics) most associated with them2.\nIt is possible to clearly distinguish in the ﬁrst four envi-\n2Access to the list environments and probabilities is available on-\nline at: https://github.com/flaviovdf/tribeflow/ on the\nfolder scripts/ismir2016annotated .ronments the styles of jazz most often associated with the\nartists, and to note that in several cases collaborators have\nnotoriously recorded with multiple artists in that environ-\nment. It is worthwhile noting that we remove collabora-\ntions where the collaborator and artist are equal. For exam-\nple, there are recordings in the data both of Dizzy Gillespie\ncollaborating in Miles Davis albums and vice-versa.\nAs for the chronology, it is possible to see in the exam-\nples that collaborations of a same period are loaded in dif-\nferent environments. For example, environments (a),band\nfare all associated with Miles Davis. However, his collab-\norators are divided in these three spaces according to the\nperiod of the collaboration. Most markedly, it is possible to\ndistinguish his collaborators from the 70s in environment\n(f)versus earlier collaborators in environment (b)and even\nearlier on (a).\nSimilarly, the set of top collaborators listed in the (d)en-\nvironment is a core part of the three bands listed as artists.\nMoreover, in this case, as in the environment e, there is a\nrelation of collaborators, artists and geography: the three\nartist groups listed in environment (d)are largely based in\nthe UK, while artists in environment (e)are mostly Ital-\nians. Along the same lines, there are latent environments\nnot listed in Table 2 that group Brazilian or Scandinavian\njazzists, among others.\nBefore continuing, we point out that various efforts\nlooked into the trajectories of users listening to music [7,\n13, 14]. In our study, we make use of TribeFlow [7], a\nrecent trajectory mining technique that has been shown to\nbe both accurate and interpretable when compared to other\nstate-of-the-art methods. We describe the method in the\nnext section.\nBesides looking at the intuitive similarities of collab-\norators or artists associated with an environment, a sec-\nond possibility for sensemaking is to look for less obvi-\nous associations. For example, the presence of Chet Baker,\nan artist mostly known for his work in the USA, in envi-636 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 201602468\n0.3 0.4 0.5 0.6 0.7 0.8\nNorm. Entropy of P[a|z]countFigure 1 . Normalized entropy of P[a|z]. The normalized\nentropy was computed for every z\nronment (e)is an example of a relationship less obvious\nfor some. An investigation of Chet Baker’s collaborators\nshows that during his work and late life in Europe, Chet\nBaker recorded more than one album with a cast of mostly\nItalian musicians. Several of these musicians (eg. Enrico\nRava) have trajectories of collaboration that touch on other\nItalian artists in Table 2.\n6. DIFFERENCES IN COLLABORATION SPACES\nBecause TribeFlow has a probabilistic interpretation, it al-\nlows an analyst to investigate differences in probabilities\nlearned for transitioning to/from different artists and latent\nenvironments. A relevant tool for doing so in our context\nis to examine the entropy of the probability distributions\nextracted with TribeFlow.\nAlong those lines, we ﬁrst investigate which latent en-\nvironments have a high/low entropy [4] in P[a|z]. Recall\nthat, entropy captures the expected uncertainty in a proba-\nbility distribution. Higher values of entropy indicate that a\ndiscrete distribution, our case, is closer to being uniform.\nLower values of entropy indicate that the distribution is\nskewed to a subset of artist in our case.\nIn other words, the entropy of P[a|z]captures the no-\ntion that after choosing to collaborate an artist from z, what\nis the uncertainty of choosing an artist. Environments with\nhigher entropy indicate that most collaborations remain\nwithin a small subset of the artists.\nIn the following, we use the normalized en-\ntropy. Normalization is performed dividing the entropy/summationtext\na−P[a|z]log(P[a|z])for each space zin the model by\nthat of a Uniform distribution over the same artists. Fig-\nure 1 displays the distribution of the values of the normal-\nized entropies for each latent environment.\nIn our data, the latent environments associated with the\nhighest values of entropy display a normalized entropy of\napproximately 0.7. These environments seem to be either\nassociated with free or experimental jazz, or to be mostly\nformed by collaborators and artists from a speciﬁc region\noutside the USA. For example, environments (d)and(e)\nare among the highest entropy environments, together with\nan environment associated with North-European jazz and\nanother one associated with experimental jazz fused with\nWorld Music. As for the environments with least entropy,\n0200040006000\n0.4 0.6 0.8\nNorm. Entropy of P[a'|a]countFigure 2 . Normalized entropy of P[a/prime|a]. The normalized\nentropy was computed for every a/prime/negationslash=a.\nP[z' | z]\n0 1 2 3 4 5 6 7 8 9 1011121314151617181920212223242526272829\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29To z'FromFrom 27 to 29 \nrepresents a transition \nfrom Bepop 1 to \nBepop 2 in Table 1The strong diagonal indicates\nthat staying with similar collaborators\nover consecutive albums is a common\ntrend for a Jazzist\nFigure 3 . The P[z/prime|z]highlighting transitions that are\nabove uniform chance ( 1/Z)\nthe apparent pattern for top-3 environments is the combi-\nnation of artists from older periods with the presence of\na seminal ﬁgure. Namely, these three environments have\nDuke Ellington, Count Basie and Louis Armstrong as their\ntop artists. This suggests that collaborators in our data as-\nsociated with these latent environments had their trajecto-\nries gravitating around these artists with not much collab-\noration with the other artists in the same environments.\nA similar approach can be used to identify who are\nthe artists which have the highest entropy considering the\nprobabilities of collaborating with other artists afterwards\n(P[a/prime|a]- Eq. (2)). The ﬁve highest-entropy artists in\nthis view are all jazzists often associated with avant-garde\nor free jazz: Anthony Braxton, Peter Br ¨otzmann, Franz\nKoglmann, Herb Robertson, and Gerry Hemingway. In a\nsense, our model points that collaborators who record with\nthese artists are follow no clear pattern in the following\ncollaborations. This can be seen as a sign of the openess\nin the choice of these artists in collaborating. On the other\nend of the spectrum, musicians collaborating with artists in\nthe former Czech Republic (eg. Czechoslovak Radio JazzProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 6371950 1960 1970 1980 1990 2000 2010\nYear0.0 0.5 1.0Likelihood (Avg over 5 years)Miles Davis\nJohn Coltrane\nFreddie Hubbard\nDonald Byrd\nHank Modbley\nWayne ShorterDizzy Gillespie\nJ.J. Johnson\nCharlies Rouse\nLucky Thompson\nPhil Woods, Al Cohn, Bernie Glow, Urbie Green\nBaker, Konitz, Demon, MarshDon Alias\nDavid Liebman\nDave Holland\n John LovanoFigure 4 . The career of Miles Davis as captured by the model.\nOrchestra, Prague Big Band, Ji Vlek) are the ones display-\ning the least entropy. This last example is likely caused by\ngeographical constraints.\n7. MOVEMENT ALONG LATENT SPACES\nConsidering the probabilities of a collaborator transition-\ning between distinct spaces P[z/prime|z](Eq. (3)) gives us a\nview of which latent environments are likely to be adja-\ncent in collaborators’ trajectories. In Figure 3, we depict\nthis transition matrix. For the sake of clarity, we highlight\nin this matrix only transitions that were above uniform ran-\ndom chance (1\nZ). In this plot, latent spaces are numbered\nfrom 0 to k−1.\nObserving the model ﬁt to the data, we see that the\npairs of latent environments with the highest probabilities\nof transition between them will be on the diagonals. That\nis, artists will likely remain collaborating within the same\nstyle after the previous records. Although this is expected,\nthere interesting examples in the non-diagonals as well.\nFor example, the highest probability in a non-diagonal hap-\npens between an environment where the most likely artist\nto collaborate with is Stan Kenton and a second environ-\nment where the correspondent artist is Woody Herman.\nThese are two Big Band leaders who led popular bands\nduring the ﬁrst half of the 20th century. The following ﬁve\nhighest probabilities follow a similar pattern, with the ﬁfth\nbeing between environments (b)and(a)in Table 2.\nA ﬁnal frame in which we explore how to use TribeFlow\nin the context of collaboration trajectories is to inspect the\ntrajectories associated with a prominent artist. Figure 4\nshows the likelihood of an environment ( P[z|c, ai+j, a]-\nEq. (1)) of all collaborator transitions to reach Miles Davis\nover a period to be associated with each latent environ-\nment. We averaged this likelihood over every ﬁve years.\nFor this case, there is a marked change in the likely\nsource of collaborators from the environments (a)to(b)from Table 2. This change correlates with a major change\nin the Miles Davis Quintet to the group that would com-\npose it during the ﬁrst half of the 60s. Wayne Shorter is one\nof the collaborators strongly associated with environment\n(b)who also recorded with multiple other artists associated\nwith this environment, such as Freddie Hubbard.\n8. DISCUSSION\nExamining collaboration patterns is an important endeavor\nin understanding artists’ inﬂuences and creations. Through\na case study of collaboration in Jazz records, we have ex-\nplored how to use TribeFlow to unveil latent structures in\nthe trajectories of collaborators. The latent environments\nfound in this case study are expressive and were able to\nhelp sense making in both popular and more niche collab-\noration groups in the data. Moreover, these environments\nseem to express at least stylish, chronological and geo-\ngraphical factors shaping collaboration trajectories. Due\nto the Bayesian approach of TribeFlow, it is possible to\nemploy a direct probabilistic approach to investigate ques-\ntions of association at different levels of relationship, such\nas artist to artist, and artist to environment.\nFuture work may extend the approach of this paper in\ndeepening the link of the analysis conducted here in its\nmusicological and historical aspects, employing a similar\napproach to other datasets, and extending both our model-\ning approach and the tools used so far to compare collabo-\nration in different communities. Moreover, understanding\nhow collaboration trajectories related to musical features\n(e.g., beat or tempo) can also help researchers better un-\nderstand collaboration in recordings.\nAcknowledgments: This research was supported by grant\n460154/2014-1 from CNPq, by the EU-BR BigSea project\n(MCTI/RNP 3rd Coordinated Call), as well as the EUBrazilCC\n(grants FP7 614048 and CNPq 490115/2013-6) project.638 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20169. REFERENCES\n[1] Arram Bae, Doheum Park, Yong-Yeol Ahn, and Juy-\nong Park. The multi-scale network landscape of col-\nlaboration. PloS one , 11(3):e0151784, 2016.\n[2] Albert-L ´aszl´o Barab ´asi, Hawoong Jeong, Zoltan N ´eda,\nErzsebet Ravasz, Andras Schubert, and Tamas Vicsek.\nEvolution of the social network of scientiﬁc collabora-\ntions. Physica A: Statistical mechanics and its applica-\ntions , 311(3):590–614, 2002.\n[3] Alex Beutel, B Aditya Prakash, Roni Rosenfeld, and\nChristos Faloutsos. Interacting viruses in networks:\ncan both survive? In Proc. KDD . ACM, 2012.\n[4] Thomas M. Cover and Joy A. Thomas. Elements of In-\nformation Theory (Wiley Series in Telecommunications\nand Signal Processing) . Wiley-Interscience, 2006.\n[5] David Easley and Jon Kleinberg. Networks, crowds,\nand markets: Reasoning about a highly connected\nworld . Cambridge University Press, 2010.\n[6] Flavio Figueiredo, Bruno Ribeiro, Jussara M Almeida,\nNazareno Andrade, and Christos Faloutsos. Mining\nOnline Music Listening Trajectories. In Proc. ISMIR ,\n2016.\n[7] Flavio Figueiredo, Bruno Ribeiro, Jussara M Almeida,\nand Christos Faloutsos. TribeFlow: Mining & Predict-\ning User Trajectories. In Proc. WWW , 2016.\n[8] Pablo M Gleiser and Leon Danon. Community struc-\nture in jazz. Advances in complex systems , 6(04):565–\n573, 2003.\n[9] Charith Gunaratna, Evan Stoner, and Ronaldo\nMenezes. Using network sciences to rank musicians\nand composers in brazilian popular music. In Proc. IS-\nMIR, 2011.\n[10] Paul Heymann and Hector Garcia-Molina. Collabora-\ntive creation of communal hierarchical taxonomies in\nsocial tagging systems. 2006.\n[11] Brian Karrer and Mark EJ Newman. Competing epi-\ndemics on complex networks. Physical Review E ,\n84(3):036106, 2011.\n[12] Paul Lamere. Social tagging and music information re-\ntrieval. Journal of new music research , 37(2):101–114,\n2008.\n[13] Andryw Marques, Nazareno Andrade, and Lean-\ndro Balby Marinho. Exploring the Relation Between\nNovelty Aspects and Preferences in Music Listening.\nInProc. ISMIR , 2013.\n[14] Joshua L. Moore, Shuo Chen, Thorsten Joachims, and\nDouglas Turnbull. Taste Over Time: The Temporal Dy-\nnamics of User Preferences. In Proc. ISMIR , 2013.\n[15] Mark EJ Newman. Scientiﬁc collaboration networks.\nPhysical review E , 64(1):016131, 2001.[16] Juyong Park, Oscar Celma, Markus Koppenberger,\nPedro Cano, and Javier M Buld ´u. The social net-\nwork of contemporary popular musicians. Interna-\ntional Journal of Bifurcation and Chaos , 17(07):2281–\n2288, 2007.\n[17] Maximilian Schich, Chaoming Song, Yong-Yeol Ahn,\nAlexander Mirsky, Mauro Martino, Albert-L ´aszl´o\nBarab ´asi, and Dirk Helbing. A network framework of\ncultural history. Science , 345(6196):558–562, 2014.\n[18] D de Lima Silva, M Medeiros Soares, MVC Henriques,\nMT Schivani Alves, SG de Aguiar, TP de Carvalho,\nG Corso, and LS Lucena. The complex network of the\nBrazilian Popular Music. Physica A: Statistical Me-\nchanics and its Applications , 332, 2004.\n[19] T Teitelbaum, P Balenzuela, P Cano, and Javier M\nBuld ´u. Community structures and role detection in mu-\nsic networks. Chaos: An Interdisciplinary Journal of\nNonlinear Science , 18(4):043105, 2008.\n[20] Jennifer Trant. Studying social tagging and folkson-\nomy: A review and framework. Journal of Digital In-\nformation , 10(1), 2009.\n[21] Brian Uzzi and Jarrett Spiro. Collaboration and creativ-\nity: The small world problem. American journal of so-\nciology , 111(2):447–504, 2005.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 639"
    },
    {
        "title": "I Said it First: Topological Analysis of Lyrical Influence Networks.",
        "author": [
            "Jack Atherton",
            "Blair Kaneshiro"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1418047",
        "url": "https://doi.org/10.5281/zenodo.1418047",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/221_Paper.pdf",
        "abstract": "We present an analysis of musical influence using intact lyrics of over 550,000 songs, extending existing research on lyrics through a novel approach using directed net- works. We form networks of lyrical influence over time at the level of three-word phrases, weighted by tf-idf. An edge reduction analysis of strongly connected compo- nents suggests highly central artist, songwriter, and genre network topologies. Visualizations of the genre network based on multidimensional scaling confirm network cen- trality and provide insight into the most influential genres at the heart of the network. Next, we present metrics for in- fluence and self-referential behavior, examining their inter- actions with network centrality and with the genre diversity of songwriters. Here, we uncover a negative correlation between songwriters’ genre diversity and the robustness of their connections. By examining trends among the data for top genres, songwriters, and artists, we address questions related to clustering, influence, and isolation of nodes in the networks. We conclude by discussing promising future applications of lyrical influence networks in music infor- mation retrieval research. The networks constructed in this study are made publicly available for research purposes.",
        "zenodo_id": 1418047,
        "dblp_key": "conf/ismir/AthertonK16",
        "content": "I SAID IT FIRST: TOPOLOGICAL ANALYSIS\nOF LYRICAL INFLUENCE NETWORKS\nJack Atherton andBlair Kaneshiro\nCenter for Computer Research in Music and Acoustics, Stanford University\n{lja, blairbo}@ccrma.stanford.edu\nABSTRACT\nWe present an analysis of musical inﬂuence using intact\nlyrics of over 550,000 songs, extending existing research\non lyrics through a novel approach using directed net-\nworks. We form networks of lyrical inﬂuence over time\nat the level of three-word phrases, weighted by tf-idf.\nAn edge reduction analysis of strongly connected compo-\nnents suggests highly central artist, songwriter, and genre\nnetwork topologies. Visualizations of the genre network\nbased on multidimensional scaling conﬁrm network cen-\ntrality and provide insight into the most inﬂuential genres\nat the heart of the network. Next, we present metrics for in-\nﬂuence and self-referential behavior, examining their inter-\nactions with network centrality and with the genre diversity\nof songwriters. Here, we uncover a negative correlation\nbetween songwriters’ genre diversity and the robustness of\ntheir connections. By examining trends among the data for\ntop genres, songwriters, and artists, we address questions\nrelated to clustering, inﬂuence, and isolation of nodes in\nthe networks. We conclude by discussing promising future\napplications of lyrical inﬂuence networks in music infor-\nmation retrieval research. The networks constructed in this\nstudy are made publicly available for research purposes.\n1. INTRODUCTION\nLyrics have been used to study many topics in music infor-\nmation retrieval (MIR) including genre classiﬁcation [6],\nhit prediction [9], similarity searching [10], cultural stud-\nies [4], and computational musicology [5]. One approach\nto lyrical analysis is the bag-of-words model, which con-\nsiders word frequencies in a text irrespective of order. In\n2004, Logan et al. used this approach to produce promising\npreliminary results for measuring artist similarity through\ntopic models, and observed that some genres naturally\ngroup with others based on shared vocabulary [9]. Fell\nand Sporleder later found that some genres (e.g. Rap,\nMetal) have relatively unique vocabularies, while others\n(e.g. Folk, Blues, and Country) cluster into groups [6].\nMost recently, Ellis et al. computed bag-of-words novelty\nc/circlecopyrtJack Atherton and Blair Kaneshiro. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Jack Atherton and Blair Kaneshiro. “I Said It First: Topo-\nlogical Analysis of Lyrical Inﬂuence Networks”, 17th International Soci-\nety for Music Information Retrieval Conference, 2016.of lyrics and found that top-100 music was less lexically\nnovel than less popular music [5].\nIn contrast, n-gram models consider ordered phrases\nofnwords. A. Smith et al. used trigrams (n-grams with\nn= 3) and rhyme structures to develop a metric for clich ´e\nin lyrics, ﬁnding that number-one hits were more clich ´ed\nthan average songs [14]; here, trigrams proved to be the\nbetter metric for measuring clich ´e. They also inspected\ntheir data by genre and found that genres had generally\nunique most-frequent phrases, though some phrases were\nshared by many genres. Later, Fell and Sporleder devel-\noped a suite of lexical features and used these, along with\nn-grams, to achieve performance gains in various classi-\nﬁcation tasks, but conﬁrmed that n-grams alone achieved\nsatisfactory baseline performance [6].\nNetworks—or graphs—are a natural and increasingly\nprevalent tool for analyzing structure between musical en-\ntities such as artists, songwriters, and genres. Networks\ncomprise sets of nodes and sets of edges, or relations, be-\ntween nodes. Most networks use unweighted, undirected\nedges, whereby edges are binary measures of whether two\nnodes are connected. Weighted edges ascribe varying im-\nportance to the relationships, and directed edges give each\nrelationship a direction or ﬂow. A 2006 study by R. Smith\nrevealed the community structure of rappers by construct-\ning a network between rappers who collaborated [15]. This\nstudy weighted edges by frequency of collaboration and\nfound that different groupings such as large communities,\nmusic labels, and groups such as the famous Wu-Tang Clan\nemerged when varying percentages of the least signiﬁcant\nedges were removed from the graph. We will refer to this\nprocess here as edge reduction . Collins later considered\nthe ﬂow of musical inﬂuence in synth-pop music [3], while\nGunaratna et al. built a collaboration network for Brazilian\nmusicians and composers [8]. Finally, Bryan and Wang\nconstructed a directed graph connecting genres based on\nsampling of musical content, showing that Funk, Soul, and\nDisco heavily inﬂuence many modern popular genres [2].\nBoth lyrics and graphs allow us to ask deep ques-\ntions about similarity, popularity, and interconnectedness\nin the music landscape. To our knowledge, no study has\nformed lyrics-based networks to analyze musical relation-\nships formed directionally over time, although Fujihara et\nal. created a system for hyperlinking between lyrics and\nfrom lyrics to audio [7]. The current study combines lyri-\ncal analysis with graphs to observe inﬂuence of genres,\nartists, and writers, through networks formed by re-use of654lyrics. Our networks’ directional edges highlight lyrical in-\nﬂuence over time, allowing us to address questions like that\nraised by A. Smith et al. of whether Pop music makes use\nof existing clich ´es or creates new ones [14]. We examine\nthe topology of inﬂuence networks of genres, artists, and\nwriters to quantiﬁably assess grouping behavior, robust-\nness of connections, and centrality within the networks.\nThe remainder of this paper is structured as follows:\nﬁrst, we explain the formation of the inﬂuence networks\nand the computation of several of their properties. Next,\nwe demonstrate and visualize the topology of the networks.\nFinally, we show how the basic building blocks of network\nproperties can be used to address many outstanding musi-\ncological and MIR research questions.\n2. METHODS\n2.1 Data Sources\nPast research has shown that n-grams (phrases) are supe-\nrior to bag-of-words (vocabulary) for lyrical MIR tasks\nsuch as classiﬁcation and computational musicology [6,\n14]. With this in mind, we obtained intact lyrics data and\nartist/writer metadata via a signed research agreement with\nLyricFind, whose data were used previously in the Ellis et\nal. bag-of-words study on lexical novelty [5]. We addition-\nally obtained primary genre and release date at the album\nlevel through the free iTunes Search API.1After ﬁltering\nout songs with no lyrics, as well as those with no entry in\nthe sparse iTunes dataset, we were left with 554,206 songs,\ncollectively representing 42,802 artists, 95,349 writers,\nand 214 genres.\n2.2 Constructing the Networks\nThe ﬁrst step was to exhaustively measure the trigrams\npresent in every song. Phrases were considered equiva-\nlent if they were cleaned to the same base phrase. We\ncleaned the lyrics using a procedure previously validated\nby Ellis et al. [5], avoiding stemming the words and using\ntheir rules for misspellings, alternate spellings, hyphens,\nand slang (modiﬁed to avoid expanding contractions and\nto correct a few inaccurate slang terms). We also ﬁltered\noutstopwords —words too common to impart any lexical\nsigniﬁcance (e.g. pronouns and articles). We used a list of\nEnglish stopwords from the Natural Language Toolkit,2\naugmented with many of the contractions ignored in the\ncleaning phase and misspellings of stopwords common to\nthe LyricFind data. If a phrase was reduced in size due\nto stopword removal, we added an additional word and re-\npeated the process until we obtained a cleaned trigram with\nno stopwords. This process allowed us to consider and\nmatch phrases originally longer than three words on the\nbasis of only semantically signiﬁcant words; for example,\ntwo four-word phrases that differed only by a speciﬁc pro-\nnoun (e.g. “he” / “she”) would be matched after stopword\n1http://apple.co/1qHOryr\n2http://www.nltk.org/removal. After initial results revealed spurious phrases cre-\nated across lines of lyrics, we modiﬁed the algorithm to\nsearch for phrases only within lines of lyrics.\nNext, songs were separated by year of release date in or-\nder to compute their phrases’ term frequency-inverse doc-\nument frequency ( tf-idf ). Tf-idf is a robust measure of sig-\nniﬁcance common to information retrieval that increases\nan item’s weight if it is common within its document and\ndecreases its weight if it is common across the dataset [11].\nPast MIR studies have used tf-idf for automatic mood clas-\nsiﬁcation of lyrics [16]—also in conjunction with rhyme\ninformation [17]—and to measure lexical novelty [5]. To\ncapture the changing signiﬁcance of phrases over time, we\ntreated each individual year as a separate dataset. This way,\nthe ﬁrst person to use a phrase would have a signiﬁcantly\nhigher idf for that phrase than a person using it when it\nis already popular. Tf-idf is computed as in equation (1),\nwhere npis the number of occurrences of a phrase pin a\nsong, nsis the number of phrases in that song, syis the\nnumber of songs in a year y, andspis the number of songs\nin that year containing p.\ntf-idf(p) =np\nns·log(sy\nsp) (1)\nWe then constructed the three inﬂuence networks, one\neach for genres, artists, and writers. For every phrase in\nthe dataset, we generated a list of all pairs of songs shar-\ning that phrase. Pairs of songs released in the same year\nwere ignored. This limit sets a minimum on the time dif-\nference necessary before a repeated phrase is considered\ninﬂuential, and also avoids forming links between potential\nduplicate entries in the dataset. For pairs of songs occur-\nring in different years, we formed an edge from the earlier\nsong to the later one. The edge’s weight was the product\nof the tf-idfs of the phrase in both songs in order to cap-\nture the signiﬁcance of the phrase in both years. Using\nsong metadata, we then added the edge to the genre, artist,\nand writer graphs. For example, if a phrase was used in a\nRock song in 1990 and in a Pop song in 1991, the resulting\nedge was drawn from the Rock node to the Pop node in the\ngenre graph. If either song had multiple artists or writers,\nwe added edges between all possible pairings. If multiple\nedges were added between two nodes, they were combined\nby summing their weights.\nNext, inﬂuence scores were computed for each node of\neach graph. The inﬂuence Iiof node iis deﬁned as the\nratio of the sum of its outgoing edge weights ( eij) to the\nsum of its incoming edge weights ( eji):\nIi=/summationtext\njeij\n/summationtext\njeji(2)\nInﬂuence is thus a measure of the degree to which a node\nimpacted future work or quoted previous work, but does\nnot depend on the node’s total volume of work.\nGenre diversity scores were computed for each artist\nand writer as the number of genres they are credited in, di-\nvided by their total number of songs. This gives a measureProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 655of how many genres the person has contributed to without\nbeing skewed by their total number of contributions.3\nFinally, each node’s self-reference score was computed\nas the weight of the edge pointing from that node to itself,\ndivided by that node’s total number of contributions. This\nnormalization again avoids skewing the score by volume,\nas edge weights were formed by summing over all possible\npairings of phrases shared between two nodes.\nThe graphs were constructed with Graph-tool.4We\nmake the graph data from this study publicly available for\nresearch purposes [1].\n2.3 Network Analyses\nThe graphs were ﬁrst assessed for clustering behavior.\nAdapting the method used by R. Smith, we analyzed the\ngraphs in stages while removing increasing percentages of\nthe least signiﬁcant edges [15]. The ﬁrst, global edge re-\nduction method removed the Xg%lowest-weighted edges\nacross the entire the graph, with Xgranging from 0 to 99.\nThe second, local method removed edges from each node\nthat have a weight less than Xl%of the strongest weight\nat that node. At each stage of both procedures, the graphs\nwere analyzed for their strongly connected components in\norder to examine the grouping behavior of the nodes.\nNext, we performed multidimensional scaling (MDS)\non the genre graph in order to embed its nodes in two di-\nmensions for visualization. MDS converts a set of pair-\nwise dissimilarities among objects into coordinates that\ncan be used to visualize the objects in a low-dimensional\nspace [13]. Here, the dissimilarities were computed using\nmutual inﬂuence Dijbelow, where eijis the weight be-\ntween nodes iandj. Graph visualizations were performed\nusing Gephi.5\nDij= log( eij∗eji)−1(3)\nFinally, we computed a series of correlations between\nthe various metrics deﬁned above, as well as the in-degree,\nout-degree, and average tf-idf of incoming and outgoing\nedges of each node. The randpvalues were computed\nwith the Scipy Statistics pearsonr function.6\n3. RESULTS\n3.1 Most Common Phrases\nTable 1 shows the phrases used across the highest number\nof years. Many of these phrases are considered timeless,\nand all are semantically signiﬁcant. Also, no phrase oc-\ncurred in every year. The repetition of “dream(s) come\ntrue” does suggest that using word stems might improve\nperformance. We note also that pronouns and other stop-\nwords are absent from all phrases, which allowed the con-\nsideration of longer phrases with internal stopwords. For\nexample, “makes feel like” is a combination of “makes\n3Un-normalized genre count did not signiﬁcantly interact with any\nother variables in the study.\n4https://graph-tool.skewed.de/\n5https://gephi.org/\n6http://docs.scipy.org/doc/scipy-0.16.0/reference/(me) feel like,” “makes (you) feel like,” and other simi-\nlar phrases. Overall, we treat these results as veriﬁcation\nthat our cleaning procedure was adequate.\nPhrase Years Phrase Years\ndreams come true 49 one two three 43\nnever let go 48 late last night 42\nnew york city 46 whole wide world 42\nlong time ago 46 come back home 42\ndream come true 45 makes feel like 41\nTable 1 : The most common phrases, ordered by number of\nyears in which they appeared (maximum possible is 62).\n3.2 Network Components\nIn our global edge reduction analysis, we expected that the\ngraphs would split into several components. Instead, each\ngraph remained concentrated in one large strongly con-\nnected component, with a few negligible side components.\nThe size of the central component at a few points in the\nglobal edge reduction process is shown in Table 2.\nThe writer graph was the most robust: its central com-\nponent was largest at nearly every point in the edge reduc-\ntion process, and with only the top 1% of edges remain-\ning in the graph ( Xg= 99 ), it still contained nearly 200\ncomponents of 5 or more writers. We believe this result\narose because many songs have multiple writers, while few\nsongs have multiple artists; therefore, more relationships\namong writers would emerge from the same songs.\nUsing the local edge reduction method, a few small, sig-\nniﬁcant components did break off of the main component.\nFor example, with Xl= 2, the pairs{Celtic, Contempo-\nrary Celtic}and{Folk-Rock, Contemporary Folk }split off\nthe main genre component. Table 3 shows the Brazilian\nand Spanish-speaking Latin American components formed\nwithXl= 3andXl= 4, respectively.\nThe graphs’ strongly connected components split apart\nmuch more quickly using local edge reduction than with\nglobal edge reduction. At Xl= 4, the main component\nconsisted of 21 genres; at Xl= 20 , the only components\nremaining of size more than 1 were the two in Table 4.\nAtXl= 25 , the main component had reduced to the pair\n{Rock, Pop}, but the Latin American component remained\nunchanged from Xl= 20 . The answer to why the Latin\nAmerican component was so robust probably lies in our\ndata preparation method: since we did not ﬁlter out the\nstopwords of any language but English, the connections\nbetween genres of other languages were strengthened by\nspurious connections with no lexical signiﬁcance. This\nresult shows the importance of a cleaning procedure that\nworks uniformly across the dataset.\nGraph Xg=0 X g=90 X g=99\nGenre 95% 54% 13%\nArtist 99% 52% 12%\nWriter 98% 66% 28%\nTable 2 : Percentage of nodes in the central component\nwithXg%edge reduction.656 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Xl=3 X l=4\nMPB Latin Pop\nSertanejo Latin Alternative & Rock\nSamba Latino\nPagode Salsa y Tropical\nAx´e Regional Mexicano\nBaladas y Boleros\nLatin Urban\nTable 3 : Genres contained in two components that split\nfrom the main component with Xl%edge reduction.\nMain Component Latin American Component\nPop Latin Pop\nRock Latin Alternative & Rock\nAlternative Latino\nR&B/Soul Salsa y Tropical\nCountry Regional Mexicano\nTable 4 : Components with Xl= 20% edge reduction.\n3.3 Multidimensional Scaling and Visualizations\nTo explore the seemingly central nature of the graphs, we\nperformed MDS and visualized the genre graph.7The vi-\nsualization (Figure 1) promisingly showed that nearly all\nedges in the graph were focused toward the center.\nTo more closely observe the behavior at the center of\nthe graph, we next visualized only the 28 genres that ap-\npear in the central component with global edge reduction\n(Xg= 99 ), and displayed only each node’s top three in-\ncoming edges. Using dissimilarities computed from all\nedges (not just those present in the visualization), we\nperformed MDS again to obtain the node positions for\nFigure 2 and Figure 3. Jazz, Pop, and Rock are ﬁrmly at\nthe center of the genre network. Here, 21 of the 28 nodes in\nthe central component include Jazz among their top three\ninﬂuences, while 16 include Pop and 13 include Rock.\n3.4 Inﬂuence and Self Reference\nWe next sought a statistical explanation for the central-\nity of the highlighted nodes in Figure 1. Turning ﬁrst\nto inﬂuence, the ratio of a node’s outgoing to incoming\nedge weights, we expected that central genres would have\nhigh inﬂuence, meaning that many genres draw from the\nphrases used in the central genres. In fact, the extremes of\nthe inﬂuence metric are dominated by outliers, including\nrare genres as well as artists and writers who appear only\nvery early or very late in the dataset. These groups do not\nhave much opportunity for incoming or outgoing edges,\nrespectively. In contrast, central genres are referenced at\nabout the same rate that they reference previous material,\nhaving inﬂuence scores close to 1.0. Figure 2 and Table 5\nshow the inﬂuence of central genres.\nWe then turned to the self-reference score, a measure\nof how much a genre re-uses lyrics from its past. Our in-\ntuition here was that genres that refer to themselves fre-\nquently create a particular subculture that is ripe for other\ngenres to draw inﬂuence from. Table 5 shows the top 10\n7Because of their sheer size, visualizations of the artist and writer\ngraphs were not feasible at the time of the study.\nFigure 1 : MDS layout of the genre graph, having 214\nnodes and 17,438 edges. World, far left, is pushed out of\nthe central area by its extreme dissimilarity to Pop Punk,\nfar right. Light gray nodes are the 28 nodes in the central\ncomponent with global edge reduction, Xg= 99% .\nJazz\nCountryTribute\nR&B/SoulRock\nEasy ListeningPopBig Band\nHolidaySinger/Songwriter WorldAlternativeSoundtrackBlues\nChristmasHip-Hop/Rap\nVocal\nChristian & GospelLatino\nReggaeDance\nBrazilianElectronic\nNew AgeVocal PopClassicalMetalVocal Jazz\nFigure 2 : MDS layout of the central component of the\ngenre graph. Node size denotes node inﬂuence, and arrow\nsize denotes edge weight. Here, the sensitivity of the in-\nﬂuence metric to outliers is shown (for example, with the\nlarge size of the Brazilian node).\nJazz\nCountryTribute\nR&B/SoulRock\nEasy ListeningPopBig Band\nHolidaySinger/Songwriter WorldAlternativeSoundtrackBlues\nChristmasHip-Hop/Rap\nVocal\nChristian & GospelLatino\nReggaeDance\nBrazilianElectronic\nNew AgeVocal PopClassicalMetalVocal Jazz\nFigure 3 : MDS layout of the central component of the\ngenre graph. Node size denotes self-reference, and arrow\nsize denotes edge weight. Self-reference, more than inﬂu-\nence (Figure 2), correlates with centrality in the network.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 657Genre Inﬂuence (%) Self-reference (%)\nJazz 1.039 (65 .3%) 166 .516 (100%)\nHoliday 1.031 (64 .5%) 82 .276 (99 .6%)\nBlues 1.176 (73 .6%) 7 .698 (99 .1%)\nV ocal 1.301 (76 .6%) 6 .818 (98 .6%)\nR&B/Soul 0.987 (63 .2%) 6 .369 (98 .1%)\nCountry 0.974 (60 .2%) 6 .202 (97 .7%)\nRock 1.083 (67 .5%) 6 .124 (97 .2%)\nPop 0.836 (51 .9%) 6 .074 (96 .7%)\nChristian & Gospel 1.149 (71 .9%) 5 .883 (96 .3%)\nChristmas 0.690 (47 .6%) 5.759 (95 .8%)\nTable 5 : Top genres by self-reference. Raw values and\npercentiles are shown.\ngenres by self-referential behavior. These genres indeed\ncorrespond with genres near the center of the graph, and\nthey are all in the central component of Figure 1. Jazz and\nHoliday have particularly outlying scores, perhaps reﬂect-\ning that these genres often consist of standards —covers of\nwidely known songs. Across the entire genre graph, log-\nself-reference correlates with centrality in the MDS graph\n(measured as the negative sum of euclidean distances from\na node to all others), with r= 0.610,p <0.0001 .\nAfter seeing the importance of self-reference in the\ngenre graph, we computed this metric for the other two\ngraphs. The list of top artists by self-reference is domi-\nnated by Jazz artists. This could reﬂect more the outlying\nnature of the genre than it does anything about Jazz artists.\nHowever, viewing the top artists by decade yields some in-\nteresting results when Jazz artists are ignored. For exam-\nple, Sam Smith, Two Door Cinema Club, and Owl City are\namong the top-10 most self-referential artists of the 2010s.\n3.5 Self-Reference and Volume\nTo better understand the metrics of self-reference and in-\nﬂuence, we assessed their correlations with other aspects\nof the data. First, we determined the extent to which\ngenre volume (number of songs in a genre) affects its self-\nreference. Using the un-normalized value of self-reference\n(i.e. the genre’s raw self edge weight, not divided by the\ngenre’s volume), log-self-reference correlates highly with\nlog-volume ( r= 0.846,p < 0.0001 ). This is because\nthe raw edge weight is a sum of all connections between\nsongs of that genre, and more songs allow more connec-\ntions. But, when self-reference is normalized by genre vol-\nume (see§2.2), log-self-reference still correlates highly\nwith log-volume ( r= 0.780,p <0.0001 ). Our intuition\nfor this is that as the number of songs in a genre increases,\nthe average quality of self-references increases, and so the\nnormalized contribution from each song increases.\n3.6 Genre Diversity, Inﬂuence, and Connectedness\nHaving investigated self-reference as a measure of phrase\nsharing within genres, we next assessed sharing of phrases\nacross genres. We expected that people with high genre di-\nversity are able to transfer phrases between genres, which\nwould increase their inﬂuence score as the transferred\nphrases are referenced by people with less genre diversity.Name Inﬂuence (%) Self-Reference (%)\nPaul McCartney 0.970 (55 .4%) 114 .061 (99 .8%)\nJohn Lennon 0.974 (55 .6%) 119 .660 (99 .8%)\nMax Martin 0.421 (31 .7%) 0.739 (70 .4%)\nMariah Carey 1.039 (59 .6%) 3 .595 (88 .9%)\nBarry Gibb 1.111 (62 .4%) 7 .661 (95 .0%)\nTable 6 : Top writers, ordered by number-one singles. Raw\nvalues and percentiles are shown.\nHowever, we found that inﬂuence has a low, though sta-\ntistically signiﬁcant correlation with genre diversity ( r=\n0.079for artists, r= 0.155for writers, p <0.0001 ).\nSurprised by this result, we investigated further the ef-\nfect of genre diversity on connections in the network. First,\nwe investigated whether drawing inﬂuence from many\ngenres correlates with more complex references and found\na low, though statistically signiﬁcant, correlation between\nwriters’ log-average incoming tf-idf value and genre diver-\nsity (r=−0.167,p < 0.0001 ). Next, we examined the\ndegree to which writing in many genres correlates with di-\nrectional inﬂuence forward and backward in time. We ac-\ntually found a negative correlation between log-out-degree\nand genre diversity ( r=−0.572,p <0.0001 ), as well as\nbetween log-in-degree and genre diversity ( r=−0.563,\np < 0.0001 ). Thus, although inﬂuence (the ratio of out-\ngoing to incoming edges) explains little genre diversity,\nincreased genre diversity correlates moderately with less\nrobust connections within the graph in both future and past\ndirections. This could suggest that writers who contribute\nto a wider variety of genres use complicated phrases that\nare less likely to be shared with other writers, or that they\nuse more stopwords that are ﬁltered by the algorithm.\nArtists showed similar correlation behavior to writers,\nbut with lower correlation magnitudes, perhaps reﬂecting\nthat artists are often a step removed from writing lyrics and\nmay perform lyrics written by a variety of writers.\n3.7 Top Genres, Writers, and Artists\nWe showed in§3.5 that the volume of a genre in the dataset\ncorrelates highly with its self-reference score. Compared\nto other popular music genres, Rap has a particularly low\nself-reference score: it is the 6th most numerous genre in\nthe data, but ranks 48th in self-reference. Similarly, Metal\nis the 8th most numerous genre in the data, but ranks 40th\nin self-reference. Rap’s low self-reference score may re-\nﬂect a particular subculture within this genre that values\nlyrical originality over references to past material.\nHaving analyzed lyrical inﬂuence between genres over\ntime, we can now address whether Pop music is more\nclich ´ed than other genres because it draws from many\nsources or because it popularizes new phrases [14]. Rock\nand Pop are the two most common genres in our dataset.\nRock’s inﬂuence score is 1.083, while Pop’s is only 0.836.\nSince Pop’s inﬂuence is less than 1.0, Pop music quotes\nphrases from other genres more often than it inﬂuences\nthem. This suggests that Pop music draws on existing\nclich ´es more than it creates new ones, especially when\ncompared to other popular genres such as Rock.658 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Name Inﬂuence Self-Reference\nThe Beatles 0.140 (21 .5%) 2 .699 (95 .4%)\nElvis Presley 0.847 (56 .1%) 9.964 (99 .6%)\nMariah Carey 1.688 (73 .0%) 3 .362 (96 .6%)\nRihanna 0.381 (36 .9%) 1.114 (90 .2%)\nMichael Jackson 0.869 (56 .8%) 6 .527 (98 .9%)\nThe Supremes 0.762 (53 .6%) 5 .068 (98 .2%)\nMadonna 3.901 (87 .1%) 1 .344 (91 .3%)\nWhitney Houston 2.010 (76 .5%) 2 .369 (94 .7%)\nStevie Wonder 0.247 (29 .0%) 2 .297 (94 .5%)\nJanet Jackson 1.162 (64 .2%) 1 .723 (92 .8%)\nTable 7 : Top artists, ordered by number-one singles. Raw\nvalues and percentiles are shown.\nThe all-time top writers of number-one hits have inﬂu-\nence scores close to 1, with more successful writers having\nslightly lower inﬂuence scores (Table 6). This result mir-\nrors that found for central genres and suggests that these\nwriters were well connected in a lyrical culture that they\nboth contributed to and drew from. The exception is Max\nMartin, whose lower inﬂuence score perhaps reﬂects the\nless quotable nonsense phrases he often uses in songs [12].\nMartin is also the least self-referential of the top writers,\nwhich might be explained by noting that he writes for a\nvariety of artists with different styles, whereas the other\nwriters are most famous for writing for themselves.\nTable 7 shows the top 10 artists by number-one hits. In\ncontrast to writer position, artist position does not seem\nstrongly affected by inﬂuence score. However, all top\nartists fall into the top decile (10%) of self-reference. Also,\nfemale artists in the list have much higher inﬂuence scores\nthan males, with the exception of The Supremes and Ri-\nhanna. We note also that Mariah Carey has a much higher\ninﬂuence score as an artist than as a writer. Further analysis\nof this phenomenon is complicated by the fact that many\npeople use a pseudonym as an artist but not as a writer.\n4. DISCUSSION\nWe explored topologies of genre, artist, and songwriter in-\nﬂuence networks constructed from links between trigrams\nover time. Through edge reduction and strongly connected\ncomponent analyses, we showed that all three graphs are\nhighly centralized around a large component with robust\nlinks. We conﬁrmed this organization with an MDS vi-\nsualization of the genre graph based on mutual inﬂuence.\nAlternative methods of edge reduction revealed separate\ncomponents, but primarily along language differences. We\nfound that the best predictor of a genre’s centrality to the\ninﬂuence network was the degree to which it referenced it-\nself, and that the network especially centered around three\npopular and self-referential genres: Jazz, Pop, and Rock.\nOur current metrics are useful building blocks for study-\ning relationships between genres, artists, and writers. The\ncentrality of our inﬂuence networks supplements earlier\nﬁndings showing clustering between some genres and iso-\nlation of others [6, 9]. However, our data do not produce\nthe Folk, Country, and Blues cluster observed by Fell and\nSporleder [6]; rather, we ﬁnd these genres have similar in-ﬂuences but do not draw signiﬁcant inﬂuence from each\nother. Fell and Sporleder also found Rap and Metal to\nbe lyrically isolated when analyzed with a bag-of-words\nmodel [6]. Our trigram analysis shows Rap and Metal to be\nwell connected in the network, though their self-reference\nscores are low compared to other popular genres. Further-\nmore, the notion of lyrical novelty [5] can be approximated\nwith inﬂuence, as the inﬂuence network incorporates nov-\nelty into the edge weights with tf-idf; clich ´e [14] can be\nunderstood as the inverse of novelty. Overall, our analyses\ndo not suggest the same degree of genre segmentation sug-\ngested in prior studies. We conclude that signiﬁcant dif-\nferences between genres may not occur at the phrase level,\nbut instead arise from key vocabulary differences [6, 9] as\nwell as musical and sociocultural factors [2, 15].\nThere are several potential areas for improvement in the\npresent study. First, a phrase shared across songs does not\nnecessarily signify direct inﬂuence. Next, albums were oc-\ncasionally labeled with incorrect years, which would im-\npact the temporal dimension of our networks. We used\nprimary iTunes album genre for our analysis, but acknowl-\nedge that such labels may not adequately characterize the\nsongs, especially for non-Western music or for songs that\nconceivably belong to more than one genre. Also, the de-\ncrease in connection robustness from cleaning may not be\nuniform across genres. In particular, the present results\ncould be reﬁned by analyzing English lyrics only or by in-\ncluding stopwords and cleaning rules for other languages.\nThe cleaning procedure we followed [5] may beneﬁt from\nstemming. Finally, the decision to ignore phrases spanning\nlines of lyrics reﬂects the organization of phrases for most\ngenres, but may have broken up phrases from genres with\nmore complicated lyrics (such as Rap).\nOur ﬁndings highlight exciting possibilities for future\nresearch. Recall that at one point in our component anal-\nyses, Jazz was excluded from the central component com-\nprising Pop and Rock—not because it was less inﬂuen-\ntial, but because it drew much less inﬂuence from Pop\nand Rock than they drew from it, leaving no path back to\nJazz from Pop or Rock when only the highest-magnitude\nedges remained in the graph. Future analyses could exam-\nine whether the edge reduction component analysis aligns\nmore closely with self-reference when graphs are treated\nas undirected. Future work could also ﬁnd cliques, which\nwould be more robustly interconnected than strongly con-\nnected components. Assessing changes in network struc-\nture when higher-order n-grams are used is another topic\nthat can be explored in future research. Finally, future\nstudies could examine further the notion of robustness\nof connection; differences in inﬂuence when people act\nas artists versus writers; trends that emerge when peo-\nple are grouped by gender, race, or geolocation; network\ntopologies when rare genres are grouped into categories;\nand could contribute visualizations to help understand the\nstructure of artist and writer networks.\nAcknowledgements: The authors thank Will Mills from\nLyricFind for facilitating access to the lyrics data; and\nCasey Baker and Ge Wang for their helpful feedback.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 6595. REFERENCES\n[1] J. Atherton and B. Kaneshiro. Lyrical inﬂuence net-\nworks dataset (LIND). In Stanford Digital Repository ,\n2016. http://purl.stanford.edu/zy061bp9773.\n[2] N. J. Bryan and G. Wang. Musical inﬂuence network\nanalysis and rank of sample-based music. In Proceed-\nings of the 12th International Society for Music Infor-\nmation Retrieval Conference , pages 329–334, 2011.\n[3] N. Collins. Computational analysis of musical inﬂu-\nence: A musicological case study using MIR tools. In\nProceedings of the 11th International Society for Mu-\nsic Information Retrieval Conference , pages 177–182,\n2010.\n[4] C. N. DeWall, R. S. Pond Jr, W. K. Campbell, and J. M.\nTwenge. Tuning in to psychological change: Linguistic\nmarkers of psychological traits and emotions over time\nin popular US song lyrics. Psychology of Aesthetics,\nCreativity, and the Arts , 5(3):200–207, 2011.\n[5] R. J. Ellis, Z. Xing, J. Fang, and Y . Wang. Quantify-\ning lexical novelty in song lyrics. In Proceedings of the\n16th International Society for Music Information Re-\ntrieval Conference , pages 694–700, 2015.\n[6] M. Fell and C. Sporleder. Lyrics-based analysis and\nclassiﬁcation of music. In Proceedings of the Interna-\ntional Conference on Computational Linguistics , pages\n620–631, 2014.\n[7] H. Fujihara, M. Goto, and J. Ogata. Hyperlinking\nlyrics: A method for creating hyperlinks between\nphrases in song lyrics. In Proceedings of the 9th Inter-\nnational Conference on Music Information Retrieval ,\npages 281–286, 2008.\n[8] C. Gunaratna, E. Stoner, and R. Menezes. Using net-\nwork sciences to rank musicians and composers in\nBrazilian popular music. In Proceedings of the 12th\nInternational Society for Music Information Retrieval\nConference , pages 441–446, 2011.\n[9] B. Logan, A. Kositsky, and P. Moreno. Semantic anal-\nysis of song lyrics. In IEEE International Conference\non Multimedia and Expo , volume 2, pages 827–830,\n2004.\n[10] J. P. G. Mahedero, A. Mart ´ınez, P. Cano, M. Koppen-\nberger, and F. Gouyon. Natural language processing of\nlyrics. In Proceedings of the 13th annual ACM Inter-\nnational Conference on Multimedia , pages 475–478,\n2005.\n[11] G. Salton and M. J. McGill. Introduction to modern\ninformation retrieval . McGraw-Hill, Inc., New York,\n1986.\n[12] J. Seabrook. Blank space: What kind of genius is Max\nMartin? New York Times , Sep. 30, 2015.[13] R. N. Shepard. Multidimensional scaling, tree-ﬁtting,\nand clustering. Science , 210(4468):390–398, 1980.\n[14] A. Smith, C. Zee, and A. Uitdenbogerd. In your eyes:\nIdentifying clich ´es in song lyrics. In Australasian Lan-\nguage Technology Workshop , pages 88–96, 2012.\n[15] R. D. Smith. The network of collaboration among rap-\npers and its community structure. Journal of Statistical\nMechanics: Theory and Experiment , 2006(02):2–15,\n2006.\n[16] M. van Zaanen and P. Kanters. Automatic mood classi-\nﬁcation using TF*IDF based on lyrics. In Proceedings\nof the 11th International Society for Music Information\nRetrieval Conference , pages 75–80, 2010.\n[17] X. Wang, X. Chen, D. Yang, and Y . Wu. Music emotion\nclassiﬁcation of Chinese songs based on lyrics using\nTF*IDF and rhyme. In Proceedings of the 12th Inter-\nnational Society for Music Information Retrieval Con-\nference , pages 765–770, 2011.660 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Towards Evaluating Multiple Predominant Melody Annotations in Jazz Recordings.",
        "author": [
            "Stefan Balke",
            "Jonathan Driedger",
            "Jakob Abeßer",
            "Christian Dittmar",
            "Meinard Müller"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1415076",
        "url": "https://doi.org/10.5281/zenodo.1415076",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/049_Paper.pdf",
        "abstract": "Melody estimation algorithms are typically evaluated by separately assessing the task of voice activity detection and fundamental frequency estimation. For both subtasks, computed results are typically compared to a single human reference annotation. This is problematic since different human experts may differ in how they specify a predom- inant melody, thus leading to a pool of equally valid ref- erence annotations. In this paper, we address the problem of evaluating melody extraction algorithms within a jazz music scenario. Using four human and two automatically computed annotations, we discuss the limitations of stan- dard evaluation measures and introduce an adaptation of Fleiss’ kappa that can better account for multiple reference annotations. Our experiments not only highlight the be- havior of the different evaluation measures, but also give deeper insights into the melody extraction task.",
        "zenodo_id": 1415076,
        "dblp_key": "conf/ismir/BalkeDADM16",
        "content": "TOWARDS EVALUATING MULTIPLE PREDOMINANT MELODY\nANNOTATIONS IN JAZZ RECORDINGS\nStefan Balke1Jonathan Driedger1Jakob Abeßer2\nChristian Dittmar1Meinard M ¨uller1\n1International Audio Laboratories Erlangen, Germany\n2Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany\nstefan.balke@audiolabs-erlangen.de\nABSTRACT\nMelody estimation algorithms are typically evaluated by\nseparately assessing the task of voice activity detection\nand fundamental frequency estimation. For both subtasks,\ncomputed results are typically compared to a single human\nreference annotation. This is problematic since different\nhuman experts may differ in how they specify a predom-\ninant melody, thus leading to a pool of equally valid ref-\nerence annotations. In this paper, we address the problem\nof evaluating melody extraction algorithms within a jazz\nmusic scenario. Using four human and two automatically\ncomputed annotations, we discuss the limitations of stan-\ndard evaluation measures and introduce an adaptation of\nFleiss’ kappa that can better account for multiple reference\nannotations. Our experiments not only highlight the be-\nhavior of the different evaluation measures, but also give\ndeeper insights into the melody extraction task.\n1. INTRODUCTION\nPredominant melody extraction is the task of estimating an\naudio recording’s fundamental frequency trajectory values\n(F0) over time which correspond to the melody. For exam-\nple in classical jazz recordings, the predominant melody\nis typically played by a soloist who is accompanied by a\nrhythm section (e. g., consisting of piano, drums, and bass).\nWhen estimating the soloist’s F0-trajectory by means of\nan automated method, one needs to deal with two issues:\nFirst, to determine the time instances when the soloist is\nactive. Second, to estimate the course of the soloist’s F0\nvalues at active time instances.\nA common way to evaluate such an automated\napproach—as also used in the Music Information Retrieval\nEvaluation eXchange (MIREX) [5]—is to split the evalua-\ntion into the two subtasks of activity detection and F0 es-\ntimation. These subtasks are then evaluated by comparing\nthe computed results to a single manually created reference\nc/circlecopyrtStefan Balke, Jakob Abeßer, Jonathan Driedger, Chris-\ntian Dittmar, Meinard M ¨uller. Licensed under a Creative Commons At-\ntribution 4.0 International License (CC BY 4.0). Attribution: Ste-\nfan Balke, Jakob Abeßer, Jonathan Driedger, Christian Dittmar, Meinard\nM¨uller. “Towards evaluating multiple predominant melody annotations\nin jazz recordings”, 17th International Society for Music Information Re-\ntrieval Conference, 2016.\nF0-trajectory Activity \nTime Vibrato Effects \nDuratio ns\nOnsets Transitions \nA1A2A3Figure 1 . Illustration of different annotations and possible\ndisagreements. A1andA2are based on a ﬁne frequency\nresolution. Annotation A3is based on a coarser grid of\nmusical pitches.\nannotation. Such an evaluation, however, is problematic\nsince it assumes the existence of a single ground-truth. In\npractice, different humans may annotate the same record-\ning in different ways thus leading to a low inter-annotator\nagreement. Possible reasons are the lack of an exact task\nspeciﬁcation, the differences in the annotators’ experi-\nences, or the usage of different annotation tools [21, 22].\nFigure 1 exemplarily illustrates such variations on the basis\nof three annotations A1,...,A 3of the same audio record-\ning, where a soloist plays three consecutive notes. A ﬁrst\nobservation is that A1andA2have a ﬁne frequency resolu-\ntion which can capture ﬂuctuations over time (e. g., vibrato\neffects). In contrast, A3is speciﬁed on the basis of semi-\ntones which is common when considering tasks such as\nmusic transcription. Furthermore, one can see that note on-\nsets, note transitions, and durations are annotated inconsis-\ntently. Reasons for this might be differences in annotators’\nfamiliarity with a given instrument, genre, or a particular\nplaying style. In particular, annotation deviations are likely\nto occur when notes are connected by slurs or glissandi.\nInter-annotator disagreement is a generally known\nproblem and has previously been discussed in the contexts\nof audio music similarity [8, 10], music structure analy-\nsis [16, 17, 23], and melody extraction [3]. In general, a246SoloID Performer Title Instr. Dur.\nBech-ST Sidney Bechet Summertime Sopr. Sax 197\nBrow-JO Clifford Brown Jordu Trumpet 118\nBrow-JS Clifford Brown Joy Spring Trumpet 100\nBrow-SD Clifford Brown Sandu Trumpet 048\nColt-BT John Coltrane Blue Train Ten. Sax 168\nFull-BT Curtis Fuller Blue Train Trombone 112\nGetz-IP Stan Getz The Girl from Ipan. Ten. Sax 081\nShor-FP Wayne Shorter Footprints Ten. Sax 139\nTable 1 . List of solo excerpts taken from the WJD. The\ntable indicates the performing artist, the title, the solo in-\nstrument, and the duration of the solo (given in seconds).\nsingle reference annotation can only reﬂect a subset of the\nmusically or perceptually valid interpretations for a given\nmusic recording, thus rendering the common practice of\nevaluating against a single annotation questionable.\nThe contributions of this paper are as follows. First,\nwe report on experiments, where several humans anno-\ntate the predominant F0-trajectory for eight jazz record-\nings. These human annotations are then compared with\ncomputed annotations obtained by automated procedures\n(MELODIA [20] and pYIN [13]) (Section 2). In particu-\nlar, we consider the scenario of soloist activity detection\nfor jazz recordings (Section 3.1). Afterwards, we adapt\nand apply an existing measure (Fleiss’ Kappa [7]) to our\nscenario which can account for jointly evaluating multi-\nple annotations (Section 3.2). Note that this paper has an\naccompanying website at [1] where one can ﬁnd the anno-\ntations which we use in the experiments.\n2. EXPERIMENTAL SETUP\nIn this work, we use a selection of eight jazz recordings\nfrom the Weimar Jazz Database (WJD) [9, 18]. For each\nof these eight recordings (see Table 1), we have a pool of\nseven annotationsA={A1,...,A 7}which all represent\ndifferent estimates of the predominant solo instruments’\nF0-trajectories. In the following, we model an annotation\nas a discrete-time function A: [1 :N]→R∪{∗} which\nassigns to each time index n∈[1 :N]either the solo’s F0\nat that time instance (given in Hertz), or the symbol ‘ ∗’.\nThe meaning of A(n) =∗is that the soloist is inactive at\nthat time instance.\nIn Table 2, we list the seven annotations. For this work,\nwe manually created three annotations A1,...,A 3by us-\ning a custom graphical user interface as shown in Fig-\nure 2 (see also [6]). In addition to standard audio player\nfunctionalities, the interface’s central element is a salience\nspectrogram [20]—an enhanced time-frequency represen-\ntation with a logarithmically-spaced frequency axis. An\nannotator can indicate the approximate location of F0-\ntrajectories in the salience spectrogram by drawing con-\nstraint regions (blue rectangles). The tool then automati-\ncally uses techniques based on dynamic programming [15]\nto ﬁnd a plausible trajectory through the speciﬁed region.\nThe annotator can then check the annotation by listening to\nthe solo recording, along with a synchronized soniﬁcation\nof the F0-trajectory.\nFigure 2 . Screenshot of the tool used for the manual anno-\ntation of the F0 trajectories.\nAnnotation Description\nA1 Human 1, F0-Annotation-Tool\nA2 Human 2, F0-Annotation-Tool\nA3 Human 3, F0-Annotation-Tool\nA4 Human 4, WJD, Sonic Visualiser\nA5 Computed, MELODIA [2, 20]\nA6 Computed, pYIN [13]\nA7 Baseline, all time instances active at 1kHz\nTable 2 . SetAof all annotations with information about\ntheir origins.\nIn addition to the audio recordings, the WJD also in-\ncludes manually annotated solo transcriptions on the semi-\ntone level. These were created and cross-checked by\ntrained jazz musicians using the Sonic Visualiser [4]. We\nuse these solo transcriptions to derive A4by interpreting\nthe given musical pitches as F0 values by using the pitches’\ncenter frequencies.\nA5andA6are created by means of automated meth-\nods.A5is extracted by using the MELODIA [20] algo-\nrithm as implemented in Essentia [2] using the default set-\ntings (sample rate = 22050 Hz, hop size = 3ms, window\nsize = 46ms). For obtaining A6, we use the tool Tony [12]\n(which is based on the pYIN algorithm [13]) with default\nsettings and without any corrections of the F0-trajectory.\nAs a ﬁnal annotation, we also consider a baseline\nA7(n) = 1 kHz for all n∈[1 :N]. Intuitively, this\nbaseline assumes the soloist to be always active. All of\nthese annotations are available on this paper’s accompany-\ning website [1].\n3. SOLOIST ACTIVITY DETECTION\nIn this section, we focus on the evaluation of the soloist\nactivity detection task. This activity is derived from the\nannotations of the F0-trajectories A1,...,A 7by only con-\nsidering active time instances, i. e., A(n)/negationslash=∗. Figure 3\nshows a typical excerpt from the soloist activity annota-\ntions for the recording Brow-JO . Each row of this ma-\ntrix shows the annotated activity for one of our annotations\nfrom Table 2. Black denotes regions where the soloist is\nannotated as active and white where the soloist is annotatedProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 247Ref.Est.A1A2A3A4A5A6A7 ∅\nA1− 0.93 0.98 0.92 0.74 0.79 1.00 0.89\nA2 0.92− 0.97 0.92 0.74 0.79 1.00 0.89\nA3 0.84 0.84− 0.88 0.69 0.74 1.00 0.83\nA4 0.85 0.86 0.94 − 0.70 0.75 1.00 0.85\nA5 0.84 0.84 0.90 0.85 − 0.77 1.00 0.87\nA6 0.75 0.76 0.81 0.77 0.65 − 1.00 0.79\nA7 0.62 0.62 0.71 0.67 0.55 0.65 − 0.64\n∅ 0.80 0.81 0.89 0.83 0.68 0.75 1.00 0.82\nTable 3 . Pairwise evaluation: Voicing Detection (VD). The\nvalues are obtained by calculating the VD for all possible\nannotation pairs (Table 2) and all solo recordings (Table 1).\nThese values are then aggregated by using the arithmetic\nmean.\nas inactive. Especially note onsets and durations strongly\nvary among the annotation, see e. g., the different durations\nof the note event at second 7.8. Furthermore, a missing\nnote event is noticeable in the annotations A1andA6at\nsecond 7.6. At second 8.2,A6found an additional note\nevent which is not visible in the other annotations. This\nexample indicates that the inter-annotator agreement may\nbe low. To further understand the inter-annotator agree-\nment in our dataset, we ﬁrst use standard evaluation mea-\nsures (e. g., as used by MIREX for the task of audio melody\nextraction [14]) and discuss the results. Afterwards, we in-\ntroduce Fleiss’ Kappa, an evaluation measure known from\npsychology, which can account for multiple annotations.\n3.1 Standard Evaluation Measures\nAs discussed in the previous section, an estimated annota-\ntionAeis typically evaluated by comparing it to a refer-\nence annotation Ar. For the pair (Ar,Ae), one can count\nthe number of time instances that are true positives #TP\n(ArandAeboth label the soloist as being active), the num-\nber of false positives #FP (onlyAelabels the soloist as\nbeing active), the number of true negatives #TN (Arand\nAeboth label the soloist as being inactive), and the number\nfalse negatives #FN (onlyAelabels the soloist as being\ninactive).\nIn previous MIREX campaigns, these numbers are used\nto derive two evaluation measures for the task of activity\ndetection. Voicing Detection (VD) is identical to Recall\nand describes the ratio that a time instance which is anno-\ntated as being active is truly active according to the refer-\nence annotation:\nVD =#TP\n#TP + #FN. (1)\nThe second measure is the Voicing False Alarm (VFA) and\nrelates the ratio of time instances which are inactive ac-\ncording to the reference annotation but are estimated as\nbeing active:\nVFA =#FP\n#TN + #FP. (2)\nIn the following experiments, we assume that all an-\nnotationsA1,...,A 7∈A have the same status, i. e., each\n7.0 7.5 8.0 8.5\nTime (s)A7A6A5A4A3A2A1Annotation\nInactive ActiveFigure 3 . Excerpt from Brow-JO .A1,...,A 4show the\nhuman annotations. A5andA6are results from automated\napproaches. A7is the baseline annotation which considers\nall frames as being active.\nRef.Est.A1A2A3A4A5A6A7 ∅\nA1− 0.13 0.30 0.27 0.22 0.44 1.00 0.39\nA2 0.12− 0.29 0.26 0.22 0.43 1.00 0.39\nA3 0.05 0.07− 0.14 0.18 0.43 1.00 0.31\nA4 0.16 0.16 0.27 − 0.24 0.46 1.00 0.38\nA5 0.34 0.35 0.48 0.44 − 0.49 1.00 0.52\nA6 0.38 0.38 0.54 0.49 0.35 − 1.00 0.52\nA7 0.00 0.00 0.00 0.00 0.00 0.00 − 0.00\n∅ 0.17 0.18 0.31 0.27 0.20 0.38 1.00 0.36\nTable 4 . Pairwise evaluation: Voicing False Alarm (VFA).\nThe values are obtained by calculating the VFA for all pos-\nsible annotation pairs (Table 2) and all solo recordings (Ta-\nble 1). These values are then aggregated by using the arith-\nmetic mean.\nannotation may be regarded as either reference or estimate.\nThen, we apply the standard measures in a pairwise fash-\nion. For all pairs (Ar,Ae)∈A×A withAr/negationslash=Ae, we\nextract VD and VFA (using the MIREVAL [19] toolbox)\nfor each of the solo recordings listed in Table 1. The mean\nvalues over the eight recordings are presented in Table 3\nfor the VD-measure and in Table 4 for the VFA-measure.\nAs for the V oicing Detection (Table 3), the values within\nthe human annotators A1,...,A 4range from 0.84for the\npair(A3,A2)to0.98for the pair (A1,A3). This high\nvariation in VD already shows that the inter-annotator dis-\nagreement even within the human annotators is substantial.\nBy taking the human annotators as reference to evaluate\nthe automatic approach A5, the VD lies in the range of\n0.69for(A3,A5)to0.74for(A2,A5). Analogously, for\nA6, we observe values from 0.74for(A3,A6)to0.79for\n(A1,A6).\nAs for the V oicing False Alarm (see Table 4), the val-\nues among the human annotations range from 0.05for\n(A3,A1)to0.30for(A1,A3). Especially annotation A3\ndeviates from the other human annotations, resulting in a\nvery high VFA (having many time instances being set as\nactive).\nIn conclusion, depending on which human annotation\nwe take as the reference, the evaluated performances of\nthe automated methods vary substantially. Having multi-\nple potential reference annotations, the standard measures248 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016n= 12345A1A2A3k = 1130127/15k = 2203218/151/3111/31/3an,kAekAon(a)(b)Figure 4 . Example of evaluating Fleiss’ κforK= 2\ncategories,N= 5frames, and three different annotations.\n(a) Annotations. (b) Number of annotations per category\nand time instance. Combining Ao= 0.6andAe= 0.5\nleads toκ= 0.2.\n<0 0−0.2 0.21−0.4 0.41−0.6 0.61−0.8 0.81−1\npoor slight fair moderate substantial almost perfect\nTable 5 . Scale for interpreting κas given by [11].\nare not generalizable to take these into account (only by\nconsidering a mean over all pairs). Furthermore, although\nthe presented evaluation measures are by design limited to\nyield values in [0,1], they can usually not be interpreted\nwithout some kind of baseline. For example, considering\nVD, the pair (A2,A3)yields a VD-value of 0.97, sug-\ngesting that A3can be considered as an “excellent” esti-\nmate. However, considering that our uninformed baseline\nA7yields a VD of 1.0, shows that it is meaningless to look\nat the VD alone. Similarly, an agreement with the trivial\nannotationA7only reﬂects the statistics on the active and\ninactive frames, thus being rather uninformative. Next, we\nintroduce an evaluation measure that can overcome some\nof these problems.\n3.2 Fleiss’ Kappa\nHaving to deal with multiple human annotations is com-\nmon in ﬁelds such as medicine or psychology. In these\ndisciplines, measures that can account for multiple anno-\ntations have been developed. Furthermore, to compensate\nfor chance-based agreement, a general concept referred to\nasKappa Statistic [7] is used. In general, a kappa value\nlies in the range of [−1,1], where the value 1means com-\nplete agreement among the raters, the value 0means that\nthe agreement is purely based on chance, and a value below\n0means that agreement is even below chance.\nWe now adapt Fleiss’ Kappa to calculate the chance-\ncorrected inter-annotator agreement for the soloist activity\ndetection task. Following [7, 11], Fleiss’ Kappa is deﬁned\nas:\nκ:=Ao−Ae\n1−Ae. (3)\nIn general,κcompares the mean observed agreement Ao∈\n[0,1]to the mean expected agreement Ae∈[0,1]which\nis solely based on chance. Table 5 shows a scale for theSoloIDComb.κHκH,5κH,6ρ5ρ6\nBech-ST 0.74 0.60 0.55 0.82 0.75\nBrow-JO 0.68 0.56 0.59 0.82 0.87\nBrow-JS 0.61 0.47 0.43 0.78 0.71\nBrow-SD 0.70 0.61 0.51 0.87 0.73\nColt-BT 0.66 0.55 0.49 0.84 0.74\nFull-BT 0.74 0.66 0.61 0.89 0.83\nGetz-IP 0.72 0.69 0.64 0.96 0.90\nShor-FP 0.82 0.65 0.58 0.80 0.70\n∅ 0.71 0.60 0.55 0.85 0.78\nTable 6 .κfor all songs and different pools of annotations.\nκHdenotes the pool of human annotations A1,...,A 4.\nThese values are then aggregated by using the arithmetic\nmean.\nagreement of annotations with the corresponding range of\nκ.\nTo give a better feeling for how κworks, we exemplar-\nily calculate κfor the example given in Figure 4(a). In this\nexample, we have R= 3different annotations A1,...,A 3\nforN= 5 time instances. For each time instance, the an-\nnotations belong to either of K= 2 categories ( active or\ninactive ). As a ﬁrst step, for each time instance, we add\nup the annotations for each category. This yields the num-\nber of annotations per category an,k∈N,n∈[1 :N],\nk∈[1 :K]which is shown in Figure 4(b). Based on these\ndistributions, we calculate the observed agreement Ao\nnfor\na single time instance n∈[1 :N]as:\nAo\nn:=1\nR(R−1)K/summationdisplay\nk=1an,k(an,k−1), (4)\nwhich is the fraction of agreeing annotations normalized\nby the number of possible annotator pairs R(R−1), e. g.,\nfor the time instance n= 2 in the example, all annotators\nagree for the frame to be active, thus Ao\n2= 1. Taking the\narithmetic mean of all observed agreements leads to the\nmean observed agreement\nAo:=1\nNN/summationdisplay\nn=1Ao\nn, (5)\nin our example Ao= 0.6. The remaining part for cal-\nculatingκis the expected agreement Ae. First, we cal-\nculate the distribution of agreements within each category\nk∈[1 :K], normalized by the number of possible ratings\nNR:\nAe\nk:=1\nNRN/summationdisplay\nn=1an,k, (6)\ne. g., in our example for k= 1(active) results in Ae\n1=7/15.\nThe expected agreement Aeis deﬁned as [7]\nAe:=K/summationdisplay\nk=1(Ae\nk)2(7)\nwhich leads to κ= 0.2for our example. According to the\nscale given in Table 5, this is a “slight” agreement.\nIn Table 6, we show the results for κcalculated for dif-\nferent pools of annotations. First, we calculate κfor theProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 2491 10 20 30 40 50\nTolerance (cent)0:00:20:40:60:81:0Raw Pitch Accuracy\n(A1;A2) (A1;A3) (A1;A4) (A1;A5) (A1;A6)Figure 5 .Raw Pitch Accuracy (RPA) for different pairs of\nannotations based on the annotations of the solo recording\nBrow-JO , evaluated on all active frames according to the\nreference annotation.\npool of human annotations H:={1,2,3,4}, denoted as\nκH.κHyields values ranging from 0.61to0.82which is\nconsidered as “substantial” to “almost perfect” agreement\naccording to Table 5.\nNow, reverting to our initial task of evaluating an auto-\nmatically obtained annotation, the idea is to see how the\nκ-value changes when adding this annotation to the pool\nof all human annotations. A given automated procedure\ncould then be considered to work correctly if it produces\nresults that are just about as variable as the human anno-\ntations. Only if an automated procedure behaves funda-\nmentally different than the human annotations, it will be\nconsidered to work incorrectly. In our case, calculating\nκfor the annotation pool H∪{5}yields values ranging\nfrom 0.47to0.69, as shown in column κH,5of Table 6.\nConsidering the annotation pool H∪{6},κH,6results in\nκ-values ranging from 0.43to0.64. Considering the aver-\nage over all individual recordings, we get mean κ-values\nof0.60and0.55forκH,5andκH,6, respectively. Compar-\ning these mean κ-values for the automated approaches to\nthe respective κH, we can consider the method producing\nthe annotation A5to be more consistent with the human\nannotations than A6.\nIn order to quantify the agreement of an automatically\ngenerated annotation and the human annotations in a single\nvalue, we deﬁne the proportion ρ∈Ras\nρ5:=κH,5\nκH,ρ6:=κH,6\nκH. (8)\nOne can interpret ρas some kind of “normalization” ac-\ncording to the inter-annotator agreement of the humans.\nFor example, solo recording Brow-JS obtains the lowest\nagreement of κH= 0.61in our test set. The algorithms\nperform “moderate” with κH,5= 0.47andκH,6= 0.43.\nThis moderate performance is partly alleviated when nor-\nmalizing with the relatively low human agreement, lead-\ning toρ5= 0.78andρ6= 0.71. On the other hand, for\nthe solo recording Shor-FP , the human annotators had\nan “almost perfect” agreement of κH,6= 0.82. While the\nautomated method’s approaches were “substantial” with\nκH,5= 0.65and “moderate” with κH,6= 0.58. However,\n1 10 20 30 40 50\nTolerance (cent)0:00:20:40:60:81:0Raw Pitch Accuracy\n(A1;A2) (A1;A3) (A1;A4) (A1;A5) (A1;A6)Figure 6 .Modiﬁed Raw Pitch Accuracy for different pairs\nof annotations based on the annotations of the solo record-\ningBrow-JO , evaluated on all active frames according to\ntheunion of reference and estimate annotation.\nalthough the automated method’s κ-values are higher than\nforBrow-JS , investigating the proportions ρ5andρ6re-\nveal that the automated method’s relative agreement with\nthe human annotations is actually the same ( ρ5= 0.78\nandρ5= 0.71forBrow-JS compared to ρ5= 0.80and\nρ5= 0.70forShor-FP ). This indicates the ρ-value’s po-\ntential as an evaluation measure that can account for mul-\ntiple human reference annotations in a meaningful way.\n4. F0 ESTIMATION\nOne of the used standard measures for the evaluation of the\nF0 estimation in MIREX is the Raw Pitch Accuracy (RPA)\nwhich is computed for a pair of annotations (Ar,Ae)con-\nsisting of a reference Arand an estimate annotation Ae.\nThe core concept of this measure is to label an F0 estimate\nAe(n)to be correct, if its F0-value deviates from Ar(n)\nby at most a ﬁxed tolerance τ∈R(usuallyτ= 50 cent).\nFigure 5 shows the RPA for different annotation pairs and\ndifferent tolerances τ∈ {1,10,20,30,40,50}(given in\ncent) for the solo recording Brow-JO , as computed by\nMIREVAL . For example, looking at the pair (A1,A4), we\nsee that the RPA ascends with increasing value of τ. The\nreason for this becomes obvious when looking at Figure 7.\nWhileA1was created with the goal of having ﬁne grained\nF0-trajectories, annotations A4was created with a tran-\nscription scenario in mind. Therefore, the RPA is low for\nvery smallτbut becomes almost perfect when considering\na tolerance of half a semitone ( τ= 50 cent).\nAnother interesting observation in Figure 5 is that the\nannotation pairs (A1,A2)and(A1,A3)yield almost con-\nstant high RPA-values. This is the case since both an-\nnotations were created using the same annotation tool—\nyielding very similar F0-trajectories. However, it is note-\nworthy that there seems to be a “glass ceiling” that can-\nnot be exceeded even for high τ-values. The reason for\nthis lies in the exact deﬁnition of the RPA as used for\nMIREX. Let µ(A) :={n∈[1 :N] :A(n)/negationslash=∗}be\nthe set of all active time instances of some annotation in\nA. By deﬁnition, the RPA is only evaluated on the refer-\nence annotation’s active time instances µ(Ar), where each250 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 201615.0 16.0\nTime (s)440.0Frequency (Hz)A1\nA4Figure 7 . Excerpt from the annotations of the solo\nBrow-JO ofA1andA4.\nn∈µ(Ar)\\µ(Ae)is regarded as an incorrect time in-\nstance (for any τ). In other words, although the term “Raw\nPitch Accuracy” suggests that this measure purely reﬂects\ncorrect F0-estimates, it is implicitly biased by the activity\ndetection of the reference annotation. Figure 8 shows an\nexcerpt of the human annotations A1andA2for the solo\nrecording Brow-JO . While the F0-trajectories are quite\nsimilar, they differ in the annotated activity. In A1, we see\nthat transitions between consecutive notes are often anno-\ntated continuously—reﬂecting glissandi or slurs. This is\nnot the case in A2, where the annotation rather reﬂects in-\ndividual note events. A musically motivated explanation\ncould be that A1’s annotator had a performance analysis\nscenario in mind where note transitions are an interesting\naspect, whereas A2’s annotator could have been more fo-\ncused on a transcription task. Although both annotations\nare musically meaningful, when calculating the RPA for\n(A1,A2), all time instances where A1is active and A2not,\nare counted as incorrect (independent of τ)—causing the\nglass ceiling.\nAs an alternative approach that decouples the activity\ndetection from the F0 estimation, one could evaluate the\nRPA only on those time instances, where reference andes-\ntimate annotation are active, i. e., µ(Ar)∪µ(Ae). This\nleads to the modiﬁed RPA-values as shown in Figure 6.\nCompared to Figure 5, all curves are shifted towards higher\nRPA-values. In particular, the pair (A1,A2)yields modi-\nﬁed RPA-values close to one, irrespective of the tolerance\nτ—now indicating that A1andA2coincide perfectly in\nterms of F0 estimation.\nHowever, it is important to note that the modiﬁed RPA\nevaluation measure may not be an expressive measure on\nits own. For example, in the case that two annotations\nare almost disjoint in terms of activity, the modiﬁed RPA\nwould only be computed on the basis of a very small num-\nber of time instances, thus being statistically meaning-\nless. Therefore, to rate a computational approach’s per-\nformance, it is necessary to consider both, the evaluation\nof the activity detection as well as the F0 estimation, si-\nmultaneously but independent of each other. Both evalua-\ntions give valuable perspectives on the computational ap-\nproach’s performance for the task of predominant melody\nestimation and therefore help to get a better understanding\nof the underlying problems.\n95.0 96.0\nTime (s)440.0Frequency (Hz)A1\nA2Figure 8 . Excerpt from the annotations of the solo\nBrow-JO ofA1andA2.\n5. CONCLUSION\nIn this paper, we investigated the evaluation of auto-\nmatic approaches for the task of predominant melody\nestimation—a task that can be subdivided into the sub-\ntask of soloist activity detection and F0 estimation. The\nevaluation of this task is not straightforward since the ex-\nistence of a single “ground-truth” reference annotation is\nquestionable. After having reviewed standard evaluation\nmeasures used in the ﬁeld, one of our main contributions\nwas to adapt Fleiss’ Kappa—a measure which accounts for\nmultiple reference annotations. We then explicitly deﬁned\nand discussed Fleiss’ Kappa for the task of the soloist ac-\ntivity detection.\nThe core motivation for using Fleiss’ Kappa as an eval-\nuation measure was to consider an automatic approach to\nwork correctly, if its results were just about as variable\nas the human annotations. We therefore extended this the\nkappa measure by normalizing it by the variability of the\nhuman annotations. The resulting ρ-values allow for quan-\ntifying the agreement of an automatically generated anno-\ntation and the human annotations in a single value.\nFor the task of F0 estimation, we showed that the stan-\ndard evaluation measures are biased by the activity de-\ntection task. This is problematic, since mixing both sub-\ntasks can obfuscate insights into advantages and draw-\nbacks of a tested predominant melody estimation proce-\ndure. We therefore proposed an alternative formulation for\nRPA which decoupled the two tasks.\n6. ACKNOWLEDGMENT\nThis work has been supported by the German Research\nFoundation (DFG MU 2686/6-1 and DFG PF 669/7-1). We\nwould like to thank all members of the Jazzomat research\nproject led by Martin Pﬂeiderer.\nThe International Audio Laboratories Erlangen are\na joint institution of the Friedrich-Alexander-Universit ¨at\nErlangen-N ¨urnberg (FAU) and the Fraunhofer-Institut f ¨ur\nIntegrierte Schaltungen IIS.\n7. REFERENCES\n[1] Accompanying website. http://www.\naudiolabs-erlangen.de/resources/MIR/\n2016-ISMIR-Multiple-Annotations/ .Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 251[2] Dmitry Bogdanov, Nicolas Wack, Emilia G ´omez,\nSankalp Gulati, Perfecto Herrera, Oscar Mayor, Ger-\nard Roma, Justin Salamon, Jos ´e R. Zapata, and Xavier\nSerra. Essentia: An audio analysis library for music in-\nformation retrieval. In Proc. of the Int. Society for Mu-\nsic Information Retrieval Conference (ISMIR) , pages\n493–498, Curitiba, Brazil, 2013.\n[3] Juan J. Bosch and Emilia G ´omez. Melody extraction\nin symphonic classical music: a comparative study of\nmutual agreement between humans and algorithms. In\nProc. of the Conference on Interdisciplinary Musicol-\nogy (CIM) , December 2014.\n[4] Chris Cannam, Christian Landone, and Mark B. San-\ndler. Sonic visualiser: An open source application for\nviewing, analysing, and annotating music audio ﬁles.\nInProc. of the Int. Conference on Multimedia , pages\n1467–1468, Florence, Italy, 2010.\n[5] J. Stephen Downie. The music information retrieval\nevaluation exchange (2005–2007): A window into mu-\nsic information retrieval research. Acoustical Science\nand Technology , 29(4):247–255, 2008.\n[6] Jonathan Driedger and Meinard M ¨uller. Verfahren zur\nSch¨atzung der Grundfrequenzverl ¨aufe von Melodies-\ntimmen in mehrstimmigen Musikaufnahmen. In Wolf-\ngang Auhagen, Claudia Bullerjahn, and Richard von\nGeorgi, editors, Musikpsychologie – Anwendungsori-\nentierte Forschung , volume 25 of Jahrbuch Musikpsy-\nchologie , pages 55–71. Hogrefe-Verlag, 2015.\n[7] Joseph L. Fleiss, Bruce Levin, and Myunghee Cho\nPaik. Statistical Methods for Rates and Proportions .\nJohn Wiley Sons, Inc., 2003.\n[8] Arthur Flexer. On inter-rater agreement in audio mu-\nsic similarity. In Proc. of the Int. Conference on Music\nInformation Retrieval (ISMIR) , pages 245–250, Taipei,\nTaiwan, 2014.\n[9] Klaus Frieler, Wolf-Georg Zaddach, Jakob Abeßer, and\nMartin Pﬂeiderer. Introducing the jazzomat project and\nthe melospy library. In Third Int. Workshop on Folk\nMusic Analysis , 2013.\n[10] M. Cameron Jones, J. Stephen Downie, and Andreas F.\nEhmann. Human similarity judgments: Implications\nfor the design of formal evaluations. In Proc. of the Int.\nConference on Music Information Retrieval (ISMIR) ,\npages 539–542, Vienna, Austria, 2007.\n[11] J. Richard Landis and Gary G. Koch. The measurement\nof observer agreement for categorical data. Biometrics ,\n33(1):159–174, 1977.\n[12] Matthias Mauch, Chris Cannam, Rachel Bittner,\nGeorge Fazekas, Justing Salamon, Jiajie Dai, Juan\nBello, and Simon Dixon. Computer-aided melody note\ntranscription using the Tony software: Accuracy andefﬁciency. In Proc. of the Int. Conference on Tech-\nnologies for Music Notation and Representation , May\n2015.\n[13] Matthias Mauch and Simon Dixon. pYIN: A funda-\nmental frequency estimator using probabilistic thresh-\nold distributions. In IEEE Int. Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , pages\n7480–7484, 2014.\n[14] MIREX. Audio melody extraction task. Website\nhttp://www.music-ir.org/mirex/wiki/\n2015:Audio_Melody_Extraction , last ac-\ncessed 01/19/2016, 2015.\n[15] Meinard M ¨uller. Fundamentals of Music Processing .\nSpringer Verlag, 2015.\n[16] Oriol Nieto, Morwaread Farbood, Tristan Jehan, and\nJuan Pablo Bello. Perceptual analysis of the F-measure\nto evaluate section boundaries in music. In Proc. of the\nInt. Society for Music Information Retrieval Confer-\nence (ISMIR) , pages 265–270, Taipei, Taiwan, 2014.\n[17] Jouni Paulus and Anssi P. Klapuri. Music structure\nanalysis using a probabilistic ﬁtness measure and a\ngreedy search algorithm. IEEE Transactions on Audio,\nSpeech, and Language Processing , 17(6):1159–1170,\n2009.\n[18] The Jazzomat Research Project. Database download,\nlast accessed: 2016/02/17. http://jazzomat.\nhfm-weimar.de .\n[19] Colin Raffel, Brian McFee, Eric J. Humphrey, Justin\nSalamon, Oriol Nieto, Dawen Liang, and Daniel P. W.\nEllis. MIR EV AL: A transparent implementation of\ncommon MIR metrics. In Proc. of the Int. Conference\non Music Information Retrieval (ISMIR) , pages 367–\n372, Taipei, Taiwan, 2014.\n[20] Justin Salamon and Emilia G ´omez. Melody extrac-\ntion from polyphonic music signals using pitch contour\ncharacteristics. IEEE Transactions on Audio, Speech,\nand Language Processing , 20(6):1759–1770, 2012.\n[21] Justin Salamon, Emilia G ´omez, Daniel P. W. Ellis,\nand Ga ¨el Richard. Melody extraction from polyphonic\nmusic signals: Approaches, applications, and chal-\nlenges. IEEE Signal Processing Magazine , 31(2):118–\n134, 2014.\n[22] Justin Salamon and Juli ´an Urbano. Current challenges\nin the evaluation of predominant melody extraction al-\ngorithms. In Proc. of the Int. Society for Music Infor-\nmation Retrieval Conference (ISMIR) , pages 289–294,\nPorto, Portugal, October 2012.\n[23] Jordan Bennett Louis Smith, John Ashley Burgoyne,\nIchiro Fujinaga, David De Roure, and J. Stephen\nDownie. Design and creation of a large-scale database\nof structural annotations. In Proc. of the Int. Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 555–560, Miami, Florida, USA, 2011.252 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Good-sounds.org: A Framework to Explore Goodness in Instrumental Sounds.",
        "author": [
            "Giuseppe Bandiera",
            "Oriol Romani Picas",
            "Hiroshi Tokuda",
            "Wataru Hariya",
            "Koji Oishi",
            "Xavier Serra"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416864",
        "url": "https://doi.org/10.5281/zenodo.1416864",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/199_Paper.pdf",
        "abstract": "We introduce good-sounds.org, a community driven framework based on freesound.org to explore the concept of goodness in instrumental sounds. Goodness is conside- red here as the common agreed basic sound quality of an instrument without taking into consideration musical ex- pressiveness. Musicians upload their sounds and vote on existing sounds, and from the collected data the system is able to develop sound goodness measures of relevance for music education applications. The core of the system is a database of sounds, together with audio features extracted from them using MTG’s Essentia library and user annota- tions related to the goodness of the sounds. The web front- end provides useful data visualizations of the sound at- tributes and tools to facilitate user interaction. To evaluate the framework, we carried out an experiment to rate sound goodness of single notes of nine orchestral instruments. In it, users rated the sounds using an AB vote over a set of sound attributes defined to be of relevance in the charac- terization of single notes of instrumental sounds. With the obtained votes we built a ranking of the sounds for each attribute and developed a model that rates the goodness for each of the selected sound attributes. Using this approach, we have succeeded in obtaining results comparable to a model that was built from expert generated evaluations.",
        "zenodo_id": 1416864,
        "dblp_key": "conf/ismir/BandieraPTHOS16",
        "content": "GOOD-SOUNDS.ORG: A FRAMEWORK TO EXPLORE GOODNESS IN\nINSTRUMENTAL SOUNDS\nGiuseppe Bandiera1Oriol Romani Picas1Hiroshi Tokuda2\nWataru Hariya2Koji Oishi2Xavier Serra1\n1Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain\n2Technology Development Dept., KORG Inc., Tokyo, Japan\ngiuseppe.bandiera@upf.edu, oriol.romani@upf.edu\nABSTRACT\nWe introduce good-sounds.org, a community driven\nframework based on freesound.org to explore the concept\nof goodness in instrumental sounds. Goodness is conside-\nred here as the common agreed basic sound quality of an\ninstrument without taking into consideration musical ex-\npressiveness. Musicians upload their sounds and vote on\nexisting sounds, and from the collected data the system is\nable to develop sound goodness measures of relevance for\nmusic education applications. The core of the system is a\ndatabase of sounds, together with audio features extracted\nfrom them using MTG’s Essentia library and user annota-\ntions related to the goodness of the sounds. The web front-\nend provides useful data visualizations of the sound at-\ntributes and tools to facilitate user interaction. To evaluate\nthe framework, we carried out an experiment to rate sound\ngoodness of single notes of nine orchestral instruments. In\nit, users rated the sounds using an AB vote over a set of\nsound attributes deﬁned to be of relevance in the charac-\nterization of single notes of instrumental sounds. With the\nobtained votes we built a ranking of the sounds for each\nattribute and developed a model that rates the goodness for\neach of the selected sound attributes. Using this approach,\nwe have succeeded in obtaining results comparable to a\nmodel that was built from expert generated evaluations.\n1. INTRODUCTION\nMeasuring sound goodness, or quality, in instrumental\nsounds is difﬁcult due to its intrinsic subjectivity. Never-\ntheless, it has been shown that there is some consistency\namong people while discriminating good or bad music per-\nformances [1]. Furthermore, recent studies have demon-\nstrated a correlation between the perceived music quality\nand the musical performance technique [2]. Bearing this\nin mind, in a previous work [3] we proposed a method\nto automatically rate goodness by deﬁning a set of sound\nc/circlecopyrtGiuseppe Bandiera, Oriol Romani Picas, Hiroshi\nTokuda, Wataru Hariya, Koji Oishi, Xavier Serra. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: Giuseppe Bandiera, Oriol Romani Picas, Hiroshi Tokuda,\nWataru Hariya, Koji Oishi, Xavier Serra. “Good-sounds.org: a frame-\nwork to explore goodness in instrumental sounds”, 17th International So-\nciety for Music Information Retrieval Conference, 2016.attributes and by using a set of good/bad labels given by\nexpert musicians. The deﬁnition of goodness was treated\nas a classiﬁcation problem and an outcome of that work\nwas a mobile application (Cortosia R/circlecopyrt) that gives goodness\nscores in real-time for single notes on a scale from 0 to\n100. This score was computed considering the distribu-\ntion of the features values in the classiﬁcation step. While\ndeveloping that system we realized that we could improve\nthe scores, specially their correlation with the perceptual\nsound goodness, if we could use more training data and in-\nclude a range of goodness levels given by users rather than\nthe binary good/bad labels that we used. However, the task\nof labeling sounds this way would have been very time\nconsuming and we would also need more sounds, cover-\ning the whole range of sound goodness. To address these\nissues we are now crowdsourcing the problem. We have\ndeveloped a website, good-sounds.org, on which users can\nupload sound content and can tag and rate sounds in vari-\nous ways.\n2. GOOD-SOUNDS.ORG\nGood-sounds.org1is an online platform to explore the\nconcept of goodness in instrumental sounds with the help\nof a community of users. It provides social community\nfeatures in the web front-end and a framework for sound\nanalysis and modeling in the background. It also includes\nan API to access the collected data.\n2.1 Description\nThe website has been designed from a user perspective,\nmeant to be modern and to provide a seamless experience.\nIt makes use of state of the art design concepts and com-\nmunity oriented web technologies. The web front-end in-\ncludes three main sections: (1) a page to list and visua-\nlize the uploaded sounds as shown in Figure 1, (2) a page\nto upload and describe sounds as shown in Figure 2 and\n(3) a section to gather user ratings and annotations. The\nvisualization page shows a list of all the sounds and it\nincludes ﬁlter options to narrow down the results, being\nable to show things like speciﬁc instruments or sounds up-\nloaded a certain date. The upload page allows users to\nadd sounds into the site and also provides a recording tool\n1https://good-sounds.org414Figure 1 . Good-sound.org sound list page.\nFigure 2 . Good-sound.org sound upload page.\nbuilt using Web Audio API2. The annotation section has\nbeen designed for the speciﬁc experiment explained in Sec-\ntion 3. The website backend is based on the experience we\nhave obtained in all these years developing and maintain-\ning freesound [2] . It is written in Python using the Django\nweb application framework. The metadata is stored in a\nPostgreSQL database while the sound ﬁles plus other ana-\nlysis ﬁles are stored locally in the server. An API accepts\nrequests from authorized clients to upload sounds (cur-\nrently through the mobile app Cortosia) and retrieve statis-\ntics from the users community. At this time, the website\nsupports 11 instruments, it includes 8470 unique sounds\nand there are 363 active users.\n2.2 Content\nThe main data stored in good-sounds.org consists of\nsounds and the metadata accompanying them. When up-\n2http://www.w3.org/TR/webaudio/loading sounds, the users can choose between three differ-\nent types of Creative Commons licenses for their content:\nUniversal, Attribution or Attribution Non-Commercial. As\nsoon as a sound is uploaded, it is analyzed using the\nfreesound extractor [4], thus obtaining a number of low-\nlevel audio features, and the system generates an mp3 ver-\nsion of the ﬁle together with images of the waveform and\nspectrogram. The audio, image and audio feature ﬁles are\nstored in the good-sounds server and the metadata is stored\nin the PostgreSQL database.\n2.2.1 Segmentation\nOne of the critical audio processing steps performed in\ngood-sounds.org is the segmentation of the uploaded sound\nﬁles to ﬁnd appropriate note boundaries. Given that the au-\ndios come from different and not well controlled sources,\nthey might include all kinds of issues (ex. silence at be-\nginning and end or background noise) that can difﬁcult the\nsubsequent feature extraction steps. Considering that the\nsounds we are working with are all monophonic pitched\ninstrument sounds, we can base the segmentation mainly\non pitch. Our approach extracts pitch using Essentia’s [5]\nimplementations of the YinFFT algorithm [8] and the Yin\ntime based algorithm [6]. Then the sound is segmented\ninto notes using pitch contours [7] and signal RMS with\nEssentias PitchContourSegmentation algorithm. The seg-\nmentation data is also stored in the database. This allows\nus to build client-side data visualizations that effectively\nreﬂect the quality of the segmentation algorithm and the\nuser can modify the parameters for this algorithm and re-\nrun it on the ﬂy from the website. The results of this ite-\nration is immediately shown on the same page, for an easy\ncomparison of the results, as it is shown in Figure 3.\nFigure 3 . Good-sound.org segmentation visualisation\npage.\n2.2.2 Descriptors\nThe feature extraction module is based on the freesound\nextractor module of Essentia. It computes a subset of its\nspectral, tonal and temporal descriptors. With it, the audios\nare ﬁrst resampled to 44.1kHz sampling rate. The descrip-\ntors are then extracted across all the frames using a 2048\nwindow size and 512 hop window size. We then compute\nstatistical measures (mean, median and standard deviation)Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 415of the descriptors which are the values stored as JSON ﬁles\nin the server. The list of the descriptors extracted is shown\nin Table 1.\n3. EXPERIMENT\nAs a test case to evaluate the usefulness of the good-\nsounds.org framework we setup an experiment to rate the\ngoodness of single notes. The goal of the experiment was\nto build models from both the uploaded sounds and the\ncommunity annotations, with which we can then automat-\nically rate the sound goodness. We compared the results of\nthe obtained models with the ones we got in our previous\nwork using expert annotations.\n3.1 Dataset\nThe data used in this experiment comes from several\nsources. First, we uploaded all the sounds from our pre-\nvious work to the website, together with the expert annota-\ntions. Since the website has been public for a few months,\nwe also had sounds uploaded by users, both directly and\nthrough the mobile app (using the API). Then, user anno-\ntations on the sounds according to a goodness scale where\ncollected using a voting task. These annotations use a set\nof sound attributes that affect sound goodness. These at-\ntributes were deﬁned in our previous article [3] by consult-\ning with a group of music experts:\n•dynamic stability : the stability of the loudness.\n•pitch stability : the stability of the pitch.\n•timbre stability : the stability of the timbre.\n•timbre richness : the quality of the timbre.\n•attack clarity : the quality of the attack.\nThe sounds from the recording sessions are also uploaded\nto freesound and thus are openly accessible. We also\nprovide a tool that allow the good-sounds.org users to\nlink their accounts to their freesound ones and upload the\nsounds there.\n3.1.1 Sounds\nFor this experiment we only used single note sounds. At\nthe time of the experiment there were sounds for 5467 sin-\ngle notes of 9 instruments. We show the list of sounds per\ninstrument in Table 2. The sounds we recorded ourselves\nfrom the recording sessions are uncompressed wave ﬁles,\nwhile the ones uploaded by users to the website are in dif-\nferent audio formats.\n3.1.2 Annotations\nWe distinguish two kinds of annotations: (1) recording\nannotations and (2) community annotations. The recor-\nding annotations are the ones coming from the recording\nsessions that we did and consists of one tag per sound.\nThis tag says if the sound is a good or a bad example\nof each sound attribute (e.g. bad-timbre-stability, good-\nattack-clarity). Those are the annotations used later on forinstrument number of sounds\ncello 935\nviolin 802\nclarinet 1360\nﬂute 1434\nalto sax 352\nbaritone sax 292\ntenor sax 292\nsoprano sax 343\ntrumpet 738\nTable 2 . Number of sounds in the experiment’s dataset.\na ﬁrst evaluation of the models and are only available for\nthe sounds we recorded ourselves. The community anno-\ntations are the ones generated from the user votes and used\nin this work to explore goodness. In order to be able to\nrate a sound in a goodness scale we need annotations on\na wide range of different goodness levels. We originally\nthought of asking the community to rate sounds in a scale\nof goodness but we discarded this option because of the\nfollowing:\n•the task can be excessively demanding.\n•without a reference sound the criteria of different\nusers can differ extremely.\n•with a reference sound we inﬂuence the users crite-\nria, thus annotations can be less generalisable.\nInstead, we designed a user task that gave as outcome\na ranked list of the sounds based on the goodness for each\nsound attribute. An A/B multi vote task was used for this\npurpose. Two sounds are presented and the user is asked to\ndecide which sound is better according to one or more of\nthe sound attributes. One vote is stored for each selected\nattribute. A list of the votes per instruments (considering\nall sound attributes) is shown in Table 3. In order to prevent\nrandom votes in the task we run checks periodically. This\nchecks consists of two sounds; one being a bad example of\na sound attribute regarding the expert annotations and the\nother being a good example. The task is presented to the\nusers the ﬁrst time they vote and also randomly after some\nvotes. If the user does not vote for the expected sound in\nthe reference task, his next votes are not used. The votes\nof this user are again taken into account if he succeeds to\npass the reference task.\n3.1.3 Rankings\nIn order to have learning data in a wide range of good-\nness we built rankings with the community votes for each\nsound attribute. The position of a sound in the ranking\nrepresents its goodness level. To build them we count the\nnumber of wins and the number of votes of each sound in\nthe database. Then the sounds are sorted according to two\nparameters:\n•total number of votes: number of participations in\nthe voting task.416 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016spectral tonal temporal\nspectrum, barkbands,\nmelbands, ﬂatness, crest, rolloff,\ndecrease, hfc, pitch salience, ﬂatness db,\nskewness, kurtosis, spectral complexity,pitch yinfft, pitch yin, pitch conﬁdence, zerocrossingrate, loudness, centroid,\nﬂatness sfx,\nTable 1 . Descriptors extracted by Essentia library present in good-sounds.org.\ninstrument number of votes\ncello 140\nviolin 90\nclarinet 293\nﬂute 305\nalto sax 78\nbaritone sax 59\ntenor sax 14\nsoprano sax 21\ntrumpet 230\nTable 3 . Number of votes in good-sounds for the dataset’s\nexperiment.\n•ratio between wins and votes: the ratio between the\nnumber of wins and the total number of participa-\ntions in the voting task.\nUsing these parameters for building the rankings we as-\nsure that the sounds in the top are the ones voted more\ntimes, as being better than others, and not sounds with few\nvotes but high percentage of wins.\n3.2 Learning\nThe goal of our learning process is to build a model for\neach instrument that is able to rate each sound attribute\nin a 0 to 100 score. To do so we want to ﬁnd a set of\nfeatures that highly correlate with the rankings extracted\nin the previous step. Our approach uses a regression model\nto predict the score. These predictions are then used as\nsamples of the ﬁnal score function. The ﬁnal score is then\ncomputed as an interpolation of the samples.\n3.2.1 Models\nWe want to ﬁnd the combination of regression model and\nset of features that better describes the rankings. For such a\npurpose we tried different regression algorithms available\nin scikit-learn [9]. As one of the outputs of the project is\na system that rates the goodness of sounds in real-time we\nwant to restrict the number of features in order to maintain\na low computational cost. For each one of the algorithms\nwe build a model for each ranking using one, two or three\nfeatures and we compute the average prediction score of\nthe model across all the options. The prediction score R2\nor Coefﬁcient of Determination is deﬁned as follows:\nR2= (1−u/v) (1)\nwhere\nu=/summationdisplay\n((ytrue−ypred)2) (2)Regression model Avg. score Score variance\nSVR 3 features -1.208 5.4436\nSVR 2 features -1.2411 5.3895\nSVR 1 feature -1.4254 5.6554\nTable 5 . Performance of SVR model with different num-\nber of features.\nand\nv=/summationdisplay\n((ytrue−n/productdisplay\ni=1ytrue)2) (3)\nWhereytrue is the set of ground truth annotations and\nypred the set of predictions, having both the same length.\nThe best possible score R2is 1.0 and it can be negative.\nThe variance of the prediction score across all the rankings\nand set of features is also computed. The number of fea-\ntures that give the best score for each ranking is taken into\naccount to compute an average number of features for each\nregression model. A comparison of the performance of the\ndifferent models is shown in Table 4.\nAs we can see in the table, the SVR (Epsilon-Support\nVector Regression) model has the best average score across\nall the rankings and using all possible combination of fea-\nture sets (up to 3 features). It also has the lowest score\nvariance so we can expect the model to be robust across\nthe different instruments and sound attributes. However\nthe average number of features is almost two and the com-\nputation of two features at each frame of all the sounds in\nthe database can be computationally expensive. For this\nreason we tested how good the model can be if we force it\nto use less than three features. We show the results of such\na comparison in Table 5.\nThe results show that the differences between using one\nor three features are not too big so we decided to use SVR\nwith a single feature in order to maintain a low computa-\ntional cost for future applications of the system. We then\ntried all possible combinations of parameters (kernel, de-\ngree of polynomially, cost parameter..) to ﬁnd the best\nmodel for each instrument and sound attribute.\n3.2.2 Scores\nFrom the model we predict the ranking position of a sound\nand we map this position into a 0 to 100 score of the sound\nattribute. The ﬁnal goodness score is computed as the aver-\nage score across the ﬁve attributes. We compute the sound\nattribute scores of all the sounds in the database to test the\ndistribution of the scores according to the feature value.\nFor example, a distribution of the score for the timbre sta-\nbility of ﬂute is shown in Figure 4.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 417Regression model Avg. score Score variance Average of features\nSVR -1.208 5.4436 1.843\nRidge -2.644 31.005 2.166\nKRR -1.79 10.798 1.906\nLinear regression -3.503 30.03 1.718\nRANSAC -3.202 17.532 1.478\nTheilsen -4.14 37.135 1.781\nTable 4 . Performance of the different regression models.\nFigure 4 . Distribution of scores of ﬂute timbre stability.\nThe resulting distributions are not balanced. For this\nreason we push the scores of each sound attribute to ﬁt a\nGaussian distribution. This gives us balanced distributions\nand it also allows us to reﬁne the scores by tweaking the\nparameters of the gaussian function. A result of this pro-\ncess is shown in Figure 5. The ﬁnal score is computed\ninterpolating the feature according to these tuned distribu-\ntions.\nFigure 5 . Distribution of scores of ﬂute timbre stability\nafter normalisation.\n3.2.3 Models evaluation\nIn order to evaluate the models we want to check the cor-\nrelation between the scores and the rankings as we ex-\npect the sounds ranked in the ﬁrst positions to have thehighest scores. We evaluate this correlation using Kendall\nRank Correlation Coefﬁcient [10], commonly referred as\nKendalls tau coefﬁcient. We use the implementation avail-\nable in the scipy library, that is based on the tau-b version.\nIts computation, given two rankings xandyof the same\nsize is deﬁned by the following equation:\nτ=(P−Q)/radicalbig\n((P+Q+T)∗(P+Q+U))(4)\nwherePis the number of concordant pairs, Qthe number\nof discordant pairs, Tthe number of ties only in x, andU\nthe number of ties only in y. If a tie occurs for the same\npair in both xandy, it is not added to either TorU. The\nvalues range from -1 to 1, where 1 indicates strong agree-\nment and -1 strong disagreement. We compute τbetween\nthe score and the ranking position for all the sounds that\nare contained in the rankings. The results for each sound\nattribute and instrument are shown in Table 6.\n4. CONCLUSIONS\nIn this article we presented a web based framework for\nexploring sound goodness in instrumental sounds using a\ncommunity of users. The framework provides an easy way\nto collect sounds and annotations as well as tools to ex-\ntract and store music descriptors. This allows us to explore\nthe concept of sound goodness in a controlled and ﬂexi-\nble environment. Furthermore, the website is useful to the\ncommunity as a place in which to discuss the issues affect-\ning sound goodness as well as a learning tool to improve\ntheir playing techniques. As a way to evaluate the frame-\nwork we extended our previous work by using annotations\nfrom the community collected through a voting task. The\nmodels built using this approach provide an automatic rat-\ning of goodness for each attribute that tends to match the\nexpert annotations collected in our previous work. The re-\nsults should improve with more annotations from the com-\nmunity. As future work we want to design new tasks to\ncollect user annotations and build new models according\nto them.\n5. ACKNOWLEDGEMENTS\nThis research has been partially funded by KORG Inc.\nThe authors would like to thank the entire good-sounds.org\ncommunity who contributed to the website with sounds\nand annotations.418 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Sound attribute Flute Violin Clarinet Trumpet Cello Violin Alto sax Baritone sax Soprano Average\ntimbre stability 0.37 0.65 0.46 0.38 0.28 0.65 0.33 0.73 0.73 0.51\ndynamic stability 0.41 0.33 0.24 0.44 0.64 0.33 0.33 0.31 0.31 0.37\npitch stability 0.42 0.46 0.22 0.38 0.58 0.46 1 1 0.81 0.59\ntimbre richness 0.04 0.35 0.32 0.11 0.21 0.35 1 0.56 1 0.43\nattack clarity 0.33 0.59 0.38 0.3 0.18 0.59 0 0.34 0.35 0.34\nTable 6 . Kendall tau coefﬁcient between the scores and the rankings of each sound attribute.\n6. REFERENCES\n[1] J. Geringer and C. Madsen. “Musicians ratings of good\nversus bad vocal and string performances,” Journal of\nResearch in Music Education , vol. 46, pages 522-534,\n1998.\n[2] Brian E. Russell. “An empirical study of a solo per-\nformance assessment model,” International Journal of\nMusic Education , vol. 33, pages 359-371, 2015.\n[3] O. Roman Picas, H. Parra Rodriguez, D. Dabiri, H.\nTokuda, W. Hariya, K. Oishi, and X. Serra. “A real-\ntime system for measuring sound goodness in instru-\nmental sounds,” Audio Engineering Society Conven-\ntion 138. Audio Engineering Society 2015.\n[4] F. Font, G. Roma, and X. Serra. “Freesound techni-\ncal demo,” Proceedings of the 21st ACM international\nconference on Multimedia , 2013.\n[5] D. Bogdanov, N. Wach, E. G ´omez, S. Gulati, P. Her-\nrera, O. Mayor, G. Roma, J. Salamon, J. R. Zapata,\nand X. Serra. “Essentia: An audio analysis library for\nmusic information retrieval,” Proceedings of the Inter-\nnational Society for Music Information Retrieval Con-\nference, pages 493-498, 2013.\n[6] A. de Cheveign and H. Kawahara. “YIN, a fundamen-\ntal frequency estimator for speech and music,” The\nJournal of the Acoustical Society of America, pages\n111-1917, 2002.\n[7] R. J. McNab, Ll. A. Smith, and I. H. Witten. “Sig-\nnal processing for melody transcription,” Australasian\nComputer Science Communications 18 , pages 301-\n307, 1996.\n[8] P. M. Brossier. “Automatic Annotation of Musical Au-\ndio for Interactive Applications,” Ph.D. Thesis, Queen\nMary University of London, UK , 2007.\n[9] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R.\nWeiss, V . Dubourg, and J. Vanderplas. “Scikit-learn:\nMachine Learning in Python,” Journal of Machine\nLearning Research 12 , pages 2825-2830, 2011.\n[10] Stepanov, Alexei. “On the Kendall Correlation Coefﬁ-\ncient,” arXiv preprint arXiv:1507.01427, 2015.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 419"
    },
    {
        "title": "Jazz Ensemble Expressive Performance Modeling.",
        "author": [
            "Helena Bantulà",
            "Sergio I. Giraldo",
            "Rafael Ramírez 0001"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1415876",
        "url": "https://doi.org/10.5281/zenodo.1415876",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/162_Paper.pdf",
        "abstract": "Computational expressive music performance studies the analysis and characterisation of the deviations that a musician introduces when performing a musical piece. It has been studied in a classical context where timing and dynamic deviations are modeled using machine learning techniques. In jazz music, work has been done previously on the study of ornament prediction in guitar performance, as well as in saxophone expressive modeling. However, little work has been done on expressive ensemble perfor- mance. In this work, we analysed the musical expressivity of jazz guitar and piano from two different perspectives: solo and ensemble performance. The aim of this paper is to study the influence of piano accompaniment into the per- formance of a guitar melody and vice versa. Based on a set of recordings made by professional musicians, we ex- tracted descriptors from the score, we transcribed the gui- tar and the piano performances and calculated performance actions for both instruments. We applied machine learning techniques to train models for each performance action, taking into account both solo and ensemble descriptors. Finally, we compared the accuracy of the induced models. The accuracy of most models increased when ensemble in- formation was considered, which can be explained by the interaction between musicians.",
        "zenodo_id": 1415876,
        "dblp_key": "conf/ismir/BantulaGR16",
        "content": "JAZZ ENSEMBLE EXPRESSIVE PERFORMANCE MODELING\nBantula, Helena Giraldo, Sergio Ramirez, Rafael\n. Music Technology Group, Universitat Pompeu Fabra, Barcelona (Spain)\nhelenabantula@gmail.com, {sergio.giraldo, rafael.ramirez }@upf.edu\nABSTRACT\nComputational expressive music performance studies\nthe analysis and characterisation of the deviations that a\nmusician introduces when performing a musical piece. It\nhas been studied in a classical context where timing and\ndynamic deviations are modeled using machine learning\ntechniques. In jazz music, work has been done previously\non the study of ornament prediction in guitar performance,\nas well as in saxophone expressive modeling. However,\nlittle work has been done on expressive ensemble perfor-\nmance. In this work, we analysed the musical expressivity\nof jazz guitar and piano from two different perspectives:\nsolo and ensemble performance. The aim of this paper is to\nstudy the inﬂuence of piano accompaniment into the per-\nformance of a guitar melody and vice versa. Based on a\nset of recordings made by professional musicians, we ex-\ntracted descriptors from the score, we transcribed the gui-\ntar and the piano performances and calculated performance\nactions for both instruments. We applied machine learning\ntechniques to train models for each performance action,\ntaking into account both solo and ensemble descriptors.\nFinally, we compared the accuracy of the induced models.\nThe accuracy of most models increased when ensemble in-\nformation was considered, which can be explained by the\ninteraction between musicians.\n1. INTRODUCTION\nMusic is a very important part in the life of milions of peo-\nple, whether they are musicians, they enjoy attending live\nmusic concerts or simply like listening to musical record-\nings at home. The engaging part of music is the human\ncomponent added to the performance: instead of a ”dead”\nscore, musicians shape the music by changing parameters\nsuch as intensity, velocity, volume and articulation. The\nstudy of music expressive performance from a computa-\ntional point of view consists of characterising the devia-\ntions that a musician introduces in a score, often in order\nto render human-like performances from inexpressive mu-\nsic scores.\nThere are numerous works which study expressive per-\nformance in classical music, and most of these studies have\nc/circlecopyrt. Licensed under a Creative Commons Attribution 4.0\nInternational License (CC BY 4.0). Attribution: . “Jazz Ensemble\nExpressive Performance Modeling”, 17th International Society for Music\nInformation Retrieval Conference, 2016.been done on piano performances (for an overview, see\nGoebl [24]). Other works analyse expressivity in a jazz\ncontext. For instance, Giraldo and Ram ´ırez [8] study and\nmodel the ornamentation introduced to a jazz melody by\nusing machine learning techniques. In an ensemble con-\ntext, the musicians’ performance is inﬂuenced by what\nis being played by the other musicians. Although most\nworks are focused on soloist performances, some works\ntake into account ensemble performances in classical mu-\nsic ( [26], [12], [15]). However, to our knowledge lit-\ntle work addresses ensemble expressive performance in a\njazz context. In this work, we present a method to study\nthe interaction between jazz musicians from a computa-\ntional perspective. Our data set consisted of 7 jazz pieces\nrecorded by a jazz quartet (guitar, piano, bass and drums),\nin which each instrument was recorded on a separate track.\nIn this study we considered the interaction between guitar\nmelodies and the accompaniment of piano. We extracted\nindividual (soloist) score descriptors as well as ensemble\ndescriptors. We calculated performance actions for both\nguitar (embellishments) and piano (chord density, range\nand weight). We applied machine learning techniques to\npredict these performance actions using Artiﬁcial Neural\nNetworks, Support Vector Machine and Decision Trees.\nWe generated individual models for each instrument and\nmeasured the level of interaction between musicians by in-\ntroducing ensemble descriptors into each individual model\nto create mixed models , and compared the individual mod-\nelswith the mixed models . Finally, we evaluated the per-\nformance of the algorithms by computing statistical signif-\nicance tests (Paired T-Test).\nThe rest of the paper is organised as follows. In Sec-\ntion 2, we present related work in expressive music per-\nformance. In section 3, we describe the materials we have\nused. In Section 4, the proposed method is described and\nthe evaluation process is explained. In Section 5, the re-\nsults of the evaluation are presented. Finally, in Section 6\nwe put forward conclusions and future improvements.\n2. RELATED WORK\nMany works study expressive performance actions in mu-\nsic, deﬁned as variations in timing (duration and onsets),\nenergy, articulation, and vibrato from different perspec-\ntives, including psychology ( [6], [19]), neurology ( [13]),\nmusicology ( [22]) and at a computational level. Previous\nwork has been done in a classical context by Friberg [5],\nwho develops a set of rules using analysis by synthesis to674generate the deviations to be applied to a score, obtaining\nhuman-like performances. Widmer [25] analyses record-\nings of 13 complete Mozart piano sonatas and uses a ma-\nchine learning approach to create predictive rules for note-\nlevel timing, dynamics and articulation. In a jazz context ,\nprevious work has focused on the saxophone: Lopez de\nM´antaras et al. [1] use case-based reasoning to develop a\nsystem capable of modeling expressive performances (on-\nset, duration and energy) of jazz saxophone. Ram ´ırez and\nHazan [20] apply classiﬁcation and regression methods to\na set of acoustic features extracted from saxophone record-\nings and a set of descriptors which characterised the con-\ntext of the performed notes. In jazz guitar, Giraldo and\nRam´ırez ( [9], [10]) use machine learning techniques to\nmodel and synthesise embellishments by training models\nto classify score notes as embellished or not, according to\nthe characteristics of the notes’ context.\n2.1 Ensemble Performance\nIn ensemble performance, the expressivity of a soloist\nmight be inﬂuenced by what the other musicians are play-\ning. Most of the literature refers to classical context ,\nstudying timing asynchrony among performers. Repp [21]\nstudies the synchronisation of the task of tapping by taking\ninto account phase and frequency correction mechanisms.\nWing et al. [26] develop a model for studying synchronisa-\ntion in string quartets in different contexts (democratic or\ndictatorial). Goebl and Palmer [12] investigate the effect of\nthe auditory and visual feedback so to study the synchro-\nnisation among musicians. More recently, Marchini [15]\nstudies the interaction between musicians by generating\nindependent machine learning models of expressive per-\nformance for each musician and taking into account the\ninﬂuence of the other musicians.\n3. MATERIALS\nWe recorded 7 jazz standards performed by a jazz quar-\ntet both in wav and MIDI format, using the digital audio\nworkstation Logic Pro X [14]. The scores were written in\nMusic-XML format using Muse Score [18] to extract de-\nscriptors. We developed code by using the computing envi-\nronment Matlab [17], concretely, the MidiToolBox Library\ndeveloped by Toiviainen and Eerola [4] to process the data\nin MIDI format. We used the fundamental frequency esti-\nmator YIN [3] to create an automatic guitar melody tran-\nscriber. We performed beat tracking of the recordings us-\ning the beat tracker developed by Zapata [27]. Finally,\nwe used the Weka Data Mining Software [16] for machine\nlearning modeling.\n4. METHODOLOGY\nThe methodology is divided into three stages, which are\ndepicted in Figure 1. Firstly, we acquired the data from\nrecordings and its respective scores (Section 4.1). Sec-\nondly, the data was analysed to extract the chords played\nby the pianist, which were aligned with the score after-\nwards so as to obtain piano performance actions. For gui-tar, we transcribed the audio into MIDI, and aligned the\nplayed notes with the score to obtain guitar performance\nactions. From the score, descriptors for notes and chords\nwere extracted. We manually transcribed the audio of the\npiano and guitar into a new score in order to also extract\ndescriptors from the performed score. The audio mix was\nused for beat tracking, and to compute a mean tempo.\nThirdly, machine learning techniques were applied using\nthe different data sets created from the extracted data to\npredict the calculated performance actions (Section 4.3).\nFigure 1 . Overall framework: the data related to the score\nis shown inside ellipses while the data related to the record-\nings is placed inside rectangles.\n4.1 Data acquisition and pre-processing\nWe recorded a jazz ensemble consisting of keyboard, elec-\ntric guitar, electric bass and drums. We only used the gui-\ntar data in wav format, the piano data in MIDI format and\nan audio mix of the band in wav format for further tempo\ncomputation. Improvisers usually play the main melody at\nthe beginning and end of a performance with improvisa-\ntions in the central part and so both the recordings and the\nscores were segmented in order to contain only the melody\npart (no introductions or solos).\n4.2 Data analysis\nThe aim of this part was to obtain a machine readable\nrepresentation from the input data (recordings and scores)\nin the form of descriptors (data extracted from the score\nwhich characterised both notes and chords by taking into\naccount their properties and the properties of their con-\ntexts) and performance actions (deviations from the score\nintroduced by the musician to add expressivity, extracted\nfrom the recordings). In this stage, there were 4 types of\ninput data: piano recordings, guitar recordings, scores and\naudio mix recordings. The following Sections explain the\nprocessing of this data.\n4.2.1 Piano Data\nWe detected chords in the piano data by grouping together\nindividual notes. The process consisted of identifyingProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 675groups of notes played at the same time and so we cre-\nated heuristic rules based on the work done by Traube and\nBernays [23], who identify groups of notes which have\nnear-synchronous onsets by analysing onset differences\nbetween two consecutive notes. Our approach consisted\nof three rules: the ﬁrst one searched for and grouped notes\nwhich were played at the same time. The second one, was\nin charge of merging chords with an inter onset difference\n<100ms. Finally, the third rule took into consideration\npedal notes (notes that remain while two or more chords\nare played consequently).\nAlignment was performed to link the detected chords\nwith the score chords. It was done at a beat level: since\nthe position of the beats was computed by the beat tracker\n(see Section 4.2.4), we converted the onsets/offsets of each\nperformed chord from seconds to beats. Based on beat in-\nformation, we aligned a chord written in the score with the\nchord (or chords) that had been played.\nBased on the alignment of the played chords to the\nscore, the performance actions for every chord in the score\nwere calculated according to what had been played. We\ncomputed three performance actions: density (Equation\n1), deﬁned as loworhigh depending on the number of\nchords used to perform a score chord (i.e. chords played\nwith a duration of half-note or more were labelled as low\nwhile a duration of less than a half note corresponded to a\n”high” label); weight (Equation 2), deﬁned as loworhigh\naccording to the total number of notes which were utilised\nto perform a score chord; and range (Equation 3), deﬁned\nasloworhigh if the distance in semitones from the high-\nest to the lowest performed note per chord in the score was\nlarger than 18 (an octave and a half) .\nden(chord S) =\n\nlow if/summationtextchords P\ndur(chord S)<1/2\nhigh if/summationtextchords P\ndur(chord S)≥1/2(1)\nWhere:\nchord S: is the corresponding chord on the score\n/summationtextchords P: is the amount of performed chords for a chord on\nthe score\ndur(chord S): is the duration of the corresponding chord on the\nscore\nwei(chord S) =\n\nlow if/summationtextnotes P/summationtextchords P<4\nhigh if/summationtextnotes P/summationtextchords P≥4(2)\nWhere:\nchord S: is the corresponding chord on the score\n/summationtextnotes P: is the total number of performed notes for a chord\non the score\n/summationtextchords P: is the amount of performed chords for a chord on\nthe scoreran(chord S) =\n\nlow ifmax (pitch PN)−min (pitch PN)<18\nhigh ifmax (pitch PN)−min (pitch PN)≥18\n(3)\nWhere:\nchord S: is the corresponding chord on the score\npitch PN: is the vector of pitch of the performed notes ( PN) for\na chord on the score\n4.2.2 Guitar Data\nWe automatically converted the guitar recording in wav\nformat into a MIDI format in order to obtain a note repre-\nsentation based on pitch, onset (in seconds) and offset (in\nseconds) by following the framework presented in Bantula\net al. work [2].\nAlignment was then performed at two levels. Firstly,\nthe onsets and offsets of the MIDI notes were converted\nfrom seconds to beats using the beats’ information com-\nputed by the beat tracker (see Section 4.2.4). Secondly, we\nperformed manual alignment between the performed notes\nand the score notes by using a graphical interface that al-\nlowed to link the performed notes and the score notes in\ntwo pianoroll representations [11]. Embellishments were\ncomputed by following the same approach by Giraldo and\nRam´ırez [11]: a note was considered to be embellished if\ntwo or more notes were played in its place. Then, each\nscore note was labelled as embellished or not (y/n) accord-\ning to the previous alignment.\n4.2.3 Score Data\nIn this stage, we extracted horizontal and vertical descrip-\ntors from the score to characterise both chords and notes.\n•Chord Descriptors (Figure 2) For chords, the hori-\nzontal context concerned harmonic information and\nthe vertical context considered melodic, ensemble\ninformation. In Table 1, the intrinsic descriptors of\nthe reference chords are listed. In Table 3, the har-\nmonic horizontal descriptors, computed according to\nthe neighbours of the reference chord are shown. Ta-\nble 2 includes the vertical descriptors computed by\naveraging or weighting the single note descriptors of\nthe notes below the region deﬁned by the reference\nchord.\nFigure 2 . Excerpt of Autumn Leaves : horizontal and verti-\ncal contexts for the reference chord F7676 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016descriptor units computation range\nid num root→number [0,11]\ntype label type{M, m, +,7,\ndim, half dim}\ntens labeltension based on\nmusical criteria{++, +, -, - -}\nchord dur beats chord dur [1,∞)\nonb beats onb [1,∞)\nTable 1 . Individual descriptors for a reference chord (no\ncontext).\ndescriptor units computation range\nonset b beats min (onset notes ) [1,∞)\ndurb beatsmax (onset notes )\n+max (durnotes )\n−min (onset notes )[1,∞)\nmeanPitch\n(mP)MIDI note mean (pitch notes ) [36,96]\nonset s seconds 60∗onset b\ntempo[1,∞)\ndurs seconds 60∗durb\ntempo[1,∞)\nchromahalf\ntonesmod 12(mP) [0,11]\nmeasure num measure [1,∞)\npredurb beats predurb [1,∞)\npredurs seconds 60∗predurb\ntempo[1,∞)\nnxtdurb beats nxtdurb [1,∞)\nnxtdurs seconds 60∗nxtdurb\ntempo[1,∞)\nprev inthalf\ntonesprev mP−mP [1,∞)\nnext inthalf\ntonesmP−next mP [1,∞)\nnote2keyhalf\ntoneschroma−key [0,11]\nnote2chordhalf\ntoneschroma−id [0,11]\nisChordN* label -{y,n}\nmtr* label mean (metpos(notes )){strong,\nweak}\nintHop* num mean (intervals ) [0,96]\nmelody* num#notes\nchord dur-\nTable 2 . Chord melodic descriptors (vertical)\ndescriptor units computation range\ntempo bpm tempo [1,300]\nkeyMode label keyMode{major,\nminor}\nnumKey numkey position in\nthe Fifths Circle[0,11]\nkeyDistancehalf\ntonesid−numKey [0,11]\nmetP* label metrical position{strongest,\nstrong,\nweak,\nweakest}\nfunction labelharmonic\nanalysis from\nkeyDistance{tonic,\nsubdom,\ndom,\nnofunc}\nnext root inthalf\ntonesid−next id [0,11]\nprev root inthalf\ntonesprev id−id [0,11]\nTable 3 . Chord harmonic descriptors (horizontal)•Note descriptors (Figure 3) For note descriptors,\nthe horizontal context included melodic information\nwhile the vertical context included harmonic, en-\nsemble information. Following the approach made\nby Giraldo [7], we computed horizontal note de-\nscriptors using the information of the reference\nnotes’ neighbours whereas we computed vertical\nnote descriptors by using the chords’ information.\nSince every note belonged to a chord, the features\nof the note were merged with the descriptors of the\ncorresponding chord by concatenating both lists and\neliminating repeated items.\nFigure 3 . Excerpt of All Of Me : Horizontal and Vertical\ncontexts for a reference note\n4.2.4 Audio mix Data\nFor every recording, we performed a semi-automatic align-\nment between the performance and the score.The tempo\nvaried during the performance because no metronome was\nused. Hence, beat positions were not equidistant and beat-\ntracking was performed to create a beat grid which allowed\nto link the performed information to the score informa-\ntion. We used the algorithm developed by Zapata et al. [27]\nto track the beats, followed by manual correction. After-\nwards, the mean tempo of each song was computed using\nEquation 4, where beats was the vector of beats computed\nin the previous step.\ntempo =round/parenleftbigg60\nmean (diff (beats ))/parenrightbigg\n(4)\n4.3 Machine learning\n4.3.1 Datasets\nAs it can be seen in Figure 1, the inputs of the Machine\nLearning stage were the performance actions for both pi-\nano and guitar as well as the score descriptors (for chords\nand notes). Hence, we constructed three types of datasets,\nshown in Figure 4.\n•Simple Datasets (D1) : Horizontal score context. It\nonly contained individual descriptors of the chords\nor notes.\n•Score Mixed Datasets (D2) : D1 plus vertical\nscore context, which contained merged descriptors\nof chords and notes.\n•Performance Mixed Datasets (D3) : D1 plus verti-\ncalperformance context (extracted from the manualProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 677transcriptions of the performances), which contained\nmerged features of chords and notes, taking into ac-\ncount the real interaction between musicians.\nFigure 4 . Three different datasets depending on the in-\ncluded descriptors\nTherefore, for piano we trained models to learn the\nfunction shown in Equation 5 while for guitar, the func-\ntion to learn is presented in Equation 6.\nf(Chord )→(Den, Wei, Ran ) (5)\nWhere:\nChord : is a chord only characterised by harmonic descriptors\nplus melodic descriptors (depending on the dataset)\nDen ,Wei ,Ran : are the predicted density, weight and range\nlabels (high/low), respectively.\nf(Note )→(Emb ) (6)\nWhere:\nNote : is a note characterised by the set of melodic descriptors\nplus harmonic descriptors (depending on the dataset)\nEmb : corresponds to the predicted embellishment label (yes/no).\n4.3.2 Feature Selection\nThe aim of this part was to identify speciﬁc score descrip-\ntors that best described the previously deﬁned performance\nactions, so as to train models with the most representative\nones. Therefore, for every dataset we evaluated the de-\nscriptors by their information gain. Tables 4, 5, 6 and 7\nshow the best ranked descriptors for density, weight, range\nand embellishments, respectively.\nD1 D2 D3\nmetP mtr metP\nchord dur metP chord dur\nfunction chord dur intHop\ntype isChordN isChordN\ntens type function\nmetP tens type\ntens\nTable 4 . Selected features for densityD1 D2 D3\ntens tens tens\nfunction function function\nchord dur type type\nmetP metP metP\nisChordN keyMode\nkeyMode isChordN\nmtr tens\nTable 5 . Selected features for weight\nD1 D2 D3\nnumKey numKey numKey\nfunction durs predurs\ntype duration b prev int\ntens predurb function\nkeyMode nxtdurb type\nmetP isChordN mtr\nfunction isChordN\nmtr tens\ntype keyMode\ntens metP\nkeyMode\nmetP\nTable 6 . Selected features for range\nD1 D2 D3\nphrase phrase phrase\ndurb durb durb\ndurs durs durs\npredurb predurb predurb\npredurs predurs predurs\nonset onset onset\ntens tens\ntype type\nfunction function\nisChordN isChordN\nkeyMode keyMode\nmetP\nTable 7 . Selected features for embellishments\n4.3.3 Algorithms\nThe aim of this stage was to compare the results of the\nwidely used algorithms Decision Trees ,Support Vector\nMachine (SVM) (with a linear kernel) and Neural Networks\n(NN) (with one hidden layer). We used the implementa-\ntion of these algorithms in the Weka Data Mining Soft-\nware [16], utilising the default parameters.\n5. RESULTS\nSince every performance action contained 3 datasets, we\ngenerated a model for each of them. Thus, the results we\npresent include a comparison between the datasets as well\nas the algorithms.\n5.1 Piano data: density, weight and range\nWe evaluated the accuracy (percentage of correct classiﬁ-\ncations) using 10-cross fold validation with 10 iterations.\nWe performed statistical testing by using the t-test with a\nsigniﬁcance value of 0.05to compare the methods with the\nbaseline (Zero Rule Classiﬁer) and decide if one produced\nmeasurably better results than the other.678 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Table 8 shows the results for density . It can be seen\nthat the accuracy increased when ensemble information\nwas considered (datasets D2 and D3). The signiﬁcant\nimprovements were achieved by the algorithms NN and\nSVM, being 65.13 the highest accuracy reached with the\ndataset D2 which consisted in both harmonic and melodic\nscore descriptors. For weight (Table 9), none of the re-\nsults was statistically signiﬁcant and the performance of\nthe three models can be interpreted as random. The high-\nest results were achieved when only piano information was\nconsidered (D1), showing no interaction between this per-\nformance action and the guitar melody. Table 10 presents\nthe results for range . In that case, the three algorithms\nreached their maximum accuracy when information of the\nensemble performance (D3) was considered, which can be\nexplained as a presence of correlation between the range of\nthe chords performed and the melody the piano player was\nhearing. Moreover, the results for the algorithms Decision\nTrees and SVM were statistically signiﬁcant.\nDataset Baseline NN SVM Decision Tree\nD1 51.82 61.19 ◦ 62.13◦ 53.75\nD2 51.82 61.72 65.13 ◦ 55.34\nD3 51.82 55.75 61.75 57.65\n◦,•statistically signiﬁcant improvement or degradation\nTable 8 . Accuracy for the models of density in comparison\nto the baseline using NN, SVM and Decision Trees\nDataset Baseline NN SVM Decision Tree\nD1 53.73 63.52 52.96 54.48\nD2 53.73 50.62 49.64 51.85\nD3 53.73 57.70 50.90 51.36\n◦,•statistically signiﬁcant improvement or degradation\nTable 9 . Accuracy for the models of weight in comparison\nto the baseline using NN, SVM and Decision Trees\nDataset Baseline NN SVM Decision Tree\nD1 56.73 54.51 62.06 63.72\nD2 56.73 57.11 60.90 60.93\nD3 56.73 58.83 67.85 ◦ 67.98◦\n◦,•statistically signiﬁcant improvement or degradation\nTable 10 . Accuracy for the models of range in comparison\nto the baseline using NN, SVM and Decision Trees\n5.2 Guitar data: embellishments\nIn that case, there was a skewed classes distribution, which\nled us to evaluate the sensitivity (true positive rate) rather\nthan the accuracy of the model. Table 11 presents the re-\nsults obtained. It can be seen that, despite the low percent-\nage of sensitivity, the results for the three algorithms in-\ncreased when considering ensemble information (D2, D3).\n6. CONCLUSIONS\nIn this work we have developed a system which studies\nthe interaction between musicians by using techniques re-Dataset NN SVM Decision Tree\nD1 26 20 12\nD2 30 38 26\nD3 30 32 24\nTable 11 . Sensitivity percentage for embellishments\nlated to computational analysis of expressive music perfor-\nmance and machine learning. We have created a database\nconsisting of recordings of 7 jazz standards played by a\nquartet (piano, guitar, bass and drums) and their corre-\nsponding scores. For processing both the recordings and\nthe scores, we have developed code libraries consisting\nof speciﬁc functions for every stage of the process: se-\nlect chords, extract vertical and horizontal descriptors for\nboth notes and chords, align and compare the recordings\nwith the score and extract performance actions. Finally,\nwe have generated models for different datasets consisting\nof information from individual performances and ensem-\nble performances. Based on the accuracy and sensitivity of\nthe models, we have obtained numerical results which have\nallowed us to estimate the level of interaction between mu-\nsicians. The data analysis indicated that, in general terms,\nthe performance actions of the accompaniment are inﬂu-\nenced by the soloist and vice versa, since both written and\nperformed descriptors contributed to a better performance\nof the models.\nIn a future work, it would be interesting to extract other\nperformance actions such as energy or duration for both\nchords and notes and to study the extent to which the\nmeasures are sensitive to the incorporation of other instru-\nments. Moreover, since we have at our disposal a database\nwhich contains the recordings of bass and drums, it would\nbe interesting to incorporate both instruments into the anal-\nysis. We have observed that the majority of the models got\nbetter results with ensemble information but the accura-\ncies of the models could still improve by collecting more\ndata (making new recordings) or extracting more descrip-\ntors. Finally, the parameters of the used algorithms could\nbe further investigated so as to improve the results.\n7. ACKNOWLEDGEMENTS\nThis work has been partly sponsored by the Spanish TIN\nproject TIMUL (TIN2013-48152-C2-2-R) and the H2020-\nICT-688268 TELMI project.\n8. REFERENCES\n[1] Josep Llu ´ıs Arcos, Ramon Lopez De Mantaras, and\nXavier Serra. Saxex: A case-based reasoning sys-\ntem for generating expressive musical performances*.\nJournal of New Music Research , 27(3):194–210, 1998.\n[2] Helena Bantul `a, Sergio Giraldo, and Rafael Ram ´ırez.\nA Rule-based System to Transcribe Guitar Melodies.\nInInernational Symposium on Computer Music Multi-\ndisciplinary Research (CMMR) , pages 1–8, 2015.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 679[3] Alain de Cheveigne and Hideki Kawahara. YIN, a\nfundamental frequency estimator for speech and mu-\nsic.The Journal of the Acoustical Society of America ,\n111(4):1917, 2002.\n[4] Tuomas Eerola and Petri Toiviainen. MIDI toolbox:\nMATLAB tools for music research. University of\nJyv¨askyl ¨a, Jyv ¨askyl ¨a, Finland , 2004.\n[5] Anders Friberg. A quantitative rule system for musical\nperformance . PhD thesis, PhD Thesis, KTH, Sweden,\n1995.\n[6] Alf Gabrielsson. The performance of music. The psy-\nchology of music , 2:501–602, 1999.\n[7] Sergio Giraldo. Modelling embellishment, duration\nand energy expressive transformations in jazz guitar,\n2012.\n[8] Sergio Giraldo and Rafael Ram ´ırez. Computational\ngeneration and synthesis of jazz guitar ornaments using\nmachine learning modeling. In International Workshop\non Machine Learning and Music (MML) , 2015.\n[9] Sergio Giraldo and Rafael Ram ´ırez. Computational\nModeling and Synthesis of Timing, Dynamics and\nOrnamentation in Jazz Guitar Music. In Inernational\nSymposium on Computer Music Multidisciplinary Re-\nsearch (CMMR) , 2015.\n[10] Sergio Giraldo and Rafael Ram ´ırez. Computational\nmodeling of ornamentation in jazz guitar music. In In-\nternational Symposium on Performance Science , 2015.\n[11] Sergio Giraldo and Rafael Ram ´ırez. Score Sequence\nMatching for Automatic Ornament Detection in Jazz\nMusic. In Inernational Conference on New Music Con-\ncepts (ICNMC) , 2015.\n[12] Werner Goebl and Caroline Palmer. Synchronization of\ntiming and motion among performing musicians. 2009.\n[13] Talar Hopyan, Maureen Dennis, Rosanna Weksberg,\nand Cheryl Cytrynbaum. Music skills and the expres-\nsive interpretation of music in children with williams-\nbeuren syndrome: pitch, rhythm, melodic imagery,\nphrasing, and musical affect. Child Neuropsychology ,\n7(1):42–53, 2001.\n[14] Apple Inc. Logic pro x. http://www.apple.com/logic-\npro/.\n[15] Marco Marchini. Analysis of Ensemble Expressive Per-\nformance in String Quartets: A Statistical and Ma-\nchine Learning Approach . PhD thesis, PhD Thesis,\nUPF, Barcelona, 2014.\n[16] Geoffrey Holmes Bernhard Pfahringer Peter Reute-\nmann Ian H. Witten Mark Hall, Eibe Frank. ”The\nWEKA Data Mining Software: An Update” , vol-\nume 11. 2009.[17] MATLAB. version 7.10.0 (R2010a) . The MathWorks\nInc., Natick, Massachusetts, 2010.\n[18] MuseScore. version 1.3 . 2013.\n[19] Caroline Palmer. Music performance. Annual review of\npsychology , 48(1):115–138, 1997.\n[20] Rafael Ramirez and Amaury Hazan. Modeling expres-\nsive music performance in jazz. In FLAIRS Confer-\nence, pages 86–91, 2005.\n[21] Bruno H Repp. Sensorimotor synchronization: a re-\nview of the tapping literature. Psychonomic bulletin &\nreview , 12(6):969–992, 2005.\n[22] John Rink. Musical performance: a guide to under-\nstanding . Cambridge University Press, 2002.\n[23] Caroline Traube and Michel Bernays. Piano Touch\nAnalysis: a Matlab Toolbox for Extracting Perfor-\nmance Descriptors from High Resolution Keyboard\nand Pedalling Data. Journ ´ees d’Informatique Musicale\n(JIM) , 2012.\n[24] G. De Poli A. Friberg R. Bresin W. Goebl, S. Dixon and\nG. Widmer. Sound to sense-sense to sound: a state of\nthe art in sound and music computing , chapter Sense in\nexpressive music performance: Data acquisition, com-\nputational studies, and models, pages 195–242. Berln,\n2008.\n[25] Gerhard Widmer and Asmir Tobudic. Playing mozart\nby analogy: Learning multi-level timing and dynamics\nstrategies. Journal of New Music Research , 32(3):259–\n268, 2003.\n[26] Alan M Wing, Satoshi Endo, Adrian Bradbury, and\nDirk V orberg. Optimal feedback correction in string\nquartet synchronization. Journal of The Royal Society\nInterface , 11(93):20131125, 2014.\n[27] Jos ´e R. Zapata, Andr ´e Holzapfel, Matthew E.P. Davies,\nJoao Lobato Oliveira, and Fabien Gouyon. Assigning a\nConﬁdence Threshold on Automatic Beat Annotation\nin Large Datasets. In ISMIR 2012, Proceedings of the\n13th International Society for Music Information Re-\ntrieval Conference. , number ISMIR, pages 157–162,\n2012.680 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "A Corpus of Annotated Irish Traditional Dance Music Recordings: Design and Benchmark Evaluations.",
        "author": [
            "Pierre Beauguitte",
            "Bryan Duggan",
            "John D. Kelleher"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417333",
        "url": "https://doi.org/10.5281/zenodo.1417333",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/148_Paper.pdf",
        "abstract": "An emerging trend in music information retrieval (MIR) is the use of supervised machine learning to train automatic music transcription models. A prerequisite of adopting a machine learning methodology is the availability of anno- tated corpora. However, different genres of music have dif- ferent characteristics and modelling these characteristics is an important part of creating state of the art MIR systems. Consequently, although some music corpora are available the use of these corpora is tied to the specific music genre, instrument type and recording context the corpus covers. This paper introduces the first corpus of annotations of au- dio recordings of Irish traditional dance music that cov- ers multiple instrument types and both solo studio and live session recordings. We first discuss the considerations that motivated our design choices in developing the corpus. We then benchmark a number of automatic music transcription algorithms against the corpus.",
        "zenodo_id": 1417333,
        "dblp_key": "conf/ismir/BeauguitteDK16",
        "content": "A CORPUS OF ANNOTATED IRISH TRADITIONAL DANCE MUSIC\nRECORDINGS: DESIGN AND BENCHMARK EVALUATIONS\nPierre Beauguitte, Bryan Duggan and John Kelleher\nSchool of Computing, Dublin Institute of Technology\npierre.beauguitte@mydit.ie ,bryan.duggan@dit.ie ,john.d.kelleher@dit.ie\nABSTRACT\nAn emerging trend in music information retrieval (MIR)\nis the use of supervised machine learning to train automatic\nmusic transcription models. A prerequisite of adopting a\nmachine learning methodology is the availability of anno-\ntated corpora. However, different genres of music have dif-\nferent characteristics and modelling these characteristics is\nan important part of creating state of the art MIR systems.\nConsequently, although some music corpora are available\nthe use of these corpora is tied to the speciﬁc music genre,\ninstrument type and recording context the corpus covers.\nThis paper introduces the ﬁrst corpus of annotations of au-\ndio recordings of Irish traditional dance music that cov-\ners multiple instrument types and both solo studio and live\nsession recordings. We ﬁrst discuss the considerations that\nmotivated our design choices in developing the corpus. We\nthen benchmark a number of automatic music transcription\nalgorithms against the corpus.\n1. INTRODUCTION\nAutomatic music transcription is one of the main chal-\nlenges in MIR. The focus of most recent research is on\npolyphonic music, since it is sometimes claimed that the\nproblem of transcribing a melody from a monophonic sig-\nnal is solved [4]. The standard method in evaluating tran-\nscription algorithms is to take a data driven approach. This\nrequires a corpus of data with original audio and ground\ntruth annotations. Several such corpora exist, for example\nthose provided for the annual MIREX evaluations. How-\never no corpus can be universal, and the difﬁculty of gath-\nering such a dataset is often discussed [20].\nWe are interested particularly in Irish traditional dance\nmusic and applications of automatic and accurate tran-\nscription in this genre. This musical tradition has sev-\neral characteristics that are not captured by the existing\ndatasets. Furthermore, to the best of our knowledge, there\nis no corpus of annotated Irish traditional dance music\navailable. Since this is of absolute necessity in order to\nevaluate any new transcription method, we have created\nc/circlecopyrtPierre Beauguitte, Bryan Duggan and John Kelleher. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Pierre Beauguitte, Bryan Duggan and John\nKelleher. “A corpus of annotated Irish traditional dance music record-\nings: design and benchmark evaluations”, 17th International Society for\nMusic Information Retrieval Conference, 2016.and released our own corpus. This paper presents its de-\nsign and some baseline results for existing melody extrac-\ntion algorithms.\nThe structure of the paper is as follows: Section 2 ex-\nplains the rationale behind the creation of this corpus. We\nalso detail the characteristics of the music we consider and\nthe challenges it presents. Section 3 presents existing work\nrelated to our own. We then give the detailed design of our\naudio annotations dataset in Section 4. In Section 5, we\nevaluate four transcription algorithms on the dataset.\n2. CHARACTERISTICS OF IRISH TRADITIONAL\nDANCE MUSIC\nIn common with similar, aurally transmitted musical tradi-\ntions, Irish traditional dance music is subject to variations\nand ornamentation in its interpretation. The melodies, or\ntunes , are usually rather short and consist of two or some-\ntimes more repeated parts.\nThe nature of the music is melodic and modal, as op-\nposed to the more common harmonic and tonal aesthet-\nics of classical and contemporary Western music. Most of\nthe tunes are played on a D major or G major scale, but\nthey can be in different keys including D ionian or mixoly-\ndian (major keys), E dorian or aeolian (minor keys) or G\nmixolydian (major key).\nMany tunes were originally dance tunes, with a ﬁxed\nrhythm, tempo and structure, though nowadays the tunes\nare mostly performed for listening rather than dancing.\nThe most common rhythms are reels, with a 4/4 time sig-\nnature, and jigs, with a 6/8 time signature. Other types\nof tunes include polkas, hornpipes, slides, barndances and\nairs. An in-depth presentation of Irish music can be found\nin [22].\nAlthough in the past Irish music was performed mostly\nby solo instrumentalists, and this type of performance is\nstill highly valued today, in contemporary practice it is\ncommonly played by ensembles of musicians. On com-\nmercial recordings, the most common conﬁguration in-\nvolves several melodic instruments with rhythmic and har-\nmonic accompaniment.\nThe environment in which this music is most commonly\nplayed is that of sessions : gatherings of musicians, profes-\nsional or amateurs, of a usually informal nature. When\nplayed in a session, Irish music can often be adequately\nqualiﬁed as heterophonic [23]. All players of melodic in-\nstruments (typically greater in number than rhytmic and53harmonic instruments) play the same tune together, but the\nresult is often far from unison for several reasons. First of\nall, different instruments can play the same melody in dif-\nferent octaves ( e.g.ﬂute and tin whistle). Additionally, due\nto the acoustic limitations of certain instruments, or as an\nintended variation, some notes of a tune can be played in\na different octave. The low B (B3) for example cannot be\nplayed on most traditional ﬂutes. Consequently ﬂute play-\ners often play a B4 instead, while a banjo player would\nplay the “correct” note B3 and a whistle player would play\nan octave higher than the ﬂute, B5. Yet, all would be con-\nsidered as playing the same tune.\nAnother important aspect is the amount of variations\npresent in Irish music. Because of personal or regional\nstylistic differences, the abundance of different sources\n(notations, archive or commercial recordings), and of the\nmusic being transmitted aurally (thus relying on the mem-\nory of the musician and therefore subject to interpretation),\nmany different versions of a same tune may exist. Al-\nthough musicians will often try to adapt to each other in\norder to play a common version during a session, it is not\nuncommon to hear some differences in the melodies. Fi-\nnally, tunes are almost always ornamented differently by\neach individual musician depending on their style and per-\nsonal preferences.\nOur goal with this project is to create a representative\ncorpus of annotated audio data. We will then be able to es-\ntablish a baseline on the performance of existing transcrip-\ntion algorithms for this particular music style. This will\nfacilitate the development and evaluation of new melody\nextraction algorithms for Irish traditional dance music. We\nbelieve this is of great interest for such applications as ac-\ncurate transcription of archive recordings, improvement of\npopular digital tools for musicians [9], and monitoring of\ncontemporary music practice [21].\n3. RELATED WORK\nAutomatic melody transcription algorithms have been de-\nsigned speciﬁcally for a cappella ﬂamenco singing in [10],\nand evaluated against manual annotations made by experts\nmusicians. Although the article cites previous work related\nto the creation of a representative audio dataset, the man-\nual ground truths annotations were created speciﬁcally for\nthat project.\nTurkish makam music has also been the subject of re-\nsearch, in particular with the SymbTr project [12], whose\ngoal was to offer a comprehensive database of symbolic\nmusic. Applications of automatic transcription algorithms\nhave been studied in [5]. Ground truth annotations were\nobtained by manually aligning the symbolic transcription\nfrom the SymbTr database to existing audio recordings.\nThe paper points out the predominance of monophonic and\npolyphonic Eurogenetic music in Music Information Re-\ntrieval evaluations, and the challenges presented by music\nfalling out of this range, such as heterophonic music.\nApplications of automatic transcription algorithms to\nlarge scale World & Traditional music archives are pro-\nposed in [1]. More than twenty-nine thousand pieces of au-dio have been analysed. The obtained data have been made\navailable through a Semantic Web server. No ground truth\nannotations were used to assess the quality of the transcrip-\ntions, that were obtained by the method presented in [3].\nManual and automatic annotations of commercial solo\nﬂute recordings of Irish traditional music have been con-\nducted in [2], [11] and [13]. The focus in these papers is\non style and automatic detection of ornaments in mono-\nphonic recordings. Consequently every ornament is ﬁnely\ntranscribed in the annotations. An HMM-based transcrip-\ntion algorithm has been developed in [11] to recognise the\ndifferent ornaments as well as the melody. An attempt was\nmade at identifying players from these stylistic informa-\ntions. The authors of [13] are planning on sharing the an-\nnotation corpus via the Semantic Web, but no data is pub-\nlicly available yet.\n4. PRESENTATION OF THE DATASET\nWe now describe the design of our corpus. First we present\nthe set of audio recordings included, that was chosen to\nmake the corpus representative of Irish traditional dance\nmusic. We then detail the annotation format used to deal\nwith the characteristics of this genre.\n4.1 Source of Audio Recordings\nThree sources of recordings are included in the corpus:\n•session recordings accompanying the Foinn Seisi ´un\nbooks published by the Comhaltas Ceolt ´oir´ı´Eireann\norganisation, available with Creative Commons li-\ncence. These offer good quality, homogeneous ex-\namples of the heterophony inherent to an Irish tradi-\ntional dance music session.\n•Grey Larsen’s MP3s for 300 Gems of Irish Music\nfor All Instruments , commercially available. These\nconsist of studio quality recordings of tunes played\non Irish ﬂute, tin whistle and anglo concertina.\n•personal recordings of the second author, a\nrenowned musician, on the Irish ﬂute. These are\navailable together with the annotations.\nThe corpus comprises thirty tunes in total, which add up\nto more than thirty minutes of audio. We chose to include\nsolo recordings as a way of comparing the performance of\ntranscription algorithms on monophonic and heterophonic\nmusic. This set of tunes has been chosen to be representa-\ntive of the corpus of Irish music in terms of type and key\nsignature. Table 1 categorises the tunes in our corpus by\ntune type, key, and performance type.\nThe complete list of tunes with all relevant metadata is\nincluded with the dataset.\n4.2 Format of the Annotations\nWe annotated each audio recording with note events, con-\nsisting of three values: onset time, duration and pitch. For\nthe larger goal of obtaining a symbolic transcription, this\nformat of annotation is more useful as well as easier to54 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Reel Jig Hornpipe Polka Slide Air\nDmaj 2 3 1 1 0 0\nGmaj 2 2 1 2 1 0\nAmin 2 2 1 1 0 0\nEmin 2 2 1 0 1 1\nBmin 1 0 0 0 1 0\nSession 5 5 1 1 1 0\nSolo 4 4 3 3 2 1\nTable 1 : Classiﬁcation of tunes in our corpus by tune type,\nkey, and performance type\nobtain than a continuous pitch track labelling every au-\ndio frame. Despite the heterophonic nature of the session\nperformances, there is always a single underlying mono-\nphonic melody. It is this melody we are interetsed in. For\nthis reason there is no overlap between the notes, and the\nresulting transcription we present is monophonic.\nDue to the heterophonic nature of Irish traditional music\nas played during a session, and to the slight tuning differ-\nences between the instruments, a single fundamental fre-\nquency cannot appropriately describe a note. Therefore we\ndecided to report only MIDI note references instead of fre-\nquencies.\nIn session performances, the playing of ornamentation\nsuch as rolls andcuts [22] often results in several suc-\ncessive onsets for a single long note. Figure 1 shows an\nexample of such a situation, where three different instru-\nments interpret differently the same note (the short notes\nplayed by the ﬂute are part of a rolland are not melodi-\ncally signiﬁcant, therefore they are not transcribed). This\nmakes it difﬁcult, even for experienced musicians and lis-\nteners, to know when repeated notes are to be considered\nas a single ornamented note or distinct notes. Because of\nthis inherent ambiguity, it is not correct to associate one\nonset with one note in the transcription. For this reason,\nwe decided to merge consecutive notes of identical pitch\ninto one single note. A note in our transcriptions then\ncorresponds to a change of pitch in the melody. For solo\nperformances, there are some clear silences between notes\n(typically when the ﬂute or whistle player takes a breath).\nWhenever such a silence occurs, we annotated two distinct\nnotes even if they are of the same pitch. In the solo record-\nings present in the corpus, a breath typically lasts around\n200ms. Notes that are repeated without pauses or cut by\nan ornament were still reported as a single note, in order to\nbe consistent with the annotations of session recordings.\nManual annotations were made by the ﬁrst author with\nthe aid of the Tony software [14]. After importing an audio\nﬁle, Tony offers estimates using the pYIN algorithm (note-\nlevel) presented in the next section. These were then man-\nually corrected, by adding new notes, merging repeated\nnotes and adjusting frequencies. The annotations were\nﬁnally post-processed to convert these frequencies to the\nclosest MIDI note references. With this annotation format,\nthe dataset comprises in total more than 8600 notes.\nFiddle\nFlute\nBanjo\nTranscribedFigure 1 : Example of ornamented notes on different in-\nstruments\n4.3 Open Publication of the Dataset\nThe dataset is publicly available as a set of csv ﬁles.1\nEach ﬁle contains the annotation for an entire audio ﬁle.\nEach line represents a note (as onset time, duration, MIDI\nnote). The annotations can be easily used in any evalua-\ntion framework as well as with the software Sonic Visu-\naliser [6].\n5. EVALUATION OF EXISTING TRANSCRIPTION\nALGORITHMS\nIn order to establish some baselines for melody extraction\nin recordings of Irish traditional dance music, we evaluate\ntranscriptions of our audio corpus returned by four differ-\nent algorithms.\n5.1 Presentation of the Algorithms\nThe melody extraction (or more generally automatic tran-\nscription) algorithms we use rely on different signal pro-\ncessing approaches, and return estimated melodies in two\ndifferent formats. Some return frame-level estimates, or\ncontinuous pitch tracks, in which case some pre-processing\ndetailed later in 5.2.1 is needed to conduct the evaluations.\nOthers return note-level estimates of the same format as\nthat used for our annotations, sometimes with a continu-\nous pitch track as an intermediate step.\n5.1.1 pYIN\npYIN [15] stands for probabilistic YIN, and is based on\nthe standard frequency estimation algorithm YIN [7], used\nin conjunction with HMM-based pitch tracking. The ini-\ntial algorithm returns frame-level estimates, but an addi-\ntional segmentation step based on HMM modelling of note\nevents was introduced in [14]. We evaluate the two ver-\nsions of the algorithm, both open source and available as a\nVamp plug-in.2\nIt is important to note that because we manually an-\nnotated our corpus with Tony, offering the note-level es-\ntimates from pYIN as ﬁrst approximations, results of the\nevaluations might be biased in favour of this method.\n5.1.2 Melodia\nMelodia [19] ﬁrst extracts a salience function by detect-\ning peaks in the time/frequency representation of the au-\n1https://github.com/pierrebeauguitte/tuneset\n2https://code.soundsoftware.ac.uk/projects/pyinProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 55dio signal and then extracts the best continuous pitch track\npossible. It is available as a Vamp plug-in.3\n5.1.3 MATT\nMATT [8] stands for Machine Annotation of Traditional\nTunes. It returns note-level transcriptions, by estimating\nthe fundamental frequency from the harmonicity of the sig-\nnal, and segmenting the resulting continuous pitch track\naccording to its continuity as well as the energy level. Al-\nthough the method used is not at the state of the art of\nmelody extraction algorithms (for an overview, see [18]),\nthe fact that it was designed speciﬁcally for and ﬁne-tuned\nto Irish traditional music makes it of great interest to us. A\nJava implementation is available online.4\n5.1.4 Silvet\nSilvet [3] is based on Principal Latent Component Anal-\nysis. Although it is designed for polyphonic music tran-\nscription, obtaining a single melody track is achievable by\nsimply limiting the number of notes occuring at any time to\none. It ﬁrst generates a pitch track by factorising the spec-\ntrogram according to predeﬁned templates. This is then\npost-processed with HMM smoothing, in a similar man-\nner to the pYIN segmentation step. This approach has a\nmuch higher computationnal cost due to the complexity of\nspectrogram factorisation. It is available as an open source\nVamp plug-in.5\n5.2 Evaluations of the Transcriptions\nIn order to be consistent with our annotation policy (see\n4.2), it is necessary to post-process the estimates of these\nalgorithms in the following manner:\n•align all frequencies to the closest MIDI note;\n•merge consecutive notes of same pitch separated by\nless than 200ms (only for note-level estimates).\nThe second step is particularly critical for the note-level\nmetrics of the MIREX Note Tracking task, but will also af-\nfect the frame-level metrics of the Melody Extraction tasks\nfor the frames in the ﬁlled gaps.\nAll evaluations are performed with the mireval open\nsource framework presented in [16].6In order to assess\nthe statistical signiﬁcance of the differences in scores, we\nuse the Wilcoxon signed rank test (to compare our sam-\nples with the performances reported in the original publica-\ntions), and the Mann-Whitney U-test (to compare between\nour different samples).\n5.2.1 Frame-Level Evaluation: Melody Extraction Task\nThe MIREX Melody Extraction task evaluates transcrip-\ntions at the frame-level. Pre-processing of both the ground\ntruth and the estimates is necessary, and simply consists of\naligning both on the same 10ms time grid. The pitch esti-\nmate for a frame is considered correct if it is distant from\n3http://mtg.upf.edu/technologies/melodia\n4https://github.com/skooter500/matt2\n5https://code.soundsoftware.ac.uk/projects/silvet\n6https://github.com/craffel/mir_evalthe ground truth by less than a quarter of a tone (50 cents).\nThe metrics also look for voicing errors: a voiced frame is\none where a melody pitch is present. Five different metrics\nare computed for each tune. Results are shown, using box-\nplots, in Figure 2, (a) for the solo recordings, and (b) for\nthe session recordings.\nThe original publications introducing frame-level pYIN\nand MATT do not report these metrics. In [3], Silvet was\nonly evaluated on corpora of polyphonic music, for which\nthese metrics are not suitable. pYIN - notes was evaluated\nwith the audio dataset from, and scores reported in [14]\nranged from 0.83 to 0.85 for Overall Accuracy, and from\n0.87 to 0.91 for Raw Pitch Accuracy. Evaluations con-\nducted in [19] for Melodia on audio datasets from the\nMIREX evaluation resulted in Overall Accuracy of 0.77,\nRaw Chroma Accuracy of 0.83 and Raw Pitch Accuracy\nof 0.81. Scores obtained on our dataset are signiﬁcantly\nlower for all these metrics and samples ( p-values <0.01),\nfor both solo and session recordings. This seems to sug-\ngest that the genre of Irish traditional dance music does\nhave some distinct characteristics, not limited to the het-\nerophonic aspect of a session, that can present challenges\nfor automatic transcription.\nComparing the Overall Accuracy on the solo and ses-\nsion subsets, it is interesting to see that MATT and both\nversions of pYIN have signiﬁcantly lower scores on the\nsession subset ( p-values <0.01), whereas Melodia and Sil-\nvet do not. We believe that this is because the Melodia\nand Silvet algorithms were speciﬁcally designed for poly-\nphonic music analysis.\nRaw chroma accuracy is, by deﬁnition, always higher\nthan raw pitch accuracy. We observe that, on the solo\nrecordings, this difference is only signiﬁcant for Melodia\n(p-value <0.05). On the session subset, it is very signif-\nicant ( p-values <0.001) for all algorithms except Silvet.\nThis suggests that Silvet is more robust for estimating fun-\ndamental frequencies.\n5.2.2 Note-Level Evaluation: Note Tracking Task\nWe now present the results of the MIREX Note Track-\ning task. Although this task is primarily aiming at poly-\nphonic music transription systems, it also applies directly\nto monophonic music as long as both ground truth annota-\ntions and returned estimates are in a note-event format. In\nour case, this applies to pYIN - notes, MATT and Silvet.\nEstimated notes are associated with their closest match\nfrom the ground truth, and a note is considered correctly\ntranscribed if its onset is distant from the reference note\nby less than 50ms and its pitch by less than a quarter of a\ntone. Another way of evaluating the transcription is to also\ntake the duration of notes into account. Commonly found\ninstruments in Irish music have a wide range of acoustical\ncharacteristics: some (like the ﬂute, the ﬁddle, the uilleann\npipes) can be played legato orstaccato , depending on per-\nsonal or regional styles, or on the type of tunes performed;\nothers (typically the banjo) can only be played staccato ,\nwith hard onsets and very little sustain. Consequently, the\noffset of the notes is of little interest for our evaluations,56 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016(a) Solo recordings\n(b) Session recordings\nFigure 2 : Scores of the MIREX Melody Extraction task evaluation metrics.\nparticularly in session recordings where all these instru-\nments can play together. This is why we only report results\nfor the ﬁrst type of evaluation.\nPrecision, recall and F-measures are computed with the\nmireval framework [16], and plotted in Figure 3, (a) for\nthe solo recordings and (b) for the session recordings. The\nﬁgures also show the results for the Chroma correctness,\nwhere a note is correctly transcribed if its onset ( ±50ms)\nand its pitch class are correct. This is of interest to us be-\ncause of the heterophonic nature of session performance\ndescribed in Section 2.\nIt is interesting to observe that MATT achieves the high-\nest F-measures on the solo recordings. However, only the\ndifference with Silvet is statistically signiﬁcant ( p-values\n<0.05). On the session subset, Silvet performs better than\nthe other two algorithms, with high statistical signiﬁcance\n(p-values <0.01).\nPrecision scores are not signiﬁcantly different on the\nsolo recordings. On the session recordings, Silvet achieves\nsigniﬁcantly higher pitch precisions ( p-values <0.001).\nHowever, when looking at the chroma precision, the differ-\nence with pYIN is no longer signiﬁcant. Scores for MATT\nremain signiﬁcantly lower ( p-values <0.001).\nSurprisingly, Silvet has higher F-measures on the ses-\nsion subset than on the solo subset, and this difference is\nstatistically signiﬁcant ( p-value <0.05). pYIN and MATT\nboth score lower on the session subset. For the pitch F-\nmeasure, this difference is highly signiﬁcant, with p-values<0.001. For the chroma F-measure, only MATT scores\nsigniﬁcantly lower ( p-value <0.01).\n6. CONCLUSION\nIn this paper, we introduced a new dataset of annotations\nof Irish traditional dance music. It is, to our knowledge,\nthe ﬁrst publicly available corpus of manually transcribed\naudio recordings in this genre. It covers a representative\nrange of tune types, keys, and performance types. From the\nresults obtained with state of the art automatic transcription\nalgorithms, it appears that the heterophonic nature of this\nmusic presents challenges for MIR. It would have been of\ngreat interest to evaluate the HMM based algorithm used\nfor ﬂute ornaments recognition in [11], but unfortunately\nno implementation of it is publicly available.\nThese ﬁndings are good motivation to work towards the\ndevelopment of new methods for automatically transcrib-\ning Irish traditional dance music. We believe that this cor-\npus will be of great use for this purpose, both for training\ndata-driven models and to evaluate new algorithms.\nWe hope to make the corpus larger in the future, so that\nit includes a wider range of instruments and performance\ntypes (violin or ﬁddle , banjo, session recordings from other\nsources). We are also planning on making use of the Mu-\nsic Ontology [17] in later releases. Adopting the standards\nof the Semantic Web will hopefully allow more interaction\nwith many resources such as, for example, the Europeana-\nSounds database.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 57(a) Solo recordings\n(b) Session recordings\nFigure 3 : Scores of the MIREX Note Tracking task evaluation metrics.\n7. REFERENCES\n[1] Samer Abdallah, Aquiles Alencar-Brayner, Em-\nmanouil Benetos, Stephen Cottrell, Jason Dykes, Nico-\nlas Gold, Alexander Kachkaev, Mahendra Mahey, Dan\nTidhar, Adam Tovell, Tillman Weyde, and Daniel\nWolff. Automatic Transcription and Pitch Analysis of\nthe British Library World and Traditional Music Col-\nlections. In Proceedings of the 5th International Work-\nshop on Folk Music Analysis , pages 10–12, 2015.\n[2] Islah Ali-MacLachlan, M ¨unevver K ¨ok¨uer, Cham Ath-\nwal, and Peter Jan ˇcoviˇc. Towards the Identiﬁcation\nof Irish Traditional Flute Players from Commercial\nRecordings. In Proceedings of the 5th International\nWorkshop on Folk Music Analysis , pages 13–17, 2015.\n[3] Emmanouil Benetos and Simon Dixon. A Shift-\nInvariant Latent Variable Model for Automatic Music\nTranscription. Computer Music Journal , 36(4):81–94,\nDecember 2012.\n[4] Emmanouil Benetos, Simon Dixon, Dimitrios Gian-\nnoulis, Holger Kirchhoff, and Anssi Klapuri. Auto-\nmatic music transcription: challenges and future di-\nrections. Journal of Intelligent Information Systems ,\n41(3):407–434, July 2013.\n[5] Emmanouil Benetos and Andr ´e Holzapfel. Automatic\ntranscription of Turkish microtonal music. The Jour-nal of the Acoustical Society of America , 138(4):2118–\n2130, 2015.\n[6] Chris Cannam, Christian Landone, and Mark Sandler.\nSonic visualiser: An open source application for view-\ning, analysing, and annotating music audio ﬁles. In\nProceedings of the 18th ACM international conference\non Multimedia , pages 1467–1468. ACM, 2010.\n[7] Alain de Cheveign ´e and Hideki Kawahara. YIN, a\nfundamental frequency estimator for speech and mu-\nsic.The Journal of the Acoustical Society of America ,\n111(4):1917, 2002.\n[8] Bryan Duggan. Machine annotation of traditional Irish\ndance music . PhD thesis, Dublin Institute of Technol-\nogy, June 2009.\n[9] Bryan Duggan and Brendan O’Shea. Tunepal: search-\ning a digital library of traditional music scores. OCLC\nSystems & Services: International digital library per-\nspectives , 27(4):284–297, October 2011.\n[10] Emilia G ´omez and Jordi Bonada. Towards Computer-\nAssisted Flamenco Transcription: An Experimental\nComparison of Automatic Transcription Algorithms\nAs Applied to A Cappella Singing. Computer Music\nJournal , 37(2):73–90, 2013.\n[11] Peter Jan ˇcoviˇc, M ¨unevver K ¨ok¨uer, and Baptiste\nWrena. Automatic Transcription of Ornamented Irish58 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Traditional Flute Music Using Hidden Markov Mod-\nels. In Proceedings of the 16th International Society\nfor Music Information Retrieval Conference , 2015.\n[12] M. Kemal Karaosmano ˘glu. A Turkish makam mu-\nsic symbolic database for music information retrieval:\nSymbTr. In Proceedings of the 13th International Soci-\nety for Music Information Retrieval Conference , 2012.\n[13] M ¨unevver K ¨ok¨uer, Daith ´ı Kearney, Islah Ali-\nMacLachlan, Peter Jan ˇcoviˇc, and Cham Athwal.\nTowards the creation of digital library content to\nstudy aspects of style in Irish traditional music. In\nProceedings of the 1st International Workshop on\nDigital Libraries for Musicology , DLfM ’14, pages\n1–3. ACM Press, 2014.\n[14] Matthias Mauch, Chris Cannam, Rachel Bittner,\nGeorge Fazekas, Justin Salamon, Jiajie Dai, Juan\nBello, and Simon Dixon. Computer-aided Melody\nNote Transcription Using the Tony Software: Accu-\nracy and Efﬁciency. In Proceedings of the First Inter-\nnational Conference on Technologies for Music Nota-\ntion and Representation , 2015.\n[15] Matthias Mauch and Sam Dixon. pYIN: A fundamen-\ntal frequency estimator using probabilistic threshold\ndistributions. In Proceedings of the 39th International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , pages 659–663. IEEE, 2014.\n[16] Colin Raffel, Brian McFee, Eric J. Humphrey, Justin\nSalamon, Oriol Nieto, Dawen Liang, and Daniel PW\nEllis. mir eval: A Transparent Implementation Of\nCommon MIR Metrics. In Proceedings of the 15th\nInternational Society for Music Information Retrieval\nConference , 2014.\n[17] Yves Raimond, Samer A. Abdallah, Mark B. Sandler,\nand Frederick Giasson. The Music Ontology. In Pro-\nceedings of the 8th International Society for Music In-\nformation Retrieval Conference , pages 417–422, 2007.\n[18] Justin Salamon, Emilia Gomez, Daniel P.W. Ellis, and\nGuilhem Richard. Melody extraction from polyphonic\nmusic signals: Approaches, applications, and chal-\nlenges. Signal Processing Magazine, IEEE , 31(2):118–\n134, 2014.\n[19] Justin Salamon and Emilia G ´omez. Melody extrac-\ntion from polyphonic music signals using pitch contour\ncharacteristics. IEEE Transactions on Audio, Speech,\nand Language Processing , 20(6):1759–1770, 2012.\n[20] Li Su and Yi-Hsuan Yang. Escaping from the Abyss\nof Manual Annotation: New Methodology of Building\nPolyphonic Datasets for Automatic Music Transcrip-\ntion. In International Symposium on Computer Music\nMultidisciplinary Research , 2015.\n[21] Norman Makoto Su and Bryan Duggan. TuneTracker:\ntensions in the surveillance of traditional music. InACM conference on Designing Interactive Systems ,\npages 845–854. ACM Press, 2014.\n[22] Fintan Vallely, editor. The Companion to Irish Tradi-\ntional Music . Cork University Press, Cork, second edi-\ntion edition, 2011.\n[23] Sean Williams. Focus: Irish Traditional Music . Rout-\nledge, 2013.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 59"
    },
    {
        "title": "A Methodology for Quality Assessment in Collaborative Score Libraries.",
        "author": [
            "Vincent Besson",
            "Marco Gurrieri",
            "Philippe Rigaux",
            "Alice Tacaille",
            "Virginie Thion"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1418105",
        "url": "https://doi.org/10.5281/zenodo.1418105",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/076_Paper.pdf",
        "abstract": "We examine quality issues raised by the development of XML-based Digital Score Libraries. Based on the authors’ practical experience, the paper exposes the quality short- comings inherent to the complexity of music encoding, and the lack of support from state-of-the-art formats. We also identify the various facets of the “quality” concept with respect to usages and motivations. We finally propose a general methodology to introduce quality management as a first-level concern in the management of score collections.",
        "zenodo_id": 1418105,
        "dblp_key": "conf/ismir/BessonGRTT16",
        "content": "A METHODOLOGY FOR QUALITY ASSESSMENT IN COLLABORATIVE\nSCORE LIBRARIES\nVincent Besson1Marco Gurrieri1Philippe Rigaux2\nAlice Tacaille3Virginie Thion4\n1CESR, Univ. Tours, France\n2CEDRIC/CNAM, Paris, France\n3IReMus, Sorbonne Universit ´es, Paris, France\n3IRISA, Univ. Rennes 1, Lannion, France\n{vincent.besson,marco.gurrieri }@univ-tours.fr, philippe.rigaux@cnam.fr,\nalice.tacaille@paris-sorbonne.fr, virginie.thion@irisa.fr\nABSTRACT\nWe examine quality issues raised by the development of\nXML-based Digital Score Libraries. Based on the authors’\npractical experience, the paper exposes the quality short-\ncomings inherent to the complexity of music encoding, and\nthe lack of support from state-of-the-art formats. We also\nidentify the various facets of the “quality” concept with\nrespect to usages and motivations. We ﬁnally propose a\ngeneral methodology to introduce quality management as a\nﬁrst-level concern in the management of score collections.\n1. INTRODUCTION\nThere is a growing availability of music scores in digital\nformat, made possible by the combination of two factors:\nmature, easy-to-use music editors, including open-source\nones like MuseScore [10], and sophisticated music nota-\ntion encodings. Leading formats today are those which\nrely on XML to represent music notation as structured doc-\numents. MusicXML [7] is probably the most widespread\none, due to its acceptance by major engraver softwares\n(Finale, Sibelius, and MuseScore) as an exchange format.\nThe MEI initiative [13, 9], inspired by the TEI, attempts\nto address the needs of scholars and music analysts with\nan extensible format [8]. Recently, the launch of the W3C\nMusic Notation Community Group [15] conﬁrms that the\nﬁeld tends towards its maturity, with the promise to build\nand preserve large collections of scores encoded with ro-\nbust and well-established standards. We are therefore fac-\ning emerging needs regarding the storage, organization and\naccess to potentially very large Digital Libraries of Scores\n(DSL). It turns out that building such a DSL, particularly\nwhen the acquisition process is collaborative in nature,\nc/circlecopyrtVincent Besson, Marco Gurrieri, Philippe Rigaux, Al-\nice Tacaille, Virginie Thion. Licensed under a Creative Commons At-\ntribution 4.0 International License (CC BY 4.0). Attribution: Vincent\nBesson, Marco Gurrieri, Philippe Rigaux, Alice Tacaille, Virginie Thion.\n“A METHODOLOGY FOR QUALITY ASSESSMENT IN COLLAB-\nORATIVE SCORE LIBRARIES”, 17th International Society for Music\nInformation Retrieval Conference, 2016.gives rise to severe quality issues. In short, we are likely\nto face problems related to validity (measure durations,\nvoices and parts synchronization), consistency (heteroge-\nneous notations, high variability in the precision of meta-\ndata, undetermined or inconsistent editorial rules), com-\npleteness (missing notes, directives, ornamentation, slurs\nor ties), and accuracy (music, lyrics).\nThere are many reasons for this situation. First, encod-\ning formats have changed a lot during the last decades. We\nsuccessively went through HumDrum and MIDI to ﬁnally\ncome up with modern XML formats such as MusicXML\nand MEI [14]. A lot of legacy collections have been con-\nverted from one encoding to the other, losing information\nalong the way. Given the cost and time to edit scores,\nincorporating these collections in a modern repository is\na strong temptation, but requires to accept, measure, and\nkeep track of their quality shortcomings.\nSecond, the ﬂexibility of music notation is such that it is\nextremely difﬁcult to express and check quality constraints\non the representation. Many of the formats we are aware\nof for instance do not impose that the sequence of events\nin a measure exactly covers the measure duration deﬁned\nby the metrics. As another example, in polyphonic mu-\nsic, nothing guarantees that the parts share the same metric\nand same duration. So, even with the most sophisticated\nencoding, we may obtain a score presentation which does\nnot correspond to a meaningful content (the deﬁnition of\nwhich is context-dependent), and will lead to an incorrect\nlayout (if not a crash) with one of the possible renderers.\nThird, scores are being produced by individuals and in-\nstitutions with highly variables motivations and skills. By\n“motivation”, we denote here the purpose of creating and\nediting a score in digital format. A ﬁrst one is obviously\nthe production of material for performers, with various lev-\nels of demands. Some users may content themselves with\nschematic notation of simple songs, whereas others will\naim at professional editing with high quality standards.\nThe focus here is on rendering, readability and manage-\nability of the score sheets in performance situation. An-\nother category of users (with, probably, some overlap) are\nscientiﬁc editors, whose purpose is rather an accurate and330long-term preservation of the source content (including\nvariants and composer’s annotations). The focus will be\nput on completeness: all variants are represented, editor’s\ncorrections are fully documented, links are provided to\nother resources if relevant, and collections are constrained\nby carefully crafted editorial rules. Overall, the quality\nof such projects is estimated by the ability of a document\nto convey as respectfully as possible the composer’s in-\ntent as it can be perceived through the available sources.\nLibrarians are particularly interested by the searchability\nof their collections, with rich annotations linked to tax-\nonomies [12]. We ﬁnally mention analysts, teachers and\nmusicologists: their focus is put on the core music mate-\nrial, minoring rendering concerns. In such a context, part\nof the content may be missing without harm; accuracy, ac-\ncessibility and clarity of the features investigated by the\nanalytic process are the main quality factors.\nFinally, even with modern editors, qualiﬁed authors,\nand strong guidelines, mistakes are unavoidable. Editing\nmusic is a creative process, sometimes akin to a free draw-\ning of some graphic features whose interpretation is be-\nyond the software constraint checking capacities. A same\nresult may also be achieved with different options (e.g., the\nlayer feature of Finale), sometimes yielding a weird and\nconvoluted encoding, with unpredictable rendering when\nsubmitted to another renderer.\nThe authors of the present paper are in charge of the pro-\nduction, maintenance and dissemination of digital libraries\nof scores encoded in XML (mostly, MEI). N EUMA is an\nopen repository of scores in various formats, managed by\nthe IReMus1, and publicly accessible at http://neuma.\nhuma-nm.fr . The CESR2publishes rare collections of\nRenaissance music for scholars and musicians (see, e.g.,\nthe “Lost voices” project, http://digitalduchemin.\norg). Both institutions have been confronted with the need\nto address issues related to the consistent production of\nhigh-level quality corpora, and had to deal with the poor\nsupport offered by existing tools. The current, ad-hoc,\nsolution adopted so far takes the form of editorial rules.\nThe approach is clearly unsatisfying and unable to solve\nthe above challenges. Even though we assume that the\nscores are edited by experts keen to comply with the rec-\nommendations, nothing guarantees that they are not misin-\nterpreted, or that the guidelines indeed result in a satisfying\nencoding. Moreover, rules that are not backed up by auto-\nmatic validation safeguards are clearly non applicable in a\ncollaborative context where un-controlled users are invited\nto contribute to the collections.\nIn the rest of the paper we position our work with\nrespect to the ﬁeld of quality management in databases\nand Digital Libraries (Section 2) and propose a general\nmethodology to cope with quality issues in the speciﬁc area\nof digital score management. Section 3 exposes a qual-\nity management model. We apply this model to represent\ndata quality metrics, usages and goals, as explained in Sec-\ntion 4, which includes our initial taxonomy of data quality\n1Institut de Recherche en Musicologie ,http://iremus.cnrs.fr .\n2Centre d’Etudes Sup ´erieures de la Renaissance ,http://cesr.\nuniv-tours.fr .metrics for score libraries. Finally, Section 5 recalls the\ncontributions and outlines our perspectives.\n2. QUALITY MANAGEMENT IN DATABASES\nAND DIGITAL LIBRARIES\nMuch published data suffers from endemic quality prob-\nlems. It is now well-recognized that these problems may\nlead to severe consequences, and that managing the quality\nof data conditions the success of most existing information\nsystems [5]. Data quality is a complex concept, which em-\nbraces different semantics depending on the context [11].\nIt is described through a set of quality dimensions aiming\nto categorize criteria of interest. Classical quality dimen-\nsions are completeness (the degree to which needed infor-\nmation is present in the collection), accuracy (the degree to\nwhich data are correct), consistency (the degree to which\ndata respect integrity constraints and business rules) and\nfreshness (the degree to which data are up-to-date). Data\nquality over a dimension is measured according to a set of\nmetrics that allow a quantitative deﬁnition and evaluation\nof the dimension. Examples of metrics are “the number of\nmissing meta-data” for the evaluation of the completeness ,\nand “the number of conﬂicting duplicates” for consistency .\nThese are simple examples but the literature proposes a\nlarge range of dimensions and metrics, conceptualized in\nquality models [4]. Of course, not all the existing dimen-\nsions and metrics may be used for evaluating data quality\nin a given operational context. An important property con-\ncerning data quality is that it is deﬁned according to ﬁtness\nfor use of data, meaning that quality measurement involves\ndimensions and metrics that are relevant to a given user for\na given usage . User u1may be concerned by some quality\nmetrics for a speciﬁc usage, by some other metrics for an-\nother one, and they can be completely different than those\nneeded by user u2.\nThe literature proposes general methodologies for man-\naging data quality [3]. We focus here on its assessment .\nRoughly speaking, each assessment methodology includes\naquality deﬁnition stage and a quality measurement one.\nIn the ﬁrst stage, the quality deﬁnition consists in eliciting\ndata quality requirements. Concretely, this means choos-\ning quality dimensions and metrics of interest, and even-\ntually thresholds associated with. Because data quality is\nﬁtness for use (depends on the context), deﬁning data qual-\nity is not trivial. Dedicated methodological guidelines may\nbe followed like the Goal Question Metric [2], which pro-\nposes to deﬁne quality metrics according to a top-down\nanalysis of quality requirements. For each user (or each\nuser role) and for each of his/her usages of data, conceptual\ngoals are identiﬁed. Goals specify the intent of measure-\nment according to a usage of data. Each goal is then re-\nﬁned into a set of operational quality questions . Each such\nquestion is itself expressed in terms of a set of quantitative\nquality metrics with possible associated thresholds (ex-\npected values). Measuring the quality metrics enables to\n(partly) answer to the quality questions, and consequently\nenables to decide whether data satisfy the requirements for\nthe given goal (and each usage by extension).Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 331Data quality methodologies are designed at a generic\nlevel, leading to difﬁculties for their implementation in a\nspeciﬁc context (operational context and available infor-\nmation system and data). Additional context-dependent\nquality methodologies are then needed. We propose such a\nmethodology for an explicit and systematic data quality as-\nsessment in DSL. To our knowledge, such a methodology\nhas never been proposed in the MIR literature so far.\n3. OUR QUALITY MANAGEMENT MODEL\nWe assume a very general organization of a DSL, where\natomic objects are scores , organized in collections . We\nfurther assume that scores are encoded as structured docu-\nments (typically in MusicXML or MEI) that supply a ﬁne-\ngrained representation of all their structural, content, and\nrendering aspects.\nThe main components of the model are (i) modelization\nof metrics at the score level and collection levels, and of\ntheir relationships, (ii) deﬁnition of usages and goals, ex-\npressed with respect to these metrics, and (iii) computation\nof quality metrics. We present these concepts in order.\n3.1 Quality schema\nThe initial step to address quality issues is to determine\nthe set of relevant indicators, or metrics , that support the\nquality evaluation, and how they are related to each other.\nFigure 1 . Quality schema: score-level and collection-level\nmetrics\nIn our context, we consider score-level metrics , com-\nputed from individual scores, and collection-level met-\nrics, essentially computed by aggregation from the score\nlevel. We use lowercase/uppercase symbols (e.g., morM\nto speciﬁcally represent, resp., score-level and collection-\nlevel elements (metrics, values, or functions), and small\ncapitals (e.g., M) when they do not need to be distin-\nguished. We denote by Mscthe set of score-level quality\nmetrics,Mcollthe set collection-level metrics, and as M\ntheir unionMsc∪M coll.\nMetrics are clustered in quality dimensions . For sim-\npliﬁcation reasons, we suppose that (i) a metric belongs to\nexactly one quality dimension and that (ii) each metric is\nrelevant for every score/collection of the library (the model\ncan easily be extended if these restrictions are too strong).\nFor each metric m∈M , we denote by dom (m)the do-\nmain of the metric.\nEach DSL has therefore to determine a two-levels\norganization of dimensions and metrics that constitutesthequality schema . Fig. 1 shows its general form.\nThe value of each (score-level) metric miis computed\nfrom an atomic object (a score) by some function fi.\nThe domain of a metric can be a Boolean ( “the tempo\nis / is not missing” ), an integer ( “nmeasures are com-\nplete” ), a rational ( “position is given for xnotes out\nofy”), etc. In the case of numeric domains, for\nconvenience, we map each value to a predeﬁned scale\nSof the form{very poor (1), poor (2), borderline (3),\ngood (4), very good (5),notrelevant (⊥)}easily adapt-\nable if needed.\nThe value of a (collection-level) metric Mjis obtained\nby an aggregation function Fjwhich operates over the\nscore-level metric vectors. As an illustration, imagine that\nwe aim at representing the syntactic consistency Msof a\ncollection, deﬁned as a standard variation from the follow-\ning score-level values: presence of bars mb, presence of\ndirectives md, presence of ornamentation mo. Then the\naggregation function Fstakes as input a set of triplets\n(vb, vd, vo), which denotes values for mb,mdandmo\nresp., one for each score of the collection. In the general\ncase, an aggregation function Fmight take into account\nthe whole set of score-level values.\n3.2 Usages, goals, and proﬁles\nConsistencym1mimi+1DimensionMetrics…Completenessmk…Accuracy………Score-level taxonomyCollection-level taxonomy\ng1gj…gj+1gk……\nConsistencyM1MpMp+1DimensionMetrics…CompletenessMq…Accuracy………\nScore-level goals\nG1Gp…Gp+1Gq……Collection-levelgoals\nG1Gp…Gp+1Gq……\nG1Gp…Gp+1Gq……usage Ausage Busage C\nVincent\nAlice\ng1gj…gj+1gk……usage ausage c\nFigure 2 . Usages, metric goals, and proﬁles\nAssume now that a quality schema is deﬁned for a DSL\nmanaging a set of collections. We are then able to propose\nto the DSL users a support to express their quality require-\nments. The main concepts at this level are usages ,metric\ngoals andproﬁles (Fig. 2). A quality metric goal assigns an\nexpected quality level for a metric as a threshold th∈S.\nA user can express requirements as a set of goals rel-\native to a subset of the metrics, ignoring those which she\ndeems irrelevant in the context of a speciﬁc usage . For\ninstance a melodic analyst can, for this usage , choose to\nsafely ignore the quality metrics that pertain to directives\nor lyrics. Conversely, for publication purposes, directives\nand lyrics quality will be required to match a high-quality\nthreshold, whereas analytic annotations are irrelevant. Re-\nquirements are therefore usage-related, and take for each\nusage the form of a set of goals on the metrics speciﬁcally\nrelevant for this usage. The speciﬁcation of such a require-\nment constitutes what we call a quality proﬁle .332 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016From a methodological point of view, each quality pro-\nﬁle, including embedded quality dimensions and metrics,\nis deﬁned by following the Goal Question Metric approach\n(see Section 2). Each metric and dimension appearing in a\nproﬁle is intrinsically added to the general quality schema.\nIn other words, the set Mof metrics may be deﬁned by\nunion of all metrics appearing in the proﬁles, which are\nthe relevant metrics identiﬁed by the users of the DSL.\n3.3 Measurements\nA speciﬁc module of the DSL is in charge of comput-\ning the metrics measurements. As summarized by Fig. 3,\nthis requires to synchronize each score swith the vector\nf1(s), f2(s),···, fk(s),···representing its quality mea-\nsurements with respect to the schema.\nA\nScoresCollection\n…\nConsistencym1mimi+1DimensionMetrics…Completenessmk…Accuracy………\nConsistencyM1MpMp+1DimensionMetrics…CompletenessMq…………\nScore-level taxonomyCollection-level taxonomy\nv1vj…vj+1vk……\nv1vj…vj+1vk……\nv1vj…vj+1vk………compute (f i, …. fk)compute (f i, …. fk)compute (f i, …. fk)score metricsvalues\nAggregation(F1…Fq…)\nV1Vp…Vp+1Vq……QualityreportAccuracy\nFigure 3 . Metrics computation\nLikewise, the set of measurements for some collec-\ntionCmust be computed from the quality measure-\nments of the scores that belong to C. One obtains\na summary of quality indicators relative to Cthat we\ncall quality report . More formally, the quality report\nof a collection C(resp. of a score s) is a function\nQRC:Mcoll→ ∪ M∈M colldom (M)(resp. QRs:\nMsc→ ∪ m∈M scdom (m)) that assigns a value for C\n(resp. score s) to each metric of Mcoll(resp.Msc).\nTechnically, the main issue is to maintain a quality re-\nport that faithfully reﬂects the content of the collection. If\nthe collection is created once for all and never updated, a\nsingle batch computation is enough. In general, though,\ncollections are extended, scores are modiﬁed, and we have\nto take those changes into account. At the score level,\na trigger seems an appropriate solution: any change of\nthe score results in the execution of the metrics functions\nf1,···, fk. Things are more complicated at the collection\nlevel. First, a change in some score does necessarily im-\npact the collections metrics, or at least all of them. Second,\none has to carefully consider aggregation functions which\ncan be computed incrementally (e.g., count() ) from those\nthat require a brand new computation from their full input\n(e.g., avg() ). Our current implementation adopts a simple\nrecompute-everything strategy, but more sophisticated ap-\nproaches need to be investigated in the future.3.4 Matching goals and measurements\nLet us now consider how we exploit our two main artifacts:\nuser goals on one side, measurements and reports on the\nother side. This actually depends on the user’s role: pub-\nlishers are responsible for inserting and updating scores\nand collections, whereas consumers can only browse and\nread. Both a them can deﬁne a proﬁle, but for different\npurposes in the system.\nConsumer role and information retrieval . A data con-\nsumer may deﬁne a proﬁle specifying the expected qual-\nity level of data that is needed for a given usage. This\nproﬁle may be used as a ﬁlter by retrieving only appro-\npriate data of the DSL, at the collection or score level, or\nfor recommendation functionalities by suggesting collec-\ntion or scores of the DSL that respects the proﬁle.\nAs an example of this ﬁltering facility, one of our DSL\nsupplies a Web front-end interface to browse the collec-\ntions, some of which exhibit a poor rendering on average,\nyet are useful for teaching purposes. To limit their access,\nwe can deﬁne a usage browse with high rendering metric\ngoals, assigned to the anonymous user. Anyone access-\ning to the Web UI without logging in will automatically\nadopt this default usage and will access only to high-level\ngraphic scores. Connected users can be given access to the\nteaching collections via another speciﬁc usage.\nPublisher role and data creation/update. A publisher\nmay deﬁne a proﬁle that speciﬁes the needed quality level\nof data to be achieved before publishing, which may sus-\npend the publishing of the collections or scores that do not\nrespect the proﬁle. This kind of practice goes beyond the\ncontrol of the quality by the publisher as it also makes it\npossible to expose a quality certiﬁcation, for speciﬁc us-\nages (proﬁles) of data available in the platform.\nMore formally, a quality report QRsatisﬁes a proﬁle P\niff each quality metric goal of Pis satisﬁed by the values of\nQR. Given a set (of collections or scores) Sand a proﬁle\nP, the ﬁltering ofSaccording to a proﬁle Pis the set\n{e∈S|QResatisﬁes P}.\n4. APPLICATION: GOALS AND USAGES FOR\nMEI COLLECTION\nWe interviewed librarians in charge of the two DSL\nNEUMA and T HELOST VOICES PROJECT (simply de-\nnoted by L OST VOICES in the following). Following the\nGoal Question Metric approach (driven by data use cases,\nsee [2] for details), we exhibited a set of relevant quality\nquestions and metrics for data quality management.\n4.1 User requirements for N EUMA\nProduction of scores. NEUMA is an open repository of\nscores in MusicXML and MEI. Contributions to N EUMA\ncome either from musicologists for on-line publication\npurposes, with highly ranked quality standards, or from\nlegacy sources (KernScores for instance, converted to\nXML formats). For on-line editions, a clean and consistent\nrendering is required, as well as an homogeneous presenta-\ntion of the scores of a same collection. The level of detailsProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 333of meta-data should not strongly vary from one score to the\nother (in a same collection). Legacy collections are incor-\nporated in N EUMA for teaching or research purposes.\nUsage of scores. All the scores submitted to the library are\nprocessed with a uniform workﬂow, which includes pro-\nduction of Lilypond scripts for rendering purposes, conver-\nsion from/to Music/MEI, extraction of textual and musical\nfeatures for indexing, etc. Applying this workﬂow exac-\nerbates the heterogeneity of the input and reveals a lot of\ndiscrepancies and variations in the tolerance levels of the\nvarious tools that need to access the score representation.\nLilypond for instance hardly accepts incomplete measures,\nwhereas this is tolerated and corrected as much as possible\nby most MusicXML-based editors. A same meta-data ﬁeld\n(e.g., authors, title) can be found in many different places\nin MusicXML (things are better with MEI), resulting in an\nerratic rendering with any tool other than the initial editor.\nQuality concerns. A common concern met by all users is\nthe need to obtain a decent visualization of a score. Here,\n“decent” means that any user accessing a score in the li-\nbrary should be able to display it with a desktop tool with-\nout a strong readability impact. Unfortunately, this turns\nout to be difﬁcult to achieve. A score created with a spe-\nciﬁc engraver E1may contain issues, which are tolerated\nbyE1but result in an awful rendering with E2. In gen-\neral, having a valid MusicXML or MEI document does not\nguarantee that it can correctly be visualized as a score.\nIf we leave apart the readability problems, speciﬁc\nusages dictate the quality demands and at which level\n(score or collections) these demands take place. The Bach\nchorales for instance are supplied by an external source\nwith accurate music representation, but missing lyrics.\nThis obviously keeps them from any use in a performance\nperspective, but preserves their interest for music analysis\npurpose.\n4.2 User requirements for L OST VOICES\nProduction of scores. LOST VOICES is a library of\nEarly European (mostly Renaissance) music scores. It is\nmaintained by a research institution with two main goals:\n(i) publish (on regular score sheets and on-line) rare col-\nlections of Renaissance works, (ii) design and promote\nadvanced editorial practices regarding scholar editions of\nearly music sources, often incomplete or fragmented. All\nthe produced scores are encoded in MEI and must comply\nto very detailed editorial rules. We cannot of course list\nthem all: they cover usage of ancient and modern clefs,\npresence of incipits, bars and alterations, signs used for\nmensural notation, text/music association, etc.\nUsage of scores. Scores intended for on-line edition\nmust be submitted to an additional manual process to be\ncompatible with MEI-based rendering tools (VexFlow3or\nVerovio4). This includes in particular a speciﬁc encoding\nof variants and missing fragments.\nQuality concerns. Most of the editorial rules cannot be au-\ntomatically checked, and this gives rise to two major issues\n3http://www.vexﬂow.com\n4http://www.verovio.orgSyntactic accuracy at the score level\nQuestion – Are measures ﬁlled and complete?\nMetric – Proportion of syntactically accurate measures over\nthe total number of measures in the score; 1 for non-measured\nmusic.\nQuestion – Do parts have the same length?\nMetric – Proportion of non outliers length parts over (all) the\nparts of the score.\nQuestion – Is the voice nomenclature correct?\nMetric – Boolean (yes/no).\nTable 1 .Syntactic accuracy questions and metrics\nrelated to quality management. First, all scores have to be\ndouble-checked (i.e., by the person that initially encodes\nthe scores and by a supervisor), a very time-consuming\nprocess. Second, the library cannot as such be opened to\nexternal contributions, due to the complexity of rules and\nof the lack of automatic control that would reject inputs\nfalling to match the required encoding requirements.\n4.3 Quality Metrics\nBased on the previous studies, we deﬁned an initial taxon-\nomy of two DSL quality schemas for, resp., N EUMA and\nLOST VOICES . It is worth noting that the two schemas are\nsigniﬁcantly distinct, which supports our design choice of\na DSL-level modeling of quality requirements.\nDue to space restrictions, we illustrate the schemas with\na tiny sample of the collected metrics requirements, fo-\ncusing on consensual quality dimensions of literature: the\nconsistency , the accuracy and the completeness . Other di-\nmensions could be considered if needed. For instance, con-\nsidering a provenance dimension of data (e.g. author, cur-\nrency, timeliness, volatility) could be relevant.\n4.3.1 Score Level\nAccuracy is deﬁned as the closeness between data value\nand their considered correct representation. Classically,\ntwo kinds of accuracy are considered: the syntactic ac-\ncuracy and the semantic one. Syntactic accuracy in turn\ntakes two forms. One might ﬁrst check if the data respect\nan adequate format (validity). External constraints may\nalso be introduced as goals representing speciﬁc editorial\nrules. For instance L OST VOICES requires a speciﬁc voice\nnomenclature (Superius; Cantus; Altus; Contratenor). In\nall cases, all the metrics in this dimension can automati-\ncally be computed from the score encoding. Table 1 con-\ntains a few examples.\nSemantic accuracy measures the closeness of a value to\na considered true real-world value. Its measurement sup-\nposes that there is somewhere a reference for the score con-\ntent, and cannot thus be evaluated by merely looking at an\nindividual document. For the time being, our schemas do\nnot include semantic accuracy metrics. We defer alterna-\ntive approaches to future work (see Section 5).\nCompleteness measures in what extent the score contains\nall the required information. Table 2 contains some exam-\nples. It is worth recalling that deﬁning a metric, and mea-\nsuring its value, does notconstitute an absolute indicator334 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Completeness at the score level\nQuestion – Is there a ﬁgured bass?\nMetric – Boolean.\nQuestion – Are lyrics present (for vocal music only)?\nMetric – Boolean.\nQuestion – Are meta-data ﬁelds present?\nMetric – Proportion of available and syntactically required\nmeta-data ﬁelds.\nTable 2 .Completeness questions and metrics\nConsistency at the score level\nQuestion – Are rendering options consistently used in the\nscore encoding?\nMetric – Proportion of rendering options detected among a set\nof given ones (e.g., note heads, beaming, positioning, spacing,\nclef changes).\nQuestion – Are performance indications uniformly present?\nMetric – Uniform encoding of slurs, articulation symbols, etc.\nTable 3 .Consistency questions and metrics\nof the DSL quality. Measuring the presence of a ﬁgured\nbass for instance is only important in some usages, and for\nspeciﬁc corpora, and its absence does not mean that the\ncorpora are not ﬁt for other usages.\nConsistency , at the score level, mostly denotes a uniform\nencoding of notational features. Positioning information\nfor score elements (notes, chords), ﬁngering, uniform and\nconstant representation of the ﬁgured bass are relevant ex-\namples for our use cases (see Table 3).\n4.3.2 Collection Level\nMost of the collection-level metrics are obtained by an ag-\ngregation process, which summarizes one or several score-\nlevel measurements spread over the collection. In the sim-\nplest form, each metric at the score-level allows to deﬁne a\ncorresponding metric at the collection-level, which is com-\nputed as an average or standard deviation of the score-level\nmetric. Another part of the collection-level metrics are\nsimply not inferred from the collection level. We give a\nfew examples of representative situations.\nAccuracy measurements are typically obtained by simple\nstatistical calculation. The (syntactic) accuracy metrics re-\nlated to measures for instance compute the ratio of scores\nthat contains incomplete measures. Another, less directly\ncomputable aspect, is related to the collection structure and\npresentation. N EUMA for instance requires a ﬁxed order-\ning of the scores in a collection (Table 4). Note that the\nlater metrics (as many of the same kinds) requires an ex-\nternal information which might be, if available, a public\nreference of the collection content.\nCompleteness of a collection (Table 5) measures to what\nextent the collection is complete enough w.r.t. an expected\npopulation size. Here, again, this either requires an exter-\nnal information of reference, or an evaluation by an expert\nor a group of experts.\nConsistency measures in what extend scores of a collection\nrespect a uniform representation (in Table 6).Collection accuracy\nQuestion – Are measures correctly encoded?\nMetric – Ratio of correct measures.\nQuestion – Are scores ordered as required?\nMetric – Deviation from the required order.\nTable 4 .Collection accuracy questions and metrics\nCollection completeness\nQuestion – Is the collection population complete enough?\nMetric – Proportion of available scores over the expected ref-\nerence population.\nTable 5 .Collection completeness questions and metrics\nCollection consistency\nQuestion – Are (meta-)data supplied uniformly supplied?\nMetrics – Standard deviations of the collection population for\nmetrics of Tables 1, 2 and 3.\nTable 6 .Collection consistency questions and metrics\n5. CONCLUSION AND PERSPECTIVES\nIn this paper, we proposed a methodology for assessing\ndata quality in a DSL, based on user preferences. Our ap-\nproach deﬁnes a generic data model that supports the spec-\niﬁcation of quality schemas, lets users deﬁne their goals\nwith respect to the schema of their DSL, and matches us-\nages against quality evaluation. We used this approach for\ntwo real digital libraries, and formalized with our model\nthe users’ requirements. The implementation is currently\nin progress for all the metrics that can be evaluated without\nany external information. This covers syntactic accuracy at\nthe score level, and collection-level aggregated metrics.\nOur proposal is a ﬁrst step that must be completed in\nseveral directions. First, several of the metrics identiﬁed\nduring our preliminary study cannot be evaluated from the\nnotation itself, but require an external reference. A ﬁrst\nsolution is a collaborative evaluation (some methodolo-\ngies were proposed e.g. in [6, 1]), for instance based on\ncrowdsourcing. This approach is particularly relevant for\nthe quality dimensions that require external skills like, e.g.,\nsemantic accuracy mentioned in 4.3. Another one is to ex-\nploit open semantic web data by interlinking the DSL col-\nlections with other data sources [16].\nA second important perspective is to address another\naspect of quality management, namely quality improve-\nment techniques [4]. Such an improvement can be fully\nautomatic in some speciﬁc cases (e.g., ﬁlling incomplete\nmeasures with rests) but in general, the goal is to help\nusers to identify the insert/update process deﬁciencies, and\nto suggest effective improvement strategies.\nTo our knowledge, no previous work in the literature\nhas proposed metrics for the quality evaluation of music\nnotation. We believe that the topic is important given the\nlack of constraints of current formats, and the growing\nproduction of XML encoded scores.\nAcknowledgement: This work has been funded by the\nFrench CNRS under the Mastodons project GioQoso.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 3356. REFERENCES\n[1] Maribel Acosta, Amrapali Zaveri, Elena Simperl, Dim-\nitris Kontokostas, S ¨oren Auer, and Jens Lehmann.\nCrowdsourcing linked data quality assessment. In Pro-\nceedings of the International Semantic Web Confer-\nence (ISWC) , pages 260–276, 2013.\n[2] Victor R. Basili, Gianluigi Caldiera, and H. Di-\neter Rombach. Encyclopedia of Software Engineering ,\nchapter The Goal Question Metric Approach. Wiley,\n1994.\n[3] Carlo Batini, Cinzia Cappiello, Chiara Francalanci,\nand Andrea Maurino. Methodologies for data qual-\nity assessment and improvement. ACM Comput. Surv. ,\n41(3):16:1–16:52, July 2009.\n[4] Carlo Batini and Monica Scannapieco. Data Qual-\nity: Concepts, Methodologies and Techniques . Data-\nCentric Systems and Applications. Springer, 2006.\n[5] Martin J. Eppler and Markus Helfert. A framework for\nthe classiﬁcation of data quality costs and an analy-\nsis of their progression. In Proceedings of the Interna-\ntional Conference on Information Quality , pages 311–\n325, 2004.\n[6] Yolanda Gil and Varun Ratnakar. Trusting information\nsources one citizen at a time. In Proceedings of the In-\nternational Semantic Web Conference (ISWC) , pages\n162–176, 2002.\n[7] Michael Good. MusicXML for Notation and Analysis ,\npages 113–124. W. B. Hewlett and E. Selfridge-Field,\nMIT Press, 2001.\n[8] Andrew Hankinson, Perry Roland, and Ichiro Fuji-\nnaga. The Music Encoding Initiative as a Document-\nEncoding Framework. In Proceedings of the Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR) , pages 293–298, 2011.\n[9] Music Encoding Initiative. http://\nmusic-encoding.org , 2015. Accessed Oct.\n2015.\n[10] MuseScore. Web site. https://musescore.org/.\n[11] Thomas C. Redman. Data Quality for the Information\nAge. Artech House Inc., 1996.\n[12] Jenn Riley and Constance A. Mayer. Ask a Librarian:\nThe Role of Librarians in the Music Information Re-\ntrieval . In Proceedings of the International Conference\non Music Information Retrieval (ISMIR) , 2006.\n[13] Perry Rolland. The Music Encoding Initiative (MEI).\nInProc. Intl. Conf. on Musical Applications Using\nXML , pages 55–59, 2002.\n[14] Eleanor Selfridge-Field, editor. Beyond MIDI : The\nHandbook of Musical Codes . Cambridge: The MIT\nPress, 1997.[15] W3C Music Notation Community Group.\nhttps://www.w3.org/community/music-notation/,\n2015. Last accessed Jan. 2016.\n[16] Amrapali Zaveri, Anisa Rula, Andrea Maurino, Ri-\ncardo Pietrobon, Jens Lehmann, and S ¨oren Auer. Qual-\nity assessment for linked data: A survey. Semantic\nWeb, 7(1):63–93, 2016.336 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Joint Beat and Downbeat Tracking with Recurrent Neural Networks.",
        "author": [
            "Sebastian Böck",
            "Florian Krebs",
            "Gerhard Widmer"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1415836",
        "url": "https://doi.org/10.5281/zenodo.1415836",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/186_Paper.pdf",
        "abstract": "In this paper we present a novel method for jointly extract- ing beats and downbeats from audio signals. A recurrent neural network operating directly on magnitude spectro- grams is used to model the metrical structure of the audio signals at multiple levels and provides an output feature that clearly distinguishes between beats and downbeats. A dynamic Bayesian network is then used to model bars of variable length and align the predicted beat and down- beat positions to the global best solution. We find that the proposed model achieves state-of-the-art performance on a wide range of different musical genres and styles.",
        "zenodo_id": 1415836,
        "dblp_key": "conf/ismir/BockKW16",
        "content": "JOINT BEAT AND DOWNBEAT TRACKING WITH RECURRENT\nNEURAL NETWORKS\nSebastian B ¨ock, Florian Krebs, and Gerhard Widmer\nDepartment of Computational Perception\nJohannes Kepler University Linz, Austria\nsebastian.boeck@jku.at\nABSTRACT\nIn this paper we present a novel method for jointly extract-\ning beats and downbeats from audio signals. A recurrent\nneural network operating directly on magnitude spectro-\ngrams is used to model the metrical structure of the audio\nsignals at multiple levels and provides an output feature\nthat clearly distinguishes between beats and downbeats.\nA dynamic Bayesian network is then used to model bars\nof variable length and align the predicted beat and down-\nbeat positions to the global best solution. We ﬁnd that the\nproposed model achieves state-of-the-art performance on a\nwide range of different musical genres and styles.\n1. INTRODUCTION\nMusic is generally organised in a hierarchical way. The\nlower levels of this hierarchy are deﬁned by the beats and\ndownbeats which deﬁne the metrical structure of a musi-\ncal piece. While considerable amount of research focused\non ﬁnding the beats in music, far less effort has been made\nto track the downbeats , although this information is cru-\ncial for a lot of higher level tasks such as structural seg-\nmentation and music analysis and applications like auto-\nmated DJ mixing. In western music, the downbeats often\ncoincide with chord changes or harmonic cues, whereas in\nnon-western music the start of a measure is often deﬁned\nby the boundaries of rhythmic patterns. Therefore, many\nalgorithms exploit one or both of these features to track the\ndownbeats .\nKlapuri et al. [18] proposed a system which jointly anal-\nyses a musical piece at three time scales: the tatum, tactus,\nand measure level. The signal is split into multiple bands\nand then combined into four accent bands before being fed\ninto a bank of resonating comb ﬁlters. Their temporal evo-\nlution and the relation of the different time scales are mod-\nelled with a probabilistic framework to report the ﬁnal po-\nsition of the downbeats.\nThe system of Davies and Plumbley [5] ﬁrst tracks the\nbeats and then calculates the Kullback-Leibler divergence\nc/circlecopyrtSebastian B ¨ock, Florian Krebs, and Gerhard Widmer.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Sebastian B ¨ock, Florian Krebs, and\nGerhard Widmer. “JOINT BEAT AND DOWNBEAT TRACKING\nWITH RECURRENT NEURAL NETWORKS”, 17th International So-\nciety for Music Information Retrieval Conference, 2016.between two consecutive band-limited beat synchronous\nspectral difference frames to detect the downbeats, exploit-\ning the fact that lower frequency bands are perceptually\nmore important.\nPapadopoulos and Peeters [24] jointly track chords and\ndownbeats by decoding a sequence of (pre-computed) beat\nsynchronous chroma vectors with a hidden Markov model\n(HMM). Two time signatures are modelled. In a later pa-\nper, the same authors [25] jointly model beat phase and\ndownbeats while the tempo is assumed to be given. Beat\nand downbeat times are decoded using a HMM from three\ninput features: the correlation of the local energy with a\nbeat-template, chroma vector variation, and the spectral\nbalance between high and low frequency content.\nThe system proposed by Khadkevich et al. [17] uses im-\npulsive and harmonic components of a reassigned spectro-\ngram together with chroma variations as observation fea-\ntures for a HMM. The system is based on the assump-\ntion that downbeats mostly occur at location with harmonic\nchanges.\nHockman et al. [14] present a method designed specif-\nically for hardcore, jungle, and drum and bass music,\nthat often employ breakbeats. The system exploits on-\nset features and periodicity information from a beat track-\ning stage, as well as information from a regression model\ntrained on the breakbeats speciﬁc to the musical genre.\nDurand et al. [10] ﬁrst estimates the time signature by\nexamining the similarity of the frames at the beat level –\nwith the beat positions given as input. The downbeats are\nthen selected by a linear support vector machine (SVM)\nmodel using a bag of complementary features, compris-\ning chord changes, harmonic balance, melodic accents and\npattern changes. In consecutive works [8, 9] they lifted the\nrequirement of the beat positions to be given and enhanced\ntheir system considerably by replacing the SVM feature se-\nlection stage by several deep neural networks which learn\nhigher level representations from which the ﬁnal downbeat\npositions are selected by means of Viterbi decoding.\nKrebs et al. [20] jointly model bar position, tempo,\nand rhythmic patterns with a dynamic Bayesian network\n(DBN) and apply their system to a dataset of ballroom\ndance music. Based on their work, [16] developed a uni-\nﬁed model for metrical analysis of Turkish, Carnatic, and\nCretan music. Both models were later reﬁned by using a\nmore sophisticated state space [21].255The same state space has also been successfully applied\nto the beat tracking system proposed by B ¨ock et al. [2].\nThe system uses a recurrent neural network (RNN) similar\nto the one proposed in [3] to discriminate between beats an\nnon-beats at a frame level. A DBN then models the tempo\nand the phase of the beat sequence.\nIn this paper, we extend the RNN-based beat tracking\nsystem in order to jointly track the whole metrical cy-\ncle, including beats and downbeats. The proposed model\navoids hand-crafted features such as harmonic change de-\ntection [8–10,17,24], or rhythmic patterns [14,16,20], but\nrather learns the relevant features directly from the spectro-\ngram. We believe that this is an important step towards sys-\ntems without cultural bias, as postulated by the “Roadmap\nfor Music Information Research” [26].\n2. ALGORITHM DESCRIPTION\nThe proposed method consists of a recurrent neural net-\nwork (RNN) similar to the ones proposed in [2, 3], and is\ntrained to jointly detect the beats and downbeats of an au-\ndio signal in a supervised classiﬁcation task. A dynamic\nBayesian network is used as a post-processing step to de-\ntermine the globally best sequence through the state-space\nby jointly inferring the meter, tempo, and phase of the\n(down-)beat sequence.\n2.1 Signal Pre-Processing\nThe audio signal is split into overlapping frames and\nweighted with a Hann window of same length before be-\ning transferred to a time-frequency representation with\nthe Short-time Fourier Transform (STFT). Two adjacent\nframes are located 10 ms apart, which corresponds to a rate\nof 100 fps (frames per second). We omit the phase por-\ntion of the complex spectrogram and use only the magni-\ntudes for further processing. To enable the network to cap-\nture features which are precise both in time and frequency,\nwe use three different magnitude spectrograms with STFT\nlengths of 1024, 2048, and 4096 samples (at a signal sam-\nple rate of 44.1 kHz). To reduce the dimensionality of the\nfeatures, we limit the frequencies range to [30, 17000] Hz\nand process the spectrograms with logarithmically spaced\nﬁlters. A ﬁlter with 12 bands per octave corresponds to\nsemitone resolution, which is desirable if the harmonic\ncontent of the spectrogram should be captured. However,\nusing the same number of bands per octave for all spectro-\ngrams would result in an input feature of undesirable size.\nWe therefor use ﬁlters with 3, 6, and 12 bands per octave\nfor the three spectrograms obtained with 1024, 2028, and\n4096 samples, respectively, accounting for a total of 157\nbands. To better match human perception of loudness, we\nscale the resulting frequency bands logarithmically. To aid\nthe network during training, we add the ﬁrst order differ-\nences of the spectrograms to our input features. Hence, the\nﬁnal input dimension of the neural network is 314. Figure\n1a shows the part of the input features obtained with 12\nbands per octave.2.2 Neural Network Processing\nAs a network we chose a system similar to the one pre-\nsented in [3], which is also the basis for the current state-\nof-the-art in beat tracking [2, 19].\n2.2.1 Network topology\nThe network consists of three fully connected bidirec-\ntional recurrent layers with 25 Long Short-Term Memory\n(LSTM) units each. Figures 1b to 1d show the output acti-\nvations of the forward (i.e. half of the bidirectional) hidden\nlayers. A softmax classiﬁcation layer with three units is\nused to model the beat,downbeat , and non-beat classes.\nA frame can only be classiﬁed as downbeat orbeat but\nnot both at the same time, enabling the following dynamic\nBayesian network to infer the meter and downbeat posi-\ntions more easily. The output of the neural network are\nthree activation functions bk,dk, andnok, which repre-\nsents the probability of a frame kbeing a beat but no down-\nbeat, downbeat or non-beat position. Figure 1e shows bk\nanddkfor an audio example.\n2.2.2 Network training\nWe train the network on the datasets described in Sec-\ntion 3.1 — except the ones marked with an asterisk (*)\nwhich are used for testing only — with 8-fold cross val-\nidation based on a random splits. We initialise the net-\nwork weights and biases with a uniform random distribu-\ntion with range [-0.1, 0.1] and train it with stochastic gradi-\nent decent minimising the cross entropy error with a learn-\ning rate of 10-5and 0.9 momentum. We stop training if no\nimprovement on the validation set can be observed for 20\nepochs. We then reduce the learning rate by a factor of ten\nand retrain the previously best model with the same early\nstopping criterion.\n2.2.3 Network output thresholding\nWe experienced that the very low activations at the begin-\nning and end of a musical excerpt can hurt the tracking\nperformance of the system. This is often the case if a song\nstarts with a (musically irrelevant) intro or has a long fade\nout at the end. We thus threshold the activations and use\nonly the activations between the ﬁrst and last time they ex-\nceed the threshold. We empirically found a threshold value\nθ= 0.05 to perform well without harming pieces with over-\nall low activations (e.g. choral works).\n2.3 Dynamic Bayesian Network\nWe use the output of the neural network as observations\nof adynamic Bayesian network (DBN) which jointly infers\nthe meter, tempo, and phase of a (down-)beat sequence.\nThe DBN is very good at dealing with ambiguous RNN\nobservations and ﬁnds the global best state sequence given\nthese observations.1We use the state-space proposed in\n[21] to model a whole bar with an arbitrary number of\n1The average performance gain of the DBN compared to simple\nthresholding and peak-picking of the RNN activations is about 15% F-\nmeasure on the validation set.256 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20160 1 2 3 4 5 601020304050607080(a) Spectrogram of audio signal\n0 1 2 3 4 5 605101520\n(b) Activations of the ﬁrst hidden layer\n0 1 2 3 4 5 605101520\n(c) Activations of the second hidden layer\n0 1 2 3 4 5 605101520\n(d) Activations of the third hidden layer\n0 1 2 3 4 5 60.00.20.40.60.81.0\n(e) Activations of the softmax output layer\nFigure 1 :Signal propagation of a 6 second song excerpt in 4/4\ntime signature through the network: (a)part of the input features,\n(b)the ﬁrst hidden layer shows activations at onset positions, (c)\nthe second models mostly faster metrical levels (e.g. 1/8th notes\nat neuron 3), (d)the third layer models multiple metrical levels\n(e.g. neuron 8 ﬁring at beat positions and neuron 16 around down-\nbeat positions), (e)the softmax output layer ﬁnally models the re-\nlation of the different metrical levels resulting in clear downbeat\n(black) and beat (green, ﬂipped for better visualisation) activa-\ntions. Downbeat positions are marked with vertical dashed lines,\nbeats as dotted lines.beats per bar. We do not allow meter changes through-\nout a musical piece, thus we can model different meters\nwith individual, independent state spaces. All parameters\nof the DBN are tuned to maximise the downbeat tracking\nperformance on the validation set.\n2.3.1 State Space\nWe divide the state space into discrete states sto make\ninference feasible. These states s(φ,˙φ,r)lie in a three-\ndimensional space indexed by the bar position state φ∈\n{1..Φ}, the tempo state ˙φ∈{1..˙Φ}, and the time signature\nstater(e.g.r∈{3/4,4/4}). States that fall on a downbeat\nposition (φ= 1) constitute the set of downbeat statesD,\nall states that fall on a beat position deﬁne the set of beat\nstatesB. The number of bar-position states of a tempo ˙φis\nproportional to its corresponding beat period 1/˙φ, and the\nnumber of tempo states depends on the tempo ranges that\nthe model accounts for. For generality, we assume equal\ntempo ranges for all time signatures in this paper but this\ncould easily changed to adapt the model towards speciﬁc\nstyles. In line with [21] we ﬁnd that by distributing the\ntempo states logarithmically across the beat intervals, the\nsize of the state space can be reduced efﬁciently without\naffecting the performance too much. Empirically we found\nthat usingN= 60 tempo states is a good compromise\nbetween computation time and performance.\n2.3.2 Transition Model\nTempo transitions are only allowed at the beats and follow\nthe same exponential distribution proposed in [21]. We\ninvestigated “peephole” transitions from the end of every\nbeat back to the beginning of the bar, but found them to\nharm performance. Thus, we assume that there are no tran-\nsitions between time signatures in this paper.\n2.3.3 Observation Model\nWe adapted the observation model of the DBN from [2] to\nnot only predict beats, but also downbeats. Since the ac-\ntivation functions ( d,b) produced by the neural network\nare limited to the range [0,1]and show high values at\nbeat/downbeat positions and low values at non-beat posi-\ntions (cf. Figure 1e), the activations can be converted into\nstate-conditional observation distributions P(ok|sk)by\nP(ok|sk) =\n\ndksk∈D\nbksk∈B\nnk\nλo−1, otherwise(1)\nwhereDandBare the sets of downbeat and beat\nstates respectively, and the observation lambda λo∈\n[Φ\nΦ−1,Φ]is a parameter that controls the propor-\ntion of the beat/downbeat interval which is consid-\nered as beat/downbeat and non-beat locations inside one\nbeat/downbeat period. On our validation set we achieved\nthe best results with the value λo= 16 . We found it to be\nadvantageous to use both bkanddkas provided by the neu-\nral network instead of splitting the probability of bkamong\ntheNbeat positions of the transition model.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 2572.3.4 Initial State Distribution\nThe initial state distribution can be used to incorporate any\nprior knowledge about the hidden states, such as meter and\ntempo distributions. In this paper, we use a uniform distri-\nbution over all states.\n2.3.5 Inference\nWe are interested in the sequence of hidden states s1:K,\nthat maximise the posterior probability of the hidden states\ngiven the observations (activations of the network). We\nobtain the maximum a-posteriori state sequence s∗\n1:Kby\ns∗\n1:K= arg max\ns1:Kp(s1:K|o1:K) (2)\nwhich can be computed efﬁciently using the well-known\nViterbi algorithm.\n2.3.6 Beat and Downbeat Selection\nThe sequence of beat Band downbeat times Dare deter-\nmined by the set of time frames kwhich were assigned to\na beat or downbeat state:\nB={k:s∗\nk∈B} (3)\nD={k:s∗\nk∈D} (4)\nAfter having decided on the sequences of beat and down-\nbeat times we further reﬁne them by looking for the\nhighest beat/downbeat activation value inside a window\nof size Φ/λo, i.e. the beat/downbeat range of the whole\nbeat/downbeat period of the observation model (Sec-\ntion 2.3.3).\n3. EV ALUATION\nIn line with almost all other publications on the topic of\ndownbeat tracking, we report the F-measure ( F1) with a\ntolerance window of ±70 ms.\n3.1 Datasets\nFor training and evaluation we use diverse datasets as\nshown in Table 1. Musical styles range from pop and rock\nmusic, over ballroom dances, modern electronic dance mu-\nsic, to classical and non-western music.\nWe do not report scores for all sets used for train-\ning, since comparisons with other works are often\nnot possible due to different evaluation metrics and/or\ndatasets. Results for all datasets, including additional\nmetrics can be found online at the supplementary website\nhttp://www.cp.jku.at/people/Boeck/ISMIR2016.html\nwhich also includes an open source implementation of the\nalgorithm.Downbeat tracking dataset # ﬁles length\nBallroom [12, 20]2685 5 h 57 m\nBeatles [4] 180 8 h 09 m\nHainsworth [13] 222 3 h 19 m\nHJDB [14] 235 3 h 19 m\nRWC Popular [11] 100 6 h 47 m\nRobbie Williams [7] 65 4 h 31 m\nRock [6] 200 12 h 53 m\nCarnatic [28] 176 16 h 38 m\nCretan [16] 42 2 h 20 m\nTurkish [27] 93 1 h 33 m\nGTZAN [23, 29] * 999 8 h 20 m\nKlapuri [18]3* 320 4 h 54 m\nBeat tracking datasets\nSMC [15] * 217 2 h 25 m\nKlapuri [18]3* 474 7 h 22 m\nTable 1 :Overview of the datasets used for training and evalua-\ntion of the algorithm. Sets marked with asterisks (*) are held-out\ndatasets for testing only.\n3.2 Results & Discussion\nTable 2 to 4 list the results obtained by the proposed\nmethod compared to current and previous state-of-the-art\nalgorithms on various datasets. We group the datasets\ninto different tables for clarity, based on whether they are\nused for testing only, cover western, or non-western music.\nSince our system jointly tracks beats and downbeats, we\ncompare with both downbeat and beat tracking algorithms.\nFirst of all, we evaluate on completely unseen data. We\nuse the recently published beat and downbeat annotations\nfor the GTZAN dataset, the Klapuri , and the SMC set (built\nspeciﬁcally to comprise hard-to-track musical pieces) for\nevaluation. Results are given in Table 2. Since these re-\nsults are directly comparable (the only exception being the\nresults of Durand et al. on the Klapuri set4and of B ¨ock\net al. on the SMC set5), we perform statistical signiﬁcance\ntests on them. We use Wilcoxon’s signed-rank test with a\np-value of 0.01.\nAdditionally, we report the performance on other sets\ncommonly used in the literature, comprising both western\nand non-western music. For western music, we give results\non the Ballroom ,Beatles ,Hainsworth , and RWC Popular\nsets in Table 3. For non-western music we use the Car-\nnatic ,Cretan , and Turkish datasets and group the results\nin Table 4. Since these sets were also used during develop-\nment and training of our system, we report results obtained\nwith 8-fold cross validation. Please note that the results\ngiven in Table 3 and 4 are not directly comparable because\nthey were either obtained via cross validation, leave-one-\ndataset-out evaluation, with overlapping train and test sets,\nor tested on unseen data. However, we still consider them\n2We removed the 13 duplicates identiﬁed by Bob Sturm:\nhttp://media.aau.dk/null space pursuits/2014/01/ballroom-dataset.html\n3The beat and downbeat annotations of this set were made indepen-\ndently, thus the positions do not necessarily match each other.\n440 out of the 320 tracks were used for training.\n5The complete set was used for training.258 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Test datasets F1beatF1downbeat\nGTZAN\nnew (bar lengths: 3, 4) *0.856 0.640\nDurand et al. [9] * - 0.624\nB¨ock et al. [2] * 0.864 -\nDavies et al. [5] * 0.806 0.462\nKlapuri et al. [18] * 0.706 0.309\nKlapuri\nnew (bar lengths: 3, 4) *0.811 0.745\nDurand et al. [9] ‡ - 0.689\nB¨ock et al. [2] * 0.798 -\nDavies et al. [5] * 0.698 0.528\nKlapuri et al. [18] ‡ 0.704 0.483\nSMC\nnew (bar lengths: 3, 4) *0.516\nB¨ock et al. [2] § 0.529\nDavies et al. [5] * 0.337\nKlapuri et al. [18] * 0.352\nTable 2 :Beat and downbeat tracking F-measure comparison\nwith state-of-the-art algorithms on the test datasets. ‡denotes\noverlapping train and test sets, §cross validation, and * testing\nonly.\nto be a good indicator for the overall performance and ca-\npabilities of the systems. For the music with non-western\nrhythms and meters (e.g. Carnatic art music contains 5/4\nand 7/4 meters) we compare only with algorithms spe-\ncialised on this type of music, since other systems typically\nfail completely on them.\nWestern music F1beatF1downbeat\nBallroom\nnew (bar lengths: 3, 4) §0.938 0.863\nDurand et al. [9] †/‡ - 0.778 / 0.797\nKrebs et al. [21] § 0.919 -\nB¨ock et al. [2] § 0.910 -\nBeatles\nnew (bar lengths: 3, 4) §0.918 0.832\nDurand et al. [9] †/‡ - 0.815 / 0.842\nB¨ock et al. [2] * 0.880 -\nHainsworth\nnew (bar lengths: 3, 4) §0.867 0.684\nDurand et al. [9] †/‡ - 0.657 / 0.664\nB¨ock et al. [2] § 0.843 -\nPeeters et al. [25] 0.630\nRWC Popular\nnew (bar lengths: 3, 4) §0.943 0.861\nDurand et al. [9] †/‡ - 0.860 / 0.879\nB¨ock et al. [2] * 0.877 -\nPeeters et al. [25] 0.840 0.800\nTable 3 :Beat and downbeat tracking F-measure comparison\nwith state-of-the-art algorithms on western music datasets. †de-\nnotes leave-one-set-out evaluation, ‡overlapping train and test\nsets, §cross validation, and * testing only.Non-western music F1beatF1downbeat\nCarnatic\nnew (bar lengths: 3, 4) § 0.804 0.365\n—– (bar lengths: 3, 5, 7, 8) § 0.792 0.593\nKrebs et al. [21] § 0.805 0.472\nCretan\nnew (bar lengths: 3, 4) § 0.982 0.605\n—– (bar lengths: 2, 3, 4) § 0.981 0.818\n—– (bar lengths: 2) § 0.980 0.909\nKrebs et al. [21] § 0.912 0.774\nTurkish\nnew (bar lengths: 3, 4) § 0.740 0.495\n—– (bar lengths: 4, 8, 9, 10) §0.777 0.631\n—– (tempo: 55..300 bpm) § 0.818 0.683\nKrebs et al. [21] § 0.826 0.639\nTable 4 :Beat and downbeat tracking F-measure comparison\nwith state-of-the-art algorithms on non-western music datasets.\n—– denotes the same system as the line above with altered pa-\nrameters in parentheses, §cross validation.\n3.2.1 Beat tracking\nCompared to the current state-of-the-art [2], the new sys-\ntem performs on par or outperforms this dedicated beat\ntracking algorithm. It only falls a bit behind on the GTZAN\nandSMC sets. However, the results on the latter might be a\nbit biased, since [2] obtained their results with 8-fold cross\nvalidation. Although the new system performs better on\ntheKlapuri set , the difference is not statistically signiﬁ-\ncant. All results compared to those of other beat tracking\nalgorithms on the test datasets in Table 2 are statistically\nsigniﬁcant.\nAlthough the new algorithm and [2] have a very similar\narchitecture and were trained on almost the same develop-\nment sets (the new one plus those sets given in Table 1,\nexcept the SMC dataset), it is hard to conclude whether the\nnew algorithm performs better sometimes because of the\nadditional – more diverse – training material or due to the\njoint modelling of beats and downbeats. Future investiga-\ntions with the same training sets should shed some light on\nthis question, but it is safe to conclude that the joint train-\ning on beats and downbeats does not harm the beat tracking\nperformance at all.\nOn non-western music the results are in the same range\nas the ones obtained by the method of Krebs et al. [21], an\nenhanced version of the algorithm proposed by Holzapfel\net al. [16]. Our system shows almost perfect beat tracking\nresults on the Cretan lap dances while performing a bit\nworse on the Turkish music.\n3.2.2 Downbeat tracking\nFrom Table 2 to 4, it can be seen that the proposed system\nnot only does well for beat tracking, but also shows state-\nof-the-art performance in downbeat tracking. We outper-\nform all other methods on all datasets – except Beatles and\nRWC Popular when comparing to the overﬁtted results ob-\ntained by the system of Durand et al. [9] – even the sys-Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 259tems designed speciﬁcally for non-western music. We ﬁnd\nthis striking, since our new system is not designed specif-\nically for a certain music style or genre. The results of\nour method w.r.t. the other systems on the test datasets in\nTable 2 are all statistically signiﬁcant.\nIt should be noted however, that the dynamic Bayesian\nnetwork must model the needed bar lengths for the respec-\ntive music in order to achieve this performance. Espe-\ncially when dealing with non-western music, this is cru-\ncial. However, we do not consider this a drawback, since\nthe system is able to chose the correct bar length reliably\nby itself.\n3.2.3 Meter selection\nAs mentioned above, for best performance the DBN must\nmodel measures with the correct number of beats per bar.\nPer default, our system works for 3/4 and 4/4 time signa-\ntures, but since the parameters of the DBN are not learnt,\nthis can be changed during runtime in order to model any\ntime signature and tempo range.\nTo investigate the system’s ability to automatically de-\ncide on which bar length to select, we performed an exper-\niment and limited the DBN to model only bars with lengths\nof three or four beats, both time signatures simultaneously\n(the default setting), or bar lengths of up to eight beats.\n3 4 3,4 3...5 3...6 3...7 3...8\nmodelled bar lengths by the DBN [beats/bar]0.40.50.60.70.80.9F-measure\nFigure 2 : Downbeat tracking performance of the new sys-\ntem with different bar lengths on the Ballroom set.\nFigure 2 shows this exemplarily for the Ballroom set,\nwhich comprises four times as many pieces in 4/4 as in\n3/4 time signature. The performance is relatively low if\nthe system is limited to model bars with only three or\nfour beats per bar. When being able to model both time\nsignatures present in the music, the system achieves it’s\nmaximum performance. The performance then slightly de-\ncreases if the DBN models bars with a length up to eight\nbeats per bar, but remains on a relatively high performance\nlevel. This shows the system’s ability to select the correct\nbar length automatically.4. CONCLUSION\nIn this paper we presented a novel method for jointly track-\ning beats and downbeats with a recurrent neural network\n(RNN) in conjunction with a dynamic Bayesian network\n(DBN). The RNN is responsible for modelling the metrical\nstructure of the musical piece at multiple interrelated lev-\nels and classiﬁes each audio frame as being either a beat,\ndownbeat, or no beat. The DBN then post-processes the\nprobability functions of the RNN to align the beats and\ndownbeats to the global best solution by jointly inferring\nthe meter, tempo, and phase of the sequence. The sys-\ntem shows state-of-the-art beat and downbeat tracking per-\nformance on a wide range of different musical genres and\nstyles. It does so by avoiding hand-crafted features such as\nharmonic changes, or rhythmic patterns, but rather learns\nthe relevant features directly from audio. We believe that\nthis is an important step towards systems without any cul-\ntural bias. We provide a reference implementation of the\nalgorithm as part of the open-source madmom [1] frame-\nwork.\nFuture work should address the limitation of the system\nof not being able to perform time signature changes within\na musical piece. Due to the large state space needed this\nis intractable right now, but particle ﬁlters as used in [22]\nshould be able to resolve this issue.\n5. ACKNOWLEDGMENTS\nThis work is supported by the European Union Sev-\nenth Framework Programme FP7 / 2007-2013 through the\nGiantSteps project (grant agreement no. 610591) and the\nAustrian Science Fund (FWF) project Z159. We would\nlike to thank all authors who shared their source code,\ndatasets, annotations and detections or made them publicly\navailable.\n6. REFERENCES\n[1] S. B ¨ock, F. Korzeniowski, J. Schl ¨uter, F. Krebs, and\nG. Widmer. madmom: a new Python Audio and Mu-\nsic Signal Processing Library. arXiv:1605.07008 ,\n2016.\n[2] S. B ¨ock, F. Krebs, and G. Widmer. A multi-model ap-\nproach to beat tracking considering heterogeneous mu-\nsic styles. In Proc. of the 15th Int. Society for Music\nInformation Retrieval Conference (ISMIR) , 2014.\n[3] S. B ¨ock and M. Schedl. Enhanced Beat Tracking with\nContext-Aware Neural Networks. In Proc. of the 14th\nInt. Conference on Digital Audio Effects (DAFx) , 2011.\n[4] M. E. P. Davies, N. Degara, and M. D. Plumbley.\nEvaluation methods for musical audio beat tracking al-\ngorithms. Technical Report C4DM-TR-09-06, Centre\nfor Digital Music, Queen Mary University of London,\n2009.260 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016[5] M. E. P. Davies and M. D. Plumbley. A spectral differ-\nence approach to downbeat extraction in musical au-\ndio. In Proc. of the 14th European Signal Processing\nConference (EUSIPCO) , 2006.\n[6] T. de Clercq and D. Temperley. A corpus analysis of\nrock harmony. Popular Music , 30(1):47–70, 2011.\n[7] B. Di Giorgi, M. Zanoni, A. Sarti, and S. Tubaro. Au-\ntomatic chord recognition based on the probabilistic\nmodeling of diatonic modal harmony. In 8th Int. Work-\nshop on Multidimensional Systems (nDS) , pages 145–\n150, 2013.\n[8] S. Durand, J. P. Bello, B. David, and G. Richard.\nDownbeat tracking with multiple features and deep\nneural networks. In Proc. of the IEEE Int. Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\n2015.\n[9] S. Durand, J. P. Bello, B. David, and G. Richard.\nFeature Adapted Convolutional Neural Networks for\nDownbeat Tracking. In Proc. of the IEEE Int. Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) , 2016.\n[10] S. Durand, B. David, and G. Richard. Enhancing down-\nbeat detection when facing different music styles. In\nProc. of the IEEE Int. Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , 2014.\n[11] M. Goto, M. Hashiguchi, T. Nishimura, and R. Oka.\nRWC Music Database: Popular, Classical, and Jazz\nMusic Databases. In Proc. of the 3rd Int. Conference\non Music Information Retrieval (ISMIR) , 2002.\n[12] F. Gouyon, A. P. Klapuri, S. Dixon, M. Alonso,\nG. Tzanetakis, C. Uhle, and P. Cano. An experimen-\ntal comparison of audio tempo induction algorithms.\nIEEE Transactions on Audio, Speech, and Language\nProcessing , 14(5), 2006.\n[13] S. Hainsworth and M. Macleod. Particle ﬁltering ap-\nplied to musical tempo tracking. EURASIP Journal on\nApplied Signal Processing , 15, 2004.\n[14] J. Hockman, M. E. P. Davies, and I. Fujinaga. One in\nthe jungle: Downbeat detection in hardcore, jungle,\nand drum and bass. In Proc. of the 13th Int. Society\nfor Music Information Retrieval Conference (ISMIR) ,\n2012.\n[15] A. Holzapfel, M. E. P. Davies, J. R. Zapata, J. L.\nOliveira, and F. Gouyon. Selective sampling for beat\ntracking evaluation. IEEE Transactions on Audio,\nSpeech, and Language Processing , 20(9), 2012.\n[16] A. Holzapfel, F. Krebs, and A. Srinivasamurthy. Track-\ning the “odd”: meter inference in a culturally diverse\nmusic corpus. In Proc. of the 15th Int. Society for Mu-\nsic Information Retrieval Conference (ISMIR) , 2014.[17] M. Khadkevich, T. Fillon, G. Richard, and M. Omol-\nogo. A probabilistic approach to simultaneous extrac-\ntion of beats and downbeats. In Proc. of the IEEE Int.\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , 2012.\n[18] A. P. Klapuri, A. J. Eronen, and J. T. Astola. Analysis\nof the meter of acoustic musical signals. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n14(1), 2006.\n[19] F. Korzeniowski, S. B ¨ock, and G. Widmer. Probabilis-\ntic extraction of beat positions from a beat activation\nfunction. In Proc. of the 15th Int. Society for Music In-\nformation Retrieval Conference (ISMIR) , 2014.\n[20] F. Krebs, S. B ¨ock, and G. Widmer. Rhythmic pattern\nmodeling for beat and downbeat tracking in musical\naudio. In Proc. of the 14th Int. Society for Music Infor-\nmation Retrieval Conference (ISMIR) , 2013.\n[21] F. Krebs, S. B ¨ock, and G. Widmer. An Efﬁcient State\nSpace Model for Joint Tempo and Meter Tracking. In\nProc. of the 16th Int. Society for Music Information Re-\ntrieval Conference (ISMIR) , 2015.\n[22] F. Krebs, A. Holzapfel, A. T. Cemgil, and G. Widmer.\nInferring metrical structure in music using particle ﬁl-\nters. IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing , 23(5):817–827, 2015.\n[23] U. Marchand and G. Peeters. Swing ratio estimation.\nInProc. of the 18th Int. Conference on Digital Audio\nEffects (DAFx) , 2015.\n[24] H. Papadopoulos and G. Peeters. Joint estimation of\nchords and downbeats from an audio signal. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 19(1), 2011.\n[25] G. Peeters and H. Papadopoulos. Simultaneous beat\nand downbeat-tracking using a probabilistic frame-\nwork: Theory and large-scale evaluation. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n19(6), 2011.\n[26] X. Serra et al. Roadmap for Music Information Re-\nSearch . Creative Commons BY-NC-ND 3.0 license,\nISBN: 978-2-9540351-1-6, 2013.\n[27] A. Srinivasamurthy, A. Holzapfel, and X. Serra. In\nsearch of automatic rhythm analysis methods for turk-\nish and indian art music. Journal of New Music Re-\nsearch , 43(1), 2014.\n[28] A. Srinivasamurthy and X. Serra. A supervised ap-\nproach to hierarchical metrical cycle tracking from au-\ndio music recordings. In Proc. of the IEEE Int. Con-\nference on Acoustics, Speech, and Signal Processing\n(ICASSP) , 2014.\n[29] G. Tzanetakis and P. Cook. Musical genre classiﬁca-\ntion of audio signals. IEEE Transactions on Speech and\nAudio Processing , 10(5), 2002.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 261"
    },
    {
        "title": "Cross-Collection Evaluation for Music Classification Tasks.",
        "author": [
            "Dmitry Bogdanov",
            "Alastair Porter",
            "Perfecto Herrera",
            "Xavier Serra"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1418131",
        "url": "https://doi.org/10.5281/zenodo.1418131",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/220_Paper.pdf",
        "abstract": "Many studies in music classification are concerned with obtaining the highest possible cross-validation result. How- ever, some studies have noted that cross-validation may be prone to biases and that additional evaluations based on independent out-of-sample data are desirable. In this paper we present a methodology and software tools for cross-collection evaluation for music classification tasks. The tools allow users to conduct large-scale evaluations of classifier models trained within the AcousticBrainz plat- form, given an independent source of ground-truth anno- tations, and its mapping with the classes used for model training. To demonstrate the application of this methodol- ogy we evaluate five models trained on genre datasets com- monly used by researchers for genre classification, and use collaborative tags from Last.fm as an independent source of ground truth. We study a number of evaluation strate- gies using our tools on validation sets from 240,000 to 1,740,000 music recordings and discuss the results.",
        "zenodo_id": 1418131,
        "dblp_key": "conf/ismir/BogdanovPHS16",
        "content": "CROSS-COLLECTION EV ALUATION FOR MUSIC CLASSIFICATION\nTASKS\nDmitry Bogdanov, Alastair Porter, Perfecto Herrera, Xavier Serra\nMusic Technology Group, Universitat Pompeu Fabra\nname.surname@upf.edu\nABSTRACT\nMany studies in music classiﬁcation are concerned with\nobtaining the highest possible cross-validation result. How-\never, some studies have noted that cross-validation may\nbe prone to biases and that additional evaluations based\non independent out-of-sample data are desirable. In this\npaper we present a methodology and software tools for\ncross-collection evaluation for music classiﬁcation tasks.\nThe tools allow users to conduct large-scale evaluations of\nclassiﬁer models trained within the AcousticBrainz plat-\nform, given an independent source of ground-truth anno-\ntations, and its mapping with the classes used for model\ntraining. To demonstrate the application of this methodol-\nogy we evaluate ﬁve models trained on genre datasets com-\nmonly used by researchers for genre classiﬁcation, and use\ncollaborative tags from Last.fm as an independent source\nof ground truth. We study a number of evaluation strate-\ngies using our tools on validation sets from 240,000 to\n1,740,000 music recordings and discuss the results.\n1. INTRODUCTION\nMusic classiﬁcation is a common and challenging Music\nInformation Retrieval (MIR) task, which provides practical\nmeans for the automatic annotation of music with seman-\ntic labels including genres, moods, instrumentation, and\nacoustic qualities of music. However, many researchers\nlimit their evaluations to cross-validation on small-sized\ndatasets available within the MIR community. This leaves\nthe question of the practical value of these classiﬁer mod-\nels for annotation, if the goal is to apply a label to any\nunknown musical input.\nIn the case of genre classiﬁcation, Sturm counts that\n42% of studies with experimental evaluation use publicly\navailable datasets, including the famous GTZAN music\ncollection (23%, or more than 100 works) and the ISMIR-\n2004 genre collection (17.4%), both of which contain no\nmore than 1000 tracks. There is some evidence that such\ncollections sizes are insufﬁcient [7]. The MIREX annual\nc/circlecopyrtDmitry Bogdanov, Alastair Porter, Perfecto Herrera,\nXavier Serra. Licensed under a Creative Commons Attribution 4.0 Inter-\nnational License (CC BY 4.0). Attribution: Dmitry Bogdanov, Alastair\nPorter, Perfecto Herrera, Xavier Serra. “Cross-collection evaluation for\nmusic classiﬁcation tasks”, 17th International Society for Music Infor-\nmation Retrieval Conference, 2016.evaluation includes a genre classiﬁcation task,1which is\ncurrently run on a collection of 7000 tracks annotated with\n10 genres, which is not publicly available to researchers.\nSome studies employ larger datasets, annotating genre us-\ning web-mined sources [11,17,18]. It has been shown that\nsome datasets have ﬂaws, including inconsistent and con-\ntroversial labeling, absence of artist ﬁlters, and presence of\nduplicate examples (for example, GTZAN [20]).\nCross-validation is routinely used as a method for ap-\nproximating a classiﬁer’s performance in real-world con-\nditions, but such estimation is not free from some pitfalls.\nThe ability of trained classiﬁer models to generalize re-\nmains under question when following this method [14].\nAs some studies have noted, typical k-fold cross-validation\non small-sized collections is prone to biases and additional\nevaluations based on independent out-of-sample data are\ndesirable in order to avoid them [2,8,12]. Cross-collection\nvalidation is also suggested in other domains [1, 3–5, 13].\nIn order to address these problems and be able to better\nassess the performance of music classiﬁer models, we pro-\npose a cross-collection evaluation process, that is, an eval-\nuation of models on independent sets of music tracks anno-\ntated with an independent ground-truth source (which we\ncallvalidation sets ). In this paper we present a methodol-\nogy and software tools for such evaluation for music clas-\nsiﬁcation tasks. We use AcousticBrainz,2a community-\nbased platform for gathering music information from au-\ndio [16]. It contains MIR-related music features for over\n3 million recordings including duplicates (we use the term\nrecording to refer to a single analysis of music track). It\nprovides the functionality to create datasets consisting of\nrecordings and associated ground truth, training classiﬁer\nmodels, and applying them to recordings present in Acous-\nticBrainz. A number of models trained on genre datasets\nused within MIR are already included. Our tools allow\nthe AcousticBrainz community to conduct cross-collection\nevaluations of classiﬁer models trained on the AcousticBrainz\nwebsite given any independent source of ground-truth an-\nnotations for recordings and a mapping between a model’s\nclasses and the classes within that ground truth.\nIn order to demonstrate the proposed methodology and\ntools, we evaluate ﬁve genre classiﬁer models trained on\nMIR genre datasets. We build a genre ground truth for\nrecordings in AcousticBrainz using collaborative tags from\n1http://music-ir.org/mirex\n2https://acousticbrainz.org379Last.fm3and consider various evaluation strategies for map-\nping the classiﬁer models’ outputs to the ground-truth classes.\nWe use our tools on validation sets from 240,000 to 1,740,000\nrecordings and discuss the obtained results. Finally, we\npublish our genre ground-truth dataset on our website.\n2. CROSS-COLLECTION EV ALUATION\nMETHODOLOGY\nWe deﬁne cross-collection evaluation to be an evaluation\nof a classiﬁer model on tracks in a validation set annotated\nwith an independent ground-truth source. We propose that\nthe validation set is obtained or collected from a differ-\nent source to the data used for training. This is distinct\nfrom holdout validation where a part of a dataset is used\nto verify the accuracy of the model. Because of the data’s\ndifferent origin it provides an alternative view on the ﬁ-\nnal performance of a system. We develop tools based on\nthis methodology for use in AcousticBrainz, because of the\ncommodity of its infrastructure for building and evaluating\nclassiﬁers, and the large amount of music recordings which\nit has analyzed.\n2.1 Evaluation strategies\nWe consider various evaluation strategies concerning the\ncomparison of a classiﬁer’s estimates and the ground-truth\nannotations in a validation set. A direct mapping of classes\nbetween the ground truth and the classiﬁer is not always\npossible due to differences in their number, names, and ac-\ntual meaning. Therefore, it is necessary to deﬁne a map-\nping between classes. Class names may imply broad cate-\ngories causing difﬁculties in determining their actual cov-\nerage and meaning, and therefore inspection of the con-\ntents of the classes is advised. The following cases may\noccur when designing a mapping: a) a class in the classi-\nﬁer can be matched directly to a class in the validation set;\nb) several classes in the classiﬁer can map to one in the val-\nidation set; c) one class in the classiﬁer can map to many\nin the validation set; d) a class in the validation set cannot\nbe mapped to any class in the classiﬁer; e) a class in the\nclassiﬁer cannot be mapped to any class in the validation\nset. The case d) represents the subset of the validation set\nwhich is “out of reach” for the classiﬁer in terms of its cov-\nerage, while e) represents the opposite, where the model is\nable to recognize categories unknown by the ground truth.\nWe show an example of such a mapping in Section 3.3.\nThe design of the mapping will affect evaluation results.\nValidation sets may vary in their size and coverage and\nmay contain a wider range of annotations than the classiﬁer\nbeing evaluated. We consider the following strategies:\n•S1: Use only recordings from the validation set whose\nground truth has a matching class in the classiﬁer. For\nexample, if a recording is only annotated with the class\nelectronic, and this class does not appear in the classiﬁer,\nwe discard it.\n3http://www.last.fm•S2: Use all recordings in the validation set and treat\nrecordings from classes that do not exist in the classi-\nﬁer as an incorrect classiﬁcation.\nThe validation set may have multiple class annotations per\nrecording (e.g., in case of genre annotations, both pop and\nrock could be assigned to the same recording). Where the\nvalidation set has more than one ground-truth class for a\nrecording we consider different methods of matching these\nclasses to classiﬁers’ estimates:\n•ONLY : Only use recordings that have one ground-truth\nclass, and discard the rest of the recordings when com-\nputing evaluation metrics.\n•ALL : When a recording has more than one ground-truth\nclass, accept an estimate as correct if it matches any of\nthem, even though for the rest of the classes it would be\nconsidered a misclassiﬁcation.\nThere may be duplicate recording representing the same\nmusic track (as is the case for AcousticBrainz, for which\ninconsistent classiﬁer estimates have been observed). We\nconsider two ways of dealing with them:\n•D1: Remove all recordings that have duplicates from the\nevaluation.\n•D2: Treat all recordings independently.\n2.2 Evaluation metrics\nUsing class mappings one can compute confusion matri-\nces for a classiﬁer model for all combinations of S1/S2\nwith ONLY/ALL and D1/D2. The confusion matrix counts\nthe percentage of correct classiﬁer class estimates for each\nground-truth class in the validation set. When a record-\ning has more than one ground-truth class in method ALL,\nthe recording is counted in all associated classes. Results\nare combined in the case when a class in the model is\nmapped to more than one class in the validation set. We\nestimate accuracy , the percentage of correctly recognized\nrecordings. This value can be skewed due to difference in\nthe sizes of each ground-truth class, and therefore we also\ncompute normalized accuracy by scaling match counts ac-\ncording to the number of recordings within each class.\n2.3 Tools for cross-collection evaluation\nWe have developed a set of tools as part of AcousticBrainz\nwhich let users train and evaluate classiﬁer models.4Our\ntools let a user evaluate the quality of this model using\nan independent validation set. They can conduct any of\nthe evaluation strategies mentioned above for any classi-\nﬁer model trained using AcousticBrainz.\nTo use our tools, a user ﬁrst creates two datasets in\nAcousticBrainz. They deﬁne one dataset to be used to train\na model, and the other to be used as the validation set. To\nensure reliability of the accuracy results, the user can per-\nform artist ﬁltering [6, 15] during both the training and the\n4We use the existing model training process, which selects best SVM\nparameters in a grid search using cross-validation and trains a model us-\ning all the data [10]. More details on the model training process are pro-\nvided at https://acousticbrainz.org/datasets380 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016cross-collection evaluation process. With artist ﬁltering,\nduring training we randomly select only one recording per\nartist for the model. On cross-collection evaluation, only\nrecordings by artists different from the ones present in the\ntraining data are used. To get the artist of a recording we\nuse the MusicBrainz API to request artist information for a\nrecording using its MBID. Classes are randomly truncated\nto the same size, with a lower limit of 200 instances per\nclass.\nAn interface lets the user map classes from the classiﬁer\ndataset to classes in the validation set. In the case that there\nare no suitable matches, a class can be discarded from con-\nsideration during evaluation. The tools generate a report\nincluding statistics on the employed dataset and ground\ntruth, accuracy values and confusion matrices. The re-\nsults can be exported as HTML, LaTeX, or other machine-\nreadable formats for further use. The tool is integrated into\nthe AcousticBrainz server5to make this cross-collection\nevaluation process available for all AcousticBrainz users.\n3. EV ALUATION OF GENRE CLASSIFIER\nMODELS\nWe applied our evaluation methodology to assess ﬁve genre\nclassiﬁers, three of which were already available within\nAcousticBrainz. We built two more classiﬁers using the\ndataset creator on the AcousticBrainz website with two\npreviously published sources for genre annotation. We built\nan independent ground truth by mining folksonomy tags\nfrom Last.fm.\n3.1 Music collections and classiﬁer models\n•GTZAN Genre Collection (GTZAN) .6A collection\nof 1000 tracks for 10 music genres (100 per genre) [23],\nincluding blues, classical, country, disco, hip-hop, jazz,\nmetal, pop, reggae, and rock. The GTZAN collection\nhas been commonly used as a benchmark dataset for\ngenre classiﬁcation, due to it being the ﬁrst dataset of\nthis kind made publicly available [22].\n•Music Audio Benchmark Data Set (MABDS) .7A col-\nlection of 1886 tracks retrieved from an online music\ncommunity and classiﬁed into 9 music genres (46–490\nper genre) [9] including alternative, electronic, funk/soul/rnb,\npop, rock, blues, folk/country, jazz, and rap/hiphop.\n•AcousticBrainz Rosamerica Collection (ROS) . A col-\nlection of 400 tracks for 8 genres (50 tracks per genre)\nincluding classical, dance, hip-hop, jazz, pop, rhythm &\nblues, rock, and speech (which is more a type of audio\nthan a musical genre). The collection was created by a\nprofessional musicologist [7].\n•MSD Allmusic Genre Dataset (MAGD) . A collection\nof genre annotations of the Million Song Dataset, de-\nrived from AllMusic [17]. We mapped MAGD to the\nAcousticBrainz collection,8which reduced the size of\n5https://github.com/metabrainz/acousticbrainz-server\n6http://marsyasweb.appspot.com/download/data_sets\n7http://www-ai.cs.uni-dortmund.de/audio.html\n8http://labs.acousticbrainz.org/million-song-dataset-mappingClassiﬁer Accuracy Normalized Random Size Number\naccuracy baseline of classes\nGTZAN 75.52 75.65 10 1000 10\nMABDS 60.25 43.5 11.1 1886 9\nROS 87.56 87.58 12.5 400 8\nMAGD 47.75 47.75 9.09 2266 11\nTAG 47.87 47.87 7.69 2964 13\nTable 1 : Cross-validation accuracies (%) for all classiﬁer\nmodels.\nthe dataset from 406,427 to 142,969 recordings and ap-\nplied an artist ﬁlter (keeping only one recording per artist).\nThe resulting dataset used for training contained 11 gen-\nres (206 tracks per genre) including pop/rock, electronic,\nrap, jazz, rnb, country, international, latin, reggae, vocal,\nand blues.9\n•Tagtraum genre annotations (TAG) . A collection of\ngenre annotations for Million Song Dataset, derived from\nLast.fm and beaTunes [18] (CD2C variation). As with\nMAGD, we mapped the dataset (reducing the size from\n191,408 to 148,960 recordings) and applied an artist ﬁl-\nter, resulting in 13 genres including rock, electronic, pop,\nrap, jazz, rnb, country, reggae, metal, blues, folk, world\nand latin (228 tracks per genre).10\nThe models for GTZAN, MABDS, and ROS are pro-\nvided in AcousticBrainz as baselines for genre classiﬁca-\ntion but these collections were trained without artist ﬁl-\ntering, as the recordings in these datasets are not asso-\nciated with MBIDs.11We inspected available metadata\nfor these collections and gathered non-complete artist lists\nfor artist ﬁltering in our cross-collection evaluation. We\nwere able to identify 245 artists for 912 of 1000 record-\nings for GTZAN [19], 1239 artists for all 1886 recordings\nfor MABDS, and 337 artists for 365 of 400 recordings for\nROS.\nTable 1 presents accuracies and normalized accuracies\nfor the considered models obtained from cross-validation\non training. The accuracies for GTZAN, MABDS, and\nROS models are reported on the AcousticBrainz website.\nIn general we observe medium to high accuracy compared\nto the random baseline. The results obtained for GTZAN\nare consistent with 78–83% accuracy observed for a num-\nber of other state-of-the-art methods on this collection [21].\nConfusion matrices12do not reveal any signiﬁcant mis-\nclassiﬁcations except for MABDS, for which alternative,\nfunk/soul/rnb, and pop classes were frequently misclassi-\nﬁed as rock (more than 35% of cases).\n3.2 Preparing an independent ground truth\nLast.fm contains tag annotations for a large number of mu-\nsic tracks by a community of music enthusiasts. While\nthese tags are freeform, they tend to include commonly\n9New age and folk categories were discarded due to insufﬁcient num-\nber of instances after artist ﬁltering keeping equal number of recordings\nper class.\n10Similarly, new age and punk categories were removed.\n11https://acousticbrainz.org/datasets/accuracy\n12Available online: http://labs.acousticbrainz.org/ismir2016Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 381recognized genres. We used the Last.fm API to get tag\nnames and counts for all recordings in the AcousticBrainz\ndatabase. Tag counts for a track are weighted, where the\nmost applied tag for the track has a weight of 100. As tags\nare collaborative between users, we expect them to repre-\nsent the “wisdom of the crowds”. We obtained tags for\n1,031,145 unique music tracks. Data includes 23,641,136\ntag-weight pairs using 781,898 unique tags.\nGenre tags as entered by users in Last.fm vary in their\nspeciﬁcity (e.g., “rock”, “progressive rock”). Meanwhile,\nthe classiﬁers that we evaluate estimate broad genre cat-\negories (according to their class names). Therefore, we\nmatched Last.fm tags to genres, and grouped these genres\nto broader “top-level” genres which we then matched to the\noutput classes of our classiﬁers. We used a tree of genres13\nfrom beets,14a popular music tagger which uses data from\nMusicBrainz, Last.fm, and other sources to organize per-\nsonal music collections. This genre tree is constructed\nbased on a list of genres on Wikipedia,15further moder-\nated by the community to add a wider coverage of music\nstyles.16The tree includes 16 top-level genres: african,\nasian, avant-garde, blues, caribbean and latin american,\nclassical, country, easy listening, electronic, folk, hip hop,\njazz, pop, rhythm & blues, rock, and ska.17The taxonomy\nprovided by the genre tree may not always be grounded on\nacoustical/musical similarity. For example, the asian top-\nlevel category includes both traditional music and western-\ninﬂuenced styles such as j-pop; jazz includes bebop, free\njazz, and ragtime; electronic includes both electroacoustic\nmusic and techno. Similar inconsistencies in the contents\nof classes are not uncommon in genre datasets [18], and\nhave been observed in GTZAN by [20], and also in our\ninformal reviews of other datasets.\nWe matched tags directly to genres or subgenres in the\ntree and then mapped them to their top-level genre. Weights\nwere combined for multiple tags mapped to the same top-\nlevel genre. Unmatched tags were discarded. We removed\ntracks where the weight of the top tag was less than 30,\nnormalized the tags so that the top tag had a weight of\n100 and again discarded tags with a weight of less than 30.\nAfter the cleaning process, we gathered genre annotations\nfor 778,964 unique MBIDs, corresponding to 1,743,674\nAcousticBrainz recordings (including duplicates).\n3.3 Genre mappings\nWe created mappings between the classes of the classiﬁer\nmodels and the validation set (Table 2). We created no\nmapping for ‘disco’ on GTZAN, ‘international’ and ‘vo-\ncal’ on MAGD, and ‘world’ on TAG as there were no clear\nmatches. For ROS we did not map ‘speech’ as it did not\nrepresent any musical genre. Recordings estimated by clas-\nsiﬁers as these classes were ignored during evaluation.\n13https://github.com/beetbox/beets/blob/0c7823b4/beetsplug/\nlastgenre/genres-tree.yaml\n14http://beets.io\n15https://en.wikipedia.org/wiki/List_of_popular_music_genres\n16The list from Wikipedia contains only modern popular music genres\n17We discarded the comedy and other genres3.4 Results\nUsing our tools, we performed an evaluation using sets\nfrom 240,000 to 1,740,000 recordings. Table 3 presents\nthe accuracies and number of recordings used for the eval-\nuation of each classiﬁer model. We bring attention to the\nS2-ONLY-D1 strategy as we feel that it reﬂects a real world\nevaluation on a variety of genres, while being relatively\nconservative in what it accepts as a correct result (the ONLY-\nD1 variation). Also of note is the S1-ONLY-D1 strategy\nas it evaluates the classiﬁers on a dataset which reﬂects\ntheir capabilities in terms of coverage. We present con-\nfusion matrices for the S2-ONLY-D1 strategy in Table 4\nfor MAGD and TAG models (confusion matrices differ lit-\ntle across all evaluation strategies according to our inspec-\ntion).18\nInspection of confusion matrices revealed a few surpris-\ning genre confusions. The MABDS model confuses all\nground-truth genres with electronic (e.g., blues 64%, folk\n62% of recordings misclassiﬁed). This tendency is consis-\ntent with inspection of this model’s estimates in the Acous-\nticBrainz database (81% estimated as electronic). No pat-\ntern of such misclassiﬁcation was present in the confu-\nsion matrix during the training stage. Although this model\nwas trained on unbalanced data, electronic was among the\nsmallest sized classes (only 6% of the MABDS collection).\nSimilarly, the GTZAN model tends to estimate all music as\njazz (>73% of recordings of all genres are misclassiﬁed),\nwhich is again consistent with genre estimates in Acous-\nticBrainz (90% estimated as jazz), with no such problems\nfound during training.\nThe ROS model does not misclassify genres as harshly,\nconfusing pop with rhythm & blues (26%), jazz with clas-\nsical (21%), electronic with hip hop and rhythm & blues,\njazz with rhythm & blues, and rhythm & blues with pop\n(<20% of cases for all confusions). For the MAGD model\nwe see misclassiﬁcations of pop with rhythm & blues (21%),\npop with carribean & latin and country, and rhythm &\nblues with blues ( <15%). The TAG model performed bet-\nter than MAGD, with no genre being misclassiﬁed for an-\nother more than 15% of the time, though we see a moderate\namount of blues, electronic, folk, and pop instances being\nconfused with rock as well as rhythm & blues with blues.\nThe confusions for all three models make sense from mu-\nsicological and computational points of view, evidencing\nhow controversial genre-tagging can be, and that the com-\nputed features may not be speciﬁc enough to differentiate\nbetween genre labels.\n3.5 Discussion\nIn general, considering exclusion/inclusion of the duplicate\nrecordings in the evaluation (D1/D2), we observed that the\ndifferences in accuracy values are less than 4 percentage\npoints for all models. We conclude that duplicates do not\ncreate any strong bias in any of our evaluation strategies\neven though the sizes of D1/D2 testing sets vary a lot.\n18Complete results for all classiﬁer models are available at:\nhttp://labs.acousticbrainz.org/ismir2016382 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Ground truth GTZAN MABDS ROS MAGD TAG\nafrican - - - - -\nasian - - - - -\navant-garde - - - - -\nblues blues blues - blues blues\ncaribbean and latin american reggae - - latin, reggae latin, reggae\nclassical classical - classical - -\ncountry country folk/country - country country\neasy listening - - - - -\nelectronic - electronic dance electronic electronic\nfolk - folk/country - - folk\nhip hop hip-hop rap/hiphop hip-hop rap rap\njazz jazz jazz jazz jazz jazz\npop pop pop pop pop/rock pop\nrhythm and blues - funk/soul/rnb rnb rnb rnb\nrock rock, metal rock, alternative rock pop/rock metal, rock\nska - - - - -\nTable 2 : Mapping between classiﬁer classes and ground-truth genres.\nModel Strategy Recordings Accuracy Normalized\naccuracy\nGTZAN S1-ALL-D1 274895 13.68 12.78\nS1-ALL-D2 1235692 10.72 12.73\nS1-ONLY-D1 242346 13.27 12.95\nS1-ONLY-D2 1053670 10.35 12.91\nS2-ALL-D1 373886 10.06 6.39\nS2-ALL-D2 1623809 8.16 6.37\nS2-ONLY-D1 292840 8.99 6.42\nS2-ONLY-D2 1214253 6.93 6.37\nMABDS S1-ALL-D1 361043 31.76 17.52\nS1-ALL-D2 1660333 31.13 16.75\nS1-ONLY-D1 292220 28.39 18.44\nS1-ONLY-D2 1277695 27.44 17.96\nS2-ALL-D1 386945 29.63 9.86\nS2-ALL-D2 1743034 29.65 9.42\nS2-ONLY-D1 302448 26.41 10.40\nS2-ONLY-D2 1302343 25.82 10.15\nROS S1-ALL-D1 320398 51.73 47.36\nS1-ALL-D2 1518024 50.12 45.32\nS1-ONLY-D1 269820 50.46 52.52\nS1-ONLY-D2 1229136 48.82 51.72\nS2-ALL-D1 379302 43.70 20.72\nS2-ALL-D2 1683696 45.19 19.83\nS2-ONLY-D1 296112 43.12 23.58\nS2-ONLY-D2 1252289 45.19 23.24\nMAGD S1-ALL-D1 323438 59.35 42.13\nS1-ALL-D2 1505105 59.91 40.41\nS1-ONLY-D1 265890 59.56 48.34\nS1-ONLY-D2 1184476 60.83 48.40\nS2-ALL-D1 347978 55.92 23.70\nS2-ALL-D2 1590395 57.36 22.73\nS2-ONLY-D1 272426 56.36 27.35\nS2-ONLY-D2 1187287 58.94 27.56\nTAG S1-ALL-D1 327825 59.85 44.46\nS1-ALL-D2 1482123 60.58 42.12\nS1-ONLY-D1 265280 59.51 52.97\nS1-ONLY-D2 1139297 60.49 52.77\nS2-ALL-D1 342544 57.35 27.79\nS2-ALL-D2 1532129 58.67 26.32\nS2-ONLY-D1 268543 56.94 33.13\nS2-ONLY-D2 1147231 58.62 33.10\nTable 3 : Cross-collection evaluation accuracies (%) for all\nclassiﬁer models.\nSimilar conclusions can be made with respect to the in-\nclusion of recordings with conﬂicting genre ground truth\n(ONLY/ALL). Conﬂicting cases of genre annotations only\naccount for 21% of our validation set. In the case of ourLast.fm annotations, multiple labeling of ground truth does\nnot affect our results, still, one should explore both strate-\ngies to ensure that the same holds for other ground truths.\nThe only large difference in accuracy values is observed\nwhen comparing S1 and S2 strategies—S1 yields higher\naccuracies as all additional recordings in S2 are considered\nincorrect no matter what the classiﬁer selects. Normalized\naccuracy allows us to assess the performance of a classiﬁer\ngiven a hypothetical validation set with an equal number of\ninstances per class. In our S2 strategy, many validation set\nclasses not matched to a classiﬁer class, and therefore con-\nsidered incorrect, contained a small number of recordings\n(e.g., african, asian, avant-garde, and easy listening; see\nTable 4). Because of this we observe a larger difference in\nnormalized accuracies between S1 and S2.\nBased on the results we conclude that the models for\nROS, MAGD and TAG perform the best. Their normalized\naccuracies are two times better than other classiﬁers un-\nder any condition. Interestingly, the ROS model is trained\non the smallest collection (400 tracks, compared to 1000\ntracks for GTZAN and 1886 tracks for MABDS, and over\n2000 for MAGD an TAG), while we expected that it would\nsuffer from insufﬁcient training size.\nWhat can be the reason for such a differing performances\nof models? MAGD uses as its source genre annotations\nmade by experts from Allmusic, while the ROS collection\nwas created by a musicologist speciﬁcally for the task of\ncontent-based music classiﬁcation, which may be the rea-\nson for their better performance. The annotations in TAG\nwere taken from two different sources, and were only used\nwhen both sources agreed on the genre [18].\n4. CONCLUSIONS\nThe majority of studies on music classiﬁcation rely on esti-\nmating cross-validation accuracy on a single ground truth,\nwhile it has been criticized as being shortsighted to shed\nlight on the real capacity of a system to recognize music\ncategories [21]. In our study we go beyond this approach\nand show an additional way to ascertain the capacity of\nclassiﬁers by evaluating across collections. We believe that\ncross-collection generalization is an interesting metric toProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 383Ground-truth Estimated genre\ngenre size (%) blues carribean & latin country electronic hip hop jazz pop, rock rhythm & blues\n(blues) (latin, reggae) (country) (electronic) (rap) (jazz) (pop/rock) (rnb)\nblues 2.7 48.30 8.54 11.27 2.88 1.31 8.94 13.16 5.61\ncarribean & latin 1.9 6.98 57.11 4.70 5.64 6.88 7.44 3.04 8.20\ncountry 4.3 14.09 8.28 61.89 1.00 0.43 2.24 8.38 3.67\nelectronic 20.1 2.01 4.57 0.90 59.28 6.22 4.35 16.22 6.45\nhip hop 2.3 1.22 10.39 0.14 8.77 65.63 1.08 3.80 8.97\njazz 7.7 9.39 7.00 3.68 2.87 1.02 67.22 4.60 4.22\npop 5.8 4.63 18.65 18.23 7.12 3.47 2.87 23.98 21.05\nrhythm & blues 3.4 16.36 13.87 11.88 3.66 5.93 5.00 10.74 32.56\nrock 43.9 7.60 6.26 6.10 7.14 0.82 2.48 67.38 2.23\nafrican 0.3 13.09 33.54 10.22 8.38 6.13 9.20 4.50 14.93\nasian 1.2 2.13 16.85 7.72 8.35 2.58 3.66 35.34 23.36\navant-garde 0.2 14.33 8.26 5.51 12.40 5.92 20.11 30.72 2.75\nclassical 2.2 4.85 4.05 5.73 4.30 0.45 66.05 13.86 0.70\neasy listening 0.4 9.52 13.79 27.59 6.62 2.21 18.07 14.06 8.14\nfolk 3.1 15.55 14.80 28.92 5.75 0.89 10.71 20.14 3.24\nska 0.7 4.80 39.28 5.71 13.90 9.21 2.16 19.66 5.28\n(a) MAGD. Columns for pop and rock are summed together as they match the same model class\nGround-truth Estimated genre\ngenre size (%) blues carribean & latin country electronic folk hip hop jazz pop rhythm & blues rock\n(blues) (latin, reggae) (country) (electronic) (folk) (rap) (jazz) (pop) (rnb) (metal, rock)\nblues 2.7 48.09 4.70 6.57 1.08 7.17 0.63 9.55 3.45 6.72 12.04\ncarribean & latin 1.9 3.47 59.11 3.26 2.93 4.37 5.56 6.96 4.47 6.77 3.12\ncountry 4.3 11.58 5.29 51.90 0.37 11.15 0.15 2.55 6.13 3.18 7.70\nelectronic 20.1 0.82 4.53 0.81 53.66 5.45 5.82 2.62 9.33 3.33 13.63\nfolk 3.1 6.03 4.46 11.23 3.20 47.22 0.35 5.30 7.53 2.59 12.09\nhip hop 2.3 0.83 9.14 0.11 7.03 0.31 67.00 1.52 2.30 8.75 3.00\njazz 7.7 7.68 5.19 2.15 1.43 4.93 0.71 65.56 3.28 6.05 3.02\npop 5.8 3.76 9.24 11.43 4.02 8.56 2.40 4.10 35.78 8.38 12.32\nrhythm & blues 3.4 12.59 11.40 8.37 1.96 3.73 4.45 5.29 11.82 31.94 8.46\nrock 43.9 4.19 2.65 3.61 2.95 5.15 0.57 1.59 7.54 1.99 69.76\nafrican 0.3 12.43 29.94 5.37 5.08 14.12 5.65 7.34 7.34 8.19 4.52\nasian 1.2 1.63 6.64 3.47 4.52 3.94 2.67 1.98 52.76 6.12 16.26\navant-garde 0.2 8.38 5.67 2.47 9.25 14.43 4.19 16.40 4.07 4.93 30.21\nclassical 2.2 5.78 1.64 4.76 1.73 30.75 0.16 41.08 5.38 2.19 6.53\neasy listening 0.4 6.81 7.07 17.38 2.97 21.05 0.79 14.76 15.20 5.94 8.03\nska 0.7 3.00 42.24 4.00 9.71 0.75 9.66 1.15 5.36 3.55 20.57\n(b) TAG\nTable 4 : Confusion matrices for S2-ONLY-D1 strategy. Original class names for the classiﬁers are listed in parentheses.\nMisclassiﬁcations >10% are shaded light gray and >20% dark gray.\ntake into account for validating the robustness of classi-\nﬁer models. We propose a methodology and software tools\nfor such an evaluation. The tools let researchers conduct\nlarge-scale evaluations of classiﬁer models trained within\nAcousticBrainz, given an independent source of ground-\ntruth annotations and a mapping between the classes.\nWe applied our methodology and evaluated the perfor-\nmance of ﬁve genre classiﬁer models trained on MIR genre\ncollections. We applied these models on the AcousticBrainz\ndataset using between 240,000 and 1,740,000 music record-\nings in our validation sets and automatically annotated these\nrecordings by genre using Last.fm tags. We demonstrated\nthat good cross-validation results obtained on datasets fre-\nquently reported in existing research may not generalize\nwell. Using any of the better-performing models on Acous-\nticBrainz, we can only expect a 43–58% accuracy accord-\ning to our Last.fm ground truth when presented with any\nrecording on AcousticBrainz. We feel that this is still not\na good result, and highlights how blurred the concept of\ngenres can be, and that these classiﬁers may be “blind”\nwith respect to some important musical aspects deﬁningsome of the genres. More research effort is required in\ndesigning musically meaningful descriptors and making\nthem error-resistant, as well as understanding the relation-\nships between different genre taxonomies.\nImportantly, the application of the proposed methodol-\nogy is not limited to genres and can be extended to other\nclassiﬁcation tasks. In addition to the proposed methodol-\nogy and tools, we release a public dataset of genre annota-\ntions used in this study.19In our future work we plan to in-\nvestigate and publish more independent sources of ground-\ntruth annotations, including annotations by genre and mood,\nthat will allow researchers to conduct more thorough eval-\nuations of their models within AcousticBrainz.\n5. ACKNOWLEDGEMENTS\nThis research has received funding from the European Unions\nHorizon 2020 research and innovation programme under\ngrant agreement No 688382. We also thank Gabriel Viglien-\nsoni for help with mining Last.fm tags.\n19https://labs.acousticbrainz.org/lastfm-genre-annotations384 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20166. REFERENCES\n[1] J. Bekios-Calfa, J. M Buenaposada, and L. Baumela.\nRevisiting linear discriminant techniques in gender\nrecognition. IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 33(4):858–864, 2011.\n[2] D. Bogdanov, J. Serr `a, N. Wack, P. Herrera, and\nX. Serra. Unifying low-level and high-level music sim-\nilarity measures. IEEE Transactions on Multimedia ,\n13(4):687–701, 2011.\n[3] V . Chud ´aˇcek, G Georgoulas, L. Lhotsk ´a, C Stylios,\nM. Petr ´ık, and M ˇCepek. Examining cross-database\nglobal training to evaluate ﬁve different methods for\nventricular beat classiﬁcation. Physiological measure-\nment , 30(7):661–677, 2009.\n[4] N. Erdogmus, M. Vanoni, and S. Marcel. Within-and\ncross-database evaluations for gender classiﬁcation via\nbeﬁt protocols. In International Workshop on Multime-\ndia Signal Processing (MMSP’14) , pages 1–6, 2014.\n[5] C. Fern ´andez, I. Huerta, and A. Prati. A compara-\ntive evaluation of regression learning algorithms for\nfacial age estimation. In Face and Facial Expression\nRecognition from Real World Videos , pages 133–144.\nSpringer, 2015.\n[6] A. Flexer and D. Schnitzer. Effects of album and artist\nﬁlters in audio similarity computed for very large mu-\nsic databases. Computer Music Journal , 34(3):20–28,\n2010.\n[7] E. Guaus. Audio content processing for automatic mu-\nsic genre classiﬁcation: descriptors, databases, and\nclassiﬁers . PhD thesis, Universitat Pompeu Fabra,\n2009.\n[8] P. Herrera, A. Dehamel, and F. Gouyon. Automatic la-\nbeling of unpitched percussion sounds. In AES 114th\nConvention , 2003.\n[9] H. Homburg, I. Mierswa, B. M ¨oller, K. Morik, and\nM. Wurst. A benchmark dataset for audio classiﬁcation\nand clustering. In International Conference on Mu-\nsic Information Retrieval (ISMIR’05) , pages 528–531,\n2005.\n[10] C. W. Hsu, C. C. Chang, and C. J. Lin. A practical\nguide to support vector classiﬁcation. 2003.\n[11] D. Liang, H. Gu, and B. O’Connor. Music genre clas-\nsiﬁcation with the Million Song Dataset. Technical re-\nport, Carnegie Mellon University, 2011.\n[12] A. Livshin and X. Rodet. The importance of cross\ndatabase evaluation in musical instrument sound clas-\nsiﬁcation: A critical approach. In International Con-\nference on Music Information Retrieval (ISMIR’03) ,\n2003.[13] M. Llamedo, A. Khawaja, and J. P. Mart ´ınez. Cross-\ndatabase evaluation of a multilead heartbeat classi-\nﬁer.IEEE Transactions on Information Technology in\nBiomedicine , 16(4):658–664, 2012.\n[14] A. Y Ng. Preventing” overﬁtting” of cross-validation\ndata. In International Conference on Machine Learn-\ning (ICML’97) , pages 245–253, 1997.\n[15] E. Pampalk, A. Flexer, and G. Widmer. Improvements\nof audio-based music similarity and genre classiﬁca-\nton. In International Conference on Music Information\nRetrieval (ISMIR’05) , pages 628–633, 2005.\n[16] A. Porter, D. Bogdanov, R. Kaye, R. Tsukanov, and\nX. Serra. AcousticBrainz: a community platform for\ngathering music information obtained from audio. In\nInternational Society for Music Information Retrieval\nConference (ISMIR’15) , 2015.\n[17] A. Schindler, R. Mayer, and A. Rauber. Facilitating\ncomprehensive benchmarking experiments on the mil-\nlion song dataset. In International Society for Music\nInformation Retrieval Conference (ISMIR’12) , pages\n469–474, 2012.\n[18] H. Schreiber. Improving genre annotations for the mil-\nlion song dataset. In International Society for Music\nInformation Retrieval Conference (ISMIR’15) , 2015.\n[19] B. L Sturm. The state of the art ten years after a state of\nthe art: Future research in music information retrieval.\nJournal of New Music Research , 43(2):147–172, 2014.\n[20] B.L. Sturm. An analysis of the GTZAN music genre\ndataset. In International ACM Workshop on Music\nInformation Retrieval with User-centered and Multi-\nmodal Strategies (MIRUM’12) , pages 7–12, 2012.\n[21] B.L. Sturm. Classiﬁcation accuracy is not enough.\nJournal of Intelligent Information Systems , 41(3):371–\n406, 2013.\n[22] B.L. Sturm. A survey of evaluation in music genre\nrecognition. In Adaptive Multimedia Retrieval: Seman-\ntics, Context, and Adaptation , pages 29–66. Springer,\n2014.\n[23] G. Tzanetakis and P. Cook. Musical genre classiﬁca-\ntion of audio signals. IEEE Transactions on Speech and\nAudio Processing , 10(5):293–302, 2002.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 385"
    },
    {
        "title": "A Comparison of Melody Extraction Methods Based on Source-Filter Modelling.",
        "author": [
            "Juan J. Bosch",
            "Rachel M. Bittner",
            "Justin Salamon",
            "Emilia Gómez"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1418167",
        "url": "https://doi.org/10.5281/zenodo.1418167",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/256_Paper.pdf",
        "abstract": "This work explores the use of source-filter models for pitch salience estimation and their combination with different pitch tracking and voicing estimation methods for auto- matic melody extraction. Source-filter models are used to create a mid-level representation of pitch that implic- itly incorporates timbre information. The spectrogram of a musical audio signal is modelled as the sum of the lead- ing voice (produced by human voice or pitched musical in- struments) and accompaniment. The leading voice is then modelled with a Smoothed Instantaneous Mixture Model (SIMM) based on a source-filter model. The main advan- tage of such a pitch salience function is that it enhances the leading voice even without explicitly separating it from the rest of the signal. We show that this is beneficial for melody extraction, increasing pitch estimation accu- racy and reducing octave errors in comparison with simpler pitch salience functions. The adequate combination with voicing detection techniques based on pitch contour char- acterisation leads to significant improvements over state- of-the-art methods, for both vocal and instrumental music.",
        "zenodo_id": 1418167,
        "dblp_key": "conf/ismir/BoschBSG16",
        "content": "A COMPARISON OF MELODY EXTRACTION METHODS BASED ON\nSOURCE-FILTER MODELLING\nJuan J. Bosch1Rachel M. Bittner2Justin Salamon2Emilia G ´omez1\n1Music Technology Group, Universitat Pompeu Fabra, Spain\n2Music and Audio Research Laboratory, New York University, USA\n{juan.bosch,emilia.gomez }@upf.edu, {rachel.bittner,justin.salamon }@nyu.edu\nABSTRACT\nThis work explores the use of source-ﬁlter models for pitch\nsalience estimation and their combination with different\npitch tracking and voicing estimation methods for auto-\nmatic melody extraction. Source-ﬁlter models are used\nto create a mid-level representation of pitch that implic-\nitly incorporates timbre information. The spectrogram of\na musical audio signal is modelled as the sum of the lead-\ning voice (produced by human voice or pitched musical in-\nstruments) and accompaniment. The leading voice is then\nmodelled with a Smoothed Instantaneous Mixture Model\n(SIMM) based on a source-ﬁlter model. The main advan-\ntage of such a pitch salience function is that it enhances\nthe leading voice even without explicitly separating it from\nthe rest of the signal. We show that this is beneﬁcial\nfor melody extraction, increasing pitch estimation accu-\nracy and reducing octave errors in comparison with simpler\npitch salience functions. The adequate combination with\nvoicing detection techniques based on pitch contour char-\nacterisation leads to signiﬁcant improvements over state-\nof-the-art methods, for both vocal and instrumental music.\n1. INTRODUCTION\nMelody is regarded as one of the most relevant aspects of\nmusic, and melody extraction is an important task in Mu-\nsic Information Retrieval (MIR). Salamon et al. [21] deﬁne\nmelody extraction as the estimation of the sequence of fun-\ndamental frequency ( f0) values representing the pitch of\nthe lead voice or instrument, and this deﬁnition is the one\nemployed by the Music Information Retrieval Evaluation\neXchange (MIREX) [7]. While this deﬁnition provides an\nobjective and clear task for researches and engineers, it is\nalso very speciﬁc to certain types of music data. Recently\nproposed datasets consider broader deﬁnitions of melody,\nwhich are not restricted to a single instrument [2, 4, 6].\nComposers and performers use several cues to make\nmelodies perceptually salient, including loudness, timbre,\nc/circlecopyrtFirst author, Second author, Third author, Fourth author,\nFifth author, Sixth author. Licensed under a Creative Commons Attribu-\ntion 4.0 International License (CC BY 4.0). Attribution: First author,\nSecond author, Third author, Fourth author, Fifth author, Sixth author. “A\ncomparison of melody extraction methods based on source-ﬁlter mod-\nelling”, 17th International Society for Music Information Retrieval Con-\nference, 2016.frequency variation or note onset rate. Melody extraction\nmethods commonly use cues such as pitch continuity and\npitch salience, and some of them group pitches into higher\nlevel objects (such as tones or contours), using principles\nfrom Auditory Scene Analysis [8, 13, 16, 18, 20]. Some\napproaches have also considered timbre, either within a\nsource separation framework [10, 17], with a machine\nlearning approach [11], or in a salience based approach\n[14, 16].\nOne of the best performing methods so far in MIREX in\nterms of overall accuracy [20] (evaluated in 2011) is based\non the creation and characterisation of pitch contours. This\nmethod uses a fairly simple salience function based on har-\nmonic summation [15] and then creates and characterises\npitch contours for melody tracking. V oicing detection (de-\ntermining if a frame contains a melody pitch or not) is\none of the strong aspects of this method, even though it\nmight be improved further by incorporating timbre infor-\nmation. In contrast, alternative approaches employ more\nsophisticated salience functions, but the pitch tracking and\nvoicing estimation components are less complex [10, 12].\nV oicing detection has in fact been identiﬁed in the litera-\nture as a crucial task for improving melody extraction sys-\ntems [10, 20].\nWhile these approaches work especially well for vo-\ncal music, their performance decreases for instrumental\npieces, as shown in [6] and [2], where a drop of 19 per-\ncentage points in overall accuracy was observed for instru-\nmental pieces compared to vocal pieces. A main challenge\nfor melody extraction methods is thus to cope with more\ncomplex and varied music material, with melodies played\nby different instruments, or with harmonised melodic lines\n[21]. A key step towards the development of more ad-\nvanced algorithms and a more realistic evaluation is the\navailability of large and open annotated datasets. In [4, 6]\nthe authors presented a dataset for melody extraction in or-\nchestral music with such characteristics, and MedleyDB\n[2] also includes a variety of instrumentation and genres.\nResults on both datasets generally drop signiﬁcantly in\ncomparison to results on datasets used in MIREX [7].\nBased on results obtained in previous work [5, 6], we\nhypothesise that a key ingredient for improving salience-\nbased melody extraction in relatively complex music data\nis the salience function itself. In particular, we propose\ncombining strong components of recently proposed algo-\nrithms: (1) a semantically rich salience function based on a571source-ﬁlter model, which proved to work especially well\nin pitch estimation [6, 9, 10], and (2) pitch-contour-based\ntracking [2, 20], which presents numerous beneﬁts includ-\ning high-performance voicing detection.\n2. RELATED METHODS\nThis section describes the pitch salience functions and\nmelody tracking methods used as building blocks for the\nproposed combinations.\n2.1 Salience functions\nMost melody extraction methods are based on the estima-\ntion of pitch salience - we focus here on the ones proposed\nby Salamon and G ´omez [20], and Durrieu et al. [9].\nDurrieu et al. [9] propose a salience function within\na separation-based approach using a Smoothed Instanta-\nneous Mixture Model (SIMM). They model the spectrum\nXof the signal as the lead instrument plus accompani-\nment ˆX=ˆXv+ˆXm. The lead instrument is mod-\nelled as: ˆXv=XΦ◦Xf0, whereXf0corresponds to\nthe source, XΦto the ﬁlter, and the symbol ◦denotes\nthe Hadamard product. Both source and ﬁlter are decom-\nposed into basis and gains matrices as Xf0=Wf0Hf0and\nXΦ=WΓHΓHΦrespectively. Hf0corresponds to the\npitch activations of the source, and can also be understood\nas a representation of pitch salience [9]. The accompani-\nment is modelled as a standard non negative matrix factor-\nization (NMF): ˆXm=ˆWmˆHm. Parameter estimation is\nbased on Maximum-Likelihood, with a multiplicative gra-\ndient method [10], updating parameters in the following\norder for each iteration: Hf0,HΦ,Hm,WΦandWm. Even\nthough this model was designed for singing voice, it can\nbe successfully used for music instruments, since the ﬁlter\npart is related to the timbre of the sound, and the source\npart represents a harmonic signal driven by the fundamen-\ntal frequency.\nSalamon and G ´omez [20] proposed a salience func-\ntion based on harmonic summation: a time-domain Equal-\nLoudness Filter (ELF) is applied to the signal, followed\nby the Short-Time Fourier Transform (STFT). Next, si-\nnusoidal peaks are detected and their frequency/amplitude\nvalues are reﬁned using an estimate of the peaks’ instan-\ntaneous frequency. The salience function is obtained by\nmapping each peak’s energy to all harmonically related f0\ncandidates with exponentially decaying weights.\n2.2 Tracking\nThe estimated pitch salience is then used to perform\npitch tracking, commonly relying on the predominance of\nmelody pitches in terms of loudness, and on the melody\ncontour smoothness [1, 10, 12, 20].\nSome methods have used pitch contour characteristics\nfor melody tracking [1, 20, 22]. Salamon and G ´omez [20]\ncreate pitch contours from the salience function by group-\ning sequences of salience peaks which are continuous in\ntime and pitch. Several parameters need to be set in thisprocess, which determine the amount of extracted con-\ntours. Created contours are then characterised by a set of\nfeatures: pitch (mean and deviation), salience (mean, stan-\ndard deviation), total salience, length and vibrato related\nfeatures.\nThe last step deals with the selection of melody con-\ntours. Salamon [20] ﬁrst proposed a pitch contour selection\n(PCS) stage using a set of heuristic rules based on the con-\ntour features. Salamon [22] and Bittner [1] later proposed\na pitch contour classiﬁcation (PCC) method based on con-\ntour features. The former uses a generative model based on\nmulti-variate Gaussians to distinguish melody from non-\nmelody contours, and the latter uses a discriminative clas-\nsiﬁer (a binary random forest) to perform melody con-\ntour selection. The latter also adds Viterbi decoding over\nthe predicted melodic-contour probabilities for the ﬁnal\nmelody selection. However, these classiﬁcation-based ap-\nproaches did not outperform the rule-based approach on\nMedleyDB. One of the important conclusions of both pa-\npers was that the sub-optimal performance of the contour\ncreation stage (which was the same in both approaches)\nwas a signiﬁcant limiting factor in their performance.\nDurrieu et al. [10] similarly use an HMM in which each\nstate corresponds to one of the bins of the salience func-\ntion, and the probability of each state corresponds to the\nestimated salience of the source ( Hf0).\n2.3 Voicing estimation\nMelody extraction algorithms have to classify frames as\nvoiced or unvoiced (containing a melody pitch or not, re-\nspectively). Most approaches use static or dynamic thresh-\nolds [8, 10, 12], while Salamon and G ´omez exploit pitch\ncontour salience distributions [20]. Bittner et al. [1] deter-\nmine voicing by setting a threshold on the contour proba-\nbilities produced by the discriminative model. The thresh-\nold is selected by maximizing the F-measure of the pre-\ndicted contour labels over a training set.\nDurrieu et al. [10] estimate the energy of the melody\nsignal frame by frame. Frames whose energy falls be-\nlow the threshold are set as unvoiced and vice versa. The\nthreshold is empirically chosen, such that voiced frames\nrepresent more than 99.95% of the leading instrument en-\nergy.\n3. PROPOSED METHODS\nWe propose and compare three melody extraction methods\nwhich combine different pitch tracking and voicing estima-\ntion techniques with pitch salience computation based on\nsource-ﬁlter modelling and harmonic summation. These\napproaches have been implemented in python and are\navailable online1. We reuse parts of code from Durrieu’s\nmethod2, Bittner et al.3, and Essentia4[3], an open\nsource library for audio analysis, which includes an im-\nplementation of [20] which has relatively small deviations\n1https://github.com/juanjobosch/SourceFilterContoursMelody\n2https://github.com/wslihgt/separateLeadStereo\n3https://github.com/rabitt/contour classiﬁcation\n4https://github.com/MTG/essentia572 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016in performance from the authors’ original implementation\nMELODIA5. We refer to the original implementation of\nMELODIA as SAL, and to the implementation in the Es-\nsentia library as ESS.\n3.1 Pitch Salience Adaptation\nThere are important differences between the characteris-\ntics of salience functions obtained with SIMM ( Hf0) and\nharmonic summation ( HS). For instance, Hf0is consider-\nably more sparse, and the range of salience values is much\nlarger than in HS since the NMF-based method does not\nprevent values (weights) from being very high or very low.\nThis is illustrated in Figure 1: (a) shows the pitch salience\nfunction obtained with the source ﬁlter model, Hf0. Given\nthe large dynamic range of Hf0we display its energy on a\nlogarithmic scale, whereas plots (b)–(d) use a linear scale.\n(b) corresponds to HSwhich is denser and results in com-\nplex patterns even for monophonic signals. Some bene-\nﬁts of this salience function with respect to Hf0(SIMM)\nis that it is smoother, and the range of possible values is\nmuch smaller.\nGiven the characteristics of Hf0, it is necessary to re-\nduce the dynamic range of its salience values in order to\nuse it as input to the pitch contour tracking framework,\nwhich is tuned for the characteristics of HS. To do so,\nwe propose the combination of both salience functions\nHS(k,i)andHf0(k,i), wherekindicates the frequency\nbink= 1...K andithe frame index i= 1...I. The\ncombination process is illustrated in Figure 1: (1) Global\nnormalization (Gn) ofHS, dividing all elements by their\nmaximum value max k,i(HS(k,i)). (2) Frame-wise nor-\nmalization (Fn) ofHf0. For each frame i, divideHf0(k,i)\nbymax k(Hf0(k,i)). (3) Convolution in the frequency\naxis kofHf0with a Gaussian ﬁlter to smooth estimated\nactivations. The ﬁlter has a standard deviation of .2 semi-\ntones. (4) Global normalization (Gn), whose output is\n/tildewidestHf0(see Figure 1 (c)). (5) Combination by means of an\nelement-wise product: Sc=/tildewidestHf0◦HS(see Figure 1 (d)).\n3.2 Combinations\nWe propose three different combination methods. The ﬁrst\n(C1) combines the output of two algorithms: estimated\npitches from DUR and voicing estimation from SAL. The\nsecond (C2) is based on Sc, which combines harmonic\nsummationHS computed with ESS with /tildewidestHf0, and em-\nploys pitch contour creation and selection as the tracking\nmethod. The last method (C3) combines Scwith pitch con-\ntour creation from [20] and the contour classiﬁcation strat-\negy from [1]. C2 and C3 correspond to Figure 1, where the\ncontour ﬁltering stage is based on pitch contour selection\nor pitch contour classiﬁcation, respectively.\n4. EV ALUATION\nEvaluation was carried out using the MedleyDB and Or-\nchset datasets, following the standard MIREX evaluation\n5http://mtg.upf.edu/technologies/melodia\nAudio \nH.Sum SIMM \nContour \nCreation \nContour \nCharacterisation \nContour \nFiltering Pitch \nSalience \nEstimation \nMelody \nTracking Gn O GnPitch \nSalience \nCombination FnH f 0 HS \n(a) H f0\n(c) H f0-GF-Fn \n(d) Combination (b) HS \nMelody Figure 1 .Left: Schema for C2 and C3. H.Sum: Harmonic\nSummation (outputs HS); SIMM: Smoothed Instanta-\nneous Mixture Model (outputs Hf0); Fn: Frame-wise nor-\nmalisation; Gn: Global normalisation; o: Hadamard prod-\nuct; Gaussian symbol: Gaussian ﬁltering. Right : Time-\nfrequency pitch salience representation of an excerpt from\n“MusicDelta FunkJazz.wav” (MedleyDB) with (a) SIMM:\nlog10(Hf0)is represented for visualisation purposes) (b)\nHarmonic Summation: HS(c)Hf0normalised per frame,\nGaussian ﬁltered and globally normalized ( /tildewidestHf0) (d) Com-\nbination (Sc).\nmethodology. We evaluate the proposed methods (C1–\nC3) and the original algorithms by Durrieu (DUR), Bittner\n(BIT) and Salamon. Table 1 provides and overview of their\nmain building blocks. In the case of Salamon’s approach,\nwe include the original implementation MELODIA (SAL),\nand the implementation in the Essentia library (ESS). The\nlatter can be viewed as a baseline for the proposed com-\nbination methods (C2, C3), since all three share the same\ncontour creation implementation.\nFor the evaluation of classiﬁcation-based methods, we\nfollowed [1], and created train/test splits using an “artist-\nconditional” random partition on MedleyDB. For Orchset\nwe created a “movement-conditional” random partition,\nmeaning all excerpts from the same movement must be\nused in the same subset: either for training or for test-\ning. Datasets are split randomly into a training, valida-\ntion and test sets with roughly 63%, 12%, and 25% of the\nsongs/excerpts in the dataset, respectively. This partition-\ning was chosen so as to have a training set that is as large as\npossible while retaining enough data in the validation and\ntest sets for results to be meaningful. In order to account\nfor the variance of the results, we repeat each experiment\nwith four different randomized splits.\nWe set the same frequency limit for all algorithms:\nfmin= 55 Hz andfmax= 1760 Hz. The number ofProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 573(Pre Proc.)+Transform Salience/Multi f0Estim. Tracking V oicing\nDUR [10] STFT SIMM Vit(S) Energy thd.\nSAL [20] (ELF)+STFT+IF H.Sum. PCS Salience-based\nBIT [1] (ELF)+STFT+IF H.Sum. PCC+Vit(C) Probability-based\nC1 (ELF)+STFT+IF H.Sum + SIMM PCS+Vit(S) Salience-based\nC2 (ELF)+STFT+IF H.Sum + SIMM PCS Salience-based\nC3 (ELF)+STFT+IF H.Sum + SIMM PCC+Vit(C) Probability-based\nTable 1 . Overview of the methods. STFT: Short Time Fourier Transform, IF: Instantaneous Frequency estimation, ELF:\nEqual-Loudness Filters, SIMM: Smoothed Instantaneous Mixture Model, using a Source-Filter model, H.Sum: Harmonic\nSummation, HMM: Hidden Markov Model, Vit(S): Viterbi on salience function, Vit(C): Viterbi on contours, PCS: Pitch\nContour Selection, PCC: Pitch Contour Classiﬁcation.\nbins per semitone was set to 10, and the hop size was 256\nsamples (5.8 ms), except for SAL which is ﬁxed to 128\nsamples (2.9 ms) given a sampling rate of 44100 Hz.\n4.1 Datasets\nThe evaluation is conducted on two different datasets:\nMedleyDB and Orchset, converted to mono using\n(left+right)/2. MedleyDB contains 108 melody annotated\nﬁles (most between 3 and 5 minutes long), with a variety\nof instrumentation and genres. We consider two different\ndeﬁnitions of melody, MEL1 : thef0curve of the predom-\ninant melodic line drawn from a single source (MIREX\ndeﬁnition), and MEL2 : thef0curve of the predominant\nmelodic line drawn from multiple sources. We did not\nuse the third type of melody annotation included in the\ndataset, since it requires algorithms to estimate more than\none melody line (i.e. multiple concurrent lines). Orchset\ncontains 64 excerpts from symphonies, ballet suites and\nother musical forms interpreted by symphonic orchestras.\nThe deﬁnition of melody in this dataset is not restricted\nto a single instrument, with all (four) annotators agreeing\non the melody notes [4, 6]. The focus is pitch estimation,\nwhile voicing detection is less important: the proportion of\nvoiced and unvoiced frames is 93.7/6.3%.\nFollowing MIREX methodology6, the output of each\nalgorithm is compared against a ground truth sequence of\nmelody pitches. Five standard melody extraction metrics\nare computed using mireval [19]: V oicing Recall Rate\n(VR), V oicing False Alarm Rate (VFA), Raw Pitch Ac-\ncuracy (RPA), Raw Chroma Accuracy (RCA) and Overall\nAccuracy (OA). See [21] for a deﬁnition of each metric.\n4.2 Contour creation results\nBefore evaluating complete melody extraction systems, we\nexamine the initial step, by computing the recall of the\npitch contour extraction stage as performed in [1]. We\nmeasure the amount of the reference melody that is cov-\nered by the extracted contours, by selecting the best possi-\nblef0curve from them. For the MEL1 deﬁnition in Med-\nleyDB the oracle output yielded an average RPA of .66\n(σ=.22) forHS and .64 (σ=.20) forSc. In the case\nof MEL2: .64 ( σ=.20) forHS and .62 (σ=.18) for\n6http://www.music-ir.org/mirex/wiki/2014:Audio Melody ExtractionSc. For Orchset we obtain .45 ( σ=.21) forHS and .58\n(σ=.18) forSc. These results represent the highest raw\npitch accuracy that could be obtained by any of the melody\nextraction methods using contours created from HS and\nSc. Note however that these values are dependent on the\nparametrization of the contour creation stage, as described\nin [20].\n4.3 Melody extraction results\nResults for all evaluated algorithms and proposed combi-\nnations are presented in Table 2 for MedleyDB (MEL1 and\nMEL2) and in Table 3 for Orchset. The ﬁrst remark is\nthat the three proposed combination methods yield a sta-\ntistically signiﬁcantly ( t-test, signiﬁcance level α=.01)\nhigher overall accuracy (OA) than the baseline (ESS) for\nboth datasets and both melody deﬁnitions. The OA of C2\nand C3 is also signiﬁcantly higher than the OA of all other\nevaluated approaches on MedleyDB (MEL1), with the ex-\nception of SAL* (SAL with a voicing threshold optimized\nfor MedleyDB/MEL1): C2-SAL* (p = .10), C3-SAL* (p =\n.27). For the MEL2 deﬁnition C2 and C3 yield an OA that\nis signiﬁcantly higher than all compared approaches. In the\ncase of Orchset, C3 is signiﬁcantly better than C1 and C2\nexcept when increasing the voicing threshold on C2* (p =\n.78), and outperforms all compared approaches but DUR.\nAs expected, pitch related metrics (RPA, RCA) are the\nsame for C1 and DUR (they output the same pitches), and\nvoicing detection metrics (VR, VFA) are the same for C1\nand SAL. This simple combination is already able to sig-\nniﬁcantly improve overall accuracy results on MedleyDB\nin comparison to all evaluated state-of-the-art approaches\nexcept SAL, thanks to the highest pitch estimation accu-\nracy obtained by DUR, and the lowest VFA yielded by\nSAL. However, OA results are not as high as with DUR\non Orchset, due to the lower recall of SAL. An important\nremark is that DUR always obtains almost perfect recall,\nsince this method outputs almost all frames as voiced. This\nhas a huge inﬂuence on the overall accuracy on Orchset,\nsince this dataset contains 93.7% of voiced frames. How-\never, the false alarm rate is also very high, which lowers\nOA results on MedleyDB, since it contains full songs with\nlarge unvoiced portions.\nSAL and BIT perform similarly on MedleyDB, but the\nusefulness of Bittner’s method becomes evident on Orch-574 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016MedleyDB-MEL1 MedleyDB-MEL2\nMethod ν VR VFA RPA RCA OA VR VFA RPA RCA OA\nDUR - 1.0 (.01) .96 (.05) .66 (.21) .73 (.16) .36 (.16) 1.0 (.01) .95 (.06) .65 (.18) .73 (.14) .42 (.14)\nSAL .2 .78 (.13) .38 (.14) .54 (.27) .68 (.19) .54 (.17) .76 (.12) .33 (.12) .52 (.24) .66 (.17) .53 (.17)\nSAL* -1 .57 (.21) .20 (.12) .52 (.26) .68 (.19) .57 (.18) .54 (.19) .17 (.09) .49 (.23) .66 (.17) .53 (.18)\nBIT - .80 (.13) .48 (.13) .51 (.23) .61 (.19) .50( .15) .79 (.10) .44 (.13) .50 (.20) .60 (.16) .50 (.14)\nESS .2 .79 (.13) .44 (.15) .55 (.26) .68 (.19) .50 (.17) .77 (.12) .39 (.14) .53 (.23) .66 (.17) .50 (.17)\nC1 .2 .78 (.13) .38 (.14) .66 (.21) .73 (.16) .56 (.14) .76 (.12) .33 (.12) .65 (.18) .73 (.14) .57 (.13)\nC2 .2 .65 (.15) .26 (.11) .63 (.21) .70 (.16) .61 (.15) .62 (.14) .21 (.08) .61 (.19) .69 (.14) .60 (.15)\nC3 - .75 (.15) .38 (.16) .58 (.23) .64 (.19) .59 (.16) .74 (.13) .34 (.13) .58 (.19) .64 (.17) .60 (.14)\nTable 2 . Mean results (and standard deviation) over all excerpts for the ﬁve considered metrics, on MedleyDB with MEL1\nand MEL2 deﬁnition. Parameter νrefers to the voicing threshold used in the methods based on pitch-contour selection [20].\nIn the case of classiﬁcation-based methods (BIT and C3), this parameter is learnt from data. SAL* refers to the results\nobtained with the best νfor MedleyDB/MEL1.\nset: with the same candidate contours, the RPA increases\nwith respect to SAL. This classiﬁcation-based method is\nthus partially able to learn the characteristics of melody\ncontours in orchestral music. Orchset is characterized by a\nhigher melodic pitch range compared to most melody ex-\ntraction datasets which often focus on sung melodies [4].\n5. DISCUSSION\n5.1 Salience function and contour creation\nBy comparing the results obtained by SAL and C2 we can\nassess the inﬂuence of the salience function on methods\nbased on pitch contour selection [20]. SAL obtains lower\npitch related accuracies (RPA, RCA) than C2, especially\nfor orchestral music. The difference between RPA and\nRCA is also greater in SAL than compared to C2, indicat-\ning SAL makes a larger amount of octave errors, especially\nfor Orchset. This indicates that the signal representation\nyielded by the proposed pitch salience function Scis ef-\nfective at reducing octave errors, in concurrence with the\nobservations made in [9]. C3 also provides a signiﬁcantly\nhigher accuracy in comparison to BIT, showing that the\nproposed salience function helps to improve melody ex-\ntraction results also when combined with a pitch contour\nclassiﬁcation based method. Once again, this is particu-\nlarly evident in orchestral music.\nNote that even if the performance ceiling when creating\nthe pitch contours from HSon MedleyDB is 2 percentage\npoints higher than with Sc(see section 4.2), melody ex-\ntraction results are better with Sc. This is due to the fact\nthat the precision of the contour creation process with the\nproposed salience function is higher than with HS.\n5.2 Pitch tracking method\nBy comparing the results of C2 and C3 we can assess the\ninﬂuence of the pitch tracking strategy, as both methods\nuse the same contours as input. In MedleyDB, there is\nno signiﬁcant difference between both methods in terms\nof overall accuracy, but the contour classiﬁcation based\nmethod (C3) has a higher voicing recall for both melody\ndeﬁnitions, while the contour selection method (C2) has a\nbetter RPA, RCA and VFA. This agrees with the ﬁndingsfrom Bittner et al. [1] who also compared between both\npitch tracking strategies using HS as the salience func-\ntion. In the case of Orchset, the difference in OA is evident\nbetween C2-C3 (p = .004), since the classiﬁcation based\napproach tends to classify most frames as voiced, which is\nbeneﬁcial when evaluating on this dataset. However, in-\ncreasing the tolerance in C2 (C2*, ν= 1.4) provides simi-\nlar OA results: C2*-C3 (p = .78).\nAn analysis of feature importance for pitch contour\nclassiﬁcation (using Sc) revealed that salience features are\nthe most discriminative in both datasets, especially mean\nsalience. This suggests that the proposed salience func-\ntionScis successful at assigning melody contours a higher\nsalience compared to non-melody contours.\nThe most important difference between C2 and C3 is\nthat C3 allows the model to be trained to ﬁt the characteris-\ntics of a dataset, avoiding the parameter tuning necessary in\nrule-based approaches like [20]. The set of rules from [20]\nused in C2 are not tuned to orchestral music, which also\nexplains why C2 obtains a lower OA on Orchset with the\ndefault parameters. Careful tuning could considerably im-\nprove the results.\n5.3 Inﬂuence of parameters\nWe ran some additional experiments with C2 in order to\ninvestigate the inﬂuence of the parameters used to com-\npute the pitch salience function and contour creation step.\nSeveral parameters affect the creation of the salience func-\ntion [9], here we focus on the number of iterations used for\nthe source-ﬁlter decomposition and how it affects the re-\nsults obtained with the proposed salience function Sc. We\nfound that on Orchset the drop in OA when reducing the\nnumber of iterations from 50 to 10 is less than 4%. On\nMedleyDB the change in OA is less than 1% when varying\nfrom 50 to 10 iterations. We also found that DUR is gener-\nally more sensitive to the decrease in number of iterations,\nwhich is a positive aspect of our proposed approach, given\nthe high computational cost of the pitch salience estima-\ntion algorithm. For instance, DUR experiments a relative\ndecrease in OA of around 7% when going from 50 to 10\niterations (on MedleyDB with MEL1 deﬁnition). The rela-\ntive decrease in the case of C2 is less than 3%. The results\nreported in this study are based on 30 iterations.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 575Method ν VR VFA RPA RCA OA\nDUR - 1.0 (.00) .99 (.09) .68 (.20) .80 (.12) .63 (.20)\nSAL .2 .60 (.09) .40 (.23) .28 (.25) .57 (.21) .23 (.19)\nSAL* 1.4 .81 (.07) .57 (.25) .30 (.26) .57 (.21) .29 (.23)\nBIT - .69 (.14) .45 (.25) .35 (.17) .53 (.15) .37 (.16)\nESS .2 .59 (.10) .38 (.22) .29 (.24) .55 (.20) .22 (.19)\nC1 .2 .60 (.09) .40 (.22) .68 (.20) .80 (.12) .42 (.14)\nC2 .2 .49 (.11) .28 (.16) .57 (.20) .69 (.14) .39 (.16)\nC2* 1.4 .70 (.11) .44 (.21) .57 (.20) .70 (.14) .52 (.19)\nC3 - .73 (.12) .46 (.23) .53 (.19) .65 (.14) .53 (.18)\nTable 3 . Mean results (and standard deviation) over all excerpts for the ﬁve considered metrics, on Orchset. Parameter ν\nrefers to the voicing threshold used in the methods based on pitch-contour selection. In the case of the classiﬁcation-based\nmethods (BIT and C3), this parameter is learnt from data. The sign * refers to the results obtained with the best v.\nWe also analysed the inﬂuence of Gaussian ﬁltering (see\nFigure 1), by suppressing it from the salience function cre-\nation process. The effect is quite small on MedleyDB, but\nis more noticeable on Orchset where it results in a 4% point\ndrop in OA. A possible explanation is that in symphonic\nmusic many instruments contribute to the melody but are\nnot perfectly in tune. By smoothing the salience function\nwe are able to increase the pitch salience of notes played\nby orchestral sections in unison. Pitch contour extraction\nand voicing detection parameters are more relevant, how-\never. Overall accuracy generally increases on MedleyDB\nwhen the maximum allowed gap between pitches in a con-\ntour is decreased from 100 ms to 50 ms (50 ms is used in\nthe reported experiments). Since SIMM can add noise to\nunvoiced frames, using the stricter threshold of 50 ms in\nthe contour creation step can help ﬁlter some of this noise\nby preventing it from being tracked as part of a contour.\nWe also conducted a study of the effect of the voicing\nparameter (ν) on both C2 and SAL. A higher value results\nin less contours being ﬁltered as unvoiced, which is bene-\nﬁcial on Orchset. A lower value (heavier ﬁltering) is ben-\neﬁcial when evaluating against the MEL1 deﬁnition, since\nthe melody is restricted to a single instrument. Varying\nνfrom -1.4 to 1.0, the OA results with SAL range from\n.46 to .57 on MedleyDB MEL1, while with C2 they only\nrange from .56 to .61. In the case of MEL2, the OA of SAL\nranges from .46 to .54, while in the case of C2 the range is\nalso smaller, from .57 to .60. This shows that the proposed\nmethod is more robust to the selection of the voicing pa-\nrameter. While default contour creation parameters in ESS\nalready provided satisfying results for C2 on MedleyDB,\nfurther tests on Orchset showed that they could be tuned\nto go up to 0.60 overall accuracy. In fact, just modifying\nthe voicing parameter to ν= 1.4already increases the OA\nof C2 to 0.52. The highest overall accuracy obtained by\nSAL with the best parameter conﬁguration on Orchset is\n0.29 (see Table 3). This again shows that the same pitch\ncontour selection based method can be improved with the\nproposed salience function, especially on orchestral music.\n5.4 Pitch salience integration in contour creation\nThe beneﬁts of combining a source-ﬁlter model and a pitch\ncontour based tracking method have become evident bynow, and each of the proposed combination approaches has\nits advantages and disadvantages. The main advantage of\nC1 is its simplicity, and that it always yields the same RPA\nas DUR, which is always the best in all datasets. The main\ndisadvantage is that the contour creation process from SAL\ndoes not take advantage of the beneﬁts of the pitch salience\nfrom DUR. This is the reason why it becomes important to\nintegrate the source-ﬁlter model into the pitch contour cre-\nation process, as performed in C2 and C3. One difﬁculty\nof the integration is that the salience function from DUR\nneeds to be adapted to the pitch contour creation frame-\nwork. However, this improves overall accuracy in both\nMedleyDB and Orchset.\n6. CONCLUSIONS AND FUTURE WORK\nThis paper presents a comparison of melody extraction\nmethods based on source-ﬁlter models within a pitch con-\ntour based melody extraction framework. We propose\nthree different combination methods, based on a melody\noriented pitch salience function which adapts a source-\nﬁlter model to the characteristics of the tracking algo-\nrithm. The adaptation is based on the combination with a\nsalience function based on harmonic summation. We have\nshown that the proposed salience function helps improve\npitch estimation accuracy and reduce octave errors in com-\nparison to harmonic summation. This salience function\nconsistently improves the mean overall accuracy results\nwhen it substitutes harmonic summation in pitch contour\nbased tracking methods. This is true for both heuristic and\nmachine-learning-based approaches, when evaluated on a\nlarge and varied set of data. Future work deals with im-\nproving the proposed salience function, in order to further\nreduce the amount of noise in unvoiced parts.\n7. ACKNOWLEDGEMENTS\nThis work is partially supported by the European Union\nunder the PHENICX project (FP7-ICT-601166) and the\nSpanish Ministry of Economy and Competitiveness under\nCASAS project (TIN2015-70816-R) and Maria de Maeztu\nUnits of Excellence Programme (MDM-2015-0502).576 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20168. REFERENCES\n[1] R. Bittner, J. Salamon, S. Essid, and J. Bello. Melody\nextraction by contour classiﬁcation. In Proc. ISMIR ,\npages 500–506, M ´alaga, Spain, Oct. 2015.\n[2] R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Can-\nnam, and J. Bello. Medleydb: a multitrack dataset\nfor annotation-intensive mir research. In Proc. ISMIR ,\npages 155–160, Taipei, Taiwan, Oct. 2014.\n[3] D. Bogdanov, N. Wack, E. G ´omez, S. Gulati, P. Her-\nrera, O. Mayor, G. Roma, J. Salamon, J. Zapata, and\nX Serra. Essentia: an open source library for audio\nanalysis. ACM SIGMM Records , 6, 2014.\n[4] J. Bosch and E. G ´omez. Melody extraction in sym-\nphonic classical music: a comparative study of mu-\ntual agreement between humans and algorithms. In\nProc. 9th Conference on Interdisciplinary Musicology\n– CIM14 , Berlin, Germany, Dec. 2014.\n[5] J. Bosch and E. G ´omez. Melody extraction by means\nof a source-ﬁlter model and pitch contour characteri-\nzation (MIREX 2015). In 11th Music Information Re-\ntrieval Evaluation eXchange (MIREX), extended ab-\nstract , M´alaga, Spain, Oct. 2015.\n[6] J. Bosch, R Marxer, and E. G ´omez. Evalua-\ntion and combination of pitch estimation meth-\nods for melody extraction in symphonic classi-\ncal music. Journal of New Music Research , DOI:\n10.1080/09298215.2016.1182191, 2016.\n[7] J Stephen Downie. The music information retrieval\nevaluation exchange (2005-2007): A window into mu-\nsic information retrieval research. Acoustical Science\nand Technology , 29(4):247–255, 2008.\n[8] K. Dressler. Towards Computational Auditory Scene\nAnalysis: Melody Extraction from Polyphonic Music.\nInProc. CMMR , pages 319–334, London, UK, 2012.\n[9] J. Durrieu, B. David, and G. Richard. A musically\nmotivated mid-level representation for pitch estimation\nand musical audio source separation. Sel. Top. Signal\nProcess. IEEE J. , 5(6):1180–1191, 2011.\n[10] J. Durrieu, G. Richard, B. David, and C. F ´evotte.\nSource/ﬁlter model for unsupervised main melody ex-\ntraction from polyphonic audio signals. Audio, Speech,\nLang. Process. IEEE Trans. , 18(3):564–575, 2010.\n[11] D. Ellis and G. Poliner. Classiﬁcation-based melody\ntranscription. Machine Learning , 65(2-3):439–456,\n2006.\n[12] B. Fuentes, A. Liutkus, R. Badeau, and G. Richard.\nProbabilistic model for main melody extraction using\nconstant-Q transform. In Proc. IEEE ICASSP , pages\n5357–5360, Kyoto, Japan, Mar. 2012. IEEE.[13] M. Goto. A Real-Time Music-Scene-Description Sys-\ntem: Predominant-F0 Estimation for Detecting Melody\nand Bass Lines in Real-World Audio Signals. Speech\nCommunication , 43(4):311–329, September 2004.\n[14] C. Hsu and J. Jang. Singing pitch extraction by voice\nvibrato/tremolo estimation and instrument partial dele-\ntion. In Proc. ISMIR , pages 525–530, Utrecht, Nether-\nlands, Aug. 2010.\n[15] A. Klapuri. Multiple fundamental frequency estima-\ntion by summing harmonic amplitudes. In Proc. IS-\nMIR, pages 216–221, Victoria, Canada, Oct. 2006.\n[16] M. Marolt. Audio melody extraction based on timbral\nsimilarity of melodic fragments. In EUROCON 2005 ,\nvolume 2, pages 1288–1291. IEEE, 2005.\n[17] A. Ozerov, P. Philippe, F. Bimbot, and R. Gribon-\nval. Adaptation of bayesian models for single-channel\nsource separation and its application to voice/music\nseparation in popular songs. Audio, Speech, and Lan-\nguage Processing, IEEE Transactions on , 15(5):1564–\n1578, 2007.\n[18] R. Paiva, T. Mendes, and A. Cardoso. Melody detec-\ntion in polyphonic musical signals: Exploiting per-\nceptual rules, note salience, and melodic smoothness.\nComputer Music Journal , 30(4):80–98, 2006.\n[19] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon,\nO. Nieto, D. Liang, and D. P. W. Ellis. mir eval: A\ntransparent implementation of common MIR metrics.\nInProc. ISMIR , pages 367–372, Taipei, Taiwan, Oct.\n2014.\n[20] J. Salamon and E. G ´omez. Melody extraction from\npolyphonic music signals using pitch contour charac-\nteristics. IEEE Trans. Audio. Speech. Lang. Processing ,\n20(6):1759–1770, 2012.\n[21] J. Salamon, E. G ´omez, D. Ellis, and G. Richard.\nMelody Extraction from Polyphonic Music Signals:\nApproaches, applications, and challenges. IEEE Signal\nProcess. Mag. , 31:118–134, 2014.\n[22] J. Salamon, G. Peeters, and A. R ¨obel. Statistical char-\nacterisation of melodic pitch contours and its applica-\ntion for melody extraction. In 13th Int. Soc. for Music\nInfo. Retrieval Conf. , pages 187–192, Porto, Portugal,\nOct. 2012.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 577"
    },
    {
        "title": "A Higher-Dimensional Expansion of Affective Norms for English Terms for Music Tagging.",
        "author": [
            "Michele Buccoli",
            "Massimiliano Zanoni",
            "György Fazekas",
            "Augusto Sarti",
            "Mark B. Sandler"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1415194",
        "url": "https://doi.org/10.5281/zenodo.1415194",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/253_Paper.pdf",
        "abstract": "The Valence, Arousal and Dominance (VAD) model for emotion representation is widely used in music analysis. The ANEW dataset is composed of more than 2000 emo- tion related descriptors annotated in the VAD space. How- ever, due to the low number of dimensions of the VAD model, the distribution of terms of the ANEW dataset tends to be compact and cluttered. In this work, we aim at finding a possibly higher-dimensional transformation of the VAD space, where the terms of the ANEW dataset are better organised conceptually and bear more relevance to music tagging. Our approach involves the use of a kernel expan- sion of the ANEW dataset to exploit a higher number of dimensions, and the application of distance learning tech- niques to find a distance metric that is consistent with the semantic similarity among terms. In order to train the dis- tance learning algorithms, we collect information on the semantic similarity from human annotation and editorial tags. We evaluate the quality of the method by clustering the terms in the found high-dimensional domain. Our ap- proach exhibits promising results with objective and sub- jective performance metrics, showing that a higher dimen- sional space could be useful to model semantic similarity among terms of the ANEW dataset.",
        "zenodo_id": 1415194,
        "dblp_key": "conf/ismir/BuccoliZFSS16",
        "content": "A HIGHER-DIMENSIONAL EXPANSION OF AFFECTIVE NORMS FOR\nENGLISH TERMS FOR MUSIC TAGGING\nMichele Buccoli1, Massimiliano Zanoni1, Gy ¨orgy Fazekas2\nAugusto Sarti1, Mark Sandler2\n1Dipartimento di Elettronica, Informatica e Bioingegneria, Politecnico di Milano, Italy\n2Centre For Digital Music, Queen Mary, University of London, UK\nmichele.buccoli@polimi.it, g.fazekas@qmul.ac.uk\nABSTRACT\nThe Valence, Arousal and Dominance (V AD) model for\nemotion representation is widely used in music analysis.\nThe ANEW dataset is composed of more than 2000 emo-\ntion related descriptors annotated in the V AD space. How-\never, due to the low number of dimensions of the V AD\nmodel, the distribution of terms of the ANEW dataset tends\nto be compact and cluttered. In this work, we aim at ﬁnding\na possibly higher-dimensional transformation of the V AD\nspace, where the terms of the ANEW dataset are better\norganised conceptually and bear more relevance to music\ntagging. Our approach involves the use of a kernel expan-\nsion of the ANEW dataset to exploit a higher number of\ndimensions, and the application of distance learning tech-\nniques to ﬁnd a distance metric that is consistent with the\nsemantic similarity among terms. In order to train the dis-\ntance learning algorithms, we collect information on the\nsemantic similarity from human annotation and editorial\ntags. We evaluate the quality of the method by clustering\nthe terms in the found high-dimensional domain. Our ap-\nproach exhibits promising results with objective and sub-\njective performance metrics, showing that a higher dimen-\nsional space could be useful to model semantic similarity\namong terms of the ANEW dataset.\n1. INTRODUCTION\nOne of the fundamental properties of music is the ability\nto convey emotions [27]. Consequently, there is a great\ninterest in representing and classifying music according to\nemotions in areas like music information retrieval, music\nrecommendation and personalisation [11–13, 27]. It has\nbeen proven that listeners enjoy looking for and discov-\nering music using mood-based queries, which represents\n33% of music queries according to [13]. This is an im-\nportant reason that urged psychologist and musicologist to\ninvestigate different paradigms for the representation and\nc/circlecopyrtMichele Buccoli, Massimiliano Zanoni, Gy ¨orgy\nFazekas, Augusto Sarti, Mark Sandler. Licensed under a Creative Com-\nmons Attribution 4.0 International License (CC BY 4.0). Attribution:\nMichele Buccoli, Massimiliano Zanoni, Gy ¨orgy Fazekas, Augusto Sarti,\nMark Sandler. “A higher-dimensional expansion of affective norms for\nEnglish terms for music tagging”, 17th International Society for Music\nInformation Retrieval Conference, 2016.\n0.0 0.2 0.4 0.6 0.8 1.0\nValence0.00.20.40.60.81.0ArousalFigure 1 : Distribution of the ANEW dataset in the V A\nspace\nmodelling of emotion related descriptors [11–13].\nDimensional approaches to emotion conceptualisation\nfocus on describing emotional states in a continuous space,\nwhere emotion states are represented as points or distribu-\ntions in an N-dimensional space. Speciﬁc emotion terms\ncan be localised in such continuous space and it is possible\nto deﬁne a metric in a way that the distance between points\nis proportional to the semantic distance between emotions.\nThe most inﬂuential dimensional models so far [1, 25] are\nproposed by Russell [19] and Thayer [24]. Russell de-\nvised a circumplex model of affect which consists of a two-\ndimensional, circular structure involving the dimensions of\narousal (A), linked to the degree of activation or excite-\nment, and valence (V), linked to the degree of pleasant-\nness. A third dimension related to dominance (D) was later\nproposed to express the degree of control and to possibly\ndistinguish different and overlapping moods. [8, 21].\nThe increasing need of a continuous affective model\nled to the creation of the ANEW dataset [4] for Psycho-\nlogical research. This dataset is composed of 2,476 En-\nglish words with positions in the V AD space. Despite it\nis a generic dataset, the large amount of terms in ANEW\nmakes it a powerful resource also for the Music Emotion\nRecognition (MER) community, including applications for316Type Symbol Num\nTermsDetails\nTerms\nDatasetDANEW 2,476 Mean in V , A, D di-\nmensions\nImplicit\nDistanceMILM 10K 240,\n450Tag compact rep-\nresentation from\nan LSA with\nk= 10,20,50,100\ncomponents\nExplicit\nDistanceMHD 180 Human annotation of\nsemantic similarity\nbetween terms from\nANEW\nExplicit\nClusteringMHC 100 Human clustering of a\nsubset of ANEW\nTable 1 : Summary of the collected data\nautomatic music annotation and retrieval [5, 20]. Unfortu-\nnately, ANEW terms in the V AD space tend to have a very\nuniform and compact distribution concentrated around the\ncentre of the space as shown in Figure 1 for the V A sub-\nspace. Although the use of a substantial set of terms pro-\nvides a very representative model of a large variety of emo-\ntions, to deal with a compact and cluttered distribution can\nbe problematic in many musical applications. For this rea-\nson, typically only a subset of the terms is used, leading\nto a loss in the exhaustiveness of the model. A higher-\ndimensional mood space drawn from the ANEW dataset,\nwhere the terms are distributed following some concep-\ntual organisation can beneﬁt several applications. These\ninclude semantic music annotation, recommendation and\nmood-based music retrieval [2].\nIn this study, we investigate the construction of higher-\ndimensional emotional spaces from ANEW by means of\nkernel expansion techniques applied to the V AD space. Al-\nthough the transformation has the effect to produce a more\nsparse distribution of terms, it is not clear if the semantic\ndistance between concepts is well represented by the met-\nric in the new space. To solve this problem, we ﬁrst aim to\nﬁnd a distance reﬂecting conceptual organisation of terms\nby applying distance learning techniques [3,10,26]. Given\nsome constraints between terms, these methods search for\na linear transformation of the space that is semantically rel-\nevant. That is, the ideal learnt distance closely correlates\nwith semantic differentiae given by users in a speciﬁc task\nfor a subset of the ANEW terms. We generate the set of\nconstraints using “a priori” information collected through\na subjective test where participants were asked to specify\nthe semantic similarity between pairs of terms in the con-\ntext of music. We then perform Latent Semantic Analy-\nsis on emotion related annotations for a large set of music\npieces.\nUnder the hypothesis that the terms may be improved\nby this transformation, we validate the approach by cluster-\ning in the new high-dimensional space. We subsequently\nevaluate the resulting clusters with objective and subjec-\ntive metrics. Tests show promising results given these new\nlearnt distance metrics and provide an insight into how a\nbetter conﬁguration in the high-dimensional space can be\nachieved.\nVA\nVAD\nILM 10\nILM 20\nILM 50\nILM 100\nnorm. ILM 10\nnorm. ILM 20\nnorm. ILM 50\nnorm. ILM 100\nHuman Dist\n0.10.20.30.40.50.60.70.80.91.0Figure 2 : Absolute Pearson Correlation of the self-\nsimilarity matrices computed or collected from different\nsources of data.\n2. DATASET CONSTRUCTION\nIn this section, we describe three datasets created to pro-\nvide constraints for semi-supervised learning as well as to\nvalidate our approach. A summary of the collected data is\nprovided in Table 1.\n2.1 Terms in the ANEW Dataset\nThe terms in the ANEW dataset [4] is already annotated for\nthe Valence, Arousal and Dominance dimensions with a\nvalue between 0 and 10 by Psychology class students. For\neach term, we consider the normalised average (between 0\nand 1) of the annotations.\n2.2 Implicit Distance Annotation\nWe compute a similarity distance matrix among emotion\nrelated descriptors by performing Latent Semantic Anal-\nysis (LSA) on the I-Like-Music1(ILM) dataset denoted\nILM10K. This is composed of 10,199 tracks from com-\nmercial music annotated with crowed sourced editorial and\nsocial tags [2,20] with weights corresponding to the preva-\nlence of each tag.\nFrom the tags in ILM10K, we ﬁrst discard the tags that\nare not included in the ANEW dataset. We then ﬁlter the\ntags that are rarely used by means of two thresholds on\nthe number of times the tag is used in ILM10K: 15times,\nleading to a set of T= 240 terms and 5times leading to\nT= 450 terms. We build a track-tag matrix using the re-\nmaining tags and compute a k-component approximation\nof this matrix via LSA, keeping different numbers of com-\nponentskto test different degrees of approximations. We\nsetk= 10,20,50,100components to produce the set of\nmatrices DILM 10K∈RT×k.\nFrom DILM 10K, we compute the similarity between\ntags as the Euclidean distance between the correspondent\n1http://www.ilikemusic.com/Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 317Figure 3 : Block diagram of the clustering approach\nrows, producing the matrix MILM 10K. We also com-\npute the normalised matrix ˆDILM 10K, composed by the\nL2normalised rows of DILM 10K, so that the Euclidean\ndistance of the former is equal to the cosine distance of the\nlatter and compute the (cosine) distance matrix ˆMILM 10K.\n2.3 Human Distance Annotation\nWe conducted an online survey and asked annotators to de-\nﬁne the perceived mood similarity in the context of music\nbetween pairs of descriptors with a value between 0(very\nsimilar) and 1(not similar at all). 504 people participated\nin the survey. From the data we kept only the terms that\nreceived at least 2 annotations leading to T= 180 and we\ncompose the sparse matrix MHD.\n2.4 Human Clustering Annotation\nWe conducted a second online survey and asked annota-\ntors to group the set of top 100descriptors in the dataset.\n15people participated in the survey, leading to the matrix\nMHCwithT= 100 terms, where each entry (i,j)indi-\ncates the number of people that grouped together the i-th\nandj-th terms.\n2.5 Further Considerations\nIn Figure 2 we show the absolute Pearson correlations be-\ntween the similarity matrices from the different sources of\ndata we have mentioned so far. The human distance anno-\ntation exhibits a modest correlation with the Euclidean dis-\ntance deﬁned in the V A or V AD space. The annotations on\ndistance between terms is also somewhat correlated with\nthe distance that can be inferred from editorial tags. In the\nrest of this study, we aim to ﬁnd a space where the deﬁned\ndistance metric provides a better representation of the per-\nceived similarity in musical emotion.\n3. HIGH-DIMENSIONAL SPACE LEARNING\nThe block diagram of our approach is shown in Figure 3.\nThe V A or V AD coordinates for the ANEW dataset are\nprocessed using kernel expansion. In order to better rep-\nresent the perceived similarity, we ﬁrst rotate and translate\nthe expanded dataset by means of distance learning algo-\nrithms.We then use the collected annotations to build con-\nstraints for learning the metric distance.\nUnsup.SemiSup.Dist. Learn.Dist. Learn. IPDist. Learn. RCA Dist. Learn. NCA0.2\n0.00.20.40.60.8SilhouetteFigure 4 : Boxplot of the Silhouette indices for the differ-\nent scenarios\n3.1 Kernel Expansion\nGiven a generic vector x∈RN, we deﬁne its kernel ex-\npansion the vector φx= Φ(x)∈RP, withP≥N,\nthat is the result of the mapping function Φ. We expand\nour original dataset (both V A and V AD coordinates) with\nthe following mapping functions: normalisation with re-\nspect to the L2norm, to include the cosine distance, i.e.,\nΦ(x) =x/|x|2; polynomial expansion with degrees two\nand three, to include nonlinear distance, i.e., Φ(x)2=\n[x1,...,x N,x2\n1,...,x2\nN,x1x2,...,x 1xN,...,x N−1,xN]for\nthe polynomial expansion of degree two, where x1,...,x N\nare the components of x(the third degree polynomial ex-\npansion is computed accordingly); approximate feature\nmap of a Radial Basis Function (RBF) kernel by Monte\nCarlo expansion of its Fourier Transform [17].\n3.2 Constrains Building\nIn order to facilitate the training of distance learning al-\ngorithms, we use similarity matrices obtained from hu-\nman annotations introduced in Sections 2.2, 2.3, 2.4. Our\nmethod relies on the deﬁnition of a set of Must-Link (ML)\nand Cannot-Link (CL) constraints. Treating the similar-\nity as a tendency of terms to be grouped together, ML\nconstraints force a pair of terms to be in the same group,\nwhereas CL constraints force a pair of terms to be in differ-\nent groups. In the distance learning techniques, the space\nis transformed such that the pairs of points of the ML set\nare closer together while the pairs of points in the CL set\nare far apart.\nAs far as the ILM10K dataset is concerned, we compute\nthe ML and CL constraints by computing the mean value\nµand standard deviation σof the matrices MILM 10K\nand empirically deﬁning two thresholds thl=µ−2σ,\nthh=µ+ 2σ. We compose the set of ML constraints\nby considering those pairs of terms which are close in the\nspace deﬁned by LSA, hence whose distance is lower than\nthl. Similarly, we compose the CL set with those pairs of\nterms that are far from each other, i.e., whose distance is\nhigher thanthh. We compose another set of constrains in\nthe same way from the matrices ˆMILM 10K.318 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20160.0 0.1 0.2\nCompleteness0.00.10.20.30.4Homogeneity\n0.050.15\n0.15Dist. Learn. IP\nDist. Learn. RCA\nDist. Learn. NCA\nSemiSup.\nUnsup.Figure 5 : Completeness and Homogeneity results for\nthe comparison of clusters with those obtained from the\nILM10K dataset. Gray contour lines indicate the V-score\n0.6 0.7 0.8 0.9 1.0\nPrecision0.20.30.40.50.60.7Recall\n0.3000.5000.700Dist. Learn. IP\nDist. Learn. RCA\nDist. Learn. NCA\nSemiSup.\nUnsup.\nFigure 6 : Precision and Recall results for the compari-\nson of clusters with those obtained from human annotation.\nGray contour lines indicate the F-measure.\nWe use a similar approach to compose the ML and\nCL constraints from the Human Distance annotations. We\ncompute the mean value µand standard deviation σof the\nannotations in the matrix MHDand we empirically deﬁne\nthe thresholds thl=µ−σ, th h=µ+σ.\nAs far as the Human Clustering annotations are con-\ncerned, we compute the mean value µand standard devi-\nationσof the non-zero entries of the matrix MHC, i.e.,\nof the number of people who annotated two terms as be-\nlonging to the same clusters, and deﬁne the soft threshold\nth={µ−σ}. We compose ML with the pair of terms that\nhave been grouped together by more than thpeople and\nwe compose CL with the zero entries of MHC.\n3.3 Distance Learning\nGiven x,y∈RNtwo generic vectors, we can weight the\ncontribution of each components and the inter-correlation\namong them by deﬁning a Mahalanobis (symmetric,square) matrix A∈RN×Nand computing:\nd(x,y) =/radicalbig\n(x−y)/intercalA(x−y)\n=/radicalbig\n(Lx−Ly)/intercal(Lx−Ly),(1)\nthat is the Euclidean distance between the projected vec-\ntors over the subspace deﬁned by L, with L/intercalL=A.\nDistance metric learning is the sub-ﬁeld of machine\nlearning that aims at ﬁnding the best subspace L, from a set\nof constraints. Using the constraints as described in Sec-\ntion 3.2, we compute a set of subspaces for each combina-\ntion of input, kernel expansions and constraints. It is worth\nremembering that with the Mahalanobis distance, only\ntranslation and rotation operations are performed. How-\never, the application of kernels on the sample data allows\nnonlinear transformation of the original space of data.\nWe employ the following distance metric learning algo-\nrithms [6]:\n•Iterative Projection [26] (IP), computes the Maha-\nlanobis matrix by means of an iterative minimisation\nof the distance of the ML data with the constraint to\nkeep CL data far apart\n•Relative Components Analysis [3] (RCA), learns a\nMahalanobis matrix that assigns large weights to\nmore relevant components and low weights to irrele-\nvant ones, by using chunklets, i.e., subset of data that\nbelong to the same group (i.e., have been deﬁned in\nthe ML set)\n•Neighbourhood Components Analysis [10] (NCA)\nis a component analysis that computes an optimal\nMahalanobis matrix for the K-nearest neighbour\nclustering algorithm.\n4. EXPERIMENTAL SETUP AND EVALUATION\nAs previously discussed, our aim is to ﬁnd a transforma-\ntion of the ANEW space with improved conceptual organ-\nisation of terms that is relevant in a musical context. We\nexpect terms that are semantically similar in this context\nto be close and dissimilar terms to be far apart. For this\nreason, we validate our approach by clustering in the trans-\nformed space.\nSpeciﬁcally, we perform three evaluations: i) we eval-\nuate the resulting clusters by mean of the Silhouette qual-\nity index as objective metric; ii) we compare the result-\ning clusters with those obtained by clustering the tags of\nthe ILM10K dataset; iii) we compare the resulting clusters\nwith those obtained from manual annotation.\nIn the ILM10K dataset we consider the results obtained\nwith bothT= 240 and450tags. This is because the for-\nmer is less noisy, while the latter provides more informa-\ntion and constrains. We will then specify in the single cases\nwhich dataset achieved the mentioned results.\n4.1 Unsupervised and Semi-Supervised Clustering\nIn order to provide a robust evaluation, we apply several\nclustering techniques. We experimentally choose to re-\ntrieve the 6best representative clusters, given the largeProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 319Scenario Features Algorithm Constraints Silhouette\nUnsupervised Norm. V A K-Means 0.5287\nSemi-Supervised Norm. V A SC MHC,th=µ−σ 0.5240\nIP Distance Learning V AD kmeans MHD 0.5432\nRCA Distance Learning V AD, poly 3 degree AHC MHC,th=µ−σ 0.6864\nNCA Distance Learning V A, poly 2 degree kmeans ˆMILM 10K,T= 240 ,k= 10 0.5456\nTable 2 : Best results for the Silhouette metric\nScenario Features Algorithm Constraints ILM10K Compl. Hom. V-score\nUnsup. V A, poly 3 degree SC AHCˆMILM 10K,k= 100 0.0877 0.1334 0.1058\nSemi-Sup. V AD poly 2 degree AHC MHD AHCˆMILM 10K,k= 100 0.0955 0.1347 0.1118\nIP Dist. V A SC MHD AHCˆMILM 10K,k= 100 0.0929 0.1371 0.1107\nRCA Dist. V AD kmeans MHD AHCˆMILM 10K,k= 100 0.0869 0.1290 0.1038\nNCA Dist. V A, poly 2 degree AHC MHD AHCˆMILM 10K,k= 100 0.0959 0.1421 0.1145\nTable 3 : Best results of the Homogeneity, Separation and V-measure metrics for the comparison with the clusters generated\nby ILM10K dataset (240 terms)\nnumber of conﬁgurations we need to test. We employ the\nfollowing algorithms resulted to be effective in the context\nof music tag analysis and aggregation (from [16]):\n•K-Means [14] is the common unsupervised cluster-\ning algorithm;\n•Semi-Supervised Non-negative Matrix Factorisation\n[7] (SS-NMF) applies NMF techniques to the self-\ndistance matrix of the samples;\n•Spectral Clustering [22] (SC) employs a low-\ndimension reduction of the similarity matrix be-\ntween samples before applying the K-means algo-\nrithm;\n•Agglomerative Hierarchical Clustering [23] (AHC)\nperforms a bottom-up clustering: initially every\nsample is a cluster, then the closest clusters merge\ntogether until the ﬁnal number of clusters is reached.\nIn this study we use the SS-NMF, SC and AHC algorithms\nin both unsupervised and semi-supervised fashion. In\nsemi-supervised algorithms, the clustering can be guided\nby constraints. Here we use the ML and CL constraints\ncomputed in Section 3.2.\nIn order to validate the actual contribution of the dis-\ntance learning techniques, we compare the results of our\napproach with the results obtained with both unsupervised\nand semi-supervised clustering of the ANEW dataset.\nHence, we have three scenarios: i) unsupervised cluster-\ning of the transformed (but not learned) space (Unsup.) ;\nii) the semi-supervised clustering of the transformed (but\nnot learned) space (SemiSup.); iii) our approach, that is the\nunsupervised clustering of the learned space (Dist. Learn).\nPlease note that the ﬁrst scenario also includes the cluster-\ning of non-transformed space using the Euclidean distance.\n4.2 Objective metrics\nWe evaluate the objective quality of clustering by using the\nSilhouette index that is deﬁned as [16]:\nsilhouette =1\n|D|/summationdisplay\ns∈Db−a\nmax(a,b)∈[−1,1], (2)whereaandbare the mean distance between the s-th sam-\nple and all other points in the same cluster and in the next\nnearest cluster, respectively and Dis the dataset of sam-\nples. High (positive) values of Silhouette indicate dense\nwell-separated clusters, values around 0indicate overlap-\nping clusters and low (or negative) values indicate incor-\nrect clusters. In Figure 4 we show the boxplots of Silhou-\nette metric for the different scenarios, while in Table 2 we\nshow the conﬁgurations that generate the best results for\neach scenario.\nIt is clear that the application of Distance Learning\ntechniques outperforms unsupervised and semi-supervised\ntechniques, even with different conﬁgurations of kernels,\nalgorithms and constraints. In particular, the best perfor-\nmance is obtained with the AHC over the third degree\npolynomial expansion of the V AD dataset, with a trans-\nlation learned using the RCA technique with Human Clus-\ntering.\nWe can also notice that the semi-supervised scenario\nperforms on average worse than the unsupervised scenario.\nThis is because the Silhouette index evaluates the resulting\nclusters with respect of the position of the input data, that\nis not moved from the original position. The estimated\nclusters are therefore more noisy, which conﬁrms the ad-\nvantage of distance learning techniques to transform the\nspace.\n4.3 Subjective metrics - Comparison with Clustering\nof ILM10K\nWe compare the clusters obtained with the different ap-\nproaches with the clusters obtained from ILM10K data.\nWe consider the homogeneity and completeness metrics\n[18]. The former evaluates whether each estimated clus-\nter contains only members of a group in the ground truth,\nwhile the latter estimates whether the samples of a given\ngroup belongs to the same estimated cluster. We also con-\nsider the V-measure, i.e., the harmonic mean of homogene-\nity and completeness. To avoid overﬁtting issues, we eval-\nuate only the conﬁgurations that are not trained with the\nconstraints generated from the ILM10K dataset.320 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Scenario Features Algorithm Constraints P R F\nUnsup. V AD, RBF kmeans 0.8012 0.3512 0.4883\nSemi-Sup. V AD, RBF AHC MHD 0.7646 0.3986 0.5240\nIP Dist. V AD, RBF AHC ˆMILM 10K,T= 240 ,k= 100 0.6979 0.3915 05016\nRCA Dist. V A, poly 3 degree AHC ˆMILM 10K,T= 240 ,k= 50 0.6622 0.7241 0.6918\nNCA Dist. V AD, RBF kmeans ˆMILM 10K,T= 240 ,k= 10 0.7971 0.4124 0.5289\nTable 4 : Best results for the Precision, Recall and F-measure metric for the comparison with the human annotated clusters\nWe show the distribution of results for the different sce-\nnarios in Figure 5. We notice the results are fairly low,\nshowing that the organisation given by the expanded and\npossibly learnt ANEW dataset is very different from that\nobtained from editorial tags on a real music annotation ap-\nplication. This conﬁrms the necessity to ﬁnd a space for the\nANEW dataset which is more useful for MIR applications.\nHowever, it is clear that our approach can only slightly im-\nprove the task. In Table 3 we list the conﬁgurations that\nlead to the best performance. These are all obtained with\nthe 240-term subset of the ILM10K dataset. The AHC over\nthe normalised ILM10K dataset with 100 components is\nthe one that best matches all scenarios, from which we can\ninfer that the clustering of the ANEW dataset resembles a\nclustering based on cosine distance with a large number of\ncomponents. The best results is reached using NCA tech-\nnique with a AHC over the polynomial- expanded dataset,\nby means of annotated distance again.\n4.4 Subjective metrics - Comparison with Human\nAnnotations\nWe ﬁnally compare the obtained clusters with those col-\nlected from human annotations. Since the correctly\ngrouped terms, i.e., the amount of True Positive ( TP) ex-\namples are more speciﬁc and relevant than the correctly\nnon-grouped ones (True Negative, TN), we consider the\nPrecision (P) and Recall ( R) metrics that focus on the\nnumber ofTPexamples. The Precision indicates the ra-\ntios of True Positive over the total estimated assignments,\ni.e.,P=|TP|/(|TP|+|FP|), while the Recall deﬁnes\nthe number of TPover the total assignments in the ground\ntruth, i.e.,R=|TP|/(|TP|+|FN|). We also consider\nthe F-measure ( F) as the harmonic mean of the two met-\nrics [15]. To avoid overﬁtting, we evaluate only conﬁgu-\nrations that are not trained with the constraints generated\nfrom the human annotations on clustering.\nIn Figure 6 we show the distribution of results for the\ndifferent scenarios and conﬁgurations. In general, Preci-\nsion is higher than Recall, showing that the estimated clus-\ntering is not capable of retrieving all the corrected groups.\nThe F-measures are mostly distributed between 0.3 and\n0.5, which is an average result. We can clearly see that\nsome conﬁguration from RCA Distance Learning are able\nto improve the average Recall and improve the F-measure\nreaching almost at 0.7. In Table 4 we list the best con-\nﬁgurations for each scenarios. The RCA Distance Learn-\ning technique clearly outperforms the other approaches and\nmatches very closely the human annotations, i.e., the hu-\nman way to organise the emotional-related descriptors. Wecan notice that once again, the best results are obtained\nby using the AHC over some polynomial expansion of the\ndataset. However, the constraints based on distance anno-\ntations are less helpful for emulating the human organisa-\ntion of terms than the ones based on editorial and social\ntags (ILM10K).\n5. CONCLUSIONS\nWe introduced a novel approach consisting of kernel ex-\npansion, constraint building from music task speciﬁc hu-\nman annotation and ﬁnally distance learning to transform\nthe ANEW dataset and obtain a distribution of terms with\nbetter conceptual organisation compared to the conven-\ntional V A or V AD space. This facilitates applications\nthat require computer representation of music emotions,\nincluding music emotion recognition and music tagging,\nmusic recommendation or interactive musical applications\nsuch as [9] and [2].\nAverage and maximum results presented in Section 4\nprove that distance learning techniques are effective in\nthe improvement of the organisation of concepts of the\nANEW dataset. In particular, hierarchical clustering of\nthe RCA-learned space based on the polynomial expan-\nsion of V A/V AD space outperforms the other conﬁgura-\ntions. The paper also introduces a new task along with a\nset of techniques that proved successful as a ﬁrst attempt.\nThis provides useful insight into improving the organisa-\ntion of mood terms to better reﬂect application speciﬁc\nsemantic spaces. Future work involves the collection of\nadditional “a priori” information for improving the robust-\nness of evaluation, as well as further assessment of the con-\nstructed high-dimensional space within other MIR applica-\ntions.\n6. ACKNOWLEDGEMENTS\nThis work was part funded by the FAST IMPACt EPSRC\nGrant EP/L019981/1 and the EC H2020 research and inno-\nvation grant AudioCommons (688382). Sandler acknowl-\nedges the support of the Royal Society as a recipient of a\nWolfson Research Merit Award.\n7. REFERENCES\n[1] A. Aljanaki, Y .-H. Yang, and M. Soleymani. Emotion\nin music task at mediaeval 2015. In Working Notes Pro-\nceedings of the MediaEval 2015 Workshop , 2015.\n[2] A. Allik, G. Fazekas, M. Barthet, and M. Sandler. my-\nmoodplay: an interactive mood-based music discovery\napp. In proc. 2nd Web Audio Conference (WAC) , 2016.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 321[3] A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall.\nLearning distance functions using equivalence rela-\ntions. In Proc. of the 20th International Conference on\nMachine Learning (ICML) , 2003.\n[4] M. M. Bradley and P. J. Lang. Affective norms for en-\nglish words (anew): Stimuli, instruction manual and\naffective ratings. Technical report, C-1, The Center for\nResearch in Psychophysiology, University of Florida.,\nGainesville, FL, USA, 1999.\n[5] M. Buccoli, M. Zanoni, A. Sarti, and S. Tubaro. A mu-\nsic search engine based on semantic text-based query.\nInIEEE International Workshop on Multimedia Signal\nProcessing (MMSP) , 2013.\n[6] C.J. Carey and Y . Tang. metric-learn python package.\nhttp://github.com/all-umass/metric-learn, 2015.\n[7] Y . Chen, M. Rege, M. Dong, and J. Hua. Non-negative\nmatrix factorization for semi-supervised data cluster-\ning. Knowledge and Info. Systems , 17(3):355–379,\n2008.\n[8] R. Cowie, G. McKeown, and E. Douglas-Cowie. Trac-\ning emotion: an overview. International Journal of\nSynthetic Emotions, Special Issue on Beneﬁts and Lim-\nitations of Continuous Representations of Emotions ,\npages 1–17, 2012.\n[9] G. Fazekas, M. Barthet, and M. Sandler. Novel meth-\nods in facilitating audience and performer interaction\nusing the mood conductor framework. Lecture Notes\nIn Computer Science (LNCS): Sound, Music, and Mo-\ntion, 8905:122–147, 2014.\n[10] J. Goldberger, G. E. Hinton, S. T. Roweis, and\nR. Salakhutdinov. Neighbourhood components analy-\nsis. In L. K. Saul, Y . Weiss, and L. Bottou, editors, Ad-\nvances in Neural Information Processing Systems 17 ,\npages 513–520. MIT Press, 2005.\n[11] P. N. Juslin and J. A. Sloboda, editors. Music and Emo-\ntion Theory and Research . Series in Affective Science.\nOxford University Press, 2001.\n[12] P. N. Juslin and J. A. Sloboda. Handbook of Music and\nEmotion: Theory, Research, Applications . OUP Ox-\nford, 2011.\n[13] J. A. Lee and J. S. Downie. Survey of music informa-\ntion needs, uses, and seeking behaviors: preliminary\nﬁndings. In Proc. of the 5th International Society for\nMusic Information Retrieval (ISMIR) , 2004.\n[14] J. MacQueen et al. Some methods for classiﬁcation\nand analysis of multivariate observations. In Proc. of\nthe 5th Berkeley Symposium on Mathematical Statis-\ntics and Probability . Oakland, CA, USA., 1967.\n[15] C. D. Manning, P. Raghavan, H. Sch ¨utze, et al. Intro-\nduction to information retrieval , volume 1. Cambridge\nuniversity press Cambridge, 2008.[16] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duches-\nnay. Scikit-learn: Machine learning in Python. Journal\nof Machine Learning Research , 12:2825–2830, 2011.\n[17] A. Rahimi and B. Recht. Weighted sums of random\nkitchen sinks: Replacing minimization with random-\nization in learning. In Advances in Neural Information\nProcessing Systems 22 , 2009.\n[18] A. Rosenberg and J. Hirschberg. V-measure: A con-\nditional entropy-based external cluster evaluation mea-\nsure. In Proc. of the Conference on Empirical Methods\nin Natural Language Processing and Computational\nNatural Language Learning (EMNLP-CoNLL) , 2007.\n[19] J. A. Russell. A circumplex model of affect. Journal of\npersonality and social psychology , 39(6):1161–1178,\n1980.\n[20] P. Saari, G. Fazekas, T. Eerola, M. Barthet, O. Lartillot,\nand M. Sandler. Genre-adaptive semantic computing\nand audio-based modelling for music mood annotation.\nIEEE Transactions on Affective Computing , (99):1–1,\n2015.\n[21] K. R. Scherer. Which emotion can be induced by mu-\nsic? what are the underlying mechanisms? and how\ncan we measure them? Journal of New Music Re-\nsearch , 33(5):239–251, 2004.\n[22] J. Shi and J. Malik. Normalized cuts and image seg-\nmentation. Pattern Analysis and Machine Intelligence,\nIEEE Transactions on , 22(8):888–905, 2000.\n[23] R. Sibson. Slink: an optimally efﬁcient algorithm for\nthe single-link cluster method. The Computer Journal ,\n16(1):30–34, 1973.\n[24] J.F. Thayer. Multiple indicators of affective responses\nto music. Dissertation Abst. Int. , 47(12), 1986.\n[25] F. Weninger, F. Eyben, and B. Schuller. On-line\ncontinuous-time music mood regression with deep re-\ncurrent neural networks. In proc. IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , 2014.\n[26] E. P. Xing, A. Y . Ng, M. I. Jordan, and S. Russell.\nDistance metric learning with application to clustering\nwith side-information. Advances in neural information\nprocessing systems , 15:505–512, 2003.\n[27] Y .-H. Yang and H. H. Chen. Music Emotion Recogni-\ntion. CRC Press, 2011.322 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Two (Note) Heads Are Better Than One: Pen-Based Multimodal Interaction with Music Scores.",
        "author": [
            "Jorge Calvo-Zaragoza",
            "David Rizo",
            "José Manuel Iñesta Quereda"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417469",
        "url": "https://doi.org/10.5281/zenodo.1417469",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/006_Paper.pdf",
        "abstract": "Digitizing early music sources requires new ways of deal- ing with musical documents. Assuming that current tech- nologies cannot guarantee a perfect automatic transcrip- tion, our intention is to develop an interactive system in which user and software collaborate to complete the task. Since conventional score post-editing might be tedious, the user is allowed to interact using an electronic pen. Al- though this provides a more ergonomic interface, this in- teraction must be decoded as well. In our framework, the user traces the symbols using the electronic pen over a digital surface, which provides both the underlying image (offline data) and the drawing made by the e-pen (online data) to improve classification. Applying this methodol- ogy over 70 scores of the target musical archive, a dataset of 10 230 bimodal samples of 30 different symbols was obtained and made available for research purposes. This paper presents experimental results on classification over this dataset, in which symbols are recognized by combin- ing the two modalities. This combination of modes has demonstrated its good performance, decreasing the error rate of using each modality separately and achieving an al- most error-free performance.",
        "zenodo_id": 1417469,
        "dblp_key": "conf/ismir/Calvo-ZaragozaR16",
        "content": "TWO (NOTE) HEADS ARE BETTER THAN ONE: PEN-BASED\nMULTIMODAL INTERACTION WITH MUSIC SCORES\nJorge Calvo-Zaragoza, David Rizo, Jose M. I ˜nesta\nPattern Recognition and Artiﬁcial Intelligence Group\nDepartment of Software and Computing Systems\nUniversity of Alicante, Spain\n{jcalvo,drizo,inesta }@dlsi.ua.es\nABSTRACT\nDigitizing early music sources requires new ways of deal-\ning with musical documents. Assuming that current tech-\nnologies cannot guarantee a perfect automatic transcrip-\ntion, our intention is to develop an interactive system in\nwhich user and software collaborate to complete the task.\nSince conventional score post-editing might be tedious, the\nuser is allowed to interact using an electronic pen. Al-\nthough this provides a more ergonomic interface, this in-\nteraction must be decoded as well. In our framework, the\nuser traces the symbols using the electronic pen over a\ndigital surface, which provides both the underlying image\n(ofﬂine data) and the drawing made by the e-pen (online\ndata) to improve classiﬁcation. Applying this methodol-\nogy over 70scores of the target musical archive, a dataset\nof10 230 bimodal samples of 30different symbols was\nobtained and made available for research purposes. This\npaper presents experimental results on classiﬁcation over\nthis dataset, in which symbols are recognized by combin-\ning the two modalities. This combination of modes has\ndemonstrated its good performance, decreasing the error\nrate of using each modality separately and achieving an al-\nmost error-free performance.\n1. INTRODUCTION\nMusic constitutes one of the main tools for cultural trans-\nmission. That is why musical documents have been pre-\nserved over the centuries, scattered through cathedrals,\nmuseums, or historical archives. In an effort to prevent\ntheir deterioration, access to these sources is not always\npossible. This implies that an important part of this histor-\nical heritage remains inaccessible for musicological study.\nOccasionally, these documents are transcribed to a digi-\ntal format for easier access, distribution and study, without\ncompromising their integrity.\nOn the other hand, it is important to point out that the\nmassive digitization of music documents also opens seve-\nc/circlecopyrtJorge Calvo-Zaragoza, David Rizo, Jose M. I ˜nesta. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Jorge Calvo-Zaragoza, David Rizo, Jose\nM. I˜nesta. “Two (note) heads are better than one: pen-based multimodal\ninteraction with music scores”, 17th International Society for Music In-\nformation Retrieval Conference, 2016.ral opportunities to apply Music Information Retrieval al-\ngorithms, which may be of great interest. Since the manual\ntranscription of these sources is a long, tedious task, the de-\nvelopment of automatic transcription systems for early mu-\nsic documents is gaining importance in the last few years.\nOptical Music Recognition (OMR) is a ﬁeld devoted to\nproviding computers the ability to extract the musical con-\ntent of a score from the optical scanning of its source. The\noutput of an OMR system is the music score encoded in\nsome structured digital format such as MusicXML, MIDI\nor MEI. Typically, the transcription of early music docu-\nments is treated differently with respect to conventional\nOMR methods due to speciﬁc features (for instance, the\ndifferent notation or the quality of the document). Al-\nthough there exist several works focused on early music\ndocuments transcription [9,10], the speciﬁcity of each type\nof notation or writing makes it difﬁcult to generalize these\ndevelopments. This is especially detrimental to the evo-\nlution of the ﬁeld because it is necessary to implement\nnew processing techniques for each type of archive. Even\nworse, new labelled data are also needed to develop tech-\nniques for automatic recognition, which might imply a sig-\nniﬁcant cost.\nNotwithstanding the efforts devoted to improving these\nsystems, their performance is far from being optimal [12].\nIn fact, assuming that a totally accurate automatic tran-\nscription is not possible, and might never be, user-centred\nrecognition is becoming an emergent framework. Instead\nof a fully-automatized process, computer-aided systems\nare being considered, with which the user collaborates ac-\ntively to complete the recognition task [16].\nThe goal of this kind of systems is to facilitate the task\nfor the user, since it is considered the most valuable re-\nsource [2]. In the case of the transcription of early mu-\nsic documents, the potential user is the expert musicologist\nwho understands the meaning of any nuance that appears\nin the score. However, very often these users ﬁnd the use\nof a pen more natural and comfortable than keyboard entry\nor drag-and-drop actions with the mouse. Using a tablet\ndevice and e-pen, it is possible to develop an ergonomic\ninterface to receive feedback from users’ drawings. This\nis specially true for score post-edition where the user, in-\nstead of sequentially inputting symbols has to correct some\nof them, and for that, direct manipulation is the preferred\ninteraction style.509Such an interface could be used to amend errors made\nby the system in a simpler way for the user, as has been\nproposed for automatic text recognition [1]. However,\nthere are studies showing that, when the task is too com-\nplex, users prefer to complete the task by themselves\nbecause the human-machine interaction is not friendly\nenough [14]. Therefore, this interface could also be used\nto develop a manual transcription system that would be\nmore convenient and intuitive than conventional score ed-\nitors. Moreover, this transcription system might be useful\nin early stages of an OMR development, as it could be used\nto acquire training data more efﬁciently and ergonomically,\nwhich is specially interesting for old music notations.\nUnfortunately, although the user is provided with a\nmore friendly interface to interact with the system, the\nfeedback input is not deterministic this way. Unlike the\nkeyboard or mouse entry, for which it is clear what the\nuser is inputting, the pen-based interaction has to be de-\ncoded and this process might have errors.\nFor all the reasons above, this article presents our re-\nsearch on the capabilities of musical notation recognition\nwith a system whose input is a pen-based interface. To\nthis end, we shall assume a framework in which the user\ntraces symbols on the score, regardless of the purpose of\nthis interaction (OMR error correction, digitizing the con-\ntent, acquire labelled data, etc.). As a result, the system\nreceives a multimodal signal: on one hand, the sequence\nof points that indicates the path followed by the e-pen on\nthe digital surface —usually referred to as online modal-\nity; on the other hand, the piece of image below the drawn,\nwhich contains the original traced symbol — ofﬂine mode.\nOne of the main hypothesis of this study is that the combi-\nnation of both modalities leads to better results than using\njust either the pen data or the symbol image.\nThe rest of the paper is structured as follows: Section 2\nintroduces the corpora collected and utilized, which com-\nprises data of Spanish early music written in White Men-\nsural notation; Section 3 describes a multimodal classi-\nﬁer that exploits both ofﬂine and online data; Section 4\npresents the results obtained with such classiﬁer; and Sec-\ntion 5 concludes the present work.\n2. MULTIMODAL DATA COLLECTION\nThis work is a ﬁrst seed of a case study to digitize a his-\ntorical musical archive of early Spanish music. The ﬁnal\nobjective of the whole project is to encode the musical con-\ntent of a huge archive of manuscripts dated between 16th\nand 18th centuries, handwritten in mensural notation, in\nthe variant of the Spanish notation at that time [5]. A short\nsample of a piece from this kind of document is illustrated\nin Figure 1.\nThis section describes the process developed to collect\nmultimodal data of isolated musical symbol from images\nof scores. A massive collection of data will allow us to\ndevelop a more effective classiﬁcation system and to go\ndeeper into the analysis of this kind of interaction. Let\nus note that the important point in our interactive system\nis to better understand user actions. While a machine is\nFigure 1 . Example of page of a music book writ-\nten in handwritten white mensural notation from Spanish\nmanuscripts of centuries 16th to 18th.\nassumed to make some mistakes, it is unacceptable to force\nthe user to draw the same symbol of score many times. To\nthis end, our intention is to exploit both ofﬂine data (image)\nand online data (e-pen user tracing) received.\nOur idea is to simulate the same scenario of a real appli-\ncation. Therefore, we loaded the images of the scores on\na digital surface to make users trace the symbols using the\nelectronic pen. The natural symbol isolation of this kind\nof input is the set of strokes —data collected between pen-\ndown and pen-up actions. To allow tracing symbols with\nseveral strokes, a ﬁxed elapsed time is used to detect when\na symbol has been completed. If a new stroke starts be-\nfore this time lapse, it is considered to belong to the same\nsymbol than the previous one.\nOnce online data is collected and manually grouped into\nsymbol classes, the ofﬂine data is also extracted from this\ninformation. A bounding box is obtained from each group\nof strokes belonging to the same symbol, storing the maxi-\nmum and minimum values of each coordinate (plus a small\nmargin) among all the trace points collected. This bound-\ning box indicates where the traced symbol can be found in\nthe image. Therefore, with the sole effort of the tracing\nprocess, both online and ofﬂine data are collected. Note\nthat the extraction of the ofﬂine data is driven by the tracing\nprocess, instead of deciding at every moment the bounds of\neach symbol.\nFigure 2 illustrates the process explained above for a\nsingle symbol. Although the online data is drawn in this\nexample, the actual information stored is the sequence of\n2D points in the same order they were collected, indicating510 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016(a) Tracing process\n(b) Ofﬂine data\n (c) Online data\nFigure 2 . Example of extraction of a minima . Above, the\nsequence of points collected by the e-pen. The box repre-\nsents the bounding box of the sequence. Below, the multi-\nmodal data extracted from the same sample.\nthe path followed by the e-pen.\nFollowing this approach, several advantages are found:\nthe ﬁnal effort of collecting multimodal data is halved,\nsince the online data collection simultaneously provides\nthe ofﬂine data collection; the collected data mimics the\nscenario that might be found in the ﬁnal application, when\nthe user interacts with the machine; and the process be-\ncomes more user-friendly, which usually leads to a lower\nnumber of errors.\nThe collection was extracted by ﬁve different users from\n70different musical scores of different styles from the\nSpanish white mensural notation of 16th-18th centuries.\nThe Samsung Galaxy Note Pro 12.2 device (247 ppi res-\nolution) was used and symbols were written by means of\nthe stylus S-Pen . All the score images used are in the same\nscale, in which staff lines spacing is about 24 DP.1Due to\nthe irregular conditions of the documents, this value is ap-\nproximate but it can be used for normalizing with respect\nto other scores.\nThe obtained dataset consists of 10230 samples, each\nof which contains both a piece of image and the strokes\nfollowed during its tracing. These samples are spread over\n30classes. Table 1 lists the set of labels, including a ty-\npographic example and the number of samples per each.\nThe number of symbols of each class is not balanced but it\ndepicts the same distribution found in the documents.\nEvery symbol that must be differentiated for preser-\nvation purposes was considered as a different class. For\n1DP stands for device independent pixels in (Android) mobile appli-\ncation developmentLabel Image Count\nbarline\n 46\nbrevis\n 210\ncoloured brevis\n 28\nbrevis rest\n 171\nc-clef\n 169\ncommon time\n 29\ncut time\n 56\ndot\n 817\ndouble barline\n 73\ncustos\n 285\nf-clef 1\n 52\nf-clef 2\n 43\nfermata\n 75\nﬂat\n 274\ng-clef\n 174\nbeam\n 85\nlonga\n 30\nlonga rest\n 211\nminima\n 2695\ncoloured minima\n 1578\nminima rest\n 427\nproportio minor\n 28\nsemibrevis\n 1109\ncoloured semibrevis\n 262\nsemibrevis rest\n 246\nsemiminima\n 328\ncoloured semiminima\n 403\nsemiminima rest\n 131\nsharp\n 170\nproportio maior\n 25\nTable 1 . Details of the dataset obtained through the tracing\nprocess over 70scores (images from ‘Capit ´an’ font).Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 511instance, there are two f-clef types because the graphical\nsymbol is quite different despite having the same musical\nmeaning. However, the orientation of the symbols does not\nmake a different class since the same graphical representa-\ntion with a vertical inversion can be found. In the case it\nwas needed, the orientation could be obtained through an\neasy post-processing step.\nWe are making the dataset freely available at\nhttp://grfia.dlsi.ua.es/ , where more informa-\ntion about the acquisition and representation of the data is\ndetailed.\n3. MULTIMODAL CLASSIFICATION\nThis section provides a classiﬁcation experiment over the\ndata described previously. Two independent classiﬁers are\nproposed that exploit each of the modalities presented by\nthe data. Eventually, a late-fusion classiﬁer that combines\nthe two previous ones will be considered.\nTaking into account the features of our case of study,\nan instance-based classiﬁer was considered. Speciﬁcally,\nthe Nearest Neighbour (NN) rule was used, as it is one of\nthe most common and effective algorithms of this kind [3].\nThe choice is justiﬁed by the fact that it is specially suitable\nfor interactive scenarios like the one found in our task: it\nis naturally adaptive, as the simple addition of new pro-\ntotypes to the training set is sufﬁcient (no retraining is\nneeded) for incremental learning from user feedback. The\nsize of the dataset can be controlled by distance-based data\nreduction algorithms [7] and its computation time can be\nimproved by using fast similarity search techniques [17].\nDecisions given by NN classiﬁers can be mapped onto\nprobabilities, which are needed for the late fusion classi-\nﬁers. LetXbe the input space, in which a pairwise dis-\ntanced:X×X → Ris deﬁned. LetYbe the set of\nlabels considered in the classiﬁcation task. Finally, let T\ndenote the training set of labelled samples {(xi,yi) :xi∈\nX,yi∈Y}|T|\ni=1.\nLet us now assume that we want to know the posterior\nprobability of each class y∈Y for the input point x∈X\n(P(y|x)) following the NN rule. A common estimation\nmakes use of the following equations [4]:\np(y|x) =1\nmin (x/prime,y/prime)∈T:y/prime=yd(x,x/prime) +/epsilon1(1)\nP(y|x) =p(y|x)/summationtext\ny/prime∈Yp(y/prime|x), (2)\nwhere/epsilon1is a negligible value used to avoid inﬁnity calcula-\ntions. That is, the probability of each class is deﬁned as the\ninverse of the distance to the nearest sample of that class in\nthe training set. Note that the second term is used to ensure\nthat the sum over the probability of each class is 1. Finally,\nthe decision ˆyof the classiﬁer for an input xis given by a\nmaximum a posteriori criterion:\nˆy= arg max\nyP(y|x) (3)\nFigure 3 . Ofﬂine modality of a cut time symbol for clas-\nsiﬁcation: feature vector containing the greyscale value of\neach position of the rescaled image.\nFigure 4 . Online modality of a cut time symbol for clas-\nsiﬁcation: sequence of coordinates indicating the path fol-\nlowed by the e-pen during the tracing process.\n3.1 Ofﬂine classiﬁer\nThe ofﬂine classiﬁer takes the image of a symbol as input.\nTo simplify the data, the images are converted to greyscale.\nThen, since they can be of different sizes, a ﬁxed resizing\nprocess is performed, in the same way that can be found in\nother works, like that of Rebelo et al. [11]. At the end, each\nimage is represented by an integer-valued feature vector of\nequal length that stores the greyscale value of each pixel\n(see Figure 3). Over this data, Euclidean distance can be\nused for the NN classiﬁer. A preliminary experimentation\nﬁxed the size of the images to 30×30(900 features), al-\nthough the values within the conﬁgurations considered did\nnot vary considerably.\n3.2 Online classiﬁer\nIn the online modality, the input is a series of 2D points\nthat indicates the path followed by the pen (see Figure 4).\nIt takes advantage of the local information, expecting that\na particular symbol follows similar paths. The information\ncontained in this modality provides a new perspective on\nthe recognition and it does not overlap with the nature of\nthe ofﬂine recognition.\nThe digital surface collects the strokes at a ﬁxed sam-\npling rate so that each one may contain a variable number\nof points. However, several distance functions can be ap-\nplied to this kind of data. Those considered in this work\nare the following:\n•Dynamic Time Warping (DTW) [15]: a technique\nfor measuring the dissimilarity between two time\nsignals which may be of different duration.\n•Edit Distance with Freeman Chain Code (FCC): the\nsequence of points representing a stroke is converted512 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 0 5 10 15 20\noﬄine onlineError rate (%)\nTrade-oﬀ between modalitiesDTW\nFCC\nOSPFigure 5 . Average results with respect to the weight ( α)\ngiven to each modality for the conﬁgurations considered,\nfrom ofﬂine (α= 0) toonline (α= 1).\ninto a string using a codiﬁcation based on Freeman\nChain Code [6]. Then, a Edit Distance [8] can be\napplied to measure distance.\n•Edit Distance for Ordered Set of Points (OSP) [13]:\nan extension of the Edit Distance for its use over or-\ndered sequences of points, such those collected by\nthe e-pen.\n3.3 Late-fusion classiﬁer\nA straightforward late fusion has been used here. The idea\nis to combine linearly the decisions taken by the two base\nclassiﬁers. That is, probabilities of individual classiﬁers\nare combined by a weighted average:\nPfusion(y|x) =α·Pon(y|x) + (1−α)·Poff(y|x)(4)\nwherePoffandPondenote the probabilities obtained by\nofﬂine and online classiﬁers, respectively. A parameter\nα∈[0,1]is established to tune the relevance given to each\nmodality. We will consider several values of αranging\nfrom 0to1during experimentation.\n4. EXPERIMENTATION\nExperimentation followed a 10-fold cross-validation\nscheme. The independent folds were randomly created\nwith the sole constraint of having the same number of sam-\nples per class (where possible) in each of them. All the dis-\nsimilarities described in Section 3 for the online classiﬁer\nwill be tested.\nTable 2 illustrates the error rate ( %) achieved with re-\nspect toαfor this experiment. Note that α= 0 column\nyields the results of the ofﬂine classiﬁer as well as α= 1\nis equal to the online classiﬁer. A summary of the average\nresults is also illustrated in Figure 5.\nAn initial remark to begin with is that the worst re-\nsults of the late-fusion classiﬁers are achieved when each is\nmodality is used separately, with an average error of 11.77\nfor the ofﬂine modality and of 11.35,9.38and5.26for\nDTW, FCC and OSP, respectively. Not surprisingly, best\nresults are those that combine both natures of the data, sat-\nisfying the hypothesis that two signals are better than one.α DTW FCC OSP\n0.0 11.8±1.5 11.8±1.5 11.8±1.5\n0.1 5.3±0.4 5.5±1.1 4.9±0.7\n0.2 4.7±0.4 4.1±1.0 3.0±0.2\n0.3 5.4±0.4 3.9±0.8 2.2±0.4\n0.4 6.4±0.3 4.1±0.7 2.0±0.4\n0.5 7.4±0.5 4.6±0.7 2.2±0.5\n0.6 8.2±0.6 5.2±0.9 2.5±0.5\n0.7 9.1±0.5 5.9±1.0 3.0±0.6\n0.8 9.8±0.5 6.6±0.9 3.4±0.6\n0.9 10.5±0.8 7.3±0.9 4.2±0.5\n1.0 11.3±0.8 9.3±0.7 5.2±0.5\nTable 2 . Error rate (average ±std. deviation) obtained for\na10-fold cross validation experiment with respect to the\nvalue used for tuning the weight given to each modality\n(α) and the distances for the online modality (DTW, FCC\nand OSP). Bold values represent the best average result for\neach conﬁguration considered.\nResults also report that the tuning of αis indeed relevant\nsince it makes the error vary noticeably. An interesting\npoint to mention is that, although the online modality is\nmore accurate than the ofﬂine one by itself, the best tuning\nin each conﬁguration always gives more importance to the\nlatter. This might be caused by the lower variability in the\nwriting style of the original scribes.\nThe best results, on average, are reported by the late-\nfusion classiﬁer considering OSP distance for the online\nmodality, with an α= 0.4. In such case, just 2 % of error\nrate is obtained, which means that the interaction is well\nunderstood by the system in most of the cases. Note that a\nmore comprehensive search of the best αmay lead to a bet-\nter performance —for instance, in the range (0.3,0.5)—\nbut the improvement is not expected to be signiﬁcant.\nAlthough the results report a fair accuracy, the use of\nsemantic music models is expected to avoid some of these\nmistakes by using contextual information. Therefore, a\nnearly optimal performance could be obtained during the\ninteraction with the user.\n5. CONCLUSIONS\nThis paper presents a new approach to interact with musi-\ncal documents, based on the use of an electronic pen. Our\nframework assumes that the user traces each musical sym-\nbol of the score, and the system receives a multimodal in-\nput accordingly: the sequence of coordinates indicating the\ntrajectory of the e-pen (online mode) and the underlying\nimage of the score itself (ofﬂine mode).\nAn interface based on this idea could be used in a num-\nber of contexts related to interact with music scores in a\nmore intuitive way for the user. For instance, to amend\nOMR errors, to acquire training data in the early stages of\nthe development, or even as a part of a complete manualProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 513transcription system.\nThis framework has been applied to a music archive of\nSpanish music from the 16th to 18th centuries, handwrit-\nten in white mensural, with the objective of obtaining data\nfor our experiments. The result of processing this collec-\ntion has been described and made available for research\npurposes.\nExperimentation with this dataset is presented, conside-\nring several classiﬁers. The overall analysis of this experi-\nments is that it is worth to consider both modalities in the\nclassiﬁcation process, as accuracy is noticeably improved\nwith a combination of them than that achieved by each se-\nparately.\nAs a future line of work, the reported analysis will be\nused to build a whole computer-aided system, in which the\nuser interacts with the system by means of an electronic\npen to digitize music content. Since the late-fusion classi-\nﬁer is close to its optimal performance, it seems to be more\ninteresting to consider the development of semantic mod-\nels that can amend misclassiﬁcations by using contextual\ninformation ( e.g., a score starts with a clef). In addition,\nfurther effort is to be devoted to visualization and user in-\nterfaces.\n6. ACKNOWLEDGEMENTS\nThis work has been supported by the Spanish Ministerio\nde Educaci ´on, Cultura y Deporte through a FPU fellow-\nship (Ref. AP2012–0939) and the Spanish Ministerio de\nEconom ´ıa y Competitividad through project TIMuL (No.\nTIN2013-48152-C2-1-R, supported by UE FEDER funds).\nAuthors would like to thank Ichiro Fujinaga for inspir-\ning the title of this paper, and Antonio Ezquerro and Luis\nAntonio Gonz ´alez (Instituci Mil `a i Fontanals, Spanish Na-\ntional Research Council), who allowed the utilization of\nthe music sources for research purposes.\n7. REFERENCES\n[1] Vicent Alabau, Carlos D. Mart ´ınez-Hinarejos,\nVer´onica Romero, and Antonio L. Lagarda. An iter-\native multimodal framework for the transcription of\nhandwritten historical documents. Pattern Recognition\nLetters , 35:195–203, 2014.\n[2] Jes ´us Andr ´es-Ferrer, Ver ´onica Romero, and Alberto\nSanchis. Multimodal Interactive Pattern Recogni-\ntion and Applications , chapter General Framework.\nSpringer, 1st edition edition, 2011.\n[3] T. Cover and P. Hart. Nearest neighbor pattern clas-\nsiﬁcation. Information Theory, IEEE Transactions on ,\n13(1):21–27, January 1967.\n[4] Richard O. Duda, Peter E. Hart, and David G. Stork.\nPattern Classiﬁcation . John Wiley & Sons, New York,\nNY , 2nd edition, 2001.\n[5] Antonio Ezquerro Esteban, editor. M´usica de la Cate-\ndral de Barcelona a la Biblioteca de Catalunya . Bib-\nlioteca de Catalunya, Barcelona, 2001.[6] Herbert Freeman. On the encoding of arbitrary geo-\nmetric conﬁgurations. IRE Transactions on Electronic\nComputers , EC-10(2):260–268, 1961.\n[7] Salvador Garc ´ıa, Juli ´an Luengo, and Francisco Her-\nrera. Data Preprocessing in Data Mining , volume 72 of\nIntelligent Systems Reference Library . Springer, 2015.\n[8] Vladimir I. Levenshtein. Binary codes capable of cor-\nrecting deletions, insertions, and reversals. Technical\nReport 8, 1966.\n[9] Jo ˜ao Rog ´erio Caldas Pinto, Pedro Vieira, and\nJo˜ao Miguel da Costa Sousa. A new graph-like classi-\nﬁcation method applied to ancient handwritten musical\nsymbols. IJDAR , 6(1):10–22, 2003.\n[10] Laurent Pugin. Optical music recognition of early ty-\npographic prints using hidden markov models. In IS-\nMIR 2006, 7th International Conference on Music In-\nformation Retrieval, Victoria, Canada, 8-12 October\n2006, Proceedings , pages 53–56, 2006.\n[11] A. Rebelo, G. Capela, and Jaime S. Cardoso. Opti-\ncal recognition of music symbols. International Jour-\nnal on Document Analysis and Recognition (IJDAR) ,\n13(1):19–31, 2010.\n[12] Ana Rebelo, Ichiro Fujinaga, Filipe Paszkiewicz,\nAndr ´e R. S. Marc ¸al, Carlos Guedes, and Jaime S. Car-\ndoso. Optical music recognition: state-of-the-art and\nopen issues. International Journal of Multimedia In-\nformation Retrieval , 1(3):173–190, 2012.\n[13] Juan R. Rico-Juan and Jose M. I ˜nesta. Edit distance\nfor ordered vector sets: A case of study. In Struc-\ntural, Syntactic, and Statistical Pattern Recognition ,\nvolume 4109 of Lecture Notes in Computer Science ,\npages 200–207. Springer Berlin Heidelberg, 2006.\n[14] V . Romero and J. Andreu Sanchez. Human Evalua-\ntion of the Transcription Process of a Marriage License\nBook. In 12th International Conference on Document\nAnalysis and Recognition (ICDAR) , pages 1255–1259,\nAug 2013.\n[15] Hiroaki Sakoe and Seibi Chiba. Readings in Speech\nRecognition. In Alex Waibel and Kai-Fu Lee, edi-\ntors, Readings in Speech Recognition , chapter Dy-\nnamic Programming Algorithm Optimization for Spo-\nken Word Recognition, pages 159–165. Morgan Kauf-\nmann Publishers Inc., San Francisco, CA, USA, 1990.\n[16] Alejandro Hector Toselli, Ver ´onica Romero, Mois ´es\nPastor, and Enrique Vidal. Multimodal interactive\ntranscription of text images. Pattern Recognition ,\n43(5):1814–1825, 2010.\n[17] Pavel Zezula, Giuseppe Amato, Vlastislav Dohnal, and\nMichal Batko. Similarity Search - The Metric Space\nApproach , volume 32 of Advances in Database Sys-\ntems. Kluwer, 2006.514 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Mixtape: Direction-Based Navigation in Large Media Collections.",
        "author": [
            "João Paulo V. Cardoso",
            "Luciana Fujii Pontello",
            "Pedro H. F. Holanda",
            "Bruno Guilherme",
            "Olga Goussevskaia",
            "Ana Paula Couto da Silva"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416072",
        "url": "https://doi.org/10.5281/zenodo.1416072",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/155_Paper.pdf",
        "abstract": "In this work we explore the increasing demand for novel user interfaces to navigate large media collections. We im- plement a scalable data structure to store and retrieve sim- ilarity information and propose a novel navigation frame- work that uses geometric vector operations and real-time user feedback to direct the outcome. In particular, we im- plement this framework in the domain of music. To eval- uate the effectiveness of the navigation process, we pro- pose an automatic evaluation framework, based on syn- thetic user profiles, which allows to quickly simulate and compare navigation paths using different algorithms and datasets. Moreover, we perform a real user study. To do that, we developed and launched Mixtape 1 , a simple web application that allows users to create playlists by provid- ing real-time feedback through liking and skipping pat- terns.",
        "zenodo_id": 1416072,
        "dblp_key": "conf/ismir/CardosoPHGGS16",
        "content": "MIXTAPE: DIRECTION-BASED NA VIGATION IN LARGE MEDIA\nCOLLECTIONS\nJo˜ao Paulo V . Cardoso Luciana Fujii Pontello Pedro H. F. Holanda\nBruno Guilherme Olga Goussevskaia Ana Paula C. da Silva\nComputer Science Department, Universidade Federal de Minas Gerais (UFMG), Brazil\njpcardoso@ufmg.br, lucianafujii@dcc.ufmg.br, holanda@dcc.ufmg.br,\nbrunoguilherme@dcc.ufmg.br, olga@dcc.ufmg.br, ana.coutosilva@dcc.ufmg.br\nABSTRACT\nIn this work we explore the increasing demand for novel\nuser interfaces to navigate large media collections. We im-\nplement a scalable data structure to store and retrieve sim-\nilarity information and propose a novel navigation frame-\nwork that uses geometric vector operations and real-time\nuser feedback to direct the outcome. In particular, we im-\nplement this framework in the domain of music. To eval-\nuate the effectiveness of the navigation process, we pro-\npose an automatic evaluation framework, based on syn-\nthetic user proﬁles, which allows to quickly simulate and\ncompare navigation paths using different algorithms and\ndatasets. Moreover, we perform a real user study. To do\nthat, we developed and launched Mixtape1, a simple web\napplication that allows users to create playlists by provid-\ning real-time feedback through liking and skipping pat-\nterns.\n1. INTRODUCTION\nInternet cloud and streaming services have become the\nstate-of-the-art in terms of storage and access to media col-\nlections. Even though the storage problem of media col-\nlections seems to have been practically solved with cloud-\nbased applications, a challenge still remains in conceptu-\nalizing and developing novel interfaces to explore them.\nUser interfaces are expected to be intuitive and easy, yet\nﬂexible and powerful in understanding and delivering what\nusers expect to see.\nIn this work we propose a framework that uses real-\ntime user feedback to provide direction-based navigation\nin large media collections. The navigation framework is\ncomprised of a data structure to store and retrieve similar-\nity information and a novel navigation interface that allows\n1www.projectmixtape.org\nc/circlecopyrtJo˜ao Paulo V . Cardoso, Luciana Fujii Pontello, Pedro H.\nF. Holanda, Bruno Guilherme, Olga Goussevskaia, Ana Paula Couto da\nSilva. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Jo˜ao Paulo V . Cardoso, Luciana Fu-\njii Pontello, Pedro H. F. Holanda, Bruno Guilherme, Olga Goussevskaia,\nAna Paula Couto da Silva. “Mixtape: Direction-based navigation in large\nmedia collections”, 17th International Society for Music Information Re-\ntrieval Conference, 2016.users to explore the content of the collection in a person-\nalized way. We begin by focusing on the music domain,\nbecause the intrinsic usage pattern behind listening to mu-\nsic is favorable to the design and veriﬁcation of a dynamic\nreal-time feedback based system.\nWe deﬁne media item-to-item similarity based on user-\ngenerated data, assuming that two items are similar if they\nfrequently co-occur in a user’s proﬁle history. Media co-\noccurrence information is increasingly available through\nmany online social networks. For example, in the do-\nmain of music, such usage information can be collected\nfrom Last.fm, a social music site. Collected co-occurrence\ndata is usually sparse (not all pairs of items will have co-\noccurred at least once in the collected dataset) and never-\ntheless might occupy a lot of memory space ( Ω(n2), where\nnis the size of the collection). To guarantee O(n)space\ncomplexity and O(1)query complexity of all-pairs sim-\nilarity information, we transform the collected pairwise\nco-occurrence values into a multi-dimensional Euclidean\nspace, by using nonlinear dimensionality reduction [21].\nOur main contribution is a novel randomized naviga-\ntion algorithm, based on the geometry of the constructed\nsimilarity space. Each navigation session is modeled as a\nMonte Carlo simulation: given a starting item and a set of\nclose neighbors in the similarity space, each neighbor is\nassigned a probability of being the next current item. If\nthe returned next item is not quite what the user wants to\nsee, they can skip it, so the previous item is used as the\nseed again. To deﬁne these probabilities, we propose a\ngeometric vector-based approach, which explores the no-\ntion of direction, using user feedback and the Euclidean\ndistances between items to establish a concept of “direc-\ntion inertia”, which creates a tendency for users to “keep\ngoing” in the direction of the items they enjoy and “turn\naway” from items, or regions, they don’t like.\nThe evaluation of the resulting system is twofold. First,\nwe propose an automatic evaluation framework, based on\nsynthetic user proﬁles, which allows to quickly simulate\nand compare navigation paths using different algorithms\nand datasets. We also propose two basic metrics: num-\nber of skips per like ratio and smoothness of consecutively\naccepted items in a navigation session. Second, we eval-\nuate real-user interaction with the system. To do that, we\ndeveloped and launched Mixtape, a simple web applica-\ntion that allows users to create playlists by providing real-454time feedback through liking and skipping patterns. Over-\nall, we analyzed over 2,000 simulated and 2,000 real-\nuser navigation sessions in a map comprised of more than\n62,000songs. Besides analyzing quantitative parameters,\nsuch as the proportion of skipped to accepted songs and\nthe smoothness of the generated trajectories, we gathered\nfeedback left by users and analyzed what they expect and\nappreciate in a media navigation system.\n2. RELATED WORK\nA closely related line of research to this work is automatic\nplaylist generation. There are techniques that use statis-\ntical analysis of radio streams [4, 5, 15, 22], are based on\nmultidimensional metric spaces [2,4,9,13,16,17], explore\naudio content [3,8,14,23], and user skipping behavior [18].\nIn particular, Chen et al [4] model playlists as Markov\nchains, which are generated through the Latent Markov\nEmbedding (LME) machine learning algorithm, using on-\nline radio streams as a training set. We use this algorithm\nas a baseline in our experiments. The idea to embed co-\noccurrence information into a multi-dimensional space has\nbeen explored before, e.g., in [1,2,9,13], where the authors\nfocus mostly on visual exploration of a collection. The idea\nto use skipping behavior to generate playlists has been ex-\nplored in [18], however, the presented algorithms do not\nscale to large collections. Our work goes beyond playlist\ngeneration, providing a real-time ﬂexible navigation inter-\nface that receives immediate user feedback through skip-\nping behavior to guide the user within the music collection\ntowards directions chosen on-the-ﬂy.\n3. NA VIGATION FRAMEWORK\nOur goal is to design a media navigation framework com-\nprised of two main components: (1) A scalable data struc-\nture to store and retrieve item-to-item similarity informa-\ntion; (2) Directed-based navigation functions, that take the\ncurrent item and user feedback in real time and return the\nnext item; moreover, we want the navigation output to be\ncomputationally efﬁcient and nondeterministic, so the user\ncan be surprised with new items in each navigation se-\nquence.\n3.1 Item-to-item similarity representation\nIn this work, we use the assumption that similarity between\ntwo items can be deduced by analyzing usage habits of a\nlarge number of media users. More speciﬁcally, we assume\nthat the more often two items co-occur in the same user’s\nproﬁle, the more similar they are. So we deﬁne pairwise\nsimilarity between two items by using cosine similarity:\ncos(i,j) =coocc (i,j)//radicalbig\nocc(i)occ(j), wherecoocc (i,j)\nis the number of co-occurrences between two items and\nocc(i)the individual occurrences in the users’ proﬁles.\nSince co-occurrence data is typically sparse, i.e., only a\nfew pairwise similarities are known, we applied the Isomap\nmethod [21], which extends classical multidimensional\nscaling (MDS) [6] by incorporating the geodesic distances\nimposed by an (intermediate) weighted graph. We deﬁnedthe weight of an edge as the complement of the cosine sim-\nilarity, (w(i,j) = 1−cos(i,j)) and built a graph Gwith\nthese weights.\nTo generate the map we calculated the complete nXn\ndistance matrix from Gand then applied the classical MDS\nalgorithm in this matrix. Building a new d-dimensional\nEuclidean space such that d << n . The ﬁnal space is a\nnXd matrix. Note that, for larger datasets one can use\napproximate algorithms, such as LMDS or LINE [7, 20].\n3.2 Navigation functions\nIn order to guide the navigation process, a navigation ses-\nsion is treated as a run of a Monte Carlo simulation, in\nwhich the choice of the next item depends on the current\nitem and a probability function that assigns different prob-\nabilities to each of its neighboring nodes. Given a starting\nitem, the navigation system retrieves the set Kof its near-\nest neighbors in the Euclidean space, and uses them as can-\ndidates to be the next item. Once an item ki∈Kis chosen\nto be next, users can provide immediate feedback to the\nsystem by accepting or skipping it explicitly, through user\ninterface feedback, or implicitly by skipping it. In case the\nnew item is accepted, it becomes the current item, and the\nprocess starts again. The probability function should have\na strong inﬂuence on the overall outcome of the navigation.\nParameter|K|is used to vary the size of each “step” of\nthe navigation process. It can be conﬁgured as a constant,\nor to be variable. In our experiments, good results were\nachieved using exponentially growing step size:\n|K|=/braceleftBigg\n2|K|,if the previous item was skipped\n|K0|,otherwise,\nwhere|K0|is conﬁgurable minimum neighborhood size.\nIn our experiments we used |K0|= 10 and|K|≤640.\nMap navigation : We start with the following basic ap-\nproach, to which we refer as Map, that explores the idea\nthat users prefer to navigate through items that are close to\neach other in the Euclidean space. We deﬁne the probabil-\nity of nodeki∈Kto be next as:\nPnextMap\ni =/braceleftBigg\n1/|K|,ifki∈K;\n0, otherwise,\nVector navigation : Vector navigation explores the notion\nof direction of navigation, assuming users would like to\ntravel through different regions in the space. To do so, it\ntreats the possible steps in the space as vectors. The hop\nvector/vectorabof any given hop from item Ato itemBcan\nbe derived from the straight line between them (see Fig-\nure 1.1).\nAs the navigation progresses, the system keeps a direc-\ntion vector/vectorV, which is recalculated after every hop. This\nvector represents the directions in which the system has\nrecently moved. As a simpliﬁed example, consider the fol-\nlowing sequence from item Ato itemD(see Figure 1.2).\n/vectorV0was derived from the ﬁrst hop, /vectorab./vectorV1is half the sum of\n/vectorV0and/vectorbc, which was the second hop. /vectorV2is half the sum\nof/vectorV1and/vectorcd, and the process goes on.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 455B\nabab\nATv1\nv0v0btA Bab\nA\nBab-ab\nv0v1A\nBC\nθcθbab ac\nv0A\nBC\nDabbc\ncd\nv0v1\nv21\n24 3 5Figure 1 . Direction-based navigation (Vector algorithm)\nIn each step, the system calculates the probabilities of\nsuggesting each neighbor by comparing the current direc-\ntion vector/vectorVto each of the vectors towards the |K|consid-\nered candidates (i.e., to vectors from the current to neigh-\nbor nodes). For example, consider the decision to move\nfrom itemAwith current direction vector /vectorV0to two neigh-\nbors,BandC(see Figure 1.3).\nThe more aligned the direction and the candidate vec-\ntors are, the smaller the angle θwill be, and the higher\nthe probability of suggesting that neighbor. In Figure 1.3,\nsinceθc<θb, nodeCwill have a higher probability to be\nthe next item.\nIt is also possible to set a target destination T, which is\nan arbitrary point on the map, where the navigation should\ntry to go to. This adds a third element to the direction\nvector update procedure, by creating vector /vectorbtin each up-\ndate and also adding it to /vectorV. Let’s consider the situation\nin which a hop was accepted from item A to item B, and\nthe target destination Twas conﬁgured (see Figure 1.4).\nNote that/vectorV1is made by adding vectors /vectorV0,/vectoraband/vectorbtand\ndividing the module of the resulting vector by three.\nLast but not least, feedback is incorporated by consid-\nering that, when a user skips a suggestion, it would be in-\nteresting to increase the probability of suggesting some-\nthing different from the skipped item. So, when an item is\nskipped, the system does not change the current node, and\nthe opposite vector is added to /vectorV. Consider the example\nin Figure 1.5, where item B was skipped, and so /vectorV1was\ncalculated by adding /vectorV0to−/vectorabto reﬂect the user’s prefer-\nence.\nDeﬁning the method formally: Consider a set Kof clos-\nest neighbors of current node Aand the current direction\nvector/vectorVwith the respective angles θibetween/vectorVand each\nhop vector /vectoraki,ki∈K. Also, consider the optional pa-\nrameter with the location of a target destination T. We de-\nﬁne the direction-based navigation function using the fol-\nlowing weight variables:\nwi= 1 +cos(θi) = 1 +/vectoraki•/vectorV\n|/vectoraki||/vectorV|.\nNote that the weight is proportional to the cosine of the\nangle between the current direction vector /vectorVand the di-\nrection of each neighbor kirelative to the current node A.\nWe add 1to avoid negative values. Finally, we deﬁne the\nprobability of node ki∈Kto be next as:\nPnextV ec\ni =wi/summationtext|K|\nj=1wj.Note that we have a proper probability distribution,\nsince the sum over probabilities PnextV ec\ni,i∈Kis 1.\nAfter the next item has been returned, say ki, and the\nuser has provided feedback by accepting or skipping it, we\nupdate the direction vector /vectorVof nodeAas follows:\n/vectorV=/braceleftBigg\n(/vectoraki+/vectorV)/2, ifkiwas accepted\n(−/vectoraki+/vectorV)/2,ifkiwas skipped .\nIf target destination Thas been deﬁned, then the calcu-\nlation also includes the new target vector /vectoratbetween the\nchosen item and the target destination:\n/vectorV=/braceleftBigg\n(/vectoraki+/vectorV+/vectorat)/3, ifkiwas accepted\n(−/vectoraki+/vectorV+/vectorat)/3,ifkiwas skipped .\nIf the item was accepted, node kibecomes the next cur-\nrent node. Otherwise, the current node does not change,\nand only the direction vector is updated. Note that this\napproach is domain-independent and uses nothing but the\ncoordinates of the embedding itself. It also carries an ex-\nplicit dependency on user feedback, since /vectorVis determined\nby the user’s skipping behavior.\n4. MUSIC DOMAIN\nThe navigation framework described in Section 3 can be\napplied to different media domains. In this work, we focus\non the domain of music.\n4.1 Last.fm Dataset\nIn order to deﬁne music similarity, we assume that the\nmore frequently two songs co-occur in a user’s listen-\ning history, the more similar they are. We collected co-\noccurrence data from Last.fm , a social music site that\ntracks user musical tastes, from November, 2014 to March,\n2015. More speciﬁcally, we collected the top-25 most lis-\ntened songs of each user, reaching a total of 372,899 users,\n2,060,173 tracks, and 374,402 artists. Moreover, we also\ncollected a total of 1,006,236 user-generated tags, asso-\nciated with songs. In particular, 75% of songs have had\nat least one associated tag in our dataset. We considered\na subset of 983,010 tracks in our dataset with a known\nMBID2, from which we selected another subset of 83,180\ntracks that co-occurred 5 or more times, forming a con-\nnected component of 62,352 songs. A detailed characteri-\nzation of the dataset can be found in [19].\n2MusicBrainz Identiﬁer (MBID) is a reliable and unambiguous form\nof music identiﬁcation ( musicbrainz.org ).456 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016The connected graph with 62,352 vertices enabled us to\nrun IsoMap [21] and Multidimensional Scaling (MDS) [6]\nwithout any approximations. By parallelizing parts of the\nalgorithm, we computed the all-pairs shortest path matrix\nof size 62,352×62,352, in7minutes on a server with\n50GB of RAM and 16CPU cores, and computed the em-\nbedding into 100 dimensions in approximately 2hours on\nthe same server. Note that a larger collection could have\nbeen embedded using a less computationally intensive ap-\nproximate algorithm, such as LMDS or LINE [7, 20]. An\nevaluation of the embedding process can be found in [12].\n4.2 Mixtape\nWe wanted to collect real user feedback in order to evalu-\nate the navigation framework in the music domain. For this\npurpose, we developed Mixtape, a web-based application\nwith a simple user interface (see Figure 2). On the server\nside, a k-d tree was loaded with a 100-dimensional space\nof62,352tracks. On the client side, the design goal was\nto provide a minimalist user interface that fully explored\nthe navigation functions deﬁned in Section 3.2. Each user\nwould choose the starting song and then be presented with\none suggestion at a time, with explicit feedback-generating\nactions. The user interface is comprised of a playlist, on\nthe left, which shows the songs the user has accepted or\nskipped, and a Youtube video window, which ﬁnds and\nplays the current suggested item. Users can then decide\nwhether they like the song or not, using the likeanddis-\nlikebuttons, one of which the user must press in order to\nreceive the next suggestion. In case the user does not press\nanything and listens to the entire song, we assume they\nliked it, and consider the song as accepted. There is also\na settings button, which allows the user to switch between\ndifferent navigation functions.\n5. EXPERIMENTS\nThe evaluation of the proposed navigation framework in\nthe domain of music is twofold: Firstly, we propose an\nautomatic evaluation framework and perform an extensive\nanalysis based on simulated user proﬁles. Furthermore,\nsince real users might behave differently, and the percep-\ntion of a song is subjective, we observed how real users\ninteracted with our Mixtape application. As a result, we\nwere able to evaluate not only how effective and engaging\nthe proposed navigation system is, but also how well the\nsimulated user proﬁles approximated real user behavior.\n5.1 Simulated user proﬁles\nTo test the navigation framework, we simulated synthetic\nuser proﬁles, in which hypothetical users intend to listen\nto 20 songs (about one hour of music), and count the num-\nber of skips (songs that are skipped by the simulated user\nproﬁle following the algorithm described below) until 20\nsongs are accepted. A similar evaluation approach was\nused in [18]. We simulated two types of users:\nTag-based user proﬁle: This user proﬁle is based on tag\ninformation and the notion of transition between two re-gions on the map. Recall from Section 4.1 that we col-\nlected over 1,006,236 user-generated tags, associated with\nsongs. We assume this user wishes to listen to a sequence\nof songs that transitions from initial tag Tito ﬁnal tagTf.\nTo do that, the simulated user accepts all songs asso-\nciated with tag Tiin the ﬁrst 1/3of the navigation path\n(skips otherwise), accepts songs with tags TiorTfin the\nsecond 1/3, and accepts only songs with tag Tfin the last\n1/3of the path, comprised of a total of 20 items. Note that\nreal users do not necessarily know what tags are associated\nto particular tracks. Since these users are hypothetical, we\ncan use the collected tag information for simulation pur-\nposes.\nWe manually selected tag transitions among the top 200\nmost popular tags in our dataset. We noticed these tags\ncould be divided in three categories: Mood tags, such as\nChill, Upbeat, Relaxing , Genre tags, such as such as Rock,\nHip Hop, Folk , and Age tags, such as 60’s, 90’s, 2000’s .\nWe then paired them up manually, selecting 14 transitions\nto experiment with. For each tag transition ( Ti⇒Tf), we\nconsidered a navigation path starting at the most popular\nsong associated to tag Ti, and applying the skipping rule\nuntil a path of 20 accepted songs was achieved.\nArtist-based user proﬁle: This user proﬁle is based on\nartist information and the notion that certain users wish to\nlisten to songs by artists they already know. Since this user\nwishes to listen to preferred artists, whenever the suggested\nsong is by an artist contained in the user’s history, it is ac-\ncepted. Otherwise, it is skipped. We collected the com-\nplete listening histories of 20 users to simulate this user\nproﬁle, and started the playlist at a random song within\neach user’s proﬁle. Moreover, for this experiment only\nusers whose proﬁles were notused to construct the em-\nbedding were simulated.\n5.2 Baselines\nAs baselines, we tested the following approaches:\nLME: Logistic Markov Embedding [4,16], a probabilistic\napproach that models sequences in a Euclidean space using\nradio streams as a training set. We used the implementation\navailable at the authors’ homepage, with all parameters set\nto default values, except for α= 5 (this value resulted in\nsuperior overall performance), as our dataset did not have\nmusic sequences, only music occurrences in a user proﬁle,\nwe used the “yes-complete” dataset (also made available\nby the authors) in a combination with our dataset, since\nLME needs a sequence of items as input, which resulted\nin an intersection set of 31,544 items with our dataset. We\nmade one modiﬁcation to the LME algorithm by incorpo-\nrating user feedback when computing the next item. More\nspeciﬁcally, whenever an item njhas been skipped after a\npreviously accepted item ni, we recompute the probabili-\nties atnisettingPr(nj|ni) = 0 , and maintaining nias the\ncurrent item.\nRandom : A random song is returned, considering all\nsongs in the dataset.\nRandom Tag : A random song with tag Tis returned. This\nbaseline was used for the tag-based navigation evaluation.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 457Figure 2 . Mixtape screenshot\n 0 0.2 0.4 0.6 0.8 1\n 1  10  100  1000CDF\nPlaylist Size Figure 3 . Mixtape user study: playlist\nlength distribution\n1\n 0 2 4 6 8 10 12\nTag-based Artist-based Mixtapeskips/like\nnavigation setupRandom\nLMEMapVectorFigure 4 . Total number of skips per like\nratio: simulated versus real-user proﬁles\n5.3 Mixtape user study setup\nWe collected all user actions on Mixtape over a course of\n2 weeks, resulting in the participation of over 800 users,\ngenerating a total of over 2000 navigation sessions. In or-\nder to compare the performance of different navigation al-\ngorithms, each navigation session was randomly assigned\neither Map, Vector or LME algorithms (with undeﬁned tag\nparameters), but the user could explicitly change the algo-\nrithm in the settings menu as well. Users could also choose\nthe Random approach, however, since the engagement in\nthis setting was very low, we did not include it in the plots.\n5.4 Results\nIn our experiments, we measure the effectiveness of the\ntwo navigation approaches proposed in this work (Map and\nVector) and the two baselines (Random and LME) in three\nnavigation setups: simulated tag-based user proﬁles, simu-\nlated artist-based user proﬁles, and real users on Mixtape.\nWe use two main metrics: skipping behavior and playlist\nsmoothness, deﬁned below. Each scenario was executed\n20 times, and all ﬁgures show the 95% conﬁdence interval.\nSkipping behavior: Figure 3 shows the CDF of playlist\nlength generated on Mixtape. It can be seen that almost\n30% of the playlists3contain 10 tracks or more, and al-\nmost 15% have size 20 or longer, which shows that many\npeople really engaged with the application.\nIn Figure 4 we compare the ratio of the total number\nof skips (dislikes) and the total number of accepted songs\n(likes) in playlists generated by all navigation algorithms\nfor simulated and real user proﬁles. Analyzing the sim-\nulated user proﬁles, we can see that the baseline algo-\nrithms present several times more skips per like (LME:\nskips/like > 7.5, Random:skips/like > 8) than Map\nand Vector ( skips/like < 2). Map and Vector have sim-\nilar results and perform especially well in the artist-based\nsimulated setup ( skips/like< 0.5), which shows they are\nmore effective not only in directing the user between dif-\nferent regions in the space, but also in presenting the user\nwith music by preferred artists.\nLooking at Mixtape results on Figure 4, we can\nsee that all three approaches perform well on average\n3We refer to the sequence of tracks accepted, or liked, by a user in one\nnavigation session as a playlist .(skips/like < 2). LME has still more skips than likes\n(skips/like > 1), whereas Map and Vector have signiﬁ-\ncantly more likes than skips (Map: skips/like< 0.8, Vec-\ntor:skips/like < 0.4), indicating that users enjoyed the\nvast majority of the suggested songs, especially by the Vec-\ntor algorithm. Note that Vector outperforms Map for real\nusers, indicating that the direction in the map, provided by\nthe real-time feedback, does matter for real users.\nComparing real and synthetic user proﬁles, we note that\nLME performed much better with real rather than simu-\nlated users. That might be because real users are more\nopen-minded and accept more diversity in their playlists.\nNevertheless, the number of skips per like for Vector and\nMap on Mixtape was similar to the simulated artist-based\nuser proﬁles, indicating that in some aspects the simulation\nwas accurate in portraying a real user.\nIn Figures 5 through 7 we analyze the number of skips\nalong each step of the navigation process. In Figure 5\nthe number of skips per step decreases in the second third\nand then reaches a maximum in the beginning of the third\npart of the playlist for all algorithms. This illustrates how\nthe algorithms react to the simulated tag-based navigation\nsetup. Afterwards, however, we can see that Map and Vec-\ntor quickly decrease the number of skips, as opposed to\nLME and Random, showing that the former algorithms\nsucceed in adjusting the direction of the navigation towards\nthe destination tag.\nPlaylist smoothness: In Figures 8 through 10 we analyze\nhow similar consecutive songs are on a navigation path, by\nmeasuring the cosine similarity of the artists of consecu-\ntive (accepted) songs.4Note that in Figure 8 we plot the\nRandomTag baseline instead of Random, to shed light on\nthe following question: if the objective of tag-based sim-\nulations is to recommend songs with a given tag, why not\nsimply choose songs from the database that have that tag?\nThat method might work when we ignore the relationship\nbetween songs in a playlist. However, we argue that a\nplaylist should be more than a group of songs with a given\ntag–it should present a relationship between the songs. We\ncan see that RandomTag and LME baselines provide al-\nmost zero similarity along the navigation path, even though\nRandomTag only returns songs with accepted tags, i.e.,\n4We deﬁne artist similarity as the cosine similarity computed from\nartist co-occurrence in our dataset.458 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 0 5 10 15 20 25\n 0 2 4 6 8 10 12 14 16 18 20number of skips\naccepted song numberMap\nRandom\nVector\nLMEFigure 5 . Skips along playlists: simu-\nlated tag-based navigation\n 0 5 10 15 20\n 0 2 4 6 8 10 12 14 16 18 20number of skips\naccepted song numberMap\nVector\nRandom\nLMEFigure 6 . Skips along playlists: simu-\nlated artist-based navigation\n-2-1 0 1 2 3 4 5 6 7\n 5  10  15  20  25number of skips\naccepted song numberMap\nVector\nLMEFigure 7 . Skips along playlists: real-\nuser navigation (Mixtape)\n 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45\n 0 2 4 6 8 10 12 14 16 18artist cosine\naccepted song numberMap\nRandomTag\nVector\nLME\nFigure 8 . Playlist smoothness: simu-\nlated tag-based navigation\n 0 0.1 0.2 0.3 0.4 0.5 0.6\n 0  2  4  6  8 10  12  14  16  18artist cosine\naccepted song numberMap\nRandom\nVector\nLMEFigure 9 . Playlist smoothness: simu-\nlated artist-based navigation\n 0 0.2 0.4 0.6 0.8 1\n 0  5  10  15  20  25artist cosine\naccepted song numberMap\nVector\nLMEFigure 10 . Playlist smoothness: real-\nuser navigation (Mixtape)\nmakes zero skips in the tag-based simulation setup. Map\nand Vector, on the other hand, trace highly smooth nav-\nigation paths, offering the user songs with high similar-\nity to the previously chosen songs, especially in the artist-\nbased simulation setup (Figure 9, artist cosine >0.4). Fig-\nure 105shows that Map and, especially, Vector playlists\non Mixtape also present high similarity between consecu-\ntive items, indicating that people prefer smooth, rather than\nabrupt, transitions in their navigation paths.\nUser feedback: To enhance our perception about how\nusers perceive our Mixtape application, we created a short\nonline survey, linked from the Mixtape application, which\nwas answered by 44 unidentiﬁed subjects. The users were\nnot provided with any information about the navigation al-\ngorithms. They were asked to provide feedback on the ex-\nperience of using Mixtape, leading to the following num-\nbers: 95% enjoyed the songs suggested by Mixtape, and\nonly 11% of the users were not able to ﬁnd most of the\nsongs they were searching for (recall from Section 4.1 that\nwe used a reduced sample of the map in our experiments:\n62,352 songs with co-occurrence at least 5.).\nInterestingly, 70% of the participants said they discov-\nered new artists or songs. Most people said they didn’t\nchange the navigation policy and they didn’t know which\npolicy they used during their navigation. From those who\ndid experiment with different policies, they equally en-\njoyed Map and Vector approaches (even though, on aver-\nage, users skipped less songs when using direction-based\nnavigation).\n5The CIs in Figures 7 and 10 are high, due to insufﬁcient data for\ncertain song numbers.To sum up this ﬁrst user study, we can conclude that\nusers enjoyed navigating music collections by giving their\nreal-time feedback and that the navigation allowed them to\ndiscover previously unknown songs they enjoyed.\n6. CONCLUSION\nIn this work we proposed a navigation framework for large\nmedia collections and evaluated an implementation of the\nframework in the domain of music. Potentially, the same\nideas could be applied to other kinds of media, e.g. movies\nor TV shows [10, 11]. Rather than creating ﬁxed playlists,\nour approach allows users to provide feedback through\nskipping behavior and direct the navigation process in real-\ntime. We evaluated the framework through simulation of\nmore that 2,000 synthetic navigation paths and performed\na real user study by launching Mixtape, a web application\nwith a minimalist user interface that allows people to navi-\ngate a collection of over 60,000 music tracks. We analyzed\nover 2,000 playlists generated by over 800 real users and\nreceived positive feedback about the application. When\ncomparing playlists generated by Mixtape and simulated\nhypothetical users, we could observe several similarities,\nindicating that in some aspects the simulation was accu-\nrate in portraying a real user. Moreover, not only did this\nuser study serve as validation of the proposed framework,\nbut it also provided insights into what users look for and\nappreciate in a media navigation system.6\n6Acknowledgments: This work is supported in part by CNPq,\nFAPEMIG and LG Electronics in cooperation with Brazilian Federal\nGovernment through Brazilian Informatics Law (n. 8.2.48/1991).Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 4597. REFERENCES\n[1] E. Bernhardsson. Systems and methods of selecting\ncontent items using latent vectors, August 18 2015. US\nPatent 9,110,955.\n[2] Lukas Bossard, Michael Kuhn, and Roger Watten-\nhofer. Visually and Acoustically Exploring the High-\nDimensional Space of Music. In IEEE International\nConference on Social Computing (SocialCom), Van-\ncouver, Canada , August 2009.\n[3] Pedro Cano, Markus Koppenberger, and Nicolas Wack.\nContent-based music audio recommendation. In Pro-\nceedings of the 13th annual ACM international confer-\nence on Multimedia , pages 211–212. ACM, 2005.\n[4] Shuo Chen, Josh L Moore, Douglas Turnbull, and\nThorsten Joachims. Playlist prediction via metric em-\nbedding. In 18th ACM SIGKDD , 2012.\n[5] Shuo Chen, Jiexun Xu, and Thorsten Joachims. Multi-\nspace probabilistic sequence modeling. In 19th ACM\nSIGKDD , 2013.\n[6] Trevor F. Cox and M.A.A. Cox. Multidimensional\nScaling . Chapman and Hall/CRC, 2000.\n[7] Vin De Silva and Joshua B Tenenbaum. Sparse multi-\ndimensional scaling using landmark points. Technical\nreport, Technical report, Stanford University, 2004.\n[8] Arthur Flexer, Dominik Schnitzer, Martin Gasser, and\nGerhard Widmer. Playlist generation using start and\nend songs. In Juan Pablo Bello, Elaine Chew, and Dou-\nglas Turnbull, editors, ISMIR , pages 173–178, 2008.\n[9] Olga Goussevskaia, Michael Kuhn, Michael Lorenzi,\nand Roger Wattenhofer. From web to map: Exploring\nthe world of music. In Web Intelligence and Intelligent\nAgent Technology, 2008. WI-IAT’08 , volume 1, 2008.\n[10] Pedro Holanda, Bruno Guilherme, Joao Paulo V . Car-\ndoso, Ana Paula Couto da Silva, and Olga Gous-\nsevskaia. Mapeando o universo da midia usando dados\ngerados por usuarios em redes sociais online. In The\n33rd Brazilian Symposium on Computer Networks and\nDistributed Systems (SBRC) , 2015.\n[11] Pedro Holanda, Bruno Guilherme, Ana Paula Couto\nda Silva, and Olga Goussevskaia. TV goes social:\nCharacterizing user interaction in an online social net-\nwork for TV fans. In Engineering the Web in the Big\nData Era - 15th International Conference, ICWE 2015,\nRotterdam, The Netherlands, June 23-26, 2015, Pro-\nceedings , pages 182–199, 2015.\n[12] Pedro Holanda, Bruno Guilherme, Luciana Fujii Pon-\ntello, Ana Paula Couto da Silva, and Olga Gous-\nsevskaia. Mixtape application: Music map method-\nology and evaluation. Technical report, Department\nof Computer Science, Universidade Federal de Minas\nGerais, Belo Horizonte, MG, Brazil, May 2016.[13] Michael Kuhn, Roger Wattenhofer, and Samuel Wel-\nten. Social audio features for advanced music retrieval\ninterfaces. In Proceedings of the international confer-\nence on Multimedia , pages 411–420. ACM, 2010.\n[14] Beth Logan. Content-based playlist generation: Ex-\nploratory experiments. In ISMIR , 2002.\n[15] Franc ¸ois Maillet, Douglas Eck, Guillaume Desjardins,\nand Paul Lamere. Steerable playlist generation by\nlearning song similarity from radio station playlists. In\nISMIR , 2009.\n[16] Joshua L Moore, Shuo Chen, Thorsten Joachims, and\nDouglas Turnbull. Learning to embed songs and tags\nfor playlist prediction. In ISMIR , pages 349–354, 2012.\n[17] Joshua L Moore, Shuo Chen, Douglas Turnbull, and\nThorsten Joachims. Taste over time: The temporal dy-\nnamics of user preferences. In ISMIR , 2013.\n[18] Elias Pampalk, Tim Pohle, and Gerhard Widmer. Dy-\nnamic playlist generation based on skipping behavior.\nInISMIR , 2005.\n[19] Luciana Fujii Pontello, Pedro Holanda, Bruno Guil-\nherme, Joao Paulo V . Cardoso, Olga Goussevskaia,\nand Ana Paula Couto da Silva. Mixtape application:\nLast.fm data characterization. Technical report, De-\npartment of Computer Science, Universidade Federal\nde Minas Gerais, Belo Horizonte, MG, Brazil, May\n2016.\n[20] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun\nYan, and Qiaozhu Mei. Line: Large-scale information\nnetwork embedding. In 24th International Conference\non World Wide Web , pages 1067–1077, 2015.\n[21] J. B. Tenenbaum, V . Silva, and J. C. Langford. A\nGlobal Geometric Framework for Nonlinear Dimen-\nsionality Reduction. Science , 290(5500):2319–2323,\n2000.\n[22] Douglas R Turnbull, Justin A Zupnick, Kristofer B\nStensland, Andrew R Horwitz, Alexander J Wolf,\nAlexander E Spirgel, Stephen P Meyerhofer, and\nThorsten Joachims. Using personalized radio to en-\nhance local music discovery. In CHI’14 , 2014.\n[23] Aaron Van den Oord, Sander Dieleman, and Benjamin\nSchrauwen. Deep content-based music recommenda-\ntion. In Advances in Neural Information Processing\nSystems , 2013.460 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Human-Interactive Optical Music Recognition.",
        "author": [
            "Liang Chen",
            "Erik Stolterman",
            "Christopher Raphael"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416184",
        "url": "https://doi.org/10.5281/zenodo.1416184",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/106_Paper.pdf",
        "abstract": "We propose a human-driven Optical Music Recognition (OMR) system that creates symbolic music data from com- mon Western notation scores. Despite decades of devel- opment, OMR still remains largely unsolved as state-of- the-art automatic systems are unable to give reliable and useful results on a wide range of documents. For this rea- son our system, Ceres, combines human input and machine recognition to efficiently generate high-quality symbolic data. We propose a scheme for human-in-the-loop recog- nition allowing the user to constrain the recognition in two ways. The human actions allow the user to impose either a pixel labeling or model constraint, while the system re- recognizes subject to these constraints. We present evalua- tion based on different users’ log data using both Ceres and Sibelius software to produce the same music documents. We conclude that our system shows promise for transcrib- ing complicated music scores with high accuracy.",
        "zenodo_id": 1416184,
        "dblp_key": "conf/ismir/ChenSR16",
        "content": "HUMAN-INTERACTIVE OPTICAL MUSIC RECOGNITION\nLiang Chen\nIndiana University Bloomington\nchen348@indiana.eduErik Stolterman\nIndiana University Bloomington\nestolter@indiana.eduChristopher Raphael\nIndiana University Bloomington\ncraphael@indiana.edu\nABSTRACT\nWe propose a human-driven Optical Music Recognition\n(OMR) system that creates symbolic music data from com-\nmon Western notation scores. Despite decades of devel-\nopment, OMR still remains largely unsolved as state-of-\nthe-art automatic systems are unable to give reliable and\nuseful results on a wide range of documents. For this rea-\nson our system, Ceres , combines human input and machine\nrecognition to efﬁciently generate high-quality symbolic\ndata. We propose a scheme for human-in-the-loop recog-\nnition allowing the user to constrain the recognition in two\nways. The human actions allow the user to impose either\na pixel labeling or model constraint, while the system re-\nrecognizes subject to these constraints. We present evalua-\ntion based on different users’ log data using both Ceres and\nSibelius software to produce the same music documents.\nWe conclude that our system shows promise for transcrib-\ning complicated music scores with high accuracy.\n1. INTRODUCTION\nOptical Music Recognition (OMR), the musical cousin of\nOptical Character Recognition (OCR), seeks to convert\nscore images into symbolic music representations. Suc-\ncess in this endeavor would usher music into the 21st cen-\ntury alongside text, paving the way for large scale symbolic\nmusic libraries, digital music stands, computational musi-\ncology, and many other important applications.\nResearch in Optical Music Recognition (OMR) dates\nback to the 1960s with efforts by a large array of re-\nsearchers on many aspects of the problem [3, 5, 10–14, 17,\n18, 20, 21, 25, 29] including several overviews [6, 23], as\nwell as well-established commercial efforts [2] [1]. While\nevaluation of OMR is a challenging task in its own right\n[8], it seems fair to say that the problem remains largely\nunsolved, in spite of this long history. The reason is sim-\nply that OMR is very hard, representing a grand challenge\nof document recognition.\nOne reason OMR is so difﬁcult stems from the heavy\ntail of symbols used in music notation. While a small\ncore of symbols account for the overwhelming majority\nof ink on the printed page, these are complemented by a\nc/circlecopyrtLiang Chen, Erik Stolterman, Christopher Raphael. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Liang Chen, Erik Stolterman, Christopher\nRaphael. “Human-Interactive Optical Music Recognition”, 17th Interna-\ntional Society for Music Information Retrieval Conference, 2016.long list of familiar symbols that may be absent in many\nor most pages. These include repeat signs, D.S. and D.C.\ndirectives, a wide variety of possible ornaments and ar-\nticulations, harmonics, ﬁngerings, pedaling, arpeggiation,\nfermati, double sharps and ﬂats, pedaling, 1st and 2nd\nendings, repeats, etc. While each of these symbols can\nbe recognized with reasonable accuracy by fairly standard\nmeans, the symbols are rare enough that the unavoidable\nfalse positives they produce often outweigh the value of\nthe correct detections we may ﬁnd. This constitutes one\nof the the essential paradoxes of OMR: we cannot simply\nignore unusual symbols, though their inclusion often leads\nto worse performance overall.\nWe sometimes refer to the heavy tail described above\nas the “sprawl of OMR.” This sprawl is not limited to the\nrange of symbols, but also includes the many exceptions to\nfamiliar notational rules. For instance, in standard notation\nbeamed groups, notes and chords carry the majority of mu-\nsical content, thus their recognition must be central to any\nOMR effort. The construction of these symbols is highly\nrule-bound, arguing strongly for recognition approaches\nthat exploit the symbols’ grammatical nature. The difﬁ-\nculty here comes from the many special cases we must\naccount for. For example, note heads usually lie on one\nside of the stem, though chords with note heads on adja-\ncent staff positions are usually rendered with “wrong side”\nnote heads; beamed groups usually have closed note heads\nthough measured alternations between two pitches is often\nabbreviated with two open note heads in a beamed group;\nbeam groups usually have stems that go in a single direc-\ntion from the beam, though one occasionally sees both di-\nrections from a single beamed group; beamed groups are\nusually associated with a single staff, but can span both\nstaves of a grand staff when necessary; augmentation dots\nare generally placed to the right of the note heads they be-\nlong to, though dense voicing can force them to wander\nfar off their nominal positions. As with the heavy tail of\nsymbols, these special cases can all be modeled and rec-\nognized, though the results are not what one would hope\nfor. The majority of notation will notcontain these rarer\ncases; allowing for these exceptions when they do not oc-\ncur begs for trouble, reliably degrading the recognized re-\nsults. However, these exceptions are common enough that\nwe doubt a useful OMR system can be developed without\naccounting for them somehow. This is essentially the same\nparadox as that posed by the heavy tail: we must recognize\nthese unusual cases but cannot allow them to result in de-\ngraded performance overall. Dealing with this sprawl is a647central issue we address in this paper.\nIn light of these (and other) obstacles we doubt that any\nfully automatic approach to OMR will ever deal effectively\nwith the wide range of situations encountered in real life\nrecognition scenarios. For this reason we formulate the\nchallenge in terms of human-in-the-loop computing , devel-\noping a mixed-initiative system [15, 19, 27, 28] fusing both\nhuman and machine abilities. The inclusion of the human\nadds a new dimension to the OMR challenge, opening a\nvast expanse of unexplored potential.\nHowever, there is another reason for the human-\ncomputer formulation we favor. While there may be some\nuses for moderate quality symbolic music data, we believe\nthe most interesting applications require a level of accu-\nracy near that of published scores. The human will not\nonly play the important role of guiding the system toward\nthe required level of accuracy, but must also “bless” the\nresults when they are complete. We expect symbolic mu-\nsic data lacking this human imprimatur will be of dubious\nvalue.\nCommercial systems often deal with this issue by\npipelining their results into a score-writing program, thus\nallowing the user to ﬁx the many recognition problems.\nThis approach creates an artiﬁcial separation into the two\nphases of recognition and correction. Rather, since we re-\nquire a human to sign-off on the end result, we propose to\ninvolve her in the heart of the process as well.\nIn what follows we present the view of human-in-the-\nloop OMR taken in our Ceres system. Our essential idea\nis to allow the user two axes of control over the recogni-\ntion engine. In one axis the user chooses the model that\ncan be used for a given recognition task, specifying both\nthe exceptions to the “rules” discussed above as well as the\nrelevant variety of symbols to be used. In the other, the\nuser labels misrecognized pixels with the correct primitive\ntype, allowing the system to re-recognize subject to user-\nimposed constraints. This provides a simple interface in\nwhich the user can provide a wealth of useful knowledge\nwithout needing to understand the inner-workings and rep-\nresentations of the system. Thus we effectively address\nthecommunication issue of mixed-initiative literature. Our\nwork has commonalities with various other human-in-the-\nloop efforts such as [26, 30], though most notably with\nother approaches that employ constrained recognition as\ndriven by a user [4, 7, 24].\n2. HUMAN-INTERACTIVE SYSTEM\nThe main part of our Ceres OMR system deals with symbol\nrecognition, which occurs after the basic structure of the\npage has been identiﬁed. For each type of symbol or group\nof symbols we use a different graphical model . Here we\ndepict only the isolated chord graph in Fig. 1 which serves\nas a template for the others, and refer the more detailed\ndiscussions to our previous papers [9, 22].\nEach individual music symbol (beamed group, chord,\nclef-key-signature, slur, etc.) is grammatically constrained\nbased on a generative graph, enabling automatic, model-\ndriven, symbolic recognition. Perhaps more difﬁcult than\n/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1\n/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1\n/0/0/1/1/0/0/1/1/0/0/0/0/0/0\n/1/1/1/1/1/1/0/0/0\n/1/1/1/0/0/1/1/0/0/1/1\n/0/0/1/1/0/0/1/1\n/0/0/0\n/1/1/1/0/0/0\n/1/1/1/0/0/0\n/1/1/1/0/0/0\n/1/1/1\n/0/0/0\n/1/1/1/0/0/0\n/1/1/1\n/0/0/0\n/1/1/1/0/0/0\n/1/1/1/0/0/0\n/1/1/1/0/0/0\n/1/1/1/0/0/0\n/1/1/1/0/0/0\n/1/1/1\n/0/0/1/1half spaces\ninitial stembetween ledgernotes/spaces\nlines\nnotes/spaces\nbetween staff lines notes/spaces\non staff linenotes/spaces\non ledger\nlines(a)\n (b)\nFigure 1 : (a) Graphical model for isolated chord. (b) Sym-\nbol samples generated via random walk over the graphical\nmodel shown in (a).\nindividual symbol recognition is the challenge of decom-\nposing the image into disjoint, grammatically-consistent\nsymbols. To address this problem, our system identiﬁes\ncandidates for the different symbol types such as chords,\nbeamed groups, dynamics, slurs, etc. The user chooses\nsome of these candidates for interactive recognition. Af-\nter each recognition task is completed the resulting sym-\nbol will be saved, thus constituting a constraint for future\ncandidate detection and recognition — we constrain our\nsystem to identify mostly non-overlapping symbols.\nOur current human-driven system performs recognition\nin a symbol-by-symbol fashion as opposed to the measure-\nbased version proposed in [9]. The symbol-based scheme\nallows for a responsive and efﬁcient interface, which func-\ntions with the symbol recognizers implemented in our sys-\ntem . Here the human is allowed to interact with all\nthree steps: candidate identiﬁcation, interactive recogni-\ntion, and post-processing. In the candidate identiﬁcation\nand post-processing steps, the user directs the decision-\nmaking process by either selecting a system-proposed can-\ndidate, adding a missing candidate, or deleting an incor-\nrectly saved symbol. In the interactive recognition step,\nthe user actions impose extra labeling or model constraints\nto the recognizer, while the system automatically invokes\nre-recognition subject to these constraints .\nThe interactive recognition begins by performing fully\nautomatic recognition of the selected candidate. In many\ncases the result will be correct and will be saved. Other-\nwise the process iterates between human action and ma-\nchine re-recognition until it yields the correct result. In\neach iteration of this process the user will click on a partic-\nular pixel to input the corresponding primitive label (solid\nnote head, stem, beam, etc.) or change the model settings\nto an appropriate choice. The whole process requires only\nbasic knowledge of music notation and thus extends the\nrange of potential users.\nOur recognizers seek hypotheses that give the highest\nprobability to the pixel intensities, g(x), where xis a par-\nticular location. More explicitly, we regard a hypothesis, h,648 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016as a labeling of pixel sites, x, with models, Mh(x), where\nMh(x)∈L={b, w, t, 0}. The labels inLcorrespond to a\nprobability models for black, white, transitional, and back-\nground pixels ( Pb, Pw, Pt, P0). We measure the quality of\na hypothesis, h, by\nS(h) =/summationdisplay\nx∈D(h)logPMh(x)(g(x))\nP0(g(x))(1)\nwhere D(h)is the support of the hypothesis.\nAs mentioned, the hypotheses are highly constrained\nand we express these constraints through graphs as in Fig-\nure 1. We denote the possibility that a graph Ggenerates\na hypothesis hbyG⇒h. Thus our recognizers seek to\ncompute\nH∗\nG= arg max\n{h|G⇒h}S(h) (2)\nBy normalizing by the background model, P0, in Eqn. 1,\nthe result S(h) = 0 means that hexplains the pixels no bet-\nter or worse than the background model (iid sample from\nthe overall gray-level distribution), thus calibrating the in-\nterpretation of our scoring function and providing a natural\nthreshold for detection. This data model is more explicitly\nexplained in our previous works [9, 22].\n2.1 Label Constraints\nWhen the user labels a pixel, x, with a primitive (the most\nfundamental unit that constitutes a symbol), l, such as\nstem, ﬂag, open note head, etc., we create a constraint of\nthe form (x, l). If several such user labelings are required\nto correctly recognize a symbol we have a collection of\nconstraints of the form L={(xi, li) :i= 1, . . . , n}.\nEach time we get a new constraint the system re-recognizes\nthe current candidate subject to these user-imposed con-\nstraints. Thus we modify our original scoring function to\nbe\nS(h) =/summationdisplay\nx∈D(h)logPMh(x)(g(x))\nP0(g(x))+t(x, Qh(x)) (3)\nwhere\nt(x, Qh(x)) =/braceleftbigg−∞ x=xi, Qh(x)/negationslash=lisome i\n0otherwise\nHereQh(x)represents the primitive label of hypothesis h\nat location x, thus t(x, Qh(x))disallows hif the pixel label\nis inconsistent with the hypothesis.\nFig. 2 illustrates a use case for a label constraint. In this\nexample the original recognition, shown in the top panel of\nthe ﬁgure, misidentiﬁes the natural sign modifying the ’e’\nas an additional note head. In the top panel the user clicks\non this pixel, labeling it with the “natural” primitive type.\nThe bottom panel shows that re-recognition subject to the\nconstraint ﬁxes the problem.\nIn addition to primitive labels, the system also allows\nthe user to label a rectangle of pixels as “white space”, thus\ndisallowing the recognizer from covering these pixels with\n(a)\n(b)\nFigure 2 : (a) During the initial recognition the natural was\nmisidentiﬁed as a note head. The user is adds the correct\nprimitive label to any location within the the natural. (b)\nRe-recognition correctly identiﬁes the whole symbol by\nusing the user-imposed label constraint.\nany primitive symbol. In practice this turns out to be one of\nthe most powerful constraints as it addresses the common\ncase in which our recognition “spills over” into adjacent\nsymbols.\n2.2 Model Constraint\nModel constraints change the graph, G, over which we op-\ntimize. Our interface contains a number of toggle switches\neach enabling a special case of recognition thus enlarging\nthe graph. In general, the recognition works best when the\nminimal possible graph is chosen, though in many cases an\noverly permissive graph structure still produces the desired\nresult without help from the user. In this case we still opti-\nmize Eqn. 2, though with a new graph, G/prime, playing the role\nofG.\nFig. 3 gives an example in which an inappropriately\nrestrictive graph generates a result with many primitive\nerrors (top panel). In this case the original recognition\nwas done without allowing note and chords to span the\ngrand staff, as they do in this example. The result com-\npletely misses the penultimate note in the beamed group,\nwhile recognizing the last note with extraneous ledger lines\nwhich would be syntactically necessary when the note be-\nlongs to the upper staff. After enabling the grand staff abil-\nity we get the correct result in the bottom panel of the ﬁg-\nure.\nAfter the interactive recognition of a symbol is com-\nplete the user can save the symbol. When this occurs, we\nreset the list of pixel constraints placed by the user — these\ndo not carry forward to future recognition tasks. The pixels\ninvolved in the recognized hypothesis are considered un-\navailable in subsequent symbol recognition, thus express-Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 649(a)\n (b)\nFigure 3 : (a) Recognition using a beamed group graph that\ndoes not allow notes to span the grand staff. (b) Recogni-\ntion using grand-staff beamed group graph.\nFigure 4 : Human-driven (blue) and Machine-driven (red)\nactions in Ceres system. The possible state transitions are\nshown by arrowed connections.\ning the notion that the symbols cannot touch. Of course,\nsymbols do sometimes touch in practice, so we allow the\nuser to label a rectangle of pixels as “reuse,” thus allowing\nthe user to override the basic non-overlapping constraint\nwhen needed.\n3. USER INTERFACE\nOur interface allows the user to control and direct the\nrecognition process. The overall process is organized in\nterms of measures, while the interaction ﬂow within a mea-\nsure is described in the “action graph” of Fig. 4.\nIn the candidate proposal step (2nd level of the ﬁg-\nure), the user can switch between the different candidate\ntypes (beamed group, isolated chord, slur, dynamics, etc.).\nWithin each candidate type the user is presented with a\nleft-to-right sequence of candidates she may choose to rec-\nognize or skip over. The color of the highlighted candidate\nreﬂects the candidate’s type, also showing the direction of\nthe stem, beam, and slurs with arrow signs, as this infor-\nmation is needed by the recognizers. The interface for this\nphase is shown in Fig. 5.\nAfter a candidate is chosen, the system moves to the\nsymbol recognition step. In this step, the system collab-\norates with user to improve the recognition in an iterative\nprocess. In each iteration the user either accepts the current\nrecognition or imposes a new constraint (label or model),\nas discussed in Section 2. For a labeling constraint, the\nuser inputs the pixel labels through a message box after\nclicking on the desired pixel position, as in Fig. 2a. For a\nmodel constraint, the user changes the current settings on\nthe checkboxes or pull-down menu. The interface is shown\nin Fig. 6.\n(a)\n (b)\n (c)\nFigure 5 : (a) The system detects and indicates a stem-up\nchord candidate for the user; (b) The system detects and\nindicates a stem-up beamed group candidates for the user;\n(c) The user adds a missing chord candidate.\nFigure 6 : Checkboxes and pull-down menu for different\nmodel settings.\nThe system uses different colors to distinguish the cur-\nrent symbol from saved symbols. When the user wants\nto revisit an incorrectly saved symbol, she can select and\ndelete the symbol before redoing the recognition.\nCeres has shortcut keys designed for the user to move\nfrom one step to another one, and also has a “cancel” key\nthat allows the user to exit this process while moving to\na “default” state. These interface units together constitute\nthe visible part of Ceres’ human-in-the-loop system.\n4. EVALUATION\nWe evaluate our system both in terms of accuracy and\nspeed. Both criteria are important since we believe the\nmost interesting applications of OMR require accuracy on\npar with published scores, while it won’t be possible to cre-\nate large quantities of such data through OMR unless the\nprocess is highly efﬁcient.\nWe measure accuracy here at the primitive level (beams,\nﬂags, note heads, accidentals, etc.), rather than, say, in\nterms of pitch and rhythm as in [16]. We prefer primitive\nevaluation because it is generic (all symbols are evaluated\nin the same way), it allows for all symbols our recognizer\ntreats — not just those carrying pitch and rhythm infor-\nmation, and it is easy to relate primitive error analysis to\nspeciﬁc aspects of the recognition engine.\nThe test set consists of ﬁrst three pages of the Bre-\nitkopf and H ¨artel 1862-90 edition of Beethoven’s Piano650 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016(a)\n (b)\n(c)\n (d)\nFigure 7 : Clock time versus accuracy for novice and ex-\nperienced Ceres users to generate one page: (a) page 1, (b)\npage 2, (c) page 3, (d) average performance.\nSonata No. 23, (the “Appassionata”), having 1606,1501 ,\nand1651 primitives respectively. We annotated the symbol\nprimitives by hand with an interactive tool, thus creating\nour ground truth. In doing this we left out a few symbols\nappearing in the document that our system does not yet\nhandle: grace notes, text and fermati . Our ground truth ac-\ncounts for the overwhelming majority of what appears on\nthe pages.\nAfter recognition we decompose our structured results\ninto an unstructured list of primitives, counting both false\npositives and false negatives. A recognized symbol counts\nas a false positive if its distance to all ground truth primi-\ntives of the same type is greater than some threshold. Anal-\nogously, a false negative occurs if a ground truth primi-\ntive is not sufﬁciently close to any recognized primitive.\nAll symbol-to-symbol distances are measured in terms of\na ﬁxed reference point on each symbol.\nOur subjects contained both novice users having about\nan hour of training, and more experienced users who were\ninvolved in the development of the user interface. Figure 7\nshows both clock time and accuracy (an F-score) measured\nfor these test subjects separately on each page. The ﬁgure\nshows overall error rates in the range of 1%, short of our\neventual goal, but also showing that highly accurate results\nare within reach. The effect of user experience is evident\nboth in accuracy and speed, though it is worth noting that\nthe novice users were still able to get usable results from\nCeres .\nA primitive-level breakdown of errors is detailed in Fig-\nure 8, which counts both false positives and negatives by\neach class and user. A number of the errors are due to\nsmall symbols, such as augmentation dots, staccato mark-\nings, and short slurs. As illustrated in Section 3, our sys-\ntem superimposes the recognized results over the original\nimage, usually making recognition errors obvious, though\n(a)\n(b)\nFigure 8 : Distribution of Ceres -user-generated (a) false\npositive and (b) false negative errors with respect to their\nprimitive labels on all the three pages.\nthey are occasionally hard to see with small symbols. This\nhighlights the need to explore better ways of visualizing re-\nsults. The ﬁngering errors were mostly due to our system’s\ninability to recognize markups in non-standard positions\nsuch as to the side of a note head — an issue we have since\naccounted for. One can also see that a number of the er-\nrors come from ledger lines. This is due to a bug causing\nour system to occasionally produce syntactically impossi-\nble conﬁgurations of these primitives. These observations\nshow the virtue of primitive-based evaluation since the er-\nrors are easily traced to their root causes.\nWe wanted to compare with a system other than our\nown, though between-system comparisons in OMR are\nchallenging due to differences in the representations of\nboth intermediate and end results. While commercial\nscore-writing programs have different goals than OMR\nsystems, both create symbolic representations of music\ndocuments that can be used to generate score images.\nAside from these basic similarities there are a great many\ndifferences that may call comparisons into question. Still,\nin order to gain a point of reference for evaluation we com-\npared our results with the commercial score-writing pro-\ngram, Sibelius .\nDue to the steep learning curve involved with this pro-\ngram we engaged a professional Sibelius user with many\nyears of professional experience, charging him with the\ntask of recreating the original notation from scratch ac-\ncording to our test images. Even when directed otherwise,\nmusic copyists can substitute equivalent or nearly equiva-\nlent notation making it impossible to ﬁnd a one-to-one cor-\nrespondence between primitives. Thus we could not make\nmeaningful accuracy comparisons with Sibelius .\nHowever, we can compare the time to create the sym-\nbolic results as in Figure 9. This ﬁgure shows the necessary\nclock time to create the various pages. The results varyProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 651Figure 9 : Clock time versus number of primitives for each\npage. Each point represents a single user from one of the\nfollowing three groups: professional Sibelius user, experi-\nenced Ceres user or novice Ceres user.\nsigniﬁcantly from page to page, but show Ceres as com-\npetitive in all cases, and on two out of three pages showing\nsigniﬁcantly faster results than Sibelius with far less expe-\nrienced users. Fig. 10 shows a similar comparison using\nkeystrokes and mouse clicks as the measure of user effort.\nHere Ceres is seen to be considerably more efﬁcient than\nSibelius producing the results with much less user activity.\nThis is because notation programs require detailed super-\nvision while our system ofﬂoads as much work as possi-\nble to the machine. We also see from this ﬁgure that the\nexperienced Ceres user makes more efﬁcient strategies by\nminimizing the number of mouse and key actions.\nThe representations produced by these two systems\nhave little in common, with different strengths and weak-\nnesses for generating notation. Sibelius understands more\nof the inherent relations between symbols which must be\npreserved under renotation. Ceres captures a great deal\nof information about notational decisions (groupings, stem\ndirections, spacing, etc.) which is also useful in renotation.\nThe images at the website below1show notation generated\nfrom the two representations thus allowing for a subjective\ncomparison of the two representations for renotation.\n5. CONCLUSION\nWe have proposed a human-in-the-loop scheme for OMR\nthat addresses several of the core difﬁculties of OMR. By\nallowing the user to select parameters of the models and\nsymbol vocabulary, we deal with the heavy tail of rare sym-\nbols and notational exceptions. We also address the funda-\nmental challenge of segmentation through recognition —\nnow facilitated by a human guide. Finally, we demonstrate\na feasible means to achieve the level of accuracy we believe\nis essential for successful application of OMR. The exper-\niments show that our system has the potential to produce\nhigh-quality symbolic data more efﬁciently than a score-\nwriting system, though we believe we must still improve\n1http://music.informatics.indiana.edu/papers/ismir16/\n(a)\n (b)\n(c)\n (d)\nFigure 10 : Clock time versus number of mouse and key\nactivities used for each page: (a) page 1, (b) page 2, (c)\npage 3, (d) average performance. Each point represents a\nsingle user from one of the following three groups: pro-\nfessional Sibelius user, experienced Ceres user or novice\nCeres user.\nsigniﬁcantly on this benchmark for our system to gain ac-\nceptance.\nOne promising application of Ceres -generated data is\nrenotation . The current renotated pages are basically a\none-to-one reconstruction of the original score, essentially\ndenoising the image. We continue to explore more gen-\neral renotation problems allowing various transformations\nof existing notation such as reformatting into arbitrarily-\nsized rectangles, transposition, and construction of parts\nfrom a score. This line of work will facilitate the ﬂexible\nrendering of scores for electronic music readers.\nWe also remain engaged with improving the perfor-\nmance of our system. On one hand, we must continue to\nreﬁne the core recognition abilities of our system, as these\npromise to improve both accuracy and speed of our system\nby handling a greater proportion of the work through auto-\nmatic means. On the other hand we see considerable room\nfor improvement and creativity in constructing the inter-\nface. We are interested in intelligent interactive approaches\nthat increase the efﬁciency of the approach, e.g. automatic\nplanning of human-machine-collaborated actions to min-\nimize time cost, active prediction of human labeling and\nmodel selection, intelligent aggregation of multiple con-\nstrains through MIDI keyboard input, or adaptive learning\nto better recognize new documents. These interesting dis-\ncussions are a part of our future plan.\n6. REFERENCES\n[1] Sharpeye. http://www.music-scanning.\ncom/sharpeye.html .\n[2] Smartscore. http://www.musitek.com .652 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016[3] P. Bellini, I. Bruno, and P. Nesi. Assessing opti-\ncal music recognition tools. Computer Music Journal ,\n31(1):68–93, 2007.\n[4] Taylor Berg-Kirkpatrick and Dan Klein. Improved\ntypesetting models for historical ocr. In ACL (2) , pages\n118–123, 2014.\n[5] H. Bitteur. Audiveris. https://audiveris.\nkenai.com , 2014.\n[6] D. Blostein and H. S. Baird. A critical survey of music\nimage analysis. In H. Bunke H. S. Baird and K. Ya-\nmamoto, editors, Structured Document Image Analy-\nsis, pages 405–434. 1992.\n[7] Nicholas J Bryan, Gautham J Mysore, and Ge Wang.\nSource separation of polyphonic music with interactive\nuser-feedback on a piano roll display. In ISMIR , pages\n119–124, 2013.\n[8] Donald Byrd and Megan Schindele. Prospects for im-\nproving omr with multiple recognizers. In ISMIR ,\npages 41–46, 2006.\n[9] Liang Chen and Christopher Raphael. Human-\ndirected optical music recognition. Electronic Imaging ,\n2016(17):1–9, 2016.\n[10] G. S. Choudhury, T. DiLauro, M. Droettboom, I. Fu-\njinaga, B. Harrington, and K. MacMillan. Optical mu-\nsic recognition system within a large-scale digitization\nproject. In ISMIR , 2000.\n[11] B. Couasnon and B. Retif. Using a grammar for a re-\nliable full score recognition system. In ICMC , pages\n187–194, 1995.\n[12] H. Fahmy and D. Blostein. A graph-rewriting paradigm\nfor discrete relaxation: Application to sheet-music\nrecognition. International Journal of Pattern Recogni-\ntion and Artiﬁcial Intelligence , 12(6):763–799, 1998.\n[13] I. Fujinaga. Optical music recognition system which\nlearns. In Proceedings of the SPIE - The International\nSociety for Optical Engineering , volume 1785, pages\n210–17, 1993.\n[14] I. Fujinaga. Exemplar-based learning in adaptive op-\ntical music recognition system. In ICMC , volume 12,\npages 55–56, 1996.\n[15] Ferguson G. and Allen G. Trips: An intelligent inte-\ngrated problem-solving assistant. In Proc. of the 15th\nNational Conf. on Art. Intel. , pages 567–573, 1998.\n[16] Rong Jin and Christopher Raphael. Interpreting rhythm\nin optical music recognition. In ISMIR , pages 151–156,\n2012.\n[17] G. Jones, B. Ong, I. Bruno, and K. Ng. Optical mu-\nsic imaging: Music document digitisation, recogni-\ntion, evaluation, and restoration. Interactive Multime-\ndia Music Technologies , pages 50–79, 2008.[18] G. E. Kopec, P. A. Chou, and D. A. Maltz. Markov\nsource model for printed music decoding. In Proceed-\nings of the SPIE - The International Society for Optical\nEngineering , volume 2422, pages 115–25, 1995.\n[19] Myers K. L., Jarvis P. A., Tyson W. M., and Wolver-\nton M. J. A mixed-initiative framework for rubust plan\nsketching. In Proc. 13th Int. Conf. on Automated Plan-\nning and Scheduling , pages 256–265, 2003.\n[20] K. C. Ng and R. D. Boyle. Recognition and reconstruc-\ntion of primitives in music scores. Image and Vision\nComputing , 14(1):39–46, 1996.\n[21] L. Pugin, J. A. Burgoyne, and I. Fujinaga. Map adapta-\ntion to improve optical music recognition of early mu-\nsic documents using hidden markov models. In ISMIR ,\npages 513–516, 2007.\n[22] Christopher Raphael and Jingya Wang. New ap-\nproaches to optical music recognition. In ISMIR , pages\n305–310, 2011.\n[23] A. Rebelo, G. Capela, and J. S. Cardoso. Optical recog-\nnition of music symbols. International Journal on\nDocument Analysis and Recognition , 13:19–31, 2009.\n[24] Ver ´onica Romero, Alejandro H Toselli, Luis\nRodr ´ıguez, and Enrique Vidal. Computer assisted\ntranscription for ancient text images. In Image\nAnalysis and Recognition , pages 1182–1193. 2007.\n[25] F. Rossant and I. Bloch. Robust and adaptive omr sys-\ntem including fuzzy modeling, fusion of musical rules,\nand possible error detection. EURASIP Journal on Ap-\nplied Signal Processing , 2007.\n[26] Branson S., Wah C., Babenko B., Schroff F., Welinder\nP., and ˜Belongie S. Perona P. Visual recognition with\nhumans in the loop. In ECCV , 2010.\n[27] Cox M. T., Edwin G., Balasubramanian K., and\nElahi M. Multiagent goal transformation and mixed-\ninitiative plannng using prodigy/agent. In Proc. 4th Int.\nMulticonf. on Systemics, Cybernetics and Informatics ,\nvolume 7, pages 1–6, 2001.\n[28] Cox M. T. and Veloso M. M. Supporting combined\nhuman and machine planning: An inteface for plan-\nning by analogical reasoning. In Proc. 2nd Int. Conf.\non Case-Based Reasoning , pages 531–540, 1997.\n[29] V . Viro. Peachnote: Music score search and analysis\nplatform. In ISMIR , pages 360–363, 2011.\n[30] Alexander Waibel, Rainer Stiefelhagen, Rolf Carlson,\nJ Casas, Jan Kleindienst, Lori Lamel, Oswald Lanz,\nDjamel Mostefa, Maurizio Omologo, Fabio Pianesi,\net al. Computers in the human interaction loop. In\nHandbook of Ambient Intelligence and Smart Environ-\nments , pages 1071–1116. 2010.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 653"
    },
    {
        "title": "Automatic Tagging Using Deep Convolutional Neural Networks.",
        "author": [
            "Keunwoo Choi",
            "György Fazekas",
            "Mark B. Sandler"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416254",
        "url": "https://doi.org/10.5281/zenodo.1416254",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/009_Paper.pdf",
        "abstract": "We present a content-based automatic music tagging algo- rithm using fully convolutional neural networks (FCNs). We evaluate different architectures consisting of 2D con- volutional layers and subsampling layers only. In the ex- periments, we measure the AUC-ROC scores of the archi- tectures with different complexities and input types using the MagnaTagATune dataset, where a 4-layer architecture shows state-of-the-art performance with mel-spectrogram input. Furthermore, we evaluated the performances of the architectures with varying the number of layers on a larger dataset (Million Song Dataset), and found that deeper mod- els outperformed the 4-layer architecture. The experiments show that mel-spectrogram is an effective time-frequency representation for automatic tagging and that more com- plex models benefit from more training data.",
        "zenodo_id": 1416254,
        "dblp_key": "conf/ismir/ChoiFS16",
        "content": "AUTOMATIC TAGGING USING\nDEEP CONVOLUTIONAL NEURAL NETWORKS\nKeunwoo Choi, Gy ¨orgy Fazekas, Mark Sandler\nQueen Mary University of London\n{keunwoo.choi, g.fazekas, mark.sandler }@qmul.ac.uk\nABSTRACT\nWe present a content-based automatic music tagging algo-\nrithm using fully convolutional neural networks (FCNs).\nWe evaluate different architectures consisting of 2D con-\nvolutional layers and subsampling layers only. In the ex-\nperiments, we measure the AUC-ROC scores of the archi-\ntectures with different complexities and input types using\ntheMagnaTagATune dataset, where a 4-layer architecture\nshows state-of-the-art performance with mel-spectrogram\ninput. Furthermore, we evaluated the performances of the\narchitectures with varying the number of layers on a larger\ndataset ( Million Song Dataset ), and found that deeper mod-\nels outperformed the 4-layer architecture. The experiments\nshow that mel-spectrogram is an effective time-frequency\nrepresentation for automatic tagging and that more com-\nplex models beneﬁt from more training data.\n1. INTRODUCTION\nMusic tags are a set of descriptive keywords that convey\nhigh-level information about a music clip, such as emo-\ntion (sad, anger, happy), genre (jazz, classical) and instru-\nmentation (guitar, strings, vocal, instrumental). Since tags\nprovide high-level information from the listeners’ perspec-\ntives, they can be used for music discovery and recommen-\ndation.\nAutomatic tagging is a classiﬁcation task that aims to\npredict music tags using the audio signal. This requires\nextracting acoustic features that are good estimators of the\ntype of tags we are interested in, followed by a single or\nmulti-label classiﬁcation or in some cases, regression stage.\nFrom the perspective of feature extraction, there have been\ntwo main types of systems proposed in the literature. Con-\nventionally, feature extraction relies on a signal processing\nfront-end in order to compute relevant features from time\nor frequency domain audio representation. The features are\nthen used as input to the machine learning stage. However,\nit is difﬁcult to know what features are relevant to the task\nat hand. Although feature selection have been widely used\nto solve this problem [29], clear recommendations which\nc/circlecopyrtKeunwoo Choi, Gy ¨orgy Fazekas, Mark Sandler.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Keunwoo Choi, Gy ¨orgy Fazekas,\nMark Sandler. “Automatic tagging using deep convolutional neural net-\nworks’, 17th International Society for Music Information Retrieval Con-\nference, 2016.provide good association of features with tag categories are\nyet to emerge. A more recent approach uniﬁes feature ex-\ntraction with machine learning to allow relevant features to\nbe learnt automatically. This approach is known as feature\nlearning and requires deep neural networks (DNNs).\nAggregating hand-crafted features for music tagging was\nintroduced in [25]. Several subsequent works rely on a\nBag of frames approach - where a collection of features\nare computed for each frame and then statistically aggre-\ngated. Typical features are designed to represent phys-\nical or perceived aspects of sound and include MFCCs,\nMFCC derivatives, and spectral features (e.g. spectral roll-\noff and centroids). Since these are frame-level features,\ntheir statistics such as mean and variance are computed\n[25], or they are clustered and vector quantised [15] to ob-\ntain clip-level features. Finally, classiﬁers such as k-NN or\nSupport Vector Machines are applied to predict tags.\nAs alternative to the above systems, DNNs have re-\ncently become widely used in audio analysis, following\ntheir success in computer vision, speech recognition [19]\nand auto-tagging [6, 8, 18, 28]. From an engineering per-\nspective, DNNs sidestep the problem of creating or ﬁnding\naudio features relevant to a task. Their general structure\nincludes multiple hidden layers with hidden units trained\nto represent some underlying structure in data.\nIn computer vision, deep convolutional neural networks\n(CNNs) have been introduced because they can simulate\nthe behaviour of the human vision system and learn hier-\narchical features, allowing object local invariance and ro-\nbustness to translation and distortion in the model [14].\nCNNs have been introduced in audio-based problems for\nsimilar reasons, showing state-of-the-art performance in\nspeech recognition [19] and music segmentation [26].\nSeveral DNN-related algorithms have been proposed for\nautomatic music tagging too. In [5] and [28], spherical k-\nmeans and multi-layer perceptrons are used as feature ex-\ntractor and classiﬁer respectively. Multi-resolution spec-\ntrograms are used in [5] to leverage the information in\nthe audio signal on different time scales. In [28], pre-\ntrained weights of multilayer perceptrons are transferred\nin order to predict tags for other datasets. A two-layer con-\nvolutional network is used in [6] with mel-spectrograms as\nwell as raw audio signals as input features. In [18], bag-\nof-features are extracted and input to stacked Restricted\nBoltzmann machines (RBM).\nIn this paper, we propose an automatic tagging algo-\nrithm based on deep Fully Convolutional Networks (FCN).805FCNs are deep convolutional networks that only consists\nof convolutional layers (and subsampling) without any fully-\nconnected layer. An FCN maximises the advantages of\nconvolutional networks. It reduces the number of parame-\nters by sharing weights and makes the learned features in-\nvariant to the location on the time-frequency plane of spec-\ntrograms, i.e., it provides advantages over hand-crafted and\nstatistically aggregated features by allowing the networks\nto model the temporal and harmonic structure of audio sig-\nnals. In the proposed architecture, three to seven convo-\nlutional layers are employed combined with subsampling\nlayers, resulting in reducing the size of feature maps to\n1×1and making the whole procedure fully convolutional.\n2D convolutional kernels are then adopted to take the local\nharmonic relationships into account.\nWe introduce CNNs in detail in Section 2 and deﬁne\nthe problem in Section 3. Our architectures are explained\nin Section 4, and their evaluation is presented in Section 5,\nfollowed by conclusion in Section 6.\n2. CNNS FOR MUSIC SIGNAL ANALYSIS\n2.1 Motivation for using CNNs for audio analysis\nIn this section, we review the properties of CNNs with re-\nspect to music signals. The development of CNNs was mo-\ntivated by biological vision systems where information of\nlocal regions are repeatedly captured by many sensory cells\nand used to capture higher-level information [14]. CNNs\nare therefore designed to provide a way of learning robust\nfeatures that respond to certain visual objects with local,\ntranslation, and distortion invariances. These advantages\noften work well with audio signals too, although the topol-\nogy of audio signals (or their 2D representations) is not the\nsame as that of a visual image.\nCNNs have been applied to various audio analysis tasks,\nmostly assuming that auditory events can be detected or\nrecognised by seeing their time-frequency representations.\nAlthough the advantage of deep learning is to learn the fea-\ntures, one should carefully design the architecture of the\nnetworks, considering to what extent the properties (e.g.\ninvariances) are desired.\nThere are several reasons which justify the use CNNs in\nautomatic tagging. First, music tags are often considered\namong the topmost high-level features representing song-\nlevel information above intermediate level features such\nas chords, beats, tonality and temporal envelopes which\nchange over time and frequency. This hierarchy ﬁts well\nwith CNNs as it is designed to learn hierarchical features\nover multilayer structures. Second, the properties of CNNs\nsuch as translation, distortion, and local invariances can be\nuseful to learn musical features when the target musical\nevents that are relevant to tags can appear at any time or\nfrequency range.\n2.2 Design of CNNs architectures\nThere have been many variants of applying CNNs to audio\nsignals. They differ by the types of input representations,\nconvolution axes, sizes and numbers of convolutional ker-\nnels or subsamplings and the number of hidden layers.2.2.1 TF-representation\nMel-spectrograms have been one of the widespread fea-\ntures for tagging [5], boundary detection [26], onset de-\ntection [21] and latent feature learning [27]. The use of the\nmel-scale is supported by domain knowledge about the hu-\nman auditory system [17] and has been empirically proven\nby performance gains in various tasks [6, 18, 21, 26, 27].\nThe Constant-Q transform (CQT) has been used predomi-\nnantly where the fundamental frequencies of notes should\nbe precisely identiﬁed, e.g. chord recognition [10] and\ntranscription [22].\nThe direct use of Short-time Fourier Transform (STFT)\ncoefﬁcients is preferred when an inverse transformation is\nnecessary [3, 23]. It has been used in boundary detec-\ntion [7] for example, but it is less popular in comparison\nto its ubiquitous use in digital signal processing. Com-\npared to CQT, the frequency resolution of STFT is inade-\nquate in the low frequency range to identify the fundamen-\ntal frequency. On the contrary, STFT provides ﬁner res-\nolutions than mel-spectrograms in frequency bands >2kHz\ngiven the same number of spectral bands which may be de-\nsirable for some tasks. So far, however, it has not been the\nmost favoured choice.\nMost recently, there have been studies focusing on learn-\ning an optimised transformation from raw audio given a\ntask. These are called end-to-end models and applied both\nfor music [6] and speech [20]. The performance is compa-\nrable to the mel-spectrogram in speech recognition [20]. It\nis also noteworthy that the learned ﬁlter banks in both [6]\nand [20] show similarities to the mel-scale, supporting the\nuse of the known nonlinearity of the human auditory sys-\ntem.\n2.2.2 Convolution - kernel sizes and axes\nEach convolution layer of size H×W×Dlearns Dfea-\ntures of H×W, where HandWrefer to the height and\nthe width of the learned kernels respectively. The kernel\nsize determines the maximum size of a component it can\nprecisely capture. If the kernel size is too small, the layer\nwould fail to learn a meaningful representation of shape (or\ndistribution) of the data. For this reason, relatively large-\nsized kernels such as 17×5are proposed in [10]. This is\nalso justiﬁed by the task (chord recognition) where a small\nchange in the distribution along the frequency axis should\nyield different results and therefore frequency invariance\nshouldn’t be allowed.\nThe use of large kernels may have two drawbacks how-\never. First, it is known that the number of parameters per\nrepresentation capacity increases as the size of kernel in-\ncreases. For example, 5×5convolution can be replaced\nwith two stacked 3×3convolutions, resulting in a fewer\nnumber of parameters. Second, large kernels do not allow\ninvariance within its range.\nThe convolution axes are another important aspect of\nconvolution layers. For tagging, 1D convolution along the\ntime axis is used in [6] to learn the temporal distribution,\nassuming that different spectral band have different distri-\nbutions and therefore features should be learned per fre-806 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016quency band. In this case, the global harmonic relation-\nship is considered at the end of the convolution layers and\nfully-connected layers follow to capture it. In contrast,\n2D convolution can learn both temporal and spectral struc-\ntures and has already been used in music transcription [22],\nonset detection [21], boundary detection [26] and chord\nrecognition [10].\n2.2.3 Pooling - sizes and axes\nPooling reduces the size of feature map with an operation,\nusually a maxfunction. It has been adopted by the majority\nof works that are relying on CNN structures. Essentially,\npooling employs subsampling to reduce the size of feature\nmap while preserving the information of an activation in\nthe region, rather than information about the whole input\nsignal.\nThis non-linear behaviour of subsampling also provides\ndistortion and translation invariances by discarding the orig-\ninal location of the selected values. As a result, pool-\ning size determines the tolerance of the location variance\nwithin each layer and presents a trade-off between two as-\npects that affect network performance. If the pooling size\nis too small, the network does not have enough distortion\ninvariance, if it is too large, the location of features may be\nmissed when they are needed. In general, the pooling axes\nmatch the convolution axes, although it is not necessarily\nthe case. What is more important to consider is the axis in\nwhich we need invariance. For example, time-axis pool-\ning can be helpful for chord recognition, but it would hurt\ntime-resolution in boundary detection methods.\n3. PROBLEM DEFINITION\nAutomatic tagging is a multi-label classiﬁcation task , i.e.,\na clip can be tagged with multiple tags. It is different from\nother audio classiﬁcation problems such as genre classiﬁ-\ncation, which are often formalised as a single-label clas-\nsiﬁcation problem. Given the same number of labels, the\noutput space of multi-label classiﬁcation can exponentially\nincrease compared to single-label classiﬁcation. Accord-\ningly, multi-label classiﬁcation tasks require more data, a\nmodel with larger capacity and efﬁcient optimisation meth-\nods to solve. If there are Kexclusive labels, the classiﬁer\nonly needs to be able to predict one among Kdifferent\nvectors, which are one-hot vectors . With multiple labels\nhowever, the number of cases increases up to 2K.\nIn crowd-sourced music tag datasets [2,13], most of the\ntags are false (0) for most of the clips, which makes ac-\ncuracy or mean square error inappropriate as a measure.\nTherefore we use the Area Under an ROC (Receiver Oper-\nating Characteristic) Curve abbreviated as AUC. This mea-\nsure has two advantages. It is robust to unbalanced datasets\nand it provides a simple statistical summary of the perfor-\nmance in a single value. It is worth noting that a random\nguess is expected to score an AUC of 0.5while a perfect\nclassiﬁcation 1.0, i.e., the effective range of AUC spans\nbetween [ 0.5,1.0].FCN-4\nMel-spectrogram (input: 96×1366×1)\nConv 3×3×128\nMP ( 2,4)(output: 48×341×128)\nConv 3×3×384\nMP ( 4,5)(output: 24×85×384)\nConv 3×3×768\nMP ( 3,8)(output: 12×21×768)\nConv 3×3×2048\nMP ( 4,8)(output: 1×1×2048)\nOutput 50×1 (sigmoid)\nTable 1 . The conﬁguration of FCN-4\n4. PROPOSED ARCHITECTURE\nTable 1 and Figure 1 show one of the proposed architec-\ntures, a 4-layer FCN ( FCN-4 ) which consists of 4convolu-\ntional layers and 4max-pooling layers. This network takes\na log-amplitude mel-spectrogram sized 96×1366 as input\nand predicts a 50dimensional tag vector. The input shape\nfollows the size of the mel-spectrograms as explained in\nSection 5.1.\nThe architecture is extended to deeper ones with 5,6\nand7layers ( FCN-{5, 6, 7}). The number of feature maps\nand subsampling sizes are summarised in Table 2. The\nnumber of feature maps of FCN-5 are adjusted based on\nFCN-4, making the hierarchy of the learned features deeper.\nFCN-6 and FCN-7 however have additional 1×1convolu-\ntional layers(s) on the top of FCN-5. Here, the motivation\nof1×1is to take advantage of increased nonlinearity [16]\nin the ﬁnal layer, assuming that the ﬁve layers of FCN-5\nare sufﬁcient to learn hierarchical features. An architec-\nture with 3layers (FCN-3) is also tested as a baseline with\na pooling strategy of [(3,5),(4,16),(8,17)] and [256, 768,\n2048] feature maps. The number of feature maps are ad-\njusted based on FCN-4 while the pooling sizes are set to\nincrease in each layer so that low-level features can have\nsufﬁcient resolutions.\nFigure 1 . A block diagram of the proposed 4-layer archi-\ntecture, FCN-4 . The numbers indicate the number of fea-\nture maps (i.e. channels) in each layer. The subsampling\nlayers decrease the size of feature maps to 1×1while the\nconvolutional layers increase the depth to 2048 .Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 807Other conﬁgurations follow the current generic optimi-\nsation methods in CNNs. Rectiﬁed Linear Unit (ReLU) is\nused as an activation function in every convolutional layer\nexcept the output layer, which uses Sigmoid to squeeze the\noutput within [0, 1]. Batch Normalisation is added after ev-\nery convolution and before activation [11]. Dropout of 0.5\nis added after every max-pooling layer [24]. This acceler-\nates the convergence while dropout prevents the network\nfrom overﬁtting.\nHomogeneous 2D ( 3×3) convolutional kernels are used\nin every convolutional layers except the ﬁnal 1×1convolu-\ntion. 2D kernels are adopted in order to encourage the sys-\ntem to learn the local spectral structures. The kernels at the\nﬁrst convolutional layer cover 64ms×72Hz. The cover-\nage increases to 7s×692Hz at the ﬁnal 3×3convolutional\nlayer when the kernel is at the low-frequency. The time\nand frequency resolutions of feature maps become coarser\nas the max-pooling layer reduces their sizes, and ﬁnally a\nsingle value (in a 1×1feature map) represents a feature of\nthe whole signal.\nSeveral features in the proposed architecture are distinct\nfrom previous studies. Compared to [28] and [18], the pro-\nposed system takes advantages of convolutional networks,\nwhich do not require any pre-training but fully trained in\na supervised fashion. The architecture of [6] may be the\nmost similar to ours. It takes mel-spectrogram as input,\nuses two 1D convolutional layers and two (1D) max-pooling\nlayers as feature extractor, and employs one fully-connected\nlayer as classiﬁer. The proposed architectures however\nconsist of 2D convolution and pooling layers, to take the\npotential local harmonic structure into account. Results\nfrom many 3s clips are averaged in [6] to obtain the ﬁnal\nprediction. The proposed model however takes the whole\n29.1s signal as input, incorporating a temporal nonlinear\naggregation into the model.\nThe proposed architectures can be described as fully-\nFCN-5 FCN-6 FCN-7\nMel-spectrogram (input: 96×1366×1)\nConv 3×3×128\nMP ( 2,4)(output: 48×341×128)\nConv 3×3×256\nMP ( 2,4)(output: 24×85×256)\nConv 3×3×512\nMP ( 2,4)(output: 12×21×512)\nConv 3×3×1024\nMP ( 3,5)(output: 4×4×1024)\nConv 3×3×2048\nMP ( 4,4)(output: 1×1×2048)\n·Conv 1×1×1024 Conv 1×1×1024\n· Conv 1×1×1024\nOutput 50×1 (sigmoid)\nTable 2 . The conﬁgurations of 5, 6, and 7-layer architec-\ntures. The only differences are the number of additional\n1×1 convolution layers.convolutional networks (FCN) since they only consist of\nconvolutional and subsampling layers. Conventional CNNs\nhave been equipped with fully-connected layers at the end\nof convolutional layers, expecting each of them to perform\nas a feature extractor and classiﬁer respectively. In general\nhowever, the fully connected layers account for the major-\nity of parameters and therefore make the system prone to\noverﬁtting. This problem can be resolved by using FCNs\nwith average-pooling at the ﬁnal convolutional layer. For\ninstance in [16], the authors assume that the target visual\nobjects may show large activations globally in the corre-\nsponding images. Our systems resemble the architecture\nin [16] except the pooling method, where we only use max-\npooling because some of the features are found to be local,\ne.g. the voice may be active only for the last few seconds\nof a clip.\n5. EXPERIMENTS AND DISCUSSION\n5.1 Overview\nTwo datasets were used to evaluate the proposed system,\nthe MagnaTagATune dataset [13] and the Million Song\nDataset (MSD) [2]. The MagnaTagATune dataset has been\nrelatively popular for content-based tagging, but similar\nperformances from recent works [5,6,18,28] seem to sug-\ngest that performances are saturated, i.e. a glass-ceiling\nhas been reached due to noise in the annotation. The MSD\ncontains more songs than MagnaTagATune, it has various\ntypes of annotations up to 1M songs. There have not been\nmany works to compare our approach with, partly because\naudio signals do not come with the dataset. Consequently,\nwe use the MagnaTagATune dataset to compare the pro-\nposed system with previous methods and evaluate the vari-\nants of the system using the MSD.\nIn Experiment I, we evaluate three architectures (FCN-\n{3,4,5}) with mel-spectrogram input as proposed in Sec-\ntion 4. Furthermore, we evaluated STFT, MFCC, and mel-\nspectrogram representations as input of FCN-4. The archi-\ntecture of STFT input is equivalent to that of mel-spectrograms\nwith small differences in pooling sizes in the frequency\naxis due to the different number of spectral bands. For the\narchitecture of MFCCs, we propose a frame-based 4-layer\nfeed-forward networks with time-axis pooling (instead of\n2D convolutions and poolings) because relevant informa-\ntion is represented by each MFCC rather than its local rela-\ntionships. In Experiment II, we evaluate ﬁve architectures\n(FCN-{3,4,5,6,7}) with mel-spectrogram input.\nComputational cost is heavily affected by the size of the\ninput layers which depends on basic signal parameters of\nthe input data. A pilot experiment demonstrated similar\nperformances with 12and16kHz sampling rates and mel-\nbins of 96and128respectively. As a result, the audio in\nboth datasets was trimmed as 29.1s clips (the shortest sig-\nnal in the dataset) and was downsampled to 12kHz. The\nhop size was ﬁxed at 256samples ( 21ms) during time-\nfrequency transformation, yielding 1,366frames in total.\nSTFT was performed using 256-point FFT while the num-\nber of mel-bands was set as 96. For each frame, 30MFCCs808 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016and their ﬁrst and second derivatives were computed and\nconcatenated.\nWe used ADAM adaptive optimisation [12] on Keras\n[4] and Theano [1] framework during the experiments. Bi-\nnary cross-entropy function is used since it shows faster\nconvergence and better performance than distance-based\nfunctions such as mean squared error and mean absolute\nerror.\n5.2 Experiment I: MagnaTagATune\nThe MagnaTagATune dataset consists of 25,856 clips of\n29.1-s,16kHz-sampled mp3 ﬁles with 188tags. We only\nuses Top- 50tags, which includes genres ( classical, rock ),\ninstruments ( piano, guitar, vocal, drums ), moods ( soft, am-\nbient ) and other descriptions ( slow, Indian ). The dataset is\nnot balanced, the most frequent tag is used 4,851 times\nwhile the 50-th most frequent one used 490times in the\ntraining set. The labels of the dataset consist of 7,644\nunique vectors in a 50-dimensional binary vector space.\nThe results of the proposed architecture and its variants\nare summarised in Table 3. There is little performance\ndifference between FCN-4 and FCN-5. It is a common\nphenomenon that an additional layer does not necessarily\nlead to an improved performance if, i) the gradient may\nnot ﬂow well through the layers or ii) the additional layer\nis simply not necessary in the task but only adds more pa-\nrameters. This results in overﬁtting or hindering the opti-\nmisation. In our case, the most likely reason is the latter\nof the two. First, the scores are only slightly different, sec-\nond, both FCN-4 and FCN-5 showed similar performances\ncompared to previous research as shown in Table 4. Simi-\nlar results were found in the comparison of FCN-5, FCN-6,\nand FCN-7 in Experiment II. These are discussed in Sec-\ntion 5.3.\nMethods AUC\nFCN-3, mel-spectrogram .852\nFCN-4, mel-spectrogram .894\nFCN-5, mel-spectrogram .890\nFCN-4, STFT .846\nFCN-4, MFCC .862\nTable 3 . The results of the proposed architectures and in-\nput types on the MagnaTagATune Dataset\nFigure 2 . The numbers of bins per 1kHz bandwidth in\nmel-spectrograms and STFTs .Methods AUC\nThe proposed system, FCN-4 .894\n2015, Bag of features and RBM [18] .888\n2014, 1D convolutions [6] .882\n2014, Transferred learning [28] .88\n2012, Multi-scale approach [5] .898\n2011, Pooling MFCC [8] .861\nTable 4 . The comparison of results from the proposed and\nthe previous systems on the MagnaTagATune Dataset\nThe degradations with other types of input signals–STFT\nand MFCC–are rather signiﬁcant. This result is aligned\nwith the preferences of mel-spectrograms over STFT on\nautomatic tagging [5,6,18,27]. However, this claim is lim-\nited to this or very similar tasks where the system is trained\non labels such as genres, instruments, and moods. Fig-\nure 2 shows how 96 frequency bins are allocated by mel-\nspectrograms and STFT in every 1kHz bandwidth. This\nﬁgure, combined with the result in Table 3 shows that high-\nresolution in the low-frequency range helps automatic tag-\nging. It also supports the use of downsampling for auto-\nmatic tagging. Focusing on low-frequency can be more\nefﬁcient.\nTable 4 shows the performance of FCN-4 in comparison\nto the previous algorithms. The proposed algorithm per-\nforms competitively against the other approaches. How-\never, many different algorithms only show small differ-\nences in the range of an AUC score of 0.88–0.89, making\ntheir performances difﬁcult to compare. This inspired the\nauthors to execute a second experiment discussed in the\nnext section. In summary, the mel-spectrograms showed\nbetter performance than other types of inputs while FCN-4\nand FCN-5 outperformed many previously reported archi-\ntectures and conﬁgurations.\n5.3 Experiment II: Million Song Dataset\nWe further evaluated the proposed structures using the Mil-\nlion Song Dataset (MSD) with last.fm tags. We select the\ntop 50 tags which include genres ( rock, pop, jazz, funk ),\neras ( 60s – 00s ) and moods ( sad, happy, chill ). 214,284\n(201,680 for training and 12,605 for validation) and 25,940\nclips are selected from the provided training/test sets by\nﬁltering out items without any top-50 tags. The number of\ntags ranges from 52,944 ( rock) to 1,257 ( happy ) and there\nare 12,348 unique tag vectors. Note that the size of the\nMSD is more than 9 times larger than the MagnaTagATune\ndataset.\nThe results of the proposed architectures with differ-\nent numbers of layers are summarised in Table 5. Unlike\nthe result from Experiment I, where FCN-4 and FCN-5\nshowed a slight difference of the performance (AUC differ-\nence of 0.008), FCN-5,6,7 resulted in signiﬁcant improve-\nments compared to FCN-4, showing that deeper structures\nbeneﬁt more from sufﬁcient data. However, FCN-6 out-\nperformed FCN-5 only by AUC 0.003while FCN-7 evenProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 809Methods AUC\nFCN-3, mel-spectrogram .786\nFCN-4, — .808\nFCN-5, — .848\nFCN-6, — .851\nFCN-7, — .845\nTable 5 . The results from different architectures of the\nproposed system on the Million Song Dataset\nFigure 3 . The learning curves of the AUC scores measured\non the validation set (on the Million Song Dataset)\nshowed a slightly worse performance than FCN-6. This\nresult agrees with a known insight in using deep neural\nnetworks. The structures of DNNs need to be designed\nfor easier training when there are a larger number of lay-\ners [9]. In theory, more complex structures can perform\nat least equal to simple ones by learning an identity map-\nping. Our results supports this. In the experiment, the per-\nformances of FCN-6 and FCN-7 were still making small\nimprovements at the end of the training, implying it may\nperform equal to or even outperform FCN-5. In practice,\nthis approach is limited by computational resources and\ntherefore very deep structures may need to be designed to\nmotivate efﬁcient training, for instance, using deep resid-\nual networks [9].\nFigure 3 illustrates the learning curves of the AUC scores\non the validation set. At the beginning of the training, there\nis a tendency that simpler networks show better perfor-\nmance because there is a fewer number of parameters to\nlearn. FCN-4 and FCN-5 show similar performance be-\ntween around 20–40epochs. Based on this, it can be as-\nsumed that learning on the MagnaTagATune dataset stayed\nwithin this region and failed to make more progress due to\nthe scarcity of training data. To summarise, FCN-5, FCN-\n6, and FCN-7 signiﬁcantly outperformed FCN-3 and FCN-\n4. The results imply that more complex models beneﬁt\nfrom more training data. The similar results obtained us-\ning FCN-5, FCN-6 and FCN-7 indicate the need for more\nadvanced design methodologies and training of deep neu-\nral networks.6. CONCLUSION\nWe presented an automatic tagging algorithm based on deep\nfully convolutional neural networks (FCN). It was shown\nthat deep FCN with 2D convolutions can be effectively\nused for automatic music tagging and classiﬁcation tasks.\nIn Experiment I (Section 5.2), the proposed architectures\nwith different input representations and numbers of layers\nwere compared using the MagnaTagATune dataset against\nthe results reported in previous works showing competi-\ntive performance. With respect to audio input represen-\ntations, using mel-spectrograms resulted in better perfor-\nmance compared to STFTs and MFCCs. In Experiments\nII (Section 5.3), different number of layers were evaluated\nusing the Million Song Dataset which contains nine times\nas many music clips. The optimal number of layers were\nfound to be different in this experiment indicating deeper\nnetworks beneﬁt most from the availability of large train-\ning data. In the future, automatic tagging algorithms with\nvariable input lengths will be investigated.\n7. ACKNOWLEDGEMENTS\nThis work was part funded by the FAST IMPACt EPSRC\nGrant EP/L019981/1 and the European Commission H2020\nresearch and innovation grant AudioCommons (688382).\nSandler acknowledges the support of the Royal Society as\na recipient of a Wolfson Research Merit Award.\n8. REFERENCES\n[1] Fr ´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu,\nJames Bergstra, Ian Goodfellow, Arnaud Bergeron,\nNicolas Bouchard, David Warde-Farley, and Yoshua\nBengio. Theano: new features and speed improve-\nments. arXiv preprint arXiv:1211.5590 , 2012.\n[2] Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whit-\nman, and Paul Lamere. The million song dataset. In\nProceedings of the 12th International Society for Mu-\nsic Information Retrieval Conference, ISMIR 2011, Mi-\nami, Florida, USA, October 24-28, 2011 , pages 591–\n596, 2011.\n[3] Keunwoo Choi, George Fazekas, Mark Sandler, and\nJeonghee Kim. Auralisation of deep convolutional neu-\nral networks: Listening to learned features. In Proceed-\nings of the 16th International Society for Music In-\nformation Retrieval Conference, ISMIR 2015, M ´alaga,\nSpain, October 26-30, 2015 , 2015.\n[4] Franc ¸ois Chollet. Keras: Deep learn-\ning library for theano and tensorﬂow.\nhttps://github.com/fchollet/keras, 2015.\n[5] Sander Dieleman and Benjamin Schrauwen. Multi-\nscale approaches to music audio feature learning. In\nProceedings of the 14th International Society for Mu-\nsic Information Retrieval Conference, ISMIR 2013,\nCuritiba, Brazil, November 4-8, 2013 , 2013.810 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016[6] Sander Dieleman and Benjamin Schrauwen. End-to-\nend learning for music audio. In Acoustics, Speech and\nSignal Processing (ICASSP), 2014 IEEE International\nConference on , pages 6964–6968. IEEE, 2014.\n[7] Thomas Grill and Jan Schl ¨uter. Music boundary detec-\ntion using neural networks on spectrograms and self-\nsimilarity lag matrices. In Proceedings of the 23rd\nEuropean Signal Processing Conference (EUSPICO\n2015), Nice, France , 2015.\n[8] Philippe Hamel, Simon Lemieux, Yoshua Bengio, and\nDouglas Eck. Temporal pooling and multiscale learn-\ning for automatic annotation and ranking of music au-\ndio. In Proceedings of the 12th International Soci-\nety for Music Information Retrieval Conference, IS-\nMIR 2011, Miami, Florida, USA, October 24-28, 2011 ,\npages 729–734, 2011.\n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition.\narXiv preprint arXiv:1512.03385 , 2015.\n[10] Eric J Humphrey and Juan P Bello. Rethinking auto-\nmatic chord recognition with convolutional neural net-\nworks. In Machine Learning and Applications, 11th In-\nternational Conference on , volume 2, pages 357–362.\nIEEE, 2012.\n[11] Sergey Ioffe and Christian Szegedy. Batch nor-\nmalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint\narXiv:1502.03167 , 2015.\n[12] Diederik P. Kingma and Jimmy Ba. Adam: A method\nfor stochastic optimization. CoRR , abs/1412.6980,\n2014.\n[13] Edith Law, Kris West, Michael I Mandel, Mert Bay,\nand J Stephen Downie. Evaluation of algorithms using\ngames: The case of music tagging. In Proceedings of\nthe 10th International Society for Music Information\nRetrieval Conference, ISMIR 2009, Kobe, Japan, Oc-\ntober 26-30, 2009 , pages 387–392. ISMIR, 2009.\n[14] Yann LeCun and Yoshua Bengio. Convolutional net-\nworks for images, speech, and time series. The hand-\nbook of brain theory and neural networks , 3361(10),\n1995.\n[15] Dawen Liang, Minshu Zhan, and Daniel PW Ellis.\nContent-aware collaborative music recommendation\nusing pre-trained neural networks. In Proceedings of\nthe 16th International Society for Music Information\nRetrieval Conference, ISMIR 2015, M ´alaga, Spain,\nOctober 26-30, 2015 , 2015.\n[16] Min Lin, Qiang Chen, and Shuicheng Yan. Network in\nnetwork. CoRR , abs/1312.4400, 2013.\n[17] Brian CJ Moore. An introduction to the psychology of\nhearing . Brill, 2012.[18] Juhan Nam, Jorge Herrera, and Kyogu Lee. A deep\nbag-of-features model for music auto-tagging. arXiv\npreprint arXiv:1508.04999 , 2015.\n[19] Tara N Sainath, Abdel-rahman Mohamed, Brian\nKingsbury, and Bhuvana Ramabhadran. Deep convo-\nlutional neural networks for lvcsr. In Acoustics, Speech\nand Signal Processing (ICASSP), 2013 IEEE Interna-\ntional Conference on , pages 8614–8618. IEEE, 2013.\n[20] Tara N Sainath, Ron J Weiss, Andrew Senior, Kevin W\nWilson, and Oriol Vinyals. Learning the speech front-\nend with raw waveform cldnns. In Proc. Interspeech ,\n2015.\n[21] Jan Schluter and Sebastian Bock. Improved musical\nonset detection with convolutional neural networks. In\nAcoustics, Speech and Signal Processing (ICASSP),\nIEEE International Conference on . IEEE, 2014.\n[22] Siddharth Sigtia, Emmanouil Benetos, and Simon\nDixon. An end-to-end neural network for polyphonic\nmusic transcription. arXiv preprint arXiv:1508.01774 ,\n2015.\n[23] Andrew JR Simpson, Gerard Roma, and Mark D\nPlumbley. Deep karaoke: Extracting vocals from mu-\nsical mixtures using a convolutional deep neural net-\nwork. arXiv preprint arXiv:1504.04658 , 2015.\n[24] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. Dropout:\nA simple way to prevent neural networks from over-\nﬁtting. The Journal of Machine Learning Research ,\n15(1):1929–1958, 2014.\n[25] George Tzanetakis and Perry Cook. Musical genre\nclassiﬁcation of audio signals. Speech and Audio Pro-\ncessing, IEEE transactions on , 10(5):293–302, 2002.\n[26] Karen Ullrich, Jan Schl ¨uter, and Thomas Grill. Bound-\nary detection in music structure analysis using convo-\nlutional neural networks. In Proceedings of the 15th\nInternational Society for Music Information Retrieval\nConference, ISMIR 2014, Taipei, Taiwan , 2014.\n[27] Aaron Van den Oord, Sander Dieleman, and Benjamin\nSchrauwen. Deep content-based music recommenda-\ntion. In Advances in Neural Information Processing\nSystems , pages 2643–2651, 2013.\n[28] A ¨aron Van Den Oord, Sander Dieleman, and Ben-\njamin Schrauwen. Transfer learning by supervised pre-\ntraining for audio-based music classiﬁcation. In Pro-\nceedings of the 15th International Society for Music\nInformation Retrieval Conference, ISMIR 2014, Taipei,\nTaiwan, October 27-31, 2014 , 2014.\n[29] Yusuf Yaslan and Zehra Cataltepe. Audio music genre\nclassiﬁcation using different classiﬁers and feature se-\nlection methods. In 18th ICPR 2006 , volume 2, pages\n573–576. IEEE, 2006.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 811"
    },
    {
        "title": "A Latent Representation of Users, Sessions, and Songs for Listening Behavior Analysis.",
        "author": [
            "Chia-Hao Chung",
            "Jing-Kai Lou",
            "Homer H. Chen"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416878",
        "url": "https://doi.org/10.5281/zenodo.1416878",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/110_Paper.pdf",
        "abstract": "Understanding user listening behaviors is important to the personalization of music recommendation. In this paper, we present an approach that discovers user behavior from a large-scale, real-world listening record. The proposed approach generates a latent representation of users, listening sessions, and songs, where each of these objects is represented as a point in the multi-dimensional latent space. Since the distance between two points is an indication of the similarity of the two corresponding objects, it becomes extremely simple to evaluate the similarity between songs or the matching of songs with the user preference. By exploiting this feature, we provide a two-dimensional user behavior analysis framework for music recommendation. Exploring the relationships between user preference and the contextual or temporal information in the session data through this framework significantly facilitates personalized music recommendation. We provide experimental results to illustrate the strengths of the proposed approach for user behavior analysis.",
        "zenodo_id": 1416878,
        "dblp_key": "conf/ismir/ChungLC16",
        "content": "A LATENT REPRESENTATION OF USERS, SESSIONS, AND \nSONGS FOR LISTENING BEHAVIOR ANALYSIS \nChia-Hao Chung Jing-Kai Lou Homer Chen \nNational Taiwan University  \nb99505003@ntu.edu.tw  KKBOX Inc. \nkaelou@kkbox.com  National Taiwan University  \nhomer@ntu.edu.tw  \nABSTRACT \nUnderstanding user listening behaviors is important to the \npersonalization of music recommendation. In this paper, \nwe present an approach that discovers user behavior from \na large-scale, real-world listening record. The proposed \napproach generates a latent representation of users, \nlistening sessions, and songs, where each of these objects \nis represented as a point in the multi-dimensional latent \nspace. Since the distance between two points is an \nindication of the similarity of the two corresponding \nobjects, it becomes extremely simple to evaluate the \nsimilarity between songs or the matching of songs with \nthe user preference. By exploiting this feature, we \nprovide a two-dimensional user behavior analysis \nframework for music recommendation. Exploring the \nrelationships between user preference and the contextual \nor temporal information in the session data through this \nframework significantly facilitates personalized music \nrecommendation. We provide experimental results to \nillustrate the strengths of the proposed approach for user \nbehavior analysis. \n1. INTRODUCTION \nAnalyzing the listening behavior of users involves \nidentifying and representing the music preferences of \nusers. However, the music preference of a user is \ndynamic and varies with the listening context [1], such as \ntime, location, user’s mood, etc. How to take the \ncontextual information into consideration for music \nrecommendation is an important research issue [1]–[3]. In \nthis work, we focus on the analysis of dynamic listening \nbehavior and use the obtained information to personalize \nmusic recommendation. \nMusic and user preference are commonly represented \nusing a taxonomy of musical genres, such as hip-hop, \nrock, jazz, etc. In this approach, the preference of a user \nis represented as a probability distribution of the genres \nthat the user listened to. Although simple and easy to \nimplement, the main drawback of this approach is that \nthere is not a uniform taxonomy for music, making genre \nidentification ambiguous and subjective [4]. In addition, \nit often lacks the kind of granularity needed to distinguish \nbetween songs of the same genre.  In practice, it is often required to describe the music \npreference of a user with a fine precision. This requires a \ngood metric to measure the similarity between songs. \nTherefore, besides the uniformity and granularity \nrequirements, a music preference representation scheme \nhas to provide an effective similarity measurement. Many \napproaches based on latent representation have been \nproposed [5], [7] to meet all three requirements. These \napproaches represent songs and user preferences by one \nsingle scheme. The unified representation, which is a \nmulti-dimensional vector, is learned from a listening \nrecord or a rating record. Each dimension of the vector \nrepresents a latent feature of songs and user preferences. \nTherefore, each song or user is an object represented by a \nvector in a latent space, making the evaluation of \nsimilarity between songs or the matching between songs \nand users a simple matter of distance measure between \nvectors.  \nThe music preference of a user may change with the \nlistening session [3], [8]. A listening session here refers \nto a sequence of songs (and the associated time code) \nwhich a user continuously listened to. It contains \ninformation related to the listening experience of the user. \nTo account for the dynamic nature of music preference, \nwe incorporate the notion of session into the learning \nstage of a latent representation. In our approach, each \nsession of the listening record of a user is also \nrepresented as an object in the latent space. The \ncontextual information, such as the time of day and the \ndevice used for music listening, associated with each \nsession enables the analysis of user preference at a fine \nlevel.  © Chia-Hao Chung, Jing-Kai Lou, Homer Chen. \nLicensed under a Creative Commons Attribution 4.0 International \nLicense (CC BY 4.0). Attribution:  Chia-Hao Chung, Jing-Kai Lou , \nHomer Chen. “A Latent Representation of Users, Sessions, and Songs\nfor Listening Behavior Analysis”, 17th International Society for Music \nInformation Retrieval Conference, 2016. \nFig. 1. A two-dimensional latent space representation of \nthree users listening to 403 songs in 60 sessions. Each \nuser, session, or song is represented by a point in the \nlatent space, and e ach user is surrounded by the songs \nplayed by the user and the listening sessions during \nwhich the songs are played. \n323  \n \nIn addition, each user, session, or song can be plotted \nas a point in a two-dimensional latent space, as illustrated \nin Fig. 1. Clearly, this provides an intuitive way to \nvisualize or analyze the relationship between songs and \nuser preferences. We exploit this feature of latent space \nfor listening behavior analysis. \nTo the best of our knowledge, this paper is among the \nfirst that introduce the notion of session to the \nrepresentation learning for music recommendation and \nproposes an approach that generates the latent \nrepresentation of users, sessions, and songs (Sections 3 \nand 4). The proposed latent representation is a powerful \nbasis for visual analysis of user preference in a two-\ndimensional space and enables the discovery of a user’s \nlistening behavior that would otherwise be difficult to do \nwith conventional representations (Section 5). In addition, \nwe propose an effective method to evaluate the \nperformance of a latent representation for listening \nbehavior discovery (Section 6). \n2.  REVIEW \nStatistical approaches that analyze the dynamic nature of \nmusic preference have been reported in the literature. \nHerrera et al. [2] adopted a circular statistic method to \nidentify the temporal pattern of music listening. Zheleva \net al. [3] proposed a session-based probabilistic graphical \nmodel to characterize music preference and showed the \nusefulness of session to capture the dynamic preference \nof a user. The importance of these two pieces of work is \nthat they show users’ listening behaviors (patterns) can be \ndiscovered and used to predict future user behaviors.  \nApproaches that generate a latent representation of \nusers and songs for music recommendation have been \nreported. Dror et al. [5] adopted a matrix factorization \nmethod to characterize each user or song as a low-\ndimensional latent vector and to approximate the user \npreference (i.e. a rating) as the inner product of a user \nvector and a song vector. The temporal dynamics of \nmusic preference and the taxonomy of musical genres are \nconsidered jointly to improve the performance of music \nrecommendation. Moore et al. [7] proposed a dynamic \nprobabilistic embedding method to generate a \nrepresentation of users and songs for music preference \nanalysis. Each user or song is represented as a point in a \ntwo-dimensional space, and the position of each point is \nallowed to gradually change over time. The trajectory of \na point shows the long-term variation in music preference. \nRecently, Chen et al. [9] introduced a network \nembedding method to enhance music recommendation. \nThe social relationship between users is exploited to learn \na latent representation of social listening, which is fed to \na factorization machine to improve the performance of \nmusic recommendation. \n3. LATENT REPRESENTATION LEARNING \nIn our latent space approach, a network that describes the \nrelationship between users, sessions, and songs stored in \na listening record is first constructed, then a network \nembedding method is applied to learn the latent representation from the network. The details are \ndescribed in this section.  \n3.1 Network Construction \nThe basic idea to construct a network that describes the \nrelationship between users and songs is to consider each \nuser or song as an object in the network and connect each \nuser with the songs the user listened to [9].  To account \nfor the dynamic nature of music preference, we further \nincorporate listening sessions into the network \nconstruction and consider each user, session, or song as \nan object in the network. A user is connected with all \nsessions of the user, and a session is connected with all \nsongs appearing in the session. This makes the network \ncapture the dynamic music preferences of users. \n3.2 Network Embedding \nA network embedding method aims at learning the latent \nrepresentation of objects in a network. Such \nrepresentation captures the relationship between the \nobjects in the network. Objects having a similar \nneighborhood in the network are represented by similar \nvectors.  In our approach, the DeepWalk algorithm [10] is \napplied to learn the latent representation. This algorithm \nconsists of a random walk procedure and an update \nprocedure. \nA network consists of a set of vertices representing the \nobjects and a set of edges connecting related vertices. The \nrandom walk procedure uniformly samples a random \nvertex as the root of a random walk, and then uniformly \nsamples a random vertex from the neighbors of the \ncurrent vertex as the next vertex until the maximum \nwalking length L is reached. The procedure repeats until \neach vertex serves as the root of R random walks, where \nR is a predetermined number. The total number of \nrandom walks generated in this procedure is thus equal to \nthe number of vertices in the network multiplied by R. \nThe vertices visited in a random walk are processed next. \nThe latent representation of each vertex of is initially a \nd-vector of random variables, where d denotes the \ndimension of the latent space. The update procedure [11], \n[12] takes the random walks one by one as input and \nprogressively refines the latent representation of objects \nin two steps.  The first step creates a probability formula \nfor each vertex in a random walk, starting from the first \none. Specifically, for a vertex vi in a random walk and a \nneighborhood window w, the conditional probability that \nthe set of vertices { vi-w, …, vi-1} appears in the backward \nwindow of vi and {vi+1, …, vi+w} appears in the forward \nwindow of vi is expressed as \n \n \nwhich is called the co-occurrence probability because it \nindicates the likelihood that these two sets of vertices are \nin the neighborhood of vi in a random walk. If the order \nof the vertices in each window is ignored, the co-\noccurrence probability can be rewritten as \n \n(1) \n 1 1 ({ , , , , , }| ),i w i i i w iP v v v v v    \n1 1\n,({ , , , , , }| ) ( | ).i w\ni w i i i w i j i\nj i w j iP v v v v v P v v\n   \n    324 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016  \n \nIn the second step, the vector of vi is optimized by \nmaximizing P(vj | vi). To enhance computational \nefficiency, a binary tree with all vertices of the network \nas the leaves is constructed to convert the maximization \nto a tree traversal process [13]. A path of the tree \ntraversal is specified by a sequence of nodes { b1, b2, …, \nbk} in the binary tree, where k is the length of the path, b1 \nis the root of the binary tree and  bk is the leaf node \nrepresenting vj. Then the conditional probability P(vj | vi)  \ncan be rewritten as \n \n(2) \n \nEach conditional probability P(bl+1 | vi, bl) is modeled by \na logistic function and can be rewritten as \n \n(3) \n \nwhere Φ( vi) maps vi to its vector, and Ψ( bl) maps bl to its \nvector. Then, a stochastic gradient descent method [14] \nand a back-propagation algorithm [15] are applied to \noptimize Φ and Ψ. The update procedure repeats until the \noptimization for each vertex in each random walk is \nprocessed. The optimized Φ is the latent representation of \neach vertex in the network. \n4. OUR APPROACH \nThe three basic steps of the proposed approach are shown \nin Fig. 2. The first step involves the preparation of a \nlistening record, the second step involves the construction \nof a network and the learning of a latent representation \nfrom the network, and the third step involves the analysis \nof user behavior. The first and second steps are described \nin this section, and the third step is described in Section 5. \n4.1 Preparation \nWe obtain a listening record of one hundred thousand \nusers from a leading online music service provider [18] \nand use it in this work. The listening record contains \nevery listening event of these users from January 1, 2015 \nto June 30, 2015, and each event contains seven fields: \ntimestamp, user, session, listening device, song title, \nartist(s) of the song, and music tag(s) of the song. All \nusers are anonymized to maintain privacy.  \nA session, which indicates the listening experience of \na user, is defined as a sequence of events of the user with \nthe following constraints: The gap between any two \nneighboring events in a session is shorter than 10 minutes, \nand the listening device stays the same in a session. \nThe music tags are used for visual analysis in Section \n5, showing either genre or language information of a song. \nA genre tag indicates the musical style of the song, and a language tag indicates the language of the song. We also \nobtain the popularity of each song by taking the logarithm \nof its playcount for the visual analysis. \nThe listening record is split in to training set and \ntesting set. We adopt the real-life split strategy [16] and \nsplit the listening record into two parts: before and after \n00:00:00, June 1, 2015. The last 80 sessions of each user \nin the first part are selected as training data, and the first \n20 sessions of each user in the second part are selected as \nthe testing data. Users with insufficient sessions or \nsessions with less than 5 songs in either set are discarded. \nIn addition, because a cold start problem [16] may occur \nif songs in the testing set are not in the training set, we \ndiscard such events from the testing set. Table 1 shows \nthe data statistics. The testing set is used for performance \nevaluation in Section 6.  \n4.2 Representation Learning \nUsing the training set, a user-session-song network is \nconstructed as described in Section 3.1, and the \nDeepWalk algorithm is applied to generate the latent \nrepresentation of users, sessions, and songs from the \nnetwork. We set the parameters of the learning algorithm \nas the following: The length of a random walk L is 40, \nthe number of random walks starting from a vertex R is \n20, and the window size w is 6. A two-dimensional latent \nrepresentation is generated for visual analysis (Section 5), \nand a 128-dimensional latent representation is generated \nfor performance evaluation (Section 6).  \n5. VISUALIZATION AND ANALYSIS \nA two-dimension latent space provides an intuitive way \nto visualize the relationship between songs and user \npreferences. We perform a visual analysis of the general \ntrend of listening preference and then the individual \nlistening behavior in such space. The details of these \noperations are described in this section.  \n5.1 General Trend of Listening Preference \nWe can get an idea of the general trend of listening \npreference with respect to the song properties (the \nlanguage of songs, the musical genre, and the song \npopularity) by examining the distribution of songs in the \nlatent space. The two-dimensional latent space learned \nfrom the training set is plotted in Fig. 3. In each plot, \n25,000 songs are plotted, and each song is marked with a \nspecific color according to the property of the song.   \nIn Fig. 3 (a), songs are colored according to the \nlanguage of songs. There are songs in six different \nlanguages in our dataset where Western songs take a \nlarge proportion. We can see that songs of the same \nlanguage are close and form a cluster in the latent space. \nThis suggests that the user listening preference is strongly \nrelated to the song language and that a user tends to listen \nto songs of the languages that the user is familiar with.  #events  #users #songs \nTraining set  33,790,690  33,292 441,796 \nTesting set 8,797,016  19,831 219,377 \nTable. 1. Data statistics.  \n \nFig. 2. Overview of our approach. Data\npreparationNetwork\nconstructionDeepWalkBehavior\nanalysisListening \nrecord Latent representation\nlearning\n1\n1\n1( | ) ( | , ).k\nj i l i l\nlP v v P b v b\n\n\n( ) ( )\n1( | , ) 1 (1 ),i lv b\nl i lP b v b e \n  Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 325  \n \nWe can also see that some clusters are located closely. \nFor example, Mandarin songs and Hokkien songs are \nlocated closely or even mixed together. This indicates \nthat a part of users who listen to Mandarin songs are \nlikely to listen to Hokkien songs as well.  \nIn Fig. 3 (b), songs are colored according to the \nmusical genre. Clearly, songs of the same genre are \nlocated in the same area in the latent space. The similarity \nbetween genres is reflected on their distance. For example, \nJazz songs are close to classical songs, and electronic \nsongs are close to hip-hop songs. Combining Figs. 3 (a) \nand (b), we can see that Westerns songs contain many \ngenres, such as hip-hop, rock, electronic, jazz, and \nclassical songs, and Mandarin songs are mostly pop \nsongs. \nIn Fig. 3 (c), songs are colored according to the \npopularity. The popularity of each song is obtained by \ntaking the logarithm of the playcount of the song. For \neasy visualization, all songs are divided in to five levels \nin terms of popularity. Clearly, the most popular songs \nare near the origin of the latent space, and unpopular \nsongs are far from the origin. It indicates that most users \nlisten to the most popular songs, a typical long tail \nphenomenon of music listening [17]. Combining Figs 3 \n(a), (b), and (c), we can see that Western, Mandarin and \nKorean pop songs are more popular than other songs in \nour dataset. \n5.2 Individual Listening Behavior  \nThe individual behavior is analyzed through the \ndistribution of sessions and songs associated with a user \nin latent space. A session is close to the songs that appear \nin the session, and sessions form a cluster if they contain \nsimilar songs. Fig. 4 shows the analyses for nine example \nusers. In each plot, the sessions and songs associated with \na user are plotted.  \nOne important discovery is that users can be divided \ninto two types, one with a single preference and the other \nwith dynamic preference. For some users, the sessions are \nmostly located in one small area in the latent space. This means that the users listens to the same songs most of the \ntime and hence belong to the first type. For other users, \nthe sessions have a wide distribution and form several \nclusters. This indicates that the users belong to the second \ntype.  \nIn order to analyze the dynamic preference of an \nindividual, we distinguish sessions by the context \ninformation (the device used for listening and the time of \nday). In Fig. 5, we color each session according to the \ndevice used for listening, and we can clearly see that the \nsessions form clusters according to the listening device. \nThis indicates there is a strong relevance between the \nmusic preference and the listening device. For example, a  \n \n \nFig. 4. Listening behavior analysis for n ine example \nusers. The sessions and songs associated with a user are \nplotted in each plot, where a red point represents a user, a \nblue point represents a session, and a gray point \nrepresents a song.  \n \n(a) (b) (c) \nFig. 3. The two-dimensional latent space learned from the training set is shown in three plots. In each plot, 25,000 songs \nare randomly selected, and each song is specified by a point and marked with a specific color according to the property \nof the song. The proportion of songs with each property is showed in the legend. (a) Each song is marked according to \nthe song language. (b) Each song is marked according to the genre, and (c) Each song is marked according to the \npopularity (Level 5 indicates the most popular songs), and songs with high popularity are overlaid over those with low \npopularity. \n326 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016  \n \nuser may listen to rock songs through computer and listen \nto pop songs through mobile phone. This kind of listening \nbehavior can be found on many users in our dataset. In \nFig. 6, we color each session according to the time of day, \nand each plot shows a user whose music preference is \nrelated to the time for music listening. However, this kind \nof listening behavior is not easy to be observed on users. \nProbably it is because the relationship between time and \nmusic preference is too complex to be explained. \nAn interactive system for individual behavior analysis \ncan be designed based on the latent space. For example, \nwhen we select a session, the system would highlight the \nsongs that appear in the session so that we can further \ndiscover the user preference in each session. \n6. PERFORMANCE EVALUATOIN \nTwo experiments are designed to evaluate the \neffectiveness of a latent representation for listening \nbehavior prediction. The first one involves retrieving \nsimilar songs (i.e. songs that appear in the same session), \nand the second one involves recommending songs that \nmatch a user’s preference in a session. Each experiment \nis considered a retrieval problem whose goal is to retrieve \nsongs relevant to a query object (user, session, or song) as \nmany as possible. Specifically, a query object is selected \naccording to a given testing session, and songs that \nappear in the testing session are considered relevant to the \nquery object. For a query object, its k nearest neighboring \nsongs in the latent space are retrieved, and the \nperformance is evaluated in terms of how well the \nretrieved songs match the relevant songs. In each \nexperiment, 200,000 sessions in the testing set are \nrandomly selected for testing. Because the average length \nof the testing sessions is 20 songs, k (the number of \nretrieved songs) is set to {10, 15, …, 50}. \nTwo standard evaluation metrics for retrieval problem \nare applied here: Recall and precision:  \n݈ܴܽܿ݁|ܵ௧ ∩ܵ௥|\n|ܵ௧|                              ሺ4ሻ \n݊݋݅ݏ݅ܿ݁ݎܲ ൌ|ܵ௧ ∩ܵ௥|\n|ܵ௥|                           ሺ5ሻ \nwhere ܵ௧ is the set of songs that appear in a testing \nsession and  ܵ௥ is the set of retrieved songs. A high recall \nmeans that most of relevant songs are retrieved, and a \nhigh precision means that most of the retrieved songs are relevant songs. The average recall and precision for all \ntesting sessions are reported. \nThe performances of the following four methods are \nreported for comparison. \n Random: k songs are randomly selected from the \ndataset as the retrieved songs. \n Popularity : The popularity of each song is obtained \nby calculating its playcount in the training set, and \nthe k most popular songs are considered the \nretrieved songs. \n Matrix factorization (MF)  [6]: The vector ࢖௨ for \nuser u and the vector ࢗ௜ for song i are learned by \nsolving the optimization problem  \nmin\n௤∗,௣∗∑ሺܿ௨௜െࢗ௜்࢖௨ሻଶ൅ߣሺ‖ࢗ௜‖ଶ൅‖࢖௨‖ଶሻ ௨,௜ ,    (6) \nwhere ܿ௨௜ൌ1൅logሺ1൅ݕ௨௜ሻ is the confidence \nvalue of the playcount ݕ௨௜ of user u for song i, and λ \nis a regularization parameter. Songs are retrieved \naccording to the inner product between vectors.  \n User-song network (U-S) : A simplified network, \nwhere a user directly connects to all songs the user \nlistened to, is constructed. The vectors for users and \nsongs are learned from the network using the \nDeepWalk algorithm. Songs are retrieved according \nto the Euclidean distance between vectors. \nFor fair comparison, the dimension of the latent \nrepresentations (vectors) learned by MF, U-S, and the \nproposed method is fixed to 128. \n6.1 Experiment for Retrieval of Similar Songs \nThis experiment evaluates the ability of the representation \nof songs to capture the similarity between songs. Because \nsongs in a session usually have similar properties, this \nexperiment is to find songs that appear in the same testing \nsession. Specifically, the first song in each testing session \nis selected as a query object to retrieve its k nearest \nneighbor songs, and the remaining songs in the testing \nsession are considered relevant to the query object. \nThe performances of various methods are compared in \nin Fig. 7. We can see that the proposed approach \noutperforms MF and U-S, showing the effectiveness of \nadding session objects into the learning stage of a latent \nrepresentation. We can also see that the popularity-based \nmethod works slightly better than the random method, \nshowing that many users tend to listen to the popular \nsongs. This is consistent with our observation (i.e. the \nlong tail phenomenon) in Section 5.1.    \nFig. 5. The sessions are distinguished by the device used \nfor music listening. Each plot illustrates a user whose \nmusic preference is related to the listening device.  \n \nFig. 6. Each plot shows a user whose music preference is \nrelated to the time for music listening. The sessions tend \nto form clusters according to the time (hour of day).  \nProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 327  \n \n6.2 Experiment for Music Recommendation \nThis experiment evaluates the effectiveness of a latent \nrepresentation to recommend songs to a user in a testing \nsession. The vector for the user is used to retrieve its k \nnearest neighbor songs, and the songs in the testing \nsession are considered relevant songs.  \nAs discussed in Section 5.2, a user may have dynamic \npreference, so only considering the user vector for every \nsession of the user is not enough. With our observation \nthat there are many users whose preferences are related to \nthe listening device, we use the session vector to \nrecommend songs. For each testing session, a reference \nsession that belongs to the same user of the testing \nsession and that is through the same listening device is \nselected as a query object, and the vector for the reference \nsession is used to retrieve its k nearest neighbor songs. \nThe performance comparison is shown in Fig. 8. We \ncan see that if the user vector is used, the proposed \napproach outperforms MF when k is higher than 20. If the \nsession vector is considered, the performance of proposed \napproach is significantly improved. It shows the benefit \nof exploiting the contextual information (device \ninformation) to capture the dynamic music preference of \na user.  \n7. DISCUSSION \nBesides the network embedding method adopted in this \nwork, factorization is another approach that can be \napplied to generate the latent representation of objects. A \nfactorization method approximates the interactions (e.g. \nratings or counts) between objects as the inner product of \nthe vectors representing the objects. In contrast, as we can \nsee in Eqn. (3), the network embedding method maps all \nobjects into the same latent space (Φ) and makes similar \nobjects close to each other in the latent space. This \nenables us to visually analyze the relationship between objects in the latent space. Moreover, the network \nembedding method learns the relationship between \nobjects that do not have explicit links between them, such \nas two users who listen to the same songs. Such \nrelationship cannot be learned by the factorization \nmethod. A comprehensive comparison between the \nnetwork embedding and factorization methods (e.g. user-\nsession-song factorization) is an interesting topic for \nfuture work. \n8. CONCLUSION \nKnowledge of the behavior of music listeners is \nimportant to music recommendation. In this paper, we \nhave described an approach to address this issue. The \nproposed approach generates the latent representation of \nusers, sessions, and songs from a listening record. Such \nrepresentation makes the relationship between these \nobjects easy to analyze. We have performed a visual \nanalysis of user behavior and preference in a two-\ndimensional latent space and illustrated the strengths of \nthe proposed approach by comparing its music \nrecommendation and retrieval performances with various \nmethods. We have also shown that the information \nobtained from the two-dimensional analysis is useful for \npersonalized music recommendation. The contextual \ninformation associated with each session enables both \nuser preference analysis and music recommendation at a \nfine level. \n9. REFERENCES \n[1] M. Kaminskas and F. Ricci, “Contextual music \ninformation retrieval and recommendation: State of the art \nand challenges,” Comput. Sci. Review , vol. 6, no. 2, pp. \n89–119, 2012.  \nFig. 7. Performance comparison of various methods for\nthe retrieval of similar songs. \n \nFig. 8. Performance comparison of various methods for \nmusic recommendation. Proposed- device indicates that \nthe device information of each testing session is \nexploited. \n328 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016  \n \n[2] P. Herrera, Z. Resa, and M. Sordo, “Rocking around the \nclock eight days a week: An exploration of temporal \npatterns of music listening,” in 1st Workshop Music \nRecommendation Discovery , 2010. \n[3] E. Zheleva, J. Guiver, E. M. Rodrigues, and N. Milic-\nFrayling, “Statistical models of music-listening sessions \nin social media,” in Proc. 19th ACM Int. Conf. World \nWide Web , pp. 1019–1028, 2010. \n[4] G. Tzanetakis and P. Cook, “Musical genre classification \nof audio signals,” IEEE Tran. Speech Audio Process.,  vol. \n10, no. 5, pp. 293 –302, 2002. \n[5] G. Dror, N. Koenigstein, and Y. Koren, “Yahoo! music \nrecommendations: Modeling music ratings with temporal \ndynamics and item taxonomy,” in Proc. Fifth ACM Int. \nConf. Recommender Syst.,  pp. 165–172, 2011. \n[6] Y. Hu, Y. Koren, and C. Volinsky, “Collaborative \nfiltering for implicit feedback datasets,” in Proc. 8th IEEE \nInt. Conf. Data Mining , pp. 263–272, 2008. \n[7] J. L. Moore, S. Chen, T. Joachims, and D. Turnbull, \n“Taste over time: The temporal dynamics of user \npreferences,” in Proc. 14th Int. Soc. Music Inform. \nRetrieval Conf. , pp. 401–406, 2013 \n[8] O. Carlsson, Cluster User Music Sessions , Master of \nScience Thesis, Department of Computer Science and \nEngineering, Chalmers University of Technology, \nGothenburg, Sweden, 2014. \n[9] C.-M. Chen, P.-C. Chien, Y.-C. Lin, M. F. Tsai, and Y.-H. \nYang, “Exploiting Latent Social Listening \nRepresentations for Music Recommendations,” in Proc \nNinth ACM Int. Conf. Recommender Syst. Poster , 2015. \n[10] B. Perozzi, R. Al-Rfou, and S. Skiena, “DeepWalk: \nOnline learning of social representations,” in Proc. 20th \nACM SIGKDD Int. Conf. Knowledge Discovery and Data \nMining, pp. 701–710, 2014. \n[11] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient \nestimation of word representations in vector space,” arXiv \npreprint arXiv: 1301.3781 , 2013. \n[12] T. Mikolov, I. Sutskever, K. Chen,G. Corrado, and J. \nDean, “Distributed representations of words and phrases \nand their compositionality,” in Proc. Advances Neural Inf. \nProcess. Syst. , pp. 3111–3119, 2013. \n[13] F. Morin and Y. Bengio, “Hierarchical probabilistic \nneural network language model,” in Proc. Int. Workshop \nArtificial Intell. Stat. , pp. 246–252, 2005. \n[14] L. Bottou, “Large-scale machine learning with stochastic \ngradient descent,” in Proc. 19th Int. Conf. Comput. Stat. , \npp. 177–186, 2010. \n[15] Y. LeCun, D. Touresky, and G. Hinton, “A theoretical \nframework for back-propagation,” in Proc. Connectionist \nModels Summer School , pp. 21–28, 1988. \n[16] S.-Y. Chou, Y.-H. Yang, and Y.C. Lin, “Evaluating music \nrecommendation in a real-world setting: On data splitting \nand evaluation metrics,” in Proc. IEEE Int. Conf. \nMultimedia Expo , pp. 1–6, 2015 \n[17] O. Celma, Music Recommendation and Discovery , \nSpringer Berlin Heidelberg, 2010. \n[18] KKBOX Inc., https://www.kkbox.com. Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 329"
    },
    {
        "title": "Transcribing Human Piano Performances into Music Notation.",
        "author": [
            "Andrea Cogliati",
            "David Temperley",
            "Zhiyao Duan"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416466",
        "url": "https://doi.org/10.5281/zenodo.1416466",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/088_Paper.pdf",
        "abstract": "Automatic music transcription aims to transcribe musical performances into music notation. However, existing tran- scription systems that have been described in research pa- pers typically focus on multi-F0 estimation from audio and only output notes in absolute terms, showing frequency and absolute time (a piano-roll representation), but not in musical terms, with spelling distinctions (e.g., A♭versus G♯) and quantized meter. To complete the transcription process, one would need to convert the piano-roll represen- tation into a properly formatted and musically meaningful musical score. This process is non-trivial and largely unre- searched. In this paper we present a system that generates music notation output from human-recorded MIDI perfor- mances of piano music. We show that the correct estima- tion of the meter, harmony and streams in a piano perfor- mance provides a solid foundation to produce a properly formatted score. In a blind evaluation by professional mu- sic theorists, the proposed method outperforms two com- mercial programs and an open source program in terms of pitch notation and rhythmic notation, and ties for the top in terms of overall voicing and staff placement.",
        "zenodo_id": 1416466,
        "dblp_key": "conf/ismir/CogliatiTD16",
        "content": "TRANSCRIBING HUMAN PIANO PERFORMANCES INTO MUSIC\nNOTATION\nAndrea Cogliati*, David Temperley+, Zhiyao Duan*\nElectrical and Computer Engineering*and Eastman School of Music+, University of Rochester\n{andrea.cogliati,zhiyao.duan }@rochester.edu\ndtemperley@esm.rochester.edu\nABSTRACT\nAutomatic music transcription aims to transcribe musical\nperformances into music notation. However, existing tran-\nscription systems that have been described in research pa-\npers typically focus on multi-F0 estimation from audio and\nonly output notes in absolute terms, showing frequency\nand absolute time (a piano-roll representation), but not in\nmusical terms, with spelling distinctions (e.g., A /flatversus\nG/sharp) and quantized meter. To complete the transcription\nprocess, one would need to convert the piano-roll represen-\ntation into a properly formatted and musically meaningful\nmusical score. This process is non-trivial and largely unre-\nsearched. In this paper we present a system that generates\nmusic notation output from human-recorded MIDI perfor-\nmances of piano music. We show that the correct estima-\ntion of the meter, harmony and streams in a piano perfor-\nmance provides a solid foundation to produce a properly\nformatted score. In a blind evaluation by professional mu-\nsic theorists, the proposed method outperforms two com-\nmercial programs and an open source program in terms of\npitch notation and rhythmic notation, and ties for the top in\nterms of overall voicing and staff placement.\n1. INTRODUCTION\nAutomatic Music Transcription (AMT) is the process of\ninferring a symbolic music representation from a music\nperformance, such as a live performance or a recording.\nThe output of AMT can be a full musical score or an in-\ntermediate representation, such as a MIDI ﬁle [5]. AMT\nhas several applications in music education (e.g., providing\nfeedback to piano students), content-based music search\n(e.g., searching for songs with a similar chord progression\nor bassline), musicological analysis of non-notated music\n(e.g., jazz improvisations and most non-Western music),\nand music enjoyment (e.g., visual representation of the mu-\nsic content).\nIntermediate representations are closer to the audio ﬁle\neven though they identify certain kinds of musical informa-\nc/circlecopyrtAndrea Cogliati*, David Temperley+, Zhiyao Duan*.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Andrea Cogliati*, David Temperley+,\nZhiyao Duan*. “Transcribing Human Piano Performances Into Music\nNotation”, 17th International Society for Music Information Retrieval\nConference, 2016.tion that are not readily accessible, such as explicit pitches\nand note onsets. However, the encoding of this information\nis generally not done in abstract musical terms but still re-\nﬂects some of the arbitrariness of a human performance;\ne.g., note onsets may be expressed in terms of absolute\ntimes instead of being quantized to a meter, and pitches\nmay be expressed in terms of frequency or MIDI note num-\nbers, instead of proper note spelling, e.g., C /sharphas the same\nMIDI note number as D /flat. Music notation provides further\ninformation such as key signature, time signature, rhyth-\nmic values, barlines, and voicing (e.g., the representation\nof multiple voices with upward and downward stems); this\ninformation is useful and indeed virtually necessary for\nfurther performance and analysis [13].\nWhile AMT was initially formulated as a method to\nconvert musical sounds into common music notation [15],\nmost AMT systems so far have opted for lower level rep-\nresentations [5]; very few systems have attempted to es-\ntimate higher level musical information, such as beats or\npattern repetitions, directly from the audio [9, 14]. Higher\nlevel musical information can also be estimated from an\nintermediate representation [6, 18]. In this paper we opt\nfor the latter approach; this allows the conversion of MIDI\nto notation, and eventually (in combination with an audio-\nto-MIDI conversion system, such as [8]) could generate\nnotation from audio as well.\nA MIDI ﬁle can represent a piano performance very ac-\ncurately; in fact, the only variables involved are note onset,\noffset, velocity and pedal activation. Moreover, MIDI rep-\nresentations of piano performances can be recorded from a\nMIDI keyboard, or from a piano with key sensors. The\nMIDI standard is capable of encoding high-level musi-\ncal information, such as key and time signatures, into\nMIDI ﬁles, but this information is not typically included\nin recorded performances, unless the performer manually\ninserts it. Furthermore, recorded MIDI performances are\ntypically unquantized, as performers continuously change\nthe speed of playing to obtain a more expressive perfor-\nmance, and may play certain notes slightly earlier or later\nthan they should be to highlight certain musical lines.\nThe process of producing a correct full music nota-\ntion from an unquantized and un-annotated MIDI ﬁle is\nnon-trivial and, to the authors’ knowledge, no system ca-\npable of producing full music notation has been imple-\nmented and documented in academic research papers thus\nfar. Without a proper estimation of the meter and the har-758 /noteheads.s2/noteheads.s1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/clefs.G/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s143/clefs.F/accidentals.sharp43/dots.dot/noteheads.s2/noteheads.s1/noteheads.s2/dots.dot/noteheads.s2/noteheads.s1/dots.dot/brace178/noteheads.s1/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2Piano  /noteheads.s2/noteheads.s2/noteheads.s1/accidentals.sharp/noteheads.s2/clefs.F/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/dots.dot/accidentals.sharp/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s17/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/dots.dot/brace178/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2(a) Ground Truth\nTime in seconds2345678910PitchF3 G3#B3 D4 F4 G4#B4 D5 F5 G5#\n(b) Unquantized MIDI piano roll\n2\n&\\\\E\n..!.#.#....#.F....D.F\n.....#.#.F....Q\n5\n&.E.!E#.!!.F...#.F....F......F...Q8\n&..F..F.......E\n.!DE .\n\"..F..F...\n11\n&E.!E#.!!...E.\n\"..F. Q....F.....\n14\n&..F.....#.F....F\n....E\n..!...Q-3%\\\\E..\n\"--.F..-#%,F..-#,%DF ..F...D.F.....#....%E.\n\"E#.\n\"\".DE#.\n\"\".E#.\n\"\"....#..%.F.Q.....#..E#.\n\"\".-\n(c) GarageBand\n /clefs.G/noteheads.s2/noteheads.s2/noteheads.s1/dots.dot/accidentals.sharp/noteheads.s243/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/dots.dot/rests.2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s1/noteheads.s2/clefs.F/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s1/rests.2/dots.dot/noteheads.s243/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/dots.dot/brace178/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2Piano  /accidentals.sharp/clefs.F/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/dots.dot/accidentals.sharp/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s17/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/dots.dot/brace178/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/clefs.F/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s1/rests.1/clefs.G/dots.dot/rests.2/noteheads.s2/noteheads.s2/noteheads.s113/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/brace178/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/rests.1/accidentals.sharp/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/dots.dot/accidentals.sharp/clefs.F/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s2/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s1/noteheads.s2/dots.dot20/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s1/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/brace186/noteheads.s2/noteheads.s1/noteheads.s2/dots.dot/accidentals.sharp/noteheads.s2/noteheads.s2/clefs.F/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/rests.1/dots.dot/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s227/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s1/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/flags.d3/brace178/rests.4\n(d) Proposed method\nFigure 1 . Transcription of a performance of the Minuet in G from Bach’s Notebook for Anna Magdalena Bach. (a) shows\nthe original score (b) shows the unquantized pianoroll of a MIDI performance. (c) shows the output from GarageBand,\nwhich does not perform any analysis on the MIDI ﬁle. (d) shows the output of the proposed method after estimating the\ncorrect meter, key signature, beats and streams. The music excerpts are of different lengths for better formatting.\nmony, the results are very poor – see Fig. 1 (c). The task\ncan be divided into two main sub-tasks: musical structure\nanalysis and note placement on the score. For the ﬁrst sub-\ntask, the MIDI ﬁle must be analyzed to estimate the key\nsignature and the correct note spelling, as well as the beats\nand the correct time signature. For the second sub-task,\nonce the notes have been correctly spelled and quantized\nto the correct meter, they must be properly positioned on\nthe staff. Piano music is normally notated on two staves.\nThe higher staff is usually notated in treble clef, and con-\ntains the notes generally played by the right hand. The\nlower staff is usually notated in bass clef, and contains the\nnotes generally played by the left hand. Notes should be\nplaced on staves to simplify the reading of the score, e.g.,\nnotes should be well spaced and typographical elements\nshould not clash with each other. The placement of the\nnotes and other typographical elements also convey mu-\nsical meanings, e.g., notes pertaining to the same voice\nshould have the stems pointing in the same direction andbeaming should follow the rhythm of the musical passage.\nFinally, concurrent notes played by a single hand as chords\nshould share the same stem. Exceptions to these basic rules\nare not uncommon, typically to simplify the reading by a\nperformer, e.g., if a passage requires both hands to play in\nthe higher range of the piano keyboard, both staves may be\nnotated in the treble clef to avoid too many ledger lines and\ntoo many notes on the same staff.\nIn this paper we present a novel method to fully no-\ntate a piano performance recorded as an unquantized and\nun-annotated MIDI ﬁle, in which only the note pitches\n(MIDI number), onsets and offsets are considered. The\ninitial analysis of the piece is done through a probabilistic\nmodel proposed by Temperley to jointly estimate meter,\nharmony and streams [18]. The engraving of the score is\ndone through the free software LilyPond [1]. The evalua-\ntion dataset and the Python code are available on the ﬁrst\nauthor’s web site1.\n1http://www.ece.rochester.edu/ ˜acogliat/Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 7592. RELATED WORKS\nThere are several free and commercial programs, such as\nFinale, Sibelius and MuseScore, that can import MIDI ﬁles\nand translate them into full music notation, but they typ-\nically require user intervention to inform the process to\na certain degree. For instance, Finale requires the user\nto manually select the time signature, while it can infer\nthe key signature from the ﬁle itself. Certain sequencers\nand Digital Audio Workstations, such as GarageBand and\nLogic Pro, have various functions to facilitate the import of\nMIDI ﬁles; for example, Logic Pro has a function to align\nthe time track to the beats in the MIDI ﬁles, but requires\nthe user to input the time signature and estimate the initial\ntempo of the piece.\nAmong the programs used for the evaluation of the pro-\nposed method, MuseScore [2] has the most advanced MIDI\nﬁle import feature. MuseScore has a speciﬁc option to im-\nport human performances, and is capable of estimating the\nmeter and the key signature. During the experiment, Mus-\neScore showed a sophisticated capability to position dif-\nferent voices on the piano staves, which resulted in high\nscores from the evaluators, especially in terms of overall\nvoicing and staff placement. Unfortunately, details on how\nall these steps are performed are not documented in the\nwebsite [2] and have not been published in research pa-\npers.\nThe task of identifying musical structures from a MIDI\nperformance has been extensively researched, especially in\nthe past two decades. Cambouropoulos [6] describes the\nkey components necessary to convert a MIDI performance\ninto musical notation: identiﬁcation of elementary musical\nobjects (i.e., chords, arpeggiated chords, and trills), beat\nidentiﬁcation and tracking, time quantization and pitch\nspelling. However, the article does not describe how to ren-\nder a musical score from the modules presented. Takeda et\nal. [16] describe a Hidden Markov Model for the automatic\ntranscription of monophonic MIDI performances. In his\nPhD thesis, Cemgil [7] presents a Bayesian framework for\nmusic transcription, identifying some issues related to au-\ntomatic music typesetting (i.e., the automatic rendering of\na musical score from a symbolic representation), in partic-\nular, tempo quantization, and chord and melody identiﬁca-\ntion. Karydis et al. [12] proposes a perceptually motivated\nmodel for voice separation capable of grouping polyphonic\ngroups of notes, such as chords or other forms of accom-\npaniment ﬁgures, into a perceptual stream. A more re-\ncent paper by Grohganz et al. [11] introduces the concepts\nof score-informed MIDI ﬁle (S-MIDI), in which musical\ntempo and beats are properly represented, and performed\nMIDI ﬁle (P-MIDI), which records a performance in abso-\nlute time. The paper also presents a procedure to approxi-\nmate an S-MIDI ﬁle from a P-MIDI ﬁle – that is, to detect\nthe beats and the meter implied in the P-MIDI ﬁle, starting\nfrom a tempogram then analyzing the beat inconsistency\nwith a salience function based on autocorrelation.\nMusical structures can also be inferred directly from au-\ndio. Ochiai et al. [14] propose a model for the joint es-\ntimation of note pitches, onsets, offsets and beats basedon Non-negative Matrix Factorization constrained with\na rhythmic structure modeled with a Gaussian mixture\nmodel. Collins et al. [9] propose a model for multiple F0\nestimation, beat tracking, quantization, and pattern discov-\nery. The pitches are estimated with a neural network. A\nHidden Markov Model (HMM) is separately used for beat\ntracking. The results are then combined to quantize the\nnotes. Note spelling is performed by estimating the key of\nthe piece and assigning to MIDI notes the most probable\npitch class given the key.\n3. PROPOSED METHOD\nThe proposed method takes an unquantized and un-\nannotated MIDI ﬁle as input. The following subsections\nexplain each step in the proposed method. The entire pro-\ncess is illustrated in Fig. 2. An example of the output is\nshown in Fig. 1 (d).\n3.1 Fix spurious overlapping notes\nThe ﬁrst step is to ﬁx spurious overlapping notes. Piano\nplayers do not play notes with the correct length all the\ntime. As we can see from Fig. 1 (b), certain notes are\nplayed shorter than they should be, resulting in gaps be-\ntween notes, while other notes are played longer than they\nshould be, resulting in overlapping notes. Gaps between\nnotes in the same melodic line might result in extra rests\nin the score, while overlapping notes might result in extra\nstreams being created by the probabilistic model [18] used\nin the next step, resulting in extra voices in the ﬁnal score.\nIn particular, the probabilistic model used in this paper al-\nways assigns overlapping notes to different streams, so it\nis critical to remove erroneous overlaps.\nTo estimate whether the overlap is correct or wrong we\nconsider pairs of overlapping notes separately. For each\npair, we calculate one overlapping ratio for each note. The\nratio is deﬁned as the length of the overlapping region over\nthe length of the note. The overlap is considered spurious\nif the sum of the two ratios is below a certain threshold.\nFor the experiment we set a threshold of 30%. The output\nof the ﬁrst step is a note list, i.e., a list of note events, each\nincluding an onset, a duration (both in milliseconds), and a\nMIDI note number. An example is shown in Fig. 3. Notice\nthe small overlaps in the top ﬁgure between the three low\nnotes in the initial chord and the second bass note, as well\nas the short overlaps in the scale in the soprano line; these\nare removed in the second ﬁgure. Also notice that correct\noverlapping notes, such as a melody line moving over the\nsame bass note, are preserved.\n3.2 Estimate meter, harmony and streams\nIn the second step, we apply the probabilistic model [18] to\nthe note list. The probabilistic model estimates the meter,\nthe harmony, and the streams. The meter and harmony are\nestimated in a single joint process. This process is mod-\neled as an HMM and is based on the concept of tactus-root\ncombination (TRC), a combination of two adjacent tactus760 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016MIDI ﬁleStreams\nMeter\nHarmonyNote ListStavesBars /VoicesScoreTime Signature\nKey SignatureNoteSpellingBeatsQuantized notes12223\n47765\n7776Figure 2 . Illustration of the proposed method. The arrows indicate dependencies between entities. The numbers refer to\nthe steps (subsection numbers) in Section 3.\nTime in seconds22.533.544.555.56PitchF3 G3 A3 B3 C4#D4#F4 G4 A4 B4 C5#D5#\n(a) Original pianoroll of a MIDI performance\nTime in seconds22.533.544.555.56PitchF3 G3 A3 B3 C4#D4#F4 G4 A4 B4 C5#D5#\n(b) Pianoroll after ﬁxing spurious overlapping notes\nFigure 3 . An example of the step of ﬁxing spurious over-\nlapping notes.\nbeats and a chord root. The probability of a TRC only de-\npends on the previous TRC, and the probability of beats\nand notes within a TRC only depends on the TRC. The\nmusical intuition behind this is that the “goodness” of a\ntactus interval depends only on its relationship to the previ-\nous tactus interval (with a preference to minimize changes\nin length from one interval to the next), the goodness of\na root depends only on the previous root (with a prefer-\nence to maintain the same root if possible, or to move to\nanother root that is a ﬁfth away), and the goodness of a\nparticular pattern of notes within a short time interval de-\npends only on the current root and the placement of beats\nwithin that interval (with a preference for note onsets ontactus beats or at plausible points–e.g., roughly halfway–\nin between them, and a preference for notes that are chord-\ntones of the root). The process also considers different di-\nvisions of the tactus interval (representing simple or com-\npound meter) and placements of strong beats (duple versus\ntriple meter). In the current context, the metrical analy-\nsis is useful for the placement of barlines and for rhythmic\nnotation; the harmonic analysis is useful for pitch spelling,\nand also inﬂuences the metrical analysis, since there is a\npreference for strong beats at changes of harmony (this is\nthe reason for estimating the meter and harmony jointly).\nThe stream segregation problem is solved with dynamic\nprogramming by grouping notes into streams such that the\nnumber of streams, the number and length of rests within\nstreams, and pitch intervals within streams are all mini-\nmized [18].\nThe output of the probabilistic model is a list of beats,\nnotes, and chord roots. Each beat includes an onset in mil-\nliseconds, and a level in a metrical hierarchy [17]. The\nprobabilistic model considers the tactus and two subdivi-\nsions in the metrical structure; e.g., in a 3/4 meter, the tac-\ntus will be the quarter note, the ﬁrst subdivision will be\nthe 8th note, and the lowest subdivision the 16th note. The\nmetrical structure also indicates the downbeats. Each note\nhas an onset and a duration in milliseconds, a midi note\nnumber, and a stream number. The chord roots are quan-\ntized to the beats. An example of the output of this stage is\nshown in Fig. 4.\n3.3 Quantize notes\nThe third step quantizes the note onsets to the closest beat\nsubdivision. The offset of each note is also set to coin-\ncide with the onset of the next note in a stream; i.e., gaps\nwithin each stream are discarded. This avoids extra rests\nin the ﬁnal scores, which could stem from notes playedProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 761Figure 4 . Sample output of the probabilistic model for\nestimating the metrical, harmonic, and stream structures.\nThe Xs above the pianoroll illustrate the meter analysis\n(only 3 levels displayed). The letters above show the chord\nroot (only roots on the downbeats are shown). The num-\nbers next to the notes indicate the stream.\nshorter than they should be. See, for instance, the two quar-\nter notes in stream 4 in the second bar of the pianoroll in\nFig. 4; they were played slightly shorter than 8th notes.\n3.4 Determine note spelling\nThe correct note spelling is determined from the harmony\ngenerated by the probabilistic model and is based on the\nproximity in the line of ﬁfths (the circle of ﬁfths stretched\nout into a line) to the chord root. For example, the MIDI\nnote 66 (F /sharp/G/flat) would be spelled F /sharpon a root of D, but\nspelled as G /flaton a root of E /flat.\n3.5 Assign streams to staves\nThe staves of the ﬁnal score are set to be notated in treble\nclef for the upper staff and bass clef for the lower staff.\nStreams are assigned to the staff that accommodates all the\nnotes with the fewest number of ledger lines.\n /noteheads.s2/rests.4/clefs.G/accidentals.flat/accidentals.flat83/noteheads.s2/clefs.F/accidentals.flat/accidentals.flat83/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/brace179/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/rests.4/noteheads.s2/dots.dot\nFigure 5 . First two measures of Bach’s Sinfonia in G mi-\nnor, BWV 797. In the second bar, two streams are assigned\nto the same staff, so two separate monophonic voices must\nbe created for proper rendering.\n3.6 Detect concurrent voices\nOnce streams have been assigned to staves, we determine\nbars and voices. Bars are easily determined by the metrical\nstructure, but note adjustments might be necessary if a note\nstarts in one bar and continues to the next bar. In that case,\nthe note has to be split into two or more tied notes. Concur-\nrent notes in the same bar and staff must be detected and\nencoded appropriately for the next step. If a staff containsstreams that overlap in time, we create monophonic voices\nconsisting of sequences of notes. A sequence is deﬁned\nas a gapless succession of notes and rests without over-\nlaps. For example, as shown in Fig. 5, concurrent streams\nin measure 1 can be treated as monophonic inputs as they\nare assigned to separate staves, but in measure 2, two con-\ncurrent streams are assigned to the same staff, so we have\nto create two monophonic sequences of notes as input for\nthe next step, one containing the F dotted quarter, the other\ncontaining the 16th notes and the D 8th note.\n3.7 Generate the score\nFinally, a Lilypond input ﬁle is generated. Lilypond is\na free, command-line oriented music engraving program,\nwhich takes a text ﬁle as input and, thus, is suitable for the\nautomatic generation of music notation. A possible alter-\nnative to Lilypond, which was considered during our re-\nsearch, is MusicXML [10]. Lilypond has the advantage\nof a simpler and more concise syntax. For instance, the\nmusic example from [10], which requires 130 lines of Mu-\nsicXML, only requires 12 lines in Lilypond.\n4. EVALUATION\nTo evaluate the proposed method, we asked ﬁve doctoral\nstudents in the Music Theory department of the Eastman\nSchool of Music, at various stages of advancement in their\nprogram, to blindly rate the output of the proposed method,\ntwo commercial programs (Finale 2015 [3] and Garage-\nBand 10 [4]) and a free engraving program (MuseScore\n2) applied to the Kostka-Payne dataset used to evaluate\nthe probabilistic model [18]. The commercial programs\nhave been chosen due to their popularity: GarageBand is\nfreely available to all Mac users, Finale is one of the two\nmajor commercial music notation programs, the other be-\ning Sibelius. We also tested the import functionality of\nSibelius but the results were very similar to the ones ob-\ntained by Finale, so we dropped this dataset to save time\nduring the human evaluation. The dataset comprises 19\nmusic excerpts, all of them piano pieces by well-known\ncomposers, for a total of 76 music scores to evaluate. The\npieces were performed on a MIDI keyboard by a semi-\nprofessional piano player. For each piece we provided the\noriginal score, i.e., the ground truth. All the scores had\nbeen anonymized, so that the source program name was\nunknown, and the order of the evaluation was randomized.\nThe evaluators were asked the following questions: 1) Rate\nthe pitch notation with regard to the key signature and the\nspelling of notes. 2) Rate the rhythmic notation with regard\nto the time signature, bar lines, and rhythmic values. 3)\nRate the notation with regard to stems, voicing, and place-\nment of notes on staves. These three questions summarize\nthe most important features that determine the formatting\nand the readability of a musical score. The three features\nare also fairly independent of each other.\nThe ratings were on a scale from 1 to 10 – 10 being\nthe best. We instructed the evaluators to rate the scores\nto reﬂect how close each output was to the ground truth.762 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Finally, we told the evaluators that, since each rating may\nreﬂect multiple aspects of the notation, it was entirely up\nto their judgment to decide how to balance them (e.g., the\nrelative importance of time signature, barline placement,\nand rhythmic values for the second question).\nThe results are shown in Figs. 6, 7 and 8. The ratings\nfrom each evaluator have been normalized (z-scores) by\nsubtracting the mean and dividing by the standard devi-\nation, and the results have been rescaled to the original\nrange by setting their mean to 5 and their standard devi-\nation to 2. The proposed method outperforms all the other\nmethods in the ﬁrst two ratings – pitch notation and rhythm\nnotation – and ties for the top in median for the third rat-\ning – voicing and staff placement. Paired sign tests show\nthat the ratings of the proposed method are signiﬁcantly\nbetter than all the three baselines for the ﬁrst two aspects,\nat a signiﬁcance level of p= 0.0001 . For the third aspect,\nthe proposed method is superior to Finale and equivalent to\nMuseScore at a signiﬁcance level of p= 0.0001 , while the\ncomparison with GarageBand is statistically inconclusive.\nMore work is needed in the note placement. One com-\nmon aspect of music notation that has not been addressed\nin the proposed method is how to group concurrent notes\ninto chords; we can see how that affects the output in\nFig. 1 (d). In the downbeat of the ﬁrst bar, the lowest three\nnotes are not grouped into a chord, as in the ground truth\n(Fig. 1 (a)). This makes the notation less readable, and\nalso introduces an unnecessary rest in the upper staff. A\npossible solution to this problem consists in grouping into\nchords notes that have the same onset and duration, and\nthat are not too far apart, i.e., so that they could be played\nby one hand. A possible drawback of this approach is that\nit may group notes belonging to different voices.\nAnother limitation of the proposed method is the posi-\ntioning of concurrent voices in polyphonic passages. Cur-\nrently, the proposed method relies on the streams detected\nin step 2 to determine the order in which the voices are po-\nsitioned in step 6. In polyphonic music, voices can cross\nso the relative positioning of voices might be appropriate\nfor certain bars but not for others. A possible solution is\nto introduce another step between 6 and 7 to analyze each\nsingle measure and determine whether the relative posi-\ntions of the voice is optimal or not. These two limitations\naffect the note positioning, reﬂected in the scores shown\nin Fig. 8. Finally, the probabilistic model does not always\nproduce the correct results, especially with respect to beats\nand streams. A more sophisticated model may improve the\nrhythm notation and the note positioning, reﬂected in the\nscores shown in Fig. 7 and Fig. 8.\n5. CONCLUSIONS\nIn this paper we presented a novel method to generate a\nmusic notation output from a piano-roll input of a piano\nperformance. We showed that the correct estimation of the\nmeter, harmony and streams is fundamental in producing a\nproperly formatted score. In a blind evaluation by profes-\nsional music theorists, the proposed method consistently\noutperforms two commercial programs and an open source\nProposed MuseScore FinaleGarageBand12345678910Figure 6 . Normalized pitch notation ratings. Each box\ncontains 76 scores from each of the 5 evaluators.\nProposed MuseScore FinaleGarageBand12345678910\nFigure 7 . Normalized rhythm notation ratings. Each box\ncontains 76 scores from each of the 5 evaluators.\nProposed MuseScore FinaleGarageBand12345678910\nFigure 8 . Normalized note positioning ratings. Each box\ncontains 76 scores from each of the 5 evaluators.\nprogram in terms of pitch notation and rhythmic notation,\nand ties for the top in voicing and staff placement on 19\nhuman performances on a MIDI keyboard. The proposed\nmethod can also be combined with any note-level auto-\nmatic music transcription method to complete the audio to\nmusic notation conversion process, but more experiments\nare needed to assess the performance. For future work, we\nalso plan to design a transcription metric for objective eval-\nuation on a larger dataset, which should include complete\npiano pieces.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 7636. REFERENCES\n[1]http://lilypond.org .\n[2]https://musescore.org .\n[3]http://www.finalemusic.com .\n[4]http://www.apple.com/mac/\ngarageband/ .\n[5] Emmanouil Benetos, Simon Dixon, Dimitrios Gian-\nnoulis, Holger Kirchhoff, and Anssi Klapuri. Auto-\nmatic music transcription: challenges and future di-\nrections. Journal of Intelligent Information Systems ,\n41(3):407–434, 2013.\n[6] Emilios Cambouropoulos. From MIDI to traditional\nmusical notation. In Proceedings of the AAAI Work-\nshop on Artiﬁcial Intelligence and Music: Towards\nFormal Models for Composition, Performance and\nAnalysis , volume 30, 2000.\n[7] Ali Taylan Cemgil. Bayesian music transcription . PhD\nthesis, Radboud University Nijmegen, 2004.\n[8] Andrea Cogliati, Zhiyao Duan, and Brendt Wohlberg.\nPiano music transcription with fast convolutional\nsparse coding. In Machine Learning for Signal Pro-\ncessing (MLSP), 2015 IEEE 25th International Work-\nshop on , pages 1–6, Sept 2015.\n[9] Tom Collins, Sebastian B ¨ock, Florian Krebs, and Ger-\nhard Widmer. Bridging the audio-symbolic gap: The\ndiscovery of repeated note content directly from poly-\nphonic music audio. In Audio Engineering Society\nConference: 53rd International Conference: Semantic\nAudio . Audio Engineering Society, 2014.\n[10] Michael Good. MusicXML for notation and analysis.\nThe virtual score: representation, retrieval, restora-\ntion, 12:113–124, 2001.\n[11] Harald Grohganz, Michael Clausen, and Meinard\nM¨uller. Estimating musical time information from per-\nformed midi ﬁles. In ISMIR , pages 35–40, 2014.\n[12] Ioannis Karydis, Alexandros Nanopoulos, Apostolos\nPapadopoulos, Emilios Cambouropoulos, and Yannis\nManolopoulos. Horizontal and vertical integration/seg-\nregation in auditory streaming: a voice separation al-\ngorithm for symbolic musical data. In Proceedings 4th\nSound and Music Computing Conference (SMC’2007) ,\n2007.\n[13] Meinard M ¨uller. Fundamentals of Music Processing:\nAudio, Analysis, Algorithms, Applications . Springer,\n2015.\n[14] Kazuki Ochiai, Hirokazu Kameoka, and Shigeki\nSagayama. Explicit beat structure modeling for non-\nnegative matrix factorization-based multipitch anal-\nysis. In Acoustics, Speech and Signal Processing\n(ICASSP), 2012 IEEE International Conference on ,\npages 133–136. IEEE, 2012.[15] Martin Piszczalski and Bernard A. Galler. Auto-\nmatic music transcription. Computer Music Journal ,\n1(4):24–31, 1977.\n[16] Haruto Takeda, Naoki Saito, Tomoshi Otsuki, Mitsuru\nNakai, Hiroshi Shimodaira, and Shigeki Sagayama.\nHidden markov model for automatic transcription of\nmidi signals. In Multimedia Signal Processing, 2002\nIEEE Workshop on , pages 428–431. IEEE, 2002.\n[17] David Temperley. Music and probability . The MIT\nPress, 2007.\n[18] David Temperley. A uniﬁed probabilistic model for\npolyphonic music analysis. Journal of New Music Re-\nsearch , 38(1):3–18, 2009.764 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Nonnegative Tensor Factorization with Frequency Modulation Cues for Blind Audio Source Separation.",
        "author": [
            "Elliot Creager",
            "Noah D. Stein",
            "Roland Badeau",
            "Philippe Depalle"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1415736",
        "url": "https://doi.org/10.5281/zenodo.1415736",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/103_Paper.pdf",
        "abstract": "We present Vibrato Nonnegative Tensor Factorization, an algorithm for single-channel unsupervised audio source separation with an application to separating instrumental or vocal sources with nonstationary pitch from music record- ings. Our approach extends Nonnegative Matrix Factor- ization for audio modeling by including local estimates of frequency modulation as cues in the separation. This per- mits the modeling and unsupervised separation of vibrato or glissando musical sources, which is not possible with the basic matrix factorization formulation. The algorithm factorizes a sparse nonnegative tensor comprising the audio spectrogram and local frequency- slope-to-frequency ratios, which are estimated at each time-frequency bin using the Distributed Derivative Method. The use of local frequency modulations as separation cues is motivated by the principle of com- mon fate partial grouping from Auditory Scene Analysis, which hypothesizes that each latent source in a mixture is characterized perceptually by coherent frequency and amplitude modulations shared by its component partials. We derive multiplicative factor updates by Minorization- Maximization, which guarantees convergence to a local optimum by iteration. We then compare our method to the baseline on two separation tasks: one considers synthetic vibrato notes, while the other considers vibrato string in- strument recordings.",
        "zenodo_id": 1415736,
        "dblp_key": "conf/ismir/CreagerSBD16",
        "content": "NONNEGATIVE TENSOR FACTORIZATION WITH FREQUENCY\nMODULATION CUES FOR BLIND AUDIO SOURCE SEPARATION\nElliot Creager1,3Noah D. Stein1Roland Badeau2,3Philippe Depalle3\n1Analog Devices Lyric Labs, Cambridge, MA, USA\n2LTCI, CNRS, T ´el´ecom ParisTech, Universit ´e Paris-Saclay, Paris, France\n3CIRMMT, McGill University, Montr ´eal, Canada\nelliot.creager@analog.com, noah.stein@analog.com,\nroland.badeau@telecom-paristech.fr, depalle@music.mcgill.ca\nABSTRACT\nWe present Vibrato Nonnegative Tensor Factorization, an\nalgorithm for single-channel unsupervised audio source\nseparation with an application to separating instrumental or\nvocal sources with nonstationary pitch from music record-\nings. Our approach extends Nonnegative Matrix Factor-\nization for audio modeling by including local estimates of\nfrequency modulation as cues in the separation. This per-\nmits the modeling and unsupervised separation of vibrato\nor glissando musical sources, which is not possible with\nthe basic matrix factorization formulation.\nThe algorithm factorizes a sparse nonnegative tensor\ncomprising the audio spectrogram and local frequency-\nslope-to-frequency ratios, which are estimated at each\ntime-frequency bin using the Distributed Derivative\nMethod. The use of local frequency modulations as\nseparation cues is motivated by the principle of com-\nmon fate partial grouping from Auditory Scene Analysis,\nwhich hypothesizes that each latent source in a mixture\nis characterized perceptually by coherent frequency and\namplitude modulations shared by its component partials.\nWe derive multiplicative factor updates by Minorization-\nMaximization, which guarantees convergence to a local\noptimum by iteration. We then compare our method to the\nbaseline on two separation tasks: one considers synthetic\nvibrato notes, while the other considers vibrato string in-\nstrument recordings.\n1. INTRODUCTION\nNonnegative matrix factorization (NMF) [11] is a popu-\nlar method for the analysis of audio spectrograms [16],\nespecially for audio source separation [17]. NMF mod-\nels the observed spectrogram as a weighted sum of rank-1\nlatent components, each of which factorizes as the outer\nproduct of a pair of vectors representing the constituent\nc/circlecopyrtElliot Creager, Noah D. Stein, Roland Badeau, Philippe\nDepalle. Licensed under a Creative Commons Attribution 4.0 Interna-\ntional License (CC BY 4.0). Attribution: Elliot Creager, Noah D. Stein,\nRoland Badeau, Philippe Depalle. “Nonnegative tensor factorization with\nfrequency modulation cues for blind audio source separation”, 17th Inter-\nnational Society for Music Information Retrieval Conference, 2016.frequencies and onset regions for some signiﬁcant com-\nponent in the mixture, e.g. a musical note. Equivalently,\nthe entire spectrogram matrix approximately factorizes as\na matrix of spectral templates times a matrix of tempo-\nral activations, typically such that the approximate factors\nhave many fewer elements than the full observation. While\nNMF can be used for supervised source separation tasks\nwith a straightforward extension of the signal model [19],\nthis necessitates pre-training NMF representations for each\nsource of interest.\nThe use of modulation cues in source separation is\npopular in the Computational Auditory Scene Analysis\n(CASA) [26] literature, which, unlike NMF, typically re-\nlies on partial tracking. E.g., [25] isolates individual par-\ntials by frequency warping and ﬁltering, while [12] groups\npartials via correlations in amplitude modulations. [2],\nwhich more closely resembles our work in the sense of\nbeing data-driven, factorizes a tensor encoding amplitude\nmodulations for speech separation.\nOur approach is inspired by [20] and [21], which\npresent a Nonnegative Tensor Factorization (NTF) incor-\nporating direction-of-arrival (DOA) estimates in an un-\nsupervised speech source separation task. Whereas use\nof DOA information in that work necessitates multi-\nmicrophone data, we address the single-channel case by\nincorporating the local frequency modulation (FM) cues at\neach time-frequency bin. These cues are combined with\nthe spectrogram as a sparse observation tensor, which we\nfactorize in a probabilistic framework. The modulation\ncues are adopted structurally by way of an NTF where each\nsource in the mixture is modeled via an NMF factor and a\ntime-varying FM factor.\n2. BACKGROUND\n2.1 Nonnegative matrix factorization\nWe now summarize NMF within a probabilistic frame-\nwork. We consider the normalized Short-Time Fourier\nTransform (STFT) magnitudes (i.e., spectrogram) of the\ninput signal as an observed discrete probability distribu-\ntion of energy over the time-frequency plane, i.e.,\npobs(f,t)/defines|X(f,t)|/summationtext\nν,τ|X(ν,τ)|, (1)211ZF\nT\n(a) Basic NMF (Section 2.1)S ZF\nT\n(b) NMF for source separation (Section 2.2)S\nRZF\nT\n(c) Vibrato NTF (Section 3.3)\nFigure 1 : Graphical models for the factorizations in this paper. In each case the input data are a distribution over the\nobserved (shaded) variables, while the model approximates the observation by a joint distribution over observed and latent\n(unshaded) variables that factorizes as speciﬁed. F,T,Z,S, andRrespectively represent the discrete frequencies, hops,\ncomponents, sources, and frequency modulations over which the data is distributed.\n∀f∈ {1,...,F},t∈ {1,...,T}, whereXis the input\nSTFT and (f,t)indexes the time-frequency plane. NMF\nseeks an approximation qto observed distribution pobsthat\nis a valid distribution over the time-frequency plane and\nfactorizes as\nq(f,t) =/summationdisplay\nzq(f|z)q(t|z)q(z) =/summationdisplay\nzq(f|z)q(z,t).(2)\nFigure 1(a) shows the graphical model for a joint distribu-\ntion with this factorization.\nWe have introduced z∈ {1,...,Z}as a latent vari-\nable that indexes components in the mixture, typically\nwithZchosen to yield an overall data reduction, i.e.,\nFZ +ZT/lessmuchFT. For a ﬁxed z0,q(f|z0)is a vec-\ntor interpreted as the spectral template of the z0-th com-\nponent, i.e., the distribution over frequency bins of energy\nbelonging to that component. Likewise, q(z0,t)is inter-\npreted as a vector of temporal activations of the z0-th com-\nponent, i.e., it speciﬁes at what time indices the z0-th com-\nponent is prominent in the observed mixture. Indeed, (2)\ncan be implemented as a matrix multiplication, with the\nusual nonnegativity constraint on the factors satisﬁed im-\nplicitly, since qis a valid probability distribution.\nThe optimization problem is typically formalized as\nminimizing the Kullback-Leibler (KL) divergence between\nthe observation and approximation, or equivalently as\nmaximizing the cross entropy between the two distribu-\ntions:\nmaximize\nq/summationdisplay\nf,tpobs(f,t) logq(f,t)\nsubject toq(f,t) =/summationdisplay\nzq(f|z)q(z,t).(3)\nWhile the non-convexity of this problem prohibits a glob-\nally optimal solution in reasonable time, a locally optimal\nsolution can be found by multiplicative updates to the fac-\ntors, which were ﬁrst presented in [10]. We refer to this\nalgorithm as KL-NMF, but note its equivalence to Proba-\nbilistic Latent Component Analysis (PLCA) [18], as well\nas a strong connection to topic modeling of counts data.2.2 NMF for source separation\nNMF can be leveraged as a source model within a source\nseparation task, such that the observed mixture is modeled\nas a sum of sources, each of which is modeled by NMF.\nWhereas the latent variable zin NMF indexes latent com-\nponents belonging to a source, we now introduce an addi-\ntional latent variable s∈{1,..,S}, which indexes latent\nsources within the mixture. The resulting joint distribution\nover observed and latent variables is expressed as\nq(f,t,s,z ) =q(s)q(f|s,z)q(z,t|s). (4)\nThus the approximation to pobs(f,t)is the marginal distri-\nbution\nq(f,t) =/summationdisplay\nsq(s)q(f,t|s)\n=/summationdisplay\nsq(s)/summationdisplay\nzq(f|s,z)q(z,t|s),(5)\nwhereq(s0)andq(f,t|s0)represent the mixing coefﬁcient\nand NMF source model for the s0-th source in the mixture,\nrespectively. Figure 1(b) shows the graphical model.\nGiven a suitable approximation q, we estimate the latent\nsources in the mixture via Wiener ﬁltering, i.e.,\nXs(f,t) =X(f,t)q(s|f,t), (6)\nwhere the Wiener gains q(s|f,t)are given by the condi-\ntional probabilities1of the latent sources given the ap-\nproximating joint distribution\nq(s|f,t) =q(f,t,s )\nq(f,t)=/summationtext\nzq(s)q(f|s,z)q(z,t|s)/summationtext\nz,s/primeq(s/prime)q(f|s/prime,z)q(z,t|s/prime).\n(7)\nThe estimated sources can then be reconstructed in the\ntime-domain via the inverse STFT.\nWe seek a qthat both approximates pobsand yields\nsource estimates q(f,t|s)close to the true sources. In a\nsupervised setting, the spectral templates for each source\nmodel can be ﬁxed by using basic NMF on some char-\nacteristic training examples in isolation. When the ap-\npropriate training data is unavailable, the basic NMF can\n1A convenient result of the Wiener ﬁlter gains being conditional distri-\nbutions over sources is that the mixture energy is conserved by the source\nestimates in the sense that/summationtext\nsXs(f,t) =X(f,t)∀f,t.212 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016be extended by introducing priors on the factors or other-\nwise adding structure to the observation model to encour-\nage, e.g., smoothness in the activations [24] or harmonicity\nin the spectral templates [3], which hopefully in turn im-\nproves the source estimates. By contrast, our approach ex-\nploits local FM cues directly in the factorization, yielding\nan observation model for latent sources consistent with the\nsorts of pitch modulations expected in musical sounds.\n2.3 Coherent frequency modulation\nWe now introduce frequency-slope-to-frequency ratios\n(FSFR) as local signal parameters under an additive sinu-\nsoidal model that are useful as grouping cues for the sep-\naration of sources with coherent FM, e.g. in the vibrato\nor glissando effects. In continuous time, the additive sinu-\nsoidal model expresses the s-th source as a sum of com-\nponent partials,2each parameterized by an instantaneous\nfrequency and amplitude, i.e.,\nxs(τ) =P/summationdisplay\np=1Ap(τ) cos/parenleftbigg\nθp(τ0) +/integraldisplayτ\nτ0ωp(u)du/parenrightbigg\n(8)\nwherepis the partial index, and θp(τ0),Ap(τ)andωp(τ)\nspecify the initial phase, instantaneous amplitude, and in-\nstantaneous frequency of the p-th partial.\nWe now consider a source under coherent FM, i.e.,\nωp(τ)/defines(1 +κs(τ))ωp(τ0)∀p (9)\nfor some modulation function κswithκs(τ0) = 0 . E.g.,\nκsresembles a slowly-varying sinusoid during frequency\nvibrato, or a gradual ramp function during glissando. The\nFSFR are then expressed as\nυp(τ)/definesω/prime\np(τ)\nωp(τ)=κ/prime\ns(τ)\n1 +κs(τ). (10)\nNote that{υp(τ)}are time-varying but independent of the\npartial index pfor a given source index s. In other words,\nthe instantaneous FSFR is common to all partials belong-\ning to the same source and can be used as a grouping cue\nin unsupervised source separation [7].\n2.4 Distributed Derivative Method\nWe now summarize the Distributed Derivative Method\n(DDM) [4, 8] for signal parameter estimation, which we\nuse to estimate the FSFR at each time-frequency bin. DDM\nestimates the parameters of a monochrome analytic signal\nunder aQ-th order generalized sinusoid model,3which is\n2We do not assume any special structure in the partial frequencies,\ne.g., harmonicity.\n3It is natural to specify the signal locally (near some time-frequency\nbin) as a generalized sinusoid even while the global model remains ad-\nditive sinusoidal. In particular, the notion of a time-frequency-localized\nsignal follows from the ﬁlterbank summation interpretation of the STFT,\nand corresponds to the heterodyned and shifted input, prior to low-pass\nﬁltering by the window and downsampling in time [1]. In a slight abuse\nof notation, we later absorb the time-frequency indices as parameters in\nthe analysis atom, i.e., we switch to the overlap-add interpretation of the\nSTFT without warning.expressed as\nx(τ) = exp/parenleftbiggQ/summationdisplay\nq=0ηqτq/parenrightbigg\n, (11)\nwhere η∈CQ+1is the vector of signal parameters, whose\nreal and imaginary parts specify the log amplitude law and\nphase law,4respectively. In this work, we specify (11) as\na constant amplitude signal with linear frequency modula-\ntion, i.e., η∈C3with/Rfractur(ηi) = 0∀i. The signal pa-\nrameters/Ifractur(η1)and/Ifractur(η2)then specify (within multiplica-\ntive constants) the instantaneous frequency and frequency\nslope, respectively.\nThe parameters of interest can be estimated by consid-\nering the inner product of the signal with a family of dif-\nferentiable analysis atoms of ﬁnite time-frequency support.\nIn particular, the continuous-time STFT can be expressed\nby inner product as\nX(f,t)/defines/angbracketleftx(τ),φ(τ;f,t)/angbracketright=/integraldisplay+∞\nτ=−∞x(τ)φ(τ;f,t)∗dτ,\n(12)\nwhereX(f,t)is the STFT, x(τ)is the input signal, and\nφ(τ;f,t)is a heterodyned window function from some\ndifferentiable family (e.g. Hann), parameterized by its lo-\ncalization (f,t)in the time-frequency plane. The signal\nparameters are solutions to equations of the form\n/angbracketleftx(τ),φ/prime(τ;f,t)/angbracketright=−Q/summationdisplay\nq=1ηq/angbracketleftqτq−1x(τ),φ(τ;f,t)/angbracketright,\n(13)\nwhich is linear in{ηq}forq > 0, and permits an STFT-\nlike computation of both inner products. The right-hand\nside of (13) is derived from the left-hand side using inte-\ngration by parts, exploiting the ﬁnite support of φ(τ;f,t),\nand substituting in the signal derivative x/prime(τ)from (11).\nTo estimate the signal parameters at a particular (f0,t0),\nwe construct a system of linear equations by evaluating\n(13) for each φ(τ;f,t)in a set of nearby atoms Φ, then\nsolve for ηin a least-squares sense. We typically use atoms\nin neighboring frequency bins at the same time step, i.e.,\nΦ ={φ(τ;t0,f0−L−1\n2), ..., φ (τ;t0,f0+L−1\n2)}for\nsome oddL.\nWhile DDM is an unbiased estimator of the signal\nparameters in continuous time, we must implement a\ndiscrete-time approximation on a computer. This intro-\nduces a small bias that can be ignored in practice since the\nSTFT window is typically longer than a few samples [4].\n3. PROPOSED METHOD\n3.1 Motivation\nThe NMF signal model is not sufﬁciently expressive\nto compactly represent a large class of musical sounds,\nnamely those characterized by slow frequency modula-\ntions, e.g., in the vibrato effect. In particular, it speci-\nﬁes a single ﬁxed spectral template per latent component\n4The frequency law is trivially computed from the phase law.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 2130 50 100 150 200 250 300\nt050100150200250fpobs(f,t)\n0.000000.000080.000160.000240.000320.000400.000480.00056(a) Spectrogram\n0 50 100 150 200 250 300\nt050100150200250fquant (υ(f,t);R)\n−25−20−15−10−505101520 (b) Discretized FSFR\nFigure 2 : Unfolding the nonzero elements in the observation tensor for a synthetic vibrato square wave note (G5). The hop\nindextspans 2 seconds of the input audio, while the bin index fspans half the sampling rate, 0–22.05 kHz.\nand thus requires a large number of components to model\nsounds with nonstationary pitch. From a separation per-\nspective, as the number of latent components grows, so\ngrows the need for a comprehensive model that can cor-\nrectly group components belonging to the same source. To\nthis end, we appeal to the perceptual theory of Auditory\nScene Analysis [5], which postulates the importance of\nshared frequency or amplitude modulations among partials\nas a perceptual cue in their grouping [6, 14]. In this work\nwe focus on FM, although in principle our approach could\nbe extended to include amplitude modulations.5We now\npropose an extension to KL-NMF that leverages this so-\ncalled common fate principle and is suitable for the analy-\nsis of vibrato signals.\n3.2 Compiling the observations as a tensor\nDDM yields the local estimates of frequency and fre-\nquency slope for each time-frequency bin, from which the\nFSFR are trivially computed. We deﬁne the (sparse) obser-\nvation tensor pobs(f,t,r )∈RF×T×R\n≥0 as an assignment of\nthe normalized spectrogram into one of Rdiscrete bins for\neach(f,t)according the local FSFR estimate, i.e.,\npobs(f,t,r )/defines/braceleftBigg\npobs(f,t)ifquant (υ(f,t);R) =r\n0 else,\n(14)\nwherepobs(f,t)is the normalized spectrogram as in (1)\nandυare the FSFR as in (10), which are quantized by\nquant (·;R), possibly after clipping to some reasonable\nrange of values. Figure 2 shows the spectrogram and FSFR\nfor a synthetic vibrato square wave.\n3.3 Vibrato NTF\nAs with NMF, we seek a joint distribution qwith a par-\nticular factorized form, whose marginal maximizes cross\nentropy against the observed data. We propose an observa-\ntion model of the form\nq(f,t,r ) =/summationdisplay\nsq(s)q(r|t,s)/summationdisplay\nzq(f|s,z)q(z,t|s)(15)\n5In turn, this would increase the dimensionality of the data.whereq(s)represents the mixing, q(r|t,s)repre-\nsents the common time-varying FSFR per source, and/summationtext\nzq(f|s,z)q(z,t|s)represents the NMF source model.\nFigure 1(c) shows the graphical model of the joint distri-\nbution. Thus, given pobs, we seek an approximation qthat\nfactorizes as in (15) and maximizes\nα(q)/defines/summationdisplay\nf,t,rpobs(f,t,r ) logq(f,t,r )\n=/summationdisplay\nf,t,rpobs(f,t,r ) log/summationdisplay\nz,sq(f,t,r,z,s ).(16)\nThe sum in the argument to the log makes this difﬁcult\nto solve outright, so we ﬁnd a local optimum by itera-\ntive Minorization-Maximization (MM) [9] instead. That\nis, givenq(i), our model at the current ( i-th) iteration, we\npick a better q(i+1)by (a) ﬁnding a concave minorizing\nfunctionβ(q;q(i))such thatβ(q;q(i))≤α(q)∀qand\nβ(q(i);q(i)) =α(q(i)), and (b) maximizing β(q;q(i))with\nrespect toq.\nIn particular, β(q;q(i))is derived6by applying\nJensen’s inequality to (16), and is expressed as\nβ(q;q(i))/defines/summationdisplay\nf,t,r,z,spobs(f,t,r )q(i)(z,s|f,t,r ) logq(f,t,r,z,s )\nq(i)(z,s|f,t,r ),\n(17)\nwhereq(i)(z,s|f,t,r )is the approximate posterior over la-\ntent variables given the model at the i-th iteration7, com-\nputed as\nq(i)(z,s|f,t,r ) =q(i)(z,s,f,t,r )/summationtext\nz/prime,s/primeq(i)(z/prime,s/prime,f,t,r ). (18)\nFor notational convenience we deﬁne\nρ(f,t,r,z,s )/definespobs(f,t,r )q(i)(z,s|f,t,r )and\ndiscarding the denominator in the log of (17) (constant\nw.r.t.q), equivalently write the optimization over the\nminorizing function as\nmax\nq/summationdisplay\nf,t,r,z,sρ(f,t,r,z,s ) logq(s)q(f|z,s)q(z,t |s)q(r|t,s).\n(19)\n6Cf. [20] for a more thorough treatment.\n7Note that the MM iteration speciﬁes an expectation-maximization.214 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20160 50 100 150 200 250 300\nt050100150200250fpobs(f,t)\n0.000000.000080.000160.000240.000320.000400.000480.00056(a) Spectrogram\n0 50 100 150 200 250 300\nt−20−1001020rq(r|t,s= 1)\n0.00.10.20.30.40.50.60.70.80.9 (b) VibNTF FM model\n0 50 100 150 200 250 300\nt050100150200250fpobs(f,t)\n0.00000.00010.00020.00030.00040.00050.00060.00070.0008\n(c) Spectrogram\n0 50 100 150 200 250 300\nt−20−1001020rq(r|t,s= 1)\n0.00.10.20.30.40.50.60.70.80.9 (d) VibNTF FM model\nFigure 3 : For single-note analyses, VibNTF encodes the time-varying pitch modulation. The top row shows a synthetic\nvibrato square wave note (G5), while the bottom row shows a real recording of a violin vibrato note (B/flat6). We plotrin the\nrange [−R\n2,R\n2]in ﬁgures 3(b) and 3(d) to clarify that the index rrepresents a zero-mean quantity (the FSFR).\nWe now alternatively update each factor by separating the\nargument in the log in (19) as a sum of logs, each term\nof which can be optimized by applying Gibb’s inequal-\nity [13]. That is, given the current model, the optimal\nchoice for some factor of q(i+1)is the marginal of ρover\nthe corresponding variables. E.g.,\nq(i+1)(s)←/summationtext\nf,t,r,zρ(f,t,r,z,s )/summationtext\nf,t,r,z,s/primeρ(f,t,r,z,s/prime). (20a)\nLikewise, the remaining factor updates are expressed as\nq(i+1)(f|z,s)←/summationtext\nt,rρ(f,t,r,z,s )/summationtext\nf/prime,t,rρ(f/prime,t,r,z,s ); (20b)\nq(i+1)(z,t|s)←/summationtext\nf,rρ(f,t,r,z,s )/summationtext\nf,t/prime,r,z/primeρ(f,t/prime,r,z/prime,s); (20c)\nq(i+1)(r|t,s)←/summationtext\nf,zρ(f,t,r,z,s )/summationtext\nf,r/prime,zρ(f,t,r/prime,z,s). (20d)\nSinceρis expressed as a product of the current factors and\nobserved data, the factor updates can be implemented efﬁ-\nciently by using matrix multiplications to sum across inner\ndimensions as necessary. The theory guarantees conver-\ngence8to a local minimum [9], although in practice we\n8For guaranteed convergence, ρmust be recomputed after each factor\nupdate, rather than once per iteration as the notation suggests. However,\nin practice we observe convergence without the recomputation.stop the algorithm after some ﬁxed number of iterations.\nThe algorithm is initialized by choosing factors of q(0)as\nrandom valid conditional probabilities.\nFigure 3 visualizes the FM factor q(r|t,s)estimated by\nthe proposed algorithm for single note analyses ( S= 1) of\nboth synthetic and real data.\n4. EVALUATION\nWe present a comparison of our proposed method with\nthe baseline KL-NMF (which our method extends) in a\nblind source separation task examining mixtures of two\nsingle-note recordings. We use the BSSEVAL criteria [23]\nto evaluate separation performance, which necessitates\nthe use of artiﬁcial mixtures. We report the source-to-\ndistortion ratio (SDR), source-to-interference ratio (SIR),\nand source-to-artifact ratio (SAR), each in dB. Each exper-\niment comprises 500 separations, with the sources in each\ntrial chosen as speciﬁed below and mixed at 0dB with a\ntotal mixture duration of 2 seconds at 44.1 kHz sampling\nrate. We report the average metrics across all sources and\ntrials.\nTo use KL-NMF for blind source separation, we must\nspecifyZ= 2, i.e., each mixture component considered as\na source. This baseline should be relatively easy to beat,\nsince empirically KL-NMF does a poor job of modeling\nvibrato signals when Zis small.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 215BSSEVAL in dB\nAlgorithm SDR SIR SAR\n(A) Synthetic data\n2-part KL-NMF -1.5 ±0.1 0.1±0.2 6.9±0.2\nVibrato NTF 14.6 ±1.0 17.0±1.2 23.6±0.7\n(B) Real data\n2-part KL-NMF 2.8 ±0.4 8.0±2.1 9.2±0.2\nVibrato NTF 5.8 ±0.5 9.7±2.2 17.7±0.5\nTable 1 : Mean and 95% conﬁdence intervals of the\nBSSEVAL metrics for 500 unsupervised separations of\ntwo-source mixtures. Experiment A considers synthetic vi-\nbrato square waves, while experiment B considers single-\nnote vibrato string instrument recordings.\nFor Vibrato NTF, we specify S= 2 andZ= 3, i.e.,\nfor each of the two sources we learn spectral templates and\ntemporal activations for three components. E.g., consider-\ning a sinusoidal vibrato, the components could model the\nsource during the crest, midpoint, and trough of the pitch\nmodulation. We estimate the signal parameters at a partic-\nular(f0,t0)using DDM with a family of L= 5 analysis\natoms (heterodyned Hann functions) in the same hop in-\ndex and nearby frequency bins. In order to avoid the inﬂu-\nence of noisy FSFR estimates in the factorization, we apply\nsome mild post-processing prior to quantization. Speciﬁ-\ncally, we implicitly discard FSFR at (f,t)withpobs(f,t)\nbelow the 10thpercentile, or outside a reasonable range\nof±4 times the sampling rate by setting them to the data\nmedian. The FSFR are then quantized evenly across their\nrange intoR= 50 discrete values.\nFor both algorithms, the STFT in (1) is speciﬁed by\na 1024-length (23 msec) Discrete Fourier Transform us-\ning a Hann window with 75% overlap between succes-\nsive frames. Thus, F= 513 , corresponding to the non-\nredundant frequency bins, and T= 346 , the number of\nhops required to cover the mixture duration. Both algo-\nrithms are initialized randomly and run for 100iterations.\nExperiment A examines synthetic data, where the\nsources are square waves with frequency vibrato, whose\nsignal parameters are generated at random. The funda-\nmental frequency corresponds to a note value selected uni-\nformly at random from the three-octave range [A3, G/sharp5].\nThe number of partials is chosen uniformly at random from\nthe range [10, 30], and subsequently reduced as necessary\nto avoid aliasing. The vibrato modulation function, i.e., κs\nin (9), is a sinusoid with depth chosen uniformly at random\nin the range of [5%, 20%] of the fundamental and rate cho-\nsen log-uniformly at random from the range [0.5, 10] Hz.\nExperiment B examines real data, where the sources are\nsingle-note recordings from the McGill University Master\nSamples (MUMS) [15], which contains over 6000 single-\nnote and single-phrase recordings of classical and popu-\nlar instruments. We focus our evaluation on string instru-\nments, which exhibit strong frequency modulation in their\nvibrato effect [22]. The MUMS subset of string instru-\nment notes with vibrato comprises a total of 250 uniquerecordings of violin, viola, cello, and double bass. The\nsources are chosen randomly from this subset and trimmed\nor padded to 2 seconds as necessary.\nResults for both experiments are provided in table 1.\nExperiment A shows a dramatic win for Vibrato NTF over\nthe baseline. We see some variability in the results, which\nreﬂects an optimization over a cost surface with many lo-\ncal optima. With random initialization, Vibrato NTF works\neither very well or very poorly, so robustness could be im-\nproved by a more careful initialization, or alternatively by\nregularizing the factorization in such a way as to avoid sub-\noptimal solutions.\nIn experiment B, we see that moving from synthetic\nto real data degrades the performance of our proposed\nmethod, although we still beat the baseline by a modest\nmargin. Interestingly, the baseline performs better on real\ndata than synthetic, likely because the pitch variations are\nless pronounced so KL-NMF fails less frequently. More-\nover, the pitch modulations in real data are more com-\nplex than in the synthetic case (compare ﬁgures 3(b) and\n3(d)), and may require more components (larger Z) to be\nproperly modeled. Vibrato NTF as proposed tends to de-\ncrease in performance as Zincreases, so additional work\nis required to improve robustness for the analysis of real\ndata. We hypothesize that an extension enforcing tempo-\nral continuity in the FM factor, which should be smooth\nand monotonic per-source, would enhance the grouping of\ncomponents, permitting a larger Zin practice.\n5. CONCLUSION\nWe proposed Vibrato NTF, a novel blind source separa-\ntion algorithm that extends NMF by leveraging local esti-\nmates of frequency modulation as grouping cues directly\nin the factorization. Experimental results using synthetic\ndata showed a substantial improvement over the baseline,\nand validated the FSFR as useful grouping cues in a source\nseparation task. In the experiment with real recordings,\nour method provided a more modest improvement. With\nregards to the analysis of real data, we believe the incorpo-\nration of sensible priors on the factors would improve the\nseparation performance, while careful initalization would\nimprove the robustness. Further work could include tai-\nloring the proposed method to the analysis of polyphonic\nsounds, or sounds with mild or no frequency modulation.\nAdditionally, an extension including coherent amplitude\nmodulations as a grouping cue is possible within the pro-\nposed tensor factorization framework.\n6. ACKNOWLEDGEMENTS\nThe research leading to this paper was partially supported\nby the French National Research Agency (ANR) as a part\nof the EDISON 3D project (ANR- 13-CORD-0008-02),\nand by the Canadian National Science and Engineering\nResearch Council (NSERC). Additional support was pro-\nvided by the Analog Garage, the emerging business accel-\nerator at Analog Devices, Inc.216 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20167. REFERENCES\n[1] J. Allen and L. Rabiner. A uniﬁed approach to short-\ntime Fourier analysis and synthesis. Proceedings of the\nIEEE , 65:1558–64, 1977.\n[2] T. Barker and T. Virtanen. Non-negative tensor fac-\ntorization of modulation spectrograms for monaural\nsound separation. In Proceedings of the 2013 Inter-\nspeech Conference , pages 827–31, Lyon, France, 2013.\n[3] N. Bertin, R. Badeau, and E. Vincent. Enforcing har-\nmonicity and smoothness in Bayesian non-negative\nmatrix factorization applied to polyphonic music tran-\nscription. IEEE Transactions on Audio, Speech, and\nLanguage Processing , 18(3):538–49, 2010.\n[4] M. Betser. Sinusoidal polyphonic parameter estimation\nusing the distribution derivative. IEEE Transactions on\nSignal Processing , 57(12):4633–45, 2009.\n[5] A. Bregman. Auditory Scene Analysis: The Perceptual\nOrganization of Sound . The MIT Press, Cambridge,\nMA, 1990.\n[6] J. M. Chowning. Computer synthesis of the singing\nvoice. In Sound Generation in Winds, Strings, Com-\nputers , pages 4–13. Kungl. Musikaliska Akademien,\nStokholm, Sweden, 1980.\n[7] E. Creager. Musical source separation by coherent fre-\nquency modulation cues. Master’s thesis, McGill Uni-\nversity, 2015.\n[8] B. Hamilton and P. Depalle. A uniﬁed view of non-\nstationary sinusoidal parameter estimation methods us-\ning signal derivatives. In Proceedings of the IEEE In-\nternational Conference on Acoustics, Speech, and Sig-\nnal Processing (ICASSP) , pages 369–72, Kyoto, Japan,\n2012.\n[9] D. Hunter and K. Lange. A tutorial on MM algorithms.\nThe American Statistician , 58(1):30–7, 2004.\n[10] D. Lee, M. Hill, and H. Seung. Algorithms for non-\nnegative matrix factorization. Advances in Neural In-\nformation Processing Systems , 13:556–62, 2001.\n[11] D. Lee and H. Seung. Learning the parts of objects\nby non-negative matrix factorization. Nature , 401:788–\n91, 1999.\n[12] Y . Li, J. Woodruff, and D.Wang. Monaural musical\nsound separation based on pitch and common ampli-\ntude modulation. IEEE Transactions on Audio, Speech,\nand Language Processing , 17(7):1361–71, 2009.\n[13] D. MacKay. Information Theory, Inference, and Learn-\ning Algorithms . Cambridge University Press, Cam-\nbridge, UK, 2005.\n[14] S. McAdams. Segregation of concurrent sounds I: Ef-\nfects of frequency modulation coherence. Journal of\nthe Acoustic Society of America , 86(6):2148–59, 1989.[15] F. Opolko and J. Wapnick. McGill University master\nsamples [Compact Disks], 1987.\n[16] P. Smaragdis and J. Brown. Non-negative matrix fac-\ntorization for polyphonic music transcription. In Pro-\nceedings of the IEEE Workshop on Applications of Sig-\nnal Processing to Audio and Acoustics , pages 177–80,\nNew Paltz, NY , 2003.\n[17] P. Smaragdis, C. F ´evotte, G. Mysore, N. Mohammadia,\nand M. Hoffman. Static and dynamic source separation\nusing nonnegative factorizations: A uniﬁed view. IEEE\nSignal Processing Magazine , 31(3):66–74, 2014.\n[18] P. Smaragdis, B. Raj, and M. Shashanka. A probabilis-\ntic latent variable model for acoustic modeling. In Pro-\nceedings of the NIPS Workshop of Advances in Models\nfor Acoustic Processing , Vancouver, Canada, 2006.\n[19] P. Smaragdis, B. Raj, and M. Shashanka. Super-\nvised and semi-supervised separation of sounds single-\nchannel mixtures. Independent Component Analysis\nand Signal Separation , (Lecture Notes in Computer\nScience, 4666):414–21, 2007.\n[20] N. Stein. Nonnegative tensor factorization for direc-\ntional unsupervised audio source separation. arXiv\npreprint , http://arxiv.org/abs/1411.5010, 2015.\n[21] J. Traa, P. Smaragdis, N. Stein, and D. Wingate. Di-\nrectional NMF for joint source localization and separa-\ntion. In Proceedings of the IEEE Workshop on Appli-\ncations of Signal Processing to Audio and Acoustics ,\nNew Paltz, NY , 2015.\n[22] V . Verfaille, C. Guastavino, and P. Depalle. Percep-\ntual evaluation of vibrato models. In Proceedings of\nthe Conference on Interdisciplinary Musicology , Mon-\ntreal, Canada, 2005.\n[23] E. Vincent, R. Gribonval, and C. F ´evotte. Performance\nmeasurements in blind audio source separation. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 14(4):1462–9, 2006.\n[24] T. Virtanen. Monaural sound source separation by non-\nnegative matrix factorization with temporal continu-\nity and sparseness criteria. IEEE Transactions on Au-\ndio, Speech, and Language Processing , 15(3):1066–\n74, 2007.\n[25] A. Wang. Instantaneous and frequency-warped tech-\nniques for source separation and signal parameteriza-\ntion. In Proceedings of the IEEE Workshop on Appli-\ncations of Signal Processing to Audio and Acoustics ,\npages 47–50, New Paltz, NY , 1995.\n[26] D. Wang and G. Brown. Computational Auditory Scene\nAnalysis: Principles, Algorithms, and Applications .\nWiley Interscience, Hoboken, NJ, 2006.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 217"
    },
    {
        "title": "Analysis of Vocal Imitations of Pitch Trajectories.",
        "author": [
            "Jiajie Dai",
            "Simon Dixon"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416318",
        "url": "https://doi.org/10.5281/zenodo.1416318",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/127_Paper.pdf",
        "abstract": "In this paper, we analyse the pitch trajectories of vocal imi- tations by non-poor singers. A group of 43 selected singers was asked to vocally imitate a set of stimuli. Five stimu- lus types were used: a constant pitch (stable), a constant pitch preceded by a pitch glide (head), a constant pitch fol- lowed by a pitch glide (tail), a pitch ramp and a pitch with vibrato; with parameters for main pitch, transient length and pitch difference. Two conditions were tested: singing simultaneously with the stimulus, and singing alternately, between repetitions of the stimulus. After automatic pitch- tracking and manual checking of the data, we calculated intonation accuracy and precision, and modelled the note trajectories according to the stimulus types. We modelled pitch error with a linear mixed-effects model, and tested factors for significant effects using one-way analysis of variance. The results indicate: (1) Significant factors in- clude stimulus type, main pitch, repetition, condition and musical training background, while order of stimuli, gen- der and age do not have any significant effect. (2) The ramp, vibrato and tail stimuli have significantly greater ab- solute pitch errors than the stable and head stimuli. (3) Pitch error shows a small but significant linear trend with pitch difference. (4) Notes with shorter transient duration are more accurate.",
        "zenodo_id": 1416318,
        "dblp_key": "conf/ismir/DaiD16",
        "content": "ANALYSIS OF VOCAL IMITATIONS OF PITCH TRAJECTORIES\nJiajie Dai, Simon Dixon\nCentre for Digital Music, Queen Mary University of London, United Kingdom\n{j.dai, s.e.dixon }@qmul.ac.uk\nABSTRACT\nIn this paper, we analyse the pitch trajectories of vocal imi-\ntations by non-poor singers. A group of 43 selected singers\nwas asked to vocally imitate a set of stimuli. Five stimu-\nlus types were used: a constant pitch ( stable ), a constant\npitch preceded by a pitch glide ( head ), a constant pitch fol-\nlowed by a pitch glide ( tail), a pitch ramp and a pitch with\nvibrato ; with parameters for main pitch, transient length\nand pitch difference. Two conditions were tested: singing\nsimultaneously with the stimulus, and singing alternately,\nbetween repetitions of the stimulus. After automatic pitch-\ntracking and manual checking of the data, we calculated\nintonation accuracy and precision, and modelled the note\ntrajectories according to the stimulus types. We modelled\npitch error with a linear mixed-effects model, and tested\nfactors for signiﬁcant effects using one-way analysis of\nvariance. The results indicate: (1) Signiﬁcant factors in-\nclude stimulus type, main pitch, repetition, condition and\nmusical training background, while order of stimuli, gen-\nder and age do not have any signiﬁcant effect. (2) The\nramp ,vibrato andtailstimuli have signiﬁcantly greater ab-\nsolute pitch errors than the stable andhead stimuli. (3)\nPitch error shows a small but signiﬁcant linear trend with\npitch difference. (4) Notes with shorter transient duration\nare more accurate.\n1. INTRODUCTION\nStudying the vocal imitations of pitch trajectories is ex-\ntremely important because most of the human produce a\nmusical tone by imitation rather than absolute. Only .01%\nof the general population can produce a musical tone with-\nout the use of an external reference pitch [22]. Although\nsing in tone is the primary element of singing performance,\nthe research of vocal imitations with unstable stimuli has\nnot been explored. It is signiﬁcant to distinguish the in-\nﬂuence factors and to quantise them, ﬁll the gap between\nresponse and stimuli, as well as create knowledge to help\nthe future music education and entertainment.\nThe accuracy of pitch in playing or singing is called into-\nnation [8, 20]. Singing in tune is extremely important for\nsolo singers and choirs because they must be accurate and\nc/circlecopyrtJiajie Dai, Simon Dixon. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: Jiajie Dai, Simon Dixon. “Analysis of vocal imitations of pitch\ntrajectories”, 17th International Society for Music Information Retrieval\nConference, 2016.blend well with accompaniments and other vocal parts [1].\nHowever, it is a practical challenge when the singers have\nto sing with an unstable reference pitch or other vocal parts\nwithout instrumental accompaniment [17, Ch. 12, p. 151].\nNevertheless, most singers rely on their sense of relative\npitch and their teammates who provide reference pitches\nwhich help them maintain tuning, as the initial tonal ref-\nerence can be forgotten over time [9, 11]. Pfordresher et\nal.[16] distinguish between pitch accuracy, the average\ndifference between the sung pitch and target pitch, and\npitch precision, the standard error of sung pitches.\nAs for vocal reference pitch (stimulus of imitation in this\npaper), it usually does not have a ﬁxed pitch for each note\nwhich is different from percussion instruments with a sta-\nble shape [4, 7, 11]. Instead, vocal notes typically ﬂuctu-\nate around the target pitch. When singing with a stable\nreference pitch, the singer will voluntarily adjust their vo-\ncal output until the auditory feedback matches the intended\nnote [28]. This adjustment especially at the beginning of\nthe note, they may sing with vibrato, and they may not sus-\ntain the pitch at the end of the note [27]. Although singers\nmake fewer errors when singing in unison or with stable\naccompaniment [24], the response of unstable stimulus or\nnotes with transient parts is still obscure.\nA transient is part of a signal (often at the beginning) dur-\ning which its properties are rapidly changing and thus un-\npredictable. For most musical tones, a short transient seg-\nment is followed by a much longer steady state segment,\nbut for singing, such a segmentation is difﬁcult, as the sig-\nnal never reaches a steady state. At the beginning of a tone,\na pitch glide is often observed as the singer adjusts the vo-\ncal cords from their previous state (the previous pitch or a\nrelaxed state). Then the pitch is adjusted as the singer uses\nperceptual feedback to correct for any error in the pitch.\nPossibly at the same time, vibrato may be applied, which\nis an oscillation around the central pitch, which is close to\nsinusoidal for skilled singers, but asymmetric for unskilled\nsingers [7]. At the end of the tone, the pitch often moves\nin the direction of the following note, or downward (to-\nward a relaxed vocal cord state) if there is no immediately\nfollowing note.\nTo investigate the response of singers to time-varying pitch\ntrajectories, we prepared a controlled experiment using syn-\nthetic stimuli, in order to test the following hypotheses:\n•The stimulus type will have a signiﬁcant effect on\nintonation accuracy.\n•A greater duration or extent of deviation from the87Sequenced conditionSimultaneous condition\nTime (seconds)0 2 4 6 8 10 12 14Stimulus\nResponse\nStimulus\nResponseFigure 1 : Experimental design showing timing of stimuli\nand responses for the two conditions.\nmain pitch will increase intonation error.\n•The direction of any deviation in the stimulus from\nthe main pitch determines the direction of any error\nin the response.\n•Singing simultaneously with the stimulus will result\nin a lower error than alternating the response with\nthe stimulus.\nWe extract the fundamental frequency ( f0) [5,10] and con-\nvert to a logarithmic scale, corresponding to non-integer\nnumbers of equal-tempered semitones from the reference\npitch (A4, 440Hz). We model responses according to stim-\nulus types in order to compute the parameters of observed\nresponses. The signiﬁcance of factors (stimulus type, stim-\nulus parameters and order of stimuli, as well as partici-\npants’ musical background, gender and age) was evaluated\nby analysis of variance (ANOV A) and linear mixed-effects\nmodels.\n2. MATERIALS AND METHODS\n2.1 Experimental Design\nThe experiment consisted of 75 trials in each of two con-\nditions. In each trial, the participant imitated the stimulus\nthree times (see Figure 1). Each stimulus was one sec-\nond in duration. In the simultaneous condition, the stim-\nulus was repeated six times, with one second of silence\nbetween the repetitions, and the participants sang simul-\ntaneously with the 2nd, 4thand 6thinstances of the stimu-\nlus. The sequenced condition was similar in that the re-\nsponses occurred at the same times as in the simultane-\nous case, but the stimulus was not played at these times.\nThere was a three second pause after each trial. The trials\nof a given condition were grouped together, and partici-\npants were given visual prompts so that they knew when to\nrespond. Each of the 75 trials within a condition used a dif-\nferent stimulus, taken from one of the ﬁve stimulus types\ndescribed in Section 2.2, and presented in a random order.\nThe two conditions were also presented in a random order.\n2.2 Stimuli\nUnlike previous imitation experiments which have used\nﬁxed-pitch stimuli, our experimental stimuli were synthe-sised from time-varying pitch trajectories in order to pro-\nvide controlled conditions for testing the effect of speciﬁc\ndeviations from constant pitch. Five stimulus types were\nchosen, representing a simpliﬁed model of the components\nof sung tones (constant pitch, initial and ﬁnal glides, vi-\nbrato and pitch ramps). The pitch trajectories of the stim-\nuli were generated from the models described below and\nsynthesised by a custom-made MATLAB program, using\na monotone male voice on the vowel /a:/.\nThe ﬁve different stimulus types considered in this work\nare: constant pitch ( stable ), a constant pitch preceded by\nan initial quadratic pitch glide ( head ), a constant pitch fol-\nlowed by a ﬁnal quadratic pitch glide ( tail), a linear pitch\nramp ( ramp ), and a pitch with sinusoidal vibrato ( vibrato ).\nThe stimuli are parametrised by the following variables:\npm, the main or central pitch; d, the duration of the tran-\nsient part of the stimulus; and pD, the extent of pitch devi-\nation frompm. For vibrato stimuli,drepresents the period\nof vibrato. Values for each of the parameters are given in\nTable 1 and the text below.\nBy assuming an equal tempered scale with reference pitch\nA4 tuned to 440 Hz, pitch pand fundamental frequency f0\ncan be related as follows [11]:\np=69+12 log2f0\n440(1)\nsuch that for integer values of pthe scale coincides with\nthe MIDI standard. Note that pitch is not constrained to\ninteger values in this representation.\nFor the stable stimulus, the pitch trajectory p(t)is deﬁned\nas follows:\np(t) =pm, 0/lessorequalslantt/lessorequalslant1. (2)\nThehead stimulus is represented piecewise by a quadratic\nformula and a constant:\np(t) =/braceleftbiggat2+bt+c, 0 /lessorequalslantt/lessorequalslantd\npm, d<t/lessorequalslant1.(3)\nThe parameters a,bandcare selected to make the curve\npass through the point (0,pm+pD)and have its vertex\nat(d,pm). The tailstimulus is similar, with p(t) =pm\nfort< 1−d, and the transient section being deﬁned for\n1−d/lessorequalslantt/lessorequalslant1. In this case the parameters a,bandcare\nchosen so that the curve has vertex (1−d,pm)and passes\nthrough the point (1,pm+pD).\nTheramp stimuli are deﬁned by:\np(t) =pm+pD×(t−0.5), 0/lessorequalslantt/lessorequalslant1. (4)\nFinally, the equation of vibrato stimuli is:\np(t) =pm+pDsin/parenleftbigg2πt\nd/parenrightbigg\n, 0/lessorequalslantt/lessorequalslant1. (5)\nThere is a substantial amount of data on the fundamental\nfrequency of the voice in the speech of speakers who differ\nin age and sex [23]. We chose three pitch values accord-\ning to gender to fall within a comfortable range for most\nsingers. The pitches C3 ( p=48), F3 (p=53) and B /flat388 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016(p=58) were chosen for male singers and C4 ( p=60),\nF4 (p=65) and B /flat4 (p=70) for female singers. For the\nvibrato stimuli, we set the vibrato rate according to a re-\nported mean vibrato rate across singers of 6.1 Hz [18], and\nthe extent or depth of vibrato to ±0.25 or 0.5 semitones, in\naccordance with values reported by [21]. Because intona-\ntion accuracy is affected by the duration of the note [4, 6],\nwe used a ﬁxed one-second duration for all stimuli in this\nexperiment.\nTable 1 : Parameter settings for each stimulus type. The\noctave for the pitch parameter was dependent on sex (3 for\nmale, 4 for female).\nType pmdpD Count\nstable {C, F, B /flat} {0.0 } {0.0 } 3\nhead {C, F, B /flat} {0.1, 0.2 } {±1,±2} 24\ntail {C, F, B /flat} {0.1, 0.2 } {±1,±2} 24\nramp {C, F, B /flat} {1.0 } {±1,±2} 12\nvibrato {C, F, B /flat} {±0.32 } {0.25, 0.5 } 12\n2.3 Participants\nA total of 43 participants (27 female, 16 male) took part\nin the experiment. 38 of them were recorded in the studio\nand 5 were distance participants from the USA, Germany,\nGreece and China (2 participants). The range of ages was\nfrom 19 to 34 years old (mean: 25.1; median: 25; std.dev.:\n2.7). Apart from 3 participants who did not complete the\nexperiment, most singers recorded all the trials.\nWe intentionally chose non-poor singers as our research\ntarget. “Poor-pitch singers” are deﬁned as those who have\na deﬁcit in the use of pitch during singing [15,25], and are\nthus unable to perform the experimental task. Participants\nwhose pitch imitations had on average at least one semi-\ntone absolute error were categorised as poor-pitch singers.\nThe data of poor-pitch singers is not included in this study,\napart from one singer who occasionally sang one octave\nhigher than the target pitch.\nV ocal training is an important factor for enhancing the sing-\ning voice and making the singer’s voice different from that\nof an untrained person [12]. To allow us to test for the\neffects of training, participants completed a questionnaire\ncontaining 34 questions from the Goldsmiths Musical So-\nphistication Index [13] which can be grouped into 4 main\nfactors for analysis: active engagement, perceptual abil-\nities, musical training and singing ability (9, 9, 7 and 7\nquestions respectively).\n2.4 Recording Procedure\nA tutorial video was played before participation. In the\nvideo, participants were asked to repeat the stimulus pre-\ncisely. They were not told the nature of the stimuli. Singers\nwho said they could not imitate the time-varying pitch tra-\njectory were told to sing a stable note of the same pitch.\nThe experimental task consisted of 2 conditions, each con-\ntaining 75 trials, in which participants sang three one-sec-\nond responses in a 16-second period. It took just over one\nhour for participants to ﬁnish the experiment. 22 singers\nTime(s)2 4 6 8 10 12 14Pitch(MIDI)\n464850\nTime(s)00.511.5Pitch(MIDI)\n464850\nTime(s)00.511.5Pitch(MIDI)\n464850\nTime(s)00.511.5Pitch(MIDI)\n464850Figure 2 : Example of extracted pitch and annotation for\nhead stimulus (pm=48,pD=1,d=0.1). The upper\npanel shows the results for pitch extraction by YIN, and\nthe three lower panels show the segmented responses.\ntook the simultaneous condition ﬁrst and 21 singers took\nthe sequenced condition ﬁrst. Although the synthetic stim-\nulus simulated the vowel /a:/, participants occasionally chose\nother vowels that felt comfortable.\nWe used an on-line system to record and manage the ex-\nperiment. After sign-up, participants completed the unﬁn-\nished tests guided by a graphical interface. After singing\neach trial, the system automatically uploaded the record-\nings to a server and the annotation results were simultane-\nously generated. All responses were labelled with singer\nID, condition, trial, order and repetition.\n2.5 Annotation\nEach recording ﬁle contains three responses, from which\nwe extract pitch information using the YIN algorithm (ver-\nsion 28th July 2003) [5]. This outputs the pitch trajectory\np(t)from which we compute the median pitch ¯pfor each\nresponse. The segmentation into individual responses is\nbased on the timing, pitch and power. If participants sang\nmore than 3 repetitions we choose the three responses that\nhave the longest duration and label them with the recording\norder. Any notes having a duration less than 0.1 seconds\nwere excluded. Any remaining notes with a duration less\nthan 0.4 seconds were ﬂagged and checked manually. Most\nof these deﬁcient notes were due to participants making no\nresponse. Figure 2 shows an example of pitch extraction\nand segmentation.\nThe main pitch ¯pof response was calculated by removing\nthe ﬁrst 10% and last 10% of the response duration, and\ncomputing the median of the remaining pitch track. The\npitch errorepis calculated as the difference between the\nmain pitch of the stimulus pmand that of the response ¯p:\nep=¯p−pm (6)\nFor avoiding bias due to large errors we exclude any re-\nsponses with |ep|>2 (4% of responses). Such errors aroseProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 89when participants sang the pitch of the previous stimulus or\none octave higher than the stimulus. The resulting database\ncontains 18572 notes, from which the statistics below were\ncalculated.\nThe mean pitch error (MPE) over a number of trials mea-\nsures the tendency to sing sharp (MPE >0) or ﬂat (MPE <\n0) relative to the stimulus. The mean absolute pitch error\n(MAPE) measures the spread of a set of responses. These\ncan be viewed respectively as inverse measures of accuracy\nand precision (cf. [16]).\nTo analyse differences between the stimulus and response\nas time series, pitch error ep\nf(t)is calculated frame-wise:\nep\nf(t) =pr(t) −ps(t), for stimulus ps(t)and response\npr(t), where the subscript fdistinguishes frame-wise re-\nsults. For frame period Tand frame index i, 0/lessorequalslanti<M ,\nwe calculate summary statistics:\nMAPE f=1\nMM−1/summationdisplay\ni=0|ep\nf(iT)| (7)\nand MPE fis calculated similarly. Equation 7 assumes that\nthe two sequences pr(t)andps(t)are time-aligned. Al-\nthough cross-correlation could be used to ﬁnd a ﬁxed off-\nset between the sequences, or dynamic time warping could\nalign corresponding features if the sequences proceed at\ndifferent or time-varying rates, in our case we consider\nsinging with the correct timing to be part of the imitation\ntask, and we align the stimulus to the beginning of the de-\ntected response.\n3. RESULTS\nWe ﬁrst report pitch error (MPE: 0.0123; std.dev.: 0.3374),\nabsolute pitch error (MAPE: 0.2441; std.dev.: 0.2332) and\nframe-wise absolute pitch error (MAPE f: 0.3968; std.dev.:\n0.2238) between all the stimuli and responses. 71.1% of\nresponses have an absolute error less than 0.3 semitones.\n51.3% of responses are higher than the stimulus ( ep>\n0). All the singers’ information, questionnaire responses,\nstimulus parameters and calculated errors were arranged\nin a single table for further processing. We ﬁrst analyse\nthe factors inﬂuencing absolute pitch error in the next two\nsubsections, and then consider pitch error in section 3.3\nand the modelling of responses in the following two sub-\nsections.\n3.1 Inﬂuence of stimulus type on absolute pitch error\nWe performed one-way independent samples analysis of\nvariance (one-way ANOV A) with the ﬁxed factor stimu-\nlus type (ﬁve levels: stable ,head ,tail,ramp andvibrato )\nand the random factor participant. There was a signiﬁcant\neffect of stimulus type ([ F(4, 18567 ) =72.3,p< .001]).\nPost hoc comparisons using the Tukey HSD test indicated\nthat the absolute epfortail,ramp andvibrato stimuli were\nsigniﬁcantly different from that of the stable stimuli, while\nthehead stimuli showed no signiﬁcant difference from sta-\nblestimuli (see Table 2). Thus tail,ramp andvibrato stim-\nuli do have an effect on pitch precision. Table 2 also showsStimulus MAPE Conﬁdence interval Effect size\nstable 0.1977 [0.1812, 0.2141] –\nhead 0.1996 [0.1938, 0.2054] 0.2 cents\ntail 0.2383 [0.2325, 0.2441]* 4.1 cents\nramp 0.3489 [0.3407, 0.3571]*** 15.1 cents\nvibrato 0.2521 [0.2439, 0.2603]*** 5.5 cents\nTable 2 : Mean absolute pitch error (MAPE) and 95% con-\nﬁdence intervals for each stimulus type (*** p < .001;\n**p< .01; *p< .05).\nthe 95% conﬁdence intervals for each stimulus type. Ef-\nfect sizes were calculated by a linear mixed-effects model\ncomparing with stable stimulus results.\n3.2 Factors of inﬂuence for absolute pitch error\nThe participants performed a self-assessment of their musi-\ncal background with questions from the Goldsmiths Musi-\ncal Sophistication Index [14] covering the four areas listed\nin Table 3, where the general factor is the sum of other four\nfactors. An ANOV A F-test found that all background fac-\ntors are signiﬁcant for pitch accuracy (see Table 3). The\ntask involved both perception and production, so it is to be\nexpected that both of these factors (perceptual and singing\nabilities) would inﬂuence results. Likewise most musical\ntraining includes some ear training which would be bene-\nﬁcial for this experiment.\nFactor Test Results\nGeneral factor F(30, 18541 ) =54.4 ***\nActive engagement F(21, 18550 ) =37.3 ***\nPerceptual abilities F(22, 18549 ) =57.5 ***\nMusical training F(24, 18547 ) =47.2 ***\nSinging ability F(20, 18551 ) =69.8 ***\nTable 3 : Inﬂuence of background factors.\nWe used R [19] and lme4 [2] to perform a linear mixed-\neffects analysis of the relationship between factors of in-\nﬂuence and |ep|. The factors stimulus type, main pitch,\nage, gender, the order of stimuli, trial condition, repetition,\nduration of pitch deviation d, extent of pitch deviation pD,\nobserved duration and the four factors describing musical\nbackground were added separately into the model, and a\none-way ANOV A between the model with and without the\nfactor tested whether the factor had a signiﬁcant effect. Ta-\nble 4 shows the p-value of ANOV A results after adding\neach factor.\nWe created a ﬁxed model with factors stimulus type, main\npitch, repetition and trial condition. As a random effect,\nwe had the factor of the singer. Visual inspection of resid-\nual plots did not reveal any obvious deviations from ho-\nmoscedasticity or normality. The p-values were obtained\nby likelihood ratio tests of the full model with the effect\nin question against the model without the effect in ques-\ntion [26].\nAccording to the modelling results on |ep|, signiﬁcant ef-\nfects were found for the factors stimulus type, main pitch90 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Table 4 : Signiﬁcance and effect sizes for tested factors\nbased on ANOV A results.\nFactors p-value Effect size (cents)\nStimulus type 2.2e-16*** See Table 2\npm 5.4e-7*** -0.19\nAge 0.51\nGender 0.56\nOrder of stimuli 0.13\nTrial condition 2.2e-16*** 3.2\nRepetition 2.2e-16*** -1.8\nDuration of transient d 2.2e-16*** 11.4\nsign(pD) 5.1e-6*** 0.8\nabs(pD) 8.3e-12*** 1.9\nObserved duration 3.3e-4*** -5.4\nActive engagement 6.9e-2\nPerceptual abilities 0.04* -0.3\nMusical training 6.2e-5*** -0.5\nSinging ability 8.2e-2\npm(effect size: -2.35 cents per octave), trial condition,\nrepetition, musical background, duration of pitch deviation\n(effect size: 11.4 cents per second), direction of pitch de-\nviation, magnitude of pitch deviation (effect size: 1.7 cents\nper semitone) and observed duration (effect size: -5.4 cents\nper second). The remaining factors (singer, age, gender\nand the order of stimuli) did not have any signiﬁcant ef-\nfect on |ep|in this model. The LME models gave different\nresults for the background questionnaire factors than the\none-way ANOV A, with only two of the factors, perceptual\nabilities and musical training, having a signiﬁcant effect.\nContrary to our hypothesis, singing simultaneously (MAPE:\n0.26; std.dev.: 0.25) is 3.2 cents less accurate than the se-\nquenced condition (MAPE: 0.23; std.dev: 0.21). Despite\nthe large spread of results, the standard errors in the means\nare small and the difference is signiﬁcant. Recall also that\nresponses with |ep|over 2 semitones were excluded.\nOther signiﬁcant factors were repetition, where we found\nthat MAPE decreases 1.8 cents for each repetition (that is,\nparticipants improved with practice), and observed dura-\ntion and main pitch, which although signiﬁcant, had very\nsmall effect sizes for the range of values they took on.\n3.3 Effect of pitch deviation on pitch error\nWe now look at speciﬁc effects on the direction of pitch er-\nror, to test the hypothesis that asymmetric deviations from\nmain pitch are likely to lead to errors in the direction of\nthe deviation. For the stable ,head andtailstimuli, a corre-\nlation analysis was conducted to examine the relationship\nbetween pitch deviation and MPE. The result was signiﬁ-\ncant on MPE ( F(4, 12642 ) =8.4,p=9.6e−7) and MAPE\n(F(4, 12642 ) =8.2,p=1.3e−6). A signiﬁcant regression\nequation was found, with R2=2.5e−3, modelling pitch\nerror aseP=0.033 +0.01pD. Pitch error increased 1 cent\nfor each semitone of pD, a signiﬁcant but small effect, as\nshown in Figure 3.\nPitch error/semitone-2 -1 0 1 2Pitch difference/semitone\n-0.2-0.15-0.1-0.0500.050.10.150.20.25\nData\nFit\nConfidence boundsFigure 3 : Boxplot of MPE for different pD, showing me-\ndian and interquartile range, regression line (red, solid)\nand 95% conﬁdence bounds (red, dotted). The regression\nshows a small bias due to the positively skewed distribu-\ntion of MPE.\n3.4 Modelling\nIn this section, we ﬁt the observed pitch trajectories to a\nmodel deﬁned by the stimulus type, to better understand\nhow participants imitated the time-varying stimuli. The\nhead andtailstimuli are modelled by a piecewise linear\nand quadratic function. Given the break point, correspond-\ning to the duration of the transient, the two parts can be\nestimated by regression. We perform a grid search on the\nbreak point and select the optimal parameters according to\nthe smallest mean square error. Figure 4 shows an example\nofhead response modelling.\nTheramp response is modelled by linear regression. The\nmodelpmof a stable response is the median of p(t)for\nthe middle 80% of the response duration. The vibrato re-\nsponses were modelled with the MATLAB nlinﬁt function\nusing Equation 5 and initialising the parameters with the\nparameters of the stimulus.\nFor the absolute pitch error between modelling results and\nstimuli, 66.5% of responses have an absolute error less\nthan 0.3 semitones, while only 29.3% of trials have an ab-\nsolute error less than 0.3 semitones between response and\nstimulus. We observed that some of the vibrato models did\nnot ﬁt the stimulus very well because the singer attempted\nto sing a stable pitch rather than imitate the intonation tra-\njectory.\n3.5 Duration of transient\nAs predicted, the duration dof the transient has a signiﬁ-\ncant effect on MPE ( F(5, 18566 ) = 51.4,p < .001). For\nthestable ,head andtailstimuli, duration of transient in-\nﬂuences MAPE ( F(2, 12644 ) = 31.5,p < .001), where\nstimuli with smaller transient length result in lower MAPE.\nThe regression equation is MAPE =0.33 +0.23dwith\nR2=0.208. MAPE increased 23.2 cents for each second\nof transient. This matches the result from the linear mixed-\neffects model, where effect size is 23.8 cents per second.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 91Time/s00.2 0.4 0.6 0.8 11.2Semitone/MIDI\n46.54747.54848.549\nReponse\nModelling line\nStimulusFigure 4 : Example of modelling the response to a head\nstimulus with parameters d=0.1,pD= − 1 andpm=\n48. The response model has d=0.24,pD= −0.997 and\npm=47.87. The forced ﬁt to the stimulus model treats as\nnoise response features such as the ﬁnal rising intonation.\nBased on the modelling results, we observed that transient\nlength in responses was longer than in the corresponding\nstimuli. 74.2% of head andtailresponses have transient\nlength longer than that of the stimulus. Stimulus transients\nare 0.1 or 0.2 seconds, but 65.5% of head and 72.0% of\ntailresponses have a transient longer than 0.2 seconds.\n4. DISCUSSION\nSince we intentionally chose non-poor singers, most par-\nticipants imitated with small error. 88.5% of responses\nwere sung with intonation error less than half a semitone.\nThe responses are characterised far more by imprecision\nthan inaccuracy. That is, there is very little systematic er-\nror in the results (MPE =0.0123), whereas the individ-\nual responses exhibit much larger errors in median pitch\n(MAPE =0.2441) and on a frame-wise level within notes\n(MAPE f=0.3968). The results for MAPE are within\nthe range reported for non-poor singers attempting known\nmelodies (19 cents [11], 28 cents [4]), and thus is better ex-\nplained by limitations in production and perception rather\nthan by any particular difﬁculty of the experimental task.\nThestable stimuli gave rise to the lowest pitch errors, al-\nthough the head responses were not signiﬁcantly different.\nThe larger errors observed for the tail,ramp andvibrato\nstimuli could be due to a memory effect. These three stim-\nulus types have in common that the pitch at the end of the\nstimulus differs from pM. Thus the most recent pitch heard\nby the participant could distract them from the main tar-\nget pitch. The ramp stimuli, having no constant or central\npitch, was the most difﬁcult to imitate, and resulted in the\nhighest MAPE.\nIt was hypothesised that the simultaneous condition would\nbe easier than the sequenced condition, as singing tends to\nbe more accurate when accompanied by other singers or\ninstruments. We propose two reasons why this experiment\nmight be exceptional. Firstly, in the sequenced condition,the time between stimulus and response was short (1 sec-\nond), so it would be unlikely that the participant would for-\nget the reference pitch. Secondly, the stimulus varied more\nquickly than the auditory feedback loop, the time from\nperception to a change in production (around 100ms [3]),\ncould accommodate. Thus the feedback acts as a distractor\nrather than an aid. Singing in practice requires staying in\ntune with other singers and instruments. If a singer takes\ntheir reference from notes with large pitch ﬂuctuations, es-\npecially at their ends, this will adversely affect intonation.\n5. CONCLUSIONS\nWe designed a novel experiment to test how singers re-\nspond to controlled stimuli containing time-varying pitches.\n43 singers vocally imitated 75 instances of ﬁve stimulus\ntypes in two conditions. It was found that time-varying\nstimuli are more difﬁcult to imitate than constant pitches,\nas measured by absolute pitch error. In particular, stim-\nuli which end on a pitch other than the main pitch ( tail,\nramp andvibrato stimuli) had signiﬁcantly higher abso-\nlute pitch errors than the stable stimuli, with effect sizes\nranging from 15 cents ( ramp ) to 4.1 cents ( tail).\nUsing a linear mixed-effects model, we determined that\nthe following factors inﬂuence absolute pitch error: stim-\nulus type, main pitch, trial condition, repetition, duration\nof transient, direction and magnitude of pitch deviation,\nobserved duration, and self-reported musical training and\nperceptual abilities. The remaining factors that were tested\nhad no signiﬁcant effect, including self-reported singing\nability, contrary to other studies [11].\nUsing one-way ANOV A and linear regression, we found a\npositive correlation between extent of pitch deviation (pitch\ndifference,pD) and pitch error. Although the effect size\nwas small, it was signiﬁcant and of similar order to the\noverall mean pitch error. Likewise we observed that the\ndurationdof the transient proportion of the stimulus cor-\nrelated with absolute pitch error. Contrary to expectations,\nparticipants performed 3.2 cents worse in the condition\nwhen they sang simultaneously with the stimulus, although\nthey also heard the stimulus between singing attempts, as\nin the sequenced condition.\nFinally, we extracted parameters of the responses by a for-\nced ﬁt to a model of the stimulus type, in order to describe\nthe observed pitch trajectories. The resulting parameters\nmatched the stimuli more closely than the raw data did.\nMany aspects of the data remain to be explored, but we\nhope that the current results take us one step closer to un-\nderstanding interaction between singers.\n6. DATA A V AILABILITY\nThere is the tutorial video which show participants how\nto ﬁnish the experiment before they start: https://www.\nyoutube.com/watch?v=xadECsaglHk . The annotated\ndata and code to reproduce our results are available in an\nopen repository at: https://code.soundsoftware.ac.\nuk/projects/stimulus-intonation/repository .92 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20167. REFERENCES\n[1] Per-Gunnar Alldahl. Choral Intonation . Gehrmans,\n2008.\n[2] Douglas Bates, Martin M ¨achler, Ben Bolker, and\nSteve Walker. Fitting Linear Mixed-Effects Models Us-\ning lme4. Journal of Statistical Software , 67(1):1–48,\n2015.\n[3] T. A. Burnett, M. B. Freedland, C. R. Larson, and T. C.\nHain. V oice F0 Responses to Manipulations in Pitch\nFeedback. Journal of the Acoustical Society of Amer-\nica, 103(6):3153–3161, 1998.\n[4] Jiajie Dai, Matthias Mauch, and Simon Dixon. Analy-\nsis of Intonation Trajectories in Solo Singing. In Pro-\nceedings of the 16th ISMIR Conference , volume 421,\n2015.\n[5] Alain De Cheveign ´e and Hideki Kawahara. YIN, A\nFundamental Frequency Estimator for Speech and Mu-\nsic.The Journal of the Acoustical Society of America ,\n111(4):1917–1930, 2002.\n[6] J. Fyk. Pitch-matching Ability In Children As A Func-\ntion of Sound Duration. Bulletin of the Council for Re-\nsearch in Music Education , pages 76–89, 1985.\n[7] David Gerhard. Pitch Track Target Deviation in Natu-\nral Singing. In ISMIR , pages 514–519, 2005.\n[8] Joyce Bourne Kennedy and Michael Kennedy. The\nConcise Oxford Dictionary of Music . Oxford Univer-\nsity Press, 2004.\n[9] Peggy A Long. Relationships Between Pitch Memory\nin Short Melodies and Selected Factors. Journal of Re-\nsearch in Music Education , 25(4):272–282, 1977.\n[10] Matthias Mauch and Simon Dixon. pYIN: A Fun-\ndamental Frequency Estimator Using Probabilistic\nThreshold Distributions. In Proceedings of the IEEE\nInternational Conference on Acoustics, Speech, and\nSignal Processing (ICASSP 2014) , 2014.\n[11] Matthias Mauch, Klaus Frieler, and Simon Dixon. In-\ntonation in Unaccompanied Singing: Accuracy, Drift,\nand a Model of Reference Pitch Memory. The Journal\nof the Acoustical Society of America , 136(1):401–411,\n2014.\n[12] Ana P Mendes, Howard B Rothman, Christine\nSapienza, and WS Brown. Effects of V ocal Training on\nthe Acoustic Parameters of the Singing Voice. Journal\nof Voice , 17(4):529–543, 2003.\n[13] Daniel M ¨ullensiefen, Bruno Gingras, Jason Musil,\nLauren Stewart, et al. The Musicality of Non-\nmusicians: An Index for Assessing Musical So-\nphistication in the General Population. PloS one ,\n9(2):e89642, 2014.[14] Daniel M ¨ullensiefen, Bruno Gingras, Lauren Stew-\nart, and J Musil. The Goldsmiths Musical Sophisti-\ncation Index (Gold-MSI): Technical Report and Doc-\numentation v0.9. London: Goldsmiths, University\nof London. URL: http://www.gold.ac.uk/music-mind-\nbrain/gold-msi , 2011.\n[15] Peter Q Pfordresher and Steven Brown. Poor-pitch\nSinging in the Absence of “Tone Deafness”. Music Per-\nception: An Interdisciplinary Journal , 25(2):95–115,\n2007.\n[16] Peter Q Pfordresher, Steven Brown, Kimberly M\nMeier, Michel Belyk, and Mario Liotti. Imprecise\nSinging is Widespread. The Journal of the Acoustical\nSociety of America , 128(4):2182–2190, 2010.\n[17] John Potter, editor. The Cambridge Companion to\nSinging . Cambridge University Press, 2000.\n[18] Eric Prame. Measurements of the Vibrato Rate of Ten\nSingers. The Journal of the Acoustical Society of Amer-\nica, 96(4):1979–1984, 1994.\n[19] R Development Core Team. R: A Language and En-\nvironment for Statistical Computing . R Foundation for\nStatistical Computing, Vienna, Austria, 2008.\n[20] John Andrew Simpson, Edmund S.C. Weiner, et al. The\nOxford English Dictionary , volume 2. Clarendon Press\nOxford, 1989.\n[21] J. Sundberg. Acoustic and Psychoacoustic Aspects of\nV ocal Vibrato. Technical Report STL-QPSR 35 (2–3),\npages 45–68, Department for Speech, Music and Hear-\ning, KTH, 1994.\n[22] Annie H Takeuchi and Stewart H Hulse. Absolute\nPitch. Psychological bulletin , 113(2):345, 1993.\n[23] Hartmut Traunm ¨uller and Anders Eriksson. The Fre-\nquency Range of the V oice Fundamental in the Speech\nof Male and Female Adults. Consult ´e le, 12(02):2013,\n1995.\n[24] Allan Vurma and Jaan Ross. Production and Perception\nof Musical Intervals. Music Perception: An Interdisci-\nplinary Journal , 23(4):331–344, 2006.\n[25] Graham F Welch. Poor Pitch Singing: A Review of the\nLiterature. Psychology of Music , 7(1):50–58, 1979.\n[26] Bodo Winter. Linear Models and Linear Mixed Ef-\nfects Models in R with Linguistic Applications. arXiv\npreprint arXiv:1308.5499 , 2013.\n[27] Yi Xu and Xuejing Sun. How Fast Can We Really\nChange Pitch? Maximum Speed of Pitch Change Re-\nvisited. In INTERSPEECH , pages 666–669, 2000.\n[28] Jean Mary Zarate and Robert J Zatorre. Experience-\ndependent Neural Substrates Involved in V ocal Pitch\nRegulation During Singing. Neuroimage , 40(4):1871–\n1887, 2008.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 93"
    },
    {
        "title": "Go with the Flow: When Listeners Use Music as Technology.",
        "author": [
            "Andrew M. Demetriou",
            "Martha A. Larson",
            "Cynthia C. S. Liem"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1415528",
        "url": "https://doi.org/10.5281/zenodo.1415528",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/068_Paper.pdf",
        "abstract": "Music has been shown to have a profound effect on lis- teners’ internal states as evidenced by neuroscience re- search. Listeners report selecting and listening to music with specific intent, thereby using music as a tool to achieve desired psychological effects within a given con- text. In light of these observations, we argue that music information retrieval research must revisit the dominant assumption that listening to music is only an end unto it- self. Instead, researchers should embrace the idea that music is also a technology used by listeners to achieve a specific desired internal state, given a particular set of circumstances and a desired goal. This paper focuses on listening to music in isolation (i.e., when the user listens to music by themselves with headphones) and surveys research from the fields of social psychology and neuro- science to build a case for a new line of research in music information retrieval on the ability of music to produce flow states in listeners. We argue that interdisciplinary collaboration is necessary in order to develop the under- standing and techniques necessary to allow listeners to exploit the full potential of music as psychological tech- nology.",
        "zenodo_id": 1415528,
        "dblp_key": "conf/ismir/DemetriouLL16",
        "content": "GO WITH THE FLOW:  WHEN LISTENERS USE MUSIC AS TECHNOLOGY Andrew Demetriou ‡ Martha Larson ‡§ Cynthia C. S. Liem ‡ ‡ Delft University of Technology, Delft, The Netherlands § Radboud University, Nijmegen, The Netherlands andrew.m.demetriou@gmail.com {m.a.larson, c.c.s.liem}@tudelft.nl ABSTRACT Music has been shown to have a profound effect on lis-teners’ internal states as evidenced by neuroscience re-search. Listeners report selecting and listening to music with specific intent, thereby using music as a tool to achieve desired psychological effects within a given con-text. In light of these observations, we argue that music information retrieval research must revisit the dominant assumption that listening to music is only an end unto it-self. Instead, researchers should embrace the idea that music is also a technology used by listeners to achieve a specific desired internal state, given a particular set of circumstances and a desired goal. This paper focuses on listening to music in isolation (i.e., when the user listens to music by themselves with headphones) and surveys research from the fields of social psychology and neuro-science to build a case for a new line of research in music information retrieval on the ability of music to produce flow states in listeners. We argue that interdisciplinary collaboration is necessary in order to develop the under-standing and techniques necessary to allow listeners to exploit the full potential of music as psychological tech-nology. 1. INTRODUCTION When the word technology is used in the context of mu-sic, it generally relates to the development of new digital devices or algorithms that support the production, stor-age, and/or transmission of music.  In this paper we break from the conventional use of the word technology in re-gards to music, reprising a conception of music as a tech-nology in and of itself.  In order to understand precisely what music as technolo-gy means, it is helpful to take a closer look at the mean-ing of the word technology. Specifically, we use technol-ogy in the sense of a manner of accomplishing a task es-pecially using technical processes, methods, or knowledge1. We do not contradict the generally accepted perspective that music may exist for its own sake. How-ever, we do take the position that other considerations may also be at stake when listeners listen to music. Spe-                                                1 http://www.merriam-webster.com/dictionary/technology  cifically, we hold that there are cases when listeners use music as a tool that is directed towards accomplishing a task. In these cases, music can be considered as part of a method applied by listeners to achieve a goal.  The notion of music as technology was already coined in the area of sociology by DeNora at the end of the millen-nium [8]. This work characterized music as part of the continuing process of self-development, and posited that individuals use it to maintain and develop a social identi-ty as well as a means to self-regulate emotions, moods, energy levels, or for the purposes of ‘self care’. In effect, it was suggested that people outsource various sorts of 'emotional work' to music, based on their goals within a given context.  We argue that the moment is now ripe for the music in-formation retrieval (MIR) community to revisit this no-tion. In the intervening years, social psychology and neu-roscience have considerably advanced our understanding of how music is used in everyday life, and how it effects the brain. Further, music recommender systems show signs that they are already reorienting themselves from music that users \"like\" to music that users find useful in a particular situation. This development is evident in the evolution of how the purpose of music recommender sys-tems is described in the literature. A 2002 publication [36] characterized this purpose as recommending music that the user will be interested in, which contrasts with the statement of a 2011 publication [12] that a good rec-ommendation system should...maximize the user's satis-faction by playing (the) appropriate song at the right time. Currently, the unprecedentedly large amount of mu-sic available online offers new possibilities of finding a tight fit with listener needs. Reflecting this focus, a 2015 publication [32] stated the purpose of music recommend-er systems to provide guidance to users navigating large collections. We draw on these contemporary findings and theory to understand how users may better use music as a tool in everyday life. The contribution of this paper is to revisit and update the notion of music as technology, and to link it to a Call to Action for MIR and neighboring psychology-oriented communities. It should be noted that the socio-psychological concept of music preference as a potential indicator of personality, values and beliefs (and as a ‘so-cial badge’) is relevant to music consumption behavior, fitting into the concept of considering music as a technol-ogy (to establish belonging), and not yet taken into ac-count sufficiently in the context of music recommender \n © Andrew Demetriou, Martha Larson, Cynthia C. S. Liem. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: Andrew Demetriou, Martha Larson, Cynthia C. S. Liem. “Go with the Flow: When Listeners User Music As Technology”, 17th International Society for Music Information Retrieval Conference, 2016. 292   \n systems [19]. However, in our paper the focus will not be on social listening, but rather on the complementary situ-ation in which the listener consumes music on their own, in relation to achieving a personal goal. In our consideration of a technological role of music, we go beyond 'self-care', and describe music as a tool that a listener may use to achieve the internal state necessary to accomplish their goal. We hypothesize that this connects to the concept of flow [24]: a desirable internal state that has been characterized by complete and total attention, a loss of a sense of self, a loss of a sense of the passing of time, and the experience that conducting the activity is, in and of itself, intrinsically rewarding. In other words, a listener in flow state is enjoying the feeling of being ab-sorbed in their task to such a degree that the passing of time is not noticed, and is therefore able to push past ob-stacles to carry out activities and achieve goals. In later sections, we will elaborate on theories regarding the pos-sible neurophysiological nature of flow states, the effects of music on the brain, and how it is that music may assist in achieving these internal states. As an initial indication of the growing importance of music that allows users to accomplish goals, we point to the growing number of art-ists1 and services2 on the Internet that are providing music to help people focus. The idea of music as technology should not be considered a paradigm shift, but rather as the explicit identification of a common phenomenon. This phenomenon has thus far escaped the attention of the MIR community because the focus of music information retrieval research has been firmly set on what music is, rather than on what music does. However, there are many examples of work that illustrates the breadth of areas in which music is used as a tool to accomplish an end. Most widely known is perhaps the use of music as a meaning-creating element in story-telling, especially in film and video, e.g., [35]. Currently expanding is the use of music in branding, e.g., within the rise of the concept of corporate audio identity [2]. Less comfortable to contemplate is the use of music for torture e.g., as studied by Cusick [7]. Finally, we mention the therapeutic uses of music, as covered recently by Koelsch [17]. Our work differs in a subtle, but important way from the-se examples. We look at music as technology from the point of view of listeners who make a conscious decision to expose themselves to the experience of music to alter their internal state in order to achieve a goal that they have set for themselves. Later, we will return to the im-portance of listener control over the choice of music for the effectiveness of music as a tool. Music as technology has serious implications for music information retrieval. If listeners may choose to use mu-sic as a psychological tool, then it is important for music search engines and recommender systems to be sensitive to the exact nature of the task that users wish to accom-plish. It also is important for researchers to judge the suc-                                                1 e.g., Delta Notch, https://www.youtube.com/user/DDRfrosh1 2 e.g., Focus at Will https://www.focusatwill.com cess of these systems in terms of their ability to support users towards accomplishing tasks. To understand music as technology more profoundly and fundamentally, collaborations between MIR and the neu-ro-, cognitive, and social psychological sciences, will be essential. Joint research lines involving collaborations between these fields will allow for the potential to deter-mine when and how flow states occur, if they vary in any way based on context, and how exactly these states are aided by music. In summary, this implies two places in which the MIR community should be active: i) learning and understand-ing what users need to put themselves into a flow state, and how this depends on what they are doing and on the surrounding circumstances, and ii) understanding how new music search engines and recommender systems can be designed to allow listeners to achieve flow states. In the remainder of this paper, we first will review how music is used as part of daily life. After this, we consider the effects of music on the brain, subsequently connect-ing to insights in relation to achieving flow state. Based on our proposed viewpoint and the reviewed literature, we discuss how the MIR research agenda can be broad-ened in this light, and finish with a Call to Action for in-terdisciplinary work worth investigating. 2. LISTENERS USING MUSIC 2.1 Music as part of daily life In the everyday life of the modern human, music has be-come a constant accompaniment to all manner of daily activities [27, 29, 34]. The advent of portable music de-vices capable of housing vast collections, the ubiquity of available musical data via streaming services, and the de-velopment of technology that allowed for greater ease of music production, have all lead to the consumption of music on an increasingly individual basis across an in-creasingly broad range of activities and contexts [10]. Music listening is a common occurrence in everyday life, yet rarely the sole focus of an activity. A number of stud-ies have pointed to this conclusion, and we mention some key examples here. In an experience sampling study where participants completed brief surveys at random in-tervals throughout their day, 44% of the surveys were completed while music listening had taken place within any 2-hour period, yet less than 2% of episodes involved listening to music as a main activity [32]. A later study showed that 38.6% of text messages sent to participants randomly throughout the day occurred during music lis-tening occasions; on occasions where the participants were not listening to music, 48.6% indicated that they had listened to music since the last text message, yet only 11.6% of these episodes occurred when music listening was the main activity [27]. A more recent survey study has shown similar results, with respondents indicating a mean of less than 1 hour of active music listening per day, yet 2-4 hours of passive music listening [15].  Along with an increase in music consumption accompa-nying other activities is the emergence of the belief that individual music selections function as a means to Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 293   \n achieve various emotional, motivational, or cognitive ef-fects to the benefit of accomplishing various activities [27]. Individuals will report that music is expected to per-form different functions based on different situations [26], an awareness of the specific songs expected to ful-fill these functions, as well as the expected psychological benefits from listening [8]. As such, people have come to use music as a piece of technology in their daily lives, effectively attempting to outsource various psychological tasks to specific song selections. We now go on to dis-cuss the factors that contribute to listeners successfully using music to achieve internal states that may be de-scribed as flow, which, in turn, support activities or goals. 2.2 Choosing music for a purpose As mentioned above, our perspective on music as tech-nology regards music as a tool in the hands of listeners themselves. In this section, we examine in more detail the importance of listener control of music. The perceived benefits of music listening have been shown to be more positive when the individuals had the ability to choose the desired music [27]. Participants indicate preferring playlists they created rather than automatically curated content [15], and those who chose the music they were listening to reported enjoying it more [27]. Furthermore, with greater control on the choice of music selection, in-dividuals reported experiencing greater changes in mood along three bipolar factors: 1) positivity, 2) present mind-edness, and 3) arousal [34]. Listeners' preference for control is consistent with the idea of music being a means to an end. A number of stud-ies have shown that listeners use music as psychological tool to optimize emotion, mood, and arousal based on the very specific needs of a given situation and/or activity [8, 27, 34]. Interviews have shown that individuals have an awareness of specific songs they feel will assist in ac-complishing various emotional tasks, such as decreasing or increasing their arousal, motivating them to take ac-tion, adjusting their moods, or assisting them to focus [8]. Reasons for listening to music have also been shown to vary by activity (e.g., doing housework, travelling, study-ing, dating, getting dressed to go out etc.) [8, 15, 29]. Along with the constant growth of the music corpus, a means to organize, retrieve and discover appropriate mu-sic selections is a growing challenge. Despite the preva-lence of current playlist curation technologies, individu-als report self-generated playlists to be the organizational method of choice [8, 15], an indication of the specificity of song selection requirements, above and beyond the specificity of individual preference. In the final section of the paper, we will return to discuss how, in order to use music as technology, users must have at their disposal appropriate music information retrieval technology. Next we turn to the neuroscience perspective on music as tech-nology. 3. MUSIC AND THE BRAIN Research in the field of music and emotion suggests there are multiple means for music to affect the individu-al, and that underlying physiological and neurological mechanisms should be researched [14]. We highlight two posited mechanisms relevant to our discussion: a) brain stem reflexes, and b) musical expectancy.  The degree and manner in which each mechanism results in a physiological or neurological response, and by ex-tension arousal, may be key in understanding why listen-ers select specific songs given the tasks they have set out to accomplish. As the demands of each situation vary, the effect of acoustic stimuli on the brain of the listener may function to moderate arousal such that an optimal internal state is reached. In other words, listeners may be selecting songs, and by extension sequences of acoustic stimuli, to alter their internal state in order to best meet the needs of their situation.  3.1 Brain stem responses The brain stem is believed to be a very old part of the brain, and has been shown to be sensitive to loud, low frequency, dissonant, suddenly changing sounds [5, 9, 22]. It is posited that sounds indicative of a sudden change, a strong force, or something of large size may coincide with an event that requires immediate, urgent and reflexive attention. These acoustic qualities shift at-tention to the stimulus, giving rise to muscular and car-diovascular responses as well; a by-product of this may be the reason bass drum sounds inspire people to dance in sync with the music, and why music with faster tem-pos is more arousing (see [14] and [17]). Furthermore, a greater number of brain regions have shown activation at the onset of musical samples as opposed to the middle or end of these samples [23].  As such, music that contains such acoustical stimuli, or dramatic changes in its acoustic features (e.g., dramatic build ups and “drops”), may shift attention to the music arousing the listener in the process. Conversely, music that is relatively constant may instead serve to 'drown out' distracting ambient sounds instead: for example, the difference between silence and the rustling of papers is far greater than the difference between the rustling of papers and background music. As such, music may pro-vide a constant acoustic backdrop thereby reducing the amount of arousal and attentional shifts caused by dis-tracting sounds in the listener’s environment.  3.2 Musical expectancy Recently, an increasing amount of attention has been de-voted to expectancy as it relates to music (e.g., as in Hu-ron's recent work [11]). The ability of the human brain to predict events is thought to have been vital to survival, and thus plays a prominent role in all cognition. As such, meeting or violating expectations in music should result in physiological and neurological effects (see [30] and [31]). Given that music is essentially an organized pattern of sounds, our brains generate predictions as the music unfolds over time based on our knowledge of the specific musical piece, but also our knowledge of all music [31].  As only so much information may be encoded at a time, the more complex the piece, the greater the number of potential prediction errors, the more exposure is required to become familiar [31]. In fact, as far back as Berlyne's [3] studies, it has been shown that familiarity of a particu-lar sequence of notes in relation to a corpus results in less physiological arousal than unfamiliar sequences of notes, 294 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016   \n as does simplicity in the melody as opposed to complexi-ty. These expectations may be used deliberately by com-posers of music to create a sense of musical tension, only to resolve the tension later on in the piece, resulting in relaxation and pleasure [18]. In addition, familiarity of a piece may lead to anticipation of the pleasure to be expe-rienced at peak moments in the music, resulting in the activation of midbrain dopamine neurons causing atten-tion to be paid to potential upcoming rewards [31].   Relevant to our topic, such arousal may divert attention from the task to the music [e.g., 13]. On one hand, music that adheres to expectations, such as a collection of very familiar pieces, may result in less overall arousal than pieces that are unfamiliar, very complex, or of an unfa-miliar genre. On the other hand, familiar pieces that result in pleasure and anticipation may also be arousing, divert-ing attention from the task to the music as well.  4. MUSIC AND FLOW Flow is characterized as a mental state in which one’s complete attention is focused on a task, one has lost sense of self and of time, and one’s perception of the experi-ence is positive and rewarding [24]. In this research tradi-tion, the definition of flow also includes a sense that one's subjective level of skill is balanced with the subjective challenge of the activity: a too-simple task evokes relaxa-tion then boredom which in turn causes attention to drift, and a too-challenging task evokes vigilance then anxiety [24]. As with music use in everyday life, the concept of flow is also intertwined with context and activity.   More recently it has been theorized that flow states may emerge during media enjoyment, resulting in neural states where attentional and reward centers in the brain are acti-vated synchronously [40]. Weber and colleagues [40] drew a theoretical link between engagement in linear me-dia (e.g., books, films and video games) and flow states. They posit that linear media require mastery of mental models: video games require a level of skill that increases as one progresses, and films require an understanding of the characters and the narrative. It is suggested that these contribute the challenge, which in addition to pleasurable engagement, coincides with activations of the brain re-gions necessary to achieve flow. While music is not spe-cifically discussed, it is a medium that can be consumed during various activities, and may function in conjunction with these activities to inspire flow states.  The dopaminergic pathway, which is involved in the ex-perience of pleasure, is posited to be active during flow states [40], and has been shown to be active during expe-riences of pleasure while listening to music [31]. Of in-terest in this pathway is the nucleus accumbens, which is also thought to be involved in automatic consummatory behavior (e.g., drinking or eating), and the striatum which also has connections to the brain stem [40]: both also been observed in pleasurable responses to music [31]. In addition, regions thought to be involved in re-ward-seeking behaviors, such as the prefrontal and or-bitofrontal cortices have also been implied in both [31] [40].   While it is not yet clear how specifically music and con-text may interact to produce a flow state, enough evi-dence has been accrued for us to suggest two aspects worthy of study. Firstly, during tasks in which boredom is likely, more arousing music may be selected to induce a flow state: by diverting attentional resources to the mu-sic the challenge of the task increases, as it now requires attention to be paid to both the activity and the music. As such, music that is more likely to be arousing either by a) resulting in responses from the brain stem (e.g., loud, frequently changing, or dissonant song selections) or b) causing prediction errors (e.g., less familiar, familiar and causing anticipation, or more complex) may be more suitable. Secondly, during tasks that are challenging or otherwise cognitively engaging (e.g., studying or read-ing) music that is likely to be less arousing either by a) resulting in less brain stem activation (e.g., relatively un-changing or consonant) or b) being predictable without anticipation (e.g., somewhat familiar and somewhat liked, more simple songs) may be more suitable.  5. NEW CHALLENGES FOR MIR We now turn back to discuss how music as technology connects with MIR. The ability of listeners to successful-ly use music as technology depends on the effectiveness of music information retrieval and recommender systems in supporting them. We argue for the necessity of multi-disciplinary research that brings together neuro-, cogni-tive, and social psychologists, and music information re-trieval researchers. Such collaboration will allow us to understand what makes music helpful for users and what makes it appropriate for different tasks. In this section, we point to several areas in which the music information retrieval is on the right track, and several areas in which more effort is needed if users are to truly benefit from music as technology. First, we return to the relation between the user choosing music, and music being perceived as having positive ben-efits. Taking this connection seriously means taking the position that for music to be used effectively as technolo-gy, it must truly be a tool in the users’ hands (i.e., fully under the control of the user). Other work that points out the critical role of user control over music selection in-cludes [38], who observe that the context and the inten-tions of the user impact which music features are im-portant. Their music selection interface provides users with control over factors such as tempo, mood, and genre, and their experiments show that users prefer this control. The findings are not surprising given the role of control in the success of recommender systems from the user point of view [28]. In order to make music a useful tool, MIR must start with the choice of the listener to change their internal state in order to accomplish a goal. The choice may be semi-conscious, or may simply consist of going to a place where certain music is playing, or ac-cepting to stay in that place. Listeners who are unwilling or who are not themselves in control are not using music as technology. In other words, piping in focus music dur-ing an exam can be predicted not to improve students' ability to concentrate. MIR systems can make music use-ful as technology by providing results and recommenda-Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 295   \n tions that are transparent. The importance of transparency for recommender systems has long been recognized [33]. They should also minimize the effort needed from the us-er to provide feedback. Second, serving listeners who want to use music as a tool requires extending today's context-aware recommender systems, which are described, for example, in [32]. Par-ticularly promising is the development of systems to rec-ommend music for activities, e.g., [39]. In [25] the au-thors propose a context-aware music recommendation system the monitors heart rate and activity level, and rec-ommends music that helps the user achieve a desired heart rate intensity. The challenge of such activity-based recommenders is to provide music that serves the com-mon needs of people engaging in an activity, while taking personal taste into account. One aspect of using music as technology is blocking out background noise. Context-aware recommenders will need to develop to be sensitive to the acoustic environment, so that they can recommend music that will mask it. A challenge that has yet to be faced is moving music rec-ommendation and retrieval away from music that listen-ers \"like\" the first time that they hear it, towards music that allows them to meet their goals. Currently, the ground truth that is used to evaluate the success of rec-ommender systems does not differentiate “love at first listen” from an appreciation that a listener develops over a longer period of time on the basis of utility given the context and activity. We suggest that collaboration between MIR and psychol-ogy may be appropriate to best determine not only how music can better be organized to suit different tasks, but also which specific features make certain music helpful, or make one selection more suitable for a given activity than another.  Recent years have seen progress in content-based and hy-brid music recommender systems [32]. These systems make use of timbral features (e.g., MFCCs), features re-lated to the temporal domain, such as rhythmic proper-ties, and tonal features such as pitch-based features. Our discussion revealed the importance of content features that might point to a sudden, unexpected event in the mu-sic that would shift the listener’s attention. We point out that recent approaches to exploiting music content may only use very short segments of the music, such as the deep learning approach in [37]. A future challenge is to determine how long a window must be considered in or-der to determine whether the song contains features that disrupt focus. Here again, task specific as well as user-specific aspects are important.  Further, the role of familiarity is critical. The importance of music freshness is well recognized. For example, Hu and Ogihara [12] relate it to a memory model. However, playing the same familiar music repeatedly does not pro-mote focus if the user's sense of anticipation becomes too strong. With the vast amounts of music currently availa-ble online, the possibility is open to creating a music rec-ommendation system that never repeats itself. When music is used as technology, it is important to keep in mind that it is the stream and not the individual song that is important. Currently, an increasing amount of work is carried out in the area of playlist recommendation [4]. Whereas many playlists are played on shuffle, playlists that most effectively allow the user to achieve internal state transformation may have a particular order, calling for more work on the generation of ordered streams of content items. Finally, we anticipate that when listeners use music as technology they will want the possibility to query the sys-tem, instead of relying on a recommendation. Such que-ries, even though context-based, may not be well fitted to the goal that they want to accomplish. Here, it is neces-sary to understand the type of language that users use to express the complexity of their task. To this end, the MIR community should further foster insights in information seeking and user studies. However, an important differ-ence with the existing paradigms under which these stud-ies are conducted (e.g., [6, 20]) is that under the ‘music as technology’ paradigm, a query would be expressed in the form of a (non-musical) task to be accomplished, rather than a directed query to an explicit song (e.g., similarly to what was done in [21] on music and narrative). 6. CALL TO ACTION In this work, we pointed out the notion of music as tech-nology, which we feel currently is overlooked in MIR solutions. Connecting this concept to existing literature from the psychological sciences, it is clear that pursuing a joint research roadmap will be beneficial in both gaining fundamental insights into processes and internal states of listeners, and finding ways to improve music search en-gines and recommender systems. To concretize this fur-ther, we conclude this paper with a Call to Action, formu-lating interdisciplinary research directions, which will be beneficial for realizing the full potential of music as tech-nology.  First, research should contribute to a better understanding of flow states. The evidence brought together in this pa-per points to the conclusion that flow is a desirable over-arching internal state, and is the target state underlying a wide range of activities. We further argued that listeners choose music that complements an activity to result in a net optimal level of cognitive engagement. Under this view, music is not an end unto itself, but rather an inex-tricable part of the activity. More research is needed to validate flow as an overarching mental state in practice, as well as its antecedents. In addition, how music leads to and moderates flow state should be investigated. Second, on the basis of a deeper understanding of flow, research should work to define new relevance criteria for music. Such work will involve understanding which kinds of music fit which kinds of tasks, zeroing in on the relevant characteristics of the music. We expect this to be a formidable challenge, since it must cover perceptual, cognitive, and social aspects of music. The contribution of users’ personal music experiences and music tastes must also be understood. On the one hand, we anticipate a certain universal character in the type of music that will allow a person to achieve flow state for a given activity. On the other hand, we anticipate that a ‘one size fits all’ solution will not be optimal, and that relevance criteria 296 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016   \n must also be flexible enough to capture individuals’ needs and preferences. Third, once we have defined relevance criteria, we should move from there to identify new features, new algo-rithms, and new system designs. We anticipate that fea-tures reflecting music complexity and unexpectedness will be important, as a few relatively isolated disruptive moments can potentially make an entire song unsuitable for an activity. This observation points to the need to consider the song as a whole, implying, in turn, new MIR algorithms. New system designs will be needed to help guide users’ music choice without effort, and ideally without interrupting their flow state. System designs will need to take into account that users may not recognize the music that will make them most productive the first time they hear it.  Further, even after listeners recognize the connection between certain music and their own produc-tivity levels, they might not be able to express their music needs explicitly in music-technical terms. Systems must be able to accommodate the types of information and feedback that users are able to provide about the kind of music that will be most effective for them. Finally, once new applications have been developed and deployed, they will provide an extremely valuable source of information about when listeners use music, allowing neuroscientists and psychologists to refine their theories of flow and how listeners achieve it in certain situations, against the backdrop of scalable and real-world use cases. Our suggestion for MIR and the (neuro)psychological sciences to connect is not new; for example, it also was reflected upon in [1], and recently further interconnection possibilities between the disciplines were suggested in [16]. Both of these works rightfully point out that such collaborations are not trivial, particularly because of methodological differences and mismatches. However, we believe that the currently described possibilities offer fruitful research questions for all disciplines. Ultimately, understanding music as technology has the potential to profoundly impact not only the MIR domain, but the whole ecosystem of music production, delivery and consumption. Currently, the success of music is judged by the number of downloads or the number of lis-tens. The idea of music as technology opens up the possi-bility of evaluating the success of music also in terms of the goals that are achieved by listeners. Besides considering music as technology, we believe that we also should continue to study and enjoy music for its own sake. However, the potential of music to help listen-ers achieve their ends opens the way for creative new us-es of music, with respect to commercial business models, as well as promoting the well-being of listeners. We hope that ultimately, music as technology will support listeners in coming to a new understanding on how they can use music to reach their goals and improve their lives.  Acknowledgments: The research leading to these results was performed in the CrowdRec and the PHENICX pro-jects, which have received funding from the European Commission’s 7th Framework Program under grant agreement no. 610594 (CrowdRec) and no. 601166 (PHENICX). 7. REFERENCES [1] Aucouturier, J., and Emmanuel B. \"Seven problems that keep MIR from attracting the interest of cognition and neuroscience,\" JIIS, 41.3, 483-497. 2013. [2] Bartholmé, R. H.  & Melewar, T.C.: “The end of silence? Qualitative findings on corporate auditory identity from the UK,” Journal of Marketing Communications, 29 Oct 2014, pp.1-18. 2014.. [3] Berlyne, D. E.: Aesthetics and psychobiology, New York: Appleton-Century-Crofts, Vol. 336, 1971. [4] Bonnin, G. & Jannach, D.: “Automated Generation of Music Playlists: Survey and Experiments,” ACM Comput. Surv. 47, 2, Article 26, 35 pages, 2014. [5] Burt, J. L., Bartolime, D. S., Burdette, D. W., & Comstock, J. R: “A psychophysiological evaluation of the perceived urgency of auditory warning signals,” Ergonomics, 38(11), 2327–2340, 1995. [6] Cunningham, S. J. & Bainbridge, D: “A search engine log analysis of music-related web searching,” In N.T. Nguyen et al. (Eds.), Advances in Intelligent Information and Database Systems: Studies in Computational Intelligence, Vol. 283, pp. 79-88, 2010. [7] Cusick, S.: “You are in a place that is out of the world: Music in the Detention Camps of the 'Global War on Terror,'” JSAM, Vol. 2/1, pp. 1-26, 2008. [8] DeNora, T.: “Music as a technology of the self.” Poetics, 27(1), 31–56, 1999. [9] Foss, J. A., Ison, J. R., Torre, J. P., & Wansack, S.: “The acoustic startle response and disruption of aiming: I. Effect of stimulus repetition, intensity, and intensity changes,” Human Factors, 31(3), 307–318. 1989. [10] Hargreaves, D. J., & North, A. C.: “The Functions of Music in Everyday Life: Redefining the Social in Music Psychology,” Psychology of Music, 27(1), 71–83, 1999. [11] Huron, D.: “Sweet Anticipation: Music and the Psychology of Expectation,” Music Perception, 24(5), 511–514, 2007. [12] Hu, Y. and Ogihara, M.: “NextOne Player: A music recommendation system based on user behavior,” 12th Int. Society for Music Information Retrieval Conference (ISMIR’11), pp. 103-108, 2011. [13] Jung, H., Sontag, S., Park, Y. S., & Loui, P.: “Rhythmic effects of syntax processing in music and language,” Frontiers in Psychology, 6(NOV), pp. 1–11, 2015. [14] Juslin, P. N., & Västfjäll, D.: “Emotional responses to music: The need to consider underlying mechanisms,” BBS, 31(06), 751, 2008. [15] Kamalzadeh, M., Baur, D., & Möller, T.: “A Survey on Music Listening and Management Behaviours,” Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 297   \n 13th Int. Society for Music Information Retrieval Conference (ISMIR’12), pp. 373–378, 2012. [16] Kaneshiro, B., and J. P. Dmochowski. \"Neuroimaging methods for music information retrieval: Current findings and future prospects,\" 16th Int. Society for Music Information Retrieval Conference (ISMIR’15), pp. 538–544, 2015. [17] Koelsch, S.: “Brain correlates of music-evoked emotions,” Nature Reviews Neuroscience, 15(3), 170–180, 2014. [18] Koelsch, S.: “Music-evoked emotions: principles, brain correlates, and implications for therapy.” Annals of the New York Academy of Sciences, 1337(1), 193–201, 2015. [19] Laplante, A. Improving music recommender systems: “What can we learn from research on music tastes?” 15th Int. Society for Music Information Retrieval Conference (ISMIR ‘14), pp. 451-456, 2014. [20] Lee, J. H.: “Analysis of user needs and information features in natural language queries seeking music information.” Journal of the Association for Information Science and Technology (JASIST), 61(5), 1532-2890, 2010. [21] Liem, C.  C.  S., Larson, M. A., & Hanjalic A.: “When Music Makes a Scene-Characterizing Music in Multimedia Contexts via User Scene Descriptions,” Int. Journal of Multimedia Information Retrieval, 2, pp.15-30, 2013. [22] Masataka, N., & Perlovsky, L.: “Cognitive interference can be mitigated by consonant music and facilitated by dissonant music,” Scientific Reports, 3, 2028, 2013. [23] Mueller, K., Fritz, T., Mildner, T., Richter, M., Schulze, K., Lepsien, J., Möller, H. E.: “Investigating the dynamics of the brain response to music: A central role of the ventral striatum/nucleus accumbens.” NeuroImage, 116, 68–79, 2015. [24] Nakamura, J., & Csikszentmihalyi, M.: “The Concept of Flow,” In J. S. Snyder & S. J. Lopez (Eds.), Flow and the Foundations of Positive Psychology, pp. 239–263. New York: Oxford University Press, 2014. [25] Nirjon, S., Dickerson, R. F., Li, Q., Asare, P., Stankovic, J. A., Hong, D., Zhang, B., Jiang, X., Shen, G., and Zhao, F.: “MusicalHeart: a hearty way of listening to music,” 10th ACM Conference on Embedded Network Sensor Systems (SenSys '12). ACM, New York, NY, USA, 43-56, 2012.  [26] North, A. C., & Hargreaves, D. J.: “Situational influences on reported musical preference.” Psychomusicology: A Journal of Research in Music Cognition, 15(1-2), 30–45, 1996. [27] North, A. C., Hargreaves, D. J., & Hargreaves, J. J.: “Uses of Music in Everyday Life,” Music Perception: An Interdisciplinary Journal, 22(1), 41–77, 2004. [28] Pu, P., Chen, L., and Hu, R.: “A user-centric evaluation framework for recommender systems.” Proceedings of the 5th ACM conference on Recommender systems (RecSys '11), pp. 157-164, 2011. [29] Rentfrow, P. J., & Gosling, S. D.: “The do re mi’s of everyday life: The structure and personality correlates of music preferences,” JPSP, 84(6), 1236–1256, 2003. [30] Rohrmeier, M. A., & Koelsch, S.: “Predictive information processing in music cognition: A critical review.” Int. Journal of Psychophysiology, 83(2), 164–175, 2012. [31] Salimpoor, V. N., Zald, D. H., Zatorre, R. J., Dagher, A., & McIntosh, A. R.: “Predictions and the brain: How musical sounds become rewarding,” Trends in Cognitive Sciences, 19(2), 86–91, 2015. [32] Schedl, M., Knees, P., McFee, B., Bogdanov, D., & Kaminskas, M.: “Music Recommender Systems,” In F. Ricci, L. Rokach, B. Shapira, (eds.) Recommender Systems Handbook (2nd ed.), pp. 453-492. Springer US., 2015. [33] Sinha, R. and Swearingen, K.: “The role of transparency in recommender systems,” CHI '02 Extended Abstracts on Human Factors in Computing Systems (CHI EA '02). ACM, New York, NY, USA, 830-831, 2002. [34] Sloboda, J. A, O’Neill, S. A. & Ivaldi, A.: “Functions of music in everyday life: an exploratory study using the experience sampling method,” Musicae Scientiae, 5(1), 9–32, 2001. [35] Tagg, P. and Clarida, B.: “Ten Little Title Tunes: Towards a Musicology of the Mass Media,” The Mass Media Scholar's Press, New York, USA and Montreal, Canada, 2001. [36] Uitdenbogerd, A. and van Schnydel, R.: “A review of factors affecting music recommender success,” 3rd Int. Conference on Music Information Retrieval (ISMIR ’02), Paris, France, 2002. [37] A. van den Oord, S. Dieleman, and B. Schrauwen: “Deep content-based music recommendation,” in NIPS, 2013. [38] Vignoli, P. and Pauws, S.: “A Music Retrieval System Based on User-Driven Similarity and its Evaluation,” 6th Int. Conference on Music Information Retrieval (ISMIR ‘05), pp. 272-279, 2005. [39] Wang, X., Rosenblum, D. and Wang, Y.: “Context-aware mobile music recommendation for daily activities,” 20th ACM Int. Conference on Multimedia (MM '12). ACM, New York, NY, USA, 99-108, 2012.  [40] Weber, R., Tamborini, R., Westcott-Baker, A., & Kantor, B.: “Theorizing flow and media enjoyment as cognitive synchronization of attentional and reward networks,” Communication Theory, 19(4), 397–422, 2009.   298 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "A Hybrid Gaussian-HMM-Deep Learning Approach for Automatic Chord Estimation with Very Large Vocabulary.",
        "author": [
            "Jun-qi Deng",
            "Yu-Kwong Kwok"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1415718",
        "url": "https://doi.org/10.5281/zenodo.1415718",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/058_Paper.pdf",
        "abstract": "We propose a hybrid Gaussian-HMM-Deep-Learning ap- proach for automatic chord estimation with very large chord vocabulary. The Gaussian-HMM part is similar to Chordino, which is used as a segmentation engine to divide input audio into note spectrogram segments. Two types of deep learning models are proposed to classify these seg- ments into chord labels, which are then connected as chord sequences. Two sets of evaluations are conducted with two large chord vocabularies. The first evaluation is conducted in a recent MIREX standard way. Results show that our approach has obvious advantage over the state-of-the-art large-vocabulary-with-inversions supportable ACE system in terms of large vocabularies, although is outperformed by in small vocabularies. Through analyzing and deduc- ing system behaviors behind the results, we see interesting chord confusion patterns made by different systems, which conceivably point to a demand of more balanced and con- sistent annotated datasets for training and testing. The sec- ond evaluation preliminarily demonstrates our approach’s superiority on a jazz chord vocabulary with 36 chord types, compared with a Chordino-like Gaussian-HMM baseline system with augmented vocabulary capacity.",
        "zenodo_id": 1415718,
        "dblp_key": "conf/ismir/DengK16",
        "content": "A HYBRID GAUSSIAN-HMM-DEEP-LEARNING APPROACH FOR\nAUTOMATIC CHORD ESTIMATION WITH VERY LARGE VOCABULARY\nJunqi Deng and Yu-Kwong Kwok\nDepartment of Electrical and Electronic Engineering\nThe University of Hong Kong\n{jqdeng,ykwok}@eee.hku.hk\nABSTRACT\nWe propose a hybrid Gaussian-HMM-Deep-Learning ap-\nproach for automatic chord estimation with very large\nchord vocabulary. The Gaussian-HMM part is similar to\nChordino, which is used as a segmentation engine to divide\ninput audio into note spectrogram segments. Two types of\ndeep learning models are proposed to classify these seg-\nments into chord labels, which are then connected as chord\nsequences. Two sets of evaluations are conducted with two\nlarge chord vocabularies. The ﬁrst evaluation is conducted\nin a recent MIREX standard way. Results show that our\napproach has obvious advantage over the state-of-the-art\nlarge-vocabulary-with-inversions supportable ACE system\nin terms of large vocabularies, although is outperformed\nby in small vocabularies. Through analyzing and deduc-\ning system behaviors behind the results, we see interesting\nchord confusion patterns made by different systems, which\nconceivably point to a demand of more balanced and con-\nsistent annotated datasets for training and testing. The sec-\nond evaluation preliminarily demonstrates our approach’s\nsuperiority on a jazz chord vocabulary with 36 chord types,\ncompared with a Chordino-like Gaussian-HMM baseline\nsystem with augmented vocabulary capacity.\n1. INTRODUCTION\nAutomatic chord estimation (ACE) is currently undergoing\na paradigm shift from Gaussian-HMM (Hidden Markov\nModel) approaches to deep learning approaches. Recently,\nthere have been quite a few deep learning powered ACE\napproaches in the ﬁeld, including a convolutional neu-\nral network (CNN) approach [10], a hybrid feedforward-\nrecurrent neural network (DNN-RNN) approach [3], a\ndeep belief network (DBN) approach [19], and a hybrid\nDBN-RNN approach [16]. Some are more purely deep\nlearning oriented, which only apply minimal amount of\nfeature extractions, while others consider combination of\ntraditional signal processing techniques and deep learning.\nc/circlecopyrtJunqi Deng and Yu-Kwong Kwok. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Junqi Deng and Yu-Kwong Kwok. “A Hybrid Gaussian-\nHMM-Deep-Learning Approach For Automatic Chord Estimation With\nVery Large V ocabulary”, 17th International Society for Music Informa-\ntion Retrieval Conference, 2016.One common point of these approaches is that they\nare all evaluated under major/minor vocabulary (MajMin),\nwhich is far from reﬂecting the complexity of chord vocab-\nulary in pop/rock music practice. In 2013, MIREX ACE\nhas introduced a new evaluation scheme [14] focusing on\nmuch more complicated chord vocabulary, the “Sevenths-\nBass”, which includes MajMin, three types of their sev-\nenth chords, and all of their inversions. The SeventhsBass,\nalthough also omitting some rare chords in pop/rock prac-\ntice, is much closer to the reality compared with MajMin.\nIt differentiates among triads, sevenths and their inversions\nbecause they all have different harmonic qualities. It is\nnot only important for ACE systems to be evaluated on\nmore complex chord vocabulary, but also to actually sup-\nport that vocabulary. Unfortunately from 2013 to 2015,\nthere have been only two systems that actually support\nSeventhsBass [6], others mostly do not even support chord\ninversions. Not being able to generate inversions is musi-\ncally problematic since in some musical context they have\nvery different harmonic qualities from their root positions.\nAs shown in Figure 1, for example, the chord inversions\nserve as a diatonic or chromatic continuations of the bass\nline. If some of these are replaced by their root positions,\nthe continuations are broken and thus the pieces will sound\nvery different.\n1)| G | D/F# | F | C/E | Cm/ Eb| \n2)| A | Bm| A/C# | D | \n3)| C | G/B | Am | Am/G | F | C/E | \n4)| C | F | C/E | D/F# | E/G# | F#/A# | Bm7 | C# |\nFigure 1 . Four chord progressions that contain bass line\ncontinuations which demand chord inversions. Progres-\nsions like 1,2 and 3 are very popular among pop/rock. Pro-\ngression 4 induces a key shift from C major to F# minor.\nFollowing the above argument, we propose an ACE\nsystem that not only supports but also be evaluated on\nSeventhsBass. This system uses a Chordino-like module\n[13] as a chord segmentation engine, and classiﬁes chords\nwithin each segment using a deep learning model. Evalua-\ntion results show that the best system variants have obvious\nadvantage over the state-of-the-art SeventhsBass support-\nable ACE system in terms of Sevenths (MajMin + maj7,\nmin7, 7) and SeventhsBass.812Besides, we also try the proposed approach on a jazz\nvocabulary. The comparison target remains the same ex-\ncept for an augmentation of its chord vocabulary capacity.\nSince the standard evaluation tool [14] does not apply for\nthis vocabulary, evaluation is done manually via compari-\nson of weighted chord symbol recall1. Results show sim-\nilar ranking as in the SeventhsBass’ results, and still the\nbest system signiﬁcantly outscores the baseline approach.\nThe rest of this paper is organized as follows: Section\n2 gives an overview of the proposed ACE system frame-\nwork and its workﬂow; Section 3 elaborates the imple-\nmentations of two deep learning based models (DBN and\nBLSTM-RNN); Section 4 reports both SeventhsBass and\njazz vocabulary evaluation results, with a detailed discus-\nsion of chord confusion and how they affect systems’ per-\nformances; Section 5 concludes the paper and puts forward\nsome possible future considerations in ACE.\n2. SYSTEM OVERVIEW\nThe proposed ACE approach2has a simple workﬂow as\nshown in Figure 2. The test data goes through a Chordino-\nlike module for segmentation. Then each note spectrogram\n(referred to as “notegram” below) segment will be classi-\nﬁed using a deep learning model. The output chord se-\nquence is obtained by connecting the classiﬁed labels.\ntest data\nChordino -likesegmentsChord Classifiertraining data\nchord labels\nFigure 2 . System overview. The audio input (test data)\ngoes through a Chordino-like process for segmentation,\nthen the segments are classiﬁed into chord labels.\nThe Chordino-like module is implemented according to\nthe algorithmic description of Chordino [12, 13]. The au-\ndio input is ﬁrst resampled at 11025 Hz, and transformed\nby a 4096-point Hamming window short-time-Fourier-\ntransform (STFT) with 512 point hop size. The linear-scale\nspectrogram is then mapped to a log-scale spectrogram, or\nnotegram. After standard tuning (tuned notegram) and fea-\nture scaling, note activation patterns are extracted from the\nnotegram via non-negative-least-square (NNLS) method.\nA piece of chromagram is derived by bass-treble proﬁling\nof the note activation patterns. The chromagram is then de-\ncoded and segmented by a Gaussian-HMM with very high\nself-transition weights.\nThe chord classiﬁer is implemented using deep learning\nmodels, which will be discussed in the following section.\nApplying different deep learning models leads to different\nsystem variants out of the proposed framework. In the fol-\n1http://www.music-ir.org/mirex/wiki/2013:Audio Chord Estimation\n2the full implementation of this ACE system is accessible via:\nhttps://github.com/tangkk/tangkk-mirex-acelowing, we refer to these “variants” as “systems”, and the\nframework as the “approach”.\n3. DEEP LEARNING MODELS\nWe consider two types of deep learning models. They both\nhave input at the tuned notegram level. The deep neu-\nral network will learn the rest of the transformations from\ntuned notegram all the way to chord label. Since there are\ndifferent numbers of frames in different chord segments, in\norder to use a ﬁxed-length input structure, we conducted a\npreliminary study and found that 6 sub-segments are good\nfor single chord classiﬁcation task. Note that the number of\nsub-segments should at least reﬂect the temporal order of\nbass line in order to differentiate root position from inver-\nsions. Thus we compute a 6-frame notegram for each seg-\nment as follows: at ﬁrst the segment is divided into 6 equal-\nsize sub-segments; if the total number of frames is not di-\nvisible by 6, the last frame is extended several times to\nmake it divisible; then notegram in each sub-segment is av-\neraged over time, resulting in one frame per sub-segment.\n3.1 DBN Model\nWe ﬁrst consider a DBN model. It contains two hidden\nlayers, each of 800 neurons. The input layer is of 6×252-\ndimension (252 is the size of a notegram frame), and the\noutput layer is a #chord-way softmax layer. The neurons\nof both input and output layers are of Gaussian type (real\nvalue from 0 to 1). The neurons in both hidden layers are\nof Bernoulli type (binary value 0 or 1).\nDuring unsupervised pre-training, the ﬁrst restricted-\nBoltzmann-machine (RBM) formed by the ﬁrst two layers\nis considered as a Gaussian-Bernoulli RBM, and the sec-\nond RBM formed by the two hidden layers is considered as\na Bernoulli-Bernoulli RBM. The pre-training is conducted\nusing persistent-contrastive-divergence-10 [17] (PCD-10),\nfor 100 epochs with learning rate 0.001. During super-\nvised ﬁne-tuning, the network connections are updated us-\ning mini-batch stochastic gradient descent, and the updates\nare regularized by dropout [8] (with 0.5 dropout probabil-\nity) and early-stopping. The stopping criteria is monitored\nby a validation set, which randomly contains 20% of the\ntraining set. The other 80% are used for computing the\ngradients. Due to the randomness of train/validation split,\nwe repeatedly train 6 models. The model with the best val-\nidation score will be saved for testing.\nFor comparison, we also consider a feed-forward mul-\ntilayer perceptron (MLP) model, whose network conﬁgu-\nration is the same as the DBN, but trained using only the\nﬁne-tuning procedure described above.\n3.2 BLSTM-RNN Model\nHistorically, long-short-term-memory (LSTM) [9] unit is\nintroduced to try to solve the gradient vanishing problem\n[2] when training a recurrent neural network with a long\nsequence of examples.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 8133.2.1 LSTM Unit\nInstead of having only one input port, an LSTM unit has\nfour input ports. As shown in Figure 3, three of them are\nused for gating purpose, and the other is used for normal\npurpose. Each gate computes an output gating signal from\nthe weighted sum of its inputs using a non-linear activation\nfunction. The gating signal computed by input gate, out-\nput gate and forget gate will interact with both the LSTM\nunit’s input value and the LSTM cell value through simple\nmultiplications, resulting in the LSTM unit’s output value.\nInput gate regulates the amount of input feeding into the\ncell; forget gate regulates the current cell value by the pre-\nvious cell value; and output gate regulates the amount of\noutput by interacting with the current cell value. Since all\nfunctions involved in an LSTM unit are differentiable or\npartially differentiable, all connections can be trained us-\ning the same back-propagation-through-time (BPTT) [7]\ntechnique as used in training a normal RNN.\nCell\nO I F\n…\n…L\nFigure 3 . LSTM unit. O = output gate; I = input gate; F =\nforget gate; Black dots indicate multiplication operations\n3.2.2 BLSTM-RNN\nWe then consider a BLSTM-RNN model [7] as shown in\nFigure 4. It has both forward and backward LSTM layers,\neach of which has 800 LSTM units. Before the #chord-way\nsoftmax output layer, it performs mean pooling to summa-\nrize results from all frames. During training, the RNN is\nalways unrolled to 6 frames, and the weights are updated\nvia BPTT using AdaDelta algorithm [18], regularized with\ndropout (with 0.5 dropout probability) and early-stopping,\nmonitored by a validation set chosen in the same way as\nin DBN’s case. Due to the randomness of train/validation\nsplit, we repeatedly train 6 models. The model with the\nbest validation score will be saved for testing.\n4. EVALUATION\nFor SeventhsBass ACE implementation, four datasets of\n266 tracks in total are used in training. They con-\ntain both eastern and western pop/rock songs. They\nare: 1, JayChou29 dataset [5]; 2, a Chinese pop song\ndataset (CNPop20)3; 3, Carole King + Queen dataset\n3containing 20 songs from both male and female singer-songwriters\nfrom Chinese cultural backgrounds including mainland China, Hong\nKong and Taiwan\ninput layer\n252 neurons /eachforward layer\n800LSTM units/ \neach#chord -way\nsoftmax\nbackward layer\n800LSTM units / \neach\nWifUf\nWibUb\n6 framesMean PoolingWoFigure 4 . Bidirectional-long-short-term-memory recur-\nrent neural network (BLSTM-RNN) used in the proposed\napproach\n(KingQueen26)4; 4, 191 songs from USPop dataset (U)5.\nIn order to see the effect of data size, all models will be in-\ncrementally trained on: 1, JayChou29 and CNPop20 (CJ);\n2, CJ + KingQueen26 (CJK); 3, all four datasets (CJKU).\nFor Jazz ACE implementation, 99 pieces of jazz chord\ncomping + soloing dataset extracted from a jazz guitar\nbook [15] (JazzGuitar99) are used as training/validation\ndataset, and 7 pieces from Gary Burton’s online course [1]\n(GaryBurton7) are used as test dataset. JazzGuitar99’s an-\nnotations are taken directly from the book, and GaryBur-\nton7’s annotations are taken from the leadsheets provided\nalong with the course. The jazz chord vocabulary contains\n36 types6. Note that inversions are not considered in this\npreliminary jazz ACE study because: 1. there are very few\ninversion notations in the currently used datasets; 2. it re-\nsults in huge number of classes based on these 36 types.\nAll training data are to be used at their tuned notegram\nlevel, which does not contain phase information. Assum-\ning well temperament, we can augment all training data\nby pitch shifting their notegrams to all 12 keys with zero\npadding. Adjusting the chord labels accordingly, this re-\nsults in 12 times of training data.\n4.1 SeventhsBass Vocabulary Systems Evaluation\nSeventhsBass evaluation is conducted in a MIREX stan-\ndard way. We use TheBeatles180 (B) as the test set and\nrun end-to-end automatic chord transcriptions from raw\naudio to chord progression for every track within. The met-\nric score is computed in a weighted chord symbol recall\n(WCSR) way using the MIREX ACE evaluation tool [14].\nAll systems are compared with each other and compared\nwith Chordino. Chordino is the only other suitable system\n4http://isophonics.net/datasets\n5https://github.com/tmc323/Chord-Annotations\n6They are: maj, min, min6, 6, maj7, maj7#5, maj7#11, maj7b5, min7,\nminmaj7, min7b5, min7#5, 7, 7b5, 7b9, 7#9, 7#5#9, 7#5b9, 7b5b9, 7#5,\n7sus4, aug7, dim7, maj9, min9, 9, 9#11, min11, min11b5, 11, min13,\nmaj13, 13, 13b9, 69 and N814 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Mm MmB S SB\nChordino 74.30 71.40 52.99 50.60\nCJ-MLP 67.25 62.27 55.15 50.86\nCJ-DBN 70.68 66.52 58.23 54.71\nCJ-BLSTM 69.09 64.51 56.47 52.74\nCJK-MLP 65.18 63.12 53.82 52.00\nCJK-DBN 67.44 65.56 55.64 54.03\nCJK-BLSTM 70.46 68.56 59.11 57.50\nCJKU-MLP 67.95 65.87 55.98 54.09\nCJKU-DBN 68.53 66.49 56.19 54.37\nCJKU-BLSTM 72.62 70.47 59.37 57.47\nTable 1 . WCSRs of four main MIREX ACE vocabulary\n(Mm = MajMin, MmB = MajMinBass, S = Sevenths, SB\n= SeventhsBass; CJ = JayChou29 + CNPop20; CJK = CJ\n+ KingQueen26; CJKU = CJK + USPop191)\nfor comparison because this is the only publicly available\nsystem which also supports SeventhsBass vocabulary [4].\nHere we argue for the validity of our evaluation method-\nology. Note that our systems are trained with combination\nof C, J, K, U, and tested on B. Some may challenge that\nsince these two sets may be drawn from two different chord\npopulations (NOT in terms of chord types, but chord ren-\ndering styles), thus the test results may not reﬂect the true\nsystem performance. It is true that they contain different\ndistributions of chord rendering styles, especially in terms\nof the “dominant sevenths” chord, as also reﬂected in the\nresults and discussions in Section 4.1.2. But as we will see\nin the results, in general, the C,J,K,U-trained systems gen-\neralize very well on B. In fact, since there could be count-\nless of possible chord rendering styles of each chord, it is\ndifﬁcult to “make sure” that two datasets are drawn from\nthe “same” population, not to mention that it is even more\ndifﬁcult to deﬁne the possible “properties” of such “pop-\nulation”. An average k-fold cross-validation score could\nbe a better indication of system performance in terms of\nthe combined CJKUB training/test set, but neither is this a\nstandard benchmarking method, nor can this score be di-\nrectly compared with an expert system such as Chordino.\n4.1.1 Overall Results of SeventhsBass\nThe WCSRs of four main MIREX ACE vocabularies are\nshown in Table 1. In MajMin and MajMinBass, Chordino\nstill does the best among all systems. But in Sevenths and\nSeventhsBass (the main focus in this paper), all our sys-\ntems perform better than Chordino, with CJ-DBN, CJK-\nBLSTM and CJKU-BLSTM performing best.\nLet’s take CJ-DBN as representative for the moment. It\nseems that it performs better at recognizing seventh chords\nbut worse at inversions compared with Chordino, but this is\nnot a correct deduction from the table. Note that Sevenths\nis a collapse of chords, regardless of root positions or in-\nversions, to their maj, min, maj7, min7 or 7 forms; and\nMajMinBass is a collapse of chords, regardless of tetrads\nor triads, to their maj, min, maj/3, maj/5, min/b3 or min/5\nforms. Considering SeventhsBass as all chords in their\noriginal forms, the score boost from SeventhsBass to Sev-\nenths indicates the amount of confusion between root po-\nsitions and inversions (let’s call it “bass confusion”); the\nscore boost from SeventhsBass to MajMinBass indicatesmaj min maj/3 maj/5 min/b3 min/5\nmaj (r) 0.66 0.03 0.00 0.02 0.00 0.00\nmin (r) 0.10 0.60 0.00 0.01 0.00 0.00\nmaj/3 (r) 0.35 0.13 0.19 0.00 0.00 0.00\nmaj/5 (r) 0.50 0.08 0.00 0.23 0.00 0.00\nmin/b3 (r) 0.36 0.30 0.01 0.06 0.00 0.00\nmin/5 (r) 0.19 0.55 0.04 0.04 0.00 0.00\nTable 3 . Chordino’s bass confusion matrix.\nmaj min maj/3 maj/5 min/b3 min/5\nmaj (r) 0.72 0.06 0.03 0.03 0.00 0.00\nmin (r) 0.15 0.63 0.02 0.02 0.00 0.00\nmaj/3 (r) 0.34 0.28 0.23 0.01 0.00 0.02\nmaj/5 (r) 0.49 0.11 0.03 0.19 0.01 0.00\nmin/b3 (r) 0.39 0.20 0.06 0.07 0.01 0.00\nmin/5 (r) 0.28 0.28 0.11 0.06 0.00 0.06\nTable 4 . CJ-DBN’s bass confusion matrix.\nthe amount of confusion between tetrads and triads (let’s\ncall it “seventh confusion”); and the score boost from Sev-\nenthsBass to MajMin approximately sums up two types of\nconfusion. It should be noted that there are yet other types\nof confusion, such as confusion of roots, or of maj and min,\nwhich could not be regarded as correct in any ways under\nthe current evaluation method.\nFollowing this deduction, the Sevenths result actu-\nally indicates that CJ-DBN still scores much better than\nChordino if bypassing bass confusion; while the MajMin-\nBass result indicates that CJ-DBN scores much lower than\nChordino if bypassing seventh confusion. Therefore com-\npared with Chordino, CJ-DBN has a better chance of bass\nconfusion, but less chance of seventh confusion. Notice\nthat in CJ-DBN, the difference between MmB and SB is\nmuch larger than that between S and SB, which means\nthe net amount of seventh confusion is much more than\nthat of bass confusion. The same is also true in Chordino.\nTherefore in both systems, there are much higher chances\nof making seventh confusion than bass confusion.\nAs for the intra-comparison among all proposed sys-\ntems, three observations are noticeable: 1, DBN has ad-\nvantage over MLP, and this advantage decreases with the\nincrease of training data size; 2, BLSTM-RNN has obvi-\nous advantage over DBN with big enough training data\nsize; 3, the investment of more data yields diminishing\nreturn. The ﬁrst point is mainly due to the intensive un-\nsupervised pre-training in DBN. The second point demon-\nstrates that the proposed BLSTM-RNN model has better\ncapability in modeling a single chord than the proposed\nDBN model. BLSTM-RNN is good at modeling tempo-\nral dependency, but DBN is good at modeling spacial de-\npendency. An input feature with 6 frames of time depen-\ndent notegrams should be more suitable for temporal mod-\neling, thus a plausible reason behind the second observa-\ntion. The third observation may possibly point to a ground\ntruth annotation consistency problem [11], which will be\nexplained in next subsection.\n4.1.2 Details of SeventhsBass\nA deeper look at the per chord-type WCSR of Sevenths-\nBass may reveal more details behind the overall scores.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 815SeventhsBass M/5 M/3 M M7/5 M7/3 M7/7 M7 7/5 7/3 7/b7 7 m/5 m/b3 m m7/5 m7/b3 m7/b7 m7\nB% 2.0 1.0 63.3 0.0 0.2 0.3 0.8 0.1 0.1 0.4 8.3 0.6 0.4 15.0 0.0 0.1 0.4 2.4\nChordino 19.9 17.1 54.4 0.0 0.0 0.0 55.6 0.0 0.0 5.7 41.0 0.0 0.0 54.3 0.0 0.0 0.0 51.0\nCJ-MLP 15.8 19.8 58.2 0.0 0.0 0.0 30.0 0.0 0.0 9.5 11.5 3.7 0.7 54.2 0.0 0.0 0.2 19.9\nCJ-DBN 19.2 21.7 63.0 0.0 0.0 0.0 35.5 0.0 0.0 20.8 9.0 5.6 0.7 59.8 0.0 0.0 0.0 21.6\nCJ-BLSTM 15.3 22.4 60.4 0.0 0.0 0.0 34.8 0.0 0.0 13.1 10.2 10.2 1.0 59.0 0.0 0.0 0.0 28.0\nCJK-MLP 5.6 14.2 62.7 0.0 0.0 0.0 30.7 0.0 0.0 2.7 10.0 1.7 0.0 46.7 0.0 0.0 0.0 22.8\nCJK-DBN 7.6 19.2 64.2 0.0 0.0 0.0 37.7 0.0 0.0 5.0 13.5 1.9 1.6 51.5 0.0 0.0 0.0 27.0\nCJK-BLSTM 6.4 12.0 70.5 0.0 0.0 0.0 37.8 0.0 0.0 10.2 8.5 9.8 1.9 48.7 0.0 0.0 0.3 32.1\nCJKU-MLP 11.8 18.3 63.8 0.0 0.0 0.0 18.4 0.0 0.0 3.7 19.4 0.5 0.4 52.7 0.0 0.0 0.6 20.9\nCJKU-DBN 8.2 16.2 64.3 0.0 0.0 0.0 19.5 0.0 0.0 1.2 20.4 1.9 2.1 52.9 0.0 0.0 0.0 20.2\nCJKU-BLSTM 22.4 16.1 66.6 0.0 0.0 0.0 33.2 0.0 0.0 8.9 23.9 2.8 3.2 59.0 0.0 0.0 0.3 26.6\nTable 2 . WCSRs of every SeventhsBass category. (M=maj, m=min). %B shows the constitution of chord in test set.\nTable 2 shows the categorical breakdowns of the Sevenths-\nBass’ WCSRs. Our systems’ advantages in M and m are\nas expected. As the training data contains huge amount of\ntheir examples, deep learning models can take full advan-\ntages and draw clear boundaries between M v.s.non-M and\nm v.s. non-m. Table 3 and 4 show a comparison of bass\nconfusion in Chordino and CJ-DBN7, which not only re-\nﬂects CJ-DBN’s advantages in M and m, but also conﬁrms\nour previous deduction that CJ-DBN makes slightly more\nbass confusion than Chordino.\nThe results of M/5, M/3 and 7/b7 deserve further inves-\ntigation. The “CJ-” systems generally perform better than\nChordino in these categories. This could be due to both\nC and J contain a large number of consistent annotations\nof these chords. In the meantime we observe their scores\ngenerally drop with introduction of K and U, seemingly in\nexchange for more score boost from M. This seems contra-\ndictory: since all three chord types (M, M/3 and M/5) have\nclear distinctions by deﬁnition, thus given a neural net-\nwork with enough modeling capacity and properly trained\n(which we assume is the case), more ground truth data\nshould yield better classiﬁcation boundaries. But instead\nthe introduction of K and U also introduces chaotic classi-\nﬁcation behaviors regarding, M/3, M/5, 7/b7 and M. Thus\nwe have to believe that these results conceivably point to a\nground truth annotation consistency problem [11], where,\nfor example, some similarly rendering M/3 chords in dif-\nferent datasets are annotated differently (as M, M/3, M/5\nor others), so that when trained on a combined dataset,\nthe classiﬁer is getting confused about the boundaries be-\ntween those similar chords. Assuming more inversions\nare “mis-annotated”8as root positions than vice versa\n(which might unfortunately be true), if such inconsisten-\ncies abound, classiﬁcations will be bias towards the domi-\nnating root position chords.\nThe most noticeable drawback of our systems is the\npoor performance of all sevenths chords (M7, 7 and m7)\ncompared with Chordino. Chordino has a very nice and\nbalanced chord confusion matrix. Shown in Table 5, al-\nmost every chord type has less than 50% confusion with\nother types. As for our approach, taking CJK-BLSTM as\nexample, the main problem is that both M7 and 7 chords\nare easily confused with maj, and m7 is easily confused\n7The numbers in the table are normalized durations. Reference labels\nare indicated by “(r)”\n8technically not necessarily a “miss” but let’s just use this expression\nfor convenience in this contextmaj min maj7 min7 7\nmaj (r) 0.66 0.03 0.11 0.03 0.13\nmin (r) 0.10 0.60 0.03 0.20 0.03\nmaj7 (r) 0.22 0.08 0.62 0.02 0.01\nmin7 (r) 0.12 0.20 0.01 0.56 0.08\n7 (r) 0.30 0.08 0.06 0.06 0.47\nTable 5 . Chordino’s seventh confusion matrix.\nmaj min maj7 min7 7\nmaj (r) 0.82 0.05 0.03 0.02 0.03\nmin (r) 0.21 0.52 0.01 0.17 0.02\nmaj7 (r) 0.42 0.07 0.39 0.03 0.01\nmin7 (r) 0.21 0.31 0.02 0.34 0.03\n7 (r) 0.67 0.12 0.01 0.05 0.10\nTable 6 . CJK-BLSTM’s seventh confusion matrix\nwith min (Table 6). The most undesirable case is the con-\nfusion between 7 and maj . The main reason behind this,\nas we try to analyze, is the different distribution of 7s in\nthe training datasets and the test dataset. The Beatles’ al-\nbums contain a lot of chord progressions that involve 7s,\nwhere the bass lines are moving by arpeggio or running\nas broken chords, but in CJK, there are very few such ex-\namples. CJK contains 7s that are mostly bass line static.\nThus CJK-BLSTM does not have enough chance to learn\n7 in dynamic bass line population, resulting in these poor\nresults. This analysis is to some degree conﬁrmed by the\nmuch better scores of 7 after adding dataset U, which con-\ntains a lot more 7 chord renderings in dynamic style.\nFor Sevenths’ inversions other than “7/b7”, since there\nare not many examples in all datasets, it is not meaningful\nfor further discussion. Actually, their WCSRs are all rela-\ntively low. This fact might in some sense invalidate the ne-\ncessity to recognize more complicated inversions, but does\nnot invalidate the need to capture inversions in general.\n4.2 Jazz Chord Vocabulary Systems Evaluation\nFollowing the MIREX ACE convention, system perfor-\nmance on jazz chord vocabulary should also be evaluated\nbased on WCSR. The WCSR score computing procedure\nin its fairest/strictest sense should count each chord as it is\nµ σ2\nBass - Chord Bass 1 0.1\nTreble - Chord Note 1 0.2\nNeither bass nor treble 0 0.2\n“N” Chord 1 0.2\nTable 7 . Gaussian model of Jazz-Chordino816 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016systems WCSR SQ\nJazz-Chordino 57.99 81.68\nJazz-MLP 61.81 76.18\nJazz-DBN 62.33 80.73\nJazz-BLSTM 66.41 80.78\nTable 8 . WCSRs and SQ (segmentation quality) of jazz\nchord vocabulary.\nwithout applying any sort of mapping scheme, as happens\nto SeventhsBass. In the following we evaluate each system\nin this way. The baseline is an augmented Chordino with\njazz vocabulary extension (Jazz-Chordino). The augmen-\ntation is done within its Gaussian-HMM engine by apply-\ning the jazz chord dictionary to the Gaussian model, whose\nsetting is described in Table 7.\nThe jazz vocabulary systems have the same system\nframework as the SeventhsBass systems, but their deep\nlearning models are trained using JazzGuitar99 dataset.\nAll systems are tested using GaryBurton7 dataset9. Re-\nsults are shown in Table 8. Jazz-BLSTM system per-\nforms the best, and outperforms Jazz-Chordino by about\n10 points. The ranking is very similar to SeventhsBass’,\nbut the results are in a sense more convincing, since the\ntest set is not dominated by chords like major and minor.\nIn fact the composition of chords in GaryBurton7 is rela-\ntively balanced, though rare chords are still rare. Therefore\nin this set of results we see clearly the advantage of hy-\nbrid Gaussian-HMM-Deep-Learning approach over a pure\nGaussian-HMM approach for very large chord vocabulary.\nMeanwhile, notice that the SQ of these systems are\nall relatively high, and these are achieved in pure jazz\ntest audio. All systems use Jazz-Chordino’s Gaussian-\nHMM as segmentation engine. The differences between\nSQ scores are caused by different merging of consecu-\ntive chord boundaries in different systems. Obviously the\nsuccess of Jazz-BLSTM is based on the success of the\nGaussian-HMM segmentation at the beginning; then based\non the robust segmentation it performs classiﬁcations with-\nout taking care of chord progression context. This task is\ncomfortable to deal with by a ﬁxed-length input deep learn-\ning model. The advantage may not be obvious under a\nsmall chord vocabulary, but is obvious under a large chord\nvocabulary.\n5. CONCLUSION\nIn this paper we propose a hybrid Gaussian-HMM-Deep-\nLearning approach towards SeventhsBass and jazz vocab-\nulary automatic chord estimation. Based on a Chordino-\nlike segmentation engine, the approach applies two types\nof deep learning models, i.e., DBN and BLSTM-RNN, for\nchord classiﬁcations.\nFor SeventhsBass implementation, we train several\nmodels of each type using four datasets in an incremen-\ntal way. The systems are tested using another dataset,\nand compared with Chordino. Results show that the\n9Composition of chords in GaryBurton7: maj:0.09; min7:0.13;\n7:0.22; min7b5:0.12; 7b9:0.06; min:0.1; maj:0.14; others:0.14.best system variant, CJKU-BLSTM obviously outper-\nforms Chordino in both Sevenths and SeventhsBass, but\nis slightly outperformed by Chordino in MajMin and Ma-\njMinBass. We ﬁnd that our system tends to make more\nbass confusion but less seventh confusion compared with\nChordino. The major success of our systems is in triads,\nwhile the major drawbacks are in sevenths chords. The\ntrends within the results along incremental training data\nsizes may indicate a possible data annotation inconsistency\nissue that conceivably leads to diminishing return effect.\nFor jazz vocabulary implementation, we train one\nmodel for each type using JazzGuitar99 dataset, test them\nusing GaryBurton7 dataset, and compare them with a\nChordino-like system augmented with jazz chord vocabu-\nlary (Jazz-Chordino). Results show a similar system rank-\ning as in SeventhsBass’ results, with high segmentation\nqualities. The best system, Jazz-BLSTM, outscores Jazz-\nChordino obviously. Given that GaryBurton7 is a rela-\ntively chord balanced dataset, the results demonstrate more\nclearly the advantage of hybrid Gaussian-HMM-Deep-\nLearning approach over pure Gaussian-HMM approach,\nwhich might not be so obvious with much smaller chord\nvocabulary.\nGenerally speaking, Chordino is an elegant music\nknowledge driven expert system that generally recognizes\nchords very well. But at times it fails also because of its\nsimplicity, which fails to capture chords rendered in ab-\nnormal ways. On the other hand, our approach is data\ndriven. The success or non-success of it depends highly on\nthe chord balancing, distribution and population of train-\ning data. While performances on some dominating chords\nbeneﬁt much from the data, other performances suffer a lot\nfrom data insufﬁciency or inconsistency.\nThere are a few concerns to be addressed. The ﬁrst con-\ncern is about the manually engineered segmentation en-\ngine. The Gaussian-HMM segmentation engine is good\nindeed, but for scientiﬁc interest, we are also very curious\nabout whether by doing a deep training on huge amount\nof data can one system learn that transformation. Prelimi-\nnary researches are ongoing, but none of our attempts have\nachieved that level yet. We believe this can be achieved\ngradually by deeper models and more data. A separate\ntraining for segmentation only might be beneﬁcial. The\nsecond concern is about datasets. A better training based\nsystem asks for more ground truth annotations, especially\nthose of skew classes, so as to train a more balanced system\nand to avoid the main contribution of performance being\ndominated by a few classes. Generally more data will lead\nto more examples of skew classes, but due to annotation\ninconsistency issue, simply “more data” may not be the ﬁ-\nnal solution at all, which leaves much more works to be\ndone in this area. Finally there is a concern of vocabulary\nsize (seems contradictory to the previous concern), which\nasks for gradually exploring ACE systems’ capabilities on\nmore complex vocabularies as it is the way to approach the\nultimate goal of ACE, which is to match human expert’s\nability of doing chord recognition.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 8176. REFERENCES\n[1] Gary Burton Jazz Improvisation Course.\nhttps://www.coursera.org/learn/jazz-improvisation/.\nAccessed: 2016-02-16.\n[2] Yoshua Bengio. Learning deep architectures for ai.\nFoundations and trends R/circlecopyrtin Machine Learning ,\n2(1):1–127, 2009.\n[3] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and\nPascal Vincent. Audio chord recognition with recurrent\nneural networks. In ISMIR , pages 335–340, 2013.\n[4] J Ashley Burgoyne, W Bas de Haas, and Johan\nPauwels. On comparative statistics for labelling tasks:\nWhat can we learn from mirex ace 2013. In Proceed-\nings of the 15th Conference of the International Society\nfor Music Information Retrieval (ISMIR 2014) , pages\n525–530, 2014.\n[5] Junqi Deng and Yu-Kwong Kwok. MIREX 2015 sub-\nmission: Automatic chord estimation with chord cor-\nrection using neural network, 2015.\n[6] Junqi Deng and Yu-Kwong Kwok. Automatic chord\nestimation on seventhsbass chord vocabulary using\ndeep neural network. In Proceedings of the 41th Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP 2016). Shanghai, China , 2016.\n[7] Alex Graves. Supervised sequence labelling . Springer,\n2012.\n[8] Geoffrey Hinton. A practical guide to training re-\nstricted boltzmann machines. Momentum , 9(1):926,\n2010.\n[9] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-\nterm memory. Neural computation , 9(8):1735–1780,\n1997.\n[10] Eric J Humphrey and Juan P Bello. Rethinking au-\ntomatic chord recognition with convolutional neu-\nral networks. In Machine Learning and Applications\n(ICMLA), 2012 11th International Conference on , vol-\nume 2, pages 357–362. IEEE, 2012.\n[11] Eric J Humphrey and Juan P Bello. Four timely insights\non automatic chord estimation. In Proceedings of the\n16th Conference of the International Society for Music\nInformation Retrieval (ISMIR 2015) , 2015.\n[12] Matthias Mauch. Automatic chord transcription from\naudio using computational models of musical con-\ntext. PhD thesis, School of Electronic Engineering and\nComputer Science Queen Mary, University of London,\n2010.\n[13] Matthias Mauch and Simon Dixon. Approximate note\ntranscription for the improved identiﬁcation of difﬁcult\nchords. In ISMIR , pages 135–140, 2010.[14] Johan Pauwels and Geoffroy Peeters. Evaluating au-\ntomatically estimated chord sequences. In Acoustics,\nSpeech and Signal Processing (ICASSP), 2013 IEEE\nInternational Conference on , pages 749–753. IEEE,\n2013.\n[15] Jeff Schroedl. Hal Leonard Guitar Method - Jazz Gui-\ntar: Hal Leonard Guitar Method Stylistic Supplement\nBk/online audio . Hal Leonard, 2003.\n[16] Siddharth Sigtia, Nicolas Boulanger-Lewandowski,\nand Simon Dixon. Audio chord recognition with a\nhybrid recurrent neural network. In Proceedings of\nthe 16th International Society for Music Information\nRetrieval Conference (ISMIR 2015). Malaga, Spain ,\n2015.\n[17] Tijmen Tieleman. Training restricted boltzmann ma-\nchines using approximations to the likelihood gradient.\nInProceedings of the 25th international conference on\nMachine learning , pages 1064–1071. ACM, 2008.\n[18] Matthew D Zeiler. Adadelta: an adaptive learning rate\nmethod. arXiv preprint arXiv:1212.5701 , 2012.\n[19] Xinquan Zhou and Alexander Lerch. Chord detection\nusing deep learning. In Proceedings of the 16th ISMIR\nConference , volume 53, 2015.818 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Towards Score Following In Sheet Music Images.",
        "author": [
            "Matthias Dorfer",
            "Andreas Arzt",
            "Gerhard Widmer"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1415548",
        "url": "https://doi.org/10.5281/zenodo.1415548",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/027_Paper.pdf",
        "abstract": "This paper addresses the matching of short music audio snippets to the corresponding pixel location in images of sheet music. A system is presented that simultaneously learns to read notes, listens to music and matches the currently played music to its corresponding notes in the sheet. It consists of an end-to-end multi-modal convolu- tional neural network that takes as input images of sheet music and spectrograms of the respective audio snippets. It learns to predict, for a given unseen audio snippet (cov- ering approximately one bar of music), the corresponding position in the respective score line. Our results suggest that with the use of (deep) neural networks – which have proven to be powerful image processing models – working with sheet music becomes feasible and a promising future research direction.",
        "zenodo_id": 1415548,
        "dblp_key": "conf/ismir/DorferAW16",
        "content": "TOWARDS SCORE FOLLOWING IN SHEET MUSIC IMAGES\nMatthias Dorfer Andreas Arzt Gerhard Widmer\nDepartment of Computational Perception, Johannes Kepler University Linz, Austria\nmatthias.dorfer@jku.at\nABSTRACT\nThis paper addresses the matching of short music audio\nsnippets to the corresponding pixel location in images of\nsheet music. A system is presented that simultaneously\nlearns to read notes, listens to music and matches the\ncurrently played music to its corresponding notes in the\nsheet. It consists of an end-to-end multi-modal convolu-\ntional neural network that takes as input images of sheet\nmusic and spectrograms of the respective audio snippets.\nIt learns to predict, for a given unseen audio snippet (cov-\nering approximately one bar of music), the corresponding\nposition in the respective score line. Our results suggest\nthat with the use of (deep) neural networks – which have\nproven to be powerful image processing models – working\nwith sheet music becomes feasible and a promising future\nresearch direction.\n1. INTRODUCTION\nPrecisely linking a performance to its respective sheet mu-\nsic – commonly referred to as audio-to-score alignment –\nis an important topic in MIR and the basis for many appli-\ncations [20]. For instance, the combination of score and\naudio supports algorithms and tools that help musicolo-\ngists in in-depth performance analysis (see e.g. [6]), al-\nlows for new ways to browse and listen to classical music\n(e.g. [9, 13]), and can generally be helpful in the creation\nof training data for tasks like beat tracking or chord recog-\nnition. When done on-line, the alignment task is known as\nscore following, and enables a range of applications like\nthe synchronization of visualisations to the live music dur-\ning concerts (e.g. [1, 17]), and automatic accompaniment\nand interaction live on stage (e.g. [5, 18]).\nSo far all approaches to this task depend on a symbolic,\ncomputer-readable representation of the sheet music, such\nas MusicXML or MIDI (see e.g. [1, 5, 8, 12, 14–18]). This\nrepresentation is created either manually (e.g. via the time-\nconsuming process of (re-)setting the score in a music no-\ntation program), or automatically via optical music recog-\nnition software. Unfortunately automatic methods are still\nhighly unreliable and thus of limited use, especially for\nmore complex music like orchestral scores [20].\nc/circlecopyrtMatthias Dorfer, Andreas Arzt, Gerhard Widmer. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Matthias Dorfer, Andreas Arzt, Gerhard\nWidmer. “Towards Score Following in Sheet Music Images”, 17th Inter-\nnational Society for Music Information Retrieval Conference, 2016.The central idea of this paper is to develop a method that\nlinks the audio and the image of the sheet music directly ,\nbylearning correspondences between these two modali-\nties, and thus making the complicated step of creating an\nin-between representation obsolete. We aim for an algo-\nrithm that simultaneously learns to read notes ,listens to\nmusic and matches the currently played music with the cor-\nrect notes in the sheet music. We will tackle the problem in\nan end-to-end neural network fashion, meaning that the en-\ntire behaviour of the algorithm is learned purely from data\nand no further manual feature engineering is required.\n2. METHODS\nThis section describes the audio-to-sheet matching model\nand the input data required, and shows how the model is\nused at test time to predict the expected location of a new\nunseen audio snippets in the respective sheet image.\n2.1 Data, Notation and Task Description\nThe model takes two different input modalities at the same\ntime: images of scores, and short excerpts from spectro-\ngrams of audio renditions of the score (we will call these\nquery snippets as the task is to predict the position in the\nscore that corresponds to such an audio snippet). For this\nﬁrst proof-of-concept paper, we make a number of simpli-\nfying assumptions: for the time being, the system is fed\nonly a single staff line at a time (not a full page of score).\nWe restrict ourselves to monophonic music , and to the pi-\nano. To generate training examples, we produce a ﬁxed-\nlength query snippet for each note (onset) in the audio.\nThe snippet covers the target note onset plus a few addi-\ntional frames, at the end of the snippet, and a ﬁxed-size\ncontext of 1.2seconds into the past, to give some temporal\ncontext. The same procedure is followed when producing\nexample queries for off-line testing.\nA training/testing example is thus composed of two in-\nputs: Input 1 is an image Si(in our case of size 40×390\npixels) showing one staff of sheet music. Input 2 is an au-\ndio snippet – speciﬁcally, a spectrogram excerpt Ei,j(40\nframes×136frequency bins) – cut from a recording of the\npiece, of ﬁxed length ( 1.2seconds). The rightmost onset\nin spectrogram excerpt Ei,jis interpreted as the target note\njwhose position we want to predict in staff image Si. For\nthe music used in our experiments (Section 3) this context\nis a bit less than one bar. For each note j(represented by\nits corresponding spectrogram excerpt Ei,j) we annotated\nitsground truth sheet location xjin sheet image Si. Coor-789(a) Spectrogram-to-sheet correspondence. In this ex-\nample the rightmost onset in spectrogram excerpt Ei,j\ncorresponds to the rightmost note (target note j) in\nsheet image Si. For the present case the temporal con-\ntext of about 1.2seconds (into the past) covers ﬁve\nadditional notes in the spectrogram. The staff image\nand spectrogram excerpt are exactly the multi-modal\ninput presented to the proposed audio-to-sheet match-\ning network. At train time the target pixel location xj\nin the sheet image is available; at test time ˆxjhas to\nbe predicted by the model (see ﬁgure below).\n(b) Schematic sketch of the audio-to-sheet matching task targeted\nin this work. Given a sheet image Siand a short snippet of au-\ndio (spectrogram excerpt Ei,j) the model has to predict the audio\nsnippet’s corresponding pixel location xjin the image.\nFigure 1 :Input data and audio-to-sheet matching task.\ndinatexjis the distance of the note head (in pixels) from\nthe left border of the image. As we work with single staffs\nof sheet music we only need the x-coordinate of the note\nat this point. Figure 1a relates all components involved.\nSummary and Task Description : For training we present\ntriples of (1) staff image Si, (2) spectrogram excerpt Ei,j\nand (3) ground truth pixel x-coordinate xjto our audio-to-\nsheet matching model. At test time only the staff image\nand spectrogram excerpt are available and the task of the\nmodel is to predict the estimated pixel location ˆxjin the\nimage. Figure 1b shows a sketch summarizing this task.\n2.2 Audio-Sheet Matching as Bucket Classiﬁcation\nWe now propose a multi-modal convolutional neural net-\nwork architecture that learns to match unseen audio snip-\npets (spectrogram excerpts) to their corresponding pixel lo-\ncation in the sheet image.\n2.2.1 Network Structure\nFigure 2 provides a general overview of the deep network\nand the proposed solution to the matching problem. As\nmentioned above, the model operates jointly on a staff im-\nageSiand the audio (spectrogram) excerpt Ei,jrelated to\na notej. The rightmost onset in the spectrogram excerpt\nis the one related to target note j. The multi-modal modelconsists of two specialized convolutional networks: one\ndealing with the sheet image and one dealing with the au-\ndio (spectrogram) input. In the subsequent layers we fuse\nthe specialized sub-networks by concatenation of the latent\nimage- and audio representations and additional process-\ning by a sequence of dense layers. For a detailed descrip-\ntion of the individual layers we refer to Table 1 in Section\n3.4. The output layer of the network and the corresponding\nlocalization principle are explained in the following.\n2.2.2 Audio-to-Sheet Bucket Classiﬁcation\nThe objective for an unseen spectrogram excerpt and a cor-\nresponding staff of sheet music is to predict the excerpt’s\nlocationxjin the staff image. For this purpose we start\nwith horizontally quantizing the sheet image into Bnon-\noverlapping buckets. This discretisation step is indicated\nas the short vertical lines in the staff image above the score\nin Figure 2. In a second step we create for each note jin\nthe train set a target vector tj={tj,b}where each vec-\ntor element tj,bholds the probability that bucket bcovers\nthe current target note j. In particular, we use soft tar-\ngets, meaning that the probability for one note is shared\nbetween the two buckets closest to the note’s true pixel lo-\ncationxj. We linearly interpolate the shared probabilities\nbased on the two pixel distances (normalized to sum up\nto one) of the note’s location xjto the respective (closest)\nbucket centers. Bucket centers are denoted by cbin the\nfollowing where subscript bis the index of the respective\nbucket. Figure 3 shows an example sketch of the compo-\nnents described above. Based on the soft target vectors we\ndesign the output layer of our audio-to-sheet matching net-\nwork as aB-way soft-max with activations deﬁned as:\nφ(yj,b) =eyj,b\n/summationtextB\nk=1eyj,k(1)\nφ(yj,b)is the soft-max activation of the output neuron rep-\nresenting bucket band hence also representing the region\nin the sheet image covered by this bucket. By applying the\nsoft-max activation the network output gets normalized to\nrange (0,1)and further sums up to 1.0over allBoutput\nneurons. The network output can now also be interpreted\nas a vector of probabilities pj={φ(yj,b)}and shares the\nsame value range and properties as the soft target vectors.\nIn training, we optimize the network parameters Θby\nminimizing the Categorical Cross Entropy (CCE) loss lj\nbetween target vectors tjand network output pj:\nlj(Θ) =−B/summationdisplay\nk=1tj,klog(pj,k) (2)\nThe CCE loss function becomes minimal when the net-\nwork output pjexactly matches the respective soft target\nvector tj. In Section 3.4 we provide further information\non the exact optimization strategy used.1\n1For the sake of completeness : In our initial experiments we started\nto predict the sheet location of audio snippets by minimizing the Mean-\nSquared-Error (MSE) between the predicted and the true pixel coordinate\n(MSE regression). However, we observed that training these networks\nis much harder and further performs worse than the bucket classiﬁcation\napproach proposed in this paper.790 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Figure 2 :Overview of multi-modal convolutional neural network for audio-to-sheet matching. The network takes a staff image and\na spectrogram excerpt as input. Two specialized convolutional network parts, one for the sheet image and one for the audio input, are\nmerged into one multi-modality network. The output part of the network predicts the region in the sheet image – the classiﬁcation bucket\n– to which the audio snippet corresponds.\nFigure 3 :Part of a staff of sheet music along with soft tar-\nget vector tjfor target note jsurrounded with an ellipse. The\ntwo buckets closest to the note share the probability (indicated as\ndots) of containing the note. The short vertical lines highlight the\nbucket borders.\n2.3 Sheet Location Prediction\nOnce the model is trained, we use it at test time to predict\nthe expected location ˆxjof an audio snippet with target\nnotejin a corresponding image of sheet music. The output\nof the network is a vector pj={pj,b}holding the prob-\nabilities that the given test snippet jmatches with bucket\nbin the sheet image. Having these probabilities we con-\nsider two different types of predictions: (1) We compute\nthe centerc∗\nbof bucketb∗=argmaxbpj,bholding the high-\nest overall matching probability. (2) For the second case\nwe take, in addition to b∗, the two neighbouring buckets\nb∗−1andb∗+ 1 into account and compute a (linearly)\nprobability weighted position prediction in the sheet im-\nage as\nˆxj=/summationdisplay\nk∈{b∗−1,b∗,b∗+1}wkck (3)\nwhere weight vector wcontains the probabilities\n{pj,b∗−1,pj,b∗,pj,b∗+1}normalized to sum up to one and\nckare the center coordinates of the respective buckets.3. EXPERIMENTAL EV ALUATION\nThis section evaluates our audio-to-sheet matching model\non a publicly available dataset. We describe the experi-\nmental setup, including the data and evaluation measures,\nthe particular network architecture as well as the optimiza-\ntion strategy, and provide quantitative results.\n3.1 Experiment Description\nThe aim of this paper is to show that it is feasible to learn\ncorrespondences between audio (spectrograms) and im-\nages of sheet music in an end-to-end neural network fash-\nion, meaning that an algorithm learns the entire task purely\nfrom data, so that no hand crafted feature engineering is re-\nquired. We try to keep the experimental setup simple and\nconsider one staff of sheet music per train/test sample (this\nis exactly the setup drafted in Figure 2). To be perfectly\nclear, the task at hand is the following: For a given au-\ndio snippet, ﬁnd its x-coordinate pixel position in a corre-\nsponding staff of sheet music. We further restrict the audio\nto monophonic music containing half, quarter and eighth\nnotes but allow variations such as dotted notes, notes tied\nacross bar lines as well as accidental signs.\n3.2 Data\nFor the evaluation of our approach we consider the Not-\ntingham2data set which was used, e.g., for piano tran-\nscription in [4]. It is a collection of midi ﬁles already split\ninto train, validation and test tracks. To be suitable for\naudio-to-sheet matching we prepare the data set (midi ﬁles)\nas follows:\n2www-etud.iro.umontreal.ca/ ˜boulanni/icml2012Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 791Sheet-Image 40×390 Spectrogram 136×40\n5×5Conv(pad-2, stride-1-2)- 64-BN-ReLu 3×3Conv(pad-1)- 64-BN-ReLu\n3×3Conv(pad-1)- 64-BN-ReLu 3×3Conv(pad-1)- 64-BN-ReLu\n2×2Max-Pooling + Drop-Out( 0.15) 2×2Max-Pooling + Drop-Out( 0.15)\n3×3Conv(pad-1)- 128-BN-ReLu 3×3Conv(pad-1)- 96-BN-ReLu\n3×3Conv(pad-1)- 128-BN-ReLu 2×2Max-Pooling + Drop-Out( 0.15)\n2×2Max-Pooling + Drop-Out( 0.15) 3×3Conv(pad-1)- 96-BN-ReLu\n2×2Max-Pooling + Drop-Out( 0.15)\nDense- 1024 -BN-ReLu + Drop-Out( 0.3) Dense- 1024 -BN-ReLu + Drop-Out( 0.3)\nConcatenation-Layer- 2048\nDense- 1024 -BN-ReLu + Drop-Out( 0.3)\nDense- 1024 -BN-ReLu + Drop-Out( 0.3)\nB-way Soft-Max Layer\nTable 1 :Architecture of Multi-Modal Audio-to-Sheet Matching Model: BN: Batch Normalization, ReLu: Rectiﬁed Linear Activation\nFunction, CCE: Categorical Cross Entropy, Mini-batch size: 100\n1. We select the ﬁrst track of the midi ﬁles (right hand,\npiano) and render it as sheet music using Lilypond.3\n2. We annotate the sheet coordinate xjof each note.\n3. We synthesize the midi-tracks to ﬂac-audio using\nFluidsynth4and a Steinway piano sound font.\n4. We extract the audio timestamps of all note onsets.\nAs a last preprocessing step we compute log-spectrograms\nof the synthesized ﬂac ﬁles [3], with an audio sample rate\nof22.05kHz, FFT window size of 2048 samples, and com-\nputation rate of 31.25frames per second. For dimension-\nality reduction we apply a normalized 24-band logarithmic\nﬁlterbank allowing only frequencies from 80Hz to 8kHz.\nThis results in 136frequency bins.\nWe already showed a spectrogram-to-sheet annotation\nexample in Figure 1a. In our experiment we use spectro-\ngram excerpts covering 1.2seconds of audio (40 frames).\nThis context is kept the same for training and testing.\nAgain, annotations are aligned in a way so that the right-\nmost onset in a spectrogram excerpt corresponds to the\npixel position of target note jin the sheet image. In ad-\ndition, the spectrogram is shifted 5 frames to the right to\nalso contain some information on the current target note’s\nonset and pitch. We chose this annotation variant with the\nrightmost onset as it allows for an online application of our\naudio-to-sheet model (as would be required, e.g., in a score\nfollowing task).\n3.3 Evaluation Measures\nTo evaluate our approach we consider, for each test note j,\nthe following ground truth and prediction data: (1) The true\npositionxjas well as the corresponding target bucket bj\n(see Figure 3). (2) The estimated sheet location ˆxjand the\nmost likely target bucket b∗predicted by the model. Given\nthis data we compute two types of evaluation measures.\nThe ﬁrst – the top-k bucket hit rate – quantiﬁes the ratio\nof notes that are classiﬁed into the correct bucket allowing\n3http://www.lilypond.org/\n4http://www.fluidsynth.org/a tolerance of k−1buckets. For example, the top-1 bucket\nhit rate counts only those notes where the predicted bucket\nb∗matches exactly the note’s target bucket bj. The top-2\nbucket hit rate allows for a tolerance of one bucket and so\non. The second measure – the normalized pixel distance –\ncaptures the actual distance of a predicted sheet location ˆxj\nto its corresponding true position xj. To allow for an eval-\nuation independent of the image resolution used in our ex-\nperiments we normalize the pixel errors by dividing them\nby the width of the sheet image as (ˆxj−xj)/width (Si).\nThis results in distance errors living in range (−1,1).\nWe would like to emphasise that the quantitative eval-\nuations based on the measures introduced above are per-\nformed only at time steps where a note onset is present. At\nthose points in time an explicit correspondence between\nspectrogram (onset) and sheet image (note head) is es-\ntablished. However, in Section 4 we show that a time-\ncontinuous prediction is also feasible with our model and\nonset detection is not required at run time.\n3.4 Model Architecture and Optimization\nTable 1 gives details on the model architecture used for\nour experiments. As shown in Figure 2, the model is struc-\ntured into two disjoint convolutional networks where one\nconsiders the sheet image and one the spectrogram (audio)\ninput. The convolutional parts of our model are inspired by\nthe VGG model built from sequences of small convolution\nkernels (e.g. 3×3) and max-pooling layers. The central\npart of the model consists of a concatenation layer bring-\ning the image and spectrogram sub-networks together. Af-\nter two dense layers with 1024 units each we add a B-way\nsoft-max output layer. Each of the Bsoft-max output neu-\nrons corresponds to one of the disjoint buckets which in\nturn represent quantised sheet image positions. In our ex-\nperiments we use a ﬁxed number of 40buckets selected as\nfollows: We measure the minimum distance between two\nsubsequent notes – in our sheet renderings – and select the\nnumber of buckets such that each bucket contains at most\none note. It is of course possible that no note is present\nin a bucket – e.g., for the buckets covering the clef at the792 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016−40−30−20−10010203040\nBucket Distance0.00.10.20.30.40.5Ratio of NotesBucket Distance Distribution\nmaxinter0.000.010.020.030.040.05|Normalized Pixel Distance|Figure 4 :Summary of matching results on test set. Left: His-\ntogram of bucket distances between predicted and true buckets.\nRight : Box-plots of absolute normalized pixel distances between\npredicted and true image position. The box-plot is shown for both\nlocation prediction methods described in Section 2.3 (maximum,\ninterpolated).\nbeginning of a staff. As activations function for the inner\nlayers we use rectiﬁed linear units [10] and apply batch\nnormalization [11] after each layer as it helps training and\nconvergence.\nGiven this architecture and data we optimize the param-\neters of the model using mini-batch stochastic gradient de-\nscent with Nesterov style momentum. We set the batch\nsize to 100and ﬁx the momentum at 0.9for all epochs.\nThe initial learn-rate is set to 0.1and divided by 10 every\n10 epochs. We additionally apply a weight decay of 0.0001\nto all trainable parameters of the model.\n3.5 Experimental Results\nFigure 4 shows a histogram of the signed bucket distances\nbetween predicted and true buckets. The plot shows that\nmore than 54% of all unseen test notes are matched ex-\nactly with the corresponding bucket. When we allow for\na tolerance of±1bucket our model is able to assign over\n84% of the test notes correctly. We can further observe that\nthe prediction errors are equally distributed in both direc-\ntions – meaning too early and too late in terms of audio.\nThe results are also reported in numbers in Table 2, as the\ntop-k bucket hit rates for train, validation and test set.\nThe box plots in the right part of Figure 4 summarize\nthe absolute normalized pixel distances (NPD) between\npredicted and true locations. We see that the probability-\nweighted position interpolation (Section 2.3) helps im-\nprove the localization performance of the model. Table 2\nagain puts the results in numbers, as means and medians of\nthe absolute NPD values. Finally, Fig. 2 (bottom) reports\nthe ratio of predictions with a pixel distance smaller than\nthe width of a single bucket.\n4. DISCUSSION AND REAL MUSIC\nThis section provides a representative prediction example\nof our model and uses it to discuss the proposed approach.\nIn the second part we then show a ﬁrst step towards match-\ningreal(though still very simple) music to its correspond-\ning sheet. By real music we mean audio that is not justTrain Valid Test\nTop-1-Bucket-Hit-Rate 79.28% 51.63% 54.64%\nTop-2-Bucket-Hit-Rate 94.52% 82.55% 84.36%\nmean(|NPD max|) 0.0316 0.0684 0.0647\nmean(|NPD int|) 0.0285 0.0670 0.0633\nmedian(|NPD max|) 0.0067 0.0119 0.0112\nmedian(|NPD int|) 0.0033 0.0098 0.0091\n|NPD max|<wb 93.87% 76.31% 79.01%\n|NPD int|<wb 94.21% 78.37% 81.18%\nTable 2 :Top-k bucket hit rates andnormalized pixel distances\n(NPD) as described in Section 3.4 for train, validation and test\nset. We report mean and median of the absolute NPDs for both\ninterpolated (int) and maximum (max) probability bucket predic-\ntion. The last two rows report the percentage of predictions not\nfurther away from the true pixel location than the width wbof one\nbucket.\nsynthesized midi, but played by a human on a piano and\nrecorded via microphone.\n4.1 Prediction Example and Discussion\nFigure 5 shows the image of one staff of sheet music along\nwith the predicted as well as the ground truth pixel location\nfor a snippet of audio. The network correctly matches the\nspectrogram with the corresponding pixel location in the\nsheet image. However, we observe a second peak in the\nbucket prediction probability vector. A closer look shows\nthat this is entirely reasonable, as the music is quite repet-\nitive and the current target situation actually appears twice\nin the score. The ability of predicting probabilities for\nmultiple positions is a desirable and important property, as\nrepetitive structures are immanent to music. The resulting\nprediction ambiguities can be addressed by exploiting the\ntemporal relations between the notes in a piece by meth-\nods such as dynamic time warping or probabilistic models.\nIn fact, we plan to combine the probabilistic output of our\nmatching model with existing score following methods, as\nfor example [2]. In Section 2 we mentioned that training a\nsheet location prediction with MSE-regression is difﬁcult\nto optimize. Besides this technical drawback it would not\nbe straightforward to predict a variable number of locations\nwith an MSE-model, as the number of network outputs has\nto be ﬁxed when designing the model.\nIn addition to the network inputs and prediction Fig. 5\nalso shows a saliency map [19] computed on the input\nsheet image with respect to the network output.5The\nsaliency can be interpreted as the input regions to which\nmost of the net’s attention is drawn. In other words, it high-\nlights the regions that contribute most to the current output\nproduced by the model. A nice insight of this visualiza-\ntion is that the network actually focuses and recognizes the\nheads of the individual notes. In addition it also directs\nsome attention to the style of stems, which is necessary to\ndistinguish for example between quarter and eighth notes.\n5The implementation is adopted from an example by Jan Schl ¨uter in\nthe recipes section of the deep learning framework Lasagne [7].Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 793Staff Image\nSpectrogram\nSaliency (Staff Image)\n0 5 10 15 20 25 30 35\nBucket0.00.20.40.60.81.0ProbabilityBucket­Distance: 0\nground truth\npredictionFigure 5 :Example prediction of the proposed model. The top row shows the input staff image Sialong with the bucket borders as thin\ngray lines, and the given query audio (spectrogram) snippet Ei,j. The plot in the middle visualizes the salience map (representing the\nattention of the neural network) computed on the input image. Note that the network’s attention is actually drawn to the individual note\nheads. The bottom row compares the ground truth bucket probabilities with the probabilities predicted by the network. In addition, we\nalso highlight the corresponding true and predicted pixel locations in the staff image in the top row.\nThe optimization on soft target vectors is also reﬂected\nin the predicted bucket probabilities. In particular the\nneighbours of the bucket with maximum activation are also\nactive even though there is no explicit neighbourhood re-\nlation encoded in the soft-max output layer. This helps the\ninterpolation of the true position in the image (see Fig. 4).\n4.2 First Steps with Real Music\nAs a ﬁnal point, we report on ﬁrst attempts at working with\n“real” music. For this purpose one of the authors played\nthe right hand part of a simple piece (Minuet in G Major\nby Johann Sebastian Bach, BWV Anhang 114) – which,\nof course, was not part of the training data – on a Yamaha\nAvantGrand N2 hybrid piano and recorded it using a sin-\ngle microphone. In this application scenario we predict\nthe corresponding sheet locations not only at times of on-\nsets but for a continuous audio stream (subsequent spec-\ntrogram excerpts). This can be seen as a simple version\nof online score following in sheet music, without taking\ninto account the temporal relations of the predictions. We\noffer the reader a video6that shows our model following\nthe ﬁrst three staff lines of this simple piece.7The ra-\ntio of predicted notes having a pixel-distance smaller than\nthe bucket width (compare Section 3.5) is 71.72% for this\n6https://www.dropbox.com/s/0nz540i1178hjp3/\nBach_Minuet_G_Major_net4b.mp4?dl=0\n7Note: our model operates on single staffs of sheet music and requires\na certain context of spectrogram frames for prediction (in our case 40\nframes). For this reason it cannot provide a localization for the ﬁrst couple\nof notes in the beginning of each staff at the current stage. In the video\none can observe that prediction only starts when the spectrogram in the\ntop right corner has grown to the desired size of 40 frames. We kept this\nbehaviour for now as we see our work as a proof of concept. The issue\ncan be easily addressed by concatenating the images of subsequent staffs\nin horizontal direction. In this way we will get a “continuous stream of\nsheet music” analogous to a spectrogram for audio.real recording. This corresponds to a average normalized-\npixel-distance of 0.0402 .\n5. CONCLUSION\nIn this paper we presented a multi-modal convolutional\nneural network which is able to match short snippets of\naudio with their corresponding position in the respective\nimage of sheet music, without the need of any symbolic\nrepresentation of the score. First evaluations on simple pi-\nano music suggest that this is a very promising new ap-\nproach that deserves to be explored further.\nAs this is a proof of concept paper, naturally our method\nstill has some severe limitations. So far our approach can\nonly deal with monophonic music, notated on a single\nstaff, and with performances that are roughly played in the\nsame tempo as was set in our training examples.\nIn the future we will explore options to lift these limi-\ntations one by one, with the ultimate goal of making this\napproach applicable to virtually any kind of complex sheet\nmusic. In addition, we will try to combine this approach\nwith a score following algorithm. Our vision here is to\nbuild a score following system that is capable of dealing\nwith any kind of classical sheet music, out of the box, with\nno need for data preparation.\n6. ACKNOWLEDGEMENTS\nThis work is supported by the Austrian Ministries BMVIT\nand BMWFW, and the Province of Upper Austria via the\nCOMET Center SCCH, and by the European Research\nCouncil (ERC Grant Agreement 670035, project CON\nESPRESSIONE). The Tesla K40 used for this research was\ndonated by the NVIDIA corporation.794 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20167. REFERENCES\n[1] Andreas Arzt, Harald Frostel, Thassilo Gadermaier,\nMartin Gasser, Maarten Grachten, and Gerhard Wid-\nmer. Artiﬁcial intelligence in the concertgebouw. In\nProceedings of the International Joint Conference\non Artiﬁcial Intelligence (IJCAI) , Buenos Aires, Ar-\ngentina, 2015.\n[2] Andreas Arzt, Gerhard Widmer, and Simon Dixon. Au-\ntomatic page turning for musicians via real-time ma-\nchine listening. In Proc. of the European Conference\non Artiﬁcial Intelligence (ECAI) , Patras, Greece, 2008.\n[3] Sebastian B ¨ock, Filip Korzeniowski, Jan Schl ¨uter, Flo-\nrian Krebs, and Gerhard Widmer. madmom: a new\nPython Audio and Music Signal Processing Library.\narXiv:1605.07008 , 2016.\n[4] Nicolas Boulanger-lewandowski, Yoshua Bengio, and\nPascal Vincent. Modeling temporal dependencies in\nhigh-dimensional sequences: Application to poly-\nphonic music generation and transcription. In Proceed-\nings of the 29th International Conference on Machine\nLearning (ICML-12) , pages 1159–1166, 2012.\n[5] Arshia Cont. A coupled duration-focused architecture\nfor realtime music to score alignment. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence ,\n32(6):837–846, 2009.\n[6] Nicholas Cook. Performance analysis and chopin’s\nmazurkas. Musicae Scientae , 11(2):183–205, 2007.\n[7] Sander Dieleman, Jan Schl ¨uter, Colin Raffel, Eben Ol-\nson, Søren Kaae Sønderby, Daniel Nouri, Eric Batten-\nberg, A ¨aron van den Oord, et al. Lasagne: First re-\nlease., August 2015.\n[8] Zhiyao Duan and Bryan Pardo. A state space model for\non-line polyphonic audio-score alignment. In Proc. of\nthe IEEE Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , Prague, Czech Republic, 2011.\n[9] Jon W. Dunn, Donald Byrd, Mark Notess, Jenn Ri-\nley, and Ryan Scherle. Variations2: Retrieving and us-\ning music in an academic setting. Communications of\nthe ACM, Special Issue: Music information retrieval ,\n49(8):53–48, 2006.\n[10] Xavier Glorot, Antoine Bordes, and Yoshua Bengio.\nDeep sparse rectiﬁer neural networks. In International\nConference on Artiﬁcial Intelligence and Statistics ,\npages 315–323, 2011.\n[11] Sergey Ioffe and Christian Szegedy. Batch normaliza-\ntion: Accelerating deep network training by reducing\ninternal covariate shift. CoRR , abs/1502.03167, 2015.\n[12] ¨Ozg¨ur˙Izmirli and Gyanendra Sharma. Bridging\nprinted music and audio through alignment using a\nmid-level score representation. In Proceedings of the\n13th International Society for Music Information Re-\ntrieval Conference , Porto, Portugal, 2012.[13] Mark S. Melenhorst, Ron van der Sterren, Andreas\nArzt, Agust ´ın Martorell, and Cynthia C. S. Liem. A\ntablet app to enrich the live and post-live experience of\nclassical concerts. In Proceedings of the 3rd Interna-\ntional Workshop on Interactive Content Consumption\n(WSICC) at TVX 2015 , 06/2015 2015.\n[14] Marius Miron, Julio Jos ´e Carabias-Orti, and Jordi\nJaner. Audio-to-score alignment at note level for or-\nchestral recordings. In Proc. of the International\nConference on Music Information Retrieval (ISMIR) ,\nTaipei, Taiwan, 2014.\n[15] Meinard M ¨uller, Frank Kurth, and Michael Clausen.\nAudio matching via chroma-based statistical features.\nInProc. of the International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , London, Great\nBritain, 2005.\n[16] Bernhard Niedermayer and Gerhard Widmer. A multi-\npass algorithm for accurate audio-to-score alignment.\nInProc. of the International Society for Music In-\nformation Retrieval Conference (ISMIR) , Utrecht, The\nNetherlands, 2010.\n[17] Matthew Prockup, David Grunberg, Alex Hrybyk, and\nYoungmoo E. Kim. Orchestral performance compan-\nion: Using real-time audio to score alignment. IEEE\nMultimedia , 20(2):52–60, 2013.\n[18] Christopher Raphael. Music Plus One and machine\nlearning. In Proceedings of the International Confer-\nence on Machine Learning (ICML) , 2010.\n[19] Jost Tobias Springenberg, Alexey Dosovitskiy,\nThomas Brox, and Martin Riedmiller. Striving for sim-\nplicity: The all convolutional net. arXiv:1412.6806 ,\n2014.\n[20] Verena Thomas, Christian Fremerey, Meinard M ¨uller,\nand Michael Clausen. Linking Sheet Music and Au-\ndio - Challenges and New Approaches. In Meinard\nM¨uller, Masataka Goto, and Markus Schedl, editors,\nMultimodal Music Processing , volume 3 of Dagstuhl\nFollow-Ups , pages 1–22. Schloss Dagstuhl–Leibniz-\nZentrum fuer Informatik, Dagstuhl, Germany, 2012.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 795"
    },
    {
        "title": "Template-Based Vibrato Analysis in Complex Music Signals.",
        "author": [
            "Jonathan Driedger",
            "Stefan Balke",
            "Sebastian Ewert",
            "Meinard Müller"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417006",
        "url": "https://doi.org/10.5281/zenodo.1417006",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/005_Paper.pdf",
        "abstract": "The automated analysis of vibrato in complex music sig- nals is a highly challenging task. A common strategy is to proceed in a two-step fashion. First, a fundamental fre- quency (F0) trajectory for the musical voice that is likely to exhibit vibrato is estimated. In a second step, the trajectory is then analyzed with respect to periodic frequency modu- lations. As a major drawback, however, such a method cannot recover from errors made in the inherently difficult first step, which severely limits the performance during the second step. In this work, we present a novel vibrato analy- sis approach that avoids the first error-prone F0-estimation step. Our core idea is to perform the analysis directly on a signal’s spectrogram representation where vibrato is evident in the form of characteristic spectro-temporal pat- terns. We detect and parameterize these patterns by locally comparing the spectrogram with a predefined set of vibrato templates. Our systematic experiments indicate that this approach is more robust than F0-based strategies.",
        "zenodo_id": 1417006,
        "dblp_key": "conf/ismir/DriedgerBEM16",
        "content": "TEMPLATE-BASED VIBRATO ANALYSIS OF MUSIC SIGNALS\nJonathan Driedger1, Stefan Balke1, Sebastian Ewert2, Meinard M ¨uller1\n1International Audio Laboratories Erlangen, Germany\n2Queen Mary University of London\n{jonathan.driedger,stefan.balke,meinard.mueller }@audiolabs-erlangen.de\nABSTRACT\nThe automated analysis of vibrato in complex music sig-\nnals is a highly challenging task. A common strategy is\nto proceed in a two-step fashion. First, a fundamental fre-\nquency (F0) trajectory for the musical voice that is likely to\nexhibit vibrato is estimated. In a second step, the trajectory\nis then analyzed with respect to periodic frequency modu-\nlations. As a major drawback, however, such a method\ncannot recover from errors made in the inherently difﬁcult\nﬁrst step, which severely limits the performance during the\nsecond step. In this work, we present a novel vibrato analy-\nsis approach that avoids the ﬁrst error-prone F0-estimation\nstep. Our core idea is to perform the analysis directly\non a signal’s spectrogram representation where vibrato is\nevident in the form of characteristic spectro-temporal pat-\nterns. We detect and parameterize these patterns by locally\ncomparing the spectrogram with a predeﬁned set of vibrato\ntemplates. Our systematic experiments indicate that this\napproach is more robust than F0-based strategies.\n1. INTRODUCTION\nThe human voice and other instruments often reveal char-\nacteristic spectro-temporal patterns that are the result of\nspeciﬁc articulation techniques. For example, vibrato is\na musical effect that is frequently used by musicians to\nmake their performance more expressive. Although a clear\ndeﬁnition of vibrato does not exist [20], it can broadly\nbe described as a musical voice’s “periodic oscillation in\npitch” [16]. It is commonly parameterized by its rate (the\nmodulation frequency given in Hertz) and its extent (the\nmodulation’s amplitude given in cents1). These parame-\nters have been studied extensively from musicological and\npsychological perspectives, often in a cumbersome process\nof manually annotating spectral representations of mono-\nphonic music signals, see for example [5, 10, 18, 20, 22].\nTo approach the topic from a computational perspec-\ntive, the signal processing community has put considerable\n1Acent is a logarithmic frequency unit. A musical semitone is subdi-\nvided into 100 cents.\nc/circlecopyrtJonathan Driedger, Stefan Balke, Sebastian Ewert,\nMeinard M ¨uller. Licensed under a Creative Commons Attribution 4.0\nInternational License (CC BY 4.0). Attribution: Jonathan Driedger,\nStefan Balke, Sebastian Ewert, Meinard M ¨uller. “Template-Based Vi-\nbrato Analysis of Music Signals”, 17th International Society for Music\nInformation Retrieval Conference, 2016.\nTime [sec]  300 600 1200 2400 4800 \n3.5 4 4.5 5 5.5 6 Frequency [Hz]  No vibrato Vibrato \nSpectrogram  Vibrato templates  𝑒 1\n𝑓 \nFigure 1 . Template-based vibrato analysis. A matching\nvibrato template lets us infer the rate fand extenteof\nvibrato present in the music signal.\nresearch efforts into developing automated vibrato analysis\nmethods for monophonic, as well as for more complex mu-\nsic signals with multiple sound sources. While some appli-\ncations implicitly exploit spectro-temporal characteristics\nof vibrato to approach higher-level tasks such as harmonic-\npercussive decomposition [9], singing voice detection [6],\nor singing voice separation [21], there also exist methods\nfor explicitly detecting and parameterizing vibrato com-\nponents in a given music signal. A common approach is\nto perform the vibrato analysis in two consecutive steps.\nIn the ﬁrst step, a fundamental frequency trajectory (F0-\ntrajectory) is estimated for the musical voice that is most\nlikely to exhibit vibrato. This trajectory is then analyzed in\nthe second step to detect and parameterize periodic mod-\nulation patterns, see for example [4, 8, 12–14, 23]. How-\never, computing F0-trajectories for complex signals with\nmultiple instruments is a highly non-trivial and error-prone\ntask by itself [15]. Therefore, a trajectory estimated in the\nﬁrst step may not appropriately reﬂect the relevant modu-\nlation patterns. This in turn renders the vibrato detection\nand parametrization in the second step problematic, if not\nimpossible.\nTo avoid the error-prone F0-estimation step, in this\nwork we propose a novel approach for automatically ana-\nlyzing vibrato components in complex music signals. Our\ncore idea is to detect spectro-temporal vibrato patterns di-239rectly in a music signal’s spectrogram by locally com-\nparing this representation with a set of predeﬁned vibrato\ntemplates2that reﬂect different vibrato rates and extents.\nThe measured similarity yields a novel mid-level feature\nrepresentation—a vibrato salience spectrogram —in which\nspectro-temporal vibrato patterns are enhanced while other\nstructures are suppressed. Figure 1 illustrates this idea,\nshowing three different vibrato templates as well as a spec-\ntrogram representation of a choir with a lead singer who\nstarts to sing with strong vibrato in the excerpt’s second\nhalf. Time-frequency bins where one of the templates is\nlocally similar to the spectrogram, thus yielding a high vi-\nbrato salience, are indicated in red. As we can see, these\ntime-frequency bins temporally coincide with the anno-\ntated vibrato passage at the top of Figure 1. Additionally,\na high vibrato salience does not only indicate the presence\nof vibrato in the music signal, but also reveals the vibrato’s\nrate and extent encoded in the similarity maximizing tem-\nplate.\nThe remainder of this paper is structured as follows. In\nSection 2 we describe our template-based vibrato analysis\napproach in detail. In Section 3, we evaluate the perfor-\nmance of our proposed method, both by means of a quanti-\ntative evaluation on a novel dataset as well as by discussing\nillustrative examples. Finally, in Section 4, we conclude\nwith an indication of possible future research directions.\nNote that this paper has an accompanying website at [2]\nwhere one can ﬁnd all audio examples and annotations\nused in this paper.\n2. TEMPLATE-BASED VIBRATO ANALYSIS\nIn this section, we describe our proposed template-based\nvibrato analysis approach. We discuss relevant spectro-\ngram representations (Section 2.1) and describe how the\nvibrato templates are modeled (Section 2.2). Both our\nchoice of spectrogram representation and the vibrato tem-\nplate’s design are motivated by the correlation-like similar-\nity measure that we use to locally compare the templates\nwith the spectrogram. We then introduce the derivation of\nthe vibrato salience spectrogram (Section 2.3) and com-\nment on our approach’s computational complexity (Sec-\ntion 2.4). As a running example, we use the choir signal\nfrom Figure 1.\n2.1 Spectral Representation\nGiven a discrete music signal x:Z→R, we ﬁrst compute\ntheshort-time Fourier transform (STFT)X:Z×Z→C\nofxby\nX(m,k) =/summationdisplay\nr∈Zw(r)·x(r+mH)·exp(−2πikr/N ),(1)\nwheremis the frame index, kis the frequency index, N\nis the frame length, wis a window function, and His the\n2Note that this approach is conceptually similar to the Hough trans-\nform [3], a mathematical tool known from image processing for the de-\ntection of parameterized shapes in binary images. However, the Hough\ntransform is known to be very sensitive to noise and therefore not suitable\nfor detecting vibrato patterns in spectrograms that are commonly rather\nnoisy.\nTime [sec]  3.5 4 4.5 5 5.5 6 500 1000 2000 \n500 1000 2000 4000 0 2 4 6 \n0 1 Frequency [Hz]  Frequency [Hz]  \n(b) \n(c) 500 1000 2000 4000 \n0 2 4 6 Frequency [Hz]  \n(a) \n100 cent  4000 Figure 2 . Spectrogram representations of the input sig-\nnalx.(a):Magnitude spectrogram. (b): Log-frequency\nspectrogram. (c):Binarized log-frequency spectrogram Y.\nhopsize (w.l.o.g. we assume m,k∈Z). Figure 2a shows\nan excerpt of our example signal’s magnitude spectrogram\n|X|where one can clearly see wave-like vibrato patterns\nin the lead singer’s fundamental frequency and its over-\ntones. However, due to the STFT’s linear frequency sam-\npling, the vibrato patterns’ amplitudes increase with higher\novertones.\nIn the context of our template-based analysis it is de-\nsirable that vibrato patterns stemming from the same fre-\nquency modulated tone have the same amplitude that di-\nrectly reﬂects the vibrato’s extent. We therefore compute\nalog-frequency spectrogram from the STFT X, using a\nphase vocoder-based reassignment approach as discussed\nin [7, Chapter 8] or [14]. In this representation, which can\nbe seen in Figure 2b, frequency bands are spaced logarith-\nmically and have a constant logarithmic bandwidth speci-\nﬁed in cents. This ensures the desired property in this spec-\ntrogram representation.\nIn a last step, we normalize the spectrogram in order to\nachieve two goals. First, we aim to make the representa-\ntion independent of the signal’s volume such that we can\nalso detect vibrato in quiet signal passages. Second, when\nlocally comparing our vibrato templates with the represen-\ntation, the resulting similarity measure should yield values\nin a ﬁxed range. A method that showed to be simple and\neffective to achieve both goals is spectrogram binarization,\nwhere we set the ten percent highest values of each frame\nin the log-frequency spectrogram to one and all remaining\nvalues to zero. This yields a binarized log-frequency spec-\ntrogramY:Z×Z→{0,1}, see Figure 2c. In our exper-\niments, we choose parameters such that Yhas a time res-240 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20160 0.015 \n-0.015 \nTime [sec]  0 \nFrequency [cent]  \n50 \n-50 0 \n𝑒 1\n𝑓 \n(b) (a) \nℓ \nFrequency [cent]  \n50 \n-50 0 \n0.3 0.1 0.4 0.2 \nFigure 3 . Generation of a vibrato template Twith a vi-\nbrato ratef= 5Hertz, extent e= 50 cent, and a duration\nof/lscript= 0.4seconds. (a):Sinusoidal vibrato trajectory s.\n(b):Vibrato template T.\nolution of roughly 150 frames per second and a frequency\nresolution of ten bands per semitone.\n2.2 Vibrato Templates\nNext, we introduce a set Tof templates that reﬂect spectro-\ntemporal vibrato patterns as expected in Y. Let us model\nsuch a template T∈ T for vibrato having a rate of f\nHertz, an extent of ecents, and a duration of at least /lscriptsec-\nonds. When locally comparing the template TwithY, one\nshould obtain high similarity values when Tis aligned with\na matching spectro-temporal vibrato pattern in Yand low\nvalues otherwise. The idea is therefore to have a positive\nportion inTthat reﬂects the spectro-temporal vibrato pat-\ntern as well as a negative portion that prevents the template\nfrom correlating well with regions in Ythat are homoge-\nneously equal to one.\nAssuming a sinusoidal vibrato, we can describe the vi-\nbrato’s trajectory (up to phase) by\ns(t) =esin(2πft), (2)\nt∈[0,/lscript]. Figure 3a shows such a trajectory for f=\n5Hertz,e= 50 cent, and/lscript= 0.4seconds. The trajec-\ntory is then discretized such that its time- and frequency\nresolution matches the binarized log-frequency spectro-\ngram. Time-frequency bins that are close to sare assigned\nwith positive values, while bins having a certain distance\nfromsget negative values. To allow for some tolerance\nof the width of vibrato patterns in Y, the remaining time-\nfrequency bins are deﬁned to be zero. Finally, positive and\nnegative entries in Tare normalized to sum up to one and\nminus one, respectively, see Figure 3b.\n2.3 Vibrato Salience\nIn order to locate and parameterize vibrato structures in\nthe binarized log-frequency spectrogram Y, we aim to\ncompute a vibrato salience spectrogram S—a kind of\nmid-level feature representation—in which vibrato struc-\ntures are enhanced while other kinds of structures are sup-\npressed. To this end, we deﬁne the vibrato salience spec-\ntrogramSTfor a single vibrato template T: [0 :A−1]×\n𝑚 𝑘 \n  \n0 \n-1 1 \n(b) (a) \n0 1 \n𝐴−1 \n0 \n𝐵−1 0 \nFigure 4 . Vibrato salience spectrogram computation.\n(a):Process to compute ST. The similarity-maximizing\nshift(µ,κ)that maps (m,k)onto an index pair in Iis indi-\ncated by a green arrow. (b):Vibrato salience spectrogram\nS.\n[0 :B−1]→R,A,B∈N. The computation process is\nillustrated in Figure 4a. Let Ibe the set of all index pairs\n(a,b)∈[0 :A−1]×[0 :B−1]such thatT(a,b)is positive\n(the indices of all red entries in Figure 3b). Furthermore,\nlet\nY(µ,κ)(m,k) =Y(m−µ,k−κ), (3)\nµ,κ∈Z, denote a version of Ythat is shifted by µandκ\nindices in time- and frequency direction, respectively. In-\ntuitively, the vibrato salience ST(m,k)should be high if\nY(m,k)is part of a spectro-temporal vibrato pattern as\nreﬂected by T. To this end, we verify if there is a shift\n(µ,κ)that alignsY(m,k)(red dot in Figure 4a) with one\nof the positive entries in the vibrato template Tsuch that\nTandY(µ,κ)are similar (the optimal shift for our exam-\nple in Figure 4a is indicated by a green arrow). To compute\nST(m,k), we therefore maximize the correlation-like sim-\nilarity measure\nc(T,Y) =A−1/summationdisplay\na=0B−1/summationdisplay\nb=0T(a,b)Y(a,b) (4)\nover all shifts (µ,κ)that map (m,k)onto one of the index\npairs inI:\nST(m,k) = max\n{(µ,κ):(m,k)−(µ,κ)∈I}c(T,Y(µ,κ)).(5)\nThe full vibrato salience spectrogram can then be com-\nputed by maximizing over all vibrato templates T∈T:\nS(m,k) = max\nT∈TST(m,k). (6)Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 241-0 dB -5 dB -10 dB\nItem name Lx Lvib TB-A F0-M TB-A F0-M TB-A F0-M BL\nSound On Sound Demo—Mystery 9.79 1.78 0.83 0.93 0.84 0.86 0.73 0.30 0.31\nGiselle—You 5.12 2.99 0.91 0.94 0.91 0.88 0.86 0.53 0.73\nLeaf—Full 5.36 1.64 0.84 0.86 0.74 0.29 0.82 0.00 0.46\nPhre The Eon—Everybody is Falling Apart 2.47 0.47 0.98 0.97 0.96 0.97 0.95 0.00 0.32\nSecretariat—Borderline 7.69 1.98 0.79 0.69 0.73 0.76 0.79 0.00 0.41\nSunshine Garcia Band—For I Am The Moon 12.54 3.36 0.63 0.73 0.67 0.62 0.74 0.44 0.42\nAngela Thomas Wade—Milk Cow Blues 4.50 2.10 0.44 0.82 0.32 0.63 0.32 0.00 0.63\nTriviul—Dorothy 5.22 0.85 0.77 0.88 0.73 0.85 0.65 0.00 0.28\nFunny Valentines—Sleigh Ride 7.18 0.00 1.00 1.00 1.00 1.00 1.00 1.00 0.00\n∅ 6.65 1.69 0.80 0.87 0.77 0.76 0.76 0.25 0.39\nTable 1 . Quantitative evaluation (F-measure), comparing our proposed template-based detection approach TB-A , F0-based\nvibrato detection F0-M (manual vibrato selection in F0-trajectories), and a baseline BL. Lengths of the signals ( Lx) and\naccumulated lengths of ground truth vibrato passages ( Lvib) are given in seconds.\nFigure 4b shows the vibrato salience spectrogram Sre-\nsulting from the binarized log-frequency spectrogram Y\nshown in Figure 4a. Note that by the vibrato template’s de-\nsign andY(m,k)∈{0,1}, one obtains S(m,k)∈[−1,1]\nfor allm,k∈Z. While the vibrato structures present in\nYare also clearly visible in S, the horizontal structures as\nwell as the glissando at the excerpt’s beginning do not cor-\nrelate well with the vibrato templates. They are therefore,\nas intended, suppressed in S.\n2.4 Computational Complexity\nThe vibrato salience spectrogram’s derivation as deﬁned in\nthe previous section is a computationally expensive pro-\ncess. When implemented naively, it is necessary to use\na quadruply nested loop to iterate over all combinations of\ntime-frequency bins (m,k)inY, vibrato templates T∈T,\nindex shifts{(µ,κ) : (m,k)−(µ,κ)∈ I} , and index\npairs (a,b)inT. However, note that many computations\nare redundant and that it is therefore possible to optimize\nthe calculation process, for example by exploiting two-\ndimensional convolutions. Furthermore, one can speed\nup the derivation by considering only a limited frequency\nrange inYas well as by applying further heuristics such\nas only taking into account vibrato salience values above a\nthresholdτ∈[−1,1]. Although still being computation-\nally demanding, the derivation therefore becomes feasible\nenough to be used in practice. For example, deriving S\nfor a music signal with a duration of 60 seconds takes our\nMATLAB implementation roughly 40 seconds on a stan-\ndard computer.\n3. EXPERIMENTS\nIn this section, we present our experimental results. In\nSection 3.1, we quantitatively evaluate our proposed ap-\nproach in the context of a vibrato detection task. Then,\nin Section 3.2 we demonstrate the method’s potential for\nautomatically analyzing vibrato rate and extent. Finally,\nin Section 3.3, we indicate open challenges and potential\nsolutions.3.1 Evaluation: Vibrato Detection\nIn a ﬁrst experiment, we considered the task of temporally\nidentifying vibrato passages in a music signal. We there-\nfore compiled a dataset of nine items (see Table 1), which\nare excerpts of music signals from the “Mixing Secrets”\nmultitrack dataset [17]. Each item consists of a mono-\nphonic vocal signal xvocand a polyphonic accompaniment\nsignalxacc. Annotations of vibrato passages in the vocal\nsignals were created manually to serve as ground truth for\nthe subsequent evaluation (none of the accompaniment sig-\nnalsxacchas vibrato). To vary the difﬁculty of the vibrato\ndetection task, we created three different mixes for each\nof the items—one were xvocandxaccwere mixed with-\nout modiﬁcation (-0 dB), one were xvocwas attenuated by\n-5 dB prior to mixing the signals, and a third mix with xvoc\nbeing attenuated by -10 dB.\nTo construct an automated vibrato detection procedure\nbased on our proposed template-based analysis approach,\nwe ﬁrst computed vibrato salience spectrograms Sfor all\nof the resulting 27 mix signals. Since only high vibrato\nsalience values in Sare likely to indicate the presence of\nspectro-temporal vibrato patterns, we then chose a thresh-\noldτ∈[−1,1]. Time instances where the maximal vibrato\nsalience in a frame exceeded τwere then labeled as having\nvibrato while all other time instances were labeled as hav-\ning no vibrato. For this experiment we used a set Tof 30\ntemplates, reﬂecting vibrato rates from ﬁve to seven Hertz\nin steps of 0.5 Hertz, as well as extents from 50 to 100\ncents in steps of 10 cents. These parameters were chosen\nparticularly to detect the vibrato in singing voice as these\nare typical vibrato rates and extents for human singing,\nsee [10, 11]. All templates had a length corresponding to\n/lscript= 0.4seconds. The threshold τwas experimentally set\ntoτ= 0.55, yielding good vibrato detection results for all\nitems in the dataset.\nOne of this experiment’s main objectives was to com-\npare our template-based method’s performance with F0-\nbased strategies as discussed in Section 1. To emulate\nsuch an approach, we used MELODIA [14]—a state-of-\nthe-art algorithm for estimating F0-trajectories of predom-\ninant musical voices in complex music signals—to esti-242 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016mate trajectories for all mix signals. Instead of automat-\nically analyzing the extracted trajectories in a second step,\nwe then manually inspected them for passages that reﬂect\nvibrato. This was done to obtain an upper bound on the\nperformance an automated procedure could achieve in this\nsecond step when detecting vibrato solely based on the es-\ntimated F0-trajectory.\nWe then computed precision ( P), recall ( R), and F-\nmeasure ( F) for the detection results of our automated\ntemplate-based procedure ( TB-A ), for the procedure based\non the manually inspected F0-trajectory ( F0-M ), as well\nas for a baseline approach that simply labels every time\ninstance as having vibrato ( BL):\nP =TP +/epsilon1\nTP + FP + /epsilon1,R =TP +/epsilon1\nTP + FN + /epsilon1,F =2PR\nP + R.(7)\nHere, TPis the number of true positives, FPthe number\nof false positives, FNthe number of false negatives, and\n/epsilon1 >0∈Ris some small number to prevent division by\nzero. Note that all music signals and annotations used in\nthe experiment can be found at this paper’s accompanying\nwebsite [2].\nThe evaluation’s results are summarized in Table 1\nwhich shows for each item its name, the music signal’s\nlength, the accumulated duration of vibrato in this signal,\nas well as the F-measures of TB-A andF0-M for the three\ndifferent mixes (-0 dB, -5 dB, and -10 dB). The F-measure\nfor the baseline BLis indicated in the last column and the\ntable’s last row indicates mean values. Here we can ob-\nserve a clear trend. For mixes where xvocwas not at-\ntenuated (-0 dB), both TB-A andF0-M yield average F-\nmeasures ( F = 0.80andF = 0.87) clearly above the\nbaseline BL(F = 0.39). For this mixing condition, F0-M\noutperforms our template-based approach. However, recall\nthatF0-M constitutes an upper bound on the performance\nof F0-based vibrato detection approaches. Automating the\nvibrato detection step may therefore result in lower scores.\nFor mixes where xvocwas attenuated by -5 dB, the av-\nerage F-measure of TB-A only slightly decreases to F =\n0.77, while the performance of F0-M drops to F = 0.76.\nThis tendency becomes even more extreme when consider-\ning vocal signals attenuated by -10 dB where TB-A ’s per-\nformance stays almost constant ( F = 0.76) while F0-M ’s\naverage F-measure goes down to F = 0.25, many of the\nindividual items scoring F-measures of zero.\nThe reason for this trend becomes obvious when inves-\ntigating individual items. Figure 5 depicts the vibrato de-\ntection results of both TB-A andF0-M in all mixing con-\nditions for the item Leaf—Full . In the condition -0 dB,\nthe results of TB-A (Figure 5a) and F0-M (Figure 5b)\ncoincide well with the ground truth (Figure 5c), leading\nto high F-measures ( F = 0.84andF = 0.86). Here,\nour template-based analysis approach detects most of the\nspectro-temporal vibrato patterns in the signal’s spectro-\ngram (time-frequency bins where the vibrato salience ex-\nceeds the threshold τare indicated in red in Figure 5a).\nF0-M also achieves a good result since the F0-trajectory\nextracted by MELODIA (indicated in blue in Figure 5b)\ncaptures the singing voice’s fundamental frequency well\n300 600 1200 2400 300 600 1200 2400 \n3.5 4 4.5 5 3.5 4 4.5 5 3.5 4 4.5 5 \nTime [sec]  Time [sec]  Time [sec]  Frequency [Hz]  Frequency [Hz]  \n(a) \n(b) \n(c) -0 dB  -5 dB  -10 dB  \nVibrato  Vibrato  Vibrato  Figure 5 . Comparison of TB-A andF0-M for the item\nLeaf—Full .(a):TB-A . Automatically derived vibrato pas-\nsages are indicated in red. (b):F0-M . Manually annotated\nvibrato passages in the trajectory are indicated in blue.\n(c):Ground truth annotation.\nin this mix. However, this changes when attenuating the\nvocal signal by -5 dB. While TB-A still identiﬁes many\nvibrato patterns, therefore detecting the vibrato present in\nthe mix ( F = 0.74), the F0-estimation becomes problem-\natic and MELODIA retrieves only a small segment of the\nsinging voice’s F0-trajectory correctly, leading to a poor\nvibrato detection ( F = 0.29). When attenuating xvoc\nby -10 dB, the F0-trajectory’s estimation fails completely\n(F = 0.00) since MELODIA’s assumption of a predom-\ninant melodic voice is violated. On the other hand, our\nproposed detection procedure is capable of detecting the\nvibrato in the mix.\nAs a ﬁnal remark, note that our proposed approach also\nsucceeds to recognize that the item Funny Valentines—\nSleigh Ride does not contain any vibrato at all.\n3.2 Evaluation: Vibrato Analysis\nAs we have seen in the previous section, the vibrato\nsalience spectrogram Scan be used to determine when vi-\nbrato is present in a music signal. Additionally, when com-\nputingS, we also implicitly obtain information about the\nvibrato’s parameters. The rate and extent of vibrato present\nin the music signal are encoded by the similarity maximiz-\ning vibrato templates Tin Equation (6). In Figure 6a, we\nsee the log-frequency spectrogram of a mixture of piano\nmusic (no vibrato) and three consecutive artiﬁcial vibrato\ntones. The tones have vibrato rates of seven, ﬁve, and ten\nHertz and extents of 40, 200, and 70 cents, respectively.\nTime-frequency bins where the vibrato salience exceeds\nτare indicated in red. Note that for this experiment we\nused a much larger template set T, consisting of 285 tem-Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 243Rate [Hz]  Extent [cent]  Frequency [Hz]  \n320 640 1280 2560 \n4 6 8 10 \n50 100 150 200 \n0 1 2 3 4 5 \nTime [sec]  (a) \n(b) \n(c) Figure 6 . Vibrato rate and extent analysis. (a): Log-\nfrequency spectrogram. Time-frequency bins (m,k)with\nS(m,k)>τare indicated in red. (b)/(c): Vibrato rate and\nextent of the template Twith the highest vibrato salience\nper frame.\nplates that reﬂected vibrato rates from four to eleven Hertz\nin steps of 0.5 Hertz, as well as extents from 30 to 210 cents\nin steps of 10 cents. Figures 6b/c indicate the vibrato rate\nand extent of the vibrato template Tthat maximized the\nvibrato salience per frame. The two plots correctly reﬂect\nthe tones’ vibrato rates and extents, while showing only a\nfew outliers. Note that values in the plots are quantized\nsince our approach can only give estimates for rates and\nextents as they are reﬂected by one of the templates in T.\nThis kind of vibrato analysis could be helpful in scenarios\nlike informed instrument identiﬁcation when it is known\nthat different instruments in a music signal perform with\ndifferent vibrato rates or extents.\n3.3 Challenges\nIn general, our proposed procedure yields useful analysis\nresults for the music examples discussed in the previous\nsections. We now want to discuss a few difﬁcult examples.\nOne potential source for incorrect analysis results are\nfalse positives as visualized in Figure 7a, which shows\na log-frequency spectrogram excerpt of Sunshine Garcia\nBand—For I Am The Moon from our dataset. In this ex-\ncerpt, one of our vibrato templates Tis similar enough\n(with respect to our similarity measure) to a non-vibrato\nspectro-temporal pattern to yield vibrato salience values\nabove the threshold τ. This could cause incorrect vibrato\ndetection results or meaningless vibrato parametrizations.\nHowever, we experienced such spurious template matches\nto often occur in an isolated fashion. Here, one could ex-\nploit additional cues such as multiple template matches at\nthe same time instance due to overtone structures of instru-\nments to reinforce the vibrato analysis’ results.\nTime [sec]  \n3.5 4 4.5 5 \n600 1200 2400 Frequency [Hz]  (b) (a) \n2 2.5 3 3.5 180 360 720 \n90 \nTime [sec]  Figure 7 . Error sources for our template-based vibrato\nanalysis. (a): Spurious template matches. (b): Vibrato\ndoes not have a sinusoidal form.\nThe opposite situation is visualized in Figure 7b. It\nshows a log-frequency spectrogram excerpt of “Gute\nNacht”, a song from Schubert’s “Winterreise” for piano\nand tenor. In this excerpt, the singer sings a long note with\nstrong vibrato. However, although there is a template re-\nﬂecting an appropriate vibrato rate and extent in our tem-\nplate setT, the vibrato is not detected by our procedure.\nThis is the case since by our vibrato template’s design—as\ndescribed in Section 2.2—we generally assumed vibrato to\nhave a sinusoidal spectro-temporal structure. This assump-\ntion is violated in the shown vibrato pattern. However, our\napproach is conceptually not limited to sinusoidal vibrato\ntemplates and one could further improve the templates’ de-\nsign in order to also capture these kind of vibrato patterns.\n4. CONCLUSIONS AND FUTURE WORK\nIn this paper we presented a novel approach for analyzing\nvibrato in complex music signals. By locally comparing a\nsignal’s spectrogram with a set of predeﬁned vibrato tem-\nplates, we derived a vibrato salience spectrogram—a kind\nof mid-level feature representation—in order to locate and\nparameterize spectro-temporal vibrato patterns. Our ap-\nproach has the advantage that the analysis does not rely on\nthe estimation of a (possibly erroneous) F0-trajectory. Ex-\nperiments indicated that our proposed procedure allows for\na more robust vibrato detection than F0-based approaches,\nin particular for complex music signals.\nIn future work we would like to further explore the use\nof vibrato templates in various application scenarios. For\nexample, deriving spectral masks from the vibrato salience\nspectrogram Scould open up novel ways of decomposing\na music signal into vibrato and non-vibrato components.\nFurthermore, we believe that the use of vibrato templates\ncould be beneﬁcial for tasks like F0-tracking [14, 19] or\nperformance analysis [1].\nAcknowledgments:\nThis work has been supported by the German Re-\nsearch Foundation (DFG MU 2686/6-1). The Inter-\nnational Audio Laboratories Erlangen are a joint insti-\ntution of the Friedrich-Alexander-Universit ¨at Erlangen-\nN¨urnberg (FAU) and Fraunhofer Institut f ¨ur Integrierte\nSchaltungen IIS. Sebastian Ewert is funded by the EPSRC\n(EP/L019981/1).244 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20165. REFERENCES\n[1] Jakob Abeßer, Hanna M. Lukashevich, and Gerald\nSchuller. Feature-based extraction of plucking and ex-\npression styles of the electric bass guitar. In Proceed-\nings of the IEEE International Conference on Acous-\ntics, Speech, and Signal Processing (ICASSP) , pages\n2290–2293, Dallas, Texas, USA, March 2010.\n[2] Jonathan Driedger, Stefan Balke, Sebas-\ntian Ewert, and Meinard M ¨uller. Accompa-\nnying website: Towards template-based vi-\nbrato analysis in complex music signals.\nhttp://www.audiolabs-erlangen.de/\nresources/MIR/2016-ISMIR-Vibrato/ .\n[3] K. Glossop, P. J. G. Lisboa, P. C. Russell, A. Sid-\ndans, and G. R. Jones. An implementation of the hough\ntransformation for the identiﬁcation and labelling of\nﬁxed period sinusoidal curves. Computer Vision and\nImage Understanding , 74(1):96–100, 1999.\n[4] Perfecto Herrera and Jordi Bonada. Vibrato extrac-\ntion and parameterization in the spectral modeling syn-\nthesis framework. In Digital Audio Effects Workshop\n(DAFX98) , Barcelona, Spain, November 1998.\n[5] Yoshiyuki Horii. Acoustic analysis of vocal vibrato:\nA theoretical interpretation of data. Journal of Voice ,\n3(1):36–43, 1989.\n[6] Bernhard Lehner, Gerhard Widmer, and Reinhard\nSonnleitner. On the reduction of false positives in\nsinging voice detection. In Proceedings of the IEEE In-\nternational Conference on Acoustics, Speech, and Sig-\nnal Processing (ICASSP) , pages 7480–7484, Florence,\nItaly, 2014.\n[7] Meinard M ¨uller. Fundamentals of Music Processing .\nSpringer Verlag, 2015.\n[8] Hee-Suk Pang. On the use of the maximum likelihood\nestimation for analysis of vibrato tones. Applied Acous-\ntics, 65(1):101–107, 2004.\n[9] Jeongsoo Park and Kyogu Lee. Harmonic-percussive\nsource separation using harmonicity and sparsity con-\nstraints. In Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 148–154, M ´alaga, Spain, 2015.\n[10] Eric Prame. Measurements of the vibrato rate of ten\nsingers. The Journal of the Acoustical Society of Amer-\nica (JASA) , 96(4):1979–1984, 1994.\n[11] Eric Prame. Vibrato extent and intonation in pro-\nfessional western lyric singing. The Journal of the\nAcoustical Society of America (JASA) , 102(1):616–\n621, 1997.\n[12] Lise Regnier and Geoffroy Peeters. Singing voice de-\ntection in music tracks using direct voice vibrato de-\ntection. In Proceedings of the IEEE International Con-\nference on Acoustics, Speech, and Signal Processing\n(ICASSP) , pages 1685–1688, Taipei, Taiwan, 2009.[13] S. Rossignol, P. Depalle, J. Soumagne, X. Rodet, and\nJ.-L. Collette. Vibrato: detection, estimation, extrac-\ntion, modiﬁcation. In Proceedings of the International\nConference on Digital Audio Effects (DAFx) , Trond-\nheim, Norway, December 1999.\n[14] Justin Salamon and Emilia G ´omez. Melody extrac-\ntion from polyphonic music signals using pitch contour\ncharacteristics. IEEE Transactions on Audio, Speech,\nand Language Processing , 20(6):1759–1770, 2012.\n[15] Justin Salamon, Emilia G ´omez, Daniel P. W. Ellis,\nand Ga ¨el Richard. Melody extraction from polyphonic\nmusic signals: Approaches, applications, and chal-\nlenges. IEEE Signal Processing Magazine , 31(2):118–\n134, 2014.\n[16] Carl E. Seashore. The natural history of the vibrato.\n17(12):623–626, 1931.\n[17] Mike Senior. Mixing secrets for the small studio—\nadditional resources. www.cambridge-mt.com/\nms-mtk.htm . Web resource, last consulted in Jan-\nuary 2016.\n[18] T. Shipp, R. Leanderson, and J. Sundberg. Some acous-\ntic characteristics of vocal vibrato. Journal of Research\nin Singing , IV(1):18–25, 1980.\n[19] Fabian-Robert St ¨oter, Nils Werner, Stefan Bayer, and\nBernd Edler. Reﬁning fundamental frequency esti-\nmates using time warping. In Proceedings of EU-\nSIPCO 2015 , Nice, France, September 2015.\n[20] Johan Sundberg. Acoustic and psychoacoustic aspects\nof vocal vibrato. STL-QPSR , 35(2-3):45–68, 1994.\n[21] Hideyuki Tachibana, Nobutaka Ono, and Shigeki\nSagayama. Singing voice enhancement in monaural\nmusic signals based on two-stage harmonic/percussive\nsound separation on multiple resolution spectrograms.\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing , 22(1):228–237, January 2014.\n[22] Renee Timmers and Peter Desain. Vibrato: Questions\nand answers from musicians and science. In Proceed-\nings of the International Conference on Music Percep-\ntion and Cognition (ICMPC) , volume 2, 2000.\n[23] Luwei Yang, Elaine Chew, and Khalid Z. Rajab. Vi-\nbrato performance style: A case study comparing erhu\nand violin. In Proceedings of the International Con-\nference on Computer Music Modeling and Retrieval\n(CMMR) , 2013.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 245"
    },
    {
        "title": "Downbeat Detection with Conditional Random Fields and Deep Learned Features.",
        "author": [
            "Simon Durand",
            "Slim Essid"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417739",
        "url": "https://doi.org/10.5281/zenodo.1417739",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/213_Paper.pdf",
        "abstract": "In this paper, we introduce a novel Conditional Random Field (CRF) system that detects the downbeat sequence of musical audio signals. Feature functions are computed from four deep learned representations based on harmony, rhythm, melody and bass content to take advantage of the high-level and multi-faceted aspect of this task. Downbeats being dynamic, the powerful CRF classification system al- lows us to combine our features with an adapted temporal model in a fully data-driven fashion. Some meters being under-represented in our training set, we show that data augmentation enables a statistically significant improve- ment of the results by taking into account class imbalance. An evaluation of different configurations of our system on nine datasets shows its efficiency and potential over a heuristic based approach and four downbeat tracking algo- rithms.",
        "zenodo_id": 1417739,
        "dblp_key": "conf/ismir/DurandE16",
        "content": "DOWNBEAT DETECTION WITH CONDITIONAL RANDOM FIELDS AND\nDEEP LEARNED FEATURES\nSimon Durand, Slim Essid\nLTCI, CNRS, T ´el´ecom ParisTech\nUniversit ´e Paris-Saclay, 75013, Paris, France\nsimon.durand@telecom-paristech.fr\nABSTRACT\nIn this paper, we introduce a novel Conditional Random\nField (CRF) system that detects the downbeat sequence\nof musical audio signals. Feature functions are computed\nfrom four deep learned representations based on harmony,\nrhythm, melody and bass content to take advantage of the\nhigh-level and multi-faceted aspect of this task. Downbeats\nbeing dynamic, the powerful CRF classiﬁcation system al-\nlows us to combine our features with an adapted temporal\nmodel in a fully data-driven fashion. Some meters being\nunder-represented in our training set, we show that data\naugmentation enables a statistically signiﬁcant improve-\nment of the results by taking into account class imbalance.\nAn evaluation of different conﬁgurations of our system\non nine datasets shows its efﬁciency and potential over a\nheuristic based approach and four downbeat tracking algo-\nrithms.\n1. INTRODUCTION\nMusical rhythm can often be organized in several hierar-\nchical levels. These levels don’t always correspond to mu-\nsical events and have a regular temporal interval that can\nchange over time to follow the musical tempo. One of\nthese levels is the tatum level and is at the time scale of\nonsets. The next one is often the beat level and can be in-\ntuitively understood as the hand clapping or foot tapping\nlevel. Then in several music traditions there is the bar level\nthat groups beats of different accentuation together. The\nﬁrst beat of the bar is called the downbeat. The aim of this\nwork is to automatically ﬁnd the downbeat positions from\nmusical audio signals. The downbeat is useful to musi-\ncians, composers and conductors to segment, navigate and\nunderstand music more easily. Its automatic estimation is\nalso useful for various applications in music information\nretrieval, computer music and computational musicology.\nThis task is receiving more attention recently. With\nthe increasing number of annotated music ﬁles and reﬁned\nlearning strategies, methods using probabilistic models and\nc/circlecopyrtSimon Durand, Slim Essid. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: Simon Durand, Slim Essid. “Downbeat Detection with Conditional\nRandom Fields and Deep Learned Features”, 17th International Society\nfor Music Information Retrieval Conference, 2016.machine learning algorithms tend to be the most success-\nful [4, 13, 18]. Once a downbeat detection function has\nbeen extracted, most systems use a temporal model to take\nadvantage of the structured organization of downbeats and\noutput the downbeat sequence. It includes heuristics [3],\ndynamic programming [25], hidden Markov models [23]\nand particle ﬁlters [18] among others.\nIn this work, we propose for the ﬁrst time a Conditional\nRandom Field (CRF) framework for the task of downbeat\ntracking. First, four complementary features related to har-\nmony, rhythm, melody and bass content are extracted and\nthe signal is segmented at the tatum level. Adapted convo-\nlutional neural networks (CNN) to each feature character-\nistics are then used for feature learning. Finally, a feature\nrepresentation concatenated from the last and/or penulti-\nmate layer of those networks is used to deﬁne observation\nfeature functions and is fed into a Markovian form of CRF\nthat will output the downbeat sequence.\n1.1 Related work\nA CRF framework is used in [7] and [16] for the ﬁeld of\nbeat tracking. However, the optimal weights of the ob-\nservations and transitions feature functions are not directly\nlearned from the data.\nThe system presented in [14] uses an interesting idea\nof limiting engineered hypotheses by segmenting the data\nin onsets and learning the activation of downbeats with a\nSupport Vector Machine classiﬁer. Contrary to our work,\nit requires manual annotation of either the ﬁrst part of the\ntested song or of a very similar song and outputs an in-\ntermediary downbeat activation function as opposed to the\nﬁnal downbeat positions.\nIn [6], the same segmentation, low-level feature extrac-\ntion and complementary CNNs are used. However, the\nproposed system includes three main differences:\n•We are not only using an individual output per down-\nbeat candidate but a detailed high-level representa-\ntion also coming from the penultimate layer of the\nneural networks. Besides, we don’t optimize indi-\nvidual features on isolated downbeat occurrences,\nbut features from all the deep networks simultane-\nously on a whole structured downbeat sequence.\n•We are using another type of classiﬁer, namely CRF,\nknown to be more effective than Hidden Markov386Models especially in high dimensional settings due\nto being a discriminative classiﬁer.\n•A fully data driven approach, after extracting low-\nlevel features, is adopted. It takes advantage of data\naugmentation procedures to allow for a proper train-\ning of the CRF classiﬁers, limiting the use of ad-hoc\nheuristics and hand-crafted data transformations.\n2. FEATURE LEARNING\nThe feature learning part of our system is the same as\nin [6]. We ﬁrst segment the audio signal in tatums as seen\nin ﬁgure 1. We then simplify the downbeat detection task\nto a classiﬁcation problem where the goal is to ﬁnd which\ntatums are at a downbeat position. Human perception of\ndownbeats depending on several musical cues, we then\nextract four low-level features related to melody, rhythm,\nharmony and bass content. Each low-level feature input,\nshown in ﬁgure 2, is fed to a convolutional neural net-\nwork adapted to its characteristics. The bass content neu-\nral network (BCNN) targets melodic and percussive bass\ninstruments. The melodic neural network (MCNN) targets\nrelative melodic patterns which are known to play a role\nin human perception of meter regardless of their absolute\npitch [29] with max pooling. The harmonic neural network\n(HCNN) learns how to detect harmonic change in the in-\nput and is trained on all different harmony transposition by\ndata augmentation. Finally, the rhythmic neural network\n(RCNN) aims at learning length speciﬁc rhythmic patterns\nwith multi-label learning, instead of sudden changes in the\nrhythm feature that are not very indicative of a downbeat\nposition. For more details about the motivations behind the\ndesign choices made for each network the interested reader\nis referred to [6].\n3. CRF SYSTEM FOR DOWNBEAT TRACKING\nTwo high-level feature representations coming from the\nlast and penultimate layer of each network are then used\nas input to a Conditional Random Field (CRF) classiﬁer.\n3.1 CRF-based classiﬁcation\nCRF [19] are a powerful class of discriminative classi-\nﬁers for structured input–structured output data prediction,\nwhich have proven successful in a variety of real-world\nclassiﬁcation tasks [26, 27] and also in combination with\nneural networks [24]. They model directly the posterior\nprobabilities of output sequences y= (y1,···,yn)given\ninput observation sequences x= (x1,···,xn)according\nto:\np(y|x;θ) =1\nZ(x,θ)expD/summationdisplay\nj=1θjGj(x,y)\nwhereGj(x,y)are feature functions describing the ob-\nservations,θjare the model parameters (assembled as\nθ= [θj]1≤j≤D), andZ(x)is a normalizing factor that\nguarantees that p(y|x)is a well deﬁned probability, which\nsums to 1.Owing to the sequential nature of the downbeat classi-\nﬁcation problem, we use a Markovian form of CRF, where\nthe transition feature functions, denoted by tj, are deﬁned\non two consecutive labels, in a linear-chain fashion, and\nobservation feature functions, denoted by vj, depend on\nsingle labels, so that:\np(y|x;θ) =1\nZ(x,θ)exp\n\nn/summationdisplay\ni=1Do/summationdisplay\nj=1θjvj(yi,x,i)\n+n/summationdisplay\ni=1Dt/summationdisplay\nj=1θjtj(yi−1,yi,x,i)\n\n.(1)\nMore speciﬁcally, the transition feature functions we\nuse are such that tj(yi−1=k,yi=l,x,i) = I(yi=\nl)I(yi−1=k), where I(.)is the indicator function (equal\nto 1 if its argument is true and otherwise equal to 0). As\nfor the observation feature functions they are chosen to be\nof the form vj(yi=l,x,i) =ejI(yi=l)whereejare\nobtained by the feature representation learned by the net-\nworks presented in section 2. Actually, two schemes are\nenvisaged here. In the ﬁrst variant, the ejfeatures are taken\nto be directly the ﬁnal outputs of the bass, melodic, har-\nmonic and rhythmic networks. Alternatively, we added the\noutput of the penultimate layer1which can be viewed as\nlower level features that were optimized, as part of the net-\nwork training processes, to discriminate downbeats from\ntatums. The deep network penultimate layer output is a\npowerful feature representation that can be used as an in-\nput to a dedicated classiﬁer to improve accuracy [9]. The\nlast layer of our networks being essentially a linear com-\nbination of the penultimate layer features followed by a\nnormalization to map them to probabilities, the CRF clas-\nsiﬁer is a good ﬁt for the ﬁnal weighting of those features,\nbased on the more optimal output-sequence level maxi-\nmum a posteriori criterion p(y|x;θ), compared to the static\ncriterion optimized in the last layer of the networks. The\nharmonic network penultimate layer dimension is of 1000\nand each of the other networks penultimate layer dimen-\nsion is of 800.\n3.2 Deﬁning the output-space\nThe set of output labels Yj\nirepresents the position iof\na tatum in a jtatum-long bar, with i∈ {1...j}and\nj∈{3,4,5,6,7,8,9,10,12,16}. We consider an addi-\ntional label for bars containing more than 16 tatums for a\ntotal of 81 labels. This way, the feature function weights\ndepend on the bar length, in tatums, and the position in-\nside the bar. For instance, the sixth tatum of a 6 tatum-long\nbarY6\n6and the sixth tatum of a 8 tatum-long bar Y8\n6have\ndifferent musical properties. In the ﬁrst case, we want the\ntransition feature functions to emphasize the next output\nto be the ﬁrst tatum of a 6 tatum-long bar Y6\n1. In the sec-\nond case, we want to emphasize the next output to be the\nseventh tatum of a 8 tatum-long bar instead Y8\n7. The ob-\nservation features, taking into account one or two bars of\n1before the ReLU to keep information about negative unitsProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 387Audio8\nsignal\nDownbeat8\npositionsSequence8labeling\n(CRF)\nTatum8\nsegmentation\ntime8state81\nstate82\ntime8(s) 10 15210\n8\ntime8(s) 10 150.20.8\n0.5Chroma Penultimate layerOnset detection functionMelodic constant-Q transform\nPenultimate layerPenultimate layerFeature8adapted8deep8learning\nLow-frequency spectrogram Penultimate layer\nDownbeat likelihoodDownbeat likelihoodDownbeat likelihoodDownbeat likelihood\n200800\n400\ntime8(s) 10 15\nFigure 1 . Model overview. The signal is quantized in tatums. Four low-level features related to harmony, rhythm, melody\nand bass content are extracted. High-level feature representations are learned with four convolutional networks adapted to\neach feature characteristics. The networks penultimate layer, along with the downbeat likelihood, are fed in a CRF to ﬁnd\nthe downbeat sequence among the tatums.\n20 40 60 800.51.52.585LMH\n85304\n4512Chroma\n10\n85(a) (b)\n(c) (d)Onset-detection-\nfunction\nLow-frequency-\nspectrogramMelodic-constant-Q-\ntransform\nFigure 2 . Low-level features with their temporal and spec-\ntral dimension, used as input of the melodic neural network\n(a), rhythmic neural network (b), harmonic neural network\n(c) and bass content neural network (d).\ntemporal context, will also be rather different and are better\ntreated separately. It is therefore important to distinguish\nthose two outputs for a consistent decoding.\n3.3 Labeling the training data\nThe training data being only annotated in beat and down-\nbeat, deﬁning its labels is not straightforward. First, bars of\n2, 11, 13, 14 and 15 tatums are not present in our model for\nefﬁciency, robustness and because they are barely present\nin most music datasets but they can’t be ignored to train\nthe model efﬁciently. They are then annotated to the most\ncommon neighbor bar-length: 14 and 15 tatum-long bars\nare annotated as 16 tatum-long bars. 11 and 13 tatum-long\nbars are annotated as 12 tatum-long bars. The last state of\nthose metrical levels is either removed or repeated to do\nso. 2 tatum-long bars are annotated as 3 tatum-long bars if\nthe following bar is a 3 tatum-long bar for continuity or as\n4 tatum-long bar otherwise as they are the most common\nneighbor for a duple meter.\nSecond, the beginning and end of songs are sometimes\nnot properly estimated or annotated and considering or\nignoring all observations before the ﬁrst or after the lastdownbeat can lead to training problems. For the begin-\nning of songs, we removed the samples that where more\nthan one bar before the ﬁrst annotated downbeat as they\nwere not reliable enough. We then annotated the bar pre-\nceding the ﬁrst downbeat with the same classes than the\nbar containing the ﬁrst downbeat for continuity, and ﬁnally\nremoved samples in this ﬁrst bar randomly. It allows the\ninitialization of the position inside the bar to be random-\nized. The procedure is applied in reverse for the end of\nsongs.\nAlthough extensive tests were not performed, we obtain\na gain in performance by about 4 percent points (pp) by us-\ning this annotation process compared to a simple represen-\ntation of all these non conventional cases by an additional\nlabel.\n3.4 Handling class-imbalance with data augmentation\nNot all metrical level are well represented in the used\ndatasets. In fact,{3,4,6,8,12,16}tatum-long bars, i.e. bars\nof 3 and 4 beats, represent more than 96% of the data and\nwill be the focus of the CRF model. In this subset, bars\nof 3 beats will be represented by 3, 6, and roughly half of\n12 tatum-long bars. This represents approximately 15% of\nthe data. Those metrical levels are then non negligible but\nunder-represented. Such data imbalance is known to create\ndifﬁculties while training classiﬁers like CRFs. We there-\nfore balance our dataset with data augmentation. We use\ntime-stretching by a factor of 1.1 and 0.9 and pitch shift-\ning by±1 semitone on 3-beats-per-bar songs to do so. The\nimplementation is done thanks to the muda package pre-\nsented in [20]. We will study in the experiments the added\nvalue of the data augmentation.\n4. EXPERIMENTAL SETUP\n4.1 Evaluation methods\nWe use the F-measure and a statistical test to assess the\nperformance of our system:\nF-measure: The F-measure is the harmonic mean of the\nprecision (ratio of detected downbeat that are relevant)\nand the recall (the ratio of relevant downbeat detected). It\nis an instantaneous measure of performance that is used\nin the MIREX downbeat tracking evaluation2. We use a\n2http://www.music-ir.org/mirex/wiki/2016:388 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016tolerance window of ±70ms. The conﬁguration with the\nbest F-measure with be highlighted in bold. We do not\ntake into account the ﬁrst 5 seconds and last 3 seconds\nof audio in our evaluation metric since the annotation is\nsometimes missing or not very reliable there.\nStatistical tests: To assess statistical signiﬁcance, we per-\nform a Friedman’s test and a Tukey’s honestly signiﬁcant\ncriterion (HSD) test with a 95% conﬁdence interval. Sys-\ntem(s) with a statistically signiﬁcant improvement over the\nrest on the whole dataset will be underlined.\n4.2 Databases\nWe use nine different databases in this work, for a total of\n1511 audio tracks of about 43 hours of audio music. Using\nmultiple datasets allows us to see the performance of our\nsystem on different music styles and be robust to different\nannotation strategies.\nRWC Classical [10]: 60 western classical pieces, from 1\nto 10 minutes. We removed the last track as the annotation\nseemed inconsistent.\nRWC Jazz [10]: 50 jazz tracks from 2 to 7 minutes.\nRWC music genre [11]: 92 music tracks from various\nmusic styles, from 1 to 10 minutes. We removed the tradi-\ntional Japanese songs and the a Capella song as we don’t\nhave the corresponding audio.\nRWC Pop [10]: 80 Japanese Pop music and 20 American\nPop music tracks from 3 to 6 minutes.\nBeatles3:179 songs from The Beatles.\nBallroom4:698 30-second long excerpts from various\nballroom dance music.\nHainsworth [12]: 222 excerpts from 30 second to 1\nminute from various music styles. It is to note that the cur-\nrent downbeat annotation can signiﬁcantly be improved.\nKlapuri subset [15]: The downbeat annotations for this\ndataset are lacking in some ﬁles. Full cleaning will be\ndone in future work but we use a subset of 4 relatively difﬁ-\ncult genres for downbeat tracking : Jazz, Electronic music,\nClassical and Blues with 10 randomly selected excerpts for\neach genre.\nQuaero5:70 songs from various Pop, Rap and Electronic\nmusic hits.\n4.3 General train/test procedure\nWe use a leave-one-dataset-out approach, meaning that we\ntrain and validate our system on all but one dataset and\ntest it on the remaining one. Compared to standard cross-\nvalidation, this procedure was chosen to be more fair to\nnon machine learning methods that are blind to the test\nset and to supervised algorithms using the same approach.\nHowever, it is limiting the ability of the deep networks and\nAudio_Downbeat_Estimation\n3http://isophonics.net/datasets\n4http://www.ballroomdancers.com\n5http://www.quaero.orgDataset ll ll + da pl pl + da\nRWC Jazz 65.3 66.0 65.5 66.1\nRWC Class 44.3 44.3 43.8 45.9\nHainsworth 62.9 65.9 64.5 66.0\nRWC Genre 66.2 68.1 69.1 69.3\nKlapuri subset 67.1 71.2 67.4 71.5\nBallroom 78.0 77.3 79.0 80.9\nQuaero 83.5 83.8 83.1 82.7\nBeatles 84.0 84.1 84.4 85.2\nRWC Pop 87.2 85.1 86.7 87.4\nMean 70.9 71.8 71.5 72.8\nTable 1 . F-measure results for different conﬁgurations of\nthe presented system. llmeans the features come from\nthe network last layer and plmeans that features from the\npenultimate layer were also used. dameans data augmen-\ntation was used.\nthe CRF model to work on test data from styles not of-\nten seen in the training set. Two notable examples are the\nRWC Classical and RWC Jazz music datasets.\n4.4 CRF training\nFor CRF training we use the Pycrfsuite toolbox [21]. The\nCRF parameters are learned as classically done in a maxi-\nmum likelihood sense using both /lscript2and/lscript1-regularisation,\nthus in an elastic-net fashion, so as to promote sparse so-\nlutions, and solved for using the L-BFGS algorithm. The\noptimal values of the regularisation parameters were se-\nlected by a 4-fold cross-validation on the training set. For\nthe last layer features, the grid for the optimal /lscript2value\nis [100,10,1.0,0.1,0.01,0.001,0.0001]. Since there are only\nfour features out of the networks, we don’t need feature se-\nlection and the /lscript1parameter was set to 0. When adding the\npenultimate layer features, the grids for the optimal /lscript1and\n/lscript2values were [10,100] and [0.1,0.01,0.001] respectively.\n5. RESULTS AND DISCUSSION\n5.1 Impact of the data augmentation:\nConﬁguration using data augmentation will be abbrevi-\nated by ”da”, and their F-measure results for each dataset\nis shown in table 1. We can see an improvement on all\ndatasets, except on the RWC Pop and Quaero datasets. In-\ndeed, the number of songs containing 3 or 6 tatums per\nbar is very limited there. Overall the F-measure improve-\nment is of +0.9 percent point using last layer features (ab-\nbreviated by ”ll”) and of +1.3 pp using penultimate layer\nfeatures (abbreviated by ”pl”).\n5.2 Impact of the penultimate layer:\nF-measure results of conﬁgurations adding the penultimate\nlayer output as features is also shown in table 1. Using\nthe penultimate layer increases the results overall by 0.6\npp with the non augmented data and by 1.0 pp with theProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 389Dataset [23] [22] [3] [17] [6] pl + da\nRWC Jazz 39.6 47.2 42.1 51.5 70.9 66.1\nRWC Class 29.9 21.6 32.7 33.5 51.0 45.9\nHainsworth 42.3 47.5 44.2 51.7 65.0 66.0\nRWC Genre 43.2 50.4 49.3 47.9 66.1 69.3\nKlapuri 47.3 41.8 41.0 50.0 67.4 71.5\nBallroom 45.5 50.3 50.0 52.5 80.1 80.9\nQuaero 57.2 69.1 69.3 71.3 81.2 82.7\nBeatles 53.3 66.1 65.3 72.1 83.8 85.2\nRWC Pop 69.8 71.0 75.8 72.1 87.6 87.4\nMean 47.6 51.7 52.2 55.8 72.6 72.8\nTable 2 . F-measure results for compared algorithms. [23],\n[22] and [3] are unsupervised. [17] and [6] are supervised\nalgorithms also trained with a leave-one-dataset-out ap-\nproach. [6] uses the same training sets and [17] uses sim-\nilar training sets, with the addition of the Boeck [1, 2],\nRock [28] and Robbie Williams [8] datasets and the sub-\ntraction of the Klapuri subset and the Quaero dataset.\naugmented data. Its impact on the bigger datasets (Ball-\nroom, Beatles, RWC Pop, RWC Genre, Hainsworth), rep-\nresenting 85% of the songs is more important than for the\nsmaller datasets. Besides, using both the data augmenta-\ntion and the penultimate layer allows the CRF model to\nhave the best performance on all datasets but one, and to\nhave a statistically signiﬁcant improvement over the other\nconﬁgurations.\n5.3 Comparison to other algorithms:\nWe compared our best system to the ones of Krebs et\nal. [17], Peeters et al. [23], Davies et al. [3] and Papadopou-\nlos et al. [22]. We also compared to a system using the\nsame neural networks but with a different feature combi-\nnation and temporal model [6]. In this system, the out-\nput of the four networks is averaged and a Viterbi model\nwith hand-crafted transition and emission probabilities is\nused to decode the downbeat sequence. Due to space con-\nstraints, we do not add [4] and [5] since they are close to [6]\nin terms of architecture and produce worse results. Results\nare shown in the table 2. With the new CRF system pro-\nposed here, the improvement is substantially better in all\ndatasets compared to [17], [23], [3] and [22]. While the im-\nprovement averaged across datasets is moderate compared\nto [6], we observe a statistically signiﬁcant improvement6.\nOverall results are held back by the performance on RWC\nClassical and RWC Jazz. The used training sets barely\ncontain these music styles while we are exploiting a fully\ndata-driven approach. The leave-one-dataset-out approach\nmight be too restrictive when dealing with very distinctive\nmusic datasets. However, when more appropriate train-\ning data is available, the CRF model has a better potential,\nas results on RWC Genre indicates. This dataset includes\n6It is to note that the comparison between the data augmented system\nand [6] is fair since the networks were trained on the same data, and the\nfeature combination and temporal model steps of the heuristic model is\nblind to any data.\nY16\nY26\nY66\nY18\nY88...\nY16 . . .Y66Y18Y88 . . .\n-202468\n1\n0.8\n0.6\n0.4\n0.2\n6 tatum-long bar\nY26...Y16\nY26\nY66\nY18\nY88......\nY16 . . .Y66Y18Y88 . . .Y26(a)\n(b)Figure 3 . Selected transition weights for the 6 and 8 tatum-\nlong bars. It corresponds to the output labels Y6\n1toY6\n6and\nY8\n1toY8\n8. (a) Weight of the transition feature function in\nthe presented CRF model. (b) Coefﬁcients of the transition\nmatrix in [6]. As an illustration, inside the red rectangle\npointed by an arrow are all the coefﬁcients corresponding\nto a transition inside a 6 tatum-long bar. There is a weight\nat the bottom left corner of this rectangle with a value close\nto 1. It corresponds to the weight of the transition from Y6\n6\ntoY6\n1.\nmore than 30% of Jazz and Classical music songs and has\na signiﬁcantly better performance with the new temporal\nmodel (69.3% F-measure compared to 66.1% in [6]). In\nthis case the RWC Jazz and RWC Classical datasets were\npart of the training set and the CRF system was able to\nmodel these styles more accurately. In fact, the perfor-\nmance on Classical and Jazz music pieces on RWC Genre\nis improved by 6.8 pp, which is even better than the 3.1\npp overall. It highlights the potential of the data-driven\nproposed system, where relevant annotated data has a big\nimpact on performance.\n5.4 Analysis of the transition features:\nThe output space being similar with the one deﬁned in [6],\nwe can compare the transition coefﬁcients. Due to space\nconstraints, we limit our analysis to bars of 6 and 8 tatum\nof thepl+daCRF model. They correspond to the most\ncommon bars in the used datasets. The transition coefﬁ-\ncients can be seen in ﬁgure 3. The ﬁrst observation is that\nthe general intuition of moving circularly inside a bar is in-\ndeed learned by the CRF model as seen with the stronger\nweights of the transition feature function close to the diag-\nonal of the ﬁgure. We also see that the proposed learned\nCRF model transition coefﬁcients are more detailed while\nthey seem more binary in [6]. The proposed system is less\nrestrictive in metrical changes as can be seen by the coef-\nﬁcient of the output transitions Y6\n6→Y8\n1andY8\n8→Y6\n1\nin particular. It can be because the observation features are390 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016IHCNNIRCNNIMCNNIBCNN\n11.3 42.1 9.0 37.7\nTable 3 . Mean impact of each feature representation on\nthe pl + da CRF model.\nreliable enough to avoid false metrical changes between 6\nand 8 tatum-long bars. Finally, we see that some transitions\nhave strong negative weights in the CRF model. Y8\n4→Y8\n1\nandY8\n7→Y8\n5have the top negative weights, both at -\n3.9. In the ﬁrst case, it corresponds to going back to the\ndownbeat after 4 tatums and in the second case to going\nback to the downbeat after 16 tatums while being in a 8\ntatum-long bar. Finding the difference between a 4, 8 and\n16 tatum-long bar is indeed quite difﬁcult perceptively and\nfor the networks. There can be one part of the song where\nthe chords or the rhythmic patterns change twice as fast or\ntwice as slow, which could misled the observation features.\nThe negative weights can therefore emphasize a metrical\ncontinuity in the decoding.\n5.5 Ability to ﬁnd the correct metrical level:\nTo evaluate the ability of the system to ﬁnd the correct met-\nrical level, we use the continuity-based metric focusing on\nthe total proportion of correct regions at the correct met-\nrical level (CMLt) with a tolerance window of ±17.5%of\nthe inter-beat-interval7. The proposed system obtains a\nCMLt of 61.5% while [6] obtains a CMLt of 56.6%. The\nCRF model is therefore more efﬁcient to ﬁnd the correct\nmetrical level compared to [6]. It can be explained by the\nfact that every downbeat and non downbeat outputs have a\ndifferent observation features while all the non downbeat\nstates and all the downbeat states had the same observation\nfeature respectively in the compared system. Besides, as\nseen above, the transition coefﬁcients of the CRF model\nare better to avoid octave errors on duple meters while the\ncompared system makes more errors there.\n5.6 Analysis of the selected features:\nWe looked at the weight of the pl+daCRF model to see\nif a feature representation had more impact than others to\ndetect the downbeat sequence. To do so we calculated the\nsum of the absolute learned weight value belonging to each\nfeature representation:\nIXCNN =/summationdisplay\nj∈XCNN|θj| (2)\nwithX∈{H,R,M,B}. Results are shown in table 3 af-\nter a normalization inside and across datasets. It is to note\nthat they are consistent for each dataset and each label. We\ncan see that the rhythmic and bass content networks have\na larger impact on the CRF model. It can be surprising\nknowing that the harmonic network is the best performing\nnetwork in [6]. However, the rhythmic and bass content\nnetworks were trained to recognize the downbeat sequence\n7We don’t consider ±17.5%of the inter-downbeat-interval since it\nwould be too permissive.on the whole input and not a single downbeat per input\nonly. It allows them to encode information about the met-\nrical level that is useful for the CRF model.\n6. CONCLUSIONS\nWe presented a Conditional Random Field system based\non multiple deep learned feature representations for the\ntask of downbeat tracking. Using the networks penultimate\nlayer feature representation with 3 beats per bar augmented\ndata, we outperformed 5 compared downbeat tracking al-\ngorithms overall. While we need the training and test data\nto come from similar music styles to make full use of our\npowerful temporal model, it holds more potential com-\npared to heuristic based approaches and could be more eas-\nily adapted to different music styles.\nFuture work will focus on learning the deep networks\nand the conditional random ﬁeld models jointly and on re-\nﬁning the initial temporal segmentation.\n7. REFERENCES\n[1] J. P. Bello and J. Pickens. A robust mid-level repre-\nsentation for harmonic content in music signals. vol-\nume 19, 2005.\n[2] S. B ¨ock, F. Krebs, and M. Schedl. Evaluating the on-\nline capabilities of onset detection methods. In Pro-\nceedings of the International Conference on Music In-\nformation Retrieval (ISMIR) , 2012.\n[3] M. E. P Davies and M. D. Plumbley. A spectral differ-\nence approach to extracting downbeats in musical au-\ndio. In Proceedings of the European Signal Processing\nConference (EUSIPCO) , 2006.\n[4] S. Durand, J. P. Bello, B. David, and G. Richard.\nDownbeat tracking with multiple features and deep\nneural networks. In Proceedings of the IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , 2015.\n[5] S. Durand, J. P. Bello, B. David, and G. Richard. Fea-\nture adapted convolutional neural networks for down-\nbeat tracking. In Proceedings of the IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , 2016.\n[6] S. Durand, J. P. Bello, B. David, and G. Richard. Ro-\nbust downbeat tracking using an ensemble of convo-\nlutional networks. arXiv preprint arXiv:1605.08396 ,\n2016.\n[7] T. Fillon, C. Joder, S. Durand, and S. Essid. A condi-\ntional random ﬁeld system for beat tracking. In Pro-\nceedings of the IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) ,\n2015.\n[8] B. D. Giorgi, M. Zanoni, A. Sarti, and S. Tubaro. Au-\ntomatic chord recognition based on the probabilistic\nmodeling of diatonic modal harmony. In ProceedingsProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 391of the International Workshop on Multidimensional\nSystems (nDS) , 2013.\n[9] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich\nfeature hierarchies for accurate object detection and\nsemantic segmentation. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , pages\n580–587, 2014.\n[10] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: Popular, classical and jazz mu-\nsic databases. In Proceedings of the International Con-\nference on Music Information Retrieval (ISMIR) , vol-\nume 2, pages 287–288, 2002.\n[11] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: Music genre database and mu-\nsical instrument sound database. In Proceedings of the\nInternational Conference on Music Information Re-\ntrieval (ISMIR) , volume 3, pages 229–230, 2003.\n[12] S. Hainsworth and M. D. Macleod. Particle ﬁltering ap-\nplied to musical tempo tracking. EURASIP Journal on\nApplied Signal Processing , 2004:2385–2395, 2004.\n[13] A. Holzapfel, F. Krebs, and A. Srinivasamurthy. Track-\ning the ”odd”: Meter inference in a culturally diverse\nmusic corpus. In Proceedings of the International Con-\nference on Music Information Retrieval (ISMIR) , pages\n425–430, 2014.\n[14] T. Jehan. Downbeat prediction by listening and learn-\ning. In Proceedings of the IEEE Workshop on Appli-\ncations of Signal Processing to Audio and Acoustics ,\npages 267–270, 2005.\n[15] A. Klapuri, A. Eronen, and J. Astola. Analysis of\nthe meter of acoustic musical signals. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n14(1):342–355, 2006.\n[16] P. Korzeniowski, S. B ¨ock, and G. Widmer. Probabilis-\ntic extraction of beat positions from a beat activation\nfunction. In Proceedings of the International Confer-\nence on Music Information Retrieval (ISMIR) , 2014.\n[17] F. Krebs, S. B ¨ock, and G. Widmer. An efﬁcient state-\nspace model for joint tempo and meter tracking. In Pro-\nceedings of the International Conference on Music In-\nformation Retrieval (ISMIR) , pages 72–78, 2015.\n[18] F. Krebs, A. Holzapfel, A. T. Cemgil, and G. Wid-\nmer. Inferring metrical structure in music using particle\nﬁlters. IEEE Transactions on Audio, Speech and Lan-\nguage Processing , 23(5):817–827, 2015.\n[19] J. Lafferty, A. McCallum, and F. Pereira. Conditional\nrandom ﬁelds : Probabilistic models for segmenting\nand labeling sequence data. In Proceedings of ICLM ,\n2001.[20] B. McFee, E.J. Humphrey, and J.P. Bello. A software\nframework for musical data augmentation. In Proceed-\nings of the International Conference on Music Infor-\nmation Retrieval (ISMIR) , 2015.\n[21] N. Okazaki. Crfsuite: a fast implementation of condi-\ntional random ﬁelds (crfs), 2007.\n[22] H. Papadopoulos and G. Peeters. Joint estimation of\nchords and downbeats from an audio signal. IEEE\nTransactions on Audio, Speech and Language Process-\ning, 19(1):138–152, 2011.\n[23] G. Peeters and H. Papadopoulos. Simultaneous beat\nand downbeat-tracking using a probabilistic frame-\nwork: Theory and large-scale evaluation. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n19(6), 2011.\n[24] J. Peng, L. Bo, and J. Xu. Conditional neural ﬁelds.\nInAdvances in neural information processing systems ,\npages 1419–1427, 2009.\n[25] A. Srinivasamurthy, A. Holzapfel, and X. Serra. In\nsearch of automatic rhythm analysis methods for turk-\nish and indian art music. Journal of New Music Re-\nsearch , 43(1):94–114, 2014.\n[26] C. Sutton and A. McCallum. Dynamic Conditional\nRandom Fields : Factorized Probabilistic Models for\nLabeling and Segmenting Sequence Data. In Proceed-\nings of ICML , 2004.\n[27] C. Sutton and A. McCallum. An introduction to Con-\nditional Random Fields for relational learning , chap-\nter 4, pages 93–128. MIT Press, 2006.\n[28] D. Temperley and T. d. Clercq. Statistical analysis of\nharmony and melody in rock music. Journal of New\nMusic Research , 42(3):187–204, 2013.\n[29] J. Thomassen. Melodic accent: Experiments and a\ntentative model. Journal of the Acoustical Society of\nAmerica , 71:1596, 1982.392 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "On the Use of Note Onsets for Improved Lyrics-To-Audio Alignment in Turkish Makam Music.",
        "author": [
            "Georgi Dzhambazov",
            "Ajay Srinivasamurthy",
            "Sertan Sentürk",
            "Xavier Serra"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1284501",
        "url": "https://doi.org/10.5281/zenodo.1284501",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/243_Paper.pdf",
        "abstract": "This release contains the annotations and the scores to test the audio-score alignment methodology explained in:\n\n\nŞentrk, S., Gulati, S., and Serra, X. (2014). Towards alignment of score and audio recordings of Ottoman-Turkish makam music. In Proceedings of 4th International Workshop on Folk Music Analysis, pages 5760, Istanbul, Turkey.\n\n\nThe dataset in this release is derived from the transcription test dataset used in the paper:\n\n\nBenetos, E.  Holzapfel, A. (2013). Automatic transcription of Turkish makam music. In Proceedings of 14th International Society for Music Information Retrieval Conference, 4 - 8 Nov 2013, Curitiba, PR, Brazil.\n\n\nThe scores for each composition are obtained from the SymbTr collection explained in:\n\n\nKaraosmanoğlu, K. (2012). A Turkish makam music symbolic database for music information retrieval: SymbTr. In Proceedings of 13th International Society for Music Information Retrieval Conference (ISMIR), pages 223228.\n\n\nFrom the annotated score onsets for some of the above recordings only the main singing voice segments have been selected. Further separately only a subset of vocal onsets crresponding to phoneme transitions rules have been explicitly annotated as annotationOnsets.txt\n\n\nDzhambazov, G., Srinivasamurthy A., Şentrk S.,  Serra X. (2016).On the Use of Note Onsets for Improved Lyrics-to-audio Alignment in Turkish Makam Music. 17th International Society for Music Information Retrieval Conference (ISMIR 2016\n\n\nUsing this dataset\n\nPlease cite the above publications if you use this dataset in a publication.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\n\n\nhttp://compmusic.upf.edu/node/233",
        "zenodo_id": 1284501,
        "dblp_key": "conf/ismir/DzhambazovSSS16",
        "content": "ON THE USE OF NOTE ONSETS FOR IMPROVED LYRICS-TO-AUDIO\nALIGNMENT IN TURKISH MAKAM MUSIC\nGeorgi Dzhambazov Ajay Srinivasamurthy\nSertan S ¸ent ¨urk Xavier Serra\nMusic Technology Group, Universitat Pompeu Fabra, Barcelona, Spain\ngeorgi.dzhambazov@upf.edu\nABSTRACT\nLyrics-to-audio alignment aims to automatically match\ngiven lyrics and musical audio. In this work we extend a\nstate of the art approach for lyrics-to-audio alignment with\ninformation about note onsets. In particular, we consider\nthe fact that transition to next lyrics syllable usually im-\nplies transition to a new musical note. To this end we for-\nmulate rules that guide the transition between consecutive\nphonemes when a note onset is present. These rules are in-\ncorporated into the transition matrix of a variable-time hid-\nden Markov model (VTHMM) phonetic recognizer based\non MFCCs. An estimated melodic contour is input to\nan automatic note transcription algorithm, from which the\nnote onsets are derived. The proposed approach is evalu-\nated on 12 a cappella audio recordings of Turkish Makam\nmusic using a phrase-level accuracy measure. Evaluation\nof the alignment is also presented on a polyphonic version\nof the dataset in order to assess how degradation in the ex-\ntracted onsets affects performance. Results show that the\nproposed model outperforms a baseline approach unaware\nof onset transition rules. To the best of our knowledge, this\nis the one of the ﬁrst approaches tackling lyrics tracking,\nwhich combines timbral features with a melodic feature in\nthe alignment process itself.\n1. INTRODUCTION\nLyrics are one of the most important aspects of vocal mu-\nsic. When a performance is heard, most listeners will fol-\nlow the lyrics of the main vocal line. The goal of auto-\nmatic lyrics-to-audio alignment is to generate a temporal\nrelationship between lyrics and recorded singing. In this\nparticular work, the goal is to detect the start and end times\nof every phrase (1-4 words) from lyrics.\nIn recent years there has been a substantial amount of\nwork on the extraction of pitch of predominant singing\nvoice from polyphonic music [18]. Some algorithms have\nbeen tailored to the music characteristics of a particular\nc/circlecopyrtGeorgi Dzhambazov, Ajay Srinivasamurthy, Sertan\nS ¸ent¨urk , Xavier Serra . Licensed under a Creative Commons Attribution\n4.0 International License (CC BY 4.0). Attribution: Georgi Dzham-\nbazov, Ajay Srinivasamurthy, Sertan S ¸ent ¨urk , Xavier Serra . “On the use\nof note onsets for improved lyrics-to-audio alignment in Turkish Makam\nmusic”, 17th International Society for Music Information Retrieval Con-\nference, 2016.singing tradition [12]. This has paved the way to an in-\ncreased accuracy of note transcription algorithms. One\nof the reasons for this is that a correctly detected melody\ncontour is a fundamental precondition for note transcrip-\ntion. On the other hand, lyrics-to-audio alignment is a\nchallenging task: to track the timbral characteristics of\nsinging voice might not be straightforward [4]. An addi-\ntional challenge is posed when accompanying instruments\nare present: their spectral peaks might overlap and occlude\nthe spectral components of voice. Despite that, most work\nhas focused on tracking the transitions from one phoneme\nto another only by timbral features [4, 14]. In fact, at a\nphoneme transition, in parallel to timbral change, a change\nof pitch or an articulation accent may be present, which\ncontributes to the perception of a distinct vocal note onset\n. For example, a note onset occurs simultaneously with the\nﬁrst vowel in a syllable. This fact has been exploited suc-\ncessfully to enhance the naturalness of synthesized singing\nvoice [21].\nIn this work we present a novel idea of how to extend\na standard approach for lyrics-to-audio alignment by us-\ning automatically detected vocal note onsets as a comple-\nmentary cue. We apply a state of the art note transcrip-\ntion method to obtain candidate note onsets. The pro-\nposed approach has been evaluated on time boundaries of\nshort lyrics phrases on a cappella recordings from Turk-\nish Makam music. An experiment on polyphonic audio\nreveals the potential of the approach for real-world appli-\ncations.\n2. RELATED WORK\n2.1 Lyrics-to-audio alignment\nThe problem of lyrics-to-audio alignment has an inherent\nrelation to the problem of text-to-speech alignment. For\nthis reason most of current studies exploit an approach\nadopted from speech: building a model for each phoneme\nbased on acoustic features [5,14]. To model phoneme tim-\nbre usually mel frequency cepstral coefﬁcients (MFCCs)\nare employed. A state of the art work following this ap-\nproach [5] proposes a technique to adapt a phonetic recog-\nnizer trained on speech: the MFCC-based speech phoneme\nmodels are adapted to the speciﬁc acoustics of singing\nvoice by means of Maximum Likelihood Linear Regres-\nsion. Further, automatic segregation of the vocal line is\nperformed, in order to reduce the spectral content of back-716ground instruments. In general, in this approach authors\nconsider only models of phonetic timbre and are thus fo-\ncused on making them more robust as a mean to improve\nperformance.\nFew works for tracking lyrics combine timbral features\nwith other melodic characteristics. For example in [7]\na system for automatic score-following of singing voice\ncombines melodic and lyrics information: observation\nprobabilities of pitch templates and vowel templates are\nfused to improve alignment. In [13] lyrics-to-audio align-\nment has been aided on a coarser level by chord-to-audio\nalignment, assuming chord annotations are available in a\npaired chord-lyrics format. However, to our knowledge,\nno work so far has employed note onsets as additional cue\nto alignment.\n2.2 Automatic note segmentation\nWhile the general problem of automatic music transcrip-\ntion has been long-investigated, automatic singing tran-\nscription has attracted the attention of MIR researchers\nonly in recent years [6, 11, 15]. A fundamental part of\nsinging transcription is automatic note segmentation. A\nprobabilistic note event model, using a HMM trained on\nmanual transcriptions is presented in [11]. The idea is that\na note consists of different states representing its attack,\nsustain and decay phase. Then an onset is detected when\nthe decoding path goes through an attack state of a new\nnote.\nA recent work on singing transcription with high onset\naccuracy has been developed for singing voice from the ﬂa-\nmenco genre [12]. It consists of two stages: predominant\nvocal extraction and note transcription. As a primary step\nof the note transcription stage, notes are segmented by a\nset of onset detection functions based on pitch contour and\nvolume characteristics, which take into account the pecu-\nliar for ﬂamenco singing high degree of microtonal orna-\nmentation.\n3. PROPOSED APPROACH\nA general overview of the proposed approach is presented\nin Figure 1. An audio recording and its lyrics are input. A\nvariable time hidden Markov model (VTHMM), guided by\nphoneme transition rules, returns start and end timestamps\nof aligned words. For brevity in the rest of the paper our\napproach will be referred to as VTHMM.\nFirst an audio recording is manually divided into seg-\nments corresponding to structural sections (e.g. verse,\nchorus) as indicated in a structural annotation, whereby\ninstrumental-only sections are discarded. All further steps\nare performed on each audio segment. If we had used auto-\nmatic segmentation instead, potential erroneous lyrics and\nfeatures could have biased the comparison of a baseline\nsystem and VTHMM. As we focus on evaluating the effect\nof VTHMM, manual segmentation is preferred. In what\nfollows each of the modules is described in details.\nFigure 1 . Overview of the modules of the proposed ap-\nproach. One can see how phoneme transition rules are\nderived. Then together with the phonemes network and\nthe features extracted from audio segments are input to the\nVTHMM alignment.\n3.1 Vocal pitch extraction\nTo extract the melody contour of singing voice, we uti-\nlize a method that performs detection of vocal segments\nand in the same time pitch extraction for the detected seg-\nments [1]. It relies on the basic methodology of [19], but\nmodiﬁes the way in which the ﬁnal melody contour is se-\nlected from a set of candidate contours, in order to reﬂect\nthe speciﬁcities of Turkish Makam music: 1) It chooses\na ﬁner bin resolution of only 7.5 cents that approximately\ncorresponds to the smallest noticeable change in Makam\nmelodic scales. 2) Unlike the original methodology, it does\nnot discard time intervals where the peaks of the pitch con-\ntours have relatively low magnitude. This accommodates\ntime intervals at the end of the melodic phrases, where\nMakam singers might sing softer.\n3.2 Note segmentation\nIn a next step, to obtain reliable estimate of singing\nnote onsets, we adapt the automatic singing transcrip-\ntion method, developed for polyphonic ﬂamenco record-\nings [12]. It has been designed to handle singing with\nhigh degree of vocal pitch ornamentation. We expect that\nthis makes it suitable for material from Makam classical\nsinging having heavily vibrato and melismas, too. We re-\nplace the original ﬁrst stage predominant vocal extractionProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 717method with the vocal pitch detection method described\nabove.\nThe algorithm [12] considers two cases of onsets: in-\nterval onsets and steady pitch onsets. A Gaussian deriva-\ntive ﬁlter detects interval onsets as long-term changes of\nthe pitch contour, whereas steady-pitch onsets are inferred\nfrom pitch discontinuities. As in the current work phoneme\ntransitions are modiﬁed only when onsets are present, we\nopt for increasing recall at the cost of losing precision. This\nis achieved by reducing the value of the parameter cF: the\nminimum output of the Gaussian ﬁlter. The extracted note\nonsets are converted into a binary onset activation at each\nframe ∆nt= (0,1). Recall rates of extracted note onsets\nare reported in Table 2.\n3.3 Phoneme models\nThe formant frequencies of spoken phonemes can be in-\nduced from the spectral envelope of speech. To this end,\nwe utilize the ﬁrst 12 MFCCs and their delta to the pre-\nvious time instant, extracted as described in [24]. For\neach phoneme a one-state HMM, for which a 9-mixture\nGaussian distribution is ﬁtted on the feature vector. The\nlyrics are expanded to phonemes based on grapheme-to-\nphoneme rules for Turkish [16, Table 1] and the trained\nHMMs are concatenated into a phonemes network. The\nphoneme set utilized has been developed for Turkish and\nis described in [16]. A HMM for silent pause spis added\nat the end of each word, which is optional on decoding.\nThis way it will appear in the detected sequence only if\nthere is some non-vocal part or the singer makes a break\nfor breathing.\n3.4 Transition model\nWe utilize a transition matrix with time-dependent self-\ntransition probabilities which falls in the general category\nof variable time HMM (VTHMM) [9]. For particular\nstates, transitions are modiﬁed depending on the presence\nof time-adjacent note onset. Let t/primebe the timestamp of the\nclosest to given time tonset ∆nt/prime= 1. Now the transition\nprobability can be rewritten as\naij(t) =/braceleftBigg\naij−g(t,t/prime)q, R 1orR3\naij+g(t,t/prime)q, R 2orR4(1)\nR1toR4stand for phoneme transition rules, which are\napplied in the phonemes network by picking the states i\nandjfor two consecutive phonemes. The term qis a con-\nstant whereas g(t,t/prime)is a weighting factor sampled from a\nnormal distribution with its peak (mean) at t/prime:\ng(t,t/prime) =/braceleftBigg\nf(t;t/prime,σ2)∼N(t/prime,σ2),|t−t/prime|≤σ\n0 else(2)\nSince singing voice onsets are regions in time, they\nspan over multiple consecutive frames. To reﬂect that fact,\ng(t,t/prime)serves to smooth in time the inﬂuence of the dis-\ncrete detected ∆nt, whereσhas been selected to be 0.075seconds. In this way an onset inﬂuences a region of 0.15\nseconds - a threshold suggested for vocal onset detection\nevaluation by [6] and used in [12]. Furthermore, this al-\nlows to handle slight timestamp inaccuracies of the esti-\nmated note onsets.\n3.4.1 Phoneme transition rules\nLetVdenote a vowel, Cdenote a consonant and Ldenote\na vowel, liquid (LL, M, NN) or the semivowel Y . Rules R1\nandR2represent inter-syllable transition, e.g. phoneme i\nis followed by phoneme jfrom the following syllable:\nR1 :i=V j =¬L\nR2 :i=C j =L(3)\nFor example, for rule R2if a syllable ends in a con-\nsonant, a note onset imposes with high probability that a\ntransition to the following syllable is done, provided that\nit starts with a vowel. Same rule applies if it starts with\na liquid, according to the observation that pitch change\ntakes place during a liquid preceding the vowel [21, timing\nof pitch change]. Rules R3andR4are for intra-syllabic\nphoneme patterns:\nR3 :i=V j =C\nR4 :i=¬L j =V(4)\nEssentially, if the current phoneme is vocal and the next\nis non-voiced (e.g. R1,R3), Eq. (1) discourages tran-\nsition no next phoneme and encourages transition in the\nopposite cases. An example of R4can be seen for the syl-\nlable KK-AA in Figure 2 where the note onset triggers the\nchange to the vowel AA, opposed, for example, to onset\nat Y for the syllable Y-E-T. Note that these rules assume\nFigure 2 . Ground truth annotation of syllables (in or-\nange/top), phonemes (in red/middle) and notes (with\nblue/changing position). Audio excerpt corresponding to\nword s ¸ikayet with syllables SH-IY , KK-AA and Y-E-T.718 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016total #sections #phrases per section #words per phrase\n75 2 to 5 1 to 4\nTable 1 . Phrase and section statistics about the dataset.\nthat a syllable has one vowel, which is the case for Turk-\nish1. The optional silent phoneme spis handled as a spe-\ncial case: transition probability from any phoneme to spis\nderived according to intra-syllable rules, and the one from\nany phoneme skipping to the phoneme following spis de-\nrived according to inter-syllable rules.\n3.4.2 Alignment\nThe most likely state sequence is found by means of a\nforced alignment Viterbi decoding.\nδt(j) = max\ni∈(j, j−1)δt−1(i)aij(t)bj(Ot) (5)\nHerebj(Ot)is the observation probability for state ifor\nfeature vector Otandδt(j)is the probability for the path\nwith highest probability ending in state jat timet(com-\nplying with the notation of [17, III. B]2.\n4. DATASET\nThe test dataset consists of 12 a cappella performances of\n11 compositions with total duration of 19 minutes. The\nperformances are drawn from CompMusic corpus of clas-\nsical Turkish Makam repertoire with provided annotations\nof musical sections [23]. Solo vocal versions of the orig-\ninals have been sung by professional singers, especially\nrecorded for this study, due to the lack of appropriate a cap-\npella material in this music tradition. A performance has\nbeen recorded in sync with the original recording, whereby\ninstrumental sections are left as silence. This assures that\nthe order, in which sections are performed, is kept the\nsame. One of the contributions of this work is that we\nmake available the annotated phrase boundaries3. A mu-\nsical phrase spans 1 to 4 words depending on the duration\nof the words (as proposed in [10]). Table 1 presents statis-\ntics about phrases, while the total number of words in the\ndataset is 732.\nAdditionally, the singing voice for 6 recordings (with a\ntotal duration of 10 minutes) from the dataset has been an-\nnotated with MIDI notes complying to the musical score4.\nOn annotation special care is taken to place the note on-\nset on the time instant, at which the pitch becomes steady.\nThus we avoid placing the onset on an unvoiced phoneme\nat the beginning of a syllable, which is assures rules R3\nand R4 make sense (see Figure 2)5.\n1Among one-vowel syllabic languages are also Japanese and to some\nextent Italian\n2To encourage reproducibility of this research an efﬁcient open-\nsource implementation together with documentation is available at\nhttps://github.com/georgid/AlignmentDuration/tree/noteOnsets\n3The audio and the annotations are available under a CC license at\nhttp://compmusic.upf.edu/turkish-makam-acapella-sections-dataset\n4Creating the annotation is a time-consuming task, but we plan to an-\nnotate the whole dataset in the future\n5Onset annotations are available at4.1 Evaluation metric\nAlignment is evaluated in terms of alignment accuracy\nas the percentage of duration of correctly aligned regions\nfrom total audio duration (see [5, Figure 9] for an exam-\nple). A value of 100 means perfect matching of all phrase\nboundaries in the evaluated audio. Thus accuracy can be\nreported not only for an audio segment, but also on total\nfor a recording, or as a total for all the recordings.\n5. EXPERIMENTS\n5.1 Experiment 1: alignment with oracle onsets\nAs a precursor to the following experiments, lyrics-to-\naudio alignment is run on these 6 recordings with manu-\nally annotated MIDI notes, which serve as an oracle for\nnote onsets. This is done to test the general feasibility\nof the proposed model on the dataset, unbiased from er-\nrors in the note segmentation algorithm, and to set a glass-\nceiling alignment accuracy. We have tested with different\nvalues ofqfrom Eq. 1 achieving best accuracy of 83.5%\natq= 0.23, which is used on all further reported experi-\nments.\n5.2 Experiment 2: recognition of phonemes\nIn general, the comparison to other lyrics alignment sys-\ntems is not feasible, because there is no current work de-\nveloped for Turkish language. However, to have an idea\nof how adequate the trained phoneme HMMs are, we have\nannotated phoneme boundaries for some excerpts of total\nlength of 6 minutes. In [8] phonemes are recognized in\na cappella singing with no lyrics given in advance. With\nphoneme MFCC-based HMMs - the same as our modeling\nsetting - a phoneme recall rate of 44% is reported. Even\nthough for forced alignment the recognition of phonemes\nis relatively easier, given that they are ordered in a se-\nquence, we measured lower overall phoneme recall of\n37%. This indicates that our phoneme models trained only\non speech might not be the most optimal choice.\n5.3 Experiment 3: comparison to a baseline\nAs a baseline we conduct alignment of the test dataset with\nunaffected phoneme transition probabilities, e.g. setting all\n∆nt= 0, which resulted in alignment accuracy of 70.2%.\nFurther, we measured the impact of the note segmentation\nmodule (introduced in Section 3.2), varying onset detec-\ntion recall by changing the minimum output of the Gaus-\nsian ﬁlter (controlled by the parameter cF). Table 2 sum-\nmarizes the alignment accuracy with VTHMM depending\non recall. On a cappella best improvement over the base-\nline is achieved at recall of 72.3% (atcF= 3.5). This is\nsomewhat lower than the best recall of 81-84% achieved\nfor ﬂamenco [12]. Setting recall higher than that degraded\nperformance because there are too many false alarms, re-\nsulting in forcing false transitions.\nhttp://compmusic.upf.edu/node/233Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 719Figure 3 . Example of boundaries of phonemes for the word s ¸ikayet (SH-IY-KK-AA-Y-E-T): on top : spectrum and pitch;\nthen from top to bottom : ground truth boundaries, phonemes detected with HMM, detected onsets, phonemes detected with\nVTHMM; (excerpt from the recording ’Kimseye etmem s ¸ikayet’ by Bekir Unluater).\ncF 5 4.5 4.0 3.5 3.0\na cappellaOR 57.2 59.7 66.8 72.3 73.2\nAA 71.1 73.3 74.5 75.7 72.0\npolyphonicOR 52.8 58.2 65.9 66.2 68.4\nAA 61.2 63.3 64.8 64.6 60.3\nTable 2 . VTHMM performance on a cappella and poly-\nphonic audio, depending on onset detection recall (OR).\nAlignment accuracy (AA) is reported as a total for all the\nrecordings.\nFigure 3 allows a glance at the level of detected\nphonemes: the baseline HMM switches to the follow-\ning phoneme after some amount of time, similar for all\nphonemes. One reason for this might be that the waiting\ntime in a state in HMMs with a ﬁxed transition matrix can-\nnot be randomly long [25]. In contrast, for VTHMM the\npresence of note onsets at vowels activates rules R1orR3,\nwhich allows waiting in the same state longer, as there are\nmore onsets (for example AA from the word SH-IY-KK-\nAA-Y-E-T has ﬁve associated onsets). We chose to modify\ncFbecause setting it to lower values increases the recall of\ninterval onsets : Often in our dataset several consecutive\nnotes with different pitch correspond to the same vowel.\nIn fact, it is characteristic of Turkish classical music that\na single syllable may have a complex melodic progression\nspanning many notes (up to 12 in our dataset) [3]. How-\never, for cases of vowels held long on same pitch, con-\nceptually VTHMM is not capable of bringing any beneﬁt.\nThis is illustrated in Figure 3 by the prematurely detected\nend boundary of E from the word SH-IY-KK-AA-Y-E-T.\nIn addition to that, we examined alignment accuracy per\nrecording (Figure 4). It can be observed that VTHMM\nperforms consistently better than the baseline HMM (with\nsome exceptions of where accuracy is close).6. EXTENSION TO POLYPHONIC MATERIAL\nTo test the feasibility of the proposed approach on poly-\nphonic material, the alignment is evaluated on the original\nversions of the recordings in the dataset. Typical for Turk-\nish Makam is that vocal and accompanying instruments\nfollow the same melodic contour in their corresponding\nregisters, with slight melodic variations. However, the\nvocal line usually has melodic predominance. This spe-\ncial type of polyphonic musical interaction is termed het-\nerophony [3]. In the dataset used in this study, a singer is\naccompanied by one to several string instruments.\nWe applied the vocal pitch extraction and note seg-\nmentation methods directly, since both are developed for\nsinging voice in a setting that has heterophonic characteris-\ntics. However, instrumental spectral peaks deteriorate sig-\nniﬁcantly the shape of the vocal spectrum. To attenuate the\nnegative inﬂuence of instrumental spectrum, a vocal resyn-\nthesis step is necessary.\n6.1 Vocal resynthesis\nFor the regions with predominant voice, the vocal con-\ntent is resynthesized as separate vocal part. Resynthesis\nis conducted based on the harmonic model of [20]: Based\non the extracted predominant pitch (see Section 3.1) and\na set of peaks from the original spectrum, the harmonic\npartials of the predominant voice are selected and resyn-\nthesized. Then MFCCs are extracted from the resynthe-\nsized vocal part as if it were monophonic singing. This is\na viable step, because the harmonic partials preserve well\nthe overall spectral shape of the singing voice, including\nthe formant frequencies, which encode the phoneme iden-\ntities [22]6. More details and examples of the resynthesis\ncan be found in previous work, which showed that the ap-\nplication of a harmonic model is suitable for aligning lyrics\nin Makam music [2]. A conceptually similar resynthesis\n6The resynthesis allowed us to verify that vocals are intelligible de-\nspite some distortions from overlap with instrumental harmonic partials720 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Figure 4 . Comparison between results for VTHMM and baseline HMM on a cappella.\nstep is an established part also in current methods for align-\nment of lyrics in polyphonic Western pop music [5, 14].\n6.2 Experiment 4: comparison of a cappella and\npolyphonic\nThe onset recall rates on polyphonic material after note\nsegmentation are not much worse than a cappella as pre-\nsented in Table 2. Even though the degree of degradation\nin onset detection is slight, degradation in alignment accu-\nracy is signiﬁcant. This can be attributed most probably to\nthe fact that our MFCC-based models are not very discrim-\ninative and get confused by artifacts, induced from other\ninstruments on resynthesis. However, applying VTHMM\non polyphonic recordings still improves over the baseline\n(see Table 3). Note that the margin in accuracy between\nthe baseline and the oracle glass ceiling is only about 6%,\nwhich is about twice much in the case of solo voice.\nHMM VTHMM oracle\na cappella 70.2 75.7 83.5\npolyphonic 61.5 64.8 67.1\nTable 3 . Comparison of accuracy of baseline HMM,\nVTHMM and, VTHMM with oracle onsets. VTHMM\nshown are the best accuracies reported in Table 2. Align-\nment accuracy is reported on total for all recordings.\n7. CONCLUSION\nIn this work we evaluated the behavior of a HMM-based\nphonetic recognizer for lyrics-to-audio alignment in two\nsettings: with and without considering singing voice on-\nsets as additional cue. Compared to existing work on lyrics\nalignment, this is, to our knowledge, the ﬁrst attempt to\ninclude onsets of the vocal melody in the inference pro-\ncess. Updating transition probabilities according to onset-\naware phoneme transition rules resulted in an improvementof absolute 5.5 percent for aligning phrases of solo voice\nfrom Turkish Makam recordings. In particular, due to rules\ndiscouraging premature transition, the states of sustained\nvowels could have longer durations.\nAlignment on same data with instrumental accompani-\nment brought also some small improvement over a base-\nline with no onset modeling. Having onset detection per-\nforming not substantially worse than a cappella indicates\nthat improving the phoneme acoustic models in the future\ncould probably lead to even more signiﬁcant improvement.\nA practical limitation of the current alignment system is\nthe prerequisite for manual structural segmentation, which\nwe plan to automate in the future.\nAcknowledgements We thank Nadine Kroher for\nproviding help with running the note segmentation mod-\nule. This work is partly supported by the European\nResearch Council under the European Union’s Seventh\nFramework Program, as part of the CompMusic project\n(ERC grant agreement 267583) and partly by the AGAUR\nresearch grant. We acknowledge as well ﬁnancial support\nfrom the Spanish Ministry of Economy and Competitive-\nness, through the ”Mar ´ıa de Maeztu” Programme for Cen-\ntres/Units of Excellence in R&D” (MDM-2015-0502)\n8. REFERENCES\n[1] Hasan Sercan Atlı, Burak Uyar, Sertan S ¸ent ¨urk, Barıs ¸\nBozkurt, and Xavier Serra. Audio feature extraction for\nexploring Turkish makam music. In 3rd International\nConference on Audio Technologies for Music and Me-\ndia, Ankara, Turkey, 2014. Bilkent University, Bilkent\nUniversity.\n[2] Georgi Dzhambazov, Sertan Sent ¨urk, and Xavier Serra.\nAutomatic lyrics-to-audio alignment in classical Turk-\nish music. In The 4th International Workshop on Folk\nMusic Analysis , pages 61–64, 2014.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 721[3] Eric Bernard Ederer. The Theory and Praxis of Makam\nin Classical Turkish Music 1910–2010 . University of\nCalifornia, Santa Barbara, 2011.\n[4] Hiromasa Fujihara and Masataka Goto. Lyrics-to-\naudio alignment and its application. Dagstuhl Follow-\nUps, 3, 2012.\n[5] Hiromasa Fujihara, Masataka Goto, Jun Ogata, and\nHiroshi G Okuno. Lyricsynchronizer: Automatic syn-\nchronization system between musical audio signals and\nlyrics. IEEE Journal of Selected Topics in Signal Pro-\ncessing , 5(6):1252–1261, 2011.\n[6] Emilia G ´omez and Jordi Bonada. Towards computer-\nassisted ﬂamenco transcription: An experimental com-\nparison of automatic transcription algorithms as ap-\nplied to a cappella singing. Computer Music Journal ,\n37(2):73–90, 2013.\n[7] Rong Gong, Philippe Cuvillier, Nicolas Obin, and\nArshia Cont. Real-time audio-to-score alignment of\nsinging voice based on melody and lyric information.\nInInterspeech 2015 , Dresden, Germany, 06/09/2015\n2015.\n[8] Jens Kofod Hansen. Recognition of phonemes in a-\ncappella recordings using temporal patterns and mel\nfrequency cepstral coefﬁcients. In Proceedings of the\n9th Sound and Music Computing Conference , pages\n494–499, Copenhagen, Denmark, 2012.\n[9] Michael T Johnson. Capacity and complexity of HMM\nduration modeling techniques. Signal Processing Let-\nters, IEEE , 12(5):407–410, 2005.\n[10] M. Kemal Karaosmano ˘glu, Barıs ¸ Bozkurt, Andre\nHolzapfel, and Nilg ¨un Do ˘grus¨oz Dis ¸iac ¸ık. A sym-\nbolic dataset of Turkish makam music phrases. In\nFourth International Workshop on Folk Music Analy-\nsis (FMA2014) , 2014.\n[11] Willie Krige, Theo Herbst, and Thomas Niesler. Ex-\nplicit transition modelling for automatic singing tran-\nscription. Journal of New Music Research , 37(4):311–\n324, 2008.\n[12] Nadine Kroher and Emilia G ´omez. Automatic tran-\nscription of ﬂamenco singing from polyphonic music\nrecordings. IEEE/ACM Transactions on Audio, Speech\nand Language Processing , 24(5):901–913, 2016.\n[13] Matthias Mauch, Hiromasa Fujihara, and Masataka\nGoto. Integrating additional chord information into\nhmm-based lyrics-to-audio alignment. Audio, Speech,\nand Language Processing, IEEE Transactions on ,\n20(1):200–210, 2012.\n[14] Annamaria Mesaros and Tuomas Virtanen. Automatic\nalignment of music audio and lyrics. In Proceedings\nof the 11th Int. Conference on Digital Audio Effects\n(DAFx-08) , 2008.[15] Emilio Molina, Lorenzo J Tard ´on, Ana M Barbancho,\nand Isabel Barbancho. Sipth: Singing transcription\nbased on hysteresis deﬁned on the pitch-time curve.\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing , 23(2):252–263, 2015.\n[16] ¨Ozg¨ul Salor, Bryan L Pellom, Tolga C ¸ ilo ˘glu, and\nM¨ubeccel Demirekler. Turkish speech corpora and\nrecognition tools developed by porting sonic: Towards\nmultilingual speech recognition. Computer Speech and\nLanguage , 21(4):580 – 593, 2007.\n[17] Lawrence Rabiner. A tutorial on hidden Markov mod-\nels and selected applications in speech recognition.\nProceedings of the IEEE , 77(2):257–286, 1989.\n[18] Justin Salamon, Emila G ´omez, Dan Ellis, and Ga ¨el\nRichard. Melody extraction from polyphonic mu-\nsic signals: Approaches, applications and chal-\nlenges. IEEE Signal Processing Magazine , 31:118–\n134, 02/2014 2014.\n[19] Justin Salamon and Emilia G ´omez. Melody extrac-\ntion from polyphonic music signals using pitch contour\ncharacteristics. IEEE Transactions on Audio, Speech,\nand Language Processing , 20(6):1759–1770, 2012.\n[20] Xavier Serra. A system for sound analy-\nsis/transformation/synthesis based on a deterministic\nplus stochastic decomposition. Technical report, 1989.\n[21] Johan Sundberg. The KTH synthesis of singing.\nAdvances in Cognitive Psychology , 2(2-3):131–143,\n2006.\n[22] Johan Sundberg and Thomas D Rossing. The science\nof singing voice. the Journal of the Acoustical Society\nof America , 87(1):462–463, 1990.\n[23] Burak Uyar, Hasan Sercan Atlı, Sertan S ¸ent ¨urk, Barıs ¸\nBozkurt, and Xavier Serra. A corpus for computational\nresearch of Turkish makam music. In 1st International\nDigital Libraries for Musicology Workshop , pages 57–\n63, London, United Kingdom, 2014.\n[24] Steve J Young. The HTK hidden Markov model toolkit:\nDesign and philosophy . Citeseer, 1993.\n[25] Shun-Zheng Yu. Hidden semi-Markov models. Artiﬁ-\ncial Intelligence , 174(2):215–243, 2010.722 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Noise Robust Music Artist Recognition Using I-Vector Features.",
        "author": [
            "Hamid Eghbal-zadeh",
            "Gerhard Widmer"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417199",
        "url": "https://doi.org/10.5281/zenodo.1417199",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/037_Paper.pdf",
        "abstract": "In music information retrieval (MIR), dealing with differ- ent types of noise is important and the MIR models are frequently used in noisy environments such as live per- formances. Recently, i-vector features have shown great promise for some major tasks in MIR, such as music sim- ilarity and artist recognition. In this paper, we introduce a novel noise-robust music artist recognition system using i-vector features. Our method uses a short sample of noise to learn the parameters of noise, then using a Maximum A Postriori (MAP) estimation it estimates clean i-vectors given noisy i-vectors. We examine the performance of multiple systems confronted with different kinds of addi- tive noise in a clean training - noisy testing scenario. Using open-source tools, we have synthesized 12 different noisy versions from a standard 20-class music artist recognition dataset encountered with 4 different kinds of additive noise with 3 different Signal-to-Noise-Ratio (SNR). Using these datasets, we carried out music artist recognition experi- ments comparing the proposed method with the state-of- the-art. The results suggest that the proposed method out- performs the state-of-the-art.",
        "zenodo_id": 1417199,
        "dblp_key": "conf/ismir/Eghbal-zadehW16",
        "content": "NOISE ROBUST MUSIC ARTIST RECOGNITION USING I-VECTOR\nFEATURES\nHamid Eghbal-zadeh Gerhard Widmer\nDepartment of Computational Perception, Johannes Kepler University of Linz, Austria\nhamid.eghbal-zadeh@jku.at\nABSTRACT\nIn music information retrieval (MIR), dealing with differ-\nent types of noise is important and the MIR models are\nfrequently used in noisy environments such as live per-\nformances. Recently, i-vector features have shown great\npromise for some major tasks in MIR, such as music sim-\nilarity and artist recognition. In this paper, we introduce\na novel noise-robust music artist recognition system using\ni-vector features. Our method uses a short sample of noise\nto learn the parameters of noise, then using a Maximum\nA Postriori (MAP) estimation it estimates clean i-vectors\ngiven noisy i-vectors. We examine the performance of\nmultiple systems confronted with different kinds of addi-\ntive noise in a clean training - noisy testing scenario. Using\nopen-source tools, we have synthesized 12 different noisy\nversions from a standard 20-class music artist recognition\ndataset encountered with 4 different kinds of additive noise\nwith 3 different Signal-to-Noise-Ratio (SNR). Using these\ndatasets, we carried out music artist recognition experi-\nments comparing the proposed method with the state-of-\nthe-art. The results suggest that the proposed method out-\nperforms the state-of-the-art.\n1. INTRODUCTION\nIn MIR, the task of music artist recognition1is to recog-\nnize an artist, from a part of a song. In real life, MIR sys-\ntems have to cope with different kinds of noise; example\nsituations include music played in a public area such as\na pub or in a live performance. MIR systems are usually\ntrained with the high quality data from studio recordings\nor noise-free audios, yet they may be used in noisy envi-\nronments.\nIn this paper, we are targeting a use-case, when an artist\nrecognition mobile app is used in a noisy environment,\nwhile the artist recognition models are trained on clean\ndata and are integrated inside the app. In such a use-case,\n1We use the term music artist or artist to refer to the singer or the band\nof a song.\nc⃝Hamid Eghbal-zadeh\nGerhard Widmer. Licensed under a Creative Commons Attribu-\ntion 4.0 International License (CC BY 4.0). Attribution: Hamid\nEghbal-zadeh Gerhard\nWidmer. “noise robust music artist recognition using i-vector features”,\n17th International Society for Music Information Retrieval Conference,\n2016.the models can not be trained or adapted on the mobile\nphone, due to the limitation of computation. But using the\napp, a short example of the noise can be prepared to im-\nprove the performance of the system.\nI-vector extraction is an unsupervised high-level feature\nextraction technique that extracts excerpt-level features us-\ning frame-level features of an audio excerpt. I-vectors were\nproposed for the ﬁrst time in the ﬁeld of speaker veriﬁca-\ntion [4], and then they were frequently used in other areas\nsuch as emotion [29], language [5], and accent recogni-\ntion [1] and audio scene detection [9]. Recently, they were\nimported into the MIR domain, for singing language iden-\ntiﬁcation [17], music artist recognition [7] and music sim-\nilarity [6]. I-vector features provide a ﬁxed-length low-\ndimensional representation for songs which can capture\nspeciﬁc variabilities from acoustic features using Factor\nAnalysis (FA). In [7], i-vector systems used with noise-\nfree data, and clean mp3 audio ﬁles were used in the artist\nrecognition experiments.\nI-vector based systems consist of 4 main modules: 1)\nframe-level feature extraction such as Mel-Frequency Cep-\nstrum Coefﬁcients (MFCC), 2) i-vector extraction, 3) inter-\nclass compensation and ﬁnally, 4) i-vector scoring. Within-\nClass Covariance Normalization (WCCN) and Linear Dis-\ncriminant Analysis (LDA) are examples of methods used\nin the inter-class compensation step. Cosine similarity and\nProbabilistic Linear Discriminant Analysis (PLDA) scor-\ning are examples of methods used in the scoring step.\nIn this paper, we propose a noise-robust artist recog-\nnition system using i-vector features that can be adapted\nto different kinds of additive noise. We add an estima-\ntion step after i-vector extraction, which estimates clean\ni-vectors given noisy i-vectors in a clean training - noisy\ntesting scenario. Our method is superior because it can be\nused to adapt i-vector based systems to different kinds of\nnoise, without training the models with noisy data. This is\ndone by learning the parameters of noise for different noisy\nenvironments given a short example of additive noise and\nestimating clean i-vectors that perform well with the mod-\nels trained on clean data.\n2. RELATED WORK\nTo make a MIR system robust to noise, a solution would\nbe to include noise information inside the MIR models\nby adding noisy samples in training data. For example,\nin [18] a method called multi-style training is proposed for709speaker veriﬁcation in noisy environments. This method\nuses noisy data provided in the training set to extract\nnoisy training i-vectors, with the i-vector extraction models\ntrained on clean data. But it trains the inter-class compen-\nsation and scoring models with noisy training i-vectors.\nBecause MIR models are usually expensive to train, if\nno noisy data is available in training, the only option would\nbe to use the models trained on clean data for testing the\nnoisy data and try to reduce the effect of noise on the MIR\nmodels.\nIn recent years, research focused on studying the vul-\nnerability of i-vectors to noise and providing methods for\nnoise compensation. The de-noising techniques used with\ni-vectors can be categorized according to the level they\nwork on (audio, frame-level features, i-vector extraction,\nor scoring) [22].\nIn [8, 25] spectral and wavelet based enhancement ap-\nproaches on the signal level were examined with i-vectors.\nIn [14], spectrum estimators were used for frame-level\nnoise compensation, while in [19–21], multiple approaches\nwere tested using vector Taylor series (VTS) in the cepstral\ndomain for noise compensation on the feature level. Both\nsignal-level and feature-level noise compensation tech-\nniques have shown inconsistencies because of their depen-\ndency on the type and level of noise. Also, they are mostly\ndesigned for speech enhancement and not for music. And\nin [18, 24] a method was proposed that improves the scor-\ning but it uses noisy audios in model training.\nIn [16], a novel method called “i-MAP” is proposed for\nthe purpose of speaker veriﬁcation in noisy environments\nthat uses an extra estimation step between i-vector extrac-\ntion and inter-class compensation steps. In the estimation\nstep, it estimates clean i-vectors given noisy i-vectors and\nfurther uses these estimations with inter-class compensa-\ntion and scoring models that are trained on clean data. This\nmethod beneﬁts from the Gaussian assumption of i-vectors\nand proposes a MAP estimation of a clean i-vector given a\nnoisy i-vector. The i-MAP method can be used with differ-\nent types and levels of additive noise with the models that\nare trained on clean data.\n3. REVIEW OF I-VECTOR SYSTEMS\nAn i-vector refers to vectors in a low-dimensional space\ncalled I-vector Space. The i-vector space models variabil-\nities encountered with both the artist and song [6] where,\nthe song variability deﬁnes as the variability exhibited by\na given artist from one song to another.\nThe i-vector space is created using a matrix Tknown\nas i-vector space matrix. This matrix is obtained by factor\nanalysis, via a procedure described in details in [4]. In the\nresulting space, a given song is represented by an i-vector\nwhich indicates the directions that best separate different\nartists. This representation beneﬁts from its low dimen-\nsionality and Gaussian distribution which enables us to use\nthe properties of Gaussians in the i-vector space.\nConceptually, a Gaussian mixture model (GMM) mean\nsupervector Madapted to a song from artist αcan be de-\ncomposed as follows:M=m+T.y (1)\nwhere mis the GMM mean supervector and T.yis an\noffset. The low-dimensional subspace vector yis a latent\nvariable with the standard normal prior and the i-vector w\nis a MAP estimate of y. The UBM is a GMM that is trained\nunsupervised on acoustic features of sufﬁcient amount of\nsongs. Also, Mis assumed to be normally distributed with\nmean vector m.\nThe obtained i-vector is an artist and song dependent\nvector. The low-rank rectangular matrix T(i-vector space\nmatrix) is used to extract i-vectors from statistical super-\nvectors of songs which are computed using UBM.\nUsing the UBM, we calculate statistical supervectors\nfor a speciﬁc song s. These statistical supervectors are\nknown as 0thand1storder statistics ( NsandFs) of song\ns:\n(0thorder statistics) Ns\nc=L∑\nt=1γt(c) (2)\n(1storder statistics) Fs\nc=L∑\nt=1γt(c)Yt (3)\nwhere γt(c)is the posterior probability of Gaussian\ncomponent cof UBM for frame tandYtis the MFCC fea-\nture vector at frame t.\nUsing the statistical supervectors of songs in training\nset, we learn the Tmatrix via an Expectation Maximiza-\ntion (EM) algorithm: E-step, computes the probability of\nP(w|X)where Xis the given song and wis its i-vector.\nM-step, optimizes Tby updating the following equation:\nw= (I+TtΣ−1N(s)T)−1·TtΣ−1F(s) (4)\nwhere N(s)andF(s)are diagonal matrices with Nsc.I\nandFsc.Ion diameter and NsandFsare0thand1storder\nstatistical supervectors of song s.Σis the diagonal covari-\nance matrix, estimated during factor analysis training. The\nactual computation of an i-vector wfor a given song scan\nbe done using (4) after training T. More information about\nthe training procedure of Tcan be found in [4, 15].\n4. MAP ESTIMATION OF A CLEAN I-VECTOR\nIn cases where only clean songs are available for training\nbut at testing the observations are noisy, the best way to im-\nprove the performance of clean models encountered with\nnoisy data would be to have an estimation of how clean\ndata looks like. In an i-vector based approach, this esti-\nmation is done in the i-vector space by estimating clean\ni-vectors given noisy i-vectors.\nThis section describes a solution for music artist recog-\nnition in noisy environments which uses a state-of-the-\nart i-vector based system with an extra estimation step.\nOur method beneﬁts from a MAP estimation of clean i-\nvectors given noisy i-vectors that was proposed in i-MAP\nmethod [16] for speaker veriﬁcation applications. The es-\ntimation step is applied after i-vector extraction, as it is710 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Figure 1 . Block diagram of the estimation step in the proposed artist recognition method. The blocks with an asterisk ( ∗)\nindicate the training and testing audio data as the starting point of the diagram.\nshown on the top of Figure 1. The resulting estimated\ni-vectors are used for inter-class compensation and scor-\ning. The estimation step in i-MAP consists of: a) detecting\nnoise using a V oice Activity Detector (V AD), b) synthesiz-\ning polluted data, c) estimating the mean and covariance\nof noise in i-vector space, and ﬁnally d) estimating clean i-\nvectors given noisy i-vectors. To estimate a clean i-vector\ngiven a noisy i-vector, i-MAP uses an estimation of the\nmean and covariance of noise in i-vector space by using\nnoise audio samples detected in the noisy testing audios.\nBased on i-MAP, the estimation step used in our pro-\nposed artist recognition method is described as follows.\nConsidering two sets of clean and noisy i-vectors, we\ndeﬁne the following random variables:\n•X: corresponding to the clean i-vectors\n•Y: corresponding to the noisy i-vectors\nAnd as i-vectors are normally distributed, we deﬁne:\nX∼ N (µX,ΣX) (5)\nY∼ N (µY,ΣY) (6)\nWe denote the probability density functions (PDF) of\nXandYbyf(X)andf(Y)with the parameters of ( µX,\nΣX) and ( µY,ΣY) as mean and covariance of clean and\nnoisy i-vectors, respectively.\nWe now consider f(X|Y0)as the conditional PDF of\nclean i-vectors given a noisy i-vector Y0. By Bayes’ rule,\nf(X|Y0) =f(Y0|X)f(X)\nf(Y0)(7)\nUsing a MAP estimator, a clean i-vector /hatwiderX0can be es-\ntimated by maximizing f(X|Y0):\n/hatwiderX0= argmax\nX{f(X|Y0)} (8)\nusing (7) and taking lnwe solve:\n∂\n∂X{lnf(Y0|X) + ln f(X)}= 0 (9)\nThe solution of (9) would provide an estimation of clean\ni-vector ˆX0from noisy i-vector Y0.4.1 Clean i-vector estimation\nIn i-MAP [2] it is assumed that when the noise is additive\nin the speech signal, the noise will be also additive in i-\nvector space. We keep this assumption and thus, the noise\nmodel deﬁnes as:\nY=X+N (10)\nwhere Nis a random variable corresponds to noise in\ni-vector space:\nN∼ N (µN,ΣN) (11)\nwhere ( µN,ΣN) are parameters of noise in i-vector\nspace.\nAlso, the Gaussian conditional PDF f(Y0|X)is deﬁned\nas:\nf(Y0|X) =1\n(2π)p\n2|ΣN|1\n2e−1\n2(Y0−X−µN)tΣN−1(Y0−X−µN)\n(12)\nand Gaussian PDF f(X)is:\nf(X) =1\n(2π)p\n2|ΣX|1\n2e−1\n2(X−µX)tΣX−1(X−µX)(13)\nwhere ( µX,ΣX) are parameters of clean i-vectors, and\n(µN,ΣN) are parameters of noise in i-vector space. Since\nf(Y0|X)andf(X)are Gaussian, the resulting estimate of\nf(X|Y0)is also a valid Gaussian PDF as discussed in [22].\nNow, by replacing f(Y0|X)andf(X)by (12) and (13)\nin (9) and solving it we will have:\n/hatwiderX0= (Σ N−1+Σ−1\nX)−1(Σ−1\nN(Y0−µN)+Σ−1\nXµX)(14)\n/hatwiderX0is the MAP estimation of a clean i-vector given a\nnoisy i-vector Y0.\n4.2 Parameters of Noise in I-vector Space\nTo use the MAP estimation provided in (14), we need an\nestimation of the parameters of clean i-vectors µX,ΣX\n(which can be estimated from clean training i-vectors2)\nand parameters of noise in i-vector space µN,ΣN.\n2We use the term clean training audios (e.g. clean training songs)\nto address the audio data in training set that does not contain any noise.\nNoisy training audios (e.g. noisy testing songs) indicates audio data in\ntesting set confronted with noise. The word polluted training audios (e.g.\npolluted training songs) indicates the data that are synthesized from cleanProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 711The parameters of clean training i-vectors ( µX,ΣX)\ncan be learned by computing the mean and covariance ma-\ntrix of clean training i-vectors. To learn the parameters\nof noise in i-vector space, we follow a similar procedure\nsuggested in i-MAP (step numbers can be found in Fig-\nure 1): a) we extract clean training i-vectors from clean\ntraining songs and noisy testing i-vectors from noisy test-\ning songs (steps 1 and 2). b) we detect noise audio samples\nin noisy testing songs (step 3). c) we use the noise audio\nsamples detected in step 3 to synthesize polluted training\nsongs from clean training songs (step 4). d) from polluted\ntraining songs, we extract a set of i-vectors known as pol-\nluted training i-vectors (step 5). e) using clean training\ni-vectors and polluted training i-vectors, we estimate the\nparameters of noise in i-vector space (step 6). f) now given\nnoisy testing i-vectors, by having the parameters of noise\nin i-vector space, we estimate clean testing i-vectors via\nMAP estimation (step 7). The clean testing i-vectors es-\ntimated in step 7 are used for testing experiments in the\nproposed method.\nIn step 3, we use a noise detector which has the fol-\nlowing steps: We ﬁrst apply a windowing of 32 ms on the\nsong’s audio signal. Then for each window, we calculate\nthe energy of the signal. Using a ﬁxed threshold in energy\nof each window, we look at the beginning and ending area\nof each song to detect the areas with lower energy in noisy\ntesting songs. We keep these areas as a set of noise audio\nsamples. These samples are low-activity and assumed to\nbe the examples of the additive noise that we are dealing\nwith. This step provides the short sample of noise (a couple\nof seconds) in the use-case example described in Section 1.\nFurther, for each noise area detected in a noisy testing\nsong, we estimate the SNR of the song and the noise sam-\nple. We select a limited number of noise areas with longer\ndurations3. By adding the selected noise areas (noise\naudio samples) to clean training songs with the estimated\nSNR, we synthesize another audio set from our clean train-\ning songs which we know as polluted training songs.\nEstimating the parameters of noise in step 6 is as fol-\nlows: After extracting i-vectors from polluted training\nsongs, we compute noise in i-vector space for each song\nby subtracting a clean training i-vector Xifrom polluted\ntraining i-vector of that speciﬁc song Yias follows:\nN′\ni=Y′\ni−Xi (15)\nwhere Y′iis the polluted training i-vector extracted\nfrom polluted training song SpiandXiis the clean training\ni-vector extracted from clean training song SciandN′\niis\nthe noise related to Y′iin i-vector space. Now the param-\neters of noise in i-vector space ( µN,ΣN) can be calculated\nby:\ntraining audios in training set, using audio samples of noise detected in\nnoisy testing set. Noisy testing i-vectors are i-vectors extracted from\nnoisy audios in testing set, polluted training i-vectors are i-vectors ex-\ntracted from polluted training audios, and clean training i-vectors are i-\nvectors extracted from clean training audios. Clean testing i-vectors are\nestimated via MAP from noisy testing i-vectors.\n3These noise areas are usually very short in time (a couple of seconds).µN=mean (N′) (16)\nΣN=cov(N′) (17)\nwhere\nN′={N′\ni|i= 1, ..., n } (18)\nandnis the number of training songs.\nTo use the proposed method in an adaptive way, we only\nneed new audio samples of noise (which in our use-case we\nassumed the mobile app can provide, also described a fea-\nsible solution in step 3 about how to prepare them) to create\na new set of polluted i-vectors to update the parameters of\nnoise in the i-vector space. When the noise is changed, our\nparameters ( µN,ΣN) can also be updated to that noise.\n5. EXPERIMENTS\n5.1 I-vector Extractor\nOur i-vector extractor consists of a UBM with 1024 Gaus-\nsian components. This UBM is trained on all the MFCCs\nof the clean training songs in each fold. The 0thand1st\norder statistics (also known as statistical supervectors) are\ncalculated for each song from MFCC features of the song\nusing the UBM. The i-vector space matrix ( T) is learned\nfrom the statistical supervectors, via an Expectation Max-\nimization (EM) algorithm described in [4, 15] where Tis\ninitialized from random and i-vector space dimensionality\nis set to 400. Using Tmatrix, 400dimensional i-vectors\nare extracted for both training and testing set. All the i-\nvector extraction procedure is done unsupervised. We use\n20-dimensional MFCCs in all of our i-vector based sys-\ntems, extracted with RASTAMAT [10] toolbox with the\nsame conﬁguration as used in [7, 11]. The i-vector space\nmatrix (T) is trained using MSR identity toolbox [26].\n5.2 Inter-class Compensation and Scoring\nAs we described in Section 3, i-vectors contain both artist\nand song variability. To reduce the song variability in i-\nvector space, multiple inter-class compensation methods\nsuch as length normalization, LDA and WCCN are found\neffective [4, 12]. Our i-vector inter-class compensation\nconsists of 3 modules: 1) length-normalization, 2) LDA\nand 3) WCCN. For the scoring, we use a simple cosine\nscoring approach as detailed in [3].\nLength of i-vectors causes negative effects in i-vector\nspace [3, 12]. To discard these effects, we normalize the\nlength of i-vectors by dividing each i-vector by its length.\nThus, both training and testing i-vectors are ﬁrst length\nnormalized [12]. Using the resulting clean training i-\nvectors, a LDA projection matrix Vis trained and both\ntraining and testing i-vectors are projected using V. Then\nthe resulting clean training i-vectors are length normalized\nagain and then used to train a WCCN projection matrix\nB. The WCCN matrix is used to project both training\nand testing i-vectors resulted from the LDA step. The ﬁ-\nnal WCCN-projected i-vectors are used for cosine scoring.\nFor each testing i-vector, a similarity score is calculated\nfor each class separately. These scores are calculated given712 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016the model i-vectors that are computed by averaging LDA-\nWCCN projected clean training i-vectors for each class.\nAnd ﬁnally, the class with the maximum score is chosen as\nthe predicted label for the testing i-vector.\n5.3 Within-Class Covariance Normalization (WCCN)\nWithin-Class Covariance Normalization (WCCN) pro-\nvides an effective compensation that can be used with co-\nsine scoring. After i-vectors are length normalized and\nprojected by LDA, they are encountered with WCCN pro-\njection matrix. WCCN scales the i-vector space in the\nopposite direction of its inter-class covariance matrix, so\nthat directions of intra-artist variability are improved for\ni-vector scoring. The within-class covariance is estimated\nusing clean training i-vectors as follows:\nW=1\nAα∑\na=11\nnana∑\ni=1(wa\ni−wa)(wa\ni−wa)t(19)\nwhere wa=1\nna∑na\ni=1waiis the mean of the LDA pro-\njected i-vectors for each artist α.Ais the total number\nof artists, and nais the number of songs of each artist α\nin training set. We use the inverse of the Wmatrix to\nnormalize the direction of the projected i-vectors. WCCN\nprojection matrix Bcan be estimated such that:\nBBt=W−1(20)\n5.4 Cosine Scoring\nIn the i-vector space, a simple cosine scoring has been suc-\ncessfully used to compare two i-vectors [3]. Given a length\nnormalized, LDA and WCCN projected i-vector wifrom\nan unknown artist, cosine score for artist αis deﬁned as:\nscore (wα, wi) =(wα)t.wi\n∥wα∥.∥wi∥(21)\nwhere wαrepresents the mean i-vector of artist α, cal-\nculated by averaging all the length normalized, LDA and\nWCCN projected clean training i-vectors extracted from\nall the songs of artist α.score (wα, wi)represents the co-\nsine score of testing i-vector wifor artist α. To predict the\nartist label for i-vector wi, the artist with the highest cosine\nscore is chosen as the label for wi.\n5.5 Dataset\nIn our experiments, we used Artist20 dataset [11], a freely\navailable 20-class artist recognition corpus which consists\nof 1413 mp3 songs from 20 different artists mostly in pop\nand rock genres. The dataset is composed of six albums\nfrom each of 20 artists. A 6-fold cross validation is also\nprovided with the dataset which is used in all of our exper-\niments. In each fold, 5 out of 6 albums from all the artists\nwere used for training and the rest were used for testing.\nFor our experiments with noisy data, we synthesized 12\n(4×3)different noisy sets from Artist20 dataset with 4\ndifferent kinds of additive noise (festival, humming, pink,\npub environment) of 3 different SNRs (5db, 10 db and 20db), by applying the noise to all the songs. For apply-\ning the noise, the open-source Audio Degradation Toolbox\n(ADT) [23] is used.\nFor all the experiments (except IVEC-CLN and EBLF-\nCLN), the models are trained on training folds of clean\ndataset and tested on the testing fold of noisy dataset.\nFor IVEC-CLN and EBLF-CLN experiments, models are\ntrained on training folds of clean dataset and tested on test-\ning fold of clean dataset.\nWe found noise samples of festival noise in the\nFreeSound repository4. The festival noise sample is an\naudio recording from a live performance during a festi-\nval with a lot of cheering sounds and human speaking in\nloudspeaker. This noise example is available upon request.\nFor the other additive noises (pub environment, pink-noise,\nhumming) the noise samples provided in ADT are used.\nThe pub environment noise sample, recorded in a crowded\npub, and the humming noise is recorded from a damaged\nspeaker.\n5.6 Evaluation\nTo evaluate the performance of different methods dealing\nwith different kinds of noise, the averaged Fmeasure over\nall the classes is used5. The reported results in Table 1\nshow the mean of the averaged Fmeasures over 6-folds of\nour cross-validation, ±the standard deviation (std) of the\naveraged Fmeasures over folds. To examine the statistical\nsigniﬁcance, a t-test is applied for each sets of experiments\nseparately, comparing the Fmeasures of 6 folds between\nthe proposed method and each of the baselines (for exam-\nple: festival noise with 3 db SNR, comparing IVEC-NSY\nand EBLF-NSY). Each set of experiments for a speciﬁc\nkind of noise with a certain SNR is done independently.\n5.7 Baseline Methods\nWe compare the performance of our proposed method\nwith two baselines. The ﬁrst baseline is a state-of-the-art\nstandard i-vector based artist recognition system known\nasIVEC-NSY . The reason we chose this baseline is to\nshow the improvements by adding the estimation step to\nthis baseline. We extract 400dimensional i-vectors us-\ning 20-dimensional MFCCs and a 1024 components GMM\nas UBM. Then we apply length normalization, LDA and\nWCCN and further apply the cosine scoring to predict the\nlabels.\nThe second baseline ( EBLF-NSY ) uses an extended ver-\nsion of Block-Level features [28] (EBLF) used in [27].\nEBLF are the winner of multiple tasks in MIREX chal-\nlenge6such as music similarity and genre classiﬁcation\nand provide a set of 8 song-level descriptors which repre-\nsent different characteristics for a song. These 8 descrip-\ntors contain a good variety of features such as rhythm and\n4http://freesond.org\n5Since the number of songs from each artist are more or less the same\n(6 albums), the averaged Fmeasure seems to be a good measurement for\nour multi-class artist recognition task.\n6Annual Music Information Retrieval eXchange (MIREX). More in-\nformation is available at: http://www.music-ir.orgProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 713timbre in a song. Since these features are frequently used\nfor multiple purposes in MIR, we chose them as our second\nbaseline to show how they perform in a noisy environment.\nFor classiﬁcation, a WEKA [13] implementation of SMO\nsupport vector machine is used as described in [27]. The\nSVM model in this baseline is trained on features of clean\ntraining songs and tested on features of noisy testing songs.\nTo demonstrate the maximum power of our baselines\non clean data, we also provided the performance of these\nmethods, dealing with only clean data. In these speciﬁc ex-\nperiments, we train and test the baselines using clean data\nand the results can be found in the description of Table 1\nasIVEC-CLN andEBLF-CLN .\nThe performance of EBLF-CLN is comparable with\nthe baselines in [7] which compares different approaches\nfor music artist recognition and therefore is a competitive\nmethod to be used as a baseline. Also, the performance of\nIVEC-CLN is comparable with the the best results achieved\nusing a state-of-the-art i-vector based artist recognition\nsystem reported in [7] (which are known to be the best\nartist recognition results on Artist20 dataset published so\nfar). Both IVEC-CLN and [7] use similar i-vector extrac-\ntors. The difference between IVEC-CLN and [7] is in the\ninter-class compensation and scoring. We used LDA fol-\nlowed by WCCN as inter-class compensation and a simple\ncosine scoring, while in [7] only LDA is used as inter-class\ncompensation and the best performance achieved with a\nDiscriminant Analysis classiﬁer.\n5.8 The Proposed Method\nOur proposed method is shown in Table 1 as IVEC-MAP .\nAll the i-vector extraction (UBM, T), inter-class compen-\nsation (LDA,WCCN) and scoring models are only trained\nwith clean training data and the only extra information\nused in the proposed method compared to the baselines,\nis the mean and covariance of noise in i-vector space. The\nnumber of at least 500i-vectors are needed to estimate the\nparameters of noise as reported in [16] in a speaker veriﬁ-\ncation scenario. We used all the polluted training i-vectors\nfor parameter estimation, since our training set is not very\nbig (∼1100 songs).\n5.9 Results and Discussion\nBy looking at Table 1 it can be seen that the proposed\nmethod outperformed the baselines in all 12 cases of 4\ndifferent additive noises with 3 different SNRs. Having\na closer look at the results suggests the IVEC-NSY base-\nline performed much better and more robust than EBLF-\nNSY .By looking at the results dealing with noises of dif-\nferent SNR levels, it can be seen that as expected the lower\nthe SNR (more noise), the lower the performance of our\nbaselines are. Unlike the baselines, the change in SNR\ndoes not affect the performance of our proposed method\nsigniﬁcantly in festival and humming noises. Considering\nthe standard deviation of the averaged Fmeasures for all the\n6-folds, results suggest that the proposed method is always\nhigher than 1 std from the performance of EBLF-NSY in\nall the noises with all different SNRs. When the noiseAveraged Fmeasure (%)\nsnr nois. IVEC-NSY EBLF-NSY IVEC-MAP\nfest. 68.22 ±8.85 19.01 ±23.31 81.08 ±7.68\nhum. 75.28 ±8.14 5.21 ±5.15 82.56 ±6.82\npink 60.44 ±10.27 6.64 ±11.58 74.88 ±6.675 db\npub 44.11 ±8.13 14.01 ±22.27 71.12 ±8.09\nfest. 74.27 ±8.97 24.32 ±27.39 81.91 ±7.55\nhum. 77.15 ±8.97 25.71 ±26.5 82.64 ±7.24\npink 68.58 ±8.22 11.89 ±16.38 78.05 ±7.210 db\npub 66.15 ±9.04 22.7 ±25.91 79.87 ±7.31\nfest. 77.28 ±7.6 36.12 ±26.76 81.89 ±7.47\nhum. 77.32 ±7.43 36.89 ±25.3 82.86 ±7.1\npink 74.57 ±7.63 13.93 ±21.31 80.54 ±7.5320 db\npub 76.74 ±7.8 26.17 ±29.05 82.63 ±7.2\nTable 1 . Comparison of artist recognition performance of\ndifferent methods on Artist20 dataset dealing with differ-\nent kinds and levels of additive noise. The numbers indi-\ncate the averaged Fmeasure (as described in Section 5.6)\nwith the standard deviation over all the folds. The per-\nformance of the baselines IVEC-CLN and EBLF-CLN on\nclean data are 83.73 ±7.58 and 72.26 ±7.42 respectively.\nis in its highest level (SNR=5 db) the proposed method’s\nFmeasure is higher than IVEC-NSY by 1 std. On average,\nthe proposed method achieved the relative averaged Fmea-\nsure of 28.41, 12.99 and 7.21 percentage points higher than\nIVEC-NSY encountering additive noises with 5, 10 and 20\ndb SNR, respectively. Applying a t-test hypothesis test-\ningon the averaged Fmeasures for different folds to ex-\namine the performance between the proposed method and\nthe baselines (IVEC-NSY and EBLF-NSY), shows that the\ntest rejects the null hypothesis at 5% signiﬁcance level for\nall the experiments and our results are statistically signiﬁ-\ncant.\n6. CONCLUSION\nIn this paper, we proposed a noise-robust artist recogni-\ntion system using i-vector features and a MAP estima-\ntion of clean i-vectors given noisy i-vectors. Our method\noutperformed the state-of-the-art standard i-vector system\nand EBLF, also showed a stable performance dealing with\nmultiple kinds of additive noise with different SNRs. We\nshowed that by adding an estimation step to a standard i-\nvector based artist recognition system, the performance in\nnoisy environments can be signiﬁcantly improved.\n7. ACKNOWLEDGMENTS\nWe would like to acknowledge the help by Mohamad\nHasan Bahari of KU Leuven University to this work. We\nalso appreciate helpful suggestions of Waad Ben-Kheder\nfrom University of Avignon. This work was supported by\nthe Austrian Science Fund (FWF) under grant no. Z159\n(Wittgenstein Award).714 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20168. REFERENCES\n[1]Mohamad Hasan Bahari, Rahim Saeidi, David Van Leeuwen,\net al. Accent recognition using i-vector, gaussian mean su-\npervector and gaussian posterior probability supervector for\nspontaneous telephone speech. In ICASSP . IEEE, 2013.\n[2]Waad Ben Kheder, Driss Matrouf, Jean-Franc ¸ois Bonastre,\nMoez Ajili, and Pierre-Michel Bousquet. Additive noise com-\npensation in the i-vector space for speaker recognition. In\nICASSP . IEEE, 2015.\n[3]Najim Dehak, Reda Dehak, James R Glass, Douglas A\nReynolds, and Patrick Kenny. Cosine similarity scoring with-\nout score normalization techniques. In Odyssey , page 15,\n2010.\n[4]Najim Dehak, Patrick Kenny, R ´eda Dehak, Pierre Du-\nmouchel, and Pierre Ouellet. Front-end factor analysis for\nspeaker veriﬁcation. Audio, Speech, and Language Process-\ning, IEEE Transactions on , 2011.\n[5]Najim Dehak, Pedro A Torres-Carrasquillo, Douglas A\nReynolds, and Reda Dehak. Language recognition via i-\nvectors and dimensionality reduction. In INTERSPEECH .\nCiteseer, 2011.\n[6]Hamid Eghbal-zadeh, Bernhard Lehner, Markus Schedl, and\nGerhard Widmer. I-vectors for timbre-based music similarity\nand music artist classiﬁcation. In ISMIR , 2015.\n[7]Hamid Eghbal-zadeh, Markus Schedl, and Gerhard Widmer.\nTimbral modeling for music artist recognition using i-vectors.\nInEUSIPCO , 2015.\n[8]A El-Solh, A Cuhadar, and RA Goubran. Evaluation of\nspeech enhancement techniques for speaker identiﬁcation in\nnoisy environments. In ISMW . IEEE, 2007.\n[9]Benjamin Elizalde, Howard Lei, and Gerald Friedland. An\ni-vector representation of acoustic environments for audio-\nbased video event detection on user generated content. In\nISM. IEEE, 2013.\n[10] Daniel PW Ellis. PLP and RASTA (and MFCC, and inver-\nsion) in Matlab, 2005. online web resource.\n[11] Daniel PW Ellis. Classifying music audio with timbral and\nchroma features. In ISMIR , 2007.\n[12] Daniel Garcia-Romero and Carol Y Espy-Wilson. Analysis of\ni-vector length normalization in speaker recognition systems.\nInINTERSPEECH , 2011.\n[13] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard\nPfahringer, Peter Reutemann, and Ian H Witten. The weka\ndata mining software: an update. ACM SIGKDD explorations\nnewsletter , 2009.\n[14] Cemal Hanilc ¸i, Tomi Kinnunen, Rahim Saeidi, Jouni Po-\nhjalainen, Paavo Alku, F Ertas ¸, Johan Sandberg, and\nMaria Hansson-Sandsten. Comparing spectrum estimators\nin speaker veriﬁcation under additive noise degradation. In\nICASSP . IEEE, 2012.\n[15] Patrick Kenny. Joint factor analysis of speaker and session\nvariability: Theory and algorithms. CRIM, Montreal,(Report)\nCRIM-06/08-13 , 2005.\n[16] Waad Ben Kheder, Driss Matrouf, Pierre-Michel Bousquet,\nJean-Franc ¸ois Bonastre, and Moez Ajili. Robust speaker\nrecognition using map estimation of additive noise in i-\nvectors space. In Statistical Language and Speech Process-\ning. Springer, 2014.[17] Anna M Kruspe. Improving singing language identiﬁcation\nthrough i-vector extraction. In DAFx , 2011.\n[18] Yun Lei, Lukas Burget, Luciana Ferrer, Martin Graciarena,\nand Nicolas Scheffer. Towards noise-robust speaker recog-\nnition using probabilistic linear discriminant analysis. In\nICASSP . IEEE, 2012.\n[19] Yun Lei, Lukas Burget, and Nicolas Scheffer. A noise robust\ni-vector extractor using vector taylor series for speaker recog-\nnition. In ICASSP . IEEE, 2013.\n[20] Yun Lei, Moray McLaren, Luciana Ferrer, and Nicolas Schef-\nfer. Simpliﬁed vts-based i-vector extraction in noise-robust\nspeaker recognition. In ICASSP . IEEE, 2014.\n[21] D Martinez, Lukas Burget, Themos Stafylakis, Yun Lei,\nPatrick Kenny, and Eduardo Lleida. Unscented transform for\nivector-based noisy speaker recognition. In ICASSP . IEEE,\n2014.\n[22] Driss Matrouf, Waad Ben Kheder, Jean-Franc ¸ois Bonastre,\nMoez Ajili, and Pierre-Michel Bousquet. Dealing with addi-\ntive noise in speaker recognition systems based on i-vector\napproach. In EUSIPCO . IEEE, 2015.\n[23] Matthias Mauch and Sebastian Ewert. The audio degradation\ntoolbox and its application to robustness evaluation. In IS-\nMIR, 2013.\n[24] Simon JD Prince and James H Elder. Probabilistic linear dis-\ncriminant analysis for inferences about identity. In Computer\nVision, ICCV . IEEE, 2007.\n[25] Seyed Omid Sadjadi and John HL Hansen. Assessment of\nsingle-channel speech enhancement techniques for speaker\nidentiﬁcation under mismatched conditions. In INTER-\nSPEECH , 2010.\n[26] Seyed Omid Sadjadi, Malcolm Slaney, and Larry Heck. Msr\nidentity toolbox-a matlab toolbox for speaker recognition re-\nsearch. Microsoft CSRC , 2013.\n[27] Klaus Seyerlehner, Markus Schedl, Tim Pohle, and Peter\nKnees. Using block-level features for genre classiﬁcation, tag\nclassiﬁcation and music similarity estimation. MIREX , 2010.\n[28] Klaus Seyerlehner, Gerhard Widmer, and Tim Pohle. Fusing\nblock-level features for music similarity estimation. In DAFx ,\n2010.\n[29] Rui Xia and Yang Liu. Using i-vector space model for emo-\ntion recognition. In INTERSPEECH , 2012.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 715"
    },
    {
        "title": "Beat Tracking with a Cepstroid Invariant Neural Network.",
        "author": [
            "Anders Elowsson"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416054",
        "url": "https://doi.org/10.5281/zenodo.1416054",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/252_Paper.pdf",
        "abstract": "We present a novel rhythm tracking architecture that learns how to track tempo and beats through layered learning. A basic assumption of the system is that humans understand rhythm by letting salient periodicities in the music act as a framework, upon which the rhythmical structure is interpreted. Therefore, the system estimates the cepstroid (the most salient periodicity of the music), and uses a neural network that is invariant with regards to the cepstroid length. The input of the network consists mainly of features that capture onset characteristics along time, such as spectral differences. The invariant proper- ties of the network are achieved by subsampling the input vectors with a hop size derived from a musically relevant subdivision of the computed cepstroid of each song. The output is filtered to detect relevant periodicities and then used in conjunction with two additional networks, which estimates the speed and tempo of the music, to predict the final beat positions. We show that the architecture has a high performance on music with public annotations.",
        "zenodo_id": 1416054,
        "dblp_key": "conf/ismir/Elowsson16",
        "content": "BEAT TRACKING WITH A CEPSTROID INVARIANT NEURAL NETWORK Anders Elowsson  KTH Royal Institute of Technology elov@kth.se ABSTRACT We present a novel rhythm tracking architecture that learns how to track tempo and beats through layered learning. A basic assumption of the system is that humans understand rhythm by letting salient periodicities in the music act as a framework, upon which the rhythmical structure is interpreted. Therefore, the system estimates the cepstroid (the most salient periodicity of the music), and uses a neural network that is invariant with regards to the cepstroid length. The input of the network consists mainly of features that capture onset characteristics along time, such as spectral differences. The invariant proper-ties of the network are achieved by subsampling the input vectors with a hop size derived from a musically relevant subdivision of the computed cepstroid of each song. The output is filtered to detect relevant periodicities and then used in conjunction with two additional networks, which estimates the speed and tempo of the music, to predict the final beat positions. We show that the architecture has a high performance on music with public annotations. 1. INTRODUCTION The beats of a musical piece are salient positions in the rhythmic structure, and generally the pulse scale that a human listener would tap their foot or hand to in conjunc-tion with the music. As such, beat positions are an emer-gent perceptual property of the musical sound, but in var-ious cases also dictated by conventional methods of no-tating different musical styles. Beat tracking is a popular subject of research within the Music Information Retriev-al (MIR) community. At the heart of human perception of beat are the onsets of the music. Therefore, onset detec-tion functions are commonly used as a front end for beat tracking. The most basic property that characterize these onsets is an increase in energy in some frequency bands. Extracted onsets can either be used in a discretized man-ner as in [9, 18, 19], or continuous features of the onset detection functions can be utilized [8, 23, 28]. As infor-mation in the pitch domain of music is important, chord changes can also be used to guide the beat tracking [26].  After relevant onset functions have been extracted, the periodicities of the music are usually determined by e.g. comb filters [28], the autocorrelation function [10, 19], or by calculating the cepstroid vector [11]. Other ways to understand rhythm are to explicitly model the rhythmic patterns [24], or to combine several different models to get better generalization capabilities [4]. To estimate the beat positions, hidden Markov models [23] or dynamic Bayesian networks (DBNs) have been used [25, 30].  Although onset detection functions often are computed by the spectral flux (SF) of the audio, it has become more common to learn onset detection functions with a neural network (NN) [3, 29]. Given the success of these net-works it is not surprising that the same framework has been successfully used also for detecting beat positions [2]. When these network try to predict beat positions, they must understand how different rhythmical elements are connected; this is a very complex task.   1.1 Invariant properties of rhythm When trying to understand a new piece of music, the lis-tener must form a framework onto which the elements of the music can be deciphered. For example, we use scales and harmony to understand pitch in western music. The tones of a musical piece are not classified by their fun-damental frequency, but by their fundamental frequency in relation to the other tones in the piece. In the same way, for the time dimension of music, the listener builds a framework, or grid, across time to understand how the different sounds or onsets relate to each other. This framework need not initially be at the beat level. In fact, in various music pieces, beat positions are not the first perceptually emergent timing property of the music. In some pieces, we may first get a strong sense of repetition at downbeat positions, or at subdivisions of the beat. In either of these cases, we identify beat positions after an initial framework of rhythm has been established. If we could establish such a correct framework for a learning algorithm, it would be able to build better representations of the rhythmical structure, as the input features would be deciphered within an underlying metrical structure. In this study we try to use this idea to improve beat tracking.  2. METHOD In the proposed system we use multiple neural networks that each try to model different aspects related to rhythm, \n © Anders Elowsson. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: Anders Elowsson. “Beat Tracking with a Cepstroid Invariant Neural Network”, 17th International Society for Music Information Retrieval Conference, 2016. 351   as shown in Figure 1. First we process the audio with harmonic/percussive source separation (HP-separation) and multiple fundamental frequency (MF0) estimation. From the processed audio, features are calculated that capture onset characteristics along time, such as the SF and the pitch flux (PF). Then we try to find the most sali-ent periodicity of the music (which we call the cepstroid), by analyzing histograms of the previously calculated on-set characteristics in a NN (Cep Network). We use the cepstroid to subsample the flux vectors with a hop size derived from a subdivision of the computed cepstroid. The subsampled vectors are used as input features in our cepstroid invariant neural network (CINN). The CINN can track beat positions in complex rhythmic patterns, because the previous processing has made the input vec-tors invariant with regards to the cepstroid of the music. This means that the same neural activation patterns can be used for MEs of different tempi. In addition, the speed of the music is estimated with an ensemble of neural net-works, using global features for onset characteristics as input. As the last learning step, the tempo is estimated. This is done by letting an ensemble of neural networks evaluate different plausible tempo candidates. Finally, the phase of the beat is determined by filtering the output of the CINN in conjunction with the tempo estimate; and beat positions are estimated.  An overview of the system is given in Figure 1. In Sections 2.1-2.4  we describe the steps to calculate the in- \n Figure 1. Overview of the proposed system. The audio is first processed with MF0 estimation and HP-separation. Raw input features for the neural networks are computed and the outputs of the neural networks are combined to build a model of tempo and beats in each song.  put features of our NNs and in Section 2.5 we give an overview of the NNs. In Section 2.6-2.9 we describe the different NNs, and in Section 2.10, we describe how the phase of the beat is calculated. 2.1 Audio Processing The audio waveform was converted to a sampling fre-quency of 44.1 kHz. Then, as a first step, HP-separation was applied. This is a common strategy (e.g. [16]), used to isolate the percussive instruments, so that subsequent learning algorithms can accurately analyze their rhythmic patterns. The source separation of our implementation is based on the method described in [15]. With a median filter across each frame in the frequency direction of a spectrogram, harmonic sounds are detected as outliers, and with a median filter across each frequency bin in the time direction, percussive sounds are detected as outliers. We use these filters to extract a percussive waveform P1 and a harmonic waveform H1, from the original wave-form O. We further suppress harmonic sounds in P1 (such as traces of the vocals or the bass guitar) by applying a median filter in the frequency direction of the Constant-Q transform (CQT), as described in [11, 13]. This additional filtering produces a clean percussive waveform P2, and a harmonic waveform H2 consisting of the traces of pitched sounds filtered out from P1.  The task of tracking MF0s of the audio is usually per-formed by polyphonic transcription algorithms (e.g. [1]). From several of these algorithms, the frame-wise MF0s can be extracted at the semi-tone level. We used a frame-wise estimate from [14], extracted at a hop size of 5.8 ms (256 samples). 2.2 Calculating Flux Matrices P', S' and V'  Three types of flux matrices (P', S' and V') were calculat-ed, all extracted at a hop size of 5.8 ms. 2.2.1 Calculating 𝑃\" Two spectral flux matrices (𝑃#\" and 𝑃$\") were calculated from the percussive waveforms P1 and P2. The short time Fourier transform (STFT) was applied to P1 and P2 with a window size of 2048 samples and the spectral flux of the resulting spectrograms was computed. Let 𝑋&,( represent the magnitude at the ith frequency bin of the jth frame of the spectrograms. The SF for each bin is then given by  𝑃′&,(\t=𝑋&,(−𝑋&,(-.                        (1) In this implementation we used a step size s of 7 (40 ms). 2.2.2 Calculating 𝑉′ The vibrato suppressed SF was computed for waveforms containing instruments with harmonics (H1, H2 and O), giving the flux matrices (𝑉01\", 𝑉02\" and 𝑉3\"). We used the algorithm for vibrato suppression first described in [12] (p. 4), but changed the resolution of the CQT to 36 bins per octave (down from 60) to get a better time resolution. \nB\nInvariant GridCepHP-SeparationMF0estimationMatricesP’S’V'Hist.HPHS CPCSGlob. SF & PFCINNSpeedTempoPhase EstimationEstimated Beat Positions\nNN-OutputNN\nRepresentationsProcessing\nRhythmical Vector Modeling352 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016   First, the spectrogram is computed with the CQT. Then, shifts of a peak by one bin, without an increase in sound level, are suppressed by subtracting the sound level of each bin of the new frame, with the maximum sound lev-el of the adjacent bins in the old frame. This means that for the vibrato suppressed SF (𝑉′), Eqn (1) is changed by including adjacent bins and calculating the maximum value before applying the subtraction.       𝑉′&,(\t=𝑋&,(−max(𝑋&-#,(-.,𝑋&,(-.,𝑋&8#,(-.)       (2) 2.2.3 Calculating 𝑆′ When listening to a melody, we use pitch in conjunction with onset positions to infer the rhythmical structure. Therefore, it seems beneficial to utilize the pitch dimen-sion of music in the beat tracking as well. We calculated the PF by applying the same function as described for the SF in Eqn (1) to the “semigram” – the estimated MF0s in a pitchogram, interpolated to a resolution of one semitone per bin. The output is the rate of change in the semigram, covering pitches between midi pitch 26 and 104, and we will denote this feature matrix as 𝑆′.  2.3 Calculating Histograms HP, HS, CP, and CS Next we compute two periodicity histograms HP and HS from the flux matrices 𝑃#\" and 𝑆\", and then transform them into the cepstroid vectors CP and CS.  The processing is based on a method recently intro-duced in [11]. In this method, a periodicity histogram of inter-onset intervals (IOIs) is computed, with the contri-bution of each onset-pair determined by their spectral similarity and their perceptual impact. The basic idea is that the IOI of two strong onsets with similar spectra (such as two snare hits) should constitute a relevant level of periodicity in the music. In our implementation we in-stead apply the processing frame-wise on 𝑃#\" and 𝑆\", using the spectral similarity and perceptual impact at each inter-frame interval. We use the same notion of spectral simi-larity and perceptual impact as in [11] when computing HP from 𝑃#\", but when we compute HS from 𝑆\", the notion of spectral distance is replaced with tonal distance. First we smooth 𝑆′ in the pitch direction with a Hann window of size 13 (approximately an octave). We then build a his-togram of tonal distances for each frame, letting n repre-sent the nth semitone of 𝑆′ and k the kth frame, giving us the tonal distance at all histogram positions a ∀𝑎∈{1\t,⋯,1900}\t𝑆′D8&\tE−𝑆′D8&8F\tE\t\t(3)#HIEJ$K&J-LH,-IL,⋯\t,LH By using the grid defined by i in Eqn (3), we try to capture similarities in a few consecutive tones. The grid stretches over 100 frames, which corresponds to roughly 0.5 seconds. The idea is that repetitions of small motives occurs at musically relevant periods. To get the cepstroid vector from a histogram, the dis-crete cosine transform (DCT) is first applied. The result-ing spectrum unveils periodically recurring peaks of the histogram. In this spectral representation, frequency rep-resents the period length and magnitude corresponds to salience in the metrical structure. We then interpolate back to the time domain by inserting spectral magnitudes at the position corresponding to their wavelength. Finally, the Hadamard product of the original histogram and the transformed version is computed to reduce noise. The re-sult is a cepstroid vector (CP, CS). The name cepstroid (derived from period) was chosen based on similarities to how the cepstrum is computed from the spectrum. 2.4 Calculating Global SF and PF  Global features for the SF and PF were calculated for our speed estimation. We extracted features from the feature matrices of Section 2.2. The matrices were divided into log-spaced frequency bands over the entire spectrum by applying triangular filters as specified in Table 1. Feature Matrices 𝑃#\" 𝑃$\" 𝑆′ 𝑉3\" 𝑉01\" 𝑉02\" Number of bands 3 3 1,2,4 3 3 3 Table 1. The feature matrices are divided into bands. After the filtering stage we have 22 feature vectors, and each feature vector X is converted into 12 global features. We compute the means 𝑋, 𝑋H.$ and 𝑋H.L, where 0.2 and 0.5 represents the element-wise power (3 features). Also, X is sorted based on magnitude into percentiles, and Hann windows of widths {41, 61}, centered at percentiles {31, 41} are applied (4 features). We finally extract the per-centiles at values {20, 30, 35, 40, 50} (5 features). 2.5 Neural Network Settings Here we define the settings for all neural networks. In the subsequent Sections 2.6-2.9, further details are provided for each individual NN. All networks were standard feed-forward neural networks with one to three hidden layers.  2.5.1 Ensemble Learning We employed ensemble learning by creating multiple in-stances of a network and averaging their predictions. The central idea behind ensemble learning is to use different models that are better than random and more or less un-correlated. The average of these models can then be ex-pected to provide a better prediction than randomly choosing one of them [27]. For the Tempo and Speed networks, we created an ensemble by randomly selecting a subset of the features for the training of 20 networks (Tempo) or 60 networks (Speed). For the CINN, only 3 networks were used in the ensemble due to time con-straints, and all features were used in each network. 2.5.2 Target values The target values in the networks are defined as: • Cep - Classifying if a frame represents a correct (1) or an incorrect cepstroid (0). The beat interval, downbeat interval, and duple octaves above the downbeat or below the beat were defined as correct. Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 353   • CINN - Classifying if the current frame is at a beat position (1), or if it is not at a beat position (0). • Speed - Fitting to the log of the global beat length.  • Tempo - Classifying which of two tempo candidates that is correct (1) and which is incorrect (0). 2.5.3 Settings of the Neural Networks We use scaled conjugate descent to train the networks. In Table 2, settings of the neural networks are defined.  Hidden Epoch EaSt EnLe OL Cep {20, 20, 20} 600 100 - LoSi CINN {25} 1000  3- LoSi Speed {6, 6, 6} 20 4 6040 Li Tempo {20, 20} 100  2060 LoSi Table 2. The settings for the neural networks of the sys-tem. Hidden, denotes the size of the hidden layers and Epoch is the maximum number of epochs we ran the net-work. EaSt defines how many epochs without an increase in performance that were allowed for the internal valida-tion set of the neural networks. EnLe is specified as NENF, where NE is the number of ensembles and NF is the number of randomly drawn features for each ensemble. OL specifies if a logistic activation function (LoSi) or a linear summation (Li) was used for the output layer.  The activation function of the first hidden layer was always a hyperbolic tangent (tanh) unit, and for subse-quent hidden layers it was always a rectified linear unit (ReLU). The use of a mixture of tanh units and ReLUs may seem unconventional but can be motivated. The suc-cess of ReLUs is often attributed to their propensity to alleviate the problem of vanishing gradients [17]. Vanish-ing gradients are often introduced by sigmoid and tanh units when those units are placed in the later layers, be-cause gradients flow backwards through the network dur-ing training. With tanh units in the first layer, only gradi-ents for one layer of weight and bias values will be af-fected. At the same time, the network will be allowed to make use of the smoother non-linearities of the tanh units. 2.6 Cepstroid Neural Network (Cep) In the first NN we compute the most salient periodicity of the music. To do this we use the cepstroid vectors (CP and CS) previously computed in Section 2.3. First, two additional vectors are created from both cepstroid vectors by filtering the vectors with a Gaussian 𝜎=7.5, and a Laplacian of a Gaussian 𝜎=7.5. Then we include octave versions, by interpolating to a time resolution given by  12E,\t\t\t\t\t\t\t\t\t\t\t12E×13,\t\t\t\t\t\t\t\t\t∀𝑛∈{\t−2,−1,0,1,2}\t\t\t\t\t\t\t(4) Finally, much like one layer and one receptive field of a convolutional neural network, we go frame by frame through the vectors, trying to classify each histogram frame as correct or incorrect, depending on if that particu-lar time position corresponds to a correct cepstroid. The input features are the magnitude values of the vectors at each frame. As true targets, the beat interval and the downbeat interval, as well as duple octaves above the downbeat and duple octaves below the beat are used. The output of the network is our final cepstroid vector (C) and the highest peak is used as our cepstroid (𝐶).  2.7 Cepstroid Invariant Neural Network (CINN) After the cepstroid has been computed, we use it to derive the hop size h for our grid in each ME, at which we will subsample the input vectors of the network. By setting h to an appropriate multiple of the cepstroid, the input vec-tors of songs with different tempo (but potentially a simi-lar rhythmical structure) will be synchronized; and the network can therefore make use of the same neural acti-vation patterns for MEs of different tempi. This enables the CINN to easily identify distinct rhythmical patterns (similar to the ability of a human listener). We want a hop size between approximately 50-100 ms, and therefore compute which duple ratio of 70 ms that is closest to the current cepstroid  \t\t\t\t\t\tminEJ⋯,-$,-#,H,#,$,⋯log$70𝐶2E\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(5)\t The value of n, which minimizes the function above, is then used to calculate the hop size h of the ME by ℎ=𝐶2E\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(6) The rather coarse hop size (50-100 ms) is used as we wish to include features from several seconds of audio, without the input layer becoming too large. However, to make the network aware of peaks that slips through the coarse grid, we perform a peak picking on the vector 𝑃#\", which we have first computed by summing 𝑃#\" across fre-quency. For each grid position, we write the magnitude of the closest peak, the absolute distance to the closest peak, as well as the sign of the computed distance to three fea-ture vectors that we will denote by 𝑃.  Just as for the speed features described in Section 2.4, we filter the feature matrices 𝑃#\", 𝑆′ and 𝑉3\" with triangular filters to extract feature vectors. In summary, for each grid position, we extract features by interpolating over the 16 feature vectors defined in Table 3.  Feature 𝑃′# 𝑃 𝑆′ 𝑉3 Number of bands/features 6 3 6 1 Table 3. Feature vectors that are interpolated to the grid defined by the cepstroid. For each frame we try to estimate if it corresponds to a beat (1) or not (0). We include 38 grid-points in each di-rection from the current frame position, resulting in a time window of 2∙ℎ∙38 seconds. At ℎ=70\tms, the time window is approximately 5.3 seconds. The comput-ed beat activation from the CINN will be denoted as the beat vector 𝐵 in the subsequent processing. 354 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016   2.8 Speed Neural Network Octave errors are a prevalent problem in tempo estima-tion and beat tracking, and different methods for choosing the correct tempo octave have previously been proposed [13]. It was recently shown that a continuous measure of the speed of the music can be very effective at alleviating octave errors [11]. We therefore compute a continuous speed estimate, which guides our tempo estimation, using the input features described in Section 2.4. The ground truth annotation of speed 𝐴., is derived from the loga-rithm of the annotated beat length 𝐴𝐵b 𝐴.=log$𝐴𝐵b                                   (7) Eqn (7) is motivated by our logarithmic perception of tempo [6]. As we have very few annotations (1 per ME), we increase the generalization capabilities with ensemble learning. We also use an inner cross validation (5-fold) for the training set. If this is not done, the subsequent tempo network will overestimate the relevance of the computed speed, rendering a decrease in test perfor-mance.  2.9 Tempo Neural Network The tempo is estimated by finding tempo candidates, and letting the neural network perform a classification be-tween extracted candidates to pick the most likely tempo. First, the candidates are extracted by creating a histogram 𝐻d of the beat vector 𝐵 (that we previously extracted with the CINN). The energy at each histogram bin is computed as the sum of the product of the magnitudes of the frames of 𝐵 at the frame offset given by a ∀𝑎∈1\t,⋯,1900\t\t\t\t\t\t\t\t\t\t\t𝐵&∙𝐵&8F&\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(8) We process the histogram to extract a cepstroid vector 𝐶d, by using the same processing scheme as described for 𝐶e in Section 2.3. Peaks are then extracted in both 𝐻d and 𝐶d, and the corresponding beat length of the histo-gram peaks are used as tempo candidates.  The neural network is not directly trained to classify if a tempo candidate is correct or incorrect. Instead, to cre-ate training data, each possible pair of tempo candidates are examined, and the network is trained to classify which of the two candidates in the pair that correspond to the correct tempo (using only pairs with one correct can-didate for the training data).  For testing, the tempo candidate that receives the high-est probability in its match-ups against the other candi-dates is picked as the tempo estimate. This idea was first described in [11] (in that case without using any preced-ing beat tracking and using a logistic regression without ensemble learning). Input features are defined for both tempo candidates in the pair by their corresponding beat length Bl. We compute: • The magnitude at Bl in 𝐻d, 𝐶d and in the feature vec-tors used for the Cep NN (see Section 2.6). We in-clude octave ratios as defined in Eqn (4). • We compute 𝑥=log2𝐵𝑙−𝑆𝑝𝑒𝑒𝑑. Then sgn(𝑥) and 𝑥 are used as features. • A Boolean vector for all musically relevant ratios defined in Eqn (4), where the corresponding index is 1 if the pair of tempo candidates have that ratio. We constrain possible tempo candidates to the range 23-270 BPM. This range is a bit excessive for the given datasets, but will allow the system to generalize better to other types of music with more extreme tempi. 2.10   Phase Estimation At the final stage, we detect the phase of the beat vector and estimate the beat positions. The tempo often drifts slightly in music, for example during performances by live musicians. To model this in a robust way, we com-pute the CQT of the beat vector. The result is a spectro-gram where each frequency corresponds to a particular tempo, the magnitude corresponds to beat strength, and where the phase corresponds to the phase of the beat at specific time positions. The beat vector is upsampled (100 times higher resolution) prior to applying the CQT, and we use 60 bins per octave. We filter the spectrogram with a Hann window of width one tempo octave (60 bins), centered at the frequency that corresponds to the previously computed tempo. As a result, any magnitudes outside of the correct tempo octave are set to 0 in the spectrogram. When the inverse CQT (ICQT) is finally applied to the filtered spectrogram, the result is a beat vector which resembles a sinusoid, where the peaks cor-respond to tentative beat positions. With this processing technique we have jointly estimated the phase and drift, using a fast transform which seems to be suitable for beat tracking. The beat estimations are finally refined slightly by comparing the peaks of the computed sinusoidal beat vec-tor with the peaks of the original beat vector from the CINN. Let us define a grid i, consisting of 100 points, onto which we interpolate phase deviations that are with-in ± 40 % of the estimated beat length. We then create a “driftogram” M by evaluating each estimated beat posi-tion j, adding 1 to each drift position Mi, j where a peak was found in the original beat vector. The driftogram is smoothed with a Hann window of size 17 across the beat direction and size 27 across the drift direction. To adjust the beat position, we use the the maximum value for each beat frame of M.  3. EVALUATION 3.1 Datasets We used the three datasets defined in Table 4 to evaluate our system. The Ballroom datasets consist of ballroom dance music and was annotated by [20, 24]. The Hains-worth dataset [21] is comprised of varying genres, and Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 355   the SMC dataset [22] consists of MEs that were chosen based on their difficulty and ambiguity. Tempo annota-tions were computed by picking the highest peak of a smoothed histogram of the annotated inter-beat intervals. Dataset Number of MEs Length Ballroom 698 6h 4m Hainsworth 222 3h 20m SMC 217 2h 25m Table 4. Datasets used for evaluation, and their size. 3.2 Evaluation Metrics There are several different metrics for beat tracking, all trying to capture different relevant aspects of the perfor-mance. For an extensive review of different evaluation metrics, we refer the reader to [7].  F-measure is calculated from Recall and Accuracy, using a limit of ± 70 ms for the beat positions. P-Score measures the correlation between annotations and detec-tions. CMLc is derived by finding the longest Correct Metrical Level with continuity required and CMLt is similar to CMLc but does not require continuity. AMLc is derived by finding the longest Allowed Metrical Level with continuity required. This measure allows for several different metrical levels and off-beats. AMLt is Similar as AMLc but does not require continuity. The standard tempo estimation metric Acc1 was computed from the output of the Tempo Network. It corresponds the fraction of MEs that are within 8 % of the annotated tempo. 3.3 Evaluation procedure We used a 5-fold cross validation to evaluate the system on the Ballroom dataset. More specifically, the training fold was used to train all the different neural networks of the system. After all networks were trained, the test fold was evaluated on the complete system and the results re-turned. Then the procedure was repeated with the next train/test-split. The Hainsworth and SMC datasets were evaluated by running the MEs on a system previously trained on the complete Ballroom dataset.  As a benchmark for our cross-fold validation results on the Ballroom dataset, we use the cross-fold validation re-sults of the state-of-the-art systems for tempo estimation [5], and beat tracking [25]. The systems were evaluated on a song-by-song basis with data provided by the au-thors. To make statistical tests we use bootstrapping for paired samples, with a significance level of p < 0.01. For the Hainsworth and SMC dataset, benchmarking is most appropriate with systems that were trained on separate training sets. We use [16] as a benchmark for tempo es-timation, and [8] as a benchmark for beat tracking. 4. RESULTS 4.1 Tempo The tempo estimation results (Acc1), are shown in Table 5, together with the results of the benchmarks.        (Acc1)   Ballroom  Hainsworth      SMC  Proposed      0.973*       0.802     0.332  Böck [5]      0.947*       0.865*     0.576*  Gkiokas [16]      0.625       0.667     0.346 Table 5. The results for our tempo estimation system in comparison with the benchmarks. Results marked with (*) were obtained from cross-fold validation. Results in bold are most relevant to compare. Statistical significance for systems with song-by-song data in comparison with the proposed system is underlined.  4.2 Beat tracking Table 6 shows the performance of the system, evaluated as described in Section 3.2.  Ballroom F-Me P-Sc CMLc CMLt AMLc AMLt Proposed 92.5* 92.2* 86.8* 90.3* 89.4* 93.2* Krebs [25] 91.6* 88.8* 83.6* 85.1* 90.4* 92.2*                                         Hainsworth Proposed 74.2 77.7 57.6 67.6 65.0 79.2 Davies [8] - - 54.8 61.2 68.1 78.9                                              SMC Proposed 37.5 49.4 14.9 22.5 20.9 33.2 Table 6. The results for our proposed system in compari-son with the benchmarks. Results marked with (*) were obtained from a cross-fold validation. Statistical signifi-cance for systems with song-by-song data in comparison with the proposed system is underlined. 5. SUMMARY & CONCLUSIONS We have presented a novel beat tracking and tempo esti-mation system that uses a cepstroid invariant neural net-work. The many connected networks make it possible to explicitly capture different aspects of rhythm. With a Cep network we compute a salient level of repetition of the music. The invariant representations that were computed by subsampling the feature vectors allowed us to obtain an accurate beat vector in a CINN. By applying the CQT to the beat vector, and then filtering the spectrogram to keep only magnitudes that corresponds to the estimated tempo before applying the ICQT, we computed the phase of the beat. Alternative post processing strategies, such as applying a DBN on the beat vector, could potentially im-prove the performance. The results are comparable to the benchmarks both for tempo estimation and beat tracking. This indicates that the ideas put forward in this paper are important, and we hope that they can inspire new network architectures for MIR. Tests on hidden datasets for the relevant MIREX tasks would be useful to draw further conclusion regarding the performance. 6. ACKNOWLEDGEMENTS Thanks to Anders Friberg for helpful discussions as well as proofreading. This work was supported by the Swedish Research Council, Grant Nr. 2012-4685. 356 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016   7. REFERENCES [1] E. Benetos: “Automatic transcription of polyphonic music exploiting temporal evolution,” Dissertation. Queen Mary, University of London, 2012. [2] S. Böck and M. Schedl: “Enhanced beat tracking with context aware neural networks,” In Proc. of DAFx, 2011. [3] S. Böck, A. Arzt, F. Krebs, and M. Sched: “Online real-time onset detection with recurrent neural networks,” In Proc. of DAFx, 2012. [4] S. Böck, F. Krebs, and G. Widmer: \"A Multi-model Ap-proach to Beat Tracking Considering Heterogeneous Mu-sic Styles,\" In Proc. of ISMIR, 2014. [5] S. Böck, F. Krebs, and G. Widmer: “Accurate tempo esti-mation based on recurrent neural networks and resonating comb filters,” In Proc. of ISMIR, pp. 625-631, 2015. [6] A. T. Cemgil, B. Kappen, P. Desain, and H. Honing: ”On tempo tracking: Tempogram Representation and Kalman filtering,” J. New Music Research, Vol. 29, No. 4, pp. 259-273, 2000. [7] M. E. P. Davies, N. Degara, and M. D. Plumbley: “Evaluation methods for musical audio beat tracking algorithms,” Queen Mary University of London, Centre for Digital Music, Tech. Rep. C4DM-TR-09-06, 2009. [8] M.  Davies and M.  Plumbley: “Context-dependent  beat  tracking  of musical audio,” IEEE Trans on Audio, Speech and Language Processing, Vol. 15, No. 3, pp. 1009–1020, 2007. [9] S. Dixon: “Evaluation of audio beat tracking system be-atroot,” J. of New Music Research, Vol. 36, No. 1, pp. 39–51, 2007. [10] D. Eck. “Beat tracking using an autocorrelation phase ma-trix,” In Proceedings of the IEEE International Confer-ence on Acoustics, Speech and Signal Processing (ICASSP 2007), Vol. 4, pp. 1313–1316, 2007. [11] A. Elowsson and A. Friberg: \"Modeling the perception of tempo,\" J. of Acoustical Society of America, Vol. 137, No. 6, pp. 3163-3177, 2015. [12] A. Elowsson and A. Friberg: “Modelling perception of speed in music audio,” Proc. of SMC, pp. 735-741, 2013. [13] A. Elowsson, A. Friberg, G. Madison, and J. Paulin: “Modelling the speed of music using features from har-monic/percussive separated audio,” Proc. of ISMIR, pp. 481-486, 2013. [14] A. Elowsson and A. Friberg: “Polyphonic Transcription with Deep Layered Learning,” MIREX Multiple Funda-mental Frequency Estimation & Tracking, 2 pages, 2014. [15] D. FitzGerald: “Harmonic/percussive separation using median filtering,” Proc. of DAFx-10, 4 pages, 2010. [16] A. Gkiokas, V. Katsouros, G. Carayannis, and T. Stafylakis: “Music tempo estimation and beat tracking by applying source separation and metrical relations,” In Proc. of ICASSP, pp. 421–424, 2012.  [17] X. Glorot, Xavier, A. Bordes, and Y. Bengio: \"Deep sparse rectifier neural networks,\" International Confer-ence on Artificial Intelligence and Statistics, 2011. [18] M. Goto and Y. Muraoka: “Music understanding at the beat level real-time beat tracking for audio signals,” in Proc. of IJCAI (Int. Joint Conf. on AI) / Workshop on CASA, pp. 68–75, 1995. [19] M. Goto and Y. Muraoka: “Beat tracking based on multiple agent architecture a real-time beat tracking system for audio signals,” In Proc. of the International Conference on Multiagent Systems, pp. 103–110, 1996. [20] F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, G. Tzane-takis, C. Uhle, and P. Cano: “An experimental comparison of audio tempo induction algorithms,” IEEE Trans. on Audio, Speech and Language Processing, Vol. 14, No. 5, pp. 1832-1844, 2006. [21] S. Hainsworth and M. Macleod: “Particle filtering applied to musical tempo tracking,” EURASIP J. on Applied Sig-nal Processing, Vol. 15, pp 2385–2395, 2004. [22] A. Holzapfel, M. E. P. Davies, J. R. Zapata, J. L. Oliveira, and F. Gouyon: “Selective sampling for beat tracking evaluation,” IEEE Trans. on Audio, Speech, and Language Processing, Vol. 20, No. 9, pp. 2539–2548, 2012. [23] A. Klapuri, A. Eronen, and J. Astola: “Analysis of the me-ter of acoustic musical signals,” IEEE Trans. on Audio, Speech and Language Processing, Vol. 14, No. 1, pp. 342–355, 2006. [24] F. Krebs, S. Böck, and G. Widmer: “Rhythmic pattern modeling for beat and downbeat tracking in musical audio,” In Proc. of ISMIR, pp. 227–232, Curitiba, Brazil, November 2013. [25] F. Krebs, S. Böck, and G. Widmer: “An Efficient State-Space Model for Joint Tempo and Meter Tracking,” In Proc. of ISMIR, pp. 72-78, 2015. [26] G. Peeters and H. Papadopoulos: “Simultaneous beat and downbeat-tracking using a probabilistic framework: Theo-ry and large-scale evaluation,” IEEE Trans. on Audio, Speech, and Language Processing, Vol. 19, No. 6, pp. 1754–1769, 2011. [27] R. Polikar: “Ensemble based systems in decision making,” Circuits and Systems Magazine, IEEE, Vol. 6, No. 3, pp. 21-45, 2006. [28] E.  Scheirer: “Tempo and beat analysis of acoustic  musi-cal signals,” J. Acoust. Soc. Am., Vol. 103, No. 1, pp. 588–601, 1998. [29] J. Schlüter, and S. Böck: \"Musical onset detection with convolutional neural networks,\" In 6th International Workshop on Machine Learning and Music (MML), Pra-gue, Czech Republic. 2013. [30] N. Whiteley, A. Cemgil, and S. Godsill: “Bayesian model-ling of temporal structure in musical audio,” In Proc. of ISMIR, pp. 29–34, 2006. Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 357"
    },
    {
        "title": "Score-Informed Identification of Missing and Extra Notes in Piano Recordings.",
        "author": [
            "Sebastian Ewert",
            "Siying Wang 0001",
            "Meinard Müller",
            "Mark B. Sandler"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1418317",
        "url": "https://doi.org/10.5281/zenodo.1418317",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/123_Paper.pdf",
        "abstract": "A main goal in music tuition is to enable a student to play a score without mistakes, where common mistakes include missing notes or playing additional extra ones. To automat- ically detect these mistakes, a first idea is to use a music transcription method to detect notes played in an audio recording and to compare the results with a corresponding score. However, as the number of transcription errors pro- duced by standard methods is often considerably higher than the number of actual mistakes, the results are often of limited use. In contrast, our method exploits that the score already provides rough information about what we seek to detect in the audio, which allows us to construct a tailored transcription method. In particular, we employ score-informed source separation techniques to learn for each score pitch a set of templates capturing the spectral properties of that pitch. After extrapolating the resulting template dictionary to pitches not in the score, we estimate the activity of each MIDI pitch over time. Finally, making again use of the score, we choose for each pitch an individ- ualized threshold to differentiate note onsets from spurious activity in an optimized way. We indicate the accuracy of our approach on a dataset of piano pieces commonly used in education.",
        "zenodo_id": 1418317,
        "dblp_key": "conf/ismir/EwertWMS16",
        "content": "SCORE-INFORMED IDENTIFICATION OF MISSING AND EXTRA NOTES\nIN PIANO RECORDINGS\nSebastian Ewert1Siying Wang1Meinard M ¨uller2Mark Sandler1\n1Centre for Digital Music (C4DM), Queen Mary University of London, UK\n2International Audio Laboratories Erlangen, Germany\nABSTRACT\nA main goal in music tuition is to enable a student to play a\nscore without mistakes, where common mistakes include\nmissing notes or playing additional extra ones. To automat-\nically detect these mistakes, a ﬁrst idea is to use a music\ntranscription method to detect notes played in an audio\nrecording and to compare the results with a corresponding\nscore. However, as the number of transcription errors pro-\nduced by standard methods is often considerably higher\nthan the number of actual mistakes, the results are often\nof limited use. In contrast, our method exploits that the\nscore already provides rough information about what we\nseek to detect in the audio, which allows us to construct\na tailored transcription method. In particular, we employ\nscore-informed source separation techniques to learn for\neach score pitch a set of templates capturing the spectral\nproperties of that pitch. After extrapolating the resulting\ntemplate dictionary to pitches not in the score, we estimate\nthe activity of each MIDI pitch over time. Finally, making\nagain use of the score, we choose for each pitch an individ-\nualized threshold to differentiate note onsets from spurious\nactivity in an optimized way. We indicate the accuracy of\nour approach on a dataset of piano pieces commonly used\nin education.\n1. INTRODUCTION\nAutomatic music transcription (AMT) has a long history\nin music signal processing, with early approaches dating\nback to the 1970s [1]. Despite the considerable interest\nin the topic, the challenges inherent to the task are still\nto overcome by state-of-the-art methods, with error rates\nfor note detection typically between 20 and 40 percent, or\neven above, for polyphonic music [2 –8]. While these error\nrates can drop considerably if rich prior knowledge can\nbe provided [9, 10], the accuracy achievable in the more\ngeneral case still prevents the use of AMT technologies in\nmany useful applications.\nThis paper is motivated by a music tuition application,\nc/circlecopyrtSebastian Ewert, Siying Wang, Meinard M ¨uller and Mark\nSandler. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Sebastian Ewert, Siying Wang,\nMeinard M ¨uller and Mark Sandler. “Score-Informed Identiﬁcation of\nMissing and Extra Notes in Piano Recordings”, 17th International Society\nfor Music Information Retrieval Conference, 2016.\n(a)\n(b)\n(c)\nFigure 1 .Given (a) an audio recording and (b) a score (e.g.\nas a MIDI ﬁle) for a piece of music, our method (c) estimates\nwhich notes have been played correctly (green/light crosses), have\nbeen missed (red/dark crosses for pitch 55) or have been added\n(blue/dark crosses for pitch 59) in the recording compared to the\nscore.\nwhere a central learning outcome is to enable the student to\nread and reproduce (simple) musical scores using an instru-\nment. In this scenario, a natural use of AMT technologies\ncould be to detect which notes have been played by the stu-\ndent and to compare the results against a reference score –\nthis way one could give feedback, highlighting where notes\nin the score have not been played ( missed notes ) and where\nnotes have been played that cannot be found in the score30(extra notes ). Unfortunately, the relatively low accuracy\nof standard AMT methods prevents such applications: the\nnumber of mistakes a student makes is typically several\ntimes lower than the errors produced by AMT methods.\nUsing a standard AMT method in a music tuition sce-\nnario as described above, however, would ignore a highly\nvaluable source of prior knowledge: the score. Therefore,\nthe authors in [11] make use of the score by ﬁrst align-\ning the score to the audio, synthesizing the score using a\nwavetable method, and then transcribing both the real and\nthe synthesized audio using an AMT method. To lower the\nnumber of falsely detected notes for the real recording, the\nmethod discards any detected note if the same note is also\ndetected in the synthesized recording while no correspond-\ning note can be found in the score. Here, the underlying\nassumption is that in such a situation, the local note con-\nstellation might lead to uncertainty in the spectrum, which\ncould cause an error in their proposed method. To improve\nthe results further, the method requires the availability of\nsingle note recordings for the instrument to be transcribed\n(under the same recording conditions) – a requirement not\nunrealistic to fulﬁl in this application scenario but leading\nto additional demands for the user. Under these additional\nconstraints, the method lowered the number of transcription\nerrors considerably compared to standard AMT methods.\nTo the best of the authors’ knowledge, the method presented\nin [11] is the only score-informed transcription method in\nexistence.\nOverall, the core concept in [11] is to use the score in-\nformation to post-process the transcription results from a\nstandard AMT method. In contrast, the main idea in this\npaper is to exploit the available score information to adapt\nthe transcription method itself to a given recording. To this\nend, we use the score to modify two central components of\nan AMT system: the set of spectral patterns used to iden-\ntify note objects in a time-frequency representation, and\nthe decision process responsible for differentiating actual\nnote events from spurious note activities. In particular, af-\nter aligning the score to the audio recording, we employ\nthe score information to constrain the learning process in\nnon-negative matrix factorization similar to strategies used\nin score-informed source separation [12]. As a result, we\nobtain for each pitch in the score a set of template vectors\nthat capture the spectro-temporal behaviour of that pitch –\nadapted to the given recording. Next, we extrapolate the\ntemplate vectors to cover the entire MIDI range (including\npitches not used in the score), and compute an activity for\neach pitch over time. After that we again make use of the\nscore to analyze the resulting activities: we set, for each\npitch, a threshold used to differentiate between noise and\nreal notes such that the resulting note onsets correspond to\nthe given score as closely as possible. Finally, the resulting\ntranscription is compared to the given score, which enables\nthe classiﬁcation of note events as correct, missing or ex-\ntra. This way, our method can use highly adapted spectral\npatterns in the acoustic model eliminating the need for ad-\nditional single note recordings, and remove many spurious\nerrors in the detection stage. An example output of our\nmethod is shown in Fig. 1, where correctly played notesare marked in green, missing notes in red and extra notes in\nblue.\nThe remainder of this paper is organized as follows. In\nSection 2, we describe the details of our proposed method.\nIn Section 3, we report on experimental results using a\ndataset comprising recordings of pieces used in piano edu-\ncation. We conclude in Section 4 with a prospect on future\nwork.\n2. PROPOSED METHOD\n2.1 Step 1: Score-Audio Alignment\nAs a ﬁrst step in our proposed method, we align a score\n(given as a MIDI ﬁle) to an audio recording of a student play-\ning the corresponding piece. For this purpose, we employ\nthe method proposed in [13], which combines chroma with\nonset indicator features to increase the temporal accuracy of\nthe resulting alignments. Since we expect differences on the\nnote level between the score and the audio recording related\nto the playing mistakes, we manually checked the temporal\naccuracy of the method but found the alignments to be ro-\nbust in this scenario. It should be noted, however, that the\nmethod is not designed to cope with structural differences\n(e.g. the student adding repetitions of some segments in the\nscore, or leaving out certain parts) – if such differences are\nto be expected, partial alignment techniques should be used\ninstead [14, 15].\n2.2 Step 2: Score-Informed Adaptive Dictionary\nLearning\nAs a result of the alignment, we now roughly know for each\nnote in the score, the corresponding or expected position in\nthe audio. Next, we use this information to learn how each\npitch manifests in a time-frequency representation of the\naudio recording, employing techniques similarly used in\nscore-informed source separation (SISS). There are various\nSISS approaches to choose from: Early methods essentially\nintegrated the score information into existing signal mod-\nels, which already drastically boosted the stability of the\nmethods. These signal models, however, were designed for\nblind source separation and thus have the trade-off between\nthe capacity to model details ( variance ) and the robustness\nin the parameter estimation ( bias) heavily leaned towards\nthe bias. For example, various approaches make speciﬁc\nassumptions to keep the parameter space small, such as\nthat partials of a harmonic sound behave like a Gaussian\nin frequency direction [16], are highly stationary in a sin-\ngle frame [17] or occur as part of predeﬁned clusters of\nharmonics [6]. However, with score information providing\nextremely rich prior knowledge, later approaches found\nthat the variance-bias trade-off can be shifted considerably\ntowards variance.\nFor our method, we adapt an approach that makes fewer\nassumptions about how partials manifests and rather learns\nthese properties from data. The basic idea is to constrain a\n(shift-invariant) non-negative matrix factorization (NMF)\nbased model using the score, making only use of rough\ninformation and allowing the learning process to identifyProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 31(a)\n (b)\n(c)\n (d)\nFigure 2 .Score-Informed Dictionary Learning: Using multi-\nplicative updates in non-negative matrix factorization, semanti-\ncally meaningful constraints can easily be enforced by setting\nindividual entries to zero (dark blue): Templates and activations\nafter the initialization (a)/(b) and after the optimization process\n(c)/(d).\nthe details, see also [12]. Since we focus on piano record-\nings where tuning shifts in a single recording or vibrato\ndo not occur, we do not make use of shift invariance. In\nthe following, we assume general familiarity with NMF\nand refer to [18] for further details. Let V∈RM×Nbe a\nmagnitude spectrogram of our audio recording, with loga-\nrithmic spacing for the frequency axis. We approximate V\nas a product of two non-negative matrices W∈RM×Kand\nH∈RK×N, where the columns of Ware called (spectral)\ntemplates and the rows in Hthe corresponding activities.\nWe start by allocating two NMF templates to each pitch\nin the score – one for the attack and one for the sustain\npart. The sustain part of a piano is harmonic in nature and\nthus we do not expect signiﬁcant energy in frequencies that\nlie between its partials. We implement this constraint as\nin [12] by initializing for each sustain template only those\nentries with positive values that are close to a harmonic of\nthe pitch associated with the template, i.e. entries between\npartials are set to zero, compare Fig. 2a. This constraint\nwill remain intact throughout the NMF learning process\nas we will use multiplicative update rules and thus setting\nentries to zero is a straightforward way to efﬁciently imple-\nment certain constraints in NMF, while letting some room\nfor the NMF process to learn where exactly each partial is\nand how it spectrally manifests. The attack templates are\ninitialized with a uniform energy distribution to account for\ntheir broadband properties.\nConstraints on the activations are implemented in a sim-\nilar way: activations are set to zero if a pitch is known\nto be inactive in a time segment, with a tolerance used toaccount for alignment inaccuracies, compare Fig. 2b. To\ncounter the lack of constraints for attack templates, the cor-\nresponding activations are subject to stricter rules: attack\ntemplates are only allowed to be used in a close vicinity\naround expected onset positions. After these initializations,\nthe method presented in [12] employs the commonly used\nLee-Seung NMF update rules [18] to minimize a gener-\nalized Kullback-Leibler divergence between VandWH .\nThis way, the NMF learning process reﬁnes the information\nwithin the unconstrained areas on WandH.\nHowever, we propose a modiﬁed learning process that\nenhances the broadband properties for the attack templates.\nMore precisely, we include attack templates to bind the\nbroadband energy related to onsets and thus reduce the\nnumber of spurious note detections. We observed, however,\nthat depending on the piece, the attack templates would\ncapture too much of the harmonic energy, which interfered\nwith the note detection later on. Since harmonic energy\nmanifest as peaks along the frequency axis, we discourage\nsuch peaks for attack templates and favour smoothness\nusing an additional spectral continuity constraint in the\nobjective function:\nf(W,H ) :=/summationdisplay\nm,nVm,nlog(Vm,n\n(WH)m,n)−Vm,n+ (WH)m,n\n+σ/summationdisplay\nm/summationdisplay\nk∈A(Wm,k−Wm−1,k)2\nwhere the ﬁrst sum is the generalized Kullback-Leibler\ndivergence and the second sum is a total variation term\nin frequency direction, with A ⊂ { 1,...,K}denoting\nthe index set of attack templates and σcontrolling\nthe relative importance of the two terms. Note that\nWm,k−Wm−1,k= (F ⋆W :,k)(m), whereW:,kdenotes\nthek-th column of WandF= (−1,1)is a high-pass ﬁlter.\nTo ﬁnd a local minimum for this bi-convex problem, we\npropose the following iterative update rules alternating be-\ntweenWandH(we omit the derivation for a lack of space\nbut followed similar strategies as used for example in [19]):\nWm,k←Wm,k·\n/summationtext\nnHk,nVm,n\n(WH )m,n+IA(k) 2σ(Wm+1,k+Wm−1,k)\n/summationtext\nnHk,n+IA(k) 4σWm,k\nWm,k←Wm,k/summationtext\n/tildewidemW/tildewidem,k\nHk,n←Hk,n·/summationtext\nmWm,kVm,n\n(WH )m,n/summationtext\nmWm,k\nwhereIAis the indicator function for A. The result of this\nupdate process is shown in Fig. 2c and d. It is clearly visible\nhow the learning process reﬁned the unconstrained areas in\nWandH, closely reﬂecting the acoustical properties in\nthe recording. Further, the total variation term led to attack\ntemplates with broadband characteristics for all pitches,\nwhile still capturing the non-uniform, pitch dependent\nenergy distribution typical for piano attacks.32 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20162.3 Step 3: Dictionary Extrapolation and Residual\nModelling\nAll notes not reﬂected by the score naturally lead to a dif-\nference or residual between VandWH as observed also\nin [20]. To model this residual, the next step in our proposed\nmethod is to extrapolate our learnt dictionary of spectral\ntemplates to the complete MIDI range, which enables us\nto transcribe pitches not used in the score. Since we use a\ntime-frequency representation with a logarithmic frequency\nscale, we can implement this step by a simple shift oper-\nation: for each MIDI pitch not in the score, we ﬁnd the\nclosest pitch in the score and shift the two associated tem-\nplates by the number of frequency bins corresponding to\nthe difference between the two pitches. After this operation\nwe can use our recording-speciﬁc full-range dictionary to\ncompute activities for all MIDI pitches. To this end, we\nadd an activity row to Hfor each extrapolated template and\nreset any zero constraints in Hby adding a small value to\nall entries. Then, without updating W, we re-estimate this\nfull-rangeHusing the same update rules as given above.\n2.4 Step 4: Onset Detection Using Score-Informed\nAdaptive Thresholding\nAfter convergence, we next analyze Hto detect note onsets.\nA straightforward solution would be to add, for each pitch\nand in each time frame, the activity for the two templates\nassociated with that pitch and detecting peaks afterwards\nin time direction. This approach, however, leads to sev-\neral problems. To illustrate these, we look again at Fig. 2c,\nand compare the different attack templates learnt by our\nprocedure. As we can see, the individual attack templates\ndo differ for different pitches, yet their energy distribution\nis quite broadband leading to considerable overlap or sim-\nilarity between some attack templates. Therefore, when\nwe compute Hthere is often very little difference with\nrespect to the objective function if we activate the attack\ntemplate associated with the correct pitch, or an attack tem-\nplate for a neighboring pitch (from an optimization point\nof view, these similarities lead to relatively wide plateaus\nin the objective function, where all solutions are almost\nequally good). The activity in these neighboring pitches led\nto wrong note detections.\nAs one solution, inspired by the methods presented\nin [21, 22], we initially incorporated a Markov process into\nthe learning process described above. Such a process can\nbe employed to model that if a certain template (e.g. for the\nattack part) is being using in one frame, another template\n(e.g. for the sustain part) has to be used in the next frame.\nThis extension often solved the problem described above as\nattack templates cannot be used without their sustain parts\nanymore. Unfortunately, the dictionary learning process\nwith this extension is not (bi-)convex anymore and in prac-\ntice we found the learning process to regularly get stuck\nin poor local minima leading to less accurate transcription\nresults.\nA much simpler solution, however, solved the above\nproblems in our experiments similar to the Markov process,\nwithout the numerical issues associated with it: we sim-ply ignore activities for attack templates. Here, the idea\nis that as long as the broadband onset energy is meaning-\nfully captured by some templates, we do not need to care\nabout spurious note detections caused by this energy and\ncan focus entirely on detecting peaks in the cleaner, more\ndiscriminative sustain part to detect the notes (compare also\nFig. 2d). Since this simpler solution turned out to be more\nrobust, efﬁcient and accurate overall, we use this approach\nin the following. The result of using only the sustain activi-\nties is shown in the background of Fig. 1. Comparing these\nresults to standard NMF-based transcription methods, these\nactivities are much cleaner and easier to interpret – a result\nof using learnt, recording-speciﬁc templates.\nAs a next step, we need to differentiate real onsets from\nspurious activity. A common technique in the AMT litera-\nture is to simply use a global threshold to identify peaks in\nthe activity. As another approach often used for sustained\ninstruments like the violin or the ﬂute, hidden Markov mod-\nels (HMMs) implement a similar idea but add capabilities to\nsmooth over local activity ﬂuctuations, which might other-\nwise be detected as onsets [2]. We tried both approaches for\nour method but given the distinctive, fast energy decay for\npiano notes, we could not identify signiﬁcant beneﬁts for\nthe somewhat more complex HMM solution and thus only\nreport on our thresholding based results. A main difference\nin our approach to standard AMT methods, however, is\nthe use of pitch-dependent thresholds, which we optimize\nagain using the score information. The main reason why\nthis pitch dependency is useful is that loudness perception\nin the human auditory system non-linearly depends on the\nfrequency and is highly complex for non-sinusoidal sounds.\nTherefore, to reach a speciﬁc loudness for a given pitch,\na pianist might strike the corresponding key with differ-\nent intensity compared to another pitch, which can lead to\nconsiderable differences in measured energy.\nTo ﬁnd pitch-wise thresholds, our method ﬁrst gener-\natesC∈Nthreshold candidates, which are uniformly dis-\ntributed between 0andmaxk,nHk,n. Next, we use each\ncandidate to ﬁnd note onsets in each activity row in Hthat\nis associated with a pitch in the score. Then, we evaluate\nhow many of the detected onsets correspond to notes in\nthe aligned score, how many are extra and how many are\nmissing – expressed as a precision, recall and F-measure\nvalue for each candidate and pitch. To increase the robust-\nness of this step, in particular for pitches with only few\nnotes, we compute these candidate ratings not only using\nthe notes for a single pitch but include the notes and onsets\nfor theNclosest neighbouring pitches. For example, to\nrate threshold candidates for MIDI pitch P, we compute\nthe F-measure using all onsets and notes corresponding to,\nfor example, MIDI pitch P−1toP+ 1. The result of this\nstep is a curve for each pitch showing the F-measure for\neach candidate, from which we choose the lowest threshold\nmaximizing the F-measure, compare Fig. 3. This way, we\ncan choose a threshold that generates the least amount of\nextra and missing notes, or alternatively, a threshold that\nmaximizes the match between the detected onsets and the\ngiven score. Thresholds for pitches not used in the score areProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 33Figure 3 .Adaptive and pitch-dependent thresholding: For each\npitch we choose the smallest threshold maximizing the F-measure\nwe obtain by comparing the detected offsets against the aligned\nnominal score. The red entries show threshold candidates having\nmaximal F-measure.\ninterpolated from the thresholds for neighbouring pitches\nthat are in the score.\n2.5 Step 5: Score-Informed Onset Classiﬁcation\nUsing these thresholds, we create a ﬁnal transcription result\nfor each pitch. As our last step, we try to identify for each\ndetected onset a corresponding note in the aligned score,\nwhich allows us to classify each onset as either correct\n(i.e. note is played and is in the score) or extra (i.e. played\nbut not in the score). All score notes without a correspond-\ning onset are classiﬁed as missing . To identify these cor-\nrespondences we use a temporal tolerance Tof±250ms,\nwhereTis a parameter that can be increased to account\nfor local alignment problems or if the student cannot yet\nfollow the rhythm faithfully (e.g. we observed concurrent\nnotes being pulled apart by students for non-musical rea-\nsons). This classiﬁcation is indicated in Fig. 1 using crosses\nhaving different colours for each class.\n3. EXPERIMENTS\n3.1 Dataset\nWe indicate the performance of our proposed method using\na dataset1originally compiled in [11]. The dataset com-\nprises seven pieces shown in Table 1 that were taken from\nthe syllabus used by the Associated Board of the Royal\nSchools of Music for grades 1 and 2 in the 2011/2012 pe-\nriod. Making various intentional mistakes, a pianist played\nthese pieces on a Yamaha U3 Disklavier, an acoustic up-\nright piano capable of returning MIDI events encoding the\nkeys being pressed. The dataset includes for each piece an\naudio recording, a MIDI ﬁle encoding the reference score,\nas well as three annotation MIDI ﬁles encoding the extra,\nmissing and correctly played notes, respectively.\nIn initial tests using this dataset, we observed that the\nannotations were created in a quite rigid way. In particular,\nseveral note events in the score were associated with one\nmissing and one extra note, which were in close vicinity\nof each other. Listening to the corresponding audio record-\ning, we found that these events were seemingly played\ncorrectly. This could indicate that the annotation process\nwas potentially a bit too strict in terms of temporal tolerance.\nTherefore, we modiﬁed the three annotation ﬁles in some\ncases. Other corrections included the case that a single\n1available online: http://c4dm.eecs.qmul.ac.uk/rdr/ID Composer Title\n1 Josef Haydn Symp. No. 94: Andante (Hob I:94-02)\n2 James Hook Gavotta (Op. 81 No. 3)\n3 Pauline Hall Tarantella\n4 Felix Swinstead A Tender Flower\n5 Johann Krieger Sechs musicalische Partien: Bourr ´ee\n6 Johannes Brahms The Sandman (WoO 31 No. 4)\n7 Tim Richards (arr.) Down by the Riverside\nTable 1 .Pieces of music used in the evaluation, see also [11].\nscore note was played more than once and we re-assigned\nin some cases which of the repeated notes should be consid-\nered as extra notes and which as the correctly played note,\ntaking the timing of other notes into account. Further, some\nnotes in the score were not played but were not found in\nthe corresponding annotation of missing notes. We make\nthese slightly modiﬁed annotation ﬁles available online2. It\nshould be noted that these modiﬁcations were made before\nwe started evaluating our proposed method.\n3.2 Metrics\nOur method yields a transcription along with a classiﬁ-\ncation into correct, extra and missing notes. Using the\navailable ground truth annotations, we can evaluate each\nclass individually. In each class, we can identify up to\na small temporal tolerance the number of true positives\n(TP), false positives (FP) and false negatives (FN). From\nthese, we can derive the PrecisionP=TP\nTP+FP, the Recall\nR=TP\nTP+FN, the F-measure 2PR/(P+R)and the Ac-\ncuracyA=TP\nTP+FP+FN. We use a temporal tolerance of\n±250ms to account for the inherent difﬁculties aligning dif-\nferent versions of a piece with local differences, i.e. playing\nerrors can lead to local uncertainties which position in the\none version corresponds to which position in the other.\n3.3 Results\nThe results for our method are shown in Table 2 for each\nclass and piece separately. As we can see for the ‘correct’\nclass, with an F-measure of more than 99% the results\nare beyond the limits of standard transcription methods.\nHowever, this is expected as we can use prior knowledge\nprovided by the score to tune our method to detect exactly\nthese events. More interestingly are the results for the events\nwe do not expect. With an F-measure of 94.5%, the results\nfor the ‘missing’ class are almost on the same level as for\nthe ‘correct’ class. The F-measure for the ‘extra’ class is\n77.2%, which would be a good result for a standard AMT\nmethod but it is well below the results for the other two\nclasses.\nLet us investigate the reasons. A good starting point is\npiece number 6 where the results for the ‘extra’ class are\nwell below average. In this recording, MIDI notes in the\nscore with a pitch of 54 and 66 are consistently replaced\nin the recording with notes of MIDI pitch 53 and 65. In\nparticular, pitches 54 and 66 are never actually played in\nthe recording. Therefore, the dictionary learning process\n2http://www.eecs.qmul.ac.uk/ ˜ewerts/34 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016ID Class Prec. Recall F-Meas. Accur.\n1C 100.0 100.0 100.0 100.0\nE 100.0 71.4 83.3 71.4\nM 100.0 100.0 100.0 100.0\n2C 100.0 99.7 99.8 99.7\nE 90.0 81.8 85.7 75.0\nM 92.3 100.0 96.0 92.3\n3C 99.2 99.2 99.2 98.4\nE 100.0 66.7 80.0 66.7\nM 100.0 100.0 100.0 100.0\n4C 98.7 100.0 99.3 98.7\nE 80.0 80.0 80.0 66.7\nM 100.0 85.7 92.3 85.7\n5C 99.5 98.6 99.1 98.1\nE 75.0 92.3 82.8 70.6\nM 87.5 100.0 93.3 87.5\n6C 99.2 99.2 99.2 98.4\nE 50.0 52.9 51.4 34.6\nM 93.3 93.3 93.3 87.5\n7C 99.5 97.1 98.3 96.7\nE 75.0 80.0 77.4 63.2\nM 76.2 100.0 86.5 76.2Avg.C 99.4 99.1 99.3 98.6\nE 81.4 75.0 77.2 64.0\nM 92.8 97.0 94.5 89.9\nTable 2 .Evaluation results for our proposed method in percent.\nFigure 4 .Cause of errors in piece 6: Activation matrix with\nground truth annotations showing the position of notes in the\n‘correct’, ‘extra’ and ‘missing’ classes.\nin step 2 cannot observe how these two pitches manifest\nin the recording and thus cannot learn a meaningful tem-\nplate. Yet, being in a direct neighbourhood, the dictionary\nextrapolation in step 3 will use the learnt templates for pitch\n54 and 66 to derive templates for pitches 53 and 65. Thus,\nthese templates, despite the harmonicity constraints which\nstill lead to some enforced structure in the templates, do not\nwell represent how pitches 53 and 65 actually manifest in\nthe recording and thus the corresponding activations will\ntypically be low. As a result the extra notes were not de-\ntected as such by our method. We illustrate these effects in\nFig. 4, where a part of the ﬁnal full-range activation matrix\nis shown in the background and the corresponding ground-\ntruth annotations are plotted on top as coloured circles. It\nis clearly visible, that the activations for pitch 53 are well\nbelow the level for the other notes. Excluding piece 6 from\nthe evaluation, we obtain an average F-measure of 82% for\n‘extra’ notes.\nFinally, we reproduce the evaluation results reported forClass C E M\nAccuracy 93.2 60.5 49.2\nTable 3 .Results reported for the method proposed in [11]. Re-\nmark: Values are not directly comparable with the results shown\nin Table 2 due to using different ground truth annotations in the\nevaluation.\nthe method proposed in [11] in Table 3. It should be noted,\nhowever, that the results are not directly comparable with\nthe results in Table 2 as we modiﬁed the underlying ground\ntruth annotations. However, some general observations\nmight be possible. In particular, since the class of ‘correct’\nnotes is the biggest in numbers, the results for this class are\nroughly comparable. In terms of accuracy, the number of\nerrors in this class is ﬁve times higher in [11] (6.8 errors vs\n1.4 errors per 100 notes). In this context, we want to remark\nthat the method presented in [11] relied on the availability\nof recordings of single notes for the instrument in use, in\ncontrast to ours. The underlying reason for the difference\nin accuracy between the two methods could be that instead\nof post-processing a standard AMT method, our approach\nyields a transcription method optimized in each step using\nscore information. This involves a different signal model\nusing several templates with dedicated meaning per pitch,\nthe use of score information to optimize the onset detection\nand the use of pitch-dependent detection thresholds. Since\nthe number of notes in the ‘extra’ and ‘missing’ classes are\nlower, it might not be valid to draw conclusions here.\n4. CONCLUSIONS\nWe presented a novel method for detecting deviations from\na given score in the form of missing and extra notes in\ncorresponding audio recordings. In contrast to previous\nmethods, our approach employs the information provided\nby the score to adapt the transcription process from the\nstart, yielding a method specialized in transcribing a spe-\nciﬁc recording and corresponding piece. Our method is\ninspired by techniques commonly used in score-informed\nsource separation that learn a highly optimized dictionary\nof spectral templates to model the given recording. Our\nevaluation results showed a high F-measure for notes in the\nclasses ‘correct’ and ‘missing’, and a good F-measure for\nthe ‘extra’ class. Our error analysis for the latter indicated\npossible directions for improvements, in particular for the\ndictionary extrapolation step. Further it would be highly\nvaluable to create new datasets to better understand the\nbehaviour of score-informed transcription methods under\nmore varying recording conditions and numbers of mistakes\nmade.\nAcknowledgements: This work was partly funded by EP-\nSRC grant EP/L019981/1. The International Audio Labora-\ntories Erlangen are a joint institution of the Friedrich-Alex-\nander-Universit ¨at Erlangen-N ¨urnberg (FAU) and Fraun-\nhofer Institut f ¨ur Integrierte Schaltungen IIS. Sandler ac-\nknowledges the support of the Royal Society as a recipient\nof a Wolfson Research Merit Award.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 355. REFERENCES\n[1]James A Moorer. On the transcription of musical sound\nby computer. Computer Music Journal , pages 32–38,\n1977.\n[2]Anssi P. Klapuri and Manuel Davy, editors. Signal\nProcessing Methods for Music Transcription . Springer,\nNew York, 2006.\n[3]Masataka Goto. A real-time music-scene-description\nsystem: Predominant-F0 estimation for detecting\nmelody and bass lines in real-world audio signals.\nSpeech Communication (ISCA Journal) , 43(4):311–329,\n2004.\n[4]Graham E. Poliner and Daniel P.W. Ellis. A dis-\ncriminative model for polyphonic piano transcription.\nEURASIP Journal on Advances in Signal Processing ,\n2007(1), 2007.\n[5]Zhiyao Duan, Bryan Pardo, and Changshui Zhang. Mul-\ntiple fundamental frequency estimation by modeling\nspectral peaks and non-peak regions. IEEE Transac-\ntions on Audio, Speech, and Language Processing ,\n18(8):2121–2133, 2010.\n[6]Emmanuel Vincent, Nancy Bertin, and Roland Badeau.\nAdaptive harmonic spectral decomposition for multiple\npitch estimation. IEEE Transactions on Audio, Speech,\nand Language Processing , 18(3):528–537, 2010.\n[7]Sebastian B ¨ock and Markus Schedl. Polyphonic piano\nnote transcription with recurrent neural networks. In\nProceedings of the IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) ,\npages 121–124, Kyoto, Japan, 2012.\n[8]Siddharth Sigtia, Emmanouil Benetos, and Simon\nDixon. An end-to-end neural network for polyphonic\npiano music transcription. IEEE/ACM Transactions on\nAudio, Speech, and Language Processing , PP(99):1–1,\n2016.\n[9]Holger Kirchhoff, Simon Dixon, and Anssi Klapuri.\nMulti-template shift-variant non-negative matrix decon-\nvolution for semi-automatic music transcription. In Pro-\nceedings of the International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , pages 415–420,\n2012.\n[10] Sebastian Ewert and Mark Sandler. Piano transcription\nin the studio using an extensible alternating directions\nframework. (to appear) , 2016.\n[11] Emmanouil Benetos, Anssi Klapuri, and Simon Dixon.\nScore-informed transcription for automatic piano tutor-\ning. In Proceedings of the European Signal Processing\nConference (EUSIPCO) , pages 2153–2157, 2012.\n[12] Sebastian Ewert, Bryan Pardo, Meinard M ¨uller, and\nMark D. Plumbley. Score-informed source separation\nfor musical audio recordings: An overview. IEEE Signal\nProcessing Magazine , 31(3):116–124, May 2014.[13] Sebastian Ewert, Meinard M ¨uller, and Peter Grosche.\nHigh resolution audio synchronization using chroma on-\nset features. In Proceedings of the IEEE International\nConference on Acoustics, Speech, and Signal Process-\ning (ICASSP) , pages 1869–1872, Taipei, Taiwan, 2009.\n[14] Andreas Arzt, Sebastian B ¨ock, Sebastian Flossmann,\nHarald Frostel, Martin Gasser, and Gerhard Widmer.\nThe complete classical music companion v0.9. In Pro-\nceedings of the AES International Conference on Seman-\ntic Audio , pages 133–137, London, UK, 18–20 2014.\n[15] Meinard M ¨uller and Daniel Appelt. Path-constrained\npartial music synchronization. In Proceedings of the In-\nternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , pages 65–68, Las Vegas, Nevada,\nUSA, 2008.\n[16] Katsutoshi Itoyama, Masataka Goto, Kazunori Ko-\nmatani, Tetsuya Ogata, and Hiroshi G. Okuno. Instru-\nment equalizer for query-by-example retrieval: Improv-\ning sound source separation based on integrated har-\nmonic and inharmonic models. In Proceedings of the In-\nternational Conference for Music Information Retrieval\n(ISMIR) , pages 133–138, Philadelphia, USA, 2008.\n[17] Romain Hennequin, Bertrand David, and Roland\nBadeau. Score informed audio source separation us-\ning a parametric model of non-negative spectrogram.\nInProceedings of the IEEE International Conference\non Acoustics, Speech, and Signal Processing (ICASSP) ,\npages 45–48, Prague, Czech Republic, 2011.\n[18] Daniel D. Lee and H. Sebastian Seung. Algorithms for\nnon-negative matrix factorization. In Proceedings of the\nNeural Information Processing Systems (NIPS) , pages\n556–562, Denver, CO, USA, 2000.\n[19] Andrzej Cichocki, Rafal Zdunek, and Anh Huy Phan.\nNonnegative Matrix and Tensor Factorizations: Appli-\ncations to Exploratory Multi-Way Data Analysis and\nBlind Source Separation . John Wiley and Sons, 2009.\n[20] Jonathan Driedger, Harald Grohganz, Thomas Pr ¨atzlich,\nSebastian Ewert, and Meinard M ¨uller. Score-informed\naudio decomposition and applications. In Proceedings\nof the ACM International Conference on Multimedia\n(ACM-MM) , pages 541–544, Barcelona, Spain, 2013.\n[21] Emmanouil Benetos and Simon Dixon. Multiple-\ninstrument polyphonic music transcription using a tem-\nporally constrained shift-invariant model. Journal of\nthe Acoustical Society of America , 133(3):1727–1741,\n2013.\n[22] Sebastian Ewert, Mark D. Plumbley, and Mark San-\ndler. A dynamic programming variant of non-negative\nmatrix deconvolution for the transcription of struck\nstring instruments. In Proceedings of the IEEE Inter-\nnational Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP) , pages 569–573, Brisbane, Aus-\ntralia, 2015.36 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Listen To Me - Don&apos;t Listen To Me: What Communities of Critics Tell Us About Music.",
        "author": [
            "Ben Fields",
            "Christophe Rhodes"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417801",
        "url": "https://doi.org/10.5281/zenodo.1417801",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/173_Paper.pdf",
        "abstract": "Social knowledge and data sharing on the Web takes many forms. So too do the ways people share ideas and opin- ions. In this paper we examine one such emerging form: the amateur critic. In particular, we examine genius.com, a website which allows its users to annotate and explain the meaning of segments of lyrics in music and other writ- ten works. We describe a novel dataset of approximately 700,000 users’ activity on genius.com, their social con- nections, and song annotation activity. The dataset en- compasses over 120,000 songs, with more than 3 million unique annotations. Using this dataset, we model overlap in interest or expertise through the proxy of co-annotation. This is the basis for a complex network model of the ac- tivity on genius.com, which is then used for community detection. We introduce a new measure of network com- munity activity: community skew. Through this analysis we draw a comparison of between co-annotation and no- tions of genre and categorisation in music. We show a new view on the social constructs of genre in music.",
        "zenodo_id": 1417801,
        "dblp_key": "conf/ismir/FieldsR16",
        "content": "LISTEN TO ME – DON’T LISTEN TO ME: WHAT CAN COMMUNITIES\nOF CRITICS TELL US ABOUT MUSIC\nBen Fields, Christophe Rhodes\nDept. of Computing\nGoldsmiths University of London\nNew Cross\nLondon SE14 6NW\nUnited Kingdom\nme@benfields.net, c.rhodes@gold.ac.uk\nABSTRACT\nSocial knowledge and data sharing on the Web takes many\nforms. So too do the ways people share ideas and opin-\nions. In this paper we examine one such emerging form:\nthe amateur critic. In particular, we examine genius.com,\na website which allows its users to annotate and explain\nthe meaning of segments of lyrics in music and other writ-\nten works. We describe a novel dataset of approximately\n700,000 users’ activity on genius.com, their social con-\nnections, and song annotation activity. The dataset en-\ncompasses over 120,000 songs, with more than 3 million\nunique annotations. Using this dataset, we model overlap\nin interest or expertise through the proxy of co-annotation.\nThis is the basis for a complex network model of the ac-\ntivity on genius.com, which is then used for community\ndetection. We introduce a new measure of network com-\nmunity activity: community skew. Through this analysis\nwe draw a comparison of between co-annotation and no-\ntions of genre and categorisation in music. We show a new\nview on the social constructs of genre in music.\n1. INTRODUCTION\nThe near-ubiquitous availability and use of the Web has en-\nabled many otherwise dispersed communities to coalesce.\nMany of these communities are concerned with the gath-\nering and transfer of knowledge. Perhaps the best known\nof this kind of community is that of the editors and con-\ntributors at Wikipedia1[16, 22]. However, people com-\ning together in a shared virtual space to exchange ideas is\nnot limited to curation of encyclopedic facts. The Web is\nfull of many communities; this paper focuses on an emerg-\ning one with a particular relevance to music: genius.com2.\n1http://wikipedia.org\n2The website and company began as rapgenius ( http://\nrapgenius.com ) with a strong focus on explaining the nuance, ref-\nc⃝Ben Fields, Christophe Rhodes. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: Ben Fields, Christophe Rhodes. “Listen to Me – Don’t Listen\nto Me: What Can Communities of Critics Tell Us About Music”, 17th\nInternational Society for Music Information Retrieval Conference, 2016.Genius.com brings users together through annotation . The\nstated purpose, and indeed, general use of the site is to ex-\nplain portions of text through annotating them. These an-\nnotations can themselves be edited and modiﬁed, much as\nwould take place on a website such as Wikipedia. Unlike\non Wikipedia, however, the goal of allowing annotations\nis speciﬁcally to generate metadata: These annotations are\nboth opinion and derivative works, criticism for the twitter\nage.\nWe have collected a signiﬁcant sample of the user activ-\nity on Genius. This sample forms the core of a dataset that\nis ripe with potential. To show this, we construct a bipartite\ngraph model of our Genius sample, connecting users and\nworks via annotations made on those works. This graph\nmodel is then used to compare the communities formed\naround annotation with the genre prescribed to the anno-\ntated works. In doing this we seek to test the ﬁtness and\ncultural relevance of the prescribed genre to these works.\nThe remainder of this paper is organized into the fol-\nlowing sections. In Section 2 we discuss the relevant con-\ntexts: social network analytics in general, speciﬁc work in\nmusic, complex networks and community detection. From\nthere, in Section 3 we describe the dataset – the collection\ntechniques along with various statistics concerning the raw\ncaptured data. In Section 4 we then explore one possible\navenue of use of our dataset, network modelling and com-\nmunity detection. We look at how detected communities\nalign with prescribed genre labels for the works in these\ncommunities with a novel metric, community skew . Fi-\nnally, we state our conclusions and consider what the next\nsteps should be in Section 5.\n2. BACKGROUND\nWhen considering a social network of criticism such as Ge-\nnius, we must consider what the landscape looks like to\nplace this work in a more complete context.\nerences, and in-jokes of rap and hip-hop lyrics. However they re-\nbranded as ‘Genius’ as they widened their focus, which now in-\ncludes lyrics from all genre of music as well as poetry, libretti,\nand factual texts such as news articles. See this announcement\nfrom 12 July 2014 http://genius.com/Genius-founders-\nintroducing-geniuscom-annotated .1992.1 Music and Social Networks\nWhile Genius has existed in some capacity since July of\n20103, it is one of many social networks with user-generated\ncontent (UGC) and an emphasis on music. One of the ear-\nliest of these networks was youtube4. While youtube is\nostensibly a site for hosting and sharing video, it is also\nthe single most proliﬁc source of music on the Web5. Fur-\nther, its social structure was one of the ﬁrst on the modern\nWeb to be extensively studied [4, 17]. It was shown that\nyoutube, like many other Web-based social networks, has\na power-law roll off in the distribution of its users’ con-\nnections to other users and that the users congregate into\nclumps of tightly connected communities, showing ‘small-\nworld’ characteristics.\nOther Web-based communities brought together con-\ntent creators with a greater explicit emphasis on social con-\nnections. In particular, myspace6has been looked at, both\nin terms of community structures [13] and as a proxy for\nunderstanding song and artist similarity [6, 7]. Further,\nthese techniques have been used to drive recommenders\nand playlist generation [8]. In recent years, Soundcloud7\nhas become the Web platform of choice for this combina-\ntion of audio recordings and social network connectivity.\nIt has broadly similar network characteristics [12, Chap-\nter 3] with its own particular traits, reﬂecting interface and\ndesign decisions as well as the different user composition\nof the network. In addition to these networks around com-\nplete works, analysis has been done showing associations\nbetween properties of the contributor network for Freesound8\n(an open collection of audio clips) and creative outcomes\namong participants [19].\nAnalogous work has also been done on the listener or\nconsumer side. In particular various aspects of listening\nand sharing behaviour on twitter9have been studied. The\ntwitter microblogging platform has been successfully used\nto model artist similarity and descriptors, based on network\nties and other attributes [20]. Extensions of this work then\nused twitter to show popularity trends across both time and\nspace [21]. Going a step further, twitter network analysis\ncan be used to create and order personalized playlists [14].\n2.2 Information and Social Networks\nWhile a signiﬁcant volume of research has been done on\ninformation gathering social networks, it nearly exclusively\nuses Wikipedia as the source social network. As mentioned\nin Section 1, Wikipedia aims to be encyclopedic in both\ntone and scope, which colours the network signiﬁcantly.\n3The beginning of their current site can be seen dating back\nto 22 July 2010 according to http://web.archive.org/web/\n20100615000000 */http://rapgenius.com\n4http://youtube.com\n5http://www.nielsen.com/us/en/press-room/2012/\nmusic-discovery-still-dominated-by-radio--says-\nnielsen-music-360.html\n6http://myspace.com , though it has decayed a great deal from\nits peak of activity circa 2006-2008\n7http://soundcloud.com\n8urlhttp://www.freesound.org/\n9http://twitter.comNevertheless, this work can offer useful insight and ap-\nproaches for networks of this type.\nComplex network techniques are effective in determin-\ning the most inﬂuential nodes across an information net-\nwork [15]. This can be used to help understand how infor-\nmation ﬂows through a social network. Wikipedia editors\ncan broken down into different classes based on their be-\nhaviour within the network [11].\n3. THE DATASET\nIn this section we describe the general structure of Genius,\nespecially as it pertains to the dataset presented in this pa-\nper. We go into detail about the process of scraping and\nspidering the site to collect the data, highlighting sampling\ndecisions and noting possible biases. Lastly, we present a\nstatistical overview of the features of the dataset.\n3.1 The Structure of Genius\nAt its core Genius is a collection of textual representations\nof works, most commonly but not exclusively lyrics. Each\nof these works are rendered such that an arbitrary sequence\nof words may be selected and a user may then write some\ncommentary about the meaning of this section of the work\n(theannotation ). An example of this display can be seen in\nFigure 1, in this case lyrics for Hypnotize by The Notori-\nous B.I.G. with the line ‘Timbs for hooligans in Brooklyn’\nhighlighted with the annotation visible.\nOnce an annotation has been placed by a user, it can be\nedited and debated. This process can involve signiﬁcant\nback and forth between users, as those interested within\ncommunity voice their point of view as to the meaning of\na line. The result is an annotation that reﬂects a collec-\ntive process: the contributions that have led to the current\nstate of an annotation are easily viewable, as can be seen in\nFigure 2 with the same annotation as the previous ﬁgure.\nA user maintains a proﬁle on Genius, as is the case on\nmany social networks. Central to this proﬁle is the his-\ntory of the annotations made by the user. As such, a user’s\npersona on Genius is effectively the collection of their an-\nnotations across the site. One such user proﬁle is shown\nin Figure 3, that of the user ‘OldJeezy’, the lead contribu-\ntor to the previously mentioned on annotation for the work\nHypnotize .\n3.2 Collecting the Data\nUntil recently Genius lacked any kind of machine-readable\nAPI10, so our data collection effort restructured data drawn\nfrom the html as presented to a user. The data collection\nefforts on Genius are made up of two parts: a spider and a\nscraper. The spider, or mechanism to automatically move\nthrough the pages to be collected, sets out to evenly sam-\nple across the space of user IDs, without preference for or\nagainst how active a particular users is on the site. This\nalgorithm is reasonably straight-forward and relies on the\n10The recently announced API ( https://docs.genius.com/ )\nmitigates the most of the need for further scraping via html, though the\nspidering and sampling techniques detailed here are unchanged.200 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Figure 1 . The lyrics to Hypnotize by The Notorious B.I.G., with an annotation shown for the line ‘Timbs for hooligans in\nBrooklyn’. Taken from http://genius.com/44369/The-notorious-big-hypnotize/Timbs-for-my-\nhooligans-in-brooklyn on 10 March 2015.\nFigure 2 . The same lyrics annotation as in Figure 1, but showing the total contribution of the three users who have edited\nthe annotation for the highlighted text.\nFigure 3 . The recent annotation history for the user ‘OldJeezy’, the top contributor to the lyrics annotation shown in Figure\n1. Taken from http://genius.com/OldJeezy on 10 March 2015Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 201fact that Genius has sequential integer user IDs. As these\nID are very nearly continuous from 0 to the most recent\nID assigned, it is trivial to approximate a fair draw random\ngenerator to visit the user annotation history pages. Be-\ncause of the ﬂat and random mechanism in this spider, a\npartial sample is far less likely to introduce a bias toward\na more densely connected graph than spidering methods\nthat move from one user to another via a common edge\n(in this case a mutually annotated work). This implies that\na partial capture will be reasonably representative of the\nwhole userbase. The corresponding drawback is that any\nparticular work may not have its entire annotation record\ncollected, so its relative position in the topography of the\ngraph (e.g. in terms of degree) may not be accurate, though\nthis problem will decrease as more of the graph is captured.\nTo gather the data for each individual page, we created\na screen scraper using Python and the BeautifulSoup11\ntoolkit. This scraper is released with an open source li-\ncense and is available from github12.\nThe spider and scraper were run during December 2014\ncollecting user metadata, annotation, works, and works meta-\ndata from the contributions of 704,438 users. This sample\ncovers 41.1% of the 1,713,700 users13.\nThis dataset is available for download and reuse, as both\nCSV and SQL dump from the Transforming Musicology\ndataset repository14.\n3.3 Statistical Overview\nA variety of statistics describing the Genius data set can\nbe seen in Table 1. As previously mentioned, the dataset\ncovers the contributions of 704,438 users: 1,256,912 an-\nnotations on 146,186 unique works. Genius, as is common\namong many social networks [2], appears to have a steep\ndrop off from users who sign up to users who do anything.\nThis can be seen in the disparity between the total captured\nusers and the contributing users (704,438 versus 71,129):\n10.1% of users have written an annotation.\ndescription count\ntotal users 704,438\ntotal annotations 1,256,912\ntotal works 146,186\ncontributing users 71,129\nannotation edits 2,196,522\nannotations with multiple contributors 194,795\nTable 1 . High-level statistics for Genius dataset.\nOur dataset covers some 146,186 unique works and\n1,256,912 annotations, giving a mean average of 8.6 an-\nnotations per work. Further, the dataset contains a total\n11http://www.crummy.com/software/BeautifulSoup/\nbs4/doc/\n12http://dx.doi.org/10.5281/zenodo.17515\n13The total user count is an approximation based on the highest suc-\ncessfully resolving user ID as on 29 April 2015.\n14Speciﬁcally http://genius-annotations.data.t-mus.\norg/ , note that this dataset does not contain the source lyrics, only the\nnetwork structure around the lyrics and their annontations. This is done\nfor reasons of copyright complianceof 2,196,522 distinct edits of annotations, giving the mean\nannotation 1.75 edits, including its ﬁrst.\nGenius has 15 top-level categories for works on the site.\nEach work is assigned exactly one category, which can be\ntaken as the work’s genre. While that is not quite right\nfor the non-musical categories, it is a helpful approxima-\ntion. The breakdown of the works per category (genre)\nare seen in Table 2. The ﬁrst thing that pops out is that\nwhile the company behind Genius may have decided to\ndrop ‘rap’ from their name, it still dominates their col-\nlection of works, making up almost three-quarters of our\ndataset. While the meanings of most of these genre names\nare fairly typical, it is worth commenting on the few that\nare particular to Genius: ‘x’ is used as a catch-all or miscel-\nlaneous; ‘screen’ is for screenplays and teleplays; ‘history’\nis for both scholarly and lay texts of a historical nature;\n‘unbranded’ means our scraper was unable to capture the\nassigned genre; ‘tech’ covers prose about technology and\nthe tech industry; ﬁnally ‘meta’ is where contributors to\nGenius discuss rules and community standards.\ncategory works count percentage\nrap 107270 73.3%\nrock 16393 11.2%\nlit 9386 6.2%\nnews 3720 2.5%\npop 3715 2.5%\nsports 1140 0.7%\nx 1014 0.6%\ncountry 744 0.5%\nscreen 697 0.4%\nr-b 655 0.4%\nhistory 502 0.3%\nunbranded 370 0.2%\nlaw 250 0.1%\ntech 159 0.1%\nmeta 151 0.1%\nTable 2 . Genre breakdown for Genius dataset.\nIn addition to the top-level categories, Genius supports\nwork-level social tags. The tags have also been captured\nin our data set for all the works. As is typical for tags\nany number may be used per work, though the top-level\ngenre category is repeated as a tag mechanically, so each\nwork has at least one. Including these top-level categories,\nour dataset contains 802 unique tags. The top 10 tags (not\nincluding the categories), along with the count of the works\nthey’ve been applied to, appears in Table 3.\n4. NETWORK ANALYTICS\nIn an effort to understand what the community of annota-\ntors on Genius can tell us about that material they’re anno-\ntating, we model our dataset as a graph. We use this graph,\nand a transform of it, to observe the community structure of\nworks based on co-annotation anduser-overlap patterns.\nHere co-annotation is when a common user annotates a\npair of works. Similarly, user-overlap is when any pair of202 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016category works count\nRap Genius France 9135\nGenius France 6009\nDeutscher Rap 5725\nPolski Rap 3298\nWest Coast 1384\nBrasil 841\nBay Area 839\nIndie Rock 716\nChicago 710\nGenius Britian 540\nTable 3 . Top tags in Genius dataset.\nusers contribute to any annotation on the same work (not\nnecessarily the same annotation).\n4.1 The Graph Model\nWe initially model the dataset as a bipartite graph [9]. That\nis a graph where each node represents one of two distinct\nclasses: a work or a user. The edges is this graph are\nformed whenever a user has contributed at least one an-\nnotation to a work. No edges join two nodes of the same\nclass.\nGiven this graph we can discuss its topological features\n[1]. The graph has 216,943 nodes across both class – 71,129\nof those nodes represent all the users that have contributed\nan annotation, 145,814 represent the works15. The graph\ncontains 439,835 edges, representing the number of unique\nuser-work pairs with annotations. While nearly half a mil-\nlion in number, this is quite sparse representing only 4.24×\n10−5of the more than 10 billion possible pairs. There-\nfore the graph has a average degree of 2.02. This bipartite\ngraph, serialised as graphml, is also included as part of the\ndataset and is available for download as mentioned in Sec-\ntion 3.\nThe remained of this section concerns the detection of\ncommunities of works. In order to do this, we project our\nbipartite graph to a songs-as-nodes single class graph with\nweighted edges representing the users that co-annotated\nlinked works. We also only consider the largest connected\ncomponent, i.e. the largest number of works for which\nthere is a path between each pair of works included. This\nreduces the number of nodes to 125,044.\n4.2 Examining Community\nWe have performed community detection with three differ-\nent algorithms: fast greedy [5], leading eigenvector [18],\nand multilevel [3]. In order to assess the suitability of each\nof these inferred community structures, we take the mod-\nularity of each. Here modularity is a measure of the ra-\ntio of connections within communities against connections\namong communities. The optimum modularity resulting\nfrom each of these communities detection methods, along\n15The careful reader may notice that this is 372 works fewer than the\n146,186 works reference in Table 1. This is due to those works URLs not\nresolving at the time of the crawl, most likely due to deletion of the work\nfrom the collection after the annotation was made.method modularity communities\nFast greedy 0.529 498\nLeading eigenvector 0.003 11488\nMultilevel 0.582 169\nTable 4 . The optimum modularity scores of each of the\nthree community detection methods used on the works\ngraph. The highest modularity, achieved the multilevel\nmethod, is shown in bold.\ncategory community count community skew\nmeta 16 90.0\nlaw 12 70.0\ntech 12 70.0\nhistory 19 36.6\nscreen 24 35.5\nr-b 23 35.0\nx 24 23.3\ncountry 19 22.0\nsports 19 15.7\npop 38 8.8\nnews 35 8.4\nunbranded 22 6.5\nlit 59 5.6\nrock 50 2.7\nrap 143 1.2\nTable 5 . The spread of each genre, across detected com-\nmunities.\nwith the number of detected communities that give said\nmodularity, can be seen in Table 4. Based on modularity,\nthe multilevel community detection measure gives the best\ngrouping, resulting in 169 distinct detected communities.\nWhile the higher modularity of the multilevel method is\ninline with previous research on other small-world graphs,\nthe low score and high number of communities generated\nby the leading eigenvector method is notable and merits\nfurther investigation.\nGiven the 169 detected communities of works, we can\ncompare these communities to the prescribed genre labels\nto see how (and if) they align. To do this, we generate\na confusion matrix, analogous to what might be used to\nevaluate a automatic classiﬁcation task. However, unlike\nin a common classiﬁcation task, our confusion matrix is\nnot square, having dimension so of 15 x 169 (the number\nof categories by the number of communities). Given the\nsize of the confusion matrix, it is not practical to visualize\nthe entire thing, rather we will consider it in the following\nreduction.16Since there are more than 10 times the de-\ntected communities as there are genre categories, we can\nsee how widespread each genre category is across commu-\nnities. That is, how many communities have more than\nzero works from a given genre. This can be seen in Ta-\nble 5, which shows that spread seems to correspond with\npopularity of the genre label.\n16The raw confusion matrix is available for download as a csv at\nhttp://genius-annotations.data.t-mus.org/Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 203Beyond the raw counts, we can examine the community\nskew of a category Scwhich we deﬁne as\nSc=Wc\nW∗Cc\nC(1)\nwhere Wcis the number of works in category c,Wis the\ntotal number of works in the corpus, Ccis the number\ncommunities in the split with at least one work category\ncamongst its members, and Cis the total number of com-\nmunities in across the network. Community skew therefore\ngives a measure of how widely distributed a given category\nlabel is across communities, normalised to how popular\nthat label is in the corpus. A community skew of 1 means\nthat the number of communities covering a genre exactly\nmirrors its overall representation in the corpus. Further as\nthe skew increase away from 1 it show a disproportionate\ncapture of the communities across the network. Looking at\nthe community skew in Table 5 this is especially the case\nfor the meta, law, and tech categories. With a few excep-\ntions, the more well represented in the dataset a genre is\nthe less its skew. This relationship implies that with more\nworks in a genre community annotators become more dis-\ntinct.\n5. CONCLUSIONS AND FUTURE WORK\nWe have introduced the Web community Genius, a collec-\ntion of (mostly music related) textual works and criticism\nin the form of annotations. We described and data gath-\nering methodology, and using that methodology, collect\nthe annotation and works metadata for the activity of over\n700,000 users, with just over 10% of them active contrib-\nutors. We then modelled this dataset as a bipartite graph\nof works and users. This graph was then projected into a\nsingle class for community detection. When performing\ncommunity detection, the multilevel method was found to\nperform best, with a modularity score of 0.582 ﬁnding 169\ncommunities. Using these communities we examined the\ncommunity skew of each genre across these communities\nof works. In these community measures, and skew in par-\nticular, we see that a genre’s deﬁnition is clearer as it is\nmore popular.\nWhile there are many further avenues of research to take\nthis dataset and these foundations in the future, one in par-\nticular stands out: hybrid-methods using content. Perform-\ning content analysis on the lyrics, such as reading compre-\nhension or rhyme structure analysis [10], and then using\nthe result in tandem with cultural structures as captured by\nthis work’s network models present many possible further\ninsights to the organisation of music.\n6. ACKNOWLEDGEMENTS\nThis work is supported in part via the Transforming Mu-\nsicology UK AHRC grant, under the Digital Transforma-\ntions in the Arts and Humanities scheme, number\nAH/L006820/1. We would also like to acknowledge and\nthank the anonymous peer reviewers for their help in reﬁn-\ning this paper.7. REFERENCES\n[1]Yong-Yeol Ahn, Seungyeop Han, Haewoon Kwak, Sue\nMoon, and Hawoong Jeong. Analysis of topological\ncharacteristics of huge online social networking ser-\nvices. In The World Wide Web Conference (WWW) , Al-\nberta, Canada, May 2007.\n[2]Fabr´ıcio Benevenuto, Tiago Rodrigues, Meeyoung\nCha, and Virg ´ılio Almeida. Characterizing user behav-\nior in online social networks. In Proceedings of the 9th\nACM SIGCOMM conference on Internet measurement\nconference , pages 49–62. ACM, 2009.\n[3]Vincent D Blondel, Jean-Loup Guillaume, Re-\nnaud Lambiotte, and Etienne Lefebvre. Fast un-\nfolding of communities in large networks. Journal\nof Statistical Mechanics: Theory and Experiment ,\n2008(10):P10008, 2008.\n[4]Xu Cheng, Cameron Dale, and Jiangchuan Liu. Statis-\ntics and social network of youtube videos. In Quality of\nService, 2008. IWQoS 2008. 16th International Work-\nshop on , pages 229–238. IEEE, 2008.\n[5]Aaron Clauset, M. E. J. Newman, and Cristopher\nMoore. Finding community structure in very large net-\nworks. Physical Review E , 70(6):066111, Dec 2004.\n[6]Ben Fields, Kurt Jacobson, Michael Casey, and Mark\nSandler. Do you sound like your friends? exploring\nartist similarity via artist social network relationships\nand audio signal processing. In International Com-\nputer Music Conference (ICMC) , August 2008.\n[7]Ben Fields, Kurt Jacobson, Christophe Rhodes, and\nMichael Casey. Social playlists and bottleneck mea-\nsurements : Exploiting musician social graphs us-\ning content-based dissimilarity and pairwise maximum\nﬂow values. In International Conference on Music In-\nformation Retrieval (ISMIR) , September 2008.\n[8]Ben Fields, Kurt Jacobson, Christophe Rhodes, Mark\nSandler, Mark d’Inverno, and Michael Casey. Analy-\nsis and exploitation of musician social networks for\nrecommendation and discovery. IEEE Transactions on\nMultimedia , PP(99):1, 2011.\n[9]Jean-Loup Guillaume and Matthieu Latapy. Bipartite\ngraphs as models of complex networks. Physica A: Sta-\ntistical Mechanics and its Applications , 371(2):795–\n813, 2006.\n[10] Hussein Hirjee and Daniel G Brown. Automatic detec-\ntion of internal and imperfect rhymes in rap lyrics. In\nInternational Society on Music Information Retrieval\nConference (ISMIR) , pages 711–716, 2009.\n[11] Takashi Iba, Keiichi Nemoto, Bernd Peters, and Pe-\nter A Gloor. Analyzing the creative editing behavior\nof wikipedia editors: Through dynamic social net-\nwork analysis. Procedia-Social and Behavioral Sci-\nences , 2(4):6441–6456, 2010.204 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016[12] Kurt Jacobson. Connections in Music . PhD thesis, Cen-\ntre for Digital Music, Queen Mary University of Lon-\ndon, 2011.\n[13] Kurt Jacobson, Ben Fields, and Mark Sandler. Using\naudio analysis and network structure to identify com-\nmunities of on-line social networks of artists. In Inter-\nnational Conference on Music Information Retrieval\n(ISMIR) , Philadelphia, PA, USA, October 2008.\n[14] Sanghoon Jun, Daehoon Kim, Mina Jeon, Seungmin\nRho, and Eenjun Hwang. Social mix: automatic mu-\nsic recommendation and mixing scheme based on so-\ncial network analysis. The Journal of Supercomputing ,\npages 1–22, 2014.\n[15] Masahiro Kimura, Kazumi Saito, and Ryohei Nakano.\nExtracting inﬂuential nodes for information diffusion\non a social network. In AAAI , volume 7, pages 1371–\n1376, 2007.\n[16] Andrea Lancichinetti, Mikko Kivel ¨a, Jari Saram ¨aki,\nand Santo Fortunato. Characterizing the community\nstructure of complex networks. PloS one , 5(8):e11976,\n2010.\n[17] Alan Mislove, Massimiliano Marcon, Krishna P Gum-\nmadi, Peter Druschel, and Bobby Bhattacharjee. Mea-surement and analysis of online social networks. In\nProceedings of the 7th ACM SIGCOMM conference on\nInternet measurement , pages 29–42. ACM, 2007.\n[18] Mark EJ Newman. Finding community structure in net-\nworks using the eigenvectors of matrices. Physical re-\nview E , 74(3):036104, 2006.\n[19] Gerard Roma, Perfecto Herrera, Massimiliano Zanin,\nSergio L Toral, Frederic Font, and Xavier Serra. Small\nworld networks and creativity in audio clip shar-\ning. International Journal of Social Network Mining ,\n1(1):112–127, 2012.\n[20] Markus Schedl. On the use of microblogging posts\nfor similarity estimation and artist labeling. In Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR) , pages 447–452, 2010.\n[21] Markus Schedl. Analyzing the potential of microblogs\nfor spatio-temporal popularity estimation of music\nartists. In In Proceedings of the IJCAI 2011: Interna-\ntional workshop on social web mining , 2011.\n[22] Karsten Steinhaeuser and Nitesh V Chawla. Identify-\ning and evaluating community structure in complex\nnetworks. Pattern Recognition Letters , 31(5):413–421,\n2010.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 205"
    },
    {
        "title": "Mining Online Music Listening Trajectories.",
        "author": [
            "Flavio V. D. de Figueiredo",
            "Bruno Ribeiro 0001",
            "Christos Faloutsos",
            "Nazareno Andrade",
            "Jussara M. Almeida"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417275",
        "url": "https://doi.org/10.5281/zenodo.1417275",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/193_Paper.pdf",
        "abstract": "Understanding the listening habits of users is a valuable undertaking for musicology researchers, artists, consumers and online businesses alike. With the rise of Online Mu- sic Streaming Services (OMSSs), large amounts of user behavioral data can be exploited for this task. In this pa- per, we present SWIFT-FLOWS, an approach that mod- els user listening habits in regards to how user attention transitions between artists. SWIFT-FLOWS combines re- cent advances in trajectory mining, coupled with mod- ulated Markov models as a means to capture both how users switch attention from one artist to another, as well as how users fixate their attention in a single artist over short or large periods of time. We employ SWIFT-FLOWS on OMSSs datasets showing that it provides: (1) semantically meaningful representation of habits; (2) accurately models the attention span of users.",
        "zenodo_id": 1417275,
        "dblp_key": "conf/ismir/FigueiredoRFAA16",
        "content": "MINING ONLINE MUSIC LISTENING TRAJECTORIES\nFlavio Figueiredo1Bruno Ribeiro2Christos Faloutsos3\nNazareno Andrade4Jussara M. Almeida5\n1IBM Research - Brazil2Purdue University3Carnegie Mellon University\n4Universidade Federal de Campina Grande5Universidade Federal de Minas Gerais\nABSTRACT\nUnderstanding the listening habits of users is a valuable\nundertaking for musicology researchers, artists, consumers\nand online businesses alike. With the rise of Online Mu-\nsic Streaming Services (OMSSs), large amounts of user\nbehavioral data can be exploited for this task. In this pa-\nper, we present S WIFT-F LOWS , an approach that mod-\nels user listening habits in regards to how user attention\ntransitions between artists. S WIFT-F LOWS combines re-\ncent advances in trajectory mining, coupled with mod-\nulated Markov models as a means to capture both how\nusers switch attention from one artist to another, as well as\nhow users ﬁxate their attention in a single artist over short\nor large periods of time. We employ S WIFT-F LOWS on\nOMSSs datasets showing that it provides: (1) semantically\nmeaningful representation of habits; (2) accurately models\nthe attention span of users.\n1. INTRODUCTION\nIs it possible to create expressive yet succinct represen-\ntations of individuals’ music listening habits? Are there\ncommon patterns on how music is listened to across dif-\nferent genres and different artists that have highly differ-\nent popularity? For a long time such questions have at-\ntracted the attention of researchers from different ﬁelds. In\nthe ﬁelds of psychology and musicology [10, 20, 21], re-\nsearchers exploit musical preferences to study social and\nindividual identity [20], mood regulation [23], as well as\nthe underlying factors of preferences [21]. Computer sci-\nentists are also tackling such questions as they become cen-\ntral to develop music recommender systems [3, 4, 7].\nWith the rise of Online Music Streaming Services\n(OMSSs) over the last decade, large datasets of user1be-\nhavior can be used to shed light on questions like the ones\nabove. More speciﬁcally, digital traces of the listening\nhabits of individuals are readily available to researchers.\n1Since our case study is on Online Music Streaming Services\n(OMSSs), we use the terms users and listeners interchangeably.\nc/circlecopyrtFigueiredo, Ribeiro, Faloutsos, Andrade, Almeida. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Figueiredo, Ribeiro, Faloutsos, Andrade,\nAlmeida. “Mining Online Music Listening Trajectories”, 17th Interna-\ntional Society for Music Information Retrieval Conference, 2016.In this paper, we focus on the online listening habits of\nusers as trajectories [7] (or trails [24]). Given that a user,\nu, listens to music by switching attention between different\nartists, a trajectory captures the sequence of artists or songs\nvisited by a user when listening to music. The main con-\ntribution of this paper is to present the S WIFT-F LOWS2\nmodel, a general technique designed to study user trajec-\ntories in OMSSs. We tackle several challenges that stem\nfrom the complexity of user behavior, such as:\n(a)Asynchronous users with mixed but similar behav-\nior: Users that consume music from a set of artists\nwill not start their playlists at the same time or listen\nto songs in the same order.\n(b)Repeated consumption : Users tend to listen to artists\nin bursts, more than what one would expect at ran-\ndom in a shufﬂed playlist.\n(c)Biased Observations & Small Subpopulations : User\nbehavior datasets are naturally sparse and biased to-\nwards more popular artists. Nevertheless, we still\nwant to be able to analyze underrepresented subpop-\nulations of users and artists.\nSWIFT-F LOWS effectiveness is evaluated in large datasets,\nwith results showing that S WIFT-F LOWS : (1) captures se-\nmantically meaningful representation of artist transitions;\n(2) accurately models the attention span of users.\n2. RELATED WORK\nUnderstanding the listening habits of individuals has at-\ntracted interest from different research ﬁelds. Among\nother problems, musicologists and social psychologists\nhave looked into the latent factors that explain musical\npreferences [20, 21], factors that affect listener experience\n(e.g., Music itself, Situational Factors and the Listener\nhim/herself) [10], as well as the relationships between mu-\nsical imagination and human creativity [10].\nRegarding the material methods listeners exploit to lis-\nten to music, Nowak [16] discussed the social-material re-\nlations of music consumption. The authors conclude that\neven the same user still relies on multiple forms of listen-\ning to music (e.g., legal and illegal downloading, streaming\nservices, CDs, etc). These various forms of consumption\nwere also discussed by Bellogin et al. [1]. Here, the au-\n2Switch and Fixation Trajectory Flows688thors showed the disagreement between different web and\nsocial music services (in terms of artist popularity).\nSeveral studies, such as the ones Marques et al. [12],\nPark et al. [18], and Moore et al. [15], characterized the ex-\nploratory behavior of users in OMSSs. Marques et al. [12]\ndescribed the habits of users when searching for novel\nsongs to listen to. Park et al. [18] deﬁned a new measure\nto compute the diverseness of musical tastes. Moore et\nal.[15] looked into the tastes of users over time through\nthe use of Latent Markov Embedding (LME) [3, 4].\nIn contrast with the aforementioned studies, our work\nwith S WIFT-F LOWS is focused on extracting the latent tra-\njectories that explain user attention when listening to music\nonline. Nevertheless, S WIFT-F LOWS can be used to tackle\nproblems as the ones described. For instance, we are able\nto aggregate the preferences of users from different demo-\ngraphics as shown in Section 4. For musicologists and psy-\nchologists, these results indicate how S WIFT-F LOWS can\nbe used as a tool to better understand the hidden factors that\ndeﬁne our consumption habits. Regarding OMSSs, previ-\nous work [12, 17, 18] usually relied on deﬁning speciﬁc\npoint estimates that are used to understand and capture lis-\ntening behavior. Such measures are susceptible to effects\n(a-c) described in Section 1.\nTo capture both the inter-artist transitions, those were\na listener changes attention from one artist to another, as\nwell the long and short tails of listener ﬁxation in a single\nartist, S WIFT-F LOWS advances the state-of-the-art [7] by\ndeﬁning a combined approach that can capture both behav-\niors. Inter-artist transitions, or switches in attention, are\ncaptured by exploring the ideas in [7]. Intra-artist transi-\ntions, or ﬁxation, is captured by modulated Markov mod-\nels [22]. In this sense, S WIFT-F LOWS provides a inter-\npretable results than [7] or [22] in isolation. We describe\nthe details of the model next.\n3. THE S WIFT-F LOWS MODEL\nWe now describe S WIFT-F LOWS . LetDbe a dataset con-\nsisting of (user,artist,timestamp )tuples observed over a\ntime window (i.e., the temporal range of our datasets).\nEach tuple registers that a user listened to songs from an\nartist at a moment in time. Let u∈U deﬁne the set of\nusers anda∈A deﬁne the set of artists. By ordering D\naccording to the timestamps, each user can be represented\nas a trajectory: Tu=< au,1,au,2,...,a u,|Tu|>. This tra-\njectory represents the history of the user listening to mu-\nsic transitioning between songs of a same artist – in intra-\nartist (au,i=au,i+1) transitions – and songs from different\nartists – in inter-artist ( au,i/negationslash=au,i+1) transitions.\nBoth inter and intra artist transitions are important\nwhen studying trajectories. Inter-artist transitions capture a\nswitch in users attention from one artist to another, whereas\nintra-artist transitions captures a ﬁxation on a same artist.\nSWIFT-F LOWS isolates both effects and exploits stochas-\ntic complementation [14] to propose two complementary\nMarkov models, as illustrated in Figure 1, that together are\nable to capture both the intra-artist and inter-artist transi-\ntion behavior. Isolation of intra from inter artist transitionsis necessary to model both the long and and short attention\ntails of repeated consumption [6, 22].\nTheintra-artist (Figure 1-b), or ﬁxation, model consists\nof a modulated Markov model that is able to capture how\nusers revisit artists. Intra/inter transition separation is pos-\nsible by treating user attention as a reducible system, where\nwe model the strong memory of intra-artist transitions –\nsome users continuously listen to the same artist for hours\n– as only interfering with the inter-artist dynamics through\nlimited user attention. This creates an effective separation\nbetween the intra-artist model and the inter-artist model.\nA play takes us to an inter-artist transition from artist sto\nartistd,s/negationslash=d, which then again transitions to the intra-\nartist model of artist d. The inter-artist (Figure 1-c) at-\ntention, or switch, transitions are captured by a graphical\nmodel, using a Bayesian approach to estimate inter-artist\ntransitions. This approach avoids problems associated with\npoint estimates [7, 19] and is robust to infrequent transi-\ntions of small sub-populations of interest.\nData Representation: Let users (artists) to be num-\nbered between one and |U|(|A|). Letndsu, the number of\ntimes useru∈U transitioned from s∈A tod∈A:\nndsu=|Tu|/summationdisplay\ni=21(au,i−1=s∧au,i=d), (1)\nwhere, 1is an indicator function that will evaluate to 1\nwhenau,i−1=sand (∧)au,i=d, 0 otherwise.\nWith these counts, we can deﬁne a tensor X(as shown\nin Figure 1-a) X=/bracketleftbig\nX1,X2,···,X|U|/bracketrightbig\n, where Xuis:\nXu=\nn11u···n1|A|u\n.........\nn|A|1u···n|A||A| u\n (2)\nThis data representation is distinct from other tensor de-\ncompositions that mine Din its original “user”, “object”\nand “time” coordinates as the three tensor modes [13, 25].\nThese techniques are meant to capture synchronous user\nbehavior. As shown in previous work [7], the represen-\ntation of Xis more suitable to capture the asynchronous\nbut similar behavior patterns that emerge when we have\na mixed population of users, spread across different time\nzones and with different activity patterns as in OMSSs.\nWe now describe both the inter-artist an intra-artist\nmodels. In the following, we use the “ ·” notation to imply\na sum over a given dimension (e.g., nds·=/summationtext\nu∈Undsu).\n3.1 Switch Model\nTo model inter-artist transitions, we deﬁne X−=X−\ndiagonals (X)by removing the cases where s=dfrom\nX, since this behavior is captured by the Fixation model\n(next subsection). Our goal with the Switch model is to\nestimate trajectories as an interpretable probability space.\nThat is, our goal is to decompose Xin a probability matrix\nP, where each cell in this matrix captures the probability\nof a user switching attention for stod(orp(d|s)).\nA na ¨ıve way to deﬁne Pis simply to deﬁne p(d|s)∝\nnds·. That is, to use maximum likelihood estimates [11].Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 689Figure 1 . The S WIFT-F LOWS model: Data representation by tensor X(left), the repeated consumption model (center)\nand the inter-artist graphical model (right).\nHowever, this approach has three undesirable proper-\nties [19]: (1) there are not enough samples to accurately\nestimate the transition probabilities for most artists; (2) the\ntransition probability matrix Pis sparse, stating that it is\nimpossible to transition from an artist stodwhen no user\nhas done so in the past; and, (3) it does not take into ac-\ncount user preferences. For example, if we observe that\nthe transition Sepultura →Beyonce is very common for a\nsingle or small group of users, this does not imply that it is\nfrequent for all of the listeners in the OMSSs.\nIn order to deal with such issues, we employ the\nBayesian model depicted in Figure 1-c. With this model,\nour goal is capture the latent spaces of inter-artist transi-\ntion patterns shared by a group of users. We call this the\nSwitch model. The latent space Zdeﬁnes a set of tran-\nsitions between pairs of artists sandd. We refer to each\nlatent factor zas an attention transition gene , and the col-\nlection of genes as a genome . These terms are inspired\nby the “Music Genome Project”, a proprietary approach\ndeveloped by Pandora that aims to describe in detail the\nmusical characteristics of individual songs3.\nEstimating the Model: Letk=|Z|(k <<|A|) be\nan input variable determining the number of genes (or la-\ntent factors) to be estimated. Later, we shall describe our\napproach to deﬁne k. The two other inputs are the hyper-\nparametersαandβ. The outputs of the Switch model are\ntwo matrices, ΘandΦ, as well as a vector z.Θhas|U|\nrows and|Z|columns, where each cell contains the prob-\nability that a user has a preference towards a given gene:\np(z|u) =Θ(u,z) =θz|u(z) =nzu+α\nn·u+|Z|α(3)\nwherenzuis estimated by the model. Matrix Φhas|Z|\nrows and|A|columns. It captures the probability that\nwhen a user is interest in gene zit will transition to a, i.e.:\np(a|z) =Φ(z,a) =φa|z(a) =naz+β\nn·z+|A|β(4)\nwhere, once again, nazis estimated from the data by the\nmodel. Finally, vector zcontains the probabilities of each\ngenez∈Z, referred to as p(z), that is:p(z)∝nz. Finally,\n3http://www.pandora.com/about/mgpthe decomposed transition matrix Pis deﬁned by:\nP(s,d) =/summationdisplay\nz∈|Z|p(z|s)p(d|z) (5)\nwherep(d,s|z) =p(s|z)p(d|z), andp(z|s)∝p(s|z)p(z).\nGibbs Sampling: We use a collapsed Gibbs sampler [8]\nto estimate matrices ΘandΦby estimating nzuandnaz,\nas well as vector z. We sample from the posterior deﬁned\nby the product θz|uφs|zφd|z[7]. We ﬁx hyper-parameters\nα=50\n|Z|, andβs=βd= 0.001, as is usually done with\nsimilar models [7, 13]. We execute the sampler for 800\niterations with 300 being discarded as burn-in.\nEstimating k:We apply the minimum description\nlength (MDL) principle [9], which is largely used for prob-\nlems of model selection, to determine the number of genes\nk=|Z|. With MDL, we ﬁne tune S WIFT-F LOWS in order\nto extract a succinct, yet still accurate, representation of the\nlistening habits of users. MDL captures how good a model\nM(Pin our case) represents the data by taking into ac-\ncount the trade-off between the “goodness” (or likelihood)\nand the complexity (or generality) of the model.\nTo apply MDL we ﬁrst deﬁne the likelihood of the data\ngiven the modelM. Givennds=n·dsthe number of tran-\nsitions from stodby all users, the log likelihood of matrix\nPis given by/summationtext\ns,d|s/negationslash=dndslog(p(d|s))4. The MDL cost\nof modelMis given by the sum:\nCost (P,M) =Cost (P|M) +Cost (M).(6)\nCost (P| M), deﬁned as the negative log-likelihood,\ncaptures the goodness-of-ﬁt of the data given the model:\nhigher-values imply on accurate but yet succinct (less fac-\ntors) recoveries of P.Cost (M)captures the complexity:\nCost (M) =log∗(|A|) +log∗(|Z|) +/summationdisplay\ns,d,z[log∗(⌈p(d|z)n··⌉)\n+log∗(⌈p(s|z)n··⌉) +log∗(⌈p(z)n··⌉)]\nwherelog∗is the universal coding cost (number of bits)\nfor integers [9]. Cost (M)represents the encoding each\nmatrix in the model in integer representation with precision\nn··(the total number of transitions)5.\n4The likelihood is the product of p(d|s)for all ndstransitions [5].\n5Since we deal with counts, the smallest probability value is ( 1/n ··).690 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20163.2 Fixation Model\nUsers’ bursty repeated consumption of artists requires\nmodeling this behavior with a stochastic process that has\nmemory. Markov modulated processes are a class of mod-\nels that are particularly versatile for this task [22]. Our goal\nhere is not only to model user behavior but also, through\nthe use of intuitive parameters, understand how users re-\npeatedly consume artists. Most importantly, we want to re-\nproduce user attention giving rise to both exponential and\npower law distributions observed in our datasets.\nOur ﬁxation model, which captures the intra-artist tran-\nsitions, is a Markov modulated process where we use an in-\nﬁnite number of states, an approach widely used to model\nsystems with bursty behavior [22]. Figure 1 (b) illustrates\nour model (only the initial states). The “start” circle repre-\nsents the initial transition from the Inter-artist model. From\nstate zero we are interested in how long it takes to exit from\nthe “exit” transition. Thus, circles “start” and “exit” in Fig-\nure 1 (b) are not states but rather entrance (exit) transitions\nfrom (to) the inter-artist model. The states of the model\ncapture the afﬁnity of the user for the artist, that is how\nmuch the user is willing to repeatedly listen the artist’s\nsongs. There is a ﬁxed residency time ∆ton each state.\nThus, higher states represent that the user has a higher\nafﬁnity and thus dedicates more play-time to the artist.\nThe model has parameters 0<r<1and1≤f<4r. The\nlimit of 4ris required as described in [22]6. Parameter r\nmodels the user “rush”, capturing how users are led to hear\nmore from an artist (e.g. entire album) after hearing some\nmusic by this artist. Parameter fmodels user “ﬁxation”,\nrepresenting how long it takes for users to get over an ini-\ntial impulse to listen to an artist, which is also a function of\nthe artist’s song inventory size. A large value of fimplies\nthat users quickly get over their initial impulse or happens\nbecause the artist has just a few songs.\nWe can ﬁt the Fixation model to the complementary cu-\nmulative distribution function (CCDF) of the time users\ndedicate to an artist using the Levenberg-Marquardt algo-\nrithm. The CCDF will deﬁne the probability of the res-\nidency time in the chain. The inﬁnite number of states\ncan be captured by using a sufﬁcient number of states (100\nin our datasets). We evaluate the algorithm on the mean\nsquared error of the real data and the residency times gen-\nerated by the model. As we shall show empirically in our\nresults, this model is capable of generating both power-law\nand exponential residency times as also discussed in [22].\nOne interesting property of the Fixation model is that\nit is able to estimate the expected amount time users will\nﬁxate on a given artist. To achieve this, we can compute\nthe expected number of steps that it takes to go from the\nStart state to the End state [11]7. If we deﬁne this value\nper artist as ea, we can couple the Switch model with the\nFixation model by estimating the expected ﬁxation steps\nper latent spaces, ez, as:\n6The authors write the model in terms of a= 2/randb=f/a.\n7https://en.wikipedia.org/wiki/Absorbing Markov chainez=/summationdisplay\na∈|A|p(a|z)ea (7)\nThat is,eais the expected number of steps a user will\nremain in artist awith regards to his/her interest in gene z.\nIn the next section we describe S WIFT-F LOWS at work.\n4. S WIFT-F LOWS AT WORK\nWe apply S WIFT-F LOWS on datasets crawled from\nLast.FM. Last.FM aggregates various forms of digital\nmusic consumption, ranging from desktop/mobile me-\ndia players to streaming services (theirs and others)8.\nLast.FM is also an online social network (OSN), allow-\ning the creation of user groups as well as providing demo-\ngraphical data. The datasets we explore are:\nLast.FM-2009 Collected in using a snowball sam-\npling [2]. After the snowball sampling, 992 uni-\nformly random users were selected. The dataset\ncontains, for each user, the complete listening his-\ntory (all plays) from February 2005 to May 2009,\nthe self-declared nationality, age (at the time), and\nregistration date [2]. This dataset accounts for 18.5\nmillion user, artist, and timestamp triples, as well as\n107,397 unique artists.\nLast.FM-2014 Crawled in 2014 by identifying users that\nparticipate in discussion groups on Last.FM. Con-\ntains the listening history (from February 2005 to\nAugust 2014) of a subset of the users that discuss\npop-artists on Last.FM discussion groups. The total\nnumber of users in this dataset is 15,329. Also, this\ndataset contains 836,625 unique artists and roughly\n218 million user, artist, and timestamp triples. As\nis the case with Last.FM-2009, this dataset provides\nthe age and nationality of all the users.\nBecause of these various means of consumption [16],\nLast.FM presents itself as an interesting platform for study-\ningonline behavior . The service aggregates user accesses\nfrom desktop media players (that incorporates legal and il-\nlegal downloads), free, and also paid streaming services.\nNevertheless, it is important to point out that the observed\nattention trajectories will be impacted by how the data\nwas gathered (e.g., Last.FM-2014 has a bias towards pop\nartists), as well as the internal mechanisms of the OMSSs\n(e.g., such as recommendation services and user inter-\nfaces). As we shall discuss in our results, regardless of\nthe data biases, S WIFT-F LOWS is able to represent the at-\ntention trajectories of under-represented user populations.\nWe run the inter-artist attention Switch model of\nSWIFT-F LOWS onX−, and the Fixation model of\nSWIFT-F LOWS on the intra-artist transitions. In both\ncases, only artists which had at least ﬁve plays by ﬁve users\nare considered. In total, the Last.FM-2014 dataset has over\n3M plays of such artists, while Last.FM-2009 has roughly\n8Aggregation is done using plugins available on other OMSSs and\nmedia players.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 691100101102103104105\nDaily Fixation Time (s)10−410−310−210−1100P(X >x )r=0.995\nf=1.770Radiohead\nData\nFit\n100101102103104105\nDaily Fixation Time (s)10−310−210−1100\nr=0.996\nf=1.002T.I. feat. Justin TimberlakeFigure 2 . Validation of Fixation Model.\n176k plays. We note that even after ﬁltering, there still re-\nmains a signiﬁcant number of rare transitions as 44% of\nthe inter-artist transitions happen less than ten times.\n4.1 The Fixation Model at Work\nWe ﬁrst discuss the Fixation model. We begin by show-\ning how it ﬁts the time users spend listening to different\nartists on any given day (referred to as daily ﬁxation time).\nFigure 2 shows the ﬁtted and empirical complementary cu-\nmulative distribution functions (CCDF) of the daily ﬁxa-\ntion time for two particular example artists, namely Ra-\ndiohead , and T.I. feat. Justin Timberlake (a collaboration\nbetween two artists). This example was extracted from the\nLast.FM-2014 dataset.\nThe distribution for Radiohead clearly has long tails,\nand is similar to the distributions for most artists. In con-\ntrast, the distribution for the T.I. feat. Justin Timberlake\ncollaboration has a much shorter tail, approaching an ex-\nponential distribution. Unlike for the other artists, there is\nonly one song by this artist collaboration in our dataset,\nwhich might explain why users tend to spend less time lis-\ntening to them. Yet, our Fixation model provides close ﬁt-\ntings for both distributions, capturing both long and short\ntails. Interestingly, we can also use the model parameters r\nandfto distinguish between these artists: compared to Ra-\ndiohead , the T.I. feat. Justin Timberlake collaboration has\na slightly higher rush parameter ( r= 0.996) but a much\nlower ﬁxation parameter ( f= 1.002). Despite the higher\ninitial surge of attention, users lose interest more quickly\nin them. If it were not for our separation of the intra-artist\nfrom the inter-artist transitions, it would be impossible to\ncapture these different distributions with S WIFT-F LOWS .\nThat is, these superior ﬁts are only possible through the use\nthe modulated Markov models as done by our intra-artist\nmodel. This allows the model to capture both long and\nshort tails of user attention [22].\nWe proceeded to ﬁt our model to the daily ﬁxation\ntimes of 36,344 and 2,570 artists in the Last.FM-2014 and\nLast.FM-2009 datasets respectively (artists with more than\n5 plays by at least 5 users). In Figure 3 we show a scatter\nplot of the ﬁxation versus rush scores for the Last.FM-2014\ndataset. We found that, the vast majority of the artists have\nvery high values of rush r(above 0.95) and values of ﬁxa-\ntionf(1.5 to 2.5). There were also two other small groups\nof artists with very low (near 1) ﬁxation.\nLooking into these groups, we found many collabora-\ntions between artists, such as the aforementioned TI feat\n0.30.40.50.60.70.80.91.0\nFixation - f0.700.750.800.850.900.951.00Rush - r\nPaul BuchananTI\nfeat\nJustin\nTimberlakeThe Blue Nile\nThe Revelations feat. Tre WilliamsLorde\n0.00.40.81.21.62.02.42.83.23.6\nlog10(#Artists)\nFigure 3 . Rush vs Fixation\nJustin Timberlake. Another example of a collaboration is\nThe Revelations feat Tre Williams, it has both low ﬁxa-\ntion and low rush scores, thus attracts little attention in our\ndatasets. We also found that Blue Nile is an interesting ex-\nample. Blue Nile is a Scottish alternative/pop band whose\nlead singer is Paul Buchanan. From the plot, we can see\nthat the band has higher rush and lower ﬁxation than the\nsolo songs by Paul Buchanan. This is likely because the\nsolo career of Paul Buchanan mostly attracts more inter-\nested fans. Another example is Lorde, a relatively new\npop singer at time (2014). The artist obtains high rush and\nsomewhat lower ﬁxation. This may be explained listeners\ndiscovering her music.\nOur ﬁtting errors are very small in most cases. The av-\nerage Mean Squared Errors (MSE) of each ﬁtted distribu-\ntion for artists in Last.FM-2014 is only of 0.02, whereas\nin the Last.FM-2009 dataset it was of 0.03. The standard\ndeviations were of 0.02 and 0.04 for the Last.FM-2014 and\nLast.FM-2009 datasets, respectively.\n4.2 Extracting Listening Trajectories\nWe now discuss the Switch model. The ﬁrst step to ex-\necute the model us to decide the number of genes (latent\nfactors)kusing the MDL-based criteria described previ-\nously. To measure the MDL score, we searched for kin\nthe rangek∈[2,400]9. With MDL, we aim at ﬁnding a\nsuccinct (smaller) yet accurate latent representation of our\ndatasets. In our search, we found that in both datasets as\nkincreases the MDL cost ﬁrst decreases and then rapidly\nincreases, reaching global minimum at k=40. This value\nwas achieved in both sets of data. For this reason, our ex-\nperiments use a genome with 40 genes.\nTable 1 describes four different genes (latent fac-\ntors) extracted by S WIFT-F LOWS from the Last.FM-2014\ndataset. For each gene, the table shows the top 7 source\nsand destination dartists in a single column ranked by\n(p(a|z)). To further examine the genes, the table also\nsummarizes the nationality and age reported in the LastFM\nproﬁle of the top 50 users which have attention transitions\nwithin each gene. Finally, we cross-referenced the top\nartists in each gene with the AllMusic guide10for an au-\nthoritative source on artist metadata. The labels given to\neach gene stem from our own interpretation.\n9We searched k∈{2,4,8,10,20,30,40,50,100,200,300,400}.\n10http://www.allmusic.com/692 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Table 1 . Genes from the Last.FM-2014 dataset: top source and destination artists, and demographics of top-50 users.\nArtists and users are sorted by probabilities p(a|z)andp(z|u), respectively. Countries are: BR = Brazil, US = USA, NL =\nNetherlands, DE = Germany, PL = Poland, FI = Finland. Age statistics presented here are the 1st,2ndand3rdquartiles.\nWe also show the expected ﬁxation ezper gene.\nGene=18 (“BR/US pop”) Gene=20 (“metal”) Gene=23 (“electronic”) Gene=39 (“pop‘”)Source/Dest\nArtistsBritney Spears Nightwish Daft Punk Britney Spears\nWanessa Within Temptation David Guetta Madonna\nChristina Aguilera Epica Deadmau5 Christina Aguilera\nt.A.T.u. Korn Skrillex Rihanna\nKaty Perry Disturbed The Prodigy Lady Gaga\nPitty Marilyn Manson Tiesto Katy Perry\nLady Gaga Rammstein Pendulum KeshaUsers\nNationalityBR=98% DE = 18% US = 18% BR=78%\nNL=2% PL = 16% BR = 10% US=10%\nUS = 12% PL = 10% PL=5%\nFI = 8% UK = 10%Age\nQuartiles1st= 19 1st= 21 1st= 20 1st= 19\n2nd= 21 2nd= 24 2nd= 22 2nd= 22\n3rd= 24 3rd= 29 3rd= 25 3rd= 25ez ez= 793.55 ez= 642.15 ez= 636.10 ez= 886.10\nOverall, the genes found through S WIFT-F LOWS point\nto a semantically sound segmentation of transition spaces\nthat combines characteristics of the artists and users of\nthe OMSS. Illustratively, gene z= 18 is predominately\nformed by female pop/rock singers as both sources and\ndestinations. This is not the only gene with similar pop\nsingers; gene z= 39 is another gene with a similar com-\nposition in this respect. Yet, the presence of Brazilian pop\nartists (e.g., Wanessa ,Claudia Leitte , and Pitty ) in gene\nz= 18 explains why the vast majority (98%) of the top\nusers in this gene are Brazilians (BR). Gene z= 20 in turn\nis mostly focused on different sub-genres of metal (e.g.,\ngoth-metal and rap-metal). A large fraction of the top-50\nusers of the “heavy metal” gene are from Germany and\nPoland. Finally, gene z= 23 represents users of different\nnationalities (American being the most frequent one) who\nlike to listen to electronic dance music, often transitioning\nbetween different artists of that genre. It is also notewor-\nthy that in a dataset mostly comprised of pop artists fans\n(Last.FM-2014), S WIFT-F LOWS is able to account for the\ntrajectories of heavy metal and electronic music fans.\nTo understand the expected ﬁxation of users per gene,\nwe make use of Equation 7 ( ez). Initially translate ezval-\nues to seconds. That is, we performed a linear regression\nusing the values of ea(see Eq. 7), expected number of\nsteps per artist, with the average ﬁxation time per day (de-\nscribed in the previous subsection). With this regression,\nwe found that each step in the chain accounts for, approx-\nimately, 1.11 seconds. From the table, we can see that\ngenes 20 and 23 have lower expected ﬁxation times. That\nis, genez= 20 expects 642 steps (roughly 12 minutes)\nin the Fixation model, whereas gene z= 23 expects 632steps (11.6 minutes). The highest value in the table is from\ngenez= 18 (14 minutes).\nNotice that both models combined provide a general\noverview of attention. That is, we are able to understand\nhow users will transition between artists, as well as the ex-\npected number of steps users will listen to a given artist.\nThis represents one of the major strengths of S WIFT-\nFLOWS when compared to previous efforts [7, 22].\n5. CONCLUSIONS\nIn this paper, we presented the S WIFT-F LOWS model. One\nof the main advantages of S WIFT-F LOWS is that it allows\nresearchers to explore user listening habits based on com-\nplementary behaviors: the ﬁxation on a single artist over\nshort or long bursts, as well as the change in attention\nfrom one artist to the next. We applied S WIFT-F LOWS to\nuncover semantically meaningful maps of attention ﬂows\nin large OMSSs datasets. Moreover, S WIFT-F LOWS pro-\nvides excellent ﬁts to the attention time dedicated to artists.\nSWIFT-F LOWS , therefore, is an useful tool for further re-\nsearch aiming to understand listening behavior.\nAcknowledgments: Research supported by the project\nFAPEMIG-PRONEX-MASWeb, process number APQ-01400-\n14, by grant 460154/2014-1 from CNPq, by the EU-BR BigSea\nproject (MCTI/RNP 3rd Coordinated Call), and by the EU-\nBrazilCC (grants FP7 614048 and CNPq 490115/2013-6) project,\nas well as by individual grants from CNPq/CAPES/Fapemig. Our\nwork was also supported by NSF grant CNS-1065133, U.S. Army\nResearch Laboratory under Cooperative Agreement W911NF-\n09-2-0053, and by the Defense Threat Reduction Agency con-\ntract No. HDTRA1-10-1-0120. The views and conclusions con-\ntained here are our own (the authors).Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 6936. REFERENCES\n[1] Alejandro Bellog ´ın, Arjen P. de Vries, and Jiyin He.\nArtist Popularity: Do Web and Social Music Services\nAgree? In Proc. ICWSM , 2013.\n[2] Oscar Celma. Music Recommendation and Discovery\nin the Long Tail . Springer, 1 edition, 2010.\n[3] Shuo Chen, Josh L Moore, Douglas Turnbull, and\nThorsten Joachims. Playlist prediction via metric em-\nbedding. In Proc. KDD , 2012.\n[4] Shuo Chen, Jiexun Xu, and Thorsten Joachims. Multi-\nspace probabilistic sequence modeling. In Proc. KDD ,\n2013.\n[5] Imre Csisz ´ar and Paul C. Shields. The consistency of\nthe bic markov order estimator. The Annals of Statis-\ntics, 28(6):1601–1619, 12 2000.\n[6] Flavio Figueiredo, Jussara M Almeida, Yasuko Mat-\nsubara, Bruno Ribeiro, and Christos Faloutsos. Revisit\nBehavior in Social Media: The Phoenix-R Model and\nDiscoveries. In PKDD/ECML , 2014.\n[7] Flavio Figueiredo, Bruno Ribeiro, Jussara M Almeida,\nand Christos Faloutsos. TribeFlow: Mining & Predict-\ning User Trajectories. In Proc. WWW , 2016.\n[8] Tom Grifﬁths. Gibbs sampling in the generative model\nof latent dirichlet allocation. Technical report, 2002.\n[9] Mark H Hansen and Bin Yu. Model Selection and the\nPrinciple of Minimum Description Length, 2001.\n[10] David J Hargreaves. Musical imagination: perception\nand production, beauty and creativity. Psychology of\nmusic , 40(5):539–557, 2012.\n[11] David A. Levin, Yuval Peres, and Elizabeth L. Wilmer.\nMarkov Chains and Mixing Times . AMS, 2008.\n[12] Andryw Marques, Nazareno Andrade, and Lean-\ndro Balby Marinho. Exploring the Relation Between\nNovelty Aspects and Preferences in Music Listening.\nInProc. ISMIR , 2013.\n[13] Yasuko Matsubara, Yasushi Sakurai, Christos Falout-\nsos, Tomoharu Iwata, and Masatoshi Yoshikawa.\nFast mining and forecasting of complex time-stamped\nevents. In Proc. KDD , 2012.\n[14] Carl D. Meyer. Stochastic Complementation, Uncou-\npling Markov Chains, and the Theory of Nearly Re-\nducible Systems. SIAM Review , 31(2):240–272, 1989.\n[15] Joshua L. Moore, Shuo Chen, Thorsten Joachims, and\nDouglas Turnbull. Taste Over Time: The Temporal Dy-\nnamics of User Preferences. In Proc. ISMIR , 2013.\n[16] Rapha ¨el Nowak. Investigating the interactions between\nindividuals and music technologies within contem-\nporary modes of music consumption. First Monday ,\n19(10):Online, 2014.[17] Chan Ho Park and Minsuk Kahng. Temporal Dynamics\nin Music Listening Behavior: A Case Study of Online\nMusic Service. In Proc. ACIS , 2010.\n[18] Minsu Park, Ingmar Weber, Mor Naaman, and Sarah\nVieweg. Understanding Musical Diversity via Online\nSocial Media. In Proc. ICWSM , 2015.\n[19] Steffen Rendle, Christoph Freudenthaler, and Lars\nSchmidt-Thieme. Factorizing personalized Markov\nchains for next-basket recommendation. Proc. WWW ,\n2010.\n[20] P. J. Rentfrow and S. D. Gosling. The Do Re Mi’s of\nEveryday Life: The Structure and Personality Corre-\nlates of Music Preferences. Journal of personality and\nsocial psychology , 84(6), 2003.\n[21] Peter J. Rentfrow, Lewis R. Goldberg, and Daniel J.\nLevitin. The structure of musical preferences: a ﬁve-\nfactor model. Journal of personality and social psy-\nchology , 100(6):1139–1157, June 2011.\n[22] Stephan Robert and Jean-Yves Le Boudec. On a\nMarkov modulated chain exhibiting self-similarities\nover ﬁnite timescale. Performance Evaluation , 27-\n28:159–173, 1996.\n[23] Suvi Saarikallio and Jaakko Erkkil ¨a. The role of music\nin adolescents’ mood regulation. Psychology of music ,\n35(1):88–109, 2007.\n[24] Philipp Singer, Denis Helic, Andreas Hotho, and\nMarkus Strohmaier. HypTrails: a bayesian approach\nfor comparing hypotheses about human trails on the\nweb. In Proc. WWW , 2015.\n[25] Liang Xiong, Xi Chen, Tzu-Kuo Huang, Jeff Schnei-\nder, and Jaime G. Carbonel. Temporal collaborative ﬁl-\ntering with bayesian probabilistic tensor factorization.\nInProc. SDM , 2010.694 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Tempo Estimation for Music Loops and a Simple Confidence Measure.",
        "author": [
            "Frederic Font",
            "Xavier Serra"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417659",
        "url": "https://doi.org/10.5281/zenodo.1417659",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/195_Paper.pdf",
        "abstract": "Tempo estimation is a common task within the music infor- mation retrieval community, but existing works are rarely evaluated with datasets of music loops and the algorithms are not tailored to this particular type of content. In addi- tion to this, existing works on tempo estimation do not put an emphasis on providing a confidence value that indicates how reliable their tempo estimations are. In current mu- sic creation contexts, it is common for users to search for and use loops shared in online repositories. These loops are typically not produced by professionals and lack anno- tations. Hence, the existence of reliable tempo estimation algorithms becomes necessary to enhance the reusability of loops shared in such repositories. In this paper, we test six existing tempo estimation algorithms against four mu- sic loop datasets containing more than 35k loops. We also propose a simple and computationally cheap confidence measure that can be applied to any existing algorithm to estimate the reliability of their tempo predictions when ap- plied to music loops. We analyse the accuracy of the algo- rithms in combination with our proposed confidence mea- sure, and see that we can significantly improve the algo- rithms’ performance when only considering music loops with high estimated confidence.",
        "zenodo_id": 1417659,
        "dblp_key": "conf/ismir/FontS16",
        "content": "TEMPO ESTIMATION FOR MUSIC LOOPS AND A SIMPLE\nCONFIDENCE MEASURE\nFrederic Font and Xavier Serra\nMusic Technology Group, Universitat Pompeu Fabra\nfrederic.font@upf.edu ,xavier.serra@upf.edu\nABSTRACT\nTempo estimation is a common task within the music infor-\nmation retrieval community, but existing works are rarely\nevaluated with datasets of music loops and the algorithms\nare not tailored to this particular type of content. In addi-\ntion to this, existing works on tempo estimation do not put\nan emphasis on providing a conﬁdence value that indicates\nhow reliable their tempo estimations are. In current mu-\nsic creation contexts, it is common for users to search for\nand use loops shared in online repositories. These loops\nare typically not produced by professionals and lack anno-\ntations. Hence, the existence of reliable tempo estimation\nalgorithms becomes necessary to enhance the reusability\nof loops shared in such repositories. In this paper, we test\nsix existing tempo estimation algorithms against four mu-\nsic loop datasets containing more than 35k loops. We also\npropose a simple and computationally cheap conﬁdence\nmeasure that can be applied to any existing algorithm to\nestimate the reliability of their tempo predictions when ap-\nplied to music loops. We analyse the accuracy of the algo-\nrithms in combination with our proposed conﬁdence mea-\nsure, and see that we can signiﬁcantly improve the algo-\nrithms’ performance when only considering music loops\nwith high estimated conﬁdence.\n1. INTRODUCTION\nTempo estimation is a topic that has received consider-\nable attention within the music information retrieval (MIR)\ncommunity and has had a dedicated task in the Music In-\nformation Retrieval Evaluation eXchange (MIREX) since\nits ﬁrst edition in 2005. Tempo estimation consists in the\nautomatic determination of the “rate of musical beats in\ntime” [10], that is to say, in the identiﬁcation of the rate\nat which periodicities occur in the audio signal that con-\nvey a rhythmic sensation. Tempo is typically expressed in\nbeats per minute (BPM), and is a fundamental property to\ncharacterise rhythm in music [13]. Applications of tempo\nestimation include, just to name a few, music recommen-\ndation, music remixing, music browsing, and beat-aware\naudio analysis and effects.\nc/circlecopyrtFrederic Font and Xavier Serra. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). Attri-\nbution: Frederic Font and Xavier Serra. “Tempo Estimation for Music\nLoops and a Simple Conﬁdence Measure”, 17th International Society for\nMusic Information Retrieval Conference, 2016.Our particular research is aimed at automatically anno-\ntating user provided music loops hosted in online sound\nsharing sites to enhance their potential reusability in music\ncreation contexts. We can deﬁne music loops as short mu-\nsic fragments which can be repeated seamlessly to produce\nan “endless” stream of music. In this context, BPM is an\nimportant music property to annotate. The kind of music\nloops we are targeting can include noisy and low quality\ncontent, typically not created by professionals. This may\nincrease the difﬁculty of the tempo estimation task. Tak-\ning that into consideration, it is particularly relevant for\nus to not only estimate the tempo of music loops, but to\nalso quantify how reliable an estimation is (i.e., to pro-\nvide a conﬁdence measure). Except for the works de-\nscribed in [10, 14] (see below), tempo estimation has been\nrarely evaluated with datasets of music loops, and we are\nnot aware of speciﬁc works describing algorithms that are\nspeciﬁcally tailored to this particular case.\nIn this paper we evaluate the accuracy of six state of\nthe art tempo estimation algorithms when used to annotate\nfour different music loop datasets, and propose a simple\nand computationally cheap conﬁdence measure that can\nbe used in combination with any of the existing methods.\nThe conﬁdence measure we propose makes the assump-\ntion that the audio signal has a steady tempo thorough its\nwhole duration. While this assumption can be safely made\nin the case of music loops, it does not necessarily hold for\nother types of music content such as music pieces. Hence,\nthe applicability of the conﬁdence measure we propose is\nrestricted to music loops. Using our conﬁdence measure\nin combination with existing tempo estimation algorithms,\nwe can automatically annotate big datasets of music loops\nand reach accuracies above 90% when only considering\ncontent with high BPM estimation conﬁdence. Such reli-\nable annotations can allow music production systems to,\nfor example, present relevant loops to users according to\nthe BPM of a music composition, not only by showing\nloops with the same BPM but also by automatically trans-\nforming loops to match a target BPM. This effectively in-\ncreases the reusability of user provided music loops in real-\nworld music creation contexts.\nThe rest of the paper is organised as follows. In Sec. 2\nwe give a quick overview of related work about tempo es-\ntimation. In Sec. 3 we describe the conﬁdence measure\nthat we propose. Sections 4 and 5 describe the evaluation\nmethodology and show the results of our work, respec-\ntively. We end this paper with some conclusions in Sec. 6.269In the interest of research reproducibility, the source code\nand one of the datasets used in this paper have been made\navailable online in a public source code repository1.\n2. RELATED WORK\nA signiﬁcant number of works within the MIR research\nﬁeld have been focused on the task of tempo estimation.\nIn general, tempo estimation algorithms are based on de-\ntecting onsets in an audio signal, either as a continuous\nfunction [3, 14, 15] or as discrete events in time [5]. Then,\na dominant period is extracted from the onsets either by\nanalysing inter-onset intervals, using autocorrelation [11]\nor resonating ﬁlters [12]. Some approaches perform more\ncomplex operations such as analysing periodicities in dif-\nferent frequency bands [8, 19], performing source separa-\ntion [6,9], or using neural networks to learn features to use\ninstead of usual onset information [1].\nWhile comparative studies of tempo estimation algo-\nrithms have been carried out in the past [10, 21], we are\nnot aware of any study solely devoted to the evaluation of\ntempo estimation algorithms for music loops. One of the\ntypical datasets that some of the existing tempo estima-\ntion works use for evaluation is the ISMIR 2004 dataset re-\nleased for the tempo induction contest of that year2. This\ndataset is divided into three subsets, one of them com-\nposed of 2k audio loops. Gouyon et. al. [10] published\nthe evaluation results for the contest considering the dif-\nferent subsets of the dataset, but no signiﬁcant differences\nare reported regarding the accuracies of the tempo esti-\nmation algorithms with the loops subset compared to the\nother subsets. To the best of our knowledge, the only other\nwork that uses the loops subset of the ISMIR 2004 dataset\nand reports its accuracy separated form other datasets is by\nOliveira et. al. [14]. The authors report lower estimation\naccuracies when evaluating with the loops dataset and at-\ntribute this to the fact that loops are typically shorter than\nthe other audio signals (in many cases shorter than 5 sec-\nonds).\nSurprisingly enough, there has not been much research\non conﬁdence measures for tempo estimation algorithms.\nExcept for the work by Zapata et al. [22] in which a con-\nﬁdence measure that can be used for tempo estimation is\ndescribed (see below), we are not aware of other works di-\nrectly targeted at this issue. Among these few, Grosche\nand M ¨uller [11] describe a conﬁdence measure for their\ntempo estimation algorithm based on the amplitude of a\npredominant local pulse curve. By analysing tempo esti-\nmation accuracy and disregarding the regions of the analy-\nsis with bad conﬁdence, the overall accuracy signiﬁcantly\nincreases. Alternatively, Percival and Tzanetakis [15] sug-\ngest that beat strength [18] can be used to derive conﬁdence\nfor tempo candidates, but no further experiments are car-\nried out to asses its impact on the accuracy of tempo esti-\nmation. Finally, a very recent work by Quinton et. al. [16]\nproposes the use of rhythmogram entropy as a measure of\nreliability for a number of rhythm features, and report a\n1https://github.com/ffont/ismir2016\n2http://mtg.upf.edu/ismir2004/contest/tempoConteststatistical correlation between measured entropy and the\nresulting accuracies for different tasks.\n3. CONFIDENCE MEASURE\nAssuming that we obtain a BPM estimate for a given au-\ndio signal, the conﬁdence measure that we propose is based\non comparing the duration of the whole audio signal with a\nmultiple of the duration of a single beat according to the es-\ntimated BPM. If the actual duration of the signal is close to\na multiple of the duration of a single beat, we hypothesise\nthat the BPM estimation is reliable. The ﬁrst thing we do to\ncompute the conﬁdence measure is to round the estimated\ntempo value to its nearest integer. The reasoning behind\nthis is that it is very unlikely that loops are created with\nless than 1 BPM resolution tempo (see Sec. 4.1), and thus\nwe consider the best BPM estimate of a tempo estimation\nalgorithm to be its nearest integer. Given the sample rate\nSRof an audio signal and its estimated tempo BPMe, we\ncan estimate the duration (or length) of an individual beat\nin number of samples lbas\nlb=60·SR\nBPMe.\nThen, potential durations for the audio signal can be com-\nputed as multiples of the individual beat duration, L[n] =\nn·lb, wheren∈Z+. In our computation, we restrict n\nto the range 1≤n≤128. This is decided so that the\nrange can include loops that last from only 1 beat to 128\nbeats, which would correspond to a maximum of 32 bars\nin 4/4 meter. In practice, what we need here is a number\nbig enough such that we won’t ﬁnd loops longer than it.\nGivenL, what we need to see at this point is if any of its\nelements closely matches the actual length of the original\naudio signal. To do that, we take the actual length of the\naudio signal la(in number of samples), compare it with all\nelements of Land keep the minimum difference found:\n∆l= min{|L[n]−la|:n≤128}.\nA value of ∆lnear 0 means that there is a close match\nbetween one of the potential lengths and the actual length\nof the audio signal. Having computed ∆l, we ﬁnally deﬁne\nour conﬁdence measure as\nconfidence (L,la) =/braceleftBigg\n0 if∆l>λ\n1−∆l\nλotherwise,\nwhereλis a parameter set to half the duration of a single\nbeat (λ= 0.5·lb). In this way, if laexactly matches one\nof the multiples of lb, the conﬁdence will be 1. If ∆lis\nas long as half the duration between beats, the conﬁdence\nwill be 0 (see Fig. 1, top).\nThe reasoning behind this simple conﬁdence measure is\nthat it is very unlikely that, only by chance, an audio sig-\nnal has a duration which closely matches a multiple of the\nbeat duration for a given estimated BPM. This means that\nwe assume that there is a relation between the duration of\nthe signal and its BPM, and therefore our proposed conﬁ-\ndence will fail if the audio signal contains silence (either at270 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20160 20000 40000 60000 80000 100000 120000 140000 1600001.0\n0.5\n0.00.51.0\nlalb\n∆lλ\n0 20000 40000 60000 80000 100000 120000 140000\nTime (samples)1.0\n0.5\n0.00.51.0\nlala\n0la\n1la\n2Figure 1 . Visualisation of conﬁdence computation output according to BPM estimation and signal duration (green curves).\nThe top ﬁgure shows a loop whose annotated tempo is 140 BPM but the predicted tempo is 119 BPM. The duration of the\nsignalladoes not closely match any multiple of lb(dashed vertical lines), and the output conﬁdence is 0.59 (i.e., 1−∆l\nλ).\nThe ﬁgure at the bottom shows a loop that contains silence at the beginning and at the end, and for which tempo has\nbeen correctly estimated as being 91 BPM. The yellow curve represents its envelope and the vertical dashed red lines the\nestimated effective start and end points. Note that la\n2closely matches a multiple of lb, resulting in a conﬁdence of 0.97. The\noutput conﬁdence computed with la,la\n0andla\n1produces lower values.\nthe beginning or at the end) which is not part of the loop\nitself (i.e., the loop is not accurately cut ). To account for\nthis potential problem, we estimate the duration that the\naudio signal would have if we removed silence at the be-\nginning, at the end, or both at the beginning and at the end.\nWe take the envelope of the original audio3and consider\nthe effective starting point of the loop as being the point in\ntimetswhere the envelope amplitude raises above 5% of\nthe maximum. Similarly, we consider the effective end te\nat the last point where the envelope goes below the 5% of\nthe maximum amplitude (or at the end of the audio signal if\nenvelope is still above 5%). Taking ts,te, andla(the orig-\ninal signal length), we can then compute three alternative\nestimates for the duration of the loop ( la\n0,la\n1andla\n2) by i)\ndisregarding silence at the beginning ( la\n0=la−ts),ii)dis-\nregarding silence at the end ( la\n1=te), and iii)disregarding\nsilence both at the beginning and at the end ( la\n2=te−ts).\nThen, we repeat the previously described conﬁdence com-\nputation with the three extra duration estimates la\n0,la\n1and\nla\n2. Note that these will produce meaningful results in cases\nwhere the original loop contains silence which is not rele-\nvant from a musical point of view, but they will not result in\nmeaningful conﬁdence values if the loop contains silence\nat the beginning or at the end which is in fact part of the\nloop (i.e., which is needed for it seamless repetition). Our\nﬁnal conﬁdence value is taken as the maximum conﬁdence\nobtained when using any of la,la\n0,la\n1andla\n2estimated sig-\nnal durations (see Fig. 1, bottom).\nBecause the conﬁdence measure that we propose only\nrelies on a BPM estimate and the duration of the audio sig-\nnal, it can be used in combination with any existing tempo\nestimation algorithm. Also, it is computationally cheap to\ncompute as the most complex operation it requires is the\nenvelope computation. However, this conﬁdence measure\n3We use the Envelope algorithm from the open-source audio analysis\nlibrary Essentia [2], which applies a non-symmetric lowpass ﬁlter and\nrectiﬁes the signal.should not be applied to content other than music loops as\nit only produces meaningful results under the assumption\nthat tempo is completely steady across the whole signal.\n4. EV ALUATION\n4.1 Datasets\nOur evaluation is conducted using 4 different datasets col-\nlected from different sources and containing a total of more\nthan 35k loops. Table 1 shows basic statistics of each\ndataset. We now brieﬂy describe each of the datasets:\n•FSL4 : This dataset contains user-contributed loops\nuploaded to Freesound [7]. It has been built in-\nhouse by searching Freesound for sounds with the\nquery terms loop andbpm, and then automatically\nparsing the returned sound ﬁlenames, tags and tex-\ntual descriptions to identify tempo annotations made\nby users. For example, a sound containing the tag\n120bpm is considered to have a ground truth of 120\nBPM. Detailed instructions on how this dataset was\ncreated and on how can be reproduced are found in\nthe source code repository (see Sec. 1).\n•APPL : This dataset is composed of the audio loops\nbundled in Apple’s Logic Pro4music production\nsoftware. We parsed the metadata embedded in the\naudio ﬁles using source code available in a public\nrepository5, and extracted in this way tempo anno-\ntations for all the loops.\n•MIXL : This dataset contains all the loops bundled\nwith Acoustica’s Mixcraft 7 music production soft-\nware6. Tempo annotations are provided in its loop\n4http://apple.com/logic-pro\n5http://github.com/jhorology/apple-loops-meta-reader\n6http://acoustica.com/mixcraftProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 271Dataset N instances Total duration Mean loop duration Duration range Tempo range Source\nFSL4 3,949 8h 22m 7.63s 0.15s - 30.00s 32 - 300 Freesound\nAPPL 4,611 9h 34m 7.47s 1.32s - 40.05s 53 - 140 Logic Pro\nMIXL 5,451 14h 11m 9.37s 0.32s - 110.77s 55 - 220 Mixcraft 7\nLOOP 21,226 50h 30m 8.57s 0.26s - 129.02s 40 - 300 Looperman\nTable 1 . Basic statistics about the datasets used for evaluation. Additional information and plots can be found in the paper’s\nsource code repository (see Sec. 1).\nbrowser and can be easily exported into a machine-\nreadable format.\n•LOOP : This dataset is composed of loops down-\nloaded from Looperman7, an online loop sharing\ncommunity. It was previously used for research pur-\nposes in [17]. Tempo annotations are available as\nmetadata provided by the site.\nBecause of the nature of how the datasets were col-\nlected, we found that some of the loops do not have a BPM\nannotation that we can use as ground truth or have a BPM\nannotation which is outside what could be intuitively con-\nsidered a reasonable tempo range. To avoid inconsistencies\nwith the annotations, we clean the datasets by removing\ninstances with no BPM annotation or with a BPM anno-\ntation outside a range of [25,300]. Interestingly, we see\nthat all the loops in our datasets are annotated with inte-\nger tempo values, meaning that it is not common for music\nloops to be produced with tempo values with less than 1\nBPM resolution. For analysis purposes, all audio content\nfrom the dataset is converted to linear PCM mono signals\nwith 44100 Hz sampling frequency and 16 bit resolution.\n4.2 Tempo estimation algorithms\nIn our evaluation we compare six existing tempo estima-\ntion algorithms. These have been chosen based on their\navailability and to represent different approaches to the\ntempo estimation task. We now brieﬂy describe each of\nthe algorithms, further details on how the algorithms work\ncan be found in corresponding papers.\n•Gkiokas12 : Gkiokas et. al. [9] propose a tempo\nestimation algorithm based on the separation of the\naudio signal into its percussive and harmonic com-\nponents. Periodicity analysis is carried out by con-\nvolving extracted features (ﬁlterbank energies for\nthe percussive component and chroma features for\nthe harmonic component) with a bank of resonators.\nOutput tempo value is computed by applying heuris-\ntics based on metrical relations knowledge (meter,\ntactus, tatum) to the periodicity vector. We use a\nMatlab implementation of the algorithm kindly pro-\nvided to us by the authors.\n•Degara12 : Degara et. al. [4] describe a probabilis-\ntic approach for beat tracking based on inter-onset-\ninterval times and a salience measure for individual\n7http://looperman.combeat estimates. This method builds from previous\nprobabilistic beat tracking methods such as Klapuri\net. al. [12]. We use the implementation provided in\nEssentia, where ﬁnal estimated tempo is given based\non the mean of estimated beat intervals (see Rhyth-\nmExtractor2013 algorithm8).\n•Zapata14 : Zapata et. al. [20] propose a beat track-\ning algorithm which estimates beat positions based\non computing the agreement of alternative outputs of\na single model for beat tracking using different sets\nof input features (i.e., using a number of onset de-\ntection functions based on different audio features).\nAgain, we use the implementation provided in Es-\nsentia, which outputs a single BPM estimate based\non estimated beat intervals.\n•Percival14 : Percival and Tzanetakis [15] describe a\ntempo estimation algorithm optimised for low com-\nputational complexity that combines several ideas\nfrom existing tempo estimation algorithms and sim-\npliﬁes their steps. The algorithm computes an onset\nstrength function based on ﬁltered spectral ﬂux from\nwhich tempo lag candidates are estimated using au-\ntocorrelation. The most prominent tempo lag is se-\nlected and a simple decision tree algorithm is used\nto chose the octave of the ﬁnal BPM output. We use\na Python implementation of the algorithm provided\nby the authors in their original paper9.\n•B¨ock15 : B¨ock et. al. [1] propose a novel tempo es-\ntimation algorithm based on a recurrent neural net-\nwork that learn an intermediate beat-level represen-\ntation of the audio signal which is then feed to a bank\nof resonating comb ﬁlters to estimate the dominant\ntempo. This algorithm got the highest score in IS-\nMIR 2015 Audio Tempo Estimation task. An im-\nplementation by the authors is included in the open-\nsource Madmom audio signal processing library10.\n•RekBox : We also include an algorithm from a com-\nmercial DJ software, Rekordbox11. Details on how\nthe algorithm works are not revealed, but a freely\ndownloadable application is provided that can anal-\nyse a music collection and export the results in a\nmachine-readable format.\n8http://essentia.upf.edu/documentation/algorithms reference.html\n9http://opihi.cs.uvic.ca/tempo\n10http://github.com/CPJKU/madmom\n11http://rekordbox.com272 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Figure 2 . Overall accuracy for the six tempo estimation algorithms tested against the four datasets.\n4.3 Methodology\nFor testing the above algorithms against the four collected\ndatasets we follow standard practice and adopt the method-\nology described by Gouyon et al. [10]. In addition to the\nstandard Accuracy 1 andAccuracy 2 measures12, we add\nan extra measure that we call Accuracy 1e and that repre-\nsents the percentage of instances whose estimated BPM is\nexactly the same as the ground truth after rounding the esti-\nmated BPM to the nearest integer. Accuracy 1e is therefore\nmore strict than Accuracy 1. The reason why we added this\nextra accuracy measure is that, imagining a music creation\ncontext where loops can be queried in a database, it is of\nspecial relevance to get returned instances whose BPM ex-\nactly matches that speciﬁed as target.\nBesides the overall accuracy measurements, we are\nalso interested in observing how accuracy varies accord-\ning to the conﬁdence values that we estimate (Sec. 3). We\ncan intuitively imagine that if we remove instances from\nour datasets whose estimated BPM conﬁdence is below\na certain threshold, the overall accuracy results will in-\ncrease. However, the higher we set the minimum conﬁ-\ndence threshold, the smaller the size of ﬁltered dataset will\nbe. Hence, we want to quantify the relation between the\noverall accuracy and the total number of music loops that\nremain in a dataset after ﬁltering by minimum conﬁdence.\nTo do that, given one of the aforementioned accuracy mea-\nsures, we can deﬁne a minimum conﬁdence threshold γ\nand a function A(γ)that represents overall BPM estima-\ntion accuracy when only evaluating loop instances whose\nestimated conﬁdence value is above γfor a given dataset\nand tempo estimation algorithm. Similarly, we can de-\nﬁne another function N(γ)which returns the percentage\nof instances remaining in a dataset after ﬁltering out those\nwhose estimated conﬁdence value (for a given tempo esti-\nmation method) is below γ.A(γ)andN(γ)can be under-\nstood as standard precision and recall curves, and therefore\nwe can deﬁne a combined score measure S(γ)doing the\nanalogy with an f-measure computation:\n12Accuracy 1 is the percentage of instances whose estimated BPM is\nwithin 4% of the annotated ground truth. Accuracy 2 is the percentage of\ninstances whose estimated BPM is within a 4% of1\n3,1\n2, 1, 2, or 3 times\nthe ground truth BPM.S(γ) = 2·A(γ)·N(γ)\nA(γ) +N(γ).\nAn overall score for a given dataset, tempo estimation\nalgorithm and accuracy measure can thus be given by tak-\ning the mean of S(γ),¯S.\n5. RESULTS\n5.1 Overall accuracy\nOverall accuracy results show that Percival14 obtains the\nhighest accuracy scores for all accuracy measures and all\ndatasets except for the LOOP dataset, in which highest\nscore for Accuracy 1 is obtained by Zapata14 (Fig. 2).\nConsidering the data from all datasets at once, mean accu-\nracy values for Percival14 range from 47% (Accuracy 1e)\nto 73% (Accuracy 2), with an average increase of 7% accu-\nracy when compared with the second best-scored method.\nWith a few exceptions, pairwise accuracy differences be-\ntween Percival14 and the second best-scored method in\nall datasets and accuracy measures are statistically signif-\nicant using McNemar’s test and a signiﬁcance value of\nα= 0.01(i.e.,p/lessmuch0.01). We also observe that accu-\nracies for the APPL dataset tend to be higher than for other\ndatasets. This can be explained by the fact that APPL con-\ntains professionally created and curated loops, while the\nother datasets contain user contributed content, not neces-\nsarily created by professionals (Mixcraft’s loop library also\ncontains content gathered from online repositories).\n5.2 Accuracy vs conﬁdence measure\nFig. 3 shows the accuracy of the three best-scoring tempo\nestimation algorithms and the number of instances remain-\ning in the dataset when ﬁltering by different values of a\nconﬁdence threshold γ(Sec. 4.3). As we expected, we\ncan see how accuracy increases with γbut the number\nof instances decreases. Interestingly, we observe that the\nnumber of instances decays later for estimations performed\nwith Percival14 algorithm than for the other algorithms.\nThis reﬂects the fact that Percival14 produces better BPM\nestimates. Filtering by the conﬁdence measure, a potentialProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 2730.0 0.2 0.4 0.6 0.8 1.0\nγ020406080100Accuracy (%)\n08001600240032004000\nNumber of instances\nPercival14\nDegara12\nZapata14Figure 3 . Accuracy vs conﬁdence measure for FSL4\ndataset. Lower bounds of the ﬁlled areas correspond to\nAccuracy 1e, while upper bounds correspond to Accuracy\n2. Solid lines represent the number of instances remaining\nin the dataset.\nuser searching for loops in a dataset could deﬁne a mini-\nmum threshold to get more accurate results at the expense\nof getting less loops returned. For instance, if we set a hard\nconﬁdence threshold of γ= 0.95(vertical line in Fig. 3),\nwe ﬁnd that the accuracies for Percival14 method range,\non average, from 67% (Accuracy 1e) to 92% (Accuracy 2)\nwhile preserving an average of 52% of the instances. In-\nterestingly enough, we observe that when setting that hard\nthreshold, reported RekBox accuracies outperform these of\nPercival14 in all datasets, with an average increase ranging\nfrom 2% for Accuracy 2 to 14% for Accuracy 1e (all sta-\ntistically signiﬁcant with p/lessmuch0.01). We attribute this to\nthe fact that RekBox seems to have a built-in conﬁdence\nmeasure thresholding step in which the algorithm outputs\n0 BPM when the analysis does not meet certain conﬁdence\nrequirements. Therefore, once ﬁltering the datasets by γ\n(even with small values), all those instances whose BPM\nestimation is 0 BPM get discarded. Nevertheless, it is\nalso important to note that ﬁltering with the hard threshold,\nRekBox only preserves an average of 31% of the instances\n(lower than the 52% reported above by Percival14).\nIf we look at the combined accuracy and conﬁdence\nmeasure ¯Sdescribed in Sec. 4.3, we again ﬁnd that Perci-\nval14 obtains the best score in all datasets and for all accu-\nracy measures (i.e. for A(γ)computed with Accuracy 1e,\nAccuracy 1 or Accuracy 2). This means that Percival14 of-\nfers the overall best balance between estimation accuracy\nand number of preserved instances in the dataset when ﬁl-\ntering by a minimum conﬁdence threshold.\n5.3 Comparison of conﬁdence measures\nZapata et. al. [22] propose a conﬁdence measure that can\nbe used for tempo estimation and that is based on comput-\ning the mutual agreement between an ensemble of tempo\nestimation algorithms. To make this conﬁdence measure\nnumerically comparable to the one we propose, we nor-\nmalise the conﬁdence output of Zapata et. al. to take val-\nues from 0 to 1. Similarly to Fig. 3, we plot the estimation\naccuracy and the number of remaining instances as a func-\ntion of a minimum conﬁdence threshold γ(Fig. 4). We ob-\n0.0 0.2 0.4 0.6 0.8 1.0\nγ020406080100Accuracy (%)\n08001600240032004000\nNumber of instances\nZapata14 - Our confidence measure\nZapata14 - Original confidence measureFigure 4 . Comparison of our proposed conﬁdence mea-\nsure with the conﬁdence measure proposed in [22] for\nFSL4 dataset and Zapata14 tempo estimation algorithm.\nserve that Zapata’s conﬁdence measure allows to achieve\naccuracies which are around 15% higher than when us-\ning our conﬁdence. However, the number of remaining\ninstances in the dataset is drastically reduced, and accu-\nracy values for γ >0.75become inconsistent. Looking at\nthe¯Sscore, we ﬁnd that Zapata14 in combination with our\nconﬁdence measure gets better results than when using the\noriginal measure, with an average ¯Sincrease of 17%, 29%\nand 31% (for the three accuracy measures respectively).\nThis indicates that our conﬁdence measure is able to better\nmaximise accuracy and number of remaining instances.\n6. CONCLUSION\nIn this paper paper we have compared several tempo esti-\nmation algorithms using four datasets of music loops. We\nalso described a simple conﬁdence measure for tempo es-\ntimation algorithms and proposed a methodology for eval-\nuating the relation between estimation accuracy and con-\nﬁdence measure. This methodology can also be applied\nto other MIR tasks, and we believe it encourages future\nresearch to put more emphasis on conﬁdence measures.\nWe found that by setting a high enough minimum conﬁ-\ndence threshold, we can obtain reasonably high tempo es-\ntimation accuracies while preserving half of the instances\nin a dataset. However, these results are still far from be-\ning optimal: if we only consider exact BPM estimations\n(Accuracy 1e), the maximum accuracies we obtain are still\ngenerally lower than 70%. We foresee two complementary\nways of improving these results by i)adapting tempo esti-\nmation algorithms to the case of music loops (e.g. taking\nbetter advantage of tempo steadiness and expected signal\nduration), and ii)developing more advanced conﬁdence\nmeasures that take into account other properties of loops\nsuch as the beat strength or the rate of onsets. Overall, the\nwork we present here contributes to the improvement of\nthe reusability of unstructured music loop repositories.\n7. ACKNOWLEDGMENTS\nThis work has received funding from the European Union’s\nHorizon 2020 research and innovation programme under\ngrant agreement No 688382.274 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20168. REFERENCES\n[1] Sebastian B ¨ock, Florian Krebs, and Gerhard Widmer.\nAccurate Tempo Estimation based on Recurrent Neu-\nral Networks and Resonating Comb Filters. In Proc. of\nthe Int. Conf. on Music Information Retrieval (ISMIR) ,\n2015.\n[2] Dmitry Bogdanov, Nicolas Wack, Emilia G ´omez,\nSankalp Gulati, Perfecto Herrera, Oscar Mayor, Gerard\nRoma, Justin Salamon, Jose Zapata, and Xavier Serra.\nESSENTIA: An audio analysis library for music infor-\nmation retrieval. In Proc. of the Int. Conf. on Music\nInformation Retrieval (ISMIR) , pages 493–498, 2013.\n[3] Matthew EP Davies and Mark D Plumbley. Context-\ndependent beat tracking of musical audio. IEEE Trans-\nactions on Audio, Speech and Language Processing ,\n15(3):1009–1020, 2007.\n[4] Norberto Degara, Enrique Argones Rua, Antonio Pena,\nSoledad Torres-Guijarro, Matthew EP Davies, and\nMark D Plumbley. Reliability-Informed Beat Track-\ning of Musical Signals. IEEE Transactions on Au-\ndio, Speech, and Language Processing , 20(1):290–\n301, 2012.\n[5] Simon Dixon. Automatic extraction of tempo and beat\nfrom expressive performances. Journal of New Music\nResearch , 30:39–58, 2001.\n[6] Anders Elowsson, Anders Friberg, Guy Madison, and\nJohan Paulin. Modelling the Speed of Music using Fea-\ntures from Harmonic/Percussive Separated Audio. In\nProc. of the Int. Conf. on Music Information Retrieval\n(ISMIR) , pages 481–486, 2013.\n[7] Frederic Font, Gerard Roma, and Xavier Serra.\nFreesound technical demo. In Proc. of the ACM Int.\nConf. on Multimedia (ACM MM) , pages 411–412,\n2013.\n[8] Mikel Gainza and Eugene Coyle. Tempo detection us-\ning a hybrid multiband approach. IEEE Transactions\non Audio, Speech and Language Processing , 19(1):57–\n68, 2011.\n[9] Aggelos Gkiokas, Vassilis Katsouros, George\nCarayannis, and Themos Stafylakis. Music Tempo\nEstimation and Beat Tracking By Applying Source\nSeparation and Metrical Relations. In Proc. of the Int.\nConf. on Acoustics, Speech and Signal Processing\n(ICASSP) , volume 7, pages 421–424, 2012.\n[10] Fabien Gouyon, Anssi Klapuri, Simon Dixon, Miguel\nAlonso, George Tzanetakis, Christian Uhle, and Pedro\nCano. An Experimental Comparison of Audio Tempo\nInduction Algorithms. IEEE Transactions on Audio,\nSpeech and Language Processing , 14(5):1832–1844,\n2006.[11] Peter Grosche and Meinard M ¨uller. Extracting Pre-\ndominant Local Pulse Information from Music Record-\nings. IEEE Transactions on Audio, Speech and Lan-\nguage Processing , 19(6):1688–1701, 2011.\n[12] Anssi Klapuri, Antti Eronen, and Jaakko Astola. Anal-\nysis of the meter of musical signals. IEEE Transac-\ntions on Audio, Speech, and Language Processing ,\n14(1):342–355, 2006.\n[13] Meinard M ¨uller. Fundamentals of Music Processing .\nSpringer, 2015.\n[14] Jo ˜ao L Oliveira, Fabien Gouyon, Luis Gustavo Mar-\ntins, and Luis Paolo Reis. IBT: A real-time tempo and\nbeat tracking system. In Proc. of the Int. Conf. on\nMusic Information Retrieval (ISMIR) , pages 291–296,\n2010.\n[15] Graham Percival and George Tzanetakis. Stream-\nlined tempo estimation based on autocorrelation and\ncross-correlation with pulses. IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing ,\n22(12):1765–1776, 2014.\n[16] Elio Quinton, Mark Sandler, and Simon Dixon. Esti-\nmation of the Reliability of Multiple Rhythm Features\nExtraction from a Single Descriptor. In Proc. of the\nInt. Conf. on Acoustics, Speech and Signal Processing\n(ICASSP) , pages 256–260, 2016.\n[17] Gerard Roma. Algorithms and representations for sup-\nporting online music creation with large-scale au-\ndio databases . PhD thesis, Universitat Pompeu Fabra,\n2015.\n[18] George Tzanetakis, Georg Essl, and Perry Cook. Hu-\nman Perception and Computer Extraction of Musical\nBeat Strength. In Proc. of the Int. Conf. on Digital Au-\ndio Effects (DAFx) , pages 257–261, 2002.\n[19] Fu-Hai Frank Wu and Jyh-Shing Roger Jang. A Super-\nvised Learning Method for Tempo Estimation of Mu-\nsical Audio. In Proc. of the Mediterranean Conf. on\nControl and Automation (MED) , pages 599–604, 2014.\n[20] Jose R Zapata, Matthew EP Davies, and Emilia\nG´omez. Multi-Feature Beat Tracking. IEEE/ACM\nTransactions on Audio, Speech, and Language Pro-\ncessing , 22(4):816–825, 2014.\n[21] Jose R Zapata and Emilia G ´omez. Comparative Eval-\nuation and Combination of Audio Tempo Estimation\nApproaches. In Proc. of the AES Int. Conf. on Seman-\ntic Audio , pages 198 – 207, 2011.\n[22] Jose R Zapata, Andr ´e Holzapfel, Matthew EP Davies,\nJo˜ao L Oliveira, and Fabien Gouyon. Assigning a Con-\nﬁdence Threshold on Automatic Beat Annotation in\nLarge Datasets. In Proc. of the Int. Conf. on Music In-\nformation Retrieval (ISMIR) , pages 157–162, 2012.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 275"
    },
    {
        "title": "Querying XML Score Databases: XQuery is not Enough!.",
        "author": [
            "Raphaël Fournier-S&apos;niehotta",
            "Philippe Rigaux",
            "Nicolas Travers"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416976",
        "url": "https://doi.org/10.5281/zenodo.1416976",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/136_Paper.pdf",
        "abstract": "The paper addresses issues related to the design of query languages for searching and restructuring collections of XML-encoded music scores. We advocate against a di- rect approach based on XQuery, and propose a more pow- erful strategy that first extracts a structured representation of music notation from score encodings, and then manipu- lates this representation in closed form with dedicated op- erators. The paper exposes the content model, the result- ing language, and describes our implementation on top of a large Digital Score Library (DSL).",
        "zenodo_id": 1416976,
        "dblp_key": "conf/ismir/Fournier-Sniehotta16",
        "content": "QUERYING XML SCORE DATABASES: XQUERY IS NOT ENOUGH!\nRapha ¨el Fournier-S’niehotta\nCNAM\nfournier@cnam.frPhilippe Rigaux\nCNAM\nphilippe.rigaux@cnam.frNicolas Travers\nCNAM\nnicolas.travers@cnam.fr\nABSTRACT\nThe paper addresses issues related to the design of query\nlanguages for searching and restructuring collections of\nXML-encoded music scores. We advocate against a di-\nrect approach based on XQuery, and propose a more pow-\nerful strategy that ﬁrst extracts a structured representation\nof music notation from score encodings, and then manipu-\nlates this representation in closed form with dedicated op-\nerators. The paper exposes the content model, the result-\ning language, and describes our implementation on top of\na large Digital Score Library (DSL).\n1. INTRODUCTION\nIt is now common to serialize scores as XML documents,\nusing encodings such as MusicXML [11, 16] or MEI [15,\n18]. Ongoing work held by the recently launched W3C\nMusic Notation Community Group [20] conﬁrms that we\ncan expect in a near future the emergence of large collec-\ntions of digital scores.\n1.1 Issues with Querying XML score databases\nA natural question in a collection management perspec-\ntive is the deﬁnition of a query and manipulation language\nto access, search, and possibly analyze these collections.\nWhile XQuery appears as a language of choice, we con-\nsider that it does not constitute, as such, a suitable solution.\nThere are several reasons that prevent XQuery from being\nable to address the complex requirements of music nota-\ntion manipulation, at least beyond the simplest operations.\n1.Issue 1: Heterogeneity. Score encodings closely\nmix information related to the content (e.g., the se-\nquence of notes of a voice) and to a speciﬁc render-\ningof this content (e.g., voice allocation to a staff,\npositioning of notes/lines/pages, and other options\npertaining to scores visualization). While it is not\nalways obvious to clearly distinguish content from\nrendering instructions, mixing both concerns leads\nto an intricate encoding from which selecting the rel-\nevant information becomes extremely difﬁcult.\nc/circlecopyrtRapha ¨el Fournier-S’niehotta, Philippe Rigaux, Nico-\nlas Travers. Licensed under a Creative Commons Attribution 4.0 In-\nternational License (CC BY 4.0). Attribution: Rapha ¨el Fournier-\nS’niehotta, Philippe Rigaux, Nicolas Travers. “QUERYING XML\nSCORE DATABASES: XQUERY IS NOT ENOUGH!”, 17th Interna-\ntional Society for Music Information Retrieval Conference, 2016.Extracting for instance melodic information from ei-\nther MusicXML or MEI turns out to be difﬁcult;\nand more sophisticated extractions (e.g., alignment\nof melodic information for voices) are almost im-\npossible.\n2.Issue 2: Operations and closure. One of the main\nrequirements of a query language is a set of well-\ndeﬁned operations that operate in closed form: given\nas input objects that comply to some structural con-\nstraints (a data model), the language should guaran-\ntee that any output obtained by combining the oper-\nators is model-compliant as well. This requirement\nallows an arbitrary composition of operations, and\nensures the safety of query results. In our context,\nit concretely means that we need operators that ma-\nnipulate music notation, and that their output should\nconsist of valid music notation as well. There is\nhowever no means to achieve that with XQuery, nor\neven to simply know whether a query supplies a com-\npliant output or not.\n3.Issue 3: Formats. Finally, a last, although less es-\nsential, obstacle is the number of possible encod-\nings available nowadays, from legacy formats such\nas HumDrum to recent XML proposal mentioned\nabove. Abstracting away from the speciﬁcs of these\nformats in favor of the core information set that they\ncommonly aim at representing would allow to get rid\nof their idiosyncrasies.\nTo summarize, we consider that XML representation of\nscores are so far mostly intended as a serialization of docu-\nments that encapsulate all kinds of information, from meta-\ndata (creation date, encoding agent) to music information\n(symbolic representation of sounds and sounds synchro-\nnization) via rendering instructions. They are by no means\ndesigned to supply a structured representation of a core as-\npect (music “content”), subject to investigation and manip-\nulation via a dedicated, model-aware query language.\n1.2 Our approach\nWe make the case for an approach that leverages music\ncontent information as virtual XML objects. Coupled with\na specialization of XQuery to this speciﬁc representation,\nwe obtain a system apt at providing search, restructuration,\nextraction, analytic services on top of large collections of\nXML-encoded scores. The system architecture is summa-\nrized in Figure 1, and addresses the above issues.723Issue 1: Bringing homogeneity . The bottom layer is a\nDigital Score Library managing collections of scores se-\nrialized in MusicXML, MEI, or any other legacy format\n(e.g., Humdrum). This encoding is mapped toward a model\nlayer where the content structured in XML corresponds to\nthe model structures. This deﬁnes, in intention, collections\nof music notation objects that we will call vScore in the fol-\nlowing. A vScore abstracts the part of the encoding (here\nthe content) we wish to focus on, and gets rid of informa-\ntion considered as useless, at least in this context.\nIssue 2: Deﬁning a domain-speciﬁc language . In order\nto manipulate these vScores, we equip XQuery with two\nclasses of operations dedicated to music content: struc-\ntural operators andfunctional operators . The former im-\nplement the idea that structured scores management cor-\nresponds, at the core level, to a limited set of fundamen-\ntal operations, grouped in a score algebra, that can be de-\nﬁned and implemented only once. The latter acknowledges\nthat the richness of music notation manipulations calls for\na combination of these operations with user-deﬁned func-\ntions at early steps of the query evaluation process. Model-\ning the invariant operators and combining them with user-\ndeﬁned operations constitutes the operational part of the\nmodel. This yields a query language whose expressions\nunambiguously deﬁne the set of transformations that pro-\nduce new vScores from the base collections.\nEncodingsMappingData model(virtual collection)\nMEI\nMusicXML\n…\nStructural opsqueries\nFunctionalops+\nVisualisation / analysis\nothers\nFigure 1 . Envisioned system\nIssue 3: Serialization independence . One mapper has to\nbe deﬁned for each possible encoding, as shown by the ﬁg-\nure which assumes that MusicXML and MEI documents\ncohabit in a single DSL. Adding a new source represented\nwith a new encoding is just a matter of adding a new map-\nper. Each document in the DSL is then mapped to a (vir-\ntual) XML document, instance of the model.\n1.3 Contributions\nIn the present paper, we describe the implementation of\nthe above ideas in N EUMA [17]. The focus is on the model\nlayer, deﬁned as a virtual XML schema, on the mapping\nfrom raw documents to vScores, and on the integration of\nXQuery with structural operators and external functions.\nThe score algebra, not presented here, is implemented inour system as XQuery functions, whose meaning should\nbe clear from the context.\nSection 2 gives the virtual XML schema for music con-\ntent notation, and Section 3 shows how to create an XML\ndatabase referring to vScores. Section 4 presents the query\nlanguage and Section 5 discusses salient implementation\nchoices. Section 6 covers related work and Section 7 con-\ncludes the paper.\n2. MUSIC NOTATION: THE SCHEMA\nWe now describe the virtual data model with XML Schema\n[22]. The model aims at representing polyphonic scores in\nCommon Music Notation (CMN). A score is composed of\nvoices, and a voice is a sequence of events.\n2.1 Event type\nAn event is a value (complex or simple) observed during a\ntime interval. Events are polymorphic: the value may be\na note representation, a chord representation, a syllable or\nany other value (e.g., an integer representing an interval).\nThe abstract deﬁnition of an event is a complex type\nwith a duration attribute.\n<xs:complexType abstract=\"true\" name=\"eventType\" >\n<xs:attribute type=\"xs:integer\" name=\"duration\"\nuse=\"required\" />\n</xs:complexType>\nFrom this abstract type, we can derive concrete event\ntypes with speciﬁc element names. The most important\nare events denoting sounds, which covers simple n≥0\nsimultaneous sounds, either rests ( n= 0), notes ( n= 1)\nor chords ( n > 1). The soundType is derived from the\neventType as follows:\n<xs:complexType name=\"soundType\" >\n<xs:complexContent>\n<xs:extension base=\"eventType\" >\n<xs:choice minOccurs=\"1\" maxOccurs=\"1\" >\n<xs:element name=\"note\" type=\"noteType\" />\n<xs:element name=\"rest\" type=\"restType\" />\n<xs:element name=\"chord\" type=\"chordType\" />\n</xs:choice>\n</xs:extension>\n</xs:complexContent>\n</xs:complexType>\nDue to space restrictions, we do not detail noteType\n(containing ’pitch’ and ’octave’ attributes), restType (em-\npty element) and chordType (list of noteType ). As\nanother example of a concrete event type, lyrics can be rep-\nresented with syllabic events (and rests), with type:\n<xs:complexType name=\"syllableType\" >\n<xs:complexContent>\n<xs:extension base=\"eventType\" >\n<xs:choice minOccurs=\"1\" maxOccurs=\"1\" >\n<xs:element name=\"syll\" type=\"xs:string\" />\n<xs:element name=\"rest\" type=\"restType\" />\n</xs:choice>\n</xs:extension>\n</xs:complexContent>\n</xs:complexType>\nEvents are not restricted to musical domains. Events\nin the xs:integer domain, for instance, can be used to\nrepresent intervals, obtained by a 2-voices scores analysis.724 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016<monody>\n<rest duration=\"24\" />\n<note duration=\"16\" p=\"D\" o=\"5\" />\n<rest duration=\"4\" />\n<note duration=\"2\" p=\"E\" o=\"5\" />\n<note duration=\"2\" p=\"F\" o=\"5\" />...\n</monody><lyrics>\n<rest duration=\"24\" />\n<syll duration=\"16\" />Ah,</syll>\n<rest duration=\"4\" />\n<syll duration=\"2\" />que</syll>\n<syll duration=\"2\" />je</syll> ...\n</lyrics><bass>\n<note duration=\"8\" p=\"D\" o=\"4\" />\n<note duration=\"4\" p=\"C\" o=\"4\" />\n<chord duration=\"4\" >\n<note p=\"D\" o=\"4\" a=\"-1\" />\n<note p=\"B\" o=\"3\" a=\"-1\" /></chord>\n<note duration=\"4\" p=\"A\" o=\"3\" />\n<note duration=\"4\" p=\"G\" o=\"3\" />...\n</bass>\nFigure 2 . V oices representation\n2.2 Voice type\nA voice is a sequence of events. Its abstract deﬁnition is\ngiven by the following schema:\n<xs:complexType name=\"voiceType\" abstract=\"true\" >\n<xs:sequence minOccurs=\"0\" maxOccurs=\"unbounded\" >\n<xs:element type=\"eventType\" />\n</xs:sequence>\n</xs:complexType>\nThis type actually represents a function from the time\ndomain to the domain of events. There is an implicit non-\noverlapping constraint: an event begins when its predeces-\nsor ends. We can instantiate concrete voice types by simply\nreplacing the abstract eventType by one of its derived\ntypes (e.g., soundType ,syllableType ,intType ),\nlike in the following example:\n<xs:complexType name=\"lyricsType\" >\n<xs:extension base=\"voiceType\" >\n<xs:sequence minOccurs=\"0\" maxOccurs=\"unbounded\" >\n<xs:element type=\"syllableType\" />\n</xs:sequence>\n</xs:extension>\n</xs:complexType>\n2.3 Score type\nFinally, our virtual notation schema contains a score type\nwhich describes a recursive structure deﬁned as follows:\n•ifva voice, then vis a score.\n•ifs1,···, snare scores, the sequence < s 1,···, sn>\nis a score.\nThe generic deﬁnition of the XML schema for this struc-\nture is given below. Note that element names for scores and\nvoices will be speciﬁed for each speciﬁc corpus.\n<xs:complexType name=\"scoreType\" abstract=\"true\" >\n<xs:sequence minOccurs=\"1\" maxOccurs=\"unbounded\" >\n<xs:choice>\n<xs:element type=\"scoreType\" />\n<xs:element type=\"voiceType\" />\n</xs:choice>\n</xs:sequence>\n</xs:complexType>\n2.4 Example\nLet’s illustrate by an example our types and structure. Fig. 3\nis partially represented by the vScore in Fig. 2.\n/accidentals.flat/noteheads.s1/noteheads.s0\nAh,\n/noteheads.s1/accidentals.sharp/noteheads.s2\n/noteheads.s2/dots.dot\n/noteheads.s2/noteheads.s2\n/flags.d3/noteheads.s1\nde !\n/noteheads.s1/noteheads.s2\nque/flags.d3\n/noteheads.s1/noteheads.s1sens/noteheads.s1/flags.d3\nje/noteheads.s2\n/noteheads.s1/accidentals.flat/noteheads.s1/rests.2\n/noteheads.s1 /accidentals.sharp/dots.dot/noteheads.s2/accidentals.sharp/noteheads.s1\n/clefs.F/noteheads.s023/clefs.G/rests.0\n/dots.dot/brackettips.up\n/brackettips.down/dots.dot\n23/accidentals.flattu/noteheads.s2\n/noteheads.s1é/noteheads.s2/rests.2\n/noteheads.s1/noteheads.s2\nd'in qui/noteheads.s1\n/noteheads.s1/noteheads.s1\nFigure 3 . A score exampleOur model decomposes the score in three voices. The\nﬁrst one represents the monody of the vocal part. It con-\nsists of a sequence of soundType events. The second\nvoice represents lyrics with syllableType events. And,\nﬁnally, the last voice is the bass. Its representation is a se-\nquence of soundType events (here, notes and chords).\nThe vScore itself illustrates the recursive structure that\nencapsulates the voices in a tree of score elements. The\nupper level combines the bass voice and an embedded\nscore which combines the monody andlyrics voices.\nThe structure is as follows.\n<air>\n<vocal>\n<monody> (...) </monody>\n<lyrics> (...) </lyrics>\n</vocal>\n<bass> (...) </bass>\n</air>\nIt should be clear that this representation abstracts a part\nof the content that can be found in all the encodings we are\naware of. The choice of the information subset which is se-\nlected is here minimal, for the sake of conciseness. We can\nobviously extend the representation with additional details\nas long as it does not affect the structure. A voice can be\n“decorated” by an instrument name, an event by the current\nmetric or the measure number, a score by its composer, all\nrepresented as additional elements. In general, the issue\nrelates to what is considered as “content” subject to search\nand analysis operations, and what is the suitable represen-\ntation for this content. We will stick in the following to the\nsimple model given above which is sufﬁcient to our needs.\nRecall that the schema intends to deﬁne a virtual score\nrepresentation which is derived at search time (according\nto rules explained in the next sections) from the actual\nserialization. We brieﬂy explain the mapping from Mu-\nsicXML or MEI documents to vScores.\n2.5 Mapping from MusicXML or MEI\nIn MusicXML, scores are organized as a tree of score ,\npart-group , andpart elements. V oices are numbered\nwith respect to the part they belong to, and represented as\nnested elements of notes, rest and chords. Our mapping\nuniﬁes the score, groups and parts in a recursive nesting of\nscore elements and (virtually) splits the music and lyrics\nas two associated voices.\nIn MEI, the score structure is based on score, staves and\ngroups of staves. V oices are represented as layer objects\ndeeply nested in a hierarchy of measure andstaff con-\ntainers. Our mapping extracts the voice events from the\ncomplex imbrication of MEI elements or organizes them\naccording to our recursive score structure.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 7253. MUSIC NOTATION: THE DATABASE\nXML databases are collections of documents. Although\nthe deﬁnition of a schema for a collection is not manda-\ntory, it is a safe practice to ensure that all the documents\nit contains share a similar structure. In our context, a col-\nlection of digital scores is a regular XML collection where\none or several elements are of scoreType type. Here is\na possible example:\n<xs:complexType name=\"opusType\" >\n<xs:sequence>\n<xs:element name=\"title\" type=\"xs:string\" />\n<xs:element name=\"composer\" type=\"xs:string\" />\n<xs:element name=\"published\" type=\"xs:string\" />\n<xs:element type=\"scoreType\" />\n</xs:sequence>\n<xs:attribute type=\"xs:ID\" name=\"id\" />\n</xs:complexType>\nNote that this schema is strict regarding meta-data (title,\ncomposer) but very ﬂexible for the music notation because\nwe are using here the generic scoreType . It follows that,\nfrom one document to the other in standard notations (mu-\nsicXML, MEI, HumDrum), the score structure, the number\nof voices and their identiﬁers may vary. Such a ﬂexibility is\ndeﬁnitely not convenient when it comes to querying a col-\nlection, since we cannot safely refer to the components of\na score. It seems more appropriate to base the organization\nof score collections on an homogeneous score structure. A\nQuartet collection for instance would only accept scores\ncomplying to the following structure:\nQuartet(id: int, title: string,\ncomposer: string, published: date,\nmusic: Score [v1, v2, alto, cello])\nThe XML Schema formalism accepts the deﬁnition of\nrestriction of a base type. In our case, the restriction con-\nsists in specializing the scoreType deﬁnition to list the\nnumber and names of voices, like in quartetType :\n<xs:complexType name=\"quartetType\" >\n<xs:complexContent>\n<xs:restriction base=\"scoreType\" >\n<xs:sequence>\n<xs:element name=\"v1\" type=\"soundVoiceType\" />\n<xs:element name=\"v2\" type=\"soundVoiceType\" />\n<xs:element name=\"alto\" type=\"soundVoiceType\" />\n<xs:element name=\"cello\" type=\"soundVoiceType\" />\n</xs:sequence>\n</xs:restriction>\n<xs:attribute type=\"xs:ID\" name=\"id\" />\n</xs:complexContent>\n</xs:complexType>\nUsing QuartetType in place of ScoreType in the\ncollection schema ensures that all the vScores in the col-\nlection match the deﬁnition. V oices’ names are speciﬁed\nand can then be used to access to the (virtual) music nota-\ntion to start applying operations and transformations. This\ncan be done with XQuery, as explained in the next section.\n4. XQUERY + SCORE ALGEBRA = QUERIES\nWith a well-deﬁned collection and a clear XML model to\nrepresent the notation of music content, we can now ex-\npress queries over this collection with XQuery. However,\nexecuting such queries gives rise to the following issues:Issue 1: Wecannot directly evaluate an XQuery expres-\nsion, since they are interpreted over instances which are\npartly virtual (scores, voices and events) and partly mate-\nrialized (all the rest: title, composer, etc.).\nIssue 2: Pure XQuery expression would remain limited to\nexploit the richness of music notation;\nThe ﬁrst issue is solved by executing, at run-time, the\nmapping that transforms the serialized score (say, in Mu-\nsicXML) to a vScore, instance of our model. This is fur-\nther explained in the next section, devoted to implementa-\ntion choices. To solve the second issue, we implemented a\nset of XQuery functions forming a score algebra. We in-\ntroduce it and illustrate the resulting querying mechanism\nwith examples.\n4.1 Designing a score algebra\nWe designed a score algebra in a database perspective, as\na set of operators that operate in closed form: each takes\none or two vScores (instances of scoreType ) as input\nand produces a vScore as output. This brings composi-\ntion, expressiveness, and safety of query results, since they\nare guaranteed to consist of vScore instances that can, if\nneeded, be serialized back in some standard encoding (see\nthe discussion in Section 1 and the system architecture,\nFig. 1). The algebra is formalized in [8], and implemented\nas a set of query functions whose meaning should be clear\nfrom the examples given next.\n4.2 Queries\nThe examples rely on the Quartet corpus (refer to the Sec-\ntion 3 for its schema). Our ﬁrst example creates a list of\nHaydn ’s quartets, reduced to the titles and violin’s parts.\nfor $sincollection(\"Quartet\")\nwhere $s/composer =\"Haydn\"\nreturn $s/title , Score($s/ music /v1, $s/ music /v2)\nRecall that music is aQuartetType element in the\nQuartet schema. This ﬁrst query shows two basic opera-\ntors to manipulate scores: projection on voices (obtained\nwith XPath), and creation of new scores from components\n(voices or scores) with the Score() operator.\nA third operator illustrated next is M AP. It represents\na higher-order function that applies a given function fto\neach event in a vScore, and returns the score built from f’s\nresults. Here is an example: we want the quartets where the\nv1 part is played by a B-ﬂat clarinet. We need to transpose\nthev1part 2 semi-tones up.\nfor $sincollection(\"Quartet\")\nwhere $s/composer =\"Haydn\"\nlet $clarinet := Map ($s/ music /v1, transpose (2))\nlet $clrange := ambitus ($clarinet)\nreturn $s/title , $clrange,\nScore($clarinet, $s/ music /v2,\n$s/music /alto , $s/ music /cello )\nThis second query shows how to deﬁne variables that\nhold new content derived from the vScore via user deﬁned\nfunctions (UDFs). For the sake of illustration we create\ntwo variables, $clarinet and$clrange , calling re-\nspectively transpose() andambitus() .726 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016In the ﬁrst case, the function has to be applied to each\nevent of the violin voice. This is expressed with M AP\nwhich yields a new voice with the transposed events. By\ncontrast, ambitus() is directly applied to the voice as a\nwhole. It produces a scalar value.\nMAPis the primary means by which new vScores can\nbe created by applying all kinds of transformations. M AP\nis also the operator that opens the query language to the\nintegration of external functions: any library can be inte-\ngrated as a ﬁrst-class component of the querying system,\nproviding some technical work to “wrap” it conveniently.\nThe selection operator takes as input a vScore, a Boolean\nexpression e, and ﬁlters out the events that do not satisfy\ne, replacing them by a null event. Note that this is dif-\nferent from selecting a score based on some property of\nits voice(s). The next query illustrates both functionali-\nties: a user-deﬁned function “lyricsContains” selects all\nthe psalms (in a Psalters collection) such that the vocal part\ncontains a given word (“ Heureux ”), “nullify” the events\nthat do not belong to the measures ﬁve to ten, and trim the\nvoice to keep only non-null events.\nfor $sincollection(\"Psalters\")\nlet $sliced := trim(select ($s/ air/vocal /monody ,\nmeasure(5, 10)))\nwhere lyricsContains ($s/ air/vocal /lyrics , \"Heureux\")\nreturn $s/title , Score($sliced)\nWe can take several vScores as input and produce a doc-\nument with several vScores as output. The following ex-\nample takes three chorals, and produces a document with\ntwo vScores associating respectively the alto and tenor voices.\nfor $c1 incollection(\"Chorals\")[@id =\"BWV49\"]/ music ,\n$c2 incollection(\"Chorals\")[@id =\"BWV56\"]/ music ,\n$c3 incollection(\"Chorals\")[@id =\"BWV12\"]/ music\nreturn <title> Excerpts of chorals 49, 56, 12 </title> ,\nScore($c1/ alto , $c2/ alto , $c3/ alto ),\nScore($c1/ tenor , $c2/ tenor , $c3/ tenor )\nFinally, our last example shows the extended concept of\nscore as a voice synchronization which are not necessarily\n“music” voices. The following query produces, for each\nquartet, a vScore containing the violin 1 and cello voices,\nand a third one measuring the interval between the two.\nfor $sincollection(\"Quartet\")/ music\nlet $intervals := Map(Score($s/ v1,$s/ cello ),interval())\nreturn Score ($s/ v1, $s/ cello , $intervals)\nSuch a “score” cannot be represented with a traditional\nrendering. Additional work on visualization tools that would\nclosely put in perspective music fragments along with some\ncomputed analytic feature is required.\n5. IMPLEMENTATION\nOur system integrates an implementation of our score alge-\nbra, a mapping that transforms serialized scores to vScores,\nand off-the-shelf tools (a native XML database, B ASEX1,\na music notation library for UDFs, MUSIC 212[4]). This\nsimple implementation yields a query system which is both\npowerful and extensible (only add new functions wrapped\nin XQuery/B ASEX). We present its salient aspects.\n1http://basex.org\n2http://web.mit.edu/music215.1 Query processing\nThe architecture presented in Figure 4 summarizes the com-\nponents involved in query processing. Data is stored in\nBASEX in two collections: the semi-virtual collection (e.g.,\nQuartet) of music documents (called opus ), and the col-\nlection of serialized scores, in MusicXML or MEI. Each\nvirtual element scoreType in the former is linked to an\nactual document in the latter.\nMEI / MusicXML\nXQuery\nXML/vScores\nXQuery functionsMappers,Operators Music21…1\n324\nCollection\nlinkvirtual instancesmapping\nquery results\n……\nconcrete instances……\nalgebraSerialized scores\nFigure 4 . Architecture\nThe evaluation of a query proceeds as follows. First\n(step 1), B ASEX scans the virtual collection and retrieves\nthe opus matching the where clause (at least for ﬁelds\nthat do not belong to the virtual part, see the discussion at\nthe end of the section). Then (step 2), for each opus, the\nembedded virtual element scoreType has to be materi-\nalized. This is done by applying the mapping that extracts\na vScore instance from the serialized score, thanks to the\nlink in each opus.\nOnce a vScore is instantiated, algebraic expressions, rep-\nresented as composition of functions in the XQuery syn-\ntax, can be evaluated (step 3). We wrapped several Python\nand Java libraries as XQuery functions, as permitted by\nthe B ASEX extensible architecture. In particular, algebraic\noperators and mappers are implemented in Java, whereas\nadditional, music-content manipulations are mostly wrap-\nped from the Python Music21 toolbox.\nThe XQuery processor takes in charge the application of\nfunctions, and builds a collection of results (that includes\ninstances of scoreType ), ﬁnally sent to the client appli-\ncation (step 4). It is worth noting that the whole mecha-\nnism behaves like an ActiveXML [1] document which ac-\ntivates the XML content on demand by calling an external\nservice (here, a function).\n5.2 Mappers\nIn order to map scores from the physical representation to\nthe virtual one, references to physical musical parts are\nmatched, according to the collection schema. To achieve\nthis, an ID is associated to each voice element of the ma-\nterialized score. This ID directly identiﬁes the underlying\npart of the physical document.\nThe main mapping challenge is to identify virtual and\nserialized voices, in particular when they are not standard-\nized according to the collection schema. We need to gen-Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 727erate IDs for each parts on the underlying encoding (Mu-\nsicXML, MEI, etc.), even for monody and lyrics parts which\ncan be merged on the physical document. Most of them\ncan be done automatically with metadata information given\nby the collection schema.\n5.3 Manipulating vScores with S CORE LIB\nThe S CORE LIBJava library is embedded in every B ASEX\nquery in order to process our algebraic operations on vS-\ncores. The library is responsible for managing the link be-\ntween the virtual and the physical score, whatever the en-\ncoding format. Whenever a function activates a vScore by\ncalling a function (whether a structural function of the al-\ngebra, or a user-deﬁned function), the link is used to get\nand materialize the corresponding score.\nOnce vScore has been materialized, it is kept in a cache\nin order to avoid repeated and useless applications of the\nmapping. Temporary vScores produced by applying alge-\nbraic operators are kept in the cache as well.\nTheScore() operator creates the ﬁnal XML docu-\nment featuring one or several instances of scoreType . It\ncombines scores produced by operators and referred to by\nXQuery variables.\n5.4 Indexing music features\nUser Deﬁned Functions ( UDFs ) are necessary to produce\nderived information from music notation, and have to be\nintegrated as external components. Getting the highest note\nof a voice for instance is difﬁcult to express in XQuery,\neven on the regular structures of our model. In general,\ngetting sophisticated features would require awfully com-\nplex expressions. In our current implementation, UDFs are\ntaken from MUSIC 21 and wrapped as XQuery functions.\nThis works with quite limited implementation efforts, but\ncan be very inefﬁcient since every score must be materi-\nalized before evaluating the UDF. Consider the following\nexample which retrieves all the quartets such that the ﬁrst\nviolin part gets higher than e6:\nfor $sincollection(\"Quartet\")\nwhere highest($s/ music /v1) > ’e6’ return $s\nA naive, direct evaluation maps the MusicXML (or MEI)\ndocument as a vScore, passes it to the XQuery function\nthat delegates the computation to the user library (e.g., MU-\nSIC21 or any other) and gets the result. This has to be done\nfor each score in the collection, even though they do not all\nmatch the selection criteria.\nA solution is to materialize the results of User Deﬁned\nFunctions as metadata in the virtual document and to index\nthis new information in B ASEX. This can directly serve as\na search criteria without having to materialize the vScore.\nThe result of the highest() function is such a feature. Index\ncreation simply scans the whole physical collections, runs\nthe functions and records it result in a dedicated index\nsub-element of each opus, automatically indexed in B A-\nSEX. To evaluate the query above, it uses the access path\nto directly get the relevant opus.\nfor $sincollection(\"Quartet\")[ index /v1/highest > ’e6’]\nreturn $s6. RELATED WORK\nAccessing to structured music notation for search, analy-\nsis and extraction is a long-term endeavor. Humdrum [13]\nworks on plain text (ASCII) ﬁle format, whereas MUSIC 21\n[4] deals with MIDI channels modeled as musical layers.\nBoth can import widely used formats like MusicXML or\nMEI. Both are powerful toolkits, but their main focus is\non the development of scripts and not database-like access\nto structured content. As a result, using, say MUSIC 21 to\nexpress the equivalent of our queries would require to de-\nvelop ad-hoc scripts possibly rather complex. It becomes\nall the more complicated when dealing with huge collec-\ntions of scores. On the other hand, there are many com-\nputations that a database language cannot express, which\nmotivated our introduction of UDFs in the language.\nOther musical score formalisms rather target generative\nprocess and computer-aided composition. This is the case\nof Euterpea [12] (in Haskell ), musical programming ap-\nproaches [3, 6, 7, 14] and operations on tiled streams in T-\nCalculus [14]. They follow the paradigm of abstract data\ntypes for music representation, bringing a simpliﬁcation to\nthe music programming task, but they are not adapted to\nthe conciseness of a declarative query language.\nSince modern score formats adopt an XML-based se-\nrialization, XQuery [23] has been considered as the lan-\nguage of choice for score manipulation [9]. THoTH [21]\nalso proposes to query MusicXML with patterns analy-\nsis. For reasons developed in the introduction, we believe\nthat a pure XQuery approach is too generic to handle the\nspeciﬁcs of music representation.\nOur work is inspired by XQuery mediation [10, 5, 2,\n19], and can be seen as an application of method that com-\nbines queries on physical and virtual instances. It borrows\nideas from ActiveXML [1], and in particular the deﬁnition\nof some elements as “triggers” that activate external calls.\n7. CONCLUSION\nWe propose in the present paper a complete methodology\nto view a repository of XML-structured music scores as\na structured database, equipped with a domain-specialized\nquery language. Our approach aims at limiting the amount\nof work needed to implement a working system. We model\nmusic notation as structured scores that can easily be ex-\ntracted from existing standards at run-time; we associate to\nthe model an algebra to access to the internal components\nof the scores; we allow the application of external func-\ntions; and ﬁnally we integrate the whole design in XQuery,\nwith limited implementation requirements.\nWe believe that this work brings a simple and promising\nframework to deﬁne a query interface on top of Digital Li-\nbraries, with all the advantages of a concise and declarative\napproach for data management. It also offers several inter-\nesting perspectives: automatic content management (split\na score in parts, distribute them to digital music stands),\nadvanced content-based search, and ﬁnally advanced min-\ning tasks (derivation of features, annotation of scores with\nthese features).728 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20168. REFERENCES\n[1] Serge Abiteboul, Omar Benjelloun, and Tova Milo.\nThe active XML project: an overview. VLDB J. ,\n17(5):1019–1040, 2008.\n[2] Serge Abiteboul and et al. WebContent: Efﬁcient P2P\nWarehousing of Web Data. In VLDB’08 Very Large\nData Base , pages 1428–1431, August 2008.\n[3] Mira Balaban. The music structures approach to\nknowledge representation for music processing. Com-\nputer Music Journal , 20(2):96–111, 1996.\n[4] Michael Scott Cuthbert and Christopher Ariza. Mu-\nsic21: A Toolkit for Computer-Aided Musicology and\nSymbolic Music Data. In Proceedings of the Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR) , pages 637–642, 2010.\n[5] AnHai Doan, Alon Halevy, and Zachary Ives. Princi-\nples of Data Integration . Morgan Kaufmann Publishers\nInc., San Francisco, CA, USA, 1st edition, 2012.\n[6] Dominique Fober, St ´ephane Letz, Yann Orlarey, and\nFr´ed´eric Bevilacqua. Programming Interactive Music\nScores with INScore. In Sound and Music Computing ,\npages 185–190, Stockholm, Sweden, July 2013.\n[7] Dominique Fober, Yann Orlarey, and St ´ephane Letz.\nScores level composition based on the guido music no-\ntation. In ICMA, editor, Proceedings of the Interna-\ntional Computer Music Conference , pages 383–386,\n2012.\n[8] R. Fournier-S’niehotta, P. Rigaux, and N. Travers.\nAn Algebra for Score Content Manipulation. Tech-\nnical Report CEDRIC-16-3616, CEDRIC laboratory,\nCNAM-Paris, France, 2016.\n[9] Joachim Ganseman, Paul Scheunders, and Wim\nD’haes. Using XQuery on MusicXML Databases for\nMusicological Analysis. In Proceedings of the Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR) , 2008.\n[10] H. Garcia-Molina, J.D. Ullman, and J. Widom.\nDatabase System Implementation . Prentice Hall, 2000.\n[11] Michael Good. MusicXML for Notation and Analysis ,\npages 113–124. W. B. Hewlett and E. Selfridge-Field,\nMIT Press, 2001.\n[12] Paul Hudak. The Haskell School of Music – From Sig-\nnals to Symphonies . (Version 2.6), January 2015.\n[13] David Huron. Music information processing using the\nhumdrum toolkit: Concepts, examples, and lessons.\nComputer Music Journal , 26(2):11–26, July 2002.\n[14] David Janin, Florent Berthaut, Myriam Desainte-\nCatherine, Yann Orlarey, and Sylvain Salvati. The T-\nCalculus : towards a structured programing of (musi-\ncal) time and space. In Proceedings of the ﬁrst ACMSIGPLAN workshop on Functional art, music, model-\ning and design (FARM’13) , pages 23–34, 2013.\n[15] Music Encoding Initiative. http://\nmusic-encoding.org , 2015. Accessed Oct.\n2015.\n[16] MusicXML. http://www.musicxml.org , 2015.\nAccessed Oct. 2015.\n[17] Philippe Rigaux, Lylia Abrouk, H. Aud ´eon, Na-\ndine Cullot, C. Davy-Rigaux, Zo ´e Faget, E. Gavi-\ngnet, David Gross-Amblard, A. Tacaille, and Virginie\nThion-Goasdou ´e. The design and implementation of\nneuma, a collaborative digital scores library - require-\nments, architecture, and models. Int. J. on Digital Li-\nbraries , 12(2-3):73–88, 2012.\n[18] Perry Rolland. The Music Encoding Initiative (MEI).\nInProc. Intl. Conf. on Musical Applications Using\nXML , pages 55–59, 2002.\n[19] Nicolas Travers, Tuy ˆet Tr ˆam Dang Ngoc, and Tianx-\niao Liu. Tgv: A tree graph view for modeling untyped\nxquery. In 12th International Conference on Database\nSystems for Advanced Applications (DASFAA) , pages\n1001–1006. Springer, 2007.\n[20] W3C Music Notation Community Group.\nhttps://www.w3.org/community/music-notation/,\n2015. Last accessed Jan. 2016.\n[21] Philip Wheatland. Thoth music learning software,\nv2.5, Feb 27, 2015. http://www.melodicmatch.com/.\n[22] XML Schema. World Wide Web Consortium, 2001.\nhttp://www.w3.org/XML/Schema.\n[23] XQuery 3.0: An XML Query Language.\nWorld Wide Web Consortium, 2007.\nhttps://www.w3.org/TR/xquery-30/.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 729"
    },
    {
        "title": "Elucidating User Behavior in Music Services Through Persona and Gender.",
        "author": [
            "John Fuller",
            "Lauren Hubener",
            "Yea-Seul Kim",
            "Jin Ha Lee 0001"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1415928",
        "url": "https://doi.org/10.5281/zenodo.1415928",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/205_Paper.pdf",
        "abstract": "Prior user studies in the music information retrieval field have identified different personas representing the needs, goals, and characteristics of specific user groups for a user- centered design of music services. However, these per- sonas were derived from a qualitative study involving a small number of participants and their generalizability has not been tested. The objectives of this study are to explore the applicability of seven user personas, developed in prior research, with a larger group of users and to identify the correlation between personas and the use of different types of music services. In total, 962 individuals were surveyed in order to understand their behaviors and preferences when interacting with music streaming services. Using a stratified sampling framework, key characteristics of each persona were extracted to classify users into specific per- sona groups. Responses were also analyzed in relation to gender, which yielded significant differences. Our findings support the development of more targeted approaches in music services rather than a universal service model.",
        "zenodo_id": 1415928,
        "dblp_key": "conf/ismir/FullerHKL16",
        "content": "ELUCIDATING  USER BEHAVIOR IN  MUSIC SERVICES \nTHROUGH PERSONA AND GENDER \nJohn Fuller  Lauren  Hubener  Yea-Seul Kim  Jin Ha Lee  \nUniversity of Washington  University of Washington  University of Washington  University of Washington  \nfuller14@uw.edu  lhubener@uw.edu  yeaseul1@uw.edu  jinhalee@uw.edu  \n \n \nABSTRACT  \nPrior user studies in the music information retrieval field \nhave identified different personas representing the needs , \ngoals,  and characteristics of specific user groups  for a user -\ncentered design of music services . However, these per-\nsonas w ere derived from a qualitative study involving a \nsmall number of participants and their  generalizability has \nnot been tested. The objectives of this study are to explore the applicability of seven user personas, developed in prior research, with a larger g roup of users and to identify the  \ncorrelation between personas and the use of different types of music services. In total, 962  individuals were surveyed \nin order to understand their  behaviors and preferences \nwhen interacting with music streaming services. Using a \nstratified sampling framework, key characteristics  of each \npersona  were extracted to classify users into specific per-\nsona groups. Responses were also analyzed in relation to gender, which yielded significant differences. Our findings \nsupport the de velopment of more targeted approaches  in \nmusic services  rather than a universal service model.  \n1. INTRODUCTION \nCommercial music streaming services represent the fastest \ngrowing sector of the music recording industry . User stud-\nies from the field of Music Infor mation Retrieval (MIR) \nthat can be  used to guide the strategic development of these \nstreaming services a s well as  new methods for the naviga-\ntion of large music collections  have been increasing . Sev-\neral studies  have pointed out that t he large majority of re-\nsearch in MIR systems has been focused on the evaluation of system accuracy and performance, creating a gap in user-centric research [ 19, 27, 29] . Although MIR studies \nare increasing in number, m any tend to employ a qualita-\ntive appro ach and derive findings from  the investigation of \na limited number of users [ 29]. Understanding users’ ex-\nperiences with MIR systems, and particularly in relation to commercial music streaming services , can benefit from t he \nadoption of more quantitative met hods of analysis in addi-\ntion to these qualitative assessment techniques.  \nThis study aims to triangulate findings from prior \nqualitative MIR user studies adopting a quantitative ap-proach with a larger sample to verify the findings and pro-vide complementary  insights . In particular, we conducted \na follow -up study to further explore the findings of Lee and Price  [18]. They  presented seven different personas sur-\nrounding the use of commercial music services, derived from interviewing and observing 40 users who regularly use at least one commercial music service. Our study aims  \nto test the applicability of the previously defined personas \nwith a larger user population, as the results of the original \nstudy involved a relatively small sample. In this study, 962  \nusers were surveyed in order to capture characteristics of \nthe indi viduals’ music listening habits based on their pre-\nferred commercial music services, and the data were ana-lyzed in connection to the results derived from the princi-pal study. In particular, t his study seeks to elaborate on the \nprevious findings and address the following questions:  \nRQ1: How similar or distinct are the seven personas when generalized to a larger stratified user sample?  \nRQ2: How similar or different are the persona distributions \nfor different commercial streaming services?  \nRQ3: Is there a sig nificant difference between genders \nwith regard to their persona distribution?  \n2. RELEVANT WORK  \n2.1 Users of Music Streaming Services  \nThe emergence of innovative music streaming services has \nraised the awareness and desire, both in academia and in-\ndustry, for a ubiquitous system that seamlessly allows for \nthe search, retrieval, and recommendation of music. How-ever, a deeper understanding of the user is crucial for the \ndevelopment of more personalized and context -aware sys-\ntems that will meet the users’ needs in a wide variety of \nsituations  [27].  \nThere have been a few  studies focusing on understand-\ning the reason s for the popularity of music  steaming ser-\nvices such as Spotify  or YouTube (e.g., [ 13, 21] ) based on \nsurvey data or specifically measuring the quality of the \nmusic recommendations provided by commercial services such as Apple iTunes Genius (e.g., [3] ). From the studies \nthat have focused on evaluating users’ experiences with music streaming services, a number of trends have emerged. Based on a large -scale survey, Lee and Water-\nman [ 20] determined an increased consumption of music \non mobile devices, an increased desire for the serendipi-tous discovery of music  and the option of customization in \ntheir listening experiences. Lee and Price  [18] found that \nmusic streaming services are often perceived as “good enough” for users’ purposes, and many individuals use a variety of services to accommodate needs across various contexts, illustrating Bates’ “berrypicking” search behav-\nior. A qualitative study conducted by L aplante and Downie \n[16] exploring the information- seeking behavior of young \n © John Fuller , Lauren Hubener , Yea-Seul Kim, Jin Ha Lee.  \nLicensed under a Creative Commons Attribution 4.0 International \nLicense (CC BY 4.0). Attribution: John Fuller , Lauren Hubener , Yea-\nSeul Kim, Jin Ha Lee . “Elucidating User Behavior in Music Services \nThrough Persona and Gender ”, 17th International Society fo r Music \nInformation Retrieval Conference, 2016.  \n626  \n \nadults in the discovery of new music found a prefere nce \nfor informal channels (e.g. friends and family) as well as a \ndistrust of experts. It was also revealed that the act of mu-\nsic seeking is strongly motivated by curiosity as opposed \nto actual information needs, which helps illustrate why browsing is an often -employed technique. Ferwerda et al. \n[10] explore how individuals’ personality traits relate to \ntheir explicit choice of music taxonomies, and propose the creation of an interface that can adapt to users’ preferred methods of music browsing. Ethnogra phic and observa-\ntional research into music collection by Cunningham et al. [9] found that users create an implicit organization to their \nown music collections (digital or physical) without being fully conscious of it, and are reluctant to remove or de lete \nsongs even if they have not listened to them in months or years.  \nIn addition, a growing number of researchers have come \nforward to suggest more comprehensive user models in \nMIR systems. Cunningham et al. [ 8] investigate d how the \nfactors of human movem ent, emotional status , and the \nphysical environment relate to musical preferences, and \nbased on the findings, present a model for playlist creation. \nIn addition, a ssuming a mobile -based consumption of mu-\nsic, systems seeking to match the pace of someone in movement have also been proposed (e.g., [2 2, 25] ). These \ntypically aim to correlate the music played to the user’s \nheartbeat, though most of the systems proposed would re-\nquire additional context -logging hardware. A study by \nBaltrunas et al . [2] outlines a context -aware recommenda-\ntion system for music consumption while driving by taking into account eight contextual facts including mood, road type, driving style, and traffic conditions. There have also been a few studies in which users were c ategorized into \ndifferent types of users or personas: Celma [ 6] identified \nfour different types of music listeners (i.e., savants, enthu-\nsiasts, casuals, and indifferents ) representing different de-\ngrees of interest in music, and Lee and Price [18] derived \nseven personas, hypothetical archetypes  of users repre-\nsenting specific goals and behavior [ 7], from empirical \nuser data to help understand the different needs they have \nregarding the use of music services , which our study is  \nbuilding  upon.  \nThere is notably  less research dedicated to exploring \nhow various users search for, interact with, and listen to music digitally since the widespread growth of commer-cial music streaming services. This paper will elaborate on how users listen to music and interact with st reaming ser-\nvices through the application of user personas developed in the principal study [ 18] in order to identify behavioral \ndifferences, preferences, and varying MIR goals.  \n2.2 Gender and Musical Interactions  \nSeveral researcher s have studied the impact of demo-\ngraphic characteristics such as age [ 5, 15] and nationality \n[11, 14] on musical interactions. These studies have found \nthat both age and cultural background prove to be signifi-cant factors in individuals’ music perceptions and prefer-\nences. To a lesser extent, the influence of gender in music \nstudies has been explored, often in the field of ethnomusi-\ncology [23, 24]  and music education [1, 26]. O’Neill, a \nleading researcher in music and education, has found strik-\ning differences in boys’ and girls’ prefere nces for music and musical activities [2 6]. Her study of 153 children aged \n9 to 11, explored the extent to which boys’ and girls’ pref-erences are a product of gender -stereotyped associations . \nShe found that girls showed a significantly stronger pro-\nclivity for the piano, violin, and flute, whereas boys ex-\npressed preference for the guitar, drums, and trumpet. Also \neach gender had similar ideas regarding which instruments \nshould not be played by boys or girls. The study supports \nthe notion that gendered perspe ctives regarding music are \ndeveloped early and could have a lasting impact on the \nway different genders go about seeking musical infor-mation. A few studies have focused on the general listen-\ning preferences and music processing of the two genders . \nKoelsch e t al. [ 12] studied the differences between genders \nin the processing of music information. They  discovered \nthat early electric brain activity occurs bilaterally in fe-\nmales, and with right hemispheric predominance in males, \nsupporting the claim that differences between genders in \nthe processing of auditory information go beyond the lin-\nguistic domain. LeBlanc et al . [17] found the variables of \nage, country, and gender to all be significant factors in in-\ndividuals’ music listening preferences, but determined \neach of these variables to be involved in complex interac-tions with other variables. Though the e ffectiveness of the \nage and gender variables was confirmed, they did not per-form the same way in each country, and therefore should be explored in relation to cultural context. In the context \nof MIR, few studies explored gender as a variable while \nexamining the needs and behavior of music listeners. In \nour study, we specifically want ed to investigate whether \nthere is a significant difference between genders with re-gard to their use of music streaming services, to comple-\nment what we already know about gender  differences in \nother musical interactions.  \n3. STUDY DESIGN AND MET HOD  \n3.1 Background  \nThe principal study [ 18] was conducted in 2015 in order \nto gain insight s into how users’ personalities and charac-\nteristics affect their interactions with and preferences for \nparticular MIR systems. In the study, 40 subjects partici-\npated in semi -structured interviews regarding how they \nevaluate music services and think -aloud sessions during \nwhich subjects described and narrated their actions as they \nused their preferred music s ervice. A card sorting activity \nrecording subjects’ comments, actions, and behaviors \nthroughout the process derived seven personas : Active \nCurator  (AC) , Addict (AD) , Guided Listener (GL) , Music \nEpicurean  (ME), Music Recluse (MR) , Non-Believer  \n(NB), and Wande rer (WA). The typical behavior s and \ntendencies for each of these personas regarding their use of commercial music services are described  in detail in \n[18].   \n3.2 Method  \nThe online survey was developed using SurveyMonkey , an \nonline survey tool, and responses were collected during April and May 2015, before the demise of streaming ser-\nvices Grooveshark and Rdio and before the release of Ap-ple Music. The survey contained a total of 26 questions Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 627  \n \npertaining to individuals’ mus ic listening habits and be-\nhaviors, their interactions with MIR systems , and preferred \nmusic streaming services. The questions were developed \nin connection to the  results of the principal study [ 18] in \norder to test the applicability of the seven personas w ith a \nlarger user population. A majority of the survey questions \nand response options targeted one or more of the previ-\nously defined personas  with the intention  to classify a \nuser’s response to a particular persona (further discussed \nin Section 4.2) . A lim ited amount of demographic infor-\nmation was also collected, including age and gender.  \nDistribution methods to recruit participants included an \nopen call for individuals age 14 or older via University of \nWashington departmental listservs and posts to online  \nmessage boards such as Reddit. A total of 1,028  responses \nwere collected , of which 962 complete responses were \nused in the analysis. The survey responses were used to generate user profiles consisting of a list of behaviors and \npreferences exhibited when interacting with music stream-\ning services.  The 26 survey questions were translated into \n32 variables. The data were then analyzed in RStudio and \nExcel environments.  \n4. FINDINGS AND DISCUSS ION \n4.1 Demographic Information  \nThe average age of respondents was 23.4 (S D = 7.8). \nSixty -one respondents did not report an age, and four re-\nspondents under the survey participation age of 14 were \nexcluded in the analysis. The breakdown of gender distri-bution were as follows: 57.3% were male, 36.4% were fe-\nmale, 1.7% selected “oth er,” and 4.7% did not specify their \ngender. 78.3% of respondents described themselves as White or Caucasian, 7.0% described themselves as Asian, \n3.7% described themselves as Multiracial, 2.4% described themselves as Hispanic,  and 2.2% described themselves as \nBlack, respectively. 5.5% of respondents did not indicate or specify an ethnic affiliation. Responses were collected from 54 countries. The five countries with the most re-sponses were: the United States (63.52%), Canada (7.1%), \nthe United Kingdom (3.8%) , Australia (2.6%), and Ger-\nmany (2.1%). 5.7% of respondents did not indicate or \nspecify a country of residence.  \n4.2 Filtering Method  \nSurvey respondents were asked a series of questions re-garding their music listening habits, behaviors, and ac-\ntions. Questions pertaining to individuals’ behavior and \nactions when searching for and listening to music were \npivotal in determining and classifying respondents to spe-\ncific personas. To classify respondents into personas, a fil-tering method using a combination of question- response (s) \nwas applied to the entire sample. For each persona, if the participant selected the primary question and r esponse pair \nand one of two secondary question and response  pairs, he \nor she w as classified as a respondent  exhibiting that par-\nticular pe rsona . These primary and secondary question- re-\nsponse pairs are summarized in Table 1.  \n \n Active  \nCurator  Primary: Regularly curate s and listen s to playlists  \nSecondary: Makes playlists more than once a month \nOR Cares about organizing collection and willing to \nspend time for it  \nAddict  Primary: Spends more time searching rather than \nbrowsing for music  \nSecondary: Likes a few songs and listens to them over \nand over OR Most likely to listen to a song repeatedly  \nwhen they hear a new song they enjoy  \nGuided \nListener  Primary: Most likely to use a new song they like to \ngenerate recommendations of similar songs  \nSecondary: Finds recommendations generated by a \nstreaming service most appealing OR Wants to en-\ngage with a music streaming service minimally  \nMusic  \nEpicurean  Primary: Likely to seek out the  whole  album or search \nonline to learn more about a song they like  \nSecondary: Very or somewhat interested in an artist’s \nrelationship to other artists, genres, or music scenes \nOR Purchases music approximately once a month or \nmore  \nMusic  \nRecluse  Primary: Not at all or not very likely to recommend a \nsong to a friend  \nSecondary: Listens to music they consider a “guilty \npleasure” often  to all the time  OR Not willing or re-\nluctant to share personal information/listening history \nwith a streaming service  \nNon- Believer  Primary: Does not trust music recommendation gen-\nerators and prefers finding music through other meth-\nods \nSecondary: Does not think an app or service can pick \nmusic they would like OR Do es not use the social fea-\ntures of streaming services  \nWanderer  Primary: Finds and listens to music from many \nsources and is always looking for something new  \nSecondary: Spends  more time browsing rather than \nsearching for music OR Takes satisfaction in discov-\nering new artists others have not heard of  \nTable 1. Filtering mechanism for assigning personas.  \n4.3 Similarity  among Personas  \nThrough t he filtering method used to create sample subsets \nfor each persona , 155 respondents  (15.4%)  were classified \nas Active Curators, 184 (18.2%) as Addicts, 47 (4.7%) as \nGuided Listeners, 121 (12.0%)  as Music Epicureans, 119 \n(11.8%)  as Music Recluses, 120 (11.9%)  as Non -Believ-\ners, and 263 (26.1%)  as Wanderers.  The filtering method \nused to classify respondents did not return mutually exclu-\nsive results in all cases, leading to a number of respondents \nto be classified with multiple music personas.  This is con-\nsistent with the observation of Lee and Price that “any user \nmay exhibit a combination of these personas as they are \nnot mutually exclusive (p. 478)”  [18]. In total, 732 re-\nspondents ( 71.2%) were identified as exhibiting one or \nmore personas. Of the total number of  responses, 493 re-\nspondents ( 67.3%) were classified as exhibiting  one per-\nsona, 204 respondents ( 27.9%) as two personas, 32 re-\nspondents ( 4.4%) as three personas, and only 3 respond-\nents (0.4%) were classified as having four personas.  These \nresults suggest th at enough distinctness exist s between at \nleast some of these personas  that they tend to not be exhib-\nited by the same person. At the same time, it indicates that 628 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016  \n \na closer relationship exist s between particular user per-\nsonas than with others, and how a respondent’s classifica-\ntion as exhibiting  more than one persona may represent a \nfluctuating or shifting of persona association, depending on the user’s momentary listening needs or context. It is \nnoteworthy that 28.8% of respondents did not show a dis-tinct pers ona; rather, they exhibited a range of different be-\nhaviors related to a number of different personas.  \nWe calculated the Jaccard similarity coefficient to \nmeasure the similarity  among personas  (Table 2).  This  sta-\ntistic  represents the degree of overlap between two sets of \ndata ranging from 0 to 1 [28]. Guided Listener and Music \nRecluse have the largest overlap among classified survey \nrespondents, while the Active Curator and Wanderer , and \nthe Addict  and Wanderer  have the least.   \n AC AD GL ME MR NB WA \nAC        \nAD 0.61       \nGL 0.73 0.68      \nME 0.66 0.58 0.77     \nMR 0.67 0.67 0.79 0.69    \nNB 0.67 0.67 0.78 0.69 0.76   \nWA 0.43 0.43 0.63 0.61 0.54 0.55  \nTable 2. Similarity among personas measured by Jaccard similarity coefficient. \nWe attempted to identify the reasons for these patterns \nby comparing these coefficient values, examining  how the \npersonas were originally defined, and reviewing the qual-\nitative data reported in the principal study including direct user quotes. In regard s to the personas w ith the largest \noverlap, the major similarity between the two is that they \nprefer not to engage or interact with the service very much. Guided Listener is passive and minimally invested when it comes to engagement with a music service . Music Recluse  \nalso t ends to limit their engagement with the system by not \nactively sharing their information or us ing social features . \nThe difference between these personas is that the Guided Listener trusts a music service enough to let the service \nselect music on their behalf while the Music Recluse has \nlittle to no  interest in sharing their music preferences  or \nlistening history  with a music  service  because they are pri-\nvate listeners . The fact that the coefficient values between \nthe Non- Believer and Music Recluse (0. 76) and  between \nthe Guided Listener and Non-Believer  (0.78) also tend to \nbe high is noteworthy. This may be stemming from the \ncommonality among these  personas  that they do not like to \nshare their personal information although their reasons \nmay be different (e.g., lack of interests, distrust with rec-\nommendation algorithms, privacy concerns).  \nWhen looking at personas with the least overlap and \ntheir characteristics, the level of willingness to explor e and \nthe level of engagement with their own colle ction seem to \nbe the core reason s for this difference. According to the \ndata reported in the principal study [ 18], the Wanderer is \nprimarily concerned with finding new music across plat-\nforms and genres  and is generally exploratory in their mu-\nsic listening while the Addict tends to repeatedly listen to a few songs they know and like . Also the Active Curator \nshows  a high level of engagement with their own music collection which takes the form of playlist creation and \ncollection organization /management  whereas  the Wan-\nderer is focused on new music discovery  which often oc-\ncurs outside of their own collection. \n4.4 Persona Classification and Preferred Services  \nWhen examining the distribution of the seven user per-\nsonas across the most popular streaming services, several notable trends emerge. Personas were assigned to respond-ents using the  filtering method described above in S ection \n4.2, and respondents ’ preferre d service choice was deter-\nmined using their responses to the survey question asking for the primary streaming service they use. Overall, the \nthree most popular music streaming services selected by survey respondents were:  Spotify (28.8%), YouTube \n(25.2%), and Pandora (17.2%) . These were followed by \niTunes (6.0%), SoundCloud (5.7%), and Google Play (4.2%), and several other services that were selected  by \nless than 2% of respondents . We conducted a chi -square \nanalysis to identify statistically significant dif ferences be-\ntween the persona distribution across the three most popu-lar services (i.e., Spotify, YouTube, and Pandora)  based on \nrespondents’ stated preference of primary streaming ser-\nvice. \nWhen looking at those respondents who selected \nSpotify as their pri mary service  (Figure 1), the Active Cu-\nrator persona had the greatest representation with 28.1% , \nmuch more than other two services ( X\n2=28.37, df=2, \np=0.000) . Spotify includes a range of features making it \neasy for users to save songs for playlist creation a s well as \ndiscover new music  based on their listening history, which \nare design elements that the Active Curator would find val-\nuable.  Conversely, the Guided Listener persona was  un-\nder-represented in  its indications of a preference for \nSpotify, as the service itself requires a certain amount of curation to take advantage of its features ( e.g. making an \naccount, importing one’s library, and understanding the ro-\nbust interface ) that the user is presented wit h upon first in-\nteracting with the service. For the less engaged personas, \nthe initial process of familiarizing oneself with Spotify \nmay not be worth the effort.  \n \nFigure 1. Persona distribution of Spotify users.  \nFor those respondents who selected YouTube as their \nprimary service  (Figure 2) , the Addict persona had the \ngreatest representation with  24.1% , significantly different Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 629  \n \nfrom other services ( X2=13.40, df=2, p=0.001). Given that \nYouTube is an ideal platform for known -item searches, \nand allows for the easy replay of videos and songs, in the \ncontext of the Addict persona’s user needs, YouTube would appear to be a realistic primary service choice. Mu-\nsic Epicureans were also most likely to choose YouTube \nas their primary ser vice. The staggering amount of content, \nincluding concert footage, interviews, and rare tracks , of-\nten not available on any other services, allows for the end-\nless discovery of new musical content. The ability to nav-igate through an extensive range of content is an appealing quality of the service for even the Non -Believer, which, \nalthough a small sam ple of the survey, was most likely to \nchoose YouTube as their primary service. Conversely, the heavily self -directed nature of YouTube is an unattractive \noption to the Guided Listener, who was the least likely of \nany persona to choose YouTube as their pref erred service.  \n \nFigure 2. Persona distribution of YouTube users. \nFor those respondents who selected Pandora as their \nprimary service (Figure 3), the Wanderer persona had the \ngreatest representation with  29.7%  (X2=1.01, df=2, \np=0.601) , although the difference with the other two ser-\nvices was not significant . The most notable difference was \nthe proportion of Guided Listeners ( X2=25.17, df=2, \np=0.000). Overall, Guided Listeners were an underrepre-\nsented group in the sample, comprising only 4.7% of our population. While the respondents classified as Guided \nListeners represented only 13.3% of those individuals that \nindicated Pandora as their preferred music service, Pan-dora was the primary service selected by Guided Listeners \nat 46.8%. Compared with other music services, Pandora \nprovides a limited number of user features and will recom-\nmend and play content closely aligned to the user’s “seed” \nartists. The user may be less likely to discover new or un-expected songs or artists but will also not have to interact \nwith the service extensively during use. Because of these \ncharacteristics, Lee and Price  [18] also expected that the \nGuided Listener persona would be the majority persona for Pandora  which was indeed the case. \n \n \nFigure 3. Persona distribution of Pandora users.  \n4.5 Gender Differences  \nChi-squared tests revealed statistically significant differ-\nences in the way that users of different genders  categorize \ntheir interactions with music services. When asked to indi-cate behaviors that described their music listening habits  \n(X\n2=30.40, df=4, p=0.000) , 31.4% of males said they liked \nto listen to an album from start to finish, as opposed to only 16.4% of females . Females, on the other hand, were more \nlikely to say that they enjoy listen ing to songs and artists \nfrom many different sources and are always looking for something new (32.6% for F, 30.7% for M ). When asked \nwhat they typically do after hearing a new song that they enjoy  (X\n2=14.48, df=4, p=0.006) , mal es were most likely \nto answe r that they seek out the album the song came from \n(23.7% for F , 32.4% for M ), while females were most \nlikely to respond that they listen to the song over and over \n(34.1% for F; 23.5% for M ). \nParticipants were also asked whether  they find them-\nselves more often searching or browsing for music  \n(X2=12.35, df=2, p=0.002) ; 48.9% of females identified \nthemselves as known- item searchers  and an equal share of \n25.5% identified themselves as browsers and as spending \nequal amounts of time both searching and browsing. On \nthe other hand, males’ responses were  more evenly distrib-\nuted; 37.6% identified themselves as searchers, 33% \nbrowsers, and 29.4% responded that they spend approxi-\nmately equal amounts of time doing both.  \nThe tendency of femal es to identify as searchers  is re-\nflected in the classification of the Addict persona, which \nfrequently relies on known- item searching in music seek-\ning, encompassing 22.1% of females versus 15.8% of males  (X\n2=6.23, df=1, p=0.013). Females also more often \nidentified as Music Recluse ( X2=5.16, df=1, p=0.023). The \nhigher percentage of male browsers as opposed to female browsers supports the gender distribution of the Music \nEpicurean ( 8.9% for F, 14.6% for M; X\n2=7.01, df=1, \np=0.008) and Non-Believer (7.4% for F, 14.7% for M; \nX2=12.28, df=1, p=0.000) personas. Table 3 shows the \noverall distribution of personas by gender. This pattern of \nmales as the more exploratory gender  in their music -seek-\ning behavior is also exhibited in the breakdown of pre-\nferred music serv ices. When asked to indicate their most \npreferred service, the top three services (Spotify, \nYouTube, and Pandora ) accounted for 67.8% of males’ top 630 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016  \n \nselection, whereas 77.6% of females named one of these \ntop three services as their most preferred one. Overall, \nmales selected 18 different options, while females chose \n14. \nPersona  Female  Male  X2 df p  \nActive  \nCurator  69 17.5%  82 14.2%  1.94 1 0.163  \nAddict  87 22.1%  91 15.8%  6.23 1 0.013  \nGuided \nListener  20 5.1%  24 4.2%  0.46 1 0.499  \nMusic  \nEpicurean  35 8.9%  84 14.6%  7.01 1 0.008  \nMusic  \nRecluse  57 14.5%  56 9.7%  5.16 1 0.023  \nNon- \nBeliever  29 7.4%  85 14.7%  12.28  1 0.000  \nWanderer  97 24.6%  155 26.9%  0.61 1 0.433  \nTotal  394 100.0%  577 100.0%  - - - \nTable 3. Persona distribution by gender.  \nOverall, males seem to exhibit more exploratory nature \nin their music discovery endeavors. When participants were asked whether they take satisfaction in finding or dis-\ncovering new artists that few people are aware of, 78% of \nmales answered affirmatively versus 67.9% of females  \n(X\n2=12.16, df =1, p=0.000) . Males also indicated that they \nmore frequently curate playlists, with 33.9% of males say-ing that they like to make playlists at least once a week, \ncompared to 23.9% of females  (X\n2=11.06, df=3, p=0.011) . \nMales were also mo re likely to claim that listening to mu-\nsic occupies their full attention ( 14.2% for F, 21.2% for M; \nX2=7.55, df=1, p=0.006).  \nThese findings suggest that it may be fruitful for devel-\nopers  of commercial music streaming services to consider \ngender -specific approaches in the design and function of \nsystem features. Services targeted at males should account for the desire of discovering novel musical content, includ-\ning robust features that consistently stimulate the encoun-ter of unfamil iar artists and songs. Females are more likely \nto require features that allow for the easy replayability  of \ncontent  and support passive engagement . \nThere are also some notable similarities across genders . \nNeither males nor females tend to care much about t he or-\nganization of their music collections. Of the respondents, 43.5% of males and 42.2% of females responded that they care only a little and are willing to spend minimal time or-\nganizing their collections  (X\n2=33.58, df =3, p=0.000). Ad-\nditionally, both male s (41.8%) and females (44.1%) re-\nsponded that they prefer to get recommendations from \nthose with similar tastes or listening habits rather than \nfriends, family, music experts and curators, or streaming services  (X\n2=21.80, df=5, p=0.000) . Therefore , services \nshould incorporate measures of user profile  and listening \nhistory  similarity as a prominent feature  for music recom-\nmendation. Interestingly, o f all the respondents, 78.3% \nsaid that they were either somewhat likely or very likely to recommend a so ng or artist to a friend they thought would \nlike it, but a mere 8.8% of all respondents said that they typically use the social features provided by a streaming \nservice. The lack of use of social feature s was consistent across both genders (91.4% for F, 90.4% for M ; X\n2=0.059, \ndf=1, p=0.808). This indicates that while users want to \nshare music discoveries or new artists, they prefer to do so through channels not associated with the streaming service \nitself. This represents a potential area of innovation for commercial streaming services in evaluating how they in-corporate  social features  into their platforms . \n5. CONCLUSION AND FUTURE WORK  \nBy testing the applicability of  personas with  a larger strat-\nified user sample, we were able to determine the relative uniqueness of some personas and the closeness of others. \nIn calculating the Jaccard coefficient, we found the Active \nCurator and Wanderer , and Addict  and Wanderer personas \nto be the m ost dissimilar, while the Guided Listener and \nMusic Recluse personas had the most overlap among the \nsurvey respondents. G oing forward, we may consider \nreevaluating those less distinctive personas by further ex-\namining the overlapping characteristics of each , and rede-\nfine them accordingly.  \nWhen looking at the persona distributions for major \ncommercial streaming services, patterns emerged between \nusers’ classified personas and their preferred services. \nWhile Spotify tends to draw in a high representation of \nmore engaged personas  like the Active Curator, it seem-\ningly repels others, such as the Guided Listener  persona. \nSimilarly, YouTube, the service most preferred by Music \nEpicureans, was not popular among Guided Listeners, who \ninstead prefer the more self -guided service, Pandora.  \nA further breakdown revealed significant differences \nbetween genders with regard to their persona distribution \nand preferred services. While the classification of the Ad-\ndict persona skewed female, the Music Epicurean persona was male skewed. The Music Recluse and Non- Believer \npersonas were also significantly different, with males more \noften identifying with the Non- Believer persona, and fe-\nmales comprising a higher representation of the Music Re-\ncluse persona. In regards  to the preferred  service distribu-\ntion, a significant proportion of males and females re-ported Spotify and YouTube as their most preferred ser-vices. However, it was found that females are much more \nlikely to favor Pandora, while males more often opt for pe-\nripheral services , such as Google Play and SoundCloud.  \nThe significant differences found between male and fe-\nmale users’ preferences, characteristics, and expectations \nof music services suggest a need for further research. Fu-\nture work will focus on obtaining a deeper understanding \nof the reasons for these gender differences and behaviors \nwhen engaging with music streaming services.  \nA revised model may be developed for classifying re-\nspondents to the defined user personas , incorporating the \nself-reported persona classifi cation data from users . Ask-\ning users to identify themselves by the sets of traits repre-\nsented by personas may help us further verify the validity \nof our filtering mechanism. In addition, f urther investiga-\ntion through a qualitative study should be conducted to in-vestigate those individuals that are classified into more \nthan one persona to determine the factors at play, such as \nshifts across different contexts .  Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 631  \n \n6. REFERENCES  \n[1] J. M. Abramo, “Popular music and gender in the \nclassroom,” Ed.D. dissertation, Teachers  College, \nColumbia University, 2009. \n[2] L. Baltrunas , M. Kaminskas, B. Ludwig, O. Moling, \nF. Ricci, A. Aydin, K.H. Lüke, and R. Schwaiger : \n“InCarMusic: Context -Aware Music \nRecommendations in a Car,” EC -Web, Vol. 11, pp. \n89-100, 2011.  \n[3] L. Barrington, O. Reid, and G. Lanckriet: “Smarter \nthan Genius? human evaluation of music \nrecommender systems,” Proc. of  ISMIR , pp. 357- 362, \n2009.  \n[4] M. J. Bates: “The design of browsing and \nberrypicking techniques for the online search interface. ” Online Rev iew, Vol. 13, No. 5, pp. 407-\n424, 1989.  \n[5] A. Bonneville -Roussy, P. J. Rentfrow, M. K. Xu, and \nJ. Potter: “Music through the ages: Trends in musical engagement and preferences from adolescence through middle adulthood.” Journal of Personality and Social Psychology , Vol. 105, No. 4, pp. 703, \n2013 \n[6] Ò. Celma: “Music recommendation and discovery in \nthe long tail,” Ph.D. dissertation, Dept. Information & \nCommunication Tech., UPF, 2008. \n[7] A. Cooper: The Inmates Are Running the A sylum , \nSams, Indianapolis, 1999. \n[8] S. Cunni ngham, S. Caulder, and V. Grout: “Saturday \nnight or fever? context -aware music playlists,” \nProceedings of Audio Mostly , pp. 1- 8, 2008.  \n[9] S. J. Cunningham, M. Jones, and S. Jones: \n“Organizing digital music for use: an examination of \npersonal music collections ,” Proc. of  ISMIR , pp. 447-\n454, 2004.  \n[10] B. Ferwerda, E. Yang, M. Schedl , and M. Tkalcic. : \n“Personality traits predict music taxonomy \npreferences,” Proc . of CHI EA ’15 , pp. 2241- 2246, \n2015.  \n[11] X. Hu and J. H. Lee: “A cross -cultural study of music \nmood perception betw een American and Chinese \nlisteners ,” Proc. of  ISMIR , pp. 535- 540, 2012.  \n[12] S. Köelsch, B. Maess, T. Grossmann, and A. D. \nFriederici: “Electric brain responses reveal gender \ndifferences in music processing,” NeuroReport , Vol. \n14 No. 5, pp. 709- 713, 2003.  \n[13] S. Komulainen, M. Karukka, and J. Häkkilä :  “Social \nmusic services in teenage life: a case study ,” Proc . of \nOZCHI’ 10 , pp. 364- 367, 2010.  \n[14] K. Kosta, Y. Song, G.  Fazekas  and M. B. Sandler : “A \nstudy of cultural dependence of perceived mood in \nGreek Music ,” Proc. of  ISMIR , pp. 317- 322, 2013. \n[15] P. A. Kostagio las, C. Lavranos, N. Korfiatis, J. \nPapadatos, and S. Papavlasopoulos : “Music, musicians and information seeking behaviour: a  case \nstudy on a community concert band,”  Journal of \nDocumentation, Vol. 71 , No. 1, pp. 3-24, 2015.  \n[16] A. Laplante and J. S. Downie : “Everyday life music \ninformation- seeking behaviour of young adults,” \nProc. of ISMIR , pp. 381- 382, 2006.  \n[17] A. LeBlanc, Y. C. Jin, L. Stamou, and J. McCrary: \n“Effect of age, country, and gender on music listening \npreferences,” Bulletin of the Council for Research in \nMusic Education, pp. 72- 76, 1999.  \n[18] J. H. Lee and R. Price: “Understanding users of commercial music services through persona: design implications,” Proc. of ISMIR , pp. 476- 482, 2015.  \n[19] J. H. Lee and R. Price: “User experience with \ncommercial music services: an empirical \nexploration,” Journal of the Association for \nInformation Science and Technology , Vol. 67, No. 4, \npp. 800- 811, 2016.  \n[20] J. H. Lee and N. M. Waterman: “Understanding User \nRequirements for Music Information Services.” Proc. \nof ISMIR , pp. 253- 258, 2012.  \n[21] L. A. Liikkanen and P. Åman: “Shuffling Services: \nCurrent Trends in Interacting with Digital Music,” \nInteracting with Computers , iwv004, 2015. \n[22] H. Liu, J. Hu, and M. Rauterberg: “Music playlist recommendation based on user heartbeat and music \npreference,” International Conference on Computer \nTechnology and Development , pp. 545- 549, 2009.  \n[23] T. Magrini: Music and Gender: Perspectives from the \nMediterranean , University of Chicago Press, 2003. \n[24] P. Moisala and B. Diamond: Music and Gender,  \nUniversity of Illinois Press, 2000.  \n[25] B. Moens, L. van Noorden, and M. Leman: “D -\njogger: Syncing music with walking,” Sound and \nMusic Computing Conference , pp. 451- 456, 2010.  \n[26] S. A. O'Neill and M. J. Boultona: “Boys' and girls' \npreferences for musical instruments: A function of gender?” Psychology of Music , Vol. 24, No. 2, pp. \n171-183, 1996.  \n[27] M. Schedl A. Flexer: “Putting the user in the center \nof music information retrieval,” Proceedings of the \nInternational Symposium on Music Information \nRetrieval , pp. 385- 390, 2012.  \n[28] R. Toldo and A. Fusiello: “Robust multipl e structures \nestimation with j- linkage” European Conference on \nComputer Vision, pp. 537- 547, 2008.   \n[29] D. M. Weigl and C. Guastavino: “User studies in the \nmusic information retrieval literature,” Proc. of  \nISMIR , pp. 335- 340, 2011.  \n 632 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Data-Driven Exploration of Melodic Structure in Hindustani Music.",
        "author": [
            "Kaustuv Kanti Ganguli",
            "Sankalp Gulati",
            "Xavier Serra",
            "Preeti Rao"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416520",
        "url": "https://doi.org/10.5281/zenodo.1416520",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/121_Paper.pdf",
        "abstract": "Indian art music is quintessentially an improvisatory mu- sic form in which the line between ‘fixed’ and ‘free’ is extremely subtle. In a r¯aga performance, the melody is loosely constrained by the chosen composition but oth- erwise improvised in accordance with the r¯aga grammar. One of the melodic aspects that is governed by this gram- mar is the manner in which a melody evolves in time in the course of a performance. In this work, we aim to dis- cover such implicit patterns or regularities present in the temporal evolution of vocal melodies of Hindustani music. We start by applying existing tools and techniques used in music information retrieval to a collection of concerts recordings of ¯al¯ap performances by renowned khayal vo- cal artists. We use svara-based and svara duration-based melodic features to study and quantify the manifestation of concepts such as v¯adi, samv¯adi, ny¯as and graha svara in the vocal performances. We show that the discovered pat- terns corroborate the musicological findings that describe the “unfolding” of a r¯aga in vocal performances of Hin- dustani music. The patterns discovered from the vocal melodies might help music students to learn improvisation and can complement the oral music pedagogy followed in this music tradition.",
        "zenodo_id": 1416520,
        "dblp_key": "conf/ismir/GanguliGSR16",
        "content": "DATA-DRIVEN EXPLORATION OF MELODIC STRUCTURES IN\nHINDUSTANI MUSIC\nKaustuv Kanti Ganguli1Sankalp Gulati2Xavier Serra2Preeti Rao1\n1Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India\n2Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain\nkaustuvkanti@ee.iitb.ac.in\nABSTRACT\nIndian art music is quintessentially an improvisatory mu-\nsic form in which the line between ‘ﬁxed’ and ‘free’ is\nextremely subtle. In a r ¯aga performance, the melody is\nloosely constrained by the chosen composition but oth-\nerwise improvised in accordance with the r ¯aga grammar.\nOne of the melodic aspects that is governed by this gram-\nmar is the manner in which a melody evolves in time in\nthe course of a performance. In this work, we aim to dis-\ncover such implicit patterns or regularities present in the\ntemporal evolution of vocal melodies of Hindustani music.\nWe start by applying existing tools and techniques used\nin music information retrieval to a collection of concerts\nrecordings of ¯al¯ap performances by renowned khayal vo-\ncal artists. We use svara-based and svara duration-based\nmelodic features to study and quantify the manifestation\nof concepts such as v ¯adi, samv ¯adi, ny ¯as and graha svara in\nthe vocal performances. We show that the discovered pat-\nterns corroborate the musicological ﬁndings that describe\nthe “unfolding” of a r ¯aga in vocal performances of Hin-\ndustani music. The patterns discovered from the vocal\nmelodies might help music students to learn improvisation\nand can complement the oral music pedagogy followed in\nthis music tradition.\n1. INTRODUCTION\nHindustani music is one of the two art music traditions of\nIndian art music [6], the other being Carnatic music [28].\nMelodies in both these performance oriented music tradi-\ntions are based on the framework of r ¯aga [3]. Performance\nof a r ¯aga in Indian art music (IAM) is primarily impro-\nvisatory in nature [26]. While some of these improvisa-\ntions are based on a composed musical piece, Bandish, oth-\ners are completely impromptu expositions of a r ¯aga, ¯Al¯ap.\nR¯aga acts as a grammar both in composition and in impro-\nvisation of melodies.\nThe rules of the r ¯aga grammar are manifested at differ-\nent time scales, at different levels of abstraction and de-\nc/circlecopyrtKaustuv Kanti Ganguli, Sankalp Gulati, Xavier Serra,\nPreeti Rao. Licensed under a Creative Commons Attribution 4.0 Inter-\nnational License (CC BY 4.0). Attribution: Kaustuv Kanti Ganguli,\nSankalp Gulati, Xavier Serra, Preeti Rao. “Data-driven exploration of\nmelodic structures in Hindustani music”, 17th International Society for\nMusic Information Retrieval Conference, 2016.mand a different degree of conformity. While some of the\nelements of the r ¯aga grammar are explicit, others are im-\nplicit and may require years of musical training to grasp.\nA number of textbooks and musicological studies exist\nthat describe different improvisatory aspects of melodies\nin IAM [1, 3, 4, 6, 10, 18, 26]. These works also attempt to\nuncover some of the implicit aspects of r ¯aga grammar.\nA majority of the studies mentioned above are musi-\ncological in nature. These typically involve either a thor-\nough qualitative analysis of a handful of chosen musical\nexcerpts or a compilation of expert domain knowledge.\nThough these studies often present interesting musical in-\nsights, there are several potential caveats in such works.\nSome of these caveats are summarized below:\n•Small repertoire used in the studies challenge the\ngeneralizability of the proposed musical models\n•Bias introduced due to the subjectivity in the analy-\nsis of musical excerpts\n•Absence of concrete quantitative evidences support-\ning the arguments\n•The kind of analysis that can be done (manually) is\nlimited by human capabilities, limited memory (both\nshort- and long-term)\n•Difﬁculty in reproducibility of the results\nIn addition to the musicological works mentioned\nabove, there are several studies that perform computa-\ntional modeling of different melodic aspects in IAM. These\nstudies address computational research tasks such as r ¯aga\nrecognition [5, 15], melodic similarity [11, 17, 20], discov-\nery and search of melodic patterns [12, 16], segmentation\nof a musical piece [27] and identiﬁcation of speciﬁc land-\nmarks in melodies [14]. These approaches typically em-\nploy signal processing and machine learning methodolo-\ngies to computationally model the relevant melodic aspect.\nThese studies can provide a ground for developing tools\nand technologies needed to navigate and organize sizable\naudio collections of music, perform r ¯aga-based search and\nretrieval from large audio archives and in several other ped-\nagogical applications.\nSeveral qualitative musicological works bring out new\nmusical insights but are prone to criticism of not having\nsupported their ﬁndings using a sizable corpus. Contrary to\nthat, quantitative computational studies manage to scale to605sizable data sets, but fall short of discovering novel musi-\ncal insights. In the majority of cases, computational studies\nattempt to automate a task that is well known and is fairly\neasy for a musician to perform. There have been some\nstudies that try to combine these two types of methodolo-\ngies of working and corroborate several concepts in mu-\nsical theories using computational approaches. In Chi-\nnese opera music, [22] has performed a comparison of the\nsinging styles of two Jingju schools where the author ex-\nploit the potential of MIR techniques for supporting and\nenhancing musicological descriptions. Autrim1(Auto-\nmated Transcription for Indian Music) has used MIR tools\nfor visualization of Hindustani vocal concerts that created a\ngreat impact on music appreciation and pedagogy in IAM.\nWe ﬁnd that such literation pertaining to melodic structures\nin Indian art music is scarce.\nIn this paper, we perform a data-driven exploration of\nseveral melodic aspects of Hindustani music. The main\nobjective of this paper is to use existing tools, techniques\nand methodologies in the domain of music information re-\ntrieval to support and enhance qualitative and descriptive\nmusicological analysis of Hindustani music. For this we\nselect ﬁve melodic aspects which are well described in mu-\nsicological texts and are implicitly understood by musi-\ncians. Using computational approaches on a music col-\nlection that comprises representative recordings of Hin-\ndustani music we aim to study these implicit structures\nand quantify different melodic aspects related with them.\nIn addition to corroborating existing musicological works,\nour ﬁndings are useful in several pedagogical applications.\nFurthermore, the proposed methodology can be used for\nanalyzing artist or gharana-speciﬁc2melodic characteris-\ntics.\n2. HINDUSTANI MUSIC CONCEPTS\nBor [3] remarks that r ¯aga, although referred to as a con-\ncept, really escapes such categories as concept, type,\nmodel, pattern etc. Meer [26] comments that technically\na r¯aga is a musical entity in which the intonation of svaras,\nas well as their relative duration and order, is deﬁned. A\nr¯aga is characterized by a set of melodic features that in-\nclude a set of notes (‘svara’), the ascending and descend-\ning melodic progression (‘ ¯ar¯ohana-avr ¯ohana’), and a set of\ncharacteristic phrases (‘pakad’). There are three broad cat-\negories of segments in the melody: stable svara regions,\ntransitory regions and pauses. While svaras comprise cer-\ntain hierarchical sub-categories like ny ¯as, graha, amsa,\nv¯adi and samv ¯adi, the transitions can also be grouped into\na set of melodic ornamentation (‘alankar’) like meend, an-\ndolan, kan, khatka etc. [26]. The third important melodic\nevent is the pause. Pauses carry much information about\nthe phrases; in fact, a musician’s skill lies in the judicious\nuse of the pause [10]. We shall next go over these three\nbroad aspects.\nMany authors [3, 26] refer to the importance of certain\nsvaras in a r ¯aga. From the phrase outline we may ﬁlter cer-\n1https://autrimncpa.wordpress.com/\n2Refers to a lineage or school of thought.tain svaras which can be used as rest, sonant or predom-\ninant; yet the individual function and importance of the\nsvaras should not be stressed [26]. Ny ¯as svara is deﬁned\nas the resting svara, also referred to as ‘pleasant pause’ [7]\nor ‘landmark’ [14] in a melody. V ¯adi and samv ¯adi are best\nunderstood in relation with melodic elaboration or vist ¯ar\n(‘barhat’). Over the course of a barhat, artists make a par-\nticular svara ‘shine’ or ‘sound’. There is often a corre-\nsponding svara which sustains the main svara and has a\nperfect ﬁfth relation with it. The subtle inner quality of a\nr¯aga certainly lies in the duration of each svara in the con-\ntext of the phraseology of the r ¯aga. A v ¯adi, therefore, is\na tone that comes to shine, i.e., it becomes attractive, con-\nspicuous and bright [26]. Another tone in the same r ¯aga\nmay become outstanding that provides an answer to the\nformer tone. This second tone is the samv ¯adi and should\nhave a ﬁfth relationship with the ﬁrst tone. This relation-\nship is of great importance.\nA r¯aga is brought out through certain phrases that are\nlinked to each other and in which the svaras have their\nproper relative duration. This does not mean that the du-\nration, the recurrence and the order of svaras are ﬁxed in\na r¯aga; they are ﬁxed only within a context [26]. The\nsvaras form a scale, which may be different in ascending\n(¯ar¯ohana) and descending (avr ¯ohana) phrases, while every\nsvara has a limited possibility of duration depending on\nthe phrase in which it occurs. Furthermore, the local order\nin which the svaras are used is rather ﬁxed. The totality\nof these musical characteristics can best be laid down in a\nset of phrases (‘calana’) which is a gestalt that is immedi-\nately recognizable to the expert. In a r ¯aga some phrases\nare obligatory while others are optional [26]. The former\nconstitute the core of the r ¯aga whereas the latter are elabo-\nrations or improvised versions. Speciﬁc ornamentation can\nadd to the distinctive quality of the r ¯aga [10].\nThere is a prescribed way in which a ‘khayal’ perfor-\nmance develops. The least mixed variety of khayal is that\nwhere an ¯al¯ap is sung, followed by a full sth ¯ayi (ﬁrst stanza\nof the composition) in madhya (medium) or drut (fast) laya\n(tempo), then layakari (rhythmic improvisation) and ﬁnally\nt¯an (fast vocal improvisation). When the barhat reaches the\nhigher (octave) Sa (root svara of Hindustani music), the an-\ntara (second stanza of the composition) is sung. If the com-\nposition is not preceded by an ¯al¯ap, the full development of\nthe r ¯aga is done through barhat. The composition is based\non the general lines of the r ¯aga, the development of the\nr¯aga is again based on the model of the composition [26].\nThere are four main sources of a pause in a melody,\nthese include: (i) gaps due to unvoiced consonants in the\nlyric, (ii) short breath pauses taken by the musician when\nout of breath, (iii) medium pauses where the musician\nshifts to a different phrase, and (iv) long pauses where the\naccompanying instruments improvise. Musically mean-\ningful or musician-intended melodic chunks are delimited\nonly by (iii) and (iv) [9].\nThough Hindustani music is often regarded as impro-\nvisatory, the improvisation is structured. On a broader\nlevel there is a well deﬁned structure within the space of606 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Predominant Melody Estimation Artist Tonic Normalization Breath-phrase (BP) Segmentation Stable Svara Transcription Pitch Histogram within BP Svara Duration Ratio Histogram MEC Feature Extraction \nAudio Data Pre-processing Segmentantion Histogram generation Figure 1 . Block diagram for data processing\nwhich an artist improvises following the r ¯aga grammar.\nThe overall skeleton of the melodies in Hindustani mu-\nsic can have deﬁned structure at different levels. Some of\nthese can be for the entire music tradition (for all r ¯agas\nand artists), some for speciﬁc r ¯agas, some of these can be\ngharana-speciﬁc, while some other artist speciﬁc. In this\nstudy, we aim to obtain overall structure in a melody and\nsome r ¯aga-speciﬁc patterns by the processing of audio con-\ncert recordings.\n3. DATA PROCESSING\nThe block diagram for data processing is shown in\nFigure 1. It contains four main processing modules: pre-\nprocessing, segmentation, histogram generation and fea-\nture extraction. We describe these modules in detail in the\nsubsequent sections.\n3.1 Pre-processing\n3.1.1 Predominant melody estimation\nWe start by estimating the pitch of the predominant\nmelodic source in the audio signal and regard that as the\nrepresentation of the melody. For predominant pitch es-\ntimation, we use the method proposed by Salamon and\nG´omez [23]. This method is reported to performed fa-\nvorably in MIREX 20113on a variety of music genres,\nincluding IAM, and has been used in several other stud-\nies [8, 16]. We use the implementation of this algorithm as\navailable in Essentia [2]. Essentia4is an open-source C++\nlibrary for audio analysis and content-based MIR. We use\nthe default values of the parameters, except for the frame\nand hop sizes, which are set to 46 and 4.44 ms, respec-\ntively.\n3.1.2 Tonic normalization\nThe base frequency chosen for a melody in a performance\nof IAM is the tonic pitch of the lead artist [13]. All other\n3http://www.music-ir.org/mirex/wiki/2011\n4https://github.com/MTG/essentiaaccompanying instruments are tuned with respect to this\ntonic pitch. Therefore, for analyzing features that are de-\nrived from the predominant melody across artists, the pre-\ndominant melody should be normalized with respect to the\ntonic pitch. For this normalization we consider the tonic\npitch ωas the reference frequency during the Hertz-to-\ncent-scale conversion, which is automatically identiﬁed us-\ning the multi-pitch approach proposed by Gulati et al. [13].\nThis approach is reported to obtain state-of-the-art results\nand has been successfully used elsewhere [12, 15].\n3.2 Melodic segmentation\n3.2.1 Breath-phrase segmentation\nAs described in Section 2 there are different types of un-\nvoiced segments in the predominant melody. While some\nof these segments are musically a part of a melodic phrase\n(short-pauses), some others delineate consecutive melodic\nphrases. A distribution of the duration of all the un-\nvoiced segments for the entire music collection revealed\nthat their type can be identiﬁed based on the duration of\nthe pause. For identifying intended breath pauses that\nseparate melodic phrases we empirically set the duration\nthreshold to be 500 ms. The duration of the intra-phrase\npauses is considerably smaller than this threshold. All the\nintra-phrase breath pauses (i.e., with duration smaller than\n500 ms) are interpolated using a cubic spline curve. We\nshall refer to a breath-phrase as BP hereafter.\n3.2.2 Stable svara transcription\nIn Indian art music, written notation has a purely prescrip-\ntive role. Transcribing the melody of a musical perfor-\nmance into a written notation is a challenging task and re-\nquires an in-depth knowledge of the r ¯aga [21, 29]. In this\nstudy we consider a simple melodic transcription that re-\ntains only the stable svara regions of a pitch contour and\ndiscards the transitory pitch regions. We ﬁrst detect all the\nvalid svaras and their precise frequencies used in a melody\nby computing a pitch histogram. Subsequently, we seg-\nment the stable svara regions by identifying the fragments\nof pitch contour that are within 35 cents [20] of the svara\nfrequencies. Next, we ﬁlter out the svara fragments that\nare smaller than 250 ms in duration, as they are too short to\nbe considered as perceptually meaningful held svaras [19].\nThis leaves a string of fragments each labeled by a svara.\nFragments with the same svara value that are separated by\ngaps less than 100 ms are merged [12]. The resulting sym-\nbol sequence thus comprises a tuple of svara name and du-\nration.\n3.3 Histogram generation\n3.3.1 Pitch histogram of breath-phrases\nWe compute the histogram of the transcribed svaras corre-\nsponding to each BP. Figure 2 shows the pitch histogram\nfor each BP of the concert of r ¯aga Todi sung by Ajoy\nChakrabarty. . The 12th bin along the y-axis corresponds\nto the tonic svara Sa (0 cents), 24th for its octave (1200\ncents).Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 607Figure 2 . Pitch histogram of svaras for each breath-phrase.\nFigure 3 . Bar graph of svara duration stacked in sorted\nmanner for each breath-phrase. We observe that breath-\nphrases often comprise one long nyas svara and several\nother svaras of less duration.\n3.3.2 Svara duration distribution\nWe consider the distribution of the svara duration for each\nBP. Figure 3 shows a stacked bar graph of the sorted du-\nrations of the svaras for each BP for the same concert.\nWe observe that the cumulative duration of the transcribed\nsvaras range from approximately 1 to 8 seconds. An im-\nportant point to note here is that there is a difference be-\ntween the absolute duration of a BP and the height of\nthe stacked bar (in Figure 3). This is caused by the tran-\nsient pitch segments that we ignored in our representation.\nReaders must note that the stable svara transcription, there-\nfore, has an implication for the further analyses.\n3.4 Post-processing\nTo capture the changes in the svara pattern at a broader\ntime-scale, we time-average the pitch histogram across ten\nBPs with a hop of one BP. This is followed by tracking\nof the most salient bin across the smoothened histogram\n. Finally, the obtained contour is further median ﬁltered\nwith one preceding and succeeding BP. We refer to this\ncontour as the evolution contour (hereafter EC). Figure 4\nshows the time-averaged histogram superimposed with the\nEC for the same concert.\n3.5 MEC feature extraction\nWe would like to compare the ECs of different concerts\nto explore whether there is any temporal trend that holds\nacross the collection. To normalize the time-scale and\npitch range of the EC, we normalize each EC within a unit\nFigure 4 . Time-averaged pitch histogram superimposed\nwith the evolution contour.\nrange in both temporal and pitch dimensions. Thus a mod-\niﬁed evolution contour (hereafter MEC) is obtained as:\nMEC =EC−min(EC)\nmax( EC)−min(EC)(1)\nwith 100 samples between [0,1] .\nWe extract a collection of heuristic features (slope-\nbased, duration-based, jump-based and level-based) from\nthe MEC. A few important features are: slope between\nthe MEC value of 0thframe and the ﬁrst frame where\nMEC = 1 (referred to as Slp) , proportion of duration\nspent on each svara (referred to as Pro), centroid (con-\nsidering salience of the bins as the weights in the centroid\ncomputation) of each svara (referred to as Cen) , starting\nand ending svaras, (second) longest svara and proportion\nof its duration, magnitude of lowest/highest jumps between\nconsecutive levels etc.\n4. MUSIC COLLECTION AND ANNOTATIONS\nThe music collection used in this study is taken from the\nHindustani music dataset (HMD) compiled as a part of\nthe CompMusic project [24, 25] (130 hours of commer-\ncially available audio recordings stored as 160 kbps mp3\nstereo audio ﬁles). All the editorial metadata for each\naudio recording is publicly available on an open-source\nmetadata repository called MusicBrainz5. The selected\nmusic material in our collection is diverse in terms of the\nnumber of artists (40), recordings (mostly live concerts of\nboth male and female renowned musicians from the last 6\ndecades) and the number of unique compositions (67). In\nthese terms, it can therefore be regarded as a representative\nsubset of real-world collections. Our collection includes a\ntotal of 75 concerts from 10 widely used r ¯agas (8 pieces\nper r ¯aga on an average) that are diverse both in terms of\nthe number of svaras and their pitch-classes (svarasth ¯an¯as).\nAll the concerts belong to either madhya or drut laya (and\nnon-metered ¯al¯ap). The pitch range of the recordings spans\napproximately two octaves (middle octave and half of the\nlower/upper octave). All of the concerts comprise elabora-\ntions based on a bandish.\nThe scope of the study is limited to only the ¯al¯ap and\nvist¯ar (barhat) [3, 4, 6, 18, 26] sections of the concert. Al-\nmost all of the concerts continue to subsequent fast im-\nprovisatory section (t ¯an) after rendering the vist ¯ar. The\n5https://musicbrainz.org/608 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016melodic cue where the antara ends and the rhythmic cue\nwhere there is a slight increase in tempo just after, is\nquite universal and musically unambiguous. For reference,\nplease observe Section 3.1 in [30]. We employ a perform-\ning Hindustani musician (trained over 20 years) to annotate\nthe time-stamps where the vist ¯ar ends. As the said anno-\ntation can be considered an obvious one (there is a less\nchance of getting subjective biases), we limit the manual\nannotation to one subject only. After cutting the concerts\nto the musician-annotated time-stamp, the average dura-\ntion per concert is 16 minutes making a total of 20 hours\nof data.\n5. ANALYSIS AND DISCUSSION\nWe choose certain music concepts which are widely dis-\ncussed among musicians and musicologists, for which\nthere has not yet been an objective way of interpreting\nthem from the audio. We cite the concepts (or knowledge-\nhypotheses, referred to as ’K’) and discuss how a data-\ndriven approach can help us validate them.\n5.1 K1: Evolution of melody in time\nAs discussed in Section 2, the barhat of a r ¯aga performance\nrefers to the gradual “unfolding” of a r ¯aga by building on\nthe svaras with a progression in a broad time-scale. But\nit is not very clearly illustrated in the musicology litera-\nture what the precise duration is spent on each svara in\ncourse of this progression. Figure 5 shows the MECs of\n37 (50% randomly chosen from our collection) concerts.\nWe observe that the MECs, in general, start from a lower\nsvara and gradually reach the highest svara in a step-like\nmanner. The slope Slpof MEC, is quite consistent (mean\n= 1.3, standard deviation = 0.34) over the whole corpus.\nThis gives an important insight that irrespective of the r ¯aga\nand concert-duration, artists take the same time to explore\nthe melody and hit the highest svara. This also reinforces\nthe nature of the time-scaling of a performance: for ei-\nther a short 20 minute- or a long 50 minute-concert, the\nmelodic organization bases more on relative and not ab-\nsolute time. We also observe a sharp fall of the MEC at\nthe end of the many concerts, this reﬂects how artists come\ndown to a lower svara to mark an end to the vist ¯ar (this co-\nincides with the musician’s annotation). This phenomenon\nhas a high resemblance with the time evolution of melody\nin course of the vist ¯ar, as shown in Figure 11 in [30].\n5.2 K2: Transitional characteristics of ny ¯as svaras\nR¯aga guidelines mention about allowed svara sequences\nwithin a melodic phrase but it would be interesting to see\nif artists maintain any speciﬁc order in choosing the ny ¯as\nsvara across BPs or take liberty to emphasize any other\nsvara. This is to be captured from the granularity of BPs\nand not in the time-averaged MEC. We generate a svara-\ntransition matrix and populate it with a uniform weight\nfor all transitions of the salient bins across BPs. Figure 6\nshows the salient svara-transition matrix where the diago-\nnal elements refer to self transitions. As indicative from\nFigure 5 . Modiﬁed evolution contours for 37 concerts in\nour music collection.\nFigure 6 . Svara-transition matrix of salient svaras of each\nbreath-phrase. Intensity of each bin is proportional to the\nnumber of transitions taken from the svara of bin index on\nx-axis to the svara of bin index on y-axis.\nwide steps of the MECs, there are quite a few self transi-\ntions but to our interest the salient transitions across BPs\nalso follow a pattern alike the allowed svara-transitions\nwithin a melodic phrase. This is not a trivial event. We\ncompute a feature to measure the steadiness quotient Stq\nof the transition matrix, deﬁned as the ratio of the trace of\nthe svara-transition matrix to the sum of all bins. We ob-\nserve a very low standard deviation (0.23) across our music\ncollection which conforms to the fact that artists ‘focus’ on\na ny¯as svara for consecutive BPs to establish that svara.\n5.3 K3: Relationship between functional roles of\nsvaras and their duration in melody\nWe discussed about functional roles of v ¯adi/samv ¯adi\nsvaras, but it is not explicitly known how their ‘promi-\nnence’ is deﬁned. Earlier work [5] use histogram and show\nthat they are one of the most used tonal pitches. But it is not\nevident from a pitch histogram whether the peak heights\nare contributed by a large number of ‘short’ svara segments\nor a fewer ‘long’ svara segments. Figure 7 shows the mean\n(left) and standard deviation (right) of all svaras (octave\nfolded) for each svara along x-axis being the salient svara\nin a BP. We observe that the role of each svara is deﬁned\nin terms of their duration in context of a ny ¯as svara. This\nconforms with the concepts discussed in Section 2. This\nalso reconﬁrms the well-deﬁned structure of the melodic\nimprovisation that any svara cannot be stretched arbitrarily\nlong, the ny ¯as svara of the BP and the phrase itself decides\nhow much variance all other svaras can incorporate. This\nalso brings out a question whether there is any special func-Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 609Figure 7 . Mean (left) and standard deviation (right) of all\nsvaras (octave folded) for each svara along x-axis being the\nsalient svara in a breath-phrase.\ntional role of v ¯adi/samv ¯adi svara in terms of their duration\nin a speciﬁc BP. One observation is that the v ¯adi/samv ¯adi\nsvara, while a salient svara in the respective BPs, constrain\nthe duration of all other svaras, making its relative promi-\nnence higher.\n5.4 K4: Duration and position of svaras in melody\nIn music theory, v ¯adi and samv ¯adi svaras are among the\nconcepts which are often discussed. But we do not have an\nobjective measure to observe how these svaras are different\nfrom other svaras in a r ¯aga. It is also interesting to know\nwhether the v ¯adi or samv ¯adi svara always takes a focal role\nirrespective of their location in a BP and the overall posi-\ntion in a performance.\nPosition in melody : An important feature of the MEC is\ntheCen. We observe that the Cen is a r ¯aga dependent\nfeature. E.g., an uttaranga v ¯adi r ¯aga would have its v ¯adi\ncentroid in the latter half of a concert. This is support-\nive of the fact that the v ¯adi is explored in due course of\nmelodic improvisation adhering to the trend observed as in\nSection 5.1. The musicological hypothesis that these are\nthe focal svaras of a r ¯aga does not necessarily imply that\nthese svaras are explored from the very beginning of the\nconcert. Rather the performance starts from a lower svara\n(graha svara) and reaches the v ¯adi in course of the gradual\ndevelopment of the melody.\nDuration in melody : We compute the average duration of\nall salient svaras per BP in two groups of svaras: (i) group\n1: v ¯adi/samv ¯adi, and (ii) group 2: rest. It is observed\non the whole corpus that Pro of group 1 is higher than\nthe group 2 for all r ¯agas. This reinforces the fact the\nterm ‘focus’ or ‘shine’ (that qualiﬁes v ¯adi) is manifested\nin the temporal dimension. This also brings out a question\nwhether we can predict the v ¯adi/samv ¯adi of a r ¯aga from\nthe melody by data-driven features. From the overall pitch\nhistogram it is difﬁcult to infer, but from our designed fea-\ntures, we observe an accuracy of 83% while predicting the\nv¯adi/samv ¯adi of a given raga.\n5.5 K5: Presence of possible pulsation in melody\nThere has been a discussion among musicians and musi-\ncologists whether there exists a pulsation in the melody of\nan¯al¯ap in Hindustani music. Musicians agree there is an\nimplicit pulsation present, but quantiﬁcation is left to sub-\njects. At the same time, the subjective bias only results\nFigure 8 . Ratio of inter-onset-interval of salient svaras\nacross breath-phrases. We see a tatum pulse (peak) at 0.8\nseconds and its harmonics.\nin an octave difference, i.e., there is a harmonic relation\namong the pace in which the subjects tap to the melody.\nWe propose a measure, through our data-driven approach,\nto estimate a possible metric for the pulsation. We as-\nsume that the pulse obtained from the melody would cor-\nrelate to the percussive pulsation. We compute the ratio of\ninter-onset-interval of the most salient svaras across BPs.\nFigure 8 shows a pulsation at 0.8 seconds and its harmon-\nics which correspond to 75 beats per minute (bpm) and the\npercussive tempo of the concert is approximately 40 bpm.\nThe noise in the estimate may also follow from a few short\nBPs (e.g., BP index 3, 7 etc.) as observed in Figure 3. This\nmeasure, therefore, needs further investigation before we\ngeneralize over the corpus.\n6. CONCLUSION AND FUTURE WORK\nIn this paper we performed a data driven exploration of\nimplicit structures in melodies of Hindustani music. We\noutlined the motivation and relevance of computational ap-\nproaches for quantitatively studying the underlying musi-\ncal concepts. We computed musically relevant and easy-\nto-interpret acoustic features such as svara frequency and\nsvara duration histogram. For computing these features we\nprimarily used existing tools and techniques often used in\ninformation retrieval of Indian art music. We performed a\nquantitative analysis of 75 music concerts in Hindustani\nmusic in 10 different r ¯agas. With that we showed how\nthe musical concepts are manifested in real-world perfor-\nmances and experimented with several ways to quantify\nthem. With this we also corroborate some of the inter-\nesting music concepts and discover implicit relationships\nbetween svaras and duration in the temporal evolution of a\nr¯aga performance. In the future, one possible research di-\nrection would be to use these ﬁndings for characterizing\nartist-speciﬁc aspects and highlighting different nuances\nacross gharanas in Hindustani music.\n7. ACKNOWLEDGEMENT\nThis work received partial funding from the European\nResearch Council under the European Union’s Sev-\nenth Framework Programme (FP7/2007-2013)/ERC grant\nagreement 267583 (CompMusic). Part of the work was\nsupported by Bharti Centre for Communication in IIT\nBombay.610 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20168. REFERENCES\n[1] S. Bagchee. N¯ad: Understanding R ¯aga Music . Busi-\nness Publications Inc, 1998.\n[2] D. Bogdanov, N. Wack, E. G ´omez, S. Gulati, P. Her-\nrera, O. Mayor, G. Roma, J. Salamon, J. Zapata, and\nX. Serra. Essentia: An audio analysis library for Music\nInformation Retrieval. In Proc. of Int. Soc. for Music\nInformation Retrieval (ISMIR) , pages 493–498, 2013.\n[3] J. Bor, S. Rao, W. van der Meer, and J. Harvey. The\nRaga Guide: A survey of 74 Hindustani ragas . Nimbus\nRecords with Rotterdam Conservatory of Music, 1999.\n[4] A. Chakrabarty. Shrutinandan: Towards Universal\nMusic . Macmillan, 2002.\n[5] P. Chordia and S. S ¸ent ¨urk. Joint recognition of raag and\ntonic in north Indian music. Computer Music Journal ,\n37(3):82–98, 2013.\n[6] A. Danielou. The ragas of Northern Indian music .\nMunshiram Manoharlal Publishers, New Delhi, 2010.\n[7] A. K. Dey. Nyasa in raga: The pleasant pause in Hin-\ndustani music . Kanishka Publishers, 2008.\n[8] S. Dutta and H. A. Murthy. Discovering typical motifs\nof a raga from one-liners of songs in Carnatic music.\nInInt. Soc. for Music Information Retrieval (ISMIR) ,\npages 397–402, Taipei, Taiwan, 2014.\n[9] K. K. Ganguli. Guruji Padmashree Pt. Ajoy\nChakrabarty: As I have seen Him. Samakalika\nSangeetham , 3(2):93–100, October 2012.\n[10] K. K. Ganguli. How do we ’See’ & ’Say’ a raga: A Per-\nspective Canvas. Samakalika Sangeetham , 4(2):112–\n119, October 2013.\n[11] K. K. Ganguli and P. Rao. Discrimination of melodic\npatterns in Indian classical music. In Proc. of Na-\ntional Conference on Communications (NCC) , Febru-\nary 2015.\n[12] K. K. Ganguli, A. Rastogi, V . Pandit, P. Kantan, and\nP. Rao. Efﬁcient melodic query based audio search for\nHindustani vocal compositions. In Proc. of Int. Soc. for\nMusic Information Retrieval (ISMIR) , pages 591–597,\nOctober 2015. Malaga, Spain.\n[13] S. Gulati, A. Bellur, J. Salamon, H. G. Ranjani, V . Ish-\nwar, H. A. Murthy, and X. Serra. Automatic tonic\nidentiﬁcation in Indian art music: approaches and\nevaluation. Journal of New Music Research (JNMR) ,\n43(1):55–73, 2014.\n[14] S. Gulati, J. Serr `a, K. K. Ganguli, and X. Serra. Land-\nmark detection in Hindustani music melodies. In Inter-\nnational Computer Music Conference, Sound and Mu-\nsic Computing Conference , pages 1062–1068, Athens,\nGreece, 2014.\n[15] S. Gulati, J. Serra, V . Ishwar, S. Senturk, and X. Serra.\nPhrase-based r ¯aga recognition using vector space mod-\neling. In IEEE Int. Conf. on Acoustics, Speech and\nSignal Processing (ICASSP) , pages 66–70, Shanghai,\nChina, 2016.[16] S. Gulati, J. Serr `a, V . Ishwar, and X. Serra. Mining\nmelodic patterns in large audio collections of Indian\nart music. In Int. Conf. on Signal Image Technology &\nInternet Based Systems (SITIS-MIRA) , pages 264–271,\nMorocco, 2014.\n[17] S. Gulati, J. Serr `a, and X. Serra. Improving melodic\nsimilarity in Indian art music using culture-speciﬁc\nmelodic characteristics. In Proc. of Int. Soc. for Mu-\nsic Information Retrieval (ISMIR) , pages 680–686,\nMalaga, Spain, 2015.\n[18] N. A. Jairazbhoy. The Rags of North Indian Music:\nTheir Structure & Evolution . Popular Prakashan, sec-\nond edition, 2011.\n[19] P. Rao, J. C. Ross, and K. K. Ganguli. Distinguishing\nraga-speciﬁc intonation of phrases with audio analysis.\nNinaad , 26-27(1):59–68, December 2013.\n[20] P. Rao, J. C. Ross, K. K. Ganguli, V . Pandit, V . Ishwar,\nA. Bellur, and H. A. Murthy. Classiﬁcation of melodic\nmotifs in raga music with time-series matching. Jour-\nnal of New Music Research (JNMR) , 43(1):115–131,\nApril 2014.\n[21] S. Rao and P. Rao. An overview of Hindustani music\nin the context of Computational Musicology. Journal\nof New Music Research (JNMR) , 43(1):24–33, April\n2014.\n[22] R. C. Repetto, R. Gong, N. Kroher, and X. Serra.\nComparison of the singing style of two jingju schools.\nInInternational Society for Music Information Re-\ntrieval (ISMIR) , pages 507–513, Malaga, Spain, Oc-\ntober 2015.\n[23] J. Salamon and E. G ´omez. Melody extraction from\npolyphonic music signals using pitch contour charac-\nteristics. IEEE Transactions on Audio, Speech, and\nLanguage Processing , 20(6):1759–1770, 2012.\n[24] X. Serra. A multicultural approach to music informa-\ntion research. In Proc. of Int. Conf. on Music Informa-\ntion Retrieval (ISMIR) , pages 151–156, 2011.\n[25] X. Serra. Creating research corpora for the compu-\ntational study of music: the case of the Compmusic\nproject. In Proc. of the 53rd AES Int. Conf. on Seman-\ntic Audio , London, 2014.\n[26] W. van der Meer. Hindustani music in the 20th century .\nMartin US Nijhoff Publishers, 1980.\n[27] P. Verma, T. P. Vinutha, P. Pandit, and P. Rao. Structural\nsegmentation of hindustani concert audio with poste-\nrior features. In Int.Conf. on Acoustics Speech and Sig-\nnal Processing (ICASSP) , pages 136–140. IEEE, 2015.\n[28] T. Viswanathan and M. H. Allen. Music in South India .\nOxford University Press, 2004.\n[29] R. Widdess. Involving the performers in transcription\nand analysis: a collaborative approach to Dhrupad.\nEthnomusicology , 38(1):59–79, 1994.\n[30] R. Widdess. Dynamics of melodic discourse in indian\nmusic: Budhaditya mukherjee’s ¯al¯ap in r ¯ag P ¯uriy¯a-\nKaly ¯an. pages 187–224, 2011.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 611"
    },
    {
        "title": "A Neural Greedy Model for Voice Separation in Symbolic Music.",
        "author": [
            "Patrick Gray",
            "Razvan C. Bunescu"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417251",
        "url": "https://doi.org/10.5281/zenodo.1417251",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/296_Paper.pdf",
        "abstract": "Music is often experienced as a simultaneous progression of multiple streams of notes, or voices. The automatic separation of music into voices is complicated by the fact that music spans a voice-leading continuum ranging from monophonic, to homophonic, to polyphonic, often within the same work. We address this diversity by defining voice separation as the task of partitioning music into streams that exhibit both a high degree of external perceptual sepa- ration from the other streams and a high degree of internal perceptual consistency, to the maximum degree that is pos- sible in the given musical input. Equipped with this task definition, we manually annotated a corpus of popular mu- sic and used it to train a neural network with one hidden layer that is connected to a diverse set of perceptually in- formed input features. The trained neural model greedily assigns notes to voices in a left to right traversal of the in- put chord sequence. When evaluated on the extraction of consecutive within voice note pairs, the model obtains over 91% F-measure, surpassing a strong baseline based on an iterative application of an envelope extraction function.",
        "zenodo_id": 1417251,
        "dblp_key": "conf/ismir/GrayB16",
        "content": "A NEURAL GREEDY MODEL FOR VOICE SEPARATION\nIN SYMBOLIC MUSIC\nPatrick Gray\nSchool of EECS\nOhio University, Athens, OH\npg219709@ohio.eduRazvan Bunescu\nSchool of EECS\nOhio University, Athens, OH\nbunescu@ohio.edu\nABSTRACT\nMusic is often experienced as a simultaneous progression\nof multiple streams of notes, or voices. The automatic\nseparation of music into voices is complicated by the fact\nthat music spans a voice-leading continuum ranging from\nmonophonic, to homophonic, to polyphonic, often within\nthe same work. We address this diversity by deﬁning voice\nseparation as the task of partitioning music into streams\nthat exhibit both a high degree of external perceptual sepa-\nration from the other streams and a high degree of internal\nperceptual consistency, to the maximum degree that is pos-\nsible in the given musical input. Equipped with this task\ndeﬁnition, we manually annotated a corpus of popular mu-\nsic and used it to train a neural network with one hidden\nlayer that is connected to a diverse set of perceptually in-\nformed input features. The trained neural model greedily\nassigns notes to voices in a left to right traversal of the in-\nput chord sequence. When evaluated on the extraction of\nconsecutive within voice note pairs, the model obtains over\n91% F-measure, surpassing a strong baseline based on an\niterative application of an envelope extraction function.\n1. INTRODUCTION AND MOTIV ATION\nThe separation of symbolic music into perceptually inde-\npendent streams of notes, i.e. voices or lines, is gener-\nally considered to be an important pre-processing step for\na number of applications in music information retrieval,\nsuch as query by humming (matching monophonic queries\nagainst databases of polyphonic or homophonic music)\n[13] or theme identiﬁcation [12]. V oice separation adds\nstructure to music and thus enables the implementation of\nmore sophisticated music analysis tasks [17]. Depending\non their deﬁnition of voice, existing approaches to voice\nseparation in symbolic music can be organized in two main\ncategories: 1) approaches that extract voices as mono-\nphonic sequences of successive non-overlapping musical\nnotes [5, 6, 8, 11, 14, 16, 17]; and 2) approaches that al-\nlow voices to contain simultaneous note events, such as\nc/circlecopyrtPatrick Gray, Razvan Bunescu. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: Patrick Gray, Razvan Bunescu. “A Neural Greedy Model\nfor V oice Separation in Symbolic Music”, 17th International Society for\nMusic Information Retrieval Conference, 2016.chords [4, 9, 10, 15]. Approaches in the ﬁrst category typ-\nically use the musicological notion of voice that is refer-\nenced in the voice-leading rules of the Western musical tra-\ndition, rules that govern the horizontal motion of individual\nvoices from note to note in successive chords [1, 4]. Start-\ning with [4], approaches in the second category break with\nthe musicological notion of voice and emphasize a percep-\ntual view of musical voice that corresponds more closely to\nthe notion of independent auditory streams [2, 3]. Orthog-\nonal to this categorization, a limited number of voice sep-\naration approaches are formulated as parametric models,\nwith parameters that are trained on music already labeled\nwith voice information [6, 8, 11].\nIn this paper, we propose a data-driven approach to\nvoice separation that preserves the musicological notion\nof voice. Our aim is to obtain a segregation of music\ninto voices that would enable a downstream system to de-\ntermine whether an arbitrary musical input satisﬁes the\nknown set of voice-leading rules, or conversely identify\nplaces where the input violates voice-leading rules.\n2. TASK DEFINITION\nAccording to Huron [7], “the principal purpose of voice-\nleading is to create perceptually independent musical\nlines”. However, if a voice is taken to be a monophonic\nsequence of notes, as implied by traditional voice-leading\nrules [1], then not all music is composed of independent\nmusical lines. In homophonic accompaniment, for exam-\nple, multiple musical lines (are meant to) fuse together\ninto one perceptual stream. As Cambouropoulos [4] ob-\nserves for homophonic accompaniment, “traditional voice-\nleading results in perceivable musical texture , not indepen-\ndent musical lines”. In contrast with the traditional no-\ntion of voice used in previous voice separation approaches,\nCambouropoulos redeﬁnes in [4] the task of ’voice’ sepa-\nration as that of separating music into perceptually inde-\npendent musical streams , where a stream may contain two\nor more synchronous notes that are perceived as fusing in\nthe same auditory stream. This deﬁnition is used in [9, 15]\nto build automatic approaches for splitting symbolic music\ninto perceptually independent musical streams.\nSince a major aim of our approach is to enable build-\ning “musical critics” that automatically determine whether\nan arbitrary musical input obeys traditional voice-leading\nrules, we adopt the musicological notion of voice as a782monophonic sequence of non-overlapping notes . This def-\ninition however leads to an underspeciﬁed voice separa-\ntion task: for any non-trivial musical input, there usually\nis a large number of possible separations into voices that\nsatisfy the constraints that they are monophonic and con-\ntain notes in chronological order that do not overlap. Fur-\nther constraining the voices to be perceptually independent\nwould mean the deﬁnition could no longer apply to music\nwith homophonic textures, as Cambouropoulos correctly\nnoticed in [4]. Since we intend the voice separation ap-\nproach to be applicable to arbitrary musical input, we in-\nstead deﬁne voice separation as follows:\nDeﬁnition 1. Voice separation is the task of partition-\ning music into monophonic sequences (voices) of non-\noverlapping notes that exhibit both a high degree of exter-\nnal perceptual separation from the other voices and a high\ndegree of internal perceptual consistency, to the maximum\ndegree that is possible in the given musical input.\nFigure 1 . Example voice separation from “Earth Song”.\nFigure 1 shows a simple example of voice separation\nobtained using the deﬁnition above. While the soprano and\nbass lines can be heard as perceptually distinct voices, we\ncannot say the same for the tenor and alto lines shown in\ngreen and red, respectively. However, clear perceptual in-\ndependence is not needed under the new task deﬁnition.\nThe two intermediate voices exhibit a high degree of per-\nceptual consistency: their consecutive notes satisfy to a\nlarge extent the pitch proximity and temporal continuity\nprinciples needed to evoke strong auditory streams [7]. In-\ndeed, when heard in isolation, both the tenor and the alto\nare heard as continuous auditory streams, the same streams\nthat are also heard when the two are played together. The\ntwo streams do not overlap, which helps with perceptual\ntracking [7]. Furthermore, out of all the streaming possi-\nbilities, they also exhibit the largest possible degree of ex-\nternal perceptual separation from each other and from the\nother voices in the given musical input.\n3. ANNOTATION GUIDELINES\nAccording to the deﬁnition in Section 2, voice separation\nrequires partitioning music into monophonic sequences of\nnon-overlapping notes that exhibit a high degree of percep-\ntual salience, to the maximum extent that is possible in the\ngiven musical input. As such, an overriding principle that\nwe followed during the manual annotation process was to\nalways give precedence to what was heard in the music,\neven when this appeared to contradict formal perceptualprinciples, such as pitch proximity. Furthermore, when-\never formal principles seemed to be violated by percep-\ntual streams, an attempt was made to explain the apparent\nconﬂict. Providing justiﬁcations for non-trivial annotation\ndecisions enabled reﬁning existing formal perceptual prin-\nciples and also informed the feature engineering effort.\nListening to the original music is often not sufﬁcient\non its own for voice separation, as not all the voices con-\ntained in a given musical input can be distinctly heard. Be-\ncause we give precedence to perception, we ﬁrst annotated\nthose voices that could be distinguished clearly in the mu-\nsic, which often meant annotating ﬁrst the melodic lines\nin the soprano and the bass. When the intermediate voices\nwere difﬁcult to hear because of being masked by more\nsalient voices, one simple test was to remove the already\nannotated most prominent voice (often the soprano [1])\nand listen to the result. Alternatively, when multiple con-\nﬂicting voice separations were plausible, we annotated the\nvoice that, after listening to it in isolation, was easiest to\ndistinguish perceptually in the original music.\nFigure 2 shows two examples where the perceptual\nprinciple of pitch proximity appears to conﬂict with what\nis heard as the most salient voice. In the ﬁrst measure,\nthe ﬁrst D 4note can continue with any of the 3 notes in\nthe following I6chord. However, although the bass note\nin the chord has the same pitch, we hear the ﬁrst D 4most\nsaliently as part of the melody in the soprano. The D 4\ncan also be heard as creating a musical line with the next\nD4notes in the bass, although less prominently. The least\nsalient voice assignment would be between the D 4and the\nintermediate line that starts on the following G 4. While we\nannotate all these streaming possibilities (as shown in Fig-\nure 7), we mark the soprano line assignment as the most\nsalient for the D 4. Similarly, in the last chord from the sec-\nond measure from Figure 2, although E 4is closer to the\nprevious F 4, it is the G 4that is most prominently heard as\ncontinuing the soprano line. This was likely reinforced by\nthe fact that the G 4in the last chord was “prepared” by the\nG4preceding F 4.\nFigure 2 . V oice separation annotations, for measures 5 in\n“Knockin’ on Heaven’s Door” and 12 in “Let It Be”.\nOther non-trivial annotation decisions, especially in the\nbeginning of the annotation effort, involved whether two\nstreams should be connected or not. Overall, we adopted\nthe guideline that we should break the music into fewer and\nconsequently longer voices, especially if validated percep-\ntually. Figure 3, for example, shows the A 4in the third\nmeasure connected to the following C 5. Even though the\ntwo notes are separated by a quarter rest, they are heard as\nbelonging to the same stream, which may also be helped\nby the relatively long duration of A 4and by the fact that\nthe same pattern is repeated in the piece. We have also dis-Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 783Figure 3 . V oice separation annotation in the treble for\nmeasures 38-41 in “Count on Me”.\ncovered that “preparation” through previous occurrences\nof the same note or notes one octave above or below can\nsigniﬁcantly attenuate the effect of a large pitch distance\nand thus help with connecting the note to an active stream.\nThis effect is shown in Figure 4, where the voice in the ﬁrst\nmeasure is most prominently heard as continuing with the\nB4in the second measure.\nFigure 4 . V oice separation annotation in the treble for\nmeasures 26-27 in “A Thousand Miles”.\nSometimes, the assignment of a note to one of the avail-\nable active voices is hard to make due to inherent musi-\ncal ambiguity. An example is shown in Figure 5, where\nit is hard to determine if the A 4in the second measure\nconnects to the top C 6or the C 5one octave below. Af-\nter being played separately, each voice assignment can be\ndistinguished perceptually in the original music. The C 5is\ncloser in pitch to the A 4and it is also in a range with better\ndeﬁned pitch sensations than the C 6. On the other hand,\nthe pitch distance between the upper C 6and the A 4is at-\ntenuated by the synchronous C 5. Eventually we annotated\nA4as connecting to the slightly more salient C 5, but also\nmarked it as ambiguous between the two C notes.\nFigure 5 . V oice separation annotation in the treble for\nmeasures 62-63 in “A Thousand Miles”.\nOther examples of harmony inﬂuencing voice assign-\nment involve the seventh scale degree notes appearing in\nVII and VII6chords. As shown in Figure 6, when such a\nchord is ﬁrst used, the ˆ7note does connect to any of the\nprevious streams, despite the closer pitch proximity.\n4. VOICE SEPARATION DATASET\nWe compiled a corpus of piano versions of 20 popular\ncompositions of varying complexity that are representative\nof many genres of music. Each song was downloaded from\nwww.musescore.com and converted to MusicXML. In se-\nlecting music, we followed a few basic criteria. First, we\navoided collecting piano accompaniments and gave pref-\nerence to piano renditions that sounded as much as pos-\nsible like the original song. Among other things, this en-\nsured that each score contained at least one clearly deﬁned\nFigure 6 . V oice separation annotation in the bass for mea-\nsures 26-28 in “Earth Song”.\nmelody. Second, we collected only tonal music. Atonal\nmusic is often comprised of unusual melodic structures,\nwhich were observed to lead to a poor perception of voices\nby the annotators. Following the annotation guidelines, we\nmanually labeled the voice for each note in the dataset. The\nannotations will be made publicly available. The names\nof the 20 musical pieces are shown in Table 1, together\nwith statistics such as the total number of notes, number\nof voices, average number of notes per voice, number of\nwithin-voice note pairs, number of unique note onsets, and\naverage number of notes per chord. The 20 songs were\nmanually annotated by the ﬁrst author; additionally, the\n10 songs marked with a star were also annotated by the\nsecond author. In terms of F-measure, the inter-annotator\nagreement (ITA) on the 10 songs is 96.08% (more de-\ntailed ITA numbers are shown in Table 2). The last col-\numn shows the (macro-averaged) F-measure of our neural\ngreedy model, to be discussed in Section 6. As can be\nseen in Table 1, the number of voices varies widely, rang-\ning between 4 for Greensleeves to 123 for 21 Guns, the\nlongest musical composition, with a variable musical tex-\nture and frequent breaks in the harmonic accompaniment\nof the melody. The last line shows the same total/average\nstatistics for the ﬁrst 50 four-part Bach Chorales available\nin Music21, for which we use the original partition into\nvoices, without the duplication of unisons.\n5. THE VOICE SEPARATION MODEL\nTo separate a musical input into its constituent voices, we\nﬁrst order all the notes based on their onsets into a se-\nquence of chordsC={c1,c2,...,cT}, where a chord is\ndeﬁned to be a maximal group of notes that have the same\nonset. Assignment of notes to voices is then performed in\nchronological order, from left to right, starting with the ﬁrst\nchordc1. Because voices are by deﬁnition monophonic,\neach note in the ﬁrst chord is considered to start a sepa-\nrate, new voice. These ﬁrst voices, together with an empty\nvoice/epsilon1, constitute the initial set of active voicesV. At each\nonsett, the algorithm greedily assigns a note nfrom the\ncurrent chord ctto one of the voices in the active set by se-\nlecting the active voice vthat maximizes a trained assign-\nment probability p(n,v), i.e.v(n) = arg maxv∈ˆVp(n,v).\nNotes from the current chord are assigned to voices in the\norder of their maximal score p(n,v(n)). If a note is as-\nsigned to the empty voice, then a new voice is added to\nthe active set. The set of candidate active voices ˆVavail-\nable for any given note nis a subset of active voices V\nconstrained such that assigning nto any of the voices in\nˆVwould not lead to crossing voices or to multiple syn-\nchronous notes being assigned to the same voice.784 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Popular Music dataset # Notes # V oices # N / V # Pairs # Onsets Synchronicity F-measure\n21 Guns (Green Day) 1969 123 16.01 1801 666 2.96 86.24\nApples to the Core (Daniel Ingram) 923 29 31.83 892 397 2.32 77.67\nCount on Me (Bruno Mars) 775 11 70.45 764 473 1.64 97.22\nDreams (Rogue)∗615 12 51.25 603 474 1.30 98.32\nEarth Song (Michael Jackson)∗431 15 28.73 416 216 2.00 93.27\nEndless Love (Lionel Richie) 909 23 39.52 886 481 1.89 96.52\nForest (Twenty One Pilots) 1784 89 20.04 1695 1090 1.64 91.93\nFur Elise (Ludwig van Beethoven)∗900 77 11.69 823 653 1.38 91.98\nGreensleeves∗231 4 57.75 213 72 3.21 92.88\nHow to Save a Life (The Fray)∗440 13 33.85 427 291 1.51 98.11\nHymn for the Weekend (Coldplay) 1269 50 25.38 1218 706 1.80 92.30\nKnockin’ on Heaven’s Door (Bob Dylan)∗355 41 8.66 312 180 1.97 90.92\nLet It Be (The Beatles)∗563 22 25.59 540 251 2.24 87.29\nOne Call Away (Charlie Puth) 993 56 17.73 937 505 1.97 91.33\nSee You Again (Wiz Khalifa)∗704 66 10.67 638 359 1.96 81.16\nTeenagers (My Chemical Romance) 315 18 17.50 297 145 2.17 91.39\nA Thousand Miles (Vanessa Carlton)∗1001 61 16.41 937 458 2.19 96.61\nTo a Wild Rose (Edward Macdowell) 307 20 15.35 287 132 2.33 88.72\nUptown Girl (Billy Joel) 606 46 13.17 560 297 2.04 93.41\nWhen I Look at You (Miley Cyrus)∗1152 82 14.05 1067 683 1.69 92.92\nTotals & Averages 16242 42.90 26.28 15313 8529 2.01 91.51\nBach Chorales dataset 12282 4 61.41 11874 4519 2.73 95.47\nTable 1 . Statistics for the Popular Music dataset and the Bach Chorales dataset.\nThe assignment probability p(n,v)captures the com-\npatibility between a note nand an active voice v. To com-\npute it, we ﬁrst deﬁne a vector Φ(n,v)of perceptually in-\nformed compatibility features (Section 5.2). The probabil-\nity is then computed as p(n,v) =σ(wThW(n,v)), where\nσis the sigmoid function and hW(n,v)is the vector of\nactivations of the neurons on the last (hidden) layer in a\nneural network with input Φ(n,v).\nTo train the network parameters θ= [w,W], we maxi-\nmize the likelihood of the training data:\nˆθ= arg max\nθT/productdisplay\nt=1/productdisplay\nn∈ct/productdisplay\nv∈ˆVp(n,v|θ)l(n,v)(1−p(n,v|θ))1−l(n,v)\n(1)\nwherel(n,v)is a binary label that indicates whether or not\nnotenwas annotated to belong to voice vin the training\ndata. This formulation of the objective function is ﬂexible\nenough to be used in 2 types of voice separation scenarios:\n1.Ranking : Assign a note to the top-ranked candidate\nactive voice, i.e. v(n) = arg max\nv∈ˆVp(n,v).\n2.Multi-label classiﬁcation : Assign a note to all can-\ndidate active voices whose assignment probability is\nlarge enough, i.e. V(n) ={v∈ˆV|p(n,v)>0.5}.\nThe ﬁrst scenario is the simplest one and rests on the work-\ning assumption that a note can belong to a single voice.\nThe second scenario is more general and allows a note to\nbelong to more than one voice. Such capability would be\nuseful in cases where a note is heard simultaneously as\npart of two musical streams. Figure 7, for example, shows\nthe voice separation performed under the two scenarios for\nthe same measure. In the ranking approach shown on the\nleft, we label the second F 4as belonging to the soprano\nvoice. Since in this scenario we can assign a note to just\none voice, we select the voice assignment that is heard as\nthe most salient, which in this case is the soprano. In themulti-label approach shown on the right, we label the sec-\nond F 4as belonging to both active voices, since the note is\nheard as belonging to both. In the experiments that we re-\nFigure 7 . Two voice separation scenarios, for measure 16\nfrom “A Thousand Miles”.\nport in this paper (Section 6), we used the simpler ranking\napproach, leaving the more general multi-label approach\nfor future work.\n5.1 Iterative Envelope Extraction\nWe also propose a baseline system for voice-separation\nthat iteratively extracts the upper envelope i.e. the topmost\nmonophonic sequence of non-overlapping notes. Figure 8\nshows how the iterative envelope extraction process works\non the second measure from Figure 2, copied here for read-\nability. The top left measure is the original measure from\nFigure 8 . V oice separation as iterative envelope extraction.\nFigure 2 and we use it as the current input. Its upper en-\nvelope is shown in the bottom left measure, which will be-\ncome the ﬁrst voice. After extracting the ﬁrst voice from\nthe input, we obtain the second measure in the top staff,\nwhich is now set to be the current input. We again applyProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 785the same envelope extraction process to obtain the second\nvoice, shown in the second measure on the bottom staff.\nAfter extracting the second voice from the current input,\nwe obtain a new current input, shown in the third measure\non the top staff. Extracting the third voice from the current\ninput results in an empty set and correspondingly the base-\nline algorithm stops. For this input, the baseline extracted\nvoice 1 without errors, however it made a mistake in the\nlast note assignment for voice 2.\n5.2 Voice Separation Features\nThe assignment probability p(n,v)is computed by the\nneural model based on a vector of input features Φ(n,v) =\n[φ0,φ1,...,φK]that will be described in this section, using\nv.last to denote the last note in the active voice v.\n5.2.1 Empty Voice Feature\nThe empty voice feature φ0is set to 1 only for the empty\nvoice, i.e.φ0(n,/epsilon1) = 1 andφ0(n,v) = 0,∀v/negationslash=/epsilon1. All the\nremaining features in any feature vector for an empty voice\nΦ(n,/epsilon1)are set to zero. This allows the empty voice to ac-\ntivate a bias parameter w0, which is equivalent to learning\na threshold−w0that the weighted combination of the re-\nmaining features must exceed in order for the note to be as-\nsigned to an existing, non-empty, active voice. Otherwise,\nthe notenwill be assigned to the empty voice, meaning it\nwill start a new voice.\n5.2.2 Pitch and Pitch Proximity Features\nAccording to Huron’s formulation of the pitch proximity\nprinciple, the coherence of an auditory stream is main-\ntained by close pitch proximity in successive tones within\nthe stream [7]. Correspondingly, we deﬁne a pitch prox-\nimity feature φ1(n,v) =pd(n,v.last ) =|ps(n)−\nps(v.last )|to be the absolute distance in half steps be-\ntween the pitch space representations of notes nandv.last .\nThe pitch proximity feature enables our system to quickly\nlearn that notes rarely pair with voices lying at intervals\nbeyond an octave. We also add two features φ2(n,v) =\nps(n)andφ3(n,v) =ps(v.last )that capture the absolute\npitch of the note nandv.last . Pitch values are taken from\na pitch space in which C 4has value 60 and a difference of\n1 corresponds to one half step, e.g. C 5has value 72. Us-\ning absolute pitches as separate input features will enable\nneurons on the hidden layer to discover possibly unknown\npitch-based rules for perceptual streaming.\n5.2.3 Temporal and Temporal Continuity Features\nWe deﬁne an inter-onset feature φ4(n,v)as the tempo-\nral distance between the note onsets of nandv.last . An\nadditional feature φ5(n,v)is computed as the temporal\ndistance between the note onset of nand the note offset\n(the time when a note ends) of v.last . These complemen-\ntary features help our system model both acceptable rest\nlengths between notes and the gradual dissipation of note\nsalience throughout the duration of a note.Notes that lie between the onsets of v.last andnmay\ninﬂuence the voice assignment. Thus, we appropriately de-\nﬁne a feature φ6(n,v)as the number of unique onsets\nbetween the onsets of v.last andn. We also deﬁne two\nfeaturesφ7(n,v) =qd(n)andφ8(n,v) =qd(v.last )\nfor the durations of nandv.last , respectively, where note\ndurations are measured relative to the quarter note. These\nfeatures, when combined in the hidden layer, enable the\nsystem to learn to pair notes that appear in common dura-\ntion patterns, such as dotted quarter followed by an eighth.\n5.2.4 Chordal Features\nNotes that reside in the soprano either alone or at the\ntop of a chord tend to be heard as the most salient. As\na result, the most prominent melodic line of a score of-\nten navigates through the topmost notes, even in situa-\ntions where a candidate active voice lies closer in pitch\nto the alto or tenor notes of the current chord. Notes in\na low bass range that stand alone or at the bottom of a\nchord exhibit a similar behavior. To enable the learning\nmodel to capture this perceptual effect, we deﬁne two fea-\nturesφ9(n,v) =cp(n)andφ10(n,v) =cp(v.last )to\nmark the relative positions of nandv.last in their respec-\ntive chords, where the chord position number (cp) starts\nat 0 from the top of a chord. To place chord positions\ninto the appropriate context, we deﬁne φ11(n,v)as the\nnumber of notes in n’s chord and φ12(n,v)as the num-\nber of notes in v.last ’s chord. For more direct compar-\nisons between notes in n’s chord and the active voice, we\ncalculate pitch proximities ( pd) betweenv.last andn’s\nupper and lower neighbors n.above andn.below . Thus,\nwe deﬁne the features φ13(n,v) =pd(v.last,n.above )\nandφ14(n,v) =pd(v.last,n.below ). We also add the\nfeaturesφ15(n,v) =pd(n,n.above )andφ16(n,v) =\npd(n,n.below )to encode the intervals between nand its\nchordal neighbors.\n5.2.5 Tonal Features\nWe use scale degrees φ17(n,v) =sd(n)andφ18(n,v) =\nsd(v.last )of the notes nandv.last as features in order to\nenable the model to learn melodic intervals that are most\nappropriate in a given key. For example, if a candidate ac-\ntive voice ends on a leading tone, then it is likely to resolve\nto the tonic. We also deﬁne a feature φ19(n,v)for the\ninterval between the note nand the root of its chord, and\nsimilarly, a feature φ20(n,v)for the interval between the\nnotev.last and the root of its chord.\nThe last tonal feature φ21(n,v)is a Boolean feature that\nis set to 1 if the note v.last in the active voice vappears in a\ntonic chord at a cadence. Tonic chords at cadences induce\na sense of ﬁnality [1], which could potentially break the\nvoice from the notes that follow.\n5.2.6 Pseudo-polyphony Features\nIn pseudo-polyphony, two perceptually independent\nstreams are heard within a rapidly alternating, monophonic\nsequence of notes separated by relatively large pitch inter-\nvals. Figure 9 presents an example of pseudo-polyphony.786 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Dataset ModelAll within-voice pairs of notes Exclude pairs of notes separated by rests\nJaccard Precision Recall F-measure Jaccard Precision Recall F-measure\nPopular MusicBaseline 59.07 74.51 74.03 74.27 67.55 80.48 80.79 80.64\nNGModel 83.55 92.08 90.01 91.03 85.73 92.74 91.89 92.31\nITA 92.45 94.96 97.21 96.08 93.04 95.12 97.70 96.41\nBach ChoralesBaseline 87.25 93.34 93.04 93.18 87.62 93.22 93.58 93.39\nNGModel 91.36 95.59 95.37 95.47 91.66 95.91 95.39 95.64\nTable 2 . Comparative results of Neural Greedy (NG) Model vs. Baseline on Popular Music and Bach Chorales; Inter-\nAnnotator (ITA) results on the subset of 10 popular songs shown in Table 1.\nAlthough the offset of each D 4note is immediately fol-\nlowed by the onset of the next note, the often large inter-\nvals and the fast tempo break the upper and lower notes\ninto two perceptually independent streams.\nFigure 9 . Example pseudo-polyphony from ”Forest”.\nWe model this phenomenon by introducing three fea-\ntures to the neural system. In designing these features,\nwe ﬁrst employ the envelope extraction method described\nin Section 5.1 to gather monophonic sequences of non-\noverlapping notes. We next ﬁnd the maximal contiguous\nsubsequences with an alternating up-down pattern of di-\nrection changes, like the one shown in Figure 9. The ﬁrst\nfeatureφ22(n,v) =apv(n)is set to be the alternating\npath value ( apv) of the note n, which is 0 if nis not on an\nalternating path, 1 if it is in the lower part of an alternating\npath, and 2 if it is in the upper part of an alternating path.\nSimilarly, we deﬁne φ23(n,v) =apv(v.last )to be the al-\nternating path value of the note v.last . The third feature is\nset to 1 if both nandv.last have the same alternating path\nvalue, i.e.φ24(n,v) = 1[apv(n) =apv(v.last )].\n6. EXPERIMENTAL EV ALUATION\nWe implemented the neural greedy model as a neural net-\nwork with one hidden layer, an input layer consisting of\nthe feature vector Φ(n,v), and an output sigmoid unit that\ncomputes the assignment probability p(n,v|θ). The net-\nwork was trained to optimize a regularized version of the\nlikelihood objective shown in Equation 1 using gradient\ndescent and backpropagation. The model was trained and\ntested using 10-fold cross-validation. For evaluation, we\nconsidered pairs of consecutive notes from the voices ex-\ntracted by the system and compared them with pairs of\nconsecutive notes from the manually annotated voices. Ta-\nble 2 shows results on the two datasets in terms of the Jac-\ncard similarity between the system pairs and the true pairs,\nprecision, recall, and micro-averaged F-measure. Preci-\nsion and recall are equivalent to the soundness and com-\npleteness measures used in [6, 11]. We also report results\nfor which pairs of notes separated by rests are ignored.\nThe results show that the newly proposed neural model\nperforms signiﬁcantly better than the envelope baseline,Dataset Model Precision Recall F-measure\n10 Fugues[6] 94.07 93.42 93.74\nNGModel 95.56 92.24 93.87\n30 Inv. 48 F.[14] 95.94 70.11 81.01\nNGModel 95.91 93.83 94.87\nTable 3 . Comparative results on Bach datasets.\nespecially on popular music. When pairs of notes sepa-\nrated by rests are excluded from evaluation, the baseline\nperformance increases considerably, likely due to the ex-\nclusion of pseudo-polyphonic passages.\nClose to our model is the data-driven approach from [6]\nfor voice separation in lute tablature. Whereas we adopt\na ranking approach and use as input both the note and the\ncandidate active voice, [6] use only the note as input and\nassociate voices with the output nodes. Therefore, while\nour ranking approach can label music with a variable num-\nber of voices, the classiﬁcation model from [6] can extract\nonly a ﬁxed number of voices. Table 3 shows that our neu-\nral ranking model, although not speciﬁcally designed for\nmusic with a ﬁxed number of voices, performs compet-\nitively with [6] when evaluated on the same datasets of\n10 Fugues by Bach. We also compare the neural rank-\ning model with the the approach from [14] on a different\ndataset containing 30 inventions and 48 fugues1.\n7. CONCLUSION AND FUTURE WORK\nWe presented a neural model for voice separation in sym-\nbolic music that assigns notes to active voices using a\ngreedy ranking approach. The neural network is trained\non a manually annotated dataset, using a perceptually-\ninformed deﬁnition of voice that also conforms to the mu-\nsicological notion of voice as a monophonic sequence of\nnotes. When used with a rich set of note-voice features,\nthe neural greedy model outperforms a newly introduced\nstrong baseline using iterative envelope extraction. In fu-\nture work we plan to evaluate the model in the more gen-\neral multi-label classiﬁcation setting that allows notes to\nbelong to multiple voices.\nWe would like to thank the anonymous reviewers for\ntheir helpful remarks and Mohamed Behairy for insightful\ndiscussions on music cognition.\n1In [14] it is stated that soundness and completeness “as suggested\nby Kirlin [11]” were used for evaluation; however, the textual deﬁnitions\ngiven in [14] are not consistent with [11]. As was done in [6], for lack of\nan answer to this inconsistency, we present the metrics exactly as in [14].Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 7878. REFERENCES\n[1] E. Aldwell, C. Schachter, and A. Cadwallader. Har-\nmony and Voice Leading . Schirmer, 4 edition, 2011.\n[2] A. S. Bregman. Auditory Scene Analysis: The Per-\nceptual Organization of Sound . The MIT Press, Cam-\nbridge, MA, 1990.\n[3] A. S. Bregman and J. Campbell. Primary Auditory\nStream Segregation and Perception of Order in Rapid\nSequences of Tones. Journal of Experimental Psychol-\nogy, 89(2):244–249, 1971.\n[4] E. Cambouropoulos. ‘V oice’ Separation: Theoretical,\nPerceptual, and Computational Perspectives. In Pro-\nceedings of the 9thInternational Conference on Music\nPerception and Cognition , pages 987–997, Bologna,\nItaly, 2006.\n[5] E. Chew and X. Wu. Separating V oices in Polyphonic\nMusic: A Contig Mapping Approach. In Computer\nMusic Modeling and Retrieval: 2ndInternational Sym-\nposium , pages 1–20, 2004.\n[6] R. de Valk, T. Weyde, and E. Benetos. A Machine\nLearning Approach to V oice Separation in Lute Tab-\nlature. In Proceedings of the 14thInternational Soci-\nety for Music Information Retrieval Conference , pages\n555–560, Curitiba, Brazil, 2013.\n[7] D. Huron. Tone and V oice: A Derivation of the Rules\nof V oice-Leading from Perceptual Principles. Music\nPerception , 19(1):1–64, 2001.\n[8] A. Jordanous. V oice Separation in Polyphonic Music:\nA Data-Driven Approach. In Proceedings of the Inter-\nnational Computer Music Conference , Belfast, Ireland,\n2008.\n[9] I. Karydis, A. Nanopoulos, A. N. Papadopoulos,\nand E. Cambouropoulos. VISA: The V oice Integra-\ntion/Segregation Algorithm. In Proceedings of the 8th\nInternational Society for Music Information Retrieval\nConference , pages 445–448, Vienna, Austria, 2007.\n[10] J. Kilian and H. Hoos. V oice Separation: A Local Op-\ntimization Approach. In Proceedings of the 3rdInter-\nnational Society for Music Information Retrieval Con-\nference , pages 39–46, Paris, France, 2002.\n[11] P. B. Kirlin and P. E. Utgoff. V oiSe: Learning to Seg-\nregate V oices in Explicit and Implicit Polyphony. In\nProceedings of the 6thInternational Society for Mu-\nsic Information Retrieval Conference , pages 552–557,\nLondon, England, 2005.\n[12] O. Lartillot. Discovering Musical Patterns Through\nPerceptive Heuristics. In Proceedings of the 4thInter-\nnational Society for Music Information Retrieval Con-\nference , pages 89–96, Washington D.C., USA, 2003.[13] K. Lemstrom and J. Tarhio. Searching Monophonic\nPatterns within Polyphonic Sources. In Proceedings\nof the 6thConference on Content-Based Multimedia\nInformation Access , pages 1261–1279, Paris, France,\n2000.\n[14] S. T. Madsen and G. Widmer. Separating V oices in\nMIDI. In Proceedings of the 7thInternational Society\nfor Music Information Retrieval Conference , pages 57–\n60, Victoria, Canada, 2006.\n[15] D. Rafailidis, E. Cambouropoulos, and Y . Manolopou-\nlos. Musical V oice Integration/Segregation: VISA Re-\nvisited. In Proceedings of the 6thSound and Music\nComputing Conference , pages 42–47, Porto, Portugal,\n2009.\n[16] D. Rafailidis, A. Nanopoulos, E. Cambouropoulos,\nand Y . Manolopoulos. Detection of Stream Segments\nin Symbolic Musical Data. In Proceedings of the 9th\nInternational Society for Music Information Retrieval\nConference , pages 83–88, Philadelphia, PA, 2008.\n[17] D. Temperley. The Cognition of Basic Musical Struc-\ntures . The MIT Press, Cambridge, MA, 2001.788 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Phrase-Level Audio Segmentation of Jazz Improvisations Informed by Symbolic Data.",
        "author": [
            "Jeff Gregorio",
            "Youngmoo E. Kim"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1414790",
        "url": "https://doi.org/10.5281/zenodo.1414790",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/245_Paper.pdf",
        "abstract": "Computational music structure analysis encompasses any model attempting to organize music into qualitatively salient structural units, which can include anything in the heirarchy of large scale form, down to individual phrases and notes. While much existing audio-based segmenta- tion work attempts to capture repetition and homogeneity cues useful at the form and thematic level, the time scales involved in phrase-level segmenation and the avoidance of repetition in improvised music necessitate alternate ap- proaches in approaching jazz structure analysis. Recently, the Weimar Jazz Database has provided transcriptions of solos by a variety of eminent jazz performers. Utilizing a subset of these transcriptions aligned to their associated audio sources, we propose a model based on supervised training of a Hidden Markov Model with ground-truth state sequences designed to encode melodic contours appearing frequently in jazz improvisations. Results indicate that rep- resenting likely melodic contours in this way allows a low- level audio feature set containing primarily timbral and harmonic information to more accurately predict phrase boundaries.",
        "zenodo_id": 1414790,
        "dblp_key": "conf/ismir/GregorioK16",
        "content": "PHRASE-LEVEL AUDIO SEGMENTATION OF JAZZ IMPROVISATIONS\nINFORMED BY SYMBOLIC DATA\nJeff Gregorio and Youngmoo E. Kim\nDrexel University, Dept. of Electrical and Computer Engineering\n{jgregorio, ykim }@drexel.edu\nABSTRACT\nComputational music structure analysis encompasses any\nmodel attempting to organize music into qualitatively\nsalient structural units, which can include anything in the\nheirarchy of large scale form, down to individual phrases\nand notes. While much existing audio-based segmenta-\ntion work attempts to capture repetition and homogeneity\ncues useful at the form and thematic level, the time scales\ninvolved in phrase-level segmenation and the avoidance\nof repetition in improvised music necessitate alternate ap-\nproaches in approaching jazz structure analysis. Recently,\nthe Weimar Jazz Database has provided transcriptions of\nsolos by a variety of eminent jazz performers. Utilizing\na subset of these transcriptions aligned to their associated\naudio sources, we propose a model based on supervised\ntraining of a Hidden Markov Model with ground-truth state\nsequences designed to encode melodic contours appearing\nfrequently in jazz improvisations. Results indicate that rep-\nresenting likely melodic contours in this way allows a low-\nlevel audio feature set containing primarily timbral and\nharmonic information to more accurately predict phrase\nboundaries.\n1. INTRODUCTION\nMusic structure analysis is an active area of research within\nthe Music Information Retrieval (MIR) community with\nutility extending to a wide range of MIR applications in-\ncluding song similarity, genre recognition, audio thumb-\nnailing, music indexing systems, among others. Musical\nstructure can be deﬁned in terms of any qualitatively salient\nunit, from large scale form (e.g. intro, verse, chorus, etc.),\nto melodic themes and motifs, down to individual phrases\nand notes.\nPaulus [8] categorizes existing approaches to audio-\nbased structural analysis according to perceptual cues as-\nsumed to have central importance in determination of\nstructure, namely into those based on repetition ,novelty ,\nandhomogeneity . A music structure analysis task typi-\ncally involves a boundary detection step, where individual\nsections are assumed to be homogeneous, and transitions\nc/circlecopyrtJeff Gregorio and Youngmoo E. Kim. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Jeff Gregorio and Youngmoo E. Kim. “Phrase-level Audio\nSegmentation of Jazz Improvisations Informed by Symbolic Data”, 17th\nInternational Society for Music Information Retrieval Conference, 2016.between sections associated with a high degree of novelty.\nNovelty is assumed to be indicated by large changes in one\nor more time-series feature representations that may corre-\nspond to perceptually-salient shifts in timbre, rhythm, har-\nmony, or instrumentation. Predicted boundaries can then\nbe used to obtain segments which can be grouped accord-\ning to similarity in the employed feature space(s). Alter-\nnatively, repetition-based methods may be used to identify\nrepeated segments and boundaries directly.\nDue to the difﬁculty in reliabily estimating individual\nnote onsets and pitches, much existing work on music seg-\nmentation at the phrase level has been limited to single in-\nstruments in the symbolic domain. In a meta-analysis of\nsymbolic phrase segmentation work, Rodr ´ıguez L ´opez [7]\nshowed that two of the best performing rule-based mod-\nels in comparative studies include Cambouropoulos’s Lo-\ncal Boundary Detection Model (LBDM) [2] and Temper-\nley’s Grouper [11]. Both relate to Gestalt discontinuity\nprinciples, placing phrase boundaries using heuristics de-\nrived in part from features of consecutive note onset times,\nincluding inter-onset intervals (IOI) and offset-onset in-\ntervals (OOI). The LBDM model additionally uses pitch\ncontour information, assuming discontinuity strength is in-\ncreased by large inter-pitch intervals (IPI). Grouper also\nincorporates knowledge of metrical context and assumes a\nprior distribution of phrase lengths.\nThe proposed work focuses on musical structure at the\nphrase level, speciﬁcally identiﬁcation of phrase bound-\naries from audio signals. Though we do not directly pre-\ndict note onsets, durations, or pitches available in sym-\nbolic representations, we take advantage of audio-aligned\nMIDI transcriptions in the supervised training of a Hid-\nden Markov Model (HMM). Using a primarily timbral and\nharmonic audio feature represenation, we hope to aid in\nthe prediction of phrase boundaries by exploiting correla-\ntions between timbral/harmonic cues and common melodic\nphrase contours represented in the dataset.\n2. MOTIVATION\nIn the audio domain, most existing structural segmenta-\ntion work attempts to model to large scale form. The self\ndistance matrix (SDM) is a useful representation in this\nmodality, where entries SDM (i, j) =d(xi,xj)represent\nthe distance between all combinations of feature vectors\nxiandxjby some distance metric d. This representation\nlends itself well to identiﬁable patterns associated with ho-482Figure 1 . Self-distance matrix with form-level annotations\nplotted as white lines. Section boundaries often coincide\nwith large blocks of relatively small distance, and some\nrepetitions can be seen as stripes parallel to the main diag-\nonal.\nmogeneity, novelty, and repetition principles. Homogene-\nity within a section is generally associated with low-valued\nblocks representing small distance, novelty can be seen in\nthe form of transitions between low and high value, and\nrepetition manifests as stripes of low value parallel to the\nmain diagonal. Figure 1 shows an example SDM computed\nusing the timbral and harmonic feature space described in\nSection 4. Note this matrix is smoothed by averaging dis-\ntance values from multiple frames around each index, as\ndescribed in [8], but hasn’t been ﬁltered or beat-aligned to\nenhance repetition patterns, though some are visible.\nWhen attempting audio segmentation at the phrase\nlevel, overall feature space homogeneity within single seg-\nments may be an unsafe assumption given the shorter\ntime scales involved, in which a performer might em-\nploy expressive modulation of timbre. Furthermore, while\nmelodic ideas in a jazz improvisation may be loosely in-\nspired by a theme, extended repetition is usually avoided\nin favor of maximally unique melodies within a single\nperformance. This context suggests that repetition-based\napproaches useful for identifying large-scale forms and\nthemes may be inappropriate. Figure 2 shows an exam-\nple SDM computed at the same resolution as Figure 1 over\n60 seconds of a jazz improvisation. Note that while this\nSDM contains block patterns associated with homogene-\nity, they don’t necessarily align well with entire phrases.\nThis SDM is also almost completely missing any identiﬁ-\nable repetition patterns.\nThere does exist, however, some degree of predictabil-\nity in jazz phrase structure that an ideal model should ex-\nploit, albeit across a corpus rather than within a single\ntrack. We propose a system based on supervised training\nof a Hidden Markov Model with a low-level audio feature\nset designed to capture novelty in the form of large timbral\nFigure 2 . Self-distance matrix with annotated phrase\nboundaries plotted as white lines. Note the absence of off-\ndiagonal striping patterns indicative of repetition, and the\ninfrequent occurrence of large homogeneous blocks over\nthe duration of entire phrases.\nand harmonic shifts indicative of phrase boundaries. Ex-\nisting HMM approaches have included unsupervised train-\ning, with a ﬁxed-number of hidden states assumed to cor-\nrespond to form-level sections [9, 10], instrument mix-\ntures [5], or simply a mid-level feature representation [6].\nOur model differs from existing HMM-based approaches\nin that it attempts to represent common elements of jazz\nphrase structure directly in the topology of the network,\nwhere the ground-truth state sequences are derived from\ninter-pitch intervals in audio-aligned transcriptions. Likely\nsequences of predicted states should therefore correspond\nto melodic contours well-represented in the training data,\naiding in the detection of phrase boundaries.\n3. DATASET AND PREPROCESSING\nIn 2014, the ﬁrst version of the Weimar Jazz Database\n(WJazzD ) was released as part of the larger Jazzomat Re-\nsearch Project [1]. The database contains transcriptions of\nmonophonic solos by eminent jazz performers, well rep-\nresenting the evolution of the genre over the 20th century.\nThe database was later expanded to include 299 solo tran-\nscriptions from 225 tracks, 70 performers, 11 instruments\n(soprano/alto/tenor/tenor-c/baritone sax, clarinet, trumpet,\ncornet, trombone, vibraphone, and guitar) and 7 styles\n(Traditional, Swing, Bebop, Hardbop, Cool, Postbop, and\nFree Jazz). The transcriptions were initially generated us-\ning state-of-the-art automatic transcription tools and man-\nually corrected by musicology students. In addition to the\ntranscriptions, the database contains a rich collection of\nmetadata and human labels including phrase boundaries,\nunderlying chord changes, form-level sections, and beat\nlocations.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 4833.1 MIDI to Audio Alignment\nThe lack of availability of the original audio tracks used as\nsource material for the database’s transcriptions presents\nsome difﬁculty in taking full advantage of the possibili-\nties for supervised machine learning methods using acous-\ntic features. Toward this end, we were able to obtain 217\nof 225 tracks containing the solo(s) as transcribed. Meta-\ndata available in WJazzD indicate the starting and end-\ning timestamps of the solo sections at a 1-second reso-\nlution, which is insufﬁcient for determining ground truth\nfor phrase boundaries associated with note onset times.\nAdditionally, pulling audio ﬁles from various sources in-\ntroduces further uncertainty, as many tracks appear on\nboth original releases and compilations which may differ\nslightly in duration or other edits.\nTo obtain ground truth, we trim the original tracks ac-\ncording to provided solo timestamps and employ a tool\ncreated by Dan Ellis [4] which uses Viterbi alignment on\nbeat-tracked versions of original audio and resynthesized\nMIDI to modify and output an aligned MIDI ﬁle. Upon in-\nspection, 90 extracted solos produced a suitable alignment\nthat required minimal manual corrections. We parse the\ndatabase and handle conversion to and from MIDI format\nin Matlab using the MIDI Toolbox [3], making extensive\nuse of the convenient note matrix format and pianoroll vi-\nsualizations.\n4. AUDIO FEATURES\nTo represent large timbral shifts, we use spectral ﬂux and\ncentroid features derived from the short-time Fourier trans-\nform (STFT), and spectral entropy derived from the power\nspectral density (PSD) of the audio tracks, sampled at\n22050 Hzwith a FFT size of 1024 samples, Hamming win-\ndowed, with 25% overlap. Due to our interest in the lead\ninstrument only, features are computed on a normalized\nportion of the spectrum between 500−5000Hzto remove\nthe inﬂuence of prominent bass lines while preserving har-\nmonic content of the lead instrument.\nWe also compute two features based on Spectral Con-\ntrast, a multi-dimensional feature computed as the decibel\ndifference between the largest and smallest values in seven\nFigure 3 . Mean positive difference between spectral con-\ntrast frames, plotted against annotated phrase boundaries.octave bands of the STFT. Since the resolution of this fea-\nture is dependent on the number of frequency bins in each\noctave, we use a larger FFT of size 4096 , which gives\nmeaningful values in four octaves above 800Hzwithout\nsacriﬁcing much time resolution. The ﬁrst feature reduces\nspectral contrast to one dimension by taking the mean dif-\nference of all bands between frames. Observing that large\npositive changes in spectral contrast correlate well with an-\nnotated phrase boundaries, we half-wave rectify this fea-\nture. The second feature takes the seven-dimensional Eu-\nclidian distance between spectral contrast frames.\nFinally, we include a standard Chromagram feature,\nwhich is a 12-dimensional feature representing the contri-\nbution in the audio signal to frequencies associated with\nthe twelve semitones in an octave. While the chromagram\nincludes contributions from fundamental frequencies of in-\nterest, it also inevitably captures harmonics and un-pitched\ncomponents. Noting that even precise knowledge of abso-\nlute pitch of the lead instrument would be uninformative\nin determining whether any note were the begining of a\nphrase, we collapse this feature to a single dimension by\ntaking the Euclidian distance between frames, with the in-\ntention of capturing harmonic shifts that may be correlated\nwith phrase boundaries and melodic contours.\nAll features are temporally smoothed by convolving\nwith a gaussian kernel of 21samples. All elements in the\nfeature vectors are squared to emphasize peaks. We then\ndouble the size of the feature set by taking the ﬁrst time dif-\nference of each feature, which amounts to a second differ-\nence for the spectral contrast and chroma features. Later,\nwhen evaluating, each feature in the training and testing\nsets is standardized to zero mean and unit variance using\nstatistics of the training set features.\n5. MELODIC CONTOUR VIA HMM\nHidden Markov Models represent the joint distribution\nof some hidden state sequence y={y1, y2, ..., y N}\nand a corresponding sequence of observations X =\n{x1,x2, ...,xN}, or equivalently the state transition prob-\nabilities P(yi|yi−1)and emission probabilities P(xi|yi).\nHMMs have been used in various forms for music struc-\nture analysis, lending well to the sequential nature of the\ndata, with hidden states often assumed to correspond to\nsome perceptually meaningful structural unit. Unsuper-\nvised approaches use feature observations and an assumed\nnumber of states as inputs to the Baum-Welch algorithm\nto estimate the model parameters, which can then be used\nwith the Viterbi algorithm to estimate the most likely se-\nquence of states to have produced the observations.\nPaulus notes that unsupervised HMM-based segmenta-\ntion tends to produce unsatisfactory results on form-level\nsegmentation tasks due to observations relating to individ-\nual sound events [8], a shortcoming which has led to ob-\nserved state sequences being treated as a mid-level rep-\nresentations in subsequent work. We revisit HMMs as a\nsegmentation method speciﬁcally for phrase-level analy-\nsis due to the particular importance of parameters of in-\ndividual sound events rather than longer sections. Speciﬁ-484 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016cally, we postulate that phrases drawn from the jazz vocab-\nulary follow predictable melodic contours, and incorporat-\ning ground-truth knowledge of the distribution and tem-\nporal evolution of these contours as observed in a large\ndataset of phrases through supervised training may help in\nidentiﬁcation of phrase boundaries.\n6. EXPERIMENTS\nWe ﬁrst evaluate a 2-state HMM, with states yi∈{0,1}\ncorresponding to the absence or presence (respectively) of\nthe lead instrument during the ithaudio frame. Though\na 2-state graphical model is trivial and offers no advan-\ntages over any other supervised classiﬁcation method, we\ninclude it here simply as a basis for comparison with the\nmulti-state models to evaluate the efﬁcacy of adding states\nbased on ground-truth pitch contour.\nTo estimate an upper bound on expected performance\nof our audio-based models, we evaluate two symbolic seg-\nmentation models using features of precisely known note\npitches and onset times. First, we evaluate the Local\nBoundary Detection Model (LBDM) implementation of-\nfered by the MIDI Toolbox [3]. The LBDM outputs a con-\ntinuous boundary strength, which we use to tune a bound-\nary prediction threshold for maximum f-score via cross\nvalidation. Second, we train a 2-state HMM, where the\nmodel state yithen corresponds to the ithnote rather than\ntheithaudio frame, and takes the value 1 if the note is the\nﬁrst in a phrase, and 0 otherwise. Observations xisimilarly\ncorrespond to features of individual note events including\nthe IOI, OOI, and IPI. Results of the symbolic segmenta-\ntion models are shown in Table 1(a).\nTo directly encode melodic contour in the network\ntopology for the multi-state, audio-based HMMs, we ex-\ntract ground-truth state sequences based on quantization\nlevels of the observed inter-pitch interval (IPI) in the tran-\nscription. The following indicate the state of the network\nfollowing each IPI, where the state remains for the duration\nof the note, rounded to the nearest audio frame:\n5-State yi=\n\n0lead instrument absent\n1ﬁrst phrase note\n2IPI < 0\n3IPI = 0\n4IPI > 0\n7-State yi=\n\n0lead instrument absent\n1ﬁrst phrase note\n2IPI <−5\n3−5≤IPI < 0\n4IPI = 0\n5 0 < IPI≤5\n6IPI > 5The 5-state model simply encodes increas-\ning/decreasing/unison pitch in the state sequence. The\n7-state model further quantizes increasing and decreasing\npitch into intervals greater than and less than a perfect\nfourth. Each HMM requires a discrete observation se-\nquence, so the 10-dimensional audio feature set described\nin Section 4 is discretized via clustering using a Gaussian\nMixture Model (GMM) with parameters estimated via\nExpectation-Maximization (EM).\nWe note that in the solo transcriptions, there are many\nexamples of phrase boundaries that occur between two\nnotes played legato (i.e. the offset-onset interval is zero or\nless than the time duration of a single audio frame). When\nparsing the MIDI data and associated boundary note an-\nnotations to determine the state sequence for each audio\nframe, in any such instance where state 1 is not preceded\nby state 0, we force a 0-1 transition to allow the model to\naccount for phrase boundaries that aren’t based primarily\non temporal discontinuity cues.\n7. RESULTS\nEvaluation of each network is performed via six fold cross-\nvalidation, where each fold trains the model on ﬁve styles\nas provided by the WJazzD metadata, and predicts on the\nremaining style. We note that WJazzD encompasses seven\nstyles, but the 90 examples successfully aligned to corre-\nsponding audio tracks did not include any traditional jazz.\nThough the sequence of states predicted by the model in-\nclude the contour-based states, our reported results only\nconsider accuracy in predicting a transition to state 1 in all\ncases.\nPrecision, recall, and f-score metrics reported in form-\nlevel segmentation experiments typically consider a true\npositive to be a boundary identiﬁed within 0.5and3sec-\nonds of the ground truth. Considering the short time scales\ninvolved with phrase-level segmentation, we report metrics\nconsidering a true positive to be within one beat and one\nhalf beat, as determined using each solo’s average tempo\nModel Pn Rn Fn\nLBDM 0.7622 0 .7720 0 .7670\nHMM, 2-State 0.8225 0 .8252 0 .8239\n(a) Symbolic models\nModel P1B R1B F1B\nHMM, 2-State 0.6114 0 .5584 0 .5837\nHMM, 5-State 0.5949 0 .6586 0 .6251\nHMM, 7-State 0.6116 0 .6565 0 .6333\n(b) Audio models, true positive within one beat of annotation\nModel P0.5BR0.5BF0.5B\nHMM, 2-State 0.4244 0 .3876 0 .4052\nHMM, 5-State 0.4039 0 .4472 0 .4245\nHMM, 7-State 0.4212 0 .4521 0 .4361\n(c) Audio models, true positive within half beat of annotation\nTable 1 . Segmentation resultsProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 485annotation provided by WJazzD . For reference, the mean\ntime per beat in the 90 aligned examples is 0.394seconds,\nwith a standard deviation of 0.178seconds. All beat dura-\ntions were less than 1 second.\nWe report precision, recall, and f-score computed over\nall examples, all folds, and 5 trials. Reported results use\n30Gaussian components as discrete observations in the\naudio-based models, and 5components for the symbolic\nmodel, and are summarized in Table 1. For greater insight\ninto the model’s performance in different stylistic contexts,\nwe also present the cross-validation results across the six\nstyles in Figure 4.\n8. DISCUSSION\nANOV A and post-hoc analysis reveals both multi-state\nmodels yielding increased recall over the 2-state model\n(F2,1332 = 30 .62,p<10−13), and increased f-score\n(F2,1332 = 11.28,p<10−4) with no signiﬁcant differ-\nence in precision. Interestingly, the most signiﬁcant re-\ncall increases from addition of the melodic contour states\nwithin styles include hardbop ( F2,297= 15.68,p<10−6),\npostbop ( F2,432= 12 .22,p<10−5), and swing styles\n(F2,177= 6.73,p<10−3).\nThese increases in recall within a style also corre-\nlate well with a high proportion of occurrences of phrase\nboundaries with no temporal discontinuity. These account\nfor22% of all phrase boundaries in hardbop, 18% in post-\nbop, and 29% in swing, while accounting for 17%,7%,\nand4%in bebop, cool, and free jazz, respectively. We be-\nlieve this suggests that incorporating ground-truth melodic\ncontour allows the model to account for the relationship\n(a) 2-State\n(b) 5-State\n(c) 7-State\nFigure 4 . Audio-based HMM segmentation results by\nstyle. Signiﬁcant ( p<0.005) increases in recall and f-\nscore observed in Hardbop, Postbop, and Swing.between contours indicative of phrase boundaries and their\nassociated timbral and harmonic shifts.\nManual inspection of segmentation results tend to re-\ninforce this idea, as shown in Figure 5. The 2-state\nmodel fails to identify four phrase boundaries preceded by\nvery small inter-onset intervals (6th, 15th, 18th, and 21st\nphrases), while the 7-state model correctly identiﬁes three\n(6th, 18th, and 21st), at the cost of some tendency toward\nover-segmentation (in this case).\n9. CONCLUSIONS\nEvaluation of a 2-state HMM established a baseline phrase\nsegmentation accuracy by detecting the presence or ab-\nsence of the lead instrument, which presents some difﬁ-\nculty in predicting phrase boundaries based on harmonic\nand melodic cues with little to no temporal discontinuity.\nIncorporating a ground-truth state sequence in the multi-\nstate HMMs using melodic contour information derived\nfrom the transcription yielded statistically signiﬁcant in-\ncreases in recall in styles containing a high proportion of\nthese phrase boundaries.\nAlthough our feature set does not attempt to predict\npitches of individual notes, we believe the increased recall\nassociated with the multi-state models indicates the model\nis exploiting a relationship between timbral and harmonic\nobservations and melodic contours associated with phrase\nboundaries. These precise relationships are undoubtedly\ndependent on the timbre of the instrument, yet demonstrate\nsome general utility when trained on a range of lead instru-\nments.\nWhile the attempted representation of melodic contour\n(a) 2-State: P1B= 0.762,R1B= 0.696,F1B= 0.727\n(b) 7-State: P1B= 0.758,R1B= 0.956,F1B= 0.846\n(c) MIDI Transcription\nFigure 5 . Segmentation of Freddie Hubbard’s solo in the\nEric Dolphy track “245”. Black lines indicate ground-truth\nannotations, and red lines show predicted boundaries.486 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016in the model topology indicates some promise, we believe\nthere are likely better alternatives to modeling contour than\narbitrary quanitzation of ground truth inter-pitch intervals.\nFuture work should examine the potential of assembling\nobserved contours from a smaller set of contour primitives\nover longer time scales than note pair transitions. Fur-\nthermore, though our approach avoided relying high-level\npitch estimates derived from the audio because of strong\npotential for propagation of errors, we will investigate the\nuse of mid-level pitch salience functions in future feature\nsets.\nMore generally, we believe that the availability of well-\naligned audio and symbolic data can allow the use of super-\nvised methods as a precursor to more scalable audio-based\nmethods, and aid in the creation of mid-level features use-\nful for a wide range of MIR problems.\n10. REFERENCES\n[1] Jakob Aeßler, Klaus Frieler, Martin Pﬂeiderer, and\nWolf-Georg Zaddach. Introducing the jazzomat project\n- jazz solo analysis using music information retrieval\nmethods. In In: Proceedings of the 10th International\nSymposium on Computer Music Multidisciplinary Re-\nsearch (CMMR) Sound, Music and Motion, Marseille,\nFrankreich. , 2013.\n[2] Emilios Cambouropoulos. Music, Gestalt, and Com-\nputing: Studies in Cognitive and Systematic Musicol-\nogy, chapter Musical rhythm: A formal model for\ndetermining local boundaries, accents and metre in a\nmelodic surface, pages 277–293. Springer Berlin Hei-\ndelberg, Berlin, Heidelberg, 1997.\n[3] Tuomas Eerola and Petri Toiviainen. MIDI Toolbox:\nMATLAB Tools for Music Research . University of\nJyv¨askyl ¨a, Jyv ¨askyl ¨a, Finland, 2004.\n[4] D.P.W. Ellis. Aligning midi ﬁles to music au-\ndio, web resource, 2013. http://www.ee.\ncolumbia.edu/ ˜dpwe/resources/matlab/\nalignmidi/ . Accessed 2016-03-11.\n[5] Sheng Gao, N. C. Maddage, and Chin-Hui Lee. A\nhidden markov model based approach to music seg-\nmentation and identiﬁcation. In Information, Commu-\nnications and Signal Processing, 2003 and Fourth Pa-\nciﬁc Rim Conference on Multimedia. Proceedings of\nthe 2003 Joint Conference of the Fourth International\nConference on , volume 3, pages 1576–1580 vol.3, Dec\n2003.\n[6] M. Levy and M. Sandler. Structural segmentation of\nmusical audio by constrained clustering. Trans. Au-\ndio, Speech and Lang. Proc. , 16(2):318–326, February\n2008.\n[7] Marcelo Rodrguez Lpez and Anja V olk. Melodic seg-\nmentation: A survey. Technical Report UU-CS-2012-\n015, Department of Information and Computing Sci-\nences, Utrecht University, 2012.[8] Jouni Paulus, Meinard M ¨uller, and Anssi Klapuri. State\nof the art report: Audio-based music structure analy-\nsis. In Proceedings of the 11th International Society for\nMusic Information Retrieval Conference, ISMIR 2010,\nUtrecht, Netherlands, August 9-13, 2010 , pages 625–\n636, 2010.\n[9] Geoffroy Peeters, Amaury La Burthe, and Xavier\nRodet. Toward automatic music audio summary gen-\neration from signal analysis. In In Proc. International\nConference on Music Information Retrieval , pages 94–\n100, 2002.\n[10] Mark Sandler and Jean-Julien Aucouturier. Segmenta-\ntion of musical signals using hidden markov models. In\nAudio Engineering Society Convention 110 , May 2001.\n[11] David Temperley. The cognition of basic musical struc-\ntures . MIT Press, 2004.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 487"
    },
    {
        "title": "Automatic Melodic Reduction Using a Supervised Probabilistic Context-Free Grammar.",
        "author": [
            "Ryan Groves"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416924",
        "url": "https://doi.org/10.5281/zenodo.1416924",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/274_Paper.pdf",
        "abstract": "This research explores a Natural Language Processing technique utilized for the automatic reduction of melodies: the Probabilistic Context-Free Grammar (PCFG). Au- tomatic melodic reduction was previously explored by means of a probabilistic grammar [11] [1]. However, each of these methods used unsupervised learning to estimate the probabilities for the grammar rules, and thus a corpus- based evaluation was not performed. A dataset of analyses using the Generative Theory of Tonal Music (GTTM) ex- ists [13], which contains 300 Western tonal melodies and their corresponding melodic reductions in tree format. In this work, supervised learning is used to train a PCFG for the task of melodic reduction, using the tree analyses pro- vided by the GTTM dataset. The resulting model is evalu- ated on its ability to create accurate reduction trees, based on a node-by-node comparison with ground-truth trees. Multiple data representations are explored, and example output reductions are shown. Motivations for performing melodic reduction include melodic identification and sim- ilarity, efficient storage of melodies, automatic composi- tion, variation matching, and automatic harmonic analysis.",
        "zenodo_id": 1416924,
        "dblp_key": "conf/ismir/Groves16",
        "content": "AUTOMATIC MELODIC REDUCTION USING A SUPERVISED\nPROBABILISTIC CONTEXT-FREE GRAMMAR\nRyan Groves\ngroves.ryan@gmail.com\nABSTRACT\nThis research explores a Natural Language Processing\ntechnique utilized for the automatic reduction of melodies:\nthe Probabilistic Context-Free Grammar (PCFG). Au-\ntomatic melodic reduction was previously explored by\nmeans of a probabilistic grammar [11] [1]. However, each\nof these methods used unsupervised learning to estimate\nthe probabilities for the grammar rules, and thus a corpus-\nbased evaluation was not performed. A dataset of analyses\nusing the Generative Theory of Tonal Music (GTTM) ex-\nists [13], which contains 300 Western tonal melodies and\ntheir corresponding melodic reductions in tree format. In\nthis work, supervised learning is used to train a PCFG for\nthe task of melodic reduction, using the tree analyses pro-\nvided by the GTTM dataset. The resulting model is evalu-\nated on its ability to create accurate reduction trees, based\non a node-by-node comparison with ground-truth trees.\nMultiple data representations are explored, and example\noutput reductions are shown. Motivations for performing\nmelodic reduction include melodic identiﬁcation and sim-\nilarity, efﬁcient storage of melodies, automatic composi-\ntion, variation matching, and automatic harmonic analysis.\n1. INTRODUCTION\nMelodic reduction is the process of ﬁnding the more struc-\ntural notes in a melody. Through this process, notes that\nare deemed less structurally important are systematically\nremoved from the melody. The reasons for removing a\nparticular note are, among others, pitch placement, metri-\ncal strength, and relationship to the underlying harmony.\nBecause of its complexity, formal theories on melodic re-\nduction that comprehensively deﬁne each step required to\nreduce a piece in its entirety are relatively few.\nComposers have long used the rules of ornamentation to\nelaborate certain notes. In the early 1900s, the music the-\norist Heinrich Schenker developed a hierarchical theory of\nmusic reduction (a comprehensive list of Schenker’s pub-\nlications was assembled by David Beach [7]). Schenker\nascribed each note in the musical surface as an elabora-\ntion of a representative musical object found in the deeper\nc/circlecopyrtRyan Groves. Licensed under a Creative Commons At-\ntribution 4.0 International License (CC BY 4.0). Attribution: Ryan\nGroves. “Automatic Melodic Reduction Using a Supervised Probabilistic\nContext-Free Grammar”, 17th International Society for Music Informa-\ntion Retrieval Conference, 2016.levels of reduction. The particular categories of ornamen-\ntation that were used in his reductive analysis were neigh-\nbor tones ,passing tones ,repetitions ,consonant skips , and\narpeggiations . Given a sequence of notes that can be iden-\ntiﬁed as a particular ornamentation, an analyst can remove\ncertain notes in that sequence so that only the more impor-\ntant notes remain.\nIn the 1980s, another theory of musical reduction was\ndetailed in the GTTM [16]. The authors’ goal was to cre-\nate a formally-deﬁned generative grammar for reducing a\nmusical piece. In GTTM, every musical object in a piece\nissubsumed by another musical object, which means that\nthe subsumed musical object is directly subordinate to the\nother. This differs from Schenkerian analysis, in that ev-\nery event is related to another single musical event. In\ndetailing this process, Lerdahl and Jackendoff begin by\nbreaking down metrical hierarchy, then move on to identi-\nfying a grouping hierarchy (separate from the metrical hi-\nerarchy). Finally, they create two forms of musical reduc-\ntions using the information from the metrical and grouping\nhierarchies—the time-span reduction, and the prolonga-\ntional reduction. The former details the large-scale group-\ning of a piece, while the latter notates the ebb and ﬂow of\nmusical tension in a piece.\nMany researchers have taken the idea—inspired by\nGTTM or otherwise—of utilizing formal grammars as\na technique for reducing or even generating music (see\nSection 2.0.0.0.2). However, most of these approaches\nwere not data-driven, and those that were data-driven of-\nten utilized unsupervised learning rather than supervised\nlearning. A dataset for the music-theoretical analysis of\nmelodies using GTTM has been created in the pursuit of\nimplementing GTTM as a software system [13]. This\ndataset contains 300 Western classical melodies with their\ncorresponding reductions, as notated by music theorists ed-\nucated in the principles of GTTM. Each analysis is notated\nusing tree structures, which are directly compatible with\ncomputational grammars, and their corresponding parse\ntrees. The GTTM dataset is the corpus used for the su-\npervised PCFG detailed in this paper.\nThis work was inspired by previous research on a PCFG\nfor melodic reduction [11], in which a grammar was de-\nsigned by hand to reﬂect the common melodic movements\nfound in Western classical music, based on the composi-\ntional rules of ornamentation. Using that hand-made gram-\nmar, the researchers used a dataset of melodies to calculate\nthe probabilities of the PCFG using unsupervised learn-\ning. This research aims to simulate and perform the pro-775cess of melodic reduction, using a supervised Probabilisitic\nContext-Free Grammar (PCFG). By utilizing a ground-\ntruth dataset, it is possible to directly induce a grammar\nfrom the solution trees, creating the set of production rules\nfor the grammar and modelling the probabilities for each\nrule expansion. In fact, this is the ﬁrst research of its type\nthat seeks to directly induce a grammar for the purpose of\nmelodic reduction. Different data representations will be\nexplored and evaluated based on the accuracy of their re-\nsulting parse trees. A standard metric for tree comparison\nis used, and example melodic reductions will be displayed.\nThe structure of this paper is as follows: The next sec-\ntion provides a brief history of implementations of GTTM,\nas well as an overview of formal grammars used for mu-\nsical purposes. Section 3 presents the theoretical founda-\ntions of inducing a probabilistic grammar. Section 4 de-\nscribes the data set that will be used, giving a more detailed\ndescription of the data structure available, and the differ-\nent types of melodic reductions that were notated. Section\n5 describes the framework built for converting the input\ndata type to an equivalent type that is compatible with a\nPCFG, and also details the different data representations\nused. Section 6 presents the experiment, including the\ncomparison and evaluation method, and the results of the\ndifferent tests performed. Section 7 provides some closing\nremarks.\n2. LITERATURE REVIEW\nIn order to reduce a melody, a hierarchy of musical events\nmust be established in which more important events are at\na higher level in the hierarchy. Methods that create such\na structure can be considered to be in the same space as\nmelodic reduction, although some of these methods may\napply to polyphonic music as well. The current section de-\ntails research regarding hierarchical models for symbolic\nmusical analysis.\n2.1 Implementing GTTM\nWhile much research has been inspired by GTTM, some\nresearch has been done to implement GTTM directly. Fred\nLerdahl built upon his own work by implementing a system\nfor assisted composition [17]. Hamanaka et al. [13] pre-\nsented a system for implementing GTTM. The framework\nidentiﬁes time-span trees automatically from monophonic\nmelodic input, and attained an f-measure of 0.60. Frank-\nland and Cohen isolated the grouping structure theory in\nGTTM, and tested against the task of melodic segmenta-\ntion [10].\n2.2 Grammars in Music\nIn 1979, utilizing grammars for music was already of much\ninterest, such that a survey of the different approaches was\nin order [20]. Ruwet [21] suggested that a generative gram-\nmar would be an excellent model for the creation of a\ntop-down theory of music. Smoliar [22] attempted to de-\ncompose musical structure (including melodies) from au-\ndio signals with a grammar-based system.Baroni et al. [4] also created a grammatical system for\nanalyzing and generating melodies in the style of Lutheran\nchorales and French chansons. The computer program\nwould create a completed, embellished melody from an in-\nput that consisted of a so-called “primitive phrase” (Baroni\net al. 1982, 208).\nBaroni and Jacoboni designed a grammar to analyze and\ngenerate melodies in the style of major-mode chorales by\nBach [5, 6]. The output of the system would generate the\nsoprano part of the ﬁrst two phrases of the chorale.\n2.3 Probabilistic Grammars\nGilbert and Conklin [11] designed a PCFG for melodic\nreduction and utilized unsupervised learning on 185 of\nBach’s chorales from the Essen Folksong Collection. This\ngrammar was also explored by Abdallah and Gold [1], who\nimplemented a system in the logical probabilistic frame-\nwork PRISM for the comparison of probabilistic systems\napplied to automatic melodic analysis. The authors im-\nplemented the melodic reduction grammar provided by\nGilbert and Conklin using two separate parameterizations\nand compared the results against four different variations\nof Markov models. The evaluation method was based on\ndata compression, given in bits per note (bpn). The authors\nfound that the grammar designed by Gilbert and Conklin\nwas the best performer with 2.68 bpn over all the datasets,\nbut one of the Markov model methods had a very simi-\nlar performance. The same authors also collaborated with\nMarsden [2] to detail an overview of probabilistic sys-\ntems used for the analysis of symbolic music, including\nmelodies.\nHamanaka et al. also used a PCFG for melodic reduc-\ntion [12]. The authors used the dataset of treebanks that\nthey had previously created [13] to run supervised learn-\ning on a custom-made grammar that he designed, in or-\nder to automatically generate time-span reduction trees.\nThis work is very similar to the work presented here, with\ntwo exceptions. First, the grammar was not learned from\nthe data. Secondly, Hamanaka used a series of processes\non the test melodies using previous systems he had built.\nThese systems notated the metrical and grouping struc-\nture of the input melody, before inputting that data into\nthe PCFG. Hamanaka achieves a performance of 76% tree\naccuracy.\n2.4 Similar Methods for Musical Reduction\nCreating a system that can perform a musical reduction\naccording to the theory of Heinrich Schenker has also\nbeen the topic of much research. Marsden explored the\nuse of Schenkerian reductions for identifying variations\nof melodies [19]. PCFGs have not yet been utilized for\nthis particular task. One notable caveat is the probabilistic\nmodelling of Schenkerian reductions, using a tree-based\nstructure [15]. Kirlin did not explicitly use a PCFG, how-\never his model was quite similar, and also was a supervised\nlearning method.776 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20163. SUPERVISED LEARNING OF A PCFG\nTo understand the theoretical framework of the PCFG, it\nis ﬁrst useful to give a brief background of formal gram-\nmars. Grammars were formalized by Chomsky [8] and\nextended by himself [9] and Backus et al. [3]. The def-\ninition of a formal grammar consists of four parameters,\nG={N,Σ,R,S}, which are deﬁned as follows [14]:\nN a set of non-terminal symbols\nΣ a set of terminals (disjoint from N)\nR a set of production rules, each of the form α→β\nS a designated start symbol\nEach production rule has a right-hand side, β, that rep-\nresents the expansion of the term found on the left-hand\nside,α. In a Context-Free Grammar (CFG), the left-hand\nside consists of a single non-terminal, and the right-hand\nside consists of a sequence of non-terminals and terminals.\nNon-terminals are variables that can be expanded (by other\nrules), while terminals are speciﬁc strings, representing el-\nements that are found directly in the sequence (for exam-\nple, the ‘dog’ terminal could be one expansion for the Noun\nnon-terminal). Given a CFG and an input sequence of ter-\nminals, the CFG can parse the sequence, creating a hierar-\nchical structure by iteratively ﬁnding all applicable rules.\nGrammars can be ambiguous; there can be multiple valid\ntree structures for one input sequence.\nPCFGs extend the CFG by modelling the probabilities\nof each right-hand side expansion for every production\nrule. The sum of probabilities for all of the right-hand side\nexpansions of each rule must sum to 1. Once a PCFG is\ncalculated, it is possible to ﬁnd the most probable parse\ntree, by cumulatively multiplying each production rule’s\nprobability throughout the tree, for every possible parse\ntree. The parse tree with the maximum probability is the\nmost likely. This process is called disambiguation.\n3.1 Inducing a PCFG\nWhen a set of parse tree solutions (called a treebank ) ex-\nists for a particular set of input sequences, it is possible\nto construct the grammar directly from the data. In this\nprocess, each parse tree from the treebank will be broken\napart, so that the production rule at every branch is isolated.\nA grammar will be formed by accumulating every rule that\nis found at each branch in each tree, throughout the en-\ntire treebank. When a rule and its corresponding expan-\nsions occurs multiple times, the probabilities of the right-\nhand side expansion possibilities are modelled. Inducing a\nPCFG is a form of supervised learning.\n4. GTTM DATASET\nThe GTTM dataset contains the hierarchical reductions\n(trees) of melodies in an Extensible Markup Language\n(XML) representation.\nThere are two different types of reduction trees that are\ncreated with the theories in GTTM: time-span reduction\ntrees, and prolongational reduction trees. The time-span\n(a)\n (b)\n(c)\nFigure 1 : The prolongational tree (a) and the time-span\ntree (b) for the second four notes in Fr ´ed´eric Chopin’s\n“Grande Valse Brillante”, as well as the score (c). The in-\ntervals between notes are notated in number of semitones.\nreduction is built upon the grouping structure analysis pro-\nvided in GTTM, which in turn uses the metrical structure\nanalysis to inﬂuence its decision-making. Time-span re-\nduction trees are generally more reliant on the metrical in-\nformation of a piece, since it utilizes the grouping structure\ndirectly. The prolongational reductions are designed to no-\ntate the ebb and ﬂow of tension and progression in a piece.\nIn fact, in GTTM, the prolongational reductions use time-\nspan reduction trees as a starting point, but then build the\nbranching system from the top, down, based on pitch and\nharmonic content in addition to the time-span information.\nAn example helps to detail their differences. Figure\n1 shows a particular phrase from one of the melodies in\nthe GTTM dataset: Fr ´ed´eric Chopin’s “Grande Valse Bril-\nlante” [13]. The note labelled P1-2-2 is attached to the\nlast note of the melody in the prolongational reduction, be-\ncause of the passing tone ﬁgure in the last 3 notes, whereas\nthe time-span tree connects note P1-2-2 to the ﬁrst note of\nthe melody, due to its metrical strength and proximity.\nThe entire dataset consists of 300 melodies, with anal-\nyses for each. However, the prolongational reduction trees\nare only provided for 100 of the 300 melodies, while the\ntime-span trees are provided for all 300 melodies. The pro-\nlongational reductions require the annotations of the un-\nderlying harmony. Likewise, there are only 100 harmonic\nanalyses in the dataset.\n5. FORMING THE PCFG\nGilbert and Conklin decided to model the relevant charac-\nteristics of the data by hand, by manually creating grammar\nrules that represented the music composition rules of orna-\nmentation [11]. The melodic embellishment rules included\nin their grammar were the following: passing tone, neigh-\nbor tone, repeated tone, and the escape tone. Additionally,\nthey created a “New” rule which was a kind of catch-all\nfor any interval sequence that could not be described by\nthe other rules. In order for the rules to be applicable atProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 777Figure 2 : A visualization of a set of melodic embellish-\nment rules, encoded manually into the production rules of\na formal grammar [11, 3].\nany pitch location, the fundamental unit of data was the in-\nterval between two notes, rather than two separate values\nfor each note. The complete ruleset is shown in Figure 2.\nWhen learning a grammar directly from a dataset of\nannotations, the most important decision to make is the\ndata representation. The representation chosen should be\nable to capture the most relevant characteristics of the data.\nSimilar to Gilbert and Conklin, each rule modelled two\nconsecutive intervals in a sequence of three notes, and had\nthe following form (notes labelled as n1through n3):\ninterval n1,n3→interval n1,n2interval n2,n3 (1)\nThe motivation was that melodic rules often involve a\nsequence of 3 notes. This is true for the passing tone,\nneighbor tone, and the escape tone. The repetition rule\nwould normally require only two notes, however to keep\na consistent format, repetitions were only reduced when\nthree consecutive notes of the same pitch were found,\nwhich were then reduced to two notes of the same pitch\n(creating one interval). The “New” rule was no longer\nneeded, since the model learns the rules directly from the\ntraining data. This form of one interval expanding into\ntwo consecutive intervals for the grammatical rules was\nadopted for this research.\n5.1 A Framework for Converting Trees\nUtilizing a representation that required a sequence of two\nintervals in every right-hand expansion presented a prob-\nlem, because the GTTM reduction trees were in a format\nthat associated pairs of notes at each branch intersection—\nnot the three consecutive notes required for the two con-\nsecutive intervals. Given this challenge, a framework was\ndeveloped to convert the note representation of the GTTM\ndata into the interval notation desired, and to build the cor-\nresponding tree structure using the interval representation.\nAn example GTTM tree is shown in Figure 3. Note that\nat the end of every branch is a single note. An algorithm\nwas developed to allow the conversion of these note-based\ntrees to any interval representation desired, based on a se-\nquence of 3 notes. The algorithm traverses the tree from\nFigure 3 : The prolongational reduction tree for half of\nthe ﬁrst melody in the GTTM dataset, Fr ´ed´eric Chopin’s\n“Grande Valse Brillante”, as displayed in the GTTM visu-\nalizer provided by Hamanaka, Hirata, and Tojo [13].\nFigure 4 : A depiction of the process for converting a tree\nthat uses a note representation to a tree that uses an inter-\nval representation, by traversing the tree breadth-wise and\nrelating sets of 3 notes.\nthe top, down, in a breadth-wise fashion. At each level of\ndepth, the sequence of notes at that depth are broken into\nsets of 3 consecutive notes, and their intervals are com-\nputed. The framework allows for any interval-based rep-\nresentation to be applied. For example, it could be regular\npitch intervals, inter-onset interval (IOI), difference in met-\nric prominence, or even representations that consider the\nnotes’ relationships to scale and harmony. Figure 4 high-\nlights the breadth-wise traversal process.\nThe framework was built in Python. It takes a function\nas input, which allows the user to deﬁne unique interval\nrepresentations. When the function is called during the tree\nconversion process, the information available for deﬁning\nthe representation consists of the two notes (which contain\nduration, onset and pitch information), the current key, and\nthe current underlying harmony (if available). The interval\nencoding that is returned by the function is then used as a\nnode in the resulting tree.\n5.2 Training/Induction\nThe Python-based Natural Language Toolkit (NLTK) was\nused for the process of PCFG induction [18]. Given a tree-\nbank of solutions, the process for inducing a PCFG is de-\nscribed as follows. For every tree in the treebank, traverse\nthrough the tree to identify each branching location. For\nevery branching location, create a rule with the node la-\nbel as the left-hand side, and the children as the right-hand\nside. Collect the set of rules found at every branch of ev-\nery tree in the treebank, and pass that list of production\nrule instances into NLTK’s induce pcfg function. The in-\nduce pcfg function will catalogue every rule, and build up\na grammar based on those rules. It will also model the778 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016probability of each rule’s unique expansions.\n5.3 Data Representations\nFor the representation of intervals between two consecu-\ntive notes, this research focused on a few certain musical\nattributes. These attributes were tested ﬁrst in isolation,\nand then in combination. The following descriptions relate\nto the attributes labelled in the results table (the key for\neach attribute is given in parentheses following the name).\nPitch The difference in pitch between two notes was\na part of every data representation tested. However, the\nencodings for these pitch values varied. Initially, a simple\npitch-class representation was used. This allowed pitch in-\ntervals at different points in the musical scale to be grouped\ninto the same production rules. It was assumed that di-\nrection of pitch would also be an important factor, so the\nPitch-Class (PC) attribute allowed the following range of\nintervals: [-11, 11]. Melodic embellishment rules often\napply to the same movements of intervals within a musi-\ncal scale. For this reason, the Key-Relative Pitch-Class\n(KPC) was also used, which allowed a range of intervals\nfrom [-7, 7], measuring the distance in diatonic steps be-\ntween two consecutive notes.\nMetrical Onset For encoding the metrical relation-\nships between two notes, the metric delta representation\nwas borrowed from previous research [11]. This metric\ndelta assigns every onset to a level in a metrical hierarchy.\nThe metrical hierarchy is composed of levels of descending\nimportance, based on their onset location within a metrical\ngrid. The onsets were assigned a level based on their clos-\nest onset location in the metrical hierarchy. This metrical\nhierarchy was also used in GTTM for the metrical structure\ntheory [16].\nBecause the GTTM dataset contains either 100 or 300\nsolutions (for prolongational reduction trees and time-span\ntrees, respectively), the data representations had to be de-\nsigned to limit the number of unique production rules cre-\nated in the PCFG. With too many production rules, there\nis an increased chance of production rules that have a zero\nprobability (due to the rule not existing in the training set),\nwhich results in the failure to parse certain test melodies.\nTherefore, two separate metrical onset attributes were cre-\nated. One which represented the full metrical hierarchy,\nnamed Metric Delta Full (Met1) , and one which repre-\nsented only the change in metric delta (whether the met-\nric level of the subsequent note was higher, the same, or\nlower than the previous note), named Metric Delta Re-\nduced (Met0) .\nHarmonic Relationship This research was also de-\nsigned to test whether or not the information of a note’s\nrelationship to the underlying harmony was useful in the\nmelodic reduction process. A Chord Tone Change (CT)\nattribute was therefore created, which labelled whether or\nnot each note in the interval was a chord tone. This created\nfour possibilities: a chord tone followed by a chord tone, achord tone followed by a non-chord tone, a non-chord tone\nfollowed by a chord tone and a non-chord tone followed by\na non-chord tone. This rule was designed to test whether\nharmonic relationships affected the reduction process.\n6. THE EXPERIMENT\nGiven a method for creating a treebank with any interval-\nbased data representation from the GTTM dataset and in-\nducing the corresponding PCFG, an experiment was de-\nsigned to test the efﬁcacy of different data representa-\ntions when applied to the process of melodic reduction.\nThis section details the experiment that was performed.\nFirst, different representations that were tested are pre-\nsented. Then, the comparison and evaluation method are\ndescribed. Finally, the results of cross-fold evaluation for\nthe PCFG created with each different data representation\nare shown.\n6.1 Comparison\nThe comparison method chosen was identical to the meth-\nods used in other experiments of the same type, in which\nthe output of the system is a tree structure, and the tree so-\nlutions are available [13, 15]. First, for a given test, the\ninput melody is parsed, which yields the most probable\nparse tree as an output. The output trees are then com-\npared with the solution trees. To do so, the tree is simply\ntraversed, and each node from the output tree is compared\nfor equivalence to the corresponding node in the solution\ntree. This method is somewhat strict, in that mistakes to-\nwards the bottom of the tree will be propagated upwards,\nso incorrect rule applications will be counted as incorrect\nin multiple places.\n6.2 Evaluation\nCross-fold evaluation was used to perform the evaluation.\nThe entire treebank of solutions were ﬁrst partitioned into 5\nsubsets, and 1 subset was used for the test set in 5 iterations\nof the training and comparison process. The results were\nthen averaged. In order to keep consistency across data\nrepresentations, the same test and training sets were used\nfor each cross-validation process.\n6.3 Results\nEach data representation that was selected was performed\non both the set of time-span reduction trees and the set of\nprolongational reduction trees, when possible. As men-\ntioned previously, the set of prolongational reduction trees\namounted to only 100 samples, while the time-span trees\namounted to 300. In some situations, the data representa-\ntion would create too many unique production rules, and\nnot all the test melodies could be parsed. All of the data\nrepresentations in the results table had at least a 90% cov-\nerage of the test melodies, meaning that at least 90% of the\ntests could be parsed and compared. There are also two\ndata representations that use time-span trees with the har-\nmonic representation. For these tests, the solution set con-\ntained only 100 samples as opposed to the usual 300 forProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 779time-span trees, since there is only harmonic information\nfor 100 of the 300 melodies.\nTree- % nodes\ntype PC KPC Met1 Met0 CT correct\nTS X 35.33\nPR X 38.57\nTS X X 40.40\nPR X X 38.50\nTS X X 44.12\nPR X X 46.55\nTS X X X 44.80\nPR X X X 46.74\nThese results mostly progress as one might expect.\nLooking at only the tests done with time-span trees, the re-\nsults improve initially when using the Key-Relative Pitch-\nClass encoding for pitch intervals paired with the Chord\nTone Change feature; it received a 5% increase as com-\npared with the PCFG that only used the Pitch-Class fea-\nture (which could be considered a baseline). It gained an\neven bigger increase when using the Metric Delta Full\nfeature, an almost 9% increase in efﬁcacy compared with\nthePitch-Class test. Combining metric and chord features\nwith the Key-Relative Pitch-Class encoding did not pro-\nvide much further gain that with the metric feature alone.\nThe prolongational reduction also improved when given\nthe metric delta information, however the harmonic rela-\ntionship feature affected the outcome very little.\nThe best performing PCFG was induced from the pro-\nlongational reduction trees, and used a data representa-\ntion that included the Key-Relative Pitch-Class encod-\ning combined with both the simpliﬁed metric delta and the\nchord tone information.\nIt is possible that the lack of data and the subsequent\nlimitation on the complexity of the data representation\ncould be avoided by the use of probabilistic smoothing\ntechniques (to estimate the distributions of those rules that\ndid not exist in the training set) [14, 97]. Indeed, the use of\ntheKey-Relative Pitch-class feature as the basis for most\nof the representations was an attempt to limit the num-\nber of resulting rules, and therefore the number of zero-\nprobability rules. This would be an appropriate topic for\nfuture experimentation.\nA speciﬁc example helps to illustrate both the effec-\ntiveness and the drawbacks of using the induced PCFG\nfor melodic reduction. Figure 5 displays the iterative re-\nductions applied by pruning a PCFG tree, level by level.\nThe grammar used to create this reduction was trained\non prolongational reduction trees, and included the Key-\nRelative Pitch-class intervals, with notations for the Met-\nric Delta Reduced feature, and the Chord Tone Change\nfeature. This PCFG was the best performing, according to\nthe evaluation metric. From a musicological perspective,\nthe PCFG initially makes relatively sound decisions when\nreducing notes from the music surface. It is only when it\nbegins to make decisions at the deeper levels of reduction\nthat it chooses incorrect notes as the more important tones.\nFigure 5 : A set of melodies that show the progressive re-\nductions, using the data representation that includes key-\nrelative pitch-class, metric delta and chord tone features.\n7. CONCLUSION\nThis research has performed for the ﬁrst time the induction\nof a PCFG from a treebank of solutions for the process of\nmelodic reduction. It was shown that, for the most part,\nadding metric or harmonic information in the data repre-\nsentation improves the efﬁcacy of the resulting probabilis-\ntic model, when analyzing the results for the model’s abil-\nity to reduce melodies in a musically sound way. A speciﬁc\nexample reduction was generated by the best-performing\nmodel. There is still much room for improvement, be-\ncause it seems that the model is more effective at identi-\nfying melodic embellishments on the musical surface, and\nis not able to identify the most important structural notes\nat deeper layers of the melodic reductions. The source\ncode for this work also allows any researcher to create\ntheir own interval representations, and convert the GTTM\ndataset into a PCFG treebank.\nThere are some speciﬁc areas of improvement that\nmight beneﬁt this method. Currently there is no way to\nidentify which chord a note belongs to with the grammar—\nthe harmonic data is simply a boolean that describes\nwhether or not the note is a chord tone. If there were a\nway to identify which chord the note belonged to, it would\nlikely help with the grouping of larger phrases in the re-\nduction hierarchy. For example, if a group of consecutive\nnotes belong to the same underlying harmony, they could\nbe grouped together, which might allow the PCFG to bet-\nter identify the more important notes (assuming they fall\nat the beginning or end of phrases/groups). Beyond that, it\nwould be greatly helpful if the sequences of chords could\nbe considered as well. Furthermore, there is no way to ex-\nplicitly identify repetition in the melodies with this model.\nThat, too, might be able to assist the model, because if it\ncan identify similar phrases, it could potentially identify\nthe structural notes on which those phrases rely.\nThe source code for this research is available to the pub-\nlic, and can be found on the author’s github account1.\n1http://www.github.com/bigpianist/SupervisedPCFG MelodicReduction780 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20168. REFERENCES\n[1] Samer A. Abdallah and Nicolas E. Gold. Comparing\nmodels of symbolic music using probabilistic gram-\nmars and probabilistic programming. In Proceedings of\nthe International Computer Music Conference , pages\n1524–31, Athens, Greece, 2014.\n[2] Samer A. Abdallah, Nicolas E. Gold, and Alan Mars-\nden. Analysing symbolic music with probabilistic\ngrammars. In David Meredith, editor, Computational\nMusic Analysis , pages 157–89. Springer International,\nCham, Switzerland, 2016.\n[3] John W. Backus. The syntax and semantics of the pro-\nposed international algebraic language of the Zurich\nACM-GAMM conference. In Proceedings of the Inter-\nnational Conference for Information Processing , pages\n125–31, Paris, France, 1959.\n[4] Mario Baroni, R. Brunetti, L. Callegari, and C. Ja-\ncoboni. A grammar for melody: Relationships between\nmelody and harmony. In Mario Baroni and L Calle-\ngari, editors, Musical Grammars and Computer Anal-\nysis, pages 201–18, Florence, Italy, 1982.\n[5] Mario Baroni and C. Jacobini. Analysis and generation\nof Bach’s chorale melodies. In Proceedings of the In-\nternational Congress on the Semiotics of Music , pages\n125–34, Belgrade, Yugoslavia, 1975.\n[6] Mario Baroni and C. Jacoboni. Proposal for a gram-\nmar of melody: The Bach Chorales . Les Presses de\nl’Universit ´e de Montr ´eal, Montreal, Canada, 1978.\n[7] David Beach. A Schenker bibliography. Journal of Mu-\nsic Theory , 13(1):2–37, 1969.\n[8] Noam Chomsky. Three models for the description of\nlanguage. Institute of Radio Engineers Transactions on\nInformation Theory , 2:113–24, 1956.\n[9] Noam Chomsky. On certain formal properties of gram-\nmars. Information and Control , 2(2):137–67, 1959.\n[10] B. Frankland and Annabel J. Cohen. Parsing of\nmelody: Quantiﬁcation and testing of the local group-\ning rules of Lerdahl and Jackendoff’s “A generative\ntheory of tonal music”. Music Perception , 21(4):499–\n543, 2004.\n[11] ´Edouard. Gilbert and Darrell Conklin. A probabilistic\ncontext-free grammar for melodic reduction. In Pro-\nceedings for the International Workshop on Artiﬁcial\nIntelligence and Music, International Joint Conference\non Artiﬁcial Intelligence , pages 83–94, Hyderabad, In-\ndia, 2007.\n[12] Masatoshi Hamanaka, K. Hirata, and Satoshi Tojo.\nσGTTM III: Learning based time-span tree genera-\ntor based on PCFG. In Proceedings of the Symposium\non Computer Music Multidisciplinary Research , Ply-\nmouth, UK, 2015.[13] Masatoshi Hamanaka, Keiji Hirata, and Satoshi Tojo.\nImplementing “A generative theory of tonal music”.\nJournal of New Music Research , 35(4):249–77, 2007.\n[14] Daniel Jurafsky and James H. Martin. Speech and\nlanguage processing: An introduction to natural\nlanguage processing, computational linguistics, and\nspeech recognition . Prentice Hall, Upper Saddle River,\nNJ, 1st edition, 2000.\n[15] Phillip B. Kirlin. A probabilistic model of hierarchi-\ncal music analysis . Ph.D. thesis, University of Mas-\nsachusetts Amherst, Amherst, MA, 2014.\n[16] Fred Lerdahl and Ray Jackendoff. A generative theory\nof tonal music . The MIT Press, Cambridge, MA, 1983.\n[17] Fred Lerdahl and Yves Potard. La composition as-\nsist´ee par ordinateur . Rapports de recherche. Insti-\ntut de Recherche et Coordination Acoustique/Musique,\nCentre Georges Pompidou, Paris, France, 1986.\n[18] Edward Loper and Steven Bird. NLTK: The natural\nlanguage toolkit. In Proceedings of the Workshop on\nEffective Tools and Methodologies for Teaching Natu-\nral Language Processing and Computational Linguis-\ntics, volume 1, pages 63–70, Stroudsburg, PA, 2002.\n[19] Alan Marsden. Recognition of variations using auto-\nmatic Schenkerian reduction. In Proceedings of the\nInternational Conference on Music Information Re-\ntrieval , pages 501–6, Utrecht, Netherlands, August 9-\n13 2010.\n[20] Christopher Roads and Paul Wieneke. Grammars as\nrepresentations for music. Computer Music Journal ,\n3(1):48–55, March 1979.\n[21] Nicolas Ruwet. Theorie et methodes dans les etudes\nmusicales. Musique en Jeu , 17:11–36, 1975.\n[22] Stephen W. Smoliar. Music programs: An approach to\nmusic theory through computational linguistics. Jour-\nnal of Music Theory , 20(1):105–31, 1976.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 781"
    },
    {
        "title": "Improving Voice Separation by Better Connecting Contigs.",
        "author": [
            "Nicolas Guiomard-Kagan",
            "Mathieu Giraud",
            "Richard Groult",
            "Florence Levé"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417825",
        "url": "https://doi.org/10.5281/zenodo.1417825",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/129_Paper.pdf",
        "abstract": "Separating a polyphonic symbolic score into monophonic voices or streams helps to understand the music and may simplify further pattern matching. One of the best ways to compute this separation, as proposed by Chew and Wu in 2005 [2], is to first identify contigs that are portions of the music score with a constant number of voices, then to progressively connect these contigs. This raises two ques- tions: Which contigs should be connected first? And, how should these two contigs be connected? Here we propose to answer simultaneously these two questions by consid- ering a set of musical features that measures the quality of any connection. The coefficients weighting the features are optimized through a genetic algorithm. We benchmark the resulting connection policy on corpora containing fugues of the Well-Tempered Clavier by J. S. Bach as well as on string quartets, and we compare it against previously pro- posed policies [2, 9]. The contig connection is improved, particularly when one takes into account the whole content of voice fragments to assess the quality of their possible connection.",
        "zenodo_id": 1417825,
        "dblp_key": "conf/ismir/Guiomard-KaganG16",
        "content": "IMPROVING VOICE SEPARATION BY BETTER CONNECTING CONTIGS\nNicolas Guiomard-Kagan1Mathieu Giraud2Richard Groult1Florence Lev ´e1,2\n1MIS, Univ. Picardie Jules Verne, Amiens, France2CRIStAL, UMR CNRS 9189, Univ. Lille, Lille, France\n{nicolas,mathieu,richard,florence}@algomus.fr\nABSTRACT\nSeparating a polyphonic symbolic score into monophonic\nvoices or streams helps to understand the music and may\nsimplify further pattern matching. One of the best ways\nto compute this separation, as proposed by Chew and Wu\nin 2005 [2], is to ﬁrst identify contigs that are portions of\nthe music score with a constant number of voices, then to\nprogressively connect these contigs. This raises two ques-\ntions: Which contigs should be connected ﬁrst? And, how\nshould these two contigs be connected? Here we propose\nto answer simultaneously these two questions by consid-\nering a set of musical features that measures the quality of\nany connection. The coefﬁcients weighting the features are\noptimized through a genetic algorithm. We benchmark the\nresulting connection policy on corpora containing fugues\nof the Well-Tempered Clavier by J. S. Bach as well as on\nstring quartets, and we compare it against previously pro-\nposed policies [2, 9]. The contig connection is improved,\nparticularly when one takes into account the whole content\nof voice fragments to assess the quality of their possible\nconnection.\n1. INTRODUCTION\nPolyphony, as opposed to monophony, is music created\nby simultaneous notes coming from several instruments or\neven from a single polyphonic instrument, such as the pi-\nano or the guitar. Polyphony usually implies chords and\nharmony, and sometimes counterpoint when the melody\nlines are independent.\nV oice separating algorithms group notes from a\npolyphony into individual voices [2,4,9,11,13,15]. These\nalgorithms are often based on perceptive rules, as studied\nby Huron [7] or Deutsch [5, chapter 2], and at the ﬁrst place\npitch proximity – voices tend to have small intervals.\nSeparating polyphony into voices is not always possible\nor meaningful: many textures for polyphonic instruments\ninclude chords with a variable number of notes. Con-\nversely, one can play several streams on a monophonic in-\nstrument. Stream separation algorithms focus thus on a\nc/circlecopyrtNicolas Guiomard-Kagan, Mathieu Giraud, Richard\nGroult, Florence Lev ´e. Licensed under a Creative Commons Attribution\n4.0 International License (CC BY 4.0). Attribution: Nicolas Guiomard-\nKagan, Mathieu Giraud, Richard Groult, Florence Lev ´e. “Improving\nvoice separation by better connecting contigs”, 17th International Soci-\nety for Music Information Retrieval Conference, 2016.narrower scale, extracting groups of coherent notes. These\nsegments are not necessarily connected throughout the\nwhole score: a voice can be split into several streams and\na stream can cluster notes from different voices [14, 16].\nBoth voice and stream segmentation algorithms provide\na better understanding of polyphony and make inference\nand matching for relevant patterns easier. We previously\nshowed that voice and stream separation algorithms are\ntwo facets of the same problem that can be compared with\nsimilar evaluation metrics [6]. Pertinent evaluation met-\nrics measure how segments or voices of the ground truth\nare grouped together in the algorithms predictions, as the\ntransition-based evaluation [2] or the measure of mutual\ninformation [6, 12].\nBased on these metrics, it appears that the contig ap-\nproach, as initially proposed by Chew and Wu [2] (Sec-\ntion 2), is one of the best approaches to separate voices,\nstarting from contigs having a constant number of voices.\nThe results depends on how the contigs are connected ,\nlarger voice or stream segments being built starting from\nsmaller ones.\nIn this article we propose and compare several criteria\nto ground the connection policy , that is both the choice of\nthe order of the contigs to be connected, and the connec-\ntion itself between contigs. In addition to the criteria used\nin the literacy, we introduce new criteria that take into ac-\ncount more musical context , averaging pitches and dura-\ntions over voice fragments (Section 3). We weight these\ncriteria using a genetic algorithm (Section 4). We show\nhow some values of these criteria can partially simulate\nthe previous methods, and evaluate the results on sets of\nfugues and string quartets. By improving this contig con-\nnection, we improve the precision of voice separation al-\ngorithms (Section 5). We further study the distribution of\nfailures, showing that a higher precision can be obtained\nby stopping the contig connection before the connection\nquality drops.\n2. VOICE SEPARATION BASED ON CONTIGS\nThe contig approach, proposed by Chew and Wu (denoted\nby CW in the following) ﬁrst separates the music score into\ncontigs that have a constant number of notes played at the\nsame time then progressively connect these contigs to the\nwhole score [2].\nThe ﬁrst step splits the input polyphonic data into\nblocks called contigs such that the number of simultaneous\nnotes in a contig does not change (Figure 1). Notes cross-164Figure 1 . In this piano-roll symbolic representation, each\nsegment describes a note. The horizontal axis represents\ntime and the vertical axis represents pitches. The notes\ncan be grouped in four contigs , each of them containing a\nconstant number of notes played at the same time. Con-\ntig 2 contains three voice fragments 2a,2band2c. The\nchallenge of contig-based voice separation algorithms is to\nconnect these voice fragments across contigs to build co-\nherent voices throughout the score. The non-vertical dotted\nlines show a possible solution of the voice separation.\ning the border of several contigs are split in several notes.\nThe idea behing building contigs is that the voice separa-\ntion is relatively easy inside them: Notes in each contig are\ngrouped by pitch height to form voice fragments .\nThe second step links together fragments from distinct\ncontigs, following some musical principles (Figure 2). The\nalgorithm has now to take two kinds of decisions, follow-\ning what we call a connection policy :\n•which contigs should be connected ﬁrst?\n•how should these two contigs be connected?\nFigure 2 . Any connection policy should decide which\ncontigs should be connected (such as, for example, 1\nand 2) and how to do this connection. There are here\nthree possible connections (without voice crossing) be-\ntween the contigs 1 and 2: C1={(1a,2a),(1b,2b)},\nC2={(1a,2a),(1b,2c)}, andC3={(1a,2b),(1b,2c)}.\nOrder of connection of contigs. In CW algorithm, the con-\nnection starts from the maximal contigs (i.e. contigs con-\ntaining the maximal number of voices). Since the voices\ntend not to cross, the voice separation and connection in\nthese contigs with many voices were thought to be more\nreliable. Then, CW continues the connection process to\nthe left and to the right of these maximal contigs. In Fig-\nure 1, the CW policy will thus connect contigs 1,2,3, then\nﬁnally 0,1,2,3.\nIshigaki, Matsubara and Saito (denoted IMS in the fol-\nlowing) suggested another connection policy, starting withminimal contigs and connecting contigs with an increasing\nnumber of fragments (i.e. the number of fragments in the\nleft contig is lower or equal to the number of fragments in\nthe right contig) [9]. The idea is that the (local) start of a\nnew voice is a more perceptible event than the (local) end\nof a voice. Once all those possible connections are done,\nmaximal contigs are considered as in CW algorithm to ter-\nminate the process. In Figure 1, IMS policy will connect\ncontigs 0,1, then 0,1,2, and ﬁnally 0,1,2,3.\nFragment connection. The policy to connect fragments of\nthe original CW algorithm, reused by IMS, is based on\ntwo principles: Intervals are minimized between succes-\nsive notes in the same stream or voice (pitch proximity);\nV oices tend not to cross. Formally, the connection between\ntwo contigs is a set of (/lscript,r)fragments that maximize a\nconnection score . This score is here based on the absolute\ndifference between the pitch of the last note of the left frag-\nment/lscriptand the pitch of the ﬁrst note of the right fragment\nr. There is moreover a very large score for the connection\nof notes split between two contigs to keep them in the same\nﬁnal voice.\n3. MORE MUSICAL FEATURES TO IMPROVE\nTHE CONNECTION POLICY\n3.1 A new view on the contig-based approach\nWe argue that the two questions of the connection pol-\nicy (which contigs should be connected? how to connect\nthem?) should be handled at a same time: to build coherent\nvoices across a piece, one should always connect the con-\ntigs yielding the “safest” connections between voice frag-\nments. The quality of these connections should be properly\nevaluated with musical features that will be introduced be-\nlow.\nGiven two successive contigs iandi+ 1, and one way\nCto connect them (set of pairs of fragments), we deﬁne a\nconnection score S(i,C), computed as a weighted sum of\nmusical features, that measures the quality of this connec-\ntion: The higher the connection score, the safer the con-\nnection. The connection scores will extend the ones used\nby CW and IMS, that did not systematically explore the re-\nlation between the two decisions of the connection policy.\nAt each step of the algorithm, the (i,C)maximizing S\nis selected, giving both the “best contigs” to connect and\nthe “best way” to connect them. Once this connection is\nmade, the connections scores between the newly formed\ncontig and its left and right neighbors have to be computed.\nDeﬁnitions. Letnbe the maximal number of simultaneous\nnotes in the piece. Let ni(respectively ni+1) be the max-\nimal number of voices of the contig i(i+ 1). After some\nconnections have been made, a contig may have a different\nnumber of simultaneous notes at its both extremities, but\nthe hanging voices are “projected” to these extremities.\nFor two successive contigs iandi+ 1, letCbe a set\nof pairs (/lscript,r), where/lscriptis a fragment of the (left) con-\ntigiandra fragment of the (right) contig i+ 1, eachProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 165fragment appearing at most once in C(Figure 2). Chas\nthus at most m= min(ni,ni+1)elements, and, in the\nfollowing, we only consider sets with melements, that is\nwith the highest possible number of connections. Denot-\ningM= max(ni,ni+1), there areM!/(M−m)!different\nsuch combinations for C, and only/parenleftbigM\nm/parenrightbig\nif one restricts to\nthe combinations without voice crossing.\nWe consider that we have N features\nf1(i,C),f2(i,C)...fN(i,C) characterizing some\nmusical properties of the connection Cbetween contigs\niandi+ 1. Each feature fk(i,C)has a value between\n0 and 1. Finally let α1,α2,...,α NbeNcoefﬁcients\nsuch that/summationtextN\nk=1αk= 1 . We then deﬁne the con-\nnection score as a linear combination of the features\nS(i,C) =/summationtextN\nk=1αkfk(i,C).\nIn the two following paragraphs, we propose different\nfeaturesfk(i,C)depending on the musical properties of\ncontigs and fragments. The values of the coefﬁcients αk\nwill be discussed in Section 4.\n3.2 Features on the contigs\nFirst we consider features that are not related to the con-\nnectionCbut depend only on the contigs, more precisely\non the maximum number of voices in each contig.\n•maximal voices(i) = max(ni,ni+1)/n. The closer\nthe number of voices to the maximal number of\nvoices, the higher the connection score .\n•minimal voices(i) = (n+1−min(ni,ni+1))/n. The\ncloser the number of voices to 1, the higher the con-\nnection score.\nOne can in particular favor some contig connection\nbased on the comparison of the number of voices between\nthe left and the right contigs:\n•difference nbvoices(i) = 1−(|ni−ni+1|/(n−1)).\nThe closer the number of voices of the left and the\nright contigs, the higher the connection score.\nOr with the following binary features, that will equal 0\nif the condition is not met:\n•increase(i) = 1iffni<ni+1;\n•increase one(i) = 1iffni+ 1 =ni+1;\n•increase equal(i) = 1iffni≤ni+1;\n•decrease(i) = 1iffni>ni+1;\n•decrease one(i) = 1iffni−1 =ni+1;\n•decrease equal(i) = 1iffni≥ni+1.\nThose features are inspired by the connection policy of\nthe existing algorithms. The maximal voices(i) feature re-\nﬂects the idea used by the CW algorithm: It is safer to\nﬁrst connect contigs having a large number of voices. The\nreverse idea, as measured by minimal voices(i) , was pro-\nposed together with the increase(i) idea by the IMS algo-\nrithm, favoring the connection of contigs with an increas-\ning number of voices. The idea is that the (local) start of anew voice is a more perceptible event than its (local) end.\nThis is even more remarkable in contrapuntal music such\nas fugues where enterings of voice on thematic patterns\n(subjects, counter-subjects) are often clearly heard.\nWe propose to further use the increase one(i) feature\nthat should better assert an entry of exactly one new\nvoice. Conversely, we also evaluate the opposite idea ( de-\ncrease(i) ,decrease one(i) ,decrease equal(i) ).\nFinally the connection could favor successive contigs\nsharing a same note:\n•maximal simnotes(i) =n=/min(ni,ni+1), where\nn=is the number of notes with the same pitch and\nsame onset (i.e. note split in two) at the extremities\nof contigsiandi+ 1. The more the contigs share\ncommon notes, the higher the connection score is.\nThis feature derives from the original implementation\nof CW, where connectig contigs with shared notes was\nawarded a very large score.\n3.3 Features on the fragments\nNow we consider features based on the individual fragment\nconnections ( /lscript,r) composing C.\nPitches. How can we measure the quality of connecting a\nfragment/lscriptto a fragment r? The main criterion of the CW\nand IMS algorithms was to follow the pitch proximity prin-\nciple, favoring connections of fragments having a small\npitch interval. Given Cand(/lscript,r)∈C, let lastpitch(/lscript)\nandﬁrst pitch(r)be the pitches of the extreme note of the\nleft fragment /lscriptand the right fragment r:\n•extreme pitch(C)= 1−/summationtext\n(/lscript,r)∈C|lastpitch(/lscript)−\nﬁrst pitch(r)|/ν. The closer the pitches between the\nconnected notes, the higher the connection score.\nThe normalization factor ν= 60·|C|semitones was\nchosen in order to range the feature value between 0 (5 oc-\ntaves between connected pitches) and 1 (equal pitches).\nHowever, this extreme pitch(C)score only considers one\nnote on each side. We propose to extend this feature by\nevaluating the pitch range coherence , taking into account\nthe average pitch ( average pitch ) of all notes of one or\nboth fragments. Indeed, voices tend to have the same pitch\nrange throughout the piece, and moreover through the frag-\nments:\n•avgpitch right(C)= 1−/summationtext\n(/lscript,r)∈C|lastpitch(/lscript)−\naverage pitch(r)|/ν;\n•avgpitch left(C) = 1 −/summationtext\n(/lscript,r)∈C|average pitch(/lscript)−lastpitch(r)|/ν;\n•avgpitch(C)= 1−/summationtext\n(/lscript,r)∈C|average pitch(/lscript)−\naverage pitch(r)|/ν.\nSome voice separation algorithms assign each note to\nthe voice with the closest average pitch [10]. These algo-\nrithms are quite efﬁcient, and the avgpitch(C)feature re-\nproduces this idea at a local scale: Given a fragment with166 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016a few notes, even if one may not know to which (global)\nvoice it belongs, one already knows a local pitch range.\nDurations. Similarly, we can measure the difference of\ndurations to favor connection of contiguous fragments with\na same rhythm. Indeed, the musical textures of each voice\ntend to have coherent rhythms. For instance, a voice in\nwhole notes and another one in eights will often be heard\nas two separate voices, even if they use very close pitches.\nGivenCand(/lscript,r)∈C, letlastdur(/lscript)andﬁrst dur(r)be\nthe durations, taken in a log scale, of the extreme notes of\nthe left fragment /lscriptand the right fragment r:\n•extreme dur(C)= 1−(/summationtext\n(/lscript,r)∈C|lastdur(/lscript)−\nﬁrst dur(r)|/λ). The closer the durations between\nthe connected notes, the higher the connection score.\nThe normalization factor λ= 6·|C|accounts for\nthe maximal difference (in a log scale) between whole\nnotes (6) and 64th notes, the shortest notes in our corpora\n(0). Once more, this feature can also be extended to take\ninto account the average log duration ( average dur) of one\nor both fragments instead of the duration of the extreme\nnote:\n•avgdurright(C)= 1−/summationtext\n(/lscript,r)∈C|lastdur(/lscript)−\naverage dur(r)|/λ;\n•avgdurleft(C)= 1−/summationtext\n(/lscript,r)∈C|average dur(/lscript)−\nlastdur(r)|/λ;\n•avgdur(C)= 1−/summationtext\n(/lscript,r)∈C|average dur(/lscript)−\naverage dur(r)|/λ.\nThese features measure how a fragment may be “mostly\nin eights” or “mostly in long notes”, even if it contains\nother durations as for ending notes. They handle also\nrhythmic patterns: a fragment repeating the pattern “one\nquarter, two eights” has an average durof about 3 + 1/3.\nVoice crossings. Finally, two features control the voice\ncrossing. On one hand, voice crossings do exist, on the\nother hand, they are hard to predict. V oice separation algo-\nrithms (such as CW and IMS) usually prevent them.\n•crossed voices(C)= 1 ifCcontains a crossing\nvoice, and 0otherwise;\n•nocrossed voices(C)= 1 ifCdoes not contain a\ncrossing voice, and 0otherwise.\n4. LEARNING COEFFICIENTS THROUGH A\nGENETIC ALGORITHM\nThe selection of features coefﬁcients α= (α1,α2,...α N)\nwas achieved with a genetic algorithm with mutation and\ncrossover operators [1]. For computation efﬁciency, a gen-\neration is a set of 60solutions, each solution being a set of\ncoefﬁcients totaling 1. The ﬁrst generation G0is a set of\nsolutions drawn with random values. The following gener-\nations are built through mutations and crossovers.Mutation. Given a generation Gt, each solution is mu-\ntated 4times, giving 4×60mutated solutions. Each muta-\ntion consists in randomly transferring a part of the value of\na randomly chosen coefﬁcient into another one. A new set\nof40solutions is selected from both the original solutions\nand the mutated solutions, by taking the 30 best solutions\nand 10 random other solutions.\nCrossover. The solutions in this set are then used to\ngenerate 20children solutions by taking random couples\nof parents. Each parent is taken only once, and a child\nsolution is the average of the coefﬁcients of the parent so-\nlutions. The new generation Gt+1is formed by the 40par-\nents and the 20children solutions.\n5. RESULTS\nWe trained the coefﬁcients weighting the features with the\ngenetic algorithm on the 24 fugues in the ﬁrst book of\ntheWell-Tempered Clavier by J. S. Bach (corpus “wtc-\ni”). This gives the set of coefﬁcients GA1 after 36 genera-\ntions (the process stabilized after that). We then evaluated\nthese GA1 coefﬁcients and other connection policies on the\n24 fugues of the second book of the Well-Tempered Cla-\nvier(corpus “wtc-ii”) and on 17 ﬁrst movements of classi-\ncal and romantic string quartets (Haydn op. 33-1 to 33-6,\nop. 54-3, op. 64-4, Mozart K80, K155, K156, K157 and\nK387, Beethoven op. 18-2, Brahms op. 51-1 and Schubert\nop. 125-1). Our implementation is based on the Python\nframework music21 [3], and we worked on .krn ﬁles\ndownloaded from kern.ccarh.org [8]. The explicit\nvoice separation coming from the spines of these ﬁles\nforms the ground truth on which the algorithms are trained\nand evaluated.\n5.1 Learned coefﬁcients\nThe column GA1 of Table 1 shows the learned coefﬁcients\nof the best solution. The high nocrossed voices(C)coef-\nﬁcient conﬁrms that trying to predict crossing voices cur-\nrently gives many false connections. It may suggest that\nsuch detection should be avoided until speciﬁc algorithms\ncould handle these cases. We draw two other observations:\n•The pitch is the most important feature\n(the four pitch coefﬁcients totaling 0.271).\nHowever, avgpitch right(C)is higher than\nextreme pitch(C)– and summing avgpitch left(C),\navgpitch right(C)and avgpitch(C)gives 0.181,\ntwice extreme pitch(C). This conﬁrms that using\nthe pitch range coherence is more reliable than\nusing the pitch proximity alone;\n•The durations are also important features, especially\nwhen one takes the average durations ( avgdur(C)\noravgdurright(C), totaling 0.121). Note that the\nextreme dur(C)coefﬁcient is very low, conﬁrming\nthe idea that even if the individual durations change,\nrhythmic textures or small-scale patterns are con-\nserved inside voice fragments.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 167Finally, the increase equal(i) feature as suggested by\nIMS is high, but, surprisingly, the decrease equal(i) fea-\nture is also high. These two features combined seem to un-\nderline that the contig connection is safer when both frag-\nments have the same number of notes. Further experiments\nshould be made to explore these features.\n5.2 Quality of the connection policy\nEvaluation metrics. Thetransition recall (TR-rec) (orcom-\npleteness ) is the ratio of correctly assigned transitions (pair\nof notes in the same voice) over the number of transi-\ntions in the ground truth. The transition precision (TR-\nprec) (orsoundness ) is the ratio of correctly assigned tran-\nsitions over the number of transitions in the predicted\nvoices [2,6,11]. The TR-rec and TR-prec metrics are equal\nfor voice separation algorithms connecting voices through-\nout all the piece. Stream segmentation algorithms usu-\nally lead to higher TR-prec values as they predict fewer\ntransitions. The ground truth and the output of the algo-\nrithms can also be considered as an assignation of a label\nto every note, enabling to compute the SoandSumet-\nrics based on normalized entropies H(output|truth)and\nH(truth|output ). These scores report how an algorithm\nmay over-segment ( So) or under-segment ( Su) a piece\n[6, 12]. They measure whether the clusters are coherent,\neven when streams cluster simultaneous notes. Moreover,\nwe point out the contig connection correctness (CC) , that is\nthe ratio of correct connections over all connections done.\nResults. Table 2 details the evaluation metrics on the train-\ning set and the evaluation sets, both for the GA1 coef-\nﬁcients and for coefﬁcients SimCW and SimIMS simu-\nlating the CW and IMS policies, displayed on Table 1.\nThe metrics reported here may be slightly different from\nthe results reported in the original CW and IMS imple-\nmentations [2, 9]. The goal of our evaluation is to eval-\nuate connection policies inside a same implementation.\nOn all corpora, the GA1 coefﬁcients obtain better TR-\nprec/TR-rec/CC results than the SimCW and SimIMS co-\nefﬁcients. The GA1 coefﬁcients indeed make better con-\nnections (more than 87% of correct connections on the test\ncorpus “wtc-ii”). The main source of improvement comes\nfrom the new features that consider the average pitches\nand/or lengths, as showed by the example on Figure 3.\n5.3 Lowering the failures by stopping the connections\nThe ﬁrst step of CW, the creation of contigs, is very reli-\nable: TR-prec is more than 99% on both fugues corpora\n(lines “no connection” in Table 2). Most errors come from\nthe connection steps. We studied the distribution of these\nerrors. With the SimIMS coefﬁcients, and even more with\nthe GA1 coefﬁcients, the ﬁrst connections are generally\nreliable, more errors being done in the last connections\n(Figure 4). This conﬁrms that considering more musical\nfeatures improves the connections.\nBy stopping the algorithm with the GA1 coefﬁcients\nwhen 75% of the connections have been done, almost halfFeature GA1 SimCW SimIMS\nincrease(i) 0.004 0 0\nincrease one(i) 0.004 0 0\nincrease equal(i) 0.137 0 0.250\ndecrease(i) 0.013 0 0\ndecrease one(i) 0.019 0 0\ndecrease equal(i) 0.112 0 0\ndifference nbvoices(i) 0.009 0 0\nmaximal voices(i) 0.026 0.500 0\nminimal voices(i) 0.007 0 0.250\nmaximal simnotes(i) 0.007 0 0\ncrossed voices( C) 0.009 0 0\nnocrossed voices( C) 0.248 0.250 0.250\nextreme pitch(C) 0.090 0.250 0.250\navgpitch right(C) 0.117 0 0\navgpitch left(C) 0.023 0 0\navgpitch(C) 0.041 0 0\nextreme dur(C) 0.007 0 0\navgdurright(C) 0.048 0 0\navgdurleft(C) 0.006 0 0\navgdur(C) 0.073 0 0\nTable 1 . Coefﬁcients weighting the musical features used\nto measure the connection quality, with best coefﬁcients\nlearned on the wtc-i corpus (GA1) and coefﬁcients simu-\nlating the connection policy of CW and IMS.\nof the bad connections are avoided, giving streams with\na good compromise between precision and consistency\n(lines “GA1-75%” in Table 2).\n5.4 Other sets of coefﬁcients\nTo assess reproducibility, we ran the experiment two other\ntimes. The learned coefﬁcients GA1/primeand GA1/prime/primeare very\nclose to GA1 (data not shown) and give comparable re-\nsults on the learning corpus “wtc-i” (TR-prec = 97.83%\nand 97.81%, instead of 97.84%). We also optimized coef-\nﬁcients to ﬁnd a worst solution (data not shown). The co-\nefﬁcients values crossed voices(C)andminimal voices(i)\nstand out. This conﬁrms that predicting crossing voices is\ndifﬁcult and than small contigs are difﬁcult to connect.\n6. CONCLUSION\nV oice and stream separation are improved when one opti-\nmizes at the same time when andhow the voice fragments\nshould be connected. We explored several features to eval-\nuate the quality of these connections on fugues and string\nquartets. Taking into account the average pitches and dura-\ntions of fragments leads to better connections. The result-\ning algorithm connects voice fragments more reliably than\nwith the previous contig policies, and especially computes\nhigh-quality connections at the ﬁrst steps. This work could\nbe extended by considering more corpora and by evaluat-\ning further melodic or structural analysis on the resulting\nvoices or streams. The proposed principles apply to contig-\nbased algorithms but may also be used by other methods\nclustering notes into voices or streams.168 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Corpus Connection policy CC TR-rec TR-prec SoSu\nno connection – 86.78% 99.32% 0.98 0.34\nGA1-75% 92.61% 93.45% 98.54% 0.91 0.42\nwtc-i GA1 89.30% 97.84% 0.72 0.72\n(training set) worst 16.93% 85.25% 0.06 0.09\nSimCW 81.26% 96.58% 0.65 0.64\nSimIMS 80.62% 96.55% 0.68 0.69\nwtc-iino connection – 86.66% 99.29% 0.98 0.35\nGA1-75% 92.54% 92.53% 98.36% 0.91 0.40\nGA1 87.50% 97.14% 0.71 0.71\nworst 25.06% 84.22% 0.05 0.07\nSimCW 83.27% 96.22% 0.69 0.68\nSimIMS 81.61% 96.07% 0.69 0.68\nstring quartetsno connection – 82.61% 97.00% 0.94 0.29\nGA1-75% 85.30% 87.06% 94.80% 0.83 0.32\nGA1 78.44% 92.59% 0.44 0.44\nworst 31.88% 80.59% 0.12 0.13\nSimCW 75.99% 92.29% 0.39 0.38\nSimIMS 74.53% 91.79% 0.62 0.61\nTable 2 . Evaluation of the quality of various connection policies. Note that the two ﬁrst policies (No connection, GA1-75%)\ndo not try to connect the whole voices: they have very high TR-prec/ Sometrics, but poorer TR-rec/ Sumetrics.\n \nSimCW and SimISM TR: 67/72\n/noteheads.s2\n/noteheads.u2triangle/dots.dot/noteheads.u2triangle/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.u2triangle\n/rests.313/rests.1/noteheads.s2\n/noteheads.u2triangle/noteheads.u2triangle/noteheads.d2triangle/rests.1\n/noteheads.s2\n/rests.3\n/rests.1/dots.dot\n/noteheads.s2\n/noteheads.u2triangle/flags.u3/noteheads.s2\n/noteheads.u2triangle/dots.dot\n/dots.dot/noteheads.s2/noteheads.d2triangle\n/noteheads.s2cross/dots.dot/noteheads.s2\n/noteheads.u2triangle\n c28/noteheads.s0harmonic/rests.0\n/noteheads.s0harmonic/noteheads.s2\n/noteheads.s0harmonic/noteheads.s2\n/noteheads.u2triangle\n/noteheads.s0harmonic/noteheads.s2\n/noteheads.u2triangle/noteheads.s2\n/noteheads.u2triangle\n/noteheads.s0harmonic/noteheads.s2 /noteheads.s2/noteheads.d2triangle\n/noteheads.s2cross\n/noteheads.s0harmonic14/dots.dot/noteheads.s2/noteheads.d2triangle\n/noteheads.s0harmonic/noteheads.s2cross/noteheads.s0harmonic/accidentals.sharp/noteheads.d2triangle12\n/flags.u3/flags.d3/brackettips.up\n/brackettips.down/noteheads.s2/noteheads.d2triangle/clefs.G\n/accidentals.sharp/timesig.C44\n/noteheads.s2cross/noteheads.s0harmonic/clefs.F/timesig.C44 /noteheads.s0harmonic/accidentals.sharp\n/noteheads.s0harmonic/noteheads.s2/noteheads.d2triangle\n/noteheads.s2cross/dots.dot/noteheads.s2\n/noteheads.s0harmonic /noteheads.s0harmonic/noteheads.s2\n/rests.2\n/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2cross/flags.u3/noteheads.s2 /noteheads.s2/rests.3\n/noteheads.s2cross\n c55/noteheads.s0harmonic/noteheads.s2\n/noteheads.s0harmonic/dots.dot/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.d2triangle\n/noteheads.s2cross\n/noteheads.s0harmonic/noteheads.s0harmonic/noteheads.d2triangle/noteheads.s2\n/noteheads.s0harmonic\nGA1 TR: 72/72\n/noteheads.d2triangle\n/noteheads.s2cross/dots.dot/noteheads.s2cross/noteheads.d2triangle/noteheads.d2triangle/noteheads.d2triangle\n/noteheads.s2cross\n/rests.313/rests.1/noteheads.d2triangle\n/noteheads.s2cross/noteheads.s2cross/noteheads.d2triangle/rests.1\n/noteheads.d2triangle\n/rests.3\n/rests.1/dots.dot\n/noteheads.d2triangle\n/noteheads.s2cross/flags.u3/noteheads.d2triangle\n/noteheads.s2cross/dots.dot\n/dots.dot/noteheads.s2/noteheads.d2triangle\n/noteheads.s2cross/noteheads.d2triangle\n/noteheads.s2cross\n/noteheads.s0harmonic/dots.dot/rests.0\n/noteheads.s0harmonic/noteheads.d2triangle\n/noteheads.s0harmonic/noteheads.d2triangle\n/noteheads.s2cross\n/noteheads.s0harmonic/noteheads.d2triangle\n/noteheads.s2cross/noteheads.d2triangle\n/noteheads.s2cross\n/noteheads.s0harmonic/noteheads.d2triangle /noteheads.s2/noteheads.d2triangle\n/noteheads.s2cross\n/noteheads.s0harmonic14/dots.dot/noteheads.s2/noteheads.d2triangle\n/noteheads.s0harmonic/noteheads.s2cross/noteheads.s0harmonic/accidentals.sharp/noteheads.d2triangle12\n/flags.u3/flags.d3/brackettips.up\n/brackettips.down/noteheads.s2/noteheads.d2triangle/clefs.G\n/accidentals.sharp/timesig.C44\n/noteheads.s2cross/noteheads.s0harmonic/clefs.F/timesig.C44 /noteheads.s0harmonic/accidentals.sharp\n/noteheads.s0harmonic/noteheads.s2/noteheads.d2triangle\n/noteheads.s2cross/dots.dot/noteheads.s2\n/noteheads.s0harmonic /noteheads.s0harmonic/noteheads.s2\n/rests.2\n/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2\n/noteheads.s2cross/flags.u3/noteheads.s2/rests.3\n/noteheads.s2cross\n/noteheads.s0harmonic/noteheads.s2\n/noteheads.s0harmonic/dots.dot/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.d2triangle\n/noteheads.s2cross\n/noteheads.s0harmonic/noteheads.s0harmonic/noteheads.d2triangle/noteheads.s2\n/noteheads.s0harmonic\nFigure 3 . Extract of the fugue\nin C major BWV 846 by J.-\nS. Bach. (Top.) The con-\nnection policy of previous algo-\nrithms fails on connection c28\nbecause of the ﬁfth leap be-\ntween the D and the G in the\ntenor voice. This error leads\nto the wrong connection c55 at\na later stage of the algorithm.\n(Bottom.) Because the coef-\nﬁcients GA1 take into account\nthe feature avgpitch(C)and the\nrelated features, the connection\nis correct here.\nFigure 4 . Errors done during the successive connection steps. The lower the curves, the better. Coefﬁcients SimCW (blue):\nthe error rate is almost constant. Coefﬁcients SimIMS (yellow): the ﬁrst connections are more reliable. Coefﬁcients\nGA1 (green): the ﬁrst connections are even more reliable, enabling to improve the algorithm by stopping before too much\nbad connections happen. The highest number of bad connections for string quartets (compared to fugues) is probably due\nto a less regular polyphonic writing, with in particular stylistic differences leading to larger intervals.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 1697. REFERENCES\n[1] Albert Donally Bethke. Genetic algorithms as func-\ntion optimizers. In ACM Computer Science Confer-\nence, 1978.\n[2] Elaine Chew and Xiaodan Wu. Separating voices in\npolyphonic music: A contig mapping approach. In In-\nternational Symposium on Computer Music Modeling\nand Retrieval (CMMR 2005) , pages 1–20. 2005.\n[3] Michael Scott Cuthbert and Christopher Ariza. mu-\nsic21: A toolkit for computer-aided musicology and\nsymbolic music data. In International Society for Mu-\nsic Information Retrieval Conference (ISMIR 2010) ,\npages 637–642, 2010.\n[4] Reinier de Valk, Tillman Weyde, and Emmanouil\nBenetos. A machine learning approach to voice separa-\ntion in lute tablature. In International Society for Music\nInformation Retrieval Conference (ISMIR 2013) , pages\n555–560, 2013.\n[5] Diana Deutsch, editor. The psychology of music . Aca-\ndemic Press, 1982.\n[6] Nicolas Guiomard-Kagan, Mathieu Giraud, Richard\nGroult, and Florence Lev ´e. Comparing voice and\nstream segmentation algorithms. In International So-\nciety for Music Information Retrieval Conference (IS-\nMIR 2015) , pages 493–499, 2015.\n[7] David Huron. Tone and voice: A derivation of the rules\nof voice-leading from perceptual principles. Music Per-\nception , 19(1):1–64, 2001.\n[8] David Huron. Music information processing using the\nHumdrum toolkit: Concepts, examples, and lessons.\nComputer Music Journal , 26(2):11–26, 2002.\n[9] Asako Ishigaki, Masaki Matsubara, and Hiroaki Saito.\nPrioritized contig combining to segragate voices in\npolyphonic music. In Sound and Music Computing\nConference (SMC 2011) , volume 119, 2011.\n[10] J ¨urgen Kilian and Holger H Hoos. V oice separation –\na local optimization approach. In International Con-\nference on Music Information Retrieval (ISMIR 2002) ,\n2002.\n[11] Phillip B Kirlin and Paul E Utgoff. V oise: Learning\nto segregate voices in explicit and implicit polyphony.\nInInternational Conference on Music Information Re-\ntrieval (ISMIR 2005) , pages 552–557, 2005.\n[12] Hanna M Lukashevich. Towards quantitative measures\nof evaluating song segmentation. In International Con-\nference on Music Information Retrieval (ISMIR 2008) ,\npages 375–380, 2008.\n[13] Andrew McLeod and Mark Steedman. HMM-based\nvoice separation of MIDI performance. Journal of New\nMusic Research , 45(1):17–26, 2016.[14] Dimitrios Rafailidis, Alexandros Nanopoulos, Yannis\nManolopoulos, and Emilios Cambouropoulos. Detec-\ntion of stream segments in symbolic musical data. In\nInternational Conference on Music Information Re-\ntrieval (ISMIR 2008) , pages 83–88, 2008.\n[15] Dimitris Rafailidis, Emilios Cambouropoulos,\nand Yannis Manolopoulos. Musical voice integra-\ntion/segregation: Visa revisited. In Sound and Music\nComputing Conference (SMC 2009) , pages 42–47,\n2009.\n[16] David Temperley. The Cognition of Basic Musical\nStructures . The MIT Press, 2001.170 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Time-Delayed Melody Surfaces for Rāga Recognition.",
        "author": [
            "Sankalp Gulati",
            "Joan Serrà",
            "Kaustuv Kanti Ganguli",
            "Sertan Sentürk",
            "Xavier Serra"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417905",
        "url": "https://doi.org/10.5281/zenodo.1417905",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/030_Paper.pdf",
        "abstract": "R¯aga is the melodic framework of Indian art music. It is a core concept used in composition, performance, organi- zation, and pedagogy. Automatic r¯aga recognition is thus a fundamental information retrieval task in Indian art music. In this paper, we propose the time-delayed melody surface (TDMS), a novel feature based on delay coordinates that captures the melodic outline of a r¯aga. A TDMS describes both the tonal and the temporal characteristics of a melody, using only an estimation of the predominant pitch. Consid- ering a simple k-nearest neighbor classifier, TDMSs out- perform the state-of-the-art for r¯aga recognition by a large margin. We obtain 98% accuracy on a Hindustani music dataset of 300 recordings and 30 r¯agas, and 87% accuracy on a Carnatic music dataset of 480 recordings and 40 r¯agas. TDMSs are simple to implement, fast to compute, and have a musically meaningful interpretation. Since the concepts and formulation behind the TDMS are generic and widely applicable, we envision its usage in other music traditions beyond Indian art music.",
        "zenodo_id": 1417905,
        "dblp_key": "conf/ismir/GulatiSGSS16",
        "content": "TIME-DELAYED MELODY SURFACES FOR R ¯AGA RECOGNITION\nSankalp Gulati1Joan Serr `a2Kaustuv K Ganguli3Sertan S ¸ent ¨urk1Xavier Serra1\n1Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain\n2Telefonica Research, Barcelona, Spain\n3Dept. of Electrical Engg., Indian Institute of Technology Bombay, Mumbai, India\nsankalp.gulati@upf.edu\nABSTRACT\nR¯aga is the melodic framework of Indian art music. It is\na core concept used in composition, performance, organi-\nzation, and pedagogy. Automatic r ¯aga recognition is thus a\nfundamental information retrieval task in Indian art music.\nIn this paper, we propose the time-delayed melody surface\n(TDMS), a novel feature based on delay coordinates that\ncaptures the melodic outline of a r ¯aga. A TDMS describes\nboth the tonal and the temporal characteristics of a melody,\nusing only an estimation of the predominant pitch. Consid-\nering a simple k-nearest neighbor classiﬁer, TDMSs out-\nperform the state-of-the-art for r ¯aga recognition by a large\nmargin. We obtain 98% accuracy on a Hindustani music\ndataset of 300 recordings and 30 r ¯agas, and 87% accuracy\non a Carnatic music dataset of 480 recordings and 40 r ¯agas.\nTDMSs are simple to implement, fast to compute, and have\na musically meaningful interpretation. Since the concepts\nand formulation behind the TDMS are generic and widely\napplicable, we envision its usage in other music traditions\nbeyond Indian art music.\n1. INTRODUCTION\nMelodies in Hindustani and Carnatic music, two art mu-\nsic traditions of the Indian subcontinent, are constructed\nwithin the framework of r ¯aga [3, 29]. The r ¯aga acts as a\ngrammar within the boundaries of which an artist com-\nposes a music piece or improvises during a performance.\nA r¯aga is characterized by various melodic attributes at dif-\nferent time scales such as a set of svaras (roughly speak-\ning, notes), speciﬁc intonation of these svaras, ¯ar¯ohana-\navr¯ohana (the ascending and descending sequences of\nsvaras), and by a set of characteristic melodic phrases or\nmotifs (also referred to as ‘catch phrases’). In addition to\nthese melodic aspects, one of the most important charac-\nteristics of a r ¯aga is its calan [23] (literally meaning move-\nment or gait). The calan deﬁnes the melodic outline of a\nr¯aga, that is, how a melodic transition is made from one\nsvara to another, the precise intonation to be followed dur-\nc/circlecopyrtSankalp Gulati, Joan Serr `a, Kaustuv K Ganguli, Sertan\nS ¸ent¨urk and Xavier Serra. Licensed under a Creative Commons Attri-\nbution 4.0 International License (CC BY 4.0). Attribution: Sankalp\nGulati, Joan Serr `a, Kaustuv K Ganguli, Sertan S ¸ent ¨urk and Xavier Serra.\n“Time-delayed melody surfaces for R ¯aga recognition”, 17th International\nSociety for Music Information Retrieval Conference, 2016.ing the transition, and the proportion of time spent on each\nsvara. It can also be thought of as an abstraction of the\ncharacteristic melodic phrases mentioned above.\nR¯aga is a core musical concept used in the composition,\nperformance, organization, and pedagogy of Indian art mu-\nsic (IAM). Numerous compositions in Indian folk and ﬁlm\nmusic are also based on r ¯agas [9]. Despite its signiﬁcance\nin IAM, there exists a large volume of audio content whose\nr¯aga is incorrectly labeled or, simply, unlabeled. This is\npartially because the vast majority of the tools and tech-\nnologies that interact with the recordings’ metadata fall\nshort of fulﬁlling the speciﬁc needs of the Indian music tra-\ndition [26]. A computational approach to automatic r ¯aga\nrecognition can enable r ¯aga-based music retrieval from\nlarge audio collections, semantically-meaningful music\ndiscovery, musicologically-informed navigation, as well as\nseveral applications around music pedagogy.\nR¯aga recognition is one of the most researched top-\nics within music information retrieval (MIR) of IAM. As\na consequence, there exist a considerable amount of ap-\nproaches utilizing different characteristic aspects of r ¯agas.\nMany of such approaches use features derived from the\npitch or pitch-class distribution (PCD) [2, 4, 5, 16]. This\nway, they capture the overall usage of the tonal material in\nan audio recording. In general, PCD-based approaches are\nrobust to pitch octave errors, which is one of the most fre-\nquent errors in the estimation of predominant melody from\npolyphonic music signals. Currently, the PCD-based ap-\nproach represents the state-of-the-art in r ¯aga recognition.\nOne of these approaches proposed by Chordia et al. [2]\nhas shown promising results with an accuracy of 91.5% on\na sizable dataset comprising 23 r ¯agas and close to 550 ex-\ncerpts of 120 s duration, extracted from 121 audio record-\nings (note that the authors use monophonic recordings\nmade under laboratory conditions).\nOne of the major shortcomings of PCD-based ap-\nproaches is that they completely disregard the temporal as-\npects of the melody, which are essential to r ¯aga character-\nization [23]. Temporal aspects are even more relevant in\ndistinguishing phrase-based r ¯agas [17], as their aesthetics\nand identity is largely deﬁned by the usage of meandering\nmelodic movements, called gamakas. Several approaches\naddress this shortcoming by modeling the temporal aspects\nof a melody in a variety of ways [18, 21, 27]. Such ap-\nproaches typically use melodic progression templates [27],\nn-gram distributions [18], or hidden Markov models [21]751to capture the sequential information in the melody. With\nthat, they primarily utilize the ¯ar¯ohana-avr ¯ohana pattern of\na r¯aga. In addition, most of them either transcribe the pre-\ndominant melody in terms of a discrete svara sequence, or\nuse only a single symbol/state per svara. Thus, they dis-\ncard the characteristic melodic transitions between svaras,\nwhich are a representative and distinguishing aspect of a\nr¯aga [23]. Furthermore, they often rely on an accurate tran-\nscription of the melody, which is still a challenging and an\nill-deﬁned task given the nature of IAM [22, 24].\nThere are only a few approaches to r ¯aga recognition that\nconsider the continuous melody contour and exploit its raw\nmelodic patterns [6, 11]. Their aim is to create dictionar-\nies of characteristic melodic phrases and to exploit them in\nthe recognition phase, as melodic phrases are prominent\ncues for the identiﬁcation of a r ¯aga [23]. Such phrases\ncapture both the svara sequence and the transition char-\nacteristics within the elements of the sequence. However,\nthe automatic extraction of characteristic melodic phrases\nis a challenging task. Some approaches show promising\nresults [11], but they are still far from being perfect. In ad-\ndition, the melodic phrases used by these approaches are\ntypically very short and, therefore, more global melody\ncharacteristics are not fully considered.\nIn this paper, we propose a novel feature for r ¯aga recog-\nnition, the time-delayed melody surface (TDMS). It is in-\nspired by the concept of delay coordinates [28], as rou-\ntinely employed in nonlinear time series analysis [15]. A\nTDMS captures several melodic aspects that are useful in\ncharacterizing and distinguishing r ¯agas and, at the same\ntime, alleviates many of the critical shortcomings found in\nexisting methods. The main strengths of a TDMS are:\n•It is a compact representation that describes both the\ntonal and the temporal characteristics of a melody\n•It simultaneously captures the melodic characteris-\ntics at different time-scales, the overall usage of the\npitch-classes in the entire recording, and the short-\ntime temporal relation between individual pitches.\n•It is robust to pitch octave errors.\n•It does not require the transcription of the melody\nnor a discrete representation of it.\n•It is easy to implement, fast to compute, and has a\nmusically-meaningful interpretation.\n•As it will be shown, it obtains unprecedented accura-\ncies in the raga recognition task, outperforming the\nstate-of-the-art by a large margin, without the use of\nany elaborated classiﬁcation schema.\nIn our experiments, we use TDMSs together with a k-\nnearest neighbor classiﬁer and a set of well known distance\nmeasures. The reported results are obtained on two scal-\nable, diverse, and representative data sets of Carnatic and\nHindustani music, one of which is originally introduced in\nthis study and made publicly available. To the best of our\nknowledge, these are the largest publicly available data sets\nfor r¯aga recognition in terms of the number of recordings,\nnumber of r ¯agas, and total audio duration. The main con-\ntributions of the present study are:\nPre-processingPost-processing\nPredominant Melody EstimationTonic NormalizationSurface GenerationPower CompressionGaussian smootheningAudio signal\nTDMSFigure 1 . Block diagram for the computation of TDMSs.\n•To perform a critical review of the existing methods\nfor r¯aga recognition and identify some of their main\nconstraints/limitations.\n•To propose a novel feature based on delay coordi-\nnates, the TDMS, that has all the previously outlined\nstrengths.\n•To carry out a comparative evaluation with the best-\nperforming state-of-the-art methods under the same\nexperimental conditions.\n•To publicly release a scalable Hindustani music\ndataset for r ¯aga recognition that contains relevant\nmetadata, annotations, and the computed features.\n•To publicly release the code used for the computa-\ntion of TMDSs and the performed evaluation.\n2. RAGA RECOGNITION WITH TIME-DELAYED\nMELODY SURFACES\n2.1 Time-delayed melody surface\nThe computation of a TDMS has three steps (Figure 1):\npre-processing, surface generation, and post-processing.\nIn pre-processing, we obtain a representation of the\nmelody of an audio recording, which is normalized by\nthe tonic or base frequency of the music piece. In sur-\nface generation, we compute a two dimensional surface\nbased on the concept of delay coordinates. Finally, in\npost-processing, we apply power compression and Gaus-\nsian smoothing to the computed surface. We subsequently\ndetail these steps.\n2.1.1 Predominant melody estimation\nWe represent the melody of an audio excerpt by the pitch\nof the predominant melodic source. For predominant\npitch estimation, we use the method proposed by Sala-\nmon and G ´omez [25]. This method performed favorably in\nMIREX 2011 (an international MIR evaluation campaign)\non a variety of music genres, including IAM, and has been\nused in several other studies for a similar task [7, 12, 13].\nWe use the implementation of this algorithm as available752 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016in Essentia [1]. Essentia1is an open-source C++ library\nfor audio analysis and content-based MIR. We use the de-\nfault values of the parameters, except for the frame and\nhop sizes, which are set to 46 and 4.44 ms, respectively. In\nsubsequent steps, we discard frames where a predominant\npitch cannot be obtained.\n2.1.2 Tonic normalization\nThe base frequency chosen for a melody in IAM is the\ntonic pitch of the lead artist [10], to which all other accom-\npanying instruments are tuned. Therefore, for a musically\nmeaningful feature for r ¯aga recognition we normalize the\npredominant melody of every recording by considering its\ntonic pitchωas the reference frequency during the Hertz-\nto-cent-scale conversion,\nci= 1200 log2/parenleftbiggfi\nω/parenrightbigg\n,\nfor0≤i<N , whereNis the total number of pitch sam-\nples,ciis the normalized ithsample of the predominant\npitch (in cents), and fiis theithsample of the predominant\npitch (in Hz). The tonic pitch ωfor every recording is iden-\ntiﬁed using the multi-pitch approach proposed by Gulati\net al. [10]. This approach is reported to obtain state-of-the-\nart results and has been successfully used elsewhere [8,11].\nWe use the implementation of this algorithm as available\nin Essentia with the default set of parameter values. The\ntonic values for different recordings of an artist are further\nmajority voted to ﬁx the Pa(ﬁfth) type error [10].\n2.1.3 Surface generation\nThe next step is to construct a two-dimensional surface\nbased on the concept of delay coordinates (also termed\nphase space embedding) [15, 28]. In fact, such two-\ndimensional surface can be seen as a discretized histogram\nof the elements in a two-dimensional Poicar ´e map [15].\nFor a given recording, we generate a surface ˇSof sizeη×η\nrecursively, by computing\nˇsij=N−1/summationdisplay\nt=τI(B(ct),i)I(B(ct−τ),j)\nfor0≤i,j < η , whereIis an indicator function such\nthatI(x,y) = 1 iffx=y,I(x,y) = 0 otherwise,Bis an\noctave-wrapping integer binning operator deﬁned by\nB(x) =/floorleftBig /parenleftBigηx\n1200/parenrightBig\nmodη/floorrightBig\n, (1)\nandτis a time delay index (in frames) that is left as a pa-\nrameter. Note that, as mentioned, the frames where a pre-\ndominant pitch could not be obtained are excluded from\nany calculation. For the size of ˇSwe useη= 120 . This\nvalue corresponds to 10 cents per bin, an optimal pitch res-\nolution reported in [2].\nAn example of the generated surface ˇSfor a music\npiece2in r¯aga Yaman is shown in Figure 2 (a). We see that\n1https://github.com/MTG/essentia\n2http://musicbrainz.org/recording/e59642ca-72bc-466b-bf4b-\nd82bfbc7b4af\n0 20 40 60 80 100 120\nIndex0\n20\n40\n60\n80\n100\n120Index(a)\n0 20 40 60 80 100 120\nIndex0\n20\n40\n60\n80\n100\n120(b)\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Figure 2 . Generated surface for a music piece before (a)\nand after (b) applying post-processing ( ˇSandˆS, respec-\ntively). For ease of visualization, both matrices are nor-\nmalized here between 0 and 1.\nthe prominent peaks in the surface correspond to the svaras\nof r¯aga Yaman. We notice that these peaks are steep and\nthat the dynamic range of the surface is high. This can be\nattributed to the nature of the melodies in these music tradi-\ntions, particularly in Hindustani music, where the melodies\noften contain long held svaras. In addition, the dynamic\nrange is high because the pitches in the stable svara re-\ngions are within a small range around the svara frequency\ncompared to the pitches in the transitory melodic regions.\nBecause of this, the frequency values in the stable regions\nare mapped to a smaller set of bins, making the prominent\npeaks more steep.\n2.1.4 Post-processing\nIn order to accentuate the values corresponding to the tran-\nsitory regions in the melody and reduce the dynamic range\nof the surface, we apply an element-wise power compres-\nsion\nS=ˇSα,\nwhereαis an exponent that is left as a parameter. Once\na more compact (in terms of the dynamic range) surface\nis obtained, we apply Gaussian smoothing. With that, we\nattempt to attenuate the subtle differences in Scorrespond-\ning to the different melodies within the same r ¯aga, while\nretaining the attributes that characterize that r ¯aga.\nWe perform Gaussian smoothing by circularly convolv-\ningSwith a two-dimensional Gaussian kernel. We choose\na circular convolution because of the cyclic (or octave-\nfolded) nature of the TDMS (Eqn (1)), which mimics the\ncyclic nature of pitch classes. The standard deviation of\nthis kernel is σbins (samples). The length of the kernel is\ntruncated to 8σ+1bins in each dimension, after which the\nvalues are negligible (below 0.01% of the kernel’s maxi-\nmum amplitude). We experiment with different values of\nσ, and also with a method variant excluding the Gaussian\nsmoothing (loosely denoted by σ=−1), so that we can\nquantify its inﬂuence on the accuracy of the system.\nOnce we have the smoothed surface ˆS, there is only one\nstep remaining to obtain the ﬁnal TDMS. Since the overall\nduration of the recordings and of the voiced regions withinProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 753them is different, the computed surface ˆSneeds to be nor-\nmalized. To do so, we divide ˆSby itsL1matrix norm:\nS=ˆS//bardblˆS/bardbl1.\nThis also yields values of S, the ﬁnal TDMS, that are in-\nterpretable in terms of discrete probabilities.\nThe result after post-processing the surface\nof Figure 2 (a) with power compression and Gaus-\nsian smoothing is shown in Figure 2 (b). We see that the\nvalues corresponding to the non-diagonal elements are\naccentuated. A visual inspection of Figure 2 (b) provides\nseveral musical insights to the melodic aspects of the\nrecording. For instance, the high salience indices along\nthe diagonal, (0,0),(20,20),(40,40),(60,60),(70,70),\n(90,90), and (110,110) , correspond to the 7 svaras used\nin r¯aga Yaman. Within which, the highest salience at\nindices (110,110) correspond to the Ni svara, which is\ntheVadi svara, i.e., musically the most salient svara of\nthe r ¯aga, in this case r ¯aga Yaman [23]. The asymmetry\nin the matrix with respect to the diagonal indicates the\nasymmetric nature of the ascending and descending svara\npattern of the r ¯aga (compare, for example, the salience at\nindices (70,90)to indices (90,70), with the former being\nmore salient than the latter). The similarity of the matrix\nbetween indices (20,20)and(70,70)with respect to the\nmatrix between indices (70,70)and(120,120) delineates\nthe tetra-chord structure of the r ¯aga. Finally, it should be\nnoted that an interesting property of TDMSs is that the\nmean of the sum across its row and columns yields a PCD\nrepresentation (see Section 1).\n2.2 Classiﬁcation and distance measurement\nIn order to demonstrate the ability of the TDMSs in cap-\nturing r ¯aga characteristics, we consider the task of classify-\ning audio recordings according to their r ¯aga label. To per-\nform classiﬁcation, we choose a k-nearest neighbor (kNN)\nclassiﬁer [20]. The reasons for our choice are manifold.\nFirstly, the kNN classiﬁer is well understood, with well\nstudied relations to other classiﬁers in terms of both per-\nformance and architecture. Secondly, it is fast, with prac-\ntically no training and with known techniques to speed up\ntesting or retrieval. Thirdly, it has only one parameter, k,\nwhich we can just blindly set to a relatively small value or\ncan easily optimize in the training phase. Finally, it is a\nclassiﬁer that is simple to implement and whose results are\nboth interpretable and easily reproducible.\nThe performance of a kNN classiﬁer highly depends on\nthe distance measure used to retrieve the kneighbors. We\nconsider three different measures to compute the distance\nbetween two recordings nandmwith TDMS features S(n)\nandS(m), respectively. We ﬁrst consider the Frobenius\nnorm of the difference between S(n)andS(m),\nD(n,m)\nF =/bardblSn−Sm/bardbl2.\nNext, we consider the symmetric Kullback-Leibler diver-\ngence\nD(n,m)\nKL =DKL/parenleftBig\nS(n),S(m)/parenrightBig\n+DKL/parenleftBig\nS(m),S(n)/parenrightBig\n,with\nDKL(X,Y) =/summationdisplay\nXlog/parenleftbiggX\nY/parenrightbigg\n,\nwhere we perform element-wise operations and sum over\nall the elements of the resultant matrix. Finally, we con-\nsider the Bhattacharyya distance, which is reported to out-\nperform other distance measures with a PCD-based feature\nfor the same task in [2],\nD(n,m)\nB =−log/parenleftBig/summationdisplay/radicalbig\nS(n)·S(m)/parenrightBig\n.\nWe again perform element-wise operations and sum over\nall the elements of the resultant matrix. Variants of our\nproposed method that use DF,DKLandDBare denoted\nbyMF,MKL, andMB, respectively.\n3. EV ALUATION METHODOLOGY\n3.1 Music collection\nThe music collection used in this study is compiled as a\npart of the CompMusic project [26]. It comprises two\ndatasets: a Carnatic music data set (CMD) and a Hin-\ndustani music data set (HMD). Due to the differences in\nthe melodic characteristics within these two music tradi-\ntions, and for a better analysis of the results, we eval-\nuate our method separately on each of these data sets.\nCMD and HMD comprise 124 and 130 hours of commer-\ncially available audio recordings, respectively, stored as\n160 kbps mp3 stereo audio ﬁles. All the editorial meta-\ndata for each audio recording is publicly available in Mu-\nsicbrainz3, an open-source metadata repository. CMD\ncontains full-length recordings of 480 performances be-\nlonging to 40 r ¯agas with 12 music pieces per r ¯aga. HMD\ncontains full-length recordings of 300 performances be-\nlonging to 30 r ¯agas with 10 music pieces per r ¯aga. The\nselected music material is diverse in terms of the number\nof artists, the number of forms, and the number of compo-\nsitions. In these terms, it can be regarded as a represen-\ntative subset of real-world collections. The chosen r ¯agas\ncontain diverse sets of svaras (notes), both in terms of the\nnumber of svaras and their pitch-classes (svarasth ¯an¯as).\nNote that CMD has already been introduced and made\npublicly available in [11]. With the same intentions\nto facilitate comparative studies and to promote repro-\nducible research, we make HMD publicly available on-\nline4. Along with the r ¯aga labels for each recording,\nwe also make predominant melody, TDMSs, and the code\nused for our experiments openly available online.\n3.2 Comparison with existing methods\nIn addition to our proposed method, we evaluate and com-\npare two existing methods under the same experimental\nsetup and evaluation data sets. The two selected methods\nare the ones proposed by Chordia & S ¸ent ¨urk [2], denoted\nbyEPCD, and by Gulati et al. [11], denoted by EVSM. Both\napproaches have shown encouraging results on scalable\n3https://musicbrainz.org/\n4http://compmusic.upf.edu/node/300754 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016datasets and can be regarded as the current, most competi-\ntive state-of-the-art in r ¯aga recognition. The former, EPCD,\nemploys PCD-based features computed from the entire au-\ndio recording. The latter, EVSM, uses automatically discov-\nered melodic phrases and vector space modeling. Read-\ners should note that the experimental setup used in [11] is\nslightly different from the one in the current study. There-\nfore, there exists a small difference in the reported accura-\ncies, even when evaluated on the same dataset (CMD). For\nbothEPCD andEVSM, we use the original implementations\nobtained from the respective authors.\n3.3 Validation strategy\nTo evaluate the performance of the considered methods we\nuse the raw overall accuracy [20]. Since both CMD and\nHMD are balanced in the number of instances per class,\nwe do not need to correct such raw accuracies to counteract\nfor possible biases towards the majority class. We perform\na leave-one-out cross validation [20], in which one record-\ning from the evaluation data set forms the testing set and\nthe remaining ones become the training set. To assess if the\ndifference in the performance between any two methods is\nstatistically signiﬁcant, we use McNemar’s test [19] with\np<0.01. To compensate for multiple comparisons, we ap-\nply the Holm-Bonferroni method [14]. Besides accuracy,\nand for a more detailed error analysis, we also compute the\nconfusion matrix over the predicted classes.\nIn the case ofM, a test recording is assigned the major-\nity class of its k-nearest neighbors obtained from the train-\ning set and, in case of a tie, one of the majority classes is\nselected randomly. Because we conjecture that none of the\nparameters we consider is critical to obtain a good perfor-\nmance, we initially make an educated guess and intuitively\nset our parameters to a speciﬁc combination. We later\nstudy the inﬂuence of every parameter starting from that\ncombination. We initially use τ= 0.3s,α= 0.75,σ= 2,\nandk= 1, and later consider τ∈{0.2,0.3,0.5,1,1.5}s,\nα∈{0.1,0.25,0.5,0.75,1},σ∈{− 1,1,2,3}, andk∈\n{1,3,5}(recall thatσ=−1corresponds to no smoothing;\nSection 2.1.4).\n4. RESULTS AND DISCUSSION\nIn Table 1, we show the results for all the variants of the\nproposed methodMF,MKLandMB, and the two state-\nof-the-art methods EPCD andEVSM, using HMD and CMD\ndata sets. We see that the highest accuracy obtained on\nHMD is 97.7% by MKLandMB. This accuracy is con-\nsiderably higher than the 91.7% obtained by EPCD, and the\ndifference is found to be statistically signiﬁcant. We also\nsee thatEPCD performs signiﬁcantly better than EVSM. Re-\ngarding the proposed variants, we see that, in HMD, MKL\nandMBperform better than MF, with a statistically sig-\nniﬁcant difference.\nIn Table 1, we see that the trend in the performance for\nCMD across different methods is similar to that for HMD.\nThe variantsMKLandMBachieve the highest accuracy\nof 86.7%, followed by EPCD with 73.1%. The differenceData setMFMKLMBEPCDEVSM\nHMD 91.3 97.7 97.7 91.7 83.0\nCMD 81.5 86.7 86.7 73.1 68.1\nTable 1 . Accuracy (%) of the three proposed variants,\nMF,MKLandMBC, and the two existing state-of-the-art\nmethodsEPCD andEVSM (see text). The random baseline\nfor this task is 3.3% for HMD and 2.5% for CMD.\n0.2 0.3 0.5 1 1.5\n⌧(seconds)020406080100Accuracy (%)(a)\n0.1 0.25 0.5 0.75 1\n↵(b)\n-1 1 2 3\n\u0000(bins)020406080100Accuracy (%)(c)\n1 3 5\nk(d)MKL(HMD)\nMKL(CMD)EPCD(HMD)\nEPCD(CMD)EVSM(HMD)\nEVSM(CMD)Br(HMD)\nBr(CMD)\nFigure 3 . Accuracy ofMKLas a function of parameter\nvalues. State-of-the-art approaches Eand random base-\nlinesBare also reported for comparison.\nbetweenMKL(MB) andEPCD is found to be statistically\nsigniﬁcant. For CMD, also MKLandMBperform better\nthanMF, with a statistically signiﬁcant difference.\nIn general, we notice that, for every method, the accu-\nracy is higher on HMD compared to CMD. This, as ex-\npected, can be largely attributed to the difference in the\nnumber of classes in HMD (30 r ¯agas) and CMD (40 r ¯agas).\nA higher number of classes makes the task of r ¯aga recog-\nnition more challenging for CMD, compared to HMD. In\naddition to that, another factor that can cause this differ-\nence could be the length of the audio recordings, which for\nHMD are signiﬁcantly longer than the ones in CMD.\nAs mentioned earlier, the system parameters corre-\nsponding to the results in Table 1 were set intuitively, with-\nout any parameter tuning. Since TDMSs are used here for\nthe ﬁrst time, we want to carefully analyze the inﬂuence\nthat each of the parameters has on the ﬁnal r ¯aga recog-\nnition accuracy, and ultimately perform a quantitative as-\nsessment of their importance. In Figure 3, we show the\naccuracy ofMKLfor different values of these parameters.\nIn each case, only one parameter is varied and the rest are\nset to the initial values mentioned above.\nIn Figure 3 (a), we observe that the performance of the\nmethod is quite invariant to the choice of τ, except for\nthe extreme delay values of 1 and 1.5 s for CMD. ThisProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 755R1-S.anmukhapriya\nR2-K ¯api\nR3-Bhairavi\nR4-Madhyam ¯avati\nR5-Bilahari\nR6-M ¯ohana ˙m\nR7-Sencurut .t.i\nR8-´Sr¯ıranjani\nR9-R ¯ıtigaul .a\nR10-Huss ¯en¯ı\nR11-Dhany ¯asi\nR12-At .¯ana\nR13-Beh ¯ag\nR14-Surat .i\nR15-K ¯amavardani\nR16-Mukh ¯ari\nR17-Sindhubhairavi\nR18-Sah ¯an¯a\nR19-K ¯anad.a\nR20-M ¯ay¯am¯al.avagaul .a\nR21-N ¯at.a\nR22- ´Sankar ¯abharan .a˙m\nR23-S ¯av¯eri\nR24-Kam ¯as\nR25-T ¯od.i\nR26-B ¯egad.a\nR27-Harik ¯ambh ¯oji\nR28- ´Sr¯ı\nR29-Kaly ¯an.i\nR30-S ¯ama\nR31-N ¯at.akurinji\nR32-P ¯urv¯ıkal.y¯an.i\nR33-Yadukula k ¯a˙mb¯oji\nR34-D ¯evag ¯andh ¯ari\nR35-K ¯ed¯aragaul .a\nR36- ¯Anandabhairavi\nR37-Gaul .a\nR38-Var ¯al.i\nR39-K ¯a˙mbh ¯oji\nR40-Karaharapriya\nR1\nR2\nR3\nR4\nR5\nR6\nR7\nR8\nR9\nR10\nR11\nR12\nR13\nR14\nR15\nR16\nR17\nR18\nR19\nR20\nR21\nR22\nR23\nR24\nR25\nR26\nR27\nR28\nR29\nR30\nR31\nR32\nR33\nR34\nR35\nR36\nR37\nR38\nR39\nR4011 1\n10 2\n10 2\n5 2 3 1 1\n10 1 1\n12\n10 1 1\n101 1\n12\n1 7 3 1\n12\n3 9\n10 1 1\n12\n11 1\n5 7\n11 1\n12\n10 1 1\n11 1\n12\n11 1\n12\n12\n12\n11 1\n7 5\n3 1 6 2\n12\n1 10 1\n12\n1 11\n1 1 10\n1 1 91\n12\n12\n12\n1 11\n2 1 9\n1 110 1 2 3 4 5 6 7 8 9 10 11 12Figure 4 . Confusion matrix of the predicted r ¯aga labels\nobtained byMKLon CMD. Shades of grey are mapped to\nthe number of audio recordings.\ncan be attributed to the melodic characteristics of Car-\nnatic music, which presents a higher degree of oscilla-\ntory melody movements and shorter stationary svara re-\ngions, as compared to Hindustani music. In Figure 3 (b),\nwe see that compression with α < 1slightly improves\nthe performance of the method for both data sets. How-\never, the performance degrades for α<0.75for CMD and\nα < 0.25for HMD. This again appears to be correlated\nwith the long steady nature of the svaras in Hindustani\nmusic melodies. Because the dynamic range of ˇSis high,\nTDMS features require a lower value for the compression\nfactorαto accentuate the surface values corresponding to\nthe transitory regions in the melodies of Hindustani mu-\nsic. In Figure 3 (c), we observe that Gaussian smoothing\nsigniﬁcantly improves the performance of the method, and\nthat such performance is invariant across the chosen values\nofσ. Finally, in Figure 3 (d), we notice that the accuracy\ndecreases with increasing k. This is also expected due to\nthe relatively small number of samples per class in our data\nsets [20]. Overall, the method appears to be invariant to\ndifferent parameter values to a large extent, which implies\nthat it is easier to extend and tune it to other data sets.\nFrom the results reported in Figure 3, we see that there\nexist a number of parameter combinations that could po-\ntentially yield a better accuracy than the one reported in\nTable 1. For instance, using τ= 0.3s,α= 0.5,σ= 2,\nandk= 1, we are able to reach 97.0% for MFand 98.0%\nfor bothMKLandMBon HMD. These accuracies are ad-\nhoc, optimizing the parameters on the testing set. However,\nand doing things more properly, we could learn the opti-mal parameters in training, through a standard grid search,\ncross-validated procedure over the training set [20]. As our\nprimary goal here is not to obtain the best possible results,\nbut to show the usefulness and superiority of TDMSs, we\ndo not perform such an exhaustive parameter tuning and\nleave it for future research.\nTo conclude, we proceed to analyze the errors made by\nthe best performing variant MKL. For CMD, we show the\nconfusion matrix of the predicted r ¯aga labels in Figure 4.\nIn general, we see that the confusions have a musical ex-\nplanation. The majority of them are between the r ¯agas in\nthe sets{Bhairavi, Mukh ¯ari},{Harik ¯ambh ¯oji, K ¯ambh ¯oji},\n{Madhyamvat ¯i, At.¯ana, ´Sr¯i}, and{K¯api, ¯Anandabhairavi}.\nR¯agas within each of these sets are allied r ¯agas [29], i.e.,\nthey share a common set of svaras and similar phrases.\nFor HMD, there are only 7 incorrectly classiﬁed record-\nings (confusion matrix omitted for space reasons). R ¯aga\nAlhaiy ¯a bil¯awal and r ¯aga D ¯e´s is confused with r ¯aga Gaud .\nMalh ¯ar, which is musically explicable as these r ¯agas share\nexactly the same set of svaras. R ¯aga R ¯ag¯e´shr¯i is con-\nfused with B ¯ag¯e´shr¯i, which differ in only one svara. In all\nthese cases, the r ¯agas which are confused also have simi-\nlar melodic phrases. For two speciﬁc cases of confusions,\nthat of r ¯aga Kham ¯aj with B ¯ag¯e´shr¯i, and r ¯aga Darb ¯ar¯i with\nBh¯up, we ﬁnd that the error lies in the estimation of the\ntonic pitch.\n5. CONCLUSION\nIn this paper, we proposed a novel melody representation\nfor r¯aga recognition, the TDMS, which is inspired by the\nconcept of delay coordinates and Poicar ´e maps. A TDMS\ncaptures both the tonal and the short-time temporal char-\nacteristics of a melody. They are derived from the tonic-\nnormalized pitch of the predominant melodic source in the\naudio. To demonstrate the capabilities of TDMSs in cap-\nturing r ¯aga characteristics, we classiﬁed audio recordings\naccording to their r ¯aga labels. For this, we used sizable\ncollections of Hindustani and Carnatic music with over\n250 hours of duration. Using a k-nearest neighbor clas-\nsiﬁer, the proposed feature outperformed state-of-the-art\nsystems in r ¯aga recognition. We also studied the inﬂu-\nence of different parameters on the accuracy obtained by\nTDMSs, and found that it is largely invariant to different\nparameter values. An analysis of the classiﬁcation errors\nrevealed that the confusions occur between musically sim-\nilar r ¯agas that share a common set of svaras and have sim-\nilar melodic phrases. In the future, we plan to investigate\nif PCD-based, phrase-based, and TDMSs can be success-\nfully combined to improve r ¯aga recognition. In addition,\nwe would like to investigate the minimum duration of the\naudio recording needed to successfully recognize its r ¯aga.\n6. ACKNOWLEDGMENTS\nThis work is partly supported by the European Research\nCouncil under the European Unions Seventh Framework\nProgram, as part of the CompMusic project (ERC grant\nagreement 267583).756 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20167. REFERENCES\n[1] D. Bogdanov, N. Wack, E. G ´omez, S. Gulati, P. Her-\nrera, O. Mayor, G. Roma, J. Salamon, J. Zapata, and\nX. Serra. Essentia: an audio analysis library for mu-\nsic information retrieval. In Proc. of Int. Soc. for Music\nInformation Retrieval Conf. (ISMIR) , pages 493–498,\n2013.\n[2] P. Chordia and S. S ¸ent ¨urk. Joint recognition of raag and\ntonic in north Indian music. Computer Music Journal ,\n37(3):82–98, 2013.\n[3] A. Danielou. The ragas of Northern Indian music .\nMunshiram Manoharlal Publishers, New Delhi, 2010.\n[4] P. Dighe, P. Agrawal, H. Karnick, S. Thota, and B. Raj.\nScale independent raga identiﬁcation using chroma-\ngram patterns and swara based features. In IEEE Int.\nConf. on Multimedia and Expo Workshops (ICMEW) ,\npages 1–4, 2013.\n[5] P. Dighe, H. Karnick, and B. Raj. Swara histogram\nbased structural analysis and identiﬁcation of Indian\nclassical ragas. In Proc. of Int. Soc. for Music Infor-\nmation Retrieval Conf. (ISMIR) , pages 35–40, 2013.\n[6] S. Dutta, S. PV Krishnaraj, and H. A. Murthy. Raga\nveriﬁcation in Carnatic music using longest common\nsegment set. In Int. Soc. for Music Information Re-\ntrieval Conf. (ISMIR) , pages 605–611, 2015.\n[7] S. Dutta and H. A. Murthy. Discovering typical motifs\nof a raga from one-liners of songs in Carnatic music.\nInInt. Soc. for Music Information Retrieval (ISMIR) ,\npages 397–402, 2014.\n[8] K. K. Ganguli, A. Rastogi, V . Pandit, P. Kantan, and\nP. Rao. Efﬁcient melodic query based audio search for\nHindustani vocal compositions. In Proc. of Int. Soc.\nfor Music Information Retrieval Conf. (ISMIR) , pages\n591–597, 2015.\n[9] T. Ganti. Bollywood: a guidebook to popular Hindi\ncinema . Routledge, 2013.\n[10] S. Gulati, A. Bellur, J. Salamon, H. G. Ranjani, V . Ish-\nwar, H. A. Murthy, and X. Serra. Automatic tonic iden-\ntiﬁcation in Indian art music: approaches and evalu-\nation. Journal of New Music Research , 43(1):55–73,\n2014.\n[11] S. Gulati, J. Serr `a, V . Ishwar, S. S ¸ent ¨urk, and X. Serra.\nPhrase-based r ¯aga recognition using vector space mod-\neling. In IEEE Int. Conf. on Acoustics, Speech and Sig-\nnal Processing (ICASSP) , pages 66–70, 2016.\n[12] S. Gulati, J. Serr `a, V . Ishwar, and X. Serra. Mining\nmelodic patterns in large audio collections of Indian\nart music. In Int. Conf. on Signal Image Technology &\nInternet Based Systems (SITIS-MIRA) , pages 264–271,\n2014.\n[13] S. Gulati, J. Serr `a, and X. Serra. An evaluation of\nmethodologies for melodic similarity in audio record-\nings of Indian art music. In IEEE Int. Conf. on Acous-\ntics, Speech and Signal Processing (ICASSP) , pages\n678–682, 2015.[14] S. Holm. A simple sequentially rejective multiple test\nprocedure. Scandinavian journal of statistics , 6(2):65–\n70, 1979.\n[15] H. Kantz and T. Schreiber. Nonlinear time series anal-\nysis. Cambridge University Press, Cambridge, UK,\n2004.\n[16] G. K. Koduri, V . Ishwar, J. Serr `a, and X. Serra. Into-\nnation analysis of r ¯agas in Carnatic music. Journal of\nNew Music Research , 43(1):72–93, 2014.\n[17] T. M. Krishna and V . Ishwar. Karn .¯at.ic music: Svara,\ngamaka, motif and r ¯aga identity. In Proc. of the 2nd\nCompMusic Workshop , pages 12–18, 2012.\n[18] V . Kumar, H Pandya, and C. V . Jawahar. Identifying\nragas in Indian music. In 22nd Int. Conf. on Pattern\nRecognition (ICPR) , pages 767–772, 2014.\n[19] Q. McNemar. Note on the sampling error of the dif-\nference between correlated proportions or percentages.\nPsychometrika , 12(2):153–157, 1947.\n[20] T. M. Mitchell. Machine Learning . McGraw-Hill, New\nYork, USA, 1997.\n[21] P. V . Rajkumar, K. P. Saishankar, and M. John. Identiﬁ-\ncation of Carnatic raagas using hidden markov models.\nInIEEE 9th Int. Symposium on Applied Machine Intel-\nligence and Informatics (SAMI) , pages 107–110, 2011.\n[22] S. Rao. Culture speciﬁc music information processing:\nA perspective from Hindustani music. In 2nd Comp-\nMusic Workshop , pages 5–11, 2012.\n[23] S. Rao, J. Bor, W. van der Meer, and J. Harvey. The\nraga guide: a survey of 74 Hindustani ragas . Nimbus\nRecords with Rotterdam Conservatory of Music, 1999.\n[24] S. Rao and P. Rao. An overview of Hindustani music\nin the context of computational musicology. Journal of\nNew Music Research , 43(1):24–33, 2014.\n[25] J. Salamon and E. G ´omez. Melody extraction from\npolyphonic music signals using pitch contour charac-\nteristics. IEEE Transactions on Audio, Speech, and\nLanguage Processing , 20(6):1759–1770, 2012.\n[26] X. Serra. A multicultural approach to music informa-\ntion research. In Proc. of Int. Conf. on Music Informa-\ntion Retrieval (ISMIR) , pages 151–156, 2011.\n[27] S. Shetty and K. K. Achary. Raga mining of Indian mu-\nsic by extracting arohana-avarohana pattern. Int. Jour-\nnal of Recent Trends in Engineering , 1(1):362–366,\n2009.\n[28] F. Takens. Detecting strange attractors in turbulence.\nDynamical Systems and Turbulence, Warwick 1980 ,\npages 366–381, 1981.\n[29] T. Viswanathan and M. H. Allen. Music in South India .\nOxford University Press, 2004.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 757"
    },
    {
        "title": "Meter Detection in Symbolic Music Using Inner Metric Analysis.",
        "author": [
            "W. Bas de Haas",
            "Anja Volk"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417345",
        "url": "https://doi.org/10.5281/zenodo.1417345",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/033_Paper.pdf",
        "abstract": "In this paper we present PRIMA: a new model tailored to symbolic music that detects the meter and the first down- beat position of a piece. Given onset data, the metrical structure of a piece is interpreted using the Inner Metric Analysis (IMA) model. IMA identifies the strong and weak metrical positions in a piece by performing a periodicity analysis, resulting in a weight profile for the entire piece. Next, we reduce IMA to a feature vector and model the detection of the meter and its first downbeat position prob- abilistically. In order to solve the meter detection prob- lem effectively, we explore various feature selection and parameter optimisation strategies, including Genetic, Max- imum Likelihood, and Expectation-Maximisation algo- rithms. PRIMA is evaluated on two datasets of MIDI files: a corpus of ragtime pieces, and a newly assembled pop dataset. We show that PRIMA outperforms autocorrelation- based meter detection as implemented in the MIDItoolbox on these datasets.",
        "zenodo_id": 1417345,
        "dblp_key": "conf/ismir/HaasV16",
        "content": "METER DETECTION IN SYMBOLIC MUSIC\nUSING INNER METRIC ANALYSIS\nW. Bas de Haas\nUtrecht University\nW.B.deHaas@uu.nlAnja Volk\nUtrecht University\nA.Volk@uu.nl\nABSTRACT\nIn this paper we present P RIMA : a new model tailored to\nsymbolic music that detects the meter and the ﬁrst down-\nbeat position of a piece. Given onset data, the metrical\nstructure of a piece is interpreted using the Inner Metric\nAnalysis ( IMA) model. IMA identiﬁes the strong and weak\nmetrical positions in a piece by performing a periodicity\nanalysis, resulting in a weight proﬁle for the entire piece.\nNext, we reduce IMA to a feature vector and model the\ndetection of the meter and its ﬁrst downbeat position prob-\nabilistically. In order to solve the meter detection prob-\nlem effectively, we explore various feature selection and\nparameter optimisation strategies, including Genetic, Max-\nimum Likelihood, and Expectation-Maximisation algo-\nrithms. P RIMA is evaluated on two datasets of MIDI ﬁles:\na corpus of ragtime pieces, and a newly assembled pop\ndataset. We show that P RIMA outperforms autocorrelation-\nbased meter detection as implemented in the MIDI toolbox\non these datasets.\n1. INTRODUCTION\nWhen we listen to a piece of music we organise the stream\nof auditory events seemingly without any effort. Not only\ncan we detect the beat days after we are born [31], as\ninfants we are able to develop the ability to distinguish\nbetween a triple meter and duple meter [18]. The pro-\ncessing of metrical structure seems to be a fundamental\nhuman skill that helps us to understand music, synchron-\nize our body movement to the music, and eventually con-\ntributes to our musical enjoyment. We believe that a sys-\ntem so crucial to human auditory processing must be able\nto offer great merit to Music Information Retrieval ( MIR)\nas well. But what exactly constitutes meter, and how can\nmodels of metrical organisation contribute to typical MIR\nproblems? With the presentation of the P RIMA1model we\naim to shed some light on these matters in this paper.\nThe automatic detection of meter is an interesting and\nchallenging problem. Metrical structure has a large inﬂu-\n1Probabilistic Reduction of Inner Metric Analysis\nc/circlecopyrtW. Bas de Haas, Anja V olk. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: W. Bas de Haas, Anja V olk. “Meter Detection in Symbolic Music\nUsing Inner Metric Analysis”, 17th International Society for Music In-\nformation Retrieval Conference, 2016.ence on the harmonic, melodic and rhythmic structure of a\npiece, and can be very helpful in many practical situations.\nFor instance, in [30] a statistical exploration of common\nsyncopation patterns in a large corpus of symbolic rag-\ntime pieces is presented. For correct analysis of synco-\npation patterns knowledge of the meter is essential. How-\never, many corpora lack reliable meter annotations, mak-\ning automatic meter detection a prerequisite for rhythmic\npattern analysis. Similarly, chord recognition algorithms\nhave been shown to improve when metrical information is\ntaken into account, e.g. [3]. Finally, also melodic similarity\nestimation beneﬁts from (automatically derived) metrical\ninformation. Humans appear to be more tolerant to note\ntransformations placed on weak metrical positions [11].\nIn this paper we present P RIMA : a new model for de-\ntecting the meter and the ﬁrst downbeat in a sequence of\nonsets. Where most other approaches reduce the problem\nto a binary duple / triple meter detection, P RIMA estimates\nall time signatures that are available in the training set and\nalso detects the ﬁrst downbeat position. P RIMA ’s architec-\nture is outlined as follows: the model employs Inner Metric\nAnalysis [28, IMA] to determine the strong and weak met-\nrical positions in an onset sequence. The IMA is folded into\none-bar proﬁles, which are subsequently optimised. This\nmetrical analysis feature serves as input to a probabilistic\nmodel which eventually determines the meter. Finally, two\nfeature optimisation strategies are discussed and evaluated.\nPRIMA is trained and tested on two datasets of MIDI\nﬁles: the RAG collection [30] and the newly collected\nFMpop collection. The main motivation for choosing\nthe RAG collection for evaluation is that there is a clear\nneed for meter and ﬁrst downbeat detection for facilitat-\ning corpus-based studies on this dataset. Since Ragtime\nis a genre that is deﬁned by syncopated rhythms [30], in-\nformation on meter and the location of the ﬁrst downbeat\nis crucial for corpus-based rhythm analyses. In order to as-\nsess the ﬂexibility of P RIMA , we also train and evaluate the\nmodel on a new dataset of pop music: the FMpop collec-\ntion. All data has been produced by music enthusiasts and\nis separated into a test and a training set. Both datasets are\ntoo big to manually check all meter annotations. There-\nfore, we assume that in the training set the majority of the\nmeters are correctly annotated. In the test set, the meter\nand ﬁrst downbeat positions are manually corrected, and\nthis conﬁrms the intuition that the majority of the meters is\ncorrect, but annotation errors do occur.\nTaks description: We deﬁne the meter detection task441as follows: given a series of onsets, automatically detect\nthe time signature and the position of the ﬁrst beat of the\nbar. This ﬁrst beat position is viewed as the offset of the\nmeter measured from the starting point of an analysed seg-\nment, and we will refer to this offset as the rotation of the\nmeter.2After all, a metrical hierarchy recurs every bar, and\nif the meter is stable, the ﬁrst beat of the bar can easily be\nmodelled by rotating the metrical grid. In this paper we\nlimit our investigation to the22,24,44,34,68,128meters that oc-\ncur at least in 40pieces of the dataset (ﬁve different meters\nin the RAG, four in the FMpop Collection). Naturally, ad-\nditional meters can be added easily. In the case of duple /\ntriple classiﬁcation22,24, and44are considered duple meters\nand34,68, and128are considered triple meters. Within this\nstudy we assume that the meter does not change through-\nout an analysed segment, and we consider only MIDI data.\nContribution: The contribution of this paper is\nthreefold. First, we present a new probabilistic model for\nautomatically detecting the meter and ﬁrst downbeat pos-\nition in a piece. P RIMA is conceptually simple, based on\na solid metrical model, ﬂexible, and easy to train on style\nspeciﬁc data, Second, we present a new MIDI dataset con-\ntaining 7585 pop songs. Furthermore, for small subsets\nof this new FMpop Collection and a collection of ragtime\npieces, we also present new ground-truth annotations of\nthe meter and rotation. Finally, we show that all variants\nof P RIMA outperform the autocorrelation-based meter de-\ntection implemented in the MIDI toolbox [5].\n2. RELATED WORK\nThe organisation of musical rhythm and meter has been\nstudied for decades, and it is commonly agreed upon that\nthis organisation is best represented hierarchically [13].\nWithin a metrical hierarchy strong metrical positions can\nbe distinguished from weaker positions, where strong po-\nsitions positively correlate with the number of notes, the\nduration of the notes, the number of equally spaced notes,\nand the stress of the notes [16]. A few (computational)\nmodels have been proposed that formalise the induction of\nmetrical hierarchies, most notable are the models of Steed-\nman [20], Longuet-Higgins & Lee [14] Temperley [21],\nand V olk [28]. However, surprisingly little of this work\nhas been applied to the automatic detection of the meter\n(as in the time signature) of a piece of music, especially in\nthe domain of symbolic music.\nMost of the work in meter detection focusses on the au-\ndio domain and not on symbolic music. Although large in-\ndividual differences exists, in the audio domain the meter\ndetection systems follow a general architecture that con-\nsists of a feature extraction front-end and a model that\naccounts for periodicities in the onset or feature data. In\nthe front-end typically features are used that are associated\nwith onset detection such as spectral difference, or ﬂux,\nand energy spectrum are used [1]. Or, in the symbolic case,\n2We chose the new term rotation for the offset of the meter because\nthe musical terms generally used to describe this phenomenon, like ana-\ncrusis ,upbeat ﬁgure , orpickup , are sometimes interpreted differently.one simply assumes that onset data is available [9, 22], like\nwe do in this paper.\nAfter feature extraction the periodicity of the onset data\nis analysed, which is typically done using auto-correlation\n[2, 23], a (beat) similarity matrix [6, 8], or hidden Markov\nmodels [17, 12]. Next, the most likely meter has to be\nderived from the periodicity analysis. Sometimes statist-\nical machine learning techniques, such as Gaussian Mix-\nture Models, Neural Networks, or Support Vector Ma-\nchines [9], are applied to this task, but this is less com-\nmon in the symbolic domain. The free parameters of these\nmodels are automatically trained on data that has meter an-\nnotations. Frequently the meter detection problem is sim-\npliﬁed to classifying whether a piece uses a duple ortriple\nmeter [9, 23], but some authors aim at detecting more ﬁne-\ngrained time signatures [19, 24] and can even detect odd\nmeters in culturally diverse music [10]. Albeit focussed on\nthe audio domain, for a relatively recent overview of the\nﬁeld we refer to [24].\n2.1 Inner Metric Analysis\nSimilar to most of the meter detection systems outlined in\nthe previous section P RIMA relies on periodicity analysis.\nHowever, an important difference is that it uses the Inner\nMetric Analysis [28, IMA] instead of the frequently used\nautocorrelation. IMA describes the inner metric structure\nof a piece of music generated by the actual onsets opposed\nto the outer metric structure which is associated with an\nabstract grid annotated by a time signature in a score, and\nwhich we try to detect automatically with P RIMA .\nWhat distinguishes IMA from other metrical models,\nsuch as Temperley’s Grouper [21], is that IMA is ﬂexible\nwith respect to the number of metric hierarchies induced.\nIt can therefore be applied both to music with a strong\nsense of meter, e.g. pop music, and to music with less pro-\nnounced or ambiguous meters. IMA has been evaluated in\nlistening experiments [25], and on diverse corpora of mu-\nsic, such as classical pieces [26], rags [28], latin american\ndances [4] and on 20th century compositions [29].\nIMA is performed by assigning a metric weight or a\nspectral weight to each onset of the piece. The general idea\nis to search for all chains of equally spaced onsets within a\npiece and then to assign a weight to each onset. This chain\nof equally spaced onsets underlying IMA is called a local\nmeter and is deﬁned as follows. Let Ondenote the set of\nall onsets of notes in a given piece. We deﬁne every subset\nm⊂Onof equally spaced onsets to be a local meter if\nit contains at least three onsets and is not a subset of any\nother subset of equally spaced onsets. Each local meter can\nbe identiﬁed by three parameters: the starting position of\nthe ﬁrst onset s, the period denoting the distance between\nconsecutive onsets d, and the number of repetitions kof\nthe period (which equals the size of the set minus one).\nThe metric weight of an onset ois calculated as the\nweighted sum of the length kmof all local meters mthat\ncoincide at this onset ( o∈m), weighted by parameter p\nthat regulates the inﬂuence of the length of the local meters\non the metric weight. Let M(/lscript)be the set of all local442 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016meters of the piece of length at least /lscript, then the metric\nweight of an onset, o∈On, is deﬁned as follows:\nW/lscript,p(o) =/summationdisplay\n{m∈M(/lscript):o∈m}kp\nm. (1)\nThe spectral weight is calculated in a similar fashion,\nbut for the spectral weight each local meter is extended\nthroughout the entire piece. The idea behind this is that the\nmetrical structure induced by the onsets stretches beyond\nthe region in which onsets occurs. The extension of a local\nmetermis deﬁned as ext(ms,d,k) ={s+id,∀i}wherei\nis an integer number. For all discrete metrical positions t,\nregardless whether it contains an onset or not, the spectral\nweight is deﬁned as follows:\nSW/lscript,p(t) =/summationdisplay\n{m∈M(/lscript):t∈ext(m)}kp\nm. (2)\nIn this paper we have used the standard parameters p= 2,\nand/lscript= 2. Hence, we consider all local meters that exist in\na piece. A more elaborate explanation of the IMA including\nexamples can be found in [28].\n3. IMA BASED METER DETECTION\nIn this section we will outline the P RIMA model in a\nbottom-up fashion. We start with the input MIDI data, and\ndescribe how we transform this into onset data, perform\nIMA and ﬁnally optimise a feature based on IMA. Next, we\nexplain how this feature is used in a probabilistic model to\ndetect the meter and rotation of sequence of onsets, and we\nelaborate on two different training strategies.\n3.1 Quantisation and Preprocessing\nBefore we can perform IMA, we have to preprocess the\nMIDI ﬁles to obtain a quantised sequence of onsets. The\nfollowing preprocessing steps are taken:\nTo be able to ﬁnd periodicities, the onset data should\nbe quantised properly. Within Western tonal music duple\nas well as triple subdivisions of the beat occur commonly.\nHence, we use a metrical grid consisting of 12equally\nspaced onset positions per quarter note. With this we can\nquantise both straight and swung eight notes. Here, swing\nrefers to the characteristic long-short rhythmical pattern\nthat is particularly common in Jazz, but is found through-\nout popular music.\nIn the quantisation process we use the length of a\nquarter note as annotated in the MIDI ﬁle. This MIDI time\ndivision speciﬁes the number of MIDI ticks per quarter note\nand controls the resolution of the MIDI data. Because the\nMIDI time division is constant, strong tempo deviations in\nthe MIDI data might distort the quantisation process and\nthe following analyses. To estimate the quality of the\nalignment of the MIDI data to the metrical grid, we collect\nthe quantisation deviation for every onset, and the average\nquantisation deviation divided by the MIDI time division\ngives a good estimate of the quantisation error. To make\nsure that the analysed ﬁles can be quantised reasonably\n2434\n0 4 3 6\nNSW{\nIMANSW bin 0\nNSW bin 4\nNSW bin 3\nNSW bin 6\nNSW(1)\n(2)\n(3){ {Figure 1 . The construction of NSW proﬁles for a piece in\n2\n4: (1) displays IMA, (2) displays the NSW proﬁles derived\nfrom IMA for a2\n4and a3\n4meter, and (3) shows how two\nbins are selected from each proﬁle and used to estimate the\nprobability of that meter. The ellipse represents the Gaus-\nsian distribution ﬁtted to selected bins of the NSW proﬁles\nin the training phase. Note that the3\n4NSW proﬁle does not\nresemble a typical3\n4and receives a low probability. Also,\nthe selected bins may differ per optimisation strategy.\nwell, we discard all MIDI ﬁles with an average quantisa-\ntion error higher than 2 percent.\nAfter quantising the MIDI data, we collect all onset\ndata from all voices and remove duplicate onsets. Next,\nthe MIDI data is segmented at all positions where a meter\nchange is annotated in the MIDI ﬁle. Segments that are\nempty or shorter than 4bars are excluded from further ana-\nlysis. Also, MIDI ﬁles that do not contain meter annota-\ntions at all are ignored in the training phase.\n3.2 Normalised spectral weight proﬁles\nWe use the spectral weights of IMA to construct a fea-\nture for detecting the meter in a piece. More speciﬁc-\nally, this feature will model the conditional probability of\na certain meter given a sequence of onsets. As we will\nexplain in more detail in the next section, the distribu-\ntion of these features will be modelled with a Gaussian\ndistribution. We call this feature a Normalised Spectral\nWeight (NSW ) proﬁle, and discern three stages in construct-\ning them: (1) perform IMA, (2) folding the IMA in one-bar\nproﬁles and normalising the weights proﬁles, and (3) se-\nlecting the most relevant bins for modelling the Gaussian\ndistribution. These three stages are displayed schematic-\nally in Figure 1, and are detailed below.\nIMA marks the metrical importance of every quantised\nonset position in a piece. Because of the large numbers\nof spectral weights and the large differences per piece,\nIMA cannot be used to detect the meter directly. How-Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 443ever, we can fold the analysis into one-bar proﬁles to get\na more concise metrical representation for every candidate\nmeter. These one-bar proﬁles are created by summing the\nspectral weights per quantised position within a bar. Con-\nsequently, the shape of these proﬁles is determined by the\nmeter (beats per bar), the length of piece (longer pieces\nyield higher spectral weights), and the number of quant-\nisation bins.\nWe normalise spectral weights in the one-bar proﬁles\nby applying Formula 3:\nnormalise (w) = log(w\nnβ+α) (3)\nHere,wis the summed spectral weight of a particular\nquantised beat position and nis the number of bars used to\ncreate the proﬁle. We use a parameter βto control the ef-\nfect of the length of the piece in the normalisation. Further-\nmore, because many quantised beat positions might have\na summed metrical weight of 0, and this will cause prob-\nlems when we ﬁt Gaussian models to these proﬁles, we use\nLaplace smoothing [15, p. 260] and add a constant factor α\nto all weights. Finally, because statistical analysis of large\namounts of proﬁles showed that differences in weights are\ndistributed normally on a logarithmic scale, we apply the\nnatural logarithm in Eq. 3. For the results in this report\nwe have used β= 2 andα= 1. We call these proﬁles\nNormalised Spectral Weight (NSW ) Proﬁles.\nThe raw NSW proﬁles cannot yet be conveniently used\nas a feature for meter detection: the dimensionality of the\nNSW proﬁles is relatively high, and the dimensionality dif-\nfers per meter. Also, not every metrical position within a\nbar is equally important. For instance, the ﬁrst beat of the\nbar will have a high spectral weight, while the metrical po-\nsition of the second eighth note will generally have a much\nlower spectral weight. Hence, we select proﬁle bins that\ncontain the information most relevant for meter detection.\nThe selection of the relevant proﬁle bins is a special case\nof feature dimensionality reduction where the feature bins\nare highly dependent on each other. In this section we in-\ntroduce two selection methods that will be experimentally\nveriﬁed in Section 4.3. A ﬁrst intuition is to select the n\nproﬁle bins that have the highest weights on average for\na given dataset. A brief analysis showed that these bins\nroughly correspond to the ﬁrst nprincipal components.\nHowever, a preliminary evaluation shows that NSW pro-\nﬁles containing only these bins perform relatively poorly.\nHence, in order to learn more about what are the distinct-\nive bins in the proﬁles, we use a Genetic Algorithm ( GA)\nto explore the space of possible bin selections.3When we\nanalyse these bin selections, we notice that the GAselects\nbins for a meter that contain weights that are maximally\ndifferent to other meters.\n3Note that these nproﬁle bins may differ between meters, and ndoes\nnot have to be the same for all meters, as long as the bin selection of\nthe examined proﬁle is exactly the same as the selection used in the tem-\nplate proﬁle for that meter. For the implementation of the GAwe use the\nHaskell library https://github.com/boegel/GA , using a popu-\nlation size of 100 candidates, a crossover rate of 0.7, a mutation rate of\n0.2, and Eq. 5 as ﬁtness function.Training a GAon large amounts of data takes a lot of\ntime, even if the NSW proﬁles are pre-calculated. Since\nwe have a clear intuition about how the GAselects proﬁle\nbins, we might be able to mimic this behaviour without ex-\nploring the complete space of possible bin selections. Re-\ncall when we classify a single piece, we calculate multiple\nNSW proﬁles for a single IMA: one for each meter. If we\nselect the same bins in each proﬁle for matching, i.e. every\nﬁrst beat of a bar, the chances are considerable that this\nselection will match multiple meters well. Hence, we se-\nlect thenbins of which the NSW proﬁles of the ground-\ntruth meter are maximally different from the NSW proﬁles\nof other meters. In this calculation we deﬁne maximally\ndifferent as having a maximal absolute difference in spec-\ntral weight, and ndoes not differ between meters. We call\nthis method the Maximally Different Bin (MDB ) Selection.\n3.3 A probabilistic meter classiﬁcation model\nTo restate our initial goal: we want to determine the meter\nand its rotation given a sequence of note onsets. Ignor-\ning rotation, a good starting point is to match NSW pro-\nﬁles with template proﬁles of speciﬁc meters. However,\nalthough the spectral weights of IMA reﬂect human intu-\nitions about the musical meter [27], it is rather difﬁcult to\ndesign such templates proﬁles by hand. Moreover, these\ntemplate proﬁles might be style speciﬁc. Hence, we pro-\npose a model that learns these templates from a dataset.\nAnother style dependent factor that inﬂuences meter de-\ntection is the distribution of meters in a dataset. For in-\nstance, in Ragtime22and24occur frequently, while pop\nmusic is predominantly notated in a44meter. Just match-\ning NSW proﬁles with meter templates will not take this\ninto account. When we combine simple proﬁle matching\nwith a weighting based on a meter distribution (prior), this\nconceptually equals a Naive Bayes classiﬁer [15]. There-\nfore, probabilistically modelling meter detection is a nat-\nural choice.\nIf we ignore the rotations for sake of simplicity, we can\nexpress the probability of a meter given a set of note onsets\nwith Equation 4:\nP(meter|onsets )∝P(onsets|meter )·P(meter ) (4)\nHere,P(onset|meter )reﬂects the probability of an onset\nsequence given a certain meter, and ∝denotes “is propor-\ntional to”. Naturally, certain meters occur more regularly\nin a dataset than others which is modelled by P(meter ).\nThe conditional probability P(onset|meter )can be estim-\nated using NSW proﬁles. Given a piece and a speciﬁc meter\nwe create an NSW proﬁle that can be used as multidimen-\nsional feature. Given a large dataset that provides us with\nsequences of onsets and meters, we can model the distri-\nbution of the NSW proﬁles as Gaussian distributions. For\nevery meter in the dataset we estimate the mean and co-\nvariance matrix of a single Gaussian distribution with the\nexpectation-maximization algorithm [7]. The prior prob-\nability of a certain meter, P(meter ), can be estimated with\nmaximum likelihood estimation, which equals the number444 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016of times a certain meter occurs in a dataset divided by the\ntotal number of meters in the dataset.\nAdding the estimation of the rotation makes the prob-\nlem slightly more complicated. A natural way of incorpor-\nating rotation is to add it as a single random variable that\nis dependent on the meter. This makes sense because it\nis likely that the kind of rotation depends on the kind of\nmeter: an anacrusis in a44meter is likely to differ from an\nanacrusis in a34meter. Hence, we can transform Eq. 4 into\nthe following equation:\nP(x,r|y)∝P(y|x,r)·P(r|x)·P(x) (5)\nSimilar to Eq. 4, we estimate the meter xgiven an onset\npatterny, but now we also add the rotation r. The term\nP(y|x,r)can again be modelled with NSW proﬁles, but\nnow the proﬁles should also be rotated according to the ro-\ntationr. The termP(x)reﬂects the probability of a meter\nand can be estimated with maximum likelihood estimation.\nWe do not consider all possible rotations. For a44meter\nthere are 4·12 = 48 possible rotations, many of which are\nnot likely to occur in practise. The rotations are modelled\nas a fraction of a bar, making the rotation meter independ-\nent. Furthermore, we rotate clock-wise, e.g.1\n4represents\nan anacrusis of one quarter note in a44meter. The space of\npossible rotations can be further reduced by only consid-\nering two kinds of rotations: rotations for duple and triple\nmeters. After all, given the very similar metrical structure\nof24and44, we expect that the rotations will be similar as\nwell (but on another absolute metrical level, e.g. eighth in-\nstead of quarter notes). For duple meters we explore eight,\nand for triple meters we explore six different rotations.\nUnfortunately, estimating the prior probability of the ro-\ntation given a certain meter, i.e. P(r|x), is not trivial be-\ncause we rely on MIDI data in which the rotation is not\nannotated. Hence, we need another way of estimating this\nprior probability of the rotation. We estimate the rotation\nby calculating all rotations of the NSW proﬁles and pick\nthe rotation that maximises probability of the annotated\nground-truth meter. Having an estimation of the best ﬁt-\nting rotation per piece, we can perform maximum likeli-\nhood estimation by counting different rotations for each\nmeter in order to obtain the rotation probabilities.\n3.4 Training\nWe train and evaluate P RIMA on two datasets (see Sec. 4.1\nand 4.2). These datasets consist of MIDI ﬁles created by\nmusic enthusiasts that might have all sorts of musical back-\ngrounds. Hence, it is safe to assume that the meter annota-\ntions in these MIDI ﬁles might sometimes be incorrect. A\nlikely scenario is, for instance, that MIDI creation software\nadds a44meter starting at the ﬁrst note onset by default,\nwhile the piece in question starts with an upbeat and is best\nnotated in34. Nevertheless, we assume that the majority of\nthe meters is annotated correctly, and that incorrect meters\nwill only marginally effect the training of P RIMA .\nIn this paper we evaluate two different ways of training\nPRIMA . We use Maximally Different Bin ( MDB ) selection\nin the feature training phase, or alternatively, we use a GAto select the most salient NSW proﬁle bins. After the bin\nselection, we use Maximum Likelihood estimation to learn\nthe priors and rotation, as described in the previous section,\nand Expectation-Maximisation for ﬁtting the Gaussian dis-\ntributions.\n4. EV ALUATION\nTo assess the quality of the meter and rotations calculated\nby P RIMA , we randomly separate our datasets into test-\ning and training sets. The test sets are manually corrected\nand assured to have a correct meter and rotation. The next\ntwo sections will detail the data used to train and evaluate\nPRIMA . The manual inspection of the meters and rotations\nconﬁrms the intuition that most of the meters are correct,\nbut the data does contain meter and rotation errors.\n4.1 RAG collection\nThe RAG collection that has been introduced in [30] cur-\nrently consists of 11545 MIDI ﬁles of ragtime pieces that\nare collected from the Internet by a community of Ragtime\nenthusiasts. The collection is accompanied by an elaborate\ncompendium4that stores additional information about in-\ndividual ragtime compositions, like year, title, composer,\npublisher, etc. The MIDI ﬁles in the RAG collection de-\nscribe many pieces from the ragtime era (approx. 1890∼\n1920 ), but also modern ragtime compositions. The dataset\nis separated randomly in a test set of 200pieces and a train-\ning set of 11345 pieces. After the preprocessing detailed\nin Sec. 3.1, 74and4600 pieces are considered suitable for\nrespectively testing and training. For one piece we had to\ncorrect the meter and for another piece the rotation.\n4.2 FMpop collection\nThe RAG corpus only contains pieces in the ragtime style.\nIn order to study how well P RIMA predicts the meter\nand rotation of regular pop music, we collected 7585\nMIDI ﬁles from the website Free-Midi.org.5This collec-\ntion comprises MIDI ﬁles describing pop music from the\n1950 onwards, including various recent hit songs, and we\ncall this collection the FMpop collection. For evaluation\nwe randomly select a test set of 200pieces and we use the\nremainder for training. In the training and test sets, 3122\nand89pieces successfully pass the preprocessing stage,\nrespectively. Most of the pieces that drop out have a quant-\nisation error greater than 2percent. For three pieces we\nhad to correct the meter, and for four pieces the rotation.\n4.3 Experiments\nWe perform experiments on both the RAG and the FMpop\ncollections in which we evaluate the detection performance\nby comparing the proportion of correctly classiﬁed meters,\nrotations, and the combinations of the two. In these experi-\nments we probe three different training variants of P RIMA :\n(1) a variant where we use Maximally Different Bin ( MDB )\n4seehttp://ragtimecompendium.tripod.com/ for more\ninformation\n5http://www.free-midi.org/Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 445RAG Collection\n(Training) model Meter Rotation Both\nDuple / Triple meters\nMIDI toolbox .76− −\nMDB selection (2 bins) .97.88.86\nMDB selection (3 bins) .97.97.95\nGAoptimized .97.99.96\nMeters:22,24,34,44,68\nMDB selection (2 bins) .85.92.80\nMDB selection (3 bins) .80.92.76\nGAoptimised .84.93.82\nTable 1 . The proportion of correctly detected meter and\nrotation in the RAG collection. The ﬁrst section shows the\nduple / triple meter classiﬁcation, the second section shows\nthe proportions for the ﬁve most used time signatures.\nFMpop Collection\n(Training) model Meter Rotation Both\nDuple / Triple meters\nMIDI toolbox .74− −\nMDB selection (2 bins) .94.90.85\nMDB selection (3 bins) .90.93.84\nGAoptimized .94.88.83\nMeters:34,44,68,128\nMDB selection (2 bins) .94.81.79\nMDB selection (3 bins) .94.81.78\nGAoptimised .94.91.87\nTable 2 . The correctly detected proportion for on the\nFMpop collection for duple / triple meter classiﬁcation and\nfor the four most used time signatures.\nselection in which we select the two most salient bins and\n(2) a variant in which we select the three most salient bins.\nFinally, (3) we also use a Genetic algorithm to select the\nbins and estimate the rotation priors.\nTo place the performance of P RIMA into context, we\ncompare the results to the meter detection model imple-\nmented in the MIDI toolbox [5]. This model only pre-\ndicts whether a meter is duple or triple and does not pre-\ndict the time signature. Therefore, we can compare the\nMIDI toolbox meter ﬁnding to P RIMA only in the duple /\ntriple case. To ensure we use the exact same input data, we\nhave written our own NMAT export script that transforms\nthe MIDI as preprocessed by P RIMA into a matrix that can\nbe parsed by the MIDI toolbox. All source code and data\nreported in this study is available on request.\n4.4 Results\nWe evaluate the performance P RIMA and its different train-\ning strategies on duple / triple meter detection and the de-\ntection of ﬁve different time signatures. In Table 1 the pro-\nportions of correctly detected meters in the RAG collection\nare displayed. In the duple / triple meter detection exper-iments all variants of P RIMA outperform the MIDI toolbox\nmeter detection. We tested the statistical signiﬁcance of all\nindividual differences between MIDI toolbox meter detec-\ntion and P RIMA using McNemar’s χ2test, and all differ-\nences are signiﬁcant ( p <0.001). In the classiﬁcation of\nﬁve different time signatures the performance drops con-\nsiderably. However, rags are mostly notated in24,44, and22\nmeters, and even experienced musicians have difﬁculty de-\ntermining what is the correct meter. Still P RIMA achieves a\n96percent correct estimation for meter and rotation in the\nduple / triple experiment and 82percent correct estimation\non the full time signature detection.\nIn Table 2 the proportions of correctly classiﬁed meters\nin the FMpop Collection are displayed. Also on onsets ex-\ntracted from popular music, P RIMA outperforms the MIDI -\ntoolbox meter ﬁnding. Again, we tested the statistical sig-\nniﬁcance of the differences between all P RIMA variants\nusing McNemar’s χ2test, and all differences are statist-\nically signiﬁcant ( p<0.002forGAand MDB selection (2\nbins), andp<0.017forMDB selection (3 bins)). Overall,\nPRIMA ’s performance on the FMpop Collection is lower\nthan on the RAG Collection for the duple / triple detection,\nbut higher for time signature detection. Respectively, 85\nand87percent correct classiﬁcation is achieved for both\nmeter and rotation. Generally, the GAseems to yield the\nbest results.\n5. DISCUSSION AND CONCLUSION\nWe presented a new model for detecting the meter and ﬁrst\ndownbeat position of a piece of music. We showed that\nIMA is valuable in the context of meter and ﬁrst down-\nbeat detection. P RIMA is ﬂexible, can be easily trained\non new data, and is conceptually simple. We have shown\nthat P RIMA performs well on the FMpop and RAG Col-\nlections and outperforms the MIDI toolbox meter ﬁnding\nmodel. However, while P RIMA can be trained on data of\nspeciﬁc styles, the parameters of the MIDI toolbox meter\ndetection model are ﬁxed. Hence, the performance of\nthe MIDI toolbox should be seen as a baseline system that\nplaces P RIMA ’s results into context.\nIn this study we applied P RIMA toMIDI data only be-\ncause we believe that corpus based analyses on collections\nlike the RAG collection can really beneﬁt from meter ﬁnd-\ning. Nevertheless, P RIMA ’sIMA based feature and probab-\nilistic model are generic and can be easily applied to onset\nsequences extracted from audio data. Hence, it would be\ninteresting to investigate how P RIMA model performs on\naudio data, and compare it to the state-of-the-art in audio\nmeter detection. We strongly believe that also in the au-\ndio domain meter detection can beneﬁt from IMA. We are\nconﬁdent that IMA has the potential to aid in solving many\nMIR tasks in both the audio and the symbolic domain.\n6. ACKNOWLEDGMENTS\nA. V olk and W.B. de Haas are supported by the Neth-\nerlands Organization for Scientiﬁc Research, through the\nNWO-VIDI-grant 276-35-001 to Anja V olk.446 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20167. REFERENCES\n[1] J.P. Bello, L. Daudet, S. Abdallah, C. Duxbury,\nM. Davies, and M.B. Sandler. A tutorial on onset detec-\ntion in music signals. IEEE Trans. ASLP , 13(5):1035–\n1047, 2005.\n[2] J.C. Brown. Determination of the meter of musical\nscores by autocorrelation. JASA , 94(4):1953–1957,\n1993.\n[3] R. Chen, W. Shen, A. Srinivasamurthy, and P. Chor-\ndia. Chord recognition using duration-explicit hidden\nmarkov models. In Proc. ISMIR , pages 445–450, 2012.\n[4] E. Chew, A. V olk, and C.-Y . Lee. Dance music classi-\nﬁcation using inner metric analysis. In The next wave\nin computing, optimization, and decision technologies ,\npages 355–370. Springer, 2005.\n[5] T. Eerola and P. Toiviainen. MIDI Toolbox: MATLAB\nTools for Music Research . University of Jyv ¨askyl ¨a,\nJyv¨askyl ¨a, Finland, 2004.\n[6] J. Foote and S. Uchihashi. The beat spectrum: a\nnew approach to rhythm analysis. Proc. IEEE ICME ,\n0:224–228, 2001.\n[7] C. Fraley and A. E. Raftery. Model-based clustering,\ndiscriminant analysis, and density estimation. Journal\nof the American Statistical Association , 97(458):611–\n631, 2002.\n[8] M. Gainza. Automatic musical meter detection. In\nProc. ICASSP , pages 329–332, April 2009.\n[9] F. Gouyon and P. Herrera. Determination of the meter\nof musical audio signals: Seeking recurrences in beat\nsegment descriptors. In Proc. AES . Audio Engineering\nSociety, 2003.\n[10] A. Holzapfel, F. Krebs, and A. Srinivasamurthy. Track-\ning the “odd”: Meter inference in a culturally diverse\nmusic corpus. In Proc. ISMIR , pages 425–430, 2014.\n[11] P. van Kranenburg, A. V olk, F. Wiering, and R. C.\nVeltkamp. Musical models for folk-song alignment. In\nProc. ISMIR , pages 507–512, 2009.\n[12] F. Krebs, S. B ¨ock, and G. Widmer. Rhythmic pattern\nmodeling for beat and downbeat tracking in musical\naudio. In Proc. ISMIR , pages 227–232, 2013.\n[13] F. Lerdahl and R. Jackendoff. A Generative Theory of\nTonal Music . MIT Press, 1996.\n[14] H. C. Longuet-Higgins and C. S. Lee. The rhythmic\ninterpretation of monophonic music. Music Perception ,\n1(4):424–441, 1984.\n[15] C. D. Manning, P. Raghavan, and H. Sch ¨utze. Intro-\nduction to Information Retrieval . Cambridge Univer-\nsity Press, New York, NY , USA, 2008.[16] C. Palmer and C. L. Krumhansl. Mental representa-\ntions for musical meter. JEPHPP , 16(4):728, 1990.\n[17] G. Peeters and H. Papadopoulos. Simultaneous beat\nand downbeat-tracking using a probabilistic frame-\nwork: Theory and large-scale evaluation. IEEE Trans.\nASLP , 19(6):1754–1769, Aug 2011.\n[18] J. Phillips-Silver and L. J. Trainor. Feeling the beat:\nMovement inﬂuences infant rhythm perception. Sci-\nence, 308(5727):1430, 2005.\n[19] A. Pikrakis, I. Antonopoulos, and S. Theodoridis. Mu-\nsic meter and tempo tracking from raw polyphonic au-\ndio. In Proc. ISMIR , pages 192–197, 2004.\n[20] M. J. Steedman. The perception of musical rhythm and\nmetre. Perception , 6(5):555–569, 1977.\n[21] D. Temperley. The Cognition of Basic Musical Struc-\ntures . Cambridge, MA, MIT Press, 2001.\n[22] P. Toiviainen and T. Eerola. The role of accent peri-\nodicities in meter induction: A classiﬁcation study. In\nProc. ICMPC , pages 422–425, 2004.\n[23] P. Toiviainen and T. Eerola. Autocorrelation in meter\ninduction: The role of accent structure. JASA ,\n119(2):1164–1170, 2006.\n[24] M. Varewyck, J.-P. Martens, and M. Leman. Musical\nmeter classiﬁcation with beat synchronous acoustic\nfeatures, DFT-based metrical features and support vec-\ntor machines. JNMR , 42(3):267–282, 2013.\n[25] A. V olk. The empirical evaluation of a mathematical\nmodel for inner metric analysis. In Proc. ESCOM ,\npages 467–470, 2003.\n[26] A. V olk. Metric investigations in Brahms symphonies.\nIn G. Mazzola, T. Noll, and E. Lluis-Puebla, editors,\nPerspectives in Mathematical and Computational Mu-\nsic Theory , pages 300–329. epOs Music, Osnabr ¨uck,\n2004.\n[27] A. V olk. Persistence and change: Local and global\ncomponents of metre induction using inner metric ana-\nlysis. J. Math. Music , 2(2):99–115, 2008.\n[28] A. V olk. The study of syncopation using inner metric\nanalysis: Linking theoretical and experimental analysis\nof metre in music. J. New Music Res. , 37(4):259–273,\n2008.\n[29] A. V olk. Applying inner metric analysis to 20th century\ncompositions. In MCM 2007 , CCIS 37, pages 204–\n210. Springer, 2009.\n[30] A. V olk and W. B. de Haas. A corpus-based study on\nragtime syncopation. In Proc. ISMIR , pages 163–168,\n2013.\n[31] I. Winkler, G. P. H ´aden, O. Ladinig, I. Sziller, and\nH. Honing. Newborn infants detect the beat in music.\nPNAS , 106(7):2468–2471, 2009.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 447"
    },
    {
        "title": "Further Steps Towards a Standard Testbed for Optical Music Recognition.",
        "author": [
            "Jan Hajic Jr.",
            "Jiri Novotný",
            "Pavel Pecina",
            "Jaroslav Pokorný"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1418161",
        "url": "https://doi.org/10.5281/zenodo.1418161",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/289_Paper.pdf",
        "abstract": "Evaluating Optical Music Recognition (OMR) is notori- ously difficult and automated end-to-end OMR evaluation metrics are not available to guide development. In “To- wards a Standard Testbed for Optical Music Recognition: Definitions, Metrics, and Page Images”, Byrd and Simon- sen recently stress that a benchmarking standard is needed in the OMR community, both with regards to data and evaluation metrics. We build on their analysis and def- initions and present a prototype of an OMR benchmark. We do not, however, presume to present a complete so- lution to the complex problem of OMR benchmarking. Our contributions are: (a) an attempt to define a multi- level OMR benchmark dataset and a practical prototype implementation for both printed and handwritten scores, (b) a corpus-based methodology for assessing automated evaluation metrics, and an underlying corpus of over 1000 qualified relative cost-to-correct judgments. We then as- sess several straightforward automated MusicXML eval- uation metrics against this corpus to establish a baseline over which further metrics can improve.",
        "zenodo_id": 1418161,
        "dblp_key": "conf/ismir/HajicNPP16",
        "content": "FURTHER STEPS TOWARDS A STANDARD TESTBED FOR OPTICAL\nMUSIC RECOGNITION\nJan Haji ˇc jr.1Jiˇr´ı Novotn ´y2Pavel Pecina1Jaroslav Pokorn ´y2\n1Charles University, Institute of Formal and Applied Linguistics, Czech Republic\n2Charles University, Department of Software Engineering, Czech Republic\nhajicj@ufal.mff.cuni.cz, novotny@ksi.mff.cuni.cz\nABSTRACT\nEvaluating Optical Music Recognition (OMR) is notori-\nously difﬁcult and automated end-to-end OMR evaluation\nmetrics are not available to guide development. In “To-\nwards a Standard Testbed for Optical Music Recognition:\nDeﬁnitions, Metrics, and Page Images”, Byrd and Simon-\nsen recently stress that a benchmarking standard is needed\nin the OMR community, both with regards to data and\nevaluation metrics. We build on their analysis and def-\ninitions and present a prototype of an OMR benchmark.\nWe do not, however, presume to present a complete so-\nlution to the complex problem of OMR benchmarking.\nOur contributions are: (a) an attempt to deﬁne a multi-\nlevel OMR benchmark dataset and a practical prototype\nimplementation for both printed and handwritten scores,\n(b) a corpus-based methodology for assessing automated\nevaluation metrics, and an underlying corpus of over 1000\nqualiﬁed relative cost-to-correct judgments. We then as-\nsess several straightforward automated MusicXML eval-\nuation metrics against this corpus to establish a baseline\nover which further metrics can improve.\n1. INTRODUCTION\nOptical Music Recognition (OMR) suffers from a lack of\nevaluation standards and benchmark datasets. There is\npresently no publicly available way of comparing vari-\nous OMR tools and assessing their performance. While\nit has been argued that OMR can go far even in the ab-\nsence of such standards [7], the lack of benchmarks and\ndifﬁculty of evaluation has been noted on multiple occa-\nsions [2, 16, 21]. The need for end-to-end system evalu-\nation (at the ﬁnal level of OMR when musical content is\nreconstructed and made available for further processing),\nis most pressing when comparing against commercial sys-\ntems such as PhotoScore,1SmartScore2or SharpEye3:\n1http://www.neuratron.com/photoscore.htm\n2http://www.musitek.com/index.html\n3http://www.visiv.co.uk\nc/circlecopyrtJan Haji ˇc jr., Ji ˇr´ı Novotn ´y, Pavel Pecina, Jaroslav\nPokorn ´y. Licensed under a Creative Commons Attribution 4.0 Interna-\ntional License (CC BY 4.0). Attribution: Jan Haji ˇc jr., Ji ˇr´ı Novotn ´y,\nPavel Pecina, Jaroslav Pokorn ´y. “Further Steps towards a Standard\nTestbed for Optical Music Recognition”, 17th International Society for\nMusic Information Retrieval Conference, 2016.these typically perform as “black boxes”, so evaluating on\nthe level of individual symbols requires a large amount of\nhuman effort for assessing symbols and their locations, as\ndone by Bellini et al. [18] or Sapp [19].\nOMR systems have varying goals, which should be\nreﬂected in evaluation. Helping speed up transcription\nshould be measured by some cost-to-correct metric; a hy-\npothetical automated score interpretation system could re-\nquire accurate MIDI, but does not need to resolve all slurs\nand other symbols; digitizing archive scores for retrieval\nshould be measured by retrieval accuracy; etc. We focus\non evaluating transcription, as it is most sensitive to errors\nand most lacking in evaluation metrics.\nSome OMR subtasks (binarization, staff identiﬁcation\nand removal, symbol localization and classiﬁcation) have\nnatural ways of evaluating, but the end-to-end task does\nnot: it is difﬁcult to say how good a semantic represen-\ntation (e.g., MusicXML) is. Manually evaluating system\noutputs is costly, slow and difﬁcult to replicate; and aside\nfrom Knopke and Byrd [12], Szwoch [20] and Padilla et\nal. [21], we know of no attempts to even deﬁne an auto-\nmatic OMR evaluation metric, much less deﬁne a method-\nology for assessing how well it actually evaluates.\nOur contribution does not presume to deﬁne an entire\nevaluation standard. Instead, we propose a robust, cumula-\ntive, data-driven methodology for creating one. We collect\nhuman preference data that can serve as a gold standard for\ncomparing MusicXML automated evaluation metrics, mir-\nroring how the BLEU metric and its derivatives has been\nestablished as an evaluation metric for the similarly elu-\nsive task of assessing machine translation based on agree-\nment with human judgements [17]. This “evaluating eval-\nuation” approach is inspired by the Metrics track of the\nWorkshop of Statistical Machine Translation competition\n(WMT) [3, 5, 14]. To collect cost-to-correct estimates for\nvarious notation errors, we generate a set of synthetically\ndistorted “recognition outputs” from a set of equally syn-\nthetic “true scores”. Then, annotators are shown examples\nconsisting of a true score and a pair of the distorted scores,\nand they are asked to choose the simulated recognition out-\nput that would take them less time to correct.\nAdditionally, we provide an OMR bechmark dataset\nprototype with ground truth at the symbol and end-to-end\nlevels.\nThe main contributions of our work are:\n•A corpus-based “evaluating evaluation” methodol-157ogy that enables iteratively improving, reﬁning and\nﬁne-tuning automated OMR evaluation metrics.\n•A corpus of 1230 human preference judgments as\ngold-standard data for this methodology, and as-\nsessments of example MusicXML evaluation met-\nrics against this corpus.\n•Deﬁnitions of ground truths that can be applied to\nCommon Western Music Notation (CWMN) scores.\n•MUSCIMA++. A prototype benchmark with multi-\nple levels of ground truth that extends a subset of the\nCVC-MUSCIMA dataset [9], with 3191 annotated\nnotation primitives.\nThe rest of this paper is organized as follows: in Sec. 2,\nwe review the state-of-the-art on OMR evaluation and\ndatasets; in Sec. 3, we describe the human judgment data\nfor developing automated evaluation metrics and demon-\nstrate how it can help metric development. In Sec. 4, we\npresent the prototype benchmark and ﬁnally, in Sec. 5, we\nsummarize our ﬁndings and suggest further steps to take.4\n2. RELATED WORK\nThe problem of evaluating OMR and creating a standard\nbenchmark has been discussed before [7,10,16,18,20] and\nit has been argued that evaluating OMR is a problem as\ndifﬁcult as OMR itself. Jones et al. [10] suggest that in or-\nder to automatically measure and evaluate the performance\nof OMR systems, we need (a) a standard dataset and stan-\ndard terminology, (b) a deﬁnition of a set of rules and met-\nrics, and (c) deﬁnitions of different ratios for each kind of\nerrors. The authors noted that distributors of commercial\nOMR software often claim the accuracy of their system\nis about 90 %, but provide no information about how that\nvalue was estimated.\nBellini et al. [18] manually assess results of OMR sys-\ntems at two levels of symbol recognition: low-level, where\nonly the presence and positioning of a symbol is assessed,\nand high-level, where the semantic aspects such as pitch\nand duration are evaluated as well. At the former level,\nmistaking a beamed group of 32nds for 16ths is a minor\nerror; at the latter it is much more serious. They deﬁned\na detailed set of rules for counting symbols as recognized,\nmissed and confused symbols. The symbol set used in [18]\nis quite rich: 56 symbols. They also deﬁne recognition\ngain, based on the idea that an OMR system is at its best\nwhen it minimizes the time needed for correction as op-\nposed to transcribing from scratch, and stress veriﬁcation\ncost: how much it takes to verify whether an OMR output\nis correct.\nAn extensive theoretical contribution towards bench-\nmarking OMR has been made recently by Byrd and Simon-\nsen [7]. They review existing work on evaluating OMR\nsystems and clearly formulate the main issues related to\nevaluation. They argue that the complexity of CWMN is\nthe main reason why OMR is inevitably problematic, and\n4All our data, scripts and other supplementary materials are available\nathttps://github.com/ufal/omreval as a git repository, in or-\nder to make it easier for others to contribute towards establishing a bench-\nmark.suggest the following stratiﬁcation into levels of difﬁculty:\n1. Music on one staff, strictly monophonic,\n2. Music on one staff, polyphonic,\n3. Music on multiple staves, but each strictly mono-\nphonic, with no interaction between them,\n4. “Pianoform”: music on multiple staves, one or more\nhaving multiple voices, and with signiﬁcant interac-\ntion between and/or within staves.\nThey provide 34 pages of sheet music that cover the var-\nious sources of difﬁculty. However, the data does not in-\nclude handwritten music and no ground truth for this cor-\npus is provided.\nAutomatically evaluating MusicXML has been at-\ntempted most signiﬁcantly by Szwoch [20], who proposes\na metric based on a top-down MusicXML node matching\nalgorithm and reports agreement with human annotators,\nbut how agreement was assessed is not made clear, no im-\nplementation of the metric is provided and the description\nof the evaluation metric itself is quite minimal. Due to the\ncomplex nature of MusicXML (e.g., the same score can\nbe correctly represented by different MusicXML ﬁles), Sz-\nwoch also suggests a different representation may be better\nthan comparing two MusicXML ﬁles directly.\nMore recently, evaluating OMR with MusicXML out-\nputs has been done by Padilla et al. [21]. While they pro-\nvide an implementation, there is no comparison against\ngold-standard data. (This is understandable, as the pa-\nper [21] is focused on recognition, not evaluation.) Align-\ning MusicXML ﬁles has also been explored by Knopke\nand Byrd [12] in a similar system-combination setting, al-\nthough not for the purposes of evaluation. They however\nmake an important observation: stems are often mistaken\nfor barlines, so the obvious simpliﬁcation of ﬁrst aligning\nmeasures is not straightforward to make.\nNo publicly available OMR dataset has ground truth\nfor end-to-end recognition. The CVC-MUSCIMA dataset\nfor stafﬂine identiﬁcation and removal and writer identi-\nﬁcation by Forn ´es et al. [9] is most extensive, with 1000\nhandwritten scores (50 musicians copying a shared set of\n20 scores) and a version with staves removed, which is\npromising for automatically applying ground truth anno-\ntations across the 50 versions of the same score. Forn ´es et\nal. [8] have also made available a dataset of 2128 clefs and\n1970 accidentals.\nThe HOMUS musical symbol collection for online\nrecognition [11] consists of 15200 samples (100 musi-\ncians, 32 symbol classes, 4-8 samples per class per mu-\nsician) of individual handwritten musical symbols. The\ndataset can be used for both online and ofﬂine symbol clas-\nsiﬁcation.\nA further dataset of 3222 handwritten and 2521 printed\nmusic symbols is available upon request [1]. Bellini et\nal. [18] use 7 selected images for their OMR assessment;\nunfortunately, they do not provide a clear description of\nthe database and its ground truth, and no more information\nis publicly available. Another stafﬂine removal dataset is\nDalitz’s database,5consisting of 32 music pages that cov-\n5http://music-staves.sourceforge.net158 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016ers a wide range of music types (CWMN, lute tablature,\nchant, mensural notation) and music fonts. Dalitz et al. [6]\ndeﬁne several types of distortion in order to test the ro-\nbustness of the different staff removal algorithms, simulat-\ning both image degradation and page deformations. These\nhave also been used to augment CVC-MUSCIMA.\nThere are also large sources such as the Mutopia\nproject6with transcriptions to LilyPond and KernScores7\nwith HumDrum. The IMSLP database8holds mostly\nprinted scores, but manuscripts as well; however, as op-\nposed to Mutopia and KernScores, IMSLP generally only\nprovides PDF ﬁles and no transcription of their musical\ncontent, except for some MIDI recordings.\n3. EV ALUATING EV ALUATION\nOMR lacks an automated evaluation metric that could\nguide development and reduce the price of conducting\nevaluations. However, an automated metric for OMR eval-\nuation needs itself to be evaluated: does it really rank as\nbetter systems that should be ranked better?\nAssuming that the judgment of (qualiﬁed) annotators is\nconsidered the gold standard, the following methodology\nthen can be used to assess an automated metric:\n1. Collect a corpus of annotator judgments to deﬁne the\nexpected gold-standard behavior,\n2. Measure the agreement between a proposed metric\nand this gold standard.\nThis approach is inspired by machine translation (MT),\na ﬁeld where comparing outputs is also notoriously difﬁ-\ncult: the WMT competition has an evaluation track [5,14],\nwhere automated MT metrics are evaluated against human-\ncollected evaluation results, and there is ongoing research\n[3, 15] to design a better metric than the current standards\nsuch as BLEU [17] or Meteor [13]. This methodology is\nnothing surprising; in principle, one could machine-learn\na metric given enough gold-standard data. However: how\nto best design the gold-standard data and collection proce-\ndure, so that it encompasses what we in the end want our\napplication (OMR) to do? How to measure the quality of\nsuch a corpus – given a collection of human judgments,\nhow much of a gold standard is it?\nIn this section, we describe a data collection scheme\nfor human judgments of OMR quality that should lead to\ncomparing automated metrics.\n3.1 Test case corpus\nWe collect a corpus Coftest cases . Each test case\nc1...cNis a triplet of music scores: an “ideal” score Ii\nand two “mangled” versions, P(1)\niandP(2)\ni, which we call\nsystem outputs . We asked our Kannotatorsa1...a Kto\nchoose the less mangled version, formalized as assigning\nra(ci) =−1if they preferred P(1)\nioverP(2)\ni, and +1for\nthe opposite preference. The term we use is to “rank” the\npredictions. When assessing an evaluation metric against\n6http://www.mutopiaproject.org\n7http://humdrum.ccarh.org\n8http://imslp.orgthis corpus, the test case rankings then constrain the space\nof well-behaved metrics.9\nThe exact formulation of the question follows the “cost-\nto-correct” model of evaluation of [18]:\n“Which of the two system outputs would take you less\neffort to change to the ideal score?”\n3.1.1 What is in the test case corpus?\nWe created 8 ideal scores and derived 34 “system outputs”\nfrom them by introducing a variety of mistakes in a nota-\ntion editor. Creating the system outputs manually instead\nof using OMR outputs has the obvious disadvantage that\nthe distribution of error types does not reﬂect the current\nOMR state-of-the-art. On the other hand, once OMR sys-\ntems change, the distribution of corpus errors becomes ob-\nsolete anyway. Also, we create errors for which we can\nassume the annotators have a reasonably accurate estimate\nof their own correction speed, as opposed to OMR outputs\nthat often contain strange and syntactically incorrect nota-\ntion, such as isolated stems. Nevertheless, when more an-\nnotation manpower becomes available, the corpus should\nbe extended with a set of actual OMR outputs.\nThe ideal scores (and thus the derived system outputs)\nrange from a single whole note to a “pianoform” fragment\nor a multi-staff example. The distortions were crafted to\ncover errors on individual notes (wrong pitch, extra acci-\ndental, key signature or clef error, etc.: micro-errors on\nthe semantic level in the sense of [16, 18]), systematic er-\nrors within the context of a full musical fragment (wrong\nbeaming, swapping slurs for ties, confusing staccato dots\nfor noteheads, etc.), short two-part examples to measure\nthe tradeoff between large-scale layout mistakes and lo-\ncalized mistakes (e.g., a four-bar two-part segment, as a\nperfect concatenation of the two parts into one vs. in two\nparts, but with wrong notes) and longer examples that con-\nstrain the metric to behave sensibly at larger scales.\nEach pair of system outputs derived from the same ideal\nscore forms a test case; there are 82 in total. We also in-\nclude 18 control examples, where one of the system out-\nputs is identical to the ideal score. A total of 15 annota-\ntors participated in the annotation, of whom 13 completed\nall 100 examples; however, as the annotations were volun-\ntary, only 2 completed the task twice for measuring intra-\nannotator agreement.\n3.1.2 Collection Strategy\nWhile Bellini et al. [18] deﬁne how to count individual\nerrors at the level of musical symbols, assign some cost\nto each kind of error (miss, add, fault, etc.) and deﬁne\nthe overall cost as composed of those individual costs, our\nmethodology does not assume that the same type of error\nhas the same cost in a different context , or that the overall\ncost can be computed from the individual costs: for in-\nstance, a sequence of notes shifted by one step can be in\n9We borrow the term “test case” from the software development prac-\ntice of unit testing: test cases verify that the program (in our case the\nevaluation metric) behaves as expected on a set of inputs chosen to cover\nvarious standard and corner cases.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 159most editors corrected simultaneously (so, e.g., clef errors\nmight not be too bad, because the entire part can be trans-\nposed together).\nTwo design decisions of the annotation task merit fur-\nther explanation: why we ask annotators to compare ex-\namples instead of rating difﬁculty, and why we disallow\nequality.\nRanking. The practice of ranking or picking the best\nfrom a set of possible examples is inspired by machine\ntranslation: Callison-Burch et al. have shown that peo-\nple are better able to agree on which proposed translation\nis better than on how good or bad individual translations\nare [4]. Furthermore, ranking does not require introducing\na cost metric in the ﬁrst place. Even a simple 1-2-3-4-5\nscale has this problem: how much effort is a “1” on that\nscale? How long should the scale be? What would the\nrelationship be between short and long examples?\nFurthermore, this annotation scheme is fast-paced. The\nannotators were able to do all the 100 available compar-\nisons within 1 hour. Rankings also make it straightforward\nto compare automated evaluation metrics that output val-\nues from different ranges: just count how often the met-\nric agrees with gold-standard ranks using some measure of\nmonotonicity, such as Spearman’s rank correlation coefﬁ-\ncient.\nNo equality. It is also not always clear which out-\nput would take less time to edit; some errors genuinely\nare equally bad (sharp vs. ﬂat). These are also impor-\ntant constraints on evaluation metrics: the costs associ-\nated with each should not be too different from each other.\nHowever, allowing annotators to explicitly mark equality\nrisks overuse, and annotators using underqualiﬁed judg-\nment. For this ﬁrst experiment, therefore, we elected not to\ngrant that option; we then interpret disagreement as a sign\nof uncertainty and annotator uncertainty as a symptom of\nthis genuine tie.\n3.2 How gold is the standard?\nAll annotators ranked the control cases correctly, except\nfor one instance. However, this only accounts for elemen-\ntary annotator failure and does not give us a better idea\nof systematic error present in the experimental setup. In\nother words, we want to ask the question: if all annota-\ntors are performing to the best of their ability, what level\nof uncertainty should be expected under the given an-\nnotation scheme? (For the following measurements, the\ncontrol cases have been excluded.)\nNormally, inter-annotator agreement is measured: if the\ntask is well-deﬁned, i.e., if a gold standard canexist, the\nannotators will tend to agree with each other towards that\nstandard. However, usual agreement metrics such as Co-\nhen’sκor Krippendorf’s αrequire computing expected\nagreement , which is difﬁcult when we do have a subset of\nexamples on which we do notexpect annotators to agree\nbut cannot a priori identify them. We therefore start by\ndeﬁning a simple agreement metric L. Recall:\n•Cstands for the corpus, which consists of Nexam-\nplesc1...cN,•Ais the set ofKannotatorsa1...a K,a,b∈A;\n•rais the ranking function of an annotator athat as-\nsigns +1 or -1 to each example in c,\nL(a,b) =1\nN/summationdisplay\nc∈C|ra(c) +rb(c)|\n2\nThis is simply the proportion of cases on which aandb\nagree: if they disagree, ra(c)+rb(c) = 0 . However, we ex-\npect the annotators to disagree on the genuinely uncertain\ncases, so some disagreements are not as serious as others.\nTo take the existence of legitimate disagreement into ac-\ncount, we modify L(a,b)to weigh the examples according\nto how certain the other annotators A\\{a,b}are about the\ngiven example. We deﬁne weighed agreement Lw(a,b):\nLw(a,b) =1\nN/summationdisplay\nc∈Cw(−a,b)(c)|ra(c) +rb(c)|\n2\nwherew(−a,b)is deﬁned for an example cas:\nw(−a,b)(c) =1\nK−2|/summationdisplay\na/prime∈A\\a,bra/prime(c)|\nThis way, it does not matter if aandbdisagree on cases\nwhere no one else agrees either, but if they disagree on an\nexample where there is strong consensus, it should bring\nthe overall agreement down. Note that while maximum\nachievableL(a,b)is 1 for perfectly agreeing annotators\n(i.e., all the sum terms equal to 1), because w(c)≤1,\nthe maximum achievable Lw(a,b)will be less than 1, and\nfurthermore depends on the choice of aandb: if we take\nnotoriously disagreeing annotators away from the picture,\nthe weights will increase overall. Therefore, we ﬁnally\nadjustLw(a,b)to the proportion of maximum achievable\nLw(a,b)for the given (a,b)pair, which is almost the same\nasLw(a,a)with the exception that bmust also be excluded\nfrom computing the weights . We denote this maximum as\nL∗\nw(a,b), and the adjusted metric ˆLwis then:\nˆLw(a,b) =Lw(a,b)/L∗\nw(a,b)\nThis metric says: “What proportion of achievable\nweighed agreement has been actually achieved?” The up-\nper bound of ˆLwis therefore 1.0again; the lower bound\nis agreement between two randomly generated annotators,\nwith the humans providing the consensus.\nThe resulting pairwise agreements, with the lower\nbound established by averaging over 10 random annota-\ntors, are visualized in Fig. 1. The baseline agreement\nˆLwbetween random annotators weighed by the full human\nconsensus was close to 0.5, as expected. There seems to\nbe one group of annotators relatively in agreement (green\nand above, which means adjusted agreement over 0.8), and\nthen several individuals who disagree with everyone – in-\ncluding among themselves (lines 6, 7, 8, 11, 12, 14).\nInterestingly, most of these “lone wolves” reported sig-\nniﬁcant experience with notation editors, while the group\nmore in agreement not as much. We suspect this is because160 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Figure 1 . Weighed pairwise agreement. The cell [a,b]rep-\nresents ˆLw(a,b). The scale goes from the average random\nagreement (ca. 0.55) up to 1.\nwith increasing notation editor experience, users develop a\npersonal editing style that makes certain actions easier than\nothers by learning a subset of the “tricks” available with the\ngiven editing tools – but each user learns a different sub-\nset, so agreement on the relative editing cost suffers. To the\ncontrary, inexperienced users might not have spent enough\ntime with the editor to develop these habits.\n3.3 Assessing some metrics\nWe illustrate how the test case ranking methodology helps\nanalyze these rather trivial automated MusicXML evalua-\ntion metrics:\n1. Levenshtein distance of XML canonization ( c14n ),\n2. Tree edit distance ( TED ),\n3. Tree edit distance with <note> ﬂattening ( TEDn ),\n4. Convert to LilyPond + Levenshtein distance ( Ly).\nc14n. Canonize the MusicXML ﬁle formatting and\nmeasure Levenshtein distance. This is used as a trivial\nbaseline.\nTED. Measure Tree Edit Distance on the MusicXML\nnodes. Some nodes that control auxiliary and MIDI infor-\nmation ( work ,defaults ,credit , and duration )\nare ignored. Replacement, insertion, and deletion all have\na cost of 1.\nTEDn. Tree Edit Distance with special handling of\nnote elements. We noticed that many errors of TED are\ndue to the fact that while deleting a note is easy in an\neditor, the edit distance is higher because the note ele-\nment has many sub-nodes. We therefore encode the notes\ninto strings consisting of one position per pitch ,stem ,\nvoice , and type . Deletion cost is ﬁxed at 1, insertion\ncost is 1 for non-note nodes, and 1 + length of code for\nnotes. Replacement cost between notes is the edit distance\nbetween their codes; replacement between a note and non-\nnote costs 1 + length of code; between non-notes costs 1.Metricrs ˆrsρ ˆρ τ ˆτ\nc14n 0.33 0.41 0.40 0.49 0.25 0.36\nTED 0.46 0.58 0.40 0.50 0.35 0.51\nTEDn 0.57 0.70 0.40 0.49 0.43 0.63\nLy 0.41 0.51 0.29 0.36 0.30 0.44\nTable 1 . Measures of agreement for some proposed evalu-\nation metrics.\nLy.The LilyPond10ﬁle format is another possible rep-\nresentation of a musical score. It encodes music scores\nin its own LaTeX-like language. The ﬁrst bar of the\n“Twinkle, twinkle” melody would be represented as d’8[\nd’8] a’8[ a’8] b’8[ b’8] a’4 | This repre-\nsentation is much more amenable to string edit distance.\nTheLymetric is Levenshtein distance on the LilyPond im-\nport of the MusicXML system output ﬁles, with all whites-\npace normalized.\nFor comparing the metrics against our gold-standard\ndata, we use nonparametric approaches such as Spear-\nman’srsand Kendall’s τ, as these evaluate monotonicity\nwithout assuming anything about mapping values of the\nevaluation metric to the [−1,1]range of preferences . To\nreﬂect the “small-difference-for-uncertain-cases” require-\nment, however, we use Pearson’s ρas well [14]. For each\nway of assessing a metric, its maximum achievable with\nthe given data should be also estimated, by computing how\nthe metric evaluates the consensus of one group of anno-\ntators against another. We randomly choose 100 splits of\n8 vs 7 annotators, compute the average preferences for the\ntwo groups in a split and measure the correlations between\nthe average preferences. The expected upper bounds and\nstandard deviations estimated this way are:\n•r∗\ns= 0.814, with standard dev. 0.040\n•ρ∗= 0.816, with standard dev. 0.040\n•τ∗= 0.69, with standard dev. 0.045\nWe then deﬁne ˆrsasrs\nr∗s, etc. Given a cost metric L, we\nget for each example ci= (Ii,P(1)\ni,P(2)\ni)the cost differ-\nence/lscript(ci) =L(Ii,P(1)\ni)−L(Ii,P(2)\ni)and pair it with the\ngold-standard consensus r(ci)to get pairwise inputs for\nthe agreement metrics.\nThe agreement of the individual metrics is summarized\nin Table 1. When developing the metrics, we did notuse\nthe gold-standard data against which metric performance is\nmeasured here; we used only our own intuition about how\nthe test cases should come out.\n4. BENCHMARK DATASET PROTOTYPE\nA benchmark dataset should have ground truth at levels\ncorresponding to the standard OMR processing stages, so\nthat sub-systems such as staff removal, or symbol local-\nization can be compared with respect to the end-to-end\npipeline they are a part of. We also suspect handwrit-\nten music will remain an open problem much longer than\nprinted music. Therefore, we chose to extend the CVC-\nMUSCIMA dataset instead of Byrd and Simonsen’s pro-\n10http://www.lilypond.orgProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 161posed test bed [7] because of the extensive handwritten\ndata collection effort that has been completed by Forn ´es\net al. and because ground truth for staff removal and bi-\nnarization is already present. At the same time, CVC-\nMUSCIMA covers all the levels of notational complexity\nfrom [7], as well as a variety of notation symbols, includ-\ning complex tuples, less common time signatures (5/4), C-\nclefs and some symbols that could very well expose the dif-\nferences between purely symbol-based and more syntax-\naware methods (e.g., tremolo marks, easily confused for\nbeams). We have currently annotated symbols in printed\nscores only, with the perspective of annotating the hand-\nwritten scores automatically or semi-automatically.\nWe selected a subset of scores that covers the various\nlevels of notational complexity: single-part monophonic\nmusic (F01), multi-part monophonic music (F03, F16),\nand pianoform music, primarily based on chords (F10) and\npolyphony (F08), with interaction between staves.\n4.1 Symbol-level ground truth\nSymbols are represented as bounding boxes, labeled by\nsymbol class. In line with the low-level and high-level\nsymbols discussed by [7], we differentiate symbols at the\nlevel of primitives and the level of signs . The relation-\nship between primitives and signs can be one-to-one (e.g.,\nclefs), many-to-one (composite signs: e.g. notehead, stem,\nand ﬂag form a note), one-to-many (disambiguation: e.g.,\na sharp primitive can be part of a key signature, acciden-\ntal, or an ornament accidental), and many-to-many (the\nsame beam participates in multiple beamed notes, but each\nbeamed note also has a stem and notehead). We include\nindividual numerals and letters as notation primitives, and\ntheir disambiguation (tuplet, time signature, dynamics...)\nas signs.\nWe currently deﬁne 52 primitives plus letters and nu-\nmerals, and 53 signs. Each symbol can be linked to a Mu-\nsicXML counterpart.11There are several groups of sym-\nbols:\n•Note elements (noteheads, stems, beams, rests...)\n•Notation elements (slurs, dots, ornaments...)\n•Part default (clefs, time and key signatures...)\n•Layout elements (staves, brackets, braces...)\n•Numerals and text.\nWe have so far annotated the primitive level. There are\n3191 primitives marked in the 5 scores. Annotation took\nabout 24 hours of work in a custom editor.\n4.2 End-to-end ground truth\nWe use MusicXML as the target representation, as it is\nsupported by most OMR/notation software, actively main-\ntained and developed and available under a sufﬁciently per-\nmissive license. We obtain the MusicXML data by manu-\nally transcribing the music and postprocessing to ensure\neach symbol has a MusicXML equivalent. Postprocessing\nmostly consists of ﬁlling in default barlines and correcting\n11The full lists of symbol classes are available in the repos-\nitory at https://github.com/ufal/omreval under\nmuscima++/data/Symbolic/specification .staff grouping information. Using the MuseScore notation\neditor, transcription took about 3.5 hours.\n5. CONCLUSIONS AND FUTURE WORK\nWe proposed a corpus-based approach to assessing auto-\nmated end-to-end OMR evaluation metrics and illustrated\nthe methodology on several potential metrics. A gold\nstandard annotation scheme based on assessment of rela-\ntive cost-to-correct of synthetic “system outputs” was de-\nscribed that avoids pre-deﬁning any cost metric, and the re-\nsulting corpus of 1230 human judgments was analyzed for\ninter-annotator agreement, taking into account the possi-\nbility that the compared system outputs may not be clearly\ncomparable. This preference-based setup avoids the need\nto pre-deﬁne any notion of cost, requires little annotator\ntraining, and it is straightforward to assess an evaluation\nmetric against this preference data.\nOur results suggest that the central assumption of a sin-\ngle ground truth for preferences among a set of system out-\nputs is weaker with increasing annotator experience. To\nmake the methodology more robust, we recommend:\n•Explicitly control for experience level; do not as-\nsume that more annotator experience is better.\n•Measure actual cost-to-correct (in time and interface\noperations) through a notation editor, to verify how\nmuch human estimation of this cost can be relied on.\n•Develop models for computing expected agreement\nfor data where the annotations may legitimately be\nrandomized (the “equally bad” cases). Once ex-\npected agreement can be computed, we can use more\nstandard agreement metrics.\nThe usefulness of the test case corpus for developing\nautomated evaluation metrics was clear: the TEDn met-\nric that outperformed the others by a large margin was de-\nveloped through analyzing the shortcomings of the TED\nmetric on individual test cases (before the gold-standard\ndata had been collected). As Szwoch [20] suggested, mod-\nifying the representation helped. However, if enough hu-\nman judgments are collected, it should even be possible\nto sidestep the difﬁculties of hand-crafting an evaluation\nmetric through machine learning; we can for instance try\nlearning the insertion, deletion, and replacement costs for\nindividual MusicXML node types.\nAn OMR environment where different systems can be\nmeaningfully compared, claims of commercial vendors are\nveriﬁable and progress can be measured is in the best inter-\nest of the OMR community. We believe our work, both on\nevaluation and on a dataset, constitutes a signiﬁcant step in\nthis direction.\n6. ACKNOWLEDGMENTS\nThis research is supported by the Czech Science Founda-\ntion, grant number P103/12/G084. We would also like to\nthank our annotators from the Jan ´aˇcek Academy of Mu-\nsic and Performing Arts in Brno and elsewhere, and Alicia\nForn ´es for providing additional background and material\nfor the CVC-MUSCIMA dataset.162 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20167. REFERENCES\n[1] Ana Rebelo, Ichiro Fujinaga, Filipe Paszkiewicz, An-\ndre R. S. Marcal, Carlos Guedes, and Jaime S. Car-\ndoso. Optical Music Recognition: State-of-the-Art and\nOpen Issues. Int J Multimed Info Retr , 1(3):173–190,\nMar 2012.\n[2] Baoguang Shi, Xiang Bai, and Cong Yao. An End-\nto-End Trainable Neural Network for Image-based Se-\nquence Recognition and Its Application to Scene Text\nRecognition. CoRR , abs/1507.05717, 2015.\n[3] Ond ˇrej Bojar, Milo ˇs Ercegov ˇcevi´c, Martin Popel, and\nOmar F. Zaidan. A Grain of Salt for the WMT Man-\nual Evaluation. Proceedings of the Sixth Workshop on\nStatistical Machine Translation , pages 1–11, 2011.\n[4] Chris Callison Burch, Cameron Fordyce, Philipp\nKoehn, Christof Monz, and Josh Schroeder. (Meta-)\nEvaluation of Machine Translation. Proceedings of the\nSecond Workshop on Statistical Machine Translation ,\npages 136–158, 2007.\n[5] Chris Callison Burch, Philipp Koehn, Christof Monz,\nKay Peterson, Mark Przybocki, and Omar F. Zaidan.\nFindings of the 2010 Joint Workshop on Statistical Ma-\nchine Translation and Metrics for Machine Translation.\nProceedings of the Joint Fifth Workshop on Statistical\nMachine Translation and MetricsMATR , pages 17–53,\n2010.\n[6] Christoph Dalitz, Michael Droettboom, Bastian Pran-\nzas, and Ichiro Fujinaga. A Comparative Study of\nStaff Removal Algorithms. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence , 30(5):753–\n766, May 2008.\n[7] Donald Byrd and Jakob Grue Simonsen. Towards a\nStandard Testbed for Optical Music Recognition: Deﬁ-\nnitions, Metrics, and Page Images. Journal of New Mu-\nsic Research , 44(3):169–195, 2015.\n[8] Alicia Forn ´es, Josep Llad ´os, Gemma S ´anchez, and\nHorst Bunke. Writer Identiﬁcation in Old Handwritten\nMusic Scores. Proceeedings of Eighth IAPR Interna-\ntional Workshop on Document Analysis Systems , pages\n347–353, 2008.\n[9] Alicia Forn ´es, Anjan Dutta, Albert Gordo, and Josep\nLlad ´os. CVC-MUSCIMA: a ground truth of hand-\nwritten music score images for writer identiﬁcation\nand staff removal. International Journal on Docu-\nment Analysis and Recognition (IJDAR) , 15(3):243–\n251, 2012.\n[10] Graham Jones, Bee Ong, Ivan Bruno, and Kia Ng. Op-\ntical Music Imaging: Music Document Digitisation,\nRecognition, Evaluation, and Restoration. Interactive\nMultimedia Music Technologies , pages 50–79, 2008.\n[11] Jorge Calvo-Zaragoza and Jose Oncina. Recognition\nof Pen-Based Music Notation: The HOMUS Dataset.22nd International Conference on Pattern Recognition ,\nAug 2014.\n[12] Ian Knopke and Donald Byrd. Towards Musicdiff : A\nFoundation for Improved Optical Music Recognition\nUsing Multiple Recognizers. International Society for\nMusic Information Retrieval Conference , 2007.\n[13] Alon Lavie and Abhaya Agarwal. Meteor: An Au-\ntomatic Metric for MT Evaluation with High Levels\nof Correlation with Human Judgments. Proceedings of\nthe Second Workshop on Statistical Machine Transla-\ntion, pages 228–231, 2007.\n[14] Matou ˇs Mach ´aˇcek and Ond ˇrej Bojar. Results of the\nWMT14 Metrics Shared Task. Proceedings of the\nNinth Workshop on Statistical Machine Translation ,\npages 293–301, 2014.\n[15] Matou ˇs Mach ´aˇcek and Ond ˇrej Bojar. Evaluating Ma-\nchine Translation Quality Using Short Segments An-\nnotations. The Prague Bulletin of Mathematical Lin-\nguistics , 103(1), Jan 2015.\n[16] Michael Droettboom and Ichiro Fujinaga. Microlevel\ngroundtruthing environment for OMR. Proceedings of\nthe 5th International Conference on Music Information\nRetrieval (ISMIR 2004) , pages 497–500, 2004.\n[17] Kishore Papineni, Salim Roukos, Todd Ward, and\nWei-Jing Zhu. BLEU: A Method for Automatic Evalu-\nation of Machine Translation. Proceedings of the 40th\nAnnual Meeting on Association for Computational Lin-\nguistics , pages 311–318, 2002.\n[18] Pierfrancesco Bellini, Ivan Bruno, and Paolo Nesi. As-\nsessing Optical Music Recognition Tools. Computer\nMusic Journal , 31(1):68–93, Mar 2007.\n[19] Craig Sapp. OMR Comparison of SmartScore and\nSharpEye. https://ccrma.stanford.edu/\n˜craig/mro-compare-beethoven , 2013.\n[20] Mariusz Szwoch. Using MusicXML to Evaluate Accu-\nracy of OMR Systems. Proceedings of the 5th Inter-\nnational Conference on Diagrammatic Representation\nand Inference , pages 419–422, 2008.\n[21] Victor Padilla, Alan Marsden, Alex McLean, and Kia\nNg. Improving OMR for Digital Music Libraries with\nMultiple Recognisers and Multiple Sources. Proceed-\nings of the 1st International Workshop on Digital Li-\nbraries for Musicology - DLfM ’14 , pages 1–8, 2014.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 163"
    },
    {
        "title": "Improving Predictions of Derived Viewpoints in Multiple Viewpoints Systems.",
        "author": [
            "Thomas Hedges",
            "Geraint A. Wiggins"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416630",
        "url": "https://doi.org/10.5281/zenodo.1416630",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/208_Paper.pdf",
        "abstract": "This paper presents and tests a method for improving the predictive power of derived viewpoints in multiple view- points systems. Multiple viewpoint systems are a well es- tablished method for the statistical modelling of sequen- tial symbolic musical data. A useful class of viewpoints known as derived viewpoints map symbols from a basic event space to a viewpoint-specific domain. Probability es- timates are calculated in the derived viewpoint domain be- fore an inverse function maps back to the basic event space to complete the model. Since an element in the derived viewpoint domain can potentially map onto multiple basic elements, probability mass is distributed between the ba- sic elements with a uniform distribution. As an alternative, this paper proposes a distribution weighted by zero-order frequencies of the basic elements to inform this probability mapping. Results show this improves the predictive perfor- mance for certain derived viewpoints, allowing them to be selected in viewpoint selection.",
        "zenodo_id": 1416630,
        "dblp_key": "conf/ismir/HedgesW16",
        "content": "IMPROVING PREDICTIONS OF DERIVED VIEWPOINTS IN MULTIPLE\nVIEWPOINT SYSTEMS\nThomas Hedges\nQueen Mary University of London\nt.w.hedges@qmul.ac.ukGeraint Wiggins\nQueen Mary University of London\ngeraint.wiggins@qmul.ac.uk\nABSTRACT\nThis paper presents and tests a method for improving the\npredictive power of derived viewpoints in multiple view-\npoints systems. Multiple viewpoint systems are a well es-\ntablished method for the statistical modelling of sequen-\ntial symbolic musical data. A useful class of viewpoints\nknown as derived viewpoints map symbols from a basic\nevent space to a viewpoint-speciﬁc domain. Probability es-\ntimates are calculated in the derived viewpoint domain be-\nfore an inverse function maps back to the basic event space\nto complete the model. Since an element in the derived\nviewpoint domain can potentially map onto multiple basic\nelements, probability mass is distributed between the ba-\nsic elements with a uniform distribution. As an alternative,\nthis paper proposes a distribution weighted by zero-order\nfrequencies of the basic elements to inform this probability\nmapping. Results show this improves the predictive perfor-\nmance for certain derived viewpoints, allowing them to be\nselected in viewpoint selection.\n1. INTRODUCTION\nMultiple viewpoint systems [7] are an established statis-\ntical learning approach to modelling multidimensional se-\nquences of symbolic musical data. Music is presented as a\nseries of events comprising of basic attributes (e.g. pitch,\nduration) modelled by a collection of viewpoints . For ex-\nample, pitch may be modelled by pitch interval, pitch class,\nor even pitch itself. Statistical structure for each view-\npoint is captured with a Markovian approach, usually in\nthe form of a Prediction by Partial Match (PPM) [2] suf-\nﬁx tree. Predictions from different viewpoints modelling\nthe same basic attribute are combined, weighting towards\nviewpoints with lower uncertainty in terms of Shannon en-\ntropy [24]. The system can be viewed as a mixture of\nexperts, or ensemble method machine learning approach\nto symbolic music, dynamically using specialised models\nwhich are able to generalise data in order to ﬁnd structure.\nThe current research explores a problem associated with\na collection of viewpoints known as derived viewpoints .\nDerived viewpoints apply some function to basic attributes\nc⃝Thomas Hedges, Geraint Wiggins. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Thomas Hedges, Geraint Wiggins. “Improving Predic-\ntions of Derived Viewpoints in Multiple Viewpoint Systems”, 17th Inter-\nnational Society for Music Information Retrieval Conference, 2016.aiming to capture some relational structure between basic\nattributes (e.g. pitch interval), or to generalise sparse data\n(e.g. pitch class). During training, elements from the basic\nattribute domain are mapped onto the derived viewpoint\ndomain with a surjective function. Viewpoint models must\nbe combined over a shared alphabet in order to calculate\nprobability estimates, therefore, an inverse function maps\nfrom the derived viewpoint domain to the basic attribute\ndomain. Where a derived element maps onto several basic\nelements, probability mass from the derived element is dis-\ntributed uniformly between the basic elements [17]. This\ncan be problematic for derived viewpoints with small do-\nmains mapping onto large basic attribute domains as the\nderived elements could refer to many basic elements. Such\nviewpoints may generalise sparse data and ﬁnd useful sta-\ntistical structure, but this information is lost when mapping\nback to the basic attribute domain. This is especially preva-\nlent where the zero-order (or unigram) distribution of the\nbasic attribute domain is of low entropy, such that a few el-\nements are very frequent and the rest relatively infrequent.\nThis paper proposes a method for improving predictions\nfrom derived viewpoints. The basic premise behind the\nmethod is to use the zero-order distribution of the basic\nattribute to weight the probabilities from the derived view-\npoint when mapping back to the basic attribute. This en-\nables the derived viewpoint to take advantage of the zero\norder statistics of basic attributes in a way which is not\npossible if the basic and derived viewpoints are modelled\nseparately. After a review of research using multiple view-\npoint systems (Section 2), the system used in the current\npaper is presented (Section 3), and a detailed description\nof the proposed method given (Section 4). The method is\ntested on individual derived viewpoints (Section 5.1) be-\nfore being applied to various full multiple viewpoint sys-\ntems, including viewpoint selection (Section 5.2).\n2. RELATED RESEARCH\nMultiple viewpoint systems have become an important tool\nfor statistical learning of music since their inception over\ntwenty-ﬁve years ago [3]. This section reviews their uses\nand applications to both musical and non-musical domains.\nEarly multiple viewpoint systems [3, 7, 16] focussed on\nmonophonic melodic music, namely chorale and folksong\nmelodies. The seminal paper [7] uses hand-constructed\nmultiple viewpoint systems with a corpus of 100 Bach\nchorales. Results show that a system of four viewpoints420capturing pitch, sequential pitch interval, scale degree, du-\nrational, and metrical information performs best. The sys-\ntem can be used as a generative tool, using a random walk\nprocess to generate a chorale in the style of the train-\ning corpus. Further work with monophonic melodic mu-\nsic can be seen with the Information Dynamics of Music\n(IDyOM) model [16], which is developed as a cognitive\nmodel of melodic expectation. The PPM* algorithm is re-\nﬁned [20] with a thorough evaluation of smoothing meth-\nods, as well as the methods for combining predictions from\nvarious individual models, and the method for construct-\ning viewpoint systems [17]. IDyOM is found to closely\ncorrelate with experimental data of melodic expectation\nfrom human participants, accounting for 78% of variance\nwhen predicting notes in English hymns [19], and 83% of\nvariance for British folksongs [21]. Multiple viewpoint\nsystems have also been applied successfully to Northern\nIndian raags [25], Turkish folk music [23], and Greek\nfolk tunes [6], strengthening their position as a general,\ndomain-independent statistical learning model for music.\nMultiple viewpoint systems can be applied to poly-\nphonic musical data, modelling some of the harmonic as-\npects of music. Musical data with multiple voices is di-\nvided into vertical slices [4] representing collections of\nsimultaneous notes, i.e. chords. Relationships between\nvoices can be captured with the use of linked viewpoints\nbetween voices. This approach has been utilised exten-\nsively for the harmonisation of four-part chorales [27, 28].\nHarmonic structure can also be modelled directly from\nchord symbols [5, 10, 22], removing the problems of spar-\nsity and equivalence associated with chord voicing.\nStrong probabilistic models of expectation for sequen-\ntial data can be used for segmentation and chunking.\nIDyOM is compared to rule-based models for boundary\ndetection in monophonic melodic music in [18], with the\nstatistical model performing comparably rule-based sys-\ntems. Similar methods have been applied to segmenting\nnatural language at the phoneme and morpheme level [9].\nThese segmentation studies utilise the fact that certain in-\nformation theoretic properties, namely information con-\ntent, can be used to predict boundaries in sequences. The\nability for multiple viewpoint systems to model the infor-\nmation theoretic properties of sequences, as well as their\ngeneral approach to statistical learning, makes them an at-\ntractive basis for cognitive architectures capable of general\nlearning, ﬁnding higher order structure, and computational\ncreativity [29].\n3. A MULTIPLE VIEWPOINT SYSTEM FOR\nCHORD SEQUENCES\nThis section presents a brief technical description of the\nmultiple viewpoint system and corpus used in the current\nresearch. The corpus consists of 348 chord sequences from\njazz standards in lead sheet format from The Real Book\n[11] compiled by [15]. This gives a suitably large corpus\nof 15,197 chord events, represented as chord symbols (e.g.\nDm7,Bdim ,G7).The Real Book is core jazz repertoire\ncomprising of a range of composers and styles, indicatingit is a good candidate for studying tonal jazz harmony. The\nviewpoint pool is derived from similar multiple viewpoint\nsystems dealing with chord symbol sequences [5, 10].\n3.1 Harmonic Viewpoints\nThree basic attributes, Root ,ChordType , and\nPosInBar , are used to represent chord labels. Root is\nthe functional root of the chord as a pitch class assuming\nenharmonic equivalence. ChordType represents the\nquality of the chord (e.g. major, minor seventh) and are\nsimpliﬁed to a set of 13 ( 7, M, m7, m, 6, m6, halfdim,\ndim, aug, sus, alt, no3rd, NC ) for practical reasons.1NC\nrepresents the special case where no harmonic instruments\nare instructed to play in the score. PosInBar represents\nthe metrical position in the current bar measured in\nquavers. Since, by deﬁnition, a chord must be stated at\nthe start of each bar, this is a sufﬁcient basic attribute to\nrepresent any durational or temporal information in the\nchord sequence.\nThe following viewpoints are derived from Root .\nRootInt is the root interval in semitones modulo-12\nbetween two adjacent chords, returning the symbol -1\nif either is NC.MeeusInt categorises root move-\nment ( RootInt ) using root progression theories [14].\nThe symbol 1 represents dominant root progressions\n(RootInt = 1,2,5,8,9), -1 for subdominant progres-\nsions ( RootInt = 3,4,7,10,11), 0 for no root movement\n(RootInt = 0), -2 for a diminished ﬁfth ( RootInt\n= 6), and -3 when either root is NC. Since tonal\nharmony progresses predominantly in perfect ﬁfths, the\nChromaDist viewpoint simply represents the mini-\nmum number of perfect ﬁfths required to get from one\nroot to the next, or the smallest distance around a cy-\ncle of ﬁfths, with -1 representing the NC case. All\nof these viewpoints return the undeﬁned symbol, ⊥,\nfor the ﬁrst event of a piece when the previous event\ndoes not exist. RootIntFiP, MeeusIntFiP , and\nChromaDistFiP , apply RootInt, MeeusInt and\nChromaDist to the current event and the ﬁrst event\nof the piece instead of the previous event. Finally, a\nthreaded viewpoint (see [7]), RootInt ⊖FiB, measures\nRootInt between chords on the ﬁrst beats of successive\nbars.\nThree viewpoints are derived from ChordType , al-\nlowing chord types to be categorised in a number of ways.\nMajType assigns a 1 to all chords where the third is ma-\njor, a 2 to all chords where the third is minor and a 0 to\nall chords without a third. 7Type assigns a 1 to all chords\nwith a minor 7th, and a 0 to all other chords, (except a\nNCwhich is given a -1 symbol.) FunctionType as-\nsigns all chords with a major third and minor seventh a 0\n(dominant chords), all other chords with a major third a 1\n(major tonics), all chords with a minor third and minor sev-\nenth a 2 (pre-dominant), all other minor chords a 3 (minor\ntonic), and NCa -1. Table 1 summarises all of the har-\nmonic viewpoints presented in this section over a sample\nchord sequence.\n1See [10] for a detailed explanation of chord type simpliﬁcation.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 421Bm7D7NC GM7\nRoot 11 2 -1 7\nChordType min 7 NC maj\nPosInBar 0 0 2 0\nRootInt ⊥ 3 -1 -1\nMeeusInt ⊥ -1 -3 -3\nChromaDist ⊥ 3 -1 -1\nRootIntFiP ⊥ 3 -1 8\nMeeusIntFiP ⊥ -1 -3 1\nChromaDistFiP ⊥ 3 -1 4\nRootInt ⊖FiB ⊥ 3 ⊥ 5\nMajType 0 1 -1 1\n7Type 1 1 -1 0\nFunctionType 2 0 -1 1\nTable 1 . Sample chord sequence with basic and derived\nviewpoints.\n3.2 System Description\nA fully detailed model description is beyond the scope\nof this paper, however, broadly the system follows the\nIDyOM model [16], branching from the publicly available\nLISP implementation [1]. The system estimates probabil-\nities of sequences of events in a basic event space ξwith\nviewpoints, τ, operating over sequences formed from el-\nements of a viewpoint alphabet [τ]. Formally, a view-\npoint modelling a type τcomprises of a partial function\nΨτ:ξ∗⇀[τ], atype set ⟨τ⟩specifying the basic types\nthe viewpoint is capable of predicting, and a PPM* model\ntrained from sequences in [τ]. In order to make predictions\nover the basic event space ξ, symbols are converted back\nfrom [τ]with the inverse function Ψ′:ξ∗×[τ]→2[τb]\nwhere τbis the basic type associated by τ. This many-\nto-one mapping means that a single derived sequence can\nrepresent multiple basic event sequences.\nLong-term (LTM) and short-term (STM) models [7] are\nused to capture both the general trends of the style mod-\nelled and the internal statistical structure of the piece being\nprocessed. An LTM consists of the full training set, whilst\nthe STM is built incrementally from the current piece and\nis discarded after it has been processed. Predictions from\nall viewpoints within the LTM/STM are combined ﬁrst,\nbefore combining the LTM and STM predictions. Predic-\ntion combination is achieved with a weighted geometric\nmean [17], favouring the least uncertain models accord-\ning to their Shannon entropy.2Various smoothing meth-\nods are employed, allowing novel symbols to be predicted\nand predictions from different length contexts to be com-\nbined in a meaningful way without assuming a ﬁxed order\nbound [20].\nMultiple viewpoint systems are assessed quantitatively\nwith methods from information theory [13]. The main per-\nformance measure is mean information content ¯h, repre-\nsenting the number of bits required on average to represent\neach symbol in the sequence of length J(1).\n2For reference, all model combinations in this paper are achieved with\nan LTM-STM bias of 7 and a viewpoint bias of 2 see [17] for details.¯h(\neJ\n1)\n=−1\nJJ∑\ni=1log2p(\nei|ei−1\ni−n+1)\n(1)\n4. USING ZERO-ORDER STATISTICS TO\nWEIGHT Ψ′\nThe focus of this paper is to improve predictions from de-\nrived viewpoints by weighting probabilities after the in-\nverse mapping function Ψ′has been applied. Firstly, it is\nuseful to show in detail cases where certain derived view-\npoints would be poor predictors for a basic attribute.\nWhere a derived viewpoint maps an element onto a\nlarge number of basic elements, a certain amount of infor-\nmation is lost by dividing the probability mass uniformly.\nSuppose a prediction from MajType returns a high prob-\nability for a major chord, mapping onto a ‘7’,‘M7’ ,‘6’,\n‘alt’ or‘aug’ ChordType .‘7’and‘M7’ chords are very\ncommon, whilst ‘alt’ and‘aug’ chords are comparatively\nrare. Since MajType must distribute probability mass\nequally to all ﬁve of these basic elements, a considerable\namount of information is lost and it remains a poor predic-\ntor of ChordType . The predictive strength of these kinds\nof viewpoints are to generalise data which will become\nsparse, speciﬁcally in sequence prediction when match-\ning contexts in the PPM* model. This strength is likely to\nbe reduced by the uniform distribution of probability mass\nand could make these viewpoints poor predictors; return-\ning high mean information content estimates and remain-\ning unselected in viewpoint selection.\nA general approach to counter this loss of information is\nto weight probabilities with the zero-order (unigram) fre-\nquencies when distributing probability mass from a derived\nelement to the relevant basic elements. For reference, (2)\nshows a probability estimate of a basic element, p(tτb),\ncalculated by uniformly distributing the probability mass\nof a derived element, p(tτ), following [17]. Brepresents\nthe set of basic elements that are mapped onto from the\nderived element tτ. The proposed alternative, shown in\n(3), uses probabilities from the zero-order model p0(tτb)\nto weight the distribution of probability mass from tτto\ntτb. As with PPM* predictions, probability mass must be\nreserved for unseen symbols in the basic element alpha-\nbet, so a smoothing method and −1thorder distribution is\nutilised. Using an established smoothing framework [20],\n(4) shows an interpolated smoothing method with escape\nmethod C, an order bound of 0 and with no update exclu-\nsion. c(tτb)is the number of times the symbol tτboccurs\nthe training set, Jis the length of the training set, [τb]is\nthe alphabet of the basic viewpoint, and [τb]sthe observed\nalphabet of the basic viewpoint.\np(tτb) =p(tτ)\n|B|(2)\npw(tτb) =p(tτ)p0(tτb)∑\ni∈Bp0(i)(3)422 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016p0(tτb) =c(tτb)\nJ+|[τb]s|+...\n|[τb]s|\nJ+|[τb]s|·1\n|[τb]|+ 1− |[τb]s|(4)\nA demonstration of this process is shown in Figure 1.\nFunctionType is used to predict the next ChordType\nsymbol with an LTM model given the context Am7, D7,\nBm7, Bbm7 . The top chart shows a strong expectation\nof a pre-dominant chord which could map onto a m7,\nhalfdim , ordimChordType . With an unweighted Ψ′(2)\nfrom FunctionType toChordType , these three ba-\nsic elements are all given equal probability (middle chart).\nHowever, since m7is far more common than halfdim and\ndim, a more accurate probability distribution could be one\nweighted (3) by the zero-order frequencies (bottom chart),\nassigning a high probability to m7. This approach allows\nthe powerful generalisation of derived viewpoint models\nto be combined efﬁciently with more speciﬁc predictions\nfrom the basic viewpoint.\n00.10.20.30.40.5\ndom.maj. tonicpre-dom.min. tonicNC\n00.10.20.30.4\n7alt.sus.no3rdM6aug.m7hdim.dim.mm6NC\n00.10.20.30.4\n7alt.sus.no3rdM6aug.m7hdim.dim.mm6NC\nFigure 1 . Top: probability distribution of\nFunctionType following the context Am7, D7,\nBm7, Bbm7 . Middle and bottom: probability distributions\nforChordType predicted by FunctionType with\nan unweighted (middle) and zero-order weighted Ψ′\n(bottom).5. TESTING THE IMPACT OF WEIGHTING Ψ′\nTo investigate the effect of weighting Ψ′\nτwith a zero or-\nder model, the mean information content, ¯h(1), is used\nas a performance metric to compare predictions with the\nweighted and unweighted inverse mapping function. In\nall cases, ¯his calculated with a 10-fold cross-validation\nof the corpus. The effect of the weighting on individ-\nual derived viewpoints is observed ﬁrst (Section 5.1) be-\nfore comparing the impact on full multiple viewpoint sys-\ntems (Section 5.2). The STM is an unbounded interpolated\nsmoothing model with escape method D using update ex-\nclusion, and the LTM an unbounded interpolated smooth-\ning model with escape method C without update exclu-\nsion [20]. These parameters have been found to be optimal\nfor the current corpus [10].\nFor the individual viewpoints, it is expected that de-\nrived viewpoints which abstract heavily from their basic\nviewpoint will beneﬁt most from weighting Ψ′. Typically,\nthese are viewpoints derived from ChordType , for ex-\nample, MajType reduces the alphabet of ChordType\nfrom 13 down to 3. By contrast, it is expected that the\nimpact of weighting Ψ′will be far smaller for derived\nviewpoints with a close to one-to-one mapping between\nalphabets (e.g. RootInt ), if signiﬁcant at all. When con-\nstructing a full multiple viewpoint system it is hoped that\nweighting Ψ′will help more derived viewpoints to be se-\nlected over basic viewpoints. Not only should this give a\nlower mean information content, but also produce a more\ncompact viewpoint model. Successful derived viewpoints\nshould abstract information away from basic viewpoints\nonto smaller alphabets without a loss in performance.\n5.1 Individual Viewpoints Results\nSix derived viewpoints for predicting Root and\nChordType are chosen for testing, as well as the\nbasic viewpoints themselves for reference. Table 2\nshows the mean information content calculated using\nboth weighted and unweighted Ψ′functions. Effect\nsize measured by Cohen’s d=¯h1−¯h2\nσpooledacross all pieces\n(n= 348) is used to quantify the relative performance for\neach viewpoint. A one-sided paired t-test across pieces\nassesses statistical signiﬁcance between the means at the\np < . 001level, marked with a *.\nStrikingly, the derived viewpoints predicting\nChordType beneﬁt most from the weighting method,\nall with effect sizes greater than 1.7 and an absolute\nimprovement of around 0.9 bit/symbol. By contrast,\nthe impact of the weighting on the viewpoints derived\nfrom Root is small and inconsistent, with effect sizes of\naround 0.1 or less. Indeed, weighting Ψ′has a marginally\nnegative impact on RootInt , although only by 0.016\nbits/symbol. It is likely that this is because in the majority\nof cases RootInt has a one-to-one mapping with Root ,\nexcept for the NCcase where a RootInt symbol of -1\nmaps onto the full alphabet of Root . It is interesting\nto note that none of the individual derived viewpoints\nare able to predict their basic viewpoint better than theProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 423Derived\nViewpointUnweighted\nΨ′Weighted\nΨ′d\nChordType 1.807 1.807 .000\nMajType 3.270 2.315 1.977*\n7Type 3.249 2.371 1.766*\nFunctionType 3.060 2.080 1.731*\nRoot 2.259 2.259 .000\nRootInt 2.297 2.313 -.030\nMeeusInt 3.152 3.076 .129*\nChromaDist 2.688 2.681 .009\nTable 2 . Predicting ChordType (top) and Root (bot-\ntom) with weighted and unweighted Ψ′. Performance dif-\nference is measured by Cohen’s d=¯h1−¯h2\nσpooled. * marks dif-\nferences which are statistically signiﬁcant at the p<.001\nlevel according to a one-sided paired t-test.\nbasic viewpoint itself, even with a weighted Ψ′. At this\npoint their impact on full multiple viewpoint systems is\nunknown and must be tested with a viewpoint selection\nalgorithm.\n5.2 Viewpoint Selection Results\nA viewpoint selection algorithm is a search algorithm to\nﬁnd the locally optimal multiple viewpoint system given\na set of candidate viewpoints. Following [17], the current\nresearch uses a forward stepwise algorithm which, starting\nfrom the empty set of viewpoints, alternately attempting\nto add and then delete viewpoints from the current system,\ngreedily selecting the best system according to ¯hat each\niteration. For this study a stopping criteria is imposed such\nthat the new viewpoint system must improve ¯hby at least\nan effect size of d > . 005, or more than 0.5% of a standard\ndeviation.\nPredicting the Root andChordType together, given\nthe metrical position in the bar ( PosInBar ), is chosen\nas a cognitively tangible task for the multiple viewpoint\nsystem to perform. In order to predict the two basic at-\ntributes simultaneously they are considered as the merged\nattribute Root ⊗ChordType . Merged attributes are sim-\nply a cross product of basic attributes, equivalent to linked\nviewpoints [7], and have been found to be an effective\nmethod for predicting multiple highly correlated basic at-\ntributes [10]. An unbounded interpolated smoothing model\nwith escape method C for both STM and LTM is found\nto be optimal for predicting merged attributes in the cur-\nrent corpus [10], with update exclusion used in the STM\nonly. Using all of the basic and derived viewpoints speci-\nﬁed in Section 3.1 and allowing linked viewpoints consist-\ning of up to two constituent viewpoints, or three if one is\nPosInBar , a pool of 64 candidate viewpoints for selec-\ntion is formed.\nThe unweighted Ψ′system goes through ﬁve iterations\nof viewpoint addition (without deletion) before termina-\ntion returning ¯h= 3.037 (Figure 2). By contrast, the\nweighted Ψ′system terminates after seven viewpoint ad-\nditions with a lower ¯hof3.012(Figure 3). The differencebetween these results is found to be statistically signiﬁcant\nwith a paired one-sided t-test at the .001 level ( d f= 347\nt= 5.422p < . 001). However, more importantly, the\neffect size is found to be small, d=.026, owing to the ab-\nsolute different of .025 bits/symbols between the means.\nSince the termination criteria is somewhat arbitrary (an\nappropriate value for dis hand-selected), the unweighted\nsystem was allowed to continue up to seven iterations to\nmatch the weighted system. This returns ¯h= 3.025,\nwhich is still found to be signiﬁcantly outperformed by the\nweighted model ( d f= 347 t= 3.725p < . 001, effect size\nd=.017).\nIn the context of the current study the viewpoints cho-\nsen from both viewpoint selection runs is highly rele-\nvant. The unweighted Ψ′selects only basic viewpoints\nand viewpoints derived from Root . No viewpoints de-\nrived from ChordType are selected, nor MeeusInt or\nChromaDist . This is to be expected given the ﬁndings in\nSection 5.1, where derived viewpoints with an unweighted\nΨ′are found to be poor predictors of ChordType . By\ncontrast, during viewpoint selection with a weighted Ψ′,\nlinked viewpoints containing FunctionType are added\non the third and sixth iterations and MeeusInt on the\nfourth iteration. This means that not only does the\nweighted Ψ′model perform slightly better in terms of\n¯h, but is also more compact since the average viewpoint\nalphabet size of the seven linked viewpoints selected is\n124.4, as opposed to 169 for the unweighted Ψ′model.3\n6. CONCLUSIONS AND DISCUSSION\nThis paper has presented a new method for improving pre-\ndictions from derived viewpoints by weighting Ψ′(the\nfunction which maps from the derived to basic alphabet of\na viewpoint) with the zero-order frequencies of the basic\nattribute. Results show that such a weighting signiﬁcantly\nimproves the performance of derived viewpoints which ab-\nstract heavily away from their basic viewpoint, notably\nMajType ,7Type , and FunctionType . On the other\nhand, viewpoints derived from Root , such as RootInt ,\nMeeusInt , and ChromaDist , see only marginal im-\nprovements or slight decreases in performance. It has been\nshown that weighting Ψ′allows more derived viewpoints\nto be chosen in viewpoint selection. This produces a model\nwhich returns a slightly lower mean information content\nthan its unweighted counterpart. This model is also slightly\nmore computationally efﬁcient owing to the smaller alpha-\nbet sizes of the selected viewpoints. In practical terms, this\ncreates a model that has a closer ﬁt to the training data\nwhilst taking slightly less time to run for any of the tasks\noutlined in Section 2 (computational modelling of expec-\ntation, segmentation, and automatic music generation).\nThis paper studied weighting only by zero-order fre-\nquency. Useful future research might explore alterna-\ntive weighting schemes beyond the zero-order frequencies,\nsuch as ﬁrst-order Markov, or even more aggressive, ex-\nponential weighting schemes. Furthermore applying the\n3Note that PosInBar is a given attribute and so contributes an al-\nphabet size of only 1 during the prediction phase.424 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20163.383\n3.1273.0653.0553.0373.0313.0253.03.13.23.33.4\n1234567Mean Information ContentIteration1 +Root ⊗ChordType ⊗PosInBar\n2 +RootInt ⊗ChordType\n3 +RootIntFiP ⊗ChordType ⊗PosInBar\n4 +Root ⊗ChordType\n5 +RootInt ⊗ChordType ⊗PosInBar\n(6 +RootIntFiP ⊗ChordType )\n(7 +RootInt ⊖FiB⊗ChordType )\nFigure 2 . Viewpoint selection for multiple viewpoint mod-\nels using unweighted Ψ′. Viewpoints added at each itera-\ntion are shown below the graph. Bracketed viewpoints and\nthe dotted line indicate viewpoints added after termination.\n3.383\n3.1273.0553.0443.0293.0203.0123.03.13.23.33.4\n1234567Mean Information ContentIteration\n1 +Root ⊗ChordType ⊗PosInBar\n2 +RootInt ⊗ChordType\n3 +RootIntFiP ⊗FunctionType ⊗PosInBar\n4 +MeeusInt ⊗ChordType ⊗PosInBar\n5 +RootIntFiP ⊗ChordType\n6 +RootInt ⊗FunctionType ⊗PosInBar\n7 +Root ⊗ChordType\nFigure 3 . Viewpoint selection for multiple viewpoint mod-\nels using weighted Ψ′. Viewpoints added at each iteration\nare shown below the graph.weighting schemes to a range of domains, genres, and cor-\npora beyond jazz harmony is necessary to prove the meth-\nods presented in this paper can be universally applied.\nThe weighting of Ψ′for derived viewpoints appears to\nbe successful as it combines a more general, abstracted\nmodel capable of ﬁnding statistical regularities with the\nmore ﬁne-grained model of the basic viewpoint. It could be\nargued that this is already achieved by multiple viewpoint\nsystems in that they combine predictions from multiple\nmodels at various levels of abstraction in an information-\ntheoretically informed manner. However, if the effect of\nweighting Ψ′with a zero-order model was entirely sub-\nsumed by viewpoint combination then almost identical\nviewpoints would be chosen during the viewpoint selec-\ntion process, which is not the case (Section 5.2). As the\nresults stand, the weighted Ψ′model selects more derived\nviewpoints, forming a more compact model and performs\nslightly better in terms of mean information content.\nThe compactness of multiple viewpoint systems is rel-\nevant both to computational complexity and their relation-\nship with cognitive representations. Searching a sufﬁx tree\nfor the PPM* algorithm with the current implementation\nusing Ukkonen’s algorithm [26] is achieved in linear time\n(to the size of the training data J), but must be done |[τ]|\ntimes to return a complete prediction set over the viewpoint\nalphabet [τ], giving a time complexity of O(J|[τ]|). Se-\nlecting viewpoints with a smaller alphabet size has, there-\nfore, a substantial impact on the time complexity for the\nsystem. As a model for human cognition [19], selecting\nviewpoints with smaller alphabets without a loss of perfor-\nmance is equivalent to building levels of abstraction when\nlearning cognitive representations [29].\nAdditionally, the weighted Ψ′model constructs more\nconvincing viewpoint systems from a musicological per-\nspective. Chord function is an important aspect of jazz\nmusic [12] and tonal harmony in general, where common\ncadences progress in pre-dominant, dominant, tonic, pat-\nterns. Therefore, the fact that ChordType is selected\noverMajType and7Type suggests that chord function as\nsigniﬁed by the third and seventh of the chord together is\nmore important than the quality of the third (modelled by\nMajType ) or seventh (modelled by 7type ) separately.\nSimilarly, the selection of MeeusInt in the model sug-\ngests that functional theories for root progressions may be\nuseful descriptors of tonal harmony. On the other hand,\nChromaDist , which considers rising and falling progres-\nsions by a perfect ﬁfth equivalent, is not selected. This sup-\nports the notion that harmonic progressions in tonal har-\nmony are goal-oriented and strongly directional [8].\n7. ACKNOWLEDGEMENTS\nThe authors would like to thank Marcus Pearce for the use\nof the IDyOM software. This work is supported by the\nMedia and Arts Technology programme, EPSRC Doctoral\nTraining Centre EP/G03723X/1.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 4258. REFERENCES\n[1]https://code.soundsoftware.ac.uk/\nprojects/idyom-project . Accessed: 23-03-\n2016.\n[2]J. Cleary and I. Witten. Data compression using adap-\ntive coding and partial string matching. Communica-\ntions, IEEE Transactions on , 32(4):396–402, 1984.\n[3]D. Conklin. Prediction and Entropy of Music . PhD the-\nsis, Department of Computer Science, University of\nCalgary, 1990.\n[4]D. Conklin. Representation and discovery of vertical\npatterns in music. In IMCAI , pages 32–42, Edinburgh,\nScotland, 2002. Springer.\n[5]D. Conklin. Discovery of distinctive patterns in music.\nIntelligent Data Analysis , 14(5):547–554, 2010.\n[6]D. Conklin and C. Anagnostopoulou. Comparative Pat-\ntern Analysis of Cretan Folk Songs. In 3rd Interna-\ntional Workshop on Machine Learning and Music ,\npages 33–36, Florence, Italy, 2010.\n[7]D. Conklin and I. Witten. Multiple viewpoint systems\nfor music prediction. Journal of New Music Research ,\n24(1):51–73, 1995.\n[8]C. Dahlhaus. Studies on the Origin of Harmonic Tonal-\nity. Princeton University Press, Princetown, NJ, 1990.\n[9]S. Grifﬁths, M. Purver, and G. Wiggins. From phoneme\nto morpheme: A computational model. In 6th Confer-\nence on Quantitative Investigations in Theoretical Lin-\nguistics , T¨ubingen, Germany, 2015.\n[10] T. Hedges and G. Wiggins. The prediction of merged\nattributes with multiple viewpoint systems. Journal of\nNew Music Research , accepted.\n[11] H. Leonard. The Real Book: Volume I, II, III, IV and V .\nHal Leonard, Winoa, MN, 2012.\n[12] M. Levine. The Jazz Theory Book . Sher Music Co.,\nPetaluma, CA, 1995.\n[13] D. Mackay. Information Theory, Inference and Learn-\ning Algorithms . Cambridge University Press, Cam-\nbridge, UK, 2003.\n[14] N. Meeus. Toward a post-schoenbergian grammar of\ntonal and pre-tonal harmonic progressions. Music The-\nory Online , 6(1):1–8, 2000.\n[15] F. Pachet, J. Suzda, and D. Mart ´ın. A comprehen-\nsive online database of machine-readable leadsheets\nfor jazz standards. In 14th International Society for\nMusic Information Retrieval Conference , pages 275–\n280, Curitiba, Brazil, 2013.\n[16] M. Pearce. The Construction and Evaluation of Statis-\ntical Models of Melodic Structure in Music Perception\nand Composition . PhD thesis, City University, London,\n2005.[17] M. Pearce, D. Conklin, and G. Wiggins. Methods for\ncombining statistical models of music. In CMMR’04:\nProceedings of the Second International Conference\non Computer Music Modeling and Retrieval , pages\n295–312. Springer-Verlag, 2005.\n[18] M. Pearce, D. Mullensiefen, and G. Wiggins. The role\nof expectation and probabilistic learning in auditory\nboundary perception: A model comparison. Percep-\ntion, 39(10):1365–1389, 2010.\n[19] M. Pearce, M. Ruiz, S. Kapasi, G. Wiggins, and\nJ. Bhattacharya. Unsupervised statistical learning\nunderpins computational, behavioural, and neural\nnanifestations of musical expectation. NeuroImage ,\n50(1):302–313, 2010.\n[20] M. Pearce and G. Wiggins. Improved methods for sta-\ntistical modelling of monophonic music. Journal of\nNew Music Research , 33(4):367–385, 2004.\n[21] M. Pearce and G. Wiggins. Expectation in melody: the\ninﬂuence of context and learning. Music Perception:\nAn Interdisciplinary Journal , 23(5):377–405, 2006.\n[22] M. Rohrmeier and T. Graepel. Comparing feature-\nbased models of harmony. In 9th International Sym-\nposium on Computer Music Modeling and Retrieval\n(CMMR 2012) , pages 357–370, London, UK, 2012.\n[23] S. Sertan and P. Chordia. Modeling Melodic Improvi-\nsation in Turkish Folk Music Using Variable-Length\nMarkov Models. In 12th International Society for Mu-\nsic Information Retrieval Conference , pages 269–274,\nMiami, FL, 2011.\n[24] C. Shannon. A Mathematical theory of communica-\ntion. The Bell System Technical Journal , 27(3):379–\n423, 1948.\n[25] A. Srinivasamurthy and P. Chordia. Multiple viewpoint\nmodeling of north Indian classical vocal compositions.\nInInternational Symposium on Computer Music Mod-\neling and Retrieval , pages 344–356, London, 2012.\n[26] E. Ukkonen. On-line construction of sufﬁx trees. Algo-\nrithmica , 14(3):249–260, 1995.\n[27] R. Whorley. The Construction and Evaluation of Sta-\ntistical Models of Melody and Harmony . PhD thesis,\nGoldsmiths, University of London, London, 2013.\n[28] R. Whorley, G. Wiggins, C. Rhodes, and M. Pearce.\nMultiple viewpoint systems: time complexity and the\nconstruction of domains for complex musical view-\npoints in the harmonization problem. Journal of New\nMusic Research , 42(3):237–266, 2013.\n[29] G. Wiggins and J. Forth. IDyOT: A computational the-\nory of creativity as everyday reasoning from learned\ninformation. In Computational Creativity Research:\nTowards Creative Machines , pages 127–148. Atlantis\nPress, 2015.426 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Long-Term Reverberation Modeling for Under-Determined Audio Source Separation with Application to Vocal Melody Extraction.",
        "author": [
            "Romain Hennequin",
            "François Rigaud"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417489",
        "url": "https://doi.org/10.5281/zenodo.1417489",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/196_Paper.pdf",
        "abstract": "In this paper, we present a way to model long-term rever- beration effects in under-determined source separation al- gorithms based on a non-negative decomposition frame- work. A general model for the sources affected by rever- beration is introduced and update rules for the estimation of the parameters are presented. Combined with a well- known source-filter model for singing voice, an applica- tion to the extraction of reverberated vocal tracks from polyphonic music signals is proposed. Finally, an objec- tive evaluation of this application is described. Perfor- mance improvements are obtained compared to the same model without reverberation modeling, in particular by significantly reducing the amount of interference between sources.",
        "zenodo_id": 1417489,
        "dblp_key": "conf/ismir/HennequinR16",
        "content": "LONG-TERM REVERBERATION MODELING FOR\nUNDER-DETERMINED AUDIO SOURCE SEPARATION WITH\nAPPLICATION TO VOCAL MELODY EXTRACTION.\nRomain Hennequin\nDeezer R&D\n10 rue d’Ath `enes, 75009 Paris, France\nrhennequin@deezer.comFranc ¸ois Rigaud\nAudionamix R&D\n171 quai de Valmy, 75010 Paris, France\nfrancois.rigaud@audionamix.com\nABSTRACT\nIn this paper, we present a way to model long-term rever-\nberation effects in under-determined source separation al-\ngorithms based on a non-negative decomposition frame-\nwork. A general model for the sources affected by rever-\nberation is introduced and update rules for the estimation\nof the parameters are presented. Combined with a well-\nknown source-ﬁlter model for singing voice, an applica-\ntion to the extraction of reverberated vocal tracks from\npolyphonic music signals is proposed. Finally, an objec-\ntive evaluation of this application is described. Perfor-\nmance improvements are obtained compared to the same\nmodel without reverberation modeling, in particular by\nsigniﬁcantly reducing the amount of interference between\nsources.\n1. INTRODUCTION\nUnder-determined audio source separation has been a key\ntopic in audio signal processing for the last two decades.\nIt consists in isolating different meaningful ‘parts’ of the\nsound, such as for instance the lead vocal from the ac-\ncompaniment in a song, or the dialog from the background\nmusic and effects in a movie soundtrack. Non-negative\ndecompositions such as Non-negative Matrix Factoriza-\ntion [5] and its derivative have been very popular in this\nresearch area for the last decade and have achieved state-\nof-the art performances [3, 9, 12].\nIn music recordings, the vocal track generally contains\nreverberation that is either naturally present due to the\nrecording environment or artiﬁcially added during the mix-\ning process. For source separation algorithms, the effects\nof reverberation are usually not explicitly modeled and\nthus not properly extracted with the corresponding sources.\nSome studies [1, 2] introduce a model for the effect of\nspatial diffusion caused by the reverberation for a multi-\nchannel source separation application. In [7] a model for\nc/circlecopyrtRomain Hennequin, Franc ¸ois Rigaud. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Romain Hennequin, Franc ¸ois Rigaud. “Long-term rever-\nberation modeling for under-determined audio source separation with ap-\nplication to vocal melody extraction.”, 17th International Society for Mu-\nsic Information Retrieval Conference, 2016. This work has been done\nwhen the ﬁrst author was working for Audionamix.the dereverberation of spectrograms is presented for the\ncase of long reverberations, i.e. when the reverberation\ntime is longer than the length of the analysis window.\nWe propose in this paper to extend the model of re-\nverberation proposed in [7] to a source separation appli-\ncation that allows extracting the reverberation of a spe-\nciﬁc source together with its dry signal. The reverbera-\ntion model is introduced ﬁrst in a general framework for\nwhich no assumption is made about the spectrogram of the\ndry sources. At this state, and as often in demixing appli-\ncation, the estimation problem is ill-posed (optimization\nof a non-convex cost function with local minima, result\nhighly dependent on the initialization, ...) and requires the\nincorporation of some knowledge about the source signals.\nIn [7], this issue is dealt with a sparsity prior on the un-\nreverberated spectrogram model. Alternatively, the spec-\ntrogram sources can be structured by using models of non-\nnegative decompositions with constraints ( e.g. harmonic\nstructure of the source’s tones, sparsity of the activations)\nand/or by guiding the estimation process with prior infor-\nmation ( e.g. source activation, multi-pitch transcription).\nThus in this paper we propose to combine the generic re-\nverberation model with a well-known source/ﬁlter model\nof singing voice [3]. A modiﬁed version of the original\nvoice extraction algorithm is described and evaluated on an\napplication to the extraction of reverberated vocal melodies\nfrom polyphonic music signals.\nNote that unlike usual application of reverberation mod-\neling, we do not aim at extracting dereverbated sources but\nwe try to extract accurately both the dry signal and the re-\nverberation within the same track. Thus, the designation\nsource separation is not completely in accordance with our\napplication which targets more precisely stem separation.\nThe rest of the paper is organized as follows: Section 2\npresents the general model for a reverberated source and\nSection 3 introduces the update rule used for its estima-\ntion. In Section 4, a practical implementation for which\nthe reverberation model is combined with a source/ﬁlter\nmodel is presented. Then, Section 5 presents experimental\nresults that demonstrate the ability of our algorithm to\nextract properly vocals affected by reverberation. Finally,\nconclusions are drawn in Section 6.206Notation\n•Matrices are denoted by bold capital letters: M. The\ncoefﬁcients at row fand column tof matrix Mis\ndenoted by Mf,t.\n•Vectors are denoted by bold lower case letters: v.\n•Matrix or vector sizes are denoted by capital letters:\nT, whereas indexes are denoted with lower case let-\nters:t.\n•Scalars are denoted by italic lower case letters: s.\n• ⊙ stands for element-wise matrix multiplication\n(Hadamard product) and M⊙λstands for element-\nwise exponentiation of matrix Mwith exponent λ.\n2. GENERAL MODEL\nFor the sake of clarity, we will present the signal model for\nmono signals only although it can be easily generalized to\nmultichannel signals as in [6]. In the experimental Section\n5, a stereo signal model is actually used.\n2.1 Non-negative Decomposition\nMost source separation algorithms based on a non-negative\ndecomposition assume that the non-negative mixture spec-\ntrogram V(usually the modulus or the squared modulus\nof a time-frequency representation such as the Short Time\nFourier Transform (STFT)) which is a F×Tnon-negative\nmatrix, can be approximated as the sum of Ksource model\nspectrograms ˆVk, which are also non-negative:\nV≈ˆV=K/summationdisplay\nk=1ˆVk(1)\nVarious structured matrix decomposition have been pro-\nposed for the source models ˆVk, such as, to name a few,\nstandard NMF [8], source/ﬁlter modeling [3] or harmonic\nsource modeling [10].\n2.2 Reverberation Model\nIn the time-domain, a time-invariant reverberation can be\naccurately modeled using a convolution with a ﬁlter and\nthus be written as:\ny=h∗x, (2)\nwhere xis the dry signal, his the impulse response of the\nreverberation ﬁlter and yis the reverberated signal.\nFor short-term convolution, this expression can be ap-\nproximated by a multiplication in the frequency domain\nsuch as proposed in [6] :\nyt=h⊙xt, (3)\nwhere xt(respectively yt) is the modulus of the t-th frame\nof the STFT of x(respectively y) andhis the modulus of\nthe Fourier transform of h.\nFor long-term convolution, this approximation does not\nhold. The support of typical reverberation ﬁlters are gener-\nally greater than half a second which is way too long for a\nSTFT analysis window in this kind of application. In thiscase, as suggested in [7], we can use an other approxima-\ntion which is a convolution in each frequency channel :\nyf=hf∗xf, (4)\nWhere yf,hfandxfare thef-th frequency channel of\nthe STFT of respectively y,handx.\nThen, starting from a dry spectrogram model ˆVdry,kof\na source with index k, the reverberated model of the same\nsource is obtained using the following non-negative ap-\nproximation:\nˆVrev,k\nf,t=Tk/summationdisplay\nτ=1ˆVdry,k\nf,t−τ+1Rk\nf,τ (5)\nwhere Rkis theF×Tknon-negative reverberation matrix\nof modelkto be estimated.\nThe model of Equation (5) makes it possible to take\nlong-term effects of reverberation into account and gen-\neralizes short-term convolution models as proposed in [6]\nsince when Tk= 1, the model corresponds to the short-\nterm convolution approximation.\n3. ALGORITHM\n3.1 Non-negative decomposition algorithms\nThe approximation of Equation (1) is generally quantiﬁed\nusing a divergence (a measure of dissimilarity) between V\nandˆVto be minimized with respect to the set of parame-\ntersΛof all the models:\nC(Λ) =D(V|ˆV(Λ)) (6)\nA commonly used class of divergence is the element-\nwiseβ-divergence which encompasses the Itakura-Saito\ndivergence ( β= 0), the Kullback-Leibler divergence ( β=\n1) and the squared Frobenius distance ( β= 2) [4]. The\nglobal cost then writes:\nC(Λ) =/summationdisplay\nf,tdβ(Vf,t|ˆVf,t(Λ)). (7)\nThe problem being not convex, the minimization is gener-\nally done using alternating update rules on each parame-\nters of Λ. The update rule for a parameter Θis commonly\nobtained using an heuristic consisting in decomposing the\ngradient of the cost-function with respect to this parameter\nas a difference of two positive terms, such as\n∇ΘC=PΘ−MΘ, PΘ≥0, MΘ≥0, (8)\nand then by updating the parameter according to:\nΘ←Θ⊙MΘ\nPΘ. (9)\nThis kind of update rule ensures that the parameter re-\nmains non-negative. Moreover the parameter is updated in\na direction descent or remains constant if the partial deriva-\ntive is zero. In some cases (including the update rulesProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 207we will present), it is possible to prove using a Majorize-\nMinimization (MM) approach [4] that the multiplicative\nupdate rules actually lead to a decrease of the cost func-\ntion.\nUsing such an approach, the update rules for a standard\nNMF model ˆV=WH can be expressed as:\nH←H⊙WT/parenleftBig\nˆV⊙β−2⊙V/parenrightBig\nWTˆV⊙β−1, (10)\nW←W⊙/parenleftBig\nˆV⊙β−2⊙V/parenrightBig\nHT\nˆV⊙β−1HT. (11)\n3.2 Estimation of the reverberation matrix\nWhen dry models ˆVdry,kare ﬁxed, the reverberation matrix\ncan be estimated using the following update rule applied\nsuccessively on each reverberation matrix:\nRk←Rk⊙/parenleftBig\nˆV⊙β−2⊙V/parenrightBig\n∗tˆVdry,k\nˆV⊙β−1∗tˆVdry,k(12)\nwhere∗tstands for time-convolution:\n/bracketleftBig\nˆV⊙β−1∗tˆVdry,k/bracketrightBig\nf,τ=T/summationdisplay\nτ=tˆV⊙β−1\nf,τˆVdry,k\nf,τ−t+1.(13)\nThe update rule (12) obtained using the procedure de-\nscribed in Section 3.1 ensures the non-negativity of Rk.\nThis update can be obtained using the MM approach which\nensures that the cost-function will not increase.\n3.3 Estimation with other free model\nA general drawback of source separation models that do\nnot explicitly account for reverberation effects is that the\nreverberation affecting a given source is usually spread\namong all separated sources. However, this issue can still\narise with a proper model of reverberation if the dry model\nof the reverberated source is not constrained enough. In-\ndeed using the generic reverberation model of Equation\n(5), the reverberation of a source can still be incorrectly\nmodeled during the optimization by other source models\nhaving more degrees of freedom. A possible solution for\nenforcing a correct optimization of the reverberated model\nis to further constrain the structure of the dry model spec-\ntrogram, e.g.through the inclusion of sparsity or pitch ac-\ntivation constraints, and potentially to adopt a sequential\nestimation scheme. For instance, ﬁrst discarding the re-\nverberation model, a ﬁrst rough estimate of the dry source\nmay be produced. Second, considering the reverberation\nmodel, the dry model previously estimated can be reﬁned\nwhile estimating at the same time the reverberation matrix.\nSuch an approach is described in the following section for\na practical implementation of the algorithm to the problem\nof lead vocal extraction from polyphonic music signals.\n4. APPLICATION TO VOICE EXTRACTION\nIn this section we propose an implementation of our rever-\nberation model in a practical case: we use Durrieu’s algo-rithm [3] for lead vocal isolation and add the reverberation\nmodel over the voice model.\n4.1 Base voice extraction algorithm\nDurrieu’s algorithm for lead vocal isolation in a song is\nbased on a source/ﬁlter model for the voice.\n4.1.1 Model\nThe non-negative mixture spectrogram model consists\nin the sum of a voice spectrogram model based on a\nsource/ﬁlter model and a music spectrogram model based\non a standard NMF:\nV≈ˆV=ˆVvoice+ˆVmusic. (14)\nThe voice model is based on a source/ﬁlter speech pro-\nduction model:\nˆVvoice= (WF0HF0)⊙(WKHK). (15)\nThe ﬁrst factor (WF0HF0)is the source part correspond-\ning to the excitation of the vocal folds: WF0is a matrix\nof ﬁxed harmonic atoms and HF0is the activation of these\natoms over time. The second factor (WKHK)is the ﬁlter\npart corresponding to the resonance of the vocal tract: WK\nis a matrix of smooth ﬁlter atoms and HKis the activation\nof these atoms over time.\nThe background music model is a generic NMF:\nˆVmusic=WRHR. (16)\n4.1.2 Algorithm\nMatrices HF0,WK,HK,WRandHRare estimated mini-\nmizing the element-wise Itakura-Saito divergence between\nthe original mixture power spectrogram and the mixture\nmodel:\nC(HF0,WK,HK,WR,HR) =/summationdisplay\nf,tdIS(Vf,t|ˆVf,t),(17)\nwheredIS(x,y) =x\ny−log(x\ny)−1. The minimization is\nachieved using multiplicative update rules.\nThe estimation is done in three steps:\n1. A ﬁrst step of parameter estimation is done using\niteratively the multiplicative update rules.\n2. The matrix HF0is processed using a Viterbi decod-\ning for tracking the main melody and is then thresh-\nolded so that coefﬁcients too far from the melody are\nset to zero.\n3. Parameters are re-estimated as in the ﬁrst step but\nusing the thresholded version of HF0for the initial-\nization.208 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20164.2 Inclusion of the reverberation model\nAs stated in Section 3, the dry spectrogram model ( i.e.the\nspectrogram model for the source without reverberation)\nhas to be sufﬁciently constrained in order to accurately es-\ntimate the reverberation part. This constraint is here ob-\ntained through the use of a ﬁxed harmonic dictionary WF0\nand mostly, by the thresholding of the matrix HF0that en-\nforces the sparsity of the activations.\nWe thus introduce the reverberation model after the step\nof thresholding of the matrix HF0. The two ﬁrst steps then\nremains the same as presented in Section 4.1.2. In the third\nstep, the dry voice model of Equation (15) is replaced by a\nreverberated voice model following Equation (5):\nˆVrev. voice\nf,t =T/summationdisplay\nτ=1ˆVvoice\nf,t−τ+1Rf,τ. (18)\nFor the parameter re-estimation of step 3, the multi-\nplicative update rule of Ris given by Equation (12). For\nthe other parameters of the voice model, the update rules\nfrom [3] are modiﬁed to take the reverberation model into\naccount:\nHF0←HF0⊙WT\nF0/parenleftBig\n(WKHK)⊙/parenleftBig\nR∗t(ˆV⊙β−2⊙V)/parenrightBig/parenrightBig\nWT\nF0/parenleftBig\n(WKHK)⊙/parenleftBig\nR∗tˆV⊙β−1/parenrightBig/parenrightBig\n(19)\nHK←HK⊙WT\nK/parenleftBig\n(WF0HF0)⊙/parenleftBig\nR∗t(ˆV⊙β−2⊙V)/parenrightBig/parenrightBig\nWT\nK/parenleftBig\n(WF0HF0)⊙/parenleftBig\nR∗tˆV⊙β−1/parenrightBig/parenrightBig\n(20)\nWK←WK⊙/parenleftBig\n(WF0HF0)⊙/parenleftBig\nR∗t(ˆV⊙β−2⊙V)/parenrightBig/parenrightBig\nHT\nK/parenleftBig\n(WF0HF0)⊙/parenleftBig\nR∗tˆV⊙β−1/parenrightBig/parenrightBig\nHT\nK\n(21)\nThe update rules for the parameters of the music model\n(HRandWR), are unchanged and thus identical to those\ngiven in Equations (10) and (11).\n5. EXPERIMENTAL RESULTS\n5.1 Experimental setup\nWe tested the reverberation model that we proposed with\nthe algorithm presented in Section 4 on a task of lead vocal\nextraction in a song. In order to assess the improvement of\nour model over the existing one, we ran the separation with\nand without reverberation modeling.\nWe used a database composed of 9song excerpts of\nprofessionally produced music. The total duration of all\nexcerpts was about 10minutes. As the use of rever-\nberation modeling only makes sense if there is a signiﬁ-\ncant amount of it, all the selected excerpts contains a fair\namount of reverberation. This reverberation was already\npresent in the separated tracks and was not added artiﬁ-\ncially by ourselves. On some excerpts, the reverberationis time-variant: active on some parts and inactive on other,\nducking echo effect . . . Some short excerpts, as well as the\nseparation results, can be played on the companion web-\nsite1.\nSpectrograms were computed as the squared modulus\nof the STFT of the signal sampled at 44100 Hz, with 4096 -\nsample ( 92.9ms) long Hamming window with 75% over-\nlap. The length Tof the reverberation matrix was arbitrar-\nily ﬁxed to 52frames (which corresponds to about 1.2s)\nin order to be sufﬁcient for long reverberations.\n5.2 Results\nIn order to quantify the results we use standard metrics of\nsource separation as described in [11]: Signal to Distorsion\nRatio (SDR), Signal to Artefact Ratio (SAR) and Signal to\nInterference Ratio (SIR).\nThe results are presented in Figure 1 for the evaluation\nof the extracted voice signals and in Figure 2 for the ex-\ntracted music signals. The oracle performance, obtained\nusing the actual spectrograms of the sources to compute\nthe separation masks, are also reported. As we can see,\nadding the reverberation modeling increases all these met-\nrics. The SIR is particularly increased in Figure 1 (more\nthan 5dB): this is mainly because without the reverbera-\ntion model, a large part of the reverberation of the voice\nleaks in the music model. This is a phenomenon which is\nalso clearly audible in excerpts with strong reverberation:\nusing the reverberation model, the long reverberation tail is\nmainly heard within the separated voice and is almost not\naudible within the separated music. In return, extracted\nvocals with the reverberation model tend to have more au-\ndible interferences. This result is in part due to the fact\nthat the pre-estimation of the dry model (step 1 and 2 of\nthe base algorithm) is not interference-free, so that apply-\ning the reverberation model increases the energy of these\ninterferences.\nFigure 1 . Experimental separation results for the voice\nstem.\n1http://romain-hennequin.fr/En/demo/reverb_\nseparation/reverb.htmlProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 209Figure 2 . Experimental separation results for the music\nstem.\n6. CONCLUSION\nIn this paper we proposed a method to model long-term ef-\nfects of reverberation in a source separation application for\nwhich a constrained model of the dry source is available.\nFuture work should focus on speeding up the algorithm\nsince, multiple convolutions at each iteration can be time-\nconsuming. Developing methods to estimate the reverber-\nation duration (of a speciﬁc source within a mix) would\nalso make it possible to automate the whole process. It\ncould also be interesting to add spatial modeling for multi-\nchannel processing using full rank spatial variance matrix\nand multichannel reverberation matrices.\n7. REFERENCES\n[1] Simon Arberet, Alexey Ozerov, Ngoc Q. K. Duong,\nEmmanuel Vincent, Fr ´ed´eric Bimbot, and Pierre Van-\ndergheynst. Nonnegative matrix factorization and spa-\ntial covariance model for under-determined reverberant\naudio source separation. In International Conference\non Information Sciences, Signal Processing and their\napplications , pages 1–4, May 2010.\n[2] Ngoc Q. K. Duong, Emmanuel Vincent, and R ´emi\nGribonval. Under-determined reverberant audio source\nseparation using a full-rank spatial covariance model.\nIEEE Transactions on Audio, Speech, and Language\nProcessing , 18(7):1830 – 1840, Sept 2010.\n[3] Jean-Louis Durrieu, Ga ¨el Richard, and Bertrand\nDavid. An iterative approach to monaural musical mix-\nture de-soloing. In International Conference on Acous-\ntics, Speech, and Signal Processing (ICASSP) , pages\n105–108, Taipei, Taiwan, April 2009.\n[4] C ´edric F ´evotte and J ´erˆome Idier. Algorithms for non-\nnegative matrix factorization with the beta-divergence.\nNeural Computation , 23(9):2421–2456, September\n2011.\n[5] Daniel D. Lee and H. Sebastian Seung. Learning the\nparts of objects by non-negative matrix factorization.\nNature , 401(6755):788–791, October 1999.[6] Alexey Ozerov and C ´edric F ´evotte. Multichannel non-\nnegative matrix factorization in convolutive mixtures\nfor audio source separation. IEEE Transactions on Au-\ndio, Speech and Language Processing , 18(3):550–563,\n2010.\n[7] Rita Singh, Bhiksha Raj, and Paris Smaragdis.\nLatent-variable decomposition based dereverberation\nof monaural and multi-channel signals. In IEEE Inter-\nnational Conference on Audio and Speech Signal Pro-\ncessing , Dallas, Texas, USA, March 2010.\n[8] Paris Smaragdis and Judith C. Brown. Non-negative\nmatrix factorization for polyphonic music transcrip-\ntion. In Workshop on Applications of Signal Processing\nto Audio and Acoustics , pages 177 – 180, New Paltz,\nNY , USA, October 2003.\n[9] Paris Smaragdis, Bhiksha Raj, and Madhusudana\nShashanka. Supervised and semi-supervised separation\nof sounds from single-channel mixtures. In 7th Inter-\nnational Conference on Independent Component Anal-\nysis and Signal Separation , London, UK, September\n2007.\n[10] Emmanuel Vincent, Nancy Bertin, and Roland Badeau.\nAdaptive harmonic spectral decomposition for mul-\ntiple pitch estimation. IEEE Transactions on Au-\ndio, Speech and Language Processing , 18(3):528–537,\nMarch 2010.\n[11] Emmanuel Vincent, R ´emi Gribonval, and C ´edric\nF´evotte. Performance measurement in blind audio\nsource separation. IEEE Transactions on Audio,\nSpeech and Language Processing , 14(4):1462–1469,\nJuly 2006.\n[12] Tuomas Virtanen. Monaural sound source separation\nby nonnegative matrix factorization with temporal con-\ntinuity and sparseness criteria. IEEE Transactions on\nAudio, Speech and Language Processing , 15(3):1066–\n1074, March 2007.210 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "The Sousta Corpus: Beat-Informed Automatic Transcription of Traditional Dance Tunes.",
        "author": [
            "Andre Holzapfel",
            "Emmanouil Benetos"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416938",
        "url": "https://doi.org/10.5281/zenodo.1416938",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/156_Paper.pdf",
        "abstract": "In this paper, we present a new corpus for research in computational ethnomusicology and automatic music tran- scription, consisting of traditional dance tunes from Crete. This rich dataset includes audio recordings, scores tran- scribed by ethnomusicologists and aligned to the audio performances, and meter annotations. A second contri- bution of this paper is the creation of an automatic music transcription system able to support the detection of multi- ple pitches produced by lyra (a bowed string instrument). Furthermore, the transcription system is able to cope with deviations from standard tuning, and provides temporally quantized notes by combining the output of the multi-pitch detection stage with a state-of-the-art meter tracking al- gorithm. Experiments carried out for note tracking using 25ms onset tolerance reach 41.1% using information from the multi-pitch detection stage only, 54.6% when integrat- ing beat information, and 57.9% when also supporting tun- ing estimation. The produced meter aligned transcriptions can be used to generate staff notation, a fact that increases the value of the system for studies in ethnomusicology.",
        "zenodo_id": 1416938,
        "dblp_key": "conf/ismir/HolzapfelB16",
        "content": "THE SOUSTA CORPUS:BEAT-INFORMED AUTOMATIC\nTRANSCRIPTION OFTRADITIONAL DANCETUNES\nAndreHolzapfel\nAustrian ResearchInstitute\nfor Artiﬁcial Intelligence (OFAI)\nandre@rhythmos.orgEmmanouilBenetos\nCentre for Digital Music\nQueenMaryUniversity of London\nemmanouil.benetos@qmul.ac.uk\nABSTRACT\nIn this paper, we present a new corpus for research in\ncomputationalethnomusicologyandautomaticmusictran-\nscription, consisting of traditional dance tunes from Cret e.\nThis rich dataset includes audio recordings, scores tran-\nscribed by ethnomusicologists and aligned to the audio\nperformances, and meter annotations. A second contri-\nbution of this paper is the creation of an automatic music\ntranscriptionsystemabletosupportthedetectionofmulti -\nple pitches produced by lyra (a bowed string instrument).\nFurthermore, the transcription system is able to cope with\ndeviations from standard tuning, and provides temporally\nquantizednotesbycombiningtheoutputofthemulti-pitch\ndetection stage with a state-of-the-art meter tracking al-\ngorithm. Experiments carried out for note tracking using\n25ms onset tolerance reach 41.1% using information from\nthe multi-pitch detection stage only, 54.6% when integrat-\ningbeatinformation,and57.9%whenalsosupportingtun-\ning estimation. The produced meter aligned transcriptions\ncan be used to generate staff notation, a fact that increases\nthe value of the systemfor studies inethnomusicology.\n1. INTRODUCTION\nAutomaticmusictranscription(AMT),theprocessofcon-\nvertingamusicrecordingintonotation,haslargelyfocuse d\non genres of eurogenetic [17] popular and classical mu-\nsic and especially on piano repertoire; see [3] for a re-\ncent overview. This is reﬂected in various AMT datasets,\nwhich consist of audio recordings along with a machine\nreadable reference notation that speciﬁes the time values\nof note onsets and offsets. Such datasets include the RWC\ndatabase [14], the MAPS dataset [12], and the Bach10\ndataset [9]. The reasons for the focus on certain styles\nAH is supported by the Austrian Science Fund (FWF: M1995-N31) ,\nandbytheViennaScienceandTechnologyFund(WWTF,projectMA 14-\n018).\nEB is supported by a UK Royal Academy of Engineering Research\nFellowship (grantno. RF/128).\nBothauthors contributedequallyto this paper.\nc/circlecopyrtAndre Holzapfel, Emmanouil Benetos. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Andre Holzapfel, Emmanouil Benetos. “The Sousta cor-\npus: Beat-informed automatic transcription of traditional d ance tunes”,\n17th International Society for Music Information Retrieval Conference,\n2016.seem manifold: Some aspects that might play a role are\nthe cultural background of the AMT engineers, the rela-\ntiveeaseofcompilingreferencenotationsforapianousing\nMIDI, and the predominant goal of transcription, i.e. the\npiano-roll, being closely related to the piano. As a point\nof fundamental importance, eurogenetic music lends itself\nnicely to the task of transcription, because in most cases\na composition is ﬁrst notated, and then performed using\nthis notation. Hence, the notation can be interpreted as the\nground-truth for an AMT system. The attempt to recon-\nstruct this ground-truth, which is seen as a hidden gener-\native concept for the performance [6], appears, at least at\nﬁrstglance, tobe awell-deﬁned task.\nHowever, in the ﬁeld of ethnomusicology, the process\nof transcribing a music performance mainly serves the\nmeans to analyse the structure of previously not notated\nmusic[11]. Asaﬁrstcontributionofthispaper,wealigna\nset of such recordings to transcriptions by ethnomusicolo-\ngists, this way compiling an evaluation corpus for AMT\nthat can enable us to monitor the performance of AMT\nsystems on the music of a speciﬁc oral tradition. The mu-\nsic consists of Cretan dance tunes that were performed by\nCretan musicians and recorded and transcribed by ethno-\nmusicologists in the Crinnos project [2] that targeted the\ndocumentation of that speciﬁc music idiom. Only a small\nsubset of the pieces recorded in the Crinnos project were\ntranscribed, due to the large amount of effort that man-\nual transcription takes. While it is clear that the building\nblocks of the tunes are small melodic phrases (see Sec-\ntion 2 for more detail), the way these phrases are strung\ntogether is largely improvised in the performance. These\nchoices are not verbalized by the musicians, and an accu-\nrate transcription method will constitute an important too l\nto infer the grammar that underlies folk dance tunes in the\nareaof the EasternMediterranean and beyond.\nTherefore, as the second contribution of this paper, we\nextend an existing transcription algorithm [5] to be able to\ncope with tuning deviations and to take into account the\nmetrical structure of the dance tunes. In Cretan music, as\nwellas inmany other musicstylesintheworld, musicians\ntunetheirinstrumentsaccordingtopersonalpreference. T o\nthe authors’ knowledge, whilst several AMT systems sup-\nport the extraction of multiple pitches in a high frequency\nresolution (e.g. [9,13]), no AMT system has yet exploited\nthatinformationforestimatingtheoveralltuninglevelan d\ntocompensatefortuningdeviationsduringthepitchquan-531tisationstep. Inaddition,formanymusicstyles,especial ly\nwhen related to dance, a clear metrical organization and\na predictable tempo development enable for synchronisa-\ntion between dancers and musicians in performances. For\nthatreason,weapplyastate-of-the-artmetertrackingalg o-\nrithm [15] for tracking beats and measures, and apply this\ninformation in order to achieve a temporal quantisation of\nnote positions obtained from our AMT system. This way,\nwe can obtain a transcription with temporal precision that\nisclearlyincreasedtothatofpreviouslypresentedsystem s.\nIn addition, this step enables us to obtain a visualisation\nof the transcription in a staff notation including bar posi-\ntions, a perspective that marks an important step beyond\nthe piano-roll as AMToutput.\nOur paper is structured as follows; Section 2 provides\nsome detail about the musical idiom and corpus, and de-\nscribestheprocessthatwasfollowedtoaligntranscriptio ns\nwithperformancesonthenotelevel. Section3summarizes\nthechosenAMTsystem,anddescribestheextensionspro-\nposed in this paper. We then evaluate the performance of\noursystems,andprovideillustrativeexamplesinSection4 .\nSection 5concludes the paper.\n2. THE SOUSTA CORPUS\n2.1 Background and motivation\nThe recordings that constitute the Sousta corpus presented\nin this paper were conducted in 2004 within the Crinnos\nproject [2] in Rethymnon, Crete, Greece. Within the Crin-\nnosproject444piecesofCretanmusicwererecorded,and\n40 of these performances were transcribed by ethnomusi-\ncologists. Thetranscriptionscontainthemelodyplayedby\nthemainmelodyinstrument,aswellasthevocalmelodyif\nvocals are present in a piece, and ignore the rhythmic ac-\ncompaniment. Half of the 40 transcriptions regard a spe-\nciﬁc dance called Sousta. These transcriptions were cho-\nsen foranote-to-note alignment for several reasons.\nFirst, this way we obtain a music corpus that is highly\nconsistent in terms of musical style, which made a uni-\nﬁed alignment strategy applicable to the recordings. The\nSoustadanceisusuallynotatedin 2/4meter,andischarac-\nterized by a relatively stable tempo that lies between 110-\n130 beats per minute (bpm). The instrumental timbres are\nhighlyconsistent,withusuallytwoCretanlutesplayingth e\naccompaniment,andoneCretanlyra(apear-shapedﬁddle)\nplaying the main melody. All recordings were performed\nin the same studio, but with differing musicians. Apart\nfrom supporting our alignment procedure, the consistency\nof the recordings will enable a style comparison between\nindividual musicians aspart of our futurework.\nThesecondreason to choose the Sousta tunes lies\nwith their value for music segmentation approaches. Like\nmanytunesintheEasternMediterranean,theSoustadance\nfollows an underlying syntax that has been termed as\nparataxis [18]. In parataxis tunes, the building elements\nareshortmelodicphrasesthatarestrungtogetherinappar-\nently arbitrary order without clear conjunctive elements.\nThese phrases have a length of typically two measuresfor the Sousta dance. The 20 transcriptions were anal-\nysedwithintheCrinnosprojectanditselementarymelodic\nphrases were identiﬁed by the experts. This way, a cat-\nalogue of 337 phrases was compiled that describes the\nmelodic content of the tunes. Each measure of the corpus\nis assigned to a particular phrase. The note-to-note align-\nmentthatismadeavailableinthispaperenablestoidentify\nthe phrase boundaries within the recordings, and this way\nthe corpus can serve for music segmentation experiments.\nSuch a corpus can form a basis for the development of an\naccurate system for syntactic analysis of music styles in\ntheEastern Mediterranean and elsewhere.\nSuch an analysis system, however, needs to be built on\nanAMTsystemthatworksasaccuratelyaspossible,inor-\nder to be able to analyse performances for which no man-\nual transcription is available. We take this as a motivation\nto use for the ﬁrst time, to the best of our knowledge, a\nset of performance transcriptions as the source for what is\nusuallycalledground-truthinMIR.Thisway,asour third\nmotivationforchoosingthisspeciﬁcstyle,wecontributet o\na larger diversity in available AMT datasets, by providing\naccess tothealigned data for research purposes. The chal-\nlenging aspects for AMT systems are the high density of\nnotes, the tuning deviations, and the necessary focus on a\nbowed string instrument (lyra) within a pitched percussive\naccompaniment (lutes).\n2.2 Alignment procedure\nThe ﬁrst step to obtain a note-to-note alignment is to cor-\nrect for transpositions between transcription and perfor-\nmance. Fouroutofthe20pieceswereplayedeitheroneor\ntwo semitones higher than the transcription implied. Ap-\nparently, transcribers preferred to notate the upper empty\nstringofthelyraasthenoteA,eveniftheplayertunedthe\ninstrument one or several semitones higher.\nAs a second step, we conduct a meter tracking to ob-\ntain estimations for beat and measure positions, using the\nalgorithmpresentedin[15]. Themetertrackerwastrained\non the meter-annotated Cretan music used in [15], and\nthen applied to track the meter in the 20 Sousta perfor-\nmances(formoredetailsonthetrackingalgorithmseeSec-\ntion3.4).\nAfter that, the MIDI ﬁle obtained from the transcrip-\ntion is synthesized, and the algorithm from [16] is used to\nobtainaninitialalignmentoftheMIDIﬁletotherecorded\nperformance. Thetimingofthemeasuresisextractedfrom\nthe aligned MIDI using the Matlab MIDI Toolbox [10].\nEach of the estimated measures in the MIDI is then cor-\nrectedtotakethetimevalueoftheclosestbeatasobtained\nfrom the meter tracker from the recording. This step was\nincludedtocompensatefortiminginaccuraciesoftheauto-\nmatic alignment. The obtained downbeats were manually\ncorrected using Sonic Visualizer1. The output of thispro-\ncess is the exact timing of all measures that are notated in\nthetranscription.\nThese manually corrected measure positions are then\nused as a source for the exact timing of the pre-aligned\n1http://sonicvisualiser.org/532 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016MIDI, by determining an alignment curve that corrects all\nnote onsets accordingly. After that, also the note dura-\ntions are edited to ﬁt the notated length in seconds ( e.g.\na quarter note at 120 bpm should last 0.5 s). The result\nwas again manually checked for inaccuracies. In addition,\nvocalsectionsweremanuallyannotated. Duringvocalsec-\ntions,themaininstrumentusuallystops,andthetranscrip -\ntion of musical phrases is of interest only for the instru-\nmentalsectionsinarecording. Totheauthors’knowledge,\nthis measure-informed process is a novel and promising\nway to generate note-level transcriptions, as opposed to\nperforming manual note corrections on an automatically\naligned MIDI ﬁle, or by relying on an expert musician to\nfollow and perform therecorded musicinreal-time[20].\nThe obtained corpus contains 35357 aligned notes in\n4455 measures, distributed along the 20 recordings with\na total length of 71m16s2,84%being instrumental. The\naveragepolyphony(onvoicedframesonly)is1.08,andthe\naverage note duration is108ms.\n3. BEAT-INFORMED TRANSCRIPTION\nThis section describes the AMT system developed to tran-\nscribe the traditional dance corpus of Section 2. The main\ncontributions of the proposed system are: (i) Supporting\nthe transcription of lyra, a bowed string instrument that is\npresent in all recordings of the corpus, by supplying the\nsystemwithlyratemplates;(ii)Estimatingtheoveralltun -\ninglevelandcompensatingfordeviationsfrom440Hztun-\ning (cf. Section 1 for a discussion on related work for tun-\ning estimation in AMT systems); (iii) Incorporating me-\nter and beat information (from either manual meter anno-\ntations or estimations from a state-of-the-art meter track -\ningsystem[15]),resultinginatemporallyquantisedmusic\ntranscription.\nAs a basis for the proposed work, the AMT system\nof [4] is adapted, which was originally aimed for tran-\nscribing 12-tone equal tempered music and supported Eu-\nrogenetic orchestral instruments. The system is based on\nprobabilisticlatentcomponentanalysis(PLCA),a spectro-\ngramfactorization methodthatdecomposesaninputtime-\nfrequencyrepresentationintoaseriesofnotetemplatesan d\nnote activations. The system of [4] also supports the ex-\ntraction of tuning information per transcribed note, which\nis used in this paper to estimate the overall tuning level.\nA diagram for the proposed system can be seen in Fig. 1,\nwithallsystemcomponentsbeingpresentedinthefollow-\ning subsections.\n3.1 Time-Frequency Representation\nAs input time-frequency representation for the transcrip-\ntion system, the variable-Q transform (VQT) spectrogram\nis used [19], denoted Vω,t(ωis the log-frequency index\nandtis the time index). Here, the interpolated VQT spec-\ntrogram has a frequency resolution of 60 bins/octave (i.e.\n20 cent resolution), using a variable-Q parameter γ= 30,\nwith a minimum frequency of 36.7 Hz (i.e. at D1). As\n2Foralistofrecordingsseewww.rhythmos.org/ISMIR2016Sou sta.htmlwith the constant-Q transform (CQT), this VQT represen-\ntation allows for pitch changes to be represented by shifts\nacross the log-frequency axis, whilst offering an increase d\ntemporal resolution in lower frequencies compared to the\nCQT.\n3.2 Multi-pitchDetection\nThe multi-pitch detection model takes as input the VQT\nspectrogram of an audio recording and returns an initial\nestimate of note events. Here, we adapt the PLCA-based\nspectrogram factorization model of [4] for transcribing\nmusicproducedbylyra. Themodelapproximates Vω,tasa\nbivariate probability distribution P(ω, t), which is in turn\ndecomposed into a series of probability distributions, de-\nnoting note templates, pitch activations, tuning deviatio ns,\nand instrument/source contributions.\nThe model isformulated as:\nP(ω, t) =\nP(t)/summationdisplay\nq,p,f,sP(ω|q, p, f, s )Pt(f|p)Pt(s|p)Pt(p)Pt(q|p)\n(1)\nwhere qdenotes the sound state (e.g. attack, sustain parts\nof a note), pdenotes pitch, sdenotes instrument source,\nandfdenotes log-frequency shifting with respect to 12-\ntone equal temperament (12-TET) at a tuning of 440 Hz\nfornoteA4. In(1), P(t)istheenergyoftheVQTspectro-\ngram, which is known. P(ω|q, p, f, s )is a 5-dimensional\ntensor that represents the pre-extracted spectral templat es\nof lyra notes, per sound state q, pitch pand instrument\nmodel s, which are also pre-shifted across log-frequency\nf(cf. Section 4.1 on the extraction of lyra templates).\nPt(f|p)is the time-varying log-frequency shifting distri-\nbution per pitch (used to estimate tuning deviations per\nproducednote), Pt(s|p)isthesourcecontributionperpitch\nover time, Pt(q|p)is the time-varying sound state acti-\nvation per pitch, and ﬁnally Pt(p)is the pitch activation,\ni.e. the resulting multi-pitch detection output. In the\nproposed model, p∈ {1. . . ,88}, with p= 1denot-\ning A0 and f∈ {1, . . . , 5}, which respectively denote\n{−40,−20,0,20,40}centdeviationfromidealtuningus-\ning12-TET.\nThe unknown model parameters ( Pt(f|p),Pt(s|p),\nPt(p),Pt(q|p)) are iteratively estimated using the\nexpectation-maximization (EM) algorithm [8], with the\nupdate rules described in [4]. With 30 iterations set in the\nsystem, the runtime for multi-pitch detection is approxi-\nmately 3 ×real-time using a Sony VAIO S15 laptop. The\noutput of the model is P(p, t) =P(t)Pt(p), which repre-\nsentspitch activation probability insemitone scale.\n3.3 Tuning Estimation- Postprocessing\nThe output of the multi-pitch detection model, P(p, t), is\nnon-binary and needs to be converted into a list of note\nevents or a MIDI ﬁle. Firstly, in order to compensate for\nany tuning deviations from A4=440 Hz, a tuning estima-\ntion step is proposed, utilising information from the pitchProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 533AUDIO TIME-FREQUENCY\nREPRESENTATIONMULTI-PITCH POSTPROCESSING/MIDI\nDETECTION TUNINGESTIMATION\nBEAT\nTRACKINGBEAT\nQUANTISATION\nFigure 1: Diagram forthe proposed system.\nshifting parameter Pt(f|p). The tuning probability vector\niscomputed as:\nP(f) =/summationdisplay\np,tPt(f|p)Pt(p)P(t). (2)\nP(f)provides an estimate on tuning deviations from\n440 Hz tuning, with the various fvalues corresponding\nto{−40,−20,0,20,40}cent deviation. The ﬁnal tuning\nestimate is given by argmaxfP(f). Then, a 20 cent reso-\nlution time-pitchrepresentation iscomputed:\nP(f′, t) = [P(f,1, t)· · ·P(f,88, t)] (3)\nwhere P(f, p, t ) = Pt(f|p)Pt(p)P(t), and f′=\n{1, . . . , 88∗5}denotespitchvaluesbetween1and88with\n20 cent resolution. The time-pitch representation is subse -\nquently shifted towards 440 Hz tuning by reassigning the\nindex f′=f′+ argmaxfP(f)−3(since f= 3repre-\nsents0centtuningdeviation). Then,atuning-compensated\npitch activation P(p, t)isre-computed from P(f′, t):\nP(p, t) =5p/summationdisplay\nf′=5p−4P(f′, t),∀p∈ {1, . . . , 88}.(4)\nFollowingtuningcompensation,thresholdingisperformed\nonP(p, t),followedbyaprocessforremovingnoteevents\nwith a duration less than 40 ms. This results in a list of\nnote events, denoted by onset, offset, and pitch, denoted\nnmat m(on,oﬀ, p), with m∈ {1, . . . , M }denoting the\nnote index, with onandoﬀbeing the onset and offset\ntimes,respectively.\n3.4 Meter Tracking &Quantisation\nSince most dance tunes have an underlying stable meter\nand a relatively predictable tempo that enables dancers to\nsynchronize, a quantisation of the estimated notes onto a\ntightmetricalgridislikelytoimprovetranscriptionperf or-\nmance. In addition, the notes in the obtained transcription\nare assigned positions within the meter, and obtain quan-\ntisednotedurations,whichenablesforanimmediatevisu-\nalisationas staffnotation including a timesignature.\nInthispaper,beatandmeasurepositionsforarecording\nare computed using the Bayesian meter tracker presented\nin [15]. Given a series of observations/features yk, with\nk∈ {1, ..., K }, computed from a music signal, a set of\nhidden variables xkis estimated. The hidden variables de-\nscribe at each analysis frame kthe position Φkwithin a\nmeasure, and the tempo in positions per frame ( ˙Φk). Thegoal is to estimate the hidden state sequence that maxi-\nmizes the posterior (MAP) probability P(x1:K|y1:K). If\nwe express the temporal dynamics as a Hidden Markov\nModel (HMM),theposterior isproportional to\nP(x1:K|y1:K)∝P(x1)K/productdisplay\nk=2P(xk|xk−1)P(yk|xk)(5)\nIn (5), P(x1)is theinitial state distribution ,\nP(xk|xk−1)is thetransition model , and P(yk|xk)is the\nobservation model . When discretising the hidden variable\nxk= [Φ k,˙Φk], the inference in this model can be per-\nformed using the Viterbi algorithm. As in [15], a uniform\ninitial state distribution P(x1)was chosen. The transition\nmodel factorizes intotwocomponents according to\nP(xk|xk−1) =P(Φk|Φk−1,˙Φk−1)P(˙Φk|˙Φk−1)(6)\nwiththetwocomponentsdescribingthetransitionsofposi-\ntion and tempo states, respectively. The position transiti on\nmodelincrementsfrom Φk−1toΦkdeterministicallyusing\nthe tempo ˙Φk−1, starting from a value of 1 (at the begin-\nningofametricalcycle)toavalueof800. Thetempotran-\nsition model allows for tempo transitions to the adjacent\ntempostates,allowingforgradualtempochanges. Theob-\nservation model P(yk|xk)divides the 2/4-bars of meter-\nannotated Sousta tunes used in [15] into 32 discrete bins.\nSpectral-ﬂuxfeaturesareassignedtooneofthesemetrical\nbins,andparametersofaGaussianMixtureModel(GMM)\nare determined. The computation follows exactly the pro-\ncedure described in [15], which lead to an almost perfect\nbeat tracking forthe Cretan tunes.\nIn order to quantise the detected note events nmat m\nwith respect to the estimated beat positions, ﬁrstly a met-\nrical grid is created from the beat positions ( beat n,n∈\n{1, . . . , N }). The metrical gridtimesare:\ngridD(n−2)+d+1= beat n−1+ (d/D)(beat n−beat n−1)\n(7)\nwhich are computed for n= 2, . . . , N. In (7), Dis the\nbeat subdivision factor ( D= 4,8corresponds to 16th and\n32ndnotesubdivisions,respectively)and d={0, . . . , D −\n1}. Then, the beat-quantised transcription is produced by\nchanging the onset time for each detected note nmat mto\ntheclosest timeinstant computed from (7).\n4. EXPERIMENTS\n4.1 Training\nSpectral templates for lyra are extracted from 20 short\nsegments of solo lyra recordings, taken from the Crinnos534 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016project [2] (disjoint from the recordings in the corpus de-\nscribed in Section 2). These are used as P(ω|q, p, f, s )\nin the model of (1). The recordings are partially anno-\ntated, identifying non-overlapping pitches. Then, for eac h\nrecording the VQT spectrogram is computed as in Section\n3.1andspectraltemplatesforeachnoteareextractedusing\nstandard PLCA, whilst keeping the pitch activation matrix\nﬁxed to the reference pitch annotations. The templates are\npre-shifted across log-frequency to account for tuning de-\nviations, and templates for missing notes are created by\nshifting the extracted templates across the log-frequency\naxis. The resulting note range for the training templates is\nB3-F5.\n4.2 Metrics\nFor assessing the performance of the proposed system in\nterms of multi-pitch detection, we utilise the onset-based\nmetric used in the MIREX note tracking evaluations [1].\nHere, a note event is assumed to be correct if its pitch cor-\nresponds to the ground truth pitch and its onset is within a\n±25msrangeofthegroundtruthonset. Thisisincontrast\nwith the ±50 ms onset tolerance setting used in MIREX,\nsincethecurrentcorpushasfasttempowithshortnotedu-\nrations. Usingtheaboverule,theprecision( P),recall( R),\nand F-measure ( F)metrics aredeﬁned:\nP=Ntp\nNsys,R=Ntp\nNref,F=2· R · P\nR+P(8)\nwhere Ntpis the number of correctly detected pitches,\nNsysis the number of detected pitches, and Nrefis the\nnumber of ground truth pitches. The above metrics are\ncomputed only for the recording regions that do not con-\ntain any vocal parts (a comparative experiment is done in\nSection 4.3).\n4.3 Results\nUsingtheevaluationmetricsofSection4.2,averageresult s\nonthecorpusdescribedinSection2arepresentedinTable\n1. Variousconﬁgurationsfortheproposedsystemareused\nto evaluate the performance of each system component.\nConﬁguration 1 refers to simply using the output of the\nmulti-pitchdetectionmethodfromSection3.2. Conﬁgura-\ntion2involvesmulti-pitchdetectionplustheproposedtun -\ning estimation method from Section 3.3. Conﬁguration 3\nrefers to multi-pitch detection combined with meter track-\ning from Section 3.4, thus producing a beat-aligned note\noutput. Conﬁguration 4 combines multi-pitch detection,\ntuning estimation, and meter tracking. Finally, Conﬁgura-\ntion 5 is an oracle version of Conﬁguration 4, with the au-\ntomaticallyestimatedbeatsbeingreplacedbythemanually\nannotatedmeasurepositions,obtainedasdescribedinSec-\ntion 2.2. In all conﬁgurations that utilise beat informatio n\nthe beat subdivision factor used is D= 4(corresponding\nto16th notes).\nAs can be seen from Table 1, when integrating tun-\ningestimationthesystemperformanceimprovesby+2.2%\nin terms of F-measure. Likewise, by incorporating me-\nter tracking, system performance improves by +13.5%,System F P R\nConﬁguration 1 41.12% 45.33% 37.79%\nConﬁguration 2 43.37% 48.12% 39.64%\nConﬁguration 3 54.61% 66.38% 46.53%\nConﬁguration 4 57.92% 70.71% 49.21%\nConﬁguration 5 58.25% 71.14% 49.47%\nTable 1: Average multi-pitch detection results using the\ncorpus of Section 2, using various system conﬁgurations\nexplained inSection 4.3.\nwhereas when integrating both tuning and meter informa-\ntion the overall improvement is at +16.8%. Finally, us-\ning the reference measure annotations (Conﬁguration 5)\nleadstoanimprovementofonly+0.3%overtheautomatic\nbeatextraction,indicatingthereliabilityofmetertrack ing.\nIndeed, comparing the manually corrected measure anno-\ntations with those obtained from the automatic tracking,\nwe obtain an F-measure [7] of 94.5%. This is an even\nhigher meter tracking performance than observed on the\nCretan recordings in [15], possibly caused by the fact that\nthe recordings used in this paper were all conducted in the\nstudio, and all tunes relate to the same dance. A discrep-\nancy is also observed between average precision and aver-\nage recall; the lower recall is mostly attributed to repeate d\nnotesinthegroundtruth,whicharemergedintosinglenote\nevents in the output transcription. The aforementioned re-\nsults are approximately at the level of the state-of-the-ar t\nfor AMT, when using other datasets [1]; results for indi-\nvidual recordings range from F= 70 .9%to34.2%(the\nlatterfor aparticularly idiosyncratic recording).\nInFigure2,anexampleoftransferringabeat-quantised\ntranscription obtained with Conﬁguration 4 ( F= 56.17%\nfor this piece) to staff notation is depicted, along with the\nmanually transcribed reference notation. Spurious differ -\nences occur ( e.g.added note G in the ﬁrst measure) and\nthe style of notation seems artiﬁcial. However, the re-\nsemblancebetweenmelodiccontourinreferenceandauto-\nmatic transcription is apparent. According to the analysis\nin the Crinnos project, the phrase depicted in Figure 3 is\nrepeated with slight variations four times in the eight bars\nof this example, and comparing the phrase with each two\nconsecutive bars in the transcriptions, this structure can be\nrecognized. Figure4depictstheVQTandthepitchactiva-\ntions for the same eight bars. Further examples, all tran-\nscriptions obtained with Conﬁguration 4 (MIDI and au-\ndio),andthereferenceannotationswillbeavailableonthe\npaper’s website3.\nA comparison with a state-of-the-art AMT method is\nalso made, employing the system of [21], which is based\non non-negative matrix factorization. The aforementioned\nsystem decomposes a pitched sound as the sum of nar-\nrowband spectra. Results using multi-pitch detection only\nreach F= 26 .08%(in contrast with 41.12%for the\nproposed system). By integrating multi-pitch detection\nwith beat information, the performance of [21] reaches\n3www.rhythmos.org/ISMIR2016Sousta.htmlProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 535(a)Automatic transcription\n(b)Manual transcription, source [2]\nFigure 2:Four repetitions ofa two-bar phrase.\nFigure 3: Sousta phrase that is repeated (in slight varia-\ntions) inFigure 2four times,source [2].\nF= 35.94%(as compared with 54.61%for the proposed\nmethod). It should be noted that tuning estimation cannot\nbeachievedusingtheaforementionedsystem,astheoutput\nisquantised on theMIDIscale.\nExperiments are also carried out using a larger onset\ntolerance for the metrics of Section 4.2, set to 50 ms (as\nin the MIREX evaluations [1]). When evaluating Conﬁg-\nuration 4, F= 60 .74%, while using the method of [21]\nF= 38 .17%. The relatively small difference between\nusing 25 ms or 50 ms tolerance is attributed to the fact\nthat the employed corpus contains several short repeated\nnotes; since the utilised evaluation metrics consider du-\nplicate notes in the same temporal region as false alarms,\na larger tolerance window penalises the systems’ perfor-\nmance.\nAs mentioned in Section 4.2, the results presented in\nTable 1 are computed only for instrumental regions of the\ncorpus, thus excluding any vocal parts. When also tran-\nscribing vocal parts, performance using Conﬁguration 4\ndrops by 1.9% ( F= 55 .84%), due to the fact that the\ntraining data do not contain vocal templates; however, the\ntranscriptionofvocalmusicisnotinthescopeofthiswork.\nFinally,experimentswerecarriedoutusingabeatsubdivi-\nsion factor D= 8, which corresponds to 32nd notes. This\nresults in F= 48.2%, which indicates that the onsets for\nsomeofthedetectednoteswereplacedinincorrecttempo-\nral positions on themetrical grid.\nt(sec)p(b)ω(a)\n40 41 42 43 44 45 46 4740 41 42 43 44 45 46 47\n3040506070100200300400\nFigure 4: (a) The VQT spectrogram for the section tran-\nscribedinFigure2. (b)Thecorrespondingpitchactivation\nP(p, t).\n5. DISCUSSION\nInthispaper,wepresentedacorpusforevaluationofAMT\nsystems that is based on performance transcriptions man-\nually compiled by experts in ethnomusicology. We then\nproposedanAMTsystemthatcancopewithtuningdevia-\ntions,andweimprovetheperformanceoftheAMTsystem\nby quantising its output on a metrical grid that was esti-\nmatedusingastateoftheartmetertracker. Apartfromthe\nperformance improvement, this quantisation enables for\na straightforward generation of staff notation. For future\nwork, we intend to improve the transcription by includ-\ning instrument templates for the accompaniment instru-\nments,whichwillenableforabetterestimationofthemain\nmelody. Furthermore,weplantoconductauserstudywith\nethnomusicologists, who will evaluate the performance of\nour AMTsystem.536 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20166. REFERENCES\n[1] Music Information Retrieval Evaluation eX-\nchange (MIREX). http://music-ir.org/\nmirexwiki/ .\n[2] Website of the Crinnos project. http://crinnos.\nims.forth.gr . Accessed: 2016-03-16.\n[3] E. Benetos, S. Dixon, D. Giannoulis, H. Kirchhoff,\nand A. Klapuri. Automatic music transcription: chal-\nlenges and future directions. J. Intelligent Information\nSystems, 41(3):407–434, December 2013.\n[4] E. Benetos and T. Weyde. An efﬁcient temporally-\nconstrained probabilistic model for multiple-\ninstrument music transcription. In 16th International\nSociety for Music Information Retrieval Conference\n(ISMIR), pages 701–707, Malaga, Spain, October\n2015.\n[5] Emmanouil Benetos and Andre Holzapfel. Automatic\ntranscription of Turkish microtonal music. Journal of\nthe Acoustical Society of America , 138(4):2118–2130,\n2015.\n[6] A. T. Cemgil, H. J. Kappen, and D. Barber. A gen-\nerative model for music transcription. IEEE Transac-\ntions on Audio, Speech, and Language Processing ,\n14(2):679–694, 2006.\n[7] M. E. P. Davies, N. Degara, and M. D. Plumbley.\nEvaluationmethodsformusicalaudiobeattrackingal-\ngorithms. Technical Report C4DM-TR-09-06, Queen\nMary University of London, Centre for Digital Music,\n2009.\n[8] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maxi-\nmumlikelihoodfromincompletedataviatheEMalgo-\nrithm.JournaloftheRoyalStatisticalSociety ,39(1):1–\n38, 1977.\n[9] Z. Duan, B. Pardo, and C. Zhang. Multiple fundamen-\ntal frequency estimation by modeling spectral peaks\nand non-peak regions. IEEE Transactions on Audio,\nSpeech, and Language Processing , 18(8):2121–2133,\nNovember 2010.\n[10] Tuomas Eerola and Petri Toiviainen. MIDI Toolbox:\nMATLAB Tools for Music Research . University of\nJyv¨askyl¨a, Jyv¨askyl¨a, Finland, 2004.\n[11] Ter Ellingson. Transcription. In Helen Myers, editor,\nEthnomusicology: An Introduction , pages pp. 110–\n152. MacMillan, London, 1992.\n[12] V. Emiya, R. Badeau, and B. David. Multipitch esti-\nmationofpianosoundsusinganewprobabilisticspec-\ntral smoothness principle. IEEE Transactions on Au-\ndio, Speech, and Language Processing , 18(6):1643–\n1654, August 2010.[13] B. Fuentes, R. Badeau, and G. Richard. Harmonic\nadaptive latent component analysis of audio and ap-\nplication to music transcription. IEEE Transactions on\nAudio,Speech,andLanguageProcessing ,21(9):1854–\n1866, September 2013.\n[14] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWCmusicdatabase: musicgenredatabaseandmusi-\ncalinstrumentsounddatabase.In InternationalSympo-\nsium forMusic InformationRetrieval , October 2003.\n[15] Andre Holzapfel, Florian Krebs, and Ajay Srini-\nvasamurthy. Tracking the “odd”: Meter inference in a\nculturally diverse music corpus. In Proceedings of IS-\nMIR - International Conference on Music Information\nRetrieval, pages 425–430, Taipei, Taiwan, 2014.\n[16] R.MacraeandS.Dixon.Accuratereal-timewindowed\ntimewarping.In InternationalSocietyforMusicInfor-\nmation Retrieval Conference , pages 423–428, Utrecht,\nNetherlands, 2010.\n[17] Robert Reigle. Reconsidering the idea of timbre: A\nbrief history and new proposals. In MusiCult '14: Mu-\nsic and Cultural Studies Conference , pages 233–243,\n2014.\n[18] Haris Sarris, Tassos Kolydas, and Panagiotis\nTzevelekos. Parataxis: A framework of structure\nanalysis for instrumental folk music. Journal of\ninterdisciplinary musicstudies , 4(1):71–90, 2010.\n[19] C. Sch ¨orkhuber, A. Klapuri, N. Holighaus, and\nM. D¨orﬂer. A Matlab toolbox for efﬁcient perfect\nreconstruction time-frequency transforms with log-\nfrequency resolution. In AES 53rd Conference on Se-\nmantic Audio , page 8 pages, London, UK, January\n2014.\n[20] L.SuandY.-H.Yang.Escapingfromtheabyssofman-\nual annotation: New methodology of building poly-\nphonic datasets for automatic music transcription. In\nInt.Symp.ComputerMusicMultidisciplinaryResearch\n(CMMR), June2015.\n[21] E. Vincent, N. Bertin, and R. Badeau. Adaptive har-\nmonic spectral decomposition for multiple pitch esti-\nmation.IEEETransactionsonAudio,Speech,andLan-\nguage Processing , 18(3):528–537, March 2010.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 537"
    },
    {
        "title": "Bayesian Meter Tracking on Learned Signal Representations.",
        "author": [
            "Andre Holzapfel",
            "Thomas Grill"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417263",
        "url": "https://doi.org/10.5281/zenodo.1417263",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/132_Paper.pdf",
        "abstract": "Most music exhibits a pulsating temporal structure, known as meter. Consequently, the task of meter tracking is of great importance for the domain of Music Information Re- trieval. In our contribution, we specifically focus on Indian art musics, where meter is conceptualized at several hierar- chical levels, and a diverse variety of metrical hierarchies exist, which poses a challenge for state of the art analysis",
        "zenodo_id": 1417263,
        "dblp_key": "conf/ismir/HolzapfelG16",
        "content": "BAYESIAN METER TRACKING ON LEARNED SIGNAL\nREPRESENTATIONS\nAndre Holzapfel, Thomas Grill\nAustrian Research Institute for Artiﬁcial Intelligence (OFAI)\nandre@rhythmos.org, thomas.grill@ofai.at\nABSTRACT\nMost music exhibits a pulsating temporal structure, known\nasmeter . Consequently, the task of meter tracking is of\ngreat importance for the domain of Music Information Re-\ntrieval. In our contribution, we speciﬁcally focus on Indian\nart musics, where meter is conceptualized at several hierar-\nchical levels, and a diverse variety of metrical hierarchies\nexist, which poses a challenge for state of the art analysis\nmethods. To this end, for the ﬁrst time, we combine Con-\nvolutional Neural Networks (CNN), allowing to transcend\nmanually tailored signal representations, with subsequent\nDynamic Bayesian Tracking (BT), modeling the recurrent\nmetrical structure in music. Our approach estimates me-\nter structures simultaneously at two metrical levels. The\nresults constitute a clear advance in meter tracking per-\nformance for Indian art music, and we also demonstrate\nthat these results generalize to a set of Ballroom dances.\nFurthermore, the incorporation of neural network output\nallows a computationally efﬁcient inference. We expect\nthe combination of learned signal representations through\nCNNs and higher-level temporal modeling to be applicable\nto all styles of metered music, provided the availability of\nsufﬁcient training data.\n1. INTRODUCTION\nThe majority of musics in various parts of the world can be\nconsidered as metered, that is, their temporal organization\nis based on a hierarchical structure of pulsations at differ-\nent related time-spans. In Eurogenetic music, for instance,\none would refer to one of these levels as the beat or tactus\nlevel, and to another (longer) time-span level as the down-\nbeat, measure, or bar level. In Indian art musics, the con-\ncepts of t¯al.afor Carnatic and t¯alfor Hindustani music de-\nﬁne metrical structures that consist of several hierarchical\nAH is supported by the Austrian Science Fund (FWF: M1995-N31).\nTG is supported by the Vienna Science and Technology Fund\n(WWTF) through project MA14-018 and the Federal Ministry for Trans-\nport, Innovation & Technology (BMVIT, project TRP 307-N23).\nWe would like to thank Ajay Srinivasamurthy for advice and com-\nments. We also gratefully acknowledge the support of NVIDIA Corpora-\ntion with the donation of a Tesla K40 GPU used for this research.\nc/circlecopyrtAndre Holzapfel, Thomas Grill. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: Andre Holzapfel, Thomas Grill. “Bayesian meter tracking\non learned signal representations”, 17th International Society for Music\nInformation Retrieval Conference, 2016.levels. However, important differences between meter(s)\nin Eurogenetic and Indian art musics are the presence of\nnon-isochronicity in some of the metrical layers and the\nfact that an understanding of the progression of the meter\nis crucial for the appreciation of the listener, see, e.g. [3,\np. 199ff]. Again, other cultures might not explicitly deﬁne\nmetrical structure on several layers, but just deﬁne certain\nrhythmic modes that determine the length of a metrical cy-\ncle and some points of emphasis within this cycle, as is the\ncase for Turkish makam music [2] or Korean music [13].\nCommon to all metered musics is the fact that the under-\nstanding of only one metrical level, such as the beat in Eu-\nrogenetic music, leads to an inferior understanding of the\nmusical structure compared to an interpretation on several\nmetrical layers; a couple dancing a Ballroom dance with-\nout a common understanding of beat and bar level will end\nup with four badly bruised feet, while a whirling dervish in\nTurkey who does not follow the long-term structure of the\nrhythmic mode will suffer pain of a rather spiritual kind.\nWithin the ﬁeld of Music Information Research (MIR),\nthe task of beat tracking has been approached by many\nresearchers, using a large variety of methodologies, see\nthe summary in [14]. Tracking of meter, i.e., tracking\non several hierarchically related time-spans, was pursued\nby a smaller number of approaches, for instance by [9].\n[15] were among the ﬁrst to include experiments that doc-\nument the importance of adapting a model automatically\nto musical styles in the context of meter tracking. In recent\nyears, several approaches to beat and meter tracking were\ndeveloped that include such adaptation to musical style, for\ninstance by applying dynamic Bayesian networks [12] or\nConvolutional Neural Networks (CNN) [6] for meter track-\ning, or by combining Bayesian networks with Recurrent\nNeural Networks (RNN) for beat tracking in [1].\nIn this paper, we combine deep neural network and\nBayesian approaches for meter tracking. To this end, we\nadapt an approach based on CNN that was previously ap-\nplied to music segmentation with great success [18]. To\nthe best of our knowledge, no other applications of CNNs\nto the task of combined tracking at several metrical lev-\nels have yet been published, although other groups apply\nCNN as well [6]. In this paper, the outputs of the CNN, i.e.,\nthe activations that imply probabilities of observing beats\nand downbeats1, are then integrated as observations into a\ndynamic Bayesian network. This way, we explore in how\nfar an approach [18] previously applied to supra-metrical\n1We use these terms to denote the two levels, for the sake of simplicity.262Dance #Pieces Cycle Length: mean (std)\nCha cha (4/4) 111 1.96 (0.107)\nJive (4/4) 60 1.46 (0.154)\nQuickstep (4/4) 82 1.17 (0.018)\nRumba (4/4) 98 2.44 (0.274)\nSamba (4/4) 86 2.40 (0.177)\nTango (4/4) 86 1.89 (0.064)\nViennese Waltz (3/4) 65 1.01 (0.015)\nWaltz (3/4) 110 2.10 (0.077)\nTable 1 : The Ballroom dataset. The columns depict\ntime signature with the names of the dances, number of\npieces/excerpts, and mean and standard deviation of the\nmetrical cycle lengths in seconds.\nstructure in music can serve to perform meter tracking as\nwell. Furthermore, we want to evaluate in how far the\nmeter tracking performed by the CNN can be further im-\nproved by imposing knowledge of metrical structure that is\nexpressed using a Bayesian model. The evaluation in this\npaper is performed on Indian musics as well as Latin and\ninternational Ballroom dances. This choice is motivated by\nthe fact that meter tracking in Indian musics revealed to be\nparticularly challenging [8], but at the same time a novel\napproach should generalize to non-Indian musics. Our re-\nsults improve over the state of the art in meter tracking on\nIndian music, while results on Ballroom music are highly\ncompetitive as well.\nWe present the used music corpora in Section 2. Sec-\ntion 3 provides detail on the CNN structure and training,\nand Section 4 on the Bayesian model and its combination\nwith the CNN activations. In both sections we aim at pro-\nviding a concise presentation of both methods, emphasiz-\ning the novel elements compared to previously published\napproaches. Section 5 illustrates our ﬁndings, and Sec-\ntion 6 provides a summary and directions for future work.\n2. MUSIC CORPORA\nFor the evaluation of meter tracking performance, we use\ntwo different music corpora. The ﬁrst corpus consists of\n697 monaural excerpts ( fs= 44.1kHz) of Ballroom dance\nmusic, with a duration of 30 s for each excerpt. The cor-\npus was ﬁrst presented in [5], and beat and bar annotations\nwere compiled by [10]. Table 1 lists all the eight con-\ntained dance styles and their time signatures, and depicts\nthe mean durations of the metrical cycles and their stan-\ndard deviations in seconds. In general, the bar durations\ncan be seen to have a range from about a second (Viennese\nWaltz) to 2.44 s (Rumba), with small standard deviations.\nThe second corpus unites two collections of Indian art\nmusic that are outcomes of the ERC project CompMusic.\nThe ﬁrst collection, the Carnatic music rhythm corpus con-\ntains 176 performance recordings of South Indian Carnatic\nmusic, with a total duration of more than 16 hours.2The\nsecond collection, the Hindustani music rhythm corpus,\n2http://compmusic.upf.edu/carnatic-rhythm-datasetCarnatic\nT¯al.a #Pieces Cycle Length: mean (std)\nAdi (8/4) 50 5.34 (0.723)\nR¯upaka (3/4) 50 2.13 (0.239)\nMi´sra ch ¯apu (7/4) 48 2.67 (0.358)\nKhanda ch ¯apu (5/4) 28 1.85 (0.284)\nHindustani\nT¯al #Pieces Cycle Length: mean (std)\nTint¯al (16/4) 54 10.36 (9.875)\nEkt¯al (12/4) 58 30.20 (26.258)\nJhapt ¯al (10/4) 19 8.51 (3.149)\nR¯upak t ¯al (7/4) 20 7.11 (3.360)\nTable 2 : The Indian music dataset. The columns depict\ntime signature with the names of the T ¯al.a/t¯al cycles, the\nnumber of pieces/excerpts, and mean and standard devia-\ntion of the metrical cycle lengths in seconds.\ncontains 151 excerpts of 2 minutes length each, summing\nup to a total duration of a bit more than 5 hours.3All sam-\nples are monaural at fs= 44.1kHz. Within this paper we\nunite these two datasets to one corpus, in order to obtain a\nsufﬁcient amount of training data for the neural networks\ndescribed in Section 3. This can be justiﬁed by the similar\ninstrumental timbres that occur in these datasets. How-\never, we carefully monitor the differences of tracking per-\nformance for the two musical styles. As illustrated in Ta-\nble 2, metrical cycles in the Indian musics have longer du-\nrations with large standard deviations in most cases. This\ndifference is in particular accentuated for Hindustani mu-\nsic, where, for instance, the Ekt ¯al cycles range from 2.23 s\nup to a maximum of 69.73 s. This spans ﬁve tempo octaves\nand represents a challenge for meter tracking. The rhyth-\nmic elaboration of the pieces within a metrical class varies\nstrongly depending on the tempo, which is likely to create\ndifﬁculties when using the recordings in these classes for\ntraining one uniﬁed tracking model.\n3. CNN FOR METER TRACKING\nCNNs are feed-forward networks that include convolu-\ntional layers , computing a convolution of their input with\nsmall learned ﬁlter kernels of a given size. This allows\nprocessing large inputs with few trainable parameters, and\nretains the input’s spatial layout. When used for binary\nclassiﬁcation, the network usually ends in one or more\ndense layers integrating information over the full input at\nonce, discarding the spatial layout. The architecture for\nthis work is based on the one used by Ullrich et al. [18]\non MLS (Mel-scaled log-magnitude spectrogram) features\nfor their MIREX submission [16]. Therein, CNN-type net-\nworks have been employed for the task of musical structure\nsegmentation. [7] have expanded on this approach by intro-\nducing two separate output units, yielding predictions for\n‘ﬁne’ and ‘coarse’ segment boundaries. For the research\nat hand, we can use this architecture to train and predict\n3http://compmusic.upf.edu/hindustani-rhythm-datasetProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 263conv 8×6 (32×)pool 3×6\ndense (→ 512 units)\nclass info (n units)\nLLS (501×80)conv 6×3 (64×)\nbeat\n downbeat\ndense (→ 2 units)Figure 1 : The CNN architecture in use.\nbeats and downbeats in the same manner with two output\nunits, enabling the network to exploit information shared\nbetween these two temporal levels.\n3.1 Data\nFor both datasets under examination, we use a train/\nvalidation/test split. The sizes are 488/70/140 for the ball-\nroom data set and 228/33/66 for the combination of the two\nIndian data sets. From the audio ﬁles, we compute log-\nscaled logarithmic-magnitude spectrograms (LLS) of 80\nbands (instead of mel-scaled MLS in [18]), ranging from\n80 Hz to 16 kHz. We have found log-scaled features to\nwork better in early stages of research, most probably be-\ncause of their harmonic translational invariance, support-\ning the convolutional ﬁlters. The STFT size used is 2048,\nwith a frame rate of 100 fps. In order to be able to train\nand predict on spectrogram excerpts near the beginning\nand ending of a music piece, we apply a simple padding\nstrategy for the LLS features. If the ﬁrst (or last, respec-\ntively) non-zero spectrogram frame has a mean volume of\n≥− 40 dBFS, we assume an abrupt boundary and pad the\nspectrogram with a −100 dBFS constant. Conversely, we\npad with repeated copies of this ﬁrst or last non-zero spec-\ntrogram frame. To either padding, we add ±3 dB of uni-\nform noise to avoid unnatural spectral clarity. Over the\nentire data sets, we normalize to zero mean and unit vari-\nance for each frequency band, yielding a suitable range of\ninput values for the CNN.\n3.2 Network Structure and training\nFigure 1 shows the network architecture used for our ex-\nperiments, unchanged from our previous experiments in\n[18]. On the input side, the CNN sees a temporal win-\ndow of 501 frames with 80 frequency bands, equivalent\nto 5 seconds of spectral information. The LLS input is\nsubjected to a convolutional layer of 32 parallel 8×6ker-\nnels (8 time frames and 6 frequency bands), a max-pooling\nlayer with pooling factors of 3×6, and another convo-\nlution of 64 parallel 6×3kernels. Both convolutional\nlayers employ linear rectiﬁer units. While the ﬁrst con-\nvolution emphasizes certain low-level aspects of the time-\nfrequency patches it processes (for example the contrastbetween patches), the subsequent pooling layer spatially\ncondenses both dimensions. This effectively expands the\nscope with regard to the input features for the second con-\nvolution. The resulting learned features are fed into a dense\nlayer of 512 sigmoid units encoding the relevance of indi-\nvidual feature components of the time-frequency window\nand the contribution of individual convolutional ﬁlters. Fi-\nnally, the network ends in a dense output layer with two\nsigmoid units. Additionally, the class information (Indian\nt¯al.a/t¯al class or ballroom style class, which can generally\nbe assumed to be known) is fed through one-hot coding di-\nrectly to the ﬁrst dense layer. Using this class information\nimproves results in the range of 1–2%.\nDuring training, the beat and downbeat units are tied to\nthe target information from the ground-truth annotations\nusing a binary cross-entropy loss function. The targets are\nset to one with a tolerance window of 5 frames, equiva-\nlent to 50 milliseconds, around the exact location of the\nbeat or downbeat. Training weights decline according to a\nGaussian window around this position (‘target smearing’).\nTraining is done by mini-batch stochastic gradient descent,\nusing the same hyper-parameters and tweaks as in [18].\nThe dense layers use dropout learning, updating only 50%\nof the weights per training step.\n3.3 Beat and downbeat prediction\nIn order to obtain beat and downbeat estimations from a\ntrained CNN, we follow the basic peak-picking strategy\ndescribed in [18] to retrieve likely boundary locations from\nthe network output. Note that the class information is pro-\nvided in the same way as in the training, which means\nthat we assume the meter type (e.g., 7/4) known, and tar-\nget the tracking of the given metrical hierarchy. The ad-\njustable parameters for peak picking have been optimized\non the validation set. Several individual network mod-\nels have been trained individually from random initializa-\ntions, yielding slightly different predictions. Differently\nthan in [18] we did not ‘bag’ (that is, average) multiple\nmodels, but rather selected the model with the best results\nas evaluated on the validation set. Although the results di-\nrectly after peak picking are inferior to bagged models by\nup to 3%, the Bayesian post-processing works better on\nnon-averaged network outputs, as also tested on the val-\nidation set. The CNN output vectors that represents the\nbeat probability will be referred to as P(b), and the vector\nrepresenting the downbeat probabilities as P(d), respec-\ntively. The results obtained from the peak picking on these\nvectors will be denoted as CNN-PP .\n4. METER TRACKING USING BAYESIAN\nNETWORKS\nThe Bayesian network used for meter tracking is an ex-\ntension of the model presented in [11]. Within the model\nin [11], activations from RNN were used as observations in\na Bayesian network for beat tracking in music, whereas in\nthis paper we extend the approach to the tracking of a met-\nrical cycle. We will shortly summarize the principle of the264 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016algorithm presented in [11] in Section 4.1. In Section 4.2,\nwe present the extension of the existing approach to meter\ntracking using activations from a CNN.\n4.1 Summary: A Bayesian meter tracking model\nThe underlying concept of the approach presented in [11]\nis an improvement of [8], and was ﬁrst described by [19]\nas the bar pointer model. In [11], given a series of obser-\nvations/features yk, with k∈{1, ..., K}, computed from\na music signal, a set of hidden variables xkis estimated.\nThe hidden variables describe at each analysis frame kthe\nposition Φkwithin a beat (in the case of beat tracking) or\nwithin a bar (in the case of meter tracking), and the tempo\nin positions per frame ( ˙Φk). The goal is to estimate the\nhidden state sequence that maximizes the posterior (MAP)\nprobability P(x1:K|y1:K). If we express the temporal dy-\nnamics as a Hidden Markov Model (HMM), the posterior\nis proportional to\nP(x1:K|y1:K)∝P(x1)K/productdisplay\nk=2P(xk|xk−1)P(yk|xk)(1)\nIn (1), P(x1)is the initial state distribution ,\nP(xk|xk−1)is the transition model , and P(yk|xk)is the\nobservation model . When discretizing the hidden variable\nxk= [Φ k,˙Φk], the inference in this model can be per-\nformed using the Viterbi algorithm. In this paper, for the\nsake of simplicity of representation we do not apply ap-\nproximate inference, as for instance in [17], but strictly\nfollow the approach in [11].\nIn [11], efﬁciency of the inference was improved by\na ﬂexible sampling of the hidden variables. The position\nvariable Φktakes M(T)values 1,2, ..., M (T), with\nM(T) =round/parenleftbiggNbeats∗60\nT∗∆/parenrightbigg\n(2)\nwhere Tdenotes the tempo in beats per minute (bpm), and\n∆the analysis frame duration in seconds. In the case of\nmeter tracking, Nbeats denotes the number of beats in a\nmeasure (e.g., nine beats in a 9/8), and is set to 1 in the\ncase of beat tracking. This sampling results in one posi-\ntion state per analysis frame. The discretized tempo states\n˙Φkwere distributed logarithmically between a minimum\ntempo Tminand a maximum tempo Tmax.\nAs in [11], a uniform initial state distribution P(x1)was\nchosen in this paper. The transition model factorizes into\ntwo components according to\nP(xk|xk−1) =P(Φk|Φk−1,˙Φk−1)P(˙Φk|˙Φk−1)(3)\nwith the two components describing the transitions of posi-\ntion and tempo states, respectively. The position transition\nmodel increments the value of Φkdeterministically by val-\nues depending on the tempo ˙Φk−1, starting from a value\nof 1 (at the beginning of a metrical cycle) to a value of\nM(T). The tempo transition model allows for tempo tran-\nsitions according to an exponential distribution in exactly\nthe same way as described in [11].We incorporated the GMM-BarTracker (GMM-BT ) as\ndescribed in [11] as a baseline in our paper. The observa-\ntion model in the GMM-BarTracker divides a whole note\ninto 64 discrete bins, using the beat and downbeat anno-\ntations that are available for the data. For instance, a 5/4\nmeter would be divided into 80 metrical bins, and we de-\nnote this number of bins within a speciﬁc meter as Nbins.\nSpectral-ﬂux features obtained from two frequency bands,\ncomputed as described in [12], are assigned to one of these\nmetrical bins. Then, the parameters of a two-component\nGaussian Mixture Model (GMM) are determined in ex-\nactly the same way as documented in [12], using the same\ntraining data as for the training of the CNN in Section 3.1.\nFurthermore, the fastest and the slowest pieces were used\nto determine the tempo range TmintoTmax. A constant\nnumber of 30 tempo states were used, a denser sampling\ndid not improve tracking on any of the validation sets.\n4.2 Extension of the Bayesian network: CNN\nobservations\nThe proposed extensions of the GMM-BT approach af-\nfect the observation model P(yk|xk), as well as the\nparametrization of the state space. We will refer to this\nnovel model as CNN-BT .\nRegarding the observation model, we incorporate the\nbeat and downbeat probabilities P(b)andP(d), respec-\ntively, obtained from the CNN as described in Section 3.\nNetwork activations were incorporated in [11] on the beat\nlevel only, and in this paper our goal is to determine in how\nfar the downbeat probabilities can help to obtain an accu-\nrate tracking not only of the beat, but the entire metrical cy-\ncle. Let us denote the metrical bins that are beat instances\nbyB(excluding the downbeat), and the downbeat position\nasD. Then we calculate the observation model P(yk|xk)\nas follows\nP(yk|xk)=\n\nPk(d)∗Pk(b), Φk∈D,D+1;\nPk(b)∗(1−Pk(d)) Φ k∈B,B+1;\n(1−Pk(b))∗(1−Pk(d))else;\n(4)\nIncluding the bin that follows a beat and downbeat was\nfound to slightly improve the performance on the evalua-\ntion data. In simple terms, the network outputs P(b)and\nP(d)are directly plugged into the observation model. The\ntwo separate probabilities for beats and downbeats com-\nbined according to the metrical bin. For instance, down-\nbeats are also instances of the beat layer, and at these posi-\ntions the activities are multiplied in the ﬁrst row of (4).\nThe columns of the obtained observation matrix of size\nNbins×Kare then normalized to sum to one.\nThe CNN activations P(b)andP(d)are characterized\nby clearly accentuated peaks in the vicinity of beats and\ndownbeats, as will be illustrated in Section 5. We take\nadvantage of this property in order to restrict the num-\nber of possible tempo hypotheses ˙Φkin the state space\nof the model. To this end, the autocorrelation function\n(ACF) of the beat activation function P(b)is computed,\nand the highest peak at tempi smaller than 500 bpm is de-\ntermined. This peak serves as an initial tempo hypothesisProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 265T0, and we deﬁne Tmin=0.4∗T0andTmax=2.2∗T0, in or-\nder to include half and double tempo as potential tempo\nhypotheses into the search space. Then we determine the\npeaks of the ACF in that range, and if their number is\nhigher than 5, we choose the 5 highest peaks only. This\nway we obtain Nhyptempo hypotheses, covering T0, its\nhalf and double value (in case the ACF has peaks at these\nvalues), as well as possible secondary tempo hypotheses.\nThese peaks are then used to determine the number of po-\nsition variables at these tempi according to (2). In order\nto allow for tempo changes around these modes, we in-\nclude for a mode Tn,n∈{1,...,N hyp}, all tempi related to\nM(Tn)−3,M(Tn)−2,...,M (Tn)+3. This means that for\neach of the Nhyptempo modes we use seven tempo sam-\nples with the maximum possible accuracy at a given anal-\nysis frame rate ∆, resulting in a total of at most 35 tempo\nstates (for Nhyp=5). Using more modes or more tempo\nsamples per mode did not result in higher accuracy on the\nvalidation data. While this focused tempo space has not\nbeen observed to lead to large improvements over a log-\narithmic tempo distribution between TminandTmax, the\nmore important consequence is a more efﬁcient inference.\nAs will be shown in Section 5, metrically simple pieces are\ncharacterized by only 2 peaks in the ACF between Tmin\nandTmax, which leads to a reduction of the state space\nsize by more than 50% over the GMM-BT.\n5. SYSTEM EVALUATION\n5.1 Evaluation measures\nWe use three evaluation measures in this paper [4]. For F-\nmeasure (0% to 100%), estimations are considered accu-\nrate if they fall within a ±70 ms tolerance window around\nannotations. Its value is measured as a function of the num-\nber of true and false positives and false negatives. AMLt\n(0% to 100%) is a continuity-based method, where beats\nare accurate when consecutive beats fall within tempo-\ndependent tolerance windows around successive annota-\ntions. Beat sequences are also accurate if the beats oc-\ncur on the off-beat, or are at double or half the annotated\ntempo. Finally, Information Gain (InfG) (0 bits to ap-\nproximately 5.3 bits) is determined by calculating the tim-\ning errors between an annotation and all beat estimations\nwithin a one-beat length window around the annotation.\nThen, a beat error histogram is formed from the resulting\ntiming error sequence. A numerical score is derived by\nmeasuring the K-L divergence between the observed error\nhistogram and the uniform case. This method gives a mea-\nsure of how much information the beats provide about the\nannotations.\nWhereas the F-measure does not evaluate the con-\ntinuity of an estimation, the AMLt and especially the\nInfG measure penalize random deviations from a more\nor less regular underlying beat pulse. Because it is not\nstraight-forward to apply such regularity constraints on the\ndownbeat level, downbeat evaluation is done using the F-\nmeasure only, denoting the F-measure at the downbeat and\nbeat levels as F(d)andF(b), respectively.Evaluation Measure F(d)F(b)AMLt InfG\nCNN-PP 54.29 75.15 60.80 1.820\nGMM-BT 63.84 77.00 74.92 1.942\nCNN-BT 69.93 80.75 87.46 2.314\nCNN-BT ( Tann) 73.63 85.27 89.22 2.499\nTable 3 : Results on Indian music.\nEvaluation Measure F(d)F(b)AMLt InfG\nCNN-PP 79.30 93.59 88.98 3.216\nGMM-BT 77.51 90.67 91.45 2.961\nCNN-BT 89.63 93.74 93.89 3.244\nCNN-BT ( Tann) 90.67 94.81 94.25 3.240\nTable 4 : Results on Ballroom music.\n5.2 Results\nResults are presented separately for the Indian and the\nBallroom datasets in Tables 3 and 4, respectively. The ﬁrst\ntwo columns represent F-scores for downbeats ( F(d)) and\nbeats ( F(b)), followed by AMLt and InfG. We evaluated\nCNNs with subsequent peak-picking on the network acti-\nvations (CNN-PP) as explained in Section 3, the Bayesian\nnetwork from [11] using Spectral Flux in its observation\nmodel (GMM-BT), and the Bayesian network that incor-\nporates the novel observation model obtained from CNN\nactivations (CNN-BT). Bold numbers indicate signiﬁcant\nimprovement of CNN-BT over CNN-PP, underlining in-\ndicates signiﬁcant improvement of CNN-BT over GMM-\nBT. Paired-sample t-tests were performed with a 5%sig-\nniﬁcance level. Performing a statistical test over both cor-\npora reveals a signiﬁcant improvement by CNN-BT over\nCNN-PP for all measures, and for F(d)and AMLt over\nGMM-BT. These results demonstrate that beat and down-\nbeat estimations obtained from a CNN can be further im-\nproved using a Bayesian model that incorporates hypothe-\nses about metrical regularity and the dynamic development\nof tempo. On the other hand, employing CNN activations\nyields signiﬁcant improvements over the Bayesian model\nthat incorporates hand-crafted features (Spectral Flux).\nFigure 2 visualizes the improvement of CNN-BT over\nCNN-PP by depicting the network outputs along with\nreference annotations, and beat and downbeat estima-\ntions from CNN-BT and CNN-PP. It is apparent that\nthe Bayesian network ﬁnds a consistent path through the\npieces that is supported by the network activations as well\nas by the underlying regular metrical structure. Both ﬁg-\nures depict examples of Carnatic Adi t ¯al.a, which has a\nsymmetric structure that caused tempo halving/doubling\nerrors when using spectral ﬂux features as in GMM-\nBT [8]. In Figure 2a, the spectrogram, especially in the ﬁrst\ntwo depicted cycles, is characterized by a similar melodic\nprogression that marks the cycle. The CNN is able to cap-\nture such regularities, leading to an improved performance.\nIn Figure 2b, the music provides no clear metrical cues in\nthe beginning, but the output of the CNN-BT can be seen\nto be nicely synchronized from the third cycle on (at about\n8 s), demonstrating the advantage of the regularity imposed\nby the Bayesian network.266 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 201610 15 20 25 30 35 4001020304050607080frequency bin\n10 15 20 25 30 35 400.00.20.40.60.81.0beat prob.\n10 15 20 25 30 35 40\ntime (seconds)0.00.20.40.60.81.0downbeat prob.10007_2-01_Anandamruta_Karshini(a) Indian music example 1\n0 5 10 15 20 25 3001020304050607080frequency bin\n0 5 10 15 20 25 300.00.20.40.60.81.0beat prob.\n0 5 10 15 20 25 30\ntime (seconds)0.00.20.40.60.81.0downbeat prob.10043_1_Jalajakshi_Varnam\n(b) Indian music example 2\nFigure 2 :Input LLS features and network outputs for beat (upper curve) and downbeat (lower curve) predictions for two music\nexamples. Ground-truth positions as green vertical marks on top, peak-picking thresholds as red dotted lines, picked peaks from the\nCNN-PP as blue circle markers, and ﬁnal predictions by the Bayesian tracking (CNN-BT) as red vertical marks on the bottom.\nCorpus Ballroom Carnatic Hindustani\nCorrect tempo (%) 97.1 100 81.8\nACF-peaks 2.67 3.60 4.15\nTable 5 : Some characteristics of the focused state space\nin CNN-BT. The ﬁrst row depicts the percentage of pieces\nfor which the true tempo was between Tmin=0.4∗T0to\nTmax=2.2∗T0that was selected using the autocorrelation\nfunction (ACF) of P(b). The second row depicts the num-\nber of peaks in the ACF in the selected tempo range.\nIn Table 5, we depict some characteristics of the tempo\nstates that are chosen in the CNN-BT, as described in Sec-\ntion 4.2. We depict the Carnatic and Hindustani musics\nseparately in order to illustrate differences. It can be seen\nthat the true tempo is almost always in the chosen range\nfromTmintoTmax for Ballroom and Carnatic music, but\ndrops to 81.8%for Hindustani music. Furthermore, the\nnumber of peaks in the ACF of P(b)is lowest for the\nBallroom corpus, while the increased number for the Hin-\ndustani music indicates an increased metrical complexity\nfor this style. Indeed, the performance values are gener-\nally lower for Hindustani musics than for Carnatic musics,\nwith, for instance, the downbeat F-measure F(d)being\n0.76 for Carnatic, and 0.64 for Hindustani musics. This is\nto some extent related to the extremely low tempi that oc-\ncur in Hindustani music, which cause the incorrect tempo\nranges for Hindustani depicted in Table 5.\nThe last rows in Tables 3 and 4 depict the performance\nthat is achieved when the correct tempo Tannis given in\nCNN-BT. To do this evaluation, we use 30 logarithmically-\nspaced tempo coefﬁcients in a range of ±20% aroundTann, in order to allow for gradual tempo changes, exclud-\ning, however, double and half tempo. For the Ballroom\ncorpus, only marginal improvement can be observed, with\nnone of the changes compared to the non-informed CNN-\nBT case being signiﬁcant. For the Indian data the improve-\nment is larger, however, again not signiﬁcantly. This illus-\ntrates that even a perfect tempo estimation cannot further\nimprove the results. The reasons for this might be, espe-\ncially for Hindustani music, the large variability within the\ndata due to the huge tempo ranges. The CNNs are not able\nto track pieces at extreme slow tempi, due to their limited\ntemporal horizon of 5 seconds – slightly shorter than the\nbeat period in the slowest pieces. However, further increas-\ning this horizon was found to generally deteriorate the re-\nsults, due to more network weights to learn with the same,\nlimited amount of training data.\n6. DISCUSSION\nIn this paper, we have combined CNNs and Bayesian net-\nworks for the ﬁrst time in the context of meter tracking.\nResults clearly indicate the advantage of this combina-\ntion that results from the ﬂexible signal representations ob-\ntained from CNNs with the knowledge of metrical progres-\nsion incorporated into a Bayesian model. Furthermore, the\nclearly accentuated peaks in the CNN activations enable us\nto restrict the state space in the Bayesian model to certain\ntempi, thus reducing computational complexity depending\non the metrical complexity of the musical signal. Limita-\ntions of the approach can be seen in the ability to track very\nlong metrical structures in Hindustani music. To this end,\nthe incorporation of RNN will be evaluated in the future.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 2677. REFERENCES\n[1] Sebastian B ¨ock, Florian Krebs, and Gerhard Widmer.\nA multi-model approach to beat tracking considering\nheterogeneous music styles. In Proceedings of the In-\nternational Society for Music Information Retrieval\nConference (ISMIR) , pages 602–607, Taipei, Taiwan,\n2014.\n[2] Baris Bozkurt, Ruhi Ayangil, and Andre Holzapfel.\nComputational analysis of makam music in Turkey:\nReview of state-of-the-art and challenges. Journal for\nNew Music Research , 43(1):3–23, 2014.\n[3] Martin Clayton. Time in Indian Music : Rhythm , Metre\nand Form in North Indian Rag Performance . Oxford\nUniversity Press, 2000.\n[4] M. E. P. Davies, N. Degara, and M. D. Plumbley.\nEvaluation methods for musical audio beat tracking al-\ngorithms. Technical Report C4DM-TR-09-06, Queen\nMary University of London, Centre for Digital Music,\n2009.\n[5] S. Dixon, F. Gouyon, and G. Widmer. Towards charac-\nterisation of music via rhythmic patterns. In Proceed-\nings of International Conference on Music Information\nRetrieval , pages 509–516, 2004.\n[6] Simon Durand, Juan Pablo Bello, Bertrand David, and\nGa el Richard. Feature adapted convolutional neural\nnetworks for downbeat tracking. In Proceedings of the\nIEEE International Conference on Acoustics, Speech,\nand Signal Processing. ICASSP , 2016.\n[7] Thomas Grill and Jan Schl ¨uter. Music Boundary De-\ntection Using Neural Networks on Combined Features\nand Two-Level Annotations. In Proceedings of the In-\nternational Society for Music Information Retrieval\nConference (ISMIR) , Malaga, Spain, 2015.\n[8] Andre Holzapfel, Florian Krebs, and Ajay Srini-\nvasamurthy. Tracking the “odd”: Meter inference in a\nculturally diverse music corpus. In Proceedings of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pages 425–430, Taipei, Taiwan,\n2014.\n[9] A. P. Klapuri, A. J. Eronen, and J. T. Astola. Analysis\nof the meter of acoustic musical signals. IEEE Trans-\nactions on Audio, Speech and Language Processing ,\n14(1):342–355, 2006.\n[10] Florian Krebs, Sebastian B ¨ock, and Gerhard Widmer.\nRhythmic pattern modeling for beat- and downbeat\ntracking in musical audio. In Proceedings of the Inter-\nnational Society for Music Information Retrieval Con-\nference (ISMIR) , Curitiba, Brazil, 2013.\n[11] Florian Krebs, Sebastian B ¨ock, and Gerhard Widmer.\nAn efﬁcient state-space model for joint tempo and me-\nter tracking. In Proceedings of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , Malaga, Spain, 2015.[12] Florian Krebs, Andre Holzapfel, Ali Taylan Cemgil,\nand Gerhard Widmer. Inferring metrical structure in\nmusic using particle ﬁlters. IEEE Transactions on Au-\ndio, Speech and Language Processing , 23(5):817–827,\n2015.\n[13] Donna Lee Kwon. Music in Korea : experiencing mu-\nsic, expressing culture . Oxford University Press, 2011.\n[14] Meinard M ¨uller, Daniel P. W. Ellis, Anssi Klapuri, and\nGa¨el Richard. Signal processing for music analysis. J.\nSel. Topics Signal Processing , 5(6):1088–1110, 2011.\n[15] Geoffroy Peeters and Helene Papadopoulos. Simulta-\nneous beat and downbeat-tracking using a probabilistic\nframework: Theory and large-scale evaluation. IEEE\nTransactions on Audio, Speech and Language Process-\ning, 19(6):1754–1769, 2011.\n[16] Jan Schl ¨uter, Karen Ullrich, and Thomas Grill. Struc-\ntural segmentation with convolutional neural networks\nmirex submission. In Tenth running of the Music Infor-\nmation Retrieval Evaluation eXchange (MIREX 2014) ,\n2014.\n[17] Ajay Srinivasamurthy, Andre Holzapfel, Ali Taylan\nCemgil, and Xavier Serra. Particle ﬁlters for efﬁcient\nmeter tracking with dynamic bayesian networks. In\nProceedings of the International Society for Music\nInformation Retrieval Conference (ISMIR) , Malaga,\nSpain, 2015.\n[18] Karen Ullrich, Jan Schl ¨uter, and Thomas Grill. Bound-\nary Detection in Music Structure Analysis using Con-\nvolutional Neural Networks. In Proceedings of the In-\nternational Society for Music Information Retrieval\nConference (ISMIR) , Taipei, Taiwan, 2014.\n[19] N. Whiteley, A. T. Cemgil, and S. J. Godsill. Bayesian\nmodelling of temporal structure in musical audio. In\nProceedings of International Conference on Music In-\nformation Retrieval , Victoria, Canada, 2006.268 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Minimax Viterbi Algorithm for HMM-Based Guitar Fingering Decision.",
        "author": [
            "Gen Hori",
            "Shigeki Sagayama"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417639",
        "url": "https://doi.org/10.5281/zenodo.1417639",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/285_Paper.pdf",
        "abstract": "Previous works on automatic fingering decision for string instruments have been mainly based on path optimization by minimizing the difficulty of a whole phrase that is typ- ically defined as the sum of the difficulties of moves re- quired for playing the phrase. However, from a practical viewpoint of beginner players, it is more important to min- imize the maximum difficulty of a move required for play- ing the phrase, that is, to make the most difficult move easier. To this end, we introduce a variant of the Viterbi algorithm (termed the “minimax Viterbi algorithm”) that finds the path of the hidden states that maximizes the min- imum transition probability (not the product of the transi- tion probabilities) and apply it to HMM-based guitar fin- gering decision. We compare the resulting fingerings by the conventional Viterbi algorithm and our proposed min- imax Viterbi algorithm to show the appropriateness of our new method.",
        "zenodo_id": 1417639,
        "dblp_key": "conf/ismir/HoriS16",
        "content": "MINIMAX VITERBI ALGORITHM FOR\nHMM-BASED GUITAR FINGERING DECISION\nGen Hori\nAsia University / RIKEN\nhori@brain.riken.jpShigeki Sagayama\nMeiji University\nsagayama@meiji.ac.jp\nABSTRACT\nPrevious works on automatic ﬁngering decision for string\ninstruments have been mainly based on path optimization\nby minimizing the difﬁculty of a whole phrase that is typ-\nically deﬁned as the sum of the difﬁculties of moves re-\nquired for playing the phrase. However, from a practical\nviewpoint of beginner players, it is more important to min-\nimize the maximum difﬁculty of a move required for play-\ning the phrase, that is, to make the most difﬁcult move\neasier. To this end, we introduce a variant of the Viterbi\nalgorithm (termed the “minimax Viterbi algorithm”) that\nﬁnds the path of the hidden states that maximizes the min-\nimum transition probability (not the product of the transi-\ntion probabilities) and apply it to HMM-based guitar ﬁn-\ngering decision. We compare the resulting ﬁngerings by\nthe conventional Viterbi algorithm and our proposed min-\nimax Viterbi algorithm to show the appropriateness of our\nnew method.\n1. INTRODUCTION\nMost string instruments have overlaps in pitch ranges of\ntheir strings. As a consequence, such string instruments\nhave more than one way to play even a single note (ex-\ncept the highest and the lowest notes that are covered only\nby a single string) and thus numerous ways to play a whole\nsong. That is why the ﬁngering decision for a given song is\nnot always an easy task for string players and therefore au-\ntomatic ﬁngering decision has been attempted by many re-\nsearchers. Previous works on automatic ﬁngering decision\nhave been mainly based on path optimization by minimiz-\ning the difﬁculty level of a whole phrase that is typically\ndeﬁned as the sum or the product of the difﬁculty levels\ndeﬁned for each move. (The product of difﬁculty levels\neasily reduces to the sum of the logarithm of the difﬁculty\nlevels and therefore the sum and the product do not make\nany essential difference.) However, whether a string player\ncan play a passage using a speciﬁc ﬁngering depends al-\nmost only on whether the most difﬁcult move included in\nthe ﬁngering is playable. Especially, from a practical view-\npoint of beginner players, it is most important to minimize\nc/circlecopyrtGen Hori, Shigeki Sagayama. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: Gen Hori, Shigeki Sagayama. “Minimax Viterbi algorithm for\nHMM-based Guitar ﬁngering decision”, 17th International Society for\nMusic Information Retrieval Conference, 2016.the maximum difﬁculty level of a move included in a ﬁn-\ngering, that is, to make the most difﬁcult move easier.\nThe purpose of this paper is to introduce a variant of\nthe Viterbi algorithm [12] termed the “minimax Viterbi al-\ngorithm” that ﬁnds the sequence of the hidden states that\nmaximizes the minimum transition probability on the se-\nquence (not the product of all the transition probabilities\non the sequence) and apply it to HMM-based guitar ﬁnger-\ning decision. We employ a hidden Markov model (HMM)\nwhose hidden states are left hand forms of guitarists and\noutput symbols are musical notes, and perform ﬁngering\ndecision by solving a decoding problem of HMM using\nour proposed minimax Viterbi algorithm for ﬁnding the se-\nquence of hidden states with the maximum minimum tran-\nsition probability. Because the transition probabilities are\nset to large for easy moves and small for difﬁcult ones,\nresulting ﬁngerings “make the most difﬁcult move easier”\nas previously discussed in this section. To distinguish the\noriginal Viterbi algorithm and our variant, we refer to the\nformer as the “conventional Viterbi algorithm” and to the\nlatter as the “minimax Viterbi algorithm” throughout the\npaper.\nAs for automatic ﬁngering decision, several attempts\nhave been made in the last two decades. Sayegh [10]\nﬁrst formulated ﬁngering decision of string instruments as\na problem of path optimization. Radicioni et al. [8] ex-\ntended Sayegh [10]’s approach by introducing segmenta-\ntion of musical phrase. Radisavljevic and Driessen [9] in-\ntroduced a gradient descent search for the coefﬁcients of\nthe cost function for path optimization. Tuohy and Pot-\nter [11] ﬁrst applied the genetic algorithm (GA) to guitar\nﬁngering decision and arrangements. As for applications\nof HMM to ﬁngering decision, Hori et al. [4] applied input-\noutput HMM [2] to guitar ﬁngering decision and arrange-\nment, Nagata et al. [5] applied HMM to violin ﬁngering\ndecision, and Nakamura et al. [6] applied merged-output\nHMM to piano ﬁngering decision. Comparing to those pre-\nvious works, the present work is new in that it introduces\n“minimax paradigm” to automatic ﬁngering decision.\nThe rest of the paper is organized as follows. Sec-\ntion 2 recalls the conventional Viterbi algorithm and in-\ntroduces our proposed minimax Viterbi algorithm. Section\n3 introduces a framework of HMM-based ﬁngering deci-\nsion for monophonic guitar phrases. Section 4 applies the\nminimax Viterbi algorithm to ﬁngering decision for mono-\nphonic guitar phrases and evaluates the results. Section 5\nconcludes the paper and discusses related future works.4482. MINIMAX VITERBI ALGORITHM\nWe start by introducing our newly proposed “minimax\nViterbi algorithm” on which we build our ﬁngering deci-\nsion method in the following section. First of all, we recall\nthe deﬁnition of HMM1and the procedure of the conven-\ntional Viterbi algorithm for ﬁnding the sequence of hidden\nstates that gives the maximum likelihood. Next, we mod-\nify the algorithm to our new one for ﬁnding the sequence of\nhidden states that gives the maximum minimum transition\nprobability.\n2.1 Hidden Markov model (HMM)\nSuppose that we have two ﬁnite sets of hidden states Qand\noutput symbols O,\nQ={q1,q2,...,q N},\nO={o1,o2,...,o K},\nand two sequences of random variables Xof hidden states\nandYof output symbols,\nX= (X1,X2,...,X T),\nY= (Y1,Y2,...,Y T),\nthen a hidden Markov model Mis deﬁned by a triplet\nM= (A,B,π )\nwhereAis anN×Nmatrix of the transition probabilities,\nA= (aij), aij≡a(qi,qj)≡P(Xt=qj|Xt−1=qi),\nBanN×Kmatrix of the output probabilities,\nB= (bik), bik≡b(qi,ok)≡P(Yt=ok|Xt=qi),\nandΠanN-dimensional vector of the initial distribution\nof hidden states,\nΠ = (πi), πi≡π(qi)≡P(X1=qi).\n2.2 Conventional Viterbi algorithm\nWhen we observe a sequence of output symbols2\ny= (y1,y2,...,y T)\nfrom a hidden Markov model M, we are interested in the\nsequence of hidden states\nx= (x1,x2,...,x T)\nthat generates the observed sequence of output symbols y\nwith the maximum likelihood,\nˆxML= arg max\nxP(y,x|M)\n= arg max\nxP(x|M)P(y|x,M)\n= arg max\nxT/productdisplay\nt=1(a(xt−1,xt)b(xt,yt)) (1)\n1See [7] for more tutorial on HMM and its applications.\n2According to the conventional notation of the probability theory, we\ndenote random variables by uppercase letters and corresponding realiza-\ntions by lowercase letters.where we write a(x0,x1) =π(x1)for convenience. The\nproblem of ﬁnding the maximum likelihood sequence ˆxML\nis called “decoding problem.” Although an exhaustive\nsearch requires iterations over the NTpossible sequences,\nwe can solve the problem efﬁciently using the Viterbi al-\ngorithm [12] based on dynamic programming (DP), which\nuses twoN×Ttables ∆ = (δit)of maximum likelihood\nandΨ = (ψit)of back pointers and the following four\nsteps.\nInitialization initializes the ﬁrst columns of the two tables\n∆andΨusing the following formulae for i= 1,2,...,N ,\nδi1=πib(qi,y1),\nψi1= 0.\nRecursion ﬁlls out the rest columns of ∆andΨusing the\nfollowing recursive formulae for j= 1,2,...,N andt=\n1,2,...,T−1,\nδj,t+1= max\ni(δitaij)b(qj,yt+1),\nψj,t+1= arg max\ni(δitaij).\nTermination ﬁnds the index of the last hidden state of the\nmaximum likelihood sequence ˆxMLusing the last column\nof the table ∆,\niT= arg max\niδiT.\nBacktracking tracks the indices of the hidden states of the\nmaximum likelihood sequence ˆxMLfrom the last to the\nﬁrst using the back pointers of Ψfort=T,T−1,..., 2,\nit−1=ψit,t\nfrom which ˆxMLis obtained as\nxt=qit(t= 1,2,...,T ).\n2.3 Modiﬁcation for minimax Viterbi algorithm\nNext, we consider the problem of ﬁnding the sequence of\nhidden states xwith the maximum minimum transition\nprobability3,\nˆxMM= arg max\nxmin\n1≤t≤T(a(xt−1,xt)b(xt,yt)), (2)\nwhich we call “minimax decoding problem4.” A naive\napproach to the problem is an exhaustive search, that is,\nto enumerate all the sequences of the Nhidden states and\nthe lengthT, calculate the minimum transition probability\nfor all the sequences, and ﬁnd the one with the maximum\nvalue, which involves iterations over NTsequences and is\nnot for an actual implementation. Instead, we introduce a\n3Because the output probabilities are 0 or 1 in our application of HMM\nto guitar ﬁngering decision, the sequence ˆxMM eventually becomes the\none with the maximum minimum transition probability, although its def-\ninition (2) depends on the output probabilities as well.\n4Although the antonym “maximin” is appropriate for probability\n(which is the reciprocal of difﬁculty), we still use “minimax” for our pro-\nposed algorithm because it is appropriate for difﬁculty and conveys our\nconcept of “make the most difﬁcult move easier.”Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 449variant of the conventional Viterbi algorithm that can solve\nthe problem efﬁciently. We modify the second step of the\nconventional Viterbi algorithm by replacing the term δitaij\nwithmin(δit,aij)where\nmin(δit,aij) =/braceleftbiggδit(δit≤aij)\naij(aij<δit).\nThe modiﬁed second step is as follows.\nRecursion for minimax Viterbi algorithm ﬁlls out the\ntwo tables ∆andΨusing the following recursive formulae\nforj= 1,2,...,N andt= 1,2,...,T−1,\nδj,t+1= max\ni(min(δit,aij))bj(yt+1),\nψj,t+1= arg max\ni(min(δit,aij)).\nWe modify only the second step and leave other steps un-\nchanged. The modiﬁed second step works as the original\none but now the element δitkeeps the value of the maxi-\nmum minimum transition probability of the subsequence\nof hidden states for the ﬁrst tobservations. The term\nmin(δit,aij)updates the value of the minimum transition\nprobability as the term δitaijin the conventional Viterbi\nalgorithm does the likelihood5.\n3. FINGERING DECISION BASED ON HMM\nWe implement automatic ﬁngering decision based on an\nHMM whose hidden states are left hand forms and output\nsymbols are musical notes played by the left hand forms.\nIn this formulation, ﬁngering decision is cast as a decod-\ning problem of HMM where a ﬁngering is obtained as a\nsequence of hidden states. Because each hidden state has a\nunique output symbol, the output probability for the unique\nsymbol is always 1. To compare the results of the conven-\ntional Viterbi algorithm and the minimax Viterbi algorithm\nclearly, we concentrate on ﬁngering decisions for mono-\nphonic guitar phrases in the present study although HMM-\nbased ﬁngering decision is able to deal with polyphonic\nsongs as well.\n3.1 HMM for monophonic ﬁngering decision\nTo play a single note with a guitar, a guitarist depresses a\nstring on a fret with a ﬁnger of the left hand and picks the\nsame string with the right hand. Therefore a form qifor\nplaying a single note can be expressed in a triplet\nqi= (si,fi,hi)\nwheresi= 1,..., 6is a string number (from the high-\nest to the lowest), fi= 0,1,... is a fret number, and\nhi= 1,..., 4is a ﬁnger number of the player’s left hand\n(1,2,3 and 4 are the index, middle, ring and pinky ﬁngers).\nThe fret number fi= 0 means an open string for which\n5Note that min(δit,aij)does not compare the probability of some\nsubsequence and some transition probability but it does two transition\nprobabilities here.the ﬁnger number hidoes not make sense. For a classi-\ncal guitar with six strings and 19 frets, the total number of\nforms is 6×(19×4 + 1) = 4626. For the standard tun-\ning (E4-B3-G3-D3-A2-E2), the MIDI note numbers of the\nopen strings are\nn1= 64,n2= 59,n3= 55,n4= 50,n5= 45,n6= 40\nfrom which the MIDI note number of the note played by\nthe formqiis calculated as\nnote(qi) =nsi+fi.\n3.2 Transition and output probabilities\nIn standard applications of HMM, model parameters such\nas the transition probabilities and the output probabilities\nare estimated from training data using the Baum-Welch al-\ngorithm [1]. However, for our application of ﬁngering de-\ncision, it is difﬁcult to prepare enough training data, that\nis, machine-readable guitar scores attached with tablatures.\nFor this reason, we design those parameters as explained in\nthe following instead of estimation from training data.\nThe difﬁculty levels of moves are implemented in the\ntransition probabilities between hidden states; a small\nvalue of the transition probability means the corresponding\nmove is difﬁcult and a large value easy. As for the move-\nment of the left hand along the neck, the transition prob-\nability should be monotone decreasing with respect to the\nmovement distance with the transition. Furthermore, the\ndistribution of the movement distance is sparse and con-\ncentrates on the center because the left hand of a guitarist\nusually stays at a ﬁxed position for several notes and then\nleaps a few frets to a new position. To approximate such a\nsparse distribution concentrated on the center, we employ\nthe Laplace distribution (Figure 1),\nf(x) =1\n2φexp/parenleftbigg\n−|x−µ|\nφ/parenrightbigg\n. (3)\nIt is known that a one dimensional Markov process with\nincrements according to the Laplace distribution is approx-\nimated by a piecewise constant function [3] that is similar\nto the movement of the left hand along the neck. The mean\nand the variance of the Laplace distribution (3) are µand\n2φ2respectively. We set µto zero and φto the time in-\nterval between the onsets of the two notes at both ends of\nthe transition so that a long interval makes the transition\nprobability larger, which reﬂects that a long interval makes\nthe move easier. For simplicity, we assume that the four\nﬁngers of the left hand (the index, middle, ring and pinky\nﬁngers) are always put on consecutive frets. This lets us\ncalculate the index ﬁnger position (the fret number the in-\ndex ﬁnger is put on) of form qias follows,\nifp(qi) =fi−hi+ 1.\n6The actual number of forms is less than this because the 19th fret\nis most often split by the sound hole and not usable for third and fourth\nstrings, the players hardly place their index ﬁngers on the 19th fret or\npinky ﬁngers on the ﬁrst fret, and so on.450 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016−10 −5 0 5 100.0 0.1 0.2 0.3 0.4 0.5\nxf(x)Figure 1 . The probability density function of the Laplace\ndistribution for µ=φ= 1, which is sparse and concen-\ntrates on the center.\nUsing the index ﬁnger position, we set the transition prob-\nability as\naij(dt) =P(Xt=qj|Xt−1=qi,dt)\n∼1\n2dtexp(−|ifp(qi)−ifp(qj)|\ndt)\n×1\n1 +|si−sj|×pH(hj) (4)\nwheredtin the ﬁrst term is set to the time interval be-\ntween the onsets of the (t−1)-th note and the t-th note.\nThe second term corresponds to the difﬁculty of changing\nbetween strings where we employ a function 1/(1 +|x|)\nwhich is less sparse than the Laplace distribution (3). The\nthird termpH(hj)corresponds to the difﬁculty level of the\ndestination form deﬁned by the ﬁnger number hj. In the\nsimulation in the following section, we set pH(1) = 0.35,\npH(2) = 0.3,pH(3) = 0.25andpH(4) = 0.1which\nmeans the form using the index ﬁnger is easiest and the\npinky ﬁnger the most difﬁcult. The difﬁculty levels of the\nforms are expressed in the transition probabilities (not in\nthe output probabilities) in such a way that the transition\nprobability is small when the destination form of the tran-\nsition is difﬁcult.\nAs for the output probability, because all the hidden\nstates have unique output symbols in our HMM for ﬁn-\ngering decision, it is 1 if the given output symbol okis the\none that the hidden state qioutputs and 0 if okis not,\nbik=P(Yt=ok|Xt=qi)\n∼/braceleftbigg1 (ifok=note(qi))\n0 (ifok/negationslash=note(qi)).\n4. EV ALUATION\nTo evaluate our proposed method, we compared the results\nof ﬁngering decision using the conventional Viterbi algo-\nrithm and the minimax Viterbi algorithm. Figures2-4 show\nFigure 2 . The results of ﬁngering decision for the C ma-\njor scale starting from C3. Comparing the two tablatures,\nthe one obtained by the minimax Viterbi algorithm is more\nnatural and one that actual guitarists would choose. As for\nthe minimum transition probability, the line chart shows\nthat the minimax Viterbi algorithm gives a larger one.\nthe results for three example monophonic phrases. In each\nﬁgure, the top and the middle tablatures show the two ﬁn-\ngerings obtained by the conventional Viterbi algorithm and\nthe minimax Viterbi algorithm. The numbers on the tabla-\ntures show the fret numbers and the numbers in parenthesis\nbelow the tablatures show the ﬁnger numbers where 1,2,3\nand 4 are the index, middle, ring and pinky ﬁngers. The\nbottom line chart shows the time evolution of the transi-\ntion probability of the conventional Viterbi algorithm (gray\nline) and the minimax Viterbi algorithm (black line). The\ntwo tablatures and the line chart share a common horizon-\ntal time axis, that is, a point on the line chart between two\nnotes in the tablature indicates the transition probability\nbetween the two notes.\nFigure 2 shows the results for the C major scale start-\ning from C3. From the line chart of the transition prob-\nability, we see that the minimum value of the gray line\n(the conventional Viterbi algorithm) at the sixth transition\nis smaller than any value of the black line (the minimax\nViterbi algorithm), that is, the minimax Viterbi algorithm\ngives a larger minimum transition probability. As for the\ntablatures, the one obtained by the minimax Viterbi algo-\nrithm is more natural and one that actual guitarists would\nchoose.\nFigure 3 shows the results for the opening part of “Ro-\nmance Anonimo.” From the line chart of the transition\nprobability, we see that the gray line (the conventional\nViterbi algorithm) keeps higher values at the cost of two\nvery small values while the black line (the minimax Viterbi\nalgorithm) avoids such very small values although it keeps\nrelatively lower values. From the line charts of FiguresProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 451Figure 3 . The results of ﬁngering decision for the opening\npart of “Romance Anonimo” (only top notes). Compar-\ning the two tablatures, the one obtained by the minimax\nViterbi algorithm avoids using the pinky ﬁnger and sup-\npresses changing between strings. We see from the line\nchart that the conventional Viterbi algorithm keeps higher\nvalues at the cost of two very small values while the mini-\nmax Viterbi algorithm avoids very small values although it\nkeeps relatively lower values.\n2 and 3, we see that the minimax Viterbi algorithm actu-\nally minimizes the maximum difﬁculty for playing a given\nphrase and makes the most difﬁcult move easier, which can\nnot be done by the conventional Viterbi algorithm. As for\nthe resulting tablatures, while the one obtained by the con-\nventional Viterbi algorithm uses the pinky ﬁnger twice and\nchanges between strings three times, the one obtained by\nthe minimax Viterbi algorithm does not use the pinky ﬁn-\nger and changes between strings only once.\nFigure 4 shows the results for the opening part of “Eine\nKleine Nachtmusik.” In both ﬁngerings, the ﬁrst eight\nnotes are played with a single ﬁnger that presses down mul-\ntiple strings across a single fret. The top tablature obtained\nby the conventional Viterbi algorithm uses the index ﬁn-\nger for the ﬁrst eight notes and the pinky ﬁnger for the\nninth note while the middle one obtained by the minimax\nViterbi algorithm prefers the ring ﬁnger for the ﬁrst eight\nnotes to avoid using the pinky ﬁnger for the ninth note.\nThe slight difference in the transition probability for the\nﬁrst eight notes comes from the difference in the difﬁculty\nof the formpH(hj)in (4) deﬁned by the ﬁnger number hj.\n5. CONCLUSION\nWe have introduced a variant of the Viterbi algorithm\ntermed the minimax Viterbi algorithm that ﬁnds the se-\nquence of the hidden states that maximizes the minimum\ntransition probability, and demonstrated the performance\nFigure 4 . The results of ﬁngering decision for the opening\npart of “Eine Kleine Nachtmusik” (only top notes). Com-\nparing the two tablatures, the one obtained by the minimax\nViterbi algorithm uses the ring ﬁnger (instead of the in-\ndex ﬁnger) for the ﬁrst eight notes to avoid using the pinky\nﬁnger for the ninth note. The slight difference in the tran-\nsition probability for the ﬁrst eight notes comes from the\ndifference in the difﬁculty of using the index ﬁnger and the\npinky ﬁnger.\nof the algorithm with guitar ﬁngering decision based on\na synthetic HMM. Fingering decision using our proposed\nvariant has turned out to be able to minimize the maximum\ndifﬁculty of the move required for playing a given phrase.\nWe have compared the resulting ﬁngerings by the conven-\ntional Viterbi algorithm and the minimax Viterbi algorithm\nto see that our proposed variant is capable of making the\nmost difﬁcult move easier that can not be done by the con-\nventional one. Those observations give rise to interests\nin the interpolation between the conventional Viterbi al-\ngorithm and the minimax Viterbi algorithm. We consider\nthat such an interpolation can be implemented using the\nLp-norm of a real vector, which is the absolute sum of the\nvector elements for p=1and the maximum absolute value\nforp=∞, and is one of our future study plans. We hope\nthat the present work draws the researcher’s attention to the\nnew “minimax paradigm” in automatic ﬁngering decision.\n6. ACKNOWLEDGMENTS\nThis work was supported by JSPS KAKENHI Grant Num-\nber 26240025.\n7. REFERENCES\n[1] Leonard E Baum and Ted Petrie. Statistical in-\nference for probabilistic functions of ﬁnite state\nMarkov chains. The annals of mathematical statistics ,\n37(6):1554–1563, 1966.452 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016[2] Yoshua Bengio and Paolo Frasconi. An input output\nHMM architecture. Advances in neural information\nprocessing systems , 7:427–434, 1995.\n[3] Stephen Boyd and Lieven Vandenberghe. Convex opti-\nmization . Cambridge University Press, 2004.\n[4] Gen Hori, Hirokazu Kameoka, and Shigeki Sagayama.\nInput-output HMM applied to automatic arrange-\nment for guitars. Journal of Information Processing ,\n21(2):264–271, 2013.\n[5] Wakana Nagata, Shinji Sako, and Tadashi Kitamura.\nViolin ﬁngering estimation according to skill level\nbased on hidden Markov model. In Proceedings of\nInternational Computer Music Conference and Sound\nand Music Computing Conference (ICMC/SMC2014) ,\npages 1233–1238, Athens, Greece, 2014.\n[6] Eita Nakamura, Nobutaka Ono, and Shigeki\nSagayama. Merged-output HMM for piano ﬁn-\ngering of both hands. In Proceedings of International\nSociety for Music Information Retrieval Conference\n(ISMIR2014) , pages 531–536, Taipei, Taiwan, 2014.\n[7] Lawrence R Rabiner. A tutorial on hidden Markov\nmodels and selected applications in speech recognition.\nProceedings of the IEEE , 77(2):257–286, 1989.\n[8] Daniele P Radicioni, Luca Anselma, and Vincenzo\nLombardo. A segmentation-based prototype to com-\npute string instruments ﬁngering. In Proceedings of\nConference on Interdisciplinary Musicology (CIM04) ,\nvolume 17, pages 97–104, Graz, Austria, 2004.\n[9] Aleksander Radisavljevic and Peter Driessen. Path dif-\nference learning for guitar ﬁngering problem. In Pro-\nceedings of International Computer Music Conference ,\nvolume 28, Miami, USA, 2004.\n[10] Samir I Sayegh. Fingering for string instruments with\nthe optimum path paradigm. Computer Music Journal ,\n13(3):76–84, 1989.\n[11] Daniel R Tuohy and Walter D Potter. A genetic algo-\nrithm for the automatic generation of playable guitar\ntablature. In Proceedings of International Computer\nMusic Conference , pages 499–502, 2005.\n[12] Andrew J Viterbi. Error bounds for convolutional codes\nand an asymptotically optimum decoding algorithm.\nIEEE Transactions on Information Theory , 13(2):260–\n269, 1967.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 453"
    },
    {
        "title": "Sparse Coding Based Music Genre Classification Using Spectro-Temporal Modulations.",
        "author": [
            "Kai-Chun Hsu",
            "Chih-Shan Lin",
            "Tai-Shih Chi"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1418099",
        "url": "https://doi.org/10.5281/zenodo.1418099",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/046_Paper.pdf",
        "abstract": "Spectro-temporal modulations (STMs) of the sound con- vey timbre and rhythm information so that they are intui- tively useful for automatic music genre classification. The STMs are usually extracted from a time-frequency representation of the acoustic signal. In this paper, we in- vestigate the efficacy of two kinds of STM features, the Gabor features and the rate-scale (RS) features, selective- ly extracted from various time-frequency representations, including the short-time Fourier transform (STFT) spec- trogram, the constant-Q transform (CQT) spectrogram and the auditory (AUD) spectrogram, in recognizing the music genre. In our system, the dictionary learning and sparse coding techniques are adopted for training the support vector machine (SVM) classifier. Both spectral- type features and modulation-type features are used to test the system. Experiment results show that the RS fea- tures extracted from the log. magnituded CQT spectro- gram produce the highest recognition rate in classifying the music genre.",
        "zenodo_id": 1418099,
        "dblp_key": "conf/ismir/HsuLC16",
        "content": "SPARSE CODING BASED MUSIC GENRE CLASSIFICATION \nUSING SPE CTRO-TEMPORAL MODULATIONS \nKai-Chun Hsu Chih-Shan Lin Tai-Shih Chi \nDepartment of Electrical and Comp uter Engineering, National Chi ao Tung University, Taiwan  \n{kch610596,  g104972}@gmail.com, tschi@mail.nctu.edu.tw\nABSTRACT \nSpectro-temporal modulations (STMs) of the sound con-\nvey timbre and rhythm information so that they are intui-\ntively useful for automatic music genre classification. \nThe STMs are usually extracted from a time-frequency \nrepresentation of the acoustic s ignal. In this paper, we in-\nvestigate the efficacy of two kinds of STM features, the \nGabor features and the rate-scale (RS) features, selective-\nly extracted from various time-frequency representations, \nincluding the short-time Fourier transform (STFT) spec-\ntrogram, the constant-Q transform (CQT) spectrogram \nand the auditory (AUD) spectrogram, in recognizing the \nmusic genre. In our system, the dictionary learning and \nsparse coding techniques are adopted for training the \nsupport vector machine (SVM) classiﬁer. Both spectral-\ntype features and modulation-type features are used to test the system. Experiment results show that the RS fea-\ntures extracted from the log. magnituded CQT spectro-\ngram produce the highest recognition rate in classifying the music genre. \n1.INTRODUCTION\nFor a classification task, selected features and the classi-\nfier are critical to the performance of the system. Since \nthe last decade, lots of researchers have proposed music \ngenre classification systems using designed features or \nclassifiers. For instance, the mel-frequency cepstral coef-\nficients (MFCCs), the pitch histogram and the beat histo-gram were used in [1] as effective features to describe \ncharacteristics of timbre, pitch and rhythm of music. The \nSVM was used in a multi-layer fashion for genre classifi-\ncation [2]. Later on, parameters of autoregressive models \nof spectral raw features were used for classification by \nincluding the temporal variations of the raw features [3]-[5]. In addition to SVM, the adaptive boosting algorithm \nwas used to train the classifier [6]. The non-negative ten-\nsor factorization (NTF) was also considered to reduce the dimensionality in a sparse representation classifier (SRC) \n[7][8]. Another approach was t o extract features from the \nseparated cleaner signal [9] by first applying the harmon-\nic-percussion signal separation (HPSS) algorithm [10] to \nthe music clip. The Gaussian supervector, which has been \nsuccessfully used in speaker identification, was also in-vestigated in music genre classification [11]. A super-vised dictionary learning process was proposed for genre classification by using codebooks generated from existing coding techniques [12]. All these methods were operated \non audio signals only. In addition, one can also combine \nfeatures from other sources such as MIDI or lyrics [13]-\n[17].  \n    In recent years, sparse c oding technique has been ap-\nplied to music genre classification. Most sparse coding \nbased automatic music genre classification systems trans-\nform the music signal into frame-level raw features, and \nthen encode the frame-level features into frame-level \nsparse codes. Since the enc oding only considers infor-\nmation in one frame, temporal pooling technique has \nbeen included in this kind of system. For instance, the combinations of statistical moments of a multiple frame \nrepresentation were used for temporal pooling on raw \nfeatures [18]. Histogram and pyramid based bag-of-\nsegments schemes were also considered for temporal \npooling on encoding [19]. \n    In addition to considering temporal pooling on spectral \nfeatures, amplitude modulations shown on the short-time Fourier transform (STFT) spectrogram, which depict the \nspectral patterns varying ac ross time, were extracted us-\ning a set of 2-D Gabor filters for genre classification [20]. It has been shown that joint spectro-temporal modula-\ntions (STMs) on the auditory (AUD) spectrogram are helpful for music signal categorization, hence helpful for \nmusic separation [21]. No doubt that STMs carry critical \ninformation and are suitable for genre classification. However, does the information conveyed by the STMs \nprovide more benefit than the spectral features? If so, \nwhat kind of spectrogram provides the most informative \nSTMs for genre classification? Is it the STFT spectro-\ngram or the hearing-morphi c spectrogram such as the \nconstant-Q transform (CQT)  spectrogram or the AUD \nspectrogram? This paper is trying to answer these ques-tions. Here, we built a sparse coding based genre classifi-\ncation system for evaluations.  \n    The rest of this paper is organized as follows. A brief \nintroduction of the tested sp ectrograms and STM features \nare presented in Section 2. Section 3 describes the sparse coding and dictionary learning. Section 4 describes the genre classification method and shows evaluation results. \nLastly, Section 5 draw s the conclusion. \n2.FEATURE EXTRACTION\nIn this section, we introduce the various features used in \nthis paper. Two types of raw features are considered: the frame-level features extracted from STFT, CQT and \nAUD spectrograms; and their corresponding STM fea-\n © Kai-Chun Hsu, Chih-Shan Lin, Tai-Shih Chi. \nLicensed under a Creative Commons A ttribution 4.0 International\nLicense (CC BY 4.0). Attribution:  Kai-Chun Hsu, Chih-Shan Lin, Tai-\nShih Chi. “Sparse Coding Based Music Genre Classification using\nSpectro-Temporal Modulations”, 17t h International Society for M usic \nInformation Retrieval Conference, 2016. \n744  \n \ntures. For the STM features,  we apply Gabor filters to \nSTFT spectrogram [20] and rate-scale (RS)  filters to the \nhearing-morphic CQT and AU D spectrograms. Features \nmentioned in this section are considered as raw features in the dictionary for the sparse coding system.  \n2.1 Frame-based Features \n2.1.1 STFT Spectrogram \nThe STFT spectrogram is the most conventional time-\nfrequency representation of audio signals. In this paper, \nwe computed 1024-point FFT for each frame and adja-cent frames are with 50% overlap. This computation re-\nsulted in a 513-dimensional magnitude spectrum which \nserved as a feature vector. \n2.1.2 CQT Spectrogram \nThe constant-Q transform (CQT) produces another kind \nof time-frequency audio representation with logarithmic \nfrequency scale and different temporal/spectral resolu-\ntions at different frequency b ands. The CQT spectrogram \nis considered closely suited to human perception of sound.  \n    In this paper, we set 8 octaves for the frequency range \nwith the frequency resolution of 64 bins per octave, re-sulting in 512-dimensional feature vectors. For imple-\nmentation, we used the Constant-Q Transform Toolbox \n[22][23] which implements the computationally-efficient CQT transform based on FFT [24]. \n2.1.3 Auditory (AUD) Spectrogram \nThe AUD spectrogram is produced by the cochlear mod-\nule of the auditory model [25]. An input sound is first fil-\ntered by a bank of 128 overlapping asymmetric bandpass \nfilters which mimic the frequency selectivity of the coch-lea. The center frequencies of the cochlear filters are \nevenly distributed along a logarithmic frequency axis, \nover 5.3 octaves (180Hz ~ 7246Hz) with the frequency \nresolution of 24 filters per o ctave. The output of each fil-\nter is fed into a non-linear compression stage, which models the saturation of inner hair cells while transducing \nthe vibrations of the basilar membrane into intracellular \npotentials. Next, a simple lateral inhibitory network (LIN) is implemented by a first-order differentiator across fil-\nters to account for the masking effect between adjacent \nfilters. A half-wave rectifier combined with a lowpass filter serves as an envelope extractor after the LIN. At the \nend, the cochlear module produces 128-dimensional fea-\nture vectors. \n    The block diagram of the cochlear module is shown in \nFigure 1. Outputs at different stages can be formulized as \nfollows: \n                     \n1(, ) ( ) ( ;)t yf t s t h t f  ( 1 )  \n                21(, ) ( (, ) ) ( )tt yf t g yf t l t   ( 2 )  \n               32(, ) m a x ( (, ) , 0 )f yf t yf t   (3) \n                     43(, ) (, ) ( ; )t yf t yf t t     (4) \n \nwhere s(t) is the input audio signal, \t݄ሺ݂;\tݐሻ  is the impulse \nresponse of the cochlear filter with the center frequency f, ∗௧ depicts convolution in time, g(.) is a sigmoid function, \nl(t) a lowpass filter, ߲௧, ߲௙ are partial derivative along t, f \naxes, ߤሺ߬;\tݐሻൌ݁ି௧/ఛݑሺݐሻ is the integration window with \nthe time constant ߬ , and ݑሺݐሻ  is the unit step function. \nDetailed discussions about this module can be assessed in \n[26]. \n \n \nFigure 1.  Block diagrams for deriving an AUD spectro-\ngram. \n2.2 Modulation Features \n2.2.1 Gabor Features \nGabor features are the spectro-temporal “visual features” \nextracted from a STFT spectro gram as proposed in [20]. \nTo obtain these features, an input audio signal was first \ntransformed into a STFT spectrogram. Then, the STFT \nspectrogram was divided int o 7 sub-spectrograms accord-\ning to the following 7 subbands: 0Hz ~ 200Hz, 200Hz ~ \n400Hz, 400Hz ~ 800Hz, 800Hz ~ 1600Hz, 1600Hz ~ \n3200Hz, 3200Hz ~ 8000Hz, and 8000Hz ~ half sampling frequency. Third, each sub-spect rogram was filtered by a \nset of 42 pre-defined 2-D Gabor filters: \n \n       \n22\n2'' 2 '', '  = exp exp2x yj xxy      (5) \n                       ' cos( ) sin( )xx y   ( 6 )  \n                       's i n ( ) c o s ( )yx y    ( 7 )  \n \nwhere x a nd  y represent the time and frequency axes of \nthe STFT sub-spectrogram , ߠ ∈ ሼ0°,30°,…,150°ሽ\t indicates \nthe orientation of the Gabor filter, ߣ∈ ሼ2.5,5,…,17.5ሽ\t de-\nnotes the thickness of the Gabor filter, and ߪ ൌ 0.5ߣ  for \nthe standard deviation of the Gaussian function. This pro-\ncess transformed the STFT spectrogram into 294 modula-\ntion sub-spectrograms. The energy contour of each modu-\nlation sub-spectrogram was obtained by averaging the modulation sub-spectrogram along the frequency axis. At \nt h i s  s t a g e ,  t h e  S T F T  s p e c t r o g r a m  w a s  t r a n s f o r m e d  i n t o  \n294 modulation energy contours in the time domain. Fi-nally, the mean and standard deviation of these contours \nwere concatenated to form the “visual features”, referred \nto as the Gabor feat ures in this paper. \n    Figure 2 demonstrates the meaning of the Gabor fea-\nt u r e s .  T h e  u p p e r  p a n e l  s h o w s  a  s e g m e n t  o f  a  s a m p l e  STFT sub-spectrogram, while the bottom panel shows \ntwo energy contours derived from outputs of the two Ga-\nbor filters ( ߣ ൌ 7.5, ߠ ൌ 0°\tand\tߣ ൌ 7.5, ߠ ൌ 90° ). We \ncan observe that strong responses of the contours result \nfrom strong vertical and horizontal patterns in the spec-trogram. \nProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 745  \n \n \n \nFigure 2. (a) A sample STFT sub-spectrogram. (b) The \nenergy contours derived from outputs of two Gabor filters \n(ߣ ൌ 7.5, ߠ ൌ 0°\tand\tߣ ൌ 7.5, ߠ ൌ 90° ). \n2.2.2 Rate-Scale (RS) Features \nRS feature is another kind of modulation feature extract-\ned by the cortical module [25]. The cortical module, which is inspired by neural recordings of the auditory \nc o r t e x  ( A 1 ) ,  m o d e l s  t h e  s p ectro-temporal selectivity of \nneurons in A1 [26]. \n    Specifically, in the auditory model, the AUD spectro-\ngram is further analyzed by neurons in A1. From func-\ntional point of view, the cortical neurons are modeled as a \nbank of two-dimensional filters with different spectro-temporal selectivity. These two-dimensional filters can be \ncharacterized by spectro-tempor al modulation parameters, \nrate and scale. The rate parameter characterizes the veloc-ity of the modulation varying along the temporal axis on \nthe AUD spectrogram and the scale parameter character-\nizes the density of the modulation distributed along the \nlogarithmic frequency axis on the AUD spectrogram. The \nfiltering process can be formulized as follows: \n \n         \n4 ,, , , ,; ,ft rf t y f t S T I Rf t       (8) \n \nwhere r denotes the 4-dimensional output of the cortical \nmodule, y4 is the AUD spectrogram, ∗௙௧ denotes the two-\ndimensional convolution along temporal and logarithmic \nfrequency axes, STIR  is the impulse response of the two-\ndimensional modulation filter, ߱ and  denote the rate \nand the scale parameter respectively. Figure 3 demon-\nstrates examples of rate-scale features of three time-frequency (T-F) units in the AUD spectrogram. The top \npanel shows a sample AUD spectrogram and the bottom \nthree panels show the rate-s cale plots, which record the \nlocal amplitude resolved by each of the rate-scale modu-\nlation filters, of the three T-F units indicated by the ar-\nrows. The rate-scale plot ref lects local modulation energy \ndistribution and the sweeping direction of the modulation \n(positive/negative rate representing the down-ward/upward directivity) of a  particular T-F unit in the \nAUD spectrogram. Detailed explanations about the in-formation encoded by the rate-scale plot can be assessed \nin [21]. \n \n \nFigure 3.  (a) A sample AUD spect rogram. (b)(c)(d) Rate-\nscale plots of the T-F units ind icated by the arrows in (a). \n     \n    In this paper, the local amplitude of the cortical output \nr is averaged in each su bband and concatenated, \n \n                      \n11,, ,Ni\nij\njirr f tN\n    (9) \n                       12 O,, rr r r            (10) \n \nwhere ݋ ∈ ሼ1,2,3,…,Oሽ  is the index of the subband, Ni is \nthe number of bins in the i-th subband, O is the total \nnumber of subbands and r is our final RS feature. We \nextract RS features from th e  A U D  s p e c t r o g r a m  a n d  t h e  \nCQT spectrogram. For the cases of AUD spectrogram, \nthe parameters were selected as ߱ ∈ േሼ2,4,8,16ሽ , Ω∈\nሼ0.25,0.5,1,2,4,8ሽ  and O=6(180Hz ~ 200Hz, 200Hz ~ \n400Hz, 400Hz ~ 800Hz, 800Hz ~ 1600Hz, 1600Hz ~ \n3200Hz, 3200Hz ~ 7246Hz), resulting in 288-dimensional feature vectors. For the cases of CQT spec-\ntrogram, the parameter were selected as ߱ ∈ േሼ2,4,8ሽ , \nΩ ∈ ሼ0.25,0.5,1,2,4,8,16ሽ  and O=7(0Hz ~ 200Hz, 200Hz \n~ 400Hz, 400Hz ~ 800Hz, 800Hz ~ 1600Hz, 1600Hz ~ \n3200Hz, 3200Hz ~ 8000Hz, 8000Hz ~ half-sampling fre-\nquency), resulting in 294-dimensional feature vectors. These parameters were selected mainly to have compara-\nble feature dimensions with the restriction posed by the \n5.3-octave frequency coverag e of the AUD spectrogram. \n3. SPARSE CODING AND DICTIONARY \nLEARNING \nGenerally speaking, the sparse coding technique decom-\nposes the original signal into a combination of a few \ncodewords in a given codebook (or dictionary). The ob-jective function of sparse coding can be formulated as: \n \n \n2\n211arg min2xD\n      ( 1 1 )  \n \n746 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016wher e ݔ∈ℝௗ is the input signal (th e feature vector in our \ncase), ߙ∈ℝ௞ is the sparse code of x, ܦ∈ℝௗൈ௞ is a given \ndictionary and λ is a parameter which controls the sparsi-\nty of ߙ .The d is the dimension of the feature vector and \nthe k is the codebook size. Equation (11) is usually re-\nferred to as the Lasso problem and can be solved by the \nLARS-lasso algorithm [27]. \nFurthermore, the objective function of dictionary \nlearning can be formulated as: \n2\n21, 111arg min2n\nii i\nD iDx Dn \n         (12) \nwhere n is the total number of data to train the dictionary. \nEquation (12) represents a joint optimization problem in \nα a n d  D. In this paper, the online dictionary learning \n(ODL) algorithm [28][29] was used to train the dictionary \nand the SPArse Modeling Software (SPAMS) [30] was \nused for implementation. \n4.EXPERIMENTS\nIn this section, we describe the settings of the experi-ments and the classification results using various kinds of \nfeatures. \n4.1 Dataset \n4.1.1 GTZAN Dataset \nThis public dataset is frequently used in literature for \nevaluation of automatic music genre classification. The \ndataset is composed of 100 30-second music clips in each \nof the ten genres (blues, clas sical, country, disco, hip-hop, \njazz, metal, pop, reggae, and rock). Each clip is sampled at 22050 Hz. \n4.1.2 H-L Dataset \nThis dataset was collected by ourselves and used in this \nwork for learning the dictionary of music. The dataset is composed of 100 30-second c lips in each of the 21 genres \n(a cappella, A-pop, blues, bossa nova, classical, C-pop, electropop, funk, hip-hop, jazz, J-pop, latin, metal, musi-cal, new age, opera, R&B, reggae, rock, romantical, and \nsoul). All the clips in this dataset are different from those \nin the GTZAN dataset. Each c lip is sampled at 44100 Hz. \n4.2 System Overview \nI n  o u r  c l a s s i f i c a t i o n  s y s t e m ,  a n  i n p u t  m u s i c  c l i p  i s  f i r s t  transformed into the frame-level feature vectors. The fea-\nture vectors are then  normalized to unit ݈\nଶ-norm vectors. \nSecond, the normalized feature vectors are encoded into \nframe-level sparse codes. Next, we summarize the frame-\nlevel sparse codes over the entire clip to obtain the song-\nlevel feature w. Finally, w is power normalized using \nEquation (13) to train/test the classifier. The block dia-\ngram of the classification sy stem is shown in Figure 4. \n()aws i g n w w (13)    In all \nof the experiments, we set the codebook size to \n1024, the regularization parameter λ t o  1/√݀ , and the \npower normalization parameter a in Equation (13) to 0.5. \nThe linear-SVM implemented in LIBSVM [31] was used \nas the classifier. Evaluation results were obtained by av-\neraging results from 100 ten-fold cross-validation. \nFigure 4\n. The block diagram of the sparse coding based \nclassification system. The H-L dataset was mainly used to generate the dic tionary of music. \n4.3 Experiment Results \nExperiment results are shown in this section. For simplic-\nity, the name of the classifica tion system is referred to as \nthe name of the used raw features (e.g., the sparse coding \nbased automatic genre classi fication system using STFT \nfeatures is referred to as the STFT system). \n4.3.1 Spectrogram Features versus Modulation Features \nRecognition rates using the spectrogram features and the \ncorresponding modulation features (STFT/Gabor, AUD/RS, CQT/RS) are shown in Figure 5. We can see \nthat corresponding modulation features of the STFT spec-\ntrogram have a negative imp act to system performance \n(75.8% to 73.8%) while they provide significant benefit \nto AUD spectrogram  (71.0 to 79.7%) and CQT spectro-\ngram (77.0% to 84.7%). \nThe main difference among the three spectrograms is \nthe frequency scale, linear scale in STFT but logarithmic \nscale in AUD and CQT spectrograms. We postulate that \nmodulation features extracted from the logarithmic fre-quency spectrogram are beneficial to genre classification. \nFor validation, Gabor featur es were extracted from all \nthree spectrograms and tested  for system performance. \nFigure 6 shows the results of three Gabor systems \n(STFT/Gabor, AUD/Gabor, CQT/Gabor). Clearly, com-\nparing with spectrogram features, Gabor features demon-\nstrate a positive effect on the genre classification rate \nwhen extracted from the AUD and CQT spectrograms but a negative effect when extracted from the STFT spectro-\ngram. Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 747  \n \n \n \nFigure 5.  The recognition rates using the spectrogram \nfeatures and corresponding modulation features \n(STFT/Gabor, AUD/RS, CQT/RS). \n \n \n \nFigure 6.  The recognition rates using the spectrogram \nfeatures and Gabor modulation features (STFT/Gabor, \nAUD/Gabor, CQT/Gabor).  \n4.3.2 AUD Spectrogram versus CQT Spectrogram \nFigure 5 and 6 show both Gabor and RS modulation fea-\ntures extracted from the CQT spectrogram perform better \nthan those extracted from the AUD spectrogram. The \nmain differences between CQT and AUD spectrograms \nare the filter shape, the frequency resolution (64 bins per \noctave on CQT and 24 bins per octave on AUD), and the covered frequency range (40Hz ~ 10700Hz on CQT and \n180Hz ~ 7246Hz on AUD). \n    To investigate the effects from the frequency resolu-tion and the frequency range, we tested RS modulation \nfeatures extracted from CQT spectrograms with different \nsettings listed in Table 1. The recognition rates are shown \nin Figure 7. We can observe that higher frequency resolu-\ntion (64 bins/octave versus 24 bins/octave) does not nec-essarily produce higher recognition rate. Finding the op-\ntimal frequency resolution for recognition rate, however, \nis beyond the scope of this work. On the other hand, wid-e r f re q ue nc y  c ove ra ge  i s  m ore  be ne fi c i a l  t o s y s t e m  pe r-\nformance. In Exp.4, the CQT spectrogram was computed \nusing the same frequency resolution and frequency cov-erage as the AUD spectrogram yet its RS features outper-\nforms the RS features of AUD (82.4% versus 79.7% \nshown in Figure 5). It is probably because the CQT spec-trogram possesses a higher Q value than the AUD spec-\ntrogram. A higher Q value generates a more sharpened \nspectrogram hence producin g better performance. Exp. frequency range bins/octave ω Ω \n1 40Hz ~ 10700Hz 64 2~16 0.25~8\n2 40Hz ~ 10700Hz 24 2~16 0.25~8\n3 180Hz ~ 7246Hz 64 2~16 0.25~8\n4 180Hz ~ 7246Hz 24 2~16 0.25~8\n \nTable 1.  Different frequency settings for generating \nCQT/RS modulation features \n \n \n \nFigure 7.  The recognition rates using CQT/RS features \nwith different frequency range and frequency resolutions as listed in Table 1. \n4.3.3 Gabor Features versus RS Features  \nFrom Figure 5 and 6, we can observe that the RS feature \nset performs better than the Gabor feature set on both of \nCQT and AUD spectrograms. R S features and Gabor fea-\ntures are produced using different sets of 2-D modulation \nfilters. The parameters of the Gabor filter, θ and λ, and \nthe parameters of the rate-scale filter, ω and Ω, affect the \nshape of the 2-D filter by changing its center frequency \nand bandwidth. \n    To demonstrate the effect  of using different modula-\ntion filters, we tested RS features extracted from CQT \nspectrogram using a different set of ω and\tΩ. Experiment \nsetting is listed as Exp.2 in Table 2 and the results are \nshown in Figure 8, where Exp.1 and Exp.3 are the \nRS/Gabor features using the original set of 2-D filters, \nrespectively. We can observe that selecting different 2-D \nmodulation filters significantly affect system perfor-mance. Therefore, selecting an appropriate set of 2-D fil-\nters for modulation feature extraction is important to sys-\ntem performance.  \nExp. 2-D \nfilterω Ω θ λ \n1 RS 2~8 0.25~16   \n2 RS 0.25~16 2~8   \n3 Gabor   0, 3 0, . . . , 1 5 0   2.5, 5,...,17.5  \n \nTable 2. Different modulation filters used to extract \nmodulation features from the CQT spectrogram \n748 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016  \n \n \n \nFigure 8.  The recognition rates using different sets of 2-\nD modulation filters on the CQT spectrogram. Parameters \nof the modulation filters are listed in Table 2. \n4.3.4 Linear Magnitude versus Log. Magnitude \nIt has been shown that logarithmic magnitude STFT pro-\nduces better features than the linear magnitude STFT for \ngenre classification [12]. In this sub-section, we demon-strate the effect of using log. magnitude on spectrograms. \nIn addition to using spectrogram features, the system per-\nformance using modulation f eatures extracted from the \nlog. magnitude spectrograms were also examined. \n    Experiment results of usi ng log. magnitude spectro-\ngrams versus using linear magnitude spectrograms are \nshown in Figure 9. Results of using their corresponding \nmodulation features (Gabor from STFT, RS from AUD, and RS from CQT) are shown in Figure 10. We can see \nthat both spectrogram featur es and modulation features \nextracted from log.  magnitude spectrograms perform bet-\nter than those extracted fro m linear magnitude spectro-\ngrams. \n \n \n \nFigure 9.  Recognition rates of using spectral profiles ex-\ntracted from log. magnitude s pectrograms an d from linear \nmagnitude spectrograms. \n5. CONCLUSIONS \nIn this paper, we investigat e the efficacy of features de-\nrived from joint spectro-temporal modulations, which in-\ntrinsically convey timbre and rhythm information of the \nsound, on music genre classification using a sparse cod-ing based classification system. We extract two kinds of \nSTM features, the Gabor an d RS features, from three \nkinds of spectrograms, STFT, auditory, and CQT spec-trograms, of the music signal and conduct several com-\nparative experiments. The results show that modulation \nfeatures do outperform spectral profiles in genre classifi-\ncation. In addition, several conclusions can be drawn from our results: 1) modulation features extracted from \nthe logarithmic frequency s caled spectrogram perform \nbetter than those extracted  from the linear frequency \nscaled spectrogram; 2) the s pectrogram with wider fre-\nquency coverage produces more effective modulation \nfeatures; 3) the selection of  modulation filters could be \ntask-dependent; 4) modulation features extracted from \nlog. magnitude spectrograms produce higher genre \nrecognition rates than those extracted from linear magni-\ntude spectrograms. \n    \n \n \nFigure 10. Recognition rates of using modulation fea-\ntures extracted from log. magnitude spectrograms and \nfrom linear magnitude spectrograms. \n \n    In this paper, the highest genre recognition rate on \nGTZAN dataset using modulation features is 86.2%, which is obtained by using RS features extracted from the \nlog. magnitude CQT spectrogram. From experiment re-\nsults shown in Section 4, we can assume the performance can probably be better by fine-tuning the parameters of \nthe classification system, in cluding the rate, scale param-\neters of the modulation filters and the frequency resolu-tion of the CQT spectrogram. \n6. ACKNOWLEDGEMENTS \nThis research is supported by the National Science \nCouncil, Taiwan under Grant No NSC 102-2220-E-009-\n049. \n7. REFERENCES \n[1] G. Tzanetakis and P. Cook, “Musical genre \nclassiﬁcation of audio signals,” IEEE Trans. Speech \nand Audio Processing , vol. 10, no. 5, pp. 293–302, \n2002. \n[2] C. Xu, N. C. Maddage, X. S hao, F. Cao, and Q. Tian, \n“Musical genre classiﬁcation using support vector machines,” Proc. IEEE Int. Conf. on Acoust., Speech \nand Signal Process. , no. 5, pp. 429–432, 2003. \n[3] A. Meng, P. Ahrendt, and J. Larsen, “Improving mu-\nsic genre classiﬁcation by s hort-time feature intergra-\ntion,” Proc. IEEE Int. Conf. on Acoust., Speech and \nSignal Process. , no. 5, pp. 497–500, 2005. \nProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 749  \n \n[4] A. Meng and J. Shawe-Taylor, “An investigation of \nfeature models for music genre classiﬁcation using \nthe support vector classiﬁer,” Proc. of the Int. Soc. \nfor Music Inform. Retrieval Conf. , pp.604–609, 2005.  \n[5] A.  M e n g,  P .  Ah re ndt ,  J .  L a rs e n ,  a n d L .  K .  Ha ns e n , \n“Temporal feature integration for music genre \nclassiﬁcation,” IEEE Trans. Audio, Speech, and \nLanguage Processing , vol. 15, no. 5, pp. 1654–1664, \n2007. \n[6] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and B. \nK´egl, “Aggregate features and adaboost for music \nclassiﬁcation,” Machine Learning , vol. 65, no. 2-3, \npp. 473–484, 2006. \n[7] Y. Panagakis, C. Kotropoulos, and G. R. Arce, “Mu-\nsic genre classiﬁcation using locality preserving non-negative tensor factorization and sparse representa-\ntions,” Proc. of the Int. Soc. for Music Inform. Re-\ntrieval Conf. , pp. 249–254, 2009. \n[8] Y .  P a n a g a k i s  a n d  C .  K o t r o p o u l o s ,  “ M u s i c  g e n r e  \nclassiﬁcation via topology preserving non-negative \ntensor factorization and sparse representations,” Proc. \nIEEE Int. Conf. on Acoust., Speech and Signal Pro-\ncess. , pp. 249–252, 2010. \n[9] H. Rump, S. Miyabe, E. Tsunoo, N. Ono, and S. \nSagama, “Autoregressive mfcc models for genre \nclassiﬁcation improved by harmonic-percussion sep-\naration,” Proc. of the Int. Soc. for Music Inform. Re-\ntrieval Conf. , pp. 87–92, 2010. \n[10] N. Ono, K. Miyamoto, H. Kameoka, and S. Saga-\nyama, “A real-time equalizer of harmonic and per-cussive componets in music signals,” Proc. of the Int. \nSoc. for Music Inform. Retrieval Conf. , pp. 139–144, \n2008. \n[11] Cao, Chuan, and Ming Li. \"Thinkit’s submissions for \nMIREX2009 audio music classification and similari-\nty tasks.\" Music Information Retrieval Evaluation \neXchange (MIREX)  (2009). \n[12] C.-C. M. Yeh and Y.-H. Yang, “Supervised diction-\nary learning for music genre classiﬁcation,” Proc. \nACM Int. Conf. on Multimedia Retrieval , no. 55, \n2012. \n[13] A. Ruppin and H. Yeshurun, “Midi genre \nclassiﬁcation by invariant features,” Proc. of the Int. \nSoc. for Music Inform. Retrieval Conf. , pp. 397–399, \n2006. \n[14] T .  L i d y ,  A .  R a u b e r ,  A .  P e r t u s a ,  a n d  J .  M .  I n e s t a ,  \n“Improving genre classiﬁcation by combination of audio and symbolic descriptors using a transcription \nsystem,” Proc. of the Int. Soc. for Music Inform. Re-\ntrieval Conf. , pp. 61–66, 2007. \n[15] R .  M a y e r ,  R .  N e u m a y e r ,  a n d  A .  R a u b e r ,  “ R h y m e  \nand style features for musi cal genre categorization by \nsong lyrics,” Proc. of the Int. Soc. for Music Inform. \nRetrieval Conf. , pp. 337–342, 2008. \n[16] C. McKay, J. A. Burgoyne, J. Hockman, J. B. L. \nSmith, G. Vigliensoni, and I. Fujinaga, “Evaluating \nthe genre classiﬁcation performance of lyrical fea-\ntures relative to audio, symbolic and cultural fea-tures,” Proc. of the Int. Soc. for Music Inform. Re-\ntrieval Conf. , pp. 213–218, 2010. \n[17] R. Mayer and A. Rauber, “Music genre classiﬁcation \nby ensembles of audio and lyrics features,” Proc. of the Int. Soc. for Music Inform. Retrieval Conf. , pp. \n675–680, 2011. \n[18] C . - C .  M .  Y e h  a n d  Y . - H .  Y a n g ,  “ T o w a r d s  a  m o r e  \neﬃcient sparse coding based audio-word feature ex-\ntraction system,” Proc. Asia-Paci ﬁc Signal and In-\nformation Processing Association Annual Summit and Conf. , pp. 1–7, 2013. \n[19] C.-C. M. Yeh, L. Su, and Y.-H. Yang, “Dual-layer \nbag-of-frames model for music genre classiﬁcation,” Proc. IEEE Int. Conf. on Acoust, Speech and Signal \nProcess , pp. 246–250, 2013. \n[20] M.-J. Wu, Z.-S. Chen, and J.-S. R. Jang, “Combining \nv i s u a l  a n d  a c o u s t i c  f e a t u r e s  f o r  m u s i c  g e n r e  \nclassiﬁcation,” Proc. IEEE Int. Conf. on Machine \nLearning and Applications and Workshops , pp. 124–\n129, 2011. \n[21] Yen, Frederick, Yin-Jyun Luo, and Tai-Shih Chi. \n\"Singing Voice Separation Using Spectro- Temporal \nModulation Features.\" Proc. of the Int. Soc. for Mu-\nsic Inform. Retrieval Conf. , pp. 617–622, 2014. \n[22] [Online]https://code.soundsoftware.ac.uk/projects/co\nnstant-q-toolbox. \n[23] Schörkhuber, Christian, and Anssi Klapuri. \"Con-\nstant-Q transform toolbox for music processing.\" \nProc.  of Sound and Music Computing Conference, \npp. 3-64, 2010. \n[24] J. C. Brown and M. S. Puckette, “An efﬁcient algo-\nrithm for the calculation of a constant q transform,” \nThe Journal of the Acoustical Society of America , vol. \n92, no. 5, pp. 2698–2701, 1992. \n[25] [Online]http://www.isr.umd.edu/Labs/NSL/Downloa\nds.html \n[26] T. Chi, P. Ru, and S. A. Shamma, “Multiresolution \nspectrotemporal analysis of complex sounds,” The \nJournal of the Acoustical Society of America , vol. \n118, no. 2, pp. 887–906, 2005. \n[27] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani, \n“Least angle regression,” The Annals of Statistics , \nvol. 32, no. 2, p p. 407–499, 2004. \n[28] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online \ndictionary learning for sparse coding,” Proc. of Int. \nConf. on Machine Learning , pp. 689–696, 2009. \n[29] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online \nlearning for matrix factori zation and sparse coding,” \nThe Journal of Machine Learning Research , pp. 19–\n60, 2010. \n[30] [Online]http://spams-\ndevel.gforge.inria.fr/downloads.html \n[31] Chang, Chih-Chung, and Chih-Jen Lin. \"LIBSVM: a \nlibrary for support vector machines.\" ACM Transac-\ntions on Intelligent Systems and Technology, vol. 2, \nno.3, Article 27, 2011. \n \n 750 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "WiMIR: An Informetric Study On Women Authors In ISMIR.",
        "author": [
            "Xiao Hu 0001",
            "Kahyun Choi",
            "Jin Ha Lee 0001",
            "Audrey Laplante",
            "Yun Hao",
            "Sally Jo Cunningham",
            "J. Stephen Downie"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1414832",
        "url": "https://doi.org/10.5281/zenodo.1414832",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/283_Paper.pdf",
        "abstract": "The Music Information Retrieval (MIR) community is becoming increasingly aware of a gender imbalance evi- dent in ISMIR participation and publication. This paper reports upon a comprehensive informetric study of the publication, authorship and citation characteristics of fe- male researchers in the context of the ISMIR confer- ences. All 1,610 papers in the ISMIR proceedings written by 1,910 unique authors from 2000 to 2015 were collect- ed and analyzed. Only 14.1% of all papers were led by female researchers. Temporal analysis shows that the percentage of lead female authors has not improved over the years, but more papers have appeared with female co- authors in very recent years. Topics and citation numbers are also analyzed and compared between female and male authors to identify research emphasis and to measure im- pact. The results show that the most prolific authors of both genders published similar numbers of ISMIR papers and the citation counts of lead authors in both genders had no significant difference. We also analyzed the col- laboration patterns to discover whether gender is related to the number of collaborators. Implications of these find- ings are discussed and suggestions are proposed on how to continue encouraging and supporting female participa- tion in the MIR field.",
        "zenodo_id": 1414832,
        "dblp_key": "conf/ismir/HuCLLHCD16",
        "content": "WIMIR: AN INFORMETRIC STUDY ON WOMEN AUTHORS \nIN ISMIR  \nXiao Hu1             Kahyun Choi2           Jin Ha Lee3          Audrey Laplante4     Yun Hao2 \nSally Jo Cunningham5                 J. Stephen Downie2 \n1University of Hong Kong  \nxiaoxhu@ hku.hk 2Universit y of Illinois  \n{ckahyu2,yunhao2, \njdownie}@ illinois.edu   3Univeristy of Washington  \njinhalee @uw.edu \n4 Université de Montréal  \naudrey.laplante@umontreal.ca 5University of Waikato  \nsallyjo@waikato.ac.nz  \nABSTRACT  \nThe Music Information Retrieval (MIR) community is \nbecoming increasingly aware of a gender imbalance ev i-\ndent in ISMIR participation and publication. This paper \nreports upon a comprehensive informetric study of the \npublication, authorship and citation  characteristics  of fe-\nmale researchers in the context of the ISMIR confe r-\nences. All 1 ,610 papers in the ISMIR proceedings written \nby 1,910 unique  authors from 2000 to 2015 were collec t-\ned and analyzed. Only 14.1 % of all papers  were led by \nfemale researchers. Temporal analysis shows that the \npercentage of  lead female authors has not improved over \nthe years, but more papers have  appeared with female co -\nauthors in very recent years. Topics and citation  numbers \nare also analyzed and compared between female and male \nauthors to  identify research emphasis and to measu re im-\npact. The results show that the  most prolific authors of \nboth genders published similar numbers of ISMIR papers  \nand the citation counts of lead authors in both genders \nhad no significant  difference. We also analyzed the co l-\nlaboration patterns to disco ver whether  gender is related \nto the number of collaborators. Implications of these  find-\nings are discussed and suggestions are proposed on how \nto continue  encouraging and supporting female particip a-\ntion in the MIR field.  \n1. INTRODUCTION  \nMusic Information Retr ieval (MIR) is a  highly interdisc i-\nplinary field with researchers from multiple domains i n-\ncluding Electrical Engineering, Computer Science, I n-\nformation Science, Musicology  and Music Theory , Psy-\nchology, and so on . Perhaps due to the strong represent a-\ntion fro m the technical domains, the MIR field has been \ndominated by male researchers  [12]. Responding to this \ntrend, supporting female researchers has emerged as an \nimportant agenda for the MIR community. Since 20 11, \n\"Women in MIR\" (W iMIR) session s have  been organized \nin order to identify current issues and challenges female \nMIR researchers face, and to brainstorm ideas for provi d-\ning more support to female MIR researchers. The se s-sions have been well attended by both female and male \nparticipants every year, and a number of initiatives have \nbeen started for ensuring the inclusion of female r e-\nsearchers in various leadership roles such as session \nchairs, conference and program chairs, reviewers and m e-\nta-reviewers, as well as  ISMIR board m embers . In add i-\ntion, a mentorship program  targeted  for junior female \nmentees has recently been established .1 \nWhile we continue encouraging  young female st u-\ndents to enter the field, we lack a solid unders tanding of \nwhere our current  female researchers come from, what \ntheir research strengths are, who they collaborate with \nand what their impact has been in the field. This makes it \ndifficult to establish a mentoring relationship between \nthese young researchers  and established scholar s, which  \nhas been identifie d as being critical for increasing the \nrepresentation of female scholars and retaining them in \nthe field. As an effort to provide useful empirical data to \nsupport such initiatives, this paper reports an informetric  \nstudy analyzing the publication, authorsh ip and citation  \npattern s of female researchers in the context of the \nISMIR conferences.  \n2. RELATED  WORK  \n2.1 Informetric S tudies in MIR  \nA few studies in MIR have used citation analysis  (exam-\nining publication and  citation counts , and co-citation pa t-\nterns ) and c o-authorship analysis to measure the impact \nof individual papers or authors and understand the pa t-\nterns of publicati on. Lee, Jones , and Downie [12] con-\nducted a citation analysis of ISMIR proceedings from \n2000 to 2 008, aiming to discover how the publication pa t-\nterns have changed over time. They were able to identify \nthe top 22 authors with the largest number of distinct co -\nauthors, distinguish  the commonly used title terms r e-\nflecting the research foci in  the ISMIR c ommunity, and \nreveal  the increasing co -authorship among the MIR \nscholars. Lee and Cunningham [13] specifically exa m-\nined 198 user studies in MIR and analyzed the overall \ngrowth, publication and citation patterns,  popular topics, \nand methods employed. They found that overall the nu m-\nber of user studies increased, but not the ratio of user \nstudies published in ISMIR proceedings over time. Add i-\ntionally, they were able to identify a few strong networks \n                                                           \n1 http://www.ismir.net/wimir.html   © Xiao Hu, Kahyun Choi, Jin Ha Lee, Audrey Laplante, \nYun Hao, Sally Jo Cunningham and J. Stephen Downie . Licensed under \na Creative Co mmons Attribution 4.0 International License (CC BY 4.0). \nAttribution:  Xiao Hu, Kahyun Choi, Jin Ha Lee, Audrey Laplante, Yun \nHao, Sally Jo Cunningham and J. Stephen Downie . “WiMIR: An I n-\nformetric Study on Women Authors in ISMIR ”, 17th International Soc i-\nety for Music Information Retrieval Conference, 2016.  \n765  \n \nof co -authorship  based on universities and labs, and also \nfound that many of the studies heavily focused on expe r-\niment and usability testing.  Another study by these a u-\nthors [4] applied informetric  method s to investigate the \ninfluence of ISMIR and MIREX research on patents, \nthrough citation and topic analysis. The results showed \nevidence of strong links between academic and comme r-\ncial MIR research.  Very recently, Sordo et al. [18] ana-\nlyzed the evolution of topics and  co-authorship networks \nin the ISMIR conference , and found larger groups with \nmore variability of topics made more impact to the field . \nNotwithstanding the significance of these studies in \nmeasuring the status and impact of the field, there has not \nbeen an y study  focusing  on the gender disparities in MIR \nresearch. The role of gender in scholarly research and a c-\nademic career has been a long standing topic in many \nfields , as briefly summarized in the next subsection.   \n2.2 Female A uthor s and Scholarly Research  \nAlthough there is abundant re search showing that female \nresearchers  are as devoted as male researchers  in the goal \nof discovery [17][19], women researchers are underrepr e-\nsented in almost all discipline s, especially in science, \ntechnology, engineering , and mathematics (STEM) [15]. \nIn a recent study, Sugimoto  et al. [11] analyzed  5,483,841 \narticles published over the period 2008 -2012, from the \nWeb of Science d atabase with over 27 million autho r-\nships. They found that only 30% of the authors were f e-\nmale , but surprisingly female authors dominated in some \ncountries, such as Latvia and Ukraine. Kosmulski [10] \nstudied publicat ion patterns of scholars in Poland and \nfound that female scientists in Poland published less  than \ntheir male counterparts . However, an examination of \nyearly statistics reveals a trend moving towards gender \nequalization  in recent years . Another study b y Aksnes et \nal. [1] analyzed  the publications of 8,500 Norwegian r e-\nsearchers from  all disci plines. Findings showed  that fe-\nmale researchers published significantly fewer  papers \nthan their male counterparts, but the difference in cita tion \nrate was not as salient. They also found that among the \nmost productive researchers, women perform as well as \nmen do.  Female researchers were even found to be more \nhighly cited than male researchers in physical scie nces, \nincluding computer science, informatics, and engi neering . \nConversely, a study of gender -based citation patterns in \nthe field of International Relations [14] found that women \nwere cited significantly less than men, even after contro l-\nling for variables includ ing tenure status, institution, and \nyear of publication. This discrepancy was identified as \npartly due to gender -based self -citation patterns (where \nmen tend to self -cite more than women) and to a tende n-\ncy for men to cite other men proportionately more tha n \nwomen —perhaps indicating that social networks can \nhave an impact on citation practices.  \n3. DATA COLLECTION AND PREPROCESSING  \nTwo sources were used to collect the titles of ISMIR p a-\npers and their authors : the ISMIR online proceed ings and \nthe ISMIR conference  web pages. First , bibliographic \nrecords of papers  published between 2000 and 2011 were downloaded from the Cumulative ISMIR Proceedings  \ndatabase1, which support s export in CSV format . Records \nfor papers  published between 2012 and 2015  were co l-\nlected by cr awling  the program webpages of the confe r-\nences since they were  only included in  dblp2, which does \nnot provide a function for exporting multiple records . The \ncrawled raw HTML pages were then  parsed with regular \nexpression s, to extract titles and author name s.  \n3.1 Standardization and Deduplication of Names  \nThe downloaded author names needed to be standardized \nin several aspects. First, some names were inverted with \nthe last name first. Second, some authors varied the form \nof their name across multiple papers (e.g., including  or \nomitting middle name initials ). Third, dia critic letters in \nnames were occasionally replaced by English  letters . Be-\nsides manual inspection of these cases, we al so made use \nof OpenRefine3, a tool for data clea nsing and exploration, \nto help identify similar  forms of names. Once different \nforms of a same name were identified, we kept the most \nfrequently used version and remove d others as duplicates.   \n3.2 Author Gender Identification  \nWe manually determined and labelled the gender of each \nauthor ba sed on their names. Some first names are excl u-\nsively or almost exclusively used for one gender  (e.g.,  \nSusan , Marie , and Yumi  are female names  by conve n-\ntion) . Some names are almost exclusively attributed to \nmales in one language but to females in another la nguage \n(e.g., Rene  is a male name in French but a female name in \nEnglish). In these cases, we tried to determine the gender  \nof authors , taking into account their cultural origin.  How-\never,  many first names are androgynous, especially Ch i-\nnese names  whose  English written form s represent the \npronunciation s rather than the meaning s, which makes \ndetermining the gender of  those names difficult. To a d-\ndress that, we relied on our collective knowledge of \nISMIR authors and we use d the affiliation information to \nsearch  for these authors on the Web. Nevertheless, this \ndid not allow us to assign genders to all authors. The last \nstep was to send a call through the ISMIR mailing list to \nask the community to help determine the gender of the \nauthors  we could not identify . We also directly contacted \na few authors and labs when possible. In the end, we \nwere able to determine the gender of 1 ,863 (97.54%) out \nof a total of the 1 ,910 unique authors on the list.  \n3.3 Citation Counts  \nGoogle Scholar (GS)4 was used as the source of citation  \ndata for this study, since ISMIR proceedings are not i n-\ndexed in Web of Science (WoS) or Scopus, the two other \nmain sources of citation data for scholarly works. Studies \nhave shown that GS coverage had grown substantially \nsince its launch [21] and now even surpasses WoS cove r-\nage in certain disciplines, including Computer Science \n                                                           \n1 http:// www.ismi r.net/proceedings/  \n2 http://dblp.uni -trier.de/db/conf/ismir/index.html  \n3 http://openrefine.org  \n4 https://scholar.google.com/  766 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016  \n \n[7]. It also indexes a wider variety of academic sources, \nincluding more books, conference papers, and working \npapers than WoS [8]. As a result, GS has been consider ed \nas a reliable source of citation data and an adequate alte r-\nnative to WoS for research evaluation  [7][8]. Since GS \ndoes not o ffer a function for exporting multiple records at \nonce, we used Publish or Per ish1, an open source sof t-\nware tool, to retrieve the citation data for ISMIR papers.  \n3.4 Limitations  \nAs mentioned previously, we relied on name convention \nto determine the gender of a large proportion of the a u-\nthors. It is possible  that some of the authors from one \ngender had a first name more traditionally attributed to \nthe other gender and have thus been mislabelled. More o-\nver, a high proportion of the 2.5 6% (48) of authors whose \ngender could not be determined are of Chinese origin. \nTher efore, Chinese authors are underrepresented in our \ndataset.  Moreover , our work only focuses on two genders  \n(i.e., male and female) based on name conve ntion and no \nother gender identity. It is possible that some of these a u-\nthors identify as neither male nor female and we are not \nable to represent that  information  in our anal ysis.  \nThe use of GS brings additional limitations.  Alt-\nhough GS is considered by researchers an adequate \nsource of citation data for  research evaluation, it still has \nsome weaknesses. Research shows that the database co n-\ntains many errors such as duplicates and false positive \ncitations [21] which can potentially inflate the number of \ncitations, but we have n o reason to believe that this would \naffect male - and female -led papers differently. Finally, \nISMIR workshops were not consistently  indexed in GS, \nand thus  we had no citation data for 35 papers.   \n4. RESULTS  \n4.1 Number of Authors and Publications   \nThere are 1 ,910 u nique authors who published at least \none paper in ISMIR  proceedings  from 2000 to 2015. The \ngender infor mation of 1 ,863 (97.54% ) authors  was ident i-\nfied. Among the identified authors, 274 (14.71 %) were  \nfemale and 1 ,589 (85.2 9%) were  male. There were 1 ,610 \npapers published  over the years. Among them, 389  papers \n(24.2% ) had female co -authors , 227 (14.1 %) had female \nfirst authors, compared to 1 ,188 (73.8 %) papers without \nany female authors and 1 ,362 (84.6 %) led by male a u-\nthors. While the number of female authors  did increase \nover time, the total number of ISMIR papers and male \nauthors also significantly increased  [12]. Figure 1 shows \nthe percentage of papers with male and female first a u-\nthors over the years as well as those with and witho ut fe-\nmale authors. There is virtually no improvement over the \nyears in terms of the proportion  of papers led by female \nauthors, but more papers with female co -authors appeared \nin recent years (2014 and 2015).  \nFigure 2 compares the number of papers led by f e-\nmale versus male researchers in histograms. The most \nproliferate female and male researchers had led almost \n                                                           \n1 http://www.harzing.com/resources/publish -or-perish  equal number of papers, with 13 papers by Jin Ha Lee \n(female) and 12 by Xiao Hu (female) compared to 14 by \nMeinard Müller (male). This demonstrates  a similar pa t-\ntern to the  finding  in [1] that the most productive women \nand men researchers perform equally  well. \n \nFigure 1 . Proportion of ISMIR papers by each gen der. \n \nFigure 2 . Number of ISMIR papers led by each gender . \n4.2 Institutions and Disciplines of Female A uthors  \nThe 227  papers with female first authors ( including  single \nauthored papers)  were analyzed  to identify the  instit u-\ntions and disciplines of the first author s at the time of \npublica tion. Table 1 shows the institutions with the larg-\nest number of such papers. The ranks  of the top three i n-\nstitutions were in fact earned by their  female students, as \nno female researchers with pe rmanent positions in these \ninstitution s has led an ISMIR pape r. This is evidence of a \nstrong contribution that female students made to the field , \nsupporting the importance of foster ing the growth of ju n-\nior female researchers through mentorship program s. \nInstitutions  Number of papers  \nUniversity of Illinois  12 \nQueen  Mary University of London  10 \nMcGill University  9 \nUniversity of Washington  9 \nIndiana University  8 \nUniversity of Waikato  8 \nUniversity of Southern California  7 \nFraunhofer IDMT  6 \nGoldsmiths, University of London  5 \nPompeu Fabra University  5 \nStanford U niversity  5 \nUtrecht University  5 \nTable 1. Institutions with the most papers of first  female \nauthors.  \nProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 767  \n \nTable 2 lists the most frequent disciplines of female \nauthors who led ISMIR papers. The discipline info r-\nmation was obtained from the departments of the female \nfirst authors’ affiliations as written in the papers. The di s-\nciplines were cleaned up such that closely related one s \nwere combined. For example, “Computer Science and \nInformatics” was combined with “Computer Science”, \nand “Aud io and Speech Processing” was combined with \n“Electronical E ngineering”. The most popular discipline \nis Computer Science, following by Library and Info r-\nmation Science and Music Technology. The latter two  \nwere interdisciplinary fields  which  historically had \nstronger female representations  [16]. When looking at \nTable s 1 and 2 together , papers from some top ranked i n-\nstitutions were contributed from authors in  Library and \nInformation Science (Univer sity of Illinois, University of \nWashington) or Music Technology ( Queen Mary Unive r-\nsity of London , McGill University), rather than the Eng i-\nneering disciplines predominant in the field. The results \nindicate that it can be promising  to try to  foster  more fe-\nmale  contributors to  ISMIR from th ese disciplines.  \n \nDiscipline  Number of papers  \nComputer Science  87 \nLibrary and Information Science  44 \nMusic Technology  40 \nElectrical Engineering  18 \nMusicology  and Music Theory  12 \nTable 2. Top disciplines of first female authors.  \nFrom the affiliations of the female authors, we ide n-\ntified the geographic locations of the authors, as shown in \nTable 3. Unsurprisingly, most of them were in North \nAmerica and Europe, in accordance with the fact that \nmost labs in the MIR field are located i n these areas. \nThese are followed by Asia and Pacific region with 39 \npapers led by female authors. Promoting international \ncollaborations between regions with more established and \nreputable research facilities and other emerging but less \ndeveloped regions can be a fruitful approach for fostering \nfemale researchers in the field. Sugimoto et al. [11] also \nadvocated international collaboration as “it might help to \nlevel the playing field” (p.213). This observation m ay al-\nso be related to a study by Ferreira [6] which reported \nthat a steady growth of PhD dissertations written by f e-\nmale in the U.S. was observed, but the increase was a t-\ntributed to international female research  students who \ncame from other parts of the world including Asia. Al t-\nhough further studies are warranted to verify whether this \ntrend also holds for ISMIR authors, in our dataset we did \nobserve circumstantial evidence in that many female a u-\nthor names with A sian origins were based at institutions \nin Europe or North America.  \n4.3 Co-authorship  \nAmong all the papers led by female and male authors, the \naverage number of co -authors is 2.69  and 2.86, respe c-\ntively. A two -sample unequal variance t -test reported a \nnon-significant difference between the two ( p = 0.2 89). \nFigure 3 illustrate s the co -authorship trend over the year s. In general, papers led by authors in either gender tend to \nhave an increasing number of co -authors.  \nContinent  Number of papers  \nNorth America  96 \nEurope  90 \nAsia 28 \nOceania  11 \nSouth America  2 \nTotal  227 \nTable 3. Continents of female leading authors.  \n \nFigure 3 . Number of co -authors per paper  (2000 -2015) . \nThere were 214 single authored papers : 35 (16.3%) \nof them written by f emale authors and 179 (83.7%) by \nmale s. This percentage of 16.3% is lower than what was \nreported in [20] in which they found that 26% of single -\nauthored papers published in the JSTOR network dat a-\nbases since 1990 were contribute d by female authors. In \nour dataset, 22 (8.0%) of female authors had single -\nauthored one or more ISMIR papers, whereas 129 (8.1%) \nof male authors ha d done so. The results indicate that \nboth female and male authors reach out for collabor a-\ntions, perhaps due to the interdisciplinary nature of the \nMIR field. In addition, similar percentages of female and \nmale authors opted t o write single authored papers.  \nWe also conduct ed social network analysis (SNA) on \nthe co -authorship networks of female researchers and \ntheir collaborators, to find out with which authors the f e-\nmale researchers most frequently  collaborated. Figure 4 \nshows the network graphs (generated by using the N o-\ndeXL SNA tool [8]). The graph s’ node s represent r e-\nsearchers who w ere grouped into clusters by using the \nClauset -Newman -Moore cluster ing algorithm  [3], such \nthat the authors who often collaborated with each other \nwere grouped into a single cluster. The siz e of a node  is \nproportional to the  number of papers written by the  re-\nsearcher . Figure 4 contains nine clusters , each of which \nhas at least five female authors. Each female author in the \ngraphs is represented by a node of diamond shape and the \nname is marked with a n asterisk.  In each graph, th e \nnames of authors with the most  co-authors are labelled.  \nAs shown in Figure 4, some clusters contain mult i-\nple female authors with a relatively high number of pu b-\nlications, such as the one with Jin Ha Lee , Xiao Hu  and \nSally Jo Cunningham , as well as the on e with Rebecca \n768 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016  \n \nFiebrink, Catherine Lai, etc. This can probably be a t-\ntributed to the research groups these authors were affilia t-\ned with , as this  result corresponds to the pattern observed \nin Table 1 : the two clusters match to the research groups \nin Universi ty of Illinois and McGill University, respe c-\ntively. Other clusters shown in this figure also reflect  re-search group s such as the Music Technology Group in \nPompeu Fabra University (the cluster with Emilia \nGomez) and Utrecht University (the cluster with Anja  \nVolk). This pattern once again verifies the importan ce of \nhaving research labs or groups that can foster the growth \nof female researchers . \n \nFigure 4.  Co-authorship networks of ISMIR female authors (groups with at least five female authors are presente d) \n4.4 Citation  analysis  \nThe average citation count  of all papers with female  re-\nsearcher as the  leading authors is 34.30 . Although this \nnumber is lower than that of papers with male leading \nauthors (43. 26), the difference was not significant ( p = \n0.259 ). When c onsidering citation counts of single -\nauthored papers, the difference is even smaller: 22.25 \nversus 25.27 for female and male authors , respectively.  \nFigure 5 shows the comparative distribution of p a-\npers led by female and male authors by number of cit a-\ntions.  A chi -square independence test shows that the two \ndistributions are very similar  (χ2 = 11.124, df = 8, p = \n0.195).  The proportion of papers with no citation is the \nsame for both groups (9%). The proportion of highly ci t-\ned papers  (papers cited more than 100 times)  is also very \nsimilar, representing 10% of female first -authored papers \nand 9% of male first -authored papers.   \nThese results indicate  that the scholarly impact of a u-\nthors in both genders is similar. Although previous  stud-\nies found that the difference between  citation rates for \nmale - and female -led papers  was smaller than that be-\ntween  publication rates [1], it is unusual to see no signif i-\ncant difference on citation rate between genders . The \nlarge scale study by Sugimoto  et al.  [11] found there were \nfewer cita tions for papers with female being sole author , \nfirst author  or last author  than in cases where a man was \nin one of these roles. As Sugimoto and her colleagues  \nworked with more than 5 million of papers  across all di s-ciplines, it is an encouraging result th at such gender di s-\nparity in scholarly impact as measured by citation counts \nis not significant in MIR.  \n \nFigure 5.  Distribution of female and male -led \nISMIR papers by number of citations . \n4.5 Topics  \nTopic analysis was conducted with the titles of the p a-\npers, to identify  the topics female authors tended to pur-\nsue. Both single terms  (unigram s) in the titles and comb i-\nnations of two consecutive terms ( bigra ms) were  extrac t-\ned. To combine different forms of the same word prefix, \nthe Porter stemmer was used. Stop  word s were also eli m-\ninated , as was the word “music” as it is related to all p a-\npers in ISMIR . The most frequently used title words \n(unigrams) are presented in Table 4 . For comparison pu r-\nposes, the table include s the top title words for six paper \nProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 769  \n \ncategories : all papers, papers with female lead authors, \npapers with  at least one female author (but not the lead) , \npapers with no female authors, papers with male lead a u-\nthors, as well as papers written by teams of all female a u-\nthors. As such,  there are  overlap ping pape rs between the \n“Female 1st” and “All Female ” categories, and between \nthe “Female non -1st” and “Male 1st” categories . \nThe first five columns in Table 4 show similar \nwords such as “audio”, “retriev” (retrieval”), “classif” \n(classify) . One exception is the fe male non -1st author ca t-\negory that contains terms such as “detect”, “evalu” (eva l-\nuation), and “record”, which suggests that female r e-\nsearchers collaborated with male to work on key dete c-\ntion and evaluation. Several terms from all female teams \nare quite diff erent from those in other categories (e.g., \n“digit”, “user”, “kei” (key) and “evalu”), which suggests \nthat areas in which female authors worked together i n-\nclude user studies, key detection and evaluation.  \nAs single words may bear limited semantics, we a l-\nso extracted b igram s from the paper titles of the afor e-\nmentioned six categories of gender authorship . Bigrams \nare two consecutive words which are often phrases, and \nthus may provide more meanings than unigrams. As \nlisted in Table 5, the differences of bigram s across paper \ncategories are even more obvious than those of unigrams. \nPapers with all female authors were most likely to be on  \naudio key finding, digital libraries, melody extraction, \nand user studies. Papers led by female authors were more likely to foc us on melody similarity, mood classification, \nretrieval systems, corpus and data sources, as well as \ncross -cultural issues. In contrast, male researchers were \nmore likely to write about Markov model, audio signals, \naudio featu res, and Web -based approaches.  These diffe r-\nences in focus may reflect the distribution of represent a-\ntion of women in Computer Science and Engineering, \nwhere proportionately more women in those fields focus \non Human -Computer Interaction (e.g., user studies, cross -\ncultural issues, digita l libraries) rather than signal pr o-\ncessing (e.g., audio signals, audio features) [2]. \nAll Female   \n1st  Female \nnon-1st All male  Male 1st All Female  \naudio audio  retriev  audio  audio  inform  \nretriev  retriev  audio  retriev  retriev  retriev  \ninform  inform  classif  model  similar  digit  \nautoma t classif  inform  featur  featur  similar  \nclassif  similar  analysi  similar  classif  user \nsimilar  model  detect  analysi  automat  audio  \nfeatur  automat  evalu  automat  inform  kei \nanalysi  polyphon  record  inform  analysi  evalu  \nrecognit  song  system  classif  system  extract  \npolyphon  featur  feature  system  recognit  find \nTable 4.  Most frequent words in paper titles (terms \nunique to female authors are bolded) . \n  \nAll Female 1st Female non -1st  All mal e Male 1st All Female  \ncontent -bas inform_retriev  inform_retriev  content -bas content -bas inform_retriev  \npolyphonic_audio  genr _classif  polyphon_audio  audio_signal  non-negativ  audio_kei  \nreal-tim melod _similar  genr _classif  markov_model  polyphon_audio  digit_ librari  \nnon-negativ  classif _us audio_record  web-bas audio_signal  kei_find  \nmarkov_model  content -bas auditori _model  audio_feature  markov_model  melodi_extract  \naudio_feature  mood _classif  base_transcrib  audio_us  audio_feature  understand_user  \naudio_signal  retriev _system  classif_us  audio -bas web-bas  \naudio_fingerprint  comput _model  corpus -bas polyphonic_audio  audio_record   \naudio_record  cross -cultur  data_sourc  audio_record  audio_us   \nautomati_chord  machin _learn  digit_imag  score_inform  audio -bas  \nTable 5 . Most  frequent bigrams in titles of papers  (terms unique to female authors are bolded).\n5. CONCLUSION  AND FUTURE WORK  \nOverall our findings show both positive and negative  as-\npects related to gender balance issues in MIR. While it is \ndiscouraging that the participat ion of female authors has \nhovered around 10-20% throughout the history of ISMIR  \nwithout much improvement over time, we also see that \nthe most prolific authors of both genders are similarly \nproductive and papers led by both genders are cited at \nsimilar rate s. Our analysis highlight s the importance of \nthe role of mentorship through co -authoring papers and \nalso being part of the same labs or research groups for \nincreasing the number of female scholars in the field. In-\nternational collaborations connecting  femal e researchers \nin less represented regions with more established groups \ncan be a promising  approach. In addition, we may en-\ncourage and attract female contributors from interdisc i-\nplinary  disciplines historically with better female repr e-\nsentations such as Inf ormation Science and Music Tec h-nology. Promoting research in these areas (whether by \nmale or female authors) has also been identified as an \nimportant step forward for the field of MIR [5][12]—it is \ncrucial to the development of usable, effective music sy s-\ntems that we understand our users and their needs, and \nwork to create new systems that integrate with both tec h-\nnological and social infrastructures.  \nIn order to conduct a more accura te inform etric \nstudy  in the future , it would be useful for the ISMIR pr o-\ngram committee to collect gender information during the \npaper submission process directly from the authors. This \nwill not only allow us to obtain a more accurate represe n-\ntation of the ISMIR community , but also enable the ana l-\nysis on paper rejection rates in terms of gender . We also \nrecommend the gathering of gender and research focus \ndata for program committee members, to examine the \npossible effect of gender in the gatekeeping aspect o f en-\ntry to the ISMIR community.  770 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016  \n \n6. REFERENCES  \n[1] D. W. Aksnes, K. Rorstad,  F. Piro, and G.  Sivertsen : \n“Are female researchers less cited? A large ‐scale \nstudy of Norwegian scientists ,” Journal of the \nAmerican Society for Information Science and \nTechnology , Vol.  62, No.  4, pp. 628–636, 2011.  \n[2] J. M. Cavero, B. Vela, P. Cáceres, C. Cuesta, and A. \nSierra -Alonso: “The evolution of female authorship \nin computing research,” Scientometrics , 103(1), 85 -\n100, 2015.  \n[3] A. Clauset, M. E. J. Newman,  and C . Moore: \n“Finding community  structure in very large \nnetworks,”  Physical R eview E 70.6 (2004): 066111.  \n[4] S. J. Cunningham  and J. H. Lee:  \"Influences of \nISMIR and MIREX research on technology patents.\" \nISMIR , pp. 137 -142. 2013.  \n[5] J. S. Downie , X. Hu, J. H. Lee, K. Cho i, S. J. \nCunningham, and Y. Hao: “Ten years of MIREX: \nReflections, challenges and opportunities.” ISMIR , \npp. 657 – 662, 2014 . \n[6] M. M. Ferreira:  “Trends in women's representation \nin science and engineering .” Journal of Women and \nMinorities in Science and Engineering , Vol. 15, No.  \n3, 2009.  \n[7] M. Franceschet: “A comparison of bibliometric \nindicators for computer science scholars and \njournals on Web of Science and Google Scholar,” \nScientometrics , vol. 83, no. 1, pp. 243 -258, 2009.  \n[8] D. L. Hansen, B. Schneiderman, and M. A. Smith: \nAnalyzin g Social Media Networks with Node XL: \nInsights from A Connected World , Burlington, MA: \nMorgan Kaufmann, 2011.  \n[9] A.-W. Harzing:  “A preliminary test of Google \nScholar as a source for citation data: a longitudinal \nstudy of Nobel prize winners,” Scientometrics, v ol. \n94, no. 3, pp. 1057 -1075, 2012 . \n[10] K. Kosmulski:  “Gender disparity in Polish science \nby year (1975 –2014) and by discipline .” Journal of \nInformetrics , Vol.  9, No.  3, pp. 658–666, 2015.  \n[11] V. Larivi ère, C. Q. Ni, Y. Gingras , B. Cronin  and C. \nR. Sugimoto : “Glob al gender disparities in science .” \nNatur e, Vol.  504, No.  7479, pp.  211–213, 2013.  \n[12] J. H. Lee, M. C. Jones and J. S. Downie: “ An \nAnalysis of ISMIR Proceedings: Patterns of \nAuthorship, Topic, and Citation. ” ISMIR , pp. 57–62, \n2009.  \n[13] J. H. Lee and S. J. Cunningh am: “Toward an \nunderstanding of the history and impact of user \nstudies in music information retrieval.” Journal of Intelligent Information Systems , Vol 41, No. 3, pp. \n499–511, 2013.  \n[14] D. Maliniak, R. Powers, and B. F. Walter: “The \ngender citation gap in inte rnational relations,” \nInternational Organization , 67(04), 889 -922, 2013.  \n[15] National Center for Education Statistics: \nPostsecondary Institutions in the United States: Fall \n2000 and degrees and other awards conferred \n1999–2000, Washington, DC, National Center for \nEducation Statistics, 2001.  \n[16] D, Rhoten. and S, Pfirman. : “Women in \ninterdisciplinary science: Exploring preferences and \nconsequences. ” 2010  Research policy , Vol.  36, No.  \n1, pp. 56–75, 200 7. \n[17] G. Sonnert and G. Holton: Gender Differences in \nScience Careers : The Project Access Study.  Rutgers \nUniversity Press, New Brunswick, NJ, 1995.  \n[18] M. Sordo, M . Ogihara, and S . Wuchty: “A nalysis of \nthe evolution of research groups and topics in the \nISMIR  conference .” ISMIR , pp. 204 – 210, 2015.  \n[19] R. Subotnik  and K. Arnold: “Passing through the \ngates: career establishment of talented women \nscientists ,” Roeper Review , Vol.  18, No.  1, pp.  55–\n61, 1995.  \n[20] J. D. West, J. Jacquet, M. M. King, S. J. Correll and \nC. T. Bergstrom: “ The role of  gender in scholarly \nauthorship,” PloS one , Vol . 8, No.  7, e66212 , 2013.  \n[21] J. C. F. Wint er, A. A. Zadpoor, and D. Dodou:  “The \nexpansion of Google Scholar versus Web of \nScience: a longitudinal study,” Scientometrics , vol. \n98, no. 2, pp. 1547 -1565, 2013.   Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 771"
    },
    {
        "title": "MusicDB: A Platform for Longitudinal Music Analytics.",
        "author": [
            "Jeremy Hyrkas",
            "Bill Howe"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417381",
        "url": "https://doi.org/10.5281/zenodo.1417381",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/284_Paper.pdf",
        "abstract": "With public data sources such as Million Song dataset, re- searchers can now study longitudinal questions about the patterns of popular music, but the scale and complexity of the data complicate analysis. We propose MusicDB, a new approach for longitudinal music analytics that adapts techniques from relational databases to the music setting. By representing song timeseries data relationally, we aim to dramatically decrease the programming effort required for complex analytics while significantly improving scal- ability. We show how our platform can improve perfor- mance by reducing the amount of data accessed for many common analytics tasks, and how such tasks can be imple- mented quickly in relational languages — variants of SQL. We further show that expressing music analytics tasks over relational representations allows the system to automati- cally parallelize and optimize the resulting programs to im- prove performance. We evaluate our system by expressing complex analytics tasks including calculating song density and beat-aligning features and showing significant perfor- mance improvements over previous results. Finally, we evaluate expressiveness by reproducing the results from a recent analysis of longitudinal music trends using the Mil- lion Song dataset.",
        "zenodo_id": 1417381,
        "dblp_key": "conf/ismir/HyrkasH16",
        "content": "MUSICDB: A PLATFORM FOR LONGITUDINAL MUSIC ANALYTICS\nJeremy Hyrkas\nUniversity of Washington\nhyrkas@cs.washington.eduBill Howe\nUniversity of Washington\nbillhowe@cs.washington.edu\nABSTRACT\nWith public data sources such as Million Song dataset, re-\nsearchers can now study longitudinal questions about the\npatterns of popular music, but the scale and complexity\nof the data complicate analysis. We propose MusicDB, a\nnew approach for longitudinal music analytics that adapts\ntechniques from relational databases to the music setting.\nBy representing song timeseries data relationally, we aim\nto dramatically decrease the programming effort required\nfor complex analytics while signiﬁcantly improving scal-\nability. We show how our platform can improve perfor-\nmance by reducing the amount of data accessed for many\ncommon analytics tasks, and how such tasks can be imple-\nmented quickly in relational languages — variants of SQL.\nWe further show that expressing music analytics tasks over\nrelational representations allows the system to automati-\ncally parallelize and optimize the resulting programs to im-\nprove performance. We evaluate our system by expressing\ncomplex analytics tasks including calculating song density\nand beat-aligning features and showing signiﬁcant perfor-\nmance improvements over previous results. Finally, we\nevaluate expressiveness by reproducing the results from a\nrecent analysis of longitudinal music trends using the Mil-\nlion Song dataset.\n1. INTRODUCTION\nOver the past decade, a concerted investment in building\nsystems to help extract knowledge from large, noisy, and\nheterogeneous datasets — big data — has had a transfor-\nmative effect on nearly every ﬁeld of science and industry.\nProgress has also been fueled by the availability of pub-\nlic datasets (e.g., the Netﬂix challenge [1], early releases\nof Twitter data [14], Google’s syntatic n-grams data [7],\netc.), which have focused and accelerated research in both\ndomain science and systems. The ﬁeld of Music Infor-\nmation Retrieval (MIR) appears to been less affected, as\ncomplications from copyright-encumbered properties have\nlimited the introduction of big datasets to the community.\nNow, however, such datasets are ﬁnally making their way\ninto the ﬁeld.\nIn other ﬁelds we have observed that as the data size\nc/circlecopyrtJeremy Hyrkas, Bill Howe.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Jeremy Hyrkas, Bill Howe. “Mu-\nsicDB: A Platform for Longitudinal Music Analytics”, 17th International\nSociety for Music Information Retrieval Conference, 2016.and scope of problems increased, issues of scale became\nthe bottleneck: single-site solutions written in R or python\ngive way to distributed shared-nothing systems that can\nreadily handle large datasets. These systems relieve the\nuser of worrying about issues such as memory manage-\nment, concurrency and distributed computing by focusing\non limited data models and APIs. General purpose systems\nsuch as Hadoop [21] and Spark [22] provide MapReduce-\nstyle dataﬂow computations [4], but can be hard to pro-\ngram and optimize because of their generality and rela-\ntively low-level interfaces. Increasingly, programmers are\nexperimenting with the models and languages of relational\ndatabases [11,12,17] for non-relational analytics over time-\nseries, graphs, images, and more due to their higher-level\nprogramming abstractions, simpler data models, and auto-\nmatic optimization.\nIn this paper, we propose a platform for longitudinal\nmusic analytics built on a relational big data system. Be-\ncause music data (which may include multi-dimensional\narrays and timeseries of extracted features) is ostensibly\nnot relational upon collection, we describe a ”relationaliza-\ntion” of the data to afford distributed, parallel processing.\nThen, we present four algorithms found in music analytics\ntasks in the MIR literature and show that they can be ex-\npressed in declarative relational languages similar to SQL,\naffording scalability, portability, and automatic optimiza-\ntion, and freeing the programmer from systematic concerns\nrelated to memory management, concurrency, and distributed\nprocessing. The four algorithms are song density, feature\nbeat-alignment, pitch keyword distribution by year, and\ntimbre keyword distribution by year. The ﬁrst two algo-\nrithms are common music analysis tasks, and the latter two\nare highly computational algorithms presented in a high\nproﬁle MIR study over the Million Song Data set [20]. We\nreproduce prior results with only a few lines of code and\na signiﬁcant performance improvement over prior experi-\nments on similar large-scale systems.\nWe propose our model as an approach for platforms to\nraise the level of abstraction for longitudinal music analyt-\nics and reduce the barrier to entry for researchers in musi-\ncology and sociology.\n1.1 Million Song Dataset\nThe key dataset used in our experiments is the The Million\nSong Dataset (MSD) [3]. The dataset includes metadata\nand extracted features from one million pop music songs\nfrom a period of decades, including information such as\ngenre tags, chroma measurements, timbre and loudness702measurements, detected beats, artist and song metadata such\nas year, duration and location, and many other attributes.\nThis dataset has been highly inﬂuential in the MIR commu-\nnity, leading to the Million Song Dataset Challenge [16],\nas well as being a key data set used to study music history\n[20] and MIR tasks such as cover song detection [2, 10].\nThe MSD is available as available as a large collection of\nHDF5 [5] ﬁles; at over 250GB even when heavily com-\npressed, it is the largest public dataset in MIR and is the\nﬁrst true “Big Data” dataset in the ﬁeld.\nThe MSD is available in a number of formats, including\na relational database. However, the database representa-\ntion include the metadata only, and cannot be used for the\ncontent-based longitudinal analytics tasks we aim to sup-\nport with MusicDB.\n2. RELATED WORK\nSerra et al. provide a longitudinal analysis of music trends\nin popular music by utilizing the MSD [20] . The au-\nthors study the chroma, timbre, and loudness components\nof the MSD and ﬁnd that the frequency of chroma key-\nwords (which can be thought of as single notes, chords,\netc) ﬁt a power law distribution which is mostly invariant\nacross time. However, the authors also ﬁnd that transi-\ntions from one keyword to the next have become more uni-\nform over time, suggesting less complexity in newer mu-\nsic. They also describe shifting trends in timbre and loud-\nness, including numerical evidence that recorded music is\nincreasingly louder on average. In Section 4 we describe\nthese tasks in detail and present new algorithms for them\nusing MusicDB. In Section 5 we evaluate our approach ex-\nperimentally.\nRaffek and Ellis analyzed MIDI ﬁles and matched them\nto corresponding songs in the MSD [18] . Their algorithm\nis fast but is not distributed; they estimate that running their\napproach on roughly 140k MIDI ﬁles against the MSD\nwould take multiple weeks even when parallelized on their\nmulti-threaded processor with 12 threads.\nBertin-Mahieux and Ellis described a method for ﬁnd-\ning cover songs in the MSD [2]. The method begins with\nan aggressive ﬁltering step, which requires computing jump-\ncodes for the entire MSD and storing them in a SQLite\ndatabase. Once the number of potential matches for a new\nsong is ﬁltered, a more accurate matching process is run\nto ﬁnd cover songs. Using three cores, they computed\nthe jumpcodes for the entire MSD in roughly three days,\nalthough matching cover songs once the jumpcodes are\ncomputed takes roughly a second per new song. The au-\nthors also mention that the jumpcodes had to be stored in\nmany different SQLite tables, as they were unable to index\nroughly 1.5M codes in a single table. MusicDB provides\na platform that can process the data directly, in parallel,\nwithout specialized engineering.\nHumphrey, Nieto, and Bello also provide a method to\ndetect cover songs [10]. Their method starts by transform-\ning beat-aligned chroma of a song into a high-dimensional,\nsparse representation by projecting its 2D Fourier Magni-\ntude Coefﬁcients. They then use PCA to reduce dimen-sionality and use the results to ﬁnd cover songs using a\ndistance function. The authors claim that using ten threads\non a machine with “plenty of RAM”, various methods can\ntake between 3-8 hours to complete this computation on\nthe MSD.\nHauger et al. describe the million musical tweets dataset\n(MMTD) [9] collected from tweets with information about\na user’s location and what they were listening to at a certain\ntime. This dataset, as well as others, can be used to aug-\nment the MSD for new MIR tasks. As it exists, the MSD\nis available as a directory hierarchy with hundreds of gi-\ngabytes of HDF5 ﬁles stored on AWS. Incorporating new\ndata in analysis tasks over this dataset requires additional\neffort in the analysis pipeline, leaving either the authors of\nthe data or the users of the data to write new code to han-\ndle the new data source and manually join it with the MSD.\nMusicDB provides a scalable substrate for such integration\ntasks.\n3. DATA MODEL\nA key step in efﬁciently analyzing the MSD is to repre-\nsent its information in an appropriate data model. The rep-\nresentation of the MSD available on the website (on mil-\nlion HDF5 ﬁles) support efﬁcient lookup by ID, but any\nmore complex processing requires custom programs to be\nwritten, and parallelization, concurrency, distribution, and\nmemory management are all the direct responsibility of the\nprogrammer. Moreover, tasks that require only a portion of\nmetadata from each song must still access and load all song\ndata from disk.\nInstead, we can organize the music data as sets of records.\nIn practice, this ”relationalization” of timeseries and mul-\ntidimensional data can signiﬁcantly increase the size of the\ndataset. In our work, the end size of our relationalized\ndata is roughly 500GB, about twice as large as the orig-\ninal dataset. However, all applications we have observed\ndo not require the entire MSD, and the subset of relation-\nalized data necessary for computation is much smaller than\nthe entire MSD in HDF5 format. Further, representing the\ndata as a set of records affords automatic partitioning and\nparallel processing, as we will see.\nThe steps to relationalization are as follows:\n•Metadata that appears only once per song is inserted\ninto one table (songs), with song ID as the key. This\nincludes ﬁelds such as song duration, artist name,\nsong name, etc.\n•Nested ﬁelds are represented in separate tables, re-\ntaining a foreign key to the songs table. To repre-\nsent the order within the nested ﬁeld, an additiona\ncolumn is added. For example, each song segment\nis represented as a record (song id, segment number,\nvalue , where segment number explicitly encodes the\nimplicit order in the original array. This additional\nﬁeld is one source of the space overhead we ﬁnd in\npractice.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 703•We use additional tables to support speciﬁc compo-\nnents of the MSD, including a separate table for each\nof the following: beat-aligned chroma features, beat-\naligned timbre features, and beat-aligned chroma fea-\ntures that have been transposed such that most songs\nare in C major or C minor.\ntable key arity non-key ﬁelds\nsongs song id 33 duration, key,\ntempo, etc\nsegments song id,\nsegnum31 timbre, loudness,\nand pitch measure-\nments\nmbtags song id, tag 3 tag count\nbars song id,\nbarnum4 bar start and conﬁ-\ndence\nbeats song id,\nbeat num4 beat start and conﬁ-\ndence\nsections song id, sec-\ntion id4 section start and\nconﬁdence\nterms song id,\nterm4 term frequency and\nweight\ntatums song id,\ntatum num4 tatum start and con-\nﬁdence\nsimilar\nartistssong id,\nartist id,\nsimi-\nlarartist id3 N/A\nTable 1 . Core tables after relationalization of the MSD.\nAdditional tables may be created, such as beat aligned fea-\ntures.\n4. ALGORITHMS\nWe describe four algorithms for scalable analysis of the\nrelationalized MSD dataset. The ﬁrst two algorithms are\ncommon MIR tasks, and the latter two come from an inﬂu-\nential study using the MSD [20].\n4.1 Song Density\nIn 2011, Lamere described a Hadoop-based approach for\ncalculating song density from the MSD [15]. A song’s\ndensity is deﬁned as the number of detected segments di-\nvided by the duration of the song in seconds. To calculate\nthis metric for every song in the MSD, Lamere provides\na MapReduce [4] algorithm that scans each song, extracts\nthe segments, and computes the density. The map function\nwas written in Java speciﬁcally for this purpose.\nThis task can be expressed directly with no custom gen-\neral purpose code in SQL. In MusicDB, song density for a\nsingle song can be expressed as a simple count query over\nthe segments table, followed by a join with the songs table\nand division by song duration. This method generalizes to\nthe following query (in an imperative dialect of SQL used\nby the Myria system [8]) that computes the density for all\nsongs.Query 1. Lines 1-2 scan the relevant relations. Lines 4-\n7 count the number of segments per song and lines 8-14\ncalculate the density by dividing the number of segments\nby the duration in seconds. Line 15 stores the result.\n1segments = SCAN(SegmentsTable);\n2songs = SCAN(SongsTable);\n3-- implicit GROUP BY song_id\n4seg_count = SELECT\n5 song_id,\n6 COUNT (segment_number) ASc\n7 FROM segments;\n8density = SELECT\n9 songs.song_id,\n10 (seg_count.c /\n11 songs.duration) ASdensity\n12 FROM songs, seg_count\n13 WHERE songs.song_id =\n14 seg_count.song_id;\n15store(song_density);\n4.2 Beat-aligning features\nWhile chroma and timbre data in the MSD are provided on\na per-segment basis, it is often more useful to align these\nfeatures to beats, which are easier to interpret musically.\nBeat-aligning these features is an extremely common and\nuseful processing step that is used in cover song detec-\ntion [2], longitudinal music studies [20], and many other\nMIR tasks, and is therefore a useful task to consider for\nMusicDB.\nIn 2011, Serra identiﬁes dynamic time warping as one\nof the best methods for beat-alignment [19]. Dynamic time\nwarping involves creating an SxB matrix, where Sis the\nnumber of segments of a song and Bis the number of\nbeats. If a segment soverlaps with a beat b, theb, sentry\nof the matrix is set to the fraction of the segment contained\nin the beat (i.e. 1 if the segment falls entirely in the beat,\n.5if the beat contains exactly half of the segment, etc). All\nother entries are set to 0, and then the rows are normalized\nsuch that each row of values sums to 1. Segment-based\nfeatures such as chroma or timbre can then be beat-aligned\nby transposing the beat matrix and performing matrix mul-\ntiplication on the features (a BxS matrix multiplied by\naSxF matrix will result in a BxF matrix, where Fis\nthe number of features). Some additional regularization of\nrows is performed for chroma features.\nIn a relational system, the time warp operation can be\ncomputed using just two operations: a join and an aggre-\ngation. We ﬁrst join the segments and beats table on the\nstart and end time of each segment and bar, such that over-\nlapping segments and beats are joined:\nQuery 2. Portion of a query that joins overlapping seg-\nments and beats of a song so that beat aligned features can\nbe computed.\n1JOIN segments, beats WHERE\n2 -- segment overlaps start of beat\n3 (seg_start <= beat_start\n4 AND seg_end <= beat_start)704 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20165 OR\n6 -- segment overlaps end of beat\n7 (seg_start < beat_end\n8 AND seg_end >= beat_end)\n9 OR\n10 -- segment fully inside beat\n11 (seg_start > beat_start\n12 AND seg_end < beat_end)\n13 OR\n14 -- beat fully inside segment\n15 (seg_start < beat_start\n16 AND seg_end > beat_end)\n17;\nWe can then perform two aggregations on the result,\nand re-join the aggregate queries to perform an operation\nidentical to multiplying the features by a time warp matrix.\nRefer to Figure 1, which visualizes this query. In each ag-\ngregation, we will calculate the fraction of a segment that\nfalls within a beat, which is deﬁned as the length of the\nsegment that falls within the beat divided by the length of\nthe segment (or 1.0 if the segment spans the beat).\nOn the right side of the diagram, we compute the ﬁrst\naggregation. We use the fraction of the segment that falls\nwithin a beat to scale each feature of that segment (i.e.\nchroma or timbre features), and then take the sum of the\nscaled features per beat.\nOn the left side of the diagram, we perform a second\naggregate which simply computes the sum of the segment\nfractions for each beat. We call this sum the divisor.\nThe two aggregates are then re-joined on beat id and the\nweighted sum from the ﬁrst aggregate is normalized by di-\nviding each value by the sum from the second aggregate.\nThis divisor serves the same function as making sure rows\nof the time warp matrix sum to 1. The result of the joined\naggregates is a table with the schema (song id, beat id, fea-\nture columns), which is a relational representation of the\nBxF feature matrix described above.\nThis algorithm, while correct, is not necessarily opti-\nmal. Speciﬁcally, performing two aggregates over the same\njoined relation and then re-joining is an expensive opera-\ntion. Relational engines that support window functions of-\nfer an alternative approach. A window function makes a\nsingle pass over a dataset, applying an aggregation over\neach window as deﬁned by a grouping value or a ﬁxed\nsize. The engine on which MusicDB is based (a variant\nof the Myria system [8]) provides a generalization of win-\ndow functions, but we do not employ that mechanism here\nto ensure reuse across platforms.\n4.3 Pitch Keyword Distribution by Year\nIn 2012, Serra et al studied the progression of chroma key-\nwords over time [20]. The authors form these keywords by\ntransposing every song to an equivalent main tonality by\ncorrelating to tonal proﬁles provided by [13]. After that,\nthe beat-aligned chroma values are discretized to binary\nvalues (1 if the value is greater than 0.5, 0 otherwise) and\nthen concatenated. Intuitively, this discretization repre-\nsents whether or not a certain pitch is present or not. These\nFigure 1 . The relational algebra expression for beat-\naligning features from segments. Segments and beats from\na song are joined on song ID and overlap conditions. Two\naggregations are performed and rejoined to form a table\nwith features now aligned to beats instead of segments.\nThe process of generated two results and joining them can\nbe costly and can be aided by using window functions pro-\nvided in many database systems.\nkeywords can then be summed over years and used to ﬁt a\ndistribution. The authors show that these chroma keywords\nﬁt a power law which has variables that are near invariant\nover time.\nThe discretization and sum of keywords can be imple-\nmented using our data model by the following SQL-like\nprogram:\nQuery 3. Lines 1-2 scan the appropriate tables. Lines 4-19\n(with some lines emitted) create an integer keyword based\non the value of each pitch column. Lines 21-26 count key-\nwords by year and line 28 stores the result.\n1songs = SCAN(SongsTable);\n2pitch = SCAN(PitchTransposed);\n3\n4keywords = SELECT\n5 p.song_id ASsong_id,\n6 p.beat_number ASbeat_number,\n7 CASE WHEN p.basis0 >= 0.5\n8 THEN int (pow(2, 11))\n9 ELSE 0\n10 END\n11 +\n12 CASE WHEN p.basis1 >= 0.5\n13 THEN int (pow(2, 10))\n14 ELSE 0\n15 END\n16 +\n17 ...\n18 ASkeyword\n19 FROM pitch p;\n20\n21-- implicit GROUP BY year, keyword\n22yearPitchKeywords = SELECT\n23 s.year , k.keyword,\n24 count (k.keyword)\n25 FROM songs s, keywords kProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 70526 WHERE s.song_id = k.song_id;\n27\n28store(yearPitchKeywords);\n4.4 Timbre Keyword Distribution by Year\nSimilar to the pitch keywords in Section 4.3, the timbre\nvalues from the MSD can be discretized and studied over\ntime. Serra et al sample timbre values from by year such\nthat no year is more represented from than any other [20].\nFrom this sample set, they estimate the tertiles for each\ntimbre value. The tertile values are then used to discretise\nthe timbre values similarly to the pitch keywords. For each\ntimbre column, the value is converted to a 0 if it is less than\nthe ﬁrst tertile, 1 if it is less than the second tertile, and 2\notherwise; concatenating these values makes one timbre\nkeyword.\nThe authors ﬁt power laws to the timbre keywords as\nbefore. However, they ﬁnd that the power law distributions\nsigniﬁcantly differ over time. They conclude that while\nthe chroma distribution appears to be time invariant, timbre\ninformation (which encodes many complex factors such as\ninstrument use, tone, and production style) has changed\nover time; additionally, the authors ﬁnd that while there\nare local shifts in timbre values, the distribution is slowly\nconverging.\nThe query for ﬁnding timbre keywords is similar to the\nquery in Section 4.3, but slightly more complex. It requires\naccess to a quantiles function that takes a quantile constant\nand a column, and returns an integer representing which\nquantile a row’s column value falls in (for example, quan-\ntile(3, col) returns 0 if col is in the ﬁrst tertile of the values\ncontained in col, 1 if it in the second, and 2 if it is in the\nthird).\nQuery 4. Lines 1-2 scan the appropriate tables. Lines 4-15\n(with some lines emitted) create an integer keyword based\non the tertile each timbre column falls in (the function re-\nturns 0 through 2). Lines 17-22 count keywords by year\nand line 24 stores the result.\n1songs = SCAN(SongsTable);\n2timbre = SCAN(TimbreBeatAligned);\n3\n4keywords = SELECT\n5 t.song_id assong_id,\n6 t.beat_number ASbeat_number,\n7 int(pow(10, 11)) *\n8 QUANTILE(3, t.basis1)\n9 +\n10 int(pow(10, 10)) *\n11 QUANTILE(3, t.basis2)\n12 +\n13 ...\n14 ASkeyword\n15 FROM timbre t;\n16\n17-- implicit GROUP BY year, keyword\n18yearTimbreKeywords = SELECT\n19 s.year , k.keyword,20 count (k.keyword)\n21 FROM songs s, keywords k\n22 WHERE s.song_id = k.song_id;\n23\n24store(yearTimbreKeywords);\n5. EXPERIMENTAL EVALUATION\nWe evaluate the feasibility of our relationalized approach\nby measuring the wall-clock performance of our imple-\nmentation on a 72-worker cluster and comparing perfor-\nmance qualitatively with reports from the literature. We\nﬁnd that the entire MSD dataset can be analyzed in sec-\nonds or minutes, where previous results on large-scale plat-\nforms report tens of minutes and required custom code,\nwhile smaller-scale implementations reported taking hours\nor days.\n5.1 Song Density\nWe ran the song density query described in Section 4.1 on\na MusicDB cluster with 72 worker threads. The compu-\ntation takes roughly half a minute, a far cry from the 20\nminutes described in [15]. These two results are not di-\nrectly comparable; the example in [15] was run on virtual\nmachines in EC2, so the hardware, software, number of\nnodes, and most other factors are not comparable. How-\never, the Hadoop implementation required custom code,\nand the underlying platform on which we implemented\nthese algorithms (a variant of the Myria system [8]) has\nbeen previously shown to signiﬁcantly outperform Hadoop\non general tasks.\nBy using a relational model to represent the MSD and\nusing a distributed analytics database, we can quickly ana-\nlyze the MSD using a simple query and allowing the sys-\ntem and optimizer handle the complexities of computation.\n5.2 Pitch Keyword Distribution by Year\nWe ran the query described in Section 4.3 on our 72-node\nproduction cluster of MusicDB. Computing the pitch key-\nwords took about three minutes, while counting keywords\nby year took an additional minute. The resulting dataset\ncontains the frequency for each keyword per year, and has\nthe schema (Keyword, Year, Count) with (Keyword, Year)\nas the primary key. It is small enough ( <1GB) to down-\nload locally and perform more complicated statistical tasks,\nsuch as ﬁtting power law distributions over counts per year\nas in [20].\nFigures 2, 3, and 4 show the power law distributions\nfor pitch keywords in the years 1965, 1975, and 1985. We\nconﬁrm the author’s results [20] that the power law coefﬁ-\ncient is invariant over time. We used the R package pow-\neRlaw [6] to perform this post-processing analysis.\nThe database system we used does not have complex\nfunctions such as power law ﬁtting, so the last step of the\ncomputation must be performed out of the system. How-\never, this step could be run in parallel as the data for each\nyear is independent. Alternatively, in a more general dis-\ntributed system, the keyword counts for each year could be706 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●\n●\n●\n●\n●\n●\n●\n1 10 100 1000 100005e−04 5e−03 5e−02 5e−011965Figure 2 . Power law distribution for pitch keywords from\nsongs released in 1965.\n●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●\n●\n●\n●\n●\n●\n●\n1 100 100005e−04 5e−03 5e−02 5e−011975\nFigure 3 . Power law distribution for pitch keywords from\nsongs released in 1975.\npartitioned and evaluated in a distributed manner. We did\nnot demonstrate this capability in this case to avoid a con-\ntrivance; the resulting data’s size was not large enough to\njustify the approach.\n6. DISCUSSION AND FUTURE WORK\nDistributed analytics systems have made it easier and faster\nto perform complex analysis on large datasets. In Sec-\ntion 2 we brieﬂy mentioned several recent studies using\nthe MSD. The authors of these studies ran experiments that\nran on single-node systems, often taking hours or days to\ncomplete. However, most of these tasks are embarrass-\ningly parallel and could be run not only in parallel on a\nsingle machine, but on thousands of nodes in a distributed\nsystem. Big data systems exist to empower users to easily\nanalyze data in such a distributed environment. As more\nlarge dataset become available in the MIR community, it is\nno longer feasible or necessary to run single or mutli-core\nalgorithms locally for weeks at a time.\nWe have shown that representing the data in the MSD\nas tables can reduce the amount of data necessary for com-\nputations (for example, only reading the segment and song\ntables in Section 3, and only the segment and beat tables in\nSection 4.2). This works especially well in a relational sys-\n●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●\n●\n●\n●\n●\n●\n●\n1 100 100005e−04 5e−03 5e−02 5e−011985Figure 4 . Power law distribution for pitch keywords from\nsongs released in 1985.\ntem, where joining tables is a common task with many op-\ntimizations. However, reading and distributed less data is\nhelpful outside of relational systems as well. Even though\nwe showed that many common MIR tasks can be expressed\nrelationally, some tasks are still very difﬁcult to imple-\nment in an imperative language. If a distributed system\nsuch as Hadoop or Spark is more preferable for a given\ntask, relationalizing the data can still be used to reduce\nthe data necessary for computation in these systems. Fi-\nnally, since these systems utilize higher level coding mod-\nels that abstract away parallelization and distributed com-\nputation, they may empower musicologists who are less\nfamiliar with these concepts to ask quickly questions over\nlarger data sets.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 7077. REFERENCES\n[1] James Bennett and Stan Lanning. The netﬂix prize. In\nProceedings of KDD cup and workshop , volume 2007,\npage 35, 2007.\n[2] Thierry Bertin-Mahieux and Daniel PW Ellis. Large-\nscale cover song recognition using hashed chroma\nlandmarks. In Applications of Signal Processing to Au-\ndio and Acoustics (WASPAA), 2011 IEEE Workshop\non, pages 117–120. IEEE, 2011.\n[3] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian\nWhitman, and Paul Lamere. The million song dataset.\nInProceedings of the 12th International Conference on\nMusic Information Retrieval (ISMIR 2011) , 2011.\n[4] Jeffrey Dean and Sanjay Ghemawat. Mapreduce: Sim-\npliﬁed data processing on large clusters. Commun.\nACM , 51(1):107–113, January 2008.\n[5] Mike Folk, Albert Cheng, and Kim Yates. Hdf5: A ﬁle\nformat and i/o library for high performance computing\napplications. In Proceedings of Supercomputing , vol-\nume 99, pages 5–33, 1999.\n[6] Colin S. Gillespie. Fitting heavy tailed distributions:\nThe poweRlaw package. Journal of Statistical Soft-\nware , 64(2):1–16, 2015.\n[7] Yoav Goldberg and Jon Orwant. A dataset of syntactic-\nngrams over time from a very large corpus of english\nbooks. 2013.\n[8] Daniel Halperin, Victor Teixeira de Almeida, Lee Lee\nChoo, Shumo Chu, Paraschos Koutris, Dominik\nMoritz, Jennifer Ortiz, Vaspol Ruamviboonsuk,\nJingjing Wang, Andrew Whitaker, et al. Demonstration\nof the myria big data management service. In Proceed-\nings of the 2014 ACM SIGMOD international confer-\nence on Management of data , pages 881–884. ACM,\n2014.\n[9] David Hauger, Markus Schedl, Andrej Ko ˇsir, and\nMarko Tkal ˇciˇc. The million musical tweets dataset:\nWhat can we learn from microblogs. In Proceedings\nof the 14th International Society for Music Information\nRetrieval Conference (ISMIR 2013) , 2013.\n[10] Eric J Humphrey, Oriol Nieto, and Juan Pablo Bello.\nData driven and discriminative projections for large-\nscale cover song identiﬁcation. In ISMIR , pages 149–\n154, 2013.\n[11] Marcel Kornacker and Justin Erickson. Cloudera\nimpala: Real time queries in apache hadoop, for real.\nht tp://blog. cloudera. com/blog/2012/10/cloudera-\nimpala-real-time-queries-in-apache-hadoop-for-real ,\n2012.\n[12] Tim Kraska, Ameet Talwalkar, John C Duchi, Rean\nGrifﬁth, Michael J Franklin, and Michael I Jordan. Ml-\nbase: A distributed machine-learning system. In CIDR ,\n2013.[13] Carol L Krumhansl. Cognitive foundations of musical\npitch , volume 17. Oxford University Press New York,\n1990.\n[14] Haewoon Kwak, Changhyun Lee, Hosung Park, and\nSue Moon. What is twitter, a social network or a news\nmedia? In Proceedings of the 19th international con-\nference on World wide web , pages 591–600. ACM,\n2010.\n[15] Paul Lamere. How to process a million songs in 20\nminutes. http://musicmachinery.com/2011/09/04/how-\nto-process-a-million-songs-in-20-minutes/, 2011.\n[16] Brian McFee, Thierry Bertin-Mahieux, Daniel PW El-\nlis, and Gert RG Lanckriet. The million song dataset\nchallenge. In Proceedings of the 21st international\nconference companion on World Wide Web , pages 909–\n916. ACM, 2012.\n[17] Sergey Melnik, Andrey Gubarev, Jing Jing Long, Ge-\noffrey Romer, Shiva Shivakumar, Matt Tolton, and\nTheo Vassilakis. Dremel: interactive analysis of web-\nscale datasets. Proceedings of the VLDB Endowment ,\n3(1-2):330–339, 2010.\n[18] Colin Raffel and Daniel PW Ellis. Large-scale content-\nbased matching of midi and audio ﬁles. In 16th Inter-\nnational Society for Music Information Retrieval Con-\nference (ISMIR 2015) .\n[19] Joan Serra. Identiﬁcation of versions of the same mu-\nsical composition by processing audio descriptions.\nDepartment of Information and Communication Tech-\nnologies , 2011.\n[20] Joan Serr `a,´Alvaro Corral, Mari ´an Bogu ˜n´a, Mart ´ın\nHaro, and Josep Ll Arcos. Measuring the evolution\nof contemporary western popular music. Scientiﬁc re-\nports , 2, 2012.\n[21] Konstantin Shvachko, Hairong Kuang, Sanjay Ra-\ndia, and Robert Chansler. The hadoop distributed ﬁle\nsystem. In Mass Storage Systems and Technologies\n(MSST), 2010 IEEE 26th Symposium on , pages 1–10.\nIEEE, 2010.\n[22] Matei Zaharia, Mosharaf Chowdhury, Michael J\nFranklin, Scott Shenker, and Ion Stoica. Spark: clus-\nter computing with working sets. In Proceedings of the\n2nd USENIX conference on Hot topics in cloud com-\nputing , volume 10, page 10, 2010.708 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Learning Temporal Features Using a Deep Neural Network and its Application to Music Genre Classification.",
        "author": [
            "Il-Young Jeong",
            "Kyogu Lee"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416416",
        "url": "https://doi.org/10.5281/zenodo.1416416",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/159_Paper.pdf",
        "abstract": "In this paper, we describe a framework for temporal feature learning from audio with a deep neural network, and apply it to music genre classification. To this end, we revisit the conventional spectral feature learning frame- work, and reformulate it in the cepstral modulation spec- trum domain, which has been successfully used in many speech and music-related applications for temporal feature extraction. Experimental results using the GTZAN dataset show that the temporal features learned from the proposed method are able to obtain classification accuracy compara- ble to that of the learned spectral features.",
        "zenodo_id": 1416416,
        "dblp_key": "conf/ismir/JeongL16",
        "content": "LEARNING TEMPORAL FEATURES USING A DEEP NEURAL\nNETWORK AND ITS APPLICATION TO MUSIC GENRE\nCLASSIFICATION\nIl-Young Jeong and Kyogu Lee\nMusic and Audio Research Group\nGraduate School of Convergence Science and Technology, Seoul National University, Korea\n{finejuly, kglee }@snu.ac.kr\nABSTRACT\nIn this paper, we describe a framework for temporal\nfeature learning from audio with a deep neural network,\nand apply it to music genre classiﬁcation. To this end,\nwe revisit the conventional spectral feature learning frame-\nwork, and reformulate it in the cepstral modulation spec-\ntrum domain, which has been successfully used in many\nspeech and music-related applications for temporal feature\nextraction. Experimental results using the GTZAN dataset\nshow that the temporal features learned from the proposed\nmethod are able to obtain classiﬁcation accuracy compara-\nble to that of the learned spectral features.\n1. INTRODUCTION\nExtracting features from audio that are relevant to the task\nat hand is a very important step in many music information\nretrieval (MIR) applications, and the choice of features has\na huge impact on the performance. For the past decades,\nnumerous features have been introduced and successfully\napplied to many different kinds of MIR systems. These\naudio features can be broadly categorized into two groups:\n1) spectral and 2) temporal features.\nSpectral features (SFs) represent the spectral character-\nistics of music in a relatively short period of time. In a mu-\nsical sense, it can be said to reveal the timbre or tonal char-\nacteristics of music. Some of popular SFs include: spec-\ntral centroid, spectral spread, spectral ﬂux, spectral ﬂatness\nmeasure, mel-frequency cepstral coefﬁcients (MFCCs) and\nchroma. On the other hand, temporal features (TFs) de-\nscribe the relatively long-term dynamics of a music signal\nover time such as temporal transition or rhythmic charac-\nteristics. These include zero-crossing rate (ZCR), temporal\nenvelope, tempo histogram, and so on. The two groups are\nnot mutually exclusive, however, and many MIR applica-\ntions use a combination of many different features.\nThe abovementioned features - be it spectral or tempo-\nral - have one thing in common: they are all ‘hand-crafted’\nc/circlecopyrtIl-Young Jeong and Kyogu Lee. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: Il-Young Jeong and Kyogu Lee. “learning temporal features\nusing a deep neural network and its application to music genre classiﬁca-\ntion”, 17th International Society for Music Information Retrieval Confer-\nence, 2016.features, which are highly based on the domain knowledge\nor signal processing techniques. With the rapid advances\nin the ﬁeld of machine learning and deep learning in par-\nticular, however, more recent works have become less de-\npendent of using the standard audio features but instead try\nto ‘learn’ optimal features [4]. These approaches usually\ntake no preprocessing step [1] or least, such as a magnitude\nspectrum [2,12] or mel-scale ﬁlter banks [1,10], but just let\nthe machine learn the optimal features for a given task. Al-\nthough a number of feature learning approaches have been\nproposed so far for many MIR-related applications, most\nof them have focused on learning SFs for a short-time sig-\nnal [2, 12]. In case of TFs, on the other hand, few studies\ntried to apply deep learning models but it was limited to\ntraining the classiﬁcation model from the high-level fea-\ntures [11, 14].\nIn this paper, we endeavor to learn TFs using a deep\nneural network (DNN) from a low-level representation. By\nreversing the conventional SF learning and temporal aggre-\ngation, we aim to learn TFs for a narrow spectral band and\nsummarize them by using spectral aggregation. Further-\nmore, we parallelize SF and TF learning frameworks, and\ncombine the two resulting features to use as a front end to\na genre classiﬁcation system. We expect this approach to\nprovide a performance gain because each learned feature\nconveys different types of information present in a musical\naudio.\n2. CONVENTIONAL FRAMEWORK FOR\nSPECTRAL FEATURE LEARNING\nIn this section, we brieﬂy revisit how SFs are extracted\nusing a DNN in a typical classiﬁcation framework [12].\nFigure 1 (a) shows the block diagram of its overall frame-\nwork, which is similar to the proposed method for temporal\nfeature learning except the input representation and feature\naggregation. Let sibe a single channel waveform of i-th\nmusic data with a label yi. Here, the label can be various\nhigh-level descriptor, including genre, mood, artist, chord,\nand tag. A magnitude spectrogram of si,Xi, is computed\nusing short-time Fourier transform (STFT) deﬁned by434(a)\n(b)input s\n1×sample\nNCMS M\nquefrency ×modulationcepstrogram C\nquefrency ×frametemporal feature\nquefrency ×feature⁞\naggregated feature\nk×featuremodulation \nspectrumaggregation\nover quefrencyDNN\n⁞⁞ ⁞aggregated feature\nfeature× k\nspectrogram X\nfrequency ×frame\n⁞\nspectral feature\nfeature× frameDNN aggregation\nover frame\n⁞⁞ ⁞input s\n1×sampleFigure 1 . Overall frameworks for (a) conventional spectral feature learning and (b) proposed temporal feature learning.\nSome details ( e.g.normalization) are omitted. kdenotes the number of aggregation methods. All ﬁgures in the paper are\nbest viewed in color.\nXi(f,t) =/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN−1/summationdisplay\nn=0si(Λt+n)w(n) exp/parenleftbigg\n−j2πfn\nN/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle,\n(1)\nwherefandtdenote a index of frequency bin and time\nframe, andNandΛindicate a size and a shift of win-\ndow function w, respectively.|·|denotes the absolute op-\nerator. A different time-frequency representation such as\nmel-spectrogram is also widely used [1, 10].\nIn order to remove the bias and reduce the variance, X\nis normalized that all frequency bins have zero-mean and\nunit variance across all the frames in training data as fol-\nlows:\n¯Xi(f,t) =Xi(f,t)−µX(f)\nσX(f), (2)\nwhereµX(f)andσX(f)denote mean and standard devia-\ntion of the magnitude spectrogram of training data in f-th\nfrequency bin, respectively. Sometimes amplitude com-\npression or PCA whitening is added to a preprocessing\nstep [10].\nThe training scheme is to learn a DNN model so that\neach normalized spectrum ¯xi,t= [¯Xi(0,t),..., ¯Xi(N/2,t)]\n(N/2instead ofNdue to its symmetry) belongs to the tar-\nget class of the original input yi. In other words, it canbe considered as a frame-wise classiﬁcation model. After\ntraining the DNN, the activations of the hidden layers are\nused as features.\nBecause many high-level musical descriptors cannot be\ndeﬁned within a very short segment of time, the frame-\nwise features usually go through a feature aggregation step\nbefore classiﬁcation. The aggregation is done within the\nspeciﬁc time range, typically 3-6s, and depending on the\napplications various aggregation methods exist, including\nmean, variance, maximum, minimum, or moments [3]. The\ndimension of the ﬁnal feature depends on the number of\naggregation methods.\nTo summarize, the above spectral feature extraction\nframework for musical applications has three steps: 1) pre-\nprocessing (STFT, normalization), 2) feature learning\n(DNN), and 3) temporal aggregation ( e.g., average and vari-\nance over frames). In the next section, we propose how\neach step can be modiﬁed to extract the temporal features.\n3. PROPOSED FRAMEWORK\nIn this section, we present the proposed method for tem-\nporal feature learning using the normalized cepstral mod-\nulation spectrum (normalized CMS or NCMS) and DNN.\nOverall procedure is illustrated in Figure 1 (b).Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 4353.1 Normalized cepstral modulation spectrum\nWe ﬁrst transform a music signal to the quefrency- normal-\nized version of CMS [8,9] because a cepstrogram is shown\nto be a more robust representation to capture the dynamics\nof the overall timbre than a spectrogram. Although there\nare some variations of CMS such as mel-cepstrum modu-\nlation spectrum [15], we expect that CMS is able to min-\nimize the information loss in the procedure. To compute\nthe NCMS, the magnitude spectrogram in Eq. (1) is ﬁrst\ntransformed into a cepstrogram domain, which is harmonic\ndecomposition of a logarithmic magnitude spectrum using\ninverse discrete Fourier transform (DFT). A cepstrogram\nis computed from a magnitude spectrogram Xas follows:\nCi(q,t) =1\nNN−1/summationdisplay\nf=0ln (Xi(f,t) +ε) exp/parenleftbigg\nj2πqf\nN/parenrightbigg\n,(3)\nwhereqis a quefrency index, and εis a small constant to\nregularize a logoperation. In this work, we empirically set\nεto be 10−4.\nSimilar to spectrogram normalization shown in Eq. (2),\ncepstrogram is normalized so as to have zero-mean and\nunit variance across quefencies:\n¯Ci(q,t) =Ci(q,t)−µC(q)\nσC(q), (4)\nwhereµC(q)andσC(q)denote mean and standard devia-\ntion ofq-th quefrency bin in a cepstrogram of training data,\nrespectively.\nTo analyze the temporal dynamics from the data, the\nshift invariance has to be considered since the extracted\nTFs are expected to be robust against its absolute location\nin time or phase. Some approaches were proposed for this\npurpose, such as l2-pooling [5], but we chose a modulation\nspectrum because it is simpler to compute. In addition,\nmodulation spectral characteristics can be analyzed over a\nfew seconds instead of a whole signal, and thus are suitable\nfor efﬁciently analyzing the local characteristics. The mod-\nulation spectrum of normalized cepstrogram is obtained as\nfollows:\nMi(q,v,u ) =/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleuΦ+T−1/summationdisplay\nt=uΦ¯Ci(q,t) exp/parenleftbigg\n−j2πvt\nT/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle,(5)\nwherevdenotes the index of modulation frequency bin and\nuis the index of the sliding window that is Tframes long\nwith a Φframes shift.\nFinally, before being used as an input to a DNN, Mis\nnormalized for each modulation frequency vto have zero-\nmean and unit variance as in Eq. (2) as follows:\n¯Mi(q,v,u ) =Mi(q,v,u )−µM(v)\nσM(v), (6)\nwhereµM(v)andσM(v)denote mean and standard devi-\nation ofv-th modulation frequency over the training data.3.2 Temporal feature learning using deep neural\nnetwork\nThe next step for temporal feature learning is the same as\nthat of the spectral feature learning. The only difference\nis that an input vector of the DNN is now a normalized\ncepstral modulation spectrum ¯mi,q,u = [¯Mi(q,0,u),...,\n¯Mi(q,T/ 2,u)],0≤q≤N/2which we expect better\ndescribes the long-term temporal properties over time for\neach quefrency.\n3.3 Feature aggregation and combination\nThe output of a DNN in the previous section is a quefrency-\nwise feature, and therefore we need to aggregate it to be\nmore appropriate as a front end to a classiﬁer. We use the\nsame aggregation method - i.e., mean and variance - as we\ndo in SF aggregation but only across quefrencies this time.\nWe believe that SFs described in Section 2 and TFs ex-\nplained above represent the musical characteristics from\ndifferent perspectives that can complement each other. By\nsetting the time window size for temporal aggregation in\nSF to be same as that for modulation analysis in TF, say\n5s, we can combine the two features and construct a com-\nplementary feature set.\nIn the following section, we test the effectiveness of the\nproposed approach and present the results obtained using a\nbenchmark music dataset.\n4. EXPERIMENTS\n4.1 Data preparation\nTo evaluate the proposed TFs and compare it with conven-\ntional SFs, we conducted genre classiﬁcation task with the\nGTZAN database, which consists of 1,000 30-second long\nmusic clips with the sampling rate of 22,050 Hz [16]. Each\nclip is annotated with one of 10 genres and for each genre\nthere are 100 clips. Even though some drawbacks and lim-\nits were indicated [13], it is still one of the most widely\nused datasets for music genre classiﬁcation.\nWe examined the two different partitioning methods.\nFirst, we randomly divided the data into three groups: 50%\nfor training, 25% for validation, and 25% for testing, main-\ntaining the balance among genres. We performed the ex-\nperiment four times to present the averaged results. This\nrandom partitioning gaurantees that the equal number of\nmusic clips is distributed among the different genres. How-\never, random partitioning of the GTZAN dataset may lead\nto the numerical evaluation results that cannot be trusted\nbecause many clips in the GTZAN dataset are from the\nsame artists. Therefore, we also tried the ‘fault-ﬁltered’\npartitioning, which manually divides the dataset into 443/\n197/290 to avoid the repetition of artist across training, val-\nidation, and testing sets [6].\n4.2 Parameter setting\nParameters in the proposed framework are basically in-\nspired from the conventional work [12]. For STFT, we436 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016used Hanning window of N=1024 samples with half over-\nlap of Λ=512. For NCMS, the number of frames and shift\nto analyze the temporal dynamics were set to be T=214\nandΦ=107, respectively, which is the closest to 5s and\n2.5s, respectively. The number of input units for DNN is\nthus 513 and 108, respectively, due to its symmetry. DNN\nis designed to have 3 hidden layers and each layer has 50\nunits for both spectral and temporal model. In other words,\nthe network has a size of 513-50-50-50-10 for SF and 108-\n50-50-50-10 for TF. Rectiﬁed linear unit (ReLU) that is\ndeﬁned asf(x) =max(0,x)was applied for the nonlin-\nearity in every hidden layer, and the softmax function was\nused for the output layer. We did not use dropout or regu-\nlarization terms since it did not help to improve the accu-\nracy in our work, which is similar as previous work [12].\nDNN was trained using mini-batch gradient descent with\n0.01 step size and 100 batch size for both conventional and\nproposed algorithm. Optimization procedure was done af-\nter 200 epoches. By means of early-stopping strategy, the\nmodel which scores the lowest cross-entropy for the vali-\ndation data is decided to be a ﬁnal model with 10 patience.\nIn the aggregation stage, the outputs in the last hidden\nlayer were aggregated using average and variance. In case\nof SF, the number of frames and shift for aggregation are\nset to be 214 and 107, respectively, which are the same\nas the temporal modulation window for TF. Although con-\nventional studies also tried more complex model with var-\nious settings [2, 12], such as increasing the number of hid-\nden units and aggregating with all the hidden layer, in this\nwork we did not consider this kind of model settings since\nit is out of our scope. As shown in Figure 3, the proposed\nmodel with a simple setting already exceed the classiﬁ-\ncation accuracy of the conventional approach with more\ncomplex model.\n4.3 Genre classiﬁcation\nWe performed genre classiﬁcation using random forest (RF)\nwith 500 trees as a classiﬁer. Each music clip of 30s was\nﬁrst divided into a number of 5s-long short segments with\n2.5s overlap. We then performed classiﬁcation on each\n5s-long segment, and used majority voting to classify the\nwhole music clip. It is noted that both training and val-\nidation data were used to train RF since it does not re-\nquire additional data for validation. The entire classiﬁca-\ntion process, including training and testing, is illustrated in\nFigure 2.\nDetailed results for each genre with the two partition-\ning methods are shown in Figure 3 and Figure 4. In case\nof random partitioning, overall accuracy of 72.6% was ob-\ntained using TFs, and 78.2% using SFs, respectively. The\naccuracy improved up to 85.0% when the two features are\njointly used. Moreover, the combined features achieved the\nhighest F-scores for all the genres except classical. Theses\nresults suggest that the each type of feature contains infor-\nmation that helps improve genre classiﬁcation.\nWith fault-ﬁltered partitioning, the accuracy decreased\nin general, which is consistent with the results presented\nin [6]. Contrary to random partioning, however, the pro-\nSpectrogram\nOutput layer\n(softmax )\nAggregation Random forestgenreDNN training\nRF training & \ntestinglossinput\nNCMSSpectral\nDNN\nTemporal \nDNNFigure 2 . Overall framework for genre classiﬁcation using\nconventional spectral features [12] and proposed temporal\nfeatures.\nposed TFs show much higher accuracy of 65.9% compared\nto 48.3% of SFs. Considering that the main difference be-\ntween random and fault-ﬁltered partitioning is artist repe-\ntition across train, validation and test sets, a possible ex-\nplanation for this is that SFs are a better representation that\ncaptures similarity between the songs by the same artists.\nFrom the combined features, we obtained 59.7% accuracy\nwhich is lower than TFs alone. We believe that this unex-\npected performance degradation is due to the fact that the\nresults were obtained from one trial with a ﬁxed partition,\nwhich may have caused a bias. From an additional experi-\nment where the classiﬁer was trained using the training and\ntesting sets and tested on the validation set, we obtained\n50.3%, 57.4%, and 63.5% accuracies from SFs, TFs, and\ncombined features, respectively.\n4.4 Feature visualization\nTo visually inspect the performance of different features,\nwe visualized the features from test data using a 2-dimen-\nsional projection with t-sne [7]. Figure 5 and Figure 6 show\nthe scatter plots of three different features, using random\nand fault-ﬁltered partitioning, respectively. Although the\nclassiﬁcation accuracies are higher with random partition-\ning, it is not clearly represented in the ﬁgures. This may\nsuggest that the higher performance with random partition-\ning is because of artist repetition, as explained in Section\n4.3.\n4.5 Discussion\nAlthough the experimental results presented in the previ-\nous section are not sufﬁcient to draw a ﬁrm conclusion, we\ncan ﬁnd some insights from our study worthy of further\ndiscussions. First, musical audio is an intrinsically time-\nvarying signal, and understanding temporal dynamics is\ncritical to better represent music. This has been done in\nvarious ways but we have demonstrated that using a more\nappropriate representation from the start helps achieve bet-\nter performance.\nThe suitable domain for the analysis of temporal char-\nacteristics also leaves a room for more in-depth discus-Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 437blues 86.0 2.0 1.0 4.0 0.0 0.0 0.0 0.0 4.0 4.0 85.1\nclassical 0.0 96.0 0.0 1.0 0.0 4.0 0.0 0.0 0.0 0.0 95.0\ncountry 5.0 1.0 89.0 3.0 1.0 1.0 1.0 3.0 9.0 6.0 74.8\ndisco 2.0 0.0 3.0 63.0 3.0 2.0 1.0 1.0 3.0 22.0 63.0\nhiphop 0.0 0.0 0.0 9.0 75.0 0.0 2.0 1.0 15.0 1.0 72.8\njazz 2.0 1.0 2.0 1.0 0.0 92.0 0.0 1.0 4.0 2.0 87.6\nmetal 1.0 0.0 1.0 1.0 8.0 0.0 91.0 0.0 0.0 6.0 84.3\npop 0.0 0.0 0.0 7.0 5.0 0.0 0.0 89.0 8.0 7.0 76.7\nreggae 2.0 0.0 3.0 5.0 7.0 0.0 0.0 1.0 54.0 5.0 70.1\nrock 2.0 0.0 1.0 6.0 1.0 1.0 5.0 4.0 3.0 47.0 67.1\nF85.6 95.5 81.3 63.0 73.9 89.8 87.5 82.4 61.0 55.3 78.2\n(a)\nblues 68.0 1.0 11.0 0.0 1.0 4.0 4.0 0.0 3.0 8.0 68.0\nclassical 0.0 93.0 0.0 0.0 0.0 7.0 0.0 0.0 2.0 2.0 89.4\ncountry 7.0 0.0 76.0 7.0 2.0 2.0 0.0 0.0 5.0 21.0 63.3\ndisco 2.0 0.0 3.0 70.0 0.0 0.0 3.0 11.0 11.0 11.0 63.1\nhiphop 1.0 0.0 0.0 6.0 82.0 0.0 1.0 7.0 5.0 0.0 80.4\njazz 2.0 3.0 2.0 0.0 0.0 82.0 0.0 0.0 0.0 1.0 91.1\nmetal 3.0 0.0 0.0 5.0 5.0 0.0 82.0 0.0 2.0 7.0 78.8\npop 0.0 0.0 1.0 2.0 7.0 0.0 0.0 70.0 10.0 2.0 76.1\nreggae 7.0 0.0 1.0 1.0 0.0 0.0 0.0 6.0 58.0 3.0 76.3\nrock 10.0 3.0 6.0 9.0 3.0 5.0 10.0 6.0 4.0 45.0 44.6\nF68.0 91.2 69.1 66.4 81.2 86.3 80.4 72.9 65.9 44.8 72.6\n(b)\nblues 91.0 1.0 0.0 2.0 0.0 1.0 0.0 0.0 2.0 4.0 90.1\nclassical 0.0 95.0 0.0 1.0 0.0 4.0 0.0 0.0 0.0 0.0 95.0\ncountry 4.0 1.0 92.0 5.0 2.0 0.0 0.0 3.0 2.0 11.0 76.7\ndisco 0.0 0.0 1.0 74.0 1.0 0.0 1.0 2.0 8.0 7.0 78.7\nhiphop 0.0 0.0 0.0 7.0 88.0 0.0 0.0 2.0 7.0 0.0 84.6\njazz 1.0 1.0 4.0 0.0 0.0 95.0 0.0 0.0 0.0 1.0 93.1\nmetal 0.0 0.0 1.0 0.0 7.0 0.0 95.0 0.0 0.0 5.0 88.0\npop 0.0 0.0 0.0 5.0 1.0 0.0 0.0 88.0 6.0 7.0 82.2\nreggae 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 70.0 3.0 93.3\nrock 4.0 2.0 1.0 6.0 0.0 0.0 4.0 5.0 5.0 62.0 69.7\nF90.5 95.0 83.6 76.3 86.3 94.1 91.3 85.0 80.0 65.6 85.0\n(c)Figure 3 . Figure of merit (FoM, ×100) with random par-\ntitioning for (a) the conventional spectral features, (b) the\nproposed temporal features, and (c) the combined features.\nEach row and column represents the predicted and true\ngenres respectively. The elements in the matrix denote\nthe recall (diagonal), precision (last column), F-score (last\nrow), confusions (off-diagonal), and overall accuracy (the\nlast element of diagonal). The higher values of recall, pre-\ncision, and F-score between (a) and (b) are emphasized in\nbold.\nsion. While NCMS shows good performance in our exper-\niments, it is probable that there exists a representation that\ncan better describe temporal properties in music. One pos-\nsible way would be analyzing temporal dynamics of SFs\nlearned from DNN. It might be able to minimize the fea-\nture extraction step, and the process should be simpler by\nconcatenating the spectral/temporal DNNs in series.\n5. CONCLUSION\nIn this paper, we presented a novel feature learning frame-\nwork using a deep neural network. In particular, while\nmost studies have been trying to learn the spectral features\nfrom a short music segment, we focused on learning the\nfeatures that represent the long-term temporal characteris-\ntics, which are expected to convey different information\nfrom that in the conventional spectral features. To this\nblues 41.9 0.0 13.3 27.6 0.0 3.7 0.0 0.0 3.8 15.6 40.6\nclassical 9.7 96.8 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 90.9\ncountry 0.0 0.0 43.3 0.0 18.5 14.8 0.0 0.0 0.0 15.6 48.1\ndisco 3.2 0.0 16.7 20.7 0.0 25.9 0.0 13.3 3.8 28.1 18.2\nhiphop 9.7 0.0 0.0 20.7 25.9 0.0 7.4 3.3 7.7 6.3 30.4\njazz 32.3 3.2 6.7 0.0 0.0 22.2 0.0 0.0 7.7 3.1 27.3\nmetal 3.2 0.0 0.0 0.0 0.0 0.0 88.9 0.0 0.0 6.3 88.9\npop 0.0 0.0 10.0 3.4 29.6 25.9 0.0 76.7 15.4 3.1 48.9\nreggae 0.0 0.0 3.3 0.0 22.2 3.7 0.0 3.3 61.5 15.6 53.3\nrock 0.0 0.0 6.7 27.6 3.7 3.7 3.7 3.3 0.0 6.3 12.5\nF41.3 93.8 45.6 19.4 28.0 24.5 88.9 59.7 57.1 8.3 48.3\n(a)\nblues 54.8 0.0 3.3 6.9 0.0 11.1 0.0 0.0 0.0 12.5 63.0\nclassical 0.0 100 6.7 0.0 0.0 18.5 0.0 0.0 0.0 0.0 81.6\ncountry 0.0 0.0 76.7 0.0 3.7 3.7 0.0 3.3 7.7 12.5 71.9\ndisco 3.2 0.0 3.3 58.6 0.0 0.0 14.8 26.7 0.0 15.6 47.2\nhiphop 3.2 0.0 3.3 3.4 88.9 0.0 14.8 13.3 11.5 3.1 61.5\njazz 32.3 0.0 0.0 0.0 0.0 66.7 0.0 0.0 0.0 3.1 62.1\nmetal 0.0 0.0 3.3 0.0 0.0 0.0 63.0 3.3 7.7 0.0 81.0\npop 0.0 0.0 0.0 0.0 7.4 0.0 0.0 50.0 11.5 0.0 75.0\nreggae 0.0 0.0 3.3 27.6 0.0 0.0 0.0 0.0 57.7 9.4 55.6\nrock 6.5 0.0 0.0 3.4 0.0 0.0 7.4 3.3 3.8 43.8 66.7\nF58.6 89.9 74.2 52.3 72.7 64.3 70.8 60.0 56.6 52.8 65.9\n(b)\nblues 67.7 0.0 13.3 17.2 0.0 3.7 0.0 0.0 3.8 9.4 60.0\nclassical 0.0 100 0.0 0.0 0.0 7.4 0.0 0.0 0.0 0.0 93.9\ncountry 0.0 0.0 66.7 0.0 0.0 40.7 0.0 0.0 0.0 25.0 51.3\ndisco 0.0 0.0 10.0 44.8 7.4 3.7 7.4 16.7 0.0 34.4 35.1\nhiphop 0.0 0.0 0.0 10.3 59.3 0.0 3.7 3.3 15.4 0.0 64.0\njazz 25.8 0.0 6.7 0.0 0.0 29.6 0.0 0.0 0.0 0.0 44.4\nmetal 0.0 0.0 0.0 0.0 0.0 0.0 85.2 0.0 0.0 6.3 92.0\npop 0.0 0.0 0.0 3.4 29.6 3.7 0.0 73.3 15.4 3.1 59.5\nreggae 0.0 0.0 0.0 17.2 3.7 0.0 0.0 0.0 65.4 15.6 60.7\nrock 6.5 0.0 3.3 6.9 0.0 11.1 3.7 6.7 0.0 6.3 15.4\nF63.6 96.9 58.0 39.4 61.5 35.6 88.5 65.7 63.0 8.9 59.7\n(c)Figure 4 . Figure of merit (FoM, ×100) with fault-ﬁltered\npartitioning. Details are the same as Figure 3.\nend, we used a normalized cepstral modulation spectrum\nas an input to DNN, and introduced a feature aggregation\nmethod over quefrencies. Experiments with genre classi-\nﬁcation show that the proposed temporal features yielded\nperformance comparable to or better than that of the spec-\ntral features, depending on the partitioning methods of the\ndataset. We plan to apply the proposed method to vari-\nous MIR-related tasks, including mood classiﬁcation or in-\nstrument identiﬁcation where spectral features are predom-\ninantly used. We also intend to develop a single framework\nin which both spectral and temporal features are jointly\nlearned.\n6. ACKNOWLEDGEMENTS\nThis research was supported by the MSIP(Ministry of Sci-\nence, ICT and Future Planning), Korea, under the ITRC\n(Information Technology Research Center) support pro-\ngram (IITP-2016-H8501-16-1016) supervised by the IITP\n(Institute for Information & communications Technology\nPromotion).438 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016(a) (b)\n(c) (d)blues\nclassical\ncountry\ndisco\nhiphopjazz\nmetal\npop\nreggae\nrockFigure 5 . 2-dimensional scatter plots using t-sne [7] with random partitioning for (a) the conventional spectral features,\n(b) the proposed temporal features, and (c) the combined features. Each marker represents a 5s excerpt of a music signal\nwhose genre is labeled as in (d).\n(a) (b)\n(c) (d)blues\nclassical\ncountry\ndisco\nhiphopjazz\nmetal\npop\nreggae\nrock\nFigure 6 . 2-dimensional scatter plots using t-sne [7] with fault-ﬁltered partitioning. Details are the same as Figure 5.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 4397. REFERENCES\n[1] Sander Dieleman and Benjamin Schrauwen. End-to-\nend learning for music audio. In Proceedings of IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing . Florence, Italy, 2014.\n[2] Philippe Hamel and Douglas Eck. Learning features\nfrom music audio with deep belief networks. In ISMIR ,\npages 339–344. Utrecht, Netherlands, 2010.\n[3] Philippe Hamel, Simon Lemieux, Yoshua Bengio, and\nDouglas Eck. Temporal pooling and multiscale learn-\ning for automatic annotation and ranking of music au-\ndio. In ISMIR , pages 729–734, 2011.\n[4] Eric J. Humphrey, Juan P. Bello, and Yann LeCun. Fea-\nture learning and deep architectures: New directions\nfor music informatics. Journal of Intelligent Informa-\ntion Systems , 41(3):461–481, 2013.\n[5] Aapo Hyv ¨arinen and Patrik Hoyer. Emergence of\nphase-and shift-invariant features by decomposition\nof natural images into independent feature subspaces.\nNeural computation , 12(7):1705–1720, 2000.\n[6] Corey Kereliuk, Bob L. Sturm, and Jan Larsen. Deep\nlearning and music adversaries. IEEE Transactions on\nMultimedia , 17(11):2059–2071, 2015.\n[7] Laurens van der Maaten and Geoffrey Hinton. Visual-\nizing data using t-SNE. Journal of Machine Learning\nResearch , 9:2579–2605, 2008.\n[8] Rainer Martin and Anil Nagathil. Cepstral modula-\ntion ratio regression (CMRARE) parameters for au-\ndio signal analysis and classiﬁcation. In Proceedings of\nIEEE International Conference on Acoustics, Speech\nand Signal Processing , 2009.\n[9] Anil Nagathil, Timo Gerkmann, and Rainer Martin.\nMusical genre classiﬁcation based on a highly-resolvedcepstral modulation spectrum. In Proceedings of the\nEuropean Signal Processing Conference , 2010.\n[10] Juhan Nam, Jorge Herrera, Malcolm Slaney, and\nJulius O Smith. Learning sparse feature representations\nfor music annotation and retrieval. In ISMIR , pages\n565–570, 2012.\n[11] Aggelos Pikrakis. A deep learning approach to rhythm\nmodeling with applications. In Proceedings of the In-\nternational Workshop on Machine Learning and Mu-\nsic, 2013.\n[12] Siddharth Sigtia and Simon Dixon. Improved music\nfeature learning with deep neural networks. In Pro-\nceedings of IEEE International Conference on Acous-\ntics, Speech and Signal Processing . Florence, Italy,\n2014.\n[13] Bob L. Sturm. The state of the art ten years after a\nstate of the art: Future research in music information\nretrieval. Journal of New Music Research , 43(2):147–\n172, 2014.\n[14] Bob L. Sturm, Corey Kereliuk, and Jan Larsen. ¿El ca-\nballo viejo? latin genre recognition with deep learn-\ning and spectral periodicity. Mathematics and Compu-\ntation in Music , pages 335–346, 2013.\n[15] Vivek Tyagi, Iain McCowan, Hemant Misra, and\nHerv ´e Bourlard. Mel-cepstrum modulation spectrum\n(MCMS) features for robust asr. In Proceedings of\nIEEE Workshop on Automatic Speech Recognition and\nUnderstanding , pages 399–404, 2003.\n[16] George Tzanetakis and Perry Cook. Musical genre\nclassiﬁcation of audio signals. IEEE Transactions on\nSpeech and Audio Processing , 10(5):293–302, 2002.440 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "On the Potential of Simple Framewise Approaches to Piano Transcription.",
        "author": [
            "Rainer Kelz",
            "Matthias Dorfer",
            "Filip Korzeniowski",
            "Sebastian Böck",
            "Andreas Arzt",
            "Gerhard Widmer"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416488",
        "url": "https://doi.org/10.5281/zenodo.1416488",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/179_Paper.pdf",
        "abstract": "In an attempt at exploring the limitations of simple ap- proaches to the task of piano transcription (as usually de- fined in MIR), we conduct an in-depth analysis of neural network-based framewise transcription. We systematically compare different popular input representations for tran- scription systems to determine the ones most suitable for use with neural networks. Exploiting recent advances in training techniques and new regularizers, and taking into account hyper-parameter tuning, we show that it is possi- ble, by simple bottom-up frame-wise processing, to obtain a piano transcriber that outperforms the current published state of the art on the publicly available MAPS dataset – without any complex post-processing steps. Thus, we propose this simple approach as a new baseline for this dataset, for future transcription research to build on and improve.",
        "zenodo_id": 1416488,
        "dblp_key": "conf/ismir/KelzDKBAW16",
        "content": "ON THE POTENTIAL OF SIMPLE FRAMEWISE APPROACHES TO\nPIANO TRANSCRIPTION\nRainer Kelz, Matthias Dorfer, Filip Korzeniowski,\nSebastian B ¨ock, Andreas Arzt, Gerhard Widmer\nDepartment of Computational Perception, Johannes Kepler University Linz, Austria\nrainer.kelz@jku.at\nABSTRACT\nIn an attempt at exploring the limitations of simple ap-\nproaches to the task of piano transcription (as usually de-\nﬁned in MIR), we conduct an in-depth analysis of neural\nnetwork-based framewise transcription. We systematically\ncompare different popular input representations for tran-\nscription systems to determine the ones most suitable for\nuse with neural networks. Exploiting recent advances in\ntraining techniques and new regularizers, and taking into\naccount hyper-parameter tuning, we show that it is possi-\nble, by simple bottom-up frame-wise processing, to obtain\na piano transcriber that outperforms the current published\nstate of the art on the publicly available MAPS dataset\n– without any complex post-processing steps. Thus, we\npropose this simple approach as a new baseline for this\ndataset, for future transcription research to build on and\nimprove.\n1. INTRODUCTION\nSince their tremendous success in computer vision in re-\ncent years, neural networks have been used for a large\nvariety of tasks in the audio, speech and music domain.\nThey often achieve higher performance than hand-crafted\nfeature extraction and classiﬁcation pipelines [20]. Un-\nfortunately, using this model class brings along con-\nsiderable computational baggage in the form of hyper-\nparameter tuning. These hyper-parameters include archi-\ntectural choices such as the number and width of layers and\ntheir type (e.g. dense, convolutional, recurrent), learning\nrate schedule, other parameters of the optimization scheme\nand regularizing mechanisms. Whereas for computer vi-\nsion these successes were possible using raw pixels as the\ninput representation, in the audio domain there seems to\nbe an additional complication. Here the choices for how to\nbest represent the input range from spectrograms, logarith-\nmically ﬁltered spectrograms over constant-Q transforms\nto even the raw audio itself [10].\nc/circlecopyrtRainer Kelz, Matthias Dorfer, Filip Korzeniowski, Se-\nbastian B ¨ock, Andreas Arzt, Gerhard Widmer. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: Rainer Kelz, Matthias Dorfer, Filip Korzeniowski, Sebastian B ¨ock,\nAndreas Arzt, Gerhard Widmer. “On the Potential of Simple Framewise\nApproaches to Piano Transcription”, 17th International Society for Music\nInformation Retrieval Conference, 2016.This is a tedious problem, and there seem to be only two\nsolutions to it: manual hyper-parameter selection, where a\nhuman expert tries to make decisions based on her past\nexperience, or automatic hyper-parameter optimization as\ndiscussed in [4, 11, 28]. In this work we pursue a mixed\nstrategy. As a ﬁrst step, we systematically ﬁnd the most\nsuitable input representation, and progress from there with\nhuman expert knowledge to ﬁnd best performing architec-\ntures for the task of framewise piano transcription.\nA variety of neural network architectures has been\nused speciﬁcally for framewise transcription of piano notes\nfrom monaural sources. Some transcription systems are\nseparated into an acoustic model and a musical language\nmodel , such as [7, 26, 27], whereas in others there is no\nsuch distinction [2, 6, 23]. As shown in [26], models that\nutilize musical language models perform better than those\nwithout, albeit the differences seem to be small. We focus\non the acoustic model here, neglecting the complementary\nlanguage model for now.\n2. INPUT, METHODS AND PARAMETERS\nIn what follows, we will describe the input representations\nwe compared, and give a brief overview of techniques for\ntraining and regularizing neural networks.\n2.1 Input Representation\nTime-frequency representations in the form of spectro-\ngrams still seem to have a distinct advantage over the raw\naudio input, as mentioned in [10]. The exact parame-\nterization of spectrograms is not entirely clear however,\nso we try to address this question in a systematic way.\nWe investigate the suitability of different types of spec-\ntrograms and constant-Q transforms as input representa-\ntions for neural networks and compare four types of input\nrepresentations: spectrograms with linearly spaced bins S,\nspectrograms with logarithmically spaced bins LS, spec-\ntrograms with logarithmically spaced bins and logarithmi-\ncally scaled magnitude LM, as well as the constant-Q trans-\nform CQT [8]. The ﬁlterbank for LSandLMhas a linear\nresponse (and lower resolution) for the lower frequencies,\nand a logarithmic response for the higher frequencies. We\nvary the sample rate sr∈{22050,44100}[Hz], number\nof bands per octave nb∈{12,24,36,48}, whether or not\nframes undergo circular shift cs∈{on,oﬀ}, the amount\nof zero padding zp∈{× 0,×1,×2}, and whether or not475sr zp cs nb norm\nCQT× ×\nS× × ×\nLS× × × × ×\nLM× × × × ×\nTable 1 : For each spectrogram type, these are the parame-\nters that were varied. See text for a description of the value\nranges.\nto use area normalized ﬁlters when ﬁlter banks are used\nnorm∈ {yes,no}. Furthermore, we re-scale the mag-\nnitudes of the spectrogram bins to be in the range [0,1].\nTable 1 speciﬁes which parameters are varied for which\ninput type. For the computation of spectrograms we used\nMadmom [5] and for the constant-Q transform we used the\nYaafe library [21].\n2.2 Model Class and Suitability\nFormally, neural networks are functions with the structure\nnetk(x) =Wkfk−1(x) +bk\nfk(x) =σk(netk(x))\nf0(x) =x\nwhere x∈Rwin,fk:Rwk−1→Rwk,σis any element-\nwise nonlinear function, Wkis a matrix in Rwk×wk−1\ncalled the weight matrix , and bk∈Rwkis a vector called\nbias. The subscript k∈ {0,...,L}is the index of the\nlayer, withk= 0denoting the input layer.\nChoosing a very narrow deﬁnition on purpose, what we\nmean by a model class Fis a ﬁxed number of layers L, a\nﬁxed number of layer widths {w0,...w L}and ﬁxed types\nof nonlinearities{σ0,...σ L}. Amodel means an instance\nffrom this class, deﬁned by its weights alone. References\nto the whole collection of weights will be made with Θ.\nFor the task of framewise piano transcription we deﬁne\nthesuitability of an input representation in terms of the\nperformance of a simple classiﬁer on this task, when given\nexactly this input representation.\nAssuming we can reliably mitigate the risk of overﬁt-\nting, we would like to argue that this method of determin-\ning suitable input representations, and using them for mod-\nels with higher capacity, is the best we can do, given a lim-\nited computational budget.\nUsing a low-variance, high-bias model class, the per-\nceptron, also called logistic regression orsingle-layer neu-\nral network, we learn a spectral template per note. To test\nwhether the results stemming from this analysis are really\nrelevant for higher-variance, lower-bias model classes, we\nrun the same set of experiments again, employing a multi\nlayer perceptron with exactly one hidden layer, colloqui-\nally called a shallow net . This small extension already\ngives the network the possibility to learn a shared, dis-\ntributed representation of the input. As we will see, this\nhas a considerable effect on how suitability is judged.2.3 Nonlinearities and Initialization\nCommon choices for nonlinearities include the logistic\nfunctionσ(a) =1\n1+e−a,hyperbolic tangent σ(a) =\ntanha, and rectiﬁed linear units (ReLU) σ(a) =\nmax(0,a). Nonlinearities are necessary to make neural\nnetworks universal function approximators [16]. Accord-\ning to [13,15], using ReLUs as the nonlinearities in neural\nnetworks leads to better behaved gradients and faster con-\nvergence because they do not saturate.\nBefore training, the weight matrices are initialized ran-\ndomly. The scale of this initialization is crucial and de-\npends on the used nonlinearity as well as the number of\nweights contributing to the activation [13, 15]. Proper ini-\ntialization plays an even bigger role when networks with\nmore than one hidden layer are trained [31]. This is also\nimportant for the transcription setting we use, as the out-\nput layer of our networks uses the logistic function, which\nis prone to saturation effects. Thus we decided on using\nReLUs throughout, initialized with a uniform distribution\nhaving a scale of±√\n2·/radicalBig\n2\nwk−1+wk. For the last layer with\nthe logistic nonlinearity, we omit the gain factor of√\n2, as\nadvised in [13].\n2.4 Weight Decay\nTo reduce overﬁtting and regularizing the network, differ-\nent priors can be imposed on the network weights. Usually\na Gaussian or Laplacian prior is chosen, corresponding to\nanL2orL1penalty term on connection weights, added to\nthe cost functionLreg=L+λ/summationtext\nk/bardblvec(Wk)/bardbl1|2[25,32],\nwhereLis an arbitrary, unregularized cost function and λ\ngoverns the extent of regularization. Adding both of these\npenalty terms corresponds to a technique called Elastic\nNet[33]. It is pointed out in [1] that using L2regulariza-\ntion plays a similar role as early stopping and thus might be\nomitted. An L1penalty on the other hand leads to sparser\nweights, as it has a tendency to drive weights with irrele-\nvant contributions to zero.\n2.5 Dropout\nApplying dropout to a layer zeroes out a fraction of the ac-\ntivations of a hidden layer of the network. For each train-\ning case, a different random fraction is dropped. This pre-\nvents units from co-adapting, and relying too much on each\nother’s presence, as reasoned in [30]. Dropout increases\nrobustness to noise, improves the generalization ability of\nnetworks and mitigates the risk of overﬁtting. Additionally\ndropout can be interpreted as model-averaging of exponen-\ntially many models [30].\n2.6 Batch Normalization\nBatch normalization [18] seeks to produce networks whose\nindividual activations per layer are zero-mean and unit-\nvariance. This is ensured by normalizing the activations\nfor each mini-batch at each training step. This effectively\nlimits how far the activation distribution can drift away\nfrom zero-mean, unit-variance during training. Not only476 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016does this alleviate the need of the weights of the subse-\nquent layer to adapt to a changing input distribution dur-\ning training, it also keeps the nonlinearities from saturating\nand in turn speeds up training. It has additional regulariz-\ning effects, which become more apparent the more layers\na network has. After training is stopped, the normalization\nis performed for each layer and for the whole training set.\n2.7 Layer Types\nWe employ three different types of layer. Their respec-\ntive functions can all be viewed as matrix expressions in\nthe end, and thus can be made to ﬁt into the framework\ndescribed in Section 2.2. For the sake of readability, we\nsimply describe their function in a procedural way.\nAdense layer consists of a dense matrix - vector pair\n(W,b)together with a nonlinearity. The input is trans-\nformed via this afﬁne map, and then passed through a non-\nlinearity.\nAconvolutional layer consists of a number Ckof con-\nvolution kernels of a certain size {(Wc,bc)}Ck\nc=0together\nwith a non-linearity. The input is convolved with each con-\nvolution kernel, leading to Ckdifferent feature maps to\nwhich the same nonlinearity is applied.\nMax pooling layers are used in convolutional networks\nto provide a small amount of translational invariance. They\nselect the units with maximal activation in a local neigh-\nborhood (wt,wf)in time and frequency in a feature map.\nThis has beneﬁcial effects, as it makes the transcriber in-\nvariant to small changes in tuning.\nGlobal average pooling layers are used in all-\nconvolutional networks to compute the mean value of fea-\nture maps.\n2.8 Architectures\nThere is a fundamental choice between a network with all\ndense layers, a network with all convolutional layers, and\na mixed approach, where usually the convolutional layers\nare the ﬁrst ones after the input layer followed by dense\nlayers. Pooling layers, batch normalization and dropout\napplication are interleaved. For all networks we have to\nchoose the number of layers, how many hidden units per\nlayer to use and when to interleave a regularization layer.\nFor convolutional networks we have to choose the num-\nber of ﬁlter kernels and their extent in time and frequency\ndirection.\n2.9 Networks for Framewise Polyphonic Piano\nTranscription\nThe output layer of all considered model classes has 88\nunits, in line with the playable notes on most modern pi-\nanos, and the output nonlinearity is the logistic function,\nwhose output ranges lie in the interval [0,1].\nThe loss function being minimized is the frame- and\nelement-wise applied binary crossentropy\nL(t)\nbce(yt,ˆyt) =−(yt·log(ˆyt) + (1−yt)·log(1−ˆyt))where ˆyt=fL(xt)is the output vector of the network,\nandytthe ground truth at time t. As the overall loss over\nthe whole training set we take the mean\nL=1\nTT/summationdisplay\nt=1L(t)\nbce\nFor the purpose of computing the performance mea-\nsures, the prediction of the network is thresholded to obtain\na binary prediction ¯yt=ˆyt>0.5.\n2.10 Optimization\nThe simplest way to adapt the weights Θof the network to\nminimize the loss is to take a small step with length αin\nthe direction of steepest descent:\nΘi+1= Θi−α·∂L\n∂Θ\nComputing the true gradient∂L\n∂Θ=1\nT/summationtextT\nt=1∂L(t)\nbce\n∂Θre-\nquires a sum over the length of the whole training set, and\nis computationally too costly. For this reason, the gradi-\nent is usually only approximated from an i.i.d. random\nsample of size M/lessmuchT. This is called mini-batch stochas-\ntic gradient descent . There are several extensions to this\ngeneral framework, such as momentum [24], Nesterov mo-\nmentum [22] or Adam [19], which try to smooth the gradi-\nent estimate, correct small missteps or adapt the learning\nrate dynamically, respectively. Additionally we can set a\nlearning rate schedule that controls the temporal evolution\nof the learning rate.\n3. DATASET AND MEASURES\nThe computational experiments have been performed with\nthe MAPS dataset [12]. It provides MIDI-aligned record-\nings of a variety of classical music pieces. They were ren-\ndered using different hi-quality piano sample patches, as\nwell as real recordings from an upright Disklavier. This en-\nsures clean annotation and therefore almost no label-noise.\nFor all performance comparisons the following framewise\nmeasures on the validation set are used:\nP=T−1/summationdisplay\nt=0TP[t]\nTP[t] +FP[t]\nR=T−1/summationdisplay\nt=0TP[t]\nTP[t] +FN[t]\nF1=2·P·R\nP+R\nThe train-test folds are those used in [26] which were\npublished online1. For each fold, the validation set con-\nsists of 43tracks randomly removed from the train set,\n1http://www.eecs.qmul.ac.uk/ ˜sss31/TASLP/info.\nhtmlProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 477deviating from the 26used in [26], and leading to a di-\nvision of 173-43-54 between the three sets. Note that the\ntest sets are the same , and are referred to as conﬁguration I\nin [26]. The exact splits for conﬁguration II were not pub-\nlished. We had to choose them ourselves, using the same\nmethodology, which has the additional constraint that only\nrecordings of the real piano are used for testing, resulting\nin a division of 180-30-60. This constitutes a more realistic\nsetting for piano transcription.\n4. ANALYSIS OF RELATIVE\nHYPER-PARAMETER IMPORTANCE\nTo identify and select an appropriate input representation\nand determine the most important hyper-parameters re-\nsponsible for high transcription performance, a multi-stage\nstudy with subsequent fANOV A analysis was conducted,\nas described in [17]. This is similar in spirit to [14], albeit\non a smaller scale.\n4.1 Types of Representation\nTo isolate the effects of different input representations on\nthe performance of different model classes, only param-\neters for the spectrogram were varied according to Table\n1. This leads to 204distinct input representations. The\nhyper-parameters for the model class as well as the opti-\nmization scheme were held ﬁxed . To make our estimates\nmore robust, we conducted multiple runs for the same type\nof input.\nThe results for each model class are summarized in\nTable 2, containing the three most inﬂuential hyper-\nparameters and the percentage of variability in perfor-\nmance they are responsible for. The most important hyper-\nparameter for both model classes is the type of spectro-\ngram used, followed by pairwise interactions. Please note\nthat the numbers in the percentage column are mainly use-\nful to judge the relative importance of the parameters. We\nwill see these relative importances put into a larger context\nlater on.\nIn Figure 1, we can see the mean performance attain-\nable with different types of spectrograms for both model\nclasses. The error bars indicate the standard deviation for\nthe spread in performance, caused by the rest of the varied\nparameters. Surprisingly, the spectrogram with logarith-\nmically spaced bins and logarithmically scaled magnitude,\nLM, enables the shallow net to perform best, even though\nit is a clear mismatch for logistic regression. The lower\nperformance of the constant-Q transform was quite unex-\npected in both cases and warrants further investigation.\n4.2 Greater context\nAttempting a full grid search on all possible input rep-\nresentation and model class hyper-parameters described\nin Section 2 to compute their true marginalized perfor-\nmance is computationally too costly. It is possible however\nto compute the predicted marginalized performance of a\nhyper-parameter efﬁciently from a smaller subsample of\nthe space, as shown in [17]. All parameters are randomlyModel Class Pct Parameters\nLogistic Regression 48.6% Spectrogram Type\n16.9% Spectrogram Type\n×Normed Area Filters\n10.4% Spectrogram Type\n×Sample Rate\nShallow Net 68.4% Spectrogram Type\n20.8% Spectrogram Type\n×Sample Rate\n5.7% Sample Rate\nTable 2 : The three most important parameters determining\ninput representation for different model classes\nLS LM S CQT\nSpectrogram Type0.10.20.30.40.50.60.7F-Measure\n(a)\nLS LM S CQT\nSpectrogram Type0.10.20.30.40.50.60.7F-Measure (b)\nFigure 1 : (a) Mean logistic regression performance de-\npendent on spectrogram (b) Mean shallow net performance\ndependent on type of spectrogram\nvaried to sample the space as evenly as possible, and a ran-\ndom forest of 100regression trees is ﬁtted to the measured\nperformance. This allows to predict the marginalized per-\nformance of individual hyper-parameters. Table 3 contains\nthe list of hyper-parameters varied.\n0 10 20 30 40 50\nRelative ImportanceLearning Rate × OptimizerLearning Rate × DropoutOptimizer × Spectrogram TypeLearning Rate × Spectrogram TypeDropout × Spectrogram TypeBatch NormalizationOptimizerDropoutSpectrogram TypeLearning Rate\nFigure 2 : Relative importance of the ﬁrst 10hyper-\nparameters for the shallow net model class.\nThe percentage of variance in performance these hyper-\nparameters are responsible for, can be seen in Figure 2 for\nthe10most important ones. A total of 3000 runs with\nrandom parameterizations were made.\nAnalyzing the results of all the runs tells us that the most\nimportant hyper-parameters are Learning Rate (47.11%),\nandSpectrogram Type (5.28%). The importance of the\nlearning rate is in line with the ﬁndings in [14]. Figure\n2 shows the relative importances of the ﬁrst 10hyper-\nparameters, and Figure 3 shows the predicted marginal per-\nformance of the learning rate dependent on its value (on a\nlogarithmic scale) in greater detail.478 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Optimizer (Plain SGD, Momentum, Nesterov Momentum, Adam)\nLearning Rate (0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0, 50.0, 100.0)\nMomentum (Off, 0.7, 0.8, 0.9)\nLearning rate Scheduler (On, Off)\nBatch Normalization (On, Off)\nDropout (Off, 0.1, 0.3, 0.5)\nL1Penalty (Off, 1e-07, 1e-08, 1e-09)\nL2Penalty (Off, 1e-07, 1e-08, 1e-09)\nTable 3 : The list of additional hyper-parameters varied,\nand their ranges.\n10-310-210-1100101102\nLearning Rate0.00.10.20.30.40.50.6F-Measure\nFigure 3 : Mean predicted performance for the shallow net\nmodel class, dependent on learning rate (on a logarithmic\nscale). The dark line shows the mean performance, and the\ngray area shows the standard deviation.\n5. STATE OF THE ART MODELS\nHaving completed the analysis of input representation,\nmore powerful model classes were tried: a deep neu-\nral network consisting entirely of dense layers ( DNN ),\na mixed network with convolutional layers directly after\nthe input followed by dense layers ( ConvNet ), and an all-\nconvolutional network ( AllConv [29]). Their architectures\nare described in detail in Table 4. To the best of our knowl-\nedge, this is the ﬁrst time an all-convolutional net has been\nproposed for the task of framewise piano transcription.\nWe computed a logarithmically ﬁltered spectrogram\nwith logarithmic magnitude from audio with a sample rate\nof44.1kHz, a ﬁlterbank with 48bins per octave, normed\narea ﬁlters, no circular shift and no zero padding. The\nchoices for circular shift and zero padding ranged very low\non the importance scale, so we simply left them switched\noff. This resulted in only 229bins, which are logarithmi-\ncally spaced in the higher frequency regions, and almost\nlinearly spaced in the lower frequency regions as men-\ntioned in Section 2.1. The dense network was presented\none frame at a time, whereas the convolutional network\nwas given a context in time of two frames to either side of\nthe current frame, summing to 5frames in total.\nAll further hyper-parameter tuning and architectural\nchoices have been left to a human expert. Models within\na model class were selected based on average F-measure\nacross the four validation sets. An automatic search via\na hyper-parameter search algorithm for these larger modelDNN ConvNet AllConv\nInput 229 Input 5x229 Input 5x229\nDropout 0.1 Conv 32x3x3 Conv 32x3x3\nDense 512 Conv 32x3x3 Conv 32x3x3\nBatchNorm BatchNorm BatchNorm\nDropout 0.25 MaxPool 1x2 MaxPool 1x2\nDense 512 Dropout 0.25 Dropout 0.25\nBatchNorm Conv 64x3x3 Conv 32x1x3\nDropout 0.25 MaxPool 1x2 BatchNorm\nDense 512 Dropout 0.25 Conv 32x1x3\nBatchNorm Dense 512 BatchNorm\nDropout 0.25 Dropout 0.5 MaxPool 1x2\nDense 88 Dense 88 Dropout 0.25\nConv 64x1x25\nBatchNorm\nConv 128x1x25\nBatchNorm\nDropout 0.5\nConv 88x1x1\nBatchNorm\nAvgPool 1x6\nSigmoid\n# Params 691288 1877880 284544\nTable 4 : Model Architectures\nclasses, as described in [4, 11, 28] is left for future work\n(the training time for a convolutional model is roughly 8−9\nhours on a Tesla K40 GPU, which leaves us with 204·3·4·8\nhours (variants×#models×#folds×hours per model),\nor on the order of 800−900days of compute time to de-\ntermine the best input representation exactly).\nFor these powerful models, we followed practical rec-\nommendations for training neural networks via gradient\ndescent found in [1]. Particularly relevant is the way of\nsetting the initial learning rate. Strategies that dynamically\nadapt the learning rate, such as Adam orNesterov Momen-\ntum[19, 22] help to a certain extent, but still do not spare\nus from tuning the initial learning rate and its schedule.\nWe observed that using a combination of batch normal-\nization and dropout together with very simple optimiza-\ntion strategies leads to low validation error fairly quickly,\nin terms of the number of epochs trained. The strategy\nthat worked best for determining the learning rate and its\nschedule was trying learning rates on a logarithmic scale,\nstarting at 10.0, until the optimization did not diverge any-\nmore [1], then training until the validation error ﬂattened\nout for a few epochs, then multiplying the learning rate\nwith a factor from the set {0.1,0.25,0.5,0.75}. The rates\nand schedules we ﬁnally settled on were:\n•DNN : SGD with Momentum, α= 0.1,µ= 0.9and\nhalving ofαevery 10epochs\n•ConvNet : SGD with Momentum, α= 0.1,µ= 0.9\nand a halving of αevery 5epochs\n•AllConv : SGD with Momentum, α= 1.0,µ= 0.9\nand a halving of αevery 10epochs\nThe results for framewise prediction on the MAPS\ndataset can be found in Table 6. It should be noted that we\ncompare straightforward, simple, and largely un-smoothedProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 479systems (ours) with hybrid systems [26]. There is a small\ndegree of temporal smoothing happening when processing\nspectrograms with convolutional nets. The term simple is\nsupposed to mean that the resulting models have a small\namount of parameters and the models are composed of a\nfew low-complexity building blocks. All systems are eval-\nuated on the same train-test splits, referred to as conﬁgu-\nration I in [26] as well as on realistic train-test splits, that\nwere constructed in the same fashion as conﬁguration II\nin [26].\nModel Class P R F1\nHybrid DNN [26] 65.66 70.34 67.92\nHybrid RNN [26] 67.89 70.66 69.25\nHybrid ConvNet [26] 72.45 76.56 74.45\nDNN 76.63 70.12 73.11\nConvNet 80.19 78.66 79.33\nAllConv 80.75 75.77 78.07\nTable 5 : Results on the MAPS dataset. Test set perfor-\nmance was averaged across 4folds as deﬁned in conﬁgu-\nration I in [26].\nModel Class P R F1\nDNN [26] - - 59.91\nRNN [26] - - 57.67\nConvNet [26] - - 64.14\nDNN 75.51 57.30 65.15\nConvNet 74.50 67.10 70.60\nAllConv 76.53 63.46 69.38\nTable 6 : Results on the MAPS dataset. Test set perfor-\nmance was averaged across 4folds as deﬁned in conﬁgu-\nration II in [26].\n6. CONCLUSION\nWe argue that the results demonstrate: the importance of\nproper choice of input representation, and the importance\nof hyper-parameter tuning, especially the tuning of learn-\ning rate and its schedule; that convolutional networks have\na distinct advantage over their deep and dense siblings, be-\ncause of their context window and that all-convolutional\nnetworks perform nearly as well as mixed networks, al-\nthough they have far fewer parameters. We propose these\nstraightforward, framewise transcription networks as a new\nstate-of-the art baseline for framewise piano transcription\nfor the MAPS dataset.\n7. ACKNOWLEDGEMENTS\nThis work is supported by the European Research\nCouncil (ERC Grant Agreement 670035, project\nCON ESPRESSIONE), the Austrian Ministries BMVIT\nand BMWFW, the Province of Upper Austria (via the\nCOMET Center SCCH) and the European Union Seventh\nFramework Programme FP7 / 2007-2013 through the\nGiantSteps project (grant agreement no. 610591). We\nwould like to thank all developers of Theano [3] and\nLasagne [9] for providing comprehensive and easy to usedeep learning frameworks. The Tesla K40 used for this\nresearch was donated by the NVIDIA Corporation.\n8. REFERENCES\n[1] Yoshua Bengio. Practical recommendations for\ngradient-based training of deep architectures. In\nNeural Networks: Tricks of the Trade , pages 437–478.\nSpringer, 2012.\n[2] Taylor Berg-Kirkpatrick, Jacob Andreas, and Dan\nKlein. Unsupervised Transcription of Piano Music. In\nAdvances in Neural Information Processing Systems ,\npages 1538–1546, 2014.\n[3] James Bergstra, Olivier Breuleux, Fr ´ed´eric Bastien,\nPascal Lamblin, Razvan Pascanu, Guillaume Des-\njardins, Joseph Turian, David Warde-Farley, and\nYoshua Bengio. Theano: a CPU and GPU Math Ex-\npression Compiler. In Proceedings of the Python for\nScientiﬁc Computing Conference (SciPy) , 2010.\n[4] James Bergstra, Dan Yamins, and David D. Cox. Hy-\nperopt: A python library for optimizing the hyperpa-\nrameters of machine learning algorithms. In Proceed-\nings of the 12th Python in Science Conference , pages\n13–20, 2013.\n[5] Sebastian B ¨ock, Filip Korzeniowski, Jan Schl ¨uter, Flo-\nrian Krebs, and Gerhard Widmer. madmom: a new\nPython Audio and Music Signal Processing Library.\narXiv preprint arXiv:1605.07008 , 2016.\n[6] Sebastian B ¨ock and Markus Schedl. Polyphonic pi-\nano note transcription with recurrent neural networks.\nInAcoustics, Speech and Signal Processing (ICASSP),\n2012 IEEE International Conference on , pages 121–\n124. IEEE, 2012.\n[7] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and\nPascal Vincent. Modeling temporal dependencies in\nhigh-dimensional sequences: Application to poly-\nphonic music generation and transcription. arXiv\npreprint arXiv:1206.6392 , 2012.\n[8] Judith C. Brown. Calculation of a constant Q spec-\ntral transform. The Journal of the Acoustical Society\nof America , 89(1):425–434, 1991.\n[9] Sander Dieleman, Jan Schl ¨uter, Colin Raffel, Eben Ol-\nson, Søren Kaae Sønderby, Daniel Nouri, Daniel Mat-\nurana, Martin Thoma, Eric Battenberg, Jack Kelly,\nJeffrey De Fauw, Michael Heilman, diogo149, Brian\nMcFee, Hendrik Weideman, takacsg84, peterderivaz,\nJon, instagibbs, Dr. Kashif Rasul, CongLiu, Britefury,\nand Jonas Degrave. Lasagne: First release., August\n2015.\n[10] Sander Dieleman and Benjamin Schrauwen. End-to-\nend learning for music audio. In Acoustics, Speech and\nSignal Processing (ICASSP), 2014 IEEE International\nConference on , pages 6964–6968. IEEE, 2014.480 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016[11] Katharina Eggensperger, Matthias Feurer, Frank Hut-\nter, James Bergstra, Jasper Snoek, Holger Hoos, and\nKevin Leyton-Brown. Towards an empirical founda-\ntion for assessing bayesian optimization of hyperpa-\nrameters. In NIPS workshop on Bayesian Optimization\nin Theory and Practice , 2013.\n[12] Valentin Emiya, Roland Badeau, and Bertrand David.\nMultipitch estimation of piano sounds using a new\nprobabilistic spectral smoothness principle. Audio,\nSpeech, and Language Processing, IEEE Transactions\non, 18(6):1643–1654, 2010.\n[13] Xavier Glorot and Yoshua Bengio. Understanding the\ndifﬁculty of training deep feedforward neural net-\nworks. In International Conference on Artiﬁcial Intel-\nligence and Statistics , pages 249–256, 2010.\n[14] Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn ´ık,\nBas R. Steunebrink, and J ¨urgen Schmidhuber.\nLSTM: A search space odyssey. arXiv preprint\narXiv:1503.04069 , 2015.\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Delving Deep into Rectiﬁers: Surpassing Human-\nLevel Performance on ImageNet Classiﬁcation. arXiv\npreprint arXiv:1502.01852 , 2015.\n[16] Kurt Hornik, Maxwell Stinchcombe, and Halbert\nWhite. Multilayer feedforward networks are universal\napproximators. Neural networks , 2(5):359–366, 1989.\n[17] Frank Hutter, Holger Hoos, and Kevin Leyton-Brown.\nAn efﬁcient approach for assessing hyperparameter im-\nportance. In Proceedings of the 31st International Con-\nference on Machine Learning (ICML-14) , pages 754–\n762, 2014.\n[18] Sergey Ioffe and Christian Szegedy. Batch Normal-\nization: Accelerating Deep Network Training by\nReducing Internal Covariate shift. arXiv preprint\narXiv:1502.03167 , 2015.\n[19] Diederik Kingma and Jimmy Ba. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[20] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.\nDeep learning. Nature , 521(7553):436444, May 2015.\n[21] Benoit Mathieu, Slim Essid, Thomas Fillon, Jacques\nPrado, and Ga ¨el Richard. YAAFE, an Easy to Use and\nEfﬁcient Audio Feature Extraction Software. In Pro-\nceedings of the International Conference on Music In-\nformation Retrieval , pages 441–446, 2010.\n[22] Yurii Nesterov. A method of solving a convex program-\nming problem with convergence rate O(1/k2).Soviet\nMathematics Doklady , 27(2):372–376, 1983.\n[23] Ken O’Hanlon and Mark D. Plumbley. Polyphonic pi-\nano transcription using non-negative matrix factori-\nsation with group sparsity. In Acoustics, Speech andSignal Processing (ICASSP), 2014 IEEE International\nConference on , pages 3112–3116. IEEE, 2014.\n[24] Boris T. Polyak. Some methods of speeding up the con-\nvergence of iteration methods. USSR Computational\nMathematics and Mathematical Physics , 4(5):1–17,\n1964.\n[25] David E. Rumelhart, Geoffrey E. Hinton, and\nRonald J. Williams. Learning representations by back-\npropagating errors. Nature , 323(6088), 1986.\n[26] Siddartha Sigtia, Emmanouil Benetos, and Simon\nDixon. An end-to-end neural network for polyphonic\npiano music transcription. IEEE/ACM Transactions on\nAudio, Speech, and Language Processing , 24(5):927–\n939, May 2016.\n[27] Siddhartha Sigtia, Emmanouil Benetos, Nicolas\nBoulanger-Lewandowski, Tillman Weyde, Artur S.\nd’Avila Garcez, and Simon Dixon. A hybrid recurrent\nneural network for music transcription. In Acoustics,\nSpeech and Signal Processing (ICASSP), 2015 IEEE\nInternational Conference on , pages 2061–2065. IEEE,\n2015.\n[28] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.\nPractical bayesian optimization of machine learning al-\ngorithms. In Advances in neural information process-\ning systems , pages 2951–2959, 2012.\n[29] Jost Tobias Springenberg, Alexey Dosovitskiy,\nThomas Brox, and Martin Riedmiller. Striving for\nSimplicity: The All Convolutional Net. arXiv preprint\narXiv:1412.6806 , 2014.\n[30] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. Dropout:\nA simple way to prevent neural networks from over-\nﬁtting. The Journal of Machine Learning Research ,\n15(1):1929–1958, 2014.\n[31] David Sussillo. Random Walks: Training Very Deep\nNonlinear Feed-Forward Networks with Smart Initial-\nization. arXiv preprint arXiv:1412.6558 , 2014.\n[32] Peter M. Williams. Bayesian Regularization and Prun-\ning Using a Laplace Prior. Neural Computation ,\n7(1):117–143, Jan 1995.\n[33] Hui Zou and Trevor Hastie. Regularization and vari-\nable selection via the elastic net. Journal of the Royal\nStatistical Society: Series B (Statistical Methodology) ,\n67(2):301–320, 2005.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 481"
    },
    {
        "title": "Aligned Hierarchies: A Multi-Scale Structure-Based Representation for Music-Based Data Streams.",
        "author": [
            "Katherine M. Kinnaird"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417405",
        "url": "https://doi.org/10.5281/zenodo.1417405",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/020_Paper.pdf",
        "abstract": "We introduce aligned hierarchies, a low-dimensional rep- resentation for music-based data streams, such as record- ings of songs or digitized representations of scores. The aligned hierarchies encode all hierarchical decompositions of repeated elements from a high-dimensional and noisy music-based data stream into one object. These aligned hi- erarchies can be embedded into a classification space with a natural notion of distance. We construct the aligned hier- archies by finding, encoding, and synthesizing all repeated structure present in a music-based data stream. For a data set of digitized scores, we conducted experiments address- ing the fingerprint task that achieved perfect precision- recall values. These experiments provide an initial proof of concept for the aligned hierarchies addressing MIR tasks.",
        "zenodo_id": 1417405,
        "dblp_key": "conf/ismir/Kinnaird16",
        "content": "ALIGNED HIERARCHIES: A MULTI-SCALE STRUCTURE-BASED\nREPRESENTATION FOR MUSIC-BASED DATA STREAMS\nKatherine M. Kinnaird\nDepartment of Mathematics, Statistics, and Computer Science\nMacalester College, Saint Paul, Minnesota, USA\nkkinnair@macalester.edu\nABSTRACT\nWe introduce aligned hierarchies , a low-dimensional rep-\nresentation for music-based data streams, such as record-\nings of songs or digitized representations of scores. The\naligned hierarchies encode all hierarchical decompositions\nof repeated elements from a high-dimensional and noisy\nmusic-based data stream into one object. These aligned hi-\nerarchies can be embedded into a classiﬁcation space with\na natural notion of distance. We construct the aligned hier-\narchies by ﬁnding, encoding, and synthesizing all repeated\nstructure present in a music-based data stream. For a data\nset of digitized scores, we conducted experiments address-\ning the ﬁngerprint task that achieved perfect precision-\nrecall values. These experiments provide an initial proof of\nconcept for the aligned hierarchies addressing MIR tasks.\n1. INTRODUCTION\nFrom Foote’s ﬁeld-shifting introduction of the self-\nsimilarity matrix visualization for music-based data\nstreams in [9] to the enhanced matrix representations in\n[11, 17] and hierarchical segmentations in [14, 18, 21],\nmusic information retrieval (MIR) researchers have been\ncreating and using representations for music-based data\nstreams in pursuit of addressing a variety of MIR tasks,\nincluding structure tasks [10, 14, 17, 18], comparison tasks\n[2–4,11], and the beat tracking task [1,5,8,13]. These rep-\nresentations are often tailored to a particular task, limited\nto a single layer of information, or committed to a single\ndecomposition of structure. As a result most of the rep-\nresentations for music-based data streams provide narrow\ninsight into the content of the data stream they represent.\nIn this work, we introduce aligned hierarchies , a novel\nrepresentation that encodes multi-scale pattern information\nand overlays all hierarchical decompositions of those pat-\nterns onto one object by aligning1these hierarchical de-\ncompositions along a common time axis. This representa-\n1We note that ‘alignment’ in this case refers to placing found structure\nalong a common axis, not to matching a score to the recording of a piece.\nc/circlecopyrtKatherine M. Kinnaird. Licensed under a Creative Com-\nmons Attribution 4.0 International License (CC BY 4.0). Attribution:\nKatherine M. Kinnaird. “Aligned Hierarchies: A Multi-Scale Structure-\nBased Representation for Music-Based Data Streams”, 17th International\nSociety for Music Information Retrieval Conference, 2016.tion uncovers repeated structures observed in matrix rep-\nresentations widely used in MIR (such as self-similarity\nmatrices, self-dissimilarity matrices, and recurrence plots)\nand can be used to visualize all decompositions of the re-\npeated structures present in a particular music-based data\nstream as well as the relationships between the repeats\nin that data stream. By including and aligning all re-\npeated structures found in a music-based data stream, the\naligned hierarchies exist in the middle ground of represen-\ntations between the density of information in Foote’s self-\nsimilarity matrix visualization [9] and the sparsity of infor-\nmation in representations like those found in [1,11,14,20].\nBeyond the visualization beneﬁts, aligned hierarchies\nhave several compelling properties. Unlike many repre-\nsentations in the literature, the aligned hierarchies can be\nembedded into a classiﬁcation space with a natural dis-\ntance function. This distance function serves as the basis\nfor comparing two music-based data streams by measuring\nthe total dissimilarity of the patterns present. Additionally,\nthe aligned hierarchies can be post-processed to narrow our\nexploration of a music-based data stream to certain lengths\nof structure, or to address numerous MIR tasks, including\nthe cover song task, the segmentation task, and the chorus\ndetection task. Such post-processing techniques are not the\nfocus of this paper and will be explored further in future\nwork. In this paper, as a proof of concept for our approach\nto MIR comparison tasks, we use aligned hierarchies to\nperform experiments addressing the ﬁngerprint task on a\ndata set of digitized scores.\nThere are previous structure-based approaches to the\ncover song task, such as [1, 11, 20], that do not use the\nformal segmentation of pieces of music and instead, use\nenhanced matrix representations of songs as the basis of\ntheir comparisons. Like those in [9], these representations\ncompare the entire song to itself, but fail to intuitively show\ndetailed structural decompositions of each song. In [2–4],\na variety of music comparison tasks are addressed by de-\nveloping a method of comparison based on audio shingles,\nwhich encode local information. In this work, we use audio\nshingles as the feature vectors to form the self-dissimilarity\nmatrices representing the scores in the data set.\nIn Section 2, we introduce the aligned hierarchies and\nthe algorithm that builds them. In Section 3, we deﬁne\nthe classiﬁcation space that aligned hierarchies embed into\nand the associated distance function. In Section 4, we re-\nport on experiments using aligned hierarchies to address337the ﬁngerprint task for a data set of digitized scores, and\nwe summarize our contributions in Section 5.\n2. ALIGNED HIERARCHIES\nIn this section, we deﬁne the aligned hierarchies for a\nmusic-based data stream and present a motivating exam-\nple. We introduce the three phases for constructing the\naligned hierarchies with discussions about the purpose and\nmotivation for each phase. For simplicity, we will use\n‘song’ to refer to any kind of music-based data stream.\nThe algorithm ﬁnds meaningful repetitive structure in\na song from the self-dissimilarity matrix representing that\nsong. The algorithm aligns all possible hierarchies of that\nstructure into one object, called the aligned hierarchies of\nthe song. The aligned hierarchies Hhas three components:\nthe onset matrix BHwith the length vector wHand anno-\ntation vector αHthat together act as a key for BH.\nThe onset matrix BHis an (n×s)-binary matrix,\nwheresis the number of time steps in the song, and where\nnis the number of distinct kinds of repeated structure\nfound in the song. We deﬁne BHas follows\n(BH)i,j=\n\n1if an instance of ithrepeated\nstructure begins at time step j,\n0otherwise(1)\nThe length vector wHrecords the lengths of the re-\npeated structure captured in the rows of BHin terms\nof number of time steps, and the annotation vector αH\nrecords the labels for the groups of repeated structure en-\ncoded in these rows. These labels restart at 1for each dis-\ntinct length of repeated structure and serve to distinguish\ngroups of repeated structure with the same length from\neach other. We note that we can exchange any two rows in\nBHrepresenting repeats with the same length without los-\ning any information stored in the aligned hierarchies and\nwithout changing either wHorαH.\n2.1 Motivating Example\nSuppose we have a song that has two kinds of non-\noverlapping repetitive structure, such as a verse and a cho-\nrus, denoted VandC, respectively, appearing in the or-\nderVCVCV . We say that the song has the segmenta-\ntionVCVCV , whereVandCare the repeated sections.\nWe can segment the song in several ways: {V,C,V,C,V},\n{(VC),(VC),V}, or{V,(CV),(CV)}, with (VC)rep-\nresenting the piece of structure composed of the Vstruc-\nture followed by the Cstructure and similarly for (CV).\nNoting that both (VC)and(CV)can be decomposed into\nsmaller pieces, we would like to ﬁnd an object that captures\nand synthesizes all possible decompositions. Figure 1 is a\nvisualization of one such object where the Vstructure is 3\nbeats long and the Cstructure is 5beats long.\nThe object that produces the visualization shown in Fig-\nure 1 is known as the aligned hierarchies, and it encodes the\noccurrences and lengths of all the repeated structure found\nin a song. In Figure 1, we see that repeats of (VC)and\nthe repeats of (CV)overlap in time, but are not containedin each other. We also note that all decompositions of the\nrepeats of (VC)and(CV)are encoded in this object.\nIn this example, we have four kinds of repeated struc-\ntures:V,C,(VC), and (CV). Therefore BHassoci-\nated to the aligned hierarchies will have four rows, one\ncorresponding to each kind of repeated structure, and 19\ncolumns, one for each beat. Listing the rows in order of\nthe lengths of the repeated structures and the initial occur-\nrences of those repeats, we have that BHis a sparse matrix\nwith 1’s for the Vstructure at{(1,1), (1,9), (1,17)}, with\n1’s for theCstructure at{(2,4), (2,12)}, with 1’s for the\n(VC)structure at{(3,1), (3,9)}, and with 1’s for the (CV)\nstructure at{(4,4), (4,12)}. ThenwHis the column vector\n[3,5,8,8]tandαHis[1,1,1,2]t.\nV V V\nC C\n(VC) (VC)\n(CV) (CV)\nBeats\n1 4 9 12 17 20\nFigure 1 : Visualization of aligned hierarchies for a song\nwith segmentation VCVCV incorporating all possible de-\ncompositions of the song with Vstructure 3beats long and\nCstructure 5beats long.\n2.2 Building the Aligned Hierarchies\nThe construction of the aligned hierarchies begins with ei-\nther a self-similarity matrix or a self-dissimilarity matrix.\nBy beginning with a matrix representation for a song, we\nassume that we do not have access to the original presen-\ntation of the song, such as the audio recording, score, or\nmidi ﬁle. In a world with proprietary data, extremely high-\ndimensional data, and limited or restricted access to data,\nwe believe that it is important to develop robust techniques\nfor representing and comparing songs beginning from a\ndata representation that cannot be reverse engineered back\nto the original presentation of the data. For this work, we\nwill use a self-dissimilarity matrix to represent each song;\nan example of one for the score of Chopin Mazurka Op. 6,\nNo. 1 is shown in Figure 3a.\nOur construction of the aligned hierarchies for a song is\nmotivated by the fact that repeated structures in a song are\nrepresented as diagonals of small-valued entries in D, the\nself-dissimilarity matrix representing the song [6, 16, 17].\nIf such a diagonal of length kexists inDbeginning at entry\n(i,j), then the section of the song beginning at time step i\nthat isktime steps long is a repeat of the ktime step long\nsection beginning at time step j, and vice versa. We call\nthese sections a pair of repeats of size k.\nWe construct the aligned hierarchies from simple and\nmeaningful repetitive structure present in a song. For ex-\nample, suppose a sequence of ﬁve chords is played repeat-\nedly in a song. We do not regard repetitions of just the338 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016ﬁrst three chords as meaningful repeats, unless there is at\nleast one instance in the song of those three chords without\nthe last two or at least one instance of the last two chords\nwithout the ﬁrst three.\nBuilding the aligned hierarchies has three phases:\n1. Extract repeated structure of all possible lengths\nfromD, the self-dissimilarity matrix of the song\n2. Distill extracted repeated structure into their essen-\ntial structure components\n3. Build aligned hierarchies for the song using the es-\nsential structure components\n2.2.1 Phase 1 - Extract Repeated Structure from\nSelf-Dissimilarity Matrix D\nThere are four steps to extracting repeated structure from\nD. First, we deﬁne what repeats are, in context of the data\nand task at hand. Second, we extract the coarsest repeated\nstructure fromD. Third, we use this found structure to\nuncover further repeated structure hidden by the presenta-\ntion of the song as D. Lastly, we create groups of repeated\nstructure from the extracted pairs of repeats. In this last\nstep, we enforce a mimicking how humans notice and in-\nterpret patterns by removing any group of repeated struc-\nture that contains overlapping repeats.\nStep 1 : Based on the data and the task of interest, we set\na thresholdTthat deﬁnes how similar two sections must\nbe in order to be considered repeats of each other and then\nthresholdDaccordingly. We note that many ways exist in\nthe literature to set this threshold, such as [2, 10, 11, 16].\nThe resulting thresholded matrix Tis a binary matrix of\nthe same dimensions as Dand is given by\nTi,j=/braceleftbigg1 ifDi,j<T\n0 otherwise(2)\nStep 2 : We next ﬁnd and extract pairs of coarse repeats\nin the song, by ﬁnding all non-zero diagonals in T, record-\ning relevant information about the pair of repeats, and ﬁ-\nnally removing the associated diagonal from T. We loop\nover all possible repeat lengths, beginning with the largest\npossible structure (the number of columns in T) and end-\ning with 1 (the smallest possible structure).\nTo ﬁnd simple and meaningful structure of exactly\nlengthkrepresented by diagonals of exactly length k, we\nmust remove all diagonals of length greater than k. Sup-\npose that we did not remove diagonals of length (k+ 1)\nbefore searching for diagonals of length k, and let/hatwidedi,jbe\none such diagonal of 1’s in T. Then along with the other\ndiagonals of length k, our algorithm would ﬁnd two diag-\nonals of length k: one starting at (i,j)and another starting\nat(i+ 1,j+ 1) . Our algorithm would not be able to tell\nthat these diagonals of length kare contained in the diago-\nnal/hatwidedi,jor that together these diagonals make the diagonal\n/hatwidedi,j. Thus our algorithm would not be ﬁnding simple and\nmeaningful repeated structure in the song as required.\nStep 3 : Once we have extracted all diagonals from T,\nwe use the smaller extracted repeated structure to ﬁnd ad-\nditional repeated structures hidden in the coarse repeats.Suppose we examine a piece of text where a certain word\nis repeated both by itself and in a repeated phrase. In the\nprevious step, our algorithm would ﬁnd the repeated word\non its own and the repeated phrase, but would not detect\nthe repeated word as part of that repeated phrase. In this\nstep, our algorithm realizes that our repeated word is part\nof the repeated phrase and that the repeated phrase breaks\nup into at most three pieces, those being: 1) the part of\nthe phrase before our repeated word, 2) the repeated word\nitself, and 3) the part of the phrase after the repeated word.\nV C N V C V\nt = 1 1015 30 4045 55\nt = 1\n10\n15\n30\n40\n45\n55\n(a)Tfor a toy song with sections marked\nStart Time Step Start Time Step Repeat Length\nVC 1 31 15\nV 1 46 10\nV 31 46 10\n(b) Pairs of repeats after Step 2 of Phase 1, the initial extraction\nfromTwith sections marked\nStart Time Step Start Time Step Repeat Length\nVC 1 31 15\nV 1 46 10\nV 31 46 10\nV 1 31 10\nC 11 41 5\n(c) Pairs of repeats after Step 3 of Phase 1, the second part of\nextraction with sections marked\nFigure 2 : Thresholded matrix Tand the pairs of repeats\nuncovered after each step of repeat extraction for toy song\nwith segmentation VCNVCV\nConsider the song with segmentation VCNVCV and\nwith the thresholded distance matrix Tshown in Figure 2a.\nIn the initial extraction, the algorithm ﬁnds three pairs of\nrepeats, two pairs encoding repeats of the Vstructure by\nitself and one pair encoding two repeats of (VC), as shown\nin Table 2b. But the algorithm has not detected that the pair\nof(VC)repeats contain the smaller found Vstructure as\nwell as the yet to be isolated Cstructure. In this step, as\nshown in Table 2c, by using either of the pairs of Vrepeats,\nwe ﬁnd that the pair of (VC)repeats does contain a pair of\nVrepeats as well as a pair of smaller repeats that is not the\nsame as theVstructure, known as the Cstructure.\nStep 4 : In the last step of this phase, we form groups\nof repeats from the pairs of repeats such that each kind of\nrepeated structure has exactly one group of repeats associ-Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 339ated to it. For the example shown in Figure 2a, we have\nthree groups: one associated to (VC), another associated\ntoV, and a third associated to C.\nTo mimic human segmentation of music, we check that\neach group does not contain repeats that overlap in time.\nFor the example in Section 2.1, we are more likely to de-\nscribe the structure of a popular song by saying “the verse\nand chorus are repeated twice together followed by the\nverse again,” than by saying “the verse, chorus, and verse\nare repeated together twice such that those two repeats\noverlap at one verse.” Thus we do not encode the repeated\nstructure (VCV )in the aligned hierarchies shown in Fig-\nure 1, even though it occurs twice in VCVCV .\n2.2.2 Phase 2 - Distill Essential Structure Components\nJust like words that are composed of syllables, musical el-\nements, such as motifs and chord progressions, are com-\nposed of smaller components. In this step, we distill re-\npeats of the song into their essential structure components ,\nthe building blocks that form every repeat in the song.\nBy deﬁnition, we allow each time step to be contained\nin at most one of the song’s essential structure compo-\nnents. In this phase, we pairwise compare groups of re-\npeats, checking if the repeats in a given pair of groups over-\nlap in time. If they do, we divide the repeats in a similar\nfashion as used in Step 3 in Phase 1, forming new groups\nof repeats that do not overlap in time. We iterate this pro-\ncess, dividing repeats as necessary, until each time step is\ncontained in at most one repeated structure. The repeats\nremaining at the end of this phase are our essential struc-\nture components. For the example in Section 2.1 shown\nin Figure 1, the essential structure components are the in-\nstances of the VandCstructures. Figure 3b is a visualiza-\ntion of the essential structure components for the score of\nChopin’s Mazurka Op. 6, No. 1.\n2.2.3 Phase 3 - Construct Aligned Hierarchies from\nEssential Structure Components\nIn this ﬁnal phase, we build the aligned hierarchies from\nthe essential structure components. We employ a process\nthat is akin to taking right and left unions of the essential\nstructure components to ﬁnd all possible non-overlapping\nrepeats in the song. We encode these repeats in the on-\nset matrix and form the length and annotation vectors that\ntogether are the key for the onset matrix. Figure 4 is a visu-\nalization of the aligned hierarchies for a score of Chopin’s\nMazurka Op. 6, No. 1.\n3. COMPARING ALIGNED HIERARCHIES\nTo compare aligned hierarchies, we embed them into a\nclassiﬁcation space with a distance function measuring\nthe total dissimilarity between pairs of songs. In Sec-\ntion 3.1, we explain how aligned hierarchies embed into\nthis classiﬁcation space, and we present the distance func-\ntion used for comparing aligned hierarchies of songs in\nSection 3.2.2\n2The proofs for the material in this section can be found in the author’s\ndoctoral thesis [12].\n(a) Self-dissimilarity matrix D. Black denotes values near 0.\n(b) Essential structure components\nFigure 3 : Visualizations for a score of Chopin’s Mazurka\nOp. 6, No. 1 with repeat markers observed.\n3.1 Classiﬁcation Space for Aligned Hierarchies\nTo deﬁne (S∗)n, the space that we embed aligned hierar-\nchies into, while simultaneously demonstrating how this\nembedding occurs, we begin by representing aligned hier-\narchies as a sequence of matrices.\nDeﬁnition 3.1. Given a particular song with stime steps\nand its aligned hierarchies H, we deﬁne a sequence of s\nbinary matrices/braceleftbig\nBk/bracerightbigs\nk=1where thekthbinary matrix Bk\nis the rows of BHsuch thatwH=k, which are the rows\ncorresponding to repeats of exactly ktime steps. If there\nare no repeats of exactly ktime steps, then Bkis a row of\nszeros. For brevity, we will use/braceleftbig\nBk/bracerightbig\nfor/braceleftbig\nBk/bracerightbigs\nk=1.\nWe note that each binary matrix in/braceleftbig\nBk/bracerightbig\ndoes not have\na pair of vectors acting as a key for it, as we have in H.\nOur deﬁnition of/braceleftbig\nBk/bracerightbig\nnaturally encodes the information\nfromwHin/braceleftbig\nBk/bracerightbig\n. Similarly, we construct αHso that the\nlabels for the groups of repeats restart at 1for each distinct\nrepeat length l. Thus, for each l, the label corresponding to\na row inBl∈/braceleftbig\nBk/bracerightbig\nis simply that row’s index in Bl.\nWe recall that we can exchange any two rows of BH\nwithwH=lwithout changing the annotation labels.\nSo we say that two matrices encoding repeats of exactly340 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Figure 4 : Visualization of aligned hierarchies for a score\nof Chopin’s Mazurka Op. 6, No. 1 with repeat markers\nobserved.\nlengthlare the same if one is a row permutation of the\nother. Therefore the space that we embed the aligned hier-\narchies into must inherit this notion of matrix equality.\nDeﬁnition 3.2. LetSbe the space of (m×t)-binary matri-\nces withm,t∈Z≥1. Consider the symmetric group Sm,\nthe group of all permutations for the rows of the matrices.\nThe matrix denoted Mσ(r)∈Sis the matrix with the rows\nofM∈S in the order prescribed by σ(r)∈Sm.\nProposition 3.1. Let∼be the relation onSsuch that for\nM,Q∈S, we say that M∼QifM=Qσ(r), for some\nσ(r)∈Sm. Then∼is an equivalence relationship on S.\nDeﬁnition 3.3. LetS∗be the quotient space S/∼. Then\nthe product space (S∗)nis composed of n-copies ofS∗.\nWe embedHinto(S∗)nby settingt=s, the number\nof time steps in the song, and choosing m=κmax, where\nκmax= max\nl∈{1,...,s}/braceleftbig\nr|ris the number of rows in Bl/bracerightbig\n. Then\nwe place each Bl∈/braceleftbig\nBk/bracerightbig\ninto thelthspace of (S∗)n. So\nthelthquotient space in (S∗)ncorresponds to the classiﬁ-\ncation space of patterns of length lpresent in songs.\nNotation : The elements of product space (S∗)nare\nsequences of elements in S∗, pedantically denoted as\n(qg)n\ng=1withqg∈ S∗, for eachg∈ {1,...,n}. For\nbrevity, we will use (qg)for(qg)n\ng=1.3.2 Metric for Comparing Aligned Hierarchies\nTo deﬁne a metric on the space (S∗)nthat will measure the\ntotal dissimilarity between two songs represented by their\naligned hierarchies, we begin by deﬁning a function that\nmeasures the dissimilarity between patterns of a ﬁxed size\npresent in those two aligned hierarchies.\nDeﬁnition 3.4. Let||·|| 1be the entry-wise 1-norm. Given\nanys1,s2∈S∗, letf:S∗×S∗→Rbe the function\ngiven by\nf(s1,s2) = min\nδ∈s1\nβ∈s2||δ−β||1 (3)\nProposition 3.2. The function f:S∗×S∗→Ris a\ndistance function.\nTo deﬁne the metric that measures the total dissimilarity\nbetween two songs, we use the above function fto com-\npute the dissimilarity between the repeated patterns at each\nsize and total the measured dissimilarities. This gives us\nthetotal dissimilarity between the repeated patterns of all\nsizes present in two aligned hierarchies.\nCorollary 1. Let(qg),(rg)∈(S∗)n. The function\ndH: (S∗)n×(S∗)n→Ris a distance function, where\ndHis given by\ndH((qg),(rg)) =n/summationdisplay\ng=1f(qg,rg). (4)\n4. PROOF OF CONCEPT RESULTS\nIn this section, we consider the ﬁngerprint task for a data\nset of digitized musical scores. These experiments serve as\na proof of concept for our method of comparing songs via\ntheir aligned hierarchies. With the exception of the feature\nextraction, the code implementing the creation and com-\nparison of aligned hierarchies is written in MATLAB.3\n4.1 Data Set and Features\nOur data set is based on 52 Mazurka scores by Chopin.\nFor each score, we download two human-coded, digitized\nversions, called **kern ﬁles , posted on the KernScore on-\nline database [19].4The ﬁrst version has the repeated sec-\ntions repeated as many times as marked in the score and\nthe second has the repeated sections presented only once\nper time written. For scores that have no sections that are\nrepeated in their entirety, we download the single **kern\nﬁle twice, marking one copy as having the repetitions re-\npeated and the second copy as having the repetitions not\nrepeated. Each version of a score is referred to as a song\nand there are 104 songs in our data set.\nIn this data set, the notion of time is in terms of beats\nwith one time step per beat. For each beat, we extract the\nchroma feature vector, encoding the amount of each of the\n12 Western pitch classes present in that beat [15]. To do\n3The URL to the code used for the experiments can be found at\nhttps://github.com/kmkinnaird/ThesisCode/releases/tag/vT.ﬁnal2\n4The **kern ﬁles can be accessed at:\nhttp://kern.humdrum.org/search?s=t&keyword=ChopinProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 341this, we used the music21 Python library [7].5We form\naudio shingles, that encode local contextual information,\nby concatenating γconsecutive chroma feature vectors, for\na ﬁxed integer γ.\nWe create a symmetric self-dissimilarity matrix Dusing\na cosine dissimilarity measure between all pairs of audio\nshingles. Let ai,ajbe the audio shingles for time steps i\nandj, respectively. Then we deﬁne\nDi,j=/parenleftbigg\n1−<ai,aj>\n||ai||2||aj||2/parenrightbigg\n(5)\nBy settingγ, we set the smallest size of repeated struc-\nture that can be detected. For this work, we set γ= 6 or\nγ= 12 . Assuming that the average tempo of a Mazurka is\napproximately 120beats per minute, if γ= 6, our shingles\nencode about 3seconds of information similar to the audio\nshingles in [2,3]. Similarly, if γ= 12 , our shingles encode\nfour bars of three beats each or about 6seconds.\n4.2 Evaluation Procedure\nFor all of our experiments, given a particular threshold, we\nconstruct the aligned hierarchies for each score. We com-\npute the pairwise distances between the songs’ representa-\ntions as described in Section 3.2. Using these pairwise dis-\ntances, we create a network for the data set with the songs\nin the data set as the nodes. In the ﬁngerprint task, we only\nmatch songs that are exact copies of each other, and so we\ndeﬁne an edge between two nodes if the distance between\nthe two aligned hierarchies associated to the songs is 0.\nWe evaluate the results of our experiments by comput-\ning the precision-recall values for the resulting network\ncompared against a network representing the ground truth,\nwhich is formed by placing edges between the two identi-\ncal copies of the score present in the data set. This ground\ntruth was informed by a data-key based on human-coded,\nmeta-information about the scores.\nFor each experiment, we set γ, the width of the audio\nshingles, and T, the threshold value for deﬁning when two\naudio shingles are repeats of each other. The choice of γ\nandTaffects the amount of structure classiﬁed as repeats,\nwhich impacts whether or not a song has aligned hierar-\nchies to represent it. If a song does not have aligned hier-\narchies, due to the choice of γandT, we remove the node\nrepresenting that song from consideration in both our ex-\nperiment network and in our ground truth network, as there\nwould be nothing for our method to use for comparison.\n4.3 Results\nWe conducted 10 experiments with the threshold\nT∈{0.01,0.02,0.03,0.04,0.05}and withγ∈{6,12}.\nEach experiment yielded a perfect precision-recall value.\nFor the experiment with T= 0.01andγ= 12 , we had\n5 songs without aligned hierarchies including 2 pairs of\nsongs based on scores without repeated sections. For the\nexperiment with T= 0.02andγ= 12 , we had 1 song\nwithout aligned hierarchies, but this song was based on a\n5See http://web.mit.edu/music21/ for information about music21 .score with repeated sections and thus, under the ﬁngerprint\ntask, would not be matched to another song in our data set.\nWe note that our method did discover an error in the\ndata key for the Mazurka scores. According to the human-\ncoded, meta-information, Mazurka Op. 17, No. 1 was clas-\nsiﬁed as having sections marked in the score as being re-\npeated. However, the score of Mazurka Op. 17, No. 1 in\nfact does not have any sections marked to be repeated. Our\nalgorithm correctly detected this error, and we corrected\nour version of the data key for these Mazurka scores. The\ncorrected data key is our ground truth, which is what our\nprecision-recall values are based on. To our knowledge,\nthere is no published work using this data set, therefore\nwe cannot provide numerical comparisons between our\nmethod and other ones.\nFor all 10 experiments, we have correctly identiﬁed all\nscores, with aligned hierarchies, that do not have sections\nmarked in the score to be repeated. Based on the construc-\ntion of the score data set, a perfect recall rate was expected.\nMore interestingly, the perfect precision rate means that we\ndo not falsely match scores using the aligned hierarchies.\n5. CONCLUSION\nIn this paper, we have introduced the aligned hierarchies,\nan innovative, multi-scale structure-based representation\nfor music-based data streams. The aligned hierarchies pro-\nvide a novel visualization for repeated structure in music-\nbased data streams. Differing from the literature of en-\nhanced matrix representations, instead of showing a com-\nparison of a data stream to itself, the visualization of the\naligned hierarchies synthesizes all possible hierarchical de-\ncompositions of that data stream onto one time axis, al-\nlowing for a straightforward understanding of the tempo-\nral relationships between the repeated structures found in a\nparticular data stream.\nThe aligned hierarchies also provide a mathemati-\ncally rigorous method for comparing music-based data\nstreams using this low-dimensional representation. We\nperformed experiments addressing the ﬁngerprint task for\ndata based on digitized scores. These experiments had per-\nfect precision-recall rates and provided a proof of concept\nfor the aligned hierarchies.\nIn future work, we will develop post-processing tech-\nniques for the aligned hierarchies. These techniques will\nallow us to address additional MIR tasks, such as the cover\nsong task and the chorus detection task. We also will con-\ntinue to develop the theory and metrics associated with\naligned hierarchies and their derivatives.\nAcknowledgements\nThis work is a portion of the author’s doctoral thesis\n[12], which was partially funded by the GK-12 Program at\nDartmouth College (NSF award # 0947790). Part of this\nwork was performed while the author was visiting the In-\nstitute for Pure and Applied Mathematics (IPAM), which\nis supported by the National Science Foundation. The au-\nthor thanks Scott Pauls, Michael Casey, and Dan Ellis for\ntheir feedback on early versions of this work.342 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20166. REFERENCES\n[1] J. Bello. Measuring structural similarity in music.\nIEEE Transactions on Audio, Speech, and Language\nProcessing , 19(7):2013–2025, 2011.\n[2] M. Casey, C. Rhodes, and M. Slaney. Analysis of min-\nimum distances in high-dimensional musical spaces.\nIEEE Transactions on Audio, Speech, and Language\nProcessing , 16(5):1015 – 1028, 2008.\n[3] M. Casey and M. Slaney. Song intersection by approx-\nimate nearest neighbor search. Proceedings of the In-\nternational Society for Music Information Retrieval ,\npages 144–149, 2006.\n[4] M. Casey and M. Slaney. Fast recognition of remixed\naudio. 2007 IEEE International Conference on Audio,\nSpeech and Signal Processing , pages IV – 1425 – IV–\n1428, 2007.\n[5] M. Casey, R. Veltkamp, M. Goto, M. Leman,\nC. Rhodes, and M. Slaney. Content-based music in-\nformation retrieval: Current directions and future chal-\nlenges. Proceedings of the IEEE , 96(4):668– 696,\n2008.\n[6] M. Cooper and J. Foote. Summarizing popular music\nvia structural similarity analysis. IEEE Workshop on\nApplications of Signal Processing to Audio and Acous-\ntics, pages 127 –130, 2003.\n[7] M. S. Cuthbert and C. Ariza. music21 : A toolkit for\ncomputer-aided musicology and symbolic music data.\n11th International Society for Music Information Re-\ntrieval Conference , 2010.\n[8] D.P.W. Ellis and G.E. Poliner. Identifying ‘cover\nsongs’ with chroma features and dynamic program-\nming beat tracking. Proceedings of the International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , pages IV – 1429–1432, 2007.\n[9] J. Foote. Visualizing music and audio using self-\nsimilarity. Proc. ACM Multimedia 99 , pages 77–80,\n1999.\n[10] M. Goto. A chorus-section detection method for musi-\ncal audio signals and its application to a music listen-\ning station. IEEE Transactions on Audio, Speech, and\nLanguage Processing , 14(5):1783–1794, 2006.\n[11] P. Grosche, J. Serr `a, M. M ¨uller, and J.Ll. Arcos.\nStructure-based audio ﬁngerprinting for music re-\ntrieval. 13th International Society for Music Informa-\ntion Retrieval Conference , 2012.\n[12] K. M. Kinnaird. Aligned Hierarchies for Sequential\nData . PhD thesis, Dartmouth College, 2014.\n[13] B. McFee and D. P. W. Ellis. Better beat tracking\nthrough robust onset aggregation. In International con-\nference on acoustics, speech and signal processing ,\nICASSP, 2014.[14] B. McFee and D. P. W. Ellis. Learning to segment\nsongs with ordinal linear discriminant analysis. In In-\nternational conference on acoustics, speech and signal\nprocessing , ICASSP, 2014.\n[15] M. M ¨uller and S. Ewert. Chroma Toolbox: MATLAB\nimplementations for extracting variants of chroma-\nbased audio features. 12th International Society for\nMusic Information Retrieval Conference , 2011.\n[16] M. M ¨uller, P. Grosche, and N. Jiang. A segment-based\nﬁtness measure for capturing repetitive structures of\nmusic recordings. 12th International Society for Mu-\nsic Information Retrieval Conference , pages 615–620,\n2011.\n[17] J. Paulus, M. M ¨uller, and A. Klapuri. Audio-based mu-\nsic structure analysis. 11th International Society for\nMusic Information Retrieval Conference , pages 625–\n636, 2010.\n[18] C. Rhodes and M. Casey. Algorithms for determining\nand labelling approximate hierarchical self-similarity.\n8th International Society for Music Information Re-\ntrieval Conference , 2007.\n[19] C.S. Sapp. Online database of scores in the humdrum\nﬁle format. Proceedings of the International Society for\nMusic Information Retrieval , pages 664–665, 2005.\n[20] D.F. Silva, H. Papadopoulos, G.E.A.P.A. Batista, and\nD.P.W. Ellis. A video compression-based approach to\nmeasure music structure similarity. Proceedings of the\nInternational Society for Music Information Retrieval ,\npages 95–100, 2013.\n[21] K. Yoshii and M. Goto. Unsupervised music under-\nstanding based on nonparametric bayesian models.\nProceedings of the International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , pages\n5353–5356, 2012.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 343"
    },
    {
        "title": "Global Properties of Expert and Algorithmic Hierarchical Music Analyses.",
        "author": [
            "Phillip B. Kirlin"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416118",
        "url": "https://doi.org/10.5281/zenodo.1416118",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/259_Paper.pdf",
        "abstract": "In recent years, advances in machine learning and in- creases in data set sizes have produced a number of viable algorithms for analyzing music in a hierarchical fashion according to the guidelines of music theory. Many of these algorithms, however, are based on techniques that rely on a series of local decisions to construct a complete music analysis, resulting in analyses that are not guaranteed to resemble ground-truth analyses in their large-scale organi- zational shapes or structures. In this paper, we examine a number of hierarchical music analysis data sets — draw- ing from Schenkerian analysis and other analytical systems based on A Generative Theory of Tonal Music — to study three global properties calculated from the shapes of the analyses. The major finding presented in this work is that it is possible for an algorithm that only makes local de- cisions to produce analyses that resemble expert analyses with regards to the three global properties in question. We also illustrate specific similarities and differences in these properties across both ground-truth and algorithmically- produced analyses.",
        "zenodo_id": 1416118,
        "dblp_key": "conf/ismir/Kirlin16",
        "content": "GLOBAL PROPERTIES OF EXPERT AND ALGORITHMIC\nHIERARCHICAL MUSIC ANALYSES\nPhillip B. Kirlin\nDepartment of Mathematics and Computer Science, Rhodes College\nkirlinp@rhodes.edu\nABSTRACT\nIn recent years, advances in machine learning and in-\ncreases in data set sizes have produced a number of viable\nalgorithms for analyzing music in a hierarchical fashion\naccording to the guidelines of music theory. Many of these\nalgorithms, however, are based on techniques that rely on\na series of local decisions to construct a complete music\nanalysis, resulting in analyses that are not guaranteed to\nresemble ground-truth analyses in their large-scale organi-\nzational shapes or structures. In this paper, we examine\na number of hierarchical music analysis data sets — draw-\ning from Schenkerian analysis and other analytical systems\nbased on A Generative Theory of Tonal Music — to study\nthree global properties calculated from the shapes of the\nanalyses. The major ﬁnding presented in this work is that\nit is possible for an algorithm that only makes local de-\ncisions to produce analyses that resemble expert analyses\nwith regards to the three global properties in question. We\nalso illustrate speciﬁc similarities and differences in these\nproperties across both ground-truth and algorithmically-\nproduced analyses.\n1. INTRODUCTION\nMusic analysis refers to a set of techniques that can illus-\ntrate the ways in which a piece of music is constructed,\ncomposed, or organized. Many of these procedures focus\non illustrating relationships between certain types of mu-\nsical objects, such as harmonic analysis , which can show\nhow chords and harmonies in a composition function in\nrelation to each other, or voice-leading analysis , whose\npurpose is to illustrate the ﬂow of a melodic line through\nthe music. Some types of analysis are explicitly hierarchi-\ncal, in that their purpose is to construct a hierarchy of mu-\nsical objects illustrating that some objects occupy places\nof higher prominence in the music than others. Different\nkinds of hierarchical analysis have different methods for\ndetermining the relative importance of objects in the hier-\narchy. The most well-known of these hierarchical proce-\ndures is Schenkerian analysis , which organizes the notes\nof a composition in a hierarchy according to how much\nc/circlecopyrtPhillip B. Kirlin. Licensed under a Creative Commons\nAttribution 4.0 International License (CC BY 4.0). Attribution: Phillip\nB. Kirlin. “Global Properties Of Expert and Algorithmic Hierarchical\nMusic Analyses”, 17th International Society for Music Information Re-\ntrieval Conference, 2016.each note contributes to the overall musical structure. The\nprocedure also illustrates how notes at one level of the hi-\nerarchy function in relation to surrounding notes at higher\nand lower levels. While Schenkerian analysis is the most\ncommon type of hierarchical analysis in the music theory\ncommunity, there are a number of similar procedures that\nwere developed in the music and linguistics community,\nspeciﬁcally the procedures put forth in the book A Genera-\ntive Theory of Tonal Music by Lerdahl and Jackendoff [10],\nabbreviated here as GTTM . GTTM applies an explicitly hi-\nerarchical view to multiple aspects of a musical composi-\ntion, leading to two types of analysis known as time-span\nreductions andprolongational reductions , both of which\nare similar to Schenkerian analysis in that all three analyt-\nical methods organize the pitches of the music in hierar-\nchies of relative importance, allowing a person to view a\nmusical composition at multiple levels of abstraction.\nNone of these types of musical analysis were origi-\nnally developed as computational algorithms, and so all\ncontain certain ambiguities in their deﬁnitions. Schenke-\nrian analysis, in particular, is known for originally being\ndeﬁned primarily through examples and not via a step-\nby-step procedure. Similarly, the two GTTM reductional\nanalysis systems contain preference rules that can con-\nﬂict with each other; the authors explicitly state that there\nis not enough information in GTTM to provide a “fool-\nproof algorithm” for analyzing a composition. Neverthe-\nless, there are now a number of automated computational\nsystems that can construct analyses in a Schenkerian fash-\nion [7, 11] or by following the rules of time-span or pro-\nlongational reductions in the GTTM formalism [4]. The\nmost common computational technique underlying these\nsystems and others like them is the context-free grammar :\nsuch a formalism is widely-adopted because such gram-\nmars are easily applied to musical objects, are inherently\nrule-based, can be adapted to work with probabilities, and\nadmit a computationally-feasibly O(n3)parsing algorithm\nthat can be used to ﬁnd the best analysis for a piece of mu-\nsic.\nThe largest downside to context-free grammars is pre-\ncisely that they are context-free : there are restrictions on\nhow much musical context can be used when applying the\nrules of a grammar to “parse” a piece of music into an\nanalysis. Most decisions made during the analysis process\nunder the context-free paradigm have to be made some-\nwhat “locally,” and are unable to consider many important\n“global” properties that are critical to producing a high-640quality musical analysis. For instance, certain composi-\ntional techniques, such as identical repetitions of an arbi-\ntrarily melodic sequence, are impossible to describe sat-\nisfactorily with a context-free grammar [13]. Other com-\nmon practices, such as the rules of musical form, manifest\nthemselves as certain shapes and structures in the hierar-\nchy [9, 10, 15], and it is unclear whether a purely context-\nfree system could identify such structures. In this work,\nwe show (a) evidence that context-free grammars can re-\nproduce certain global structures in music analyses, and\n(b) similarities and differences in those global structures\nacross various types of hierarchical music analysis.\n2. REPRESENTATION OF HIERARCHICAL\nANALYSES\nFor simplicity, we assume we are analyzing a monophonic\nsequence of notes. The two most common ways to do this\nin a hierarchical manner are (a) to create a hierarchy di-\nrectly over the notes, and (b) to create a hierarchy over\nthe melodic intervals between the notes. Each representa-\ntion has distinct advantages and disadvantages [2, 12], but\nSchenkerian analysis is more easily represented by a hi-\nerarchy of melodic intervals. We explain this through the\nfollowing example. Imagine we wish to analyze the ﬁve-\nnote descending passage in Figure 1, which takes place\nover G-major harmony. Schenkerian analysis is guided by\nthe concept of a prolongation : a situation where a note or\npair of notes controls a musical passage even though the\ngoverning note or notes may not be sounding throughout\nthe entire passage. For instance, in Figure 1, the sequence\nD–C–B contains the passing tone C, and we say the C pro-\nlongs the motion from the D to the B. The effect is simi-\nlar for the notes B–A–G. However, there is another level\nof prolongation at work: the entire ﬁve-note span is gov-\nerned by the beginning and ending notes D and G. This\ntwo-level structure can be represented by the binary tree\nshown in Figure 2(a), which illustrates this hierarchy of\nprolongations through the melodic intervals they encom-\npass. A more succinct representation is shown in Figure\n2(b): this structure is known as a maximal outerplanar\ngraph orMOP , and illustrates the same hierarchy as the\nbinary tree [14].\n/noteheads.s2/accidentals.sharp/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2DCBAG\nFigure 1 . An arpeggiation of a G-major chord with passing\ntones. The slurs are a Schenkerian notation used to indicate\nthe locations of prolongations.\nA MOP is a graph representing a complete triangulation\nof a polygon. Like their binary tree equivalents, MOPs are\nrooted, but by an edge, rather than a vertex; this edge rep-\nresents the most abstract level of the melodic hierarchy.\nEvery triangle within a MOP corresponds to a hierarchical\nrelationship among the three notes that form the triangle,\nwith the middle note taking on a subservient role in relation\n(a)(b)D–GD–BB–GD–CC–BB–AA–GDGBCAFigure 2 . The prolongational hierarchy of a G-major chord\nwith passing tones represented as (a) a tree of melodic in-\ntervals, and (b) a MOP.\nto the left and right notes. It is equivalent to say that every\ntriangle has a parent edge and two child edges, or has two\nparent vertices and a child vertex. Triangles closer to the\nroot of the MOP express more abstract relationships than\nthose farther away. Though originally developed to repre-\nsent Schenkerian prolongations, we will use MOPs later in\nthis paper to work with the GTTM reductional systems as\nwell.\n2.1 Large-Scale Organization of MOPs\nLike binary trees, MOPs can be described by a variety\nof global attributes that are determined from their overall\nshape. We explore three such attributes and how common\nmusic composition and analysis practices affect these at-\ntributes.\nHeight: Theheight of a MOP is deﬁned to be the num-\nber of triangles in the longest possible sequence of trian-\ngles from the root edge moving through subsequent child\nedges to the bottom of the MOP. It is analogous to the\nheight of the equivalent binary tree. For a MOP with a\nﬁxed number of triangles, there are a certain range of pos-\nsible heights; for instance, Figure 3 shows two MOPs with\nﬁve triangles, one with a height of 5 and one with a height\nof 3. Because a MOP with nvertices will always con-\ntainn−2triangles, we can say the maximum height of\nsuch a MOP is n−2, whereas the minimum height is\n⌈log2(n−1)⌉.\nheight = 3height = 5\nFigure 3 . Two MOPs, each with ﬁve triangles, but differ-\nent heights.\nInvestigating the heights of the MOPs that result from\nhierarchical analysis gives us insight into the composi-\ntional structure of the music from which the MOPs were\ncreated. MOPs with large heights result from situations\nwhere the notes of critical importance in a hierarchy are\npositioned towards the beginning and end of a musical\npassage, with importance decreasing monotonically as one\nmoves towards the middle of the passage in question (as in\nthe left MOP of Figure 3. In contrast, MOPs with small\nheights result from hierarchies where the structural impor-\ntance does not increase or decrease monotonically over\ntime during a passage, but rather rises and falls in a pat-\ntern similar to how strong and weak beats fall rhythmicallyProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 641within a piece.\nAverage Path Length : While height provides a coarse\nmeasure of the shape of a MOP, a different metric, average\npath length , provides ﬁner-grained detail about the bal-\nancedness or skewedness of the structure [5]. Measuring\nthe level of balancedness or unbalancedness in a tree gives\nan overall sense of whether the leaves of the tree are all at\nroughly the same level or not; when applied to a MOP, this\ndetermines whether the leaf edges are all roughly the same\ndistance away from the root edge. Towards that end, the\naverage (external) path length in a MOP is calculated by\naveraging the lengths of all the paths from the root edge to\nall leaf child edges. For a MOP with nvertices, the mini-\nmum average path length is very close to log2n+ 1, while\nthe maximum average path length is(n−1)(n+2)\n2n[1, 8].\nGTTM and related literature posits that music is con-\nstructed around structures that are largely balanced, that is,\ntending towards shorter average path lengths. One reason\nfor this, from a prolongational standpoint, is that a melodic\nhierarchy expressed as a MOP illustrates patterns of ten-\nsion and relaxation: each triangle represents tension rising\nfrom the left parent vertex to the child, then relaxing from\nthe child to the right parent. Balanced MOP structures im-\nply waves of tension and relaxation roughly on the same\nlevel, whereas unbalanced MOPs imply tension and relax-\nation patterns that may seem implausible to a listener: “it is\nmost unlikely that a phrase or piece begins in utmost ten-\nsion and proceeds more or less uniformly towards relax-\nation, or that it begins in relaxation and proceeds toward a\nconclusion of utmost tension.” [9, 10].\nLeft-Right Skew : An important consideration not ac-\ncounted for by the concepts of height or average path\nlength is determining whether the MOP has more left-\nbranching structures or more right-branching structures.\nTo this end, we deﬁne a variant of path length by choos-\ning to count a left-branch within a path as −1and a right-\nbranch as +1. It is clear that this metric assigns negative\nnumbers to all MOP edges that lie on paths from the root\nedge with more left branches than right branches, and pos-\nitive numbers to edges in the opposite situation. Using\nthis measure of distance, we deﬁne the left-right skew of\na MOP to be the sum of these numbers for all paths in a\nMOP from the root edge to a leaf edge, giving us an over-\nall sense of whether the MOP is skewed to the left or to the\nright. Due to the organization of leaf edges within a MOP,\na fully right-branching MOP with nvertices will achieve a\nleft-right skew of\nn−4/summationdisplay\ni=−1i+ (n−2) =n2\n2−5n\n2+ 3\nand a fully left-branching MOP will achieve a correspond-\ning negative value.\n3. FIRST EXPERIMENT\nOur ﬁrst experiment is intended to answer the question,\n“Can a fully-automated algorithm for music analysis based\non context-free parsing techniques produce MOPs withglobal structural attributes matching those of ground-truth\nMOPs?” Note that we are assuming that the three global at-\ntributes — MOP height, average path length, and left-right\nskew — are not randomly distributed; this assumption is\nbased on the previous work described earlier detailing that\nthe overall shape of a hierarchical music analysis is most\ndecidedly not random, but inﬂuenced by the way compo-\nsitions are constructed and the manner in which listeners\nhear and interpret them.\nWe used the P ARSE MOPsystem in concert with\nthe S CHENKER 41 data set to conduct this experiment.\nSCHENKER 41 is a data set of 41 common practice period\nmusical excerpts along with corresponding Schenkerian\nanalyses in MOP form for each excerpt [6]. The excerpts\nare homogeneous: they are all in major keys, written or ar-\nranged for a keyboard instrument or voice with keyboard\naccompaniment, and do not modulate. The correspond-\ning analyses of the excerpts are all derived from textbooks\nor other expert sources, and can be regarded as ground\ntruth. S CHENKER 41 serves as training data for the P ARSE -\nMOPmachine-learning system, which learns the rules of\nSchenkerian analysis by inferring a probabilistic context-\nfree grammar from patterns extracted from S CHENKER 41\n[7]. After training, P ARSE MOPcan produce MOP analy-\nses for new, previously-unseen pieces of music.\nThere are three variants of P ARSE MOP, which vary only\nin treatment of the Urlinie , a uniquely Schenkerian con-\ncept. According to Schenker, all tonal music compositions\nshould have, at the most abstract level of the melodic hier-\narchy, one of three possible background structures. These\nthree structures, representing Schenker’s fundamental con-\nception of melody, consist of a stepwise descent from the\nthird, ﬁfth, or eighth scale degree to the tonic below, and\nthe entire melodic content of the piece serves as an elabo-\nrate prolongation of this descending melodic line. P ARSE -\nMOP-A, when trained on the S CHENKER 41 corpus, does\nnot have any a priori knowledge of the Urlinie : it does\nnot even know that such a concept exists in music. There-\nfore, P ARSE MOP-A produces output MOPs that usually\ndo not contain an Urlinie , except if by chance. P ARSE -\nMOP-B, on the other hand, is given information about the\nUrlinie for the pieces of music it is analyzing. P ARSE -\nMOP-B produces output MOPs that always contain the cor-\nrectUrlinie (the structure is copied from the input music).\nClearly, P ARSE MOP-A and P ARSE MOP-B represent two\nopposite ends of the spectrum with regard to the Urlinie .\nPARSE MOP-C is a compromise between the two: it uses\nextra rules in the context-free grammar to guarantee that\nanUrlinie will be produced in the output MOP analyses,\nbut it may not match the notes of the correct Urlinie ex-\nactly.\nWe ran the three P ARSE MOPvariants using leave-one-\nout cross-validation on each of the 41 excerpts in the\nSCHENKER 41 corpus, leaving us with four sets of MOPs:\none ground-truth, and three algorithmically produced. Be-\ncause the minimum and maximum values for each of the\nthree global MOP attributes are dependent on the number\nof vertices in a MOP (corresponding to the length of the642 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016musical excerpt in question), we normalized the values for\nthe attributes as follows. MOP height was scaled to always\noccur between 0 and 1, with 0 corresponding to a MOP\nof minimum height for a given piece, and 1 corresponding\nto the maximum height. Average path length was simi-\nlarly scaled to be between 0 and 1. Left-right skew was\nscaled to be between −1and+1, with 0 corresponding to a\nperfectly left-right balanced MOP, and −1/+1correspond-\ning to maximally left- or right-branching MOPs. Finally,\nwe compared the distribution of the attributes obtained for\neach P ARSE MOPvariant against corresponding attribute\ndistributions calculated from the ground-truth MOPs; his-\ntograms can be seen in Figure 4.\nThe data illustrate a number of phenomena. Firstly, the\nhistograms for height and average path length suggest that\nall the data sets, both ground-truth and algorithmically-\nproduced, show a preference for MOPs that are a com-\npromise between balanced and unbalanced, but tending to-\nward balanced: very deep MOPs or MOPs with long path\nlengths (values close to 1) are avoided in all data sets.\nPARSE MOP-B and -C have higher average heights and av-\nerage path lengths than P ARSE MOP-A due to the presence\nof an Urlinie , which is always triangulated in a deep, un-\nbalanced fashion. This is most evident in the P ARSE MOP-\nB and -C plots for left-right skew: these show a dramatic\nleft-branching structure that is almost certainly due to the\nUrlinie , especially when compared to the histogram for\nPARSE MOP-A.\nSecondly, it is clear that in some situations, the context-\nfree grammar formalism does a remarkably good job at\npreserving the overall shape of the distribution of the\nattributes, at least through visual inspection of the his-\ntograms. We can conﬁrm this by computing Spearman’s\nrank correlation coefﬁcient ρfor each pair of data — this\ncalculates the correlation between a list of the 41 pieces\nsorted by a ground-truth metric and the same metric after\nhaving been run through P ARSE MOP. All pairs show pos-\nitive correlation coefﬁcients, with eight of the nine being\nstatistically signiﬁcant at the α= 0.005 level (obtained\nvia the ˇSid´ak correction on 0.05). In particular, rank corre-\nlation coefﬁcients for all three P ARSE MOP-B comparisons\nare all greater than 0.9, indicating a strong correlation.\nHowever, a high Spearman ρcoefﬁcient does not nec-\nessarily imply the distributions are identical. We ran two-\nsample two-tailed t-tests on each pair of data to determine\nif the means of the two data sets in question were dif-\nferent (the null hypothesis being that the means of each\ndata set within a pair were identical). Two cases resulted\ninp-values signiﬁcant at the α= 0.005 level, indicat-\ning rejection of the null hypothesis: the height compar-\nisons for P ARSE MOP-B, and the left-right skew compar-\nisons for P ARSE MOP-C. This implies a situation where\nPARSE MOP-B is apparently very good preserving relative\nranks of MOP heights in the data set (this rank correlation\nbetween algorithmic MOP heights and ground-truth MOP\nheights was revealed above), but there is also a statistically\nsigniﬁcant, though small, difference in the means of the\ndistribution of these heights.4. SECOND EXPERIMENT\nOur ﬁrst experiment suggests that there may be some bias\nin our calculations being introduced by the Urlinie , namely\nbecause it has a particular structure that is always present in\nthe resulting analyses. This situation is further complicated\nby the presence of a number of short pieces of music in the\nSCHENKER 41 data set, where, for instance, the music may\nconsist of ten notes, ﬁve of which constitute the Urlinie .\nIn a situation like this, the MOP structure is already likely\ndetermined by the locations of the notes of the Urlinie , and\nso the P ARSE MOPalgorithm has very little effect on the\nﬁnal shape of the MOP analysis. In short, we suspect that\nthese two factors may be artiﬁcially increasing the height\nand average path length of the algorithmically-produced\nMOPs.\nTo address this, we replicated the ﬁrst experiment but\nonly calculated the global MOP attributes for pieces with at\nleast 18 notes (leaving 23 pieces out of 41), hypothesizing\nthat having more notes in the music would outweigh the\neffects of the Urlinie . The leave-one-out cross-validation\nstep was not altered (this still used all the data). Figure 5 il-\nlustrates the new histograms compiled for this experiment.\nIn short, these new data support our hypothesis: removing\nshort pieces largely eliminates very deep MOPs and those\nwith very long average path lengths.\nWe can again address similarities and differences us-\ning tests involving Spearman’s correlation coefﬁcient ρ\nand pairedt-tests. All of the statistically signiﬁcant re-\nsults forρrelating to P ARSE MOP-B still remain: all three\nglobal MOP structure attributes calculated on the P ARSE -\nMOP-B MOPs are strongly positively rank-correlated ( ρ>\n0.8) with the ground-truth MOP attributes. There is a\nweaker rank correlation ( ρ≈0.583) between the left-\nright skew attribute calculated on the P ARSE MOP-C data\nand its ground-truth that is also statistically signiﬁcant\n(p < 0.005). In contrast, the two statistical signiﬁcances\nidentiﬁed via the t-tests in the ﬁrst experiment both disap-\npear when run on only the pieces of at least 18 notes, sug-\ngesting that these association may have spurious, caused\nby noise in the shorter pieces.\n5. THIRD EXPERIMENT\nIn our third experiment, we branched out from Schenke-\nrian analysis to explore A Generative Theory of Tonal Mu-\nsic’s time-span and prolongational reductions. These are\ntwo forms of music analysis that, like Schenkerian analy-\nsis, are designed to illustrate a hierarchy among the notes\nof a musical composition.\nTime-span reduction is introduced in GTTM as\ngrounded in the concept of pitch stability: listeners con-\nstruct pitch hierarchies based primarily on the relative con-\nsonance or dissonance of a pitch as determined by the prin-\nciples of Western tonal music theory. However, pitch sta-\nbility is not a sufﬁcient criteria upon which to found a re-\nductional system, because pitches do not occur in a vac-\nuum, but take place over time: there are temporal and\nrhythmic considerations that are required. Lerdahl andProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 6430.0 0.2 0.4 0.6 0.8 1.0051015202530Frequency¯x=0.313, 0.289ParseMop A\n0.0 0.2 0.4 0.6 0.8 1.0\nHeight¯x=0.389, 0.362ParseMop B\n0.0 0.2 0.4 0.6 0.8 1.0¯x=0.366, 0.362ParseMop C\n0.0 0.2 0.4 0.6 0.8 1.0051015202530Frequency¯x=0.268, 0.274\n0.0 0.2 0.4 0.6 0.8 1.0\nAverage path length¯x=0.379, 0.362\n0.0 0.2 0.4 0.6 0.8 1.0¯x=0.343, 0.362\n0.6\n 0.4\n 0.2\n0.0 0.2 0.4 0.6051015202530Frequency¯x=-0.003, -0.024\n0.6\n 0.4\n 0.2\n0.0 0.2 0.4 0.6\nLeft-right skew¯x=-0.311, -0.313\n0.6\n 0.4\n 0.2\n0.0 0.2 0.4 0.6¯x=-0.264, -0.313Figure 4 . Histograms displaying the distribution of global MOP attributes comparing algorithmically-generated MOPs\n(grey bars) and ground-truth MOPs (black bars). Sample means are shown for algorithmic and truth MOPs, respectively.\nThe ground-truth bars for P ARSE MOP-A are different from -B and -C because P ARSE MOP-A has no conception of the\nUrlinie , and therefore the Urlinie in the ground-truth MOPs is triangulated slightly differently in the training data for\nPARSE MOP-A.\n0.0 0.2 0.4 0.6 0.8 1.00246810121416Frequency¯x=0.214, 0.184ParseMop A\n0.0 0.2 0.4 0.6 0.8 1.0\nHeight¯x=0.307, 0.289ParseMop B\n0.0 0.2 0.4 0.6 0.8 1.0¯x=0.28, 0.289ParseMop C\n0.0 0.2 0.4 0.6 0.8 1.00246810121416Frequency¯x=0.162, 0.151\n0.0 0.2 0.4 0.6 0.8 1.0\nAverage path length¯x=0.301, 0.287\n0.0 0.2 0.4 0.6 0.8 1.0¯x=0.265, 0.287\n0.6\n 0.4\n 0.2\n0.0 0.2 0.4 0.60246810121416Frequency¯x=-0.01, -0.011\n0.6\n 0.4\n 0.2\n0.0 0.2 0.4 0.6\nLeft-right skew¯x=-0.278, -0.276\n0.6\n 0.4\n 0.2\n0.0 0.2 0.4 0.6¯x=-0.214, -0.276\nFigure 5 . Histograms of the same variety as in Figure 4, but only for excerpts containing 18 or more notes.\n0.0 0.2 0.4 0.6 0.8 1.0\nHeight0102030405060Frequency¯x=0.14, 0.158\n0.0 0.2 0.4 0.6 0.8 1.0\nAverage path length¯x=0.145, 0.161\n0.3\n0.2\n0.1\n0.00.10.20.3\nLeft-right skew¯x=-0.007, -0.041\nFigure 6 . Histograms of the global MOP structure attributes comparing prolongational reductions (grey bars) and time-span\nreductions (black bars).644 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Jackendoff address this by basing time-span reductions on\ntheir conceptualizations of metrical and grouping struc-\nture, where metrical structure is determined from analyz-\ning the strong and weak beats of a composition, while the\ngrouping structure comes from a listener perceiving notes\ngrouped into motives, phrases, themes, and larger sections.\nThough similar to time-span reduction, prolongational\nreduction adds the concepts of tension and relaxation to the\ncriteria that are used to form a musical hierarchy. The mo-\ntivation for the need for two types of reduction is that time-\nspan reductions cannot express some structural relation-\nships that take place across grouping boundaries, which\ndetermine the overall form of a time-span analysis. In con-\ntrast, prolongational reductions are not tied to grouping\nboundaries, and therefore can represent rising and falling\ntension across such boundaries. The two types of trees\nproduced by the reductional systems are often similar in\nbranching structure at the background levels, but become\nmore dissimilar at lower levels of the hierarchies [10].\nBecause time-span and prolongation reductions seem\nsimilar on the surface, it is appropriate to address their sim-\nilarities and differences through a study of the three global\nattributes calculated for MOPs in the previous section. We\nperform this study by using the GTTM database developed\nby Hamanaka et al. [3], through their research on the fea-\nsibility of automating the analytical methods described in\nGTTM [4]. This database consists of 300 eight-bar ex-\ncerpts of music from the common practice period, along\nwith time-span reductions and prolongational reductions\nfor certain subsets of the pieces. Speciﬁcally, there are 99\nexcerpts that include both time-span and prolongational re-\nductions. Note that these reductions in the database were\nproduced by a human expert, not an algorithm.\nOur ﬁrst task was to convert the time-span and prolon-\ngation reductions into MOPs. This is necessary because\nalthough time-span and prolongational reductions are ex-\npressed through binary trees (which are structurally equiv-\nalent to MOPs), the GTTM reductions use binary trees cre-\nated over the notes of a composition, whereas MOPs are\nequivalent to binary trees created over melodic intervals\nbetween notes, as shown earlier in Figure 2. Therefore, we\nrequire an algorithm to convert between these two funda-\nmentally different representations.\nTime-span and prolongational reductions are repre-\nsented by trees with primary and secondary branching, like\nthat of Figure 7(a). Phase one of the conversion algorithm\nconverts these trees into an intermediate representation: a\nmulti-way branching tree where all children of a note are\nrepresented at the same level, as in Figure 7(b). Phase\ntwo converts this intermediate representation to a MOP\nby adding edges in appropriate places, as in Figure 7(c).\nThis conversion algorithm is guaranteed to preserve all hi-\nerarchical parent-child relationships present in the original\ntime-span or prolongational tree. It may introduce other\nrelationships through adding additional edges, however.\nOnce all the time-span and prolongational reductions\nwere converted into MOPs, we computed histograms of the\nMOP height, average path length, and left-right skew for\nWXYZYWXZYWXZ(a)(b)(c)Figure 7 . Illustration of a time-span/prolongational tree\nstructure converted into a MOP.\nboth the time-span reductions and prolongational MOPs.\nThese are shown in Figure 6. Spearman’s rank coefﬁcient\ntest reveals positive rank-correlations between MOP height\n(ρ= 0.660), average path length ( ρ= 0.762), and left-\nright skew (ρ= 0.300) calculated from time-span analyses\nand the corresponding attribute for prolongational analy-\nses. At the same time, paired t-tests suggest that the sample\nmeans have statistically signiﬁcant differences for all three\nattributes as well, when comparing time-span and prolon-\ngational reductions. Lastly, though the paired histograms\nfor height and average path length may appear similar, the\nleft-right skew paired histograms seem more visually dif-\nferent. This is conﬁrmed via a two-sample Kolmogorov-\nSmirnov test, which indicates the left-right skew values for\ntime-span versus prolongational reductions are drawn from\ndifferent distributions. All of these statistical signiﬁcances\naccount for multiple comparisons using the ˇSid´ak correc-\ntion (α= 0.05→α= 0.017).\n6. CONCLUSIONS\nThe data presented here suggest a number of conclusions.\nThe ﬁrst two experiments involving P ARSE MOPimply that\nwhen P ARSE MOPmakes mistakes in analyzing music, the\nmistakes do not drastically change the overall shape or\nstructure of the corresponding ground-truth analysis. This\ninformation is challenging to reconcile with the fact that\nPARSE MOP, like any music analysis algorithm derived\nfrom context-free parsing techniques, does no global cal-\nculations related to shape or structure during the analyti-\ncal process. One explanation is that the notes of a music\ncomposition imply an overall shape and structure that the\nanalytical process simply reveals, in that the shape is in-\nherently present in the music and does not have to be given\nexplicitly to the grammar. If this were true, then using\na formal grammar class higher in the Chomsky hierarchy\n(e.g., a grammar with some amount of context-sensitivity)\nmay not be necessary to create algorithms that can analyze\nmusic satisfactorily.\nThe third experiment comparing time-span and prolon-\ngational analyses reveals fundamental differences and sim-\nilarities in the overall structure of the two analytical forms.\nFor instance, it is clear that both types of reductional sys-\ntem strongly prefer balanced, shallow trees, as is clear\nfrom the histograms on height, average path length, and\nleft-right skew. Also, both analysis varieties produce trees\nthat slightly skew to the left. However, our statistical tests\nalso strongly suggest that the underlying distributions of\nthe global MOP structure attributes are different, even if\nthe differences in means happen to be small.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 6457. REFERENCES\n[1] Helen Cameron and Derick Wood. Maximal path\nlength of binary trees. Discrete Applied Mathematics ,\n55(1):15–35, 1994.\n[2]´Edouard Gilbert and Darrell Conklin. A probabilistic\ncontext-free grammar for melodic reduction. In Pro-\nceedings of the International Workshop on Artiﬁcial\nIntelligence and Music, 20th International Joint Con-\nference on Artiﬁcial Intelligence , pages 83–94, Hyder-\nabad, India, 2007.\n[3] Masatoshi Hamanaka, Keiji Hirata, and Satoshi Tojo.\nMusical structural analysis database based on GTTM.\nInProceedings of the 15th International Society for\nMusic Information Retrieval Conference , pages 325–\n330, 2015.\n[4] Masatoshi Hamanaka, Keiji Hirata, and Satoshi Tojo.\nσGTTM III: Learning based time-span tree genera-\ntor based on PCFG. In Proceedings of the 11th In-\nternational Symposium on Computer Music Multidis-\nciplinary Research , pages 303–317, 2015.\n[5] Maarten Keijzer and James Foster. Crossover bias in\ngenetic programming. In Proceedings of the European\nConference on Genetic Programming , pages 33–44,\n2007.\n[6] Phillip B. Kirlin. A data set for computational stud-\nies of Schenkerian analysis. In Proceedings of the 15th\nInternational Society for Music Information Retrieval\nConference , pages 213–218, 2014.\n[7] Phillip B. Kirlin and David D. Jensen. Using super-\nvised learning to uncover deep musical structure. In\nProceedings of the 29th AAAI Conference on Artiﬁcial\nIntelligence , pages 1770–1776, 2015.\n[8] Donald E. Knuth. The Art of Computer Programming,\nVolume 3: Sorting and Searching . Addison Wesley\nLongman, Redwood City, CA, second edition, 1998.\n[9] Fred Lerdahl. Tonal Pitch Space . Oxford University\nPress, Oxford, 2001.\n[10] Fred Lerdahl and Ray Jackendoff. A Generative Theory\nof Tonal Music . MIT Press, Cambridge, MA, 1983.\n[11] Alan Marsden. Schenkerian analysis by computer: A\nproof of concept. Journal of New Music Research ,\n39(3):269–289, 2010.\n[12] Panayotis Mavromatis and Matthew Brown. Parsing\ncontext-free grammars for music: A computational\nmodel of Schenkerian analysis. In Proceedings of the\n8th International Conference on Music Perception &\nCognition , pages 414–415, 2004.\n[13] Mariusz Rybnik, Wladyslaw Homenda, and Tomasz\nSitarek. Foundations of Intelligent Systems: 20th Inter-\nnational Symposium, ISMIS 2012 , chapter Advanced\nSearching in Spaces of Music Information, pages 218–\n227. 2012.[14] Jason Yust. Formal Models of Prolongation . PhD the-\nsis, University of Washington, 2006.\n[15] Jason Yust. Organized Time , chapter Structural Net-\nworks and the Experience of Musical Time. 2015. Un-\npublished manuscript.646 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Integration and Quality Assessment of Heterogeneous Chord Sequences Using Data Fusion.",
        "author": [
            "Hendrik Vincent Koops",
            "W. Bas de Haas",
            "Dimitrios Bountouridis",
            "Anja Volk"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1415954",
        "url": "https://doi.org/10.5281/zenodo.1415954",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/019_Paper.pdf",
        "abstract": "Two heads are better than one, and the many are smarter than the few. Integrating knowledge from multiple sources has shown to increase retrieval and classification accu- racy in many domains. The recent explosion of crowd- sourced information, such as on websites hosting chords and tabs for popular songs, calls for sophisticated algo- rithms for data-driven quality assessment and data integra- tion to create better, and more reliable data. In this pa- per, we propose to integrate the heterogeneous output of multiple automatic chord extraction algorithms using data fusion. First we show that data fusion creates significantly better chord label sequences from multiple sources, outper- forming its source material, majority voting and random source integration. Second, we show that data fusion is capable of assessing the quality of sources with high pre- cision from source agreement, without any ground-truth knowledge. Our study contributes to a growing body of work showing the benefits of integrating knowledge from multiple sources in an advanced way.",
        "zenodo_id": 1415954,
        "dblp_key": "conf/ismir/KoopsHBV16",
        "content": "INTEGRATION AND QUALITY ASSESSMENT OF HETEROGENEOUS\nCHORD SEQUENCES USING DATA FUSION\nHendrik Vincent Koops1W. Bas de Haas2Dimitrios Bountouridis1Anja Volk1\n1Department of Information and Computing Sciences, Utrecht University, the Netherlands\n{h.v.koops, d.bountouridis, a.volk }@uu.nl\n2Chordify, the Netherlands {bas}@chordify.net\nABSTRACT\nTwo heads are better than one, and the many are smarter\nthan the few. Integrating knowledge from multiple sources\nhas shown to increase retrieval and classiﬁcation accu-\nracy in many domains. The recent explosion of crowd-\nsourced information, such as on websites hosting chords\nand tabs for popular songs, calls for sophisticated algo-\nrithms for data-driven quality assessment and data integra-\ntion to create better, and more reliable data. In this pa-\nper, we propose to integrate the heterogeneous output of\nmultiple automatic chord extraction algorithms using data\nfusion. First we show that data fusion creates signiﬁcantly\nbetter chord label sequences from multiple sources, outper-\nforming its source material, majority voting and random\nsource integration. Second, we show that data fusion is\ncapable of assessing the quality of sources with high pre-\ncision from source agreement, without any ground-truth\nknowledge. Our study contributes to a growing body of\nwork showing the beneﬁts of integrating knowledge from\nmultiple sources in an advanced way.\n1. INTRODUCTION AND RELATED WORK\nWith the rapid growth and expansion of online sources\ncontaining user-generated content, a large amount of con-\nﬂicting data can be found in many domains. For exam-\nple, different encyclopediæ can provide conﬂicting infor-\nmation on the same subject, and different websites can\nprovide conﬂicting departure times for public transporta-\ntion. A typical example in the music domain is provided\nby websites offering data that allows for playing along with\npopular songs, such as tabs or chords. These websites of-\nten provide multiple, conﬂicting chord label sequences for\nthe same song. The availability of these large amounts\nof data poses the interesting problem of how to combine\nthe knowledge from different sources to obtain better, and\nmore reliable data. In this research, we address the prob-\nlem of ﬁnding the most appropriate chord label sequence\nc/circlecopyrtHendrik Vincent Koops, W. Bas de Haas, Dimitrios\nBountouridis, Anja V olk. Licensed under a Creative Commons Attri-\nbution 4.0 International License (CC BY 4.0). Attribution: Hendrik\nVincent Koops, W. Bas de Haas, Dimitrios Bountouridis, Anja V olk. “In-\ntegration and Quality Assessment of Heterogeneous Chord Sequences us-\ning Data Fusion”, 17th International Society for Music Information Re-\ntrieval Conference, 2016.for a piece out of conﬂicting chord label sequences. Be-\ncause the correctness of chord labels is hard to deﬁne (see\ne.g. [26]), we deﬁne “appropriate” in the context of this\nresearch as agreeing with a ground truth. An example of\nanother evaluation context could be user satisfaction.\nA pivotal problem for integrating data from different\nsources is determining which source is more trustworthy.\nAssessing the trustworthiness of a source from its data is a\nnon-trivial problem. Web sources often supply an external\nquality assessment of the data they provide, for example\nthrough user ratings (e.g. three or ﬁve stars), or popularity\nmeasurements such as search engine page rankings. Un-\nfortunately, Macrae et al. have shown in [18] that no cor-\nrelation was found with the quality of tabs and user ratings\nor search engine page ranks. They propose that a better\nway to assess source quality is to use features such as the\nagreement (concurrency) between the data. Naive meth-\nods of assessing source agreement are often based on the\nassumption that the value provided by the majority of the\nsources is the correct one. For example, [1] integrates mul-\ntiple symbolic music sequences that originate from differ-\nent optical music recognition ( OMR ) algorithms by picking\nthe symbol with the absolute majority at every position in\nthe sequences. It was found that OMR may be improved us-\ning naive source agreement measures, but that substantial\nimprovements may need more elaborate methods.\nImproving results by combining the power of multi-\nple algorithms is an active research area in the music do-\nmain, whether it is integrating the output of similar algo-\nrithms [28], or the integration of the output of different\nalgorithms [15], such as the integration of features into a\nsingle feature vector to combine the strengths of multiple\nfeature extractors [12, 19, 20]. Nevertheless, none of these\ndeal with the integration and quality assessment of hetero-\ngeneous categorical data provided by different sources.\nRecent advancements in data science have resulted in\nsophisticated data integration techniques falling under the\numbrella term data fusion , in which the notion of source\nagreement plays a central role. We show that data fusion\ncan achieve a more accurate integration than naive methods\nby estimating the trustworthiness of a source, compared to\nthe more naive approach of just looking at which value is\nthe most common among sources. To our knowledge no\nresearch into data fusion exists in the music domain. Re-178search in other domains has shown that data fusion is ca-\npable of assessing correct values with high precision, and\nsigniﬁcantly outperforms other integration methods [7,25].\nIn this research, we apply data fusion to the problem\nof ﬁnding the most appropriate chord label sequence for a\npiece by integrating heterogeneous chord label sequences.\nWe use a method inspired by the A CCUCOPY model that\nwas introduced by Dong et al. in [7, 8] to integrate con-\nﬂicting databases. Instead of databases, we propose to in-\ntegrate chord label sequences. With the growing amount\nof crowd-sourced chord label sequences online, integration\nand quality assessment of chord label sequences are impor-\ntant for a number of reasons. First, ﬁnding the most appro-\npriate chord labels from a large amount of possibly noisy\nsources by hand is a very cumbersome process. An au-\ntomated process combining the shared knowledge among\nsources solves this problem by offering a high quality in-\ntegration. Second, to be able to rank and offer high quality\ndata to their users, websites offering conﬂicting chord la-\nbel data need a good way to separate the wheat from the\nchaff. Nevertheless, as was argued above, both integration\nand quality assessment have shown to be hard problems.\nTo measure the quality of chord label sequence inte-\ngration, we propose to integrate the outputs of different\nMIREX Audio Chord Estimation ( ACE) algorithms. We\nchose this data, because it offers us the most reliable\nground truth information, and detailed analysis of the algo-\nrithms to make a high quality assessment of the integrated\noutput. Our hypothesis is that through data fusion, we can\ncreate a chord label sequence that is signiﬁcantly better in\nterms of comparison to a ground truth than the individual\nestimations. Secondly, we hypothesize that the results of\nintegrated chord label sequences have a lower standard de-\nviation on their quality, hence are more reliable.\nContribution. The contribution of this paper is three-\nfold. First, we show the ﬁrst application of data fusion in\nthe domain of symbolic music. In doing so, we address\nthe question how heterogeneous chord label sequences de-\nscribing a single piece of music can be combined into an\nimproved chord label sequence. We show that data fusion\noutperforms majority voting and random picking of source\nvalues. Second, we show how data fusion can be used to\naccurately estimate the relative quality of heterogeneous\nchord label sequences. Data fusion is better at capturing\nsource quality than the most frequently used source quality\nassessment methods in multiple sequence analysis. Third,\nwe show that our purely data-driven method is capable\nof capturing important knowledge shared among sources,\nwithout incorporating domain knowledge.\nSynopsis. The remainder of this paper is structured as\nfollows: Section 2 provides an introduction to data fusion.\nSection 3 details how integration of chord label sequences\nusing data fusion is evaluated. Section 4 details the results\nof integrating submissions of the MIREX 2013 automatic\nchord extraction task. The paper closes with conclusions\nand a discussion, which can be found in Section 5.2. DATA FUSION\nWe investigate the problem of integrating heterogeneous\nchord label sequences using data fusion. Traditionally, the\ngoal of data fusion is to ﬁnd the correct values within au-\ntonomous and heterogeneous databases (e.g. [9]). For ex-\nample, if we obtain meta-data (ﬁelds such as year, com-\nposer, etc) from different web sources of the song “Black\nBird” by The Beatles, there is a high probability that some\nsources will contradict each other on some values. Some\nsources will attribute the composer correctly to “Lennon\n- McCartney”, but others will provide just “McCartney”,\n“McCarth ey”, etc. Typos, malicious editing, data corrup-\ntion, incorrectly predicted values, and human ignorance are\nsome of the reasons why sources are hardly ever error-free.\nNevertheless, if we assume that most of the values that\nsources provide are correct, we can argue that values that\nare shared among a large amount of sources are often more\nprobable to be correct than values that are provided by only\na single source. Under the same assumption, we can also\nargue that sources that agree more with other sources are\nmore accurate , because they share more values that are\nlikely to be correct. Therefore, if a value is provided by\nonly a single but very accurate source, we can prefer it over\nvalues with higher probabilities from less accurate sources,\nthe same way we are more open to accepting a deviating\nanswer from a reputable source in an everyday discussion.\nIn the above examples, we assume that each source is\nindependent. In real-life this is rarely the case: informa-\ntion can be copied from one website to the other, students\nrepeat what their teacher tells them and one user can en-\nter the same values in a database twice, which can lead\nto inappropriate values being copied by a large number\nof sources: “A lie told often enough becomes the truth”\n(Lenin1) [8]. Intuitively, we can predict the dependency\nof sources from their sharing of inappropriate values. In\ngeneral, inappropriate values are assumed to be uniformly\ndistributed, which implies that sharing a couple of identical\ninappropriate values is a rare event. For example, the rare\nevent of two students sharing a number of identical inap-\npropriate answers on an exam is indicative of copying from\neach other. Therefore, by analyzing which values with low\nprobabilities are shared between sources, we can calculate\na probability of their dependence.\nIn this research, instead of using databases, we address\nthese issues through data fusion on heterogeneous chord\nlabel sequences. Our goal is to take heterogeneous chord\nlabel sequences of the same song and create a chord label\nsequence that is better than the individual ones. We take\ninto account: 1) the accuracy of sources, 2) the probabil-\nities of the values provided by sources, and 3) the prob-\nability of dependency between sources. In the following\nsections, we refer to different versions of the same song as\nsources , each providing a sequence of values called chord\nlabels . See Table 1 for an example, showing four sources\n(S0...3), each providing a sequence of three chord labels,\nand FUSION , an example of data fusion output.\n1Ironically, this quote’s origin is unclear, but most sources cite Lenin.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 179S0 C:maj A:min A:min F:maj\nS1 C:maj F:maj G:maj F:maj\nS2 C:maj F:maj A:min D:min\nS3 C:maj F:maj A:min D:min\nMV C:maj F:maj A:min ?\nDF C:maj F:maj A:min D:min\nTable 1 : Example of four sources S(0...3) providing different\nchord label sequences for the same song. DFshows an example\noutput of data fusion on these sources. DFis identical to majority\nvote ( MV) on the ﬁrst three chord labels. For the last chord label,\nDFchooses D:min by taking into account source accuracy, while\nmajority vote would randomly pick either F:maj orD:min .\n2.1 Source Accuracy\nBy taking into account the accuracy of a source, we can\ndeal with issues that arise from simple majority voting. For\nexample in Table 1, the ﬁnal chord labels in the sequence\n(F:maj andD:min ) are provided by the same number of\nsources. Solving which chord to choose here would re-\nquire picking randomly one of the two, or using auxiliary\nknowledge such as harmony theory to make a good choice.\nAnother problem is that sometimes a source can pro-\nvide an appropriate chord label that contradicts all other\nsources. Majority vote would assign the lowest probability\nto this chord, although it might come from a source that\noverall agrees a lot with other sources. Intuitively, we have\nmore trust in a source that we believe is more accurate,\nwhich is implemented as follows. The chord labels of a\nsource are weighted according to the overall performance\nof that source: if a source provides a large number of val-\nues that agree with other sources, we consider it to be more\naccurate and more trustworthy, and vice versa.\nThe accuracy of a source is deﬁned by Dong et al. in [7]\nas follows. We calculate source accuracy by taking the\narithmetic mean of the probabilities of all chord labels the\nsource provides. As an example, suppose we estimate the\nprobabilities of the chords in Table 1 based on their fre-\nquency count (c.q. likelihood). That is, C:maj for the\nﬁrst column is 1, A:min for the second column is 1/4, etc.\nThen, if we take the average of the chord label probabil-\nities of the ﬁrst source in our example of Table 1 we can\ncalculate the source accuracy A(S0)ofS0as follows:\nA(S0) =1 + 1/4+3/4+1/2\n4= 0.625 (1)\nIn the same way, we can calculate the source accuracies for\nthe other three sources which are 0.625, 0.75 and 0.75 for\nS1,S2and S3respectively.\nAssuming that the sources are independent, then the\nprobability that a source provides an appropriate chord la-\nbel is its source accuracy. Conversely, the probability that\na source provides an inappropriate chord is the fraction\nof the inverse of the source accuracy over all possible in-\nappropriate values n:(1−A(S))\nn. For example, for major\nand minor chord labels we have 12 roots and 2 modes,\nwhich means that for every correct chord label there are\nn= (12∗2)−1 = 23 inappropriate chord labels. With\nmore complex chord labels (sevenths, added notes, inver-\nsions),nincreases combinatorially.\nThe chord labels of sources with higher accuracies will\nbe more likely to be selected through the use of vote counts ,which are used as weights for the probabilities of the chord\nlabels they provide. With nandA(Si)we can derive a vote\ncountVS(Si)of a sourceSi. The vote count of a source is\ncomputed as follows:\nVS(Si) = lnnA(Si)\n1−A(Si)(2)\nApplied to our example, this results in vote counts of 2.62\nfor S0and S1, and 2.80 for S2and S3. The higher vote\ncount for S2and S3means that its values are more likely to\nbe appropriate than those of S0and S1.\n2.2 Chord Label Probabilities\nAfter having deﬁned the accuracy of a source, we can now\ndetermine which chord labels provided by all the sources\nare most likely the appropriate labels, by taking into ac-\ncount source accuracy. In the computation of chord label\nprobabilities we take into account a) the number of sources\nthat provide those chord labels and b) the accuracy of their\nsources. With these values we calculate the vote count\nVC(L)of a chord labelL, which is computed as the sum\nof the vote counts of its providers:\nVC(L) =/summationdisplay\nσ∈SLVS(σ) (3)\nwhereSLis the set of all sources that provide the chord\nlabelL. For example, for the vote count of F:maj in the\nlast column of the example in Table 1, we take the sum of\nthe vote counts of S0and S1. For the vote count of D:min\nwe take the sum of the vote counts of S2and S3. To calcu-\nlate chord label probabilities from chord label vote counts,\nwe take the fraction of the chord label vote count and the\nchord label vote counts of all possible chord labels ( D):\nP(L) =exp(VC(L))\nΣl∈Dexp(VC(l))(4)\nApplied to our example from Figure 1, we see that solv-\ning this equation for F:maj results in a probability of\nP(F:maj )≈0.39, and for D:min results in a proba-\nbility ofP(D:min )≈0.56. Instead of having to choose\nrandomly as would be necessary in a majority vote, we\ncan now see that D:min is more probable to be the cor-\nrect chord label, because it is provided by sources that are\noverall more trustworthy.\n2.3 Source Dependency\nIn the sections above we assumed that all sources are in-\ndependent. This is not always the case when we deal with\nreal-world data. Often, sources derive their data from a\ncommon origin, which means there is some kind of de-\npendency between them. For example, a source can copy\nchord labels from another source before changing some\nlabels, or some Audio Chord Estimation ( ACE) algorithm\ncan estimate multiple (almost) equal chord label sequences\nwith different parameter settings. This can create a bias in\ncomputing appropriate values. To account for the bias that\ncan arise from source dependencies, we weight the values\nof sources we suspect to have a dependency lower. In a\nsense, we award independent contributions from sources180 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016and punish values that we suspect are dependent on other\nsources.\nIn data fusion, we can detect source dependency di-\nrectly from the data by looking at the amount of shared un-\ncommon (rare) chord labels between sources. The intuition\nis that sharing a large number of uncommon chord labels is\nevidence for source dependency. With this knowledge, we\ncan compute a weight I(Si,L)for the vote count VC(L)\nof a chord labelL. This weight tells us the probability that\na sourceSiprovides a chord label Lindependently.\n2.4 Solving Catch-22: Iterative Approach\nThe chord label probabilities, source accuracy and source\ndependency are all deﬁned in terms of each other, which\nposes a problem for calculating these values. As a solution,\nwe initialize the chord label probabilities with equal prob-\nabilities and iteratively compute source dependency, chord\nlabel probabilities and source accuracy until the chord la-\nbel probabilities converge or oscillation of values is de-\ntected. The resulting chord label sequence is composed\nof the chord labels with the highest probabilities.\nFor detailed Bayesian analyses of the techniques men-\ntioned above we refer to [7,10]. With regard to the scalabil-\nity of data fusion, it has been shown that DFwith source de-\npendency runs in polynomial time [7]. Furthermore, [17]\npropose a scalability method for very large data sets, re-\nducing the time for source dependency calculation by two\nto three orders of magnitude.\n3. EXPERIMENTAL SETUP\nTo evaluate the improvement of chord label sequences us-\ning data fusion we use the output of submissions to the Mu-\nsic Information Retrieval Evaluation eXchange ( MIREX )\nAudio Chord Estimation ( ACE) task. For the task, partic-\nipants extract a sequence of chord labels from an audio\nmusic recording. The task requires the estimation chord\nlabels sequences that include the full characterization of\nchord labels (root, quality, and bass note), as well as their\nchronological order, speciﬁc onset times and durations.\nOur evaluation uses estimations from twelve submis-\nsions for two Billboard datasets (Section 3.1). Each of\nthese estimations is sampled at a regular time interval to\nmake them suitable for data fusion (Section 3.2). We\ntransform the chord labels of the sampled estimations to\ndifferent representations (root only, major/minor and ma-\njor/minor with sevenths) (Section 3.3) to evaluate the in-\ntegration of different chord types. The sampled estima-\ntions are integrated using data fusion per song. To measure\nthe quality of the data fusion integration, we calculate the\nWeighted Chord Symbol Recall ( WCSR ) (Section 3.4).\n3.1 Billboard datasets\nWe evaluate data fusion on chord label estimations for two\nsubsets of the Billboard dataset2, which was introduced\nby Burgoyne et al. in [3]. The Billboard dataset contains\ntime-aligned transcriptions of chord labels from songs that\n2available from http://ddmal.music.mcgill.ca/billboardappeared in the Billboard “Hot 100” chart in the United\nStates between 1958 and 1991. All transcriptions are anno-\ntated by trained jazz musicians and veriﬁed by independent\nmusic experts. For the MIREX 2013 ACE task, two subsets\nof the Billboard dataset were used: the 2012 Billboard set\n(BB) and the 2013 Billboard ( BB) set. BBcontains\nchord label annotations for 188 songs, corresponding to\nentries 1000—1300 in the Billboard set. BBcontains the\nannotations for 188 different songs: entries 1300—1500.\nTwelve teams participated for both datasets, some with\nmultiple submissions: CB3 & CB4 [5], CF2 [4], KO1 &\nKO2 [16], NG1 & NG2 [13], NMSD1 & NMSD2 [21],\nPP3 & PP4 [22], and SB [27]. Their submissions are used\nto evaluate data fusion, for which the Billboard annotations\nserve as a ground truth.\n3.2 Sampling\nThe MIREX ACE task requires teams to not only estimate\nwhich chord labels appear in a song, but also when they\nappear. Because of differences in approaches, timestamps\nof the estimated chord labels do not necessarily agree be-\ntween teams. This is a problem for data fusion, which ex-\npects an equal length and sampling rate of the sources that\nwill be integrated. As a solution, we sample the estima-\ntions at a regular interval.\nIn the past, MIREX used a 10 millisecond sampling ap-\nproach to calculate the quality of an estimated chord label\nsequence. Since MIREX 2013, the ground-truth and es-\ntimated chord labels are viewed as continuous segmenta-\ntions of the audio [23]. Because of our data constraint, we\nuse the pre-2013 10 millisecond sampling approach. An\ninitial evaluation using different sampling frequencies in\nthe range 0.1 millisecond to 0.5 seconds, we found only\nminor differences in data fusion output. The estimated\nchord label sequences are sampled per song from each\nteam, and used as input to the data fusion algorithm.\n3.3 Chord Types\nThe MIREX ACE task is evaluated on different chord types.\nTo accurately compare our results with those of the teams,\nand to investigate the effect of integrating different chord\ntypes, we follow the chord vocabulary mappings that were\nintroduced by [23] and are standardized in the MIREX eval-\nuation. We map the sampled sequences of estimated chord\nlabels into three chord vocabularies before applying data\nfusion: root notes only ( R), major/minor only chords ( MM),\nand major/minor with sevenths ( MM).\nNote that the MIREX 2013 evaluation also includes ma-\njor/minor with inversions and major/minor seventh chords\nwith inversions. Since there are only two teams that esti-\nmated inversions we did not take these into account in our\nevaluation.\n3.4 Evaluation\nFrom the data fusion output sequences for all songs, we\ncalculate the Weighted Chord Symbol Recall ( WCSR ). The\nWCSR reﬂects the proportion of correctly labeled chords in\na single song, weighted by the length of the song [14, 23].\nTo measure the improvement of data fusion, we compareProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 181itsWCSR with the WCSR of the best scoring team. In ad-\ndition to data fusion, we compute baseline measurements.\nWe compare the data fusion results with a majority vote\n(MV) and random picking ( RND) technique.\nFor MVwe simply take the most frequent chord la-\nbel every 10 milliseconds. In case multiple chord labels\nare most frequent, we randomly pick from the most fre-\nquent chord labels. For the example in Table 1, the output\nwould be either C:maj, F:maj, A:min, F:maj or\nC:maj, F:maj, A:min, D:min . For RND we se-\nlect a chord from a random source every 10 milliseconds.\nFor the example in Table 1, RND essentially picks one from\n44possible chord label combinations by picking a chord\nlabel from a randomly chosen source per column.\n4. RESULTS\nWe are interested in obtaining improved ,reliable chord se-\nquences from quality assessed existing estimations. There-\nfore, we analyze our results in three ways. Firstly, to mea-\nsure improvement, we show the difference in WCSR be-\ntween the best scoring team and RND,MVand DF. This\nway, we can analyze the performance increase (or de-\ncrease) for each of these integration methods. The differ-\nences are visualized in Figure 1 for the BBand BB\ndatasets. For each of the three methods, it shows the dif-\nference in WCSR for root notes Rmajor/minor only chords\nMM, and major/minor + sevenths chords ( MM). For de-\ntailed individual results an analyses of the teams on both\ndatasets, we refer to [2] and MIREX .3\nSecondly, to measure the reliability of the integrations,\nwe analyze the standard deviation of the scores of MVand\nDF. We leave RND out of this analysis because of its poor\nresults. The ideal integration should have 1) a high WCSR\nand 2) a low standard deviation, because this means that\nthe integration is 1) good and 2) reliable. Table 2 shows\nthe difference with the average standard deviation of the\nteams. Sections 4.1 - 4.2 report the results in WCSR differ-\nence and standard deviation.\nThirdly, in Section 4.3 we analyze the correlation be-\ntween source accuracy and WCSR , and compare the corre-\nlation with other source quality assessments. These corre-\nlations will tell us to which extent DFis capable of assess-\ning the quality of sources compared to other, widely used\nmultiple sequence analysis methods.\n3http://www.music-ir.org/mirex/wiki/2013:MIREX2013 Results\nR\nMM\nMM7\u000012\u000011\u000010\u00009\u00008\u00007\u00006\u00005\u00004\u00003\u00002\u000010123456WCSR difference with best teamBB12\nR\nMM\nMM7BB13\nRND\nMV\nDF\nFigure 1 : Difference in WCSR with best team for random picking\n(RND), majority vote ( MV) and data fusion ( DF).R= root notes,\nMM= major/minor chords and MM= major/minor + sevenths.BB BB\nR MM MM  R MM MM \nDF -2.5 -2.8 -2.2 -0.5 -0.9 -1.8\nMV -1.4 -1.8 -0.97 -0.3 -0.4 -1\nTable 2 : Difference in standard deviation for DFand MVcom-\npared to the average standard deviation of the teams. Lower is\nbetter, best values are bold.\n4.1 Results of Integrating R,MMand MM\nThe left hand sides of the triple-bar groups in Figure 1\nshow that for both BBand BB,RND performs the\nworst among RND,MVand DF.RND decreases the WCSR\nbetween 8.7% and 12% point, compared to the best per-\nforming teams (CB3 and KO1 for BBand BBrespec-\ntively) for all chord types. This means that picking ran-\ndom values from sources does not capture shared knowl-\nedge in a meaningful way. The middle bars in Figure 1\nshow that MVintegrates knowledge better than RND.MV\nmoderately improves the best algorithm with a difference\nbetween 0.6% and 2.1% point.\nThe right hand sides of the bar groups in Figure 1 show\nthat in both datasets and in all chord types, DFoutperforms\nall other methods with an increase between 3.6% point and\n5.4% point compared to the best team. We tested the scores\nofRND,MVand DFand the best performing teams using\na Friedman test for repeated measurements, accompanied\nby Tukeys Honest Signiﬁcant Difference tests for each pair\nof algorithms. We ﬁnd that DFsigniﬁcantly outperforms\nthe best submission, RND and MVon all datasets on all\ndatasets (p<0.01). These results combined show that DF\nis capable of capturing knowledge shared among sources\nneeded to outperform all other methods.\nIn Table 2, we ﬁnd that for both BBand BB, both\nMVand DFdecrease the standard deviation compared to\nthe average standard deviation of the teams. In fact, we ﬁnd\nthat DFoutperforms MV, improving the standard deviation\nby a factor two compared to MV. Together, these results\nmean that on average, DFcreates the best sequences with\nthe least errors for all datasets and all chord types.\n4.2 Inﬂuence of Chord Types on Integration\nThe results detailed above show that DFis not only capa-\nble to signiﬁcantly outperform all other tested methods on\nall tested chord labels types, but also produces the most\nreliable output, because of the low standard deviation.\nComparing the RND,MVand DFresults between chord\ntypes in Figure 1, we see that the WCSR ofRND decreases\nwith a larger chord vocabulary. Because speciﬁcity in-\ncreases the probability of random errors for any algorithm,\nthe probability that RND will pick a good chord label ran-\ndomly goes down with an increase of the chord vocabulary.\nFor MV, we see that the results are somewhat stable with\nan increase of the chord vocabulary. Nevertheless, MVis\nalso sensitive for randomly matching chord labels, which\nexplains the drop in accuracy for MMfor BBon the\nleft hand side of Figure 1. Most interestingly, we observe\nthat the performance of DFincreases with a larger chord\nvocabulary. The explanation is that speciﬁcity helps DF\nto separate good sources from bad sources. With a larger\nchord vocabulary, sources will agree with each other on182 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20160:0 0:2 0:4 0:6 0:8\nWCSR0:00:20:40:60:81:0DFSource Accuracy\nBB12\n0:0 0:2 0:4 0:6 0:8 1:0\nWCSR\nBB13Figure 2 : Correlation between WCSR and source accuracy. Plot-\nted are R,MMand MM. One dot is one estimated chord label\nsequence for one song from one team.\nmore speciﬁc chord labels, which decreases the probabil-\nity of unwanted random source agreement.\n4.3 Source Quality Assessment\nThe previous sections show that data fusion is capable of\nselecting good chord labels from the coherence between\nthe sources, without ground truth knowledge. A pivotal\npart of data fusion is the computation of source accuracy,\nwhich provides a relative score for each source compared\nto the other sources. There are circumstances in which we\nare more interested in the estimation of source accuracy\nthan the actual integration of source data. For example,\nranking a number of different crowd sourced chord label\nsequences of the same song obtained from web sources,\n(e.g. investigated by [18]). Investigating the relationship\nbetween source accuracy and the WCSR provides insight\nwhether data fusion is capable of assessing the accuracy of\nthe sources in a way that reﬂects WCSR .WCSR reﬂects the\nquality of the chord sequences and therefore the quality of\nthe algorithm. This relationship is shown in Figure 2, in\nwhich the WCSR is plotted against the DFsource accuracy.\nInitial observation of Figure 2 shows that for both\nBBand BB,WCSR and source accuracy are distributed\nalong a more or less diagonal line, meaning that a higher\nWCSR is associated with a higher DFsource accuracy, and\nvice versa. This indicates a strong correlation, which is\nconﬁrmed by the Spearman’s rank correlation coefﬁcient\n(SRCC ). To analyze the relative performance of source\nquality assessment of DF, we compare its correlation with\nwidely used sequence scoring methods. These are often\nused in bioinformatics, where sequence ranking is at the\nroot of a multitude of problems. Table 3 compares the\nSRCC of different similarity scoring methods for BBand\nBB. The table shows the correlations between WCSR\nand DF, bigrams ( BIGRAM ), proﬁle hidden Markov mod-\nels ( PHMM ), percentage identity ( PID), and neigbor-joining\ntrees ( NJT).BIGRAM compares the relative balance of spe-\nciﬁc character pairs appearing in succession, also known\nas bigrams. Sequences belonging to the same group should\nbe stochastic products of the same probabilistic model [6].\nPHMM turns the sources into a position-speciﬁc scoring\nsystem by creating a proﬁle with position-probabilities. A\nsource is scored through comparison with the proﬁle of all\nother sources [11]. PIDis the fraction of equal characters\ndivided by the length of the source. NJTis a bottom-up\nclustering method for the creation of phylogenetic trees, in\nwhich the distance from the root is the score [24].BB BB\nR MM MM  R MM MM \nDF 0.87 0.85 0.82 0.77 0.77 0.76\nBIGRAM 0.18 0.18 0.16 0.2 0.22 0.29\nPHMM40.22 — — 0.22 — —\nPID 0.18 0.2 0.19 0.25 0.27 0.29\nNJT 0.2 0.22 0.21 0.24 0.25 0.27\nTable 3 : Spearman’s rank correlation coefﬁcient ( ρ) of WCSR\nand other source scoring methods. Best performing algorithms\nare bold. All values are signiﬁcant with p <0.01.\nThe table shows that DFsource accuracy has the highest\ncorrelation with WCSR among all other methods. These\nresults show that data fusion is capable of assessing the\nquality of the sources without any ground-truth knowledge\nin a way that is closely related to the actual source quality.\n5. DISCUSSION AND CONCLUSION\nThrough this study, we have shown for the ﬁrst time that\nusing data fusion, we can integrate the knowledge con-\ntained in heterogeneous ACE output to create improved,\nand more reliable chord label sequences. Data fusion inte-\ngration outperforms all individual ACE algorithms, as well\nas majority voting and random picking of source values.\nFurthermore, we have shown that with data fusion, one can\nnot only generate high quality integrations, but also accu-\nrately estimate the quality of sources from their coherence,\nwithout any ground truth knowledge. Source accuracy out-\nperforms other popular sequence ranking methods.\nOur ﬁndings demonstrate that knowledge from multiple\nsources can be integrated effectively, efﬁciently and in an\nintuitive way. Because the proposed method is agnostic\nto the domain of the data, it could be applied to melodies\nor other musical sequences as well. We believe that fur-\nther analysis of data fusion in crowd-sourced data has the\npotential to provide non-trivial insights into musical varia-\ntion, ambiguity and perception. We believe that data fusion\nhas many important applications in music information re-\ntrieval research and in the music industry for problems re-\nlating to managing large amounts of crowd-sourced data.\nAcknowledgements We thank anonymous reviewers for provid-\ning valuable comments on an earlier draft on this text. H.V . Koops\nand A. V olk are supported by the Netherlands Organization for\nScientiﬁc Research, through the NWO -VIDI-grant--to\nA. V olk. D. Bountouridis is supported by the FESproject COM -\nMIT/.\n6. REFERENCES\n[1] E.P. Bugge, K.L. Juncher, B.S. Mathiesen, and J.G. Si-\nmonsen. Using sequence alignment and voting to im-\nprove optical music recognition from multiple recog-\nnizers. In Proc. of the International Society for Mu-\nsic Information Retrieval Conference , pages 405–410,\n2011.\n[2] J.A. Burgoyne, W.B. de Haas, and J. Pauwels. On com-\nparative statistics for labelling tasks: What can we\nlearn from MIREX ACE 2013. In Proc. of the 15th\n4The MMand MMchord label alphabets are too large for the used\nPHMM application, which only accepts a smaller bioinformatics alphabet.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 183Conference of the International Society for Music In-\nformation Retrieval , pages 525–530, 2014.\n[3] J.A. Burgoyne, J. Wild, and I. Fujinaga. An expert\nground truth set for audio chord recognition and mu-\nsic analysis. In Proc. of the International Society for\nMusic Information Retrieval Conference , volume 11,\npages 633–638, 2011.\n[4] C. Cannam, M. Mauch, M.E.P. Davies, S. Dixon,\nC. Landone, K. Noland, M. Levy, M. Zanoni, D. Stow-\nell, and L.A. Figueira. MIREX 2013 entry: Vamp plu-\ngins from the centre for digital music, 2013.\n[5] T. Cho and J.P. Bello. MIREX 2013: Large vocab-\nulary chord recognition system using multi-band fea-\ntures and a multi-stream hmm. Music Information Re-\ntrieval Evaluation eXchange (MIREX) , 2013.\n[6] M.J. Collins. A new statistical parser based on bi-\ngram lexical dependencies. In Proc. of the 34th annual\nmeeting on Association for Computational Linguistics ,\npages 184–191. Association for Computational Lin-\nguistics, 1996.\n[7] X.L. Dong, L. Berti-Equille, and D. Srivastava. Inte-\ngrating conﬂicting data: the role of source dependence.\nProc. of the VLDB Endowment , 2(1):550–561, 2009.\n[8] X.L. Dong and F. Naumann. Data fusion: resolving\ndata conﬂicts for integration. Proc. of the VLDB En-\ndowment , 2(2):1654–1655, 2009.\n[9] X.L. Dong and D. Srivastava. Big data integration. In\nData Engineering (ICDE), 2013 IEEE 29th Interna-\ntional Conference on , pages 1245–1248. IEEE, 2013.\n[10] X.L. Dong and D. Srivastava. Big data integration.\nSynthesis Lectures on Data Management , 7(1):1–198,\n2015.\n[11] S.R. Eddy. Proﬁle hidden markov models. Bioinfor-\nmatics , 14(9):755–763, 1998.\n[12] R. Foucard, S. Essid, M. Lagrange, G. Richard, et al.\nMulti-scale temporal fusion by boosting for music clas-\nsiﬁcation. In Proc. of the International Society for Mu-\nsic Information Retrieval Conference , pages 663–668,\n2011.\n[13] N. Glazyrin. Audio chord estimation using chroma re-\nduced spectrogram and self-similarity. Music Informa-\ntion Retrieval Evaluation Exchange (MIREX) , 2012.\n[14] C. Harte. Towards automatic extraction of harmony in-\nformation from music signals . PhD thesis, Department\nof Electronic Engineering, Queen Mary, University of\nLondon, 2010.\n[15] A. Holzapfel, M.E.P. Davies, J.R. Zapata, J.L. Oliveira,\nand F. Gouyon. Selective sampling for beat tracking\nevaluation. Audio, Speech, and Language Processing,\nIEEE Transactions on , 20(9):2539–2548, 2012.[16] M. Khadkevich and M. Omologo. Time-frequency re-\nassigned features for automatic chord recognition. In\nAcoustics, Speech and Signal Processing (ICASSP),\n2011 IEEE International Conference on , pages 181–\n184. IEEE, 2011.\n[17] X. Li, X.L. Dong, K.B. Lyons, W. Meng, and D. Sri-\nvastava. Scaling up copy detection. arXiv preprint\narXiv:1503.00309 , 2015.\n[18] R. Macrae and S. Dixon. Guitar tab mining, analysis\nand ranking. In Proc. of the International Society for\nMusic Information Retrieval Conference , pages 453–\n458, 2011.\n[19] A. Meng, P. Ahrendt, and J. Larsen. Improving mu-\nsic genre classiﬁcation by short time feature integra-\ntion. In Acoustics, Speech, and Signal Processing,\n2005. Proc..(ICASSP’05). IEEE International Confer-\nence on , volume 5, pages v–497. IEEE, 2005.\n[20] A. Meng, J. Larsen, and L.K. Hansen. Temporal feature\nintegration for music organisation . PhD thesis, Techni-\ncal University of Denmark, Department of Informatics\nand Mathematical Modeling, 2006.\n[21] Y . Ni, M. Mcvicar, R. Santos-Rodriguez, and\nT. De Bie. Harmony progression analyzer for MIREX\n2013. Music Information Retrieval Evaluation eX-\nchange (MIREX) .\n[22] J. Pauwels, J-P. Martens, and G. Peeters. The ircamk-\neychord submission for MIREX 2012.\n[23] J. Pauwels and G. Peeters. Evaluating automatically\nestimated chord sequences. In Acoustics, Speech and\nSignal Processing (ICASSP), 2013 IEEE International\nConference on , pages 749–753. IEEE, 2013.\n[24] N. Saitou and M. Nei. The neighbor-joining method:\na new method for reconstructing phylogenetic trees.\nMolecular biology and evolution , 4(4):406–425, 1987.\n[25] N.T. Siebel and S. Maybank. Fusion of multiple track-\ning algorithms for robust people tracking. In Computer\nVisionECCV 2002 , pages 373–387. Springer, 2002.\n[26] J.B.L Smith, J.A. Burgoyne, I. Fujinaga, D. De Roure,\nand J.S. Downie. Design and creation of a large-scale\ndatabase of structural annotations. In Proc. of the Inter-\nnational Society for Music Information Retrieval Con-\nference , volume 11, pages 555–560, 2011.\n[27] Nikolaas Steenbergen and John Ashley Burgoyne.\nJoint optimization of an hidden markov model-neural\nnetwork hybrid for chord estimation. MIREX-Music\nInformation Retrieval Evaluation eXchange. Curitiba,\nBrasil , pages 189–190, 2013.\n[28] C. Sutton, E. Vincent, M. Plumbley, and J. Bello. Tran-\nscription of vocal melodies using voice characteristics\nand algorithm fusion. In 2006 Music Information Re-\ntrieval Evaluation eXchange (MIREX) , 2006.184 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Feature Learning for Chord Recognition: The Deep Chroma Extractor.",
        "author": [
            "Filip Korzeniowski",
            "Gerhard Widmer"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416314",
        "url": "https://doi.org/10.5281/zenodo.1416314",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/178_Paper.pdf",
        "abstract": "We explore frame-level audio feature learning for chord recognition using artificial neural networks. We present the argument that chroma vectors potentially hold enough information to model harmonic content of audio for chord recognition, but that standard chroma extractors compute too noisy features. This leads us to propose a learned chroma feature extractor based on artificial neural net- works. It is trained to compute chroma features that en- code harmonic information important for chord recogni- tion, while being robust to irrelevant interferences. We achieve this by feeding the network an audio spectrum with context instead of a single frame as input. This way, the network can learn to selectively compensate noise and re- solve harmonic ambiguities. We compare the resulting features to hand-crafted ones by using a simple linear frame-wise classifier for chord recognition on various data sets. The results show that the learned feature extractor produces superior chroma vectors for chord recognition.",
        "zenodo_id": 1416314,
        "dblp_key": "conf/ismir/KorzeniowskiW16",
        "content": "FEATURE LEARNING FOR CHORD RECOGNITION:\nTHE DEEP CHROMA EXTRACTOR\nFilip Korzeniowski and Gerhard Widmer\nDepartment of Computational Perception, Johannes Kepler University Linz, Austria\nfilip.korzeniowski@jku.at\nABSTRACT\nWe explore frame-level audio feature learning for chord\nrecognition using artiﬁcial neural networks. We present\nthe argument that chroma vectors potentially hold enough\ninformation to model harmonic content of audio for chord\nrecognition, but that standard chroma extractors compute\ntoo noisy features. This leads us to propose a learned\nchroma feature extractor based on artiﬁcial neural net-\nworks. It is trained to compute chroma features that en-\ncode harmonic information important for chord recogni-\ntion, while being robust to irrelevant interferences. We\nachieve this by feeding the network an audio spectrum with\ncontext instead of a single frame as input. This way, the\nnetwork can learn to selectively compensate noise and re-\nsolve harmonic ambiguities.\nWe compare the resulting features to hand-crafted ones\nby using a simple linear frame-wise classiﬁer for chord\nrecognition on various data sets. The results show that the\nlearned feature extractor produces superior chroma vectors\nfor chord recognition.\n1. INTRODUCTION\nChord Recognition (CR) has been an active research ﬁeld\nsince its inception by Fujishima in 1999 [10]. Since\nthen, researchers have explored many aspects of this ﬁeld,\nand developed various systems to automatically extract\nchords from audio recordings of music (see [20] for a re-\ncent review). Chord recognition meets this great interest\nin the MIR (music information research) community be-\ncause harmonic content is a descriptive mid-level feature\nof (Western) music that can be used directly (e.g. for creat-\ning lead sheets for musicians) and as basis for higher-level\ntasks such as cover song identiﬁcation, key detection or\nharmonic analysis.\nMost chord recognition systems follow a common\npipeline of feature extraction, pattern matching, and chord\nsequence decoding (also called post-ﬁltering ) [7]. In this\npaper, we focus on the ﬁrst step in this pipeline: feature\nextraction.\nc/circlecopyrtFilip Korzeniowski and Gerhard Widmer. Licensed un-\nder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Filip Korzeniowski and Gerhard Widmer. “Feature\nLearning for Chord Recognition:\nThe Deep Chroma Extractor”, 17th International Society for Music Infor-\nmation Retrieval Conference, 2016.Two observations lead us to explore better features for\nchord recognition: (1) The capabilities of chord models for\npattern matching are limited . In [7], Cho and Bello con-\nclude that appropriate features largely redeem the beneﬁts\nof complex chord models. (2) The capabilities of post-ﬁl-\ntering are limited . As shown in [6, 7], post-ﬁltering meth-\nods are useful because they enforce continuity of individ-\nual chords rather than providing information about chord\ntransitions. Incorporating such information did not consid-\nerably improve recognition results in both studies. Chen\net al. [6] also observed quantitatively that in popular mu-\nsic “chord progressions are less predictable than it seems”,\nand thus knowing chord history does not greatly narrow the\npossibilities for the next chord. Given these apparent limi-\ntations of the pattern matching and post-ﬁltering stages, it\nis not surprising that they only partly compensate the per-\nformance gap between features [7]. We therefore have to\ncompute better features if we want to improve chord recog-\nnition.\nIn this paper, we take a step towards better features for\nchord recognition by introducing a data-driven approach\nto extract chromagrams that speciﬁcally encode content\nrelevant to harmony. Our method learns to discard irrel-\nevant information like percussive noise, overtones or tim-\nbral variations automatically from data. We argue that it\nis thus able to compensate a broader range of interferences\nthan hand-crafted approaches.\n2. CHROMAGRAMS\nThe most popular feature used for chord recognition is the\nChromagram . A chromagram comprises a time-series of\nchroma vectors, which represent harmonic content at a spe-\nciﬁc time in the audio as c∈R12. Eachcistands for a pitch\nclass, and its value indicates the current saliency of the cor-\nresponding pitch class. Chroma vectors are computed by\napplying a ﬁlter bank to a time-frequency representation of\nthe audio. This representation results from either a short-\ntime Fourier transform (STFT) or a constant-q transform\n(CQT), the latter being more popular due to a ﬁner fre-\nquency resolution in the lower frequency area.\nChromagrams are concise descriptors of harmony be-\ncause they encode tone quality and neglect tone height.\nIn theory, this limits their representational power: with-\nout octave information, one cannot distinguish e.g. chords\nthat comprise the same pitch classes, but have a different\nbass note (like G vs. G/5, or A:sus2 vs. E:sus4). In prac-37tice, we can show that given chromagrams derived from\nground truth annotations, using logistic regression we can\nrecognise 97% of chords (reduced to major/minor) in the\nBeatles dataset. This result encourages us to create chroma\nfeatures that contain harmony information, but are robust\nto spectral content that is harmonically irrelevant.\nChroma features are noisy in their basic formulation be-\ncause they are affected by various interferences: musical\ninstruments produce overtones in addition to the funda-\nmental frequency; percussive instruments pollute the spec-\ntrogram with broadband frequency activations (e.g. snare\ndrums) and/or pitch-like sounds (tom-toms, bass drums);\ndifferent combinations of instruments (and different, pos-\nsibly genre-dependent mixing techniques) create different\ntimbres and thus increase variance [7, 20].\nResearchers have developed and used an array of meth-\nods that mitigate these problems and extract cleaner chro-\nmagrams: Harmonic-percussive source separation can ﬁl-\nter out broadband frequency responses of percussive in-\nstruments [22, 27], various methods tackle interferences\ncaused by overtones [7, 19], while [21, 27] attempt to\nextract chromas robust to timbre. See [7] for a recent\noverview and evaluation of different methods for chroma\nextraction. Although these approaches improve the qual-\nity of extracted chromas, it is very difﬁcult to hand-craft\nmethods for all conceivable disturbances, even if we could\nname and quantify them.\nThe approaches mentioned above share a common limi-\ntation: they mostly operate on single feature frames. Single\nframes are often not enough to decide which frequencies\nsalient in the spectrum are relevant to harmony and which\nare noise. This is usually countered by contextual aggrega-\ntion such as moving mean/median ﬁlters or beat synchro-\nnisation, which are supposed to smooth out noisy frames.\nSince they operate only after computing the chromas, they\naddress the symptoms (noisy frames) but do not tackle the\ncause (spectral content irrelevant to harmony). Also, [7]\nfound that they blur chord boundaries and details in a sig-\nnal and can impair results when combined with more com-\nplex chord models and post-ﬁltering methods.\nIt is close to impossible to ﬁnd the rules or formulas\nthat deﬁne harmonic relevance of spectral content manu-\nally. We thus resort to the data-driven approach of deep\nlearning. Deep learning was found to extract strong, hierar-\nchical, discriminative features [1] in many domains. Deep\nlearning based systems established new state-of-the-art re-\nsults in computer vision1, speech recognition, and MIR\ntasks such as beat detection [3], tempo estimation [4] or\nstructural segmentation [28].\nIn this paper, we want to exploit the power of deep\nneural networks to compute harmonically relevant chroma\nfeatures. The proposed chroma extractor learns to ﬁlter\nharmonically irrelevant spectral content from a context of\naudio frames . This way, we circumvent the necessity to\ntemporally smooth the features by allowing the chroma ex-\ntractor to use context information directly. Our method\n1See https://rodrigob.github.io/are wethere yet/build/classiﬁcation\ndatasets results.html for results on computer vision.computes cleaner chromagrams while retaining their ad-\nvantages of low dimensionality and intuitive interpretation.\n3. RELATED WORK\nA number of works used neural networks in the context\nof chord recognition. Humphrey and Bello [14] applied\nConvolutional Neural Networks to classify major and mi-\nnor chords end-to-end. Boulanger-Lewandowski et al. [5],\nand Sigtia et al. [24] explored Recurrent Neural Networks\nas a post-ﬁltering method, where the former used a deep\nbelief net, the latter a deep neural network as underlying\nfeature extractor. All these approaches train their models\nto directly predict major and minor chords, and follow-\ning [1], the hidden layers of these models learn a hier-\narchical, discriminative feature representation. However,\nsince the models are trained to distinguish major/minor\nchords only, they consider other chord types (such as sev-\nenth, augmented, or suspended) mapped to major/minor as\nintra-class variation to be robust against, which will be re-\nﬂected by the extracted internal features. These features\nmight thus not be useful to recognise other chords.\nWe circumvent this by using chroma templates derived\nfrom chords as distributed (albeit incomplete) representa-\ntion of chords. Instead of directly classifying a chord label,\nthe network is required to compute the chroma representa-\ntionof a chord given the corresponding spectrogram. We\nexpect the network to learn which saliency in the spectro-\ngram is responsible for a certain pitch class to be harmon-\nically important, and compute higher values for the corre-\nsponding elements of the output chroma vector.\nApproaches to directly learn a mapping from spectro-\ngram to chroma include those by ˙Izmirli and Dannen-\nberg [29] and Chen et al. [6]. However, both learn only\na linear transformation of the time-frequency representa-\ntion, which limits the mapping’s expressivity. Addition-\nally, both base their mapping on a single frame, which\ncomes with the disadvantages we outlined in the previous\nsection.\nIn an alternative approach, Humphrey et al. apply deep\nlearning methods to produce Tonnetz features from a spec-\ntrogram [15]. Using other features than the chromagram\nis a promising direction, and was also explored in [6] for\nbass notes. Most chord recognition systems however still\nuse chromas, and more research is necessary to explore to\nwhich degree and under which circumstances Tonnetz fea-\ntures are favourable.\n4. METHOD\nTo construct a robust chroma feature extractor, we use a\ndeep neural network (DNN). DNNs consist of Lhidden\nlayershlofUlcomputing units. These units compute val-\nues based on the results of the previous layer, such that\nhl(x) =σl(Wl·hl−1(x) +bl), (1)\nwherexis the input to the net, Wl∈RUl×Ul−1and\nbl∈RUlare the weights and the bias of the lthlayer re-38 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016spectively, and σlis a (usually non-linear) activation func-\ntionapplied point-wise.\nWe deﬁne two additional special layers: an input layer\nthat is feeding values to h1ash0(x) =x, withU0being\nthe input’s dimensionality; and an output layer hL+1that\ntakes the same form as shown in Eq. 1, but has a speciﬁc\nsemantic purpose: it represents the output of the network,\nand thus its dimensionality UL+1and activation function\nσL+1have to be set accordingly.2\nThe weights and biases constitute the model’s parame-\nters. They are trained in a supervised manner by gradient\nmethods and error back-propagation in order to minimise\nthe loss of the network’s output. The loss function de-\npends on the domain, but is generally some measure of dif-\nference between the current output and the desired output\n(e.g. mean squared error, categorical cross-entropy, etc.)\nIn the following, we describe how we compute the input\nto the DNN, the concrete DNN architecture and how it was\ntrained.\n4.1 Input Processing\nWe compute the time-frequency representation of the sig-\nnal based on the magnitude of its STFT X. The STFT\ngives signiﬁcantly worse results than the constant-q trans-\nform if used as basis for traditional chroma extractors, but\nwe found in preliminary experiments that our model is not\nsensitive to this phenomenon. We use a frame size of 8192\nwith a hop size of 4410 at a sample rate of 44100 Hz. Then,\nwe apply triangular ﬁlters to convert the linear frequency\nscale of the magnitude spectrogram to a logarithmic one in\nwhat we call the quarter-tone spectrogram S=F/triangle\nLog·|X|,\nwhereF/triangle\nLogis the ﬁlter bank. The quarter-tone spectro-\ngram contains only bins corresponding to frequencies be-\ntween 30 Hz and 5500 Hz and has 24 bins per octave. This\nresults in a dimensionality of 178 bins. Finally, we apply\na logarithmic compression such that Slog= log (1 +S),\nwhich we will call the logarithmic quarter-tone spectro-\ngram . To be concise, we will refer to SLogas “spectro-\ngram” in rest of this paper.\nOur model uses a context window around a target frame\nas input. Through systematic experiments on the validation\nfolds (see Sec.5.1) we found a context window of ±0.7s to\nwork best. Since we operate at 10 fps, we feed our network\nat each time 15 consecutive frames, which we will denote\nassuper-frame .\n4.2 Model\nWe deﬁne the model architecture and set the model’s\nhyper-parameters based on validation performance in sev-\neral preliminary experiments. Although a more systematic\napproach might reveal better conﬁgurations, we found that\nresults do not vary by much once we reach a certain model\ncomplexity.\n2For example, for a 3-class classiﬁcation problem one would use 3\nunits in the output layer and a softmax activation function such that the\nnetwork’s output can be interpreted as probability distribution of classes\ngiven the data.\nFigure 1 . Model overview. At each time 15 consecutive\nframes of the input quarter-tone spectrogram SLogare fed\nto a series of 3 dense layers of 512 rectiﬁer units, and ﬁ-\nnally to a sigmoid output layer of 12 units (one per pitch\nclass), which represents the chroma vector for the centre\ninput frame.\nOur model is a deep neural network with 3 hidden layers\nof 512 rectiﬁer units [11] each. Thus, σl(x) = max(0,x)\nfor1≤l≤L. The output layer, representing the chroma\nvector, consists of 12 units (one unit per pitch class) with a\nsigmoid activation function σL+1(x) = 1/1+exp(−x). The\ninput layer represents the input super-frame and thus has a\ndimensionality of 2670. Fig. 1 shows an overview of our\nmodel.\n4.3 Training\nTo train the network, we propagate back through the net-\nwork the gradient of the loss Lwith relation to the net-\nwork parameters. Our loss is the binary cross-entropy\nbetween each pitch class in the predicted chroma vector\np=hL+1(Slog)and the target chroma vector t, which is\nderived from the ground truth chord label. For a single data\ninstance,\nL=1\n1212/summationdisplay\ni=1−tilog(pi)−(1−ti) log(1−pi).(2)\nWe learn the parameters with mini-batch training (batch\nsize 512) using the ADAM update rule [16]. We also tried\nsimple stochastic gradient descent with Nesterov momen-\ntum and a number of manual learn rate schedules, but could\nnot achieve better results (to the contrary, using ADAM\ntraining usually converged earlier). To prevent over-ﬁtting,\nwe apply dropout [26] with probability 0.5 after each hid-\nden layer and early stopping if validation accuracy does\nnot increase after 20 epochs.\n5. EXPERIMENTS\nTo evaluate the chroma features our method produces, we\nset up a simple chord recognition task. We ignore any post-\nﬁltering methods and use a simple, linear classiﬁer (logis-\ntic regression) to match features to chords. This way we\nwant to isolate the effect of the feature on recognition ac-\ncuracy. As it is common, we restrict ourselves to distinct\nonly major/minor chords, resulting in 24 chord classes and\na ’no chord’ class.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 390.1 0.7 1.1 1.5 2.1 2.7 3.1\nAudio Context [s]0.500.550.600.650.700.750.80Maj/Min WCSR\nC\nCW\nLog\nSLog\nCDFigure 2 . Validation WCSR for Major/minor chord recog-\nnition of different methods given different audio context\nsizes. Whiskers represent 0.95 conﬁdence intervals.\nOur compound evaluation dataset comprises the Beat-\nles [13], Queen and Zweieck [18] datasets (which form the\n“Isophonics” dataset used in the MIREX3competition),\nthe RWC pop dataset4[12], and the Robbie Williams\ndataset [8]. The datasets total 383 songs or approx. 21\nhours and 39 minutes of music.\nWe perform 8-fold cross validation with random splits.\nFor the Beatles dataset, we ensure that each fold has the\nsame album distribution. For each test fold, we use six of\nthe remaining folds for training and one for validation.\nAs evaluation measure, we compute the Weighted\nChord Symbol Recall (WCSR), often called Weighted Av-\nerage Overlap Ratio (WAOR) of major and minor chords\nusing the mir eval library [23].\n5.1 Compared Features\nWe evaluate our extracted features CDagainst three\nbaselines: a standard chromagram Ccomputed from\na constant-q transform, a chromagram with frequency\nweighting and logarithmic compression of the underlying\nconstant-q transform CW\nLog, and the quarter-tone spectro-\ngramSLog. The chromagrams are computed using the li-\nbrosa library5. Their parametrisation follows closely the\nsuggestions in [7], where CW\nLogwas found to be the best\nchroma feature for chord recognition.\nEach baseline can take advantage of context informa-\ntion. Instead of computing a running mean or median,\nwe allow logistic regression to consider multiple frames of\neach feature6. This is a more general way to incorporate\ncontext, because running mean is a subset of the context\naggregation functions possible in our setup. Since training\nlogistic regression is a convex problem, the result is at least\nas good as if we used a running mean.\n3http://www.music-ir.org/mirex\n4Chord annotations available at https://github.com/tmc323/\nChord-Annotations\n5https://github.com/bmcfee/librosa\n6Note that this description applies only to the baseline methods. For\nour DNN feature extractor, “context” means the amount of context the\nDNN sees. The logistic regression always sees only one frame of the\nfeature the DNN computed.Btls Iso RWC RW Total\nC 71.0±0.1 69.5±0.1 67.4±0.2 71.1±0.1 69.2±0.1\nCW\nLog 76.0±0.1 74.2±0.1 70.3±0.3 74.4±0.2 73.0±0.1\nSLog 78.0±0.2 76.5±0.2 74.4±0.4 77.8±0.4 76.1±0.2\nCD80.2±0.1 79.3±0.1 77.3±0.1 80.1±0.1 78.8±0.1\nTable 1 . Cross-validated WCSR on the Maj/min task of\ncompared methods on various datasets. Best results are\nbold-faced ( p < 10−9). Small numbers indicate stan-\ndard deviation over 10 experiments. “Btls” stands for the\nBeatles, “Iso” for Isophonics, and “RW” for the Robbie\nWilliams datasets. Note that the Isophonics dataset com-\nprises the Beatles, Queen and Zweieck datasets.\nWe determined the optimal amount of context for\neach baseline experimentally using the validation folds, as\nshown in Fig. 2. The best results achieved were 79.0% with\n1.5 s context for CD, 76.8% with 1.1 s context for SLog,\n73.3% with 3.1 s context for CW\nLog, and 69.5% with 2.7 s\ncontext forC. We ﬁx these context lengths for testing.\n6. RESULTS\nTable 1 presents the results of our method compared to the\nbaselines on several datasets. The chroma features Cand\nCW\nLogachieve results comparable to those [7] reported on\na slightly different compound dataset. Our proposed fea-\nture extractor CDclearly performs best, with p < 10−9\naccording to a paired t-test. The results indicate that the\nchroma vectors extracted by the proposed method are bet-\nter suited for chord recognition than those computed by the\nbaselines.\nTo our surprise, the raw quarter-tone spectrogram SLog\nperformed better than the chroma features. This indicates\nthat computing chroma vectors in the traditional way mixes\nharmonically relevant features found in the time-frequency\nrepresentation with irrelevant ones, and the ﬁnal classiﬁer\ncannot disentangle them. This raises the question of why\nchroma features are preferred to spectrograms in the ﬁrst\nplace. We speculate that the main reason is their much\nlower dimensionality and thus ease of modelling (e.g. us-\ning Gaussian mixtures).\nArtiﬁcial neural networks often give good results, but\nit is difﬁcult to understand what they learned, or on which\nbasis they generate their output. In the following, we will\ntry to dissect the proposed model, understand its workings,\nand see what it pays attention to. To this end, we com-\npute saliency maps using guided back-propagation [25],\nadapting code freely available7for the Lasagne library [9].\nLeaving out the technical details, a saliency map can be in-\nterpreted as an attention map of the same size as the input.\nThe higher the absolute saliency at a speciﬁc input dimen-\nsion, the stronger its inﬂuence on the output, where pos-\nitive values indicate a direct relationship, negative values\nan indirect one.\nFig. 3 shows a saliency map and its corresponding\nsuper-frame, representing a C major chord. As expected,\n7https://github.com/Lasagne/Recipes/40 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016-0.7 -0.3 0+0.3 +0.7\nTime [s]CD   EF   GAB   CD   EF   GAB   CD   EF   GAB   CD   EF   GAB   CD   EF   GAB   CD   EF   GAB   CD   EF   GAB   CD   ESpectrogram\n-0.7 -0.3 0+0.3 +0.7\nTime [s]CD   EF   GAB   CD   EF   GAB   CD   EF   GAB   CD   EF   GAB   CD   EF   GAB   CD   EF   GAB   CD   EF   GAB   CD   ESaliency\n- 0 +CD   EF   GAB   CD   EF   GAB   CD   EF   GAB   CD   EF   GAB   CD   EF   GAB   CD   EF   GAB   CD   EF   GAB   CD   E/summationdisplay\nframes SaliencyFigure 3 . Input example (C major chord) with correspond-\ning saliency map. The left image shows the spectrogram\nframes fed into the network. The centre image shows the\ncorresponding saliency map, where red pixels represent\npositive, blue pixels negative values. The stronger the sat-\nuration, the higher the absolute value. The right plot shows\nthe saliency summed over the time axis, and thus how each\nfrequency bin inﬂuences the output. Note the strong posi-\ntive inﬂuences of frequency bins corresponding to c,e, and\ngnotes that form a C major chord.\nthe saliency map shows that the most relevant parts of the\ninput are close to the target frame and in the mid frequen-\ncies. Here, frequency bins corresponding to notes con-\ntained in a C major chord (c, e, and g) showing posi-\ntive saliency peeks, with the third, e, standing out as the\nstrongest. Conversely, its neighbouring semitone, f, ex-\nhibits strong negative saliency values. Fig. 4 depicts av-\nerage saliencies for two chords computed over the whole\nBeatles corpus.\nFig. 5 shows the average saliency map over all super-\nframes of the Beatles dataset summed over the frequency\naxis. It thus shows the magnitude with which individ-\nual frames in the super-frame contribute to the output of\nthe neural network. We observe that most information is\ndrawn from a±0.3s window around the centre frame. This\nis in line with the results shown in Fig. 2, where the pro-\nposed method already performed well with 0.7 s of audio\ncontext.\nFig. 6 shows the average saliency map over all super-\nframes of the Beatles dataset, and its sum over the time\naxis. We observe that frequency bins below 110 Hz and\nabove 3136 Hz (wide limits) are almost irrelevant, and that\nthe net focuses mostly on the frequency range between\n196 Hz and 1319 Hz (narrow limits). In informal exper-\niments, we could conﬁrm that recognition accuracy drops\nonly marginally if we restrict the frequency range to the\nwide limits, but signiﬁcantly if we restrict it to the narrow\nCDEFGABCDEFGABCDEFGABCDEFGABCDEFGABCDEFGABCDEFGABCDE-0+\nA:min7\nCDEFGABCDEFGABCDEFGABCDEFGABCDEFGABCDEFGABCDEFGABCDE-0+\nF/sharp:minFigure 4 . Average saliency map summed over the time\naxis for A:min7 and F /sharp:min chords computed on the Beat-\nles dataset. As expected, we observe mostly positive peaks\nfor frequency bins corresponding to notes present in the\nchords (a, c, e, g for A:min7; f /sharp, a, c/sharpfor F/sharp:min).\n-0.7 -0.3 0 +0.3 +0.7\nTime [s]0\n/summationdisplay\nfreqs Saliency\npositive\nnegative\nFigure 5 . Average positive and negative saliencies of all\ninput frames of the Beatles dataset, summed over the fre-\nquency axis. Most of the important information is within\n±0.3s around the centre frame, and past data seems to be\nmore important than future data. Around the centre frame,\nthe network pays relatively more attention to what should\nbemissing than present in a given chroma vector, and vice\nversa in areas further away from the centre. The differ-\nences are statistically signiﬁcant due to the large number\nof samples.\nlimits. This means that the secondary information captured\nby the additional frequency bins of the wide limits is also\ncrucial.\nTo allow for a visual comparison of the computed fea-\ntures, we depict different chromagrams for the song “Yes-\nterday” by the Beatles in Fig. 7. The images show that\nthe chroma vectors extracted by the proposed method are\nless noisy and chord transitions are crisper compared to the\nbaseline methods.\n7. CONCLUSIONS AND FUTURE WORK\nIn this paper, we presented a data-driven approach to\nlearning a neural-network-based chroma extractor for\nchord recognition. The proposed extractor computes\ncleaner chromagrams than state-of-the-art baseline meth-\nods, which we showed quantitatively in a simple chord\nrecognition experiment and examined qualitatively by vi-\nsually comparing extracted chromagrams.\nWe inspected the learned model using saliency maps\nand found that a frequency range of 110 Hz to 3136 HzProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 41CDEFGABCDEFGABCDEFGABCDEFGABCDEFGABCDEFGABCDEFGABCDE\nPitch-0.70+0.7Time [s]CDEFGABCDEFGABCDEFGABCDEFGABCDEFGABCDEFGABCDEFGABCDE110 Hz 196 Hz 1319 Hz 3136 HzFigure 6 . Average saliency of all input frames of the Bea-\ntles dataset (bottom image), summed over the time axis\n(top plot). We see that most relevant information can be\ncollected in barely 3 octaves between G3 at 196 Hz and\nE6 at 1319 Hz. Hardly any harmonic information resides\nbelow 110 Hz and above 3136 Hz. The plot is spiky at\nfrequency bins that correspond to clean semitones because\nmost of the songs in the dataset seem to be tuned to a refer-\nence frequency of 440 Hz. The network thus usually pays\nlittle attention to the frequency bins between semitones.\ncd efgabCD\ncd efgabCW\nLog\n10 15 20 25 30\nTime [s]cd efgabCW\nLog\nF:maj(*3)\nE:min\nA:7\nD:min\nD:min/b7\nBb:maj7\nC:7\nF\nF/7\nD:min\nG\nBb\nF\nF\nA:sus4/5\nA\nD:min\nA:min/b3\nBb:maj\nC\nBb/5\nF\nFigure 7 . Excerpts of chromagrams extracted from the\nsong “Yesterday” by the Beatles. The lower image shows\nchroma computed by the CW\nLogwithout smoothing. We see\na good temporal resolution, but also noise. The centre im-\nage shows the same chromas after a moving average ﬁlter\nof 1.5 seconds. The ﬁlter reduced noise considerably, at\nthe cost blurring chord transitions. The upper plot shows\nthe chromagram extracted by our proposed method. It dis-\nplays precise pitch activations and low noise, while keep-\ning chord boundaries crisp. Pixel values are scaled such\nthat for each image, the lowest value in the respective chro-\nmagram is mapped to white, the highest to black.seems to sufﬁce as input to chord recognition methods. Us-\ning saliency maps and preliminary experiments on valida-\ntion folds we also found that a context of 1.5 seconds is\nadequate for local harmony estimation.\nThere are plenty possibilities for future work to extend\nand/or improve our method. To achieve better results, we\ncould use DNN ensembles instead of a single DNN. We\ncould ensure that the network sees data for which its pre-\ndictions are wrong more often during training, or similarly,\nwe could simulate a more balanced dataset by showing\nthe net super-frames of rare chords more often. To fur-\nther assess how useful the extracted features are for chord\nrecognition, we shall investigate how well they interact\nwith post-ﬁltering methods; since the feature extractor is\ntrained discriminatively, Conditional Random Fields [17]\nwould be a natural choice.\nFinally, we believe that the proposed method extracts\nfeatures that are useful in any other MIR applications that\nuse chroma features (e.g. structural segmentation, key esti-\nmation, cover song detection). To facilitate respective ex-\nperiments, we provide source code for our method as part\nof the madmom audio processing framework [2]. Informa-\ntion and source code to reproduce our experiments can be\nfound at http://www.cp.jku.at/people/korzeniowski/dc.\n8. ACKNOWLEDGEMENTS\nThis work is supported by the European Research Coun-\ncil (ERC) under the EU’s Horizon 2020 Framework Pro-\ngramme (ERC Grant Agreement number 670035, project\n”Con Espressione”). The Tesla K40 used for this research\nwas donated by the NVIDIA Corporation.\n9. REFERENCES\n[1] Y . Bengio, A. Courville, and P. Vincent. Representa-\ntion Learning: A Review and New Perspectives. IEEE\nTransactions on Pattern Analysis and Machine Intelli-\ngence , 35(8):1798–1828, Aug. 2013.\n[2] S. B ¨ock, F. Korzeniowski, J. Schl ¨uter, F. Krebs,\nand G. Widmer. madmom: a new Python Audio\nand Music Signal Processing Library. arXiv preprint\narXiv:1605.07008 , 2016.\n[3] S. B ¨ock, F. Krebs, and G. Widmer. A multi-model ap-\nproach to beat tracking considering heterogeneous mu-\nsic styles. In Proceedings of the 15th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , Taipei, Taiwan, 2014.\n[4] S. B ¨ock, F. Krebs, and G. Widmer. Accurate tempo es-\ntimation based on recurrent neural networks and res-\nonating comb ﬁlters. In Proceedings of the 16th Inter-\nnational Society for Music Information Retrieval Con-\nference (ISMIR) , M´alaga, Spain, 2015.\n[5] N. Boulanger-Lewandowski, Y . Bengio, and P. Vin-\ncent. Audio chord recognition with recurrent neural\nnetworks. In Proceedings of the 14th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , Curitiba, Brazil, 2013.42 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016[6] R. Chen, W. Shen, A. Srinivasamurthy, and P. Chor-\ndia. Chord recognition using duration-explicit hidden\nMarkov models. In Proceedings of the 13th Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , Porto, Portugal, 2012.\n[7] T. Cho and J. P. Bello. On the Relative Importance\nof Individual Components of Chord Recognition Sys-\ntems. IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing , 22(2):477–492, Feb. 2014.\n[8] B. Di Giorgi, M. Zanoni, A. Sarti, and S. Tubaro. Au-\ntomatic chord recognition based on the probabilistic\nmodeling of diatonic modal harmony. In Proceedings\nof the 8th International Workshop on Multidimensional\nSystems , Erlangen, Germany, 2013.\n[9] S. Dieleman, J. Schl ¨uter, C. Raffel, E. Olson, S. K.\nSønderby, D. Nouri, E. Battenberg, A. van den Oord,\net al. Lasagne: First release, 2015.\n[10] T. Fujishima. Realtime Chord Recognition of Musical\nSound: a System Using Common Lisp Music. In Pro-\nceedings of the International Computer Music Confer-\nence (ICMC) , Beijing, China, 1999.\n[11] X. Glorot, A. Bordes, and Y . Bengio. Deep sparse rec-\ntiﬁer neural networks. In Proceedings of the 14th In-\nternational Conference on Artiﬁcial Intelligence and\nStatistics (AISTATS) , Fort Lauderdale, USA, 2011.\n[12] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC Music Database: Popular, Classical and Jazz\nMusic Databases. In Proceedings of the 3rd Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR) , Paris, France, 2002.\n[13] C. Harte. Towards Automatic Extraction of Harmony\nInformation from Music Signals . Dissertation, Depart-\nment of Electronic Engineering, Queen Mary, Univer-\nsity of London, London, United Kingdom, 2010.\n[14] E. J. Humphrey and J. P. Bello. Rethinking Auto-\nmatic Chord Recognition with Convolutional Neural\nNetworks. In 11th International Conference on Ma-\nchine Learning and Applications (ICMLA) , Boca Ra-\nton, USA, 2012.\n[15] E. J. Humphrey, T. Cho, and J. P. Bello. Learning\na robust tonnetz-space transform for automatic chord\nrecognition. In International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , Kyoto, Japan,\n2012.\n[16] D. Kingma and J. Ba. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980 , 2014.\n[17] J. D. Lafferty, A. McCallum, and F. C. N. Pereira.\nConditional Random Fields: Probabilistic Models for\nSegmenting and Labeling Sequence Data. In Proceed-\nings of the 18th International Conference on Machine\nLearning (ICML) , Williamstown, USA, 2001.\n[18] M. Mauch, C. Cannam, M. Davies, S. Dixon, C. Harte,\nS. Kolozali, D. Tidhar, and M. Sandler. OMRAS2\nmetadata project 2009. In Late Breaking Demo of the10th International Conference on Music Information\nRetrieval (ISMIR) , Kobe, Japan, 2009.\n[19] M. Mauch and S. Dixon. Approximate note tran-\nscription for the improved identiﬁcation of difﬁcult\nchords. In Proceedings of the 11th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , Utrecht, Netherlands, 2010.\n[20] M. McVicar, R. Santos-Rodriguez, Y . Ni, and T. D.\nBie. Automatic Chord Estimation from Audio: A Re-\nview of the State of the Art. IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing ,\n22(2):556–575, Feb. 2014.\n[21] M. M ¨uller, S. Ewert, and S. Kreuzer. Making chroma\nfeatures more robust to timbre changes. In Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , Taipei, Taiwan, 2009.\n[22] N. Ono, K. Miyamoto, J. Le Roux, H. Kameoka, and\nS. Sagayama. Separation of a monaural audio sig-\nnal into harmonic/percussive components by comple-\nmentary diffusion on spectrogram. In 16th European\nSignal Processing Conference (EUSIPCO) , Lausanne,\nFrance, 2008.\n[23] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon,\nO. Nieto, D. Liang, and D. P. W. Ellis. mir eval: a\ntransparent implementation of common MIR metrics.\nInProceedings of the 15th International Conference on\nMusic Information Retrieval (ISMIR) , Taipei, Taiwan,\n2014.\n[24] S. Sigtia, N. Boulanger-Lewandowski, and S. Dixon.\nAudio chord recognition with a hybrid recurrent neural\nnetwork. In 16th International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , M´alaga, Spain,\n2015.\n[25] J. T. Springenberg, A. Dosovitskiy, T. Brox, and\nM. Riedmiller. Striving for Simplicity: The All Con-\nvolutional Net. arXiv preprint arXiv:1412.6806 , 2014.\n[26] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,\nand R. Salakhutdinov. Dropout: A Simple Way to Pre-\nvent Neural Networks from Overﬁtting. The Journal of\nMachine Learning Research , 15(1):1929–1958, 2014.\n[27] Y . Ueda, Y . Uchiyama, T. Nishimoto, N. Ono, and\nS. Sagayama. HMM-based approach for automatic\nchord detection using reﬁned acoustic features. In In-\nternational Conference on Acoustics Speech and Sig-\nnal Processing (ICASSP) , Dallas, USA, Mar. 2010.\n[28] K. Ullrich, J. Schl ¨uter, and T. Grill. Boundary detection\nin music structure analysis using convolutional neural\nnetworks. In Proceedings of the 15th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , Taipei, Taiwan, 2014.\n[29] ¨O.˙Izmirli and R. B. Dannenberg. Understanding Fea-\ntures and Distance Functions for Music Sequence\nAlignment. In Proceedings of the 11th International\nSociety for Music Information Retrieval Conference\n(ISMIR) , Utrecht, Netherlands, 2010.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 43"
    },
    {
        "title": "Downbeat Tracking Using Beat Synchronous Features with Recurrent Neural Networks.",
        "author": [
            "Florian Krebs",
            "Sebastian Böck",
            "Matthias Dorfer",
            "Gerhard Widmer"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417819",
        "url": "https://doi.org/10.5281/zenodo.1417819",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/249_Paper.pdf",
        "abstract": "In this paper, we propose a system that extracts the down- beat times from a beat-synchronous audio feature stream of a music piece. Two recurrent neural networks are used as a front-end: the first one models rhythmic content on multiple frequency bands, while the second one models the harmonic content of the signal. The output activations are then combined and fed into a dynamic Bayesian network which acts as a rhythmical language model. We show on seven commonly used datasets of Western music that the system is able to achieve state-of-the-art results.",
        "zenodo_id": 1417819,
        "dblp_key": "conf/ismir/KrebsBDW16",
        "content": "DOWNBEAT TRACKING USING BEAT-SYNCHRONOUS FEATURES AND\nRECURRENT NEURAL NETWORKS\nFlorian Krebs, Sebastian B ¨ock, Matthias Dorfer, and Gerhard Widmer\nDepartment of Computational Perception\nJohannes Kepler University Linz, Austria\nFlorian.Krebs@jku.at\nABSTRACT\nIn this paper, we propose a system that extracts the down-\nbeat times from a beat-synchronous audio feature stream\nof a music piece. Two recurrent neural networks are used\nas a front-end: the ﬁrst one models rhythmic content on\nmultiple frequency bands, while the second one models the\nharmonic content of the signal. The output activations are\nthen combined and fed into a dynamic Bayesian network\nwhich acts as a rhythmical language model. We show on\nseven commonly used datasets of Western music that the\nsystem is able to achieve state-of-the-art results.\n1. INTRODUCTION\nThe automatic analysis of the metrical structure in an au-\ndio piece is a long-standing, ongoing endeavour. A good\nunderlying meter analysis system is fundamental for vari-\nous tasks like automatic music segmentation, transcription,\nor applications such as automatic slicing in digital audio\nworkstations.\nThe meter in music is organised in a hierarchy of pulses\nwith integer related frequencies. In this work, we concen-\ntrate on one of the higher levels of the metrical hierarchy,\nthemeasure level. The ﬁrst beat of a musical measure is\ncalled a downbeat , and this is typically where harmonic\nchanges occur or speciﬁc rhythmic pattern begin [23].\nThe ﬁrst system that automatically detected beats and\ndownbeats was proposed by Goto and Muraoka [15]. It\nmodelled three metrical levels, including the measure level\nby ﬁnding chord changes. Their system, built upon hand-\ndesigned features and rules, was reported to successfully\ntrack downbeats in 4/4music with drums. Since then,\nmuch has changed in the meter tracking literature. A gen-\neral trend is to go from hand-crafted features and rules\nto automatically learned ones. In this line, rhythmic pat-\nterns are learned from data and used as observation model\nin probabilistic state-space models [23, 24, 28]. Support\nVector Machines (SVMs) were ﬁrst applied to downbeat\nc/circlecopyrtFlorian Krebs, Sebastian B ¨ock, Matthias Dorfer, Ger-\nhard Widmer. Licensed under a Creative Commons Attribution 4.0 In-\nternational License (CC BY 4.0). Attribution: Florian Krebs, Sebas-\ntian B ¨ock, Matthias Dorfer, Gerhard Widmer. “DOWNBEAT TRACK-\nING USING BEAT-SYNCHRONOUS FEATURES AND RECURRENT\nNEURAL NETWORKS”, 17th International Society for Music Informa-\ntion Retrieval Conference, 2016.\nBeat synchronous\nfeatures\nRecurrent Neural\nNetworkDynamic Bayesian\nNetwork\n1 1.5 2 2.5 3 3.5 4 4.5 5 5.500.10.20.30.40.50.60.70.80.91\nRNN output DBN output\nFigure 1 : Model overview\ntracking in a semi-automatic setting [22] and later used\nin a fully automatic system that operated on several beat-\nsynchronous hand-crafted features [12]. The latter sys-\ntem was later reﬁned by using convolutional neural net-\nworks (ConvNets) instead of SVMs and a new set of fea-\ntures [10,11] and is the current state-of-the-art in downbeat\ntracking on Western music.\nRecurrent neural networks (RNNs) are Neural Net-\nworks adapted to sequential data and therefore are the nat-\nural choice for sequence analysis tasks. In fact, they have\nshown success in various tasks such as speech recogni-\ntion [19], handwriting recognition [17] or beat tracking [2].\nIn this work, we would like to explore the application of\nRNNs to the downbeat tracking problem. We describe a\nsystem that detects downbeats from a beat-synchronous in-\nput feature sequence, analyse the performance of two dif-\nferent input features, and discuss shortcomings of the pro-\nposed model. We report state-of-the-art performance on\nseven datasets.\nThe paper is organised as follows: In Section 2 we de-\nscribe the proposed RNN-based downbeat tracking system,\nin Section 3 we explain the experimental set-up of our eval-\nuation and present and discuss the results in Section 4.\n2. METHOD\nAn overview of the system is shown in Fig. 1. Two beat-\nsynchronised feature streams (Section 2.1) are fed into two\nparallel RNNs (Section 2.2) to obtain a downbeat acti-\nvation function which indicates the probability whether a\nbeat is a downbeat. Finally, the activation function is de-\ncoded into a sequence of downbeat times by a dynamic\nBayesian network (DBN) (Section 2.3).1292.1 Feature extraction\nIn this work we assume that the beat times of an audio\nsignal are known, by using either hand-annotated or auto-\nmatically generated labels. We believe that the segmenta-\ntion into beats makes it much more easy for the subsequent\nstage to detect downbeats because it does not have to deal\nwith tempo or expressive timing on one hand and it greatly\nreduces the computational complexity by both reducing the\nsequence length of an excerpt and the search space. Beat-\nsynchronous features have successfully been used before\nfor downbeat tracking [5, 10, 27]. Here, we use two fea-\ntures: A spectral ﬂux with logarithmic frequency spacing\nto represent percussive content ( percussive feature ) and\na chroma feature to represent the harmonic progressions\nthroughout a song ( harmonic feature ).\n2.1.1 Percussive feature\nAs a percussive feature, we compute a multi-band spec-\ntral ﬂux: First, we compute the magnitude spectrogram by\napplying the Short-time Fourier Transform (STFT) with a\nHann window, hopsize of 10ms, and a frame length of 2048\nsamples, as shown in Fig. 2a. Then, we apply a logarithmic\nﬁlter bank with 6 bands per octave, covering the frequency\nrange from 30 to 17 000 Hz, resulting in 45 bins in total.\nWe compress the magnitude by applying the logarithm and\nﬁnally compute for each frame the difference between the\ncurrent and the previous frame. The feature sequence is\nthen beat-synchronised by only keeping the mean value per\nfrequency bin in a window of length ∆b/np, where ∆bis\nthe beat period and np= 4 is the number of beat subdivi-\nsions, centred around the beginning of a beat subdivision.\nAn example of the percussive feature is shown in Fig. 2b.\n2.1.2 Harmonic feature\nAs harmonic feature, we use the CLP chroma feature [26]\nwith a frame rate of 100frames per second. We synchro-\nnise the features to the beat by computing the mean over a\nwindow of length ∆b/nh, yieldingnh= 2 feature values\nper beat interval. We found that for the harmonic feature\nthe resolution can be lower than for the percussive feature,\nas for chord changes the exact timing is less critical. An\nexample of the harmonic feature is shown in Fig. 2d.\n2.2 Recurrent Neural Network\nRNNs are the natural choice for sequence modelling tasks\nbut often difﬁcult to train due to the exploding and vanish-\ning gradient problems. In order to overcome these prob-\nlems when dealing with long sequences, Long-Short-Term\nmemory (LSTM) networks were proposed [20]. Later, [4]\nproposed a simpliﬁed version of the LSTMs named Gated\nRecurrent Units (GRUs), which were shown to perform\ncomparable to the traditional LSTM in a variety of tasks\nand have less parameters to train. Therefore, we will use\nGRUs in this paper.\nThe time unit modelled by the RNNs is the beat period ,\nand all feature values that fall into one beat are condensed\ninto one vector. E.g., using the percussive feature with 45\n12 13 14 15 16 17 18 1902153430664598613107661291915073Frequency [Hz](a) Spectrogram\n12 13 14 15 16 17 18 194325888227998871Frequency [Hz]\n(b) Beat-synchronous percussive feature\n12 13 14 15 16 17 18 190.00.51.0RNN output\n(c) Activation and target of the rhythmic network\n12 13 14 15 16 17 18 19CC#DD#EFF#GG#AA#BPitch class\n(d) Beat-synchronous chroma feature\n12 13 14 15 16 17 18 190.00.51.0RNN output\n(e) Activation and target of the harmonic network\nFigure 2 : Visualisation of the two feature streams and their\ncorresponding network output of an 8-second excerpt of\nthe song Media-105701 (Ballroom dataset). The dashed\nline in (c) and (e) represents the target (downbeat) se-\nquence, the solid line the networks’ activations. The x-axis\nshows time in seconds. The time resolution is one fourth\nof the beat period in (b), and half a beat period in (d).130 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016frequency bins and a resolution of np= 4 beat subdivi-\nsions yields an input dimension of 45×4 = 180 for the\nrhythmic RNN. In comparison to an RNN that models sub-\ndivisions of the beat period as underlying time unit, this\nvectorisation of the temporal context provided an impor-\ntant speed-up of the network training due to the reduced\nsequence length, while maintaining the same level of per-\nformance.\nIn preliminary tests, we investigated possible architec-\ntures for our task and compared their performances on the\nvalidation set (see Section 3.3). We made the following\ndiscoveries: First, adding bidirectional connections to the\nmodels was found to greatly improve the performance.\nSecond, the use of LSTMs/GRUs further improved the\nperformance compared to the standard RNN. Third, using\nmore than two layers did not further improve the perfor-\nmance.\nWe therefore chose to use a two layer bidirectional\nnetwork with GRU units and standard tanh non-linearity.\nEach hidden layer has 25 units. The output layer is a dense\nlayer with one unit and a sigmoid non-linearity. Due to the\ndifferent number of input units the rhythmic model has ap-\nproximately 44k, and the harmonic model approximately\n19k parameters.\nThe activations of both the rhythmic and harmonic\nmodel are ﬁnally averaged to yield the input activation for\nthe subsequent DBN stage.\n2.3 Dynamic Bayesian Network\nThe language model incorporates musical prior knowledge\ninto the system. In our case it implements the following\nassumptions:\n1. Beats are organised into bars, which consist of a con-\nstant number of beats.\n2. The time signature of a piece determines the number\nof beats per bar.\n3. Time signature changes are rare within a piece.\nThe DBN stage is similar to the one used in [10], with\nthree differences: First, we model beats as states instead\nof tatums. Second, as our data mainly contains 3/4and\n4/4time signatures, we only model these two. Third, we\nforce the state sequence to always transverse a whole bar\nfrom left to right, i.e., transitions from beat 2 to beat 1 are\nnot allowed. In the following we give a short review of the\nDBN stage.\nA states(b,r)in the DBN state space is determined by\ntwo hidden state variables: the beat counter band the time\nsignaturer. The beat counter counts the beats within a bar\nb∈{1..Nr}whereNris the number of beats in time sig-\nnaturer. E.g.,r∈{3,4}for the case where a 3/4 and a\n4/4 time signature are modelled. The state transition prob-\nabilities can then be decomposed using\nP(sk|sk−1) =P(bk|bk−1,rk−1)×P(rk|rk−1,bk,bk−1)\n(1)where\nP(bk|bk−1,rk−1) =/braceleftbigg1ifbk= (bk−1modrk−1) + 1\n0otherwise.\n(2)\nEq. 2 ensures that the beat counter can only move steadily\nfrom left to right. Time signature changes are only allowed\nto happen at the beginning of a bar ( (bk< b k−1)), as im-\nplemented by\nif (bk<bk−1)\nP(rk|rk−1,bk,bk−1) =/braceleftbigg1−prif(rk=rk−1)\npr/R if(rk/negationslash=rk−1)\nelse\nP(rk|rk−1,bk,bk−1) = 0\n(3)\nwherepris the probability of a time signature change. We\nlearnedpron the validation set and found pr= 10−7to be\nan overall good value, which makes time signature changes\nimprobable but possible. However, the exact choice of this\nparameter is not critical, but it should be greater than zero\nas mentioned in Section 4.5.\nAs the sigmoid of the output layer of the RNN yields a\nvalue between 0 and 1, we can interpret its output as the\nprobability that a speciﬁc beat is a downbeat and use it as\nobservation likelihood for the DBN. As the RNN outputs a\nposterior probability P(s|features ), we need to scale it by\na factorλ(s)which is proportional to 1/P(s)in order to\nobtain\nP(features|s)∝P(s|features )/P(s), (4)\nwhich is needed by the observation model of the DBN. Ex-\nperiments have shown that a value of λ(s(b= 1,r)) = 100\nfor downbeat states and λ(s(b >1,r)) = 1 for the other\nstates performed best on our validation set, and will be\nused in this paper.\nFinally, we use a uniform initial distribution over the\nstates and decode the most probably state sequence with\nthe Viterbi algorithm.\n3. EXPERIMENTS\n3.1 Data\nIn this work, we restrict the data to Western music only\nand leave the evaluation of Non-Western music for future\nwork. The following datasets are used:\nBallroom [16, 24]: This dataset consists of 685 unique\n30 second-long excerpts of Ballroom dance music. The\ntotal length is 5h 57m.\nBeatles [6]: This dataset consists of 180 songs of the\nBeatles. The total length is 8h 09m.\nHainsworth [18]: This dataset consists of 222 excerpts,\ncovering various genres. The total length is 3h 19m.\nRWC Pop [14]: This dataset consists of 100 American\nand Japanese Pop songs. The total length is 6h 47m.\nRobbie Williams [13]: 65 full songs of Robbie\nWilliams. The total length is 4h 31m\nRock [7]: This dataset consists of 200 songs of the\nRolling Stone magazine’s list of the “500 Greatest Songs\nof All Time“. The total length is 12h 53m.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 131System Ballroom Beatles Hainsworth RWC pop Robbie Williams Klapuri Rock Mean\nWith annnotated beats:\nRhythmic 83.9 87.1 75.7 91.9 93.4 - 87.0 84.4\nHarmonic 77.2 89.9 80.1 92.9 92.6 - 86.0 82.2\nCombined 91.8 89.6 83.6 94.4 96.6 - 89.4 90.4\nWith detected beats:\nCombined 80.3 79.8 71.3 82.7 83.4 69.3 79.0 77.3\n[11] 77.8 81.4 65.7 86.1 83.7 68.9 81.3 76.1\nBeat tracking results:\nBeat tracker [1, 25] 89.0 88.4 88.2 88.6 88.2 85.2 90.5 88.3\nTable 1 : Mean downbeat tracking F-measures across all datasets. The last column shows the mean over all datasets used.\nThe last row shows beat tracking F-measure scores.\nKlapuri [23]: This dataset consists of 320 excerpts,\ncovering various genres. The total length is 4h 54m. The\nbeat annotations of this dataset have been made indepen-\ndently of the downbeat annotations and therefore do not\nalways match. Hence, we cannot use the dataset in experi-\nments that rely on annotated beats.\n3.2 Evaluation measure\nFor the evaluation of downbeat tracking we follow [10,25]\nand report the F-measure which is computed by F=\n2RP/(R+P), where the recall Ris the ratio of correctly\ndetected downbeats within a ±70mswindow and the total\nnumber of annotated downbeats, and the precision Pis the\nratio of correctly detected downbeats within this window\nand all the reported downbeats.\n3.3 Training procedure\nAll experiments in this section have been carried out using\nthe leave-one-dataset-out approach, to be as comparable as\npossible with the setting in [11]. After removing the test\ndataset, we use 75% of the remaining data for training and\n25% for validation. To cope with the varying lengths of\nthe audio excerpts, we split the training data into segments\nof15beats and an overlap of 10beats. For training, we\nuse cross entropy cost, and AdaGrad [9] with a constant\nlearn rate of 0.04for the rhythmic model and 0.02for the\nharmonic model. The hidden units and the biases are ini-\ntialised with zero, and the weights of the network are ran-\ndomly sampled from a normal distribution with zero mean\nand a standard deviation of 0.1. We stop the learning after\n100epochs or when the validation error does not decrease\nfor15epochs. For training the GRUs, we used the Lasagne\nframework [8].\n4. RESULTS AND DISCUSSION\n4.1 Inﬂuence of features\nIn this section we investigate the inﬂuence of the two dif-\nferent input features described in Section 2.1.\nThe performance of the two different networks is shown\nin the upper part of Table 1. Looking at the mean scores\nover all datasets, the rhythmic and harmonic networkachieve a comparable performance. The biggest differ-\nence between the two was found in the Ballroom and the\nHainsworth dataset, which we believe is mostly due to dif-\nfering musical content. While the Ballroom set consists\nof music with clear and prominent rhythm which the per-\ncussive feature seems to capture well, the Hainsworth set\nalso includes chorales with less clear-cut rhythm but more\nprominent harmonic content which in turn is better repre-\nsented by the harmonic feature. Interestingly, combining\nboth networks (by averaging the output activations) yields\na score that is almost always higher than the score of the\nsingle networks. Apparently, the two networks concentrate\non different, relevant aspects of the audio signal and com-\nbining them enables the system exploiting both. This is in\nline with the observations in [11] who similarly combined\nthe output of three networks in their system.\n4.2 Estimated vs. annotated beat positions\nIn order to have a fully automatic downbeat tracking sys-\ntem we use the beat tracker proposed in [1] with an en-\nhanced state space [25] as a front-end to our system.1\nWe show the beat tracking F-measures per dataset in the\nbottom row of Table 1. With regard to beat tracking, the\ndatasets seem to be balanced in terms of difﬁculty.\nThe detected beats are then used to synchronise the fea-\ntures of the test set.2The downbeat scores obtained with\nthe detected beats are shown in the middle part of Table 1.\nAs can be seen, the values are around 10%−15% lower\nthan if annotated beats were used. This makes sense, since\nan error in the beat tracking stage cannot be corrected in\na later stage. This might be a drawback of the proposed\nsystem compared to [11], where the tatum (instead of the\nbeat) is the basic time unit and the downbeat tracking stage\ncan still decide whether a beat consists of one, two or more\ntatums.\nAlthough the beat tracking performance is balanced\namong the datasets, we ﬁnd clear differences in the down-\nbeat tracking performance. For example, while the beat\ntracking performance on the Hainsworth and the Robbie\nWilliams dataset are similar, the downbeat accuracy dif-\nfers more than 12%. Apparently, the mix of genres, in-\n1We use the DBNBeatTracker included in madmom [3] version 0.13.\n2We took care that there is no overlap between the train and test sets.132 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20160 0.5 105001000\nF−measureCounts(a)\n0 0.5 105001000\nF−measureCounts (b)\nFigure 3 : Histogram of the downbeat F-measures of the\nproposed system (a) and the reference system [11] (b)\ncluding time signatures of 2/2,3/2,3/4and6/8, in the\nHainsworth set represents a challenge to downbeat track-\ning compared to the more simple Robbie Williams , which\nmostly contains 4/4time signatures.\n4.3 Importance of the DBN stage\nSystem annotated detected\nRNN 85.0 73.7\nRNN+DBN 90.4 77.3\nTable 2 : Mean downbeat tracking F-measures across all\ndatasets of the proposed, combined system. annotated and\ndetected means that annotated or detected beats were re-\nspectively used to synchronise the features. RNN uses\npeak-picking to select the downbeats, while RNN+DBN\nuses the DBN language model.\nTo assess the importance of the DBN stage (Section 2.3)\nwe implemented a simple baseline, which simply reports\ndownbeats if the resulting (combined) RNN activations ex-\nceed a threshold. A threshold of 0.2was found to yield\nthe best results on the validation set. In Table 2, we show\nthe results of the baseline (RNN) and the results of the\ncombined system (RNN+DBN). As can be seen, the com-\nbination of RNN and DBN signiﬁcantly outperforms the\nbaseline, conﬁrmed by a Wilcoxon signed-rank test with\np<0.01.\n4.4 Comparison to the state-of-the-art\nIn this section we investigate the performance of our sys-\ntem in relation to the state-of-the-art in downbeat tracking,\nrepresented by [11]. Unfortunately, a direct comparison is\nhindered by various reasons: The datasets used for train-\ning the ConvNets [11] are not freely available and the beat\ntracker at their input stage is different to the one that we\nuse in this work. Therefore, we can only check whether the\nwhole end-to-end system is competitive and leave a modu-\nlar comparison of the approaches to future work.\nIn the middle of Table 1 we show the results of the sys-\ntem described in [11], as provided by the authors. The last\ncolumn shows the mean accuracy over all 1771 excerpts in\nour dataset. A paired-sample t-test did not show any sta-\ntistically signiﬁcant differences in the mean performancebetween the two approaches considering all data points.\nHowever, a Wilcoxon signed-rank test revealed that there\nis a signiﬁcant ( p < 0.01) difference in the median F-\nmeasure over all data points, which is 89.7%for [11] and\n96.2%for the proposed system. Looking at histograms of\nthe obtained scores (see Fig. 3), we found a clear peak at\naround 66% F-measure, which is typically caused by the\nbeat tracking stage reporting half or double of the correct\ntempo. The peak is more prominent for the system [11]\n(Fig. 3b), hence we believe the system might beneﬁt from\na more accurate beat tracker.\nFrom this we conclude that overall the proposed sys-\ntem (in combination with the beat tracker [1, 25]) per-\nforms comparable to the state-of-the-art when looking at\nthe mean performance and even outperforms the state-of-\nthe-art in terms of the median performance.\n4.5 Error analysis\nIn order to uncover the shortcomings of the proposed\nmodel we analysed the errors of a randomly-chosen, small\nsubset of 30 excerpts. We identiﬁed two main factors that\nlead to a low downbeat score. The ﬁrst one, obviously,\nare beat tracking errors which get propagated through to\nthe downbeat stage. Most beat tracking errors are octave\nerrors, and among them, the beat tracker mostly tapped\ntwice as fast as the groundtruth tempo. In some cases this\nis acceptable and therefore would make sense to also al-\nlow these metrical levels as, e.g., done in [23]. The sec-\nond common error is that the downbeat tracker chooses the\nwrong time signature or has problems following time sig-\nnature changes or coping with inserted or removed beats.\nPhase errors are relatively rare. Changing time signatures\nappear most frequently in the Beatles dataset. For this\ndataset, reducing the transition probability of time signa-\nture changes prfrom 10−7to0leads to a relative perfor-\nmance drop of 6%, while the results for other datasets re-\nmain largely unaffected. Besides, the used datasets mainly\ncontain 3/4and4/4time signatures making it impossible\nfor the RNN to learn something meaningful about other\ntime signatures. Here, creating a more balanced training\nset regarding time signatures would surely help.\n5. CONCLUSIONS AND FUTURE WORK\nWe have proposed a downbeat tracking back-end system\nthat uses recurrent Neural networks (RNNs) to analyse a\nbeat-synchronous feature stream. With estimated beats as\ninput, the system performs comparable to the state-of-the-\nart, yielding a mean downbeat F-measure of 77.3%on a\nset of 1771 excerpts of Western music. With manually an-\nnotated beats the score goes up to 90.4%.\nFor future work, a good modular comparison of down-\nbeat tracking approaches needs to be undertaken, possibly\nwith collaboration between several researchers. In partic-\nular, standardised dataset train/test splits need to be de-\nﬁned. Second, we would like to train and test the model\nwith non-Western music and ‘odd’ time signatures, such\nas done in [21].Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 133The source code will be released as part of the\nmadmom library [3], including all trained models\nand can be found together with additional material\nunder http://www.cp.jku.at/people/krebs/\nismir2016/index.html .\n6. ACKNOWLEDGMENTS\nThis work is supported by the European Union Seventh\nFramework Programme FP7 / 2007-2013 through the Gi-\nantSteps project (grant agreement no. 610591), the Aus-\ntrian Science Fund (FWF) project Z159, the Austrian Min-\nistries BMVIT and BMWFW, and the Province of Upper\nAustria via the COMET Center SCCH. For this research,\nwe have made extensive use of free software, in particu-\nlar Python, Lasagne, Theano and GNU/Linux. The Tesla\nK40 used for this research was donated by the NVIDIA\ncorporation. We’d like to thank Simon Durand for giving\nus access to his downbeat tracking code.\n7. REFERENCES\n[1] S. B ¨ock, F. Krebs, and G. Widmer. A multi-model ap-\nproach to beat tracking considering heterogeneous mu-\nsic styles. In Proceedings of the 15th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , Taipei, 2014.\n[2] S. B ¨ock and M. Schedl. Enhanced beat tracking with\ncontext-aware neural networks. In Proceedings of the\nInternational Conference on Digital Audio Effects\n(DAFx) , 2011.\n[3] S. B ¨ock, F. Korzeniowski, J. Schl ¨uter, F. Krebs, and\nG. Widmer. madmom: a new python audio and music\nsignal processing library. arXiv:1605.07008 , 2016.\n[4] K. Cho, B. Van Merri ¨enboer, C. Gulcehre, Dzmitry\nBahdanau, F. Bougares, H. Schwenk, and Y . Ben-\ngio. Learning phrase representations using RNN\nencoder-decoder for statistical machine translation.\narXiv:1406.1078 , 2014.\n[5] M. E. P. Davies and M. D. Plumbley. A spectral differ-\nence approach to downbeat extraction in musical au-\ndio. In Proceedings of the European Signal Processing\nConference (EUSIPCO) , Florence, 2006.\n[6] M.E.P. Davies, N. Degara, and M.D. Plumbley. Eval-\nuation methods for musical audio beat tracking algo-\nrithms. Queen Mary University of London, Tech. Rep.\nC4DM-09-06 , 2009.\n[7] T. De Clercq and D. Temperley. A corpus analysis of\nrock harmony. Popular Music , 30(01):47–70, 2011.\n[8] Sander Dieleman, Jan Schl ¨uter, Colin Raffel, Eben Ol-\nson, Søren Kaae Sønderby, Daniel Nouri, Eric Batten-\nberg, A ¨aron van den Oord, et al. Lasagne: First re-\nlease., August 2015.[9] J. Duchi, E. Hazan, and Y . Singer. Adaptive subgra-\ndient methods for online learning and stochastic opti-\nmization. The Journal of Machine Learning Research ,\n12:2121–2159, 2011.\n[10] S. Durand, J. Bello, D. Bertrand, and G. Richard.\nDownbeat tracking with multiple features and deep\nneural networks. In Proceedings of the International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , Brisbane, 2015.\n[11] S. Durand, J.P. Bello, Bertrand D., and G. Richard.\nFeature adapted convolutional neural networks for\ndownbeat tracking. In The 41st IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , 2016.\n[12] S. Durand, B. David, and G. Richard. Enhancing down-\nbeat detection when facing different music styles. In\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , pages 3132–3136.\nIEEE, 2014.\n[13] B. D. Giorgi, M. Zanoni, A. Sarti, and S. Tubaro. Au-\ntomatic chord recognition based on the probabilistic\nmodeling of diatonic modal harmony. In Proceedings\nof the 8th International Workshop on Multidimensional\nSystems , 2013.\n[14] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: Popular, classical and jazz mu-\nsic databases. In Proceedings of the 3rd International\nSociety for Music Information Retrieval Conference\n(ISMIR) , Paris, 2002.\n[15] M. Goto and Y . Muraoka. Real-time rhythm tracking\nfor drumless audio signals: Chord change detection for\nmusical decisions. Speech Communication , 27(3):311–\n335, 1999.\n[16] F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, G. Tzane-\ntakis, C. Uhle, and P. Cano. An experimental compari-\nson of audio tempo induction algorithms. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n14(5):1832–1844, 2006.\n[17] A. Graves, M. Liwicki, S. Fern ´andez, R. Bertolami,\nH. Bunke, and J. Schmidhuber. A novel connection-\nist system for unconstrained handwriting recognition.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence , 31(5):855–868, 2009.\n[18] S. Hainsworth and M. Macleod. Particle ﬁltering ap-\nplied to musical tempo tracking. EURASIP Journal on\nApplied Signal Processing , 2004:2385–2395, 2004.\n[19] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Di-\namos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta,\nA. Coates, and A. Ng. Deep speech: Scaling up end-\nto-end speech recognition. arXiv:1412.5567v2 , 2014.\n[20] S. Hochreiter and J. Schmidhuber. Long short-term\nmemory. Neural computation , 9(8):1735–1780, 1997.134 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016[21] A. Holzapfel, F. Krebs, and A. Srinivasamurthy. Track-\ning the “odd”: Meter inference in a culturally diverse\nmusic corpus. In Proceedings of the 15th International\nSociety for Music Information Retrieval Conference\n(ISMIR) , Taipei, 2014.\n[22] T. Jehan. Downbeat prediction by listening and learn-\ning. In Applications of Signal Processing to Audio and\nAcoustics, 2005. IEEE Workshop on , pages 267–270.\nIEEE, 2005.\n[23] A. Klapuri, A. Eronen, and J. Astola. Analysis of\nthe meter of acoustic musical signals. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n14(1):342–355, 2006.\n[24] F. Krebs, S. B ¨ock, and G. Widmer. Rhythmic pattern\nmodeling for beat and downbeat tracking in musical\naudio. In Proceedings of the 14th International Society\nfor Music Information Retrieval Conference (ISMIR) ,\nCuritiba, 2013.\n[25] F. Krebs, S. B ¨ock, and G. Widmer. An efﬁcient state\nspace model for joint tempo and meter tracking. In Pro-\nceedings of the 16th International Society for Music\nInformation Retrieval Conference (ISMIR) , Malaga,\n2015.\n[26] Meinard M ¨uller and Sebastian Ewert. Chroma Tool-\nbox: MATLAB implementations for extracting vari-\nants of chroma-based audio features. In Proceedings\nof the 12th International Conference on Music Infor-\nmation Retrieval (ISMIR) , Miami, 2011.\n[27] H. Papadopoulos and G. Peeters. Joint estimation of\nchords and downbeats from an audio signal. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 19(1):138–152, 2011.\n[28] G. Peeters and H. Papadopoulos. Simultaneous beat\nand downbeat-tracking using a probabilistic frame-\nwork: theory and large-scale evaluation. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n(99):1–1, 2011.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 135"
    },
    {
        "title": "Bootstrapping a System for Phoneme Recognition and Keyword Spotting in Unaccompanied Singing.",
        "author": [
            "Anna M. Kruspe"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417553",
        "url": "https://doi.org/10.5281/zenodo.1417553",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/070_Paper.pdf",
        "abstract": "Speech recognition in singing is still a largely unsolved problem. Acoustic models trained on speech usually pro- duce unsatisfactory results when used for phoneme reco- gnition in singing. On the flipside, there is no phonetically annotated singing data set that could be used to train more accurate acoustic models for this task. In this paper, we attempt to solve this problem using the DAMP data set which contains a large number of re- cordings of amateur singing in good quality. We first align them to the matching textual lyrics using an acoustic model trained on speech. We then use the resulting phoneme alignment to train new acoustic models using only subsets of the DAMP sin- ging data. These models are then tested for phoneme reco- gnition and, on top of that, keyword spotting. Evaluation is performed for different subsets of DAMP and for an un- related set of the vocal tracks of commercial pop songs. Results are compared to those obtained with acoustic mo- dels trained on the TIMIT speech data set and on a version of TIMIT augmented for singing. Our new approach shows significant improvements over both.",
        "zenodo_id": 1417553,
        "dblp_key": "conf/ismir/Kruspe16",
        "content": "BOOTSTRAPPING A SYSTEM FOR PHONEME RECOGNITION AND\nKEYWORD SPOTTING IN UNACCOMPANIED SINGING\nAnna M. Kruspe\nFraunhofer IDMT, Ilmenau, Germany\nkpe@idmt.fraunhofer.de\nABSTRACT\nSpeech recognition in singing is still a largely unsolved\nproblem. Acoustic models trained on speech usually pro-\nduce unsatisfactory results when used for phoneme reco-\ngnition in singing. On the ﬂipside, there is no phonetically\nannotated singing data set that could be used to train more\naccurate acoustic models for this task.\nIn this paper, we attempt to solve this problem using\ntheDAMP data set which contains a large number of re-\ncordings of amateur singing in good quality. We ﬁrst align\nthem to the matching textual lyrics using an acoustic model\ntrained on speech.\nWe then use the resulting phoneme alignment to train\nnew acoustic models using only subsets of the DAMP sin-\nging data. These models are then tested for phoneme reco-\ngnition and, on top of that, keyword spotting. Evaluation\nis performed for different subsets of DAMP and for an un-\nrelated set of the vocal tracks of commercial pop songs.\nResults are compared to those obtained with acoustic mo-\ndels trained on the TIMIT speech data set and on a version\nofTIMIT augmented for singing. Our new approach shows\nsigniﬁcant improvements over both.\n1. INTRODUCTION\nAutomatic speech recognition encompasses a large variety\nof research topics, but the developed algorithms have so\nfar rarely been adapted to singing. Most of these tasks be-\ncome harder when used on singing because singing data\nhas different characteristics, which are also often more va-\nried than in pure speech [12] [2]. For example, the typical\nfundamental frequency for women in speech is between\n165and200Hz, while in singing it can reach more than\n1000Hz. Other differences include harmonics, durations,\npronunciation, and vibrato.\nSpeech recognition in singing can be used in many in-\nteresting practical applications, such as automatic lyrics-\nto-music alignment, keyword spotting in songs, language\nidentiﬁcation of musical pieces or lyrics transcription.\nA ﬁrst step in many of these tasks is the recognition\nof phonemes in the audio recording. We showed in [9]\nthat phoneme recognition is a bottleneck in tasks such as\nc/circlecopyrtAnna M. Kruspe. Licensed under a Creative Commons\nAttribution 4.0 International License (CC BY 4.0). Attribution: Anna\nM. Kruspe. “Bootstrapping a system for phoneme recognition and key-\nword spotting in unaccompanied singing”, 17th International Society for\nMusic Information Retrieval Conference, 2016.language identiﬁcation and keyword spotting in singing.\nOther publications also demonstrate that phoneme reco-\ngnition on singing is more difﬁcult than on speech [15] [5]\n[12]. This is further compounded by the fact that models\nare usually trained on pure speech data.\nAs shown on a small scale in [5] and [9], recognition\ngets better when singing is used as part of the training\ndata. This has so far not been done comprehensively due\nto the lack of singing data sets annotated with phonemes or\nwords.\nIn this paper, we present a new approach to training\nacoustic models on actual singing data. This is done by\nﬁrst assembling the data from a set of recordings of unac-\ncompanied singing and the matching textual lyrics. These\nlyrics are then automatically aligned to the audio data using\nmodels trained solely on speech. Next, the resulting an-\nnotated data sets are used to train new acoustic models for\nphoneme recognition in singing. We then evaluate the pho-\nneme recognition results on different subsets of the singing\ncorpus and on an unrelated data set of vocal tracks. Finally,\nwe also use the recognized phonemes to perform keyword\nspotting.\nThis paper is structured as follows: We ﬁrst present the\nstate of the art in section 2 and the data sets in section 3.\nThen, we describe our proposed approach in more detail in\nsection 4. The experiments and their results are presented\nin sections 5 and 6. Finally, we give a conclusion in section\n7 and make suggestions for future experiments in section\n8.\n2. STATE OF THE ART\n2.1 Phoneme recognition in singing\nAs described in [12], [2], and [9], there are signiﬁcant dif-\nferences between speech and singing audio, such as pitch\nand harmonics, vibrato, phoneme durations and pronuncia-\ntion. These factors make phoneme recognition on singing\nmore difﬁcult than on speech. It has only been a topic of\nresearch for the past few years.\nFujihara et al. ﬁrst presented an approach using Pro-\nbabilistic Spectral Templates to model phonemes in [3].\nThe phoneme models are gender-speciﬁc and only model\nﬁve vowels, but also work for singing with instrumental ac-\ncompaniment. The best result is 65% correctly classiﬁed\nframes.\nIn [4], Gruhne et al. describe a classical approach that\nemploys feature extraction and various machine learning358algorithms to classify singing into 15 phoneme classes.\nIt also includes a step that removes non-harmonic com-\nponents from the signal. The best result of 58% correctly\nclassiﬁed frames is achieved with Support Vector Machine\n(SVM) classiﬁers. The approach is expanded upon in [17].\nMesaros presented a complex approach that is based\non Hidden Markov Models which are trained on Mel-\nFrequency Cepstral Coefﬁcients (MFCCs) and then adap-\nted to singing using three phoneme classes separately [15]\n[14]. The approach also employs language modeling and\nhas options for vocal separation and gender and voice ad-\naptation. The achieved phoneme error rate on unaccompa-\nnied singing is 1.06without adaptation and 0.8with sin-\nging adaptation using 40 phonemes (the error rate greater\nthan one means that there were more insertion, deletion,\nor substitution errors than phoneme instances). The results\nalso improve when using gender-speciﬁc adaptation (to an\naverage of 0.81%) and even more when language modeling\nis included (to 0.67%).\nHansen presents a system in [5] which combines the\nresults of two Multilayer Perceptrons (MLPs), one using\nMFCC features and one using TRAP (Temporal Pattern)\nfeatures. Training is done with a small amount of singing\ndata. Viterbi decoding is then performed on the resulting\nposterior probabilities. On a set of 27 phonemes, this ap-\nproach achieves a recall of up to 48%.\nFinally, we trained new models for phoneme recogni-\ntion in singing by modifying speech data to make it more\n“song-like” [11]. We employed time-stretching, pitch-\nshifting, and vibrato generation algorithms. Using a model\ntrained on speech data with all three modiﬁcations, we ob-\ntained 18% correctly classiﬁed frames ( 6%improvement)\nand a weighted phoneme error rate of 0.71(0.06improve-\nment).\nGenerally, comparing the existing approaches is not tri-\nvial since different datasets, different phoneme sets, and\ndifferent evaluation measures are used.\n2.2 Keyword spotting in singing\nA ﬁrst approach to keyword spotting in singing was presen-\nted in [9]. This approach employs keyword-ﬁller HMMs\nwhich detect the keyword. The recognition is performed\non phoneme posteriorgrams, which were generated with\nacoustic models trained on speech. We obtained F1mea-\nsures of 33% for spoken lyrics and 24% for a-capella sin-\nging. Using post-processing techniques on the posterior-\ngrams, the a-capella result was improved up to 27%.\nIn [10], we improved upon this result by employing\nphoneme duration modeling algorithms. The best result\non a-capella singing was an F1measure of 39%.\nIn [1], HMM models and position-HMM-DBNs were\nemployed to search for certain phrases of lyrics in traditio-\nnal Turkish music. The approach obtained an F1measure\nof13% for the 1-best result.3. DATA SETS\n3.1 Speech data sets\nFor training our baseline phoneme recognition models, we\nused the well-known Timit speech data set [7]. Its trai-\nning section consists of 4620 phoneme-annotated English\nutterances spoken by native speakers. Each utterance is a\nfew seconds long.\nAdditionally, we also trained phoneme models on a mo-\ndiﬁcation of Timit where pitch-shifting, time-stretching,\nand vibrato were applied to the audio data. This process\nwas described in [11]. The data set will be referred to as\nTimitM .\n3.2 Singing data sets\n3.2.1 Damp\nFor training models speciﬁc to singing, we used the DAMP\ndata set, which is freely available from Stanford Univer-\nsity1[16]. This data set contains more than 34,000 re-\ncordings of amateur singing of full songs with no back-\nground music, which were obtained from the Smule Sing!\nkaraoke app. Each performance is labeled with metadata\nsuch as the gender of the singer, the region of origin, the\nsong title, etc. The singers performed 301 English lan-\nguage pop songs. The recordings have good sound quality\nwith little background noise, but come from a lot of diffe-\nrent recording conditions.\nNo lyrics annotations are available for this data set, but\nwe obtained the textual lyrics from the Smule Sing! web-\nsite2. These were, however, not aligned in any way. We\nperformed such an alignment on the word and phoneme\nlevels automatically (see section 4.1).\nOut of all those recordings, we created several different\nsub-data sets:\nDampB Contains 20 full recordings per song (6000 in\nsum), both male and female.\nDampBB Same as before, but phoneme instances were\ndiscarded until they were balanced and a maximum\nof 250,000 frames per phoneme where left, where\npossible. This data set is about 4% the size of\nDampB .\nDampBB small Same as before, but phoneme instances\nwere discarded until they were balanced and 60,000\nframes per phoneme were left (a bit fewer than the\namount contained in Timit ). This data set is about\nhalf the size of DampBB .\nDampFB and DampMB Using 20 full recordings per\nsong and gender (6000 each), these data sets were\nthen reduced in the same way as DampBB .DampFB\nis roughly the same size, DampMB is a bit smaller\nbecause there are fewer male recordings.\nDampTestF and DampTestM Contains one full recor-\nding per song and gender (300 each). These data\nsets were used for testing. There is no overlap with\nany of the training data sets.\n1https://ccrma.stanford.edu/damp/\n2http://www.smule.com/songsProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 359# Keywords\n2 eyes\n3 love, away, time, life, night\n4never, baby, world, think, heart,\nonly, every\n5 always, little\nTable 1 : All 15 tested keywords, ordered by number of\nphonemes.\nOrder-13 MFCCs plus deltas and double-deltas were ex-\ntracted from all data sets and used in all experiments.\n3.2.2 Acap\nWe also ran some tests on a small data set of the vocal\ntracks of 15 pop songs, which were hand-annotated with\nphonemes and words. This data set was ﬁrst presented\nin [5]. Despite the small size, we provide results on this\ndata set for comparison with our previous approaches, and\nbecause the ground truth annotations can be assumed to be\ncorrect (in contrast with the automatically generated anno-\ntations of the Damp -based data sets).\n3.3 Keywords\nFrom the 301 different song lyrics of the Damp data sets,\n15 keywords were chosen by semantic content and fre-\nquency to test our keyword spotting algorithm. Each key-\nword occurs in at least 50 of the 301 songs. The keywords\nare shown in table 1.\n4. PROPOSED APPROACH\n4.1 Lyrics alignment\nSince the textual lyrics were not aligned to the singing au-\ndio data, we ﬁrst performed an automatic alignment step.\nA monophone HMM acoustic model trained on Timit using\nHTK was used. Alignment was performed on the word and\nphoneme levels. This is the same principle of so-called\n“Forced Alignment” that is commonly used in Automatic\nSpeech Recognition [8] (although it is commonly done on\nshorter utterances). We hand-checked some examples and\nfound the alignment to already be very good over-all. Of\ncourse, errors cannot be avoided when doing automatic for-\nced alignment. We considered repeating this process with\nthe newly trained models, but preliminary tests suggested\nthat this would not improve the alignments very much.\nThe resulting annotations were used in the following ex-\nperiments. This approach provided us with a large amount\nof annotated singing data, which could not feasibly have\nbeen done manually.\n4.2 New acoustic models\nUsing these automatically generated annotations, we\nthen trained new acoustic models on DampB ,DampBB ,\nDampBB small ,DampFB , and DampMB . Models were\nalso trained on Timit andTimitM .\nAll models are DNNs with three hidden layers of 1024,\n850, and again 1024 dimensions with a sigmoid activationfunction. The output layer corresponds to 37 monophones.\nInputs are either frame-wise MFCCs (39 dimensions) or\nMFCCs with 4 context frames on either side (351 dimen-\nsions).\n4.3 Phoneme recognition and evaluation\nUsing these models, phoneme posteriorgrams were then\ngenerated on the test data sets ( DampTestF ,DampTestM ,\nandAcap ). For all non-gender dependent acoustic models,\nresults over both of the DampTest sets were averaged.\nThe recognized phonemes were then evaluated using\nthe percentage of correct frames, the phoneme error rate,\nand the weighted phoneme error rate as evaluation measu-\nres (see [11]). In the case of the DampTest data sets, the\nresults were compared to the automatic alignment results,\nwhich is a potential source of error.\n4.4 Keyword spotting and evaluation\nFinally, the phoneme posteriorgrams were evaluated for\nkeyword spotting. A keyword-ﬁller HMM algorithm was\nemployed. Keyword-ﬁller HMMs consist of two sub-\nHMMs: One to model the keyword and one to model eve-\nrything else (=ﬁller). The keyword HMM has a simple\nleft-to-right topology with one state per keyword phoneme.\nThe ﬁller HMM is a fully connected loop of all phone-\nmes. When the Viterbi path with the highest likelihood\npasses through the keyword HMM rather than the ﬁller\nloop, the keyword is detected. We previously employed\nthis approach in [9]. However, the evaluation data set ba-\nsed on Damp is a different, much bigger set of recordings.\nThe keyword set was changed to better reﬂect frequently\noccurring words in these songs. Additionally, the keyword\ndetection was performed on whole songs, which may be\nmore realistic for practical applications. For comparison,\nresults on our old data set ( Acap ) for whole songs are also\nprovided below. Song-wise F1measures were calculated\nfor evaluation.\n5. PHONEME RECOGNITION EXPERIMENTS\nAND RESULTS\n5.1 Experiment A: Comparison of models trained on\nTimit andDamp data sets\nIn our ﬁrst experiments, we generated phoneme posterior-\ngrams on the data sets DampTestF andDampTestM using\nthe models trained on the two variants of Timit and on the\nthree differently-sized Damp training sets that were not\nsplit by gender. The results are averaged over both sets.\nFor comparison, we also generated these posteriorgrams\nonAcap . The results for the DampTest sets in terms of per-\ncentage of correct frames, phoneme error rate, and weigh-\nted phoneme error rate are shown in ﬁgure 1.\nModels trained on the modiﬁed version of Timit already\nshow some improvement over plain Timit [11], but even\nthe small Damp training set improves the result signiﬁ-\ncantly more. As mentioned before, this data set is actually\nsmaller than Timit . The percentage of correct frames rises360 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016(a) Percentage of correct frames\n (b) Phoneme error rate\n (c) Weighted phoneme error rate\nFigure 1 : Evaluation measures for the results obtained on the DampTest data sets using models trained on Timit and on\nvarious Damp -based data sets.\nfrom 13% to19%, the phoneme error rate sinks from 1.27\nto1.04, and the weighted phoneme error rate from 0.9to\n0.77.\nWhen the whole set of 6000 recordings is used for trai-\nning ( DampB ), the percentage of correct frames even rises\nto23%, while the phoneme error rate falls to 0.8and the\nweighted phoneme error rate to 0.6. When using the smal-\nler, more balanced version ( DampBB ), these results are so-\nmewhat worse, but not much, with 23% correct frames, a\nphoneme error rate of .86, and a weighted phoneme error\nrate of 0.65. This is particularly interesting because this\ndata set is only 4% the size of the bigger one and training\nis therefore much faster.\nThe results on the Acap data set show a similar impro-\nvement, but are better in general. The percentage of cor-\nrectly classiﬁed frames jumps from 12% to22%,25%, and\n27% forDampBB small ,DampB , and DampBB respec-\ntively. The weighted phoneme error rate sinks from 0.8\nto0.69,0.61, and 0.56. Since this data set is has been an-\nnotated by hand and is completely different material from\nthe training data sets, we are conﬁdent that our approach is\nable to model the properties of each phoneme, rather than\nreproducing the model that was used for aligning the sin-\nging training sets. The somewhat better values might be\ncaused by these more accurate annotations, too, or by the\nfact that these are recordings of professional singers who\nenunciate more clearly.\n5.2 Experiment B: Inﬂuence of context frames\nWe then ran the same experiment again, but this time used\nmodels that were trained with 4 context frames on either\nside of each input frame. This provides more long-term\ninformation. The results are shown in ﬁgure 2. (In each\nﬁgure, the “No context” part is the result from the previous\nexperiment).\nSurprisingly, using context frames did not improve the\nresult in any case except for the DampBB small models.\nSince this is the smallest data set, this improvement might\nhappen just because the context frames virtually provide\nmore training data for each phoneme. In the other cases,\nthere already seems to be a sufﬁcient amount of training\ndata and the context frames may blur the training data ins-tead of providing more information about the context of\neach phoneme. Additionally, it is possible that this ap-\nproach compounds error that were made in the automatic\nalignment in the case of the bigger Damp training data sets.\nThe same effect can be observed when calculating these\nvalues on the hand-annotated Acap test data set. We there-\nfore decided to not employ context frames in the following\nexperiments. This also speeds up the training process.\n5.3 Experiment C: Comparison of gender-dependent\nmodels\nFinally, we generated phoneme posteriorgrams using mo-\ndels that were only trained on recordings of the same gen-\nder. I.e., for phoneme recognition on the DampTestF set,\nwe used a model trained only on female singing recordings\n(DampFB ). The results are shown in ﬁgure 3. (Note that\nthe results for DampB andDampBB are different from the\nprevious experiments because the test data sets were split\nby gender).\nSurprisingly, the results do not improve when using\ngender-speciﬁc acoustic models. The percentage of cor-\nrect frames, when compared to the results using the mo-\ndels trained on the DampBB drops sligthly from 23% to\n21% for the female test set, and stays at 23% for the male\none. The weighted phoneme error rate rises from 0.65to\n0.68and from 0.65to0.69for the female and male test\nsets respectively.\nThis might happen because the training data sets are\nslightly smaller, but, more likely, because some variation in\nthe singing voices might be lost when using training data\nof only one gender. In singing, pitches cover a broader\nrange than in speech. This effect might take away some of\nthe improvement usually seen in speech recognition when\nusing gender-speciﬁc models.\n6. KEYWORD SPOTTING EXPERIMENTS AND\nRESULTS\n6.1 Experiment D: Comparison of models trained on\nTimit andDamp data sets\nWe then performed keyword spotting on the phoneme pos-\nteriorgrams from Experiment A. The results in terms of F1Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 361(a) Percentage of correct frames\n (b) Phoneme error rate\n (c) Weighted phoneme error rate\nFigure 2 : Evaluation measures for the results obtained on the DampTest data sets using models trained on Timit and on\nvarious Damp -based data sets with no context and with 8 context frames.\n(a) Percentage of correct frames\n (b) Phoneme error rate\n (c) Weighted phoneme error rate\nFigure 3 : Evaluation measures for the results obtained on the DampTestM andDampTestF data sets using models trained\nonDamp -based data sets, mixed and split by gender.\nmeasure across the whole DampTest sets are shown in ﬁ-\ngure 4a. Figure 4b show the results of the same experiment\non the small Acap data set.\nAcross all keywords, we obtain a document-wise F1\nmeasure of 0.35using the posteriorgrams generated with\ntheTimit model on the DampTest data sets. This result\nis slightly higher for the TimitM models and rises to 0.45\nusing the model trained on the small DampBB small sin-\nging data set. Surprisingly, the model trained on DampBB\nis only slightly better than the much smaller one. Using the\nvery big DampB training data set, the F1measure reaches\n0.47.\nOn the hand-annotated Acap test set, the difference is\neven more pronounced, rising from 0.29for the Timit mo-\ndel to 0.5forDampB . This might, again, be caused by the\nmore accurate annotations or by the higher-quality singing.\nAdditionally, the data set is much smaller with fewer oc-\ncurrences of each keyword, which could emphasize both\npositive and negative tendencies in the detection.\n6.2 Experiment E: Comparison of gender-dependent\nmodels\nWe also performed keyword spotting on the posteriorgrams\ngenerated with the gender-dependent models from Experi-\nment C. The results are shown in ﬁgure 5.\nIn contrast to the phoneme recognition resultes from\nExperiment C, the gender-dependent models perform\nslightly better for keyword spotting than the mixed one of\nthe same size, and almost as good as the one trained on\nmuch more data ( DampB ). The F1measures for the fe-\nmale test set are 0.48for the DampB model, 0.45for the\n(a)F1measures for keyword\nspotting results on the Damp-\nTestdata sets.\n(b) Keyword spotting results\non the Acap data set.\nFigure 4 :F1measures for keyword spotting results using\nposteriorgrams generated with various acoustic models.\nDampBB model, and 0.46for the DampFB model. For the\nmale test set, they are 0.46and0.45for the ﬁrst two, and\n0.46for the DampMB model.\n6.3 Experiment F: Individual analysis of keyword\nresults\nFigure 6 shows the individual F1measures for each key-\nword using the best model ( DampB ), ordered by their oc-\ncurrence in the DampTest sets from high to low (i.e. num-\nber of songs which include the song). There appears to\nbe a tendency for more frequent keywords to be detected\nmore accurately. This happens because a high recall is of-\nten achievable, while the precision depends very much on\nthe accuracy of the input posteriorgrams. The more fre-\nquent a keyword, the easier it also becomes to achieve a\nhigher precision for it.362 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Figure 5 :F1measures for keyword spotting results on\ntheDampTestM andDampTestF data sets using mixed and\ngender-dependent models.\nFigure 6 : Individual F1measures for the results for each\nkeyword, using the acoustic model trained on DampB .\nAs shown in literature [18], the detection accuracy also\ndepends on the length of the keyword: Keywords with\nmore phonemes are usually easier to detect. This might ex-\nplain the relative peak for “every”, “little”, and “always”,\nin contrast to “eyes” or “world”. Since keyword detection\nsystems tend to perform better for longer words and most\nof our keywords only have 3 or 4 phonemes, this result is\nespecially interesting.\nOne potential source of error are sequences of phone-\nmes that overlap with our keywords, but are not included\nin the calculation of the precision. Equally spelled words\nwere included, but split phrases or other spellings were not\n(e.g. “away” as part of “castaway” would be counted, but\n“a way” would not be counted as “away”). This might arti-\nﬁcially lower our results and we will look into possibilities\nfor improvement in the future. Additionally, only one pro-\nnunciation for each keyword was provided, but there may\nbe several possible.\n7. CONCLUSION\nIn this paper, we trained new acoustic models on a large\ncorpus of unaccompanied singing recordings. Since no an-\nnotations for these existed, we ﬁrst had to automatically\nalign lyrics to them. The new models could then directly\nbe trained on these automatic annotations. To our know-\nledge, this has not been done before for singing.\nWe trained three different models with mixed gender\nrecordings: One on 6000 full recordings of 301 songs, one\non just 4% of this data, and one which was balanced by\nphonemes and is roughly half the size of the medium-sizedone. We then tested their performance on two other subsets\nof the same corpus which did not overlap with the training\ndata, and on a small unrelated data set of commercial vocal\ntracks which were hand-annotated.\nIn all cases, the three new models showed a strong im-\nprovement over those trained only on speech. Even the\nmodel trained on the smallest set produced a jump in cor-\nrectly classiﬁed frames from 13% to19%, and in weighted\nphoneme error rate from 0.9to0.77on the large test set.\nWith the model trained on the medium-sized data set, we\nobtained 23% correct frames and a weighted phoneme er-\nror rate of 0.65. With the biggest one, the weighted pho-\nneme error rate falls to 0.6. The results are similar on the\nsmall hand-annotated test set.\nWe also tested acoustic models with 8 context frames,\nand models trained on gender-speciﬁc data. Neither of\nthem showed improvement over the ﬁrst ones.\nWe then performed keyword spotting for 15 keywords\non phoneme posteriorgrams generated with these new mo-\ndels using a keyword-ﬁller approach. The resulting F1\nmeasure rises from 0.35for the models trained on speech\nto0.47for our new models. This result is especially inte-\nresting because most of the keywords have few phonemes.\nFor keyword spotting, gender-dependent models perform\nslightly better than mixed-gender models of the same size.\n8. FUTURE WORK\nSo far, we have only tested this approach using MFCC fea-\ntures. As shown in our previous experiments [9], other\nfeatures like TRAP or PLP may work better on singing.\nSo-called log-mel ﬁlterbank features have also been used\nsuccessfully with DNNs [6]. Another interesting factor is\nthe size and conﬁguration of the classiﬁers, of which we\nhave only tested one so far. Since the alignment appears\nto provide valid training data, we believe the features and\nmodel conﬁguration could be the greatest sources of im-\nprovement.\nWe showed that there is only a slight amount of impro-\nvement between the model trained on all 6000 songs and\nthe one trained only on 4%of this data. It would be interes-\nting to ﬁnd the exact point at which additional training data\ndoes not further improve the models. On the evaluation\nside, a keyword spotting approach that allows for pronun-\nciation variants or sub-words may produce better results.\nLanguage modeling might also help to alleviate some of\nthe errors made during phoneme recognition.\nThese models have not yet been applied to singing with\nbackground music, which would be interesting for practi-\ncal applications. Since this would probably decrease the\nresult when used on big, unlimited data sets, more speci-\nﬁed systems would be more manageable, e.g. for speciﬁc\nmusic styles, sets of songs, keywords, or specialized ap-\nplications. Searching for whole phrases instead of short\nkeywords could also make the results better usable in prac-\ntice.\nAs shown in [13] and [2], alignment of textual lyrics\nand singing already works well. A combined approach that\nalso employs textual information could be very practical.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 3639. REFERENCES\n[1] G. Dzhambazov, S. Sent ¨urk, and X. Serra. Searching\nlyrical phrases in a-capella turkish makam recordings.\nInProceedings of the 16th International Conference on\nMusic Information Retrieval (ISMIR) , Malaga, Spain,\n2015.\n[2] H. Fujihara and M. Goto. Multimodal Music Proces-\nsing, chapter Lyrics-to-audio alignment and its appli-\ncations. Dagstuhl Follow-Ups, 2012.\n[3] H. Fujihara, M. Goto, and H. G. Okuno. A novel fra-\nmework for recognizing phonemes of singing voice in\npolyphonic music. In WASPAA , pages 17–20. IEEE,\n2009.\n[4] M. Gruhne, K. Schmidt, and C. Dittmar. Phoneme re-\ncognition on popular music. In Proceedings of the 8th\nInternational Conference on Music Information Retrie-\nval (ISMIR) , Vienna, Austria, September 2007.\n[5] J. K. Hansen. Recognition of phonemes in a-cappella\nrecordings using temporal patterns and mel frequency\ncepstral coefﬁcients. In 9th Sound and Music Compu-\nting Conference (SMC) , pages 494–499, Copenhagen,\nDenmark, 2012.\n[6] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed,\nN. Jaitly, A. Senior, V . Vanhoucke, P. Nguyen, T. Sai-\nnath, and B. Kingsbury. Deep Neural Networks for\nAcoustic Modeling in Speech Recognition. Signal Pro-\ncessing Magazine , 2012.\n[7] J. S. Garofolo et al. TIMIT Acoustic-Phonetic Con-\ntinuous Speech Corpus. Technical report, Linguistic\nData Consortium, Philadelphia, 1993.\n[8] D. Jurafsky and J. H. Martin. Speech and language pro-\ncessing: An introduction to Natural Language Proces-\nsing, Computational Linguistics, and Speech Recogni-\ntion. Prentice Hall, 2009.\n[9] A. M. Kruspe. Keyword spotting in a-capella singing.\nIn15th International Conference on Music Information\nRetrieval (ISMIR) , Taipei, Taiwan, 2014.\n[10] A. M. Kruspe. Keyword spotting in a-capella singing\nwith duration-modeled HMMs. In EUSIPCO , Nice,\nFrance, 2015.\n[11] A. M. Kruspe. Training phoneme models for singing\nwith ”songiﬁed” speech data. In 15th International\nConference on Music Information Retrieval (ISMIR) ,\nMalaga, Spain, 2015.\n[12] A. Loscos, P. Cano, and J. Bonada. Low-delay singing\nvoice alignment to text. In Proceedings of the ICMC ,\n1999.\n[13] A. Mesaros and T. Virtanen. Automatic alignment of\nmusic audio and lyrics. In DaFX-08 , Espoo, Finland,\n2008.[14] A. Mesaros and T. Virtanen. Automatic recognition of\nlyrics in singing. EURASIP J. Audio, Speech and Music\nProcessing , 2010, 2010.\n[15] A. Mesaros and T. Virtanen. Recognition of phonemes\nand words in singing. In ICASSP , pages 2146–2149.\nIEEE, 2010.\n[16] J. C. Smith. Correlation analyses of encoded music\nperformance . PhD thesis, Stanford University, 2013.\n[17] G. Szepannek, M. Gruhne, B. Bischl, S. Krey, T. Har-\nczos, F. Klefenz, C. Dittmar, and C. Weihs. Classiﬁ-\ncation as a tool for research , chapter Perceptually Ba-\nsed Phoneme Recognition in Popular Music. Springer,\nHeidelberg, 2010.\n[18] A. J. K. Thambiratnam. Acoustic keyword spotting in\nspeech with applications to data mining . PhD thesis,\nQueensland University of Technology, 2005.364 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Melody Extraction on Vocal Segments Using Multi-Column Deep Neural Networks.",
        "author": [
            "Sangeun Kum",
            "Changheun Oh",
            "Juhan Nam"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1414788",
        "url": "https://doi.org/10.5281/zenodo.1414788",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/119_Paper.pdf",
        "abstract": "Singing melody extraction is a task that tracks pitch con- tour of singing voice in polyphonic music. While the ma- jority of melody extraction algorithms are based on com- puting a saliency function of pitch candidates or sepa- rating the melody source from the mixture, data-driven approaches based on classification have been rarely ex- plored. In this paper, we present a classification-based approach for melody extraction on vocal segments us- ing multi-column deep neural networks. In the proposed model, each of neural networks is trained to predict a pitch label of singing voice from spectrogram, but their outputs have different pitch resolutions. The final melody contour is inferred by combining the outputs of the networks and post-processing it with a hidden Markov model. In order to take advantage of the data-driven approach, we also aug- ment training data by pitch-shifting the audio content and modifying the pitch label accordingly. We use the RWC dataset and vocal tracks of the MedleyDB dataset for train- ing the model and evaluate it on the ADC 2004, MIREX 2005 and MIR-1k datasets. Through several settings of experiments, we show incremental improvements of the melody prediction. Lastly, we compare our best result to those of previous state-of-the-arts.",
        "zenodo_id": 1414788,
        "dblp_key": "conf/ismir/KumON16",
        "content": "MELODY EXTRACTION ON VOCAL SEGMENTS USING\nMULTI-COLUMN DEEP NEURAL NETWORKS\nSangeun Kum, Changheun Oh, Juhan Nam\nGraduate School of Culture Technology\nKorea Advanced Institute of Science and Technology\n{keums, thecow, juhannam }@kaist.ac.kr\nABSTRACT\nSinging melody extraction is a task that tracks pitch con-\ntour of singing voice in polyphonic music. While the ma-\njority of melody extraction algorithms are based on com-\nputing a saliency function of pitch candidates or sepa-\nrating the melody source from the mixture, data-driven\napproaches based on classiﬁcation have been rarely ex-\nplored. In this paper, we present a classiﬁcation-based\napproach for melody extraction on vocal segments us-\ning multi-column deep neural networks. In the proposed\nmodel, each of neural networks is trained to predict a pitch\nlabel of singing voice from spectrogram, but their outputs\nhave different pitch resolutions. The ﬁnal melody contour\nis inferred by combining the outputs of the networks and\npost-processing it with a hidden Markov model. In order to\ntake advantage of the data-driven approach, we also aug-\nment training data by pitch-shifting the audio content and\nmodifying the pitch label accordingly. We use the RWC\ndataset and vocal tracks of the MedleyDB dataset for train-\ning the model and evaluate it on the ADC 2004, MIREX\n2005 and MIR-1k datasets. Through several settings of\nexperiments, we show incremental improvements of the\nmelody prediction. Lastly, we compare our best result to\nthose of previous state-of-the-arts.\n1. INTRODUCTION\nMelody is a pitch sequence with which one might hum\nor whistle a piece of polyphonic music in an identiﬁable\nmanner [10]. Among others, singing voice has been used\nas a main source of the melody, particularly in popular\nmusic. Thus, extracting melodies from singing voice can\nbe used for not only music retrieval, for example, query-\nby-humming [5] or cover song identiﬁcation [16] but also\nvoice separation as a guide to inform the voice source.\nA number of melody extraction algorithms, which can\nbe applied for singing voice with an additional voice de-\ntection step, have been proposed so far and they are well\nsummarized in [13]. The majority of the algorithms are\nc/circlecopyrtSangeun Kum, Changheun Oh, Juhan Nam. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Sangeun Kum, Changheun Oh, Juhan Nam. “Melody\nExtraction on V ocal Segments Using Multi-Column Deep Neural Net-\nworks”, 17th International Society for Music Information Retrieval Con-\nference, 2016.based on computing a saliency function of pitch candidates\nor separating the melody source from the mixture. They\ntypically return melody as a continuous pitch stream. On\nthe other hand, data-driven approaches based on classiﬁ-\ncation, which categorizes melody into a ﬁnite set of pitch\nlabels, have rarely been explored. An early work by Ellis\nand Poliner used a support vector machine classiﬁer to pre-\ndict a pitch label from spectrogram [7]. Recently, Bittner\net. al. proposed a method using a random forest classiﬁer\nthat predicts a pitch contour from highly hand-crafted fea-\ntures [3]. To the best of our knowledge, no other attempts\nhave been made so far.\nThis scarcity of classiﬁcation-based approach might be\nattributed to the following limitations. First, the extracted\nmelody is supposed to be quantized by the pitch catego-\nrization (e.g. semitone unit in [7]). While this discrete\noutcome may be useful for some applications that require\na MIDI-level pitch notation, it loses detailed information\nabout singing styles, for example, vibrato or note-to-note\ntransition patterns. Second, the data-driven approach typ-\nically requires sufﬁcient labeled training data to achieve\ngood performance. Finer pitch resolutions may need even\nmore training data and possibly more complicated classi-\nﬁers that can handle it.\nIn this paper, we address these limitations of the\nclassiﬁcation-based approach using multi-column deep\nneural networks (MCDNN). In the proposed model, each\nof DNN is trained to predict a pitch label of singing voice\nwith different pitch resolutions. The outputs of the net-\nworks are combined and post-processed with a hidden\nMarkov model to produce the ﬁnal melody contour. Given\na single DNN and training data, we observed that perfor-\nmance is inversely proportional to pitch resolutions. By\ncombining the multiple DNNs, we show that the model can\nachieve higher pitch resolutions and better performance at\nthe same time. In addition, we augment the training data\nby pitch-shifting the audio content and modifying the pitch\nlabel accordingly. We show that this is an effective tech-\nnique to improve classiﬁcation performance of the model.\n2. RELATED WORK\nThe MCDNN was originally devised as an ensemble\nmethod to improve the performance of DNN for image\nclassiﬁcation [4]. In this model, each column (or single\nDNN) share the same network conﬁguration and training819data. However, they are randomly initialized, and the in-\nput data may be preprocessed in different ways for each\ncolumn. The predictions from all columns are averaged to\nproduce the ﬁnal output. The multi-column approach was\napplied to image denoising as well [1]. In this approach,\neach column is trained on a different type of noise, and the\noutputs are adaptively weighted to handle a variety of noise\ntypes. Our proposed model may pose half-way between\nthese two approaches. Each column is trained to conduct a\ndifferent role, having a different number of outputs. How-\never, we combine the outputs with even weights as they are\nthe same pitch quantity with different resolutions.\nAs aforementioned, classiﬁcation-based melody extrac-\ntion is rarely attempted. Among them, our proposed model\nis similar to the SVM approach by Ellis and Poliner [7] in\nthat both of them predict a pitch label from spectrogram\nusing a classiﬁer and a hidden Markov model for post-\nprocessing. However, our model produces a ﬁner pitch\nresolution. Also, we take advantage of deep neural net-\nworks, which recently has proved to be capable of having\ngreat performance with sufﬁcient labeled data and comput-\ning power.\n3. PROPOSED METHODS\n3.1 Multi-Column Deep Neural Networks\nOur architecture of the MCDNN is illustrated in Figure 1.\nEach of the DNN columns takes an odd-numbered spec-\ntrogram frames as input to capture contextual information\nfrom neighboring frames and predicts a pitch label at the\ncenter position of the context window. The DNNs are con-\nﬁgured with three hidden layers and ReLUs for the non-\nlinear function in common, but the output layers predict a\npitch label with different resolutions. The lowest resolu-\ntion is semitone, corresponding to the leftmost one. The\nnext ones progressively have higher resolutions by two\ntimes (e.g. 0.5 semitones, 0.25 semitones, ...), thereby\nhaving as much pitch labels as the increased resolutions.\nGiven the outputs of the columns, we compute the com-\nbined posterior as follows:\nyN\nMCDNN =N/productdisplay\ni=1(yi\nDNN +/epsilon1) (1)\nwhereyi\nDNN corresponds to the prediction from ithcol-\numn DNN, and Ncorresponds to the number of total\ncolumns. We use multiplication in a maximum-likelihood\nsense, assuming that the column DNNs are independent.\nWe add a small value, /epsilon1to prevent numerical underﬂow.\nNote that, before combining the predictions, those with\nlower resolutions are actually expanded by locally repli-\ncating each element so that the output sizes are the same\nfor all columns. For example, the leftmost DNN in Fig-\nure 1, which predicts pitch in semitone, expands the output\nvector by a factor of 4. As a result, the merged posterior\nmaintains the highest pitch resolution.\nFigure 1 : Block diagram of our proposed multi-column\ndeep neural networks for singing melody extraction\n3.2 Data Augmentation\nRecent advances in deep learning are attributed to the\navailability of large-scale labeled data among others. Con-\nsidering that melody-labeled public datasets are not much\navailable, and manual labeling is laborious, it is desirable\nto augment existing datasets. In our experiments, we aug-\nment our training set by changing the global pitch of the\naudio content. Instead of pitch-shifting by resampling [10],\nwhich carries out time-stretching at the same time, we use\na phase-vocoder method approach to achieve more natu-\nral transposition [9]. Pitch shifting proved to be an effec-\ntive method of data augmentation for singing voice detec-\ntion [15]. We will show that it works for singing melody\nextraction as well. On top of this, we also augment the\ntraining data by simply using an extra dataset that covers\nmore music genres, as melody characteristics are quite dis-\ncriminative over different music genres [14].\n3.3 Temporal Smoothing by HMM\nAlthough the MCDNN is trained to capture contextual in-\nformation by taking multiple frames as input data, this may\nbe limited to learn long-term temporal dependencies that\nappear on the pitch contours of singing voices. Also, the\nprediction is performed independently every time step. In\norder to incorporate the sequential structure further, we\nconduct temporal smoothing for the combined output of\nthe MCDNN using HMM. We implemented the HMM, fol-\nlowing the procedure in [7].\n3.4 Singing Voice Detection\nThe MCDNN is trained with only voiced frames for pitch\nclassiﬁcation. Therefore, a separate singing voice detec-\ntion step is necessary for the test phase. However, since\nsinging voice detection itself is a challenging task and not820 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016our main concern in this paper, we evaluate the test data\nusing two scenarios. In the ﬁrst scenario, we assume that a\nperfect singing voice detector is available so that we focus\non the performance of our model only on voiced frames. In\nthe second scenario, we use a simple energy-based singing\nvoice detector introduced in [7]. The detector sums spec-\ntral energy between 200 Hz and 1800 Hz where the singing\nvoice is likely to have a higher level than background mu-\nsic. The sum is normalized by the median energy in the\nband, and a threshold is used to determine the presence\nof singing voice. We expect that the performance of our\nmodel will range between the results from the two scenar-\nios if a better singing voice detector is available.\n4. DATASETS\n4.1 Training Datasets\nWe use the RWC pop music database as our main train-\ning set [8]. It contains 100 popular songs with singing\nvoice melody annotations. We divide the database into two\nsplits, 85 songs for training and the remaining 15 songs for\nvalidation. In order to avoid bias by gender and the num-\nber of singers, we select the songs such that male/female\nsingers and solo/chorus singing are evenly distributed over\nthe training and validation sets. We also prevent the same\nsinger’s songs from being split over the two sets so that\nsinger voices in the validation stage are never heard. In or-\nder to train the MCDNN more effectively, we augment the\ntraining set by applying pitch-shifting by ±1,2semitones.\nThis increases the amount of the training set by ﬁve times.\nAlso, we modify the corresponding pitch label accordingly.\nSince the RWC database includes only pop music, the\nmodel trained on the set may not work well for other gen-\nres. We thus increase the size of training set and genre di-\nversity by using 60 vocal tracks of the MedleyDB dataset\nas an additional training set [3].\n4.2 Test Datasets\nWe examine our proposed model with three publicly\navailable datasets: ADC2004, MIREX05, and MIR1k.\nDue to the limited accessibility to the datasets1and the\nlimitation of our model that can handle singing voice\nonly, we test them with several options. Speciﬁcally,\nthe ADC2004 dataset includes some instrumental pieces\nwhere the melody is played by saxophones or other mu-\nsical instruments. The MIREX05 dataset we obtained has\nonly 13 out of the total 25 songs. Furthermore, only 9\nof the 13 songs contain singing voice. For these reasons,\nwe evaluate our model on all songs and those with singing\nvoices separately for the two sets.\nWe report various evaluation metrics for melody ex-\ntraction, including overall accuracy, raw pitch accuracy,\nraw chroma accuracy, voicing detection rate and voicing\nfalse alarm rate. We compute them using mireval [11],\n1We downloaded the ADC2004 and MIREX05 datasets from\nhttp://labrosa.ee.columbia.edu/projects/melody/ and the MIR1k dataset\nfrom https://sites.google.com/site/unvoicedsoundseparation/mir-1ka Python library designed for objective evaluation in MIR\ntasks.\n4.3 Preprocessing\nWe resample the audio ﬁles to 8 kHz and merge stereo\nchannels into mono. We then compute spectrogram with\nHann window of 1024 samples and hop size of 80 sam-\nples, and ﬁnally compress the magnitude by a log scale.\nFollowing the strategy in [7], we use only 256 bins from\n0 Hz to 2000 Hz where the human singing voices have a\nrelatively greater level than in other frequency bands with\nregard to background music.\n5. EXPERIMENTS\nGiven the MCDNN model and training data, we conduct\nseveral experiments to ﬁgure out the effect of different set-\ntings in the model. In the followings, we describe options\nin training the MCDNN and the experiments.\n5.1 DNN Training\nWe conﬁgure the DNN to have three hidden layers, each\nwith 512, 512 and 256 units, and ReLUs for the nonlin-\near function. For the output layer, we use the sigmoid\nfunction instead of the softmax function, which is a typ-\nical choice in the categorical classiﬁcation, because the\nsigmoid slightly worked better in our experiments. Thus,\nwe use binary cross-entropy between the output layer and\nthe one-hot representation of pitch labels as an objective\nfunction to minimize. The pitch labels cover from D2 to\nF#5 in semitone unit. The label vectors are expanded as\npitch resolution increases. We initialize the weights with\nrandom values from the uniform distribution and optimize\nthe objective function using RMSprop and 20% dropout\nfor all hidden layers to avoid overﬁtting to the training set.\nFor fast computing, we run the code using Keras2, a deep\nlearning library in Python, on a computer with two GPUs.\n5.2 Context Size\nOur model takes multiple frames of spectrogram as input\nto take contextual information into account. Our ﬁrst ex-\nperiment is to ﬁgure out an optimal size of the input for\ndifferent pitch resolutions. For this experiment, we train a\nsingle-column DNN using one million examples from the\nRWC training set. Every training iteration, we randomly\nselect a subset from the pool. We then verify classiﬁcation\naccuracy using only voiced frames on the RWC validation\nset. Figure 2 shows the classiﬁcation accuracy for a vary-\ning size of the spectrogram input. We experimented with\nmulti-frame as inputs of DNN where the input data were\ntaken fromNneighbor spectrogram frames. The accuracy\nprogressively increases up to 7 or 9 frames and then con-\nverge to a certain level. This is expected because pitch con-\ntours of singing voices usually have continuous curve pat-\nterns and this temporal features can be captured better by\n2https://github.com/fchollet/kerasProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 8211 3 5 7 9 11 13\nInput size (number of frames)0.20.30.40.50.60.70.8Classification Accuracyres = 1\nres = 2\nres = 4\nres = 8Figure 2 : Classiﬁcation accuracy on the validation\nset. “res=1” indicates pitch resolution in semitone unit.\n“res=2”, “res=4”, and “res=8” indicate progressively\nhigher resolutions than semitone by a factor of 2.\ntaking multiple frames. The result also shows that the val-\nidation accuracy is inversely proportional to the pitch res-\nolution. That is, as the resolution increases, the accuracy\ndrops quite signiﬁcantly. This is also expected because the\nnumber of input data per label will decrease given the same\ntraining condition and also the accuracy criterion becomes\nmore strict (i.e., slight missing between neighboring pitch\nlabels could have been regarded as a correct prediction).\nFor the following experiments, we ﬁx the input size to 11\nframes.\n5.3 Data Augmentation\nAs described in Section 4.1, we augment the training set\nin two folds. One is by expanding the existing train-\ning set using pitch shifting and the other is by making\nup with another dataset, i.e., 60 songs including singing\nvoices among the MedleyDB dataset. For this experiment,\nwe train a single-column DNN using the increased train-\ning pool, speciﬁcally, six million examples from the aug-\nmented RWC training set and additional 200,000 examples\nor so from the MedleyDB songs. Again, we verify classi-\nﬁcation accuracy using only voiced frames on the RWC\nvalidation set.\nFigure 3 shows the classiﬁcation accuracy for a varying\nsize of pitch resolution when the pitch-shifted RWC data\nand MedleyDB data are added to the training data pool in\nturn. Overall, the accuracy increases by 2 to 3 % with\nthe additional sets. An interesting result is that, with the\npitch-shifted data, the accuracy increases more when pitch\nresolution is low (1 or 2) and, with the additional Med-\nleyDB songs, the accuracy increases more when pitch res-\nolution is high (4 or 8). This is probably because the RWC\ndata is pitch-shifted in semitone units and so technically in-\ncreases data with low pitch resolutions whereas strong vi-\nbrato voices in the opera songs included in the MedleyDB\ndataset increase data with high pitch resolutions relatively\nmore.\n5.4 Single-column vs. Multi-column\nAs shown in the previous experiments, the classiﬁcation\naccuracy is inversely proportional to the pitch resolution\n0.40.50.60.70.8Classification Accuracy\nRWC RWC\n+ pitch shiftRWC \n+ pitch shift + MedleyDB res = 1\n res = 2\n res = 4\n res = 8Figure 3 : Classiﬁcation accuracy on the validation set\nwhen the pitch-shifted versions of the RWC dataset and\n60 vocal songs of the MedleyDB dataset are added in turn\nto the training set.\nin the single-column DNN (SCDNN). That is, as the reso-\nlution becomes ﬁner, the classiﬁcation accuracy decreases,\nand vice versa. The MCDNN was devised from this empir-\nical result, hoping to achieve both high accuracy and high\npitch resolution simultaneously by using the SCDNN with\ndifferent pitch resolutions together. In this experiment, we\nvalidate the idea by comparing the SCDNN and two dif-\nferent combinations of MCDNN. In particular, we evalu-\nate them on the three test sets (ADC2004, MIREX05 and\nMIR1k), assuming the voiced frames are perfectly detected\n(the ﬁrst singing voice detection scenario in Section 3.4).\nFigure 4 displays the raw pitch accuracy (RPA) and raw\nchroma accuracy (RCA). Note that we evaluate the models\non the ADC2004 and MIREX05 datasets separately for all\nsongs including instrumental pieces and a subset excluding\nthem (for the latter, the dataset name is sufﬁxed with “vo-\ncal”). Overall, the MCDNN improves the melody extrac-\ntion accuracies. An interesting result is that the MCDNN\nincreases the accuracies on the sets with singing voices\nquite signiﬁcantly (about 5 % in RPA and RCA on the\nMIREX05-vocal) whereas it can be even worse than the\nSCDNN when instrumental pieces are included. This is\nactually expected because our model is trained only using\nvoiced frames. This indicates that our model is a special-\nized melody extraction algorithm that works only on mu-\nsic including singing voices. Comparing the two MCDNN\nmodels, there is no signiﬁcant difference in performance.\nThus, the simpler model (the 1-2-4 MCDNN) seems to be\na better choice.\n5.5 HMM-based Postprocessing\nWe conduct the Viterbi decoding based on a HMM model\nfor temporal smoothing of the combined prediction. We\nestimate the prior probabilities and transition matrix from\nground-truth of the training set. We then use the prediction\nof whole tracks as posterior probabilities. Table 1 shows\nthe results as performance increments after applying the\nViterbi decoding for the 1-2-4 MCDNN on the test sets.\n5.6 A Case Example of Singing Melody Extraction\nOur proposed model is capable of predicting temporally\nsmooth pitch contours by using multi-resolution pitch la-822 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20160.650.70.750.80.850.9Raw  Pitch / Chroma Accuracy\nRPA:\nADC04RCA:\nADC04RPA:\nMIREX05RCA:\nMIREX05RPA:\nADC04\n-vocalRCA:\nADC04\n-vocalRPA:\nMIREX05\n-vocalRCA:\nMIREX05\n-vocalRPA:\nMIR1k\n-vocalRCA:\nMIR1k\n-vocalSCDNN(4)\nMCDNN(1-2-4)\nMCDNN(1-2-4-8)Figure 4 : Raw pitch accuracy (RPA) and raw chroma accuracy (RCA) on the ADC2004, MIREX2005 and MIR1K dataset\nthat compare one SCDNN and two different MCDNNs. The “vocal” sufﬁx indicates their subsets that include songs with\nvocals. Here we assume that we have a perfect voice detector to focus on accuracy on voiced frames.\nwithout HMM with HMM\nDataset RPA RCA RPA RCA\nADC2004 0.749 0.806 0.762 0.816\nADC2004-vocal 0.827 0.852 0.835 0.856\nMIREX05 0.801 0.817 0.803 0.817\nMIREX05-vocal 0.869 0.875 0.871 0.877\nMIR1k 0.766 0.813 0.769 0.813\nTable 1 : Performance increment by HMM-based smooth-\ning on the 1-2-4 MCDNN.\nbels, and the capability is supported more by the aug-\nmented datasets. Here we verify it by illustrating an ex-\nample of singing melody extraction. We selected an opera\nsong from the ADC2004 dataset because the singing voices\nhave dynamic pitch motions such as strong vibrato. Fig-\nure 5 shows the results from three different melody ex-\ntraction models. The left one is from the SCDNN with\na pitch resolution of 4 (i.e. 1/4 semitone) and trained only\nwith the RWC dataset. The middle one is from the same\nSCDNN but trained with additional pitch-shifted RWC\ndataset and MedleyDB dataset. The right one is from the\n1-2-4 MCDNN that has the three pitch resolutions. Com-\nparing the ﬁrst two models, the additional songs help track-\ning the vibrato but the second model still misses the whole\nexcursion. With the additional resolutions, the MCDNN\nmakes further improvement, tracking the pitch contours\nquite precisely.\n5.7 Comparison to State-of-the-art Methods\nWe compare our proposed method with state-of-the-art al-\ngorithms on the three test datasets in Table 2. The com-\npared algorithms are all based on pitch saliency [2, 6, 12].\nThe evaluation metrics include overall accuracy (OA), raw\npitch accuracy (RPA), raw chroma accuracy (RCA), voice\nrecall (VR) and voice false alarm (VFA). As mentionedAlgorithm OA RPA RCA VR VFA\nArora [2] 0.690 0.814 0.859 0.765 0.235\nDressler [6] 0.853 0.883 0.889 0.901 0.158\nSalamon [12] 0.735 0.763 0.787 0.805 0.151\nMCDNN(all) 0.655 0.703 0.759 0.874 0.469\nMCDNN(vocal) 0.731 0.758 0.783 0.889 0.412\n(a) ADC2004\nAlgorithm OA RPA RCA VR VFA\nArora [2] 0.634 0.692 0.765 0.810 0.344\nDressler [6] 0.715 0.770 0.806 0.831 0.300\nSalamon [12] 0.657 0.676 0.762 0.773 0.263\nMCDNN(all) 0.616 0.733 0.752 0.894 0.585\nMCDNN(vocal) 0.684 0.776 0.786 0.870 0.490\n(b) MIREX05\nAlgorithm OA RPA RCA VR VFA\nMCDNN(vocal) 0.613 0.726 0.770 0.934 0.658\n(c) MIR-1K\nTable 2 : Melody extraction results on three test datasets.\nIn this evaluation, we used a simple energy-based voice\ndetector for fair comparison.\nin Section 4.2, we have only 13 songs in the MIREX05\ndataset. Since we use a simple energy-based voice detec-\ntor (the second singing voice detection scenario in Section\n3.4), the results of our model were not very impressive.\nHowever, even with it, the accuracies are quite comparable\nto some of the algorithms when the test sets include singing\nvocals. Also, from Figure 4, we can see the RPA and RCA\nwhen we have a perfect voice detector. This shows that the\naccuracies signiﬁcantly increase, being comparable to the\ntop-notch one.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 8231800 1900 2000 2100 2200 2300 2400 2500\nTime (samples)500550600650700750Frequency (Hz)Annotation\nProposed Method(a) SCDNN with a pitch resolution\nof 4 trained with the RWC dataset\n1800 1900 2000 2100 2200 2300 2400 2500\nTime (samples)500550600650700750Frequency (Hz)Annotation\nProposed Method(b) SCDNN with a pitch resolu-\ntion of 4 trained with the addi-\ntional pitch-shifted RWC dataset\nand MedleyDB dataset\n1800 1900 2000 2100 2200 2300 2400 2500\nTime (samples)500550600650700750Frequency (Hz)Annotation\nProposed Method(c) 1-2-4 MCDNN trained with\nthe additional pitch-shifted RWC\ndataset and MedleyDB dataset\nFigure 5 : A case example of melody extraction on an opera song using different models and training data\n6. CONCLUSIONS\nIn this paper, we proposed a novel classiﬁcation-based\nmelody extraction algorithm on vocal segments using the\nmulti-column deep neural networks. We showed how the\ndata-driven approach can be improved by different settings\nof the model such as input size, data augmentation, use\nof multi-column DNN with different pitch resolutions and\nHMM-based smoothing. The limitation of this model is\nthat it works well only for singing voice because we trained\nit only with songs where vocals lead the melody. However,\nthis also indicates that our model can be improved to a gen-\neral melody extractor if a sufﬁcient amount of instrumental\npieces are included in the training sets. We compared our\nmodel to previous state-of-the-arts. Since we used a sim-\nple energy-based singing voice detector, the performance\nof our model has limitations. However, the results show\nthat, with a better voice detector, our model can be im-\nproved further.\n7. ACKNOWLEDGMENT\nThis work was supported by Korea Advanced Institute\nof Science and Technology (Project No. G04140049),\nNational Research Foundation of Korea (Project No.\nN01150671) and BK21 Plus Postgraduate Organization for\nContent Science.\n8. REFERENCES\n[1] Forest Agostinelli, Michael R Anderson, and Honglak\nLee. Adaptive multi-column deep neural networks with\napplication to robust image denoising. In Advances\nin Neural Information Processing Systems 26 , pages\n1493–1501, 2013.\n[2] Vipul Arora and Laxmidhar Behera. On-line melody\nextraction from polyphonic audio using harmonic clus-\nter tracking. Audio, Speech, and Language Processing,\nIEEE Transactions on , 21(3):520–530, 2013.[3] Rachel M Bittner, Justin Salamon, Slim Essid, and\nJuan P Bello. Melody extraction by contour classiﬁca-\ntion. In Proceedings of the 16th International Society\nfor Music Information Retrieval Conference, ISMIR ,\n2015.\n[4] Dan Ciresan, Ueli Meier, and J ¨urgen Schmidhuber.\nMulti-column deep neural networks for image classi-\nﬁcation. In Computer Vision and Pattern Recognition\n(CVPR), 2012 IEEE Conference on , pages 3642–3649.\nIEEE, 2012.\n[5] Roger B Dannenberg, William P Birmingham, George\nTzanetakis, Colin Meek, Ning Hu, and Bryan Pardo.\nThe musart testbed for query-by-humming evaluation.\nComputer Music Journal , 28(2):34–48, 2004.\n[6] Karin Dressler. An auditory streaming approach for\nmelody extraction from polyphonic music. In Proceed-\nings of the 12th International Society for Music In-\nformation Retrieval Conference, ISMIR , pages 19–24,\n2011.\n[7] Daniel P. W. Ellis and Graham E Poliner.\nClassiﬁcation-based melody transcription. Machine\nLearning , 65(2):439–456, 2006.\n[8] Masataka Goto, Hiroki Hashiguchi, Takuichi\nNishimura, and Ryuichi Oka. Rwc music database:\nPopular, classical, and jazz music databases. In Pro-\nceedings of the 3rd International Conference on Music\nInformation Retrieval, ISMIR , pages 287–288, 2002.\n[9] Jean Laroche. Applications of Digital Signal Process-\ning to Audio and Acoustics , chapter Time and Pitch\nScale Modiﬁcation of Audio Signals, pages 279–309.\nSpringer US, Boston, MA, 2002.\n[10] Graham E Poliner, Daniel P. W. Ellis, Andreas F\nEhmann, Emilia G ´omez, Sebastian Streich, and\nBeesuan Ong. Melody transcription from music audio:824 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Approaches and evaluation. Audio, Speech, and Lan-\nguage Processing, IEEE Transactions on , 15(4):1247–\n1256, 2007.\n[11] Colin Raffel, Brian McFee, Eric J. Humphrey, Justin\nSalamon, Oriol Nieto, Dawen Liang, and Daniel P. W.\nEllis. mir eval : A transparent implementation of com-\nmon mir metrics. In Proceedings of the 15th Interna-\ntional Conference on Music Information Retrieval, IS-\nMIR, 2014.\n[12] Justin Salamon and Emilia G ´omez. Melody extrac-\ntion from polyphonic music signals using pitch contour\ncharacteristics. Audio, Speech, and Language Process-\ning, IEEE Transactions on , 20(6):1759–1770, 2012.\n[13] Justin Salamon, Eva Gomez, Daniel P. W. Ellis, and\nGael Richard. Melody extraction from polyphonic mu-\nsic signals: Approaches, applications, and challenges.\nSignal Processing Magazine, IEEE , 31(2):118–134,\n2014.\n[14] Justin Salamon, Bruno Rocha, and Emilia G ´omez. Mu-\nsical genre classiﬁcation using melody features ex-\ntracted from polyphonic music signals. In IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , 2012.\n[15] Jan Schl ¨uter and Thomas Grill. Exploring data aug-\nmentation for improved singing voice detection with\nneural networks. In Proceedings of the 16th Interna-\ntional Society for Music Information Retrieval Confer-\nence, ISMIR , pages 121–126, 2015.\n[16] Joan Serra, Emilia G ´omez, and Perfecto Herrera. Au-\ndio cover song identiﬁcation and similarity: back-\nground, approaches, evaluation, and beyond. In Ad-\nvances in Music Information Retrieval , pages 307–332.\nSpringer, 2010.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 825"
    },
    {
        "title": "Adaptive Frequency Neural Networks for Dynamic Pulse and Metre Perception.",
        "author": [
            "Andrew John Lambert",
            "Tillman Weyde",
            "Newton Armstrong"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1418305",
        "url": "https://doi.org/10.5281/zenodo.1418305",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/228_Paper.pdf",
        "abstract": "Beat induction, the means by which humans listen to mu- sic and perceive a steady pulse, is achieved via a perceptual and cognitive process. Computationally modelling this phenomenon is an open problem, especially when process- ing expressive shaping of the music such as tempo change. To meet this challenge we propose Adaptive Frequency Neural Networks (AFNNs), an extension of Gradient Fre- quency Neural Networks (GFNNs). GFNNs are based on neurodynamic models and have been applied successfully to a range of difficult music perception problems including those with syncopated and polyrhythmic stimuli. AFNNs extend GFNNs by applying a Hebbian learning rule to the oscillator frequencies. Thus the frequencies in an AFNN adapt to the stimulus through an attraction to local areas of resonance, and allow for a great dimensionality reduction in the network. Where previous work with GFNNs has focused on fre- quency and amplitude responses, we also consider phase information as critical for pulse perception. Evaluating the time-based output, we find significantly improved re- sponses of AFNNs compared to GFNNs to stimuli with both steady and varying pulse frequencies. This leads us to believe that AFNNs could replace the linear filtering meth- ods commonly used in beat tracking and tempo estimation systems, and lead to more accurate methods.",
        "zenodo_id": 1418305,
        "dblp_key": "conf/ismir/LambertWA16",
        "content": "ADAPTIVE FREQUENCY NEURAL NETWORKS\nFOR DYNAMIC PULSE AND METRE PERCEPTION\nAndrew J. Lambert, Tillman Weyde, and Newton Armstrong\nCity University London\n{andrew.lambert.1; t.e.weyde; newton.arstrong.1 }@city.ac.uk\nABSTRACT\nBeat induction, the means by which humans listen to mu-\nsic and perceive a steady pulse, is achieved via a perceptual\nand cognitive process. Computationally modelling this\nphenomenon is an open problem, especially when process-\ning expressive shaping of the music such as tempo change.\nTo meet this challenge we propose Adaptive Frequency\nNeural Networks (AFNNs), an extension of Gradient Fre-\nquency Neural Networks (GFNNs).\nGFNNs are based on neurodynamic models and have\nbeen applied successfully to a range of difﬁcult music\nperception problems including those with syncopated and\npolyrhythmic stimuli. AFNNs extend GFNNs by applying\na Hebbian learning rule to the oscillator frequencies. Thus\nthe frequencies in an AFNN adapt to the stimulus through\nan attraction to local areas of resonance, and allow for a\ngreat dimensionality reduction in the network.\nWhere previous work with GFNNs has focused on fre-\nquency and amplitude responses, we also consider phase\ninformation as critical for pulse perception. Evaluating\nthe time-based output, we ﬁnd signiﬁcantly improved re-\nsponses of AFNNs compared to GFNNs to stimuli with\nboth steady and varying pulse frequencies. This leads us to\nbelieve that AFNNs could replace the linear ﬁltering meth-\nods commonly used in beat tracking and tempo estimation\nsystems, and lead to more accurate methods.\n1. INTRODUCTION\nAutomatically processing an audio signal to determine\npulse event onset times (beat tracking) is a mature ﬁeld,\nbut it is by no means a solved problem. Analysis of beat\ntracking failures has shown that beat trackers have great\nproblems with varying tempo and expressive timing [5, 6].\nThe neuro-cognitive model of nonlinear resonance\nmodels the way the nervous system resonates to audi-\ntory rhythms by representing a population of neurons as a\ncanonical nonlinear oscillator [15]. A Gradient Frequency\nNeural Network (GFNN) is an oscillating neural network\nc/circlecopyrtAndrew J. Lambert, Tillman Weyde, and Newton Arm-\nstrong. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Andrew J. Lambert, Tillman Weyde,\nand Newton Armstrong. “Adaptive Frequency Neural Networks\nfor Dynamic Pulse and Metre Perception”, 17th International Society for\nMusic Information Retrieval Conference, 2016.model based on nonlinear resonance. The network con-\nsists of a number of canonical oscillators distributed across\na frequency range. The term ‘gradient’ is used to refer\nto this frequency distribution, and should not be confused\nwith derivative-based learning methods in Machine Learn-\ning. GFNNs have been shown to predict beat induction\nbehaviour from humans [16,17]. The resonant response of\nthe network adds rhythm-harmonic frequency information\nto the signal, and the GFNN’s entrainment properties allow\neach oscillator to phase shift, resulting in deviations from\ntheir natural frequencies. This makes GFNNs good candi-\ndates for modelling the perception of temporal dynamics\nin music.\nPrevious work on utilising GFNNs in an MIR context\nhas shown promising results for computationally difﬁcult\nrhythms such as syncopated rhythms where the pulse fre-\nquency may be completely absent from the signal’s spec-\ntrum [17, 23], and polyrhythms where there is more than\none pulse candidate [2]. However, these studies have\nplaced a focus on the frequencies contained in the GFNN’s\noutput, often reporting the results in the form of a magni-\ntude spectrum, and thus omitting phase information. We\nbelieve that when dealing with pulse and metre perception,\nphase is an integral part as it constitutes the difference be-\ntween entraining to on-beats, off-beats, or something in-\nbetween. In the literature, the evaluation of GFNNs’ pulse\nﬁnding predictions in terms of phase has, to our knowl-\nedge, never been attempted.\nOur previous work has used GFNNs as part of a ma-\nchine learning signal processing chain to perform rhythm\nand melody prediction. An expressive rhythm prediction\nexperiment showed comparable accuracy to the state-of-\nthe-art beat trackers. However, we also found that GFNNs\ncan sometimes become noisy, especially when the pulse\nfrequency ﬂuctuates [12, 13].\nThis paper presents a novel variation on the GFNN,\nwhich we have named an Adaptive Frequency Neural Net-\nwork (AFNN). In an AFNN, an additional Hebbian learn-\ning rule is applied to the oscillator frequencies in the net-\nwork. Hebbian learning is a correlation-based learning ob-\nserved in neural networks [9]. The frequencies adapt to\nthe stimulus through an attraction to local areas of reso-\nnance. A secondary elasticity rule attracts the oscillator\nfrequencies back to their original values. These two new\ninteracting adaptive rules allow for a great reduction in the\ndensity of the network, minimising interference whilst also\nmaintaining a frequency spread across the gradient.60The results of an experiment with a GFNNs and AFNNs\nare also presented, partially reproducing the results from\nVelasco and Large’s last major MIR application of a\nGFNN [23], and Large et al.’s more recent neuroscientiﬁc\ncontribution [17]. However, we place greater evaluation\nfocus on phase accuracy. We have found that AFNNs can\nproduce a better response to stimuli with both steady and\nvarying pulses.\nThe structure of this paper is as follows: Section 2\nprovides a brief overview of the beat-tracking literature\nand the GFNN model, Section 3 introduces a phase based\nevaluation method, Section 4 introduces our new AFNN\nmodel, Section 5 details the experiments we have con-\nducted and shares the results, and ﬁnally Section 6 pro-\nvides some conclusions and points to future work.\n2. BACKGROUND\n2.1 Pulse and Metre\nLerdahl and Jackendoff’s Generative Theory of Tonal Mu-\nsic[19] was one of the ﬁrst formal theories to put forward\nthe notion of hierarchical structures in music which are not\npresent in the music itself, but perceived and constructed\nby the listener. One such hierarchy is metrical structure ,\nwhich are layers of beats existing in a hierarchically lay-\nered relationship with the rhythm. Each metrical level is\nassociated with its own period, which divides the previous\nlevel’s period into a certain number of parts.\nHumans often choose a common, comfortable metrical\nlevel to tap along to, which is known as a preference rule\nin the theory. This common metrical level is commonly re-\nferred to as ‘the beat’, but this is a problematic term since\na beat can also refer to a singular rhythmic event or a met-\nrically inferred event. To avoid that ambiguity, we use the\nterm ‘pulse’ [4].\n2.2 Beat Tracking\nDiscovering the pulse within audio or symbolic data is\nknown as beat tracking and has a long history of research\ndating back to 1990 [1]. There have been many varied ap-\nproaches to beat tracking over the years, and here we focus\non systems relevant to the proposed model. Some early\nwork by Large used a single nonlinear oscillator to track\nbeats in performed piano music [14]. Scheirer used linear\ncomb ﬁlters [22], which operate on similar principles to\nLarge and Kolen’s early work on nonlinear resonance [18].\nA comb ﬁlter’s state is able to represent the rhythmic con-\ntent directly, and can track tempo changes by only consid-\nering one metrical level. Klapuri et al.’s system builds on\nScheirer’s design by also using comb ﬁlters, and extends\nthe model to three metrical levels [10]. More recently,\nB¨ock et al. [3] used resonating feed backward comb ﬁlters\nwith a particular type of Recurrent Neural Network called\na Long Short-Term Memory Network (LSTM) to achieve a\nstate-of-the-art beat tracking result in the MIR Evaluation\neXchange (MIREX)1.\n1http://www.music-ir.org/mirex/2.3 Nonlinear Resonance\nJones [7] proposed a psychological entrainment theory to\naddress how humans are able to attend temporal events.\nJones posited that rhythmic patterns such as music and\nspeech potentially entrain a hierarchy of oscillations, form-\ning an attentional rhythm . Thus, entrainment assumes an\norganisational role for temporal patterns and offers a pre-\ndiction for future events, by extending the entrained period\ninto the future.\nLarge then extended this theory with the notion of non-\nlinear resonance [15]. Musical structures occur at simi-\nlar time scales to fundamental modes of brain dynamics,\nand cause the nervous system to resonate to the rhythmic\npatterns. Certain aspects of this resonance process can\nbe described with the well-developed theories of neurody-\nnamics, such as oscillation patterns in neural populations.\nThrough the use of neurodynamics, Large moves between\nphysiological and psychological levels of modelling, and\ndirectly links neural activity with music. Several musical\nphenomena can all arise as patterns of nervous system acti-\nvation, including perceptions of pitch and timbre, feelings\nof stability and dissonance, and pulse and metre percep-\ntion.\nThe model’s basis is the canonical model of Hopf nor-\nmal form oscillators, which was derived as a model oscil-\nlating neural populations [16]. Eqn (1) shows the differ-\nential equation that deﬁnes the canonical model, which is\na Hopf normal form oscillator with its higher order terms\nfully expanded:\ndz\ndt=z(α+iω+ (β1+iδ1)|z|2+(β2+iδ2)ε|z|4\n1−ε|z|2)\n+kP(/epsilon1,x(t))A(/epsilon1,¯z) +/summationdisplay\ni /negationslash=jcijzj\n1−√/epsilon1zj.1\n1−√/epsilon1¯zi,\n(1)\nzis a complex valued output where the real and imaginary\nparts represent excitation and inhibition, ¯zis its complex\nconjugate, and ωis the driving frequency in radians per\nsecond.αis a linear damping parameter, and β1,β2are\namplitude compressing parameters, which increase stabil-\nity in the model. δ1,δ2are frequency detuning parameters,\nandεcontrols the amount on nonlinearity in the system.\nx(t)is a time-varying external stimulus, which is also cou-\npled nonlinearly and consists of passive part, P(ε,x(t)),\nand an active part, A(ε,¯z), controlled by a coupling pa-\nrameterk.cjiis a complex number representing phase\nand magnitude of a connection between the ithandjth\noscillator (zi,zj). These connections can be strengthened\nthrough unsupervised Hebbian learning, or set to ﬁxed val-\nues as in [23]. In our experiments presented here, we set\ncijto0.\nBy varying the oscillator parameters, a wide range of\nbehaviours not encountered in linear models can be in-\nduced (see [15]). In general, while the cannonical model\nmaintains an oscillation according to its parameters, it en-\ntrains to and resonates with an external stimulus nonlin-\nearly. Theαparameter acts as a bifurcation parameter:Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 610 10 20 30 40\nTime (s)0.501.002.004.008.00Oscillator natural frequency (Hz)\n0.000.020.040.060.080.100.120.14\nAmplitudeFigure 1 . Amplitudes of oscillators over time. The dashed\nline shows stimulus frequency. The stimulus itself is\nshown in Figure 2. There is an accelerando after approxi-\nmately 25s.\nwhenα<0the model behaves as a damped oscillator, and\nwhenα>0the model oscillates spontaneously, obeying a\nlimit-cycle. In this mode, the oscillator is able to maintain\na long temporal memory of previous stimulation.\nCanonical oscillators will resonate to an external stim-\nulus that contains frequencies at integer ratio relationships\nto its natural frequency. This is known as mode-locking,\nan abstraction on phase-locking in which kcycles of os-\ncillation are locked to mcycles of the stimulus. Phase-\nlocking occurs when k=m= 1, but in mode-locking\nseveral harmonic ratios are common such as 2:1, 1:2, 3:1,\n1:3, 3:2, and 2:3 and even higher order integer ratios are\npossible [17], which all add harmonic frequency informa-\ntion to a signal. This sets nonlinear resonance apart from\nmany linear ﬁltering methods such as the resonating comb\nﬁlters used in [10] and Kalman ﬁlters [8].\n2.4 Gradient Frequency Neural Networks\nConnecting several canonical oscillators together with a\nconnection matrix forms a Gradient Frequency Neural\nNetwork (GFNN) [16]. When the frequencies in a GFNN\nare distributed within a rhythmic range and stimulated with\nmusic, resonances can occur at integer ratios to the pulse.\nFigure 1 shows the amplitude response of a GFNN to\na rhythmic stimulus over time. Darker areas represent\nstronger resonances, indicating that that frequency is rel-\nevant to the rhythm. A hierarchical structure can be seen\nto emerge from around 8 seconds, in relation to the pulse\nwhich is just below 2Hz in this example. At around 24\nseconds, a tempo change occurs, which can be seen by the\nchanging resonances in the ﬁgure. These resonances can\nbe interpreted as a perception of the hierarchical metrical\nstructure.\nVelasco and Large [23] connected two GFNNs together\nin a pulse detection experiment for syncopated rhythms.\nThe two networks were modelling the sensory and motor\ncortices of the brain respectively. In the ﬁrst network, the\noscillators were set to a bifucation point between damped\n0.81.01.21.41.61.82.0Phase\n10 15 20 25 30\nTime (s)0.00.20.40.60.81.0StimulusFigure 2 . Weighted phase output, Φ, of the GFNN over\ntime. The stimulus is the same as Figure 1.\nand spontaneous oscillation ( α= 0,β1=−1,β2=\n−0.25,δ1=δ2= 0 andε= 1 ). The second net-\nwork was tuned to exhibit double limit cycle bifurcation\nbehaviour (α= 0.3,β1= 1,β2=−1,δ1=δ2= 0\nandε= 1), allowing for greater memory and threshold\nproperties. The ﬁrst network was stimulated by a rhythmic\nstimulus, and the second was driven by the ﬁrst. Internal\nconnections were set to integer ratio relationships such as\n1:3 and 1:2, these connections were ﬁxed and assumed to\nhave been learned through a Hebbian process. The results\nshowed that the predictions of the model conﬁrm observa-\ntions in human performance, implying that the brain may\nbe adding frequency information to a signal to infer pulse\nand metre [17].\n3. PHASE BASED EVALUATION\nThus far in the literature, evaluation of GFNNs has not con-\nsidered phase information. The phase of oscillations is an\nimportant output of a GFNN; in relation to pulse it consti-\ntutes the difference between predicting at the correct pulse\ntimes, or in the worst-case predicting the off-beats. This\nis concerning in Velasco and Large’s evaluation of pulse\ndetection in syncopated beats, which by deﬁnition contain\nmany off-beat events [23].\nPhase and frequency are interlinked in that frequency\ncan be expressed as a rate of phase change and indeed the\ncanonical oscillators’ entrainment properties are brought\nabout by phase shifts. Since the state of a canonical oscil-\nlator is represented by a complex number, both amplitude\nand phase can be calculated instantaneously by taking the\nmagnitude (r=|z|), and angle ( ϕ=arg(z)) respectively.\nWe propose calculating the weighted phase output, Φ, of\nthe GFNN as a whole, shown in (2).\nΦ =N/summationdisplay\ni=0riϕi (2)\nFigure 2 shows the weighted phase output, Φ, over time.\nEven though the amplitude response to the same stimu-62 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20160 5 10 15 20 25 30 35 40\nTime (s)0.501.002.004.008.00Oscillator natural frequency (Hz)\n0.0000.0080.0160.0240.0320.0400.0480.0560.0640.072\nAmplitudeFigure 3 . A low density (4opo) GFNN output. The dashed\nline shows stimulus frequency. Frequency information is\nnot being captured as successfully, as can be observed by\nthe low resonances.\nlus shows a clear corresponding metrical hierarchy (see\nFigure 1), the phase response remains noisy. This is due to\nthe high density of oscillators required in a GFNN. Velasco\nand Large used 289 oscillators per layer in their experi-\nment, a density of 48 oscillators per octave (opo). These\nhigh densities are often used in GFNNs to capture a wide\nrange of frequencies, but can cause interference in the net-\nwork. The term ‘interference’ is used here to mean in-\nteracting signals amplifying or cancelling each other when\nsummed. Since each oscillator can only entrain to a narrow\nrange of frequencies, using a lower density not only in-\ncreases the likelihood missing a relevant frequency, it also\nstops local frequency populations from reinforcing one an-\nother. An example of this can be seen in Figure 3, where\nfrequency information is not being captured as success-\nfully in a 4opo GFNN.\nIn our previous work, we have addressed this issue by\nusing only the real part of the oscillator as a single mean-\nﬁeld output [11, 12]. This retained a meaningful represen-\ntation of the oscillation, but ultimately removed important\ninformation. A selective ﬁlter could also be applied, by\ncomparing each oscillator with the mean amplitude of the\nGFNN, and only retaining resonating oscillators. However,\nthis is not an ideal solution to the interference problem as it\nrequires an additional, non real-time processing step which\ncannot be easily incorporated into an online machine learn-\ning chain. Furthermore, new frequencies would not be\nselected until they begin to resonate above the selection\nthreshold, meaning that new resonances in changing tem-\npos may be missed.\n4. ADAPTIVE FREQUENCY NEURAL NETWORK\nThe Adaptive Frequency Neural Network (AFNN) at-\ntempts to address both the interference within high density\nGFNNs, and improve the GFNNs ability to track changing\nfrequencies, by introducing a Hebbian learning rule on the\nfrequencies in the network. This rule is an adapted form of\n0 5 10 15 20 25 30 35 40\nTime (s)1.002.004.008.00Frequency (Hz)Figure 4 . AFNN frequencies adapting to a sinusoidal stim-\nulus. The dashed line shows stimulus frequency.\nthe general model introduced by Righetti et al. [21] shown\nin (3):\ndω\ndt=−/epsilon1\nrx(t)sin(ϕ) (3)\nTheir method depends on an external driving stimulus\n(x(t)) and the state of the oscillator ( r,ϕ), driving the fre-\nquency (ω) toward the frequency of the stimulus. The fre-\nquency adaptation happens on a slower time scale than the\nrest of the system, and is inﬂuenced by the choice of /epsilon1,\nwhich can be thought of as a force scaling parameter. /epsilon1\nalso scales with r, meaning that higher amplitudes are af-\nfected less by the rule.\nThis method differs from other adaptive models such\nas McAuley’s phase-resetting model [20] by maintaining a\nbiological plausibility ascribed to Hebbian learning [9]. It\nis also a general method that has been proven to be valid for\nlimit cycles of any form and in any dimension, including\nthe Hopf oscillators which form the basis of GFNNs (see\n[21]).\nWe have adapted this rule to also include a linear elas-\nticity, shown in (4).\ndω\ndt=−/epsilon1f\nrx(t)sin(ϕ)−/epsilon1h\nr(ω−ω0\nω0) (4)\nThe elastic force is an implementation of Hooke’s Law,\nwhich describes a force that strengthens with displace-\nment. We have introduced this rule to ensure the AFNN\nretains a spread of frequencies (and thus metrical struc-\nture) across the gradient. The force is relative to natural\nfrequency, and can be scaled through the /epsilon1hparameter. By\nbalancing the adaptive ( /epsilon1f) and elastic ( /epsilon1h) parameters, the\noscillator frequency is able to entrain to a greater range of\nfrequencies, whilst also returning to its natural frequency\n(ω0) when the stimulus is removed. Figure 4 shows the fre-\nquencies adapting over time in the AFNN under sinusoidal\ninput.\nThe AFNN preserves the architecture of the GFNN;\nthe main difference is the frequency learning procedure.\nFigure 5 shows the weighted phase output ( Φ) of an AFNNProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 630.10.20.30.40.5Phase\n10 15 20 25 30\nTime (s)0.00.20.40.60.81.0StimulusFigure 5 . Weighted phase output, Φ, of the AFNN over\ntime. Reduced interference can be seen compared with\nFigure 2.\nstimulated with the same stimulus as in Figure 2. One can\nobserve that a reduced level of interference is apparent.\n5. EXPERIMENT\nWe have conducted a pulse detection experiment designed\nto test two aspects of the AFNN.\nFirstly, we wanted to discover how the output of the\nAFNN compares with the GFNN presented in [23]. To\nthis end, we are using similar oscillator parameters ( α=\n0,β1=β2=−1,δ1=δ2= 0andε= 1). This is known\nas the ‘critical’ parameter regime, poised between damped\nand spontaneous oscillation. We are retaining their GFNN\ndensity of 48opo, but reducing the number of octaves to\n4 (0.5-8Hz, logarithmically distributed), rather than the 6\noctaves (0.25-16Hz) used in [23]. This equates to 193 os-\ncillators in total. This reduction did not affect our results\nand is more in line with Large’s later GFNN ranges (see\n[17]).\nThe AFNN uses the same oscillator parameters and dis-\ntribution, but the density is reduced to 4opo, 16 oscillators\nin total./epsilon1fand/epsilon1hwere hand-tuned to the values of 1.0 and\n0.3 respectively. For comparison with the AFNN, a low\ndensity GFNN is also included, with the same density as\nthe AFNN but no adaptive frequencies.\nWe have selected two of the same rhythms used by Ve-\nlasco and Large for use in this experiment, the ﬁrst is an\nisochronous pulse and the second is the more difﬁcult ‘son\nclave’ rhythm. We supplemented these with rhythms from\nthe more recent Large et al. paper [17]. These rhythms are\nin varying levels of complexity (1-4), varied by manipulat-\ning the number of events falling on on-beats and off-beats.\nA level 1 rhythm contains one off-beat event, level 2 con-\ntains two off-beat events and so forth. For further informa-\ntion about these rhythms, see [17]. Two level 1 patterns,\ntwo level 2 patterns, two level 3 patterns, and four level 4\npatterns were used.\nThe second purpose of the experiment was to test\nthe AFNN and GFNN’s performance on dynamic pulses,therefore we have included two additional stimulus\nrhythms: an accelerando and a ritardando.\nWe are additionally testing these rhythms at 20 differ-\nent tempos selected randomly from a range 80-160bpm.\nNone of the networks tested had any internal connections\nactivated, ﬁxed or otherwise ( cij= 0). An experiment to\nstudy of the effect of connections is left for future work.\nIn summary, the experiment consisted of 5 stimulus cat-\negories, 20 tempos per category and 3 networks. There\nare two initial evaluations, one for comparison with previ-\nous work with GFNNs, and the second is testing dynamic\npulses with accelerando and ritardando. The experiment\nused our own open-source PyGFNN python library, which\ncontains GFNN and AFNN implementations2.\n5.1 Evaluation\nAs we have argued above (see Section 2.4), we believe that\nwhen predicting pulse, phase is an important aspect to take\ninto account. Phase information in the time domain also\ncontains frequency information, as frequency equates the\nrate of change in phase. Therefore our evaluation com-\npares the weighted phase output ( Φ) with a ground truth\nphase signal similar to an inverted beat-pointer model [24].\nWhile a beat-pointer model linearly falls from 1 to 0 over\nthe duration of one beat, our inverted signal rises from 0 to\n1 to represent phase growing from 0 to 2πin an oscillation.\nThe entrainment behaviour of the canonical oscillators will\ncause phase shifts in the network, therefore the phase out-\nput should align to the phase of the input.\nTo make a quantitative comparison we calculate the\nPearson product-moment correlation coefﬁcient (PCC) of\nthe two signals. This gives a relative, linear, mean-free\nmeasure of how close the target and output signals match.\nA value of 1 represents a perfect correlation, whereas -1\nindicates an anti-phase relationship. Since the AFNN and\nGFNN operate on more than one metrical level, even a\nsmall positive correlation would be indicative of a good\nfrequency and phase response, as some of the signal repre-\nsents other metrical levels.\n5.2 Results\nFigure 6 shows the results for the pulse detection experi-\nment described above in the form of box plots.\nWe can observe from Figure 6a that the GFNN (A) is\neffective for tracking isochronous rhythms. The resonance\nhas enough strength to dominate the interference from the\nother oscillators. The low density GFNN (B) performs sig-\nniﬁcantly worse with little positive correlation and some\nnegative correlation, showing the importance of having a\ndense GFNN. The outliers seen can be explained by the\nrandomised tempo; sometimes by chance the tempo falls\ninto an entrainment basin of one or more oscillators. De-\nspite its low density, the AFNN (C) fairs as well as the\nGFNN, showing a matching correlation to the target sig-\nnal, especially in the upper quartile and maximum bounds.\nExploring more values for /epsilon1fand/epsilon1hmay yield even better\nresults here.\n2https://github.com/andyr0id/PyGFNN64 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016A B* C\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7(a) Isochronous\nA B C*\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7 (b) Son Clave\nA B* C*\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7 (c) Levels 1-4\nA B* C*\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n(d) Accel.\nA B* C*\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7 (e) Rit.\nFigure 6 . Box and Whisker plots of the PCC results. A) GFNN, B) Low density GFNN, C) AFNN. Boxes represent the\nﬁrst and third quartiles, the red band is the median, and whiskers represent maximum and minimum non-outliers. *Denotes\nsigniﬁcance in a Wilcoxon signed rank test ( p<0.05).\nIn the son clave results (Figure 6b) all networks perform\npoorly. A poorer result here was expected due to the difﬁ-\nculty of this rhythm. However, we can see a signiﬁcant im-\nprovement in the AFNN, which may be due to the reduced\ninterference in the network. In the Large et al. rhythms\n(Figure 6c) we notice the same pattern.\nWe can see from the Accelerando and Ritardando\nrhythms (Figure 6d and 6e) that Φis poorly correlated, in-\ndicating the affect of the interference from other oscilla-\ntors in the system. The AFNNs response shows a signiﬁ-\ncant improvement, but still has low minimum values. This\nmay be due to the fact that the adaptive rule depends on\nthe amplitude of the oscillator, and therefore a frequency\nchange may not be picked up straight away. Changing the\noscillator model parameters to introduce more amplitude\ndamping may help here. Nevertheless the AFNN model\nstill performs signiﬁcantly better than the GFNN, with a\nmuch lower oscillator density.\n6. CONCLUSIONS\nIn this paper we proposed a novel Adaptive Frequency\nNeural Network model (AFNN) that extends GFNNs with\na Hebbian learning rule to the oscillator frequencies, at-\ntracting them to local areas of resonance. Where previ-\nous work with GFNNs focused on frequency and ampli-\ntude responses, we evaluated the outputs on their weighted\nphase response, considering that phase information is crit-\nical for pulse detection tasks. We conducted an experi-\nment partially reproducing Velasco and Large’s [23] and\nLarge et al.’s [17] studies for comparison, adding two newrhythm categories for dynamic pulses. When compared\nwith GFNNs, we showed an improved response by AFNNs\nto rhythmic stimuli with both steady and varying pulse fre-\nquencies.\nAFNNs allow for a great reduction in the density of the\nnetwork, which can improve the way the model can be\nused in tandem with other machine learning models, such\nas neural networks or classiﬁers. Furthermore the system\nfunctions fully online for use in real time. In future we\nwould like to explore this possibility by implementing a\ncomplete beat-tracking system with an AFNN at its core.\nWe have a lot of exploration to do with regards to the\nGFNN/AFNN parameters, including the testing values for\nthe adaptive frequency rule, oscillator models and inter-\nnal connectivity. The outcome of this exploration may im-\nprove the results presented here.\nThe mode-locking to high order integer ratios, nonlin-\near response, and internal connectivity set GFNNs apart\nfrom many linear ﬁltering methods such as the resonating\ncomb ﬁlters and Kalman ﬁlters used in many signal pre-\ndiction tasks. Coupled with frequency adaptation we be-\nlieve that that the AFNN model provides very interesting\nprospects for applications in MIR and further aﬁeld. In\nfuture we would like to explore this possibility by imple-\nmenting a complete beat-tracking system with an AFNN at\nits core and perform an evaluation with more realistic MIR\ndatasets.\n7. ACKNOWLEDGEMENTS\nAndrew J. Lambert is supported by a PhD studentship from\nCity University London.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 658. REFERENCES\n[1] Paul E. Allen and Roger B. Dannenberg. Tracking mu-\nsical beats in real time. In Proceedings of the 1990 In-\nternational Computer Music Conference , pages 140–3,\nSan Fancisco, CA, 1990.\n[2] Vassilis Angelis, Simon Holland, Paul J. Upton, and\nMartin Clayton. Testing a Computational Model of\nRhythm Perception Using Polyrhythmic Stimuli. Jour-\nnal of New Music Research , 42(1):47–60, 2013.\n[3] Sebastian B ¨ock, Florian Krebs, and Gerhard Widmer.\nAccurate tempo estimation based on recurrent neural\nnetworks and resonating comb ﬁlters. In Proceedings\nof the 16th International Society for Music Information\nRetrieval Conference , pages 625–31, Malaga, Spain,\n2015.\n[4] Simon Grondin. Psychology of Time . Emerald Group\nPublishing, 2008.\n[5] Peter Grosche, Meinard M ¨uller, and Craig Stuart Sapp.\nWhat Makes Beat Tracking Difﬁcult? A Case Study on\nChopin Mazurkas. In in Proceedings of the 11th Inter-\nnational Society for Music Information Retrieval Con-\nference, 2010 , pages 649–54, Utrecht, Netherlands,\n2010.\n[6] Andre Holzapfel, Matthew E.P. Davies, Jos ´e R. Zap-\nata, Jo ˜ao L. Oliveira, and Fabien Gouyon. Selective\nSampling for Beat Tracking Evaluation. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n20(9):2539–48, 2012.\n[7] Mari R. Jones. Time, our lost dimension: Toward a new\ntheory of perception, attention, and memory. Psycho-\nlogical Review , 83(5):323–55, 1976.\n[8] Rudolph E. Kalman. A New Approach to Linear Fil-\ntering and Prediction Problems. Journal of Basic Engi-\nneering , 82(1):35–45, 1960.\n[9] Richard Kempter, Wulfram Gerstner, and J Leo\nVan Hemmen. Hebbian learning and spiking neurons.\nPhysical Review E , 59(4):4498–514, 1999.\n[10] Anssi P. Klapuri, Antti J. Eronen, and Jaakko T. As-\ntola. Analysis of the meter of acoustic musical signals.\nIEEE Transactions on Audio, Speech, and Language\nProcessing , 14(1):342–55, 2006.\n[11] Andrew Lambert, Tillman Weyde, and Newton Arm-\nstrong. Beyond the Beat: Towards Metre, Rhythm and\nMelody Modelling with Hybrid Oscillator Networks.\nInJoint 40th International Computer Music Confer-\nence and 11th Sound & Music Computing conference ,\nAthens, Greece, 2014.\n[12] Andrew Lambert, Tillman Weyde, and Newton Arm-\nstrong. Studying the Effect of Metre Perception on\nRhythm and Melody Modelling with LSTMs. In Tenth\nArtiﬁcial Intelligence and Interactive Digital Enter-\ntainment Conference , 2014.[13] Andrew J. Lambert, Tillman Weyde, and Newton Arm-\nstrong. Perceiving and Predicting Expressive Rhythm\nwith Recurrent Neural Networks. In 12th Sound & Mu-\nsic Computing Conference , Maynooth, Ireland, 2015.\n[14] Edward W. Large. Beat tracking with a nonlinear oscil-\nlator. In Working Notes of the IJCAI-95 Workshop on\nArtiﬁcial Intelligence and Music , pages 24–31, 1995.\n[15] Edward W. Large. Neurodynamics of Music. In\nMari R. Jones, Richard R. Fay, and Arthur N. Pop-\nper, editors, Music Perception , number 36 in Springer\nHandbook of Auditory Research, pages 201–231.\nSpringer New York, 2010.\n[16] Edward W. Large, Felix V . Almonte, and Marc J.\nVelasco. A canonical model for gradient frequency\nneural networks. Physica D: Nonlinear Phenomena ,\n239(12):905–11, 2010.\n[17] Edward W. Large, Jorge A. Herrera, and Marc J. Ve-\nlasco. Neural networks for beat perception in musi-\ncal rhythm. Frontiers in Systems Neuroscience , 9(159),\n2015.\n[18] Edward W. Large and John F. Kolen. Resonance and\nthe Perception of Musical Meter. Connection Science ,\n6(2-3):177–208, 1994.\n[19] Fred Lerdahl and Ray Jackendoff. A generative theory\nof tonal music . MIT press, Cambridge, Mass., 1983.\n[20] J. Devin McAuley. Perception of time as phase: To-\nward an adaptive-oscillator model of rhythmic pattern\nprocessing . PhD thesis, Indiana University Blooming-\nton, 1995.\n[21] Ludovic Righetti, Jonas Buchli, and Auke J. Ijspeert.\nDynamic hebbian learning in adaptive frequency oscil-\nlators. Physica D: Nonlinear Phenomena , 216(2):269–\n81, 2006.\n[22] Eric D. Scheirer. Tempo and beat analysis of acoustic\nmusical signals. The Journal of the Acoustical Society\nof America , 103(1):588–601, 1998.\n[23] Marc J. Velasco and Edward W. Large. Pulse Detection\nin Syncopated Rhythms using Neural Oscillators. In\n12th International Society for Music Information Re-\ntrieval Conference , pages 185–90, Miami, FL, 2011.\n[24] Nick Whiteley, Ali T. Cemgil, and Simon J. Godsill.\nBayesian Modelling of Temporal Structure in Musical\nAudio. In Proceedings of the 7th International Society\nfor Music Information Retrieval Conference , pages 29–\n34, Victoria, Canada, 2006.66 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Genre Specific Dictionaries for Harmonic/Percussive Source Separation.",
        "author": [
            "Clement Laroche",
            "Hélène Papadopoulos",
            "Matthieu Kowalski",
            "Gaël Richard"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417147",
        "url": "https://doi.org/10.5281/zenodo.1417147",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/042_Paper.pdf",
        "abstract": "Blind source separation usually obtains limited perfor- mance on real and polyphonic music signals. To overcome these limitations, it is common to rely on prior knowledge under the form of side information as in Informed Source Separation or on machine learning paradigms applied on a training database. In the context of source separation based on factorization models such as the Non-negative Matrix Factorization, this supervision can be introduced by learning specific dictionaries. However, due to the large diversity of musical signals it is not easy to build sufficiently compact and precise dictionaries that will well characterize the large array of audio sources. In this paper, we argue that it is relevant to construct genre- specific dictionaries. Indeed, we show on a task of harmonic/percussive source separation that the dictionaries built on genre-specific training subsets yield better perfor- mances than cross-genre dictionaries.",
        "zenodo_id": 1417147,
        "dblp_key": "conf/ismir/LarochePKR16",
        "content": "GENRE SPECIFIC DICTIONARIES FOR HARMONIC/PERCUSSIVE\nSOURCE SEPARATION\nCl´ement Laroche1,2H´el`ene Papadopoulos2Matthieu Kowalski2,3Ga¨el Richard1\n1LTCI, CNRS, T ´el´ecom ParisTech, Univ Paris-Saclay, Paris, France\n2Univ Paris-Sud-CNRS-CentraleSupelec, L2S, Gif-sur-Yvette, France\n3Parietal project-team, INRIA, CEA-Saclay, France\n1name.lastname@telecom-paristech.fr,2name.lastname@lss.supelec.fr\nABSTRACT\nBlind source separation usually obtains limited perfor-\nmance on real and polyphonic music signals. To overcome\nthese limitations, it is common to rely on prior knowledge\nunder the form of side information as in Informed Source\nSeparation or on machine learning paradigms applied on\na training database. In the context of source separation\nbased on factorization models such as the Non-negative\nMatrix Factorization , this supervision can be introduced\nby learning speciﬁc dictionaries. However, due to the\nlarge diversity of musical signals it is not easy to build\nsufﬁciently compact and precise dictionaries that will well\ncharacterize the large array of audio sources. In this\npaper, we argue that it is relevant to construct genre-\nspeciﬁc dictionaries. Indeed, we show on a task of\nharmonic/percussive source separation that the dictionaries\nbuilt on genre-speciﬁc training subsets yield better perfor-\nmances than cross-genre dictionaries.\n1. INTRODUCTION\nSource separation is a ﬁeld of research that seeks to\nseparate the components of a recorded audio signal. Such\na separation has many applications in music such as up-\nmixing [9] (spatialization of the sources) or automatic\ntranscription [35] (it is easier to work on single sources).\nThe separation task is difﬁcult due to the complexity and\nthe variability of the music mixtures.\nThe large collection of audio signals can be classiﬁed\ninto various musical genres [34]. Genres are labels cre-\nated and used by humans for categorizing and describing\nmusic. They have no strict deﬁnitions and boundaries but\nparticular genres share characteristics typically related to\ninstrumentation, rhythmic structure, and pitch content of\nthe music. This resemblance between two pieces of music\nc/circlecopyrtCl´ement Laroche, H ´el`ene Papadopoulos, Matthieu\nKowalski, Ga ¨el Richard. Licensed under a Creative Commons Attribu-\ntion 4.0 International License (CC BY 4.0). Attribution: Cl´ement\nLaroche, H ´el`ene Papadopoulos, Matthieu Kowalski, Ga ¨el Richard.\n“Genre speciﬁc dictionaries for harmonic/percussive source separation”,\n17th International Society for Music Information Retrieval Conference,\n2016.has been used as an information to improve chord tran-\nscription [23, 27] or downbeat detection [13] algorithms.\nGenre information can be obtained using annotated labels.\nWhen the genre information is not available, it can be\nretrieved using automatic genre classiﬁcation algorithms\n[26, 34]. Such classiﬁcation have never been used to\nguide a source separation problem and this may be due\nto the lack of annotated databases. The recent availability\nof large evaluation databases for source separation that\nintegrate genre information motivates the development of\nsuch approaches. Furthermore, Most datasets used for\nBlind Audio Source Separation (BASS) research are small\nin size and they do not allow for a thorough comparison of\nthe source separation algorithms. Using a larger database\nis crucial to benchmark the different algorithms.\nIn the context of BASS, Non-negative Matrix Factoriza-\ntion (NMF) is a widely used method. The goal of NMF is\nto approximate a data matrix V∈Rn×m\n+ as\nV≈˜V=WH (1)\nwithW∈Rn×k\n+,H∈Rk×m\n+ and wherekis the rank\nof factorization [21]. In audio signal processing, the input\ndata is usually a Time-Frequency representation such as\na Short Time Fourier Transform (STFT) or a constant-\nQ transform spectrogram. Blind source separation is a\ndifﬁcult problem and the plain NMF decomposition does\nnot provide satisfying results. To obtain a satisfying\ndecomposition, it is necessary to exploit various features\nthat make each source distinguishable from one another.\nSupervised algorithms in the NMF framework exploit\ntraining data or prior information in order to guide the\ndecomposition process. For example, information from the\nscores or from midi signals can be used to initialize the\nlearning process [7]. The downside of these approaches is\nthat they require well organized prior information that is\nnot always available. Another supervised method consists\nin performing prior training on speciﬁc databases. A\ndictionary matrix Wtrain can be learned from database\nin order to separate the target instrument [16, 37]. Such\nmethod requires minimum tuning from the user. However,\nwithin different music pieces of an evaluation database, the\nsame instrument can sound differently depending on the\nrecording conditions and post processing treatments.407In this paper, we focus on the task of Harmonic Per-\ncussive Source Separation (HPSS). HPSS has numerous\napplications as a preprocessing step for other audio tasks.\nFor example the HPSS algorithm [8] can be used as a\npreprocessing step to increase the performance for singing\npitch extraction and voice separation [14]. Similarly, beat\ntracking [6] and drum transcription algorithms [29] are\nmore accurate if the harmonic instruments are not part of\nthe analyzed signal.\nWe built our algorithm using the method developed\nin [20]: an unconstrained NMF decomposes the audio\nsignal in a sparse orthogonal part that are well suited for\nrepresenting the harmonic component, while the percus-\nsive part is represented by a regular nonnegative matrix\nfactorization decomposition. In [19], we have adapted\nthe algorithm using a trained drum dictionary to improve\nthe extraction of the percussive instruments. As the user\ndatabases typically cover a wide variety of genres, instru-\nmentation may strongly differ from one piece to another. In\norder to better manage the variability and to build effective\ndictionaries, we propose here to use genre speciﬁc training\ndata.\nThe main contribution of this article is that we develop\na genre speciﬁc method to build NMF drum dictionaries\nthat gives consistent and robust results on a HPSS task.\nThe genre speciﬁc dictionaries are able to improve the\nseparation score compared to a universal dictionary trained\nfrom all available data (i.e. a cross-genre dictionary).\nThe rest of the paper is organized as follows. Section\n2 deﬁnes the context of our work, Section 3 presents the\nproposed algorithm while Section 4 describes the construc-\ntion of speciﬁc dictionaries. Finally Section 5 details the\nresults of the HPSS on 65audio ﬁles and we suggest some\nconclusions in Section 6.\n2. TOWARD GENRE SPECIFIC INFORMATION\n2.1 Genre information\nMusical genre is one of the most prominent high level mu-\nsic descriptors. Electronic Music Distribution has become\nmore and more popular in recent years and music cata-\nlogues never stop to increase (the biggest online services\nnow propose around 30 million tracks). In that context,\nassociating a genre to a musical piece is crucial to help\nusers ﬁnding what they are looking for. As mentioned in\nthe introduction, genre information has been used as a cue\nto improve some content-based music information retrieval\nalgorithms. If an explicit deﬁnition of musical genres is\nnot really available [3], musical genre classiﬁcation can be\nperformed automatically [24].\nSource separation has been used extensively in order to\nhelp the genre classiﬁcation process [18,30] but, at the best\nof our knowledge, the genre information has never been\nexploited to guide source separation algorithm.\n2.2 Methods for dictionary learning\nAudio data is largely redundant as it often contains mul-\ntiple correlated versions of the same physical event (note,drum hits...) [33] hence the idea to exploit this redundancy\nto reduce the amount of information necessary for the\nrepresentation of a musical signal.\nMany rank reduction methods, such as Single Value De-\ncomposition (K-SVD) [1], Vector Quantization (VQ) [10],\nPrincipal Component Analysis (PCA) [15], or Non neg-\native matrix factorization (NMF) [32] are based on the\nprinciple that our observations can be described by a sparse\nsubset of atoms taken from a redundant representation.\nThese methods provide a small subset of relevant templates\nthat are later used to guide the extraction of a target\ninstrument.\nBuilding a dictionary using K-SVD has been a suc-\ncessful approach in image processing [39]. However this\nmethod does not scale well to process large audio signals\nas the computational time is unrealistic. Thus a genre\nspeciﬁc dictionary scenario cannot be considered in this\nframework.\nVQ has been mainly used for audio compression [10]\nand PCA has been used for voice extraction [15]. However\nthese methods have not been used yet as a pre-processing\nstep to build a dictionary.\nFinally, in the NMF framework, some work has been\ndone to perform a decomposition with learned dictionaries.\nIn [12], a dictionary is built using a physical model of the\npiano. This method is not adapted to build genre speciﬁc\ndictionaries as the model cannot easily take into account\nthe genre information. A second way to build a dictionary\nis to directly use the STFT of an instrument signal [37].\nThis method does not scale well if the training data is\nlarge, thus it is not possible to use it to build genre speciﬁc\ndictionaries. Finally, another method to build a dictionary\nis to compute a NMF decomposition on a large training set\nspeciﬁc to the target source [31]. After the optimization\nprocess of the NMF, the Wmatrix from this decomposition\nis used as a ﬁxed dictionary matrix Wtrain . This method\ndoes not give satisfying results on pitched instruments\n(i.e., harmonic instruments) and the dictionary needs to be\nadaptated for example using linear ﬁltering on the ﬁxed\ntemplates [16]. Compared to state of the art methods, ﬁxed\ndictionaries provide good results for HPSS [19]. However,\nthe results have a high variance because the dictionaries\nare learned on general data that do not take into account\nthe large variability of drum sounds. A nice property of\nthe NMF framework is that the rank of the factorization\ndetermines the ﬁnal size of the dictionary and it can be\nchosen small enough to obtain a strong compression of\nthe original data. The limitations of the current methods\nmotivated us to build genre speciﬁc data using NMF in\norder to obtain relevant compact dictionaries.\n2.3 Genre information for HPSS\nCurrent state-of-the-art unsupervised methods for HPSS\nsuch as complementary diffusion [28] and constrained\nNMF [5] cannot be easily adapted to use genre informa-\ntion. We will not discuss these methods in this article.\nHowever supervised methods can be modiﬁed to uti-\nlize genre information. In [17] the drum source sepa-408 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016ration is done using a Non-Negative Matrix Partial Co-\nFactorization (NMPCF). The spectrogram of the signal\nand the drum-only data (obtained from prior learning) are\nsimultaneously decomposed in order to determine com-\nmon basis vectors that capture the spectral and temporal\ncharacteristics of the drum sources. The percussive part of\nthe decomposition is constrained while the harmonic part\nis completely unconstrained. As a result, the harmonic part\ntends to decompose a lot of information from the signal\nand the separation is not satisfactory (i.e., the harmonic\npart contains some percussive instruments). A drawback of\nthis method is that it does not scale when the training data\nis large and the computation time is signiﬁcantly larger\ncompared to other methods.\nBy contrast, the approach introduced and detailled\nin [19, 20] appears to be a good candidate to test the genre\nspeciﬁc dictionaries: they can be easily integrated to the\nalgorithm without increasing the computation time.\n3. STRUCTURED PROJECTIVE NMF (SPNMF)\n3.1 Principle of the SPNMF\nUsing a similar model as in our preliminary work [20], let\nVbe the magnitude spectrogram of the input data. The\nmodel is then given by\nV≈˜V=VH+VP, (2)\nwithVPthe spectrogram of the percussive part and VH\nthe spectrogram of the harmonic part. VHis approximated\nby the projective NMF decomposition [38] while VPis\ndecomposed by NMF components which leads to:\nV≈˜V=WHWT\nHV+WPHP. (3)\nThe data matrix is approximated by an almost orthogonal\nsparse part that codes the harmonic instruments VH=\nWHWT\nHVand a non constrained NMF part that codes the\npercussive instruments VP=WPHP. As a fully unsu-\npervised SPNMF model does not allow for a satisfying\nharmonic/percussive source separation [20], we propose\nhere to use a ﬁxed genre speciﬁc drum dictionary WPin\nthe percussive part of the SPNMF.\n3.2 Algorithm optimization\nIn order to obtain such a decomposition, we can use a\nmeasure of ﬁt D(x|y)between the data matrix Vand the\nestimated matrix ˜V.D(x|y)is a scalar cost function and\nin this article, we use the Itakura Saito (IS) divergence. A\ndiscussion about the possible use of other divergences can\nbe found in [19].\nThe SPNMF model gives the optimization problem:\nmin\nWH,WP,HP≥0D(V|WHWT\nHV+WPHP) (4)\nA solution to this problem can be obtained by iterative\nmultiplicative update rules following the same strategy\nas in [22, 38]. Using formula from Appendix 7, the\noptimization process is given in Algorithm 1, where ⊗is\nthe Hadamard product and all division are element-wise\noperation.Input:V∈Rm×n\n+ andWtrain∈Rm×e\n+ Output:\nWH∈Rm×k\n+ andHP∈Re×n\n+Initialization;\nwhilei≤number of iterations do\nHP←HP⊗[∇HPD(V|˜V)]−\n[∇HPD(V|˜V)]+\nWH←WH⊗[∇WHD(V|˜V)]−\n[∇WHD(V|˜V)]+\ni=i+ 1\nend\nXP=WtrainHPandXH=WHWT\nHV\nAlgorithm 1: SPNMF with a ﬁxed trained drum dictio-\nnary matrix.\n3.3 Signal reconstruction\nThe percussive signal xp(t)is synthesized using the mag-\nnitude percussive spectrogram XP=WPHP. To recon-\nstruct the phase of the percussive part, we use a Wiener\nﬁlter [25] to create a percussive mask as:\nMP=X2\nP\nX2\nH+X2\nP(5)\nTo retrieve the percussive signal as:\nxp(t) =SFTF−1(MP⊗X). (6)\nWhereXis the complex spectrogram of the mixture. We\nuse a similar procedure for the harmonic part.\n4. CONSTRUCTION OF THE DICTIONARY\nIn this section we detail the building process of the drum\ndictionary. We present in Section 4.1 tests conducted on\nthe SiSEC 2010 database [2] in order to ﬁnd the optimal\nsize to build the genre speciﬁc dictionaries. In Section\n4.2 we describe the training and the evaluation database.\nFinally, in Section 4.3, we detail the protocol to build the\ngenre speciﬁc dictionaries.\n4.1 Optimal size for the dictionary\nThe NMF model is given by (1). If Vis the power\nspectrum of a drum signal, The matrix Wis adictionary\nor a set of patterns that codes the frequency information of\nthe drum. The ﬁrst step to build a NMF drum dictionary is\nto select the rank of factorization. In order to avoid over-\nﬁtting, the algorithm is optimized using databases different\nfrom the database used for evaluation, described in Section\n4.2.\nWe run the optimization tests on the public SiSec\ndatabase [2]. The database is composed of four poly-\nphonic real-world music excerpts and each music signal\ncontains percussive, harmonic instruments and vocals. The\nduration of the recordings is ranging from 14to24s. In\nthe context of HPSS, following the same protocol as in [5],\nwe do not consider the vocal part and we build the mixture\nsignals from the percussive and harmonic instruments only.\nThe signals are sampled at 44.1kHz. We compute the\nSTFT with a 2048 sample long Hann window with a 50%Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 409overlap. Furthermore, the rank of factorization of the\nharmonic part of the SPNMF algorithm is set to k= 100 ,\nas in [19].\nA ﬁxed drum dictionary is built using the database\nENST-Drums [11]. For this, we concatenate 30ﬁles where\nthe drummer is playing a drum phrase that result in an\nexcerpt of around 10min duration. We then compute an\nNMF decomposition with different ranks of factorization\n(k= 12 ,k= 50 ,k= 100 ,k= 200 ,k= 300 ,k= 500 ,\nk= 1000 andk= 2000 ) on the drum signal alone to\nobtain 8drum dictionaries.\nThese dictionaries are then used to perform a HPSS on\nthe four songs of the SiSEC database using the SPNMF\nalgorithm (see Algorithm 1). The results are compared by\nmeans of the Signal-to-Distortion Ratio (SDR), the Signal-\nto-Interference Ratio (SIR) and the Signal-to-Artifact Ra-\ntio (SAR) of each of the separated sources using the BSS\nEval toolbox provided in [36].\nk=12 k=50 k=100 k=200 k=300 k=500 k=1000 k=2000051015Mean decomposition resultsdB\n  \nSDR\nSIR\nSAR\nFigure 1 : Inﬂuence of k on the S(D/I/A)R on the SiSEC\ndatabase.\nThe results in Figure 1 show that the optimal value\nfor the SDR and SIR is reached for k= 100 , then the\nSDR decreases for k/greaterorequalslant200. Fork/greaterorequalslant500the harmonic\nsignal provided by the algorithm contains most of the\noriginal signal therefore the SAR is very high but the\ndecomposition quality is poor. For the rest of the article,\nthe size of the drum dictionaries will be k= 100 .\n4.2 Training and evaluation database\nThe evaluation tests are conducted on the Medley-dB\ndatabase [4] composed of polyphonic real-world music\nexcerpts. It consists in 122music signals and 85of them\ncontain percussive instruments, harmonic instruments and\nvocals. The signals that do not contain a percussive part\nare excluded from evaluation. The genres are distributed\nas follows: Classical (8songs), Singer/Songwriter (17\nsongs), Pop(10songs), Rock (20songs), Jazz (11songs),\nElectronic/Fusion (13songs) and World/Folk (6songs). It\nis important to note that, because the notion of genre is\nquite subjective (see Section 2), the Medley-dB database\nuses general genre labels that cannot be considered to be\nprecise. There are many instances where a song could\nhave fallen in multiple genres, and the choices were made\nso that each genre would be as acoustically homogeneous\nas possible. Moreover, as we are only working with theGenre Artist Song\nClassical JoelHelander Deﬁnition\nMatthewEntwistle AnEveningWithOliver\nMusicDelta Beethoven\nElectronic/Fusion EthanHein 1930sSynthAndUprightBass\nTablaBreakbeatScience Animoog\nTablaBreakbeatScience Scorpio\nJazz CroqueMadame Oil\nMusicDelta BebopJazz\nMusicDelta ModalJazz\nPop DreamersOfTheGhetto HeavyLove\nNightPanther Fire\nStrandOfOaks Spacestation\nRock BigTroubles Phantom\nMeaxic TakeAStep\nPurlingHiss Lolita\nSinger/Songwriter AimeeNorwich Child\nClaraBerryAndWooldog Boys\nInvisibleFamiliars DisturbingWildlife\nWorld/Folk AimeeNorwich Flying\nKarimDouaidy Hopscotch\nMusicDelta ChineseYaoZu\nNon speciﬁc JoelHelander Deﬁnition\nTablaBreakbeatScience Animoog\nMusicDelta BebopJazz\nDreamersOfTheGhetto HeavyLove\nBigTroubles Phantom\nAimeeNorwich Flying\nMusicDelta ChineseYaoZu\nTable 1 : Song selected for the training database.\ninstrumental part of the song (the vocals are omitted), the\nPoplabel (for example) is similar to the Singer/Songwriter .\nWe separate the database into training and evaluation ﬁles,\nas detailed in the next section.\n4.3 Genre speciﬁc dictionaries\nSeven genre-speciﬁc drum dictionaries are built using 3\nsongs of each genre. In addition, a cross-genre drum\ndictionary is built using half of one song of each genre.\nFinally, a dictionary is built using the 10min excerpt\nof pure drum signals from the ENST-Drums database\ndescribed in Section 4.1. The Medley-dB ﬁles selected for\ntraining are given in Table 1 and excluded from evaluation.\nWith the results from Section 4.1 the dictionaries are\nbuilt as follows: for every genre speciﬁc subset of the\ntraining database, we perform a NMF on the drum signals\nwithk= 100 . The resulting Wmatrices of the NMF are\nthen used in the SPNMF algorithm as the WPmatrix (see\nAlgorithm 1).\n5. RESULTS\nIn this section, we present the results of the SPNMF with\nthe genre speciﬁc dictionaries on the evaluation database\nfrom Medley-dB.\n5.1 Comparison of the dictionaries\nWe perform a HPSS on the audio ﬁles using the SPNMF\nalgorithm with the 9dictionaries built in Section 4.3. The\nresults on each song are then sorted by genres and the\naverage results are displayed using box-plots. Each box-\nplot is made up of a central line indicating the median of410 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016the data, upper and lower box edges indicating the 1stand\n3rdquartiles while the whiskers indicate the minimum and\nmaximum values.\nFigures 2, 3 and 4 show the SDR, SAR and SIR results\nfor all the dictionaries on the Popsubset, giving an overall\nidea of the performance of the dictionaries inside a speciﬁc\nsub-database. The Popdictionary leads to the highest SDR\nand SIR and the non speciﬁc dictionaries are not perform-\ning as well. On this sub-database, the genre speciﬁc data\ngives relevant information to the algorithm. As stated in\nSection 4.2, some genres are similar to others, explaining\nwhy the Rock and the Singer dictionaries are also providing\ngood results. An interesting result is that compared to the\nnon speciﬁc dictionaries, the Pop dictionary has a lower\nvariance. Genre information allows for a higher robustness\nto the variety of the songs within the same genre. Samples\nof the audio results can be found on the website1.\nFigure 2 : Percussive (left bar)/Harmonic (right bar) SDR\nresults on the Popsub-database using the SPNMF with\nthe 9 dictionaries.\nFigure 3 : Percussive (left bar)/Harmonic (right bar) SIR\nresults on the Popsub-database using the SPNMF with\nthe 9 dictionaries.\nOn Table 2, we display the mean separation score for all\nthe genre speciﬁc dictionaries compared to the non speciﬁc\ndictionary. The dictionary built on the ENST-drums is\ngiving results very similar to the universal dictionary built\non the Medley-dB database. For the sake of concision\nwe only display the results using the universal dictionary\nfrom Medley-dB. On the database Singer/Songwriter ,Pop,\nRock ,Jazz andWorld/Folk , the genre speciﬁc dictionaries\n1https://goo.gl/4X2jk5\nFigure 4 : Percussive (left bar)/Harmonic (right bar) SAR\nresults on the Popsub-database using the SPNMF with\nthe 9 dictionaries.\noutperform the universal dictionary on the harmonic and\npercussive separation.\n5.2 Discussion\nThe cross-genre dictionary as well as the ENST-drum\ndictionary are outperformed by the genre speciﬁc dic-\ntionaries. The information from the music of the same\ngenre is not altered by the NMF compression and provides\ndrum templates closer to the target drum. The databases\nClassical and Electronic/Fusion are composed of songs\nwhere the drum is only playing for a few moments. Sim-\nilarly on some songs of the Electronic/Fusion database,\nthe electronic drum reproduces the same pattern during\nthe whole song making the drum part very redundant.\nAs a result, in both cases the drum dictionary does not\ncontain a sufﬁcient amount of information to outperform\nthe universal dictionary. Because of these two factors, the\ngenre speciﬁc dictionaries are not performing correctly.\nIt can be noticed that overall the harmonic separation is\ngiving much better results than the percussive extraction.\nThe ﬁxed dictionaries are creating artefact as the percus-\nsive templates do not correspond exactly to the target drum\nsignal. A possible way to alleviate this problem would\nbe to adapt the dictionaries but this would require the use\nof hyper parameters and that is not the philosophy of this\nwork [20].\n6. CONCLUSION\nUsing genre speciﬁc information in order to build more\nrelevant drum dictionaries is a powerful approach to im-\nprove the HPSS. The dictionaries still have an imprint of\nthe genre after the NMF decomposition and the additional\ninformation is properly used by the SPNMF to improve\nthe source separation quality. This is a ﬁrst step in order to\nproduce dictionaries capable of separating a wide variety\nof audio signal.\nFuture work will be dedicated into building a blind\nmethod to select the genre speciﬁc dictionary in order to\nperform the same technique on database where the genre\ninformation is not available.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 411Genre Classical Electronic/Fusion Jazz Pop Rock Singer/Songwriter World/Folk\nPercussive separation\nGenre speciﬁc (dB)\nSDR -1.6 -0.6 0.4 2.5 -0.2 0.6 0.4\nSIR 8.2 15.2 9.6 12.3 19.8 11.5 6.1\nSAR 5.9 0.3 2.1 3.4 0.3 4.5 16.3\nNon speciﬁc (dB)\nSDR -0.0 -0.3 -0.7 2.0 -2.2 -0.0 -3.6\nSIR 11.3 17.0 9.6 12.6 18.3 13.0 2.8\nSAR 8.1 0.4 0.9 2.7 2.3 1.8 12.1\nHarmonic Separation\nGenre speciﬁc (dB)\nSDR 7.5 1.6 13.0 5.1 2.1 7.2 4.9\nSIR 10.6 1.8 13.3 5.0 2.2 11.5 13.5\nSAR 18.2 23.5 28.5 24.5 36.0 28.5 22.7\nNon speciﬁc (dB)\nSDR 6.0 1.3 12.7 4.8 1.9 7.5 4.6\nSIR 7.1 1.4 12.8 4.9 2.9 7.5 13.3\nSAR 27.2 27.7 29.9 26.2 34.3 31.9 21.6\nTable 2 : Average SDR, SIR and SAR results on the Medley-dB database.\n7. APPENDIX: SPNMF WITH THE IS\nDIVERGENCE\nThe Itakura Saito divergence gives us the problem,\nmin\nWH,WP,HP≥0V\n˜V−log(V\n˜V)−1.\nThe gradient wrt WHgives\n[∇WHD(V|˜V)]−\ni,j= (ZVTWH)i,j+ (VZTWH)i,j,\nwithZi,j= (V\nWHWT\nHV+WPHP)i,j. The positive part of the\ngradient is\n[∇WHD(V|˜V)]+\ni,j= (φVTWH)i,j+ (VφTWH)i,j,\nwith\nφi,j= (I\nWHWT\nHV+WPHP)i,j.\nandI∈Rf×t;∀i,j I i,j= 1.\nSimilarly, the gradient wrt HPgives\n[∇HPD(V|˜V)]−=WT\nPV\nand\n[∇HPD(V|˜V)]+= 2WT\nPWHWT\nHV+WT\nPWPHP.\n8. REFERENCES\n[1] M. Aharon, M. Elad, and Alfred A. Bruckstein. K-\nSVD: An algorithm for designing overcomplete dictio-\nnaries for sparse representation. IEEE Transactions on\nSignal Processing , pages 4311–4322, 2006.\n[2] S. Araki, A. Ozerov, V . Gowreesunker, H. Sawada,\nF. Theis, G. Nolte, D. Lutter, and N. Duong. The 2010\nsignal separation evaluation campaign: audio source\nseparation. In Proc. of LVA/ICA , pages 114–122, 2010.[3] J.J. Aucouturier and F. Pachet. Representing musical\ngenre: A state of the art. Journal of New Music\nResearch , pages 83–93, 2003.\n[4] R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Can-\nnam, and J. Bello. MedleyDB: A multitrack dataset for\nannotation-intensive MIR research. In Proc. of ISMIR ,\n2014.\n[5] F. Canadas-Quesada, P. Vera-Candeas, N. Ruiz-Reyes,\nJ. Carabias-Orti, and P. Cabanas-Molero. Percus-\nsive/harmonic sound separation by non-negative ma-\ntrix factorization with smoothness/sparseness con-\nstraints. EURASIP Journal on Audio, Speech, and\nMusic Processing , pages 1–17, 2014.\n[6] D. Ellis. Beat tracking by dynamic programming. Jour-\nnal of New Music Research , pages 51–60, 2007.\n[7] S. Ewert and M. M ¨uller. Score-informed source sepa-\nration for music signals. Multimodal music processing ,\npages 73–94, 2012.\n[8] D. Fitzgerald. Harmonic/percussive separation using\nmedian ﬁltering. In Proc. of DAFx , 2010.\n[9] D. Fitzgerald. Upmixing from mono-a source separa-\ntion approach. In Proc. of IEEE DSP , pages 1–7, 2011.\n[10] A. Gersho and R.M. Gray. Vector quantization and sig-\nnal compression . Springer Science & Business Media,\n2012.\n[11] O. Gillet and G. Richard. Enst-drums: an extensive\naudio-visual database for drum signals processing. In\nProc. of ISMIR , pages 156–159, 2006.\n[12] R. Hennequin, B. David, and R. Badeau. Score in-\nformed audio source separation using a parametric412 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016model of non-negative spectrogram. In Proc. of IEEE\nICASSP , 2011.\n[13] J. Hockman, M. Davies, and I. Fujinaga. One in the\njungle: Downbeat detection in hardcore, jungle, and\ndrum and bass. In Proc. of ISMIR , pages 169–174,\n2012.\n[14] C. Hsu, D. Wang, J.R. Jang, and K. Hu. A tandem\nalgorithm for singing pitch extraction and voice sepa-\nration from music accompaniment. IEEE Transactions\non Audio, Speech, and Language Processing. , pages\n1482–1491, 2012.\n[15] P. Huang, S.D. Chen, P. Smaragdis, and M. Hasegawa-\nJohnson. Singing-voice separation from monaural\nrecordings using robust principal component analysis.\nInProc. of IEEE ICASSP , pages 57–60, 2012.\n[16] X. Jaureguiberry, P. Leveau, S. Maller, and J. Burred.\nAdaptation of source-speciﬁc dictionaries in non-\nnegative matrix factorization for source separation. In\nProc. of IEEE ICASSP , pages 5–8, 2011.\n[17] M. Kim, J. Yoo, K. Kang, and S. Choi. Nonnegative\nmatrix partial co-factorization for spectral and tempo-\nral drum source separation. Journal of Selected Topics\nin Signal Processing , pages 1192–1204, 2011.\n[18] A. Lampropoulos, P. Lampropoulou, and\nG. Tsihrintzis. Musical genre classiﬁcation enhanced\nby improved source separation technique. In Proc. of\nISMIR , pages 576–581, 2005.\n[19] C. Laroche, M. Kowalski, H. Papadopoulous, and\nG.Richard. Structured projective non negative ma-\ntrix factorization with drum dictionaries for har-\nmonic/percussive source separation. Submitted to IEEE\nTransactions on Acoustics, Speech and Signal Process-\ning.\n[20] C. Laroche, M. Kowalski, H. Papadopoulous, and\nG.Richard. A structured nonnegative matrix factoriza-\ntion for source separation. In Proc. of EUSIPCO , 2015.\n[21] D. Lee and S. Seung. Learning the parts of objects by\nnonnegative matrix factorization. Nature , pages 788–\n791, 1999.\n[22] D. Lee and S. Seung. Algorithms for non-negative\nmatrix factorization. Proc. of NIPS , pages 556–562,\n2001.\n[23] K. Lee and M. Slaney. Acoustic chord transcription and\nkey extraction from audio using key-dependent hmms\ntrained on synthesized audio. IEEE Transactions on\nAudio, Speech, and Language Processing , pages 291–\n301, 2008.\n[24] T. Li, M.Ogihara, and Q. Li. A comparative study on\ncontent-based music genre classiﬁcation. In Proc. of\nACM , pages 282–289, 2003.[25] A. Liutkus and R. Badeau. Generalized wiener ﬁltering\nwith fractional power spectrograms. In Proc. of IEEE\nICASSP , pages 266–270, 2015.\n[26] C. McKay and I. Fujinaga. Musical genre classiﬁca-\ntion: Is it worth pursuing and how can it be improved?\nInProc. of ISMIR , pages 101–106, 2006.\n[27] Y . Ni, M. McVicar, R. Santos-Rodriguez, and T. De\nBie. Using hyper-genre training to explore genre in-\nformation for automatic chord estimation. In Proc. of\nISMIR , pages 109–114, 2012.\n[28] N. Ono, K. Miyamoto, J. Le Roux, H. Kameoka, and\nS. Sagayama. Separation of a monaural audio signal\ninto harmonic/percussive components by complemen-\ntary diffusion on spectrogram. In Proc. of EUSIPCO ,\n2008.\n[29] J. Paulus and T. Virtanen. Drum transcription with\nnon-negative spectrogram factorisation. In Proc. of\nEUSIPCO , pages 1–4, 2005.\n[30] H. Rump, S. Miyabe, E. Tsunoo, N. Ono, and\nS. Sagayama. Autoregressive mfcc models for genre\nclassiﬁcation improved by harmonic-percussion sepa-\nration. In Proc. of ISMIR , pages 87–92, 2010.\n[31] M.N. Schmidt and R.K. Olsson. Single-channel speech\nseparation using sparse non-negative matrix factoriza-\ntion. In Proc. of INTERSPEECH , 2006.\n[32] P. Smaragdis and JC. Brown. Non-negative matrix\nfactorization for polyphonic music transcription. In\nWorkshop on Applications of Signal Processing to\nAudio and Acoustics , pages 177–180, 2003.\n[33] I. To ˇsi´c and P. Frossard. Dictionary learning. IEEE\nTransactions on Signal Processing , pages 27–38, 2011.\n[34] G. Tzanetakis and P. Cook. Musical genre classiﬁca-\ntion of audio signals. IEEE transactions on Speech and\nAudio Processing , pages 293–302, 2002.\n[35] E. Vincent, N. Bertin, and R. Badeau. Adaptive har-\nmonic spectral decomposition for multiple pitch es-\ntimation. IEEE Transactions on Audio, Speech, and\nLanguage Processing. , pages 528–537, 2010.\n[36] E. Vincent, R. Gribonval, and C. F ´evotte. Performance\nmeasurement in blind audio source separation. IEEE\nTransactions on Audio, Speech, Language Process. ,\npages 1462–1469, 2006.\n[37] C. Wu and A. Lerch. Drum transcription using partially\nﬁxed non-negative matrix factorization. In Proc. of\nEUSIPCO , 2008.\n[38] Z. Yuan and E. Oja. Projective nonnegative matrix\nfactorization for image compression and feature extrac-\ntion. Image Analysis , pages 333–342, 2005.\n[39] Q. Zhang and B. Li. Discriminative K-SVD for dic-\ntionary learning in face recognition. In Proc. of IEEE\nCVPR , pages 2691–2698. IEEE, 2010.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 413"
    },
    {
        "title": "A Look at the Cloud from Both Sides Now: An Analysis of Cloud Music Service Usage.",
        "author": [
            "Jin Ha Lee 0001",
            "Yea-Seul Kim",
            "Chris Hubbles"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417627",
        "url": "https://doi.org/10.5281/zenodo.1417627",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/087_Paper.pdf",
        "abstract": "Despite the increasing popularity of cloud-based music services, few studies have examined how users select and utilize these services, how they manage and access their music collections in the cloud, and the issues or challeng- es they are facing within these services. In this paper, we present findings from an online survey with 198 respons- es collected from users of commercial cloud music ser- vices, exploring their selection criteria, use patterns, per- ceived limitations, and future predictions. We also inves- tigate differences in these aspects by age and gender. Our results elucidate previously under-studied changes in mu- sic consumption, music listening behaviors, and music technology adoption. The findings also provide insights into how to improve the future design of cloud-based mu- sic services, and have broader implications for any cloud- based services designed for managing and accessing per- sonal media collections.",
        "zenodo_id": 1417627,
        "dblp_key": "conf/ismir/LeeKH16",
        "content": "A LOOK AT  THE CLOUD FROM BOTH SIDES NOW:  \nAN ANALYSIS OF CLOUD MUSIC SERVICE  USAGE  \nJin Ha Lee  Yea-Seul Kim  Chris Hubbles  \nUniversity of Washington  \njinhalee@uw.edu  University of Washington  \nyeaseul1@uw.edu  University of Washington  \nchubbles@uw.edu  \nABSTRACT  \nDespite the increasing popularity of cloud -based music \nservices, few studies have examined  how users select and \nutilize these services, how they manage and access their \nmusic collection s in the cloud, and the issues or challeng-\nes they are facing within the se services. In this paper, we \npresent findings from an online survey with 198 respons-\nes collected from users of commercial cloud music ser-\nvices , exploring  their selection criteria , use patterns , per-\nceived limitations, and future predictions. We also inves-\ntigate differences in these aspects by age and gender. Our \nresults elucidate previously under -studied changes in mu-\nsic consumption, music listening behaviors, and music \ntechnology adoption. The findings also provide insights \ninto how to improve the future design of cloud -based mu-\nsic services, and have broader implications for any cloud -\nbased services designed for managing and accessing per-\nsonal media collections.  \n1. INTRODUCTION  \nThe last decade  has been marked by  significant and rapid \nchange  in the means by wh ich people store and access \nmusic. New technologies, tools, and services have result-\ned in a plethora of choices for users. Mobile devices are \nbecoming increasingly ubiquitous , and different access \nmethods , including streaming and subscription models , \nhave started to replace the traditional model of music \nownership via personal collections  [30]. Cloud -based \nmusic services are one of the more recent ly developed \nconsumer  options for storing and accessing music, and \nthe use of cloud -based systems in general is expected to \nincrease in the near future. As the popularity of cloud \ncomputing  grow s, a number of studies have been pub-\nlished regarding uses and attitudes of cloud -based sys-\ntems (e.g., [21]) . However, few studies specifically inves-\ntigate cloud -based music s ervices ; many questions re-\ngarding the use of those services are virtually unexplored. \nFor instance, what makes  people  choose  cloud -based mu-\nsic services , given numerous streaming choices for ac-\ncessing music ? What works , and what  does not work , in \nexisting services, and how can user experience s be im-\nproved ? What opinions do users hold about  cloud -based \nservices, especially regarding the longevity, privacy, and security of such systems ? Answering these questions will \nhelp elucidate  the challenges users are facing in today’s \ncomplex music access environment, and will inform fu-\nture music access  and organization  models .  \nIn this paper, we aim to answer the following res earch \nquestions: 1) How do people commonly use cloud music \nservices and manage their cloud music collection s, and \nhow does streaming usage interact with, support, or sup-\nplant cloud music usage?; 2 ) How do users explain their  \npreference s for particular cloud music service s and func-\ntionalities ?; 3) What do users perceive as limitations of \ncurrent services , and what kinds of features do users want \nin a cloud -based music access and management system?; \nand 4) Are there  significant difference s in perception s \nand usage of cloud music services which correlate to  de-\nmographic  differences , such as age or gender ? \nThis study is part of  a larger agenda seek ing to empiri-\ncally ground current understandings of music collecting \nand information -seeking behavior . The explosive growth \nof cloud services  in the past five years has demonstrated a \nburgeoning, robust commercial market of products which \nwill benefit from new empirical analyses.  This work is \ncritical in an age where technology and society undergo \nupheava ls so frequently that previous models of human \nactivity often prove to be oversimplified or obsolete when \napplied to new problems. Empirical  work in this area has \nimplications for device and software design  and devel-\nopment , structuring of metadata, consume r behavior, and \nmusic industry planning, in addition to offering contribu-\ntions to academic theory in  multiple disciplines . \n2. RELEVANT WORK  \nCloud computing has exploded in popularity since the \nmid-2000s, and scholarly inquiry on the topic has corre-\nspondingly increased. User studies of cloud services have \nfound a variety of factors influencing consumer adoption \nand retention of cloud services, including ease of use and \non-demand ubiquity [24, 28] , functionality and perceived \nusefulness [1, 28] , accessibility ac ross web -enabled de-\nvices [21], and support for collaborative projects [21, 24] . \nWhile online music discovery and consumption has also \ngrown dramatically over the course of the nascent 21st \ncentury, cloud platforms designed specifically for music \nlistening and storage are still relatively new; for instance, \nApple iCloud and Google Play Music, two major compet-\nitors in the cloud music marketplace, both launched in \n2011. A great deal of speculative and anecdotal literature \nhas arisen around cloud music, includi ng on the cloud’s \nphilosophical implications and its potential to disrupt so- © Jin Ha Lee, Yea -Seul Kim, Chris Hubbles . Licensed un-\nder a Creative Commons Attribution 4.0 International License (CC BY \n4.0). Attribution:  Jin Ha Lee, Yea -Seul Kim, Chris Hubbles.  “A Look \nat the Cloud from Both Sides Now: An Analysis of Cloud Music Ser-\nvice Usage”. 1 7th International Society for  Music Information Retrieval \nConference, 2016.  \n299  \ncioeconomic and cultural notions of ownership [4, 22, \n30]. However, actual user attitudes toward services and \nbehavior within these services remain underexplored, re-\nflecting a gene ral lack of focus on user experience in \nMIR  studies [27]. Furthermore, cloud services afford and \nfacilitate functions such as transfer of files between de-\nvices, automated organization of files and metadata, shar-\ning, and backup, which previously were cumber some but \ncommon user tasks [3]. User behavior thus may have \nchanged significantly, or be in transition, from that de-\nscribed in studies which are only a few years old.  \nCloud music services also complement , or compete \nwith, streaming services for listeners’ ears. User behavior \non streaming services has received more empirical atten-\ntion as the popularity of platforms like Spotify and Pan-\ndora has swelled. Hagen [9] conducted a mixed -methods \nstudy to examine playlist -making beh avior in music \nstreaming services, finding a heterogeneous set of man-\nagement and use strategies. Kamalzadeh  et al. [1 4] inves-\ntigated music listening and management both online and \noffline, and found that streaming service use was less fre-\nquent than offline  listening to personal digital music col-\nlections. Lee  et al. [1 5, 16] inquired into user needs for \nmusic information services and user experience  within \ncommercial music platforms, noting increased use of \nstreaming services and exploring opinions about  services \nand features in some depth. Zhang  et al.  [31] examined \nuser behavior on Spotify through quantitative analysis of \nuse logs, focusing on device switching habits and fre-\nquency and periodicity of listening sessions. Liikkanen \nand Aman [19] conducted a la rge-scale survey of digital \nmusic habits in Finland, finding that online streaming \nthrough Spotify and YouTube were predominant. \nCesareo and Pastore [ 5] and Nguyen et al. [23]  both exe-\ncuted large -scale surveys of streaming music use to as-\nsess consumer will ingness to pay for services and stream-\ning’s effect on music purchasing and illegal downloading. \nHowever, d etailed user -centered studies which examine \nboth cloud and streaming services in concert are lacking \nin the extant literature.  \nOur study seeks to enri ch understanding s of online \nmusic listeners’ needs, desires, attitudes, and behaviors \nthrough a large -scale survey of cloud music usage. We \nalso seek to explore whether differences in behaviors and \nattitudes about cloud and streaming services correlate to \ndemographic differences, particularly age and gender . \nMusic sociology, music psychology, and music infor-\nmation studies research ers have noted gender differences \nin some aspects of music tastes [8], experiences [18], and \nlistening habits [7, 8], but not oth ers [6, 13, 26]. Technol-\nogy use can also differ markedly by gender, e.g. in choice \nof smartphone applications [25], and in adoption and use \nof mobile phones [12] and social networking services \n[10]. Comparatively little attention has been paid to \nwhether a nd how these differences are mirrored in online \nmusic service usage; exceptions include Berkers [2], who \nused Last.FM user data to examine differences in musical \ntaste between genders, and Makkonen et al. [20]  and Suki \n[29], both of whom found gender and age differences in \nonline music purchasing intentions.  3. STUDY DESIGN AND MET HOD  \nThis study is a follow -up to an earlier project which in-\nvestigated current cloud music usage and the future of \ncloud music practices through semi -structured interviews \nwith 20 a dult and 20 teen users  [17]. This study s eeks to \nvalidate findings from the interviews and surface new in-\nsights by surveying a larger number of cloud music ser-\nvice users.  \nThe online survey consisted of 24 questions which \nasked about users ’ cloud music ser vice usage, cloud mu-\nsic collection management, and general music listening \nbehavior. Our question set was generated after the com-\npletion of the interview project, and so our choice of \nquestions was partly informed by our interview findings. \nParticipants we re recruited via online venues  such as  e-\nmail lists , Facebook groups targeted for students attend-\ning the University of Washington , the first author’s social \nnetwork websites, Craigslist, and  several  online listservs \nand forums related to music (e.g., ISMIR  community \nlistserv, Allaccessplaylists reddit). We also distributed \nand mailed flyers to 50 physical venues including campus \nlocations, record shops, businesses, libraries, and com-\nmunity centers. Participants were offered an opportunity \nto enter their nam es in a raffle to win Amazon .com  gift \ncards.  \nThe survey data included quantitative numerical re-\nsponses, radio -button and check -all-that-apply multiple \nchoice  questions , and free response text boxes. Quantita-\ntive data was processed via SPSS  and Microsoft E xcel. \nAnswers from open -ended questions were qualitatively \ncoded by two coders , employing  an iterative process. The \ncodebook from [17] was adopted as an initial framework, \nand then was slightly expanded and revised after the first \nround of coding to fully represent the themes in all re-\nsponses. Afterwards, we adopted a consensus model [11] \nwhere two coders compared their coded results and dis-\ncussed instances where disagreement s in code application \noccurred, aiming to reach a consensus.  \nOur recruitment metho ds, both online and real -world, \noften centered on areas populated by young adults in their \ntwenties and thirties, and while it seems intuitively rea-\nsonable that this population would be more likely to pat-\nronize cloud services than other demographics, there  may \nbe significant cloud -using populations we did not reach. \nOur outreach efforts occurred mostly within the United \nStates, especially the Puget Sound region, and while we \nallowed for worldwide access to the survey, the majority \nof our respondents were Americans. Of our survey re-\nspondents, over 70% were male, which may not neces-\nsarily be indicative of actual cloud us age patterns.  \nDespite employing a variety of recruitment tactics and \npublicizing the survey in several waves, we received a \ntotal of 371 res ponses, of which 198 were complete re-\nsponses. Since cloud services are a relatively new service \nindustry, we speculate that our recruitment difficulties \nmay be due to a general lack of widespread adoption. \nFurthermore, many online music consumers are elect ing \nto use streaming rather than cloud platforms, making \nthem ineligible for our study.  300 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016  \n4. FINDINGS AND DISCUSSI ON \n4.1 Participants’ Demographics and Characteristics  \nThe average age of participants was 29. 7 (Stdev: 8.5). \nMost participants ( 80.8%) were from the United States , \nwith the rest from Canada, the United Kingdom , and 16 \nother countries. 7 0.7% of respondents were male, 27. 8% \nwere female, and the rest selected ‘other ’. Participants \nlistened to a wide variety of music as well as spoken -\nword  audio  (e.g., comed y, podcasts), with rock, pop, and \nelectronic music being the most  preferred genres.  \n4.2 Usage of Cloud Music Services  \nOf the three most commonly used cloud music services, \nGoogle Play was the predominant service (7 1.7%), with \nabout a quarter of respondents  using each of the other \nmajor services  (Amazon Cloud, 25.8%; Apple iCloud, \n23.7%) . These services were primarily accessed by \nsmartphone (9 1.9%), laptop (75. 8%), desktop computer \n(60.1%), and tablet (5 1.5%). Devices designed specifical-\nly for music listenin g, such as cloud -enabled home stereo \nsystems (e.g., Sonos) (10. 6%) and portable music players  \n(8.1%), were  much less common . The average reported \nlength of cloud music service use was 3 5.5 months \n(Stdev: 25. 8). The frequency of service use tended to be \nhigh; 66. 2% used them on a daily basis ( ‘almost every \nday’ or ‘more than once a day ’), and 20. 7% on a weekly \nbasis ( ‘about once a week’  or ‘a few times a week ’). \nTable 1 summarizes how participants reported  using \ncloud music services. Easier access to music w hich users  \nmay or may not own was the primary reason for using \nservices , followed by discovery, preservation, manage-\nment, and sharing purposes.  When they do use cloud ser-\nvices for discovery of new music, 59.6% reported using \nan automatically -generated playlist or using a cloud radio \nfeature, 4 1.9% relied on new music suggestions by the \nservice (e.g., advertisement s or promotion s), and 23. 7% \ntook suggestions from friends on the cloud. Approximate-\nly one out of four participants ( 25.3%) did not use cloud \nservices for discovering new music. In the prior study, \ninterviewees reported that they primarily rely on stream-\ning services like Spotify and Pandora for music discovery \n[17].  \nUsage of cloud music services  Total \n(n=198) \nTo stream music from my collection which \nI do not have on my music playing devices  171  \n(86.4%) \nTo listen to music I do not have in my col-\nlection  138  \n(69.7%) \nTo discover new music or get recommen-\ndations about songs and artists  128  \n(64.6%) \nTo hold copies of my digital music files in \ncase my hard drive dies  97  \n(49.0%) \nTo transfer digital music files between \ncomputers and/or mobile devices  89 \n(44.9%) \nTo share music with other people  38 \n(19.2%) \nTable 1. Usage of cloud music services.  4.3 Management  of Cloud Music Collection s \nThe median value of the estimated size of participants ’ \nmusic collection s was 2,908  songs ( 1Q: 300, 3Q: 10,000, \nmax: 100,000 ) or 29.74 GB of disk space ( 1Q: 5.75, 3Q: \n60, max: 2,500) . While many participants had sizable col-\nlections, organization was not a pressing issue for most of \nthem, as 72. 2% stated they relied on automatic organiza-\ntion by the service, compared to 24. 2% who manually \norganize their collection s. 56.6% of participants respond-\ned that they have music that is not u ploaded to the cloud. \nThe reasons varied, from lack of time/resources to issues \nof limited access (presented in Table 2).  \nReasons for having music not uploaded to \nthe cloud  Total \n(n=112)  \nI have not had time to add all of them yet  63  \n(56.3%) \nI have enough music in the cloud for my \nneeds right now  40  \n(35.7%) \nThey are physical items that are hard to \ndigitize  36  \n(32.1 %) \nMy cloud storage is limited  30 \n(26.8%) \nI prefer listening to physical items for \nsome music and/or like to have physical \ncopies of  things as well  28  \n(25.0 %) \nThey are physical items which are not \nreadily accessible to me  15  \n(13.4%) \nTable 2. Reasons for having music not uploaded to the \ncloud.  \nAlthough 55 .1% of participants responded that they \npurchase or obtain music from cloud services, few did so \nfrequently , with  approximately three out of four partici-\npants (7 2.5%) doing it about once a month or less.  \nWe also asked participants whether they back up their \nmusic collection in general, and if so, what kinds of strat-\negies they use . Of all participants, 58. 6% responded that \nthey do back up their collection; of th ose answering yes , \n48.3% keep local copies of music files as backup on a \nsecondary storage device , and 11.2% keep copies on a \ncomputer. Some participants considered the clou d music \nservices to be their backup (23.3%) or backed up their \nmusic in the cloud using another cloud service such as \nCrashPlan or Google Drive (8.6%). Most of the backup \nefforts were done in digital file formats; only 3. 4% kept \nphysical copies of CDs, vinyl, etc. as backup.   \n4.4 Music Listening Behavior  \nYouTube (6 5.8%), Spotify (5 7.8%) and Pandora (5 2.9%) \nwere the most popular streaming services, followed by \nSoundCloud (4 0.6%) and Last.FM (23.5%) . With the in-\ncreasing availability of music streaming features offered \nby cloud and other online music services, we wanted to \nknow how much of the music our participants listen to is \nactually owned by them (versus access via streaming). As \nshown in Table 3, the proportions of participants who al-\nmost always own or almost always stream the music they \nlisten to were about equal. Approximately one out of four \nlisten to owned music  and stream music about the same \namount. Overall , the distribution is fairly spread out Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 301  \nacross the different categories , although there were slight-\nly more participants who tend to stream more than own \nmusic  rather  than the vice versa.  \nOwnership vs. Streaming  Total \n(n=197)  \nI own almost all the music I listen to  29 (14.7% ) \nI mostly liste n to the music I own, but \nsometimes stream music I don’t own  36 (18.3%) \nI listen to music I own and stream about \nthe same amount  52 (26.4%) \nI mostly stream music I don’t own, but \nsometimes listen to the music I own  50 (25.4%) \nI almost always stream music I don’t own  27 (13.7%)  \nOther  3 (1.5%)  \nTable 3. Ownership versus Streaming.  \n89.4% of participants responded that they use playlists. \nCriteria  for generating playlists included personal  prefer-\nence (7 2.9%), mood ( 59.9%), genre/style (5 5.4%), ac-\ncompanying activity  (e.g., working out, partying, travel-\ning) (50.8%), artists (3 5.6%), and rece nt acquisition \n(33.3%) . More than half of participants (5 3.1%) listen to \nplaylists that are automatically generated by the services \ninstead of  (or in addition to) creating their own.  \n4.5 Selection Factors, Perceived Limitations, and De-\nsired Features  \nWe asked respondents how they came to use cloud music \nservices, what they desired from the services, and what \nkinds of limitations or frustrations had surfaced in their \nusage of the services. When asked how they initially \nfound services, respondents chose the option ‘I sought \nout cloud services to fit my music listening needs ’ most \nfrequently from a predetermined list of choices (4 7.0%). \nOthers had cloud services preinsta lled on devices \n(21.7%), found out from friends or family (2 1.7%), \nthrough advertising (20 .7%), or were signed up automati-\ncally due to an existing connection with a cloud provider \n(12.6%). Free-form responses given via the ‘other ’ option \nindicated that sev eral users discovered their cloud service \nproviders through Internet information sources, such as \npress coverage or blog posts (11 responses). 6 4.1% of re-\nspondents were paying for cloud music access.  \nWe also asked users which service they preferred of \nthose they had tried and why. 18 4 users responded to this \nopen -ended question, though 15 of the m noted that  they \nonly used one service. Qualitative coding of the respons-\nes indicated that the most popular reasons  were device \ncompatibility ( 29.9%), ease of uploa d and size of storage \nspace (23 .4%), brand loyalty (1 9.0%), price (18 .5%), and \nvariety and availability of desired music (16 .3%). A rep-\nresentative user explained that he chose Google Play Mu-\nsic “because 1) I use an Android phone & tablet, 2) they \nuploaded my library to their cloud, 3) I jumped on early \n& have a discounted monthly price.” ( ID: 103)  \n51.0% of participants responded that there is some-\nthing they would like to change about the service they \nuse. From a predetermined bank of answers, users indi-\ncated that the most common factors hindering their use of \nservices were lack of good sharing features (4 0.6%), clumsy or unappealing visual design (3 0.7%), poor gen-\neral functionality or bugginess (3 0.7%), other missing \nfeatures (2 6.7%), difficulties wi th transferring music \n(22.8%), high cost (1 1.9%), device compatibility issues \n(9.9%), and a lack of storage space ( 7.9%). Free -form re-\nsponses to this question indicated that song access was \nalso an issue for s ome users, due to  services’  incomplete \nartist l ibraries or problems uploading certain file formats. \nOther free -form responses from dissatisfied users  related \nto suboptimal playlist or automated radio features , poor \norganizational or metadata -curating functionalities, \nstreaming options (such as lack of support for simultane-\nous streaming from  multiple devices ), and sharing.  \nWe also asked whether and why users would consider \nswitching to another service. Of the 1 70 respondents who \nanswered th is question,  47.6% indicated they would con-\nsider switching, whil e 34.7% indicated they would not,  \nand 1 7.6% answered that they might switch  or were non-\ncommittal . Of those who  said they would switch , pricing \nwas by far the most common reason given (43 responses), \nwith artist selection (21) and device compatibility (17) \ndistant runners -up. For those who said they would not \nswitch , the most common thread undergirding responses \n(11) was a sense of inertia. Moving collections from ser-\nvice to service is time -consuming and cumbersome, mak-\ning it unappealing to users who have  settled in with a \ncloud provider - especially if the user ha s bought into a \nfull software/hardware combination  (such as Google Play \nMusic and Android devices, or iCloud and Apple devic-\nes). For instance, one user noted, “I would not consider \nswitching at this time. It would be a hassle to move my \npersonal music collection to a new service.” ( ID: 342), \nand another r eplied, “Only if I were to switch to another \nmobile ecosystem.” ( ID: 197) The need for compatibility \nacross devices and services surfaced repeatedly in quali-\ntative coding of the no-switch  responses (9 codes, plus \nsome inertia comments obliquely referenced this); other \nconcerns include artist selection (8), upload/storage needs \n(7) and price (7). Pricing, artist selection, and device \ncompatibility also surfaced in the replies of the maybe -\nswitch  respondents, making these common concerns.  \n4.6 Differences in Gende r and Age  \nWe initially speculated that there might be marked differ-\nences in cloud service usage by age based on the fact that \ncloud services were introduced recently, but our data in-\ndicate that age, overall, was a relatively minor factor in \nexplaining clo ud service usage variability. We divided \nthe participants into three age groups  of approximately \nequal size  (25 and younger , 26-30, 31 and older) and ran \nchi-square analys es on the responses for most of the sur-\nvey questions (excluding open -ended questions)  to identi-\nfy statistically significant differences. Significant differ-\nences between age groups were observed in questions re-\ngarding music purchase and paying behavior, as well as \nin choice of device for accessing  cloud music services. \nParticipants who were  31 or older  were more likely to \npay to use cloud services ( X2=11.34, df=2, p=0.00 3), \nthough younger people  more frequently purchased or ob-\ntained music from cloud services ( X2=21.06 , df=8, \np=0.00 6) (cf. Makkonen’s  [20] findings regarding age \nand willingness to pay for music downloads ). Older par-302 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016  \nticipants also tended to access cloud music via  desktop \ncomputers ( X2=12.76, df=2, p=0.002) more than younger \nparticipants. Younger participants were more likely to use \nYouTube for streaming ( X2=7.17, df=2, p=0.028). Nota-\nbly, n o significant difference was observed by age for the \nquestion asking about listening to owned music versus \nstreaming unowned music, challenging presumptions that \nyounger listeners are less concerned  with owning music.  \nOur survey results indicated that, rather than age, gen-\nder seemed to play a larger role in cloud music behavioral \ndifferences. Almost half of the respondents reported us-\ning cloud services more than once a day, but men tended \ntoward daily usage (90 .7% of male users reported using \ncloud services ‘a few times a week’ or more), while \nwomen’s usage was much more evenly distributed be-\ntween daily (‘more than once a day’ + ‘almost every \nday’: 36.4%), weekly (‘a few times a week’ + ‘about \nonce a week’: 36.4%), or monthly (‘2 or 3 times a month’ \n+ ‘once a month or less’: 27.3%) access and usage  \n(X2=42.13, df=5, p=0.000) .  \nIn general, we noted a trend across multiple questions \nindicating that women tended to listen to music within \ntheir collecti ons and were less likely to listen to music \nthey did not already know than men were. Nearly half of \nfemale participants noted that they ‘mostly’ (20 .0%) or \n‘almost always’ (27.3%) listened to music they owned, \nwhereas almost half of male participants ‘most ly’ \n(30.7%) or ‘almost always’ ( 15.0%) streamed music  \n(X2=15.05, df=5, p=0.010). Women were far less likely to \nreport that they used the services for listening to music \nthey did not have in their collections (47.3% for women \n[W]; 79.3% for men [M]; X2=19.37 , df=1, p=0.000) , and \nmade far less use of cloud recommendation and discovery \nfunctions  (36.4% for W; 77.1% for M ; X2=29.12, df=1, \np=0.000) , such as new  music suggestions (29.1% for W; \n47.1%  for M;  X2=5.28, df=1, p=0.02 ), automatically gen-\nerated playlists  (38.2% for W; 69.3%  for M;  X2=15.99 , \ndf=1, p=0.000) , and suggestions from friends (12.7% for \nW; 28.6%  for M;  X2=5.42, df=1, p=0.020 ), than men did . \n38.2% of female respondents noted that they did not use \ncloud services for music discovery at all,  compared with \n19.3% of men  (X2=7.60, df=1, p=0.006) . One possible \ncaveat here is that women reported much higher usage of \nthe Pandora streaming service alongside cloud services \n(70.4% for W; 45.4% for M;  X2=9.56, df=1, p=0.00 2). \nPandora, an Internet radio service with personalization \nfeatures, does not allow for collection building or search \naccess to specific songs, and so may be a route to music \ndiscovery for some female users. However, it is possible \nthat the heavier usage of  Pandora among women may \nsimply be an issue of convenience (Pandora requires no \nupkeep or maintenance once a station is chosen, unless \nthe user decides to vote up or down songs she likes or \ndislikes). Women may also be  using Pandora’s playlists \nfor listeni ng to similar songs (generated based on already \nfamiliar and preferred song s/artist s) rather than seeking \nout channels playing new and unfamiliar music , or for \nlistening to more mainstream genres,  which they prefer \nmore than men , according to Berkers [ 2]. Lastly, Pando-\nra’s prominence among female users could merely  be in-dicative of targeted advertising; it is mirrored in the site’s \ngeneral user demographics.1 \nWomen reported using cloud services to purchase mu-\nsic more than men did (67.3% for W; 50 .0% for M; \nX2=4.76, df=1, p=0.029), but were much less likely to pay \nfor the cloud service as a whole  than men were (29.1% \nfor W; 78. 6% for M; X2=42.28, df=1, p=0.000) , both  con-\nfirming and complicating  Makkonen ’s [20] finding  that \nwomen express a higher willingness to pay for music al-\nbums and tracks . When asked how they initially found \nout about cloud music services, more males chose the op-\ntions ‘I sought out cloud services to fit my music listen-\ning needs ’ (32.7% for W;  53.6%  for M; X2=6.877 , df=1, \np=0.00 9) or  ‘through an advertisement ’ (9.1% for W;  \n24.3%  for M; X2=5.70, df=1, p=0.017), while women \nwere m ore likely to choose the responses ‘the service was \npreinstalled on a device I obtained ’ (45.5% for W; 1 2.9% \nfor M; X2=24.41, df=1, p=0.000)  or ‘a company automat-\nically signed me up for a cloud music service ’ (30.9% for \nW; 5 .0% for M; X2=24.56, df=1, p=0.000) . Perhaps not \ncoincidentally, men were far more likely than women to \nreport using Google Play Music though many women al-\nso used this service (45.5% for W; 82.9%  for M; \nX2=27.59, df=1, p=0.000) , while women were much more \nlikely to use Apple iCloud and very  few men were iCloud \nusers (54.5% for W; 12.1%  for M; X2=38.81 , df=1, \np=0.000) . Apple tend s to focus on integration of software \nand hardware, and frequently bundle s services together.  \nThis seems to indicate that women are exercising less \novert consumer cho ice in selecting a cloud provider, \nwhich may have implications for service fit and user sat-\nisfaction. For instance, women were much more likely \nthan men to use the services for transfer between devices \n(70.9% for W; 34.3% for M; X2=21.43, df=1, p=0.000), \nand they were more likely to report problems with trans-\nferring files  (47.6% for W; 15.4% for M; X2=9.95, df=1, \np=0.002)  and device compatibility  issues  (23.8% for W; \n6.4% for M; X2=5.52 , df=1, p=0.019 ) when asked about \nservice deficiencies . Suki [29] reports a similar tendency \nof men having a higher level  of perceived ease of use \nthan women when using online music. Women have \nmore music not uploaded to the cloud (76.4% for W;  \n49.3%  for M; X2=11.50, df=1, p=0.001)  which may re-\nflect that they have enough music in the cloud for their \nneeds now (45.2% for W;  30.4%  for M , although not sig-\nnificant ) and that they prefer to listen to physical copies \n(35.7% for W; 18.8% for M; X2=3.941, df=1, p=0.047 ). \n4.7 Thoughts on the Trend of Moving to the  Cloud  \nOur survey concluded with an open -ended question ask-\ning respondents to express other thoughts or opinions \nthey had about cloud computing and cloud music storage. \n98 users responded with statements of length varying \nfrom a single sentence fragment to several paragraph s. \nThese responses were qualitatively coded and examined \nfor common patterns using a consensus code str ategy \n[11]. We found that the codebook developed for our in-\nterview project [ 17] was useful as a starting point, and \nonly a few codes were added to this preexisting frame-\n                                                           \n1 Alexa.com reports that Pandora’s userbase skews strongly  fe-\nmale. http://www.alexa.com/siteinfo/pandora.com  Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 303  \nwork during coding iterations. The most common topic \nwhich surfaced in these  responses was the relationship \nbetween cloud and streaming music platforms  and their \nrelative benefits and drawbacks. Alongside this was an \nabiding concern over issues of ownership and access, \npresent in nearly a quarter of responses. Users express ed \nkeen and someti mes profuse opinions about  ownership \nand access modes of listening, just as the interviewees did  \nin our project’s first phase  [17] - but without explicit \nprompting, and with minimal addressing of the topic in \nearlier  survey  questions (only one question, di scussed in \nSection 4.4, indirectly references this issue). As in [17] , \nparticipants  expressed a variety of positions: one uneasy \nuser noted, “The entire system of ‘owning music ’ is near-\nly obsolete. The legal as well as social ramifications of \nidentity ties  to cultural objects to which someone else \ncontrols all access is little understood and downright \nfrightening” ( ID: 36), and another cloud skeptic stated, \n“It’s scary to think of everything being online without a \nphysical copy anywhere. I still purchase CD s and import \nthem to my online service because I enjoy having a real \nCD, but appreciate the probabilities of cloud streaming.” \n(ID: 110) Still others saw cloud -based access models as \nan nigh -unstoppable new wave: “These [record] labels \nneed to wake up the internet/cloud is not a fad it is the \nfuture[. S]ure it will be improved upon but I have not \nbought a physical album in years and eventually no one \nwill.” ( ID: 311) Once again, a ge was not a reliable pre-\ndictor of opinion on ownership/access matters; many u n-\nder-26 users  favored owning files , and sever al over -30 \nusers  favored access -only streaming systems . Concerns \nover service cost (22 responses), praise or circumspection \nregarding service convenience (20), opinions about artist \nand genre availability (15), and fears or experiences of \nnetwork  and data issues (20)  and storage caps (15) also \nfactored prominently into responses to th is call for opin-\nions.  \nOne topic which was more prominent in our survey \nthan the interviews was artist royalties , perhaps influ-\nenced by recent news coverage of court cases involving \nstreaming royalty payments, as well as the weighing -in of \nhigh-profile musicians (such as country/pop superstar \nTaylor Swift) on the subject. Some wrote approvingly of \nservice handling of royalty payments,  such as the user \nwho wrote, “I like the fact that the music is now more \navailable to more people and that it can be accessed more \nglobally while still generating revenue for the artist.” ( ID: \n101) Others had more ambi valent  reactions: “While as a \nmusician  I recognize the damage st[r]eaming services \n[have done] to the industry, as a listener the convenience \nis absolutely incredible and has introduc ed me to so much \nnew music.” (ID : 192) Also more prominent in survey \nresponses than in the interviews were comm ents regard-\ning audio quality of services; one user replied, “I would \nnever consider going all -streaming, unless I (and the in-\nfrastructure) were able to do this with full -quality un-\ncompressed audio... I'm interested in services like PONO \nand TIDAL with ‘hig h-quality’ audio streaming, but, they \nare too expensive for me to opt in.” ( ID: 103)  5. CONCLUSION AND FUTUR E WORK  \nOur survey results show that cloud music services are \nprimarily used to improve music access by overcoming \nlimitation s imposed by device storage  or lack of owner-\nship. While listening  from participants’ own music col-\nlection s was the top usage of  cloud services, streaming \nmusic they do not own was important as well. This seems \nto signal a desire for merged systems with both cloud and \nstreaming featu res. The services  are also used for music  \ndiscovery and management, though  less so for sharing \nmusic. Exploring and implementing better way s to share \nlistening experience s may help improve users’ experienc-\nes with cloud services. Collection -building and streaming \napproaches  divide online music usage,  although there is a \nslight preference toward streaming.   \nApproximately half of participants reported choosing \nservices to fit their needs, although a substantial number \nwere influenced by preinstalled options, w ord of mouth, \nand advertising. Major contributing factors in user service \nchoice included  device compatibility, ease of upload, \nstorage space , brand loyalty, price, and music availabil-\nity. Over half of the participants indicated the desire to \nchange something about the service s they use. Again , the \nlack of good sharing features was the most commonly \nmentioned factor, followed by dissatisfaction regarding \nthe design and functioning of the service. Difficult y \ntransferring music was also mentioned by abo ut a quarter \nof participants. Nearly half of respondents indicated they \nwould consider switching to another service based on \nprice, artist selection, and device compatibility.  \nDifferences regarding  use of cloud music services \nwere much more prominent by gender rather than age.  \nWomen reported listening to music they owned more \nthan men , sought  out new music less than men, paid for \nservices less often, and asserted less consumer choice in \nselectin g services than men did.  This warrants future in-\nvestigation o f the underlying reasons for these differ-\nences, and also suggest s opportunit ies for developing mu-\nsic services tailored to gender -specific usage.  \nIn future work, we plan to continue our investigation \nof music users , focusing on two aspects: 1) the meaning \nof personal collections in an increasingly streaming -\ndomina ted environment , and 2) investigation of reasons \nfor the differences observed in music selection, listening, \nand sharing between genders.   \n6. ACKNOWLEDGEMENTS  \nThe authors extend special thanks to Lara A ase and Ra-\nchel Wishkoski  for their contributions to survey design, \nand Rebecca Fronczak  for assisting in recruiting survey \nparticipants. This research is supported by the University \nof Washington  Office of Research.  \n7. REFERENCES  \n[1] P. Ambrose  and A. Chiravuri : An empirical \ninvestigation of cloud computing for personal use. \nMWAIS 2010 Proceedings , Paper 24 , 2010.  304 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016  \n[2] P. Berkers : Gendered scrobbling: Listening behavior \nof young adults on Last.fm. Interactions: Studies in \nCommunication & Culture , 2(3), pp. 279-296, 201 0. \n[3] J. Brinegar  and R. Capra : Managing music across \nmultiple devices and computers . Proceedings of the \niConference , pp. 489 -495, 2011.  \n[4] P. Burkart : Music in the cloud and the digital \nsublime . Popular Music and Society , 37(4), pp. 393-\n407, 2014.  \n[5] L. Cesareo  and A. P astore : Consumers ’ attitude and \nbehavior towards online music piracy and \nsubscription -based services. J. Consumer Marketing,  \n31(6/7), pp. 515-525, 2014.  \n[6] T. Chamorro -Premuzic  and A.  Furnham : Personality \nand music: Can traits explain how people use music \nin everyday life? Brit. J. Psych ol., 98, pp. 175-185, \n2007.  \n[7] T. Chamorro -Premuzic,  V. Swami and B.  \nCermakova : Individual differences in music con - \nsumption  are predicted by uses of music and age \nrather than emotional intelligence, neuroticism, \nextraversion or openness. Psychology of Music , \n40(3), pp. 285-300, 2012.  \n[8] T. DeNora: Music in everyday life . Cambridge \nUniversity Press , Cambridge, UK , 2000.  \n[9] A. Hagen : The playlist experience: Personal playlists \nin music streaming services. Popular Music and \nSociety , 38(5), pp. 625-645, 2015.  \n[10] E. Hargittai : Whose space? Differences among users \nand non -users of social network sites. J. Comp .-\nMediated Comm ., 13, pp. 276-297, 2008.  \n[11] C. Hill et al.: Consensual qualitative research: an \nupdate. J. Couns . Psych . 52(2), pp. 196 -205, 2005.  \n[12] R. Junco, D. Merson  and D. Salter : The effect of \ngender, ethnicity and income on college students’ \nuse of communication technologies.  \nCyberpsychology, Behavior, and Social Networking , \n13(6), pp. 619-627, 2010.  \n[13] P. Juslin  et al. : An experience sampling study of \nemotional reactions to music: Listener, music, and \nsituation. Emotion , 8(5), pp. 668-683, 2008 . \n[14] M. Kamalzadeh,  D. Baur  and T.  Mölle r: A survey on \nmusic listening and management behaviours . \nProceedings of the ISMIR, pp. 373 -378, 2012 . \n[15] J. H. Lee and N. Waterman: Understanding user \nrequirements for music information services . \nProceedings of the ISMIR,  pp. 253 -258, 2012 . \n[16] J. H. Lee and R. Price:  User experience with \ncommercial music services: an empirical \nexploration. JASIST , 2015 . DOI: 10.1002/asi.23433  \n[17] J. H. Lee, R. Wishkoski, L. Aase, P. Meas and C. \nHubbles: Understanding users of cloud music \nservices: selection factors, managemen t and access \nbehavior, and perceptions. JASIST , 2016. In press.  [18] M. Lesaffre, L. De Voogt and M. Leman:  How \npotential users of music search and retrieval systems \ndescribe the semantic quality of music. JASIST , \n59(5), pp. 695-707, 2008 . \n[19] L. Liikkanen  and P. Aman:  Shuffling services: \nCurrent trends in interacting with digital music. \nInteracting with Comp. , 2015.  http://iwc.oxfordjour  \nnals.org/content/early/2015/03/27/iwc.iwv004.full  \n[20] M. Makkonen,  V. Halttunen and L.  Frank : The \neffects of gender, age, and income  on the \nwillingness to pay for music downloads. Bled \neConference Proceedings , paper 39 , 2011 . \n[21] C. Marshall  and J.  Tang : That syncing feeling: Early \nuser experiences with the cloud. Proceedings of DIS, \npp. 544 -553, 2012 . \n[22] J. Morris : Sounds in the cloud: Cloud computing \nand the digital music commodity. First Monday, \n16(5), 2011. \nhttp://firstmonday.org/article/view/3391/2917  \n[23] G. Nguyen,  D. Dejean and S.  Moreau : On the \ncomplementarity between online and offline music \nconsumption: The cas e of free streaming. J. Cultural \nEconomics,  38(4), pp. 315-330, 2014.  \n[24] S. Park and S.  Ryoo : An empirical investigation of \nend-users’ switching toward cloud computing: A \ntwo factor theory perspective. Comp . in Human \nBehavior , 29(1), pp. 160-170, 2013.  \n[25] K. Purcell,  R. Entner and N.  Henderson : The rise of \napps culture. Pew Research , 2010. http://www.  \npewinternet.org/2010/09/14/the -rise-of-apps-culture/  \n[26] P. Rentfrow  and S. Gosling : The do re mi’s of \neveryday life: The structure and personality \ncorrelates of mus ic preferences . J. Psychology and \nSocial Psychology , 84(6), pp. 1236 -1256 , 2003.  \n[27] M. Schedl  and A. Flexer : Putting the user in the \ncenter of music information retrieval. Proceedings of \nthe ISMIR,  pp. 385-390, 2012 . \n[28] V. Stantchev  et al. : Learning management systems \nand cloud file hosting services: A study on students’ \nacceptance. Comp . in Human Behavior , 31, pp. 612-\n619, 2014.  \n[29] N. Suki: Gender, age, and education: Do they really \nmoderate online music acceptance? Communications \nof the IBIMA , 2011 . \n[30] P. Wikström : Music industry: Music in the cloud , \n2nd edition. Polity Press , Cambridge, UK, 2013.  \n[31] B. Zhang  et al. : Understanding user behavior in \nSpotify. Proceedings of IEEE INFOCOM , pp. 220 -\n224, 2013.  Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 305"
    },
    {
        "title": "Instrumental Idiom in the 16th Century: Embellishment Patterns in Arrangements of Vocal Music.",
        "author": [
            "David Lewis 0003",
            "Tim Crawford",
            "Daniel Müllensiefen"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1415964",
        "url": "https://doi.org/10.5281/zenodo.1415964",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/165_Paper.pdf",
        "abstract": "Much surviving 16th-century instrumental music consists of arrangements (‘intabulations’) of vocal music, in tabla- ture for solo lute. Intabulating involved deciding what to omit from a score to fit the instrument, and making it fit under the hand. Notes were usually added as embellish- ments to the original plain score, using idiomatic patterns, typically at cadences, but often filling simple intervals in the vocal parts with faster notes. Here we test whether such patterns are both character- istic of lute intabulations as a class (vs original lute mu- sic) and of different genres within that class. We use pat- terns identified in the musicological literature to search two annotated corpora of encoded lute music using the SIA(M)ESE algorithm. Diatonic patterns occur in many chromatic forms, accidentals being added depending how the arranger applied the conventions of musica ficta. Rhythms must be applied at three different scales as nota- tion is inconsistent across the repertory. This produced over 88,000 short melodic queries to search in two corpo- ra totalling just over 6,000 encodings of lute pieces. We show that our method clearly discriminates be- tween intabulations and original music for the lute (p < .001); it also can distinguish sacred and secular genres within the vocal models (p < .001).",
        "zenodo_id": 1415964,
        "dblp_key": "conf/ismir/LewisCM16",
        "content": "INSTRUMENTAL IDIOM IN THE 16TH CENTURY: EMBELLISHMENT PATTERNS IN ARRANGEMENTS OF VOCAL MUSIC David Lewis, Tim Crawford, Daniel Müllensiefen Goldsmiths, University of London {d.lewis, t.crawford, d.mullensiefen}@gold.ac.uk ABSTRACT Much surviving 16th-century instrumental music consists of arrangements (‘intabulations’) of vocal music, in tabla-ture for solo lute. Intabulating involved deciding what to omit from a score to fit the instrument, and making it fit under the hand. Notes were usually added as embellish-ments to the original plain score, using idiomatic patterns, typically at cadences, but often filling simple intervals in the vocal parts with faster notes.  Here we test whether such patterns are both character-istic of lute intabulations as a class (vs original lute mu-sic) and of different genres within that class. We use pat-terns identified in the musicological literature to search two annotated corpora of encoded lute music using the SIA(M)ESE algorithm. Diatonic patterns occur in many chromatic forms, accidentals being added depending how the arranger applied the conventions of musica ficta. Rhythms must be applied at three different scales as nota-tion is inconsistent across the repertory. This produced over 88,000 short melodic queries to search in two corpo-ra totalling just over 6,000 encodings of lute pieces. We show that our method clearly discriminates be-tween intabulations and original music for the lute (p < .001); it also can distinguish sacred and secular genres within the vocal models (p < .001). 1. INTRODUCTION A large proportion of surviving solo instrumental music from before the 17th century is made up of arrangements, or intabulations, of pre-existing vocal music, much of it printed in the 16th century. [1] Most of these are for solo lute and are notated in tablature; several collections of intabulations for keyboard instruments also exist, though they are not considered in this paper. These arrangements are rarely strict reproductions of the scores of their vocal models. Even those which aim at faithful representation of the vocal parts almost invariably contain pitch and rhythmic alterations, as well as omitted and added notes. These changes come in different forms and might arise from various motivations: • They may reflect unnotated aspects of performance practice for the vocal model; • They may reflect attempts to make the music better match the idioms of instrumental style; • They may result from the practical limitations of playing multiple voices on a single instrument; • They may arise from the individual style of the in-tabulator, the musical genre or current fashion. Studying the process of intabulation, then, has the po-tential to reveal much about several aspects of vocal and instrumental music of the time, along with the nature of individual style, by allowing us to draw attention to ex-plicit and conscious changes to a text and to attempt to infer the motivation for those changes. In this paper, we describe an experiment tracing the use, in two collections of encoded lute music, of typical melodic embellishment patterns identified in the musico-logical literature. The collections, which are broadly rep-resentative of the 16th-century lute repertory, contain a mixture of original idiomatic music composed for the lute and intabulations of vocal music, and we wanted to de-termine the extent to which embellishment patterns might be viewed as diagnostic features – can we use them to distinguish intabulations from music originally composed for the lute? We also carried out a further investigation into whether the occurrence of the patterns can be associ-ated with the style of the music being arranged – were they applied differently to sacred and secular vocal music by the arrangers? These may be considered as simple proxies for more complex questions about the distinct identity of lute intabulation as a genre and are intended as a first step towards richer and larger-scale musicological studies which, for example, might compare intabulations with encoded scores of the original vocal music, or in-duce patterns of embellishment automatically. 2. BACKGROUND About half of the surviving repertory of sixteenth-century lute music consists of intabulations. The Early Music Online (EMO) resource hosted by the British Library and RHUL comprises digital images of 300 books of printed music from before 1700.1 About 10% are books of lute music, written in tablature. A number of these have been                                                              1 http://www.earlymusiconline.org \n © David Lewis, Tim Crawford, Daniel Müllensiefen. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: David Lewis, Tim Crawford, Daniel Müllensiefen, “Instrumental idiom in the 16th century: embellishment patterns in arrangements of vocal music”, 17th International Society for Music Information Retrieval Conference, 2016. 524   incorporated in the Electronic Corpus of Lute Music (ECOLM).2 In the Transforming Musicology project3 one of the three main work-packages is a programme of re-search on workflows and methods for musicological in-vestigation of these resources. The tablature encodings used in this paper, together with detailed metadata, will soon be released as Linked Open Data.4 There is a considerable degree of repertorial overlap between the vocal and instrumental music in EMO; much of the vocal music also appears in intabulations for lute or keyboard, sometimes in multiple arrangements. This overlap provides the basis for ongoing musicological in-vestigations within Transforming Musicology. There is some specialist literature on the methods of intabulation adopted by various lutenist-composers of the 16th century [9][10][11], but systematic (computational) study has not hitherto been possible owing to a general lack of suitable annotated corpora of encodings. Here we describe an experiment to test the notion that, while some of the melodic patterns found in the corpus may be part of the common stylistic currency of a period, others may be diagnostic of an idiom of arrangement, or of a musical genre (in the musicological, rather than MIR, sense). That is, we ask whether some patterns are found across all lute music of a certain time or place, while others are encoun-tered more in intabulations than in more exclusively idi-omatic music composed for the instrument such as danc-es, preludes or fantasias. Since we know that there were generally stylistic dif-ferences between vocal music composed for religious contexts and those for secular use, we further ask whether the use of embellishment patterns suggest that intabula-tors treated sacred music differently than secular in their arrangements. In this period, and with this repertory, a distinction based on language is generally safe – that is, Latin masses and motets are classified as sacred, whilst vernacular chansons and madrigals in French and Italian (or occasionally English or German), are considered secular even if the events they describe are biblical.5 The chromatic inflection of pitches in 16th-century staff-based music notation is not fully explicit. When such music is intabulated, however, the precise chromatic pitch of each note is given in the tablature. The resulting chromatic changes to the plain diatonic vocal model could be assigned to two contrasting motivations on the part of an intabulator: either as a record of a contempo-rary interpretation of standard musica ficta practice, or as the result of idiosyncracies of instrumental music which may in turn have had an effect on the emergence of the                                                              2 http://www.ecolm.org 3 http://www.transforming-musicology.org 4 Much of the metadata is already published at http://slickmem.data.t-mus.org/ . 5 An example of the latter case is the enormously popular chanson by Orlande de Lassus, ‘Susanne un jour’, which concerns the story of Susanna and the Elders from the Book of Daniel. modern tonal system. Considered as the former, they have been used to attempt to reconstruct elements of vo-cal performance practice [2].  As an example of the practice, in Figure 1 we show the opening of a popular madrigal, first published in 1541. The original vocal version is shown (a) in short score, to-gether with (b) one of its many intabulations, from a late 16th-century source. The interpolated notes in the bass (bar 1) and alto (bar 3) parts represent typical embellish-ment patterns of the type we discuss in this paper. Note also that the lute version supplies musica ficta accidentals not present in the vocal original; these are likely to differ from those supplied in other intabulations. \n Figure 1. The opening of Berchem’s ‘O s’io potessi donna’: (a) the original for voices, and (b) as intabulated by Emanuel Adriansen (Pratum Musicum, 1584) For this paper, we study melodic embellishment pat-terns as possible stylistic markers of instrumental idiom. A few scholars have published ‘vocabularies’ of such pat-terns, inspired by the significant number of treatises from between 1535 and 1620 which give copious examples by way of instruction for players of string and wind instru-ments. [7][8]  In this preliminary study, we focus on a seminal ex-ample by a leading musicologist of the last century. In [3], Brown considers the embellishment patterns ap-plied by three different early 16th-century intabulators to a single popular (secular) madrigal, Giachet Berchem’s ‘O si’o potessi donna’, and provides a table of the patterns he identifies (78 distinct patterns). This table is effectively a set of embellishment templates that might be used by in-tabulators to expand simple intervals in the vocal parts of the model. An extract from Brown’s table, showing some of the embellishment patterns used by Dominico Bianchini i n his intabulation in [4] is shown as Figure 2. The patterns are given without clefs or accidentals, as the precise pitches and intervals used can be expected to vary depending on context (including diatonic transposition and musica ficta). In [5] Robison studied a single source of lute intabula-tions from 1558, listing a set of 95 such patterns found in the 76 arrangements therein. Neither he nor Brown at-tempted to generalize this work or to assess empirically the extent to which the patterns identified are themselves indicative of something more general, although in [7] Brown had discussed the tradition of instrumental and vocal embellishment through the numerous treatises pub-lished in the 16th and early 17th centuries. Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 525   \n Figure 2. An extract from Brown’s table of embellish-ment patterns found in lute intabulations of Berchem’s ‘O s’io potessi donna’ (from [3]) 3. METHOD For this experiment, we use two test corpora:6  • a snapshot of the works currently in ECOLM, con-taining some 1,351 lute pieces. Works in ECOLM are transcribed as they appear in the original sources (including printing or scribal errors) and have been added and selected based on a series of academic re-search projects funded by the UK Arts and Humani-ties Research Council since 1999; • a private collection, created by Sarge Gerbode, of 4,723 performing editions of lute pieces. These have been edited by the compiler, who is not an academic musicologist. The test corpus used is a subset of a larger collection, including only pieces from the 16th or very early 17th century. The corpus is known to contain some duplicates, for example to support performance on different instruments, but these represent a very small proportion of the works.  Intabulations Other Total ECOLM 147 1,204 1,351 Gerbode 1,048 3,675 4,723 Total 1,195 4,879 6,074 Table 1. Summary of our two test corpora Metadata for the corpora includes genre and subgenre labels. The distribution of pieces is shown in Table 1. Dates, places and attributions are also available for some of the pieces, but there is likely to be some approximation involved. For the purpose of this experiment, pieces in these collections are represented as a sequence of <on-set, pitch> tuples and interrogated using a Javas-cript implementation of SIA(M)ESE. [6] All versions of the subject of Brown’s study [3], Berchem’s madrigal, ‘O s’io potessi donna’, were deliberately excluded from both corpora to avoid bias.                                                              6 Both corpora, in the format we used for our experiment, and related metadata will be made available in due course to researchers via the Transforming Musicology web-site (see note 3). Lute tablature has no voicing, pitch spelling or indi-vidual note-duration built explicitly into the notation. In-stead, it indicates the fret/string positions of the left-hand fingers and the duration between successive chords or single notes struck with the right hand. We have encoded Brown’s set of 78 (monophonic) embellishment tem-plates using diatonic note-names. To search for passages matching one of the templates in the onset/pitch matrix, we need to realize all the possible chromatic inflections of all notes in each template. These are represented using chromatic pitch. Each pattern was taken to start on (diatonic) C and all its notes realized in such a way that they could be spelled using 16th-century staff notation. This allowed pitches to be spelled as any of [C, C#, D, Eb, E, F, F#, G, G#, Ab, A, Bb, B]. All possible spellings of the pattern’s note se-quence starting on C and corresponding queries were generated, and then the process was repeated starting on D, and so on.  Figure 3a shows one of the simplest patterns from the Brown set, a plain passing-note motion. This is listed by Brown as one of those used to fill a rise of a third in a vo-cal line of the model (cf Figure 2, (iii)). In practice, the third might (in modern terms) be major or minor and filled by a combination of tones and semitones dependent on the local tonal context. Figure 3b-d are all what, in modern terms, would be called diatonic, in either major or minor modes. Figure 3e-g are less ‘tonal’ in that sense, and they are unlikely to occur in this context in 16th-century music. This does not mean that the note patterns within them will not occur in our corpora. Figure 3e, for example, is a simple chromatic scale and, while it is un-likely to happen in the context of ‘filling a third’, since the diminished third it connects probably never happens, in other contexts, it may be fairly common.  Figure 3. An embellishment pattern from Brown’s list (the first at (iii) in Fig. 2) and some of its realisations. For the 78 unique patterns in [3], this produced a total of 29,476 queries, which we have classified, based on the scale degrees chosen, according to whether or not they make tonal sense within the modern major/minor mode system, the idea being that we might be able to eliminate a priori those transformations with vanishingly small probability of occurrence. Of the realizations, we found 58 that could be interpreted in either a major or minor context, 88 just in major and 4,936 in minor (permitting the sixth and seventh degrees to be flattened or natural). This may be a fairly naïve approach from a music-theoretic standpoint, but it is reasonably complete and inclusive. The instrumental music of the period was changing rapidly in terms of scales, tonality and chro-\n526 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016   matic inflection, and we have not attempted to generate rules from the musical data itself. On the other hand, the only cost of generating too many patterns is computation-al time – if our searches include unlikely patterns, their effect on the final analysis will be negligible, since they will produce very few or no results. In the event we car-ried out all our searches using the full set of 29,476 pat-terns. If the major or minor forms had proved as diagnos-tically useful as the full set of patterns, then searches in future experiments could be limited to these, dramatically reducing the time required for searching, however this was not the case. Turning to the time dimension, the relationship be-tween metrical level and rhythmic notation is especially varied during this period, particularly so for lute music, the notation of which favors short durations. For this rea-son, each pattern is tested using not only the rhythms giv-en by Brown, but also with the durations doubled and halved, trebling the final number of search patterns to 88,428. As we have indicated, some of the shorter patterns de-scribed are trivial, and can be expected to occur univer-sally, not only as elaboration patterns in other genres, but also as melodic elements in their own right. (The example given in Figure 3 is such a case; the other, more elaborate patterns in Figure 2 (iii) are less likely to appear ubiqui-tously.)  As an alternative method of a priori selection, those of Brown’s list of patterns judged by an expert musicologist to be most characteristic of intabulations were labelled in advance. Although this was not used for pruning the search as originally intended, it gives an informal set of ‘ground truth’ judgements which offer us the opportunity to carry out a further test of our method.7 In general, the patterns preselected by our expert were longer than those rejected, so we also did a similar evaluation based on pat-tern length.  Analyses were carried out on a complete set of results obtained from the exhaustive search of our two test-sets using all 88,428 transformations of the patterns as que-ries; the resulting ‘hits’ were stored in an SQL database together with metadata from the annotated corpus as the search was done. Since SIA(M)ESE is a partial-matching algorithm (as opposed to approximate-matching) we had the possibility of recording incomplete matches, which we limited to a threshold of 80% of the notes in the que-ry; a simple SQL operation enables us to filter out the partial matches if necessary. 4. RESULTS 58.3% of all queries from our list of patterns found matches in intabulations, while only 48.3% of all queries on other lute pieces produced matches. This association                                                              7 The full set of patterns we used, with those preselected as ‘likely’ highlighted, is available in music notation from: http://intabulations.data.t-mus.org/ . of patterns with the intabulation repertoire was confirmed by a c2-test, comparing observed and expected frequen-cies of hits and non-hits between intabulations and other lute pieces (c2 = 6041, df = 1, p < .001).  63.7% of the queries on intabulations from the sacred repertoire found patterns, while only 57.1% of queries on non-sacred intabulations contained patterns. A subse-quent c2-test using data only from intabulations con-firmed the strong association of pattern occurrences with the sacred repertoire (c2 = 508, df = 1, p < .001).  However, simple c2-tests do not offer the possibility of investigating for the influence of other variables that might affect the likelihood of a pattern occurring in a giv-en piece beyond the fact that it is an intabulation or not. Therefore we analysed the results with binomial mixed effects models where the variables of primary interest (isIntabulation and isSacred) can be entered alongside other variables we wish to control for. The de-pendent variable was a binary indicator recording wheth-er or not a particular query returned a ‘hit’ (i.e. the query pattern was found in the queried piece). The main inde-pendent variable in the first model was isIntabula-tion. But we also controlled for the influence of three additional variables in the model (all described above): isLong (long patterns were defined to have between 7 and 13 notes, short patterns ranged from 3 to 6 notes), IsMajor, and HasExpertLabel. We used the identi-fier of the 78 patterns as a random effects variable.  Modelling was done using the glmer function in the R package lme4 [13] and started from a fully saturated model specifying all main effects as well as all 2-way, 3-way and the 4-way interaction effect. We then used a step-wise backward model selection procedure based on improvements in the Bayesian Information Criterion (BIC) to arrive at a more parsimonious final model which only contained highly significant effect terms (all p-values < .001). The parameter estimates, their standard errors, z-values of the Wald-statistic, associated signifi-cance level and the odds ratios derived from the parame-ter estimates of the final model are given in Table 2.  The model confirms the result from the first c2-test. The significant main effect for IsIntabulation indi-cates that patterns are more likely to be found in intabula-tions compared to the other pieces (odds ratio = 2.14). Thus, the collection of patterns can therefore be regarded as strongly characteristic of the intabulation repertoire. Further insights from the other three variables in the model suggest that overall: shorter patterns occur more frequently; major variants are less likely; and patterns preselected by the expert musicologist are less frequent. Insight from the significant 2-way interaction effects sug-gests that versions of patterns that are longer or are major are more likely in intabulations and also that longer ver-sions with expert labels are more likely in intabulations.  Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 527   \nTable 2. Intabulations vs. non-intabulations: coefficient estimates, standard errors, z-values of Wald statistic with associated significance levels, and odds ratios for the independent variables in the binomial mixed effects model of all database queries. Coefficients represent the difference between the binary feature being present in the query compared with the feature being absent (the reference level). Significance levels are coded as fol-lows: * < .05, ** < .01, *** < .001.  To answer the question whether the patterns are more common in secular music, a second binomial mixed ef-fects model was fitted to the data from the intabulations only. This model included the same variables as fixed ef-fects. However, the main variable of interest was isSa-cred, a binary variable indicating whether the queried piece was from a sacred or secular repertoire. The same modelling selection strategy was employed, starting from a fully saturated model including all higher-order interac-tion effects and then applying a step-wise backward se-lection procedure based on the improvement in BIC. The parameter estimates, their standard errors, z-values of the Wald-statistic, associated significance level and the odds ratios derived from the parameter estimates of the final model are given in Table 3. In this model isSacred on its own was not a signifi-cant predictor (p = .259; OR = 1.05). However, two 2nd order effects involving isSacred were highly signifi-cant (p-values < .001): queries that were either major var-iants or were longer queries on pieces from the sacred repertoire had a higher chance of returning a hit. Thus, the second order effects suggest that once pattern variants possess certain characteristics, they are more likely to be found in the sacred repertoire.  In summary, the results of the statistical analysis show that a) the embellishment patterns we used are more common in intabulations and b) the variants of these pat-terns with certain features (relatively long or major) are more common in the sacred repertoire. In sum, the results indicate the usefulness and relevance of embellishment patterns as descriptors of the idiom of intabulations but also highlight the importance of structural features (length, mode) of the patterns which should be taken into consideration in future studies to select specific subsets of patterns for musicological queries. Factors Coeff. SE z OR (Intercept) 2.85 0.13 22.51*** 17.29 IsSacred 0.05 0.05 1.13 1.05 isLong -3.93 0.20 -20.06*** 0.02 Major -1.42 0.02 -60.87*** 0.24 isSacred x isLong 0.60 0.05 11.03*** 1.82 isSacred x Major 0.59 0.06 10.46*** 1.80 isLong x Major -0.08 0.03 -2.29* 0.92 isSacred x isLong x     Major  -0.55 0.07 -7.38*** 0.58 Table 3. Sacred vs. secular: coefficient estimates, stand-ard errors, z-values of Wald statistic with associated significance levels and odds ratios for the independent variables in the binomial mixed effects model of data-base queries including intabulations only. Coefficients represent the difference between the binary feature be-ing present in the query compared with the feature be-ing absent (the reference level). Significance levels are coded as follows: * < .05, ** < .01, *** < .001. 5. CONCLUSIONS This experiment shows that there is a clear stylistic dif-ference between lute intabulations of vocal music and music composed directly for the lute in our corpus of 6,000 pieces, and that this can be revealed by comparing the frequency of occurrence of embellishment patterns identified in the musicological literature. To our knowledge, this is the first time a corpus of early instru-mental music has been analysed in this way, and this re-sult suggests that these patterns have promise for further use in stylistic analysis. There are some biases in the corpora that affect this finding. Possibly the most important consideration from the statistical point of view is that not only do the corpora contain different arrangements of the same vocal piece, but they also may have multiple instances of the same intabulation from different sources. These versions should be note-identical in principle, but will often differ either in a few details (due to printing or transmission er-rors) or more substantially. The extent of these concord-ances is hard to estimate from the metadata we have, nor is it clear how they should be treated once identified, but it is certain that they will affect the assumption of inde-pendence of samples. We consider that identifying these and assessing their significance is an important musico-logical task, which will also help to make our methodolo-gy more sound. Most of the intabulations present are from a narrower date range than the corpus as a whole. For the ECOLM collection especially, this bias is not particularly grave, due to the data gathering policy used so far, but evaluat-ing its extent is not straightforward. Dating works is not easy and, although most printed collections have a known year of publication during this period, this may be later than the date of composition of the works in the book.  Factors Coeff. SE z OR (Intercept) 2.15 0.08 27.32*** 8.58 isIntabulation 0.76 0.02 48.79*** 2.14 isLong -3.23 0.11 -28.88*** 0.04 Major -1.45 0.01 -178.40*** 0.23 HasExpertLabel -0.77 0.11 -6.76*** 0.46 isIntabulation\tx isLong  0.13 0.02 5.20*** 1.14 isIntabulation x Major 0.10 0.02 6.13*** 1.11 isLong x Major -0.11 0.01 -8.15*** 0.90 isIntabulation x  HasExpertLabel -0.33 0.04 -7.42*** 0.72 isLong x HasExpertLabel -0.12 0.12 -0.96 0.89 isIntabulation x  isLong x HasExpertLabel 0.30 0.05 6.00** 1.35 528 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016   Another bias that is hard to evaluate is due to the rela-tive length of pieces. The intabulations in both collections are longer, on average, than other genres and contain more notes. This could increase the likelihood of any giv-en pattern occurring in any given piece; however, the re-lationship is unlikely to be straightforwardly linear and, since French chansons of the time (the favourite vocal models for intabulation) are characterized by a significant degree of repetition, even the increased note count cannot be taken to indicate a larger amount of distinct musical material. Further analysis is needed to determine the ex-tent to which the same embellishment patterns were re-used upon repetitions in the model. 6. FURTHER WORK In the present study, we used melodic indicators drawn by a musicologist from three exemplary arrangements of a single vocal work published at around the same time. The madrigal in question in fact survives in over a dozen 16th-century intabulations (in both printed and manuscript sources for lute and keyboard), so an obvious next step would be to broaden our list of embellishment patterns to include at least some of these. The list could be further augmented by including patterns given in treatises for other instruments. In general, our assertions here would be strengthened by a more fine-grained analysis. In particular, if a date and place of composition can be established for a suffi-ciently large number of works, then we should be able to evaluate the extent of any biases in our corpora. Similar-ly, where the intabulator is known and also composed original lute music, the two can be compared. Intabulations are also distinguished by other parame-ters, most obviously texture. Even when one or more original voice-parts are omitted in an intabulation, one might expect the latter to maintain the texture of the re-maining voices fairly consistently throughout the piece, whereas in freely-composed lute music there is in general no such obligation on the part of the composer. Though some fantasias and recercars (the main contrapuntal gen-res of ‘pure’ lute music) maintain a strict three, four or even five-voice texture, this is much less likely in dance music, for example. In future studies, it will be helpful to make use of voice-leading where it can be derived from the tablature. In the case of much 16th-century keyboard music, espe-cially that notated in so-called ‘German organ tablature’ the notes are separated into voices and given durations; with lute tablature the voices and durations (sometimes ambiguous, even for experienced players) have to be de-duced, so in future work on lute music we shall apply re-cently-developed techniques such as that described in [12]. From a musicological standpoint, this experiment rep-resents a first step towards a more detailed, corpus-level understanding of how intabulation worked as an artistic activity. Separating out the different influences and inten-tions of a composer or arranger is difficult, but it is our belief that some steps towards that separation can be made using approaches like this one. The methods presented here can easily be replicated with a larger set of examples and on an enlarged corpus. Given encodings of the vocal models for the intabulations in the collections and a means of aligning the two, we hope to perform more nuanced studies, looking at ar-rangement as a process as well as simply studying the end product. In particular, we intend to investigate the notion of ‘playability’ as an aspect of this process and of the choice of repertory for intabulation, in the belief that the prescriptive nature of tablature itself captures much use-ful evidence which has not yet been exploited by schol-ars. 7. ACKNOWLEDGEMENTS We are grateful to Sarge Gerbode for permission to use his personal collection of lute-music encodings, and to John Robinson for advice on lute-related matters. The work reported here was funded by the Arts and Humani-ties Research Council under grant AH/L006820/1. 8. REFERENCES [1] H. M. Brown: Instrumental Music Printed be-fore 1600: A Bibliography, Harvard University Press, Cambridge, Mass., 1965. [2] F. Jürgensen, “Cadential Accidentals in the ‘Buxheim Organ Book’ and its Concordances: A Mid-Fifteenth-Century Context for ‘musica ficta’ Practice”, Acta Musicologica, vol. 83, fasc. 1, pp. 39-68, 2011. [3] H. M. Brown, “Embellishment in Early Six-teenth-century Italian Intabulations”. Proceedings of the Royal Musical Association, vol. 100, pp. 49–83, 1973. Available online (accessed 16 March 2016): http://www.jstor.org/stable/766176 [4] D. Bianchini, Intabolatura de lauto … di recer-cari motetti madrigali canzon francese napolitane et bal-li, Antonio Gardane, Venice, 1546. [5] J. Robison, “Ornamentation in Sebastian Ochsenkun’s Tabulaturbuch auff die Lauten,” Journal of the Lute Society of America, Vol. xv, pp. 5–26, 1982. [6] G. Wiggins, K, Lemström and D. Meredith, “SIA(M)ESE: An algorithm for transposition invariant, polyphonic content-based music retrieval,” Proceedings of the Third International Symposium on Music Infor-mation Retrieval (ISMIR 2002), Paris 2002, p. 283-284. [7] H. M. Brown, Embellishing Sixteenth-century Music, Early music series, vol. 1, Oxford University Press, 1976. [8] R. Erig and V. Gutmann, Italian Diminutions from 1553-1638, Zurich: Amadeus Press, 1979. Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 529   [9] H. M. Brown, “Intabulation,” Grove Music Online, http://www.oxfordmusiconline.com/subscriber/book/omo_gmo, accessed 11 March 2016. [10] H. Minamino, “Sixteenth-century lute treatises with emphasis on process and techniques of intabula-tion,” unpub. PhD dissertation, Chicago University, 1988. [11] J. Ward, “The Use of Borrowed Material in 16th-Century Instrumental Music,” Journal of the Ameri-can Musicological Society, vol. 5, no. 2, pp. 88-98, 1952 [12] R. de Valk, “Bringing ‘Musicque into the tab-leture’: machine-learning models for polyphonic tran-scription of 16th-century lute tablature”, Early Music, vol. 43, no. 4, pp. 563-576, 2015. [13] D. Bates, M. Maechler, B. Bolker and S. Walk-er,  “Fitting linear mixed-effects models using lme4”, Journal of Statistical Software, vol. 67, no. 1, pp. 1-48, 2015   530 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Impact of Music on Decision Making in Quantitative Tasks.",
        "author": [
            "Elad Liebman",
            "Peter Stone",
            "Corey N. White"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417743",
        "url": "https://doi.org/10.5281/zenodo.1417743",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/272_Paper.pdf",
        "abstract": "The goal of this study is to explore which aspects of people’s analytical decision making are affected when ex- posed to music. To this end, we apply a stochastic sequen- tial model of simple decisions, the drift-diffusion model (DDM), to understand risky decision behavior. Numerous studies have demonstrated that mood can affect emotional and cognitive processing, but the exact nature of the impact music has on decision making in quantitative tasks has not been sufficiently studied. In our experiment, participants decided whether to accept or reject multiple bets with dif- ferent risk vs. reward ratios while listening to music that was chosen to induce positive or negative mood. Our re- sults indicate that music indeed alters people’s behavior in a surprising way - happy music made people make bet- ter choices. In other words, it made people more likely to accept good bets and reject bad bets. The DDM de- composition indicated the effect focused primarily on both the caution and the information processing aspects of de- cision making. To further understand the correspondence between auditory features and decision making, we stud- ied how individual aspects of music affect response pat- terns. Our results are particularly interesting when com- pared with recent results regarding the impact of music on emotional processing, as they illustrate that music af- fects analytical decision making in a fundamentally differ- ent way, hinting at a different psychological mechanism that music impacts.",
        "zenodo_id": 1417743,
        "dblp_key": "conf/ismir/LiebmanSW16",
        "content": "IMPACT OF MUSIC ON DECISION MAKING IN QUANTITATIVE TASKS\nElad Liebman\nDepartment of Computer Science\nThe University of Texas at Austin\neladlieb@cs.utexas.eduPeter Stone\nDepartment of Computer Science\nThe University of Texas at Austin\npstone@cs.utexas.eduCorey N. White\nDepartment of Psychology\nSyracuse University\ncnwhite@syr.edu\nABSTRACT\nThe goal of this study is to explore which aspects of\npeople’s analytical decision making are affected when ex-\nposed to music. To this end, we apply a stochastic sequen-\ntial model of simple decisions, the drift-diffusion model\n(DDM), to understand risky decision behavior. Numerous\nstudies have demonstrated that mood can affect emotional\nand cognitive processing, but the exact nature of the impact\nmusic has on decision making in quantitative tasks has not\nbeen sufﬁciently studied. In our experiment, participants\ndecided whether to accept or reject multiple bets with dif-\nferent risk vs. reward ratios while listening to music that\nwas chosen to induce positive or negative mood. Our re-\nsults indicate that music indeed alters people’s behavior in\na surprising way - happy music made people make bet-\nter choices. In other words, it made people more likely\nto accept good bets and reject bad bets. The DDM de-\ncomposition indicated the effect focused primarily on both\nthe caution and the information processing aspects of de-\ncision making. To further understand the correspondence\nbetween auditory features and decision making, we stud-\nied how individual aspects of music affect response pat-\nterns. Our results are particularly interesting when com-\npared with recent results regarding the impact of music\non emotional processing, as they illustrate that music af-\nfects analytical decision making in a fundamentally differ-\nent way, hinting at a different psychological mechanism\nthat music impacts.\n1. INTRODUCTION\nThere is plentiful evidence that one’s mood can affect how\none processes information. When the information being\nprocessed has emotional content (words, for instance), this\nphenomenon is referred to as mood-congruent processing,\nor bias, and it’s been found that positive mood induces a\nrelative preference for positive emotional content and vice\nversa [2,7]. However, what effect does music have on non-\nemotional decision making? This study focuses on the im-\npact of music on risky decision behavior which requires\nc/circlecopyrtElad Liebman, Peter Stone, Corey N. White. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Elad Liebman, Peter Stone, Corey N. White. “Impact\nof Music on Decision Making in Quantitative Tasks”, 17th International\nSociety for Music Information Retrieval Conference, 2016.quantitative reasoning. To this end, we design an experi-\nment in which participants decide whether to accept or re-\nject gambles with different win-loss ratios (meaning they\nhave different expected payoff).\nPrevious work in this area shows robust effects of loss\naversion, whereby participants put more weight on poten-\ntial losses than potential gains. Loss aversion in this con-\ntext manifests as subjects being unwilling to accept gam-\nbles unless the potential gain signiﬁcantly outweighs the\npotential loss (e.g., only accepting the gamble if the gain\nis twice as large as the loss [12, 13]). The present study\nfocuses on whether and how emotional music inﬂuences\nsuch risky decision behavior.\nNot much work has studied the direct connection be-\ntween music and risky decision making. Some previous\nwork has studied the general connection between gambling\nbehavior and ambiance factors including music [1, 3, 11]\nin an unconstrained casino environment. Additionally,\nNoseworthy and Finlay have studied the effects of music-\ninduced dissociation and time perception in gambling es-\ntablishments [6]. In this paper, we take a deeper and more\ncontrolled look at how music impacts decision making in\nthis type of risk-based analytical decision making. To this\neffect, we use a popular model of simple decisions, the\ndrift-diffusion model (DDM; [8]), to explore how music-\ninduced mood affects the different components of the de-\ncision process in such tasks. Our results indicate that mu-\nsic indeed has a nontrivial and unexpected effect, and that\ncertain types of music led to better decision making than\nothers.\nThe structure of the paper is as follows. In Section 2\nwe outline the characteristics of the drift-diffusion model,\nwhich we use in this study. In Section 3 we discuss our\nexperimental design and how data was collected from par-\nticipants. In Section 4 we present and analyze the results\nof our behavioral study. In Section 5 we analyze how indi-\nvidual auditory components correlate with the behavioral\npatterns observed in our human study. In Section 6 we re-\ncap our results and discuss them in a broader context.\n2. THE DRIFT-DIFFUSION MODEL\nThis study employs the Drift Diffusion Model (DDM)\nof simple decisions to decompose the observed decision\nbehavior into its underlying decision components. The\nDDM, shown in Figure 1, belongs to a family of evi-\ndence accumulation models which frame simple decisions661in terms of gradual sequential accumulation of noisy evi-\ndence until a decision criterion is met. In the model, the\ndecision process starts between the two boundaries that\ncorrespond to the response alternatives. Evidence is ac-\ncumulated over time to drive the process toward one of\nthe boundaries. Once a boundary is reached, it marks the\nchoice of a speciﬁc response. The time taken to reach\nthe boundary represents the decision time. The overall\nresponse time is the sum of the time it takes to make a\ndecision plus the time required for processes outside the\ndecision process like encoding and motor execution. The\nmodel includes a parameter for this nondecision time (Ter),\nto account for the duration of these processes.\nThe primary components of the decision process in the\nDDM are the boundary separation, the starting point, and\nthe drift rate. Boundary separation represents response\ncaution or the speed vs. accuracy tradeoff exhibited by the\nparticipant. Wide boundaries indicate a cautious response\nstyle where more evidence needs to be accumulated before\na decision is reached. The need for more evidence makes\nthe decision process slower, but also more accurate, since it\nis less likely to reach the wrong boundary by mistake. The\nstarting point of the diffusion process (z) reﬂects response\nexpectancy. If z is closer to the top boundary, for instance,\nit means less evidence is required to reach that boundary,\nso “positive” responses will be faster and more probable\nthan “negative” responses. Finally, the drift rate (v) rep-\nresents the processing of evidence from stimulus which\ndrives the accumulation process. Positive values indicate\nevidence for the top boundary, and negative values for the\nbottom boundary. Furthermore, the absolute value of the\ndrift rate represents the strength of the stimulus evidence,\nwith larger values indicating strong evidence and leading\nto fast and accurate responses.\nIn the framework of the DDM, there are two mecha-\nnisms that can drive behavioral bias. Changes in the start-\ning point (z) reﬂect a response expectancy bias, whereby\nthere is an a-priori preference for one response even before\nthe stimulus is shown [5,14]. Experimentally, response ex-\npectancy bias is observed when participants have an expec-\ntation that one response is more likely to be correct and/or\nrewarded than the other. In contrast, changes in the drift\nrate (v) reﬂect a stimulus evaluation bias, whereby there is\na shift in how the stimulus is evaluated to extract the de-\ncision evidence. Experimentally, stimulus evaluation bias\nis observed when there is a shift in the stimulus strength\nand/or the criterion value used to classify the stimuli. Thus\nresponse expectancy bias, reﬂected by the starting point in\nthe DDM, indicates a shift in how much evidence is re-\nquired for one response relative to the other, whereas stim-\nulus evaluation bias, reﬂected by a shift in the drift rates in\nthe DDM, indicates a shift in what evidence is extracted by\nthe stimulus under consideration. Importantly, both mech-\nanisms can produce behavioral bias (faster and more prob-\nable responses for one choice [14]), but they differentially\naffect the distribution of response times. In brief, response\nexpectancy bias only affects fast responses, whereas stim-\nulus evaluation bias affects both fast and slow responses\nFigure 1 . An Illustration of the Drift-Diffusion Model.\n(see [14]). It is this differential effect on the response time\n(abbreviated RT) distributions that allows the DDM to be\nﬁtted to behavioral data to estimate which of the two com-\nponents, starting point or drift rates, is driving the bias\nobserved in the RTs and choice probabilities. The DDM\nhas been shown to successfully differentiate these two bias\nmechanisms from behavioral data in both perceptual and\nrecognition memory tasks [14].\nThis study used the DDM approach described above to\ninvestigate how music-induced mood affects the different\ndecision components when performing a quantitative task.\nParticipants listened to happy or sad music while deciding\nwhether to bet or fold as bets with different win-loss ra-\ntios were proposed to them. The DDM was then ﬁtted to\neach participant’s behavioral data to determine whether the\nmood induction affected response expectancy bias, stimu-\nlus evaluation bias, or both.\n3. METHODS\nParticipants were presented with simple binary gambles\nand were asked whether to accept (bet) or reject them\n(fold). Each gamble had a 50%-50% chance of success,\nwith varying win to loss ratio, reﬂecting how much was\nto be gained vs. lost. For example, a 15:5 win-loss ratio\nreﬂect a 50% chance to win 15 points and a 50% chance\nof losing 5 points. After a ﬁxation cue was shown for 500\nms, each gamble was presented in the center of the screen\nand remained on screen until a response was given. If no\nresponse was given after 3.5 seconds, the trial ended as a\n“no response” trial. Responses were indicated with the “z”\nand “/” keys, and mapping between the key and response\nwas counterbalanced across participants.\nThe gamble stimuli were partitioned to very negative\n(win-loss ratio in range [0.33,0.66)), negative (win-loss\nratio in range [0.66,1)), positive (win-loss ratio in range\n[1,2)), and very positive (win-loss ratio in range [2,3]).\nThe actual values of the bets were randomized in the\nrange of [3,60]. Each experiment comprised 20 batches\nof 20 gambles, such that in each batch each stimuli condi-\ntion was repeated 5 times (gamble order was randomized).\nSubjects were not shown the outcome of their gambles im-\nmediately as that would be distracting. Instead, between\neach batch subjects were shown the overall score they ac-662 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016crued for the previous batch (whereas each batch score\nstarts as 0). To encourage competitive behavior, they were\nalso shown the expected score for that batch. A different\nsong was played during each block of 5 batches, alternat-\ning from positive to negative music across blocks. The or-\nder of the songs was counterbalanced across subjects. The\nentire experiment lasted less than 30 minutes. To ensure\nthat the results were not speciﬁc to the particular choice\nof songs, the entire experiment was repeated with a large\nsample of participants ( N= 84 ), and two separate sets of\nsongs to assess result reliability.\nThe music used for this experiment is the same as that\nused in [4]. It is a collection of 8 publicly available songs\nwhich was surveyed to isolate two clear types - music that\nis characterized by slow tempo, minor keys and somber\ntones, typical to traditionally “sad” music, and music that\nhas upbeat tempo, major scales and colorful tones, which\nare traditionally considered to be typical to “happy” music.\nThe principal concern in selecting these musical stimuli,\nrather than their semantic categorization as either happy or\nsad, was to curate two separate “pools” of music sequences\nthat were broadly characterized by a similar temperament\n(described above), and show they produced consistent re-\nsponse patterns. In [4], it has been shown experimentally\nthat the selected music was effective for inducing the ap-\npropriate mood. This was done by selecting a separate pool\nof 40 participants and having them rate each song on a 7-\npoint Likert scale, with 1 indicating negative mood and 7\nindicating positive mood. It was then shown that the songs\ndesignated as positive received meaningfully and statisti-\ncally signiﬁcantly higher scores than those denoted as sad.\nThe DDM was ﬁtted to each participant’s data, sepa-\nrately for positive and negative music blocks, to estimate\nthe values of the decision components. The data entered\ninto the ﬁtting routine were the choice probabilities and\nRT distributions (summarized by the .1, .3, .5, .7, and .9\nquantiles) for each response option and stimulus condi-\ntion. The parameters of the DDM were adjusted in the\nﬁtting routine to minimize the χ2value, which is based\non the misﬁt between the model predictions and the ob-\nserved data (see [9]). For each participant’s data set, the\nmodel estimated a value of boundary separation, nondeci-\nsion time, starting point, and a separate drift rate for each\nstimulus condition. Because of the relatively low number\nof observations used in the ﬁtting routine, the variability\nparameters of the full DDM were not estimated (see [8]).\nThis resulted in two sets of DDM parameters for each par-\nticipant, one for the positive music blocks and one for the\nnegative music blocks.\n4. EXPERIMENTAL RESULTS\nThe response times and choice probabilities shown in Fig-\nure 2 indicate that the mood-induction successfully af-\nfected the decision making behavior observed across par-\nticipants. The left column shows the response proportions,\nthe center column shows normalized response times for\nbetting decisions, and the right panel shows normalized\nresponse times for the folding decisions. The betting pro-portions and response time (or RT) measures for the two\nconditions - the happy songs and the sad songs - indicate a\nclear difference between the conditions. Generally speak-\ning, happy music led to more “correct” behavior - partici-\npants were more likely to accept good bets and reject bad\nbets under the happy song condition than the sad song con-\ndition. These trends are evident across all gamble propor-\ntions and bet-fold decisions, but were only shown to be\nstatistically signiﬁcant for some of the settings; the differ-\nence in betting proportions is shown to be signiﬁcant for\nvery negative, positive and very positive gambles, whereas\nthe difference in response times is only shown to be signif-\nicant for folding decisions in very positive gambles. Sig-\nniﬁcance was evaluated using a paired t-test with p≤0.05.\nFigure 3 shows the DDM parameters ﬁtted for the ex-\nperiment. Although the two bias-related measures (start-\ning point and drift rates) are of primary interest, all of the\nDDM parameters were compared across music conditions.\nIt is possible that the different music conditions could af-\nfect response caution and nondecision time. For example,\nthe slower tempo of the sad songs could lead participants\nto become more cautious and have slower motor execu-\ntion time. Thus all parameters were investigated. As the\ntop-left and top-center panels of Figure 3 show, the music\nconditions did not differentially affect response caution or\nencoding/motor time, as neither boundary separation nor\nnondecision time differed between happy and sad music\nblocks. Of primary interest were the starting point and\ndrift rate parameters, which provide indices of response\nexpectancy and stimulus evaluation bias, respectively. In-\nterestingly, as apparent in the top-right and bottom-right\npanels of Figure 3, overall, we did not observe any stimu-\nlus (evidence processing) bias nor starting point (response\nexpectancy) bias in the two music conditions. However,\nthe key difference lied in the drift rates themselves. Fitting\nparameters for the drift rates for the four gamble types indi-\ncate an overall change in evidence processing in the happy\nvs. the sad music conditions, which is statistically signiﬁ-\ncant for all gamble proportions. This outcome is shown in\nthe bottom-left panel of Figure 3. In other words, people\nwere faster to process the evidence and make betting deci-\nsions for good gambles and folding decisions for bad gam-\nbles in happy vs. sad music. This difference is summarized\nin the bottom-center panel of Figure 3, which presents the\ndiscriminability factor in the happy vs. the sad condition.\nDiscriminability is deﬁned as the sum of the drift rates for\ngood bets minus the sum of the drift rates for the bad bets,\n(dpositive +dvery−positive−dnegative−dvery−negative ).\nThis measure represents the “processing gap” between\ngood evidence (good bets) and bad evidence (bad bets).\nThe discriminability was dramatically higher for happy\nsongs compared to sad songs.\nThe DDM results show that the music-based manipula-\ntion of mood affected the overall processing of information\nin the quantitative task of deciding when to bet and when\nto fold, rather than any single bias component. There were\nno effects of music on response caution, nondecision time,\nor response or stimulus bias, meaning that people weren’tProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 663Figure 2 . Response patterns in terms of response times and bet-fold proportions for the behavioral experiment. A statis-\ntically signiﬁcant difference between the happy song and the sad song conditions is evident for betting proportions given\nthe four clusters of betting ratios (very negative, negative, positive and very positive). There is also a large statistically\nsigniﬁcant difference between response times for folding in the two different conditions. Error bars reﬂect 95% conﬁdence\nintervals. * =p<.05; **=p<.01; *** =p<.001.\nmore likely to accept bets or reject them in one condition\nor the other, but rather the change impacted the entire de-\ncision process. In other words, the mood change induced\nby music neither affected the a-priori inclination of people\nto bet or to fold, nor has it led to a relative difference in\nprocessing one type of bet vs. the other, but rather simply\nmade people make better decisions (more likely to accept\ngood bets and reject bad ones).\n5. CORRELATING RESPONSES AND MUSICAL\nFEATURES\nThe partition between “positive” and “negative” mood-\ninducing songs is easy to understand intuitively, and in it-\nself is enough to induce the different behavioral patterns\ndiscussed in the previous section. However, similarly to\nthe analysis performed in [4], we are interested in ﬁnd-\ning a deeper connection between the behavior observed\nin the experiment and the different characteristics of mu-\nsic. More exactly, we are interested in ﬁnding the corre-\nspondence between various musical features, which also\nhappen to determine how likely a song is to be perceived\nas happy or sad, and the gambling behavior manifested\nby participants. To this end, we considered the 8 songs\nused in this experiment, extracted key characterizing fea-\ntures which we assume are relevant to their mood classiﬁ-\ncation, and examined how they correlate with the subject\ngambling behavior we observed.\n5.1 Extracting Raw Auditory Features\nWe focused on three major auditory features: a) overall\ntempo; b) overall “major” vs. “minor” harmonic character;\nc) average amplitude, representing loudness. Features (a)\nand (c) were computed using the Librosa library [10]. To\ncompute feature (b), we implemented the following proce-\ndure, similar to that described in [4]. For each snippet of20 beats an overall spectrum was computed and individual\npitches were extracted. Then, for that snippet, according\nto the amplitude intensity of each extracted pitch, we iden-\ntiﬁed whether the dominant harmonic was major or minor.\nThe major/minor score was deﬁned to be the proportion of\nmajor snippets out of the overall song sequence. Analy-\nsis done in [4] conﬁrms these three features were indeed\nassociated with our identiﬁcation as “positive” vs. “nega-\ntive”. Having labeled “positive” and “negative” as 1 and\n0 respectively, a Pearson correlation of 0.7−0.8with p-\nvalues≤0.05was observed between these features and\nthe label. Signiﬁcance was further conﬁrmed by applying\nan unpaired t-test for each feature for positive vs. negative\nsongs (p-values <.05).\n5.2 Processing Observed Gambling Behavior\nGiven the complexity of the behavioral experiment dis-\ncussed in this paper, several behavioral breakdowns of\nparticipant behavior were extracted. Normalizing the re-\nsponse times (RTs) for each participant, we separately con-\nsidered the average response times for betting and for fold-\ning for all four gamble types and songs (64 values overall).\nSubsequently, we aggregated these average response times\nper decision (bet or fold), per gamble type (very negative,\nnegative, positive and very positive), per song (4 happy\nsongs, 4 sad songs overall), to obtain 64 average response\ntimes and response time variance per /angbracketleftdecision, gamble\ntype, song/angbracketrightconﬁguration. Then we could correlate these\nvalues per/angbracketleftdecision, gamble type /angbracketrightsetting with the features\nextracted for each song. Similarly, we extracted the aver-\nage bet-fold ratio and bet-fold variance across all partic-\nipants for each/angbracketleftdecision, gamble type, song /angbracketrightconﬁgura-\ntion as well. As a result we were also able to examine the\nrelationship between bet-fold ratios per /angbracketleftdecision, gamble\ntype/angbracketrightsetting with the features extracted for the songs.664 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Figure 3 . Drift-Diffusion Model parameters ﬁtted for the behavioral experiment. A statistically signiﬁcant difference\nbetween the happy song and the sad song conditions is evident for betting proportions given the four clusters of betting\nratios (very negative, negative, positive and very positive). There is also a large statistically signiﬁcant difference between\nresponse times for folding in the two different conditions. Error bars reﬂect 95% conﬁdence intervals. * =p < . 05; **\n=p<.01; *** =p<.001.\nDecision Gamble RT Avg RT Var. Avg. p-val Var. p-val\nbet v. negative -0.73 -0.61 0.03 0.1\nbet negative -0.65 0.48 0.07 0.22\nbet positive -0.77 -0.64 0.02 0.07\nbet v. positive -0.78 -0.59 0.02 0.11\nfold v. negative -0.81 -0.65 0.01 0.07\nfold negative -0.78 -0.56 0.02 0.14\nfold positive -0.45 -0.45 0.25 0.25\nfold v. positive -0.77 0.76 0.02 0.02\nTable 1 . Correlation values between tempo and response\ntimes (average and variance). Results with p-value ≤0.1\nare marked in bold.\n5.3 Observed Correlations\nIn this section we discuss how the auditory features corre-\nsponded with the normalized response time and bet-fold\nratio information extracted from the behavioral experi-\nment. We proceed to analyze the more exact correspon-\ndence between the DDM parameters as extracted per song\nindividually and the auditory features of the songs. We\nnote that since we are correlating continuous scalar aggre-\ngates across users with continuous auditory features, using\nthe assumptions implicit in a standard Pearson correlation\nis reasonable.\n5.3.1 Correlation with RTs and Bet-Fold Ratio\nExamining the relationship between the features extracted\nper song and the response time and bet-fold ratio data dis-\ncussed in 5.2 reveals a compound and interesting picture.Tempo was consistently and in most cases statistically\nsigniﬁcantly inversely correlated with response times. This\nwas true for all gamble types and decision combinations.\nTempo also tended to be inversely proportional to the ob-\nserved response time variance. Again, this result was con-\nsistent across all gamble type and decision combinations.\nIn other words, generally speaking, not only people re-\nsponded faster (lower response times) the faster the music\nwas, the variance in response times also tended to be re-\nduced. The observed Pearson correlations for average nor-\nmalized response times and response time variances across\nthe 8 gamble type and decision combinations is provided\nin Table 1.\nTempo was also inversely correlated with the average\nbet-fold ratio for very negative gambles ( r=−0.74,p=\n0.03). This also manifested in the correlation with the bet-\nfold variance ( r=−0.66,p= 0.06). However, it was lin-\nearly correlated with the bet-fold ratio in the very positive\ngambles case ( r= +0.71,p= 0.04). Furthermore, in the\nvery positive gambles case, the variance was still reduced,\nleading to a negative correlation ( r=−0.71,p= 0.04).\nIn other words, the faster the music, the more people are\nlikely to bet on very good bets, and more consistently (re-\nducing variance). Furthermore, the faster the music, the\nmore likely people are to fold on bad bets, and more con-\nsistently (reducing variance). This is a strong signal for\nhow tempo improves the quality of decision making in\nquantitative tasks.\nThere is evidence that the major dominance feature (de-Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 665termining the major to minor chord proportion in each\nsong) is inversely correlated to the average bet-fold ra-\ntio and the bet-fold ratio variance in the very negative\ngambles case (average: r=−0.6,p= 0.11, variance:\nr=−0.61,p= 0.10). Similarly, there is some evidence\nthat major dominance is linearly correlated with the aver-\nage bet-fold ratio and inversely correlated to the bet-fold\nvariance in the strong-positive case, but this result wasn’t\nas convincing (average: r= +0.42,p= 0.29, variance:\nr=−0.58,p= 0.14). This result, though inconclusive,\nhints at the possibility that the more major chords there\nare in a song, the better the analytical decision making that\nsubjects manifest.\nInterestingly, the major dominance feature (determining\nthe major to minor chord proportion in each song) was in-\nversely proportional to the variance in response times when\nfolding on a very positive bet ( r=−0.71,p= 0.04).\nMajor dominance was also inversely proportional to vari-\nance in response times betting on a very negative bet\n(r=−0.65,p= 0.07). In other words, the more major\nchords appeared in a song, the less variability people dis-\nplayed in the time it took them to make a poor decision.\nThis could be a side effect of people making fewer such\nmistakes in these gamble - decision combinations, as was\ndocumented in previous sections.\nThe average amplitude was inversely correlated to the\naverage bet-fold ratio and the bet-fold ratio variance for\nnegative and very negative gambles. These observations\nseem borderline signiﬁcant (average: r=−0.59,p= 0.12\nfor negative, r=−0.53,p= 0.17for very negative, vari-\nance:r=−0.58,p= 0.13for negative, r=−0.7,p=\n0.05for very negative). This would imply that the louder\nthe music, the less likely people are to make betting deci-\nsions on bad gambles, and variance is also reduced.\n5.3.2 Correlation with DDM Decomposition\nFinally, we were also interested in examining how the in-\ndividual DDM parameters ﬁtted for each song separately\ncorresponded with the song features. Comparing the DDM\nparameters per song with the tempo, major dominance and\namplitude data, we observed a statistically signiﬁcant cor-\nrelation between the tempo and the drift rate for very pos-\nitive gambles (Figure 4(a), r=−0.72,p= 0.04), tempo\nand very negative gambles (Figure 4(b), r= +0.79,p=\n0.01), and, interestingly, between the mean amplitude and\nthe response caution, a connection that was also suggested\nin [4] (Figure 4(c), r=−0.67,p= 0.06). These obser-\nvations corroborate both the observations in Section 5.3.1,\nand in Section 4.\n6. SUMMARY & DISCUSSION\nIn this paper, we study how music-induced mood affects\ndecision making in risky quantitative tasks. Subjects were\npresented with gambles and needed to decide whether to\naccept or reject these gambles as different types of music\nwere played to them. Our results show that while there is\nno evidence for music-induced bias in the decision making\nprocess, music does have a differential effect on decision\nFigure 4 . (a) Correlation between tempo and the drift rate\nfor very negative gambles. (b) Correlation between tempo\nand the drift rate for very positive gambles. (c) Correlation\nbetween mean amplitude and the overall response caution\n(boundary separation).\nmaking behavior. Participants who listened to music cate-\ngorized as happy were faster to make decisions than people\nwho listened to music categorized as sad. Moreover, the\ndecisions participants made while listening to happy mu-\nsic were consistently better than those made while listening\nto sad music, implying increased discriminabilty. Further\nanalysis indicates there is a correlation between tempo and\nthe speed and quality of decision making in this setting. In-\nterestingly, previous work on gambling behavior has found\na connection between the tempo and the speed of decision\nmaking, but was unable to isolate further impact on the\nquality of decision making, due to a fundamentally differ-\nent design and different research questions [1].\nOf particular note is the comparison between the results\nof a recent paper studying the connection between music-\ninduced mood and mood-congruent bias [4]. In that pa-\nper, participants were requested to classify words as happy\nor sad as music categorized as happy or sad was played.\nResults indicated a clear expectancy bias, meaning mu-\nsic affected people’s a-priori tendency to classify words as\nhappy or sad. This paper, which uses the exact same set of\nsongs, has reported no such bias effect, or any bias effect,\nfor that matter. This difference suggests the psychological\nmechanisms involved in emotional classiﬁcation and risky\nanalytical decision making are inherently different.\nThis paper is a meaningful step towards a better under-\nstanding of the impact music has on commonplace cogni-\ntive processes which involve quantitative reasoning and de-\ncision making. In future work, additional tasks and other\nmusic stimuli should be studied to better understand the\nrelationship between music and this type of cognitive pro-\ncessing.666 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20167. REFERENCES\n[1] Laura Dixon, Richard Trigg, and Mark Grifﬁths. An\nempirical investigation of music and gambling be-\nhaviour. International Gambling Studies , 7(3):315–\n326, 2007.\n[2] Rebecca Elliott, Judy S Rubinsztein, Barbara J Sa-\nhakian, and Raymond J Dolan. The neural basis\nof mood-congruent processing biases in depression.\nArchives of general psychiatry , 59(7):597–604, 2002.\n[3] Mark Grifﬁths and Jonathan Parke. The psychology of\nmusic in gambling environments: An observational re-\nsearch note. Journal of Gambling Issues , 2005.\n[4] Elad Liebman, Peter Stone, and Corey N. White. How\nmusic alters decision making - impact of music stimuli\non emotional classiﬁcation. In Proceedings of the 16th\nInternational Society for Music Information Retrieval\nConference, ISMIR 2015, M ´alaga, Spain, October 26-\n30, 2015 , pages 793–799, 2015.\n[5] Martijn J Mulder, Eric-Jan Wagenmakers, Roger Rat-\ncliff, Wouter Boekel, and Birte U Forstmann. Bias in\nthe brain: a diffusion model analysis of prior probabil-\nity and potential payoff. The Journal of Neuroscience ,\n32(7):2335–2343, 2012.\n[6] Theodore J Noseworthy and Karen Finlay. A compar-\nison of ambient casino sound and music: Effects on\ndissociation and on perceptions of elapsed time while\nplaying slot machines. Journal of Gambling Studies ,\n25(3):331–342, 2009.\n[7] Kristi M Olafson and F Richard Ferraro. Effects of\nemotional state on lexical decision performance. Brain\nand Cognition , 45(1):15–20, 2001.\n[8] Roger Ratcliff and Gail McKoon. The diffusion deci-\nsion model: theory and data for two-choice decision\ntasks. Neural computation , 20(4):873–922, 2008.\n[9] Roger Ratcliff and Francis Tuerlinckx. Estimating pa-\nrameters of the diffusion model: Approaches to dealing\nwith contaminant reaction times and parameter vari-\nability. Psychonomic bulletin & review , 9(3):438–481,\n2002.\n[10] Brian McFee ; Matt McVicar ; Colin Raf-\nfel ; Dawen Liang ; Douglas Repetto. Librosa.\nhttps://github.com/bmcfee/librosa , 2014.\n[11] Jenny Spenwyn, Doug JK Barrett, and Mark D Grif-\nﬁths. The role of light and music in gambling be-\nhaviour: An empirical pilot study. International Jour-\nnal of Mental Health and Addiction , 8(1):107–118,\n2010.\n[12] Sabrina M Tom, Craig R Fox, Christopher Trepel, and\nRussell A Poldrack. The neural basis of loss aversion in\ndecision-making under risk. Science , 315(5811):515–\n518, 2007.[13] Amos Tversky and Daniel Kahneman. Advances in\nprospect theory: Cumulative representation of uncer-\ntainty. Journal of Risk and uncertainty , 5(4):297–323,\n1992.\n[14] Corey N White and Russell A Poldrack. Decompos-\ning bias in different types of simple decisions. Journal\nof Experimental Psychology: Learning, Memory, and\nCognition , 40(2):385, 2014.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 667"
    },
    {
        "title": "Predicting Missing Music Components with Bidirectional Long Short-Term Memory Neural Networks.",
        "author": [
            "I-Ting Liu",
            "Richard Randall"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417239",
        "url": "https://doi.org/10.5281/zenodo.1417239",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/097_Paper.pdf",
        "abstract": "Successfully predicting missing components (entire parts or voices) from complex multipart musical textures has attracted researchers of music information retrieval and music theory. However, these applications were limited to either two-part melody and accompaniment (MA) tex- tures or four-part Soprano-Alto-Tenor-Bass (SATB) tex- tures. This paper proposes a robust framework appli- cable to both textures using a Bidirectional Long-Short Term Memory (BLSTM) recurrent neural network. The BLSTM system was evaluated using frame-wise accura- cies on the Nottingham Folk Song dataset and J. S. Bach Chorales. Experimental results demonstrated that adding bidirectional links to the neural network improves predic- tion accuracy by 3% on average. Specifically, BLSTM out- performs other neural-network based methods by 4.6% on average for four-part SATB and two-part MA textures (em- ploying a transition matrix). The high accuracies obtained with BLSTM on both two-part and four-part textures indi- cated that BLSTM is the most robust and applicable struc- ture for predicting missing components from multi-part musical textures.",
        "zenodo_id": 1417239,
        "dblp_key": "conf/ismir/LiuR16",
        "content": "PREDICTING MISSING MUSIC COMPONENTS WITH BIDIRECTIONAL\nLONG SHORT-TERM MEMORY NEURAL NETWORKS\nI-Ting Liu\nCarnegie Mellon University\nSchool of Music\nitingl@andrew.cmu.eduRichard Randall\nCarnegie Mellon University\nSchool of Music\nCenter for the Neural Basis of Cognition\nrandall@cmu.edu\nABSTRACT\nSuccessfully predicting missing components (entire parts\nor voices) from complex multipart musical textures has\nattracted researchers of music information retrieval and\nmusic theory. However, these applications were limited\nto either two-part melody and accompaniment (MA) tex-\ntures or four-part Soprano-Alto-Tenor-Bass (SATB) tex-\ntures. This paper proposes a robust framework appli-\ncable to both textures using a Bidirectional Long-Short\nTerm Memory (BLSTM) recurrent neural network. The\nBLSTM system was evaluated using frame-wise accura-\ncies on the Nottingham Folk Song dataset and J. S. Bach\nChorales. Experimental results demonstrated that adding\nbidirectional links to the neural network improves predic-\ntion accuracy by 3% on average. Speciﬁcally, BLSTM out-\nperforms other neural-network based methods by 4.6% on\naverage for four-part SATB and two-part MA textures (em-\nploying a transition matrix). The high accuracies obtained\nwith BLSTM on both two-part and four-part textures indi-\ncated that BLSTM is the most robust and applicable struc-\nture for predicting missing components from multi-part\nmusical textures.\n1. INTRODUCTION\nThis paper presents a method for predicting missing com-\nponents from complex multipart musical textures. Specif-\nically, we examine two-part melody and accompaniment\n(MA) and Soprano-Alto-Tenor-Bass (SATB) chorale tex-\ntures. We treat each voice as a part (e.g. the melody of the\nMA texture or the Soprano of the SATB texture) and the\nproblem we address is given an incomplete texture, how\nsuccessfully can we generate the missing part. This project\nproposes a robust approach that is capable of handling both\ntextures elegantly and has applications to any style of mu-\nsic. Predictions are made using a Bidirectional Long-Short\nTerm Memory (BLSTM) recurrent neural network that is\nable to learn the relationship between components, and\nc/circlecopyrtI-Ting Liu, Richard Randall. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: I-Ting Liu, Richard Randall. “Predicting Missing Music Com-\nponents with Bidirectional Long Short-Term Memory Neural Networks”,\n17th International Society for Music Information Retrieval Conference,\n2016.can thus be trained to predict missing components. This\nwork demonstrates the capability of the BLSTM system by\nconducting experiments on the two tasks mentioned above\nwith two distinct datasets.\nAnalyzing music with the aid of computer programs\nhas attracted researchers of music information retrieval and\nmusic theory over the past twenty years. Music (especially\nwestern tonal music) has always been regarded as a kind\nof art with rigorous formalization. Various complex rules\nregulate how notes can be and cannot be played together in\ncomplex multipart textures. Such rules change over time\nand are subject to multiple factors. As artiﬁcial intelli-\ngence and machine-learning research advances, it is natu-\nral that computer scientists apply such technique to music\nanalysis in order to elucidate these rules [2]. Two popu-\nlar tasks investigated in this area are (1) generating chord\naccompaniments for a given melody in a two-part MA\ntexture and (2) generating a missing voice for an incom-\nplete four-part SATB texture. Successfully accomplish-\ning either task manually is time-consuming and requires\nconsiderable style-speciﬁc knowledge and the applications\ndiscussed below are designed to automate and help non-\nprofessional musicians compose and analyze music.\nApproaches that treat these problems can be categorized\ninto two types according to the level of human engagement\nin discovering and applying music rules. Early works that\nhandle incomplete four-part SATB textures were mostly\nknowledge-based models. Steels [28] proposed a represen-\ntation system to encode musical information and exploited\nheuristic search, which takes the form of if-then musical\nrules that speciﬁes solutions under different conditions to\ngenerate voices. Ebcio ˘glu built CHORAL, a knowledge-\nbased system that includes over 350 rules modeling the\nstyle of Johann Sebastian Bach [8]. Due to the large num-\nber of rules involved, some studies modeled the problem\nas a constraint satisfaction problem, as was used by Pachet\nand Roy [22] on four-part textures and Ramirez, et al. [25]\non two-part textures. Knowledge-based genetic algorithms\nwere also used as an alternative method to represent the\nrules. Mcintyre [21] implemented a system that harmo-\nnizes user-deﬁned melody in Baroque style, and Hall [17]\npresented a system that selects combination of attributes to\nmodel the harmonization of J. S. Bach’s chorales. Freitas\nand Guimaraes also implemented a system based on ge-\nnetic algorithms in [11]. The ﬁtness function and genetic225operators rely on “music knowledges” to suggest chord\nprogressions for given melodies.\nWhile rules in knowledge-based systems have to be\nmanually encoded into these systems, rules in probabilis-\ntic models and neural networks can be derived by training\ncorpora without human intervention by the models. Hid-\nden Markov Models (HMM) are one of the most com-\nmon probabilistic models for the task of generating a chord\nsequence given melodies for two-part textures [27]. In\nHMM, a pre-selected dataset is used to train a transi-\ntion probability matrix, which represents the probability\nof changing from one chord to another, and a melody ob-\nservation matrix, the probability of encountering each note\nwhen different chords are being played. The optimal chord\nsequence is then generated using dynamic programming,\nor Viterbi Algorithm. HMM are also used by Allan [1]\nto harmonize four-part chorales in the style of J. S. Bach.\nIn addition to HMM, Markov Model and Bayesian Net-\nworks are alternative models used for four-part textures by\nBiyikoglu [3] and Suzuki, et al. [29]. Raczy ´nski, et al. [24]\nproposed a statistical model that combines multiple sim-\nple sub-models. Each sub-model captures different music\naspects such as metric and pitch information, and all of\nthem are then interpolated into a single model. Paiement,\net al. [23] proposed a multi-level graphical model, which is\nproved to capture the long-term dependency among chord\nprogression better than traditional HMM. One drawback\nof probabilistic models is that they cannot correctly handle\ndata that are not seen in training data. Chuan and Chew [5]\nreduced this problem by using a hybrid system for style-\nspeciﬁc chord sequence generation with statistical learning\napproach and music theory.\nNeural networks have also been used by some re-\nsearchers. Gang, et al. [13] were one of the earliest that\nused neural networks to produce chord harmonization for\ngiven melodies. Jordan’s sequential neural network con-\nsisted of a sub-net that learned to identify chord notes for\nthe melody in each measure, and the result was fed into\nthe network to learn the relationship between melodies and\nchords. The network was later adopted real-time applica-\ntion [12, 14]. Consisting of 3 layers, the input layer takes\npitch, metric information and the current chord context,\nand the output layer predicts the next chord. Cunha, et\nal. [6] also proposed a real-time chord harmonization sys-\ntem using multi-layer perceptron (MLP) neural network\nand a rule-based sequence tracker that analyzes the struc-\nture of the song in real-time, which provides additional in-\nformation on the context of the notes being played.\nHoover, et al. [20] used two Artiﬁcial Neural Networks\n(ANN) to model the relationship between melodies and ac-\ncompaniment as a function of time. The system was later\nextended to generate multi-voice accompaniment by in-\ncreasing the size of the output layer in [19]. Bellgard and\nTsand [2] trained an effective Bolzmann machine and in-\ncorporated external constraints so that harmonization fol-\nlows the rules of a chorale. Fuelner developed a feed-\nforward neural network that harmonizes melodies in spe-\nciﬁc styles in [9]. De Prisco, et al. [7] proposed a neuralnetwork that ﬁnds appropriate chords to harmonize given\nbass lines in four-part SATB chorales by combining three\nbase networks, each of which models context of different\ntime lengths.\nAlthough all these previous studies provide valuable in-\nsights, a number of constraints exist in their applications.\nMost rules encoded in knowledge-based systems are style-\nspeciﬁc, making them hard to apply to other types of mu-\nsic efﬁciently. Probabilistic models and neural networks,\non the other hand, provide a much more adaptable solution\nthat can be applied to music of different styles by learning\nrules from different styles of training data. Nevertheless,\nmany of the probabilistic models can only handle music\npieces of ﬁxed length. In addition, the transition matrix of\nprobabilistic models has to be learned using speciﬁc mu-\nsic representation (e.g. chords) and cannot be generalized\nto other representations. Moreover, probabilistic models\ntend to ignore long-term dependency among music com-\nponents as they mainly focus on local transitions between\ntwo consecutive components. Existing studies using neural\nnetworks captured long-term dependencies in music and\nalso are capable of dealing with music pieces of arbitrary\nlengths. However, neural networks have been notoriously\nhard to train, and their ability to utilize long-term informa-\ntion was limited until the introduction of Long-Short Term\nMemory (LSTM) cells.\nAlthough BRNNs have access to both past and future\ninformation, they have been notoriously hard to train be-\ncause of “vanishing gradients,” a problem commonly seen\nin RNNs when training with gradient based methods. Gra-\ndient methods, such as Back-Propagation Through Time\n(BPTT) [31], Real-Time Recurrent Learning (RTRL) [26]\nand their combinations, update the network by ﬂowing er-\nrors “back in time.” As the error propagates from layer to\nlayer, it tends to either explode or shrink exponentially de-\npending on the magnitude of the weights. Therefore, the\nnetwork fails to learn long-term dependency between in-\nputs and outputs. Tasks with time lags that are greater than\n5-10 time steps are already difﬁcult to learn, not to mention\nthat dependency of music usually spans across tens to hun-\ndreds of notes in time, which contributes to music’s unique\nphrase structures. Long short term memory (LSTM) [18]\nalgorithm was designed to tackle the error-ﬂow problem.\nIn an LSTM hidden layer, fully-connected memory\nblocks replace nonlinear units that are often used in feed-\nforward neural network. The core of a memory block is\na linear cell that sums up the inputs, which has a self-\nrecurrent connection of ﬁxed weight 1.0, preserving all\nprevious information and ensuring they would not vanish\nas they are propagated in time. A memory block also con-\ntains three sigmoid gating units: input gate, output gate,\nand forget gate. An input gate learns to control when in-\nputs are allowed to pass into the cell in the memory block\nso that only relevant contents are remembered; an output\ngate learns to control when the cell’s output should be\npassed out of the block, protecting other units from in-\nterference from current irrelevant memory contents; a for-\nget gate learns to control when it is time to forget already226 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016remembered value, i.e. to reset the memory cell. When\ngates are closed, irrelevant information does not enter the\ncell and the state of the cell is not altered. The outputs of\nall memory blocks are fed back recurrently to all memory\nblocks to remember past values. Finally, adding bidirec-\ntional links and LSTM cells improves a neural network’s\nability to employ additional timing information. All of the\nabove contributes to the fact that the proposed BLSTM\nmodel is ﬂexible and effective in generating the missing\ncomponent in an incomplete multipart texture.\n2. METHOD\n2.1 Music Representation\nMIDI ﬁles are used as input in both training and testing\nphases in this project. Multiple input and output neurons\nare used to represent different pitches. At each time, the\nvalue of the neuron associated with the particular pitch\nplayed at that time is 1.0. The values of the rest of the\nneurons are 0.0. We avoid distributed encodings and other\ndimension reduction techniques and represent the data in\nthis simple form because this representation is common\nand assumes that neural networks can learn a more dis-\ntributed representation within hidden layers.\nThe music is split into time frames and the length of\neach frame depends on the type of music. Finding missing\nmusic component can then be formulated as a supervised\nclassiﬁcation problem. For a song of length t1, for every\ntimetfromt0tot1, given input x(t), the notes played at\ntimet, ﬁnd the output y(t), which is the missing compo-\nnent we try to predict. In other words, for two-part MA\ntextures, y(t)is the chord played at time t, while for four-\npart SATB textures, y(t)is the pitch of the missing part at\ntimet.\n2.2 Generating Accompaniment in Two-Part MA\nTexture\n2.2.1 Input and Output\nThe MIDI ﬁles are split into eighth note fractions. The\ninputs at time t,x(t), are the notes of the melody played\nat timet. Instead of representing the notes by their MIDI\nnumber, which spans the whole range of 88 notes on a key-\nboard, we used pitch-class representation to encode note\npitches into their corresponding pitch-class number. Pitch\nclass, also known as “chroma,” is the set of all pitches re-\ngardless of their octaves. That is, all C notes (C0, C1, ...\netc.) are all classiﬁed as pitch-class C. All notes are repre-\nsented with one of the 12 numbers corresponding to the 12\nsemitones in an octave. In addition to pitch-class informa-\ntion, two additional values are added as inputs: Note-Begin\nunit and Beat-On unit. In order to be able to tell when a\nnote ends, a Note-Begin unit is used to differentiate two\nconsecutive notes of the same pitch from one note that is\nheld for two time frames as was done by [30]. If the note in\nthe melody is beginning at the time, the value of the Note-\nBegin unit is 1.0; if the note is present but duplicates the\nprevious note or is not played at all, the value of the unit is0.0. The Beat-On unit, on the other hand, provides metric\ninformation to the network. If the time tis on a beat, the\nvalue of the Beat-On unit is 1.0, otherwise 0.0. If time t\nis a rest, the values of all input neurons are 0.0. The time\nsignature information is obtained via meta-data in MIDI\nﬁles.\nThe outputs at time t,y(t), is the chord played at time\nt. We limit chord selection to major, minor, diminished,\nsuspended, and augmented triads as in [27], resulting in\n52 chords in total1. The output units represent these 52\nchords in a manner similar to the input neurons: the value\nof the neuron corresponding to the chord played at that\ntime has a value of 1.0, and the values of the rest of the\nneurons are all 0.0.\n2.2.2 Training the Network\nThe input layer has 14 input neurons: 12 neurons for each\npitch in the pitch class, one neuron for note-begin and one\nfor beat-on unit. The network consists of two hidden lay-\ners for both forward and backward states, resulting in four\nhidden layers in total. In every hidden layer are 20 LSTM\nblocks with one memory cell. The output layer uses the\nsoftmax activation function and cross entropy error func-\ntion as in [15]. Softmax function is a standard function for\nmulti-class classiﬁcation that squashes a K-dimensional\nvectorxin the range of (0,1), which takes the form\nσ(x)j=exj\n/summationtextK\nk=1exk,forj= 1,...,K (1)\nThe softmax function ensures that all the output neurons\nsum to one at every time step, and thus can be regarded as\nthe probability of the output chord given the inputs at that\ntime.\nEach music piece is presented to the network one at a\ntime, frame-by-frame. The network is trained via standard\ngradient-descent Back-Prorogation. A split of data is used\nas the validation set for early-stopping in order to avoid\nover-ﬁtting of the training data. If there is no improvement\non the validation set for 30 epochs, training is ﬁnished and\nthe network setting with the lowest classiﬁcation error on\nthe validation set is used for testing.\n2.2.3 Markov Model as Post-Processing\nThe network trained in 2.2.2 can then be used to predict the\nchord associated with each melody note by choosing the\noutput neuron that has the highest activation at each time.\nHowever, the predicted chord at each time is independent\nof the chord predicted in the previous and succeeding time.\nWhile there are forward and backward links in the hidden\nlayers of the network, there is no recurrent connections\nfrom the ﬁnal neuron output to the network. The chord\nmight sound good with the melody, but the transition from\none chord to another might not make sense at all. In fact,\n1We represent the note of the chords with their pitches rather than\npitch names. Therefore, A augmented chord would have the same repre-\nsentation as F augmented: the former consists of A, C#, and E#, whose\npitches are the same as that of the component of the latter, F, A, and C#.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 227how one chord transits from and to the other typically fol-\nlows speciﬁc chord-progression rules depending on differ-\nent music styles. A bi-gram Markov Model is thus added\nto learn the probability of transitioning from each chord\nto possible successors independent of the melody, which\nwill be referred to as the transition matrix. The transition\nmatrix is smoothed using linear interpolation with a uni-\ngram model. The model also learns the statistics of the\nstart chords.\nInstead of selecting the output neuron with the highest\nactivations, the ﬁrst kneurons with the highest activations\nare chosen as candidates. Dynamic programming is then\nused to determine the optimal chord sequence among the\ncandidates using the previously-learned transition matrix.\n2.3 Generating the Missing Part in Four-Part SATB\nTextures\n2.3.1 Input and Output\nWithout loss of generality, we sample the melody at every\neighth note for similar reasons as explained by Prisco, et\nal. [7]. Notes that are shorter in length are considered as\npassing notes and are ignored here. The inputs at time t,\nx(t), are the pitches of the notes played at time t, span-\nning the whole range of 88 notes (A0, C8) on a keyboard,\nresulting a 88-dimensional vector. If a note iis played at\ntimet, the value of the neuron associated with the particu-\nlar pitch is 1.0, i.e. xi(t) = 1.0. The number of non-zero\nelements in x(t), which are the number notes played each\ntime, ranges from one to three, depending on the number\nof voices present.\nFor the task of predicting the missing voice in a four-\npart texture where the other three voices are present, the\ninput is polyphonic music. In this case, there are at\nmost three non-zero elements in xtfor every time t, i.e.\n∀t88/summationtext\ni=1xi(t)≤3. If the task is to predict one missing\nvoice given only one of the three other voices, there is at\nmost one non-zero element in x(t). The reason why we do\nnot represent the notes with their pitch-class proﬁle as we\ndid when handling two-part MA texture is that the network\ndepends on octave information to identify which voice the\nnotes belong to. The outputs at time t,y(t), is the pre-\ndicted missing note at time t, which falls in the pitch range\nof any of the four voices, depending on the task speciﬁed\nby our training data. Similarly, the value of the neuron as-\nsociated with the particular pitch played at the time tis 1.0,\notherwise 0.0.\n2.3.2 Training the Network\nThe network structure is the same as the one used in Sec-\ntion 2.2.2 except that the number of input neurons and out-\nput neurons are 88, and that we use 20 LSTM blocks for\nthe ﬁrst hidden layer and 50 LSTM blocks for the second\nhidden layer. Similar to what we did for two-part MA tex-\ntures, each music piece is presented to the network one at\na time, frame-by-frame. If the task is to generate one miss-\ning voice given any of the three other voices, then the three\npresent voices are given to the network individually as ifthey are independent melodies. In this case, each music\npiece is actually presented to the network three times and\neach time only one of the three voices is presented. This\nmethod gave the best results.\n2.3.3 Predict Missing Voice with the Trained Network\nThe trained network is ready to predict the missing voice\nby doing an 88-class classiﬁcation on the input voice. At\neach time frame, the neuron with the highest activations\nis selected, and the pitch it represents is considered as the\npitch of the missing voice.\n3. EVALUATION\n3.1 Generating Missing Accompaniment in Two-Part\nMA Texture\n3.1.1 Dataset\nThe system’s performance on two-part MA textures is eval-\nuated using the Nottingham Dataset [10] transcribed from\nABC format, which is also used in [4] for composing poly-\nphonic music. The dataset consists of 1024 double-track\nMIDI ﬁles, with melody on one track and accompaniment\non the other. The length of the pieces ranges from 10 sec-\nonds to 7.5 minutes, the median being 1 minute and 4 sec-\nonds. Those without accompaniment and those whose ac-\ncompaniment are more complicated than simple chord pro-\ngressions are discarded, resulting in 962 MIDI ﬁles com-\nprising more than 1000 minutes, in total. Songs not in the\nkey of C major nor A minor (874 of them) were trans-\nposed to C major/A minor after probabilistically deter-\nmining their original key using the Krumhansl-Schmuckler\nkey-ﬁnding algorithm.\nThe chords were annotated at every beat or at every\nquarter note. Seventh chords were reduced to triads, and\nrests were replaced with previous chords. 60% of the\ndataset is selected randomly as training data, 20% as val-\nidation data, and 20% as testing data. Training ﬁnishes\nwhen validation accuracy does not improve for 30 epochs.\nAll results for the training and testing sets were recorded\nat the time when the classiﬁcation error on the validation\nset is lowest.\n3.1.2 Effects of Including Metric Information in Input\nSince the network learns the input melody as a sequence in\ntime and has no access to information other than pitches,\nwe added Beat-On ﬂag to a frame when it is on a beat\naccording to the time signature meta-data in MIDI ﬁles\n(Group iii and iv). We also added Note-Begin (Group ii\nand iv) to differentiate two consecutive notes of the same\npitch from two distinctive notes, as mentioned in Sec-\ntion 2.2.1. All three groups were sampled every eighth\nnote, and the MIDI note range (50, 95) was used as the\ninput range. Table 3.1.2 shows the classiﬁcation accuracy\nof the three groups as well as the one where neither ﬂag is\nprovided as a reference. Two groups where Beat-On ﬂag\nis added, Group iii and iv, perform signiﬁcantly better than\nthe groups without the beat information (Group i), with a228 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Training Set Test Set\n(i) Pitch Information only 72.88% 68.54\n(ii) Note-Begin 72.11% 68.86\n(iii) Beat-On 75.82% 70.34\n(iv) Note-Begin and Beat-On 75.76% 70.61\nTable 1 . Classiﬁcation accuracy of the dataset when a\nNote-Begin ﬂag, Beat-On ﬂag, and both ﬂags are added\nto the inputs.\nTraining Set Test Set\n(i) 8th Note + Range 75.76% 70.65 %\n(ii) 8th Note + PC 73.13% 72.05 %\n(iii) 16th Note + Range 73.10% 69.50 %\n(iv) 16th Note + PC 74.02% 70.67 %\nTable 2 . Classiﬁcation accuracy of the dataset when us-\ning various representations of pitches at various sampling\nrates.\n95% conﬁdence interval of 0.84%, 0.80% and 0.79% indi-\nvidually. This is consistent with the fact that chords always\nchange on a beat or multiples of a beat. Therefore, such in-\nformation is crucial to the timing of chord changes in the\nnetwork. Note-Begin, on the other hand, does not seem to\nimprove the accuracy, which is due to the fact that whether\nthe note is held from the previous time or it is newly started\ndoes not affect chord choices.\n3.1.3 Choice of Data Representations\nTo see how different resolutions of the melody affects the\nchord prediction result, we evaluated the performance of\nthe system using different frame lengths. “8th Note” or\n“16th Note” indicates the melodies and accompaniments\nwere sampled every eighth note or sixteenth note. We\nrepresented the input to the network using only the ac-\ntual pitch range that melody notes are played in, which is\nMIDI note 50 (D3) to 95 (B6) (Groups i and iii, “Melody\nRange”), and using pitch class representation (Groups ii\nand iv, “Pitch Class”).\nSince the network learns the input melody as a se-\nquence in time and have no access to information other\nthan pitches, we added Beat-On ﬂags to a frame when\nit is on a beat according to the time signature meta-data\nin MIDI ﬁles. We also added Note-Begin ﬂags. Repre-\nsenting the melodies with their pitch-class number at ev-\nery 8th note (Group ii) could correctly predict the missing\nchords approximately 72% of the time when both Note-\nBegin and Beat-On information are available. With a 95%\nconﬁdence interval at 0.76%, it also signiﬁcantly outper-\nforms the other representation. Table 2 shows the result.\n3.1.4 Comparison with Other Approaches\nWe compared the architecture used in this paper with four\nother neural network architectures: Unidirectional LSTM,\nBidirectional recurrent neural network (BRNN), Unidirec-\ntional recurrent neural network (RNN), and Multi-layerNetwork Training Set Test Set Epochs\nBLSTM 75.76% 71.13 % 103\nLSTM 71.51% 67.57 % 130\nBRNN 68.77% 68.86 % 136\nRNN 68.33% 66.58 % 158\nMLP 55.16% 54.66 % 120\nTable 3 . Classiﬁcation accuracy of the dataset using dif-\nferent neural network architectures.\nperceptron network (MLP). Given the variety of differ-\nent datasets and accessibility to code, our comparison is\nbased on the BLSTM methods described above. Neurons\nin BRNN, RNN and MLP networks were sigmoid neurons.\nThe size of the hidden layers were selected so that the num-\nber of weights are approximately the same (around 32,000)\nfor all of the networks as in [15]\nTable 3 shows the classiﬁcation accuracy and the num-\nber of epochs required to converge. All groups were sam-\npled at every eighth note, and were provided with both met-\nric information, (Note On and Beat On), during training\nand testing. The 95% conﬁdence interval for BLSTM and\nLSTM are 0.80% and 0.76%. Using approximately same\nnumber of weights, BLSTM performs signiﬁcantly better\nthan other neural networks and also converges the fastest.\n3.2 Finding the Missing Part in Four-Part SATB\nTextures\n3.2.1 Dataset\nWe evaluated our approach using 378 of J. S. Bach’s four-\npart chorales acquired from [16]. MIDI ﬁles were all\nmulti-tracked, one voice on each track. The average length\nof the pieces is approximate 45 seconds, the maximum and\nminimum being 6 minutes and 17 seconds. Among all\nchorales, 102 pieces are in minor mode. All of the chorales\nwere transposed to C major/A minor using Krumhansl-\nSchmuckler key-ﬁnding algorithm. As in section 3.1, 60%\nof the ﬁles were used as training set, 20% as test set, and\n20% as validation set, resulting in 226, 76, 76 pieces re-\nspectively.\n3.2.2 Predicting Missing Voice Given the Other Three\nVoices\nTable 3.2.2 shows the frame-wise classiﬁcation accuracy\nof the predicted missing voices (Soprano, Alto, Tenor, or\nBass) when the three other voices are given on training and\ntest sets. The accuracy of predicting missing voices on the\noriginal non-transposed set is also listed for comparison.\nAll songs were sampled at every eighth note. From the ta-\nble, we can observe a few interesting phenomena. First,\ntransposing the songs remarkably improves prediction ac-\ncuracy in both training and test set. This is not surpris-\ning since transposing songs in advance reduces complex-\nity. The same pre-processing is also used by [3] [4] [27].\nSecond, we see that the network could correctly predict\nSoprano, Alto, and Tenor approximately 70% of the time\nwhen the songs were transposed. Speciﬁcally, Alto seemsProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 229Soprano Alto\nTraining Test Training Test\nNot Transposed 69.15% 46.82% 63.61% 47.61%\nTransposed 77.90% 71.52% 82.65% 73.90%\nTenor Bass\nTraining Test Training Test\nNot Transposed 47.25% 39.85% 45.40% 36.93%\nTransposed 78.47% 69.76% 70.09% 61.22%\nTable 4 . Classiﬁcation accuracy of the predicted missing\nvoices, either Soprano, Alto, Tenor, or Bass, when the three\nother voices are given on training and testing sets.\nSoprano Alto\nTraining Test Training Test\nBLSTM 84.88% 73.86 % 82.65% 73.90 %\nBRNN 90.25% 74.37% 85.37% 74.30 %\nLSTM 85.27% 70.39% 77.14% 70.45%\nRNN 81.90% 72.29% 80.31% 71.73%\nMLP 68.74% 66.54% 73.51% 70.03%\nTenor Bass\nTraining Test Training Test\nBLSTM 78.47% 69.76% 70.09% 61.22 %\nBRNN 80.95% 70.13% 74.58% 63.74%\nLSTM 73.84% 64.89% 65.86% 57.69%\nRNN 75.48% 67.20% 69.68% 59.69%\nMLP 68.85% 65.68% 58.58% 56.14%\nTable 5 . Classiﬁcation accuracy of the predicted missing\nvoices when three other voices are given using different\nnetwork architecture.\nto be the easiest to predict (in bold), while Bass is the most\ndifﬁcult.\n3.2.3 Comparison with Other Approaches\nSimilar to our approach in Section 3.1.4, the size of the hid-\nden layers were selected so that the number of weights are\napproximately the same (around 63,000) for all of the net-\nworks. Table 3.2.3 shows the classiﬁcation accuracy of the\nmissing voices (either Soprano, Alto, Tenor, or Bass) when\nall of the three other voices are present. From the result,\nwe can see that BLSTM does not have a statistically sig-\nniﬁcant performance from BRNN on Soprano, Alto, and\nTenor parts (in bold) and outperforms other neural-network\nbased methods on all parts. It also shows that including fu-\nture information by using bidirectional connection effec-\ntively improves accuracy by 3% on average no matter us-\ning LSTM cells (in BLSTM and LSTM) or logistic cells\n(in BRNN and RNN). Note that LSTM, while powerful,\nis really hard to train since it requires parameter tuning\nand a large dataset. We will need to conduct more experi-\nments in larger scale to explain what properties of LSTM\nand BRNN favor which tasks.\n4. CONCLUSION\nThis paper has presented an approach to predicting missing\nmusic components for complex multipart musical texturesusing Bidirectional Long-Short Term Memory (BLSTM)\nneural networks. We demonstrated the ﬂexibility and ro-\nbustness of the system by applying the method to two dis-\ntinctive but popular tasks in the computer-music ﬁeld: gen-\nerating chord accompaniment for given melodies in two-\npart MA textures and ﬁlling the missing voice in four-part\nSATB textures. The proposed approach is capable of han-\ndling music pieces of arbitrary length as well as various\nstyles. In addition, the network could be used to generate\nmissing music components of different forms, i.e. single\nnotes for four-part SATB textures or chords for two-part\nMA textures, by simply altering the number of input and\noutput neurons.\nTwo sets of experiments were conducted regarding the\ntwo tasks on two datasets of completely different styles,\nand issues that inﬂuence prediction accuracies were dis-\ncussed. For the task of predicting chord accompaniment in\ntwo-part MA texture, the experimental results showed that\nBLSTM network could correctly generate chords for given\nmelodies 72% of the time, which is signiﬁcantly higher\nthan 68%, the best accuracy achieved by using other neural\nnetwork based approaches. We also discovered that repre-\nsenting the melodies using their pitch class proﬁle yielded\nthe best result.\nAs for the problem of ﬁnding the missing voice in\nfour-part SATB texture, the experiment demonstrated that\nBLSTM network could correctly predict the missing voice\napproximately 70% of the time on average when three\nother voices are present. Putting the experimental results\non two datasets together, the fact that BLSM outperforms\nother neural-network based networks for two-part MA tex-\ntures and performs as well as BRNN for four-part SATB\ntextures showed that the BLSTM network is the optimal\nstructure for predicting missing components from multi-\npart musical textures.\n5. REFERENCES\n[1] Moray Allan and Christopher KI Williams. Harmon-\nising chorales by probabilistic inference. Advances\nin neural information processing systems , 17:25–32,\n2005.\n[2] Matthew I Bellgard and Chi-Ping Tsang. Harmonizing\nmusic the Boltzmann way. Connection Science , 6(2-\n3):281–297, 1994.\n[3] Kaan M Biyikoglu. A Markov model for chorale har-\nmonization. In Preceedings of the 5 th Triennial ES-\nCOM Conference , pages 81–84, 2003.\n[4] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and\nPascal Vincent. Modeling temporal dependencies in\nhigh-dimensional sequences: Application to poly-\nphonic music generation and transcription. arXiv\npreprint arXiv:1206.6392 , 2012.\n[5] Ching-Hua Chuan and Elaine Chew. A hybrid system\nfor automatic generation of style-speciﬁc accompani-\nment. In 4th Intl Joint Workshop on Computational\nCreativity , 2007.230 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016[6] Uraquitan Sidney Cunha and Geber Ramalho. An in-\ntelligent hybrid model for chord prediction. Organised\nSound , 4(02):115–119, 1999.\n[7] Roberto De Prisco, Antonio Eletto, Antonio Torre, and\nRocco Zaccagnino. A neural network for bass func-\ntional harmonization. In Applications of Evolutionary\nComputation , pages 351–360. Springer, 2010.\n[8] Kemal Ebcio ˘glu. An expert system for harmonizing\nfour-part chorales. Computer Music Journal , pages 43–\n51, 1988.\n[9] Johannes Feulner. Neural networks that learn and re-\nproduce various styles of harmonization. In Proceed-\nings of the International Computer Music Conference ,\npages 236–236. International Computer Music Associ-\nation, 1993.\n[10] Eric Foxley. Nottingham dataset. http://ifdo.ca/ sey-\nmour/nottingham/nottingham.html, 2011. Accessed:\n04-19-2015.\n[11] Alan Freitas and Frederico Guimaraes. Melody harmo-\nnization in evolutionary music using multiobjective ge-\nnetic algorithms. In Proceedings of the Sound and Mu-\nsic Computing Conference , 2011.\n[12] Dan Gang, D Lehman, and Naftali Wagner. Tuning a\nneural network for harmonizing melodies in real-time.\nInProceedings of the International Computer Music\nConference, Ann Arbor, Michigan , 1998.\n[13] Dan Gang and Daniel Lehmann. An artiﬁcial neural net\nfor harmonizing melodies. Proceedings of the Interna-\ntional Computer Music Association , 1995.\n[14] Dan Gang, Daniel Lehmann, and Naftali Wagner. Har-\nmonizing melodies in real-time: the connectionist ap-\nproach. In Proceedings of the International Computer\nMusic Association, Thessaloniki, Greece , 1997.\n[15] Alex Graves and J ¨urgen Schmidhuber. Framewise\nphoneme classiﬁcation with bidirectional LSTM and\nother neural network architectures. Neural Networks ,\n18(5):602–610, 2005.\n[16] Margaret Greentree. www.jsbchorales.net/index.shtml,\n1996. Accessed: 04-19-2015.\n[17] Mark A Hall. Selection of attributes for modeling Bach\nchorales by a genetic algorithm. In Artiﬁcial Neu-\nral Networks and Expert Systems, 1995. Proceedings.,\nSecond New Zealand International Two-Stream Con-\nference on , pages 182–185. IEEE, 1995.\n[18] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-\nterm memory. Neural computation , 9(8):1735–1780,\n1997.\n[19] Amy K Hoover, Paul A Szerlip, Marie E Norton,\nTrevor A Brindle, Zachary Merritt, and Kenneth OStanley. Generating a complete multipart musical com-\nposition from a single monophonic melody with func-\ntional scaffolding. In International Conference on\nComputational Creativity , page 111, 2012.\n[20] Amy K Hoover, Paul A Szerlip, and Kenneth O Stan-\nley. Generating musical accompaniment through func-\ntional scaffolding. In Proceedings of the Eighth Sound\nand Music Computing Conference (SMC 2011) , 2011.\n[21] Ryan A McIntyre. Bach in a box: The evolution\nof four part baroque harmony using the genetic al-\ngorithm. In Evolutionary Computation, 1994. IEEE\nWorld Congress on Computational Intelligence., Pro-\nceedings of the First IEEE Conference on , pages 852–\n857. IEEE, 1994.\n[22] Franc ¸ois Pachet and Pierre Roy. Mixing constraints and\nobjects: A case study in automatic harmonization. In\nProceedings of TOOLS Europe , volume 95, pages 119–\n126. Citeseer, 1995.\n[23] Jean-Franc ¸ois Paiement, Douglas Eck, and Samy Ben-\ngio. Probabilistic melodic harmonization. In Advances\nin Artiﬁcial Intelligence , pages 218–229. Springer,\n2006.\n[24] Stanisław A Raczy ´nski, Satoru Fukayama, and Em-\nmanuel Vincent. Melody harmonization with interpo-\nlated probabilistic models. Journal of New Music Re-\nsearch , 42(3):223–235, 2013.\n[25] Rafael Ramirez and Julio Peralta. A constraint-based\nmelody harmonizer. In Proceedings of the Workshop on\nConstraints for Artistic Applications (ECAI’98) , 1998.\n[26] AJ Robinson and Frank Fallside. The utility driven dy-\nnamic error propagation network . University of Cam-\nbridge Department of Engineering, 1987.\n[27] Ian Simon, Dan Morris, and Sumit Basu. Mysong: au-\ntomatic accompaniment generation for vocal melodies.\nInProceedings of the SIGCHI Conference on Human\nFactors in Computing Systems , pages 725–734. ACM,\n2008.\n[28] Luc Steels. Learning the craft of musical composition .\nAnn Arbor, MI: MPublishing, University of Michigan\nLibrary, 1986.\n[29] Syunpei Suzuki, Tetsuro Kitahara, and Nihon Uni-\nvercity. Four-part harmonization using probabilistic\nmodels: Comparison of models with and without chord\nnodes. Stockholm, Sweden , pages 628–633, 2013.\n[30] Peter M Todd. A connectionist approach to algorithmic\ncomposition. Computer Music Journal , pages 27–43,\n1989.\n[31] Paul J Werbos. Backpropagation through time: what\nit does and how to do it. Proceedings of the IEEE ,\n78(10):1550–1560, 1990.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 231"
    },
    {
        "title": "Towards Modeling and Decomposing Loop-Based Electronic Music.",
        "author": [
            "Patricio López-Serrano",
            "Christian Dittmar",
            "Jonathan Driedger",
            "Meinard Müller"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417999",
        "url": "https://doi.org/10.5281/zenodo.1417999",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/065_Paper.pdf",
        "abstract": "Electronic Music (EM) is a popular family of genres which has increasingly received attention as a research subject in the field of MIR. A fundamental structural unit in EM are loops—audio fragments whose length can span several seconds. The devices commonly used to produce EM, such as sequencers and digital audio workstations, impose a mu- sical structure in which loops are repeatedly triggered and overlaid. This particular structure allows new perspectives on well-known MIR tasks. In this paper we first review a prototypical production technique for EM from which we derive a simplified model. We then use our model to illus- trate approaches for the following task: given a set of loops that were used to produce a track, decompose the track by finding the points in time at which each loop was activated. To this end, we repurpose established MIR techniques such as fingerprinting and non-negative matrix factor deconvo- lution.",
        "zenodo_id": 1417999,
        "dblp_key": "conf/ismir/Lopez-SerranoDD16",
        "content": "TOWARDS MODELING AND DECOMPOSING LOOP-BASED\nELECTRONIC MUSIC\nPatricio L ´opez-Serrano Christian Dittmar Jonathan Driedger Meinard M ¨uller\nInternational Audio Laboratories Erlangen, Germany\npatricio.lopez.serrano@audiolabs-erlangen.de\nABSTRACT\nElectronic Music (EM) is a popular family of genres which\nhas increasingly received attention as a research subject\nin the ﬁeld of MIR. A fundamental structural unit in EM\nare loops—audio fragments whose length can span several\nseconds. The devices commonly used to produce EM, such\nas sequencers and digital audio workstations, impose a mu-\nsical structure in which loops are repeatedly triggered and\noverlaid. This particular structure allows new perspectives\non well-known MIR tasks. In this paper we ﬁrst review a\nprototypical production technique for EM from which we\nderive a simpliﬁed model. We then use our model to illus-\ntrate approaches for the following task: given a set of loops\nthat were used to produce a track, decompose the track by\nﬁnding the points in time at which each loop was activated.\nTo this end, we repurpose established MIR techniques such\nas ﬁngerprinting and non-negative matrix factor deconvo-\nlution.\n1. INTRODUCTION\nWith the advent of affordable electronic music production\ntechnology, various loop-based genres emerged: techno,\nhouse, drum’n’bass and some forms of hip hop; this family\nof genres is subsumed under the umbrella term Electronic\nMusic (EM). EM has garnered mainstream attention within\nthe past two decades and has recently become a popular\nsubject in MIR: standard tasks have been applied to EM\n(structure analysis [17]); new tasks have been developed\n(breakbeat analysis and resequencing [7, 8]); and special-\nized datasets have been compiled [9].\nA central characteristic of EM that has not been exten-\nsively considered is its sequencer-centric composition. As\nnoted by Collins [4], loops are an essential element of EM:\nloops are short audio fragments that are “generally associ-\nated with a single instrumental sound” [3]. Figure 1 illus-\ntrates a simpliﬁed EM track structure similar to that en-\ncouraged by digital audio workstations (DAWs) such as\nAbleton Live [1]. The track starts with the activation of\n© Patricio L ´opez-Serrano, Christian Dittmar, Jonathan\nDriedger, Meinard M ¨uller. Licensed under a Creative Commons Attri-\nbution 4.0 International License (CC BY 4.0). Attribution: Patricio\nL´opez-Serrano, Christian Dittmar, Jonathan Driedger, Meinard M ¨uller.\n“Towards Modeling and Decomposing Loop-Based Electronic Music”,\n17th International Society for Music Information Retrieval Conference,\n2016.\nFigure 1 . A condensed EM track built with three loop\nlayers: drums (D), melody (M) and bass (B). Each block\ndenotes the activation of the corresponding pattern during\nthe time it spans.\na drum loop (blue, bottom row). After one cycle, a melody\nloop (yellow, middle row) is added, while the drum loop\ncontinues to play. A third layer—the bass (red, top row)—\nis activated in the third cycle. Over the course of the track,\nthese loops are activated and deactivated. An important\nobservation is that all appearances of a loop are identical;\na property that can be modeled and exploited in MIR tasks.\nIn particular, we consider the task of decomposing an EM\ntrack: given the set of loops that were used to produce a\ntrack and the ﬁnal, downmixed version of the track itself,\nwe wish to retrieve the set of timepoints at which each loop\nwas activated.\nThis work offers three main contributions. First, we re-\nview the production process of EM and how it leads to the\nprototypical structure outlined previously (Section 2). Sec-\nond, we propose a simpliﬁed formal model that captures\nthese structural characteristics (Section 3). Third, we use\nour model to approach the EM decomposition task from\ntwo angles: ﬁrst, we interpret it within a standard retrieval\nscenario by using ﬁngerprinting and diagonal matching\n(Section 4). Our second approach is based on non-negative\nmatrix factor deconvolution (NMFD), a technique com-\nmonly used for audio source separation (Section 5). We\nsummarize our ﬁndings and discuss open issues in Sec-\ntion 6.\n2. STRUCTURE AND PRODUCTION PROCESS\nUnlike other genres, EM is often produced by starting with\na single distinct musical pattern [19] (also called loop)\nand then adding and subtracting further musical material\nto shape the tension and listener’s expectation. An EM\ntrack is built by combining layers (with potentially differ-502ent lengths) in looping cyclical time—where the overall\nform corresponds to the multitrack layout of sequencers\nand digital audio workstations (DAWs) [4]. Figure 1 pro-\nvides a simple example of such a track (total duration 56 s),\nconsisting of three layers or loops: drums (D), bass (B) and\nmelody (M), each with a duration of 8 s. We will be using\nthis track as a running example to clarify the points made\nthroughout this paper.\nA common characteristic of EM tracks is their relative\nsparseness or low timbral complexity during the intro and\noutro—in other words, a single loop is active. This prac-\ntice is rooted in two facts: Firstly, EM tracks are conceived\nnot as isolated units, but rather as part of a seamless mix\n(performed by a DJ), where two or more tracks are over-\nlaid together. Thus, in what could be termed DJ-friendly\ntracks [4], a single, clearly percussive element at the be-\nginning and end facilitates the task of beat matching [3]\nand helps avoid unpleasantly dense transitions. We have\nconstructed our running example following this principle:\nin Figure 1, the only active layer during the intro and outro\nis the drum loop (bottom row, blue).\nThe second reason for having a single-layer intro is that\nthis section presents the track’s main elements, making the\nlistener aware of the sounds [3]. Once the listener has be-\ncome familiar with the main musical idea expressed in the\nintro, more layers are progressively brought in to increase\nthe tension (also known as a buildup ), culminating in what\nButler [3] designates as the track’s core: the “thicker mid-\ndle sections” where all loop layers are simultaneously ac-\ntive. This is reﬂected in Figure 1, where the melody is\nbrought in at 8 s and the bass at 16 s, overlapping with un-\ninterrupted drums. After the core has been reached, the\nmajority of layers are muted or removed—once again, to\ncreate musical anticipation—in a section usually known\nasbreak orbreakdown (see the region between 24–32 s in\nFigure 1, where only the melody is active). To release the\nmusical tension, previous loops are reintroduced after the\nbreakdown , (seconds 32–40, Figure 1) only to be gradually\nremoved again, arriving at the outro. In the following sec-\ntions we will develop a model that captures these structural\ncharacteristics and provides a foundation for analyzing EM\ntracks.\n3. SIMPLIFIED MODEL FOR EM\nIn Section 2 we illustrated the typical form of loop-based\nelectronic music. With this in mind, our goal is to analyze\nan EM track’s structure. More speciﬁcally, our method\ntakes as input the set of loops or patterns that were used\nto produce a track, as well as the ﬁnal, downmixed version\nof the track itself. From these, we wish to retrieve the set\nof timepoints at which each loop was activated within the\ntrack. We begin by formalizing the necessary input ele-\nments.\nLetV∈RK×Mbe the feature representation of an\nEM track, where K∈Nis the feature dimensional-\nity and M∈Nrepresents the number of elements or\nframes along the time axis. We assume that the track\nwas constructed from a set of Rpatterns Pr∈RK×Tr,\nFigure 2 .(Left) : Tensor Pwith three patterns (drums,\nbass, melody). (Right) : Activation matrix Hwith three\nrows; the colored cells denote an activation of the corre-\nsponding pattern.\nr∈[0 :R−1] :={0, . . . , R−1}. The parameter Tr∈N\nis the number of feature frames or observations for pattern\nPr. In practice, the patterns can have different lengths—\nhowever, without loss of generality, we deﬁne their lengths\nto be the same T:=T0=. . .=TR−1, which could\nbe achieved by adequately zero-padding shorter patterns\nuntil they reach the length of the longest. Based on this as-\nsumption, the patterns can be grouped into a pattern tensor\nP∈RK×R×T. In the case of our running example, seen\nin Figure 1, Tˆ = 8s and the number of patterns is R= 3.\nConsequently, the subdimension of the tensor which refers\nto a speciﬁc pattern with index risPr:=P(·, r,·)(i. e.,\nthe feature matrix for either (D), (M), or (B) in our ex-\nample); whereas Pt:=P(·,·, t)refers to frame index t\nsimultaneously in all patterns.\nIn order to construct the feature representation Vfrom\nthe pattern tensor P, we require an activation matrix H∈\nBR×MwithB:={0,1}, such that\nVˆ =T−1/summationdisplay\nt=0Pt·t→\nH , (1)\nwheret→\n(·)denotes a frame shift operator [18]. Figure 2 de-\npictsPandHas constructed for our running example. The\nmodel assumes that the sum of pattern signals and their\nrespective transformations to a feature representation are\nlinear, which may not always be the case. The additive\nassumption of Eq. 1 implies that no time-varying and/or\nnon-linear effects were added to the mixture (such as com-\npression, distiortion, or ﬁltering), which are often present\nin real-world EM. Aside from this, we specify a number of\nfurther constraints below.\nThe devices used to produce early EM imposed a se-\nries of technical constraints which we formalize here. Al-\nthough many of these constraints were subsequently elim-\ninated in more modern equipment and DAWs, they have\nbeen ingrained into the music’s aesthetic and remain in use\nup to the present day.\nNon-overlap constraint : A pattern is never superim-\nposed with itself, i. e., the distance between two activations\nof any given pattern is always equal to or greater than the\npattern’s length. Patterns are loaded into a device’s mem-Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 503(a) \n(b) Log-frequency  Cosine  similarity  1 \n \n \n \n \n0 \n1 \n \n \n \n \n0 \nTime (s)  Figure 3 .(a): Log-frequency spectrogram for entire track.\n(b): Matching curves computed using cosine similarity for\ndrums, melody, and bass (bottom to top). The dashed line\nrepresents the curve’s global mean; the dash-dotted line\nis the GT mean (see Section 4.4 for deﬁnition of gain).\nColored triangles indicate GT loop activation positions.\nory and triggered by a sequencer—usually without further\nactivation signals before it has played through to the end.\nIf a pattern Pris activated at time m∈[0 :M−1], then\nHr(m)/negationslash= 0⇒Hr(m+ 1) = . . .=Hr(m+T−1) = 0 .\nLength constraint : As noted by [4], multiple layers in EM\nare complementary, creating aggregate effects and capable\nof being independently inserted and removed. For this rea-\nson, we make the simplifying assumption that T:=T0=\nT1=. . .=TR−1, i. e., that all patterns have the same\nlength.\nT-grid constraint : Motivated by the use of centralized\nMIDI clocks and the ﬁxed amount of musical time avail-\nable on prevalent devices such as drum machines (which\ntypically allow programming one musical measure at a\ntime, in 16 steps), we enforce a timing grid which restricts\nthe possible activation points in H. In Figure 1, patterns\nare always introduced and removed at multiples of 8 s.\nAmplitude constraint : We assume that a pattern is always\nactivated with the same intensity throughout a track, and\ntherefore each row rin the activation matrix Hfulﬁlls\nHr:=H(r,·)∈B1×M.\n4. FINGERPRINT-BASED EM DECOMPOSITION\nIn the running example, multiple patterns are overlaid in\ndifferent conﬁgurations to form the track. If we know a\npriori which patterns are included and wish to ﬁnd their\nrespective activation positions, we need a technique capa-\nble of identifying an audio query within a database where\nfurther musical material is superimposed. We ﬁrst exam-\nTime (s )(a) \n(b) Log -f requency Ja cca rdindex \nFigure 4 .(a): Log-frequency spectral peak map for the\nentire track (black dots) and for each query (red dots en-\nclosed in red, from left to right: drums, melody, and bass).\n(b): Matching curves computed with the Jaccard index and\neach pattern as a query for drums, melody, and bass (bot-\ntom to top).\nine log-frequency spectrograms and diagonal matching as\na baseline approach, and continue with audio ﬁngerprint-\ning techniques based on spectral peaks in combination with\nvarious similarity measures. In Section 5 we discuss an al-\nternative approach based on NMFD. The running example\nis constructed with one audio ﬁle for each pattern and a\ngeneric EM track arrangement seen in Figure 1. The com-\nplete track is generated in the time domain by summing\nthe individual patterns that are active at a given point in\ntime. All audio ﬁles have been downmixed to mono with a\nsampling rate Fs= 22050 Hz.\n4.1 Diagonal Matching\nWe implement the diagonal matching procedure outlined\nin [13, pp. 376–378] to measure the similarity between\neach query pattern Prand the track feature matrix V.\nIn simple terms, to test if and where the query Pr=\n(Pr\n0, . . . , Pr\nT−1)is contained in V= (V0, . . . , V M−1), we\nshift the sequence Prover the sequence Vand locally\ncompare Prwith suitable subsequences of V. In general,\nletFbe the feature space (for example, F=RKin the\ncase of log-frequency spectrograms). A similarity mea-\nsures:F×F→ R∩[0,1]between two feature frames\nwill yield a value of 1if the query is identical to a certain\nregion of the database, and 0if there is no resemblance at\nall.504 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20164.2 Baseline Procedure\nFor each individual pattern, as well as the entire track, we\ncompute an STFTXwith the following parameters: block\nsizeN= 4096 , hop size H=N/2and a Hann win-\ndow. From these, magnitude spectrograms (abbreviated as\nMS) are computed and mapped to a logarithmically spaced\nfrequency axis with a lower cutoff frequency of 32 Hz, an\nupper cutoff frequency of 8000 Hz and spectral selectiv-\nity of 36 bins per octave (abbreviated LS). Under these\nSTFT settings, the 36-bin spectral selectivity does not hold\nin the two lowest octaves; however, their spectral peak\ncontribution is negligible. Preliminary experiments have\nshown that this musically meaningful feature representa-\ntion is beneﬁcial for the matching procedure both in terms\nof efﬁciency and accuracy. We begin with a baseline ex-\nperiment (Figure 3), using LS and cosine similarity:\nCosine :scos(u, v) := 1−/angbracketleftu|v/angbracketright\n||u||·||v||, u, v∈RK\\{0}.\n(2)\nNotice that the clearest peak is produced by the melody ac-\ntivation at 24 s (Figure 3b, middle row), which occurs with-\nout any other patterns being overlaid. The three remain-\ning activation points for the melody have a very low gain\nrelative to their neighboring values. The matching curve\nfor the drums (Figure 3b, bottom row) displays a coarse\ndownwards trend starting at 0 s and reaching a global min-\nimum at 24 s (the point at which the drum pattern is not ac-\ntivated in our example); this trend is reversed as the drums\nare added again at 32 s. The internal repetitivity (or self-\nsimilarity) of the drum pattern causes the periodic peaks\nseen throughout the matching curve. Overall, it is evi-\ndent from all three curves that the combination of LS with\ncosine similarity is insufﬁcient to capture the activations\nwhen multiple patterns are superimposed—motivating our\nnext experimental conﬁguration which uses spectral peak\nmaps.\n4.3 Fingerprinting with Peak Maps\nAlthough our scenario is slightly different to that of audio\nﬁngerprinting and identiﬁcation, both require a feature rep-\nresentation which captures an individual pattern’s charac-\nteristics despite the superposition of further sound sources.\nTo this end, we use spectral peak maps as described in [13].\nConceptually, we are following an early approach for loop\nretrieval inside hip hop recordings which was presented\nin [20] and later reﬁned in [2]. Their method is based on a\nmodiﬁcation of the ﬁngerprinting procedure originally de-\nscribed in [21].\nFor each time-frequency bin in the respective LS, a rect-\nangular analysis window is constructed. The maximum\nvalue within each window is kept (with the value 1 on the\noutput) and all neighbors are set to 0 on the output. In\nFigure 4a we show the spectral peak map for the entire\ntrack (black dots) and the query peak map for each query\npattern (red dots in red rectangles). These log-frequency\npeak maps populate a pattern tensor P∈BK×R×T, where\nK= 286 . Thus Prcorresponds to the peak map for theGain Pearson\nµ σ µ σ\nMS/cos 1.72 0.31 0.13 0.05\nLS/cos 1.57 0.29 0.11 0.05\nPLS/cos 19.46 10.45 0.52 0.18\nPLS/inc 21.69 11.90 0.51 0.19\nPLS/Jac 19.54 9.76 0.53 0.18\nTable 1 . Results for diagonal matching experiments\nwith magnitude spectrograms (MS), log-frequency spec-\ntrograms (LS), and log-frequency peak maps (PLS) us-\ning the cosine, inclusion and Jaccard similarity measures.\nEach column shows the mean and variance for peak gain\nand Pearson correlation.\nrthpattern, while Vcorresponds to the entire track.\nIn addition to the cosine measure deﬁned in Eq. 2, we\ntest different similarity measures s:\nJaccard :sJac(u, v) := 1−||u∧v||\n||u∨v||, u, v∈BK,(3)\nInclusion :sinc(u, v) := 1−||u∧v||\n||u||, u, v∈BK,(4)\nwhere we set0\n0:= 1. The inclusion metric aims to quantify\nthe extent to which the query is contained or included in the\ndatabase and has a similar deﬁnition to the Jaccard index.\n4.4 Evaluation\nWe use two measures to quantify how well the matching\ncurves capture the pattern activations. For the ﬁrst mea-\nsure, termed gain, we compute the average of the activa-\ntion values at the ground truth (GT) activation points: in\nFigures 3b and 4b, these locations are marked by colored\ntriangles, corresponding to each loop in the running exam-\nple; their mean value is shown as a dash-dotted line. We\nalso compute the mean value for the entire curve (dashed\nline) and use the ratio between these two means in order to\nassess the quality of the matching curve. Ideally, the curve\nassumes large values at the GT activation points and small\nvalues elsewhere, resulting in a larger gain. As a second\nmeasure we take the Pearson correlation between a com-\nputed matching curve and its corresponding row Hrin the\nGT activation matrix, where the activation points have a\nvalue of 1, and 0 elsewhere. Again, a high Pearson corre-\nlation reﬂects high matching curve quality.\nWe generated a set of patterns used to build proto-\ntypical EM tracks. To foster reproducible research, we\nproduced them ourselves, avoiding potential copyright\nissues—they are available under a Creative Commons\nAttribution-ShareAlike 4.0 International license and can\nbe obtained at the companion website1. We chose seven\nprominent EM subgenres such as big beat ,garage and\ndrum’n’bass (in a tempo range between 120–160 BPM).\nFor each subgenre, we generated four patterns in the cate-\ngories of drums, bass, melody and additional effects.\n1https://www.audiolabs-erlangen.de/resources/\nMIR/2016-ISMIR-EMLoopProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 505Time (s)  NMFD activation  gain Figure 5 . Activation curves learned by NMFD applied to\nthe magnitude spectrogram of the running example. The\ndashed lines represent the mean value for the complete\ncurve, but are very close to the x-axis.\nAs stated by Wang [21], spectral peak maps are robust\nto superposition of multiple sources; a fact which becomes\nclear when comparing Figures 3b and 4b. In Figure 4b,\nthepeak gain has greatly increased compared to the base-\nline approach with LS. In Table 1 we list the mean peak\ngain and Pearson correlation for all seven tracks, along\nwith standard deviations for each value. The ﬁrst two rows,\nMS/cos and LS/cos, correspond to the baseline approach—\nthe last three rows summarize the experiments with spec-\ntral peak maps. Note that spectral peak maps have approx-\nimately ten times the peak gain of MS/LS, whereas the\nPearson correlation increases by a factor of four. Figures 3\nand 4 illustrate the results in Table 1 at an intuitive level.\nWith MS, the spectral content shared among different types\nof patterns impedes distinct peaks from emerging. By dis-\ncarding this irrelevant information, LS better represent the\ncharacteristics of each pattern. From the perspective of\npeak quality, only the self-similarity of the drum pattern\ncontinues to pose a challenge.\n5. NMFD-BASED EM DECOMPOSITION\nBy design, our model for EM is very close to the formula-\ntion of NMFD; in this section we explore the performance\nof NMFD and compare it with our ﬁngerprinting methods.\n5.1 Related Work\nIn this section, we brieﬂy review the NMFD method that\nwe employ for decomposing the feature representation V.\nWeiss and Bello [22] used non-negative matrix factoriza-\ntion (NMF) to identify repeating patterns in music. By\nadding sparsity constraints and shift-invariant probabilis-\ntic latent component analysis (SI-PLCA), they automati-\ncally identify the number of patterns and their lengths—\napplied to beat-synchronous chromagrams in popular mu-\nsic. Masuda et al. [12] propose a query-by-audio system\nbased on NMF to identify the locations where a query mu-sical phrase is present in a musical piece. Among more\ngeneral techniques for investigating alleged music plagia-\nrism, Dittmar et al. [5] proposed a method for retrieval of\nsampling. Their approach, based on NMF, was not sup-\nplemented with systematic evaluation, but was further in-\nvestigated in [23]. Previous works [6, 11, 16, 18] success-\nfully applied NMFD—a convolutive version of NMF—for\ndrum transcription and separation. Hockman et al. [7, 8]\nspeciﬁcally focused on analyzing breakbeats, i. e., drum-\nonly loops as used in hip hop and drum’n’bass . Detecting\nsample occurrences throughout a track is a secondary as-\npect, as they address the more challenging scenario of esti-\nmating the loop resequencing [8]. All these previous works\nhave in common that they attempt to retrieve one loop in-\nside a song, whereas we pursue a more holistic approach\nthat allows to deconstruct the whole track into loops.\n5.2 NMFD Model\nOur objective is to decompose Vinto component mag-\nnitude spectrograms that correspond to the distinct musi-\ncal elements. Conventional NMF can be used to com-\npute a factorization V≈W·H, where the columns of\nW∈RK×R\n≥0represent spectral basis functions (also called\ntemplates) and the rows of H∈RR×M\n≥0 contain time-\nvarying gains (also called activations). The rank R∈Nof\nthe approximation (i. e., number of components) is an im-\nportant but generally unknown parameter. NMFD extends\nNMF to the convolutive case by using two-dimensional\ntemplates so that each of the Rspectral bases can be in-\nterpreted as a magnitude spectrogram snippet consisting of\nT/lessmuchMspectral frames. The convolutive spectrogram\napproximation V≈Λis modeled as\nΛ :=T−1/summationdisplay\nt=0Wt·t→\nH, (5)\nwheret→\n(·)denotes a frame shift operator (see also Eq. 1).\nAs before, each column in Wt∈RK×R\n≥0represents the\nspectral basis of a particular component, but this time\nwe have Tdifferent versions Wt, with t∈[0 :T−1]\navailable. If we take lateral slices along the columns of\nWt, we can obtain Rprototype magnitude spectrograms\nUr∈RK×T\n≥0. NMFD typically starts with a suitable ini-\ntialization (with random values or constant values) of ma-\ntrices W(0)\ntandH(0). These matrices are iteratively up-\ndated to minimize a suitable distance measure between the\nconvolutive approximation ΛandV. In this work, we use\nthe update rules detailed in [18], which extend the well-\nknown update rules for minimizing the Kullback-Leibler\nDivergence (KLD) [10] to the convolutive case.\n5.3 Evaluation\nFor our experiments with NMFD we used MS and LS to\nconduct the procedure in two variants. For the ﬁrst variant\n(referred to as R in Table 2), the only a priori informa-\ntion used is the number of patterns (or templates) Rand\ntheir length T. The templates are initialized randomly and506 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Gain Pearson\nµ σ µ σ\nNMFD@MS, R 55.20 29.80 0.65 0.25\nNMFD@MS, RP 89.62 39.67 0.88 0.11\nNMFD@LS, R 52.39 35.95 0.64 0.22\nNMFD@LS, RP 79.59 34.99 0.87 0.12\nTable 2 . Results for NMFD with magnitude spectrograms\n(MS) and log-frequency spectrograms (LS), initializing the\nnumber of templates (R) and also the loop templates (RP).\nEach column shows the mean and variance for peak gain\nand Pearson correlation.\nﬁfty iterative updates are used to minimize the KLD. To\naccount for the effects of random initialization, we carry\nout ten initialization passes per track. The results in Ta-\nble 2 reﬂect the mean and standard deviation across all\npasses. For the second variant (RP), we supply the pat-\ntern templates themselves at initialization (i. e., R,Tand\nPare known). We also disallow template updates and only\nallow activation updates. Since the templates in variant\nR are initialized randomly, there is no direct relationship\nbetween the learned activation curves and the correspond-\ning ground truth curves. We deal with this permutation in-\ndeterminacy by comparing all computed activation curves\nwith all ground truth curves and taking the results which\nmaximize the overall score. For all conﬁgurations in Ta-\nble 2, we observe a peak gain at least twice as high as\nthat obtained through diagonal matching; the Pearson cor-\nrelation increases by a factor of 1.2–1.7, depending on the\nNMFD conﬁguration taken for comparison. Focusing on\nthe differences among NMFD conﬁgurations, RP brings\npeak gain improvements by a factor slightly greater than\n1.5; the Pearson correlation increases by about 1.35. The\nfeature choice (MS or LS) does not play a signiﬁcant role\nin result quality. Due to the amount of prior knowledge\nused to initialize the RP conﬁguration, we consider it as an\nupper bound for less informed approaches.\n6. CONCLUSIONS AND FUTURE WORK\nIn the preceding sections we developed a better under-\nstanding of the feature representations and matching tech-\nniques that are commonly used for pattern activation dis-\ncovery. In this section, we reﬂect on some of the limita-\ntions of our work, further research topics, and computa-\ntional performance issues.\nClearly, our work only provides a baseline for further\nwork towards more realistic scenarios. As to our model’s\ninherent shortcomings, real-world EM tracks usually con-\ntain more than four individual patterns, which are rarely\navailable. Moreover, activations of a given pattern are of-\nten (spectrally) different from one another due to the use\nof effects such as delay and reverb, ﬁlter sweeps or re-\nsequencing. Thus, we consider this study as a stepping\nstone towards a fully-developed pipeline for EM structure\nanalysis and decomposition. One potential research direc-\ntion would be the automatic identiﬁcation of suitable pat-\ntern candidates. A repetition-based analysis technique asMethod Time (s)\nPLS 0.2\nNMFD@LS,(R/RP) 2.5\nNMFD@MS,(R/RP) 36.0\nTable 3 . Computation times for diagonal matching with\nlog-spectral peak maps (PLS), NMFD with magnitude\nspectrograms (MS), and NMFD with log-frequency spec-\ntrograms (LS). The choice of initialization R or RP for\nNMFD does not impact execution time.\ndescribed in [14] could be used in conjunction with spec-\ntral peak maps to compute self-similarity matrices (SSMs)\nthat saliently encode inclusion relationships. Furthermore,\nsemi-informed variants of NMFD might be helpful in dis-\ncovering additional patterns that are not explicitly given,\nwhere the use of rhythmic structure can serve as prior\nknowledge to initialize the activations. Although diago-\nnal matching curves can be computed efﬁciently with a\nstraightforward implementation, we have seen they have\ncertain shortcomings; we wish to investigate the feasi-\nbility of using them as rough initial guesses and leaving\nthe reﬁnement up to NMFD. Beyond each method’s ca-\npabilities, as seen in Tables 1 and 2, there is also the is-\nsue of their running time and memory requirements. For\nthe running example, we tested our MATLAB implementa-\ntion on a 3.2 GHz Intel Core i5 CPU with 16 GB RAM,\nyielding the mean execution times in Table 3. From Ta-\nbles 3 and 2 we can conclude that NMFD@LS offers\nthe best balance between quality and resource intensity.\nNMFD@MS takes approximately 14 times longer to com-\npute than NMFD@LS and only produces marginally bet-\nter results. Indeed, recall that the feature dimensionality\nK= 286 for LS and K= 2049 for MS, which explains\nthe large difference in execution times.\nAs a ﬁnal remark, musical structure analysis is an ill-\ndeﬁned problem, primarily because of ambiguity; a seg-\nmentation may be based on different principles (homo-\ngeneity, repetition, novelty) that can conﬂict with each\nother [15]. The main advantage of our method is that we\navoid the philosophical issue of how a track’s structure is\nperceived , and rather attempt to determine how it was pro-\nduced —a univocal problem. It can then be argued that the\nlisteners’ perception is inﬂuenced by the cues inherent to\nEM’s compositional style.\n7. ACKNOWLEDGMENTS\nPatricio L ´opez-Serrano is supported in part by CONACYT-\nDAAD. The authors are supported by the German Re-\nsearch Foundation (DFG MU 2686/6-1, DFG MU 2686/7-\n1). The International Audio Laboratories Erlangen are\na joint institution of the Friedrich-Alexander-Universit ¨at\nErlangen-N ¨urnberg (FAU) and Fraunhofer Institute for In-\ntegrated Circuits IIS. This work’s core ideas were born at\nHAMR@ISMIR 2015: we thank our teammate Hendrik\nSchreiber, Colin Raffel, the organizers, and the reviewers\nfor their valuable input.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 5078. REFERENCES\n[1] Ableton. Live. https://www.ableton.com/\nen/live/ (web source, retrieved March 2016),\n2016.\n[2] Jan Balen, Joan Serr `a, and Mart ´ın Haro. From Sounds\nto Music and Emotions: 9th Int. Symposium, CMMR\n2012, London, UK, June 19-22, 2012, Revised Se-\nlected Papers , chapter Sample Identiﬁcation in Hip\nHop Music, pages 301–312. Springer Berlin Heidel-\nberg, Berlin, Heidelberg, 2013.\n[3] Mark J. Butler. Unlocking the Groove: Rhythm, Meter,\nand Musical Design in Electronic Dance Music . Pro-\nﬁles in popular music. Indiana University Press, 2006.\n[4] Nick Collins, Margaret Schedel, and Scott Wilson.\nElectronic Music . Cambridge Introductions to Music.\nCambridge University Press, Cambridge, United King-\ndom, 2013.\n[5] Christian Dittmar, Kay Hildebrand, Daniel G ¨artner,\nManuel Winges, Florian M ¨uller, and Patrick Aichroth.\nAudio forensics meets music information retrieval - a\ntoolbox for inspection of music plagiarism. In Euro-\npean Sig. Proc. Conf. (EUSIPCO) , pages 1249–1253,\nBucharest, Romania, August 2012.\n[6] Christian Dittmar and Meinard M ¨uller. Towards tran-\nsient restoration in score-informed audio decomposi-\ntion. In Proc. of the Int. Conf. on Digital Audio Effects\n(DAFx) , pages 145–152, Trondheim, Norway, Decem-\nber 2015.\n[7] Jason A. Hockman, Matthew E. P. Davies, and Ichiro\nFujinaga. One in the Jungle: Downbeat Detection in\nHardcore, Jungle, and Drum and Bass. In Proc. of the\nInt. Soc. for Music Inf. Retrieval Conf. (ISMIR) , pages\n169–174, Porto, Portugal, October 2012.\n[8] Jason A. Hockman, Matthew E. P. Davies, and Ichiro\nFujinaga. Computational strategies for breakbeat clas-\nsiﬁcation and resequencing in Hardcore, Jungle and\nDrum & Bass. In Proc. of the Int. Conf. on Digital\nAudio Effects (DAFx) , Trondheim, Norway, December\n2015.\n[9] Peter Knees, ´Angel Faraldo, Perfecto Herrera, Richard\nV ogl, Sebastian B ¨ock, Florian H ¨orschl ¨ager, and Mick-\nael Le Goff. Two data sets for tempo estimation and\nkey detection in electronic dance music annotated from\nuser corrections. In Proc. of the Int. Soc. for Music Inf.\nRetrieval Conf., ISMIR 2015, M ´alaga, Spain, October\n26-30, 2015 , pages 364–370, 2015.\n[10] Daniel D. Lee and H. Sebastian Seung. Algorithms for\nnon-negative matrix factorization. In Proc. of the Neu-\nral Inf. Processing Systems (NIPS) , pages 556–562,\nDenver, CO, USA, 2000.\n[11] Henry Lindsay-Smith, Skot McDonald, and Mark San-\ndler. Drumkit transcription via convolutive NMF. InProc. of the Int. Conf. on Digital Audio Effects Conf.\n(DAFx) , York, UK, September 2012.\n[12] Taro Masuda, Kazuyoshi Yoshii, Masataka Goto, and\nShigeo Morishima. Spotting a query phrase from poly-\nphonic music audio signals based on semi-supervised\nnonnegative matrix factorization. In Proc. of the Int.\nSoc. for Music Inf. Retrieval Conf. (ISMIR) , pages 227–\n232, Taipei, Taiwan, 2014.\n[13] Meinard M ¨uller. Fundamentals of Music Processing .\nSpringer Verlag, 2015.\n[14] Meinard M ¨uller, Nanzhu Jiang, and Harald Grohganz.\nSM Toolbox: MATLAB implementations for comput-\ning and enhancing similiarty matrices. In Proc. of the\nAudio Engineering Soc. AES Conf. on Semantic Audio ,\nLondon, UK, 2014.\n[15] Jouni Paulus, Meinard M ¨uller, and Anssi P. Klapuri.\nAudio-based music structure analysis. In Proc. of the\nInt. Soc. for Music Inf. Retrieval Conf. (ISMIR) , pages\n625–636, Utrecht, The Netherlands, 2010.\n[16] Axel R ¨obel, Jordi Pons, Marco Liuni, and Mathieu La-\ngrange. On automatic drum transcription using non-\nnegative matrix deconvolution and itakura saito diver-\ngence. In Proc. of the IEEE Int. Conf. on Acoustics,\nSpeech and Sig. Proc. (ICASSP) , pages 414–418, Bris-\nbane, Australia, April 2015.\n[17] Bruno Rocha, Niels Bogaards, and Aline Honingh.\nSegmentation and timbre similarity in electronic dance\nmusic. In Proc. of the Sound and Music Computing\nConf. (SMC) , pages 754–761, Stockholm, Sweden,\n2013.\n[18] Paris Smaragdis. Non-negative matrix factor decon-\nvolution; extraction of multiple sound sources from\nmonophonic inputs. In Proc. of the Int. Conf. on In-\ndependent Component Analysis and Blind Signal Sep-\naration ICA , pages 494–499, Grenada, Spain, 2004.\n[19] Rick Snoman. Dance Music Manual: Tools, Toys, and\nTechniques . Taylor & Francis, 2013.\n[20] Jan Van Balen. Automatic recognition of samples in\nmusical audio. Master’s thesis, Universitat Pompeu\nFabra, Barcelona, Spain, 2011.\n[21] Avery Wang. An industrial strength audio search al-\ngorithm. In Proc. of the Int. Soc. for Music Inf. Re-\ntrieval Conf. (ISMIR) , pages 7–13, Baltimore, Mary-\nland, USA, 2003.\n[22] Ron J. Weiss and Juan Pablo Bello. Unsupervised dis-\ncovery of temporal structure in music. IEEE Journal of\nSelected Topics in Sig. Proc. , 5:1240–1251, 2011.\n[23] Jordan L. Whitney. Automatic recognition of samples\nin hip-hop music through non-negative matrix factor-\nization. Master’s thesis, University of Miami, 2013.508 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Deep Convolutional Networks on the Pitch Spiral For Music Instrument Recognition.",
        "author": [
            "Vincent Lostanlen",
            "Carmine-Emanuele Cella"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416928",
        "url": "https://doi.org/10.5281/zenodo.1416928",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/093_Paper.pdf",
        "abstract": "Musical performance combines a wide range of pitches, nuances, and expressive techniques. Audio-based classifi- cation of musical instruments thus requires to build signal representations that are invariant to such transformations. This article investigates the construction of learned con- volutional architectures for instrument recognition, given a limited amount of annotated training data. In this con- text, we benchmark three different weight sharing strate- gies for deep convolutional networks in the time-frequency domain: temporal kernels; time-frequency kernels; and a linear combination of time-frequency kernels which are one octave apart, akin to a Shepard pitch spiral. We pro- vide an acoustical interpretation of these strategies within the source-filter framework of quasi-harmonic sounds with a fixed spectral envelope, which are archetypal of musical notes. The best classification accuracy is obtained by hy- bridizing all three convolutional layers into a single deep learning architecture.",
        "zenodo_id": 1416928,
        "dblp_key": "conf/ismir/LostanlenC16",
        "content": "DEEP CONVOLUTIONAL NETWORKS ON THE PITCH SPIRAL FOR\nMUSIC INSTRUMENT RECOGNITION\nVincent Lostanlen and Carmine-Emanuele Cella\n´Ecole normale sup ´erieure, PSL Research University, CNRS, Paris, France\nABSTRACT\nMusical performance combines a wide range of pitches,\nnuances, and expressive techniques. Audio-based classiﬁ-\ncation of musical instruments thus requires to build signal\nrepresentations that are invariant to such transformations.\nThis article investigates the construction of learned con-\nvolutional architectures for instrument recognition, given\na limited amount of annotated training data. In this con-\ntext, we benchmark three different weight sharing strate-\ngies for deep convolutional networks in the time-frequency\ndomain: temporal kernels; time-frequency kernels; and a\nlinear combination of time-frequency kernels which are\none octave apart, akin to a Shepard pitch spiral. We pro-\nvide an acoustical interpretation of these strategies within\nthe source-ﬁlter framework of quasi-harmonic sounds with\na ﬁxed spectral envelope, which are archetypal of musical\nnotes. The best classiﬁcation accuracy is obtained by hy-\nbridizing all three convolutional layers into a single deep\nlearning architecture.\n1. INTRODUCTION\nAmong the cognitive attributes of musical tones, pitch is\ndistinguished by a combination of three properties. First,\nit is relative: ordering pitches from low to high gives rise\nto intervals and melodic patterns. Secondly, it is intensive:\nmultiple pitches heard simultaneously produce a chord, not\na single uniﬁed tone – contrary to loudness, which adds up\nwith the number of sources. Thirdly, it does not depend\non instrumentation: this makes possible the transcription\nof polyphonic music under a single symbolic system [5].\nTuning auditory ﬁlters to a perceptual scale of pitches\nprovides a time-frequency representation of music signals\nthat satisﬁes the ﬁrst two of these properties. It is thus a\nstarting point for a wide range of MIR applications, which\ncan be separated in two categories: pitch- relative (e.g.\nchord estimation [13]) and pitch- invariant (e.g. instrument\nThis work is supported by the ERC InvariantClass grant 320959. The\nsource code to reproduce ﬁgures and experiments is freely available at\nwww.github.com/lostanlen/ismir2016 .\nc⃝Vincent Lostanlen and Carmine-Emanuele Cella. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Vincent Lostanlen and Carmine-Emanuele\nCella. “Deep convolutional networks on the pitch spiral for music in-\nstrument recognition”, 17th International Society for Music Information\nRetrieval Conference, 2016.recognition [9]). Both aim at disentangling pitch from tim-\nbral content as independent factors of variability, a goal\nthat is made possible by the third aforementioned property.\nThis is pursued by extracting mid-level features on top of\nthe spectrogram, be them engineered or learned from train-\ning data. Both approaches have their limitations: a ”bag-\nof-features” lacks ﬂexibility to represent ﬁne-grain class\nboundaries, whereas a purely learned pipeline often leads\nto uninterpretable overﬁtting, especially in MIR where the\nquantity of thoroughly annotated data is relatively small.\nIn this article, we strive to integrate domain-speciﬁc\nknowledge about musical pitch into a deep learning frame-\nwork, in an effort towards bridging the gap between feature\nengineering and feature learning.\nSection 2 reviews the related work on feature learning\nfor signal-based music classiﬁcation. Section 3 demon-\nstrates that pitch is the major factor of variability among\nmusical notes of a given instrument, if described by\ntheir mel-frequency cepstra. Section 4 presents a typical\ndeep learning architecture for spectrogram-based classiﬁ-\ncation, consisting of two convolutional layers in the time-\nfrequency domain and one densely connected layer. Sec-\ntion 5 introduces alternative convolutional architectures for\nlearning mid-level features, along time and along a Shep-\nard pitch spiral, as well as aggregation of multiple models\nin the deepest layers. Sections 6 discusses the effective-\nness of the presented systems on a challenging dataset for\nmusic instrument recognition.\n2. RELATED WORK\nSpurred by the growth of annotated datasets and the democ-\nratization of high-performance computing, feature learning\nhas enjoyed a renewed interest in recent years within the\nMIR community, both in supervised and unsupervised set-\ntings. Whereas unsupervised learning (e.g. k-means [25],\nGaussian mixtures [14]) is employed to ﬁt the distribution\nof the data with few parameters of relatively low abstrac-\ntion and high dimensionality, state-of-the-art supervised\nlearning consists of a deep composition of multiple non-\nlinear transformations, jointly optimized to predict class\nlabels, and whose behaviour tend to gain in abstraction as\ndepth increases [27].\nAs compared to other deep learning techniques for au-\ndio processing, convolutional networks happen to strike\nthe balance between learning capacity and robustness. The\nconvolutional structure of learned transformations is de-\nrived from the assumption that the input signal, be it a one-612dimensional waveform or a two-dimensional spectrogram,\nis stationary — which means that content is independent\nfrom location. Moreover, the most informative dependen-\ncies between signal coefﬁcients are assumed to be concen-\ntrated to temporal or spectrotemporal neighborhoods. Un-\nder such hypotheses, linear transformations can be learned\nefﬁciently by limiting their support to a small kernel which\nis convolved over the whole input. This method, known\nas weight sharing, decreases the number of parameters of\neach feature map while increasing the amount of data on\nwhich kernels are trained.\nBy design, convolutional networks seem well adapted\nto instrument recognition, as this task does not require a\nprecise timing of the activation function, and is thus essen-\ntially a challenge of temporal integration [9, 14]. Further-\nmore, it beneﬁts from an unequivocal ground truth, and\nmay be simpliﬁed to a single-label classiﬁcation problem\nby extracting individual stems from a multitrack dataset [2].\nAs such, it is often used a test bed for the development of\nnew algorithms [17, 18], as well as in computational stud-\nies in music cognition [20, 21].\nSome other applications of deep convolutional networks\ninclude onset detection [23], transcription [24], chord\nrecognition [13], genre classiﬁcation [3], downbeat track-\ning [8], boundary detection [26], and recommendation\n[27].\nInterestingly, many research teams in MIR have con-\nverged to employ the same architecture, consisting of two\nconvolutional layers and two densely connected layers\n[7,13,15,17,18,23,26], and this article makes no exception.\nHowever, there is no clear consensus regarding the weight\nsharing strategies that should be applied to musical audio\nstreams: convolutions in time or in time-frequency coex-\nist in the recent literature. A promising paradigm [6, 8],\nat the interaction between feature engineering and feature\nlearning, is to extract temporal or spectrotemporal descrip-\ntors of various low-level modalities, train speciﬁc convolu-\ntional layers on each modality to learn mid-level features,\nand hybridize information at the top level. Recognizing\nthat this idea has been successfully applied to large-scale\nartist recognition [6] as well as downbeat tracking [8], we\naim to proceed in a comparable way for instrument recog-\nnition.\n3. HOW INVARIANT IS THE MEL-FREQUENCY\nCEPSTRUM ?\nThe mel scale is a quasi-logarithmic function of acoustic\nfrequency designed such that perceptually similar pitch in-\ntervals appear equal in width over the full hearing range.\nThis section shows that engineering transposition-invariant\nfeatures from the mel scale does not sufﬁce to build pitch\ninvariants for complex sounds, thus motivating further in-\nquiry.\nThe time-frequency domain produced by a constant-Q\nﬁlter bank tuned to the mel scale is covariant with respect\nto pitch transposition of pure tones. As a result, a chro-\nmatic scale played at constant speed would draw parallel,\ndiagonal lines, each of them corresponding to a different\nFigure 1 : Constant-Q spectrogram of a chromatic scale\nplayed by a tuba. Although the harmonic partials shift pro-\ngressively, the spectral envelope remains unchanged, as re-\nvealed by the presence of a ﬁxed cutoff frequency. See text\nfor details.\npartial wave. However, the physics of musical instruments\nconstrain these partial waves to bear a negligible energy\nif their frequencies are beyond the range of acoustic reso-\nnance.\nAs shown on Figure 1, the constant-Q spectrogram of\na tuba chromatic scale exhibits a ﬁxed, cutoff frequency\nat about 2.5 kHz , which delineates the support of its spec-\ntral envelope. This elementary observation implies that re-\nalistic pitch changes cannot be modeled by translating a\nrigid spectral template along the log-frequency axis. The\nsame property is veriﬁed for a wide class of instruments,\nespecially brass and woodwinds. As a consequence, the\nconstruction of powerful invariants to musical pitch is not\namenable to delocalized operations on the mel-frequency\nspectrum, such as a discrete cosine transform (DCT) which\nleads to the mel-frequency cepstral coefﬁcients (MFCC),\noften used in audio classiﬁcation [9, 14].\nTo validate the above claim, we have extracted the\nMFCC of 1116 individual notes from the RWC dataset\n[10], as played by 6 instruments, with 32 pitches, 3 nu-\nances, and 2 interprets and manufacturers. When more\nthan 32 pitches were available (e.g. piano), we selected\na contiguous subset of 32 pitches in the middle register.\nFollowing a well-established rule [9, 14], the MFCC were\ndeﬁned the 12 lowest nonzero ”quefrencies” among the\nDCT coefﬁcients extracted from a ﬁlter bank of 40 mel-\nfrequency bands. We then have computed the distribution\nof squared Euclidean distances between musical notes in\nthe 12-dimensional space of MFCC features.\nFigure 2 summarizes our results. We found that restrict-\ning the cluster to one nuance, one interpret, or one manu-\nfacturer hardly reduces intra-class distances. This suggests\nthat MFCC are fairly successful in building invariant rep-\nresentations to such factors of variability. In contrast, the\ncluster corresponding to each instrument is shrinked if de-\ncomposed into a mixture of same-pitch clusters, sometimes\nby an order of magnitude. In other words, most of the vari-\nance in an instrument cluster of mel-frequency cepstra is\ndue to pitch transposition.\nKeeping less than 12 coefﬁcients certainly improves\ninvariance, yet at the cost of inter-class discriminability,\nand vice versa. This experiment shows that the mel-\nfrequency cepstrum is perfectible in terms of invariance-Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 613Figure 2 : Distributions of squared Euclidean distances\namong various MFCC clusters in the RWC dataset.\nWhisker ends denote lower and upper deciles. See text for\ndetails.\ndiscriminability tradeoff, and that there remains a lot to be\ngained by feature learning in this area.\n4. DEEP CONVOLUTIONAL NETWORKS\nA deep learning system for classiﬁcation is built by stack-\ning multiple layers of weakly nonlinear transformations,\nwhose parameters are optimized such that the top-level\nlayer ﬁts a training set of labeled examples. This section\nintroduces a typical deep learning architecture for audio\nclassiﬁcation and describes the functioning of each layer.\nEach layer in a convolutional network typically consists\nin the composition of three operations: two-dimensional\nconvolutions, application of a pointwise nonlinearity, and\nlocal pooling. The deep feed-forward network made of\ntwo convolutional layers and two densely connected lay-\ners, on which our experiment are conducted, has become\nade facto standard in the MIR community [7, 13, 15,\n17, 18, 23, 26]. This ubiquity in the literature suggests\nthat a four-layer network with two convolutional layers is\nwell adapted to supervised audio classiﬁcation problems of\nmoderate size.\nThe input of our system is a constant-Q spectrogram,\nwhich is very comparable to a mel-frequency spectrogram.\nWe used the implementation from the librosa package [19]\nwith Q= 12 ﬁlters per octave, center frequencies ranging\nfrom A1(55 Hz ) toA9(14 kHz ), and a hop size of 23 ms .\nFurthermore, we applied nonlinear perceptual weighting of\nloudness in order to reduce the dynamic range between the\nfundamental partial and its upper harmonics. A 3-second\nsound excerpt x[t]is represented by a time-frequency ma-trixx1[t, k1]of width T= 128 samples and height K1=\n96frequency bands.\nA convolutional operator is deﬁned as a family\nW2[τ, κ1, k2]ofK2two-dimensional ﬁlters, whose im-\npulse repsonses are all constrained to have width ∆tand\nheight ∆k1. Element-wise biases b2[k2]are added to the\nconvolutions, resulting in the three-way tensor\ny2[t, k1, k2]\n=b2[k2] +W2[t, k1, k2]t,k1∗x1[t, k1]\n=b2[k2] +∑\n0≤τ<∆t\n0≤κ1<∆k1W2[τ, κ1, k2]x1[t−τ, k1−κ1].(1)\nThe pointwise nonlinearity we have chosen is the rectiﬁed\nlinear unit (ReLU), with a rectifying slope of α= 0.3for\nnegative inputs.\ny+\n2[t, k1, k2] ={αy2[t, k1, k2]ify2[t, k1, k2]<0\ny2[t, k1, k2]ify2[t, k1, k2]≥0(2)\nThe pooling step consists in retaining the maximal acti-\nvation among neighboring units in the time-frequency do-\nmain (t, k1)over non-overlapping rectangles of width ∆t\nand height ∆k1.\nx2[t, k1, k2] = max\n0≤τ<∆t\n0≤κ1<∆k1{\ny+\n2[t−τ, k1−κ1, k2]}\n(3)\nThe hidden units in x2are in turn fed to a second layer of\nconvolutions, ReLU, and pooling. Observe that the cor-\nresponding convolutional operator W3[τ, κ1, k2, k3]per-\nforms a linear combination of time-frequency feature maps\ninx2along the variable k2.\ny3[t, k1, k3]\n=∑\nk2b3[k2, k3] +W3[t, k1, k2, k3]t,k1∗x2[t, k1, k2].(4)\nTensors y+\n3andx3are derived from y3by ReLU and pool-\ning, with formulae similar to Eqs. (2) and (3). The third\nlayer consists of the linear projection of x3, viewed as a\nvector of the ﬂattened index (t, k1, k3), over K4units:\ny4[k4] =b4[k4] +∑\nt,k1,k3W4[t, k1, k3, k4]x3[t, k1, k3](5)\nWe apply a ReLU to y4, yielding x4[k4] =y+\n4[k4]. Fi-\nnally, we project x4, onto a layer of output units y5that\nshould represent instrument activations:\ny5[k5] =∑\nk4W5[k4, k5]x4[k4]. (6)\nThe ﬁnal transformation is a softmax nonlinearity, which\nensures that output coefﬁcients are non-negative and sum\nto one, hence can be ﬁt to a probability distribution:\nx5[k5] =expy5[k5]∑\nκ5expy5[κ5]. (7)\nGiven a training set of spectrogram-instrument pairs\n(x1, k), all weigths in the network are iteratively updated614 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Figure 3 : A two-dimensional deep convolutional network trained on constant-Q spectrograms. See text for details.\nto minimize the stochastic cross-entropy loss L(x5, k) =\n−logx5[k]over shufﬂed mini-batches of size 32with uni-\nform class distribution. The pairs (x1, k)are extracted on\nthe ﬂy by selecting non-silent regions at random within\na dataset of single-instrument audio recordings. Each 3-\nsecond spectrogram x1[t, k1]within a batch is globally nor-\nmalized such that the whole batch had zero mean and unit\nvariance. At training time, a random dropout of 50% is\napplied to the activations of x3andx4. The learning rate\npolicy for each scalar weight in the network is Adam [16],\na state-of-the-art online optimizer for gradient-based learn-\ning. Mini-batch training is stopped after the average train-\ning loss stopped decreasing over one full epoch of size\n8192 . The architecture is built using the Keras library [4]\nand trained on a graphics processing unit within minutes.\n5. IMPROVED WEIGHT SHARING STRATEGIES\nAlthough a dataset of music signals is unquestionably sta-\ntionary over the time dimension – at least at the scale of\na few seconds – it cannot be taken for granted that all fre-\nquency bands of a constant-Q spectrogram would have the\nsame local statistics [12]. In this section, we introduce two\nalternative architectures to address the nonstationarity of\nmusic on the log-frequency axis, while still leveraging the\nefﬁciency of convolutional representations.\nMany are the objections to the stationarity assumption\namong local neighborhoods in mel frequency. Notably\nenough, one of the most compelling is derived from the\nclassical source-ﬁlter model of sound production. The ﬁl-\nter, which carries the overall spectral envelope, is affected\nby intensity and playing style, but not by pitch. Conversely,\nthe source, which consists of a pseudo-periodic wave, is\ntransposed in frequency under the action of pitch. In order\nto extract the discriminative information present in both\nterms, it is ﬁrst necessary to disentangle the contributions\nof source and ﬁlter in the constant-Q spectrogram. Yet,\nthis can only be achieved by exploiting long-range correla-\ntions in frequency, such as harmonic and formantic struc-\ntures. Besides, the harmonic comb created by the Fourier\nseries of the source makes an irregular pattern on the log-\nfrequency axis which is hard to characterize by local statis-\ntics.5.1 One-dimensional convolutions at high frequencies\nFacing nonstationary constant-Q spectra, the most conser-\nvative workaround is to increase the height ∆κ1of each\nconvolutional kernel up to the total number of bins K1in\nthe spectrogram. As a result, W1andW2are no longer\ntransposed over adjacent frequency bands, since convolu-\ntions are merely performed over the time variable. The\ndeﬁnition of y2[t, k1, k2]rewrites as\ny2[t, k1, k2]\n=b2[k2] +W2[t, k1, k2]t∗x1[t, k1]\n=b2[k2] +∑\n0≤τ<∆tW2[τ, k1, k2]x1[t−τ, k1], (8)\nand similarly for y3[t, k1, k3]. While this approach is the-\noretically capable of encoding pitch invariants, it is prone\nto early specialization of low-level features, thus not fully\ntaking advantage of the network depth.\nHowever, the situation is improved if the feature maps\nare restricted to the highest frequencies in the constant-Q\nspectrum. It should be observed that, around the nthpartial\nof a quasi-harmonic sound, the distance in log-frequency\nbetween neighboring partials decays like 1/n, and the un-\nevenness between those distances decays like 1/n2. Con-\nsequently, at the topmost octaves of the constant-Q spec-\ntrum, where nis equal or greater than Q, the partials appear\nclose to each other and almost evenly spaced. Furthermore,\ndue to the logarithmic compression of loudness, the poly-\nnomial decay of the spectral envelope is linearized: thus,\nat high frequencies, transposed pitches have similar spec-\ntra up to some additive bias. The combination of these two\nphenomena implies that the correlation between constant-\nQ spectra of different pitches is greater towards high fre-\nquencies, and that the learning of polyvalent feature maps\nbecomes tractable.\nIn our experiments, the one-dimensional convolutions\nover the time variable range from A6(1.76 kHz ) toA9\n(14 kHz ).\n5.2 Convolutions on the pitch spiral at low frequencies\nThe weight sharing strategy presented above exploits the\nfacts that, at high frequencies, quasi-harmonic partials areProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 615numerous, and that the amount of energy within a fre-\nquency band is independent of pitch. At low frequencies,\nwe make the exact opposite assumptions: we claim that\nthe harmonic comb is sparse and covariant with respect to\npitch shift. Observe that, for any two distinct partials taken\nat random between 1andn, the probability that they are in\noctave relation is slightly above 1/n. Thus, for nrelatively\nlow, the structure of harmonic sounds is well described by\nmerely measuring correlations between partials one octave\napart. This idea consists in rolling up the log-frequency\naxis into a Shepard pitch spiral, such that octave intervals\ncorrespond to full turns, hence aligning all coefﬁcients of\nthe form x1[t, k1+Q×j1]forj1∈Zonto the same ra-\ndius of the spiral. Therefore, correlations between power-\nof-two harmonics are revealed by the octave variable j1.\nTo implement a convolutional network on the pitch spi-\nral, we crop the constant-Q spectrogram in log-frequency\nintoJ1= 3 half-overlapping bands whose height equals\n2Q, that is two octaves. Each feature map in the ﬁrst layer,\nindexed by k2, results from the sum of convolutions be-\ntween a time-frequency kernel and a band, thus emulating\na linear combination in the pitch spiral with a 3-d tensor\nW2[τ, κ1, j1, k2]at ﬁxed k2. The deﬁnition of y2[t, k1, k2]\nrewrites as\ny2[t, k1, k2] =b2[k2]\n+∑\nτ,κ1,j1W2[τ, κ1, j1, k2]\n×x1[t−τ, k1−κ1−Qj1].(9)\nThe above is different from training two-dimensional ker-\nnel on a time-chroma-octave tensor, since it does not suffer\nfrom artifacts at octave boundaries.\nThe linear combinations of frequency bands that are\none octave apart, as proposed here, bears a resemblance\nwith engineered features for music instrument recogni-\ntion [22], such as tristimulus, empirical inharmonicity, har-\nmonic spectral deviation, odd-to-even harmonic energy ra-\ntio, as well as octave band signal intensities (OBSI) [14].\nGuaranteeing the partial index nto remain low is\nachieved by restricting the pitch spiral to its lowest frequen-\ncies. This operation also partially circumvents the problem\nof ﬁxed spectral envelope in musical sounds, thus improv-\ning the validness of the stationarity assumption. In our ex-\nperiments, the pitch spiral ranges from A2(110 Hz ) toA6\n(1.76 kHz ).\nIn summary, the classical two-dimensional convolutions\nmake a stationarity assumption among frequency neigh-\nborhoods. This approach gives a coarse approximation\nof the spectral envelope. Resorting to one-dimensional\nconvolutions allows to disregard nonstationarity, but does\nnot yield a pitch-invariant representation per se: thus, we\nonly apply them at the topmost frequencies, i.e. where\nthe invariance-to-stationarity ratio in the data is already\nfavorable. Conversely, two-dimensional convolutions on\nthe pitch spiral addresses the invariant representation of\nsparse, transposition-covariant spectra: as such, they are\nbest suited to the lowest frequencies, i.e. where partials are\nfurther apart and pitch changes can be approximated byminutes tracks minutes tracks\npiano 58 28 44 15\nviolin 51 14 49 22\ndist. guitar 15 14 17 11\nfemale singer 10 11 19 12\nclarinet 10 7 13 18\nﬂute 7 5 53 29\ntrumpet 4 6 7 27\ntenor sax. 3 3 6 5\ntotal 158 88 208 139\nTable 1 : Quantity of data in the training set (left) and test\nset (right). The training set is derived from MedleyDB. The\ntest set is derived from MedleyDB for distorted electric gui-\ntar and female singer, and from [14] for other instruments.\nlog-frequency translations. The next section reports exper-\niments on instrument recognition that capitalize on these\nconsiderations.\n6. APPLICATIONS\nThe proposed algorithms are trained on a subset of Med-\nleyDB v1.1. [2], a dataset of 122 multitracks annotated\nwith instrument activations. We extracted the monophonic\nstems corresponding to a selection of eight pitched instru-\nments (see Table 1). Stems with leaking instruments in the\nbackground were discarded.\nThe evaluation set consists of 126 recordings of solo\nmusic collected by Joder et al. [14], supplemented with 23\nstems of electric guitar and female voice from MedleyDB.\nIn doing so, guitarists and vocalists were thoroughly put\neither in the training set or the test set, to prevent any\nartist bias. We discarded recordings with extended instru-\nmental techniques, since they are extremely rare in Med-\nleyDB. Constant-Q spectrograms from the evaluation set\nwere split into half-overlapping, 3-second excerpts.\nFor the two-dimensional convolutional network, each of\nthe two layers consists of 32kernels of width 5and height\n5, followed by a max-pooling of width 5and height 3. Ex-\npressed in physical units, the supports of the kernels are\nrespectively equal to 116 ms and580 ms in time, 5and10\nsemitones in frequency. For the one-dimensional convolu-\ntional network, each of two layers consists of 32kernels\nof width 3, followed by a max-pooling of width 5. Ob-\nserve that the temporal supports match those of the two-\ndimensional convolutional network. For the convolutional\nnetwork on the pitch spiral, the ﬁrst layer consists of 32\nkernels of width 5, height 3semitones, and a radial length\nof3octaves in the spiral. The max-pooling operator and\nthe second layer are the same as in the two-dimensional\nconvolutional network.\nIn addition to the three architectures above, we build hy-\nbrid networks implementing more than one of the weight\nsharing strategy presented above. In all architectures, the\ndensely connected layers have K4= 64 hidden units and616 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016piano violin dist. female clarinet ﬂute trumpet tenor average\nguitar singer sax.\nbag-of-features 99.7 76.2 92.7 81.6 49.9 22.5 63.7 4.4 61.4\nand random forest (±0.1) (±3.1) (±0.4) (±1.5) ( ±0.8) ( ±0.8) ( ±2.1) ( ±1.1) (±0.5)\nspiral 86.9 37.0 72.3 84.4 61.1 30.0 54.9 52.7 59.9\n(36k parameters) (±5.8) ( ±5.6) ( ±6.2) ( ±6.1) ( ±8.7) ( ±4.0) ( ±6.6) ( ±16.4) (±2.4)\n1-d 73.3 43.9 91.8 82.9 28.8 51.3 63.3 59.0 61.8\n(20k parameters) (±11.0) ( ±6.3) ( ±1.1) ( ±1.9) ( ±5.0) (±13.4) (±5.0) (±6.8) (±0.9)\n2-d,32kernels 96.8 68.5 86.0 80.6 81.3 44.4 68.0 48.4 69.1\n(93k parameters) (±1.4) ( ±9.3) ( ±2.7) ( ±1.7) ( ±4.1) ( ±4.4) ( ±6.2) ( ±5.3) (±2.0)\nspiral & 1-d 96.5 47.6 90.2 84.5 79.6 41.8 59.8 53.0 69.1\n(55k parameters) (±2.3) ( ±6.1) ( ±2.3) ( ±2.8) ( ±2.1) ( ±4.1) ( ±1.9) ( ±16.5) (±2.0)\nspiral & 2-d 97.6 73.3 86.5 86.9 82.3 45.8 66.9 51.2 71.7\n(128k parameters) (±0.8) ( ±4.4) ( ±4.5) (±3.6) (±3.2) (±2.9) ( ±5.8) ( ±10.6) (±2.0)\n1-d & 2-d 96.5 72.4 86.3 91.0 73.3 49.5 67.7 55.0 73.8\n(111k parameters) (±0.9) ( ±5.9) ( ±5.2) ( ±5.5) ( ±6.4) ( ±6.9) ( ±2.5) ( ±11.5) (±2.3)\n2-d & 1-d & spiral 97.8 70.9 88.0 85.9 75.0 48.3 67.3 59.0 74.0\n(147k parameters) (±0.6) ( ±6.1) ( ±3.7) ( ±3.8) ( ±4.3) ( ±6.6) ( ±4.4) (±7.3) (±0.6)\n2-d, 48 kernels 96.5 69.3 84.5 84.2 77.4 45.5 68.8 52.6 71.7\n(158k parameters) (±1.4) ( ±7.2) ( ±2.5) ( ±5.7) ( ±6.0) ( ±7.3) ( ±1.8) ( ±10.1) (±2.0)\nTable 2 : Test set accuracies for all presented architectures. All convolutional layers have 32kernels unless stated otherwise.\nK5= 8output units.\nIn order to compare the results against shallow classi-\nﬁers, we also extracted a typical ”bag-of-features” over\nhalf-overlapping, 3-second excerpts in the training set.\nThese features consist of the means and standard devia-\ntions of spectral shape descriptors, i.e. centroid, bandwidth,\nskewness, and rolloff; the mean and standard deviation of\nthe zero-crossing rate in the time domain; and the means\nof MFCC as well as their ﬁrst and second derivative. We\ntrained a random forest of 100decision trees on the result-\ning feature vector of dimension 70, with balanced class\nprobability.\nResults are summarized in Table 2. First of all, the bag-\nof-features approach presents large accuracy variations be-\ntween classes, due to the unbalance of available training\ndata. In contrast, most convolutional models, especially\nhybrid ones, show less correlation between the amount of\ntraining data in the class and the accuracy. This suggests\nthat convolutional networks are able to learn polyvalent\nmid-level features that can be re-used a test time to dis-\ncriminate rarer classes.\nFurthermore, 2-d convolutions outperform other non-\nhybrid weight sharing strategies. However, a class with\nbroadband temporal modulations, namely the distorted\nelectric guitar, is best classiﬁed with 1-d convolutions.\nHybridizing 2-d with either 1-d or spiral convolutions\nprovide consistent, albeit small improvements with respect\nto 2-d alone. The best overall accuracy is reached by the\nfull hybridization of all three weight sharing strategies, be-\ncause of a performance boost for the rarest classes.\nThe accuracy gain by combining multiple models could\nsimply be the result of a greater number of parameters. To\nrefute this hypothesis, we train a 2-d convolutional networkwith 48kernels instead of 32, so as to match the budget\nof the full hybrid model, i.e. about 150k parameters. The\nperformance is certainly increased, but not up to the hy-\nbrid models involving 2-d convolutions, which have less\nparameters. Increasing the number of kernels even more\ncause the accuracy to level out and the variance between\ntrials to increase.\nRunning the same experiments with broader frequency\nranges of 1-d and spiral convolutions often led to a de-\ngraded performance, and are thus not reported.\n7. CONCLUSIONS\nUnderstanding the inﬂuence of pitch in audio streams is\nparamount to the design of an efﬁcient system for auto-\nmated classiﬁcation, tagging, and similarity retrieval in mu-\nsic. We have presented deep learning methods to address\npitch invariance while preserving good timbral discrim-\ninability. It consists in training a feed-forward convolu-\ntional network over the constant-Q spectrogram, with three\ndifferent weight sharing strategies according to the type of\ninput: along time at high frequencies (above 2 kHz ), on a\nShepard pitch spiral at low frequencies (below 2 kHz ), and\nin time-frequency over both high and low frequencies.\nA possible improvement of the presented architecture\nwould be to place a third convolutional layer in the time\ndomain before performing long-term max-pooling, hence\nmodelling the joint dynamics of the three mid-level feature\nmaps. Future work will investigate the association of the\npresented weight sharing strategies with recent advances in\ndeep learning for music informatics, such as data augmen-\ntation [18], multiscale representations [1,11], and adversar-\nial training [15].Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 6178. REFERENCES\n[1]Joakim And ´en, Vincent Lostanlen, and St ´ephane Mal-\nlat. Joint time-frequency scattering for audio classiﬁca-\ntion. In Proc. MLSP , 2015.\n[2]Rachel Bittner, Justin Salamon, Mike Tierney,\nMatthias Mauch, Chris Cannam, and Juan Bello. Med-\nleyDB: a multitrack dataset for annotation-intensive\nMIR research. In Proc. ISMIR , 2014.\n[3]Keunwoo Choi, George Fazekas, Mark Sandler,\nJeonghee Kim, and Naver Labs. Auralisation of deep\nconvolutional neural networks: listening to learned fea-\ntures. In Proc. ISMIR , 2015.\n[4]Franc ¸ois Chollet. Keras: a deep learning library for\nTheano and TensorFlow, 2015.\n[5]Alain de Cheveign ´e. Pitch perception. In Oxford Hand-\nbook of Auditory Science: Hearing , chapter 4, pages\n71–104. Oxford University Press, 2005.\n[6]Sander Dieleman, Phil ´emon Brakel, and Benjamin\nSchrauwen. Audio-based music classiﬁcation with a\npretrained convolutional network. In Proc. ISMIR ,\n2011.\n[7]Sander Dieleman and Benjamin Schrauwen. End-to-\nend learning for music audio. In Proc. ICASSP , 2014.\n[8]Simon Durand, Juan P. Bello, Bertrand David, and\nGa¨el Richard. Feature-adapted convolutional neural\nnetworks for downbeat tracking. In Proc. ICASSP ,\n2016.\n[9]Antti Eronen and Anssi Klapuri. Musical instrument\nrecognition using cepstral coefﬁcients and temporal\nfeatures. In Proc. ICASSP , 2000.\n[10] Masataka Goto, Hiroki Hashiguchi, Takuichi\nNishimura, and Ryuichi Oka. RWC music database:\nmusic genre database and musical instrument sound\ndatabase. In Proc. ISMIR , 2003.\n[11] Philippe Hamel, Yoshua Bengio, and Douglas Eck.\nBuilding musically-relevant audio features through\nmultiple timescale representations. In Proc. ISMIR ,\n2012.\n[12] Eric J. Humphrey, Juan P. Bello, and Yann Le Cun. Fea-\nture learning and deep architectures: New directions\nfor music informatics. JIIS, 41(3):461–481, 2013.\n[13] Eric J. Humphrey, Taemin Cho, and Juan P. Bello.\nLearning a robust tonnetz-space transform for auto-\nmatic chord recognition. In Proc. ICASSP , 2012.\n[14] Cyril Joder, Slim Essid, and Ga ¨el Richard. Tempo-\nral integration for audio classiﬁcation with applica-\ntion to musical instrument classiﬁcation. IEEE TASLP ,\n17(1):174–186, 2009.[15] Corey Kereliuk, Bob L. Sturm, and Jan Larsen. Deep\nLearning and Music Adversaries. IEEE Trans. Multi-\nmedia , 17(11):2059–2071, 2015.\n[16] Diederik P. Kingma and Jimmy Lei Ba. Adam: a\nmethod for stochastic optimization. In Proc. ICML ,\n2015.\n[17] Peter Li, Jiyuan Qian, and Tian Wang. Automatic in-\nstrument recognition in polyphonic music using convo-\nlutional neural networks. arXiv preprint , 1511.05520,\n2015.\n[18] Brian McFee, Eric J. Humphrey, and Juan P. Bello. A\nsoftware framework for musical data augmentation. In\nProc. ISMIR , 2015.\n[19] Brian McFee, Matt McVicar, Colin Raffel, Dawen\nLiang, Oriol Nieto, Eric Battenberg, Josh Moore,\nDan Ellis, Ryuichi Yamamoto, Rachel Bittner, Dou-\nglas Repetto, Petr Viktorin, Jo ˜ao Felipe Santos, and\nAdrian Holovaty. librosa: 0.4.1. zenodo. 10.5281/zen-\nodo.18369, October 2015.\n[20] Michael J. Newton and Leslie S. Smith. A neurally in-\nspired musical instrument classiﬁcation system based\nupon the sound onset. JASA , 131(6):4785, 2012.\n[21] Kailash Patil, Daniel Pressnitzer, Shihab Shamma, and\nMounya Elhilali. Music in our ears: the biological\nbases of musical timbre perception. PLoS Comput.\nBiol., 8(11):e1002759, 2012.\n[22] Geoffroy Peeters. A large set of audio features for\nsound description (similarity and classiﬁcation) in the\nCUIDADO project. Technical report, Ircam, 2004.\n[23] Jan Schl ¨uter and Sebastian B ¨ock. Improved musical\nonset detection with convolutional neural networks. In\nProc. ICASSP , 2014.\n[24] Siddharth Sigtia, Emmanouil Benetos, and Simon\nDixon. An end-to-end neural network for polyphonic\nmusic transcription. arXiv preprint , 1508.01774, 2015.\n[25] Dan Stowell and Mark D. Plumbley. Automatic large-\nscale classiﬁcation of bird sounds is strongly improved\nby unsupervised feature learning. PeerJ , 2:e488, 2014.\n[26] Karen Ullrich, Jan Schl ¨uter, and Thomas Grill. Bound-\nary detection in music structure analysis using convo-\nlutional neural networks. In Proc. ISMIR , 2014.\n[27] Aaron van den Oord, Sander Dieleman, and Benjamin\nSchrauwen. Deep content-based music recommenda-\ntion. In Proc. NIPS , 2013.618 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Automatic Outlier Detection in Music Genre Datasets.",
        "author": [
            "Yen-Cheng Lu",
            "Chih-Wei Wu",
            "Alexander Lerch 0001",
            "Chang-Tien Lu"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1418193",
        "url": "https://doi.org/10.5281/zenodo.1418193",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/215_Paper.pdf",
        "abstract": "Outlier detection, also known as anomaly detection, is an important topic that has been studied for decades. An outlier detection system is able to identify anomalies in a dataset and thus improve data integrity by removing the detected outliers. It has been successfully applied to different types of data in various fields such as cyber-security, finance, and transportation. In the field of Music Information Re- trieval (MIR), however, the number of related studies is small. In this paper, we introduce different state-of-the-art outlier detection techniques and evaluate their viability in the context of music datasets. More specifically, we present a comparative study of 6 outlier detection algorithms ap- plied to a Music Genre Recognition (MGR) dataset. It is determined how well algorithms can identify mislabeled or corrupted files, and how much the quality of the dataset can be improved. Results indicate that state-of-the-art anomaly detection systems have problems identifying anomalies in MGR datasets reliably.",
        "zenodo_id": 1418193,
        "dblp_key": "conf/ismir/LuWLL16",
        "content": "AUTOMATIC OUTLIER DETECTION IN MUSIC GENRE DATASETS\nYen-Cheng Lu1Chih-Wei Wu2Chang-Tien Lu1Alexander Lerch2\n1Department of Computer Science, Virginia Tech, USA\n2Center for Music Technology, Georgia Institute of Technology, USA\nkevinlu@vt.edu, cwu307@gatech.edu, ctlu@vt.edu, alexander.lerch@gatech.edu\nABSTRACT\nOutlier detection, also known as anomaly detection, is an\nimportant topic that has been studied for decades. An outlier\ndetection system is able to identify anomalies in a dataset\nand thus improve data integrity by removing the detected\noutliers. It has been successfully applied to different types\nof data in various ﬁelds such as cyber-security, ﬁnance,\nand transportation. In the ﬁeld of Music Information Re-\ntrieval (MIR), however, the number of related studies is\nsmall. In this paper, we introduce different state-of-the-art\noutlier detection techniques and evaluate their viability in\nthe context of music datasets. More speciﬁcally, we present\na comparative study of 6 outlier detection algorithms ap-\nplied to a Music Genre Recognition (MGR) dataset. It is\ndetermined how well algorithms can identify mislabeled or\ncorrupted ﬁles, and how much the quality of the dataset can\nbe improved. Results indicate that state-of-the-art anomaly\ndetection systems have problems identifying anomalies in\nMGR datasets reliably.\n1. INTRODUCTION\nWith the advance of computer-centric technologies in the\nlast few decades, various types of digital data are being gen-\nerated at an unprecedented rate. To account for this drastic\ngrowth in digital data, exploiting its (hidden) information\nwith both efﬁciency and accuracy became an active research\nﬁeld generally known as Data Mining.\nOutlier detection, being one of the most frequently stud-\nied topics in Data Mining, is a task that aims to identify\nabnormal data points in the investigated dataset. Generally\nspeaking, an outlier often refers to the instance that does not\nconform to the expected behavior and should be highlighted.\nFor example, in a security surveillance system an outlier\ncould be the intruder, whereas in credit card records, an\noutlier could be a fraud transaction.\nMany algorithms have been proposed to identify out-\nliers in different types of data, and they have been proven\nsuccessful in ﬁelds such as cyber-security [1], ﬁnance [4],\nc/circlecopyrtYen-Cheng Lu, Chih-Wei Wu, Chang-Tien Lu, Alexander\nLerch. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Yen-Cheng Lu, Chih-Wei Wu, Chang-\nTien Lu, Alexander Lerch. “Automatic Outlier Detection in Music Genre\nDatasets”, 17th International Society for Music Information Retrieval\nConference, 2016.and transportation [17]. Outlier detection techniques can\nalso be used as a pre-processing step to remove anomalous\ndata. In the work of Smith and Martinez [24], a set of out-\nlier detection methods are used to remove anomalies from\nthe dataset, followed by several widely-used classiﬁcation\nmethods in order to compare the performance before and\nafter outlier removal. The result indicates that removing\noutliers could lead to statistically signiﬁcant improvements\nin the training quality as well as the classiﬁcation accuracy\nfor most of the cases.\nMusic datasets offer similar challenges to researchers in\nthe ﬁeld of MIR. Schedl et al. point out that many MIR stud-\nies require different datasets and annotations depending on\nthe task [22]. However, since the annotation of music data\nis complex and subjective, the quality of the annotations cre-\nated by human experts varies from dataset to dataset. This\ninaccuracy may potentially introduce errors to the system\nand decrease the resulting performance.\nOne MIR task known for this issue is Music Genre\nRecognition (MGR). According to Sturm [26], the most\nfrequently used dataset in MGR is GTZAN [29], and many\nof the existing systems are evaluated based on their perfor-\nmance on this dataset. Sturm points out that this dataset\ncontains corrupted ﬁles, repeated clips, and misclassiﬁed\ngenre labels. These are undesirable for the proper training\nand testing of a MGR system.\nTo address the problem of identifying such anomalies\nin music datasets, an investigation into existing outlier de-\ntection algorithms is a good starting point. The goal of\nthis paper is to assess the viability of state-of-the-art outlier\ndetection methods in the context of music data. The contri-\nbution of this paper can be summarized as follows: ﬁrst, this\nearly stage investigation provides a systematic assessment\nof different outlier detection algorithms applied to a music\ndataset. Second, the use of standard audio features reveals\nthe capability as well as the limitations of this feature repre-\nsentation for outlier detection. Third, we provide insights\nand future directions for related studies.\nThis paper is structured as follows. In Sect. 2, the re-\nlated work of outlier detection in music data is summarized.\nThe methods used in this paper, such as feature extraction\nand different outlier detection algorithms, are described in\nSect. 3. Section 4 presents the results and discusses the\nexperiments. Finally, the conclusion and future work are\ngiven in Sect. 5.1012. RELATED WORK\nOutlier detection methods are typically categorized into ﬁve\ngroups: (a) distance-based [14, 19], (b) density-based [3,\n16], (c) cluster-based [30], (d) classiﬁcation-based [8, 21],\nand (e) statistics-based [5, 6, 13, 18, 20, 28] methods.\nThe ﬁrst group ( distance-based ), proposed by Knorr\net al. [14], computes the distances between samples and\ndetects outliers by setting a distance threshold. Methods\nin this category are usually straightforward and efﬁcient,\nbut the accuracy is compromised when the data is sparse\nor unevenly distributed. The basic idea was extended by\ncombining the distance criterion with the k-nearest neigh-\nbor (KNN) based method [19], which adapts the distance\nthreshold by the k-nearest neighboring distances.\nThe second group ( density-based ) estimates the local\ndensities around the points of interest in order to determine\nthe outliers. Different variations use different methods to de-\ntermine the local density, for example, the local outlier fac-\ntor (LOF) [3] and the local correlation integral (LOCI) [16].\nThese approaches are popular and have been widely used\nin different ﬁelds.\nThe third group ( clustering-based ), as proposed in [30],\nﬁrst applies a clustering algorithm to the data, and then\nlabels the wrongly clustered instances as outliers.\nThe fourth group ( classiﬁcation-based ) assumes that the\ndesignation of anomalies can be learned by a classiﬁca-\ntion algorithm. Here, classiﬁcation models are applied to\nclassify the instances into inliers and outliers. This is ex-\nempliﬁed by Das et al. [8] with a one-class Support Vector\nMachine (SVM) based approach and Roth [21] with Kernel\nFisher Discriminants.\nThe ﬁfth group ( statistics-based ) assumes the data has\na speciﬁc underlying distribution, and the outliers can be\nidentiﬁed by ﬁnding the instances with low probability den-\nsities. A number of works apply the similar concept with\nvariations, including techniques based on the robust Maha-\nlanobis distance [20], direction density ratio estimation [13],\nand minimum covariance determinant estimator [6]. One of\nthe main challenges of these approaches is the reduction of\nmasking and swamping effects: outliers can bias the estima-\ntion of distribution parameters, yielding biased probability\ndensities. This effect could result in a biased detector iden-\ntifying normal instances as outliers, and outliers as normal,\nrespectively. Recent advances have generally focused on ap-\nplying robust statistics to outlier detection [5, 18, 28]. This\nis usually achieved by adopting a robust inference technique\nto keep the model unbiased from outliers in order to capture\nthe normal pattern correctly.\nAlthough the above mentioned approaches have been\napplied to different types of data, the number of studies\non music datasets is relatively small. Flexer et al. [10]\nproposed a novelty detection approach to automatically\nidentify new or unknown instances that are not covered\nby the training data. The method was tested on a MGR\ndataset with 22 genres and was shown to be effective in a\ncross-validation setting. However, in real-world scenarios,\nthe outliers are usually hidden in the dataset, and an outlier-\nfree training dataset may not be available. As a result, theproposed method might not be directly applicable to other\nmusic datasets. Hansen et al. [12] proposed the automatic\ndetection of anomalies with a supervised method based on\nparzen-window and kernel density estimation. The pro-\nposed algorithm was evaluated on a 4-class MGR dataset,\nwhich consisted of audio data recorded from radio stations.\nA commonly used set of audio features, the Mel Frequency\nCepstral Coefﬁcients (MFCCs), was extracted to represent\nthe music signals. This approach, nevertheless, has two\nunderlying problems. First, the dataset used for evaluation\ndoes not have a ground truth agreed on by human experts.\nSecond, while MFCCs are known to be useful in a multi-\ntude of MIR tasks, they might not be sufﬁcient to represent\nmusic signals for outlier detection tasks.\nTo address these issues, two approaches have been taken\nin this paper: First, for evaluation, a commonly-used MGR\ndataset with reliable ground truth is used. In Sturm’s analy-\nsis [26], a set of outliers (i.e., repeated, distorted, and misla-\nbeled music clips) were identiﬁed manually in the popular\nGTZAN [29] dataset. This analysis provides a solid ground\nfor the evaluation of an anomaly detection system in the\nMGR dataset. Second, we extend the set of descriptors for\nthe music data. In addition to the MFCCs, audio features\nthat are commonly used in MIR tasks are also extracted in\norder to evaluate the compatibility of current audio features\nwith the existing outlier detection methods.\n3. METHOD\n3.1 Feature Extraction\nFeature extraction is an important stage that transforms an\naudio signal into a vector-based representation for further\ndata analysis. In an early study of automatic music genre\nclassiﬁcation, Tzanetakis and Cook proposed three feature\nsets that characterized any given music signal based on its\ntimbral texture, rhythmic content and pitch content [29].\nThese features have shown their usefulness in music genre\nclassiﬁcation, and have been used in many music-related\ntasks. Although many studies presented more sophisticated\nfeatures (e.g., [11]) with higher classiﬁcation accuracy on\nthe GTZAN dataset, the original set of features still seem\nto provide a good starting point for representing music data.\nTherefore, a set of baseline features based on Tzanetakis\nand Cook’s features [29] is extracted to allow for easier\ncomparison with prior work. The extracted features can\nbe divided into three categories: spectral, temporal and\nrhythmic. All of the features are extracted using a block-\nwise analysis method. To begin with, the audio signal is\ndown-mixed to a mono signal. Next, a Short Time Fourier\nTransform (STFT) is performed using a block size of 23 ms\nand a hop size of 11 ms with a Hann window in order to\nobtain the time-frequency representation. Finally, differ-\nent instantaneous features are extracted from every block.\nSpectral features are computed using the spectrum of each\nblock. Temporal features are computed from the time do-\nmain signal of each block directly. The rhythmic features\nare extracted from the beat histogram of the entire time\ndomain signal. The extracted features are (for the details of\nthe implementations, see [15]):102 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20161.Spectral Features (d= 16 ): Spectral Centroid (SC),\nSpectral Roll-off (SR), Spectral Flux (SF), 13 Mel\nFrequency Cepstral Coefﬁcients (MFCCs)\n2.Temporal Features (d= 1): Zero Crossing Rate\n(ZCR)\n3.Rhythmic Features (d= 8): Period0 (P0), Ampli-\ntude0 (A0), RatioPeriod1 (RP1), Amplitude1 (A1),\nRatioPeriod2 (RP2), Amplitude2 (A2), RatioPeriod3\n(RP3), Amplitude3 (A3).\nAll of the features are aggregated into texture vectors\nfollowing the standard procedure as mentioned in [29]; the\nlength of the current texture block is 0.743 s. The mean\nand standard deviation of the feature vectors within this\ntime span will be computed to create a new feature vector.\nFinally, all the texture blocks will be summarized again by\nthe mean and standard deviation of these blocks, generating\none feature vector to represent each individual recording in\nthe dataset.\n3.2 Outlier Detection Methods\n3.2.1 Problem Deﬁnition\nGivenNmusic clips that have been converted into a set of\nfeature vectors X={X1,...,XN}with the corresponding\ngenre label Y={Y1,...,YN}, where each Ynis belong\nto one of the Mgenres (i.e., Yn∈ {C1,...,CM}), the\nobjective is to ﬁnd the indices of the abnormal instances\nwhich have an incorrect label Yn.\nFor this study, we choose 6 well-known outlier detection\nmethods from different categories as introduced in Sect. 2\nand compare their performances on a MGR dataset. The\nmethods are described in details in the following sections:\n3.2.2 Method 1: Clustering\nClustering is a cluster-based approach as described in\nSect. 2. In our implementation of this method, we apply\nk-means to cluster the data into 10 groups. Based on the\nassumption that normal data are near the cluster centroids\nwhile the outliers are not [7, 25], the anomalous score of a\ngiven instance is deﬁned by the distance between the point\nand the centroid of the majority within the same class.\n3.2.3 Method 2: KNN\nKNN method is a distance-based approach that typically\ndeﬁnes the anomalous score of each instance by its distance\nto theknearest neighbors [9]. It can be expressed in the\nfollowing equation:\nk–distance (P) =d(P,knn (P)) (1)\nwhereknn is the function that returns the k-th nearest neigh-\nbor of a point P, anddis the function that calculates the\ndistance between two points. Finally, we may compute the\noutlier score as:\n1\nk/summationdisplay\np∈neighbors k(P)k–distance (p) (2)Settingkto a larger number usually results in a model\nmore robust against outliers. When kis small, the anoma-\nlous score given by this method may be biased by a small\ngroup of outliers. In our implementation of this method,\nwe applyk= 6 in order to maintain a balance between\nrobustness and efﬁciency.\n3.2.4 Method 3: Local Outlier Factor\nThe Local Outlier Factor (LOF) [3] is a density-based ap-\nproach that extends the KNN method with a calculation of\nthe local densities of the instances. It is one of the most pop-\nular anomaly detection methods. It starts with the deﬁnition\nofk-reachability distance:\nk–reachDist (P,O) =max(k–distance (P),d(O,P))\n(3)\nThis represents the distance from OtoP, but not less\nthan thek–distance ofP. The local reachability density\nof a given sample is deﬁned by the inverse of the average\nlocal rechability distances of k-nearest neighbors:\nlrd(P) = 1//parenleftBigg/summationtext\nP0∈neighbors k(P)k–reachDist (P,P 0)\n|neighbors k(P)|/parenrightBigg\n(4)\nFinally, thelofcalculates the average ratio of the local\nreachability densities of the k-nearest neighbors against the\npointP:\nlof(P) =/summationtext\nP0∈neighbors k(P)lrd(P0)\nlrd(P)|neighbors k(P)|(5)\nIn a dataset that is densely distributed, a point may have\nshorter average distance to its neighbors, and vice versa.\nSince LOF uses the ratio instead of the distance as the\noutlier score, it is able to detect outliers in clusters with\ndifferent densities.\n3.2.5 Method 4: One-Class SVM\nThe One-Class SVM [23] is a classiﬁcation-based approach\nthat identiﬁes outliers with a binary classiﬁer. Given a genre\nm∈{1,...,M}, every sample in mcan be classiﬁed as in-\nclass or off-class, and the off-class instances are most likely\nto be the outliers. A One-Class SVM solves the following\nquadratic programming problem:\nmin\nw,ξ i,ρ1\n2||w||2+1\nνN/summationtext\niξi−ρ\nsubject to (wΦ(xi))≥ρ−ξi,i= 1...N\nξi≥0,i= 1...N (6)\nwhereξ,w, andρare the parameters to construct the sepa-\nration hyperplane, νis a parameter that serves as the upper\nbound fraction of outliers and a lower bound fraction of\nsamples used as support vectors, and Φis a function that\nmaps the data into an inner product space such that the\nprojected data can be modeled by some kernels such as\na Gaussian Radial Basis Function (RBF). By optimizing\nthe above objective function, a hyperplane is then created\nto separate the in-class instances from the outliers. In the\nexperiment, we construct a One-Class SVM for each of the\ngenres, and identify the music clips that are classiﬁed as\noff-class instances as outliers.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 1033.2.6 Method 5: Robust PCA\nRobust PCA [5] is a statistics-based approach that considers\nthe problem of decomposing a matrix Xinto the superposi-\ntion of a low-rank matrix L0and a sparse matrix S0, such\nthat:\nX=L0+S0\nThe problem can be solved by the convex optimization\nof the Principal Component Pursuit [5]:\nminimize||L||∗+λ||S||1 (7)\nsubject toL+S=X (8)\nwhere||L||∗is the nuclear norm of L, andλis the sparsity\nconstraint that determines how sparse Swould be.\nThe matrixS0is a sparse matrix consisting of mostly\nzero entries with a few non-zero entries being the outliers.\nIn the experiment, we apply this method to the music data\nand calculate the sparse matrices for every genre. Next,\nwe normalize the features using a standard Z-score nor-\nmalization process, and identify the outliers by ﬁnding the\ninstances with a maximum sparse matrix value that is 3\ntimes greater than the unity standard deviation.\n3.2.7 Method 6: Robust Categorical Regression\nRobust Categorial Regression (RCR) is another statistics-\nbased method that identiﬁes outliers based on a regression\nmodel. First, we formulate the relation of YandXbased\non a linear input-output assumption:\ng(Y) =Xβ+ε, (9)\nwheregis the categorical link function, βis the regres-\nsion coefﬁcient matrix, and εis a random variable that\nrepresents the white-noise vector of each instance. The link\nfunctiongis a logit function paired with a category CM, i.e.,\nln (P(Yn=Cm)/P(Yn=CM)) =Xnβm+εnm. Since\nthe probabilities of the categories will sum up to one, we\ncan derive the following modeling equation:\nP(Yn=Cm) =exp{Xnβm+εnm}\n1 +/summationtextM−1\nl=1exp{Xnβl+εnl}(10)\nand\nP(Yn=CM) =1\n1 +/summationtextM−1\nl=1exp{Xnβl+εnl}(11)\nThe coefﬁcient vector βusually represents the decision\nboundary in a classiﬁcation problem. In this approach, βis\nused to capture the normal behavior of the data.\nThe robust version of categorical regression applies a\nheavy-tailed distribution, which is a zero-mean Student-t\ndistribution to capture the error effect caused by outliers.\nThe solution to this regression model is approximated\nwith a variational Expectation-Maximization (EM) algo-\nrithm [2]. Once converged, the errors of the instances are\nexpected to be absorbed in the εvariables. Finally, the\noutliers can be identiﬁed by ﬁnding the instances with ε\nthat is 3 times greater than the unity standard deviation.4. EXPERIMENT\n4.1 Experiment Setup\nTo evaluate the state-of-the-art outlier detection methods as\ndescribed in Sect. 3.2, different experiments are conducted\non the well-known GTZAN dataset [29]. This dataset con-\nsists of 10 music genres (i.e., blues, classical, country, disco,\nhip-hop, jazz, metal, pop, reggae, and rock), with each genre\ncontaining 100 audio tracks; each track is a 30-second long\nexcerpt from a complete mixture of music. For each method,\ntwo sets of experiments are conducted.\nIn the ﬁrst set of experiments, we use a puriﬁed GTZAN\ndataset, which excludes the conspicuous misclassiﬁed and\njitter music clips reported in [26]. This setup simulates the\nbest case scenario, where the dataset is clean and all genres\nare well separated in the feature space. The results can\nserve as a sanity check of all the methods. Two types of\ninjection experiments are conducted on this puriﬁed dataset,\nnamely label injection and noise injection. The label in-\njection process is performed by randomly choosing 5% of\ninstances, and swapping their genre labels to create outliers.\nIn this experiment, two sets of features are used to represent\nthe music data, one is the full feature set as described in\nSect. 3.1, and the other is the baseline feature set using only\n13 MFCCs as reported in the work of Hansen et al. [12] for\ncomparison. The noise injection process is performed by\nrandomly choosing 5% of instances in the data, and shifting\n20% of their feature values by 5 times the standard devi-\nation. This experiment uses the full feature set to test the\nmethods’ capability of detecting corrupted data. For each of\nthe experiments above, we generate 10 random realizations\nand report the averaged evaluation results.\nIn the second set of experiments, we apply all the meth-\nods to the full GTZAN dataset directly, and the identiﬁed\noutliers are compared with the list of conspicuous genre\nlabels and the obviously corrupted clips ( Hip-hop (38), Pop\n(37), Reggae (86) ) reported in [26]. This experiment pro-\nvides the real-world scenario, in which case the outlier\ndetection should ﬁnd the outliers identiﬁed by human ex-\nperts.\nAll of the experiments use the same metrics for the per-\nformance measurements, which include the standard calcu-\nlation of Precision, Recall, F-measure, and the Area Under\nROC Curves (AUC).\n4.2 Experiment Results\nThe results of the ﬁrst set of experiments, evaluating the\nperformance of the methods on detecting injected misclassi-\nﬁcation labels with full features, are shown in Table 1. With\nF-measures in the range from 0.1–0.57, the results do not\nhave high reliability but are usable for some methods. The\nRobust Categorical Regression approach outperforms the\nother algorithms. Since RCR explicitly models the input-\noutput relationship between the features and the labels, it\nﬁts the data better compared to the other methods. Sur-\nprisingly, the simple methods such as Clustering andKNN\nalso perform relatively well in terms of AUC, and they out-\nperform the more sophisticated approaches such as LOF104 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Method Precision Recall F-measure AUC\nCLUS 0.23 0.23 0.23 0.74\nKNN 0.26 0.26 0.26 0.77\nLOF 0.11 0.11 0.11 0.57\nSVM 0.06 0.32 0.10 0.52\nRPCA 0.47 0.34 0.39 0.78\nRCR 0.59 0.55 0.57 0.91\nTable 1 . Average Detection Rate comparison of label injec-\ntion with full features\nMethod Precision Recall F-measure AUC\nCLUS 0.06 0.06 0.06 0.61\nKNN 0.10 0.10 0.10 0.71\nLOF 0.09 0.09 0.09 0.62\nSVM 0.05 0.38 0.09 0.50\nRPCA 0.30 0.20 0.24 0.65\nRCR 0.52 0.40 0.45 0.87\nTable 2 . Average Detection Rate comparison of label injec-\ntion with MFCCs only\nandOne-Class SVM . One possible explanation is that the\nlabel injection datasets contain outliers generated by swap-\nping dissimilar genres, e.g., swapping the label from Jazz\nto Metal. As a result, the decision boundaries of LOF and\nOne-Class SVM might be biased towards the extreme values\nand perform poorly. On the other hand, the simple methods\nsuch as Clustering andKNN , which are based on Euclidean\ndistance, were able to identify these instances without being\nbiased. Generally speaking, the statistics-based approaches,\nsuch as the robust statistics based methods RPCA andRo-\nbust Categorical Regression , perform better on the label\ninjection datasets.\nThe results of the same experiments with only MFCCs as\nfeatures are shown in Table 2. In general, the performance\ndrops drastically. This result implies that MFCCs might not\nbe representative enough for the outlier detection task.\nTable 3 shows the results for the noise injection experi-\nment. It can be observed that density and distance methods,\nsuch as CLUS ,KNN , and LOF , have better results on de-\ntecting corrected data. The main distinction of this kind of\noutlier is that the abnormal behavior is explicitly shown in\nthe feature space instead of implicitly embedded in the rela-\ntionship between genre labels and the features. Therefore,\nthe methods that directly detect outliers in the feature space\ntend to outperform the other methods such as SVM ,RCR\nandRPCA .\nIn the second set of experiments, we perform the\nanomaly detection on the complete GTZAN dataset with\nfull features, and aim to detect the misclassiﬁed music clips\nreported by Sturm [26]. The experiment result is shown in\nTable 4. Based on these metrics, none of these methods are\nable to detect the anomalies with high accuracy. Compared\nMethod Precision Recall F-measure AUC\nCLUS 0.92 0.90 0.91 0.99\nKNN 0.99 0.98 0.99 1.00\nLOF 1.00 0.98 0.99 1.00\nSVM 0.05 0.41 0.09 0.50\nRPCA 0.32 0.23 0.27 0.72\nRCR 0.61 0.50 0.55 0.75\nTable 3 . Average Detection Rate comparison of noise in-\njection with full featuresMethod Precision Recall F-measure AUC\nCLUS 0.15 0.13 0.14 0.54\nKNN 0.18 0.15 0.16 0.56\nLOF 0.18 0.15 0.16 0.59\nSVM 0.09 0.63 0.15 0.66\nRPCA 0.08 0.09 0.08 0.51\nRCR 0.17 0.22 0.19 0.60\nTable 4 . Detection Rate comparison on GTZAN detecting\nSturm’s anomalies with full features\nto the other methods, SVM andRCR present AUCs that\nare relatively higher, however, the Precision, Recall and\nF-measures are still too low to be applicable in real-world\nscenarios. The One-Class SVM method performs better in\nthis experiment than it does in the previous experiment. We\nspeculated that in the case of injection, the model is biased\nby the extreme values introduced by the injected outliers. In\nthe real-world scenario, however, the differences between\nthe outliers and the non-outliers are relatively subtle. When\nOne-Class SVM expands its in-class region moderately, it\nlearns a better decision boundary. Therefore, it has a better\ncapability of detecting the outliers.\nIt can be observed that both statistics-based approaches,\nRPCA andRCR , do not perform well compared to the re-\nsults of the previous experiment. Since these methods are\ngood at capturing extreme values and prevent the model\nfrom being biased by the outliers, they are relatively weak\nin differentiating subtle differences in the feature space.\nTherefore, the resulting performances are not ideal.\n4.3 Discussion\nTo further reveal the relationship between different methods\nand outliers from different genres, we list the distribution\nof top 20 true and false outliers ranked by the anomalous\nscores of different methods as well as the true distribution\nreported by Sturm [26]. The results are shown in Table\n5. Interestingly, majority of the approaches have most of\nthe true outliers in Disco andReggae except the One-Class\nSVM . For the One-Class SVM , its top 20 includes 14 metal\noutliers, which are barely detected by the other methods.\nMore speciﬁcally, the One-Class SVM had a high precision\nof 14/26 in the Metal genre. Since most of the true out-\nliers in the Metal genre can be categorized to punk rock\naccording to the deﬁnition on the online music library,1\nthey could exhibit similar features with subtle differences\nin the feature space, and they are still detected by the One-\nClass SVM . InReggae , there is a jitter music clip which\npresents extreme values in the feature space, along with the\nother outliers. For the One-Class SVM in the context of Reg-\ngae, however, only the jitter instance is captured while the\nother outliers are missing. These two observations conﬁrm\nthatOne-Class SVM is especially good at distinguishing\nthe outliers that have subtle differences, and could be easily\nbiased by the outliers with extreme values.\nThree of methods have about 10 of the top 20 false out-\nliers in Pop. This may due to the variety of Popmusic in the\ndataset. For example, although Pop (12) - Aretha Franklin,\nCeline Dion, Mariah Carey, Shania Twain, and Gloria Es-\n1AllMusic: http://www.allmusic.com/Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 105True CLUS KNN LOF SVM RPCA RCR\nBlues 0 0/2 0/0 0/0 0/0 0/4 0/0\nClassical 0 0/0 0/3 0/0 0/0 0/1 0/0\nCountry 4 2/0 2/0 3/0 0/0 2/0 1/2\nDisco 7 5/0 5/0 2/0 0/0 4/0 5/2\nHip-hop 3 1/0 3/3 2/2 3/5 1/1 2/3\nJazz 2 0/7 1/6 1/5 0/0 0/5 2/0\nMetal 17 1/0 2/0 5/1 14/0 2/2 2/1\nPop 4 4/10 2/2 2/11 2/8 3/3 2/0\nReggae 7 7/1 5/3 5/1 1/5 7/4 5/2\nRock 2 0/0 0/3 0/0 0/2 1/0 1/10\nTable 5 . Distribution of the Top 20 Ranked True Out-\nliers/False Outliers among Methods.\ntefan ”You Make Me Feel Like A Natural Woman” is not\nidentiﬁed by the expert as an outlier, it can be argued to be\nSoul music. By the nature of the One-Class SVM , this clip\nis also ranked at the top by its anomalous score. Another in-\nteresting observation is that four methods have about 5 jazz\ninstances in the top 20 false outliers. Although jazzmusic\nhas strong characteristics and can be easily distinguished by\nhumans, it shows the most signiﬁcant average variance on\nits features comparing with other genres. Thus, the methods\nthat calculate the Euclidean distances such as Clustering ,\nKNN , and LOF , and the approaches that absorb variances\nas errors such as RPCA could report more false outliers\nin this circumstance. We also noticed that RCR includes\n10Rock false outliers in its top 20. This may because it\nmodels the input-output relationship among all genres, and\nthis global-view property thus causes the model mixing up\nRock with other partially overlapping genres such as Metal\nandBlues .\nTo summarize, outlier detection in music data faces the\nfollowing challenges compared to other types of data: ﬁrst,\ndue to the ambiguity in the genre deﬁnitions, some of the\ntracks can be considered as both outliers and non-outliers.\nThis inconsistency may impact the training and testing re-\nsults for both supervised and unsupervised approaches. In\nSturm’s [27] work, a risk model is proposed to model the\nloss of misclassiﬁcation by the similarity of genres. Second,\nthe music data has temporal dependencies. In the current\nframework, we aggregate the block-wise feature matrix into\na single feature vector as it allows for the immediate use in\nthe context of the state-of-the-art methods. This approach,\nhowever, does not keep the temporal changes of the mu-\nsic signals and potentially discards important information\nfor identifying outliers with subtle differences. Third, the\nextracted low-level features might not be able to capture\nthe high-level concept of music genre, therefore, it is difﬁ-\ncult for the outlier detection algorithms to ﬁnd the outliers\nagreed on by the human experts. Finally, the outliers are un-\nevenly distributed among genres (e.g., Metal has 16 while\nBlues andClassical have none), and the data points are also\ndistributed differently in the feature space in each genre.\nAn approach or a speciﬁc parameter setting may perform\nwell on some of the genres and fail on others.\n5. CONCLUSION\nIn this paper, we have presented the application of outlier\ndetection methods on a music dataset. Six state-of-the-artapproaches have been investigated in the context of music\ngenre recognition, and their performance is evaluated based\non their capability of ﬁnding the outliers identiﬁed by hu-\nman experts [26]. The results show that all of the methods\nfail to identify the outliers with reasonably high accuracy.\nThis leaves room for future improvement in the automatic\ndetection of outliers in music data. The experiment results\nalso reveal the main challenges for outlier detection in mu-\nsic genre recognition: ﬁrst, genre deﬁnitions are usually\nsubjective and ambiguous. Second, the temporal depen-\ndencies of music need to be modeled. Third, the low-level\naudio features might not be able to capture the high-level\nconcepts. These challenges may also generalize to other\nmusic datasets, and they should be further addressed in\nfuture work.\nWe identify possible directions for future work as: First,\nas shown in the experiments, a better feature representation\nshould lead to a better performance for the majority of the\nmethods. Therefore, to robustly isolate outliers, a better\nfeature representation for outlier detection algorithms seems\nto be necessary. Second, since music data has temporal\ndependencies, the static approach in the current framework\nmight not be feasible. An outlier detection method that\ncan handle the temporal dependencies could potentially\nshow improved performance Third, in the top 20 list for\ndifferent methods, it is shown that different methods could\nbe sensitive to different types of outliers. An ensemble\napproach that takes advantage of multiple methods might\nbe considered in future studies.\nWith our results, we have shown that outlier detection\nin music datasets is still at a very early stage. To fully\ncharacterize a music signal, many challenges and questions\nstill need to be answered. With current advances in feature\ndesign and feature learning, however, we expect signiﬁcant\nprogress to be made in the near future.\n6. REFERENCES\n[1]Mikhail Atallah, Wojciech Szpankowski, and Robert\nGwadera. Detection of signiﬁcant sets of episodes in\nevent sequences. In Data Mining, 2004. ICDM ’04.\nFourth IEEE International Conference on , pages 3–10,\nNov 2004.\n[2]Christopher M. Bishop. Pattern Recognition and Ma-\nchine Learning (Information Science and Statistics) .\nSpringer-Verlag New York, Inc., Secaucus, NJ, USA,\n2006.\n[3]Markus M. Breunig, Hans-Peter Kriegel, Raymond T.\nNg, and J ¨org Sander. Lof: identifying density-based\nlocal outliers. SIGMOD Record , 29(2):93–104, May\n2000.\n[4]Patrick L. Brockett, Xiaohua Xia, and Richard A. Der-\nrig. Using kohonen’s self-organizing feature map to\nuncover automobile bodily injury claims fraud. The\nJournal of Risk and Insurance , 65(2):245–274, 1998.\n[5]Emmanuel J. Cand `es, Xiaodong Li, Yi Ma, and John\nWright. Robust principal component analysis? Journal\nof the ACM , 58(3):11:1–11:37, June 2011.106 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016[6]Andrea Cerioli. Multivariate outlier detection with high-\nbreakdown estimators. Journal of the American Statisti-\ncal Association , 105(489):147–156, 2009.\n[7]Varun Chandola, Arindam Banerjee, and Vipin Kumar.\nAnomaly detection: A survey. ACM Computer Survey ,\n41(3):15:1–15:58, July 2009.\n[8]Santanu Das, Bryan L. Matthews, Ashok N. Srivastava,\nand Nikunj C. Oza. Multiple kernel learning for het-\nerogeneous anomaly detection: algorithm and aviation\nsafety case study. In KDD 10’: 16th ACM SIGKDD\ninternational conference on Knowledge discovery and\ndata mining , pages 47–56, 2010.\n[9]Eleazar Eskin, Andrew Arnold, Michael Prerau, Leonid\nPortnoy, and Sal Stolfo. A geometric framework for\nunsupervised anomaly detection. In Daniel Barbar ´a and\nSushil Jajodia, editors, Applications of Data Mining in\nComputer Security , pages 77–101. Springer US, Boston,\nMA, 2002.\n[10] Arthur Flexer, Elias Pampalk Gerhard, and Gerhard\nWidmer. Novelty detection based on spectral similar-\nity of songs arthur ﬂexer. In in Proc. of the 6 th Int.\nSymposium on Music Information Retrieval . Ms, 2005.\n[11] Zhouyu Fu, Guojun Lu, Kai Ming Ting, and Dengsheng\nZhang. A Survey of Audio-Based Music Classiﬁca-\ntion and Annotation. IEEE Transactions on Multimedia ,\n13(2):303–319, April 2011.\n[12] Lars Kai Hansen, Tue Lehn-Schiler, Kaare Brandt Pe-\ntersen, Jeronimo Arenas-Garcia, Jan Larsen, and Sren\nHoldt Jensen. Learning and clean-up in a large scale\nmusic database. In European Signal Processing Confer-\nence (EUSIPCO) , pages 946–950, 2007.\n[13] Shohei Hido, Yuta Tsuboi, Hisashi Kashima, Masashi\nSugiyama, and Takafumi Kanamori. Statistical outlier\ndetection using direct density ratio estimation. Knowl-\nedge Information Systems , 2011.\n[14] Edwin M. Knorr, Raymond T. Ng, and Vladimir Tu-\ncakov. Distance-based outliers: algorithms and applica-\ntions. The VLDB Journal , 8(3–4):237–253, 2000.\n[15] Alexander Lerch. An Introduction to Audio Content\nAnalysis: Applications in Signal Processing and Music\nInformatics . John Wiley and Sons, 2012.\n[16] Spiros Papadimitriou, Hiroyuki Kitagawa, Christos\nFaloutsos, and Phillip B. Gibbons. Loci: fast outlier de-\ntection using the local correlation integral. In Data En-\ngineering, 2003. Proceedings. 19th International Con-\nference on , pages 315–326, March 2003.\n[17] Eun Park, Shawn Turner, and Clifford Spiegelman. Em-\npirical approaches to outlier detection in intelligent\ntransportation systems data. Transportation Research\nRecord: Journal of the Transportation Research Board ,\n1840:21–30, 2003.\n[18] Cludia Pascoal, M. Rosrio Oliveira, Antnio Pacheco,\nand Rui Valadas. Detection of outliers using ro-\nbust principal component analysis: A simulation\nstudy. In Christian Borgelt, Gil Gonz ´alez-Rodr ´ıguez,Wolfgang Trutschnig, Mar ´ıa Asunci ´on Lubiano,\nMar´ıa´Angeles Gil, Przemysław Grzegorzewski, and Ol-\ngierd Hryniewicz, editors, Combining Soft Computing\nand Statistical Methods in Data Analysis , pages 499–\n507. Springer Berlin Heidelberg, Berlin, Heidelberg,\n2010.\n[19] Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok\nShim. Efﬁcient algorithms for mining outliers from\nlarge data sets. SIGMOD Record , 29(2):427–438, May\n2000.\n[20] Marco Riani, Anthony C. Atkinson, and Andrea Cerioli.\nFinding an unknown number of multivariate outliers.\nJournal of the Royal Stats Society Series B , 71(2):447–\n466, 2009.\n[21] V olker Roth. Outlier detection with one-class kernel\nﬁsher discriminants. In Advances in Neural Information\nProcessing Systems 17 , pages 1169–1176, 2005.\n[22] Markus Schedl, Emilia G ´omez, and Juli ´an Urbano. Mu-\nsic Information Retrieval: Recent Developments and\nApplications. Foundations and Trends in Information\nRetrieval , 8:127–261, 2014.\n[23] Bernhard Sch ¨olkopf, John C. Platt, John C. Shawe-\nTaylor, Alex J. Smola, and Robert C. Williamson. Esti-\nmating the support of a high-dimensional distribution.\nNeural Comput. , 13(7):1443–1471, July 2001.\n[24] Michael R. Smith and Tony Martinez. Improving classi-\nﬁcation accuracy by identifying and removing instances\nthat should be misclassiﬁed. In Proceedings of Inter-\nnational Joint Conference on Neural Networks , pages\n2690–2697, 2011.\n[25] Raheda Smith, Alan Bivens, Mark Embrechts, Chan-\ndrika Palagiri, and Boleslaw Szymanski. Clustering\napproaches for anomaly based intrusion detection. In\nProceedings of intelligent engineering systems through\nartiﬁcial neural networks , 2002.\n[26] Bob L. Sturm. An analysis of the GTZAN music genre\ndataset. In Proceedings of the second international ACM\nworkshop on Music Information Retrieval with user-\ncentered and multimodal strategies (MIRUM) , 2012.\n[27] Bob L. Sturm. Music genre recognition with risk and\nrejection. In 2013 IEEE International Conference on\nMultimedia and Expo (ICME) , pages 1–6, July 2013.\n[28] David E. Tyler. Robust statistics: Theory and meth-\nods. Journal of the American Statistical Association ,\n103:888–889, 2008.\n[29] George Tzanetakis and Perry Cook. Musical genre clas-\nsiﬁcation of audio signals. IEEE Transactions on Speech\nand Audio Processing , 10(5):293–302, 2002.\n[30] Dantong Yu, Gholam Sheikholeslami, and Aidong\nZhang. Findout: Finding outliers in very large datasets.\nTechnical report, Dept. of CSE SUNY Buffalo, 1999.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 107"
    },
    {
        "title": "A Plan for Sustainable MIR Evaluation.",
        "author": [
            "Brian McFee",
            "Eric J. Humphrey",
            "Julián Urbano"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417775",
        "url": "https://doi.org/10.5281/zenodo.1417775",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/257_Paper.pdf",
        "abstract": "The Music Information Retrieval Evaluation eXchange (MIREX) is a valuable community service, having es- tablished standard datasets, metrics, baselines, method- ologies, and infrastructure for comparing MIR methods. While MIREX has managed to successfully maintain op- erations for over a decade, its long-term sustainability is at risk. The imposed constraint that input data cannot be made freely available to participants necessitates that all algorithms run on centralized computational resources, which are administered by a limited number of people. This incurs an approximately linear cost with the number of submissions, exacting significant tolls on both human and financial resources, such that the current paradigm be- comes less tenable as participation increases. To alleviate the recurring costs of future evaluation campaigns, we pro- pose a distributed, community-centric paradigm for system evaluation, built upon the principles of openness, trans- parency, reproducibility, and incremental evaluation. We argue that this proposal has the potential to reduce oper- ating costs to sustainable levels. Moreover, the proposed paradigm would improve scalability, and eventually result in the release of large, open datasets for improving both MIR techniques and evaluation methods.",
        "zenodo_id": 1417775,
        "dblp_key": "conf/ismir/McFeeHU16",
        "content": "A PLAN FOR SUSTAINABLE MIR EV ALUATION\nBrian McFee\nCenter for Data Science / MARL\nNew York University\nbrian.mcfee@nyu.eduEric J. Humphrey\nSpotify, Ltd.\nejhumphrey@spotify.comJuli´an Urbano\nMusic Technology Group\nUniversitat Pompeu Fabra\nurbano.julian@gmail.com\nABSTRACT\nThe Music Information Retrieval Evaluation eXchange\n(MIREX) is a valuable community service, having es-\ntablished standard datasets, metrics, baselines, method-\nologies, and infrastructure for comparing MIR methods.\nWhile MIREX has managed to successfully maintain op-\nerations for over a decade, its long-term sustainability is\nat risk. The imposed constraint that input data cannot\nbe made freely available to participants necessitates that\nall algorithms run on centralized computational resources,\nwhich are administered by a limited number of people.\nThis incurs an approximately linear cost with the number\nof submissions, exacting signiﬁcant tolls on both human\nand ﬁnancial resources, such that the current paradigm be-\ncomes lesstenable as participation increases. To alleviate\nthe recurring costs of future evaluation campaigns, we pro-\npose a distributed, community-centric paradigm for system\nevaluation, built upon the principles of openness, trans-\nparency, reproducibility, and incremental evaluation. We\nargue that this proposal has the potential to reduce oper-\nating costs to sustainable levels. Moreover, the proposed\nparadigm would improve scalability, and eventually result\nin the release of large, open datasets for improving both\nMIR techniques and evaluation methods.\n1. INTRODUCTION\nEvaluation plays a central role in the development of music\ninformation retrieval (MIR) systems. In addition to em-\npirical results reported in individual articles, community\nevaluation campaigns like MIREX [6] provide a mecha-\nnism to standardize methodology, datasets, and metrics to\nbenchmark research systems. MIREX has earned a spe-\ncial role within the MIR community as the central forum\nfor system benchmarking. However, the annual operating\ncosts incurred by MIREX are unsustainable by the MIR\ncommunity. Much of these costs derive from one-time ex-\npenditures — e.g., the time spent getting a participant’s\nalgorithm to run — which primarily beneﬁt individual par-\nticipants, but not the MIR community at large. If we, as\na community, are to continue hosting regular evaluation\nc/circlecopyrtBrian McFee, Eric J. Humphrey, Juli ´an Urbano. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Brian McFee, Eric J. Humphrey, Juli ´an\nUrbano. “A Plan for Sustainable MIR Evaluation”, 17th International\nSociety for Music Information Retrieval Conference, 2016.campaigns, we will soon require a more efﬁcient and sus-\ntainable model.\nThe evaluation problem has lurked the MIR community\nfor years. The MIReS Roadmap for Music Information Re-\nSearch identiﬁed it as one of the main technical-scientiﬁc\ngrand challenges in MIR research [20], and during the\nISMIR 2012 conference a discussion panel was held to\nexplicitly address this issue [17]. Previous research has\ndiscussed some limitations of MIREX-like evaluation and\nmade general proposals to avoid them [21, 23], and other\ncommunity-led platforms have been put forward to try to\nminimize them in practice, most notably MusiClef [14]\nand the MSD Challenge [11]. However, for different rea-\nsons, they have been unable to continue operating.\nReﬂecting upon the prior work, we propose in this ar-\nticle a sustainable, open framework for community-driven\nMIR evaluation campaigns. Our proposal is motivated by\nthree complementary factors. First, we strive to reduce the\ncost of running and maintaining the evaluation framework.\nSecond, we hope to improve transparency and openness\nwherever possible. Third, we plan to establish a sustain-\nable framework that will produce open, public data sets\nconsisting of both inputs and reference annotations. By di-\nrecting the majority of resources toward the production of\nopen data, the proposed framework will be of value to the\ngreater MIR community in perpetuity, and beneﬁts will not\nbe limited to participants in a particular year’s campaign.\nWe stress that this document describes not a fully imple-\nmented framework, but a speciﬁc proposal put forward by\na group of authors dedicated to seeing it put into practice.\nOur goal in writing this document at this early stage is to\nsolicit input from the MIR community before implementa-\ntion details have been ﬁnalized. In collaboration with the\ncommunity, we hope to develop a framework that beneﬁts\nas many people as possible and requires minimal ﬁnancial\nsupport for years to come.\n2. MIREX\nThe Music Information Retrieval Evaluation eXchange is\na framework for the community driven evaluation of MIR\nalgorithms [6, 7]. The annual tradition of MIREX was es-\ntablished early in the lifetime of ISMIR, and drew signif-\nicant inspiration from the well-established TREC frame-\nwork [24]. Thanks in large part to the vision of MIR pio-\nneers, the ﬁrst ofﬁcial iteration took place at ISMIR 2005\nafter much preliminary work, including a trial run the year2852005 2006 2007 2008 2009 2010 2011 2012 2013\nMIREX Year50100150200250300350Algorithm Runs\nFigure 1 . Number of algorithms run at MIREX over the\ncourse of almost a decade.\nprior called the Audio Description Contest (ADC). The\npracticalities of MIREX are hosted by the IMIRSEL group\nat UIUC, and the organization has successfully earned\nmultiple grants to jump-start the evaluation effort at IS-\nMIR.\nAt a high level, MIREX operates in the following steps:\n1. Identify some task of interest, e.g., chord estimation.\n2. Formulate the problem and evaluation metrics.\n3. Construct (and annotate) a corpus of data.\n4. Release a subset of the data for development pur-\nposes; retain the rest as private data for evaluation.\n5. Invite participants to submit system implementa-\ntions, which then are executed on private servers.\n6. Evaluate predicted outputs against reference annota-\ntions or human judgments.\n7. Repeat steps 5–6 annually. Intermittently revisit step\n2 if needed, and steps 3 and 4 if possible.\nImportantly, this approach differs from TREC-style\nevaluation by operating in an “algorithm-to-data” model,\nwhere facilitators oversee the application of code submis-\nsions to privately held data, rather than participants sub-\nmitting predictions over a freely available dataset. The\nrationale for this decision is understandable. In contrast\nto other machine perception domains, such as natural lan-\nguage processing, speech recognition, or computer vision,\nintellectual property and copyright law imposes stiff penal-\nties on the illegal distribution of recorded music. Due to a\nhistory of litigation from the recording industry, there is a\npervasive sense of fear in the MIR community that sharing\ncopyrighted audio data would invite crippling lawsuits [6].\nHowever, experience with MIREX over the last decade\nhas demonstrated that bringing algorithms to data entails\nfundamental limitations. First, as a matter of practical-\nity, doing so incurs steep computational and ﬁnancial costs\nthat the community cannot hope to sustain indeﬁnitely.\nRunning hundreds of research systems demands signiﬁcant\ncomputational resources, which must be either rented or\npurchased outright. More often than not, these systems are\nresearch prototypes which require substantial, manual in-\ntervention to operate correctly, and are seldom optimized\nfor efﬁciency or ease-of-use. While task-dependent run-\ntime limits are placed on algorithm execution (between 6\nand 72 hours), MIREX requires months, if not years, of\nannual compute-time.\nInput\nReference\nModel\nAnnotator\nPrediction\nMetrics\nScores\nStatistics\nCollectionFigure 2 . Diagram of the standard approach to the bench-\nmarking of MIR systems.\nThe ﬁnancial burden of computation can be negligible\nin comparison to the requisite human effort. As a point\nof reference, MIREX 2007 alone required “nearly 1000\nperson-hours” to supervise the execution of 122 algorithms\nfrom 40 teams [6]. As illustrated in Figure 1, the number\nof algorithm runs at MIREX has nearly tripled in the years\nsince.1As a rough estimate, the last decade has likely\nconsumed on the order of 10,000 person-hours just bring-\ning algorithms to data. Not only is this rate unsustainable,\nbut the combined operating costs only increase with partic-\nipation. Said differently, the worst thing that could happen\nto MIREX in its current form is growth.\nOperating costs aside, MIREX has indeed produced\nvaluable insights into the development of MIR systems [6].\nUnfortunately, many scientiﬁc endeavors are largely im-\npeded or, at worst, wholly obfuscated in the current\nparadigm. To illustrate, consider the standard approach to\nbenchmarking MIR systems as depicted in Figure 2. An\ninput,Xi, is observed by an annotator, A, who produces a\nreference output, Yi. Similarly, a system Foperates on the\nsame input, and produces an estimate /hatwideYi. Each of several\nperformance metrics, Mj, are applied to these two repre-\nsentations, yielding a number of performance scores, Si,j\nThis process is repeated over a sample of ninput-output\npairs, {Xi,Yi} ∈ D , and the sample-wise scores are ag-\ngregated into summary statistics, µ, the reliability of which\ngenerally increases with n=|D|.\nIn the current MIREX formulation, a lack of trans-\nparency renders participants scientiﬁcally blind in a num-\nber of ways [23]. First, there is no direct access to the\nreference annotations Yi, and, in most cases, no access to\nthe inputXieither. Furthermore, /hatwideYiis of little use with-\noutXifor context. This makes it exceedingly difﬁcult to\nlearn from the results of the evaluation. Without access to\nthe underlying data, how can one diagnose the cause of an\nerroneous estimate, or discover avenues for improvement?\nSimilarly, there is no way to gauge the distribution of the\ndata or estimate any bias in the sampling, beyond what may\nbe inferred from public development sets.\nThe behavior of the human annotators is also obscured,\nas are the instructions provided when the annotation was\ninitially performed [16]. Consequently, the problem for-\nmulation itself is effectively hidden, and subject to drift\nover time. For the sake of visibility into the evaluation\nmetrics Mj, the original NEMA infrastructure is open\n1https://www.hathitrust.org/documents/mirex_\nhtrc_same_rda.pdf286 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016source, and an ongoing community-led effort continues to\nstandardize and improve upon these implementations [18].\nStill, without access to the data, it is exceedingly difﬁcult\nto perform meta-evaluation , like comparing new metrics\non old data, without seeking privileged access to the his-\ntorical records.\nFinally, it is only fair to admit that large scale evaluation\nis a considerable undertaking with plenty of room for error\nand misinterpretations. Conducting these campaigns in the\nopen makes it easier to detect and diagnose any missteps.\nDue to the issues highlighted above, resources that\ncould have been devoted to constructing and enlarging\nopen data sets have instead been absorbed by irrecover-\nable operating costs. This is not only detrimental to system\ndevelopment, but also to evaluation, because it is critical\nto routinely refresh the evaluation data to reduce bias and\nprevent statistical over-ﬁtting. Without a fresh source of\nannotated data, early concerns about the community even-\ntually over-ﬁtting to benchmark datasets are beginning to\nmanifest.\nIn some cases, this is because the data used for evalu-\nation exists in the wild: one submission in 2011 achieved\nnearly perfect scores in chord estimation due to being pre-\ntrained on the test data.2In other cases, participants may\nmis-interpret the task guidelines, as evidenced by submis-\nsions of ofﬂine systems for tasks that are online, or by oth-\ners that mistakenly use annotations from previous folds in a\ntrain-test task. Across the board, hidden evaluation data is\nslowly being over-ﬁt by trial and error, as teams implicitly\nleverage the weak error signal across campaigns to “im-\nprove” systems. These results can be misleading due to the\nﬁxed but unknown biases in the data distribution, which\nbecome apparent when datasets are expanded — like the\nintroduction of the Billboard dataset to the chord estima-\ntion task in 2012 [2] — or as further insight is gained, as\nin the disclosure of the meta-data in the music similarity\ncorpus.\nTo make matters worse, there is no feasible plan in\nplace to replenish the evaluation datasets currently used by\nMIREX, nor any long-term plans to replace that data when\nit inevitably becomes stale. MIREX has primarily relied\nupon the generosity of the community to both curate and\ndonate data for the purposes of evaluation. This approach\nalso struggles with transparency, making it susceptible to\nissues of methodology and problem formulation, and can\nhardly be relied upon as a regular resource. Data collection\nis a challenging, unavoidable reality that demands a viable\nplan going forward.\nAfter a decade of MIREX, we have learned which tech-\nniques work and which do not. Most importantly, we\nhave learned the importance of establishing a collective\nendeavor to periodically and systematically evaluate MIR\nsystems. Unfortunately, we have also learned of the bur-\ndens entailed by such an initiative, and the limitations of\nits current form.\n2http://nema.lis.illinois.edu/nema_out/\nmirex2011/results/ace/nmsd2.html3. OPEN EV ALUATION OF MIR SYSTEMS\nSummarizing the previous section, MIREX suffers from\nthree deﬁciencies that render the situation untenable:\n(i) the ﬁnancial and labor costs cannot be sustained indeﬁ-\nnitely, (ii) the lack of data transparency limits the scientiﬁc\nvalue of the endeavor, (iii) the lack of a strategy for obtain-\ning and annotating new data.\nThus, to address these deﬁciencies, our proposed plan\nhas three key differentiators from the MIREX model:\n(i) Distributed computation reduces operating costs to\nscale favorably with increased participation.\n(ii) Freely distributable audio facilitates reproducibility\nand beneﬁts the entire community.\n(iii) Incremental evaluation reduces bias by keeping test\ndata fresh, and provides a feasible strategy for col-\nlecting new data.\n3.1 Distributed computation\nThe primary difﬁculty in running an evaluation campaign\nis computing the outputs of all participating systems. This\ndifﬁculty stems from two sources. First is the obvious\ncomputational complexity of running msubmissions over\nninputs. Second is the less obvious “human” complex-\nity entailed in the task captains successfully executing the\nsubmitted programs in a foreign environment. While the\ncomputational issues can be ameliorated by running sys-\ntems in parallel over multiple machines, the cost in human\neffort has no easy solution in the MIREX framework.\nAn alternative to this framework is exempliﬁed by Kag-\ngle.3Kaggle competitions are conducted with all input\ndata publicly available, and participants submit only the\noutputs of their systems, e.g., predictions made for each\ndata point. This paradigm effectively resolves the difﬁ-\nculties listed above: both computation and human effort\nare distributed to the participants, rather than centralized\nat the evaluation site and task captains. This dramatically\nreduces the cost of maintenance and administration. How-\never, we note a few potential challenges inherent to bring-\ning data to the computation.\nFirst, the input data must be made openly available to\nparticipants. This may increase the risk of bias if partici-\npants (unintentionally) tune their systems to the evaluation\nset. To mitigate bias, we propose the use of a large and di-\nverse corpus of common tracks which are shared across\nall tasks , rather than a collection of small, task-speciﬁc\ndatasets as is done in MIREX. For any given task, only\na subset of the data need to be considered when compar-\ning systems, and the evaluation set may be independently\nselected for each task. The knowledge of which items com-\nprise a given evaluation subset would remain hidden from\nparticipants at submission time. This implies that each sub-\nmission must span the entire corpus, reducing the feasibil-\nity of participants tuning their algorithms to a particular\nsubset. Moreover, while this requirement increases com-\nputational overhead for the participant, it results in a large\n3http://kaggle.comProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 287collection of outputs for various methods on a common\ncorpus, which is a valuable data source in its own right.\nSecond, distributed computation entails its own chal-\nlenges with respect to transparency. While MIREX re-\nquires submissions to execute on a remote server beyond\nthe participants’ control, the scheme proposed here drops\nthat requirement in favor of visibility into system inputs.\nConsequently, restrictions on the implementation ( e.g.,\nrunning time) would become infeasible, and it may open\nthe possibility of cheating by manual participant interven-\ntion. Using a large corpus with opaque evaluation sets will\nlimit the practicality of this approach.4Obscuring which\nitems belong to the evaluation subset comes at the expense\nof sample-wise measures, however, as doing so would re-\nveal the partition. This is not inherently problematic if\ndone following the completion of a campaign (instead of\npowering a continuous leader-board), but would require\nchanging the evaluation set annually. These are minor con-\ncessions, and we argue that open data beneﬁts the commu-\nnity at large, since its availability will serve a broader set\nof interests over time.\nFinally, the proposed scheme assumes that multiple\ntasks operate on a common input form, i.e., recorded au-\ndio. While the majority of current MIREX tasks do ﬁt into\nthis framework, at present it precludes those which require\nadditional input data (score following, singing voice sep-\naration) or user interaction (grand challenges, query-by-\nhumming/tapping). Our long-term plan is to devise ways\nof incorporating these tasks as well, while keeping with\nthe principles outlined above. This is of course an open\nquestion for further research.\n3.2 Open content\nFor the distributed computation model to be practical, we\nﬁrst need a source of diverse and freely distributable audio\ncontent. This is signiﬁcantly easier to acquire now than\nwhen MIREX began, and in particular, a wealth of data\ncan be obtained from services like Jamendo5and the Free\nMusic Archive (FMA).6Both of these sites host a variety\nof music content under either public domain or Creative\nCommons (CC) licenses.7Since CC-licensed music can\nbe freely redistributed (with attribution), it is (legally) pos-\nsible to create and share persistent data archives.\nThe Jamendo and FMA collections are both large and\ndiverse, and both can be linked to meta-data: Jamendo\nvia DBTune to MusicBrainz [19] and FMA to Echo\nNest/Spotify identiﬁers. Jamendo claims over 500,000\ntracks charted under six major categories: classical ,elec-\ntronic ,hip-hop ,jazz,pop, and rock. FMA houses approx-\nimately 90,000 tracks which are charted under ﬁfteen cat-\negories: blues ,classical ,country ,electronic ,experimen-\ntal,folk,hip-hop ,instrumental ,international ,jazz,old-\ntime/historic ,pop,rock,soul/rnb , and spoken . These cate-\n4Even in the unlikely event that a participant “cheats” by obtaining\nhuman-generated annotations, the results can be publicly redistributed as\nfree training data, so the community ultimately wins out.\n5http://jamendo.com\n6http://freemusicarchive.org/\n7https://creativecommons.org/licenses/gories should not necessarily be taken as ground truth an-\nnotations, but they reﬂect the tastes and priorities of their\nrespective user communities. While there is undoubtedly a\nstrong western bias in these corpora, the same can be said\nof MIREX’s private data and the MIR ﬁeld itself. How-\never, using open content at least permits practitioners to\nquantify and possibly correct this bias.\nAside from western/non-western bias, there is also the\npotential for free/commercial bias. A common criticism\nof basing research on CC-licensed music is that the music\nis of substantially lower “quality” — which may refer to\neither artistry or production value, or both — than com-\nmercial music. This point is obviously valid for high-level\ntasks such as recommendation, which depend on a vari-\nety of cultural, semantic, and subjective factors beyond the\nraw acoustic content of a track. However, for the majority\nof MIREX tasks, in particular perceptual tasks like onset\ndetection or source identiﬁcation, this is a much more tenu-\nous case. We do, however, note that FMA includes content\nby a variety of commercially successful artists,8but the\nvast majority of content in both sources is provided by rel-\natively unknown artists, which makes it difﬁcult to control\nfor “quality”.\nTo help users navigate the collections, both Jamendo\nand FMA provide popularity-based charts and community-\nbased curation, in addition to the previously mentioned\ngenre categories. Taken in combination, these features can\nbe leveraged to pre-emptively ﬁlter the collections down\nto subsets of tracks which are either of interest to a large\nnumber of listeners, of interest to a small number of lis-\nteners with speciﬁc tastes, or representative of particular\nstyles. This approach is similar in spirit to previous work\nusing chart-based sampling, e.g.Billboard [2].\n3.3 Incremental evaluation\nIn its ﬁrst cycle of operation, the proposed framework re-\nquires a new, unannotated corpus of audio. Rather than\nfully annotating the corpus up front for each task, we plan\nto adopt an incremental evaluation strategy [4].\nWith incremental evaluation, the set of reference anno-\ntations need not be complete: some (most) input examples\nwill not have corresponding annotations. Consequently,\nperformance scores are estimated over a subset of anno-\ntated tracks, which may itself grow over time. Systems are\nranked as usual, but with a degree of uncertainty inversely\nproportional to the number of available annotations.\nInitially, uncertainty in the performance estimates will\nbe maximal, due to a small number of available annota-\ntions. Subsequently, as reference annotations are collected\nand integrated to the evaluation system, they will be com-\npared to the submissions’ estimated annotations, and pro-\nvide incrementally accurate and precise performance esti-\nmates. Prior research in both text and video IR has demon-\nstrated that evaluation against incomplete reference data is\nfeasible, even when only a small fraction of the annotations\nare known [3, 15, 25].\n8https://en.wikipedia.org/wiki/Free_Music_\nArchive#Notable_artists288 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016This raises the question: which inputs are worth anno-\ntating? Consider two systems that produce the same anno-\ntation for a ﬁxed input example. Whether they are both\nright or wrong with respect to the reference annotation,\nthere is no way to distinguish between them according to\nthat example, so there is little value in seeking its annota-\ntion. Conversely, examples upon which multiple systems\ndisagree are more likely to produce useful information for\ndistinguishing among competing systems. Several recent\nstudies have investigated the use of algorithm disagree-\nment for this issue, be it for beat tracking [8], structural\nanalysis [13, chapter 4], or chord recognition [9]. Others\nhave studied alternative methods for music similarity [22],\nchoosing for annotation the examples that will be most in-\nformative for differentiating between systems. In general,\nthese methods allow us to minimize the required annotator\neffort by prioritizing the most informative examples. With\nmany participating systems, and multiple complex perfor-\nmance metrics, prioritizing data for annotation is by no\nmeans straightforward. We hope that the community will\nassist in specifying these procedures for each task.\nIn the proposed framework, annotations may come from\ndifferent sources, so it is imperative that we can trust or val-\nidate whatever information is submitted as reference data.\nAnother line of work is thus the development and enforce-\nment of standards, guidelines, and tools to collect and man-\nage annotations. This will require developing web services\nfor music annotation, as well as appropriate versioning and\nquality control mechanisms. Quality control is particularly\nimportant if annotations are collected via crowd-sourcing\nplatforms like Amazon Mechanical Turk.9\n3.4 Putting it all together\nIn contrast to the outline in Section 2, our proposed strat-\negy would proceed as follows:\n1. Identify and collect freely distributable audio.\n2. Deﬁne or update tasks, e.g., chord transcription.\n3. Release a development set (with annotation), and the\nremaining unlabeled data for evaluation.\n4. Collect predictions over the unlabeled data from par-\nticipating systems.\n5. Collect reference annotations, prioritized by dis-\nagreement among predictions and informativeness.\n6. Estimate and summarize each submission’s perfor-\nmance against the reference annotations.\n7. Retire newly annotated evaluation data, adding it to\nthe training set for the next campaign.\n8. Go to step 3 and repeat. Revisit steps 1–2 if needed.\nSteps 1–3 essentially constitute the startup cost, and\nare unavoidable for tasks which lack existing, open data\nsets. However, from the perspective of administration,\nonly steps 3 and 5 require signiﬁcant human intervention\n(i.e., annotation), and both steps directly result in public\ngoods. In this way, the proposed system will be signiﬁ-\ncantly more efﬁcient and cost-effective than MIREX.\n9https://www.mturk.com/4. DISCUSSION\nIn this section, we enumerate the goals and implementation\nof the proposed framework.\n4.1 Timing: why now?\nThe challenges described in this document, which our pro-\nposed strategy aims to address, are not news to the com-\nmunity. The operating costs of MIREX became apparent\nearly in its lifetime, and concerns about its sustainability\nloom large among researchers. That said, little has been\ndone to resolve the situation in the intervening years. This\nraises an obvious question: why will things change now?\nIn many ways, the approach taken by MIREX made\nperfect sense in the early 2000’s. However, recent infras-\ntructural advances, coupled with growth and maturation\nof the MIR community, have introduced new possibilities.\nFirst and foremost, creative commons music is now ample,\nbringing the dream of sharing data within grasp. Cloud\ncomputing is cheap and ubiquitous, and can dramatically\nreduce the administrative costs of evaluation infrastructure\nand persistent data storage. Improvements in web infras-\ntructure have also resolved many of the challenges of large-\nscale data distribution. Finally, browser support for inter-\nactive applications enables the development of web-based\nannotation tools, which signiﬁcantly reduces the barrier to\nentry for annotators.\nMore broadly speaking, the community has matured\nsigniﬁcantly since MIREX began in 2005. Open source\ndevelopment and reproducible methods are now common-\nplace, but we remain hindered by the lack of open data\nfor evaluation. Only by developing frameworks for open\nevaluation and data collection, can we further develop as a\nscientiﬁc discipline.\n4.2 Implementation details\nEffectively deploying the proposed framework will require\ntwo things: infrastructure development, and hosting. On\nthe infrastructure side, we can leverage several existing\nopen source projects: mireval for scoring [18], JAMS\nfor data format standardization [10], and CodaLab for run-\nning the evaluation campaign.10The last remaining soft-\nware component is a platform for collecting annotations.\nIn addition to traditional desktop software, browser-based\nannotation tools would facilitate distributed data collec-\ntion, and a simple content management system could col-\nlect annotations as they are completed.\nAs for hosting, since the burden of executing arbitrary\n(submitted) code is removed, the remaining software com-\nponents can reside on either a private university server, or,\nmore likely, cloud-based hosting such as Amazon Web Ser-\nvices.11Similarly, the audio data can be distributed via\nBitTorrent to participants, or hosted (at some minor cost)\nfor traditional download.\n10http://codalab.org/\n11https://aws.amazon.com/Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 2894.3 Data and evaluation integrity\nAllowing participants to submit predictions, rather than\nsoftware, may raise questions about integrity: how can we\nverify the process which generated the predictions? Ulti-\nmately, we must rely on participants to be honest in de-\nscribing their methods. Although it is possible for a par-\nticipant to manually annotate each track and achieve high\nscores, we hope that the scale of the dataset will make\nthis approach fundamentally impractical. Additionally, in\nkeeping with the spirit of open science and reproducible\nmethods, we will encourage participants to link to a repos-\nitory containing their software implementation, which can\nbe used to independently verify the predictions.\nWhen it comes to data integrity, we acknowledge that\nmusic is unique in that its perception is impacted by a\nplethora of cultural and experiential factors. In particu-\nlar, CC-licensed music may lie outside of the gamut of\ncommonly studied music, and differences in compositional\nstyle, instrumentation, or production, may lead to difﬁcul-\nties in validation and generalization. While this is unlikely\nto impact low-level tasks such as onset detection or instru-\nment identiﬁcation, more abstract tasks, such as chord es-\ntimation or structural analysis, may be more sensitive to\nselection bias. Relying on curation and chart popularity\nas a pre-ﬁlter in data selection may help to mitigate these\neffects. After collecting an initial set of annotated CC-\nlicensed music, it will be possible to quantitatively mea-\nsure the differences in system performance compared to\nexisting corpora of commercial music.\n4.4 Collecting annotations\nStatistically valid evaluation requires a continuous source\nof freshly annotated data. At present, we see three potential\nsources to satisfy this need.\nFirst is the traditional route of raising funds and paying\nexpert annotators. This option incurs both a direct ﬁnancial\ncost and various hidden human costs, but is also the most\nlikely to produce high-quality data, and may in fact be the\nonly viable path for certain tasks. In the grand scheme of\nthings, however, the ﬁnancial burden may not be so se-\nvere. As a point of reference, MedleyDB, a well-curated\ndataset of over 100 multi-track recordings for a number of\nMIR tasks, cost approximately $12 per track to annotate\n($1240 total) [1].12The ISMIR Society maintains a mem-\nbership of roughly 250 individuals each year: a $5 increase\nin membership dues would cover the annotation cost of a\nnew dataset like MedleyDB annually. Grants or industrial\npartnerships could also subsidize annotation costs.\nSecond, for tasks that require minimal annotator train-\ning, we can leverage either crowd-sourcing platforms, e.g.,\nMechanical Turk, or seek out enthusiastic communities in-\nterested in voluntary citizen science for music. Websites\nsuch as Genius13(6M monthly active users) and Ulti-\nmate Guitar14(3M monthly active users) demonstrate the\n12Figures provided via personal communication with the authors.\n13http://genius.com/\n14http://www.ultimate-guitar.com/existence of these communities for lyrics and guitar tabla-\nture.15As witnessed by the success of eBird16or Acous-\nticBrainz,17motivated people with the right tools can play\na substantial role in scientiﬁc endeavors.\nFinally, if no funding can be found to annotate data, we\nmay solicit annotations directly from participants and the\nISMIR community at large. This approach has been effec-\ntively used to collect judgements for the audio and sym-\nbolic music similarity tasks.\nIn each case, we will institute a ratiﬁcation system so\nthat annotations are independently validated before inclu-\nsion in the evaluation set. As mentioned in section 4.2,\nweb-based annotation tools will enable volunteer contribu-\ntion, which can supplement paid annotations.\n5. NEXT STEPS: A TRIAL RUN\nWe conclude by advocating a large-scale instrument iden-\ntiﬁcation task for ISMIR2017. In this task, the presence\nof active instruments (under a pre-deﬁned taxonomy) are\nestimated over an entire recording. The taxonomy may\nbe readily adapted from WordNet [12], and reﬁned by\nthe community, starting at this year’s ISMIR conference.\nThere are a number of strong motivations for pursuing in-\nstrument identiﬁcation. It is an important, classic problem\nin MIR, but is currently absent from MIREX. Researchers\ntypically explore the topic with disparate datasets, and the\nproblem remains unsolved to an unknown degree. Com-\npared with other music perception tasks, annotation re-\nquires a simple interface, e.g., “check all that apply”, and\nthe task deﬁnition itself is relatively unambiguous: a par-\nticular instrument is either present in the mix or not. In-\nstrument occurrence is largely independent of popularity,\nwhich results in a fairly minimal bias due to the use of CC-\nlicensed music. Finally, computer vision found fantastic\nsuccess with a similar endeavor, known as ImageNet [5],\nin which algorithms detect objects in natural images.\nThe following steps are necessary to realize this goal\nwithin the coming year: (i) establish a robust instrument\ntaxonomy; (ii) acquire a large sample of audio content;\n(iii) build a web-based annotation tool and storage system;\n(iv) construct a development set; (v) implement or collect a\nfew simple algorithms to prioritize content for initial anno-\ntation; (vi) perform data collection, through some combi-\nnation of paid annotation, crowd-sourcing, and community\nsupport; and ﬁnally, deploy an evaluation server and lead-\ner-board to accept and score submissions.\nEach of these components, while requiring some en-\ngineering and organizational efforts, are achievable goals\nwith the help of the ISMIR community.\nAcknowledgments. B.M. is supported by the Moore-\nSloan Data Science Environment at NYU. J.U. is sup-\nported by the Spanish Government: JdC postdoctoral fel-\nlowship, and projects TIN2015-70816-R and MDM-2015-\n0502.\n15Data gathered from http://compete.com , March 2016\n16http://ebird.org/\n17http://acousticbrainz.org/290 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20166. REFERENCES\n[1] R.M. Bittner, J. Salamon, M. Tierney, M. Mauch,\nC. Cannam, and J.P. Bello. Medleydb: A multitrack\ndataset for annotation-intensive mir research. In Inter-\nnational Society for Music Information Retrieval Con-\nference , ISMIR, pages 155–160, 2014.\n[2] J.A. Burgoyne, J. Wild, and I. Fujinaga. An expert\nground truth set for audio chord recognition and music\nanalysis. In International Society for Music Informa-\ntion Retrieval Conference , ISMIR, 2011.\n[3] B. Carterette. Robust Test Collections for Retrieval\nEvaluation. In SIGIR , pages 55–62, 2007.\n[4] B. Carterette and J. Allan. Incremental test collections.\nInACM International conference on Information and\nKnowledge Management , pages 680–687. ACM, 2005.\n[5] J. Deng, W. Dong, R. Socher, L.J. Li, K. Li, and\nL. Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In Computer Vision and Pattern Recognition ,\nCVPR, pages 248–255. IEEE, 2009.\n[6] J.S. Downie. The music information retrieval evalua-\ntion exchange (2005-2007): A window into music in-\nformation retrieval research. Acoustical Science and\nTechnology , 29(4):247–255, 2008.\n[7] J.S. Downie, A.F. Ehmann, M. Bay, and M.C. Jones.\nThe music information retrieval evaluation exchange:\nSome observations and insights. In Advances in music\ninformation retrieval , pages 93–115. Springer, 2010.\n[8] A. Holzapfel, M.E.P. Davies, J.R. Zapata, J.L. Oliveira,\nand F. Gouyon. Selective sampling for beat tracking\nevaluation. Audio, Speech, and Language Processing,\nIEEE Transactions on , 20(9):2539–2548, 2012.\n[9] E.J. Humphrey and J.P. Bello. Four timely insights on\nautomatic chord estimation. In International Society\nfor Music Information Retrieval Conference , ISMIR,\npages 673–679, 2015.\n[10] E.J. Humphrey, J. Salamon, O. Nieto, J. Forsyth, R.M.\nBittner, and J.P. Bello. JAMS: A JSON annotated mu-\nsic speciﬁcation for reproducible MIR research. In IS-\nMIR, pages 591–596, 2014.\n[11] B. McFee, T. Bertin-Mahieux, D. Ellis, and G. Lanck-\nriet. The million song dataset challenge. In 4th Inter-\nnational Workshop on Advances in Music Information\nResearch , AdMIRe, April 2012.\n[12] G.A. Miller. Wordnet: a lexical database for english.\nCommunications of the ACM , 38(11):39–41, 1995.\n[13] O. Nieto. Discovering structure in music: Automatic\napproaches and perceptual evaluations . PhD thesis,\nNew York University, 2015.[14] N. Orio, D. Rizo, R. Miotto, M. Schedl, N. Montec-\nchio, and O. Lartillot. Musiclef: a benchmark activity\nin multimodal music information retrieval. In Interna-\ntional Society for Music Information Retrieval Confer-\nence, ISMIR, pages 603–608, 2011.\n[15] P. Over, J. Fiscus, G. Sanders, D. Joy, M. Michel,\nG. Awad, A.F. Smeaton, W. Kraaij, and G. Quenot.\nTRECVID 2014–An Overview of the Goals, Tasks,\nData, Evaluation Mechanisms, and Metrics. In TREC\nVideo Retrieval Evaluation Conference , 2014.\n[16] G. Peeters and K. Fort. Towards a (Better) Deﬁnition of\nthe Description of Annotated MIR Corpora. In Interna-\ntional Society for Music Information Retrieval Confer-\nence, pages 25–30, 2012.\n[17] G. Peeters, J. Urbano, and G.J.F. Jones. Notes from the\nISMIR 2012 Late-Breaking Session on Evaluation in\nMusic Information Retrieval. In International Society\nfor Music Information Retrieval Conference , 2012.\n[18] C. Raffel, B. McFee, E.J. Humphrey, J. Salamon,\nO. Nieto, D. Liang, and D.P.W. Ellis. mir eval: A\ntransparent implementation of common mir metrics. In\nInternational Society for Music Information Retrieval\nConference , ISMIR, 2014.\n[19] Y . Raimond and M.B. Sandler. A web of musical infor-\nmation. In ISMIR , pages 263–268, 2008.\n[20] X. Serra, M. Magas, E. Benetos, M. Chudy, S. Dixon,\nA. Flexer, E. G ´omez, F. Gouyon, P. Herrera, S. Jorda,\nO. Paytuvi, G. Peeters, J. Schl ¨uter, H. Vinet, and\nG. Widmer. Roadmap for Music Information Re-\nSearch, 2013.\n[21] B.L. Sturm. The state of the art ten years after a state of\nthe art: Future research in music information retrieval.\nJournal of New Music Research , 43(2):147–172, 2014.\n[22] J. Urbano and M. Schedl. Minimal Test Collections for\nLow-Cost Evaluation of Audio Music Similarity and\nRetrieval Systems. International Journal of Multime-\ndia Information Retrieval , 2(1):59–70, 2013.\n[23] J. Urbano, M. Schedl, and X. Serra. Evaluation in Mu-\nsic Information Retrieval. Journal of Intelligent Infor-\nmation Systems , 41(3):345–369, 2013.\n[24] E.M. V oorhees and D.K. Harman. TREC: Experiment\nand Evaluation in Information Retrieval . MIT Press,\n2005.\n[25] E. Yilmaz, E. Kanoulas, and J.A. Aslam. A Simple\nand Efﬁcient Sampling Method for Estimating AP and\nNDCG. In SIGIR , pages 603–610, 2008.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 291"
    },
    {
        "title": "Musical Typicality: How Many Similar Songs Exist?.",
        "author": [
            "Tomoyasu Nakano",
            "Daichi Mochihashi",
            "Kazuyoshi Yoshii",
            "Masataka Goto"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417915",
        "url": "https://doi.org/10.5281/zenodo.1417915",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/191_Paper.pdf",
        "abstract": "We propose a method for estimating the musical “typical- ity” of a song from an information theoretic perspective. While musical similarity compares just two songs, musi- cal typicality quantifies how many of the songs in a set are similar. It can be used not only to express the uniqueness of a song but also to recommend one that is representative of a set. Building on the type theory in information the- ory (Cover & Thomas 2006), we use a Bayesian generative model of musical features and compute the typicality of a song as the sum of the probabilities of the songs that share the type of the given song. To evaluate estimated results, we focused on vocal timbre which can be evaluated quan- titatively by using the singer’s gender. Estimated typicality is evaluated against the Pearson correlation coefficient be- tween the computed typicality and the ratio of the number of male singers to female singers of a song-set. Our result shows that the proposed measure works more effectively to estimate musical typicality than the previous model based simply on generative probabilities. 1 INTRODUCTION The amount of digital content that can be accessed has been increasing and will continue to do so in the future. This is desirable with regard to the diversity of the content, but is making it harder for listeners to select from this con- tent. Furthermore, since the amount of similar content is also increasing, creators will be more concerned with the originality of their creations. All kinds of works are influ- enced by some existing content, and it is difficult to avoid an unconscious creation of content partly similar in some way to other content. This paper focuses on musical typicality which reflects the number of songs having high similarity with the tar- get song as shown in Figure 1. This definition of musi- cal typicality is based on central tendency, which in cog- nitive psychology is one of the determinants of typical- ity [2]. Musical typicality can be used to recommend a unique or representative song for a set of songs such as those in a particular genre or personal collection, those on c⃝Tomoyasu Nakano, Daichi Mochihashi, Kazuyoshi Yoshii, Masataka Goto. Licensed under a Creative Commons Attribu- tion 4.0 International License (CC BY 4.0). Attribution: Tomoyasu Nakano, Daichi Mochihashi, Kazuyoshi Yoshii, Masataka Goto. “Mu- sical Typicality: How Many Similar Songs Exist?”, 17th International Society for Music Information Retrieval Conference, 2016. low fi musical similarity song b song a low song set A fi musical typicality song a song b high fi fi fi fi fi fi fi fi fi fi fi fi a particular genre a personal collection a specific playlist e.g., Figure 1. Musical similarity and typicality. fi Generative modeling Typicality estimation feature vectors vector quantization type (topic distribution) ... ... fi fifi 1 2 3 5 1 topic sequence ... 2 2 1 2 1 feature vectors vector quantization song models (LDA) ... ... 1 2 3 5 1 generative model generative model generative model song model (LDA) song-set model song-set model calculate probability calculate probability Proposed approach Previous approach Figure 2. Estimation of music typicality represented by a discrete sequence based on the type theory. Both the pre- vious and the proposed approach are illustrated. a specific playlist, or those released in a given year or a decade. And it can help listeners to understand the rela- tionship between a song and such a song set. However, human ability with regard to typicality is limited. Judg- ing similarity between two songs is a relatively simple task but is time-consuming, so judging the similarities of a mil- lion songs is impossible. Consequently, despite the coming of an era in which people other than professional creators can enjoy creating and sharing works, the monotonic in- crease in content means that there is a growing risk that one’s work will be denounced as being similar to some- one else’s. This could make it difficult for people to freely create and share content. The musical typicality proposed in this paper can help create an environment in which spe- cialists and general users alike can know the answers to the questions “How often does this occur?” and “How many similar songs are there?”. Much previous work has focused on musical similarity because it is a central concept of MIR and is also impor- tant for purposes other than retrieval. For example, the use 695 type 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1 2/3 1/3 0 generative probability high typicality high Previous approach Proposed approach song a song b song c Figure 3. Examples of a song having high generative prob- ability and songs having high typicality. of similarity to automatically classify musical pieces (into genres, music styles, etc.) has been studied [10,13], as has its use for music auto-tagging [17]. Each of these applica- tions, however, is different from musical typicality: musi- cal similarity is usually defined by comparing two songs, music classification is defined by classifying a given song into one out of a set of categories (category models, cen- troids, etc.), and music auto-tagging is defined by compar- ing a given song to a set of tags (tag models, the closest neighbors, etc.). Nakano et al. proposed a method for estimating mu- sical typicality by using a generative model trained from the song set (Figure 2) [16] and showed its application to visualizing relationships between songs in a playlist [14]. Their method estimates acoustic features of the target song at each frame and represents the typicality of the target song represented as an average probability of each frame of the song calculated using the song-set model. However, we posit that the generative probability is not truely appro- priate to represent typicality. The method we propose here, in contrast, introduces the type from information theory for improving estimated mu- sical typicality by a bag-of-features approach [16]. In this context, the type is same meaning with the unigram distri- bution. We first model musical features of songs by using a vector quantization method and latent Dirichlet alloca- tion (LDA) [4]. We then estimate a song-set model from the song models. Finally, we compute the typicality of the target song by calculating the probability of a type of the musical sequence (quantized acoustic features) calculated using the song-set model (Figure 2). 2 METHOD The key concept of the method in this paper is the type of a sequence on which we consider the typicality of a given music. Previous work have mentioned/used simple gen- erative probabilities to compute musical similarity [1] or typicality [16] of a music and for singer identification [8]. However, simple generative probability will not conform to our notion of typicality. Imagine the simplest example in Figure 3: here, each song consists of alphabets of {0, 1} and the stationary information source has a probability dis- tribution on alphabets Q(0) = 2/3, Q(1) = 1/3. Clearly, while the song “a” has the highest probability of generation, we can see that the sequences like “b” and “c” will occur more typically. This means that we should think about the sum of the probabilities of songs that are similar to the song to measure the typicality.",
        "zenodo_id": 1417915,
        "dblp_key": "conf/ismir/NakanoMYG16",
        "content": "MUSICAL TYPICALITY:HOWMANYSIMILARSONGSEXIST?\nTomoyasuNakano1DaichiMochihashi2KazuyoshiYoshii3MasatakaGoto1\n1National Institute of AdvancedIndustrial Science and Technology(AIST) ,Japan\n2TheInstitute ofStatistical Mathematics,Japan\n3KyotoUniversity,Japan\n1{t.nakano, m.goto }[at]aist.go.jp2daichi[at]ism.ac.jp\n3yoshii[at]kuis.kyoto-u.ac.jp\nABSTRACT\nWe propose a method for estimating the musical “typical-\nity” of a song from an information theoretic perspective.\nWhile musical similarity compares just two songs, musi-\ncaltypicalityquantiﬁeshowmanyofthesongsinasetare\nsimilar. It can be used not only to express the uniqueness\nof a song but also to recommend one that is representative\nof a set. Building on the type theory in information the-\nory(Cover&Thomas2006),weuseaBayesiangenerative\nmodel of musical features and compute the typicality of a\nsongasthesumoftheprobabilitiesofthesongsthatsharethe type of the given song. To evaluate estimated results,\nwe focused on vocal timbre which can be evaluated quan-\ntitativelybyusingthesinger’sgender. Estimatedtypicality\nis evaluated against the Pearson correlation coefﬁcient be-\ntween the computed typicality and the ratio of the number\nof male singers to female singers of a song-set. Our result\nshowsthattheproposedmeasureworksmoreeffectivelyto\nestimate musical typicality than the previous model based\nsimplyongenerativeprobabilities.\n1 INTRODUCTION\nThe amount of digital content that can be accessed has\nbeen increasing and will continue to do so in the future.Thisisdesirablewithregardtothediversityofthecontent,\nbutismakingitharderforlistenerstoselectfromthiscon-\ntent. Furthermore, since the amount of similar content is\nalso increasing, creators will be more concerned with the\noriginality of their creations. All kinds of works are inﬂu-\nenced by some existing content, and it is difﬁcult to avoid\nan unconscious creation of content partly similar in some\nwaytoothercontent.\nThis paper focuses on musical typicality which reﬂects\nthe number of songs having high similarity with the tar-\nget song as shown in Figure 1. This deﬁnition of musi-\ncal typicality is based on central tendency , which in cog-\nnitive psychology is one of the determinants of typical-\nity [2]. Musical typicality can be used to recommend a\nunique or representative song for a set of songs such as\nthose in a particular genre or personal collection, those on\nc/circlecopyrtTomoyasu Nakano, Daichi Mochihashi, Kazuyoshi\nYoshii, Masataka Goto. Licensed under a Creative Commons Attribu-\ntion 4.0 International License (CC BY 4.0). Attribution: Tomoyasu\nNakano, Daichi Mochihashi, Kazuyoshi Yoshii, Masataka Goto. “Mu-\nsical Typicality: How Many Similar Songs Exist?”, 17th International\nSociety for Music Information RetrievalConference, 2016.lowﬁmusical\nsimilarity\nsong b song alow\nsong set Aﬁ\nmusical\ntypicalitysong a\nsong bhighﬁ\nﬁ ﬁﬁ ﬁﬁﬁﬁ\nﬁﬁﬁﬁ\na particular genre\na personal collection\na specific playliste.g.,\nFigure1. Musical similarityandtypicality.\nﬁGenerative modeling\nTypicality estimation\nfeature vectors vector quantization\ntype\n(topic distribution)... ... ﬁﬁﬁ\n123 5 1\ntopic sequence... 221 2 1feature vectors vector quantizationsong models\n(LDA)... ... 123 5 1generative model\ngenerative model\ngenerative model\nsong model (LDA)song-set\nmodel\nsong-set\nmodelcalculate\nprobability\ncalculate\nprobability\nProposed approachPrevious approach\nFigure 2. Estimation of music typicality represented by a\ndiscrete sequence based on the type theory. Both the pre-\nviousand the proposed approach are illustrated.\na speciﬁc playlist, or those released in a given year or a\ndecade. And it can help listeners to understand the rela-\ntionship between a song and such a song set. However,\nhuman ability with regard to typicality is limited. Judg-\ningsimilaritybetweentwosongsisarelativelysimpletask\nbutistime-consuming,sojudgingthesimilaritiesofamil-\nlionsongsisimpossible. Consequently,despitethecoming\nof an era in which people other than professional creatorscan enjoy creating and sharing works, the monotonic in-\ncrease in content means that there is a growing risk that\none’s work will be denounced as being similar to some-\none else’s. This could make it difﬁcult for people to freely\ncreate and share content. The musical typicality proposed\nin this paper can help create an environment in which spe-\ncialistsandgeneralusersalikecanknowtheanswerstothe\nquestions “How often does this occur?” and “How many\nsimilarsongsare there?”.\nMuch previous work has focused on musical similarity\nbecause it is a central concept of MIR and is also impor-\ntantforpurposesotherthanretrieval. Forexample,theuse695type000000\n010100\n1000 0112/3\n1/3\n0generative probability high\ntypicality highPrevious approach\nProposed approachsong a\nsong b\nsong c\nFigure3. Examplesofasonghavinghighgenerativeprob-\nabilityandsongshavinghightypicality.\nof similarity to automatically classify musical pieces (into\ngenres,musicstyles,etc.) hasbeenstudied[10,13],ashas\nits use for music auto-tagging [17]. Each of these applica-\ntions, however, is different from musical typicality: musi-\ncal similarity is usually deﬁned by comparing two songs,\nmusic classi ﬁcationis deﬁned by classifying a given song\ninto one out of a set of categories (category models, cen-\ntroids,etc.),and music auto-tagging isdeﬁnedbycompar-\ning a given song to a set of tags (tag models, the closest\nneighbors, etc.).\nNakano et al.proposed a method for estimating mu-\nsical typicality by using a generative model trained from\nthe song set (Figure 2) [16] and showed its application to\nvisualizing relationships between songs in a playlist [14].\nTheirmethodestimatesacousticfeaturesofthetargetsong\nat each frame and represents the typicality of the target\nsong represented as an average probability of each frame\nofthesongcalculatedusingthesong-setmodel. However,wepositthatthegenerativeprobabilityisnottruelyappro-\npriate torepresent typicality.\nThemethodweproposehere,incontrast,introducesthe\ntypefrominformationtheoryforimprovingestimatedmu-\nsical typicality by a bag-of-features approach [16]. In this\ncontext, the type is same meaning with the unigram distri-\nbution. We ﬁrst model musical features of songs by using\na vector quantization method and latent Dirichlet alloca-\ntion (LDA) [4]. We then estimate a song-set model from\nthe song models. Finally, we compute the typicality of the\ntarget song by calculating the probability of a type of the\nmusical sequence (quantized acoustic features) calculatedusing the song-set model (Figure 2).\n2 METHOD\nThe key concept of the method in this paper is the typeof\na sequence on which we consider the typicality of a given\nmusic. Previous work have mentioned/used simple gen-\nerative probabilities to compute musical similarity [1] or\ntypicality [16] of a music and for singer identiﬁcation [8].\nHowever, simple generative probability will not conform\nto our notion of typicality. Imagine the simplest example\ninFigure3: here,eachsongconsistsofalphabetsof {0,1}\nandthestationaryinformationsourcehasaprobabilitydis-\ntributiononalphabets Q(0) = 2/3,Q(1) = 1/3.\nClearly, while the song “a” has the highest probability\nof generation, we can see that the sequences like “b” and\n“c” will occur more typically. This means that we shouldthink about the sumof the probabilities of songs that are\nsimilartothe songtomeasure the typicality.\n2.1 Typeandthe Typicality\nLet us formalize our ideas from the viewpoint of informa-\ntiontheory[5–7]. Let x={x\n1,x2,···,xn}beasequence\nof length nwhose alphabet xcomes from a set X.W e\nassume that xcomes from a stationary memoryless infor-\nmation source, i.e. we can drop the order of symbols in x\nand regard xas a bag of words. Next, we introduce some\ndeﬁnitions:\nDeﬁnition 1 (type).LetN(x|x)be the number of times\nthatx∈Xappeared in sequence x.T h e typePxof the\nsequence xisanempiricalprobabilitydistributionofsym-\nbolsin x:\nPx=/braceleftbigg1\nnN(x|x)/vextendsingle/vextendsingle/vextendsingle/vextendsinglex∈X/bracerightbigg\n. (1)\nWedenote the space of all P\nxasPn.\nDeﬁnition 2. LetP∈Pn. A set of sequences of length n\nthat share the same type Piscalleda type class TnofP:\nTn(P)={x∈Xn|Px=P}. (2)\nNow let us denote the probability of a sequence xfrom\nan memoryless information whose symbol probabilities\nareQ(x):\np(x)=Qn(x)=n/productdisplay\ni=1Q(xi). (3)\nGiventhesedeﬁnitions,thefollowingsimpletheoremsfol-\nlow:\nTheorem 1. The probability of a sequence xhaving type\nPfrom a stationary memoryless information source Qis\nexpressedasfollows:\nQn(x)=e x p/bracketleftbig\n−n(H(P)+D(P||Q))/bracketrightbig\n(4)\nHere,H(P)andD(P||Q)are an entropy of Pand\nKullback-Leiblerdivergenceof PfromQ,respectively.\nH(P)=−/summationdisplay\nx∈XP(x)logP(x) (5)\nD(P||Q)=/summationdisplay\nx∈XP(x)logP(x)\nQ(x)(6)\nProof.\nQn(x)=n/productdisplay\ni=1Q(xi)=/productdisplay\nxQ(x)N(x|x)=/productdisplay\nxQ(x)nP(x)\n=/productdisplay\nxexp/bracketleftbig\nnP(x)logQ(x)/bracketrightbig\n(7)\n=e x p/bracketleftBig\n−n/parenleftBig\n−/summationdisplay\nxP(x)logQ(x)/parenrightBig/bracketrightBig\n(8)\n=e x p/bracketleftBig\n−n/parenleftBig\nH(P)+D(P||Q)/parenrightBig/bracketrightBig\n./square (9)\nTheorem 2 (lower and upper bounds) .For any type P∈\nPn,\n1\n(n+1)|X|− 1exp{nH(P)}\n≤|Tn(P)|≤exp{nH(P)}. (10)696 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Usingthetheoremsabove,thefollowingimportantthe-\norem can be proved.\nTheorem 3. For any type P∈Pnand any probability\ndistribution Q,\nQn(Tn(P)).=e x p{−nD(P||Q)},(11)\nwherean.=bniflimn→∞(1/n)log(an/bn)=0.\nProof.Using (4) and (10),\nQn(Tn(P)) =/summationtext\nx∈Tn(P)Qn(x)\n=|Tn(P)|exp(−n(H(P)+D(P||Q)))\n.=e x p (nH(P))·exp(−n(H(P)+D(P||Q)))\n=e x p{−nD(P||Q)}./square (12)\nThis theorem says that the sumof the probabilities of\nsequences that share the same type Pis given by an expo-\nnential of Kullback-Leibler divergence from the informa-\ntion source Q. While the equation (11) is usually used in\ninformation theory to formalize that such a probability ex-\nponentially decays with the length n, here we do not care\nfornbut for the form of the function. Thus, we normalize\n(11) for a unit observation like the well-known perplexity ,\nyieldingthe deﬁnitionoftypicalityasfollows:\nDeﬁnition3 (Typicality).\nTypicality( P|Q)=e x p{−D(P||Q)}(13)\nwherePis the type of a musical sequence and Qis a gen-\nerativemodel ofitsmusical features.\n2.2 Generativemodeling andType\nTo evaluate the typicality estimation method, we compute\nthe type of each song by modeling them in a way based\non our previous work [16]. From polyphonic musical au-\ndiosignalsincludingasingingvoiceandsoundsofvarious\nmusicalinstruments,weﬁrstextractvocaltimbre. Wethen\nmodel the timbre of songs by using a vector-quantization\nmethod and latent Dirichlet allocation (LDA) [4]. Finally,\na song-set model Qis estimated by integrating all song\nmodels(Figure 2).\nIn addition, we use the expectation of Dirichlet topic\ndistributionasa type Pbecausethe hyperparameterof the\nposterior Dirichlet distribution can be interpreted as thenumber of observations of the corresponding topic. In the\nother words, the Pindicates mixing weights of the multi-\nple topics.\n2.2.1 Extracting acoustic features: Vocal timbre\nWe use the mel-frequency cepstral coefﬁcients of the LPC\nspectrum of the vocal (LPMCCs) and the ΔF\n0of the vo-\ncal to represent vocal timbre because they are effective for\nidentifying singers [8,15]. In particular, the LPMCCs rep-\nresent the characteristics of the singing voice well, since\nsinger identiﬁcation accuracy is greater when using LPM-\nCCs than when using the standard mel-frequency cepstral\ncoefﬁcients(MFCCs) [8].\nWe ﬁrst use Goto’s PreFEst [11] to estimate the F0of\nthepredominantmelodyfromanaudiosignalandthentheF0is used to estimate the ΔF0and the LPMCCs of the\nvocal. To estimate the LPMCCs, the vocal sound is re-\nsynthesized by using a sinusoidal model based on the esti-\nmatedvocal F0andtheharmonicstructureestimatedfrom\ntheaudiosignal. Ateachframethe ΔF0andtheLPMCCs\nare combinedasa feature vector.\nThen reliable frames (frames little inﬂuenced by ac-\ncompaniment sound) are selected by using a vocal GMM\nand a non-vocal GMM (see [8] for details). Feature vec-\ntors of only the reliable frames are used in the followingprocesses(model trainingandprobabilityestimation).\n2.2.2 Quantization\nVectorquantizationisappliedusingthe k-meansalgorithm\nto convert acoustic feature vectors of each element to a\nsymbolic time series representation. In that algorithm the\nvectors are normalized by subtracting the mean and divid-\ningbythestandarddeviation,andthenthenormalizedvec-\ntors are quantized by prototype vectors (centroids) trained\npreviously. Hereafter,wecallthequantizedsymbolictimeseries acoustic words .\n2.2.3 Probabilistic generative model: song model\nTheobserveddataweconsiderforLDAare Dindependent\nsongsX={X\n1,...,XD}. A song XdisNdacoustic\nwordsXd={xd,1,...,xd,N d}. The size of the acous-\ntic words vocabulary equals to the number of clusters of\nthek-means algorithm, V. We consider a K-dimensional\nmultinomial of latent topic proportions θdfor each Xd,\nandwrite θ=(θ1,···,θD).\nIntroducing latent topic assignments Zd=\n{zd,1,...,zd,N d}forXdand collectively write\nZ={Z1,...,ZD}, the full joint distribution of our\nLDAmodel isgivenby\np(X,Z,θ,φ)=p(X|Z,φ)p(Z|θ)p(θ)p(φ)(14)\nwhereφindicates the emission distribution of each topic.\nThe ﬁrst two terms are likelihood functions, and the other\ntwo are prior distributions. The likelihood functions are\ndeﬁnedas\np(X|Z,φ)=D/productdisplay\nd=1Nd/productdisplay\nn=1V/productdisplay\nv=1/parenleftBiggK/productdisplay\nk=1φzd,n,k\nk,v/parenrightBiggxd,n,v\n(15)\nand\np(Z|θ)=D/productdisplay\nd=1Nd/productdisplay\nn=1V/productdisplay\nv=1θzd,n,k\nd,k. (16)\nWeendow θandφconjugateDirichlet priors:\np(θ)=D/productdisplay\nd=1Dir(θd|α0)∝D/productdisplay\nd=1K/productdisplay\nk=1θα0−1\nd,k(17)\np(φ)=K/productdisplay\nk=1Dir(φk|β0)∝K/productdisplay\nk=1V/productdisplay\nv=1φβ0−1\nk,v.(18)\nwherep(θ)andp(φ)areproductsofDirichletdistributions\nandα0,β0are their prior hyperparameters.\nFinally, we obtain a type of each song Xdas an expec-\ntationofthe Dirichlet posteriordistributionof θd.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 697Dir(6,2,2) Dir(6,2,6) Dir(6,6,2)\nDir(18,10,10) Dir(5.4, 2.9, 2.8)\nPrevious approach Proposed methodDir(7,1,1) Dir(1,1,7) Dir(1,7,1)\nDir(9,9,9) Dir(0.92, 0.89, 0.97)\nPrevious approach Proposed method\nFigure4. Song-set model estimationofpreviousapproachanda model estimatedbyo ur proposed method.\n2.3 Typicalityovera set of Songs\nGiven the type of each song, we wish to compute the typi-\ncalityofasongascomparedtoasetofothersongs. InSec-\ntion 2.1, we deﬁned the typicality of a sequence of type P\nfrom an information source having distribution Q. There-\nfore,weneedsomewaytoestimate Qfromthesetofsongs\n(i.e. types) beforehand. Actually, we do not have to esti-\nmate a single Qbutcompute anexpectationaroundit:\nTypicality( P|Θ) = E[exp(−D(P||θ))]θ∼Dir(α)(19)\nwhereΘ={θ1,···,θn}is a set of types of other songs\nandDir(α)is a prior Dirichlet distribution from which\neachθi∈Θisdeemedtobe generated.\nIn the previous work [16], we estimated the hyper-\nparameter αby just summing the topic distributions\nθ1,···,θn. AsshowninFigure4,however,thiscouldlead\ntoanundesirableresultandweemployaBayesianformula\nto estimate α. This derivation is based on the following\nDirichlet andGamma distributions:\nDir(θ|α)=Γ(/summationtext\nkαk)/producttext\nkΓ(αk)/productdisplay\nkθαk−1\nk(20)\nGa(α|a,b)=ba\nΓ(a)αa−1e−bα(21)\nTherefore,\np(α|Θ)∝p(Θ|α)p(α) (22)\n∝/productdisplay\nkαa−1\nke−bαk·/productdisplay\njΓ(/summationtext\nkαk)/producttext\nkΓ(αk)/productdisplay\nkθαk\nk,(23)\nwhich leadsto\np(αk|αk−1,Θ)∝αa−1\nke−bαk·/productdisplay\njΓ(/summationtext\nkαk)\nΓ(αk)θαk\nk.(24)\nBecause we cannot expand Γ(/summationtext\nkαk)/Γ(αk),w em a k ea\nfollowing approximation with nbeing a nearest integer to/summationtext\nj/negationslash=kαk[18]:\nΓ(/summationtext\nkαk)\nΓ(αk)=Γ(αk+/summationtext\nj/negationslash=kαj)\nΓ(αk)/similarequalΓ(αk+n)\nΓ(αk)(25)\n=αk(αk+1)···(αk+n−1)(26)\n=n−1/productdisplay\ni=0αk(αk+i) (27)=n−1/productdisplay\ni=0/summationdisplay\ny∈{0,1}(αk)yi(i)1−yi. (28)\nTherefore,introducingauxiliaryvariables\nyi∼Bernoulli/parenleftbiggαk\nαk+i/parenrightbigg\n, (29)\nwe can makea followingGamma proposal for αk:\np(αk|αk−1,Θ) (30)\n/similarequalαa−1\nke−bαk·/productdisplay\njeαklogθjk·/productdisplay\njn−1/productdisplay\ni=0αyji\nk(31)\n=αa+/summationtext\nj/summationtextn−1\ni=0yji−1\nk·e−αk(b−/summationtext\njlogθjk)(32)\n=G a (a+/summationdisplay\njn−1/summationdisplay\ni=0yji,b−/summationdisplay\njlogθjk).(33)\nBecause this is just a proposal, we further correct the bias\nusingaMetropolis-Hastingsalgorithmwiththeexactlike-\nlihood formula (24).\n2.4 Computing the Expectation\nOnceweobtain αfromΘ,wecancomputetheexpectation\n(19)analytically. Denoting P=(p1,···,pK)andwriting\nE[]as/angbracketleft/angbracketright,\nTypicality( P|Θ) =/angbracketleftbig\nexp(−D(P||θ))/angbracketrightbig\nθ∼Dir(α)(34)\n=/angbracketleftBigg\nexpK/summationdisplay\nk=1pklogθk\npk/angbracketrightBigg\nθ∼Dir(α)\n=1\nexp(/summationtext\nkpklogpk)/angbracketleftBigg\nexp/summationdisplay\nkpklogθk/angbracketrightBigg\nθ∼Dir(α)\n=e x p (H(P))/angbracketleftBiggK/productdisplay\nk=1θpk\nk/angbracketrightBigg\nθ∼Dir(α). (35)\nHere, the second term is\n/angbracketleftBiggK/productdisplay\nk=1θpk\nk/angbracketrightBigg\nθ∼Dir(α)=Γ(/summationtext\nkαk)/producttext\nkΓ(αk)/integraldisplay/productdisplay\nkθαk−1\nk·/productdisplay\nkθpk\nkdθ\n=Γ(/summationtext\nkαk)/producttext\nkΓ(αk)/integraldisplay/productdisplay\nkθαk+pk−1\nkdθ\n=Γ(/summationtext\nkαk)/producttext\nkΓ(αk)/producttext\nkΓ(αk+pk)\nΓ(/summationtext\nkαk+pk)698 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016=1/summationtext\nkαk/productdisplay\nkΓ(αk+pk)\nΓ(αk).(36)\nTherefore, from (35) we ﬁnally obtain\nTypicality( P|Θ) =exp(H(P))/summationtext\nkαk/productdisplay\nkΓ(αk+pk)\nΓ(αk).(37)\n3 EXPERIMENTS\nThe proposed methods were tested in an experiment eval-\nuating the estimated typicality. To evaluate estimated re-\nsults, we focused on vocal timbre which can be evaluated\nquantitativelybyusingthe singer’sgender.\n3.1 Dataset\nThesongsetusedfortheLDA-model-trainingandtypical-\nity estimation comprised 3,278 Japanese popular songs1\nthat appeared on a popular music chart in Japan ( http:\n//www.oricon.co.jp/ ) and were placed in the top\ntwenty on weekly charts appearing between 2000 and\n2008. Here we refer to thissong set asthe JPOPMDB.\nThesongsetusedforGMMtrainingand k-meanstrain-\ning to extract the acoustic features consisted of 100 popu-\nlar songs from the RWC Music Database (RWC-MDB-P-\n2001) [9]. These 80 songs in Japanese and 20 in English\nreﬂect styles of the Japanese popular songs (J-Pop) and\nWestern popular songs in or before 2001. Here we refer\nthissong set asthe RWCMDB.\n3.2 Experimental Settings\nConditionsandparametersofthemethodsdescribedinthe\nMETHODS section are described here in detail. Some\nconditions and each parameter value were based on pre-\nviouswork[15,16].\n3.2.1 Typicality estimation\nThe number of iterations of the Bayesian song-set model\nestimationdescribedinSubsection2.3was 1000.\n3.2.2 Extracting acoustic features\nFor vocal timbre features, we targeted monaural 16-kHz\ndigitalrecordingsandextracted ΔF0and12th-orderLPM-\nCCsevery10ms. Toestimatethefeatures,thevocalsoundwas re-synthesized by using a sinusoidal model. The ΔF\n0\nwascalculatedeveryﬁveframes(50ms).\nThe feature vectors were extracted from each song, us-\ning as reliable vocal frames the top 15% of the feature\nframes. Using the 100 songs of the RWC MDB, a vocal\nGMM and a non-vocal GMM were trained by variational\nBayesian inference [3].\n3.2.3 Quantization\nTo quantize the vocal features, we set the number of clus-\nters of the k-means algorithm to 100 and used the 100\nsongsof the RWCMDB to train the centroids.\n1Notethat some are Westernpopular songsand English in them .3.2.4 Training the generative models\nTraining song models and song-set models of the vocal\ntimbre by LDA, we used all of the 3,278 original record-ingsof the JPOPMDB.\nThenumberoftopics, K,wassetto 100,andthemodel\nparametersofLDAweretrainedusingthecollapsedGibbs\nsampler [12]. The hyperparameters of the Dirichlet distri-\nbutionsfortopicsandwordswereinitiallysetto 1and0.1,\nrespectively.\n3.3 Fourtypicality measures\nWeevaluatedthefollowingfourtypicalitycomputingcon-\nditions.\nT1: computingthe Euclideandistance\nT2: computingthe generativeprobability[16]\nT3: computingthe KL-divergence,equation(13)\nT4: computingthe KL-divergence,equation(37)\nAsabaselinemethod,undertheT1condition,onesim-\nple method used to estimate the typicality of vocal timbre\ncalculated the Euclidean distance between mean feature\nvectors of a song and a song-set. Each mean vector was\ncalculatedfromeachsong,usingthereliablevocalframes,\nand was normalized by subtracting the mean and dividing\nbythe standarddeviationofall meanvectors.\nUnder the T2 condition, one typicality between a song\nand a set of songs is obtained by calculating a genera-tive probability [16] of song Pcalculated using a song-set\nmodel of song Q. This typicality p\ng(P|Q)is deﬁned as\nfollows:\nlogpg(P|Q)=1\nNPNP/summationdisplay\nn=1logp(xP,n|E[θQ],E[φ]),(38)\np(xP,n|E[θQ],E[φ]) =K/summationdisplay\nk=1(E[θQ,k]·E[φk,v]),(39)\nwhere E[·]is the expectation of a Dirichlet distribution,\nNPis the number of frames, and vis the corresponding\nindex(thewordid)ofthe K-dimensional1-of- Kobserva-\ntionvector xb,n.\nThe other two typicalities, under the T3 and T4 condi-\ntions, are calculated Typicality( P,Q)by using equations\n(13) and (37), respectively.\n3.4 Experiment: musical typicality estimation\nWeevaluatedthefourtypicalitycomputingconditions(T1-\nT4)incombinationwiththefollowingthreesong-setmod-\nelingconditions.\nM1: computinga meanvector\nM2: summingthe Dirichlet hyperparameters[16]\nM3: Bayesian estimation of the hyperparameters de-\nscribed in Subsection 2.3\nWe computed typicalities under ﬁve evaluation conditions\nT1+M1, T2+M2, T3+M2, T3+M3, and T4+M3.\nOur typicality evaluation experiment used ﬁve hundred\nsongs by a hundred singers (50 male and 50 female), eachsingersungﬁvesongs. ThesongsaretakenfromtheJPOP\nMDB and each of the songs included only one vocal. ToProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 699Evaluated Firstselection Secondselection Thirdselection Fourthselection Fifthselection\nconditions ρmρfρmfρmρfρmfρmρfρmfρmρfρmfρmρfρmf\nT1+M1 .855 .866 .855 .775 .821 .798 .935 .835 .882 .870 .876 .872 .914 .842 .876\nT2+M2 .924 .930 .860 .905 .921 .866 .953 .918 .879 .925 .938 .875 .945 .910 .864\nT3+M2 .921 .927 .861 .912.921 .871 .951 .919 .880 .924 .935 .876 .944 .907 .865\nT3+M3 .940.961 .931 .910 .961 .926 .962 .955 .944 .936.967 .934 .952 .950 .933\nT4+M3 .936 .973 .942.844 .973 .896.968.962.952.930 .976 .936.970.949 .939\nTable 1. Pearsoncorrelationcoefﬁcientsofahundredsongsundertheﬁveevaluat edconditions(“T4+M3”istheproposed\nmethod)andthe underline meansthe highest value. The songsare randomlyselectedﬁvetimesfrom ﬁve hundred songs.\n \n  \n  \n  \n  \n  \n  \n  \n \n  \n1.41.61.82\n0.010.02\n0.20.40.6\n0.20.40.6\n0.10.20.3\n50 male vocalists\n (50 songs)50 female vocalists\n (50 songs)Typicality\nT1+M1\nT2+M2\n(previous method)\nT3+M3T3+M2\nT4+M3\n(proposed method)Typicality (scaling, range in [0, 1] for each song)\n01\n01\n01\n01\n01\n49:1\nratio\n(male:female)\n1:49\n49:1\n1:49\n49:1\n1:49\n49:1\n1:49\n49:1\n1:49T1+M1\nT2+M2\n(previous method)\nT3+M3T3+M2\nT4+M3\n(proposed method)49:1\nratio\n(male:female)\n1:49\n49:1\n1:49\n49:1\n1:49\n49:1\n1:49\n49:1\n1:49\nFigure 5. Estimated typicalities (ﬁrst selection) and those\nscaledvaluesforeachofthe ﬁveevaluationconditions.\nestimate musical typicality, a hundred songs by different\nsingersarerandomlyselectedﬁvetimes. Then,tointegrate\nor estimate song-set models, ﬁfty songs are randomly se-\nlectedfromthesongswithdifferentratiosofthenumberof\nmale singers to female singers ( 1:4 9,2:4 8,...,49 : 1).\nWhen a model with a high proportion of female songs is\nused,thetypicalityofsongssungbyfemalesishigherthan\nthe typicalityofsongssungbymales(andvice versa).\nEstimated typicality was evaluated against the Pearson\nproduct-moment correlation coefﬁcient between the com-\nputed typicality the ratio of the number of male singers to\nfemale singers with respect to song-set modeling. Beforecomputingthecoefﬁcients,thetypicalityforeachsongwas\nscaled to have values from 0to1for evaluating smooth\ntransition. Let ρ\nm,ρf,a n dρmfdenote the coefﬁcients un-\nder a set of songs consist of 50 songs by male singers, 50\nsongsby female singers, and all 100 songs, respectively.\nThe estimated typicalities and those scaled values are\nshown in Figure 5 for each of the ﬁve evaluation con-\nditions. The Pearson’s correlation coefﬁcients are listed\nin Table 1. The results show that the proposed methodachieved the highest value of the correlation coefﬁcient\n(T4+M3). This means that the proposed method worksbetterthanthebaselinemethodbasedontheEuclideandis-\ntance of mean vectors (T1+M1) and the previous method\nbasedoncomputingthegenerativeprobabilities(T2+M2).\nThe results also show that estimated musical typicality by\nusing the proposed method can reﬂect the ratio between\nthenumberofsongsbelongingtoaclass( e.g.,malesinger)\nand the number of songs belonging to another class ( e.g.,\nfemale singer).\n4 CONCLUSION\nWe proposed a method for estimating musical typicality\nbasedonthetypetheory. Althoughthismethodisusedfor\nquantized acoustic features for vocal timbre in this paper,\nit can be used for other discrete sequence representations\nof music, such as quantized other acoustic features ( e.g.,\nMFCCstorepresentmusicaltimbre/genre),lyricsandmu-\nsicalscore. Itcanalsobeusedwithprobabilisticrepresen-\ntation instead of estimating musical similarities of all pos-\nsible song-pairs by using a model trained from each song,\nforintegratingorcollaborationwithotherprobabilisticap-\nproachasa uniﬁedframework.\nOur deﬁnition of musical typicality was based on the\ncentraltendency[2]whichisonlythedeﬁnitiontobecom-\nputed from the audio data; this is the reason to adopt it. In\nfuture work we expect to deal with two other deﬁnitionsin cognitive psychology are frequency of instantiation and\nideals. The frequency of instantiation is a perspective on\nsocial recognition, that is, things with a lot of exposure on\nmedia or in advertisements are typical, and ideals focuses\nonanidealconditionofthecategory,thatis,thingsthatare\nclose toanideal conditionare typical.\nMusical typicality can be used not only for music-\nlisteningsupportinterfacesuchasretrievinganuniqueness\nsongorvisualizingtypicalities,butalsotodothisbydevel-opingamusic-creationsupportinterfaceenablinghigh/low\ntypicality elements ( e.g., timbre and lyrics) to be used to\nincreaseoriginalityorvisualizetypicalityinordertoavoid\nunwarranted accusations of plagiarism. We also want to\npromote a proactive approach to encountering and appre-\nciating content by developing music-appreciation support\ntechnology that enables people to encounter new content\ninwaysbasedonitstypicalitytoothercontent.\n5 ACKNOWLEDGMENT\nThispaperutilizedtheRWCMusicDatabase(PopularMu-\nsic). Thisworkwassupported in part by CREST,JST.700 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20166 REFERENCES\n[1] Jean-Julien Aucouturier and Francois Pachet. Music\nsimilarity measures: What’s the use? In Proc. ISMIR\n2002, pages157–163, 2002.\n[2] Lawrence W. Barsalou. Ideals, central tendency,\nand frequency of instantiation as determinants of\ngraded structure in categories. journal of Experimen-\ntal Psychology: Learning, Memory, and Cognition ,\n11(4):629–654,1985.\n[3] Christopher M. Bishop. Pattern recognition and ma-\nchine learning . Springer-VerlagNewYork,Inc., 2006.\n[4] David M. Blei, Andrew Y. Ng, and Michael I. Jordan.\nLatentDirichletallocation. Journal of Machine Learn-\ning Research , 3:993–1022, 2003.\n[5] ThomasM.CoverandJoyA.Thomas. Elements of In-\nformation Theory . New-York: Wiley,2006.\n[6] ImreCsisz ´ar. Themethodoftypes. IEEE Trans. on In-\nformation Theory , 44(6):2505–2523, 1998.\n[7] Imre Csisz ´ar and J´anos K orner. Information Theory:\nCoding Theorems for Discrete Memoryless Systems .\nAcademicPress, 1981.\n[8] Hiromasa Fujihara, Masataka Goto, Tetsuro Kitahara,\nand Hiroshi G. Okuno. A modeling of singing voice\nrobust to accompaniment sounds and its application to\nsinger identiﬁcation and vocal-timbre-similarity based\nmusic information retrieval. IEEE Trans. on ASLP ,\n18(3):638–648,2010.\n[9] Masataka Goto, Hiroki Hashiguchi, Takuichi\nNishimura, and Ryuichi Oka. RWC music database:Popular, classical, and jazz music databases. In Proc.\nISMIR 2002 , pages287–288, 2002.\n[10] Masataka Goto and Keiji Hirata. Recent studies on\nmusic information processing. Acoustical Science and\nTechnology (edited by the Acoustical Society of Japan) ,\n25(6):419–425,2004.\n[11] Masatala Goto. A real-time music scene descrip-\ntion system: Predominant-F0 estimation for detect-\ning melody and bass lines in real-world audio signals.\nSpeech Communication , 43(4):311–329, 2004.\n[12] Thomas L. Grifﬁths and Mark Steyvers. Finding sci-\nentiﬁc topics. In Proc. Natl. Acad. Sci. USA (PNAS) ,\nvolume1, pages5228–5235, 2004.\n[13] Peter Knees and Markus Schedl. A survey of mu-\nsicsimilarityandrecommendationfrommusiccontext\ndata. ACM Trans. on Multimedia Computing, Commu-\nnications and Applications , 10(1):1–21, 2013.\n[14] Tomoyasu Nakano, Jun Kato, Masahiro Hamasaki,\nand Masataka Goto. PlaylistPlayer: An interface us-\ning multiple criteria to change the playback order of a\nmusicplaylist. In Proc. ACM IUI 2016 , 2016.\n[15] Tomoyasu Nakano, Kazuyoshi Yoshii, and Masataka\nGoto. Vocal timbre analysis using latent Dirichlet al-\nlocation and cross-gender vocal timbre similarity. In\nProc. ICASSP 2014 , pages5239–5343, 2014.\n[16] Tomoyasu Nakano, Kazuyoshi Yoshii, and Masataka\nGoto. Musical similarity and commonness estimationbased on probabilistic generative models. In Proc.\nIEEE ISM 2015 ,2015.[17] Markus Schedl, Emilia G ´omez, and Juli ´an Urbano.\nMusic informationretrieval: Recent developmentsand\napplications. Foundations and Trends in Information\nRetrieval\n,8(2–3):127–261, 2014.\n[18] Yee Whye Teh. A bayesian interpretation of interpo-\nlatedkneser-ney. Technical Report TRA2/06 ,2006.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 701"
    },
    {
        "title": "Systematic Exploration of Computational Music Structure Research.",
        "author": [
            "Oriol Nieto",
            "Juan Pablo Bello"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417661",
        "url": "https://doi.org/10.5281/zenodo.1417661",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/043_Paper.pdf",
        "abstract": "In this work we present a framework containing open source implementations of multiple music structural seg- mentation algorithms and employ it to explore the hy- per parameters of features, algorithms, evaluation metrics, datasets, and annotations of this MIR task. Besides testing and discussing the relative importance of the moving parts of the computational music structure eco-system, we also shed light on its current major challenges. Additionally, a new dataset containing multiple structural annotations for tracks that are particularly ambiguous to analyze is intro- duced, and used to quantify the impact of specific anno- tators when assessing automatic approaches to this task. Results suggest that more than one annotation per track is necessary to fully address the problem of ambiguity in mu- sic structure research.",
        "zenodo_id": 1417661,
        "dblp_key": "conf/ismir/NietoB16",
        "content": "SYSTEMATIC EXPLORATION OF COMPUTATIONAL MUSIC\nSTRUCTURE RESEARCH\nOriol Nieto\nPandora Media, Inc.\nonieto@pandora.comJuan Pablo Bello\nMusic and Audio Research Laboratory\nNew York University\njpbello@nyu.edu\nABSTRACT\nIn this work we present a framework containing open\nsource implementations of multiple music structural seg-\nmentation algorithms and employ it to explore the hy-\nper parameters of features, algorithms, evaluation metrics,\ndatasets, and annotations of this MIR task. Besides testing\nand discussing the relative importance of the moving parts\nof the computational music structure eco-system, we also\nshed light on its current major challenges. Additionally, a\nnew dataset containing multiple structural annotations for\ntracks that are particularly ambiguous to analyze is intro-\nduced, and used to quantify the impact of speciﬁc anno-\ntators when assessing automatic approaches to this task.\nResults suggest that more than one annotation per track is\nnecessary to fully address the problem of ambiguity in mu-\nsic structure research.\n1. INTRODUCTION\nIn recent years, numerous open source packages have been\npublished to facilitate research in the ﬁeld of music infor-\nmation retrieval. These publications tend to focus on a\nspeciﬁc part of the standard methodology of MIR: audio\nfeature extraction (e.g., Essentia [2], librosa [14]), datasets\n(e.g., SALAMI [22], MSD [1]), evaluation metrics (e.g.,\nmireval [20]), and task-speciﬁc algorithm implementations\n(e.g., segment boundary detection [13], pattern discovery\n[16], beat tracking [4]). What is often missing are inte-\ngrated frameworks where the choice of different moving\nblocks of the whole process (i.e., feature design, algorithm\nimplementations, annotated datasets and evaluation met-\nrics) can be interchanged in a seamless fashion, allowing\nthe type of in-depth comparative studies on state of the\nart techniques that are virtually impossible in the context\nof MIREX1: e.g., what combination of features or pre-\nprocessing stages maximize results? What mixture of ap-\n1One exception would be MARSYAS [25], where feature extraction,\nalgorithm implementations for a limited number of tasks, dataset annota-\ntions, and evaluations coexist in a single environment.\nc/circlecopyrtOriol Nieto, Juan Pablo Bello.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Oriol Nieto, Juan Pablo Bello. “Sys-\ntematic Exploration of Computational Music Structure Research”, 17th\nInternational Society for Music Information Retrieval Conference, 2016.proaches should be used if highly accurate boundary lo-\ncalization is important? What implementations are more\nresilient to changes in data, features or prior information?\nIn this work we introduce an open source framework to\nfacilitate reproducibility and encourage research in music\nstructural segmentation. Building on top of existing open\nprojects [9, 14, 20, 22], this framework combines feature\ncomputation, algorithm implementations, evaluation met-\nrics, and annotated datasets in a standalone software fo-\ncused on this area of MIR. Besides describing the architec-\nture of this framework, we show its potential by compiling\na new dataset composed of poly-annotated tracks carefully\nselected by the presented software, and conducting a se-\nries of experiments to systematically explore the impact of\neach moving part of this task. These new data and explo-\nrations reinforce the notion that this task is highly ambigu-\nous [3], since we show that the ranking of computational\napproaches in terms of performance depends not only on\nwhat feature or dataset is employed, but on which annota-\ntion is used as reference.\nThe rest of this article is organized as follows: In Sec-\ntion 2 the framework is introduced. Section 3 discusses the\ncreation of the new dataset. In Section 4 the explorations\nof the different moving parts of the structural segmenta-\ntion eco-system are presented. Finally, in Section 5, the\nconclusions are drawn.\n2. MUSIC STRUCTURE ANALYSIS\nFRAMEWORK\nMSAF2is an open source framework written in Python\nthat allows to thoroughly analyze the entire music structure\nsegmentation eco-system. In this section we provide an\noverview of this MIR task and a description of the most\nrelevant characteristics of this framework.\n2.1 Structural Segmentation\nThis task, whose main goal is to automatically identify\nthe large-scale, non-overlapping segments of a given au-\ndio signal (e.g., verse, chorus), has been investigated in\nMIR for over a decade [19], and nowadays it is still one\nof the most active in MIREX [23]. Potential applications\nto motivate its research are numerous, e.g., improve intra-\ntrack navigation, yield enhanced segment-level music rec-\n2https://github.com/urinieto/msaf547ommendation systems, produce educational visualisation\ntools to better understand musical pieces. This task is of-\nten divided in two subproblems: boundary detection and\nstructural grouping . The former identiﬁes the beginning\nand end times of each music segment within a piece, and\nthe latter labels these segments based on their acoustic sim-\nilarity.\nSeveral open source implementations to approach this\nproblem have been published [10, 12, 13, 26], but given\nthe differences in feature extraction, datasets, and evalu-\nation metrics, it can be challenging to easily compare their\nresults (e.g., Weiss’ implementation [26] expects features\ncomputed with Ellis’ code [5]; Levy’s implementation [10]\nis only available in the form of a Vamp Plugin; McFee’s\npublication [12] reports non-standard evaluation metrics\nwith the ﬁrst and last boundary removed). Our proposed,\nopen-source MSAF seeks to address these issues by inte-\ngrating these various components.\nIn the following subsections, the main parts of this\nframework are described. MSAF is written such that any\nof these parts could be easily extended.\n2.2 Features\nMost music structure algorithms accept different types of\nfeatures in order to discover structural relations in har-\nmony, timbre, loudness or a combination of them. Here we\nlist the set of features that MSAF can compute by making\nuse of librosa [14]: Pith Class Proﬁles (PCPs, representing\nharmony), Mel-Frequency Cepstral Coefﬁcients (MFCCs,\nrepresenting timbre), Tonal Centroids (or Tonnetz [7], rep-\nresenting harmony), and Constant-Q Transform (CQT, rep-\nresenting harmony, timbre and loudness).\nEach of these features depend on additional analysis pa-\nrameters such as sampling rate, FFT size, and hop size.\nFurthermore, a beat-tracker [4] (contained in librosa) is\nemployed to aggregate all the features at a beat level, thus\nobtaining the so-called beat-synchronous representations.\nThis process, which is common in structural segmentation,\nreduces the number of feature vectors while introducing\ntempo invariance. In this work we rely on this type of fea-\ntures exclusively, even though MSAF can operate both on\nbeat- or frame-synchronous descriptors.\n2.3 Algorithms\nAlgorithms of this task are commonly classiﬁed based\non the subtask that they aim to solve. MSAF includes:\nseven algorithms that detect boundaries, and ﬁve that group\nstructure (see Table 1).\nThe implementations in MSAF are either forked from\nthe public repositories of their original publications [10,\n12, 13, 26] or implemented from scratch when no access\nto the source code is available. Some differences in the\nresults might arise given the difﬁculty of exactly recreating\nall implementation details, even though these differences\nappear to be minor.Algorithm Boundary Grouping\n2D-Fourier Magnitude Coeffs [15] No Yes\nCheckerboard Kernel [6] Yes No\nConstrained Cluster [10] Yes Yes\nConvex NMF [18] Yes Yes\nLaplacian Segmentation [12] Yes Yes\nOrdinal LDA [13] Yes No\nShift Invariant PLCA [26] Yes Yes\nStructural Features [21] Yes No\nTable 1 :Approaches included in MSAF and used in the experi-\nments.\n2.4 Evaluation Metrics\nStructural segmentation employs multiple metrics to eval-\nuate each of its two subproblems. For boundary detection,\nthe Hit Rate is the most standard one, where the estimated\nboundaries are considered “hits” if they fall within a cer-\ntain time window from the reference ones. This yields Pre-\ncision (how many estimated boundaries are correct) and\nRecall (how many reference boundaries were estimated)\nscores, which are weighted with the standard F-measure.\nThe time windows are typically 3 or 0.5 seconds. More-\nover, sometimes the ﬁrst and last boundaries are “trimmed”\n(i.e., not considered) given the fact that they should corre-\nspond to the beginning and end of the track, which should\nbe trivial to detect. It has been shown that having a stronger\nweight on Precision than Recall tends to better align with\nperception [17], therefore this weight parameter is also part\nof MSAF. The other standard metric to report the quality\nof the boundaries is the Median Deviation [24], where the\nmedian deviation from each estimated boundary to its clos-\nest reference, and vice versa, are reported.\nThe most standard metric to assess the quality of the\nstructural grouping subproblem is the Pairwise Frame\nClustering [10]. This metric compares each pair of frames\nby checking whether they belong to the same label (or clus-\nter), both for the estimation and reference. The ratio be-\ntween the two sets of pairs over the number of similar pairs\nin the estimation yields the Precision metric, while Recall\nis the ratio between the two sets over the number of sim-\nilar pairs in the reference. Again, the F-measure weights\nthese two scores. Finally, an alternative metric named Nor-\nmalized Conditional Entropy [11], based on the entropy of\neach frame between the estimation and reference, is also\nreported. This metric is formed by the under- and over-\nsegmentation scores, which, again, can be compacted in a\nsingle score with the F-measure.\nThese metrics are reported in MIREX, and are trans-\nparently implemented in mir eval [20], which MSAF em-\nploys.\n2.5 Datasets\nThe following annotated datasets are the most common\nfor assessing structural segmentation: Isophonics – 298\nannotated tracks mostly of popular music3; SALAMI –\ntwo human references plus three levels of annotation per\n3http://isophonics.net/datasets548 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016track [22]. It contains 769 musical pieces ranging from\nwestern popular music to world music4; The Beatles TUT\n– reﬁned version of 174 annotations of The Beatles cor-\nrected and published by members of the Tampere Univer-\nsity of Technology5.\nAdditionally, we make use of these uncommon and\nnovel datasets: Cerulean – 104 songs collected by a com-\npany, subjectively deemed to be challenging tracks within\na large collection. The genre of the songs varies from clas-\nsical to heavy metal; Epiphyte – another industrial set of\n1002 tracks composed mainly of pop music songs; Sargon\n– small set of 30 minutes of heavy metal tracks released\nunder a Creative Commons license; SPAM – new dataset\ndiscussed in the next section.\nAll of these datasets are converted to the JAMS for-\nmat [9], which is the default format that MSAF employs,\nand are publicly available in the MSAF repository (except\nCerulean and Epiphyte, which are privately owned). This\nformat is JSON-compatible and allows for multiple anno-\ntations in a single ﬁle for numerous tasks operating on a\ngiven audio track, making it ideal for the purposes of this\nwork.\n3. STRUCTURAL POLY-ANNOTATIONS OF\nMUSIC\nSPAM is a new dataset composed of 50 tracks sampled\nfrom a large collection containing all the previously dis-\ncussed sets (a total of 2,173 tracks). Following an approach\ninspired by [8], all MSAF algorithms were run on these\n2,173 tracks. The tracks were then ranked based on the\naverage Hit Rate F-Measure with 3 seconds window (i.e.,\nthe most standard metric for boundary detection) across all\nalgorithms. Formally, the rank is computed using the mean\nground-truth precision (MGP) score, deﬁned as follows:\nMGP i(B, g) =1\nMM/summationdisplay\nj=1g(bij) (1)\nwhere B∈RN,Mis the matrix containing all the boundary\nestimations bij∈Bfor track i∈[1, N]using algorithm\nj∈[1, M], andgis the evaluation function (i.e., Hit Rate\nat 3 seconds). Ranking the tracks using this metric yields\na list sorted by how challenging these tracks are for auto-\nmatic segmentation.\nThe SPAM dataset is composed by the 45 most chal-\nlenging tracks (i.e., the 45 at the bottom of the ranked list)\nplus the 5 least challenging (i.e., the top 5 tracks in the list).\nThe number of tracks was kept small to facilitate the col-\nlection of ﬁve additional annotations using the same guide-\nlines as in SALAMI. These ﬁve annotations were collected\nby music students (four graduates and one undergraduate)\nfrom the Steinhardt School at New York University, with\nan average number of years in musical training of 15.3 ±\n4.9, and with at least 10 years of experience as players\n4Only the ﬁrst half of the full SALAMI annotations were used, since\nthe authors did not have access to the rest of audio ﬁles.\n5http://www.cs.tut.fi/sgn/arg/paulus/\nbeatles sections TUT.zipof a musical instrument. The goal was to create a set in\nwhich to explore the variability of structural annotations\nacross subjects, focusing on the most challenging tracks\n(45) while still having a reduced control group (5). This\nsplit could foster further investigation on the differences\nbetween easy andchallenging tracks.\nThe type of music ranges between jazz and blues, clas-\nsical, world music, rock, western pop, and live recordings.\nDue to legal copyright issues, the audio of these tracks\nis not available, however, the features described in Sec-\ntion 2.2 are included along with the ﬁve annotations for\neach of the 50 tracks of SPAM.\n4. EXPERIMENTS\nIn this section we report a series of experiments to fur-\nther explore the task of structural segmentation carried out\nusing MSAF, classiﬁed by the moving parts described pre-\nviously. Each experiment can be subdivided based on the\nsubproblems of boundary detection and structural group-\ning. For each experiment the default parameters are the\nfollowing, unless otherwise speciﬁed: sampling rate is\n11025Hz; FFT and hop sizes are 2048 and 512 samples,\nrespectively; default feature type is beat-synchronous PCP;\nnumber of octaves and starting frequency for the PCPs are\n7 and 27.5Hz, respectively; number of MFCCs is ﬁxed to\n14; number of CQT bins, starting at 27.5Hz, is 87; eval-\nuation metrics are the F-measures of the Hit Rate with\n3 seconds window and the Pairwise Frame Clustering for\nboundary detection and structural grouping, respectively;\nthe boundaries used as input to the structural grouping al-\ngorithms are annotated; and the default dataset is The Bea-\ntles TUT. Code to reproduce the plots and results is avail-\nable online6.\n4.1 Features\nWe start by running all MSAF algorithms7using differ-\nent types of features. In Figure 1 we can see, as expected,\nthat the average scores of the boundary algorithms vary\nbased on the feature types. This aligns with the results of a\ntwo-way ANOV A on the F-measure of the Hit Rate using\nthe algorithms and features as factors, where the effect on\nthe type of features is signiﬁcant ( F(3,3460) = 4 .20, p <\n0.01). Also as expected, there is signiﬁcant interaction be-\ntween factors ( F(12,3460) = 15 .15, p < 0.01), which can\nbe seen in the plot when observing the poor performance\nof the Constrained Clustering algorithm for the Constant-Q\nfeatures in comparison with the rest of the features.\nA similar behavior occurs when analyzing the perfor-\nmance of the structural grouping algorithms, as can be\nseen in Figure 2. The two-way ANOV A conﬁrms de-\npendency of the type of features for these algorithms\n(F(3,2768) = 18 .07, p < 0.01), with signiﬁcant inter-\naction ( F(9,2768) = 14 .5, p < 0.01) mostly due to the\nbehavior, again, of the Constrained Clustering algorithm\n6https://github.com/urinieto/msaf-experiments\n7Except Ordinal LDA and Laplacian Segmentation, since they only\naccept a speciﬁc combination of features as input.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 549CQT PCP MFCC Tonnetz\nFeature0.450.500.550.600.650.700.75Hit Rate F-measure\nCheckerboard Kernel\nConstrained Clustering\nConvex NMF\nShift-Invariant PLCA\nStructural FeaturesFigure 1 : Boundary algorithms’ performance depending\non the type of features.\nCQT PCP MFCC Tonnetz\nFeature0.500.550.600.650.700.750.80Pairwise Frame Clustering\nF-measure\n2D Fourier Magnitude Coeffs\nConstrained Clustering\nConvex NMF\nShift-Invariant PLCA\nFigure 2 : Structural algorithms’ performance depending\non the type of features.\nwhen using Constant-Q features. Convex NMF still per-\nforms slightly better with this type of features, while the\nrest of the algorithms seem to be optimized to operate on\nthe features suggested in their original publications.\nThis experiment yields two major points: (i) features\ndescribing timbre information (CQT, MFCCs) seem to\nbe slightly better than those describing pitch information\n(PCPs, Tonnetz) for boundary detection, but the reverse\nseems to be true for clustering, and (ii) the Structural Fea-\ntures and Convex NMF methods obtain better results when\nusing CQT, while in their original publications they rec-\nommend using harmonic features such as PCPs.\n4.2 Algorithms\nThe quality of the segment boundaries can impact the re-\nsults of the structural grouping subproblem [21]. MSAF\nlets us explore this by using the output of several bound-\nary algorithms as input to the structural algorithms. Fig-\nure 3 shows average scores of the structural algorithms in\nMSAF. Additionally, the results with annotated boundaries\nare used and plotted in the ﬁrst column. The boundary\nmethods are sorted from left to right based on their perfor-\nmance on The Beatles TUT dataset. As expected, the qual-\nity of the boundary detection process affects the structural\nsubproblem. A two-way ANOV A on the F-measure of the\nPairwise Frame Cluster, with boundary and structural algo-\nrithms as factors, conﬁrms this ( F(7,6920) = 183 .10, p <\n0.01). A signiﬁcant interaction between the two factors is\nalso present ( F(28,6920) = 16 .44, p < 0.01), suggest-\ning that the ranking of the algorithms will vary depend-\ning on the boundaries employed. This is conﬁrmed by the\nFriedman test, which ranks the structural algorithms us-\ning Structural Features boundaries ( F(4) = 242 .31, p <\nHuman SF Laplacian CC OLDA Checkboard C-NMF SI-PLCA\nBoundary Detection Algorithm0.40.50.60.70.8Pairwise Frame Cluster F-measure\n2D Fourier Magnitude Coeffs\nConstrained Clustering\nConvex NMF\nLaplacian Segmentation\nShift-Invariant PLCAFigure 3 : Performance of the structural algorithms con-\ntained in MSAF when using different types of previously\nestimated boundaries as input.\n0.01) differently than when using Convex NMF bound-\naries ( F(4) = 225 .05, p < 0.01). For example, the\n2D Fourier Magnitude Coefﬁcients method becomes lower\nranked than Convex NMF in the latter case, as can be seen\nin the plot.\nInteresting conclusions can be drawn: ﬁrst, some struc-\ntural algorithms are more robust to the quality of the\nboundaries than others (e.g., 2D-FMC sees a strong impact\non its performance when not using annotated boundaries,\nespecially when compared with the Laplacian method).\nSecond, the best performing boundary algorithm will not\nnecessarily make the results of a structural algorithm bet-\nter, as can be seen in the structural results of C-NMF and\nSI-PLCA. To exemplify this, note how SF (which tends\nto outperform all other methods in terms of the standard\nmetric, see Figure 1) produce, in fact, one of the lowest re-\nsults in structural grouping for the C-NMF method. On the\nother hand, the Laplacian method (which outputs bound-\naries that are comparable to the ones by the Checkerboard\nkernel), obtains results on the structural part that are much\nbetter than those by SF. Finally, depending on the bound-\naries used, structural algorithms will be ranked differently\nin terms of performance (especially when using annotated\nboundaries as input). This is something that is not cur-\nrently taken into account in the MIREX competition, and\nmight be an interesting asset to add in the future for a\ndeeper evaluation of the subtask of structural grouping.\n4.3 Evaluation Metrics\nIn this section we explore the different results obtained\nby MSAF algorithms when assessed using the available\nmetrics. For boundary detection, the metrics described\nin Section 2.4 are explored, which are depicted in Fig-\nure 4a as “Dev E2R” for the median deviations from Esti-\nmations to References (R2E for the swapped version), and\n“HRn” for the Hit Rate with a time window of nsec-\nonds (the wandtindicate the weighted and trimmed ver-\nsions, respectively). The median deviations are divided\nby 4 in order to normalize the scores within the range\nof 0 to 1, and then inversed in order to indicate a bet-\nter performance with a higher score. As expected, scores\nare signiﬁcantly different depending on the metric used,550 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Dev_E2R Dev_R2E HR_3 HR_3w HR_3t HR_0.5 HR_0.5w HR_0.5t\nMetrics0.10.20.30.40.50.60.70.8Scores\nCheckerboard Kernel\nConstrained Clustering\nConvex NMF\nLaplacian Segmentation\nOrdinal LDA\nShift-Invariant PLCA\nStructural Features(a) Boundaries\nPWF NCE\nMetrics0.50.60.70.80.91.0Scores\n2D Fourier Magnitude Coeffs\nConstrained Clustering\nConvex NMF\nLaplacian Segmentation\nShift-Invariant PLCA(b) Structure\nFigure 4 : Scores of MSAF algorithms depending on eval-\nuation metrics.\nwhich is conﬁrmed by the two-way ANOV A of the scores\nwith metrics and algorithms as factors (the metric effect\nisF(7,9688) = 458 , p < 0.01). But perhaps more in-\nteresting is the fact that some algorithms perform better\nwith some metrics than others (as suggested by the in-\nteraction effect of the two-way ANOV A: F(42,9688) =\n11.24, p < 0.01). For example, SF is the best algorithm\nin terms of the Hit Rate with a 3 seconds window, but it\nis surpassed by the Laplacian and OLDA algorithms when\nusing a shorter window of 0.5, as the Friedman test con-\nﬁrms ( F(6) = 200 .13, p < 0.01) for the ranking of the\nHit Rate with 3 seconds, which is different than the one\nfor 0.5 seconds ( F(6) = 210 .67, p < 0.01). Therefore,\nwe can state that, amongst these algorithms, SF is ideal\nif precise boundary localization is not necessary (HR 3),\nwhereas Laplacian outperforms other methods when this\nlocalization has to be accurate (HR 0.5).\nIn terms of structural algorithms, two metrics (Pairwise\nFrame Clustering and Normalized Conditional Entropies)\nare depicted in Figure 4b. A similar behavior occurs\nhere, where algorithms will be ranked differently depend-\ning on the metric of choice (Friedman test for the struc-\ntural algorithms evaluated using the PWF yields F(4) =\n230.11, p < 0.01and different ranking than the one for\nNCE, which results in F(4) = 215 .12, p < 0.01). Interest-\ningly, all algorithms except Laplacian tend to yield better\nresults when using the NCE scores. Given these results, it\nwould be hard to ﬁrmly conclude what the best structural\nalgorithm is for this dataset, since 2D-FMC outperforms\nLaplacian when evaluated using the NCE scores, which is\nthe opposite behavior when using the PWF.\n4.4 Datasets\nIn Figure 5, the average scores for all boundary algorithms\nin MSAF on different datasets are depicted. If a dataset\ncontains more than one annotation per track, the ﬁrst an-\nnotator in their JAMS ﬁles is used. As expected, differ-\nent results are obtained depending on the dataset, as con-\nﬁrmed by the two-way ANOV A on the evaluation met-\nric with dataset and algorithm as factors (dataset effect:\nF(5,16604) = 512 .18, p < 0.01). From the plot it can\nBeatles Cerulean Epiphyte Isophonics SALAMI Sargon SPAM\nDataset0.350.400.450.500.550.600.650.70Hit Rate F-measure\nCheckerboard Kernel\nConstrained Clustering\nConvex NMF\nLaplacian Segmentation\nOrdinal LDA\nShift-Invariant PLCA\nStructural FeaturesFigure 5 : Boundary algorithms’ performance depending\non dataset.\nBeatles Cerulean Epiphyte Isophonics SALAMI Sargon SPAM\nDataset0.50.60.70.8Pair-wise Frame Clustering F-measure\n2D Fourier Magnitude Coeffs\nConstrained Clustering\nConvex NMF\nLaplacian Segmentation\nShift-Invariant PLCA\nFigure 6 : Structural algorithms’ performance depending\non dataset.\nalso be seen that some algorithms perform better than oth-\ners depending on the dataset, which might indicate that\nthey are tuned to solve this problem for a speciﬁc type of\nmusic. Overall, some datasets seem generally more chal-\nlenging than others, the SPAM dataset being the one that\nobtains the worst results, which aligns with the method\nused to collect their data explained in Section 3.\nIn terms of the structural algorithms (Figure 6), the two-\nway ANOV A identiﬁes signiﬁcant variation, with a rele-\nvant effect on the dataset of F(6,11875) = 133 .16, p <\n0.01. Contrasting with the boundary results, the scores for\nthe SPAM dataset are, on average, one of the highest in\nterms of structural grouping. This, by itself, warrants dis-\ncussion, since this dataset was chosen to be particularly\nchallenging from a machine point of view, but only when\ntaking the boundary detection subproblem into account.\nWhat these results suggest is that, (i) given the human ref-\nerence boundaries (which are supposed to be difﬁcult to\ndetect), the structural algorithms perform well at cluster-\ning the predeﬁned segments, and/or (ii) we might need a\nbetter evaluation metric for the structural subproblem.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 5510 1 2 3 4\nAnnotator0.360.380.400.420.440.460.480.50Hit Rate F-measure\nCheckerboard Kernel\nConstrained Clustering\nConvex NMF\nLaplacian Segmentation\nOrdinal LDA\nShift-Invariant PLCA\nStructural FeaturesFigure 7 : Scores of the boundary algorithms for each hu-\nman reference in the SPAM dataset.\n4.5 Human References\nThe last experiment focuses on analyzing the amount of\nvariation of the MSAF algorithms depending on the an-\nnotator used. For this purpose, the ﬁve annotations per\ntrack of the SPAM dataset become particularly helpful.\nStarting with the boundaries, we can see in Figure 7 how\nvariable the scores become when using different annota-\ntors for the same exact set of audio ﬁles. The two-way\nANOV A of HR 3 with annotators and algorithms as fac-\ntors validates this by reporting a signiﬁcant annotator ef-\nfect (F(4,1705) = 4 .05, p < 0.01). This suggests that\nsubjectivity plays an important role for this subtask, and\nmore than one set of boundaries would be actually valid\nfrom a human perspective. Therefore, the idea of a sin-\ngle “ground-truth” for boundary detection can potentially\nbe misleading. Given this amount of variation depending\non the annotator, it is interesting to see that the ranking\nalso changes, making it difﬁcult to compare algorithm be-\nhaviors. Even though the Laplacian algorithm performs\nthe best for the majority of annotators, it is ranked as sec-\nond when using annotator 0 by the Friedman test ( F(5) =\n21.24, p < 0.01), while it is ranked as ﬁrst for the rest\nof annotators. These results suggest that, given the sub-\njectivity effect in this task, it is indeed important to col-\nlect as many references as possible in order to better assess\nboundary algorithms.\nLastly, the results of the structural algorithms contrast\nwith the previously discussed ones. In this case, there is\nlittle dependency on the human reference chosen, as there\nis no signiﬁcant effect for the annotator factor in the two-\nway ANOV A ( F(4,1225) = 1 .08, p= 0.37), without sig-\nniﬁcant interaction ( F(16,1225) = 0 .93, p= 0.53). This\nadvocates that the structural grouping subproblem, when\napplied to a dataset where the grouping is not particularly\nchallenging (as depicted in Figure 6), is not as affected\nby subjectivity as the boundary detection one, even though\nfurther analysis with larger and more challenging datasets\n—and perhaps with automatically estimated boundaries—\nshould be performed in order to conﬁrm this.\n0 1 2 3 4\nAnnotator0.550.600.650.700.750.800.85Pairwise Frame Cluster F-measure\n2D Fourier Magnitude Coeffs\nConstrained Clustering\nConvex NMF\nLaplacian Segmentation\nShift-Invariant PLCAFigure 8 : Scores of the structural algorithms for each hu-\nman reference in the SPAM dataset.\n5. CONCLUSIONS\nWe have presented an open-source framework that facili-\ntates the task of analyzing, assessing, and comparing multi-\nple implementations of structural segmentation algorithms\nand have employed it to compile a new poly-annotated\ndataset and to systematically explore the different moving\nparts of this MIR task. These experiments show that the\nrelative rankings between algorithms tend to change de-\npending on these parts, making it difﬁcult to choose the\n“best” computational approach. Results also illustrate the\nproblem of ambiguity in this task, and it is our hope that the\nnew SPAM dataset will help researchers to further address\nthis problem. In the future, we wish not only to include\nmore algorithms in this open framework, but to have ac-\ncess to similar frameworks to encourage research on other\nareas of MIR.\n6. REFERENCES\n[1] Thierry Bertin-Mahieux, Daniel P. W. Ellis, Brian Whitman,\nand Paul Lamere. The Million Song Dataset. In Proc of the\n12th International Society of Music Information Retrieval ,\npages 591–596, Miami, FL, USA, 2011.\n[2] Dmitry Bogdanov, Nicolas Wack, Emilia G ´omez, Sankalp\nGulati, Perfecto Herrera, Oscar Mayor, Gerard Roma, Justin\nSalamon, Jos ´e Zapata, and Xavier Serra. Essentia: An Audio\nAnalysis Library for Music Information Retrieval. In Proc.\nof the 14th International Society for Music Information Re-\ntrieval Conference , pages 493–498, Curitiba, Brazil, 2013.\n[3] Michael J. Bruderer. Perception and Modeling of Segment\nBoundaries in Popular Music . PhD thesis, Universiteits-\ndrukkerij Technische Universiteit Eindhoven, 2008.\n[4] Daniel P. W. Ellis. Beat Tracking by Dynamic Programming.\nJournal of New Music Research , 36(1):51–60, 2007.\n[5] Daniel P. W. Ellis and Graham E. Poliner. Identifying ’Cover\nSongs’ with Chroma Features and Dynamic Programming\nBeat Tracking. In Proc. of the 32nd IEEE International Con-\nference on Acoustics Speech and Signal Processing , pages\n1429–1432, Honolulu, HI, USA, 2007.\n[6] Jonathan Foote. Automatic Audio Segmentation Using a\nMeasure Of Audio Novelty. In Proc. of the IEEE Interna-\ntional Conference of Multimedia and Expo , pages 452–455,\nNew York City, NY , USA, 2000.552 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016[7] Christopher Harte, Mark Sandler, and Martin Gasser. Detect-\ning Harmonic Change in Musical Audio. In Proceedings of\nthe 1st ACM Workshop on Audio and Music Computing Mul-\ntimedia , pages 21–26, Santa Barbara, CA, USA, 2006. ACM\nPress.\n[8] Andr ´e Holzapfel, Matthew E. P. Davies, Jos ´e R. Zapata,\nJ. Lobato Oliveira, and Fabien Gouyon. Selective Sampling\nfor Beat Tracking Evaluation. IEEE Transactions On Audio,\nSpeech, And Language Processing , 20(9):2539–2548, 2012.\n[9] Eric J. Humphrey, Justin Salamon, Oriol Nieto, Jon Forsyth,\nRachel M. Bittner, and Juan P. Bello. JAMS: A JSON Anno-\ntated Music Speciﬁcation for Reproducible MIR Research. In\nProc. of the 15th International Society for Music Information\nRetrieval Conference , pages 591–596, Taipei, Taiwan, 2014.\n[10] Mark Levy and Mark Sandler. Structural Segmentation of\nMusical Audio by Constrained Clustering. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n16(2):318–326, feb 2008.\n[11] Hanna Lukashevich. Towards Quantitative Measures of Eval-\nuating Song Segmentation. In Proc. of the 10th Interna-\ntional Society of Music Information Retrieval , pages 375–\n380, Philadelphia, PA, USA, 2008.\n[12] Brian McFee and Daniel P. W. Ellis. Analyzing Song Struc-\nture with Spectral Clustering. In Proc. of the 15th Interna-\ntional Society for Music Information Retrieval Conference ,\npages 405–410, Taipei, Taiwan, 2014.\n[13] Brian McFee and Daniel P. W. Ellis. Learnign to Segment\nSongs With Ordinal Linear Discriminant Analysis. In Proc. of\nthe 39th IEEE International Conference on Acoustics Speech\nand Signal Processing , pages 5197–5201, Florence, Italy,\n2014.\n[14] Brian Mcfee, Colin Raffel, Dawen Liang, Daniel P. W. Ellis,\nMatt Mcvicar, Eric Battenberg, and Oriol Nieto. librosa: Au-\ndio and Music Signal Analysis in Python. In Proc. of the 14th\nPython in Science Conference , pages 1–7, Austin, TX, USA,\n2015.\n[15] Oriol Nieto and Juan Pablo Bello. Music Segment Similar-\nity Using 2D-Fourier Magnitude Coefﬁcients. In Proc. of the\n39th IEEE International Conference on Acoustics Speech and\nSignal Processing , pages 664–668, Florence, Italy, 2014.\n[16] Oriol Nieto and Morwaread M. Farbood. Identifying Poly-\nphonic Patterns From Audio Recordings Using Music Seg-\nmentation Techniques. In Proc. of the 15th International\nSociety for Music Information Retrieval Conference , pages\n411–416, Taipei, Taiwan, 2014.\n[17] Oriol Nieto, Morwaread M. Farbood, Tristan Jehan, and\nJuan Pablo Bello. Perceptual Analysis of the F-measure for\nEvaluating Section Boundaries in Music. In Proc. of the 15th\nInternational Society for Music Information Retrieval Con-\nference , pages 265–270, Taipei, Taiwan, 2014.\n[18] Oriol Nieto and Tristan Jehan. Convex Non-Negative Matrix\nFactorization For Automatic Music Structure Identiﬁcation.\nInProc. of the 38th IEEE International Conference on Acous-\ntics Speech and Signal Processing , pages 236–240, Vancou-\nver, Canada, 2013.\n[19] Jouni Paulus, Meinard M ¨uller, and Anssi Klapuri. Audio-\nBased Music Structure Analysis. In Proc of the 11th Inter-\nnational Society of Music Information Retrieval , pages 625–\n636, Utrecht, Netherlands, 2010.[20] Colin Raffel, Brian Mcfee, Eric J. Humphrey, Justin Salamon,\nOriol Nieto, Dawen Liang, and Daniel P. W. Ellis. mir eval:\nA Transparent Implementation of Common MIR Metrics. In\nProc. of the 15th International Society for Music Information\nRetrieval Conference , pages 367–372, Taipei, Taiwan, 2014.\n[21] Joan Serr `a, Meinard M ¨uller, Peter Grosche, and Josep Llu ´ıs\nArcos. Unsupervised Music Structure Annotation by Time\nSeries Structure Features and Segment Similarity. IEEE\nTransactions on Multimedia, Special Issue on Music Data\nMining , 16(5):1229 – 1240, 2014.\n[22] Jordan B. Smith, J. Ashley Burgoyne, Ichiro Fujinaga, David\nDe Roure, and J. Stephen Downie. Design and Creation of\na Large-Scale Database of Structural Annotations. In Proc.\nof the 12th International Society of Music Information Re-\ntrieval , pages 555–560, Miami, FL, USA, 2011.\n[23] Jordan B. L. Smith and Elaine Chew. A Meta-Analysis of the\nMIREX Structure Segmentation Task. In Proc. of the 14th\nInternational Society for Music Information Retrieval Con-\nference , Curitiba, Brazil, 2013.\n[24] Douglas Turnbull, Gert Lanckriet, Elias Pampalk, and Masa-\ntaka Goto. A Supervised Approach for Detecting Boundaries\nin Music Using Difference Features and Boosting. In Proc. of\nthe 5th International Society of Music Information Retrieval ,\npages 42–49, Vienna, Austria, 2007.\n[25] George Tzanetakis and Perry Cook. MARSYAS: a frame-\nwork for audio analysis. Organised Sound , 4(3):169–175,\n2000.\n[26] Ron Weiss and Juan Pablo Bello. Unsupervised Discovery\nof Temporal Structure in Music. IEEE Journal of Selected\nTopics in Signal Processing , 5(6):1240–1251, 2011.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 553"
    },
    {
        "title": "Musical Note Estimation for F0 Trajectories of Singing Voices Based on a Bayesian Semi-Beat-Synchronous HMM.",
        "author": [
            "Ryo Nishikimi",
            "Eita Nakamura",
            "Katsutoshi Itoyama",
            "Kazuyoshi Yoshii"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1418023",
        "url": "https://doi.org/10.5281/zenodo.1418023",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/004_Paper.pdf",
        "abstract": "This paper presents a statistical method that estimates a se- quence of discrete musical notes from a temporal trajectory of vocal F0s. Since considerable effort has been devoted to estimate the frame-level F0s of singing voices from music audio signals, we tackle musical note estimation for those F0s to obtain a symbolic musical score. A na¨ıve approach to musical note estimation is to quantize the vocal F0s at a semitone level in every time unit (e.g., half beat). This approach, however, fails when the vocal F0s are signifi- cantly deviated from those specified by a musical score. The onsets of musical notes are often delayed or advanced from beat times and the vocal F0s fluctuate according to singing expressions. To deal with these deviations, we pro- pose a Bayesian hidden Markov model that allows musical notes to change in semi-synchronization with beat times. Both the semitone-level F0s and onset deviations of musi- cal notes are regarded as latent variables and the frequency deviations are modeled by an emission distribution. The musical notes and their onset and frequency deviations are jointly estimated by using Gibbs sampling. Experimen- tal results showed that the proposed method improved the accuracy of musical note estimation against baseline meth- ods.",
        "zenodo_id": 1418023,
        "dblp_key": "conf/ismir/NishikimiNIY16",
        "content": "MUSICAL NOTE ESTIMATION FOR F0 TRAJECTORIES OF SINGING\nVOICES BASED ON A BAYESIAN SEMI-BEAT-SYNCHRONOUS HMM\nRyo Nishikimi1Eita Nakamura1Katsutoshi Itoyama1Kazuyoshi Yoshii1\n1Graduate School of Informatics, Kyoto University , Japan\n{nishikimi, enakamura }@sap.ist.i.kyoto-u.ac.jp, {itoyama, yoshii }@kuis.kyoto-u.ac.jp\nABSTRACT\nThis paper presents a statistical method that estimates a se-\nquence of discrete musical notes from a temporal trajectory\nof vocal F0s. Since considerable effort has been devoted to\nestimate the frame-level F0s of singing voices from music\naudio signals, we tackle musical note estimation for those\nF0s to obtain a symbolic musical score. A na ¨ıve approach\nto musical note estimation is to quantize the vocal F0s at\na semitone level in every time unit ( e.g., half beat). This\napproach, however, fails when the vocal F0s are signiﬁ-\ncantly deviated from those speciﬁed by a musical score.\nThe onsets of musical notes are often delayed or advanced\nfrom beat times and the vocal F0s ﬂuctuate according to\nsinging expressions. To deal with these deviations, we pro-\npose a Bayesian hidden Markov model that allows musical\nnotes to change in semi-synchronization with beat times.\nBoth the semitone-level F0s and onset deviations of musi-\ncal notes are regarded as latent variables and the frequency\ndeviations are modeled by an emission distribution. The\nmusical notes and their onset and frequency deviations are\njointly estimated by using Gibbs sampling. Experimen-\ntal results showed that the proposed method improved the\naccuracy of musical note estimation against baseline meth-\nods.\n1. INTRODUCTION\nSinging voice analysis is one of the most important topics\nin the ﬁeld of music information retrieval because singing\nvoice usually forms the melody line of popular music and\nit has a strong impact on the mood and impression of a\nmusical piece. The widely studied tasks in singing voice\nanalysis are fundamental frequency (F0) estimation [1, 4,\n5,8,10,15,22] and singing voice separation [9,13] for mu-\nsic audio signals. These techniques can be used for singer\nidentiﬁcation [11, 23], Karaoke systems based on singing\nvoice suppression [2,20], and a music listening system that\nhelps a user focus on a particular musical element ( e.g., vo-\ncal part) for deeper music understanding [7].\nc⃝Ryo Nishikimi, Eita Nakamura, Katsutoshi Itoyama,\nKazuyoshi Yoshii. Licensed under a Creative Commons Attribution 4.0\nInternational License (CC BY 4.0). Attribution: Ryo Nishikimi, Eita\nNakamura, Katsutoshi Itoyama, Kazuyoshi Yoshii. “Musical Note Esti-\nmation for F0 Trajectories of Singing V oices based on a Bayesian Semi-\nbeat-synchronous HMM”, 17th International Society for Music Informa-\ntion Retrieval Conference, 2016.\nMusical Score Component\n+ Onset Deviation \n+ Frequency DeviationTime\nTime\nTimePitch\nPitch\nPitchFigure 1 : The process generating F0 trajectories of singing\nvoices.\nIn this study we tackle a problem called musical note es-\ntimation that aims to recover a sequence of musical notes\nfrom an F0 trajectory of singing voices. While a lot of\neffort has been devoted to F0 estimation of singing voices,\nmusical note estimation should be investigated additionally\nto complete automatic music transcription, i.e., convert the\nestimated F0 trajectory to a musical score containing only\ndiscrete symbols. If beat information is available, a na ¨ıve\napproach to this problem is to quantize the vocal F0s con-\ntained in every time unit ( e.g., half beat) into a semitone-\nlevel F0 with a majority vote [7]. This approach, however,\noften fails to work when the vocal F0s are signiﬁcantly\ndeviated from exact semitone-level F0s speciﬁed by a mu-\nsical score or the melody is sung in a tight or lazy singing\nstyle such that the onsets of musical notes are signiﬁcantly\nadvanced or delayed from exact beat times.\nTo solve this problem, we propose a statistical method\nbased on a hidden Markov model (HMM) that represents\nhow a vocal F0 trajectory is generated from a sequence of\nlatent musical notes (Fig. 1). The F0s of musical notes in\na musical score can take only discrete values with the in-\nterval of semitones and tend to vary at a beat, half-beat,\nor quarter-beat level. The vocal F0 trajectory in an actual\nperformance, on the other hand, is a continuous signal that\ncan dynamically and smoothly vary over time. To deal with\nboth types of F0s from a generative viewpoint, we formu-\nlate a semi-beat-synchronous HMM (SBS-HMM) allow-\ning the continuous F0s of a sung melody to deviate from\nthe discrete F0s of written musical notes along the time and\nfrequency directions. In the proposed HMM, the semitone-\nlevel F0s and onset deviations of musical notes are encoded461as latent variables and the F0 deviations of musical notes\nare modeled by emission probability distributions. Given\nan F0 trajectory and beat times, all the variables and distri-\nbutions are estimated jointly using Gibbs sampling.\n2. RELATED WORK\nThis section introduces related work on singing voices.\n2.1 Pitch Estimation of Singing Voice\nMany studies on estimating a vocal F0 trajectory in a music\naudio signal have been conducted [1,4,5,8,10,15,22]. Sub-\nharmonic summation (SHS) [8] is a method in which the\nfundamental frequency for each time is determined by cal-\nculating the sum of the powers of the harmonic componen-\nts of each candidate fundamental frequency {f0,...,f M}.\nPreFEst [5] is a method that estimates the F0 trajectories of\na melody and a bass line by extracting the most predom-\ninant harmonic structure from a polyphonic music audio\nsignal. Ikemiya et al. [10] proposed a method in which sin-\nging voice separation and F0 estimation are performed mu-\ntually. First a singing voice is separated, from the spectro-\ngram obtained by the short-time Fourier transform (STFT)\nfor a music audio signal, by using a robust principal com-\nponent analysis (RPCA) [9], and then a vocal F0 trajectory\nis obtained with the Viterbi algorithm by using SHS for\na separated singing voice. Salamon et al. [22] used the\ncharacteristics of vocal F0 contours for melody extraction.\nDurrieu et al. [4] proposed a method for melody extrac-\ntion in which the main melody is represented as a source-\nﬁlter model and the accompaniment of the music mixture is\nrepresented as a non-negative matrix factorization (NMF)-\nbased model. De Cheveign ´e et al. [1] proposed a auto-\ncorrelation based method for fundamental frequency esti-\nmation which is expanded to decrease an error rate. This\nmethod is called YIN. Mauch et al. [15] extended YIN in a\nprobabilistic way to output multiple pitch candidates. This\nmethod is called pYIN.\n2.2 Note Estimation of Singing Voice\nA method for estimating the sequence of musical notes by\nquantizing pitches of a vocal F0 trajectory has been pro-\nposed. A majority-vote method described in Sec. 1 was\nimplemented in Songle [7]. The method has a limit be-\ncause it doesn’t consider the singing expression nor the\ntypical occurrence of pitches in succession. Paiva et al.\n[17] proposed a method that has ﬁve stages and detects\nmelody notes in polyphonic musical signals, and Raphael\n[19] proposed an HMM-based method that simultaneously\nestimates rhythms, tempos, and notes from a solo singing\nvoice acoustic signal. Poliner et al. [18] proposed a method\nbased on a support vector machines (SVM) classiﬁer which\ndoesn’t need the assumption that a musical pitch is realized\nas a set of harmonics of a particular fundamental. Laakso-\nnen [12] proposed a melody transcription method that uses\nchord information, and Ryyn ¨anen et al. [21] proposed a\nmethod for transcribing the melody, bass line, and chords\nin polyphonic music. A software tool called Tony devel-\nMusic Audio Signal\nVocal F0 Estimation Beat Tracking\nNote Estimation\nFigure 2 : Overview of the proposed musical note estima-\ntion method based on a semi-beat-synchronous HMM.\noped by Mauch et al. [14] estimates musical notes from\nthe output of pYIN by Viterbi-decoding of an HMM.\n2.3 Analysis of Vocal F0 Trajectories\nStudies on extracting the personality and habit of singing\nexpression from vocal F0 trajectories have been conducted.\nOhishi et al. [16] proposed a model that represents the gen-\nerating process of vocal F0 trajectories in consideration of\nthe time and frequency deviations. In that model the vocal\nF0 trajectory consists of three components: note, expres-\nsion, and ﬁne deviation components. The note component\ncontains the note transition and overshoot, and the expres-\nsion component contains vibrato and portamento. The note\nand expression components are represented as the outputs\nof second-order linear systems driven by the note and ex-\npression commands. The note and expression commands\nrepresent the sequence of musical notes and the musical\nexpressive intentions, respectively. The note command and\nthe expression command are represented with HMMs. Al-\nthough the method can extract the personality of the singing\nexpression from vocal F0 trajectories, it assumes that the\nmusic score is given in advance and cannot be directly ap-\nplied for note estimation.\n3. PROPOSED METHOD\nThis section explains the proposed method for estimating\na sequence of latent musical notes from the observed vocal\nF0 trajectories by formulating an SBS-HMM which repre-\nsents the generating process of the observations. An ob-\nserved F0 trajectory is stochastically generated by impart-\ning frequency and onset deviations to a step-function-like\nF0 trajectory that varies exactly on a 16th-note-level grid\naccording to a music score. The semitone-level F0s (called\npitches for simplicity in this paper) between adjacent grid\nlines and the onset deviations are represented as latent vari-\nables (states) of the HMM. Since the frequency deviations\nare represented by emission probability distributions of the\nHMM, a semi-beat-synchronous step-function-like F0 tra-\njectory is generated in the latent space and its ﬁnely-ﬂuctu-\nated version is then generated in the observed space.\n3.1 Problem Speciﬁcation\nThe problem of musical note estimation is formally deﬁned\n(Fig. 2) as follows:462 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Input : a vocal F0 trajectory X={xt}T\nt=1and 16th-note-\nlevel beat times ψ={ψn}N\nn=1automatically estimated\nfrom a music audio signal.\nOutput : a sequence of pitches Z={zn}N\nn=1.\nHere,tis the time frame index, Tis the number of time\nframes in the target signal, xtindicates a log-frequency in\ncents at frame t,Nis the number of beat times, ψnis the\nn-th beat time, and znindicates a pitch between ψn−1and\nψntaking one of {µ1,...,µ K}, whereKis the number\nof kinds of pitches that can appear in a music score. The\nbeginning and end of music are represented as ψ0= 1and\nψN=T+1respectively. In this paper we assume that\neachzncorresponds to a 16th note for simplicity. A longer\nmusical note is represented by a subsequence of {zn}N\nn=1\nhaving the same pitch in succession.\n3.2 Model Formulation\nWe explain how to formulate the SBS-HMM that simulta-\nneously represents the pitch transitions and onset and fre-\nquency deviations of musical notes.\n3.2.1 Modeling Pitch Transitions\nA sequence of latent pitches Zforms a ﬁrst-order Markov\nchain given by\nzn|zn−1,A∼Categorical( zn|azn−1), (1)\nwhereA= [aT\n1,· · ·,aT\nK]is aK-by-Ktransition proba-\nbility matrix such that∑K\nk=1ajk= 1for anyj. The initial\nlatent statez1is determined as follows:\nz1|π∼Categorical( z1|π), (2)\nwhereπ= [π1,· · ·,πK]Tis aK-dimensional vector such\nthat∑K\nk=1πk= 1.\n3.2.2 Modeling Onset Deviations\nThe onset deviations of musical notes τ={τn}N\nn=1are\nrepresented as discrete latent variables taking integer val-\nues between −GandG. Letϕn=ψn+τnbe the actual\nonset time of the n-th musical note. Note that τ0= 0and\nτN= 0 at the beginning and end of a vocal F0 trajectory.\nWe assume that τnis stochastically generated as follows:\nτn|ρ∼Categorical( τn|ρ), (3)\nwhereρ= [ρ−G,...,ρ G]Tis a(2G+1)-dimensional vec-\ntor such that∑G\ng=−Gρg= 1.\n3.2.3 Modeling Frequency Deviations\nThe observed F0 xt(ϕn−1≤t < ϕ n)is stochastically\ngenerated by imparting a probabilistic frequency deviation\nto the semitone-level pitch µzkassigned to each beat inter-\nval. Assuming that xtis independently generated at each\nframe, the emission probability of the n-th beat interval in\nthe case ofzn=k,τn−1=f,τn=gis given by\nbnkfg≡\n\nϕn−1∏\nt=ϕn−1p(xt|zn=k)\n\n1\nϕn−ϕn−1\n, (4)wherep(xt|zn=k)is the emission probability of each\nframe. To balance the effects of transition probabilities\nand emission probabilities, we exponentiate the product of\nemission probabilities of frames in a beat interval by the\nnumber of frames in a beat interval. We use as p(xt|zn)\nthe Cauchy distribution, which is robust against outliers\nand is deﬁned by\nCauchy(x;µ,λ) =λ\nπ{(x−µ)2+λ2}, (5)\nwhereµis a location parameter that deﬁnes the mode of\nthe distribution and λis a scale parameter. When the pitch\nof then-th beat interval is zn=k,µtakes the value µk.\nThe scale parameter takes a value that does not depend on\nthe pitchzn.\nSince actual vocal F0s are signiﬁcantly deviated from\nthose speciﬁed by a musical score, the scale parameter of a\nCauchy distribution is allowed to change according to the\ndifference of adjacent F0s; i.e.,∆xt≡xt−xt−1. The\nscale parameter is set to be proportional to the absolute\nvalue of ∆xtand deﬁned for each frame tas follows:\nλt=c· |∆xt|+d, (6)\nwherecis an coefﬁcient and dis a constant term. If d= 0,\np(xt|zn)cannot be calculated when λt= 0and∆xt= 0.\nTo avoid this problem, we introduce the constant term d.\n3.2.4 Incorporating Prior Distributions\nWe put conjugate Dirichlet priors on model parameters A,\nπ, andρas follows:\naj∼Dirichlet(aj|ξj), (7)\nπ∼Dirichlet(π|ζ), (8)\nρ∼Dirichlet(ρ|η), (9)\nwhereξj= [ξ1,...,ξ K]Tandζ= [ζ1,...,ζ K]TareK-\ndimensional vectors and η= [η−G,...,η G]Tis a(2G+\n1)-dimensional vector.\nWe then put on gamma priors on nonnegative Cauchy\nparameterscanddas follows:\nc∼Gamma(c|c0,c1), (10)\nd∼Gamma(d|d0,d1), (11)\nwherec0andd0are shape parameters and c1andd1are\nrate parameters.\n3.3 Bayesian Inference\nThe goal of Bayesian inference is to calculate the posterior\ndistributionp(Z,τ,A,π,ρ,c,d|X). Since this computa-\ntion is analytically intractable, we use Markov chain Monte\nCarlo (MCMC) methods for sampling the values of those\nvariables. Let Θ={A,π,ρ}be a set of model parame-\nters. To get samples of Θ, the Gibbs sampling algorithm is\nused. To get samples of sequential latent variables Zand\nτ, on the other hand, a kind of blocked Gibbs sampling al-\ngorithms called a forward ﬁltering-backward sampling al-\ngorithm is used. These steps are iterated in a similar wayProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 463to an expectation maximization (EM) algorithm called the\nBaum-Welch algorithm used for unsupervised learning of\nan HMM. Since the conjugacy is not satisﬁed for the distri-\nbutions regarding candd, we use the Metropolis-Hastings\n(MH) algorithm.\n3.3.1 Inferring Latent Variables Zandτ\nWe explain how to sample a sequence of latent variables Z\nandτ. For each beat interval, we calculate the probability\ngiven by\nβnjf=p(zn=j,τn=f|zn+1:N,τn+1:N,x1:T),(12)\nwherezn+1:N,τn+1:N, andx1:Trepresentzn+1,...,z N,\nτn+1,...,τ N, andx1,...,x T, respectively. The latent vari-\nables of the n-th beat interval (zn,τn)are sampled in ac-\ncordance with βnjf. The calculation of Eq. (12) and the\nsampling of the latent variables are performed by using for-\nward ﬁltering-backward sampling.\nIn forward ﬁltering, we recursively calculate the proba-\nbility given by\nαnkg=p(X10τ1,...,X (n−1)τn−2f,Xnfg,zn=k,τn=g),\nwhereXnfgrepresents the observations xtin the beat in-\nterval fromϕn−1toϕnwhenτn−1=fandτn=g.αnkg\nis calculated as follows:\nα1kg=p(X10g,z1=k,τ1=g)\n=p(X10g|z1=k,τ1=g)p(z1=k)p(τ1=g)\n=b1k0gπkρg, (13)\nαnkg=p(X10τ1,...,X nfg,zn=k,τn=g)\n=G∑\nf=−Gp(Xnfg|zn=k,τn−1=f,τn=g)\n·K∑\nj=1p(X10τ1,...,X (n−1)τn−2f,zn−1=j,τn−1=f)\n·p(zn=k|zn−1=j)p(τn=g)\n=G∑\nf=−GbnkfgK∑\nj=1α(n−1)jfajkρg. (14)\nIn backward sampling, Eq. (12) is calculated in the n-\nth beat interval by using the value of αnkg, and the states\n(zn,τn)are sampled recursively. When the (n+1)-th sam-\npled states are (zn+1,τn+1) = (k,g),βnjfis calculated as\nfollows:\nβnjf∝p(X(n+1)fg|zn+1=k,τn=f,τn+1=g)\n·p(zn+1=k|zn=j)p(τn+1=g)\n·p(X10τ1,...,X nτnf,zn=j,τn=f)\n=b(n+1)kfgajkρgαnjf. (15)\nSpeciﬁcally, the latent variables (zN,τN)are sampled in\naccordance with the probability given by\nβNjf∝αNjf. (16)3.3.2 Learning Model Parameters A,π, andρ\nWe explain how to learn the values of Θ. In a sequence of\nlatent variables {zn,τn}N\nn=1which are sampled in back-\nward sampling, the number of transitions such that zn=j\nandzn+1=kis represented as sjkand the number of on-\nset deviations such that τn=gis represented as ug. The\nvalue ofvkis1atz1=k, and else where is 0. The pa-\nrametersajk,ρgandπkare updated by sampling from the\nposterior distributions given by\np(aj|ξj+sj) = Dirichlet( aj|ξj+sj), (17)\np(ρ|η+u) = Dirichlet( ρ|η+u), (18)\np(π|ζ+v) = Dirichlet( π|ζ+v), (19)\nwheresj= [sj1,...,s jK]T,u= [u−G,...,u G]T, and\nv= [v1,...,v K]T.\n3.3.3 Learning Cauchy Parameters candd\nTo estimate the parameters candd, we use the MH algo-\nrithm. It is hard to analytically calculate the posterior dis-\ntributions of canddbecause a Cauchy distribution doesn’t\nhave conjugate prior distributions. When the values of c\nanddare respectively cianddi, we deﬁne proposal distri-\nbutions ofcanddas follows:\nqc(c|ci) = Gamma( c|γci,γ), (20)\nqd(d|di) = Gamma( d|δdi,δ), (21)\nwhereγandδare hyperparameters of the proposal distri-\nbutions. Using the value of c∗sampled from qc(c|ci), we\ncalculate the acceptance ratio given by\ngc(c∗,ci) = min{\n1,fc(c∗)qc(ci|c∗)\nfc(ci)qc(c∗|ci)}\n, (22)\nwherefc(c)is a likelihood function given by\nfc(c)≡p(c|x1:T,z1:N,τ1:N,Θ,di)\n∝N∏\nn=1ρnbnznτn−1τnN∏\nn=2azn−1znπz1q(c)\n=N∏\nn=1ρn\n\nϕn−1∏\nt=ϕn−1Cauchy(xt|µzn,λc\nt)\n\n1\nϕn−ϕn−1\n·N∏\nn=2azn−1znπz1Gamma(c|c0,c1), (23)\nλc\nt=ci·∆xt+di, (24)\nFinally, if the value of gc(c∗,ci)is larger than the random\nnumberrsampled from a uniform distribution on the inter-\nval[0,1], thenci+1=c∗, and otherwise ci+1=ci, where\nc0is sampled from the prior distribution q(c).\nThe value of dis updated in the same way as that of c.\nUsing the value of d∗sampled from qd(d|di), we calculate\nthe acceptance criteria given by\ngd(d∗,di) = min{\n1,fd(d∗)qd(di|d∗)\nfd(di)qd(d∗|di)}\n, (25)464 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016wherefd(d)is a likelihood function given by\nfd(d)≡p(d|x1:T,z1:N,τ1:N,Θ,ci+1)\n∝N∏\nn=1ρnbnznτn−1τnN∏\nn=2azn−1znπz1q(c)\n=N∏\nn=1ρn\n\nϕn−1∏\nt=ϕn−1Cauchy(xt|µzn,λd\nt)\n\n1\nϕn−ϕn−1\n·N∏\nn=2azn−1znπz1Gamma(d|d0,d1), (26)\nλd\nt=ci+1·∆xt+di, (27)\nFinally, if the value of gd(d∗,di)is larger than the random\nnumberrsampled from a uniform distribution on the inter-\nval[0,1], thendi+1=d∗, and otherwise di+1=di, where\nd0is sampled from the prior distribution q(d).\n3.4 Viterbi Decoding\nA latent sequence of musical notes is estimated by using\nthe Viterbi algorithm that uses the parameters at the time\nwhen the likelihood given by\np(x1:T) =K∑\nk=1G∑\ng=−Gαnkg (28)\nis the maximum in the learning process. The musical notes\nthat we want to estimate are the latent variables that max-\nimize the value given by p(Z,τ|X). In the Viterbi algo-\nrithm, we deﬁne ωnkgas follows:\nωnkg=\nmaxz1:n−1τ1:n−1lnp(X10τ1,...,X nτn−1g,z1:n−1,zn=k,τ1:n−1,τn=g),\n(29)\nandωnkgis calculated recursively with the equations\nω1kg= lnρg+ lnb1k0g+ lnπk, (30)\nωnkg= lnρg+ max\nf[\nlnbnkfg+ max\nj{\nlnajk+ω(n−1)jf}]\n.\n(31)\nIn the recursive calculation of ωnkg, when the states that\nmaximize the value of ωnkgarezn−1=j,τn−1=f,\nthose states are memorized as h(z)\nnk=j,h(τ)\nng=f. Af-\nter calculating {ωNkg}K,G\nk=1,g=−Gwith Eqs. (30) and (31),\nthe sequence of latent variables {zn,τn}N\nn=1is recursively\nestimated with the equations given by\n(zN,τN) = arg max\nk,g{ωnkg}, (32)\nzn=h(z)\n(n+1)zn+1, (33)\nτn=h(τ)\n(n+1)τn+1. (34)\nThe note sequence is retrieved by revising the onset devia-\ntions represented by the estimated latent variables {τn}N\nn=1.Model Concordance rate\nSBS-HMM 66.3±1.0\nMajority vote 56.9±1.1\nFrame-based HMM 56.1±1.1\nBS-HMM 67.0±1.0\nTable 1 : Average concordance rates and their standard er-\nrors.\nMajority vote Frame-based\nHMMBS-HMM SBS-HMM \n(proposed)30405060708090Concordance rate [%]\nFigure 3 : Concordance rates [%]. In the box plots, the red\nline indicates the median, the blue box indicates the range\nfrom the ﬁrst to third quantile, the black cross indicates the\nmean, and the outliers are plotted with red crosses.\n4. EVALUATION\nWe conducted an experiment to evaluate the proposed and\nprevious methods in the accuracy of estimating musical\nnotes from vocal F0 trajectories.\n4.1 Experimental Conditions\nThe 100 pieces of popular music in RWC database [6] were\nused for the experiments. For each song, we trained model\nparameters, estimated the sequence of musical notes, and\nmeasured the accuracy of estimated musical notes. The\ninput F0 trajectories were obtained from monaural music\nacoustic signals by the method of Ikemiya et al. [10]. We\nused the beat times obtained by a beat tracking system by\nDurand et al. [3]. This system estimates the beat times in\nunits of a whole note, and the interval between adjacent\nbeat times were divided into 16 equal intervals to obtain\nthe beat times for 16th-note units were calculated.\nFor the proposed method, the sequence of musical notes\nwas estimated with the Viterbi algorithm. The hyperpa-\nrameters were ξ= /x31,ζ=1,η=1,c0=d1=d0=\nd1= 1, where /x31and1respectively represent the matrix\nand vector whose elements are all ones. The parameters of\nthe proposal distributions were γ=δ= 1. The maximum\nvalueGthatτncould take was G= 5(i.e., 50cents).\nA majority-vote method was tested as a baseline. It\nestimates a musical note in each time unit corresponding\nto a 16th note by taking a majority vote for vocal F0s in\nthe time unit. For comparison, a frame-based HMM and\na beat-synchronous HMM (BS-HMM) were also tested.\nThe frame-based HMM assumes that all beat intervals have\nonly on frame. The BS-HMM is the same as SBS-HMM\nexcept that the onset deviation is not considered.\nThe estimated sequence of musical notes was compared\nwith the ground-truth MIDI data synchronized to the vocal\nmelody, and the concordance rate, i.e., the rate of frames\nin which pitches are correctly estimated, was used as the\nevaluation measure.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 465−4 −2 0 2 4\nOnset deviation [time frame]0.00.10.20.30.40.50.6ProbabilityFigure 4 : Example of a learned distribution of the model\nparameterρ.\n−15 −10 −5 0 5 10\n(Estimated pitch) - (Correct pitch) [semitone]0100200300400500600700The number of frames\nFigure 5 : A example of pitch estimation error. The case\nthat an estimated pitch is equal to a correct pitch is omitted.\n4.2 Experimental Results\nThe results of note estimation are listed in Table 1 and\nFigure 3. The proposed model clearly outperformed the\nmajority-vote method and the frame-based HMM in the\naverage concordance rate. On the other hand, the average\nconcordance rates for the proposed model and BS-HMM\nwere similar and the difference was not statistically signif-\nicant.\nThe results indicate that the model of music scores by\ntransition probabilities and that of frequency deviations by\noutput probabilities are both effective for improving the\naccuracy of musical note estimation. The cause of the re-\nsult that the model of onset deviations did not improve the\naccuracy is probably that the model parameter ρwas not\nproperly learned (Fig. 4). Capturing onset deviations by a\nsingle categorical distribution would be difﬁcult, since on-\nset depends on the duration of the pitches on either side of\nthe onset, and on the over all tempo of the song. It would\nbe necessary to model onset deviations in detail, for exam-\nple by using a separate hidden state to represent the F0s\nduring pitch transition.\n4.2.1 Pitch Estimation Error\nTwo types of errors were mainly observed (Fig. 5). The\nﬁrst type was caused by singing styles of singers, and ap-\npears as errors that span one semitone and two semitones.\nThis means that the frequency deviations affect the accu-\nracy of note estimation. The second type was caused by the\nerror of F0 estimation, and appears as the errors that span\nseven semitones and twelve semitones. The intervals of\nseven semitones and 12 semitones correspond to a perfect\nﬁfth and an octave, respectively.\n4.2.2 Singing Style Extraction and Robustness\nExample results of note estimation in Figure 6 show the\npotential of the proposed model to capture the singers’\nsinging style. In the upper ﬁgure, the onset at the ﬁrst beat\nis signiﬁcantly delayed from the beat time. Whereas the\nproposed model correctly detected the delayed onset, the\nmajority-vote method mis-identiﬁed the beat position of\n0 50 100 150 200 250 300 350 400 450\nTime [ms]300032003400360038004000Pitch [cent](a) Onset deviation\n050100 150 200 250 300 350 400 450 500 550 600 650 700\nTime [ms]49004950500050505100Pitch [cent]\n(b) Frequency deviation\nFigure 6 : Examples of note estimation results. The pink,\nblue, green, red, and black vertical lines respectively in-\ndicate a MIDI note which is the ground-truth, the F0 tra-\njectory of a singing voice including onset deviations, the\npitches estimated by the majority-vote method, the pitches\nestimated by the proposed method, and the beat times esti-\nmated in advance.\nthe pitch onset. The lower ﬁgure is an example of vibrato.\nWith the majority-vote method, the estimation result was\naffected by the large frequency deviation. With the pro-\nposed method, on the other hand, the robustness due to the\nCauchy distribution enabled the correct estimation of the\npitch without being affected by the vibrato.\n5. CONCLUSION\nThis paper presented a method for estimating the musical\nnotes of music from the vocal F0 trajectory. When mod-\neling the process generating the vocal F0 trajectory, we\nconsidered not only the musical score component but also\nonset deviation and frequency deviation. The SBS-HMM\nestimated pitches more accurately than the majority-vote\nmethod and the frame-based method.\nThe onset deviation and frequency deviation that were\nobtained using the proposed method are important for grasp-\ning the characteristics of singing expression. Future work\nincludes precise modeling of vocal F0 trajectories based\non second-order ﬁlters and extraction of individual singing\nexpression styles. In the proposed method, F0 estimation,\nbeat tracking, and musical note estimation are conducted\nseparately. It is necessary to integrate these methods. The\nproposed method cannot deal with the non-vocal regions\nin actual music, so we plan to also appropriately deal with\nnon-vocal regions.\nAcknowledgement: This study was partially supported by JST\nOngaCREST Project, JSPS KAKENHI 24220006, 26700020,\n26280089, 16H01744, and 15K16054, and Kayamori Founda-\ntion.466 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20166. REFERENCES\n[1]A. de Cheveign ´e and H. Kawahara. YIN, a fundamen-\ntal frequency estimator for speech and music. The Jour-\nnal of the Acoustical Society of America , 111(4):1917–\n1930, 2002.\n[2]A. Dobashi, Y . Ikemiya, K. Itoyama, and K. Yoshii. A\nmusic performance assistance system based on vocal,\nharmonic, and percussive source separation and con-\ntent visualization for music audio signals. SMC , 2015.\n[3]S. Durand, J. P. Bello, B. David, and G. Richard.\nDownbeat tracking with multiple features and deep\nneural networks. In 2015 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) , pages 409–413. IEEE, 2015.\n[4]J.-L. Durrieu, G. Richard, B. David, and C. F ´evotte.\nSource/ﬁlter model for unsupervised main melody ex-\ntraction from polyphonic audio signals. IEEE Trans-\nactions on Audio, Speech, and Language Processing, ,\n18(3):564–575, 2010.\n[5]M. Goto. PreFEst: A predominant-F0 estimation\nmethod for polyphonic musical audio signals. Proceed-\nings of the 2nd Music Information Retrieval Evaluation\neXchange , 2005.\n[6]M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: Popular, classical and jazz mu-\nsic databases. In The International Society for Music\nInformation Retrieval (ISMIR) , volume 2, pages 287–\n288, 2002.\n[7]M. Goto, K. Yoshii, H. Fujihara, M. Mauch, and\nT Nakano. Songle: A web service for active music lis-\ntening improved by user contributions. In The Inter-\nnational Society for Music Information Retrieval (IS-\nMIR) , pages 311–316, 2011.\n[8]D. J. Hermes. Measurement of pitch by subharmonic\nsummation. The journal of the acoustical society of\nAmerica , 83(1):257–264, 1988.\n[9]P.-S. Huang, S. D. Chen, P. Smaragdis, and\nM. Hasegawa-Johnson. Singing-voice separation from\nmonaural recordings using robust principal component\nanalysis. In 2012 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) ,\npages 57–60. IEEE, 2012.\n[10] Y . Ikemiya, K. Yoshii, and K. Itoyama. Singing voice\nanalysis and editing based on mutually dependent F0\nestimation and source separation. In 2015 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , pages 574–578. IEEE, 2015.\n[11] Y . E. Kim and B. Whitman. Singer identiﬁcation in\npopular music recordings using voice coding features.\nInProceedings of the 3rd International Conference\non Music Information Retrieval , volume 13, page 17,\n2002.[12] A. Laaksonen. Automatic melody transcription based\non chord transcription. In The International Society for\nMusic Information Retrieval (ISMIR) , pages 119–124,\n2014.\n[13] Y . Li and D. Wang. Separation of singing voice from\nmusic accompaniment for monaural recordings. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 15(4):1475–1487, 2007.\n[14] M. Mauch, C. Cannam, R. Bittner, G. Fazekas, J Sala-\nmon, J. Dai, J. Bello, and S Dixon. Computer-aided\nmelody note transcription using the Tony software: Ac-\ncuracy and efﬁciency. In Proceedings of the First Inter-\nnational Conference on Technologies for Music Nota-\ntion and Representation - TENOR2015 , pages 23–30,\nParis, France, 2015.\n[15] M. Mauch and S. Dixon. pYIN: A fundamental fre-\nquency estimator using probabilistic threshold distri-\nbutions. In 2014 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) ,\npages 659–663. IEEE, 2014.\n[16] Y . Ohishi, H. Kameoka, D. Mochihashi, and\nK. Kashino. A stochastic model of singing voice F0\ncontours for characterizing expressive dynamic com-\nponents. In INTERSPEECH , pages 474–477, 2012.\n[17] R. P. Paiva, T. Mendes, and A. Cardoso. On the de-\ntection of melody notes in polyphonic audio. In The\nInternational Society for Music Information Retrieval\n(ISMIR) , pages 175–182, 2005.\n[18] G. E. Poliner and D. P. W. Ellis. A classiﬁcation ap-\nproach to melody transcription. The International So-\nciety for Music Information Retrieval (ISMIR) , pages\n161–166, 2005.\n[19] C. Raphael. A graphical model for recognizing sung\nmelodies. In The International Society for Music Infor-\nmation Retrieval (ISMIR) , pages 658–663, 2005.\n[20] M. Ryyn ¨anen, T. Virtanen, J. Paulus, and A. Kla-\npuri. Accompaniment separation and karaoke appli-\ncation based on automatic melody transcription. In\n2008 IEEE International Conference on Multimedia\nand Expo , pages 1417–1420. IEEE, 2008.\n[21] M. P. Ryyn ¨anen and A. P. Klapuri. Automatic tran-\nscription of melody, bass line, and chords in poly-\nphonic music. Computer Music Journal , 32(3):72–86,\n2008.\n[22] J. Salamon and E. G ´omez. Melody extraction from\npolyphonic music signals using pitch contour charac-\nteristics. IEEE Transactions on Audio, Speech, and\nLanguage Processing , 20(6):1759–1770, 2012.\n[23] W.-H. Tsai and H.-M. Wang. Automatic singer recog-\nnition of popular music recordings via estimation and\nmodeling of solo vocal signals. IEEE Transactions on\nAudio, Speech, and Language Processing , 14(1):330–\n341, 2006.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 467"
    },
    {
        "title": "An Evaluation Framework and Case Study for Rhythmic Concatenative Synthesis.",
        "author": [
            "Cárthach Ó Nuanáin",
            "Perfecto Herrera",
            "Sergi Jordà"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1415648",
        "url": "https://doi.org/10.5281/zenodo.1415648",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/180_Paper.pdf",
        "abstract": "In this paper we present and report on a methodology for evaluating a creative MIR-based application of concatena- tive synthesis. After reviewing many existing applications of concatenative synthesis we have developed an applica- tion that specifically addresses loop-based rhythmic pat- tern generation. We describe how such a system could be evaluated with respect to its its objective retrieval perfor- mance and subjective responses of humans in a listener survey. Applying this evaluation strategy produced posi- tive findings to help verify and validate the objectives of our system. We discuss the results of the evaluation and draw conclusions by contrasting the objective analysis with the subjective impressions of the users.",
        "zenodo_id": 1415648,
        "dblp_key": "conf/ismir/NuanainHJ16",
        "content": "AN EV ALUATION FRAMEWORK AND CASE STUDY FOR\nRHYTHMIC CONCATENATIVE SYNTHESIS\nC´arthach ´O Nuan ´ain, Perfecto Herrera, Sergi Jord `a\nMusic Technology Group\nUniversitat Pompeu Fabra\nBarcelona\n{carthach.onuanain, perfecto.herrera, sergi.jorda }@upf.edu\nABSTRACT\nIn this paper we present and report on a methodology for\nevaluating a creative MIR-based application of concatena-\ntive synthesis. After reviewing many existing applications\nof concatenative synthesis we have developed an applica-\ntion that speciﬁcally addresses loop-based rhythmic pat-\ntern generation. We describe how such a system could be\nevaluated with respect to its its objective retrieval perfor-\nmance and subjective responses of humans in a listener\nsurvey. Applying this evaluation strategy produced posi-\ntive ﬁndings to help verify and validate the objectives of\nour system. We discuss the results of the evaluation and\ndraw conclusions by contrasting the objective analysis with\nthe subjective impressions of the users.\n1. INTRODUCTION\nMIR-based applications are becoming increasingly\nwidespread in creative scenarios such as composition and\nperformance [14] [7] [8]. This is commensurate with\nthe prevalence of sampling-based approaches to sound\ngeneration, thus the desire is to develop more rich and\ndescriptive understanding of the underlying content being\nused.\nOne of the primary difﬁculties faced with designing in-\nstruments for creative and compositional tasks remains the\nelaboration of an appropriate evaluation methodology. In-\ndeed, this is a trending challenge facing many researchers\n[2], and numerous papers address this directly with various\nproposals for methodological frameworks, some drawing\nfrom the closely related ﬁeld of HCI (Human Computer\nInteraction) [13], [16], [11]. More generally the evaluation\nof computer composition systems has also been the subject\nof much discussion in the literature. One frequent bench-\nmark for evaluating algorithmic music systems is a type\nof Turing test where the success criterion is determined\nby the inability of human listener to discern between hu-\nman and computer-generated music. As Hiraga [11] notes,\nc/circlecopyrtC´arthach ´O Nuan ´ain, Perfecto Herrera, Sergi Jord `a. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: C´arthach ´O Nuan ´ain, Perfecto Herrera, Sergi\nJord`a. “An Evaluation Framework and Case Study for\nRhythmic Concatenative Synthesis”, 17th International Society for Music\nInformation Retrieval Conference, 2016.however, these kind of tests can be problematic for two\nreasons. Firstly, it makes the assumption that the music\ngenerated by the algorithm is intended to sound like music\nproduced by humans, rather than something to be treated\ndifferently. Secondly it ignores other facets of the system\nthat imperatively needs evaluation, such as the interface\nand the experience . Pachet also ﬁnds issue with simplistic\nTuring test approaches to music evaluation [18]. He re-\npeats, for instance, the view that unlike the traditional Tur-\ning test which evaluated the ability to synthesis believable\nnatural language, no such “common-sense” knowledge ex-\nists for aspects of music.\nWe have designed and developed an MIR-driven instru-\nment that uses concatenative synthesis to generate looped\nrhythmic material from existing content. In terms of eval-\nuation we face the challenge of evaluating an MIR driven\nsoftware system, thus subject to the same scrutiny facing\nany information retrieval system that needs to be appraised.\nWe also face the challenge of evaluating the system as a\nmusical composition system that needs to serve the com-\nposer and listener alike.\nIn the next section we will give the reader brief fa-\nmiliarity with the instrument in terms of its implementa-\ntion and functionality. Subsequently, existing concatena-\ntive systems will be reported on in terms of their evalu-\nation methodologies (if any). Section 3 will propose the\nevaluation framework in questions and the results will be\nreported. We will conclude the paper with our impressions\non what we have learnt and scope for improvement in terms\nof the system itself and the evaluation methodology.\n2. INSTRUMENT DESCRIPTION\nConcatenative synthesis builds new sounds by combining\ntogether existing ones from a corpus. It is similar to gran-\nular synthesis differing only in the order of size of the\ngrains: granular synthesis operates on microsound scales\nof 20-200ms whereas concatenative synthesis uses musi-\ncally relevant unit sizes such as notes or even phrases. The\nprocess by which these sounds are selected for resynthesis\nis a fundamentally MIR-driven task. The corpus is deﬁned\nby selecting sound samples, optionally segmenting them\ninto onsets and extracting a chosen feature set to build de-\nscriptions of those sounds. New sounds can ﬁnally be syn-\nthesised by selecting sounds from the corpus according to67Figure 1 : Instrument Interface\na unit selection algorithm and connecting them in series,\nmaybe applying some cross-fading to smooth disparities\nin the process.\nConcatenative synthesis has a long history of applica-\ntion in speech synthesis [15]. One of the most well-known\nworks in the area of musical concatenative synthesis is\nCataRT [22] but there are many other systems referenced\nin the literature including some commercial implementa-\ntions. Bernardes [3] provides a thorough summary of these\nbased on similar reports in [25] and [21].\nOur system (Figure 1) resembles many concatenative\nsynthesis applications that offer a 2D timbre space for ex-\nploration. Where it distinguishes itself is in its sound gen-\neration strategy and mode of interaction for the user. Im-\nplemented as a VST plugin, it is an inherently loop-based\ninstrument. It records and analyses incoming audio from\nthe host as target segments according to a metrical level\nand concatenates units of sound from the corpus to gen-\nerate new loops with varying degrees of similarity to the\ntarget loop. This similarity is determined by the unit se-\nlection algorithm, the central component in concatenative\nsystems.\n2.1 Unit Selection\nThe unit selection algorithm is quite straightforward. For\neach unitiin the segmented target sequence (e.g. 16-step)\nand each corpus unit j(typically many more), the con-\ncatenation unit costCi,jis calculated by the weighted Eu-\nclidean distance of each feature kas given by Equation 1,\nwhereaandbare the values of the features in question.\nCi,j=/radicaltp/radicalvertex/radicalvertex/radicalbtn/summationdisplay\nk=1wk(ak−bk)2(1)\nIn terms of feature selection, after consulting a number\nof different articles [10], [20] and [27], dealing with fea-\nture extraction and rhythm we decided on a combination of\nMFCCs, loudness, spectral centroid and spectral ﬂatness.\nThese unit costs are stored in similarity matrix M. Next\nwe create a matrix M/primeof the indices of the ascendinglyAuthor (Year) Evaluation\nSchwarz (2000) No\nZils & Pachet (2001) No\nHazel (2001) No\nHoskinson & Pai (2001) No\nXiang (2002) No\nKobayashi (2003) No\nCardle et al. (2003) Videos of use cases\nLazier & Cook (2003) No\nSturm (2004) No\nCasey (2005) Retrieval Accuracy\nAucouturier & Pachet, (2005) User Experiment\nSimon et al. (2005) Algorithm Performance\nJehan (2005) Algorithmic evaluation\nSchwarz (2005) No\nWeiss et al. (2009) No\nFrisson et al. (2010) No\nHackbarth (2010) No\nBernardes (2014) Author’s impressions\nTable 1 : Evaluation in Concatenative Systems\nsorted elements of M. Finally a concatenated sequence\ncan be generated by returning a vector of indices Ifrom\nthis sorted matrix and playing back the associated sound\nﬁle. To retrieve the closest sequence V0one would only\nneed to return the ﬁrst row (Equation 3).\nV0= (I0,i,I0,i+1...,I 0,N) (2)\nReturning sequence vectors solely based on the row re-\nstricts the possibilities to the number of rows in the matrix\nand is quite limited. We can extend the number of possi-\nbilities toij−Tunits if we deﬁne a similarity threshold T\nand return a random index between 0 and j−Tfor each\nstepiin the new sequence.\n3. EV ALUATION OF CONCATENATIVE\nSYNTHESIS\nAs we were researching existing works in the table pre-\nsented by Bernardes, [3] we were struck by the absence\nof discussion regarding evaluation in most of the accom-\npanying articles. This table we reproduce here (Table 1)\namended and abridged with our details on the evaluation\nprocedures (if any) that were carried out.\nSome of the articles provided descriptions of use cases\n[4] or at least provided links to audio examples [24]. No-\ntably, many of the articles [23], [9] consistently made ref-\nerences to the role of “user”, but only one of those actually\nconducted a user experiment [1]. By no means is this in-\ntended to criticise the excellent work presented by these\nauthors. Rather it is intended to highlight that although\nevaluation is not always an essential part of such exper-\niments - especially in ”one-off” designs for single users\nsuch as the author as composer - it is an underexplored as-\npect that could beneﬁt from some contribution.\nWe can identify two key characteristics of our research\nthat can inform what kind of evaluation can be carried out.\nFirstly it’s a retrieval system, and can be analysed to deter-\nmined its ability to retrieve relevant items correctly. Sec-68 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016ondly it is a system that involves users or more precisely,\nmusical users. How do we evaluate this crucial facet?\nColeman has identiﬁed and addressed the lack of sub-\njective evaluation factors in concatenative synthesis [5]. In\nhis doctoral thesis he devotes a chapter to a listening sur-\nvey conducted to determine the quality of a number of dif-\nferent algorithmic components of the system under con-\nsideration. He asks the listeners to consider how well the\nharmony and timbre of the concatenated sequences are re-\ntained. In a previous paper [17] we conducted a similar-\nstyle listening survey to determine the ability of a genetic\nalgorithm to create symbolic rhythmic patterns that also\nmimic a target input pattern. Evaluation strategies need\nto be tailored speciﬁcally for systems, but if the system\nis intended to retrieve items according to some similarity\nmetric, and the material is musical, then a listening survey\nshould be critical. Furthermore, and echoing Coleman’s\nwork, we would emphasise that whatever the application of\na concatenative system, the evaluation of the timbral qual-\nity is essential.\nIn light of these elements we also devised a quantitative\nlistening survey to examine musical output of the system\nnot only in terms of its facility in matching the target con-\ntent perceptually but also in producing musically pleasing\nand meaningful content.\n4. METHOD\n4.1 Evaluation and Experimental Design\nGiven the rhythmic characteristics of the system we for-\nmulated an experiment that evaluated its ability to generate\nnew loops based on acoustic drum sounds. We gathered a\ndataset of 10 breakbeats ranging from 75 BPM to 142BPM\nand truncated them to single bar loops. Breakbeats are\nshort drum solo sections from funk music records in the\n1970s and exploited frequently as sample sources for hip-\nhop and electronic music. This practice has been of interest\nto the scientiﬁc community, as evident in work by Ravelli\net al. [19], Hockman [12] and Collins [6].\nIn turn, each of these loops was then used as a seed loop\nfor the system with the sound palette derived from the re-\nmaining 9 breakbeats. Four variations were then generated\nwith 4 different distances to the target. These distances\ncorrespond to indices into the similarity matrix we alluded\nto in Section 2, which we normalise by dividing the index\nby the size of the table. The normalised distances then cho-\nsen were at 0.0 (the closest to the target), 1.0 (the furthest\nfrom the target) and two random distances in ranges 0.0 -\n0.5 and 0.5 - 1.0.\nRepeating this procedure 10 times for each target loop\nin the collection for each of the distance categories, we\nproduced a total of 40 generated ﬁles to be compared with\nthe target loop. Each step in the loop was labelled in terms\nof its drum content, for example the ﬁrst step might have\na kick and a hi-hat. Each segmented onset (a total of 126\naudio samples) in the palette was similarly labelled with\nits corresponding drum sounds producing a total of 169\nlabelled sounds. The labellings we used were K = Kick,S = Snare, HH = Hi-hat, C = Cymbal and ﬁnally X when\nthe content wasn’t clear. Figure 2 shows the distribution\nof onsets by type in the full corpus of segmented units.\nAnother useful statistic is highlighted in Figure 3, which\nplots the distribution of onsets for each step in the 16 step\nsequence for the predominant kick, snare and hi-hat for the\n10 target loops. Natural trends are evident in these graphs,\nnamely the concentration of the kick on the 1st beat, snares\non the 2nd and 4th beat and hi-hats on off beats.\nFigure 2 : Distribution of Sounds in Corpus\nFigure 3 : Distribution of Sounds in Target Loops\n4.2 Retrieval Evaluation\nThe aim of the experiment was ﬁrst to determine how well\nthe system was able to retrieve similarly labelled ”units”\nfor each 1/16th step in the seed loop. To evaluate the ability\nof the algorithm to retrieve correctly labelled sounds in the\ngenerated loops we deﬁned the accuracy A by equation 3,\nbased on a similar approach presented in [26]. We make a\nsimpliﬁcation that corresponding HH and X and C labels\nalso yield a match based on our observation that their noisy\nqualities are very similar, and some of the target loops used\ndid not have onsets sounding at each 1/16th step.\nA=numberof correctlyretrievedlabels\ntotalnumberof labelsintargetloop(3)\n4.2.1 Retrieval Results\nBy studying the Pearson’s correlation between the retrieval\nratings and the distance, we can conﬁrm the tendency of\nsmaller distances to produce more similar patterns by ob-\nserving the moderate negative correlation ( ρ= -0.516, pProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 69<0.001) between increased distance and the accuracy rat-\nings (Figure 4).\nAn interesting observation is that when we isolate the\nretrieval accuracy ratings to kick and snare we see this cor-\nrelation increase sharply to ( ρ= -0.826, p<0.001), as can\nbe seen in Figure 5.\nFigure 4 : Scatter Plot and Regression Line of Retrieval\nAccuracy with Distance for all Drum Sounds\nFigure 5 : Scatter Plot and Regression Line of Retrieval\nAccuracy with Distance for Kick and Snare\nDelving into the data further, we can identify 3 different\ncategorical groupings that demonstrate predictable trends\nin terms of the central tendencies and descending retrieval\naccuracy (Figure 6). We label these categories A, B and\nC with the breakdown of the number of patterns and their\ncorresponding distance ranges as follows:\n•A - 10 patterns - 0.0\n•B - 9 patterns - [0.2 - 0.5]\n•C - 21 patterns - [0.5 - 1.0]\nFigure 6 : Central Tendencies of Retrieval Ratings for the\nSimilarity/Distance Categories\n4.3 Listener Evaluation\nThe retrieval accuracy gives the objective ratings of the\nsystem’s capability for retrieving correctly labelled items.\nThis information may not be consistent with the human lis-\ntener’s perceptual impression of similarity, nor does it give\nany indication whether the retrieved items are musically\nacceptable or pleasing. To assess the human experience of\nthe sonic output and to compare with the objective ratings\nof the system, we conducted a listening survey which will\nbe described here.\nFigure 7 : Screenshot of Web Survey\nTo directly compare and contrast with the retrieval eval-\nuation the same 40 loops generated by the system and used\nin the retrieval analysis were transferred to the listening\nsurvey. The survey itself was web-based (Figure 7) and\ntook roughly 15-20 minutes to complete. Participants were\npresented with the seed pattern and the generated patterns\nand could listen as many times as they liked. Using a 5\npoint Likert scale the participants were then asked to rate\ntheir agreement with the following statements:70 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Figure 8 : Correlations Between Distance and Subjective\nRatings of Pattern Similarity, Timbre Similarity and Liking\n•Is the rhythmic pattern similar?\n•Is the timbre similar?\n•Did you like the loop?\nTwenty one participants in all took part in total, drawn\nfrom researchers at the authors’ institute as well as friends\nand colleagues with an interest in music. Twenty out of the\n21 participants declared they were able to play a musical\ninstrument Ten of the 21 participants speciﬁed they played\na percussion instrument and 9 reported they could read no-\ntation. In the instructions we provided brief explanations\nof the key terms and audio examples demonstrating con-\ntrasting rhythmic patterns and timbres.\n4.3.1 Listener Evaluation Results\nThe survey data was analysed using Spearman’s rank cor-\nrelation on the mode of the participants’ responses to each\nloop stimulus with the associated distance of that loop.\nWe identiﬁed a moderate to strong negative correlation for\neach of the pattern, timbre and ”likeness” aspects (p <0.01\nin all instances). This can be observed in the red values in\nthe correlation matrix presented in Figure 8.\nIt should be evident that the subjective listening data\nconforms quite well to the ﬁndings of the objective re-\ntrieval rating. Increased distance resulted in decreased re-\ntrieval accuracy which in turn corresponded to a decrease\nin listener ratings for qualities pertaining to pattern sim-\nilarity and impression of timbral similarity in the sounds\nthemselves. Furthermore, it was revealed that the aestheti-\ncal judgement of the generated loops, encapsulated by the\n”likeness” factor, also followed the trend set out by the ob-\njective algorithm. We were curious to establish whether\nany particular subject did not conform to this preference\nfor similar loops, but examining the individual correlation\ncoefﬁcients revealed all to be negative (all participants pre-\nferred more similar sounding patterns).5. CONCLUSIONS\nIn this paper we presented a proposal for a framework that\nevaluates concatenative synthesis systems. Using a system\nthat we developed which speciﬁcally generates rhythmic\nloops as a use case we demonstrated how such a framework\ncould be applied in practice. An application-speciﬁc exper-\niment was devised and the objective results and subjective\nresults showed favourably the performance of the similar-\nity algorithm involved. It is hoped that by providing a well-\ndocumented account of this process other researchers can\nbe encouraged to adapt comparable evaluation strategies in\ncreative applications of MIR such as concatenative synthe-\nsis.\n6. ACKNOWLEDGMENTS\nThis research has been partially supported by the EU-\nfunded GiantSteps project (FP7-ICT-2013-10 Grant agree-\nment nr 610591).1\n7. REFERENCES\n[1] Jean-Julien Aucouturier and Franc ¸ois Pachet. Ringo-\nmatic: A Real-Time Interactive Drummer Using\nConstraint-Satisfaction and Drum Sound Descriptors.\nProceedings of the International Conference on Music\nInformation Retrieval , pages 412–419, 2005.\n[2] Jeronimo Barbosa, Joseph Malloch, Marcelo M. Wan-\nderley, and St ´ephane Huot. What does ” Evaluation ”\nmean for the NIME community? NIME 2015 - 15th In-\nternational Conference on New Interfaces for Musical\nExpression , page 6, 2015.\n[3] Gilberto Bernardes. Composing Music by Selection:\nContent-based Algorithmic-Assisted Audio Composi-\ntion. PhD thesis, University of Porto, 2014.\n[4] Mark Cardle, Stephen Brooks, and Peter Robinson.\nAudio and User Directed Sound Synthesis. Proceed-\nings of the International Computer Music Conference\n(ICMC) , 2003.\n[5] Graham Coleman. Descriptor Control of Sound Trans-\nformations and Mosaicing Synthesis . PhD thesis, Uni-\nversitat Pompeu Fabra, 2015.\n[6] Nick Collins. Towards autonomous agents for live\ncomputer music: Realtime machine listening and inter-\nactive music systems . PhD thesis, University of Cam-\nbridge, 2006.\n[7] Matthew E. P. Davies, Philippe Hamel, Kazuyoshi\nYoshii, and Masataka Goto. AutoMashUpper: An Au-\ntomatic Multi-Song Mashup System. Proceedings of\nthe 14th International Society for Music Information\nRetrieval Conference, ISMIR 2013 , pages 575—-580,\n2013.\n1http://www.giantsteps-project.euProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 71[8] Dimitri Diakopoulos, Owen Vallis, Jordan Hochen-\nbaum, Jim Murphy, and Ajay Kapur. 21st century elec-\ntronica: Mir techniques for classiﬁcation and perfor-\nmance. In International Society for Music Information\nRetrieval Conference , pages 465–469, 2009.\n[9] Benjamin Hackbarth. Audioguide : A Framework for\nCreative Exploration of Concatenative Sound Synthe-\nsis.IRCAM Research Report , 2011.\n[10] Perfecto Herrera, Amaury Dehamel, and Fabien\nGouyon. Automatic labeling of unpitched percussion\nsounds. In Audio Engineering Society 114th Conven-\ntion, 2003.\n[11] Rumi Hiraga, Roberto Bresin, Keiji Hirata, and\nHaruhiro Katayose. Rencon 2004: Turing Test for\nMusical Expression. Proceedings of the International\nConference on New Interfaces for Musical Expression ,\npages 120–123, 2004.\n[12] Jason A. Hockman and Matthew E. P. Davies. Com-\nputational Strategies for Breakbeat Classiﬁcation and\nResequencing in Hardcore, Jungle and Drum & Bass.\nInProc. of the 18th Int. Conference on Digital Audio\nEffects (DAFx-15) , pages 1–6, 2015.\n[13] William Hsu and Marc Sosnick. Evaluating interactive\nmusic systems: An HCI approach. In Proceedings of\nNew Interfaces for Musical Expression , pages 25–28,\n2009.\n[14] Eric J Humphrey, Douglas Turnbull, and Tom Collins.\nA brief review of creative MIR. International Society\nfor Music Information Retrieval , 2013.\n[15] Andrew J. Hunt and Alan W. Black. Unit selection in\na concatenative speech synthesis system using a large\nspeech database. 1996 IEEE International Conference\non Acoustics, Speech, and Signal Processing Confer-\nence Proceedings , 1:373–376, 1996.\n[16] Chris Kiefer, Nick Collins, and Geraldine Fitzpatrick.\nHCI Methodology For Evaluating Musical Controllers:\nA Case Study. Proceedings of the 2008 International\nConference on New Interfaces for Musical Expression\n(NIME-08) , pages 87–90, 2008.\n[17] C ´arthach ´O Nuan ´ain, Perfecto Herrera, and Sergi\nJorda. Target-Based Rhythmic Pattern Generation and\nVariation with Genetic Algorithms. In Sound and Mu-\nsic Computing Conference 2015 , Maynooth, Ireland.\n[18] Franc ¸ois Pachet and Pierre Roy. (Manufac) Turing\nTests for Music. In Proceedings of the 29th Confer-\nence on Artiﬁcial Intelligence (AAAI 2015), Workshop\non ”Beyond the Turing Test , 2015.\n[19] Emmanuel Ravelli, Juan P. Bello, and Mark Sandler.\nAutomatic rhythm modiﬁcation of drum loops. IEEE\nSignal Processing Letters , 14(4):228–231, 2007.[20] Pierre Roy, Franc ¸ois Pachet, and Sergio Krakowski.\nAnalytical Features for the classiﬁcation of Percussive\nSounds: the case of the pandeiro. In 10th Int. Confer-\nence on Digital Audio Effects (DAFx-07) , pages 1–8,\n2007.\n[21] Diemo Schwarz. Current Research In Concatenative\nSound Synthesis. In Proceedings of the International\nComputer Music Conference , pages 9–12, 2005.\n[22] Diemo Schwarz, G Beller, B Verbrugghe, and S Brit-\nton. Real-Time Corpus-Based Concatenative Synthe-\nsis with CataRT. Proceedings of the 9th International\nConference on Digital Audio Effects , pages 18–21,\n2006.\n[23] Ian Simon, Sumit Basu, David Salesin, and Maneesh\nAgrawala. Audio analogies: creating new music from\nan existing performance by concatenative synthesi.\nProceedings of the International Computer Music Con-\nference , 2005:65–72, 2005.\n[24] Bob L. Sturm. Matconcat: An Application for Explor-\ning Concatenative Sound Synthesis Using Matlab. In\n7th International Conference On Digital Audio Effects\n(DAFx) , pages 323–326, 2004.\n[25] Bob L. Sturm. Adaptive Concatenative Sound Synthe-\nsis and Its Application to Micromontage Composition.\nComputer Music Journal , 30(4):46–66, 2006.\n[26] Lucas Thompson, Simon Dixon, and Matthias Mauch.\nDrum Transcription via Classiﬁcation of Bar-Level\nRhythmic Patterns. In International Society for Mu-\nsic Information Retrieval Conference , pages 187–192,\n2014.\n[27] Adam Tindale, Ajay Kapur, George Tzanetakis, and\nIchiro Fujinaga. Retrieval of percussion gestures us-\ning timbre classiﬁcation techniques. Proceedings of the\nInternational Conference on Music Information Re-\ntrieval , pages 541–545, 2004.72 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "A Hierarchical Bayesian Model of Chords, Pitches, and Spectrograms for Multipitch Analysis.",
        "author": [
            "Yuta Ojima",
            "Eita Nakamura",
            "Katsutoshi Itoyama",
            "Kazuyoshi Yoshii"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1414968",
        "url": "https://doi.org/10.5281/zenodo.1414968",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/003_Paper.pdf",
        "abstract": "This paper presents a statistical multipitch analyzer that can simultaneously estimate pitches and chords (typical pitch combinations) from music audio signals in an unsu- pervised manner. A popular approach to multipitch anal- ysis is to perform nonnegative matrix factorization (NMF) for estimating the temporal activations of semitone-level pitches and then execute thresholding for making a piano- roll representation. The major problems of this cascading approach are that an optimal threshold is hard to determine for each musical piece and that musically inappropriate pitch combinations are allowed to appear. To solve these problems, we propose a probabilistic generative model that fuses an acoustic model (NMF) for a music spectrogram with a language model (hidden Markov model; HMM) for pitch locations in a hierarchical Bayesian manner. More specifically, binary variables indicating the existences of pitches are introduced into the framework of NMF. The la- tent grammatical structures of those variables are regulated by an HMM that encodes chord progressions and pitch co- occurrences (chord components). Given a music spectro- gram, all the latent variables (pitches and chords) are esti- mated jointly by using Gibbs sampling. The experimental results showed the great potential of the proposed method for unified music transcription and grammar induction.",
        "zenodo_id": 1414968,
        "dblp_key": "conf/ismir/OjimaNIY16",
        "content": "A HIERARCHICAL BAYESIAN MODEL OF CHORDS, PITCHES, AND\nSPECTROGRAMS FOR MULTIPITCH ANALYSIS\nYuta Ojima1Eita Nakamura1Katsutoshi Itoyama1Kazuyoshi Yoshii1\n1Graduate School of Informatics, Kyoto University , Japan\n{ojima, enakamura }@sap.ist.i.kyoto-u.ac.jp, {itoyama, yoshii }@kuis.kyoto-u.ac.jp\nABSTRACT\nThis paper presents a statistical multipitch analyzer that\ncan simultaneously estimate pitches and chords (typical\npitch combinations) from music audio signals in an unsu-\npervised manner. A popular approach to multipitch anal-\nysis is to perform nonnegative matrix factorization (NMF)\nfor estimating the temporal activations of semitone-level\npitches and then execute thresholding for making a piano-\nroll representation. The major problems of this cascading\napproach are that an optimal threshold is hard to determine\nfor each musical piece and that musically inappropriate\npitch combinations are allowed to appear. To solve these\nproblems, we propose a probabilistic generative model that\nfuses an acoustic model (NMF) for a music spectrogram\nwith a language model (hidden Markov model; HMM) for\npitch locations in a hierarchical Bayesian manner. More\nspeciﬁcally, binary variables indicating the existences of\npitches are introduced into the framework of NMF. The la-\ntent grammatical structures of those variables are regulated\nby an HMM that encodes chord progressions and pitch co-\noccurrences (chord components). Given a music spectro-\ngram, all the latent variables (pitches and chords) are esti-\nmated jointly by using Gibbs sampling. The experimental\nresults showed the great potential of the proposed method\nfor uniﬁed music transcription and grammar induction.\n1. INTRODUCTION\nThe goal of automatic music transcription is to estimate the\npitches ,onsets , and durations of musical notes contained\nin polyphonic music audio signals. These estimated values\nmust be directly linked with the elements of music scores.\nMore speciﬁcally, in this paper, a pitch means a discrete\nfundamental frequency (F0) quantized in a semitone level,\nan onset means a discrete time point quantized on a regular\ngrid ( e.g., eighth-note-level grid), and a duration means a\ndiscrete note value (integer multiple of the grid interval).\nIn this study we tackle multipitch estimation (subtask of\nautomatic music transcription) that aims to make a binary\npiano-roll representation from a music audio signal, where\nc/circlecopyrtYuta Ojima, Eita Nakamura, Katsutoshi Itoyama,\nKazuyoshi Yoshii. Licensed under a Creative Commons Attribution 4.0\nInternational License (CC BY 4.0). Attribution: Yuta Ojima, Eita Naka-\nmura, Katsutoshi Itoyama, Kazuyoshi Yoshii. “A Hierarchical Bayesian\nModel of Chords, Pitches, and Spectrograms for Multipitch Analysis”,\n17th International Society for Music Information Retrieval Conference,\n2016.\nLanguage model\nAcoustic modelSpectrogramsPitchesChords\nBases Activations\nA♭E♭E♭ FFigure 1 . Overview of the proposed model consisting of\nlanguage and acoustic models that are linked through bi-\nnary variables Srepresenting the existences of pitches.\nonly the existences of pitches are estimated at each frame.\nA popular approach to this task is to use non-negative ma-\ntrix factorization (NMF) [1–7]. It approximates the mag-\nnitude spectrogram of an observed mixture signal as the\nproduct of a basis matrix (a set of basis spectra correspond-\ning to different pitches) and an activation matrix (a set of\ntemporal activations corresponding to those pitches). The\nexistence of each pitch is then determined by executing\nthresholding or Viterbi decoding based a hidden Markov\nmodel (HMM) for the estimated activations [7, 8].\nThis NMF-based cascading approach, however, has two\nmajor problems. First, it is hard to optimize a threshold for\neach musical piece. Second, the estimated results are al-\nlowed to be musically inappropriate because the relation-\nships between different pitches are not taken into account.\nIn fact, music has simultaneous and temporal structures;\ncertain kinds of pitches ( e.g., C, G, and E) tend to simulta-\nneously occur to form chords ( e.g., C major), which vary\nover time to form typical progressions. If such structural\ninformation is unavailable for multipitch analysis, we need\nto tackle the chicken-and-egg problem that chords are de-\ntermined by pitch combinations, and vice versa.\nTo solve these problems, we propose a statistical method\nthat can discover chords and pitches from music audio sig-\nnals in an unsupervised manner while taking into account\ntheir interdependence (Fig.1). More speciﬁcally, we for-\nmulate a hierarchical Bayesian model that represents the\ngenerative process of an observed music spectrogram by\nunifying an acoustic model (probabilistic model underly-\ning NMF) that represents how the spectrogram is generated\nfrom pitches and a language model (HMM) that represents\nhow the pitches are generated from chords. A key fea-\nture of the uniﬁed model is that binary variables indicating\nthe existences of pitches are introduced into the framework\nof NMF. This enables the HMM to represent both chord309transitions and pitch combinations using only discrete vari-\nables forming a piano-roll representation with chord labels.\nGiven a music spectrogram, all the latent variables (pitches\nand chords) are estimated jointly by using Gibbs sampling.\nThe major contribution of this study is to realize unsu-\npervised induction of musical grammars from music audio\nsignals by unifying acoustic and language models. This ap-\nproach is formally similar to, but essentially different from\nthat to automatic speech recognition (ASR) because both\nthe models are jointly learned in an unsupervised manner.\nIn addition, our uniﬁed model has a three-level hierarchy\n(chord–pitch–spectrogram) while ASR is usually based on\na two-level hierarchy (word–spectrogram). The additional\nlayer is introduced by using an HMM instead of a Markov\nmodel (n-gram model) as a language model.\n2. RELATED WORK\nThis section reviews related work on multipitch estimation\n(acoustic modeling) and on music theory implementation\nand musical grammar induction (language modeling).\n2.1 Acoustic Modeling\nThe major approach to music signal analysi is to use non-\nnegative matrix factorization (NMF) [1–6, 9]. Cemgil et\nal.[9] developed a Bayesian inference scheme for NMF,\nwhich enabled the introduction of various hierarchical prior\nstructures. Hoffman et al. [3] proposed a Bayesian non-\nparametric extension of NMF called gamma process NMF\nfor estimating the number of bases. Liang et al. [6] pro-\nposed beta process NMF, in which binary variables are in-\ntroduced to indicate the needs of individual bases at each\nframe. Another extension is source-ﬁlter NMF [4], which\nfurther decomposes the bases into sources (corresponding\nto pitches) and ﬁlters (corresponding to timbres).\n2.2 Language Modeling\nThe implementation and estimation of music theory behind\nmusical pieces are composed have been studied [10–12].\nFor example, some attempts have been made to compu-\ntationally formulate the Generative Theory of Tonal Mu-\nsic (GTTM) [13], which represents the multiple aspects of\nmusic in a single framework. Hamanaka et al. [10] re-\nformalized GTTM through a computational implementa-\ntion and developed a method for automatically estimating\na tree that represents the structure of music, called a time-\nspan tree. Nakamura et al. [11] also re-formalized GTTM\nusing a probabilistic context-free grammar model and pro-\nposed inference algorithms. These methods enabled au-\ntomatic analysis of music. On the other hand, induction\nof music theory in an unsupervised manner has also been\nstudied. Hu et al. [12] extended latent Dirichlet allocation\nand proposed a method for determining the key of a mu-\nsical piece from symbolic and audio music based on the\nfact that the likelihood of appearance of each note tends\nto be similar among musical pieces in the same key. This\nmethod enabled the distribution of notes in a certain key to\nbe obtained without using labeled training data.Assuming that the concept of chords is a kind of music\ngrammar, statistical methods of supervised chord recogni-\ntion [14–17] are deeply related with unsupervised musi-\ncal grammar induction. Rocher et al. [14] attempted chord\nrecognition from symbolic music by constructing a directed\ngraph of possible chords and then calculating the optimal\npath. Sheh et al. [15] used acoustic features called chroma\nvectors to estimate chords from music audio signals. They\nconstructed an HMM whose latent variables are chord la-\nbels and whose observations are chroma vectors. Maruo\net al. [16] proposed a method that uses NMF for extract-\ning reliable chroma features. Since these methods need\nlabeled training data, the concept of chords is required in\nadvance. Approaches to make use of a sequence of chords\nin estimating pitches has also been proposed [18,19]. This\nmethod estimates chord progressions and multiple pitches\nsimultaneously by using a dynamic Bayesian network and\nshows better performance even with a simple acoustic model.\nRecent works employ recurrent neural networks as a lan-\nguage model to describe the relations between pitch com-\nbinations [20, 21].\n3. PROPOSED METHOD\nThis section explains the proposed method of multipitch\nanalysis that simultaneously estimates pitches and chords\nat the frame level from music audio signals. Our approach\nis to formulate a probabilistic generative model for ob-\nserved music spectrograms and then solve the “inverse”\nproblem, i.e., given a music spectrogram, estimate unknown\nrandom variables involved in the model. The proposed\nmodel has a hierarchical structure consisting of acoustic\nand language models that are connected through a piano\nroll,i.e., a set of binary variables indicating the existences\nof pitches (Fig. 1). The acoustic model represents the gen-\nerative process of a music spectrogram from the piano roll,\nbasis spectra, and temporal activations of individual pitches.\nThe language model represents the generative process of\nchord progressions and pitch locations from chords.\n3.1 Problem Speciﬁcation\nThe goal of multipitch estimation is to make a piano roll\nfrom a music audio signal. Let X∈RF×T\n+ be the mag-\nnitude spectrogram of a target signal, where Fis the num-\nber of frequency bins and Tis the number of time frames.\nWe aim to convert Xinto a piano roll S∈{0,1}K×T,\nwhich represents the existences of Kkinds of pitches over\nTframes. In addition, we attempt to estimate a sequence\nof chordsZ={zt}T\nt=1.\n3.2 Acoustic Modeling\nThe acoustic model is formulated in a similar way to beta-\nprocess NMF having binary masks [6] (Fig. 2). The given\nspectrogramX∈RF×T\n+ is factorized into bases W∈\nRF×K\n+ , activationsH∈RK×T\n+ , and binary variables S∈\n{0,1}K×Tas follows:\nXft|W,H,S∼Poisson/parenleftBig/summationtextK\nk=1WfkHktSkt/parenrightBig\n,(1)310 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Hadamard product\nActivations\nBasesBinary variables\nSpectrogramsCorresponding to \na piano-roll representation\nFigure 2 . The overview of the acoustic model based on a\nvariant of NMF having binary variables (masks).\nwhere{Wfk}F\nf=1is thek-th basis spectrum, Hktis the\nvolume of basis kat framet, andSktis a binary variable\nindicating whether or not basis kis used at frame t.\nA set of basis spectra Wis divided into two parts: har-\nmonic spectra and noise spectra. In this study we prepare\nKhharmonic basis spectra corresponding to Khdifferent\npitches and one noise basis spectrum ( K=Kh+ 1). As-\nsuming that the harmonic structures of the same instrument\nhave the shift-invariant relationships, the harmonic part of\nWare given by\n{Wfk}F\nf=1= shift/parenleftbig\n{Wh\nf}F\nf=1, ζ(k−1)/parenrightbig\n, (2)\nfork= 1,...Kh, where{Wh\nf}F\nf=1is a harmonic template\nstructure common to harmonic basis spectra used for NMF,\nshift (x,a)is an operator that shifts x= [x1,...,xn]T\nto[0,..., 0,x1,...,xn−a]T, andζis the number of fre-\nquency bins corresponding to the semitone interval.\nWe put two kinds of priors on the harmonic template\nspectrum{Wh\nf}F\nf=1and a noise basis spectrum {Wn\nf}F\nf=1.\nTo make the harmonic spectrum sparse, we put a gamma\nprior on{Wh\nf}F\nf=1as follows:\nWh\nf∼G/parenleftbig\nah,bh/parenrightbig\n(3)\nwhereahandbhare hyperparameters. On the other hand,\nwe put an inverse-gamma chain prior [22] on {Wn\nf}F\nf=1to\ninduce the spectral smoothness as follows:\nGW\nf|Wn\nf−1∼IG/parenleftBig\nηW,ηW\nWf−1/parenrightBig\n,\nWn\nf|GW\nf∼IG/parenleftbigg\nηW,ηW\nGW\nf/parenrightbigg\n, (4)\nwhereηWis a hyperparameter that determines the strength\nof smoothness and GW\nfis an auxiliary variable that induces\npositive correlation between Wn\nf−1andWn\nf.\nA set of activations His represented in the same way\nasW. IfHkttakes almost zero, Skthas no impact on\nNMF. This allows Sktto take one (the corresponding pitch\nis judged to be activated) even though the activation Hkt\nis almost zero. We can avoid this problem by putting an\ninverse-gamma prior for Hktto induce non-zero values.\nTo induce the temporal smoothness in addition, we put the\nfollowing inverse-gamma chain prior on H:\nGH\nkt|Hk(t−1)∼IG/parenleftBig\nηH,ηH\nHk(t−1)/parenrightBig\n,\nHkt|GH\nkt∼IG/parenleftBig\nηH,ηH\nGH\nkt/parenrightBig\n, (5)\nwhereηHis a hyperparameter that determines the strength\nof smoothness and GH\nktis an auxiliary variable that induces\npositive correlation between Hk(t−1)andHkt.\nChord progression\nE♭\n84 pitchesBinary variables\nfollows emission probabilitiesfollows transition probabilitiesA♭ E♭ F B♭Figure 3 . The overview of the language model based on\nan HMM that stochastically emits binary variables.\n3.3 Language Modeling\nThe language model is an HMM that has a Markov chain of\nlatent variables Z={z1,...,zT}(zt∈{1,...,I})and\nemits binary variables S={s1,...,sT}(st∈{0,1}Kh),\nwhereIrepresents the number of states (chords) and Kh\nrepresents the number of possible pitches. Note that Sis\nactually a set of latent variables in the proposed uniﬁed\nmodel. The HMM is deﬁned as:\nz1|φ∼Categorial(φ), (6)\nzt|zt−1,ψzt−1∼Categorical(ψzt−1), (7)\nSkt|zt,πztk∼Bernoulli(πztk) (8)\nwhereψi∈RIis a set of transition probabilities of chord\ni,φ∈RIis a set of initial probabilities, and πztkindicates\nthe probability that the k-th pitch is emitted under a chord\nzt, We put conjugate priors on these parameters as:\nψi∼Dir(1I),φ∼Dir(1I), πztk∼Beta(e,f),\n(9)\nwhere 1Iis theI-dimensional all-one vector and eandf\nare hyperparameters.\nIn practice, we represent only the emission probabili-\nties of 12 pitch classes (C, C#, ..., B) in one octave. Those\nprobabilities are copied and pasted to recover the emission\nprobabilities of Khkinds of pitches. In addition, the emis-\nsion probabilities{πik}Kh\nk=1of chordiare forced to have\ncircular-shifting relationships with those of other chords of\nthe same type. In this paper, we consider only major and\nminor chords as chord types ( I= 2×12) for simplicity.\n3.4 Posterior Inference\nGiven the observed data X, our goal is to calculate the\nposterior distribution p(W,H,S,z,π,ψ|X). Since ana-\nlytic calculation is intractable, we use Markov chain Monte\nCarlo (MCMC) methods as in [23]. Since the acoustic\nand language models share only the binary variables, each\nmodel can be updated independently when the binary vari-\nables are given. These models and binary variables are it-\neratively sampled. Finally, the latent variables (chord pro-\ngressions) of the language model are estimated by using\nthe Viterbi algorithm and the binary variables (pitch loca-\ntions) are determined by using parameters having the max-\nimum likelihood.\n3.4.1 Sampling Binary Variables\nThe binary variables Sare sampled from a posterior distri-\nbution that is calculated by integrating the acoustic modelProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 311as a likelihood function and the language model as a prior\ndistribution according the Bayes’ rule. Note that as shown\nin Fig. 1, the binary variables Sare involved in both acous-\ntic and language models ( i.e., the probability of each pitch\nbeing used is determined by a chord, and whether or not\neach pitch is used affects the reconstructed spectrogram).\nThe conditional posterior distribution of Sktis given by\nSkt∼Bernoulli/parenleftBig\nP1\nP1+P0/parenrightBig\n, (10)\nwhereP1andP0are given by\nP1=p(Skt= 1|S¬k,t,xt,W,H,π,z,α) (11)\n∝πα\nzk/producttext\nf/parenleftBig\nˆX¬k\nft+WfkHkt/parenrightBigXftexp{−WfkHkt},\nP0=p(Skt= 0|S¬k,t,xt,W,H,π,α)\n∝(1−πzk)α/producttext\nf/parenleftBig\nˆX¬k\nft/parenrightBigXft, (12)\nwhere ˆX¬k\nft≡/summationtext\nl/negationslash=kWflHltSltdenotes the magnitude\nat frametreconstructed without using the k-th basis and\nαis a parameter that determines the weight of the lan-\nguage model relative to that of the acoustic model. Such a\nweighting factor is also needed in ASR. If αis not equal to\none, Gibbs sampling cannot be used because the normal-\nization factor cannot be analytically calculated. Instead,\nthe Metropolis-Hastings (MH) algorithm is used by regard-\ning Eq. (10) is used as a proposal distribution\n3.4.2 Updating the Acoustic Model\nThe parameters of the acoustic model Wh,Wn, andH\ncan be sampled using Gibbs sampling. These parameters\nare categorized into those having gamma priors ( Wh) and\nthose inverse-gamma chain priors ( WnandH).\nUsing the Bayes’ rule, the conditional posterior distri-\nbution ofWhis given by\nWh\nfk∼G/parenleftbig/summationtext\ntXftλftk+ah,/summationtext\ntHktSkt+bh/parenrightbig\n,(13)\nwhereλftkis a normalized auxiliary variable that is cal-\nculated with the latest sampled variables ˆW,ˆH, and ˆS,\nas:\nλftk=ˆWfkˆHktˆSkt/summationtext\nlˆWflˆHltˆSlt. (14)\nThe other parameters are sampled through auxiliary vari-\nables. SinceHandGHare interdependent in Eq. (5) and\ncannot be sampled jointly, GHandHare sampled alte-\nnately. The conditional posterior of GHis given by\nGH\nkt∼IG/parenleftBig\n2ηH,ηH/parenleftBig\n1\nHkt+1\nHk(t−1)/parenrightBig/parenrightBig\n. (15)\nSimilarly, the conditional posteriors of H,GW, andWn\nare given by\nHkt∼IG/parenleftbigg\n2ηH,ηH/parenleftbigg\n1\nGH\nk(t+1)+1\nGH\nkt/parenrightbigg/parenrightbigg\n, (16)\nGW\nf∼IG/parenleftBig\n2ηW,ηW/parenleftBig\n1\nWn\nf+1\nWn\nf−1/parenrightBig/parenrightBig\n, (17)\nWn\nf∼IG/parenleftbigg\n2ηW,ηW/parenleftbigg\n1\nGW\nf+1+1\nGW\nf/parenrightbigg/parenrightbigg\n, (18)\nif the observation Xis not taken into account. Using the\nBayes’ rule and Jensen’s inequality as in Eq. (13) and re-\ngarding Eq. (16) as a prior, the conditional posterior con-sidering the observation Xis written as follows:1\nHkt∼GIG/parenleftBig\n2Skt/summationtext\nfWfk,δH,/summationtext\nfXftλftk−γH/parenrightBig\n,\nwhereγH= 2ηHandδH=ηH(1\nGH\nk(t+1)+1\nGH\nkt). The\nconditional posterior of Wncan be derived in the same\nmanner as follows:\nWn\nfk∼GIG (2/summationtext\ntHktSkt,δW,/summationtext\ntXftλftk−γW),\nwhereγW= 2ηWandδW=ηW(1\nGW\nf+1+1\nGW\nf)\n3.4.3 Updating the Language Model\nThe latent variables Zare sampled from the following con-\nditional posterior distribution:\np(zt|S,π,φ,Ψ)∝p(s1,...,st,zt), (19)\nwhereπis the emission probabilities, φis the initial prob-\nabilities, and Ψ ={ψ1,...,ψI}is a set of the transi-\ntion probabilities from each state. The right-hand side of\nEq. (19) is further factorized using the conditional inde-\npendence over ZandSas follows:\np(s1,...,st,zt)\n=p(st|zt)/summationtext\nzt−1p(s1,...,st−1,zt−1)p(zt|zt−1),(20)\np(s1,z1) =p(z1)p(s1|z1) =φz1p(s1|πz1). (21)\nUsing Eqs. (20) and (21) recursively, p(s1,...,sT|zT)can\nbe efﬁciently calculated via forward ﬁltering and the last\nvariablezTis sampled according to zT∼p(s1,...,sT|zT).\nIf the latent variables zt+1,...,zTare given,ztis sampled\nfrom a posterior given by\np(zt|S,zt+1,...,zT)∝p(s1,...,st,zt)p(zt+1|zt).(22)\nSincep(s1,...,st,zt)can be calculated in Eq. (20), ztis\nrecursively sampled from zt∼p(s1,...,st,zt)p(zt+1|zt)\nvia backward sampling.\nThe posterior distribution of the emission probabilities\nπis given by using the Bayes’ rule as follows:\np(π|S,z,φ,Ψ)∝p(S|π,z,φ,Ψ)p(π). (23)\nThis is analytically calculable because p(π)is a conjugate\nprior ofp(S|π,z,φ,Ψ). LetCibe the number of occur-\nrences of chord i∈{1...I}inZandci≡/summationtext\nt∈{t|zt=i}st\nbe aK-dimensional vector that denotes the sum of stun-\nder the condition zt=i. The parameters πare sampled\naccording to a conditional posterior given by\nπ∼Beta (e+cik,f+Ci−cik). (24)\nThe posterior distributions of the transition probabili-\ntiesψand the initial probabilities πare given similarly as\nfollows:\np(φ|S,z,π,Ψ)∝p(z1|φ)p(φ) (25)\np(ψ|S,z,π,φ)∝/producttext\ntp(zt|zt−1,ψzt−1)p(ψzt−1).(26)\nSincep(φ)andp(ψi)are conjugate priors of p(z1|φ)and\np(zt|zt−1,ψzt−1), respectively, these posteriors can be eas-\nily calculated. Let eibe the unit vector whose i-th element\n1GIG( a, b, p )≡(a/b)p\n2\n2Kp(√\nab)xp−1exp(−ax+b\nx\n2)denotes a general-\nized inverse Gaussian distribution.312 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016is 1 andaibe theI-dimensional vector whose j-th element\ndenotes the number of transition from state ito statej. The\nparametersφandψiare sampled according to conditional\nposteriors given by\nφ∼Dir (1I+ez1),ψi∼Dir (1I+ai). (27)\n4. EV ALUATION\nWe report comparative experiments we conducted to eval-\nuate the performance of our proposal model in pitch esti-\nmation. First, we conﬁrmed in a preliminary experiment\nthat correct chord progressions and emission probabilities\nwere estimated from the piano-roll by the language model.\nThen, we estimated the piano-roll representation from acous-\ntic audio signals by using the hierarchical model and the\nacoustic model.\n4.1 Experimental Conditions\nWe used 30 pieces (labeled as “ENSTDkCl”) selected from\nthe MAPS database [24]. We converted them into monau-\nral signals and truncated each of them to 30 seconds from\nthe beginning. The magnitude spectrogram was made by\nusing the variable-Q transform [25]. The 926×10075\nspectrogram thus obtained was resampled to 926×3000\nby using MATLAB’s resample function. Moreover, we\nused harmonic and percussive source separation (HPSS)\n[26] as a preprocessing. Unlike the original study, HPSS\nwas performed in the log-frequency domain. Median ﬁl-\nter is applied over 50 time frames and 40 frequency bins\neach. Hyperparameters were empirically determined as\nI= 24,ah= 1,bh= 1,an= 2,bn= 1,c= 2,d= 1,e=\n5,f= 80,α= 1300,ηW= 800000 andηH= 15000 .\nThe emission probabilities are obtained for 12 notes, which\nare expanded to cover 84 pitches. In practice, we ﬁxed the\nprobability of internal transition ( i.e.p(zt+1=zt|zt)) to\na large value ( 1−8.0×10−8) and assumed that the prob-\nabilities of transition to a different state follow Dirichlet\ndistribution as shown in section 3.4.3 We implemented the\nproposed method by using C++ and a linear algebra library\ncalled Eigen3. The estimation was conducted with a stan-\ndard desktop computer with an Intel Core i7-4770 CPU\n(8-core, 3.4 GHz) and 8.0 GB of memory. The processing\ntime for the proposed method with one music piece (30\nseconds as mentioned above) was 15.5 minutes .\n4.2 Chord Estimation for Piano Rolls\nWe ﬁrst veriﬁed that the language model properly esti-\nmated the emission probabilities and a chord progression.\nAs an input, we combined correct binary piano-roll repre-\nsentations for 84 pitches (MIDI numbers 21–104) of the\npieces we used. Since each representation has 3000 time-\nframes and we used 30 pieces, the input was 84 ×90000\nmatrix. We evaluated the precision of chord estimation\nas the ratio of the number of frames whose chords were\nestimated correctly to the total number of frames. Since\nwe prepared two chord types for each root note, we treated\n“major” and “7th” in the ground-truth chords as “major” in\nthe estimated chords, and “minor” and “minor 7th” in the\nFigure 4 . Emission probabilities estimated in the prelimi-\nnary experiment. The left corresponds to major chords and\nthe right corresponds to minor chords.\nground-truth chords as “minor” in the estimated chords.\nIn evaluation, other chord types were not used in evalua-\ntion and chord labels were estimated to maximize the pre-\ncision since we estimated chords in an unsupervised man-\nner. Since original MAPS database doesn’t contain chord\ninformation, one of the authors labeled chord information\nfor each music piece by hand2.\nThe experimental results shown in Fig. 4 shows that ma-\njor chords and minor chords, which are typical chord types\nin tonal music, were obtained as emission probabilities.\nThis implies that we can obtain the concept of chord from\npiano-roll data without any prior knowledge. The pre-\ncision was 61.33%, which indicates our model estimates\nchords correctly to some extent even in an unsupervised\nmanner. On the other hand, other studies on chord estima-\ntion have reported higher score [15, 16]. This is because\nthat they used labeled training data and that they evaluated\ntheir method with popular music, which has clearer chord\nstructure than classical music we used.\n4.3 Multipitch Estimation for Music Audio Signals\nWe then evaluated our model in terms of the frame-level\nrecall/precision rates and F-measure:\nR=/summationtext\ntct/summationtext\ntrt,P=/summationtext\ntct/summationtext\ntet,F=2RP\nR+P, (28)\nwherert,et, andctare respectively the numbers of ground\ntruth, estimated and correct pitches at the t-th time-frame.\nTo cope with the arbitrariness in octaves of the obtained\nbases, estimated results for the whole piece were shifted\nby octaves and the most accurate one was used for the\nevaluation. We conducted a few comparative experiments\nunder the following conditions: 1) Chords were ﬁxed and\nunchanged during a piece (the acoustic model), 2) the lan-\nguage model was pre-trained using the correct chord labels\nand a correct piano-roll, and the learned emission proba-\nbilities were used in estimation (pre-trained with chord),\n3) the language model was pre-trained using only a cor-\nrect piano-roll, and the learned emission probabilities were\nused in estimation (pre-trained without chord). we evalu-\nated the performances under the second and the third con-\nditions by using cross-validation.\nAs shown in Table 1, the performance of the proposed\nmethod in the unsupervised setting (65.0%) was better than\nthat of the acoustic model (64.7%). As shown in Fig. 5, the\nF-measure improvement due to integrating the language\nmodel for each piece correlated positively with the preci-\n2The annotation data used for evaluation is available on\nhttp://sap.ist.i.kyoto-u.ac.jp/members/ojima/mapschord.zipProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 313Condition F R P\nThe integrated model 65.0 67.3 62.8\nThe acoustic model 64.7 64.7 64.7\nPre-trained w/ chord 65.5 65.3 65.6\nPre-trained w/o chord 65.0 65.5 64.6\nTable 1 . Experimental results of multipitch analysis for 30\npiano pieces labeled as ENSTDkCl.\n−4 −3 −2 −1 0 1 2 3 4 5\nImprovement [%]2030405060708090Chord precision [%]\nFigure 5 . Correlation between estimated chord precision\nand the improvement of F-measure.\nsion of chord estimation for each piece (correlation coefﬁ-\ncientr= 0.33). This indicates that reﬁning the language\nmodel also improves the pitch estimation.\nMoreover, as shown in Fig. 6, major and minor chords\nlike those in Fig. 4 were obtained as emission probabilities\ndirectly from music audio signals without any prior knowl-\nedge. This implies that frequently used chord types can\nbe inferred from music audio signals automatically, which\nwould be useful in music classiﬁcation or similarity anal-\nysis. The performance in the supervised setting (65.5%)\nwas better than the performance obtained in the unsuper-\nvised settings. Since there exist published piano scores\nwith chord labels, this setting is considered to be prac-\ntical. Although this difference was statistically insigniﬁ-\ncant (standard error was about 1.5%), F-measures were im-\nproved for 25 pieces out of 30. Moreover, the improvement\nexceeded 1% for 15 pieces. The example of pitch estima-\ntion shown in Fig. 7 indicates that insertion errors at low\npitches are reduced by integrating the language model. On\nthe other hand, insertion errors in total increased in the in-\ntegrated model. This is because the constraint on harmonic\npartials (shift-invariant) is too strong to appropriately esti-\nmate the spectrum of each pitch. As a result, the overtones\nthat should be expressed by a single pitch are expressed\nby multiple inappropriate pitches that do not exist in the\nground-truth.\nThere would be much room for improving the perfor-\nmance. The acoustic model has the strong constraint on\nharmonic partials as mentioned above. This constraint can\nbe relaxed by introducing source-ﬁlter NMF [4], which\nfurther decomposes the bases into sources corresponding\nto pitches and ﬁlters corresponding to timbres. Our model\ncorresponds the case the number of ﬁlters is one, and in-\ncrement of the number of ﬁlters would contribute to ex-\npress difference in timbres ( e.g., difference between the\ntimbre of high pitches and that of low pitches). The lan-\nguage model, on the other hand, can be reﬁned by intro-\nducing other music theory such as keys. Some methods\nthat treat the relationship between keys and chords [27],\nFigure 6 . Emission probabilities learned from estimated\npiano-roll. Chord structures like those in Fig. 4 were ob-\ntained.\nFigure 7 . Estimated piano-rolls for MUS-\nbkxmas5 ENSTDkCl. Integrating the language model\nredeuced Insertion errors at low pitches.\nor keys and notes [12], have been studied. Moreover, the\nlanguage model focus on reducing unmusical errors such\nas insertion errors in adjacent pitches, and is difﬁcult to\ncope with errors in octaves or overtones. Modeling tran-\nsitions between notes (horizontal relations) will contribute\nto solve this problem and to improve the accuracy.\n5. CONCLUSION\nWe presented a new statistical multipitch analyzer that can\nsimultaneously estimate pitches and chords from music au-\ndio signals. The proposed model consists of an acoustic\nmodel (a variant of Bayesian NMF) and a language model\n(Bayesian HMM), and each model can make use of each\nother’s information. The experimental results showed the\npotential of the proposed method for uniﬁed music tran-\nscription and grammar induction from music audio signals.\nOn the other hand, each model has much room for perfor-\nmance improvement: the acoustic model has a strong con-\nstraint, and the language model is insufﬁcient to express\nmusic theory. Therefore, we plan to introduce a source-\nﬁlter model as the acoustic model and to introduce the con-\ncept of key in the language model.\nOur approach has a deep connection to language acqui-\nsition. In the ﬁeld of natural language processing (NLP),\nunsupervised grammar induction from a sequence of words\nand unsupervised word segmentation for a sequence of char-\nacters have actively been studied [28,29]. Since our model\ncan directly infer musical grammars ( e.g., concept of chords)\nfrom either music scores (discrete symbols) or music audio\nsignals, the proposed technique is expected to be useful for\nan emerging topic of language acquisition from continuous\nspeech signals [30].\nAcknowledgement: This study was partially supported by JST\nOngaCREST Project, JSPS KAKENHI 24220006, 26700020,\n26280089, 16H01744, and 15K16054, and Kayamori Founda-\ntion.‘’314 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20166. REFERENCES\n[1] P. Smaragdis and J. C. Brown. Non-negative matrix\nfactorization for polyphonic music transcription. In\nIEEE WASPAA , pages 177–180, 2003.\n[2] K. Ohanlon, H. Nagano, N. Keriven, and M. Plumb-\nley. An iterative thresholding approach to L0 sparse\nhellinger NMF. In ICASSP , pages 4737–4741, 2016.\n[3] M. Hoffman, D. M. Blei, and P. R. Cook. Bayesian\nnonparametric matrix factorization for recorded music.\nInICML , pages 439–446, 2010.\n[4] T. Virtanen and A. Klapuri. Analysis of polyphonic au-\ndio using source-ﬁlter model and non-negative matrix\nfactorization. In Advances in models for acoustic pro-\ncessing, neural information processing systems work-\nshop . Citeseer, 2006.\n[5] J. L. Durrieu, G. Richard, B. David, and C. F ´evotte.\nSource/ﬁlter model for unsupervised main melody ex-\ntraction from polyphonic audio signals. IEEE TASLP ,\n18(3):564–575, 2010.\n[6] D. Liang and M. Hoffman. Beta process non-negative\nmatrix factorization with stochastic structured mean-\nﬁeld variational inference. arXiv , 1411.1804, 2014.\n[7] E. Vincent, N. Bertin, and R. Badeau. Adaptive har-\nmonic spectral decomposition for multiple pitch esti-\nmation. IEEE TASLP , 18(3):528–537, 2010.\n[8] G. E. Poliner and D. P. Ellis. A discriminative model\nfor polyphonic piano transcription. EURASIP Journal\non Applied Signal Processing , 2007.\n[9] A. T. Cemgil. Bayesian inference for nonnegative ma-\ntrix factorisation models. Computational Intelligence\nand Neuroscience , 2009.\n[10] M. Hamanaka, K. Hirata, and S. Tojo. Implementing a\ngenerative theory of tonal music. Journal of New Music\nResearch , 35(4):249–277, 2006.\n[11] E. Nakamura, M. Hamanaka, K. Hirata, and K. Yoshii.\nTree-structured probabilistic model of monophonic\nwritten music based on the generative theory of tonal\nmusic. In ICASSP , 2016.\n[12] D. Hu and L. K. Saul. A probabilistic topic model for\nunsupervised learning of musical key-proﬁles. In IS-\nMIR, pages 441–446, 2009.\n[13] R. Jackendoff and F. Lerdahl. A generative theory of\ntonal music . MIT Press, 1985.\n[14] M. Rocher, T.and Robine, P. Hanna, and R. Strandh.\nDynamic Chord Analysis for Symbolic Music . Ann Ar-\nbor, MI: MPublishing, University of Michigan Library,\n2009.\n[15] A. Sheh and D. P. Ellis. Chord segmentation and recog-\nnition using EM-trained hidden Markov models. In IS-\nMIR, pages 185–191, 2003.\n[16] S. Maruo, K. Yoshii, K. Itoyama, M. Mauch, and\nM. Goto. A feedback framework for improved chord\nrecognition based on NMF-based approximate note\ntranscription. In ICASSP , pages 196–200, 2015.[17] Y . Ueda, Y . Uchiyama, T. Nishimoto, N. Ono, and\nS. Sagayama. HMM-based approach for automatic\nchord detection using reﬁned acoustic features. In\nICASSP , pages 5518–5521, 2010.\n[18] S. Raczynski, E. Vincent, F. Bimbot, and S. Sagayama.\nMultiple pitch transcription using DBN-based musico-\nlogical models. In ISMIR , pages 363–368, 2010.\n[19] S. A. Raczynski, E. Vincent, and S. Sagayama. Dy-\nnamic bayesian networks for symbolic polyphonic\npitch modeling. IEEE TASLP , 21(9):1830–1840, 2013.\n[20] S. Sigtia, E. Benetos, and S. Dixon. An end-to-end neu-\nral network for polyphonic piano music transcription.\nIEEE TASLP , 24(5):927–939, 2016.\n[21] S. Sigtia, E. Benetos, S. Cherla, T. Weyde, A. Garcez,\nand S. Dixon. An RNN-based music language model\nfor improving automatic music transcription. In ISMIR ,\npages 53–58, 2014.\n[22] A. T. Cemgil and O. Dikmen. Conjugate Gamma\nMarkov random ﬁelds for modelling nonstationary\nsources. In Independent Component Analysis and Sig-\nnal Separation , pages 697–705. Springer, 2007.\n[23] M. Davy and S. J. Godsill. Bayesian harmonic mod-\nels for musical signal analysis. Bayesian Statistics ,\n(7):105–124, 2003.\n[24] V . Emiya, R. Badeau, and B. David. Multipitch estima-\ntion of piano sounds using a new probabilistic spectral\nsmoothness principle. IEEE TASLP , 18(6):1643–1654,\n2010.\n[25] C. Sch ¨orkhuber, A. Klapuri, N. Holighaus, and\nM. D ¨orﬂer. A Matlab toolbox for efﬁcient perfect\nreconstruction time-frequency transforms with log-\nfrequency resolution. In Audio Engineering Society\nConference , 2014.\n[26] D. Fitzgerald. Harmonic/percussive separation using\nmedian ﬁltering. In DAFx , pages 1–4, 2010.\n[27] K. Lee and M. Slaney. Acoustic chord transcription and\nkey extraction from audio using key-dependent HMMs\ntrained on synthesized audio. IEEE TASLP , 16(2):291–\n301, 2008.\n[28] M. Johnson. Using adaptor grammars to identify syner-\ngies in the unsupervised acquisition of linguistic struc-\nture. In Proceedings of the 46th Annual Meeting of the\nAssociation of Computational Linguistics , pages 398–\n406, 2008.\n[29] D. Mochihashi, T. Yamada, and N. Ueda. Bayesian un-\nsupervised word segmentation with nested Pitman-Yor\nlanguage modeling. In ACL, pages 100–108. Associa-\ntion for Computational Linguistics, 2009.\n[30] T. Taniguchi and S. Nagasaka. Double articulation an-\nalyzer for unsegmented human motion using Pitman-\nYor language model and inﬁnite hidden markov model.\nInIEEE/SICE International Symposium on System In-\ntegration , pages 250–255. IEEE, 2011.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 315"
    },
    {
        "title": "Exploring Customer Reviews for Music Genre Classification and Evolutionary Studies.",
        "author": [
            "Sergio Oramas",
            "Luis Espinosa Anke",
            "Aonghus Lawlor",
            "Xavier Serra",
            "Horacio Saggion"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1415544",
        "url": "https://doi.org/10.5281/zenodo.1415544",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/240_Paper.pdf",
        "abstract": "In this paper, we explore a large multimodal dataset of about 65k albums constructed from a combination of Ama- zon customer reviews, MusicBrainz metadata and Acous- ticBrainz audio descriptors. Review texts are further en- riched with named entity disambiguation along with po- larity information derived from an aspect-based sentiment analysis framework. This dataset constitutes the corner- stone of two main contributions: First, we perform ex- periments on music genre classification, exploring a va- riety of feature types, including semantic, sentimental and acoustic features. These experiments show that modeling semantic information contributes to outperforming strong bag-of-words baselines. Second, we provide a diachronic study of the criticism of music genres via a quantitative analysis of the polarity associated to musical aspects over time. Our analysis hints at a potential correlation between key cultural and geopolitical events and the language and evolving sentiments found in music reviews.",
        "zenodo_id": 1415544,
        "dblp_key": "conf/ismir/OramasALSS16",
        "content": "EXPLORING CUSTOMER REVIEWS FOR MUSIC GENRE\nCLASSIFICATION AND EVOLUTIONARY STUDIES\nSergio Oramas1, Luis Espinosa-Anke2, Aonghus Lawlor3, Xavier Serra1, Horacio Saggion2\n1Music Technology Group, Universitat Pompeu Fabra\n2TALN Group, Universitat Pompeu Fabra\n3Insight Centre for Data Analytics, University College of Dublin\n{sergio.oramas, luis.espinosa, xavier.serra, horacio.saggion }@upf.edu, aonghus.lawlor@insight-centre.org\nABSTRACT\nIn this paper, we explore a large multimodal dataset of\nabout 65k albums constructed from a combination of Ama-\nzon customer reviews, MusicBrainz metadata and Acous-\nticBrainz audio descriptors. Review texts are further en-\nriched with named entity disambiguation along with po-\nlarity information derived from an aspect-based sentiment\nanalysis framework. This dataset constitutes the corner-\nstone of two main contributions: First, we perform ex-\nperiments on music genre classiﬁcation, exploring a va-\nriety of feature types, including semantic, sentimental and\nacoustic features. These experiments show that modeling\nsemantic information contributes to outperforming strong\nbag-of-words baselines. Second, we provide a diachronic\nstudy of the criticism of music genres via a quantitative\nanalysis of the polarity associated to musical aspects over\ntime. Our analysis hints at a potential correlation between\nkey cultural and geopolitical events and the language and\nevolving sentiments found in music reviews.\n1. INTRODUCTION\nWith the democratisation of Internet access, vast amounts\nof information are generated and stored in online sources,\nand thus there is great interest in developing techniques\nfor processing this information effectively [27]. The Mu-\nsic Information Retrieval (MIR) community is sensible to\nthis reality, as music consumption has undergone signiﬁ-\ncant changes recently, especially since users are today just\none click away from millions of songs [4]. This context re-\nsults in the existence of large repositories of unstructured\nknowledge, which have great potential for musicological\nstudies or tasks within MIR such as music recommenda-\ntion.\nIn this paper, we put forward an integration proce-\ndure for enriching with music-related information a large\nc/circlecopyrtSergio Oramas1, Luis Espinosa-Anke2, Aonghus\nLawlor3, Xavier Serra1, Horacio Saggion2. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: Sergio Oramas1, Luis Espinosa-Anke2, Aonghus Lawlor3, Xavier\nSerra1, Horacio Saggion2. “Exploring Customer Reviews for Music\nGenre Classiﬁcation and Evolutionary Studies”, 17th International So-\nciety for Music Information Retrieval Conference, 2016.dataset of Amazon customer reviews [18,19], with seman-\ntic and acoustic metadata obtained from MusicBrainz1\nand AcousticBrainz2, respectively. MusicBrainz (MB) is\na large open music encyclopedia of music metadata, whist\nAcousticBrainz (AB) is a database of music and audio de-\nscriptors, computed from audio recordings via state-of-the-\nart Music Information Retrieval algorithms [26]. In addi-\ntion, we further extend the semantics of the textual content\nfrom two standpoints. First, we apply an aspect-based sen-\ntiment analysis framework [7] which provides speciﬁc sen-\ntiment scores for different aspects present in the text, e.g.\nalbum cover, guitar, voice or lyrics. Second, we perform\nEntity Linking (EL), so that mentions to named entities\nsuch as Artist Names or Record Labels are linked to their\ncorresponding Wikipedia entry [24].\nThis enriched dataset, henceforth referred to as Multi-\nmodal Album Reviews Dataset (MARD), includes affec-\ntive, semantic, acoustic and metadata features. We beneﬁt\nfrom this multidimensional information to carry out two\nexperiments. First, we explore the contribution of such\nfeatures to the Music Genre classiﬁcation task, consisting\nin, given a song or album review, predict the genre it be-\nlongs to. Second, we use the substantial amount of infor-\nmation at our disposal for performing a diachronic analysis\nof music criticism. Speciﬁcally, we combine the metadata\nretrieved for each review with their associated sentiment\ninformation, and generate visualizations to help us investi-\ngate any potential trends in diachronic music appreciation\nand criticism. Based on this evidence, and since music\nevokes emotions through mechanisms that are not unique\nto music [16], we may go as far as using musical infor-\nmation as means for a better understanding of global af-\nfairs. Previous studies argue that national conﬁdence may\nbe expressed in any form of art, including music [20], and\nin fact, there is strong evidence suggesting that our emo-\ntional reactions to music have important and far-reaching\nimplications for our beliefs, goals and actions, as members\nof social and cultural groups [1]. Our analysis hints at a\npotential correlation between the language used in music\nreviews and major geopolitical events or economic ﬂuctu-\nations. Finally, we argue that applying sentiment analysis\nto music corpora may be useful for diachronic musicolog-\nical studies.\n1http://musicbrainz.org/\n2http://acousticbrainz.org1502. RELATED WORK\nOne of the earliest attempts on review genre classiﬁcation\nis described in [15], where experiments on multiclass genre\nclassiﬁcation and star rating prediction are described. Sim-\nilarly, [14] extend these experiments with a novel approach\nfor predicting usages of music via agglomerative cluster-\ning, and conclude that bigram features are more infor-\nmative than unigram features. Moroever, part-of-speech\n(POS) tags along pattern mining techniques are applied\nin [8] to extract descriptive patterns for distinguishing neg-\native from positive reviews. Additional textual evidence is\nleveraged in [5], who consider lyrics as well as texts re-\nferring to the meaning of the song, and used for training a\nkNN classiﬁer for predicting song subjects (e.g. war, sex\nor drugs).\nIn [23], a dataset of music reviews is used for album\nrating prediction by exploiting features derived from sen-\ntiment analysis. First, music-related topics are extracted\n(e.g. artist or music work), and this topic information is\nfurther used as features for classiﬁcation. One of the most\nthorough works on music reviews is described in [28].\nIt applies Natural Language Processing (NLP) techniques\nsuch as named entity recognition, text segmentation and\nsentiment analysis to music reviews for generating texts\nexplaining good aspects of songs in recommender systems.\nIn the line of review generation, [9] combine text analysis\nwith acoustic descriptors in order to generate new reviews\nfrom the audio signal. Finally, semantic music information\nis used in [29] to improve topic-wise classiﬁcation (album,\nartist, melody, lyrics, etc.) of music reviews using Sup-\nport Vector Machines. This last approach differs from ours\nin that it enriches feature vectors by taking advantage of\nad-hoc music dictionaries, while in our case we take ad-\nvantage of Semantic Web resources.\nAs for sentiment classiﬁcation of text, there is abundant\nliterature on the matter [21], including opinions, reviews\nand blog posts classiﬁcation as positive or negative. How-\never, the impact of emotions has received considerably less\nattention in genre-wise text classiﬁcation. We aim at bridg-\ning this gap by exploring aspect-level sentiment analysis\nfeatures.\nFinally, concerning studies on the evolution of music\ngenres, these have traditionally focused on variation in au-\ndio descriptors, e.g. [17], where acoustic descriptors of\n17,000 recordings between 1960 and 2010 are analyzed.\nDescriptors are discretized and redeﬁned as descriptive\nwords derived from several lexicons, which are subse-\nquently used for topic modeling. In addition, [12] analyze\nexpressions located near the keyword jazzin newswire col-\nlections from the 20th century in order to study the advent\nand reception of jazz in American popular culture. This\nwork has resemblances to ours in that we also explore how\ntextual evidence can be leveraged, with a particular focus\non sentiment analysis, for performing descriptive analyses\nof music criticism.3. MULTIMODAL ALBUM REVIEWS DATASET\nMARD contains texts and accompanying metadata origi-\nnally obtained from a much larger dataset of Amazon cus-\ntomer reviews [18, 19]. The original dataset provides mil-\nlions of review texts together with additional information\nsuch as overall rating (between 0 to 5), date of publica-\ntion, or creator id. Each review is associated to a product\nand, for each product, additional metadata is also provided,\nnamely Amazon product id, list of similar products, price,\nsell rank and genre categories. From this initial dataset,\nwe selected the subset of products categorized as CDs &\nVinyls , which also fulﬁll the following criteria. First, con-\nsidering that the Amazon taxonomy of music genres con-\ntains 27 labels in the ﬁrst hierarchy level, and about 500\nin total, we obtain a music-relevant subset and select 16 of\nthe 27 which really deﬁne a music style and discard for in-\nstance region categories (e.g. World Music) and other cate-\ngories non speciﬁcally related to a music style (e.g. Sound-\ntrack, Miscellaneous, Special Interest), function-oriented\ncategories (Karaoke, Holiday & Wedding) or categories\nwhose albums might also be found under other categories\n(e.g. Opera & Classical V ocal, Broadway & V ocalists).\nWe compiled albums belonging only to one of the 16 se-\nlected categories, i.e. no multiclass. Note that the original\ndataset contains not only reviews about CDs and Vinyls,\nbut also about music DVDs and VHSs. Since these are not\nstrictly speaking music audio products, we ﬁlter out those\nproducts also classiﬁed as ”Movies & TV”. Finally, since\nproducts classiﬁed as Classical and Pop are substantially\nmore frequent in the original dataset, we compensate this\nunbalance by limiting the number of albums of any genre\nto 10,000. After this preprocessing, MARD amounts to a\ntotal of 65,566 albums and 263,525 customer reviews. A\nbreakdown of the number of albums per genre is provided\nin Table 1.\nGenre Amazon MusicBrainz AcousticBrainz\nAlternative Rock 2,674 1,696 564\nReggae 509 260 79\nClassical 10,000 2,197 587\nR&B 2,114 2,950 982\nCountry 2,771 1,032 424\nJazz 6,890 2,990 863\nMetal 1,785 1,294 500\nPop 10,000 4,422 1701\nNew Age 2,656 638 155\nDance & Electronic 5,106 899 367\nRap & Hip-Hop 1,679 768 207\nLatin Music 7,924 3,237 425\nRock 7,315 4,100 1482\nGospel 900 274 33\nBlues 1,158 448 135\nFolk 2,085 848 179\nTotal 66,566 28,053 8,683\nTable 1 : Number of albums by genre with information\nfrom the different sources in MARD\nHaving performed genre ﬁltering, we enrich MARD by\nextracting artist names and record labels from the Ama-\nzon product page. We pivot over this information to query\nthe MB search API to gather additional metadata such as\nrelease id, ﬁrst release date, song titles and song ids. Map-\nping with MB is performed using the same methodology\ndescribed in [25], following a pair-wise entity resolutionProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 151Music Reviews\nAspect ExtractionSentiment Analysisbi-gramsnounsthresholding / filteringShallow NLP (POS tagging)sentiment termsOpinion Pattern MiningSentiment MatchingSentiment Assignment\u0002ь,\u0002э,...opinion patternsJJ_FEA TURE\u0013$,\u0002%,\u0014$%,(+,−,=),...M1MnMi→{R1,...,Rn}Ri\nFigure 1 : Overview of the opinion mining and sentiment\nanalysis framework.\napproach based on string similarity with a threshold value\nofθ= 0.85. We successfully mapped 28,053 albums to\nMB. Then, we retrieved songs’ audio descriptors from AB.\nFrom the 28,053 albums mapped to MB, a total of 8,683\nalbums are further linked to their corresponding AB en-\ntry, which encompasses 65,786 songs. The ﬁnal dataset is\nfreely available for download3.\n4. TEXT PROCESSING\nIn this section we describe how we extract linguistic, sen-\ntimental and semantic information from textual reviews.\nThis information will serve both as input features for our\ngenre classiﬁcation experiments, and also constitute the\nbasis for the diachronic study described in Section 6.\n4.1 Sentiment Analysis\nFollowing the work of [6,7] we use a combination of shal-\nlow NLP, opinion mining, and sentiment analysis to extract\nopinionated features from reviews. For reviews Riof each\nalbum, we mine bi-grams and single-noun aspects (or re-\nview features), see [13]; e.g. bi-grams which conform to\na noun followed by a noun (e.g. chorus arrangement ) or\nan adjective followed by a noun (e.g. original sound ) are\nconsidered, excluding bi-grams whose adjective is a sen-\ntiment word (e.g. excellent ,terrible ). Separately, single-\nnoun aspects are validated by eliminating nouns that are\nrarely associated with sentiment words in reviews, since\nsuch nouns are unlikely to refer to item aspects. We refer\nto each of these extracted aspects Ajas review aspects.\nFor a review aspect Ajwe determine if there are any\nsentiment words in the sentence containing Aj. If not,\nAjis marked neutral, otherwise we identify the sentiment\nwordwminwith the minimum word-distance to Aj. Next\nwe determine the POS tags for wmin,Aiand any words\nthat occur between wminandAi. We assign a sentiment\nscore between -1 and 1 to Ajbased on the sentiment of\nwmin, subject to whether the corresponding sentence con-\ntains any negation terms within 4words ofwmin. If there\nare no negation terms, then the sentiment assigned to Aj\nis that of the sentiment word in the sentiment lexicon; oth-\nerwise this sentiment is reversed. Our sentiment lexicon\nis derived from SentiWordNet [10] and is not speciﬁcally\ntuned for music reviews. An overview of the process is\nshown in Figure 1. The end result of sentiment analysis\n3http://mtg.upf.edu/download/datasets/mardis that we determine a sentiment label Sijfor each aspect\nAjin reviewRi. A sample annotated review is shown in\nFigure 2\n“V erymelodicgreatguitarriffsbutthevocalsareshrill”SAAAS+ve+ve-ve\nFigure 2 : A sentence from a sample review annotated with\nopinion and aspect pairs.\n4.2 Entity Linking\nEntity Linking (EL) is the task to provide, given a men-\ntion to a named entity (e.g. person, location or organi-\nzation), its most suitable entry in a reference Knowledge\nBase (KB) [22]. In our case, EL was performed taking ad-\nvantage of Tagme4[11], an EL system that matches en-\ntity candidates with Wikipedia links, and then performs\ndisambiguation exploiting both the in-link graph and the\nWikipedia page dataset. TagMe provides for each detected\nentity, its Wikipedia page id and Wikipedia categories.\n5. MUSIC GENRE CLASSIFICATION\n5.1 Dataset Description\nStarting from MARD, our purpose is to create a subset\nsuitable for genre classiﬁcation, including 100 albums per\ngenre class. We enforce these albums to be authored by\ndifferent artists, and that review texts and audio descrip-\ntors of their songs are available in MARD. Then, for every\nalbum, we selected audio descriptors of the ﬁrst song of\neach album as representative sample of the album. From\nthe original 16 genres, 3 of them did not have enough in-\nstances complying with these prerequisites (Reggae, Blues\nand Gospel). This results in a classiﬁcation dataset com-\nposed of 1,300 albums, divided in 13 different genres, with\naround 1,000 characters of review per album.\n5.2 Features\n5.2.1 Textual Surface Features\nWe used a standard Vector Space Model representation of\ndocuments, where documents are represented as bag-of-\nwords (BoW) after tokenizing and stopword removal. All\nwords and bigrams (sequences of two words) are weighted\naccording to tf-idf measure.\n5.2.2 Semantic Features\nWe enriched the initial BoW vectors with semantic infor-\nmation thanks to the EL step. Speciﬁcally, for each named\nentity disambiguated with Tagme, its Wikipedia ID and its\nassociated categories are added to the feature vector, also\nwith tf-idf weighting. Wikipedia categories are organized\nin a taxonomy, so we enriched the vectors by adding one\nlevel more of broader categories to the ones provided by\n4http://tagme.di.unipi.it/152 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Alt. Rock Classical Country Electronic Folk Jazz Latin Metal New Age Pop R&B Hip-Hop Rock\nAlt. Rock 28 / 42 1 / 3 3 / 1 10 / 10 7 / 1 1 / 2 2 / 0 18 / 12 10 / 2 4 / 10 3 / 6 3 / 2 10 / 9\nClassical 0 / 0 87 / 95 1 / 0 0 / 0 1 / 1 1 / 1 2 / 2 1 / 0 5 / 1 1 / 0 0 / 0 0 / 0 1 / 0\nCountry 2 / 1 0 / 0 51 / 84 3 / 0 9 / 1 9 / 0 3 / 0 0 / 1 3 / 0 8 / 8 6 / 4 1 / 0 5 / 1\nElectronic 7 / 3 3 / 1 1 / 2 40 / 61 4 / 1 1 / 2 2 / 2 6 / 0 7 / 5 6 / 5 6 / 7 13 / 5 4 / 7\nFolk 4 / 6 11 / 0 13 / 10 7 / 0 27 / 55 6 / 1 7 / 3 4 / 2 6 / 9 5 / 9 6 / 4 1 / 0 3 / 1\nJazz 7 / 0 10 / 1 6 / 2 2 / 2 5 / 0 45 / 82 6 / 3 3 / 0 8 / 2 3 / 5 4 / 1 1 / 1 0 / 1\nLatin 4 / 3 6 / 4 9 / 2 1 / 2 5 / 1 10 / 2 28 / 78 3 / 0 6 / 2 11 / 4 7 / 2 5 / 0 5 / 0\nMetal 13 / 5 1 / 0 1 / 1 2 / 2 1 / 0 0 / 1 1 / 0 63 / 87 1 / 0 1 / 0 3 / 1 1 / 0 12 / 3\nNew Age 9 / 2 7 / 6 9 / 0 7 / 4 10 / 10 9 / 2 7 / 6 3 / 3 15 / 53 10 / 7 6 / 1 2 / 1 6 / 5\nPop 6 / 2 9 / 1 10 / 2 9 / 2 5 / 3 9 / 2 5 / 2 2 / 0 7 / 1 19 / 73 7 / 6 2 / 2 10 / 5\nR&B 8 / 2 0 / 1 16 / 3 8 / 4 2 / 0 5 / 3 5 / 0 1 / 0 3 / 0 7 / 10 24 / 71 17 / 5 4 / 1\nHip-Hop 8 / 2 0 / 0 2 / 1 8 / 2 0 / 1 0 / 1 1 / 0 4 / 3 2 / 0 4 / 1 7 / 2 61 / 86 3 / 1\nRock 17 / 15 1 / 2 6 / 8 4 / 7 10 / 5 2 / 4 7 / 1 12 / 13 4 / 1 9 / 7 7 / 4 6 / 2 15 / 31\nTable 2 : Confusion matrix showing results derived from AB acoustic-based classiﬁer/BoW+SEM text-based approach.\nTagme. Broader categories were obtained by querying DB-\npedia5.\n5.2.3 Sentiment Features\nBased on those aspects and associated polarity extracted\nwith the opinion mining framework, with an average num-\nber of aspects per review around 37, we follow [21] and\nimplement a set of sentiment features, namely:\n•Positive to All Emotion Ratio: fraction of all senti-\nmental features which are identiﬁed as positive (sen-\ntiment score greater than 0).\n•Document Emotion Ratio: fraction of total words\nwith sentiments attached. This feature captures the\ndegree of affectivity of a document regardless of its\npolarity.\n•Emotion Strength: This document-level feature is\ncomputed by averaging sentiment scores over all as-\npects in the document.\n•F-Score6: This feature has proven useful for de-\nscribing the contextuality/formality of language. It\ntakes into consideration the presence of a priori “de-\nscriptive” POS tags (nouns and adjectives), as op-\nposed to “action” ones such as verbs or adverbs.\n5.2.4 Acoustic Features\nAcoustic features are obtained from AB. They are com-\nputed using Essentia7. These encompass loudness, dy-\nnamics, spectral shape of the signal, as well as additional\ndescriptors such as time-domain, rhythm, and tone [26].\n5.3 Baseline approaches\nTwo baseline systems are implemented. First, we imple-\nment the text-based approach described in [15] for music\nreview genre classiﬁcation. In this work, a Na ¨ıve Bayes\nclassiﬁer is trained on a collection of 1,000 review texts,\nand after preprocessing (tokenisation and stemming), BoW\nfeatures based on document frequencies are generated.\nThe second baseline is computed using the AB frame-\nwork for song classiﬁcation [26]. Here, genre classiﬁca-\ntion is computed using multi-class support vector machines\n5http://dbpedia.org\n6Not to be confused with the evaluation metric.\n7http://essentia.upf.edu/BoW BoW+SEM BoW+SENT\nLinear SVM 0.629 0.691 0.634\nRidge Classiﬁer 0.627 0.689 0.61\nRandom Forest 0.537 0.6 0.521\nTable 3 : Accuracy of the different classiﬁers\n(SVMs) with a one-vs.-one voting strategy. The classiﬁer\nis trained with the set of low-level features present in AB.\n5.4 Experiments\nWe tested several classiﬁers typically used for text classi-\nﬁcation, namely Linear SVM, Ridge Classiﬁer and Near-\nest Centroid, using the implementations provided by the\nscikit-learn library8. Among them, Linear SVM has\nshown better performance when combining different fea-\nture sets (see Table 3). Therefore, we trained a Lin-\near SVM classiﬁer with L2 penalty over different sub-\nsets of the features described in Section 5.2, which are\ncombined via linear aggregation. Speciﬁcally, we com-\nbine the different feature sets into ﬁve systems, namely\nBoW (BoW), BoW+Semantic without broader categories\n(BoW+SEM), BoW+Semantic Broader with broader cat-\negories (BoW+SEMb), BoW+Sentiment (BoW+SENT)\nandBoW+Semantic+Sentiment (BoW+SEM+SENT). In\nthis way, we aim at understanding the extent to which sen-\ntiment and semantic features (and their interaction) may\ncontribute to the review genre classiﬁcation task. Note that\nthis paper is focused on the inﬂuence of textual features\nin genre classiﬁcation, and classiﬁcation based on acous-\ntic features is simply used as a baseline for comparison. A\nproper combination of acoustic and textual features in text\nclassiﬁcation is a challenging problem and would require\na deeper study that is out of the scope of this paper. The\ndataset is split 80-20% for training and testing, and accu-\nracy values are obtained after 5-fold cross validation.\n5.5 Results and Discussion\nAccuracy results of the two baseline approaches intro-\nduced in Section 5.3 along with our approach variants\nare shown in Figure 3. At ﬁrst sight, we may conclude\nthat sentiment features contribute to slightly outperform-\ning purely text-based approaches. This result implies that\n8http://scikit-learn.org/Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 153Figure 3 : Percentage of accuracy of the different ap-\nproaches. AB refers to the AcousticBrainz framework. NB\nrefers to the method based on Na ¨ıve Bayes from [15].\naffective language present in a music review is not a salient\nfeature for genre classiﬁcation (at least with the technology\nwe applied), although it certainly helps. On the contrary,\nsemantic features clearly boost pure text-based features,\nachieving 69.08% of accuracy. The inclusion of broader\ncategories does not improve the results in the semantic ap-\nproach. The combination of semantic and sentiment fea-\ntures improves the BoW approach, but the achieved accu-\nracy is slightly lower than using semantic features only.\nLet us review the results obtained with baseline sys-\ntems. The Na ¨ıve Bayes approach from [15] is reported to\nachieve an accuracy of 78%, while in our results it is below\n55%. The difference in accuracy may be due to the sub-\nstantial difference in length of the review texts. In [15], re-\nview texts were at least 3,000 characters long, much larger\nthat ours. Moreover, the addition of a distinction between\nClassic Rock and Alternative Rock is penalizing our re-\nsults. As for the acoustic-based approach, although the\nobtained accuracy may seem low, it is in fact a good re-\nsult for purely audio-based genre classiﬁcation, given the\nhigh number of classes and the absence of artist bias in the\ndataset [3]. Finally, we refer to Table 2 to highlight the\nfact that the text-based approach clearly outperforms the\nacoustic-based classiﬁer, although in general both show a\nsimilar behaviour across genres. Also, note the low accu-\nracy for both Classic Rock and Alternative Rock, which\nsuggests that their difference is subtle enough for making\nit a hard problem for automatic classiﬁcation.\n6. DIACHRONIC STUDY OF MUSIC CRITICISM\nWe carried out a study of the evolution of music criticism\nfrom two different temporal standpoints. Speciﬁcally, we\nconsider when the review was written and, in addition,\nwhen the album was ﬁrst published. Since we have sen-\ntiment information available for each review, we ﬁrst com-\nputed an average sentiment score for each year of review\npublication (between 2000 and 2014). In this way, we may\ndetect any signiﬁcant ﬂuctuation in the evolution of affec-\ntive language during the 21st century. Then, we also cal-\nculated the average sentiment for each review by year of\nalbum publication. This information is obtained from MB\nand complemented with the average of the Amazon rating\nscores.\nIn what follows, we show visualizations for sentiment\nscores and correlation with ratings given by Amazon users,according to these two different temporal dimensions. Al-\nthough arriving to musicological conclusions is out of\nthe scope of this paper, we provide food for thought and\npresent the readers with hypotheses that may explain some\nof the facts revealed by these data-driven trends.\n6.1 Evolution by Review Publication Year\nWe applied sentiment and rating average calculations to the\nwhole MARD dataset, grouping album reviews by year of\npublication of the review. Figure 4a shows the average of\nthe sentiment scores associated to every aspect identiﬁed\nby the sentiment analysis framework in all the reviews pub-\nlished in a speciﬁc year, whilst Figure 4b shows average\nreview ratings per year. At ﬁrst sight, we do not observe\nany correlation between the trends illustrated in the ﬁgures.\nHowever, the sentiment curve (Figure 4a) shows a remark-\nable peak in 2008, a slightly lower one in 2013, and a low\nbetween 2003 and 2007, and also between 2009 and 2012.\nIt is not trivial to give a proper explanation of this vari-\nations on the average sentiment. We speculate that these\ncurve ﬂuctuations may suggest some inﬂuence of econom-\nical or geopolitical circumstances in the language used in\nthe reviews, such as the 2008 election of Barack Obama\nas president of the US. As stated by the political scientist\nDominique Mo ¨ısi in [20]:\nIn November 2008, at least for a time, hope pre-\nvailed over fear. The wall of racial prejudice fell as\nsurely as the wall of oppression had fallen in Berlin\ntwenty years earlier [...] Yet the emotional dimen-\nsion of this election and the sense of pride it created\nin many Americans must not be underestimated.\nAnother factor that might be related to the positiveness\nin use of language is the economical situation. After sev-\neral years of continuous economic growth, in 2007 a global\neconomic crisis started9, whose consequences were visi-\nble in the society after 2008 (see Figure 4c). In any case,\nfurther study of the different implied variables is necessary\nto reinforce any of these hypotheses.\n.\n6.2 Evolution by Album Publication Year\nIn this case, we study the evolution of the polarity of lan-\nguage by grouping reviews according to the album publica-\ntion date. This date was gathered from MB, meaning that\nthis study is conducted on the 42,1% of the MARD that\nwas successfully mapped. We compared again the evo-\nlution of the average sentiment polarity (Figure 4d) with\nthe evolution of the average rating (Figure 4e). Contrary\nto the results observed by review publication year, here\nwe observe a strong correlation between ratings and sen-\ntiment polarity. To corroborate that, we computed ﬁrst a\nsmoothed version of the average graphs, by applying 1-D\nconvolution (see line in red in Figures 4d and 4e). Then we\ncomputed Pearson’s correlation between smoothed curves,\nobtaining a correlation r= 0.75, and a p-value p/lessmuch0.001.\nThis means that in fact there is a strong correlation between\n9https://research.stlouisfed.org154 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016(a) Sentiment\n (b) Rating\n(c) USA GDP trend\n(d) Sentiment\n (e) Rating\n (f) Sentiment by genre\nFigure 4 : Sentiment and rating averages by review publication year (a and b); GDP trend in USA from 2000 to 2014 (c),\nand sentiment and rating averages by album publication year (d, e and f)\nthe polarity identiﬁed by the sentiment analysis framework\nin the review texts, and the rating scores provided by the\nusers. This correlation reinforces the conclusions that may\nbe drawn from the sentiment analysis data.\nTo further dig into the utility of this polarity measure for\nstudying genre evolution, we also computed the smoothed\ncurve of the average sentiment by genre, and illustrate it\nwith two idiosyncratic genres, namely PopandReggae (see\nFigure 4f). We observe in the case of Reggae that there is a\ntime period where reviews have a substantial use of a more\npositive language between the second half of the 70s and\nthe ﬁrst half of the 80s, an epoch which is often called the\ngolden age of Reggae [2]. This might be related to the pub-\nlication of Bob Marley albums, one of the most inﬂuential\nartists in this genre, and the worldwide spread popularity\nof reggae music. In the case of Pop, we observe a more\nconstant sentiment average. However, in the 60s and the\nbeginning of 70s there are higher values, probably con-\nsequence by the release of albums by The Beatles. These\nresults show that the use of sentiment analysis on music re-\nviews over certain timelines may be useful to study genre\nevolution and identify inﬂuential events.\n7. CONCLUSIONS AND FUTURE WORK\nIn this work we have presented MARD, a multimodal\ndataset of album customer reviews combining text, meta-\ndata and acoustic features gathered from Amazon, MB and\nAB respectively. Customer review texts are further en-\nriched with named entity disambiguation along with polar-\nity information derived from aspect-based sentiment analy-\nsis. Based on this information, a text-based genre classiﬁer\nis trained using different combinations of features. A com-\nparative evaluation of features suggests that a combination\nof bag-of-words and semantic information has higher dis-\ncriminative power, outperforming competing systems in\nterms of accuracy. Our diachronic study of the sentiment\npolarity expressed in customer reviews explores two in-teresting ideas. First, the analysis of reviews classiﬁed\nby year of review publication suggests that geopolitical\nevents or macro-economical circumstances may inﬂuence\nthe way people speak about music. Second, an analy-\nsis of the reviews classiﬁed by year of album publication\nis presented. The results show how sentiment analysis\ncan be very useful to study the evolution of music gen-\nres. The correlation observed between average rating and\nsentiment scores suggest the suitability of the proposed\nsentiment-based approach to predict user satisfaction with\nmusical products. Moreover, according to the observed\ntrend curves, we can state that we are now in one of the\nbest periods of the recent history of music. Further work is\nnecessary to elaborate on these hypotheses. In addition, the\ncombination of audio and textual features is still an open\nproblem, not only for classiﬁcation but also for the study\nof the evolution of music. We expect the released dataset\nwill be explored in multiple ways for the development of\nmultimodal research approaches in MIR. In conclusion,\nthe main contribution of this work is a demonstration of\nthe utility of applying systematic linguistic processing on\ntexts about music. Furthermore, we foresee our method to\nbe of interest for musicologists, sociologists and humani-\nties researchers in general.\n8. ACKNOWLEDGEMENTS\nThis work was partially funded by the Spanish Min-\nistry of Economy and Competitiveness under the Maria\nde Maeztu Units of Excellence Programme (MDM-2015-\n0502), by the TUNER project (TIN2015-65308-C5-5-R,\nMINECO/FEDER, UE), by the Keystone COST Action\nIC1302 and by the Insight Centre for Data Analytics un-\nder grant number SFI/12/RC/2289.\n9. REFERENCES\n[1] C S Alcorta, R Sosis, and D Finkel. Ritual harmony:\nToward an evolutionary theory of music. BehavioralProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 155and Brain Sciences , 31(5):576–+, 2008.\n[2] Michael Randolph Alleyne and Sly Dunbar. The Ency-\nclopedia of Reggae: The Golden Age of Roots Reggae .\n2012.\n[3] Dmitry Bogdanov, Alastair Porter, Perfecto Herrera,\nand Xavier Serra. Cross-collection evaluation for mu-\nsic classiﬁcation tasks. In ISMIR’16 , 2016.\n[4]`Oscar Celma and Perfecto Herrera. A new approach\nto evaluating novel recommendations. In RecSys’08 ,\npages 179–186, 2008.\n[5] Kahyun Choi, Jin Ha Lee, and J. Stephen Downie.\nWhat is this song about anyway?: Automatic classi-\nﬁcation of subject using user interpretations and lyrics.\nProceedings of the ACM/IEEE Joint Conference on\nDigital Libraries , pages 453–454, 2014.\n[6] Ruihai Dong, Michael P O’Mahony, and Barry Smyth.\nFurther Experiments in Opinionated Product Recom-\nmendation. In ICCBR’14 , pages 110–124, Cork, Ire-\nland, September 2014.\n[7] Ruihai Dong, Markus Schaal, Michael P. O’Mahony,\nand Barry Smyth. Topic Extraction from Online Re-\nviews for Classiﬁcation and Recommendation. IJ-\nCAI’13, pages 1310–1316, 2013.\n[8] J. Stephen Downie and Xiao Hu. Review mining for\nmusic digital libraries:phase II. Proceedings of the 6th\nACM/IEEE-CS Joint Conference on Digital Libraries ,\npage 196, 2006.\n[9] Daniel P W Ellis. Automatic record reviews. ICMIR ,\n2004.\n[10] Andrea Esuli and Fabrizio Sebastiani. Sentiwordnet: A\npublicly available lexical resource for opinion mining.\nInProceedings of LREC , volume 6, pages 417–422.\nCiteseer, 2006.\n[11] Paolo Ferragina and Ugo Scaiella. Fast and Accurate\nAnnotation of Short Texts with Wikipedia Pages. Soft-\nware, IEEE , 29(1), June 2012.\n[12] Maristella Johanna Feustle. Lexicon of Jazz invec-\ntive: Hurling insults across a century with Big Data.\nIAML/IMS’15 , 2015.\n[13] Minqing Hu and Bing Liu. Mining Opinion Features in\nCustomer Reviews. In AAAI’04 , pages 755–760, San\nJose, California, 2004.\n[14] Xiao Hu and J Stephen Downie. Stylistics in customer\nreviews of cultural objects. SIGIR Forum , pages 49–51,\n2006.\n[15] Xiao Hu, J Stephen Downie, Kris West, and Andreas\nEhmann. Mining Music Reviews: Promising Prelimi-\nnary Results. ISMIR , 2005.[16] Patrik N Juslin and Daniel V ¨astfj¨all. Emotional re-\nsponses to music: the need to consider underly-\ning mechanisms. The Behavioral and brain sciences ,\n31(5):559–621, 2008.\n[17] Matthias Mauch, Robert M MacCallum, Mark Levy,\nand Armand M Leroi. The evolution of popular music:\nUSA 1960-2010. Royal Society Open Science , 2015.\n[18] Julian McAuley, Rahul Pandey, and Jure Leskovec. In-\nferring Networks of Substitutable and Complementary\nProducts. KDD’15 , page 12, 2015.\n[19] Julian McAuley, Christopher Targett, Qinfeng Shi, and\nAnton Van Den Hengel. Image-based Recommenda-\ntions on Styles and Substitutes. SIGIR’15 , pages 1–11,\n2015.\n[20] Dominique Moisi. The Geopolitics of Emotion: How\nCultures of Fear, Humiliation, and Hope are Reshaping\nthe World . Anchor Books, New York, NY , USA, 2010.\n[21] Calkin Suero Montero, Myriam Munezero, and Tuomo\nKakkonen. Computational Linguistics and Intelligent\nText Processing , pages 98–114, 2014.\n[22] Andrea Moro, Alessandro Raganato, and Roberto Nav-\nigli. Entity Linking meets Word Sense Disambigua-\ntion: A Uniﬁed Approach. TACL’14 , 2:231–244, 2014.\n[23] Tony Mullen and Nigel Collier. Sentiment Analysis us-\ning Support Vector Machines with Diverse Information\nSources. EMNLP’04 , pages 412–418, 2004.\n[24] Sergio Oramas, Luis Espinosa-anke, Mohamed Sordo,\nHoracio Saggion, and Xavier Serra. ELMD: An Auto-\nmatically Generated Entity Linking Gold Standard in\nthe Music Domain. In LREC’16 , 2016.\n[25] Sergio Oramas, Francisco G ´omez, Emilia G ´omez, and\nJoaqu ´ın Mora. Flabase: Towards the creation of a ﬂa-\nmenco music knowledge base. In ISMIR’15 , 2015.\n[26] Alastair Porter, Dmitry Bogdanov, Robert Kaye, Ro-\nman Tsukanov, and Xavier Serra. Acousticbrainz: a\ncommunity platform for gathering music information\nobtained from audio. ISMIR’15 , pages 786–792, 2015.\n[27] Maria Ruiz-Casado, Enrique Alfonseca, Manabu Oku-\nmura, and Pablo Castells. Information extraction and\nsemantic annotation of wikipedia. Ontology Learning\nand Population: Bridging the Gap between Text and\nKnowledge , pages 145–169, 2008.\n[28] Swati Tata and Barbara Di Eugenio. Generating Fine-\nGrained Reviews of Songs from Album Reviews. Pro-\nceedings of the 48th ACL Anual Meeting , (July):1376–\n1385, 2010.\n[29] Wei Zheng, Chaokun Wang, Rui Li, Xiaoping Ou, and\nWeijun Chen. Music Review Classiﬁcation Enhanced\nby Semantic Information. Web Technologies and Ap-\nplications , 6612(60803016):5–16, 2011.156 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Enhancing Cover Song Identification with Hierarchical Rank Aggregation.",
        "author": [
            "Julien Osmalskyj",
            "Marc Van Droogenbroeck",
            "Jean-Jacques Embrechts"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1418109",
        "url": "https://doi.org/10.5281/zenodo.1418109",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/064_Paper.pdf",
        "abstract": "Cover song identification involves calculating pairwise si- milarities between a query audio track and a database of reference tracks. While most authors make exclusively use of chroma features, recent work tends to demonstrate that combining similarity estimators based on multiple audio features increases the performance. We improve this ap- proach by using a hierarchical rank aggregation method for combining estimators based on different features. More precisely, we first aggregate estimators based on global features such as the tempo, the duration, the overall loud- ness, the number of beats, and the average chroma vector. Then, we aggregate the resulting composite estimator with four popular state-of-the-art methods based on chromas as well as timbre sequences. We further introduce a refine- ment step for the rank aggregation called “local Kemeniza- tion” and quantify its benefit for cover song identification. The performance of our method is evaluated on the Sec- ond Hand Song dataset. Our experiments show a signifi- cant improvement of the performance, up to an increase of more than 200 % of the number of queries identified in the Top-1, compared to previous results.",
        "zenodo_id": 1418109,
        "dblp_key": "conf/ismir/OsmalskyjDE16",
        "content": "ENHANCING COVER SONG IDENTIFICATION WITH HIERARCHICAL\nRANK AGGREGATION\nJulien Osmalskyj, Marc Van Droogenbroeck, Jean-Jaques Embrechts\nINTELSIG Laboratory - University of Liège - Belgium\njosmalsky@ulg.ac.be, jjembrechts@ulg.ac.be\nABSTRACT\nCover song identiﬁcation involves calculating pairwise si-\nmilarities between a query audio track and a database of\nreference tracks. While most authors make exclusively use\nof chroma features, recent work tends to demonstrate that\ncombining similarity estimators based on multiple audio\nfeatures increases the performance. We improve this ap-\nproach by using a hierarchical rank aggregation method for\ncombining estimators based on different features. More\nprecisely, we ﬁrst aggregate estimators based on global\nfeatures such as the tempo, the duration, the overall loud-\nness, the number of beats, and the average chroma vector.\nThen, we aggregate the resulting composite estimator with\nfour popular state-of-the-art methods based on chromas as\nwell as timbre sequences. We further introduce a reﬁne-\nment step for the rank aggregation called “local Kemeniza-\ntion” and quantify its beneﬁt for cover song identiﬁcation.\nThe performance of our method is evaluated on the Sec-\nond Hand Song dataset. Our experiments show a signiﬁ-\ncant improvement of the performance, up to an increase of\nmore than 200 % of the number of queries identiﬁed in the\nTop-1, compared to previous results.\n1. INTRODUCTION\nGiven an audio query track, the goal of a cover song iden-\ntiﬁcation system is to retrieve at least one different version\nof the query in a reference database, in order to identify it.\nIn that context, a version can be described as a new per-\nformance or recording of a previously recorded track [22].\nRetrieving covers is a challenging task, as the different ren-\nditions of a song can differ from the original track in terms\nof tempo, pitch, structure, instrumentation, etc. The usual\nway of retrieving cover songs in a database involves ex-\ntracting meaningful features from an audio query ﬁrst in\norder to compare them to the corresponding features com-\nputed for the other tracks of the database using a pairwise\nsimilarity function. The function returns a score or a prob-\nability of similarity. Many researchers have been using ex-\nclusively chroma features [10, 13, 14, 22] to characterize\nc/circlecopyrtJulien Osmalskyj, Marc Van Droogenbroeck, Jean-\nJaques Embrechts. Licensed under a Creative Commons Attribution 4.0\nInternational License (CC BY 4.0). Attribution: Julien Osmalskyj,\nMarc Van Droogenbroeck, Jean-Jaques Embrechts. “Enhancing cover\nsong identiﬁcation with hierarchical rank aggregation”, 17th International\nSociety for Music Information Retrieval Conference, 2016.\nT op-100\nMean aggregation rule\nT empo\nBeats\nDurationLoudnessAvg ChromaMinimum aggregation rule\n2D-FTM\nXCorr TimbreQuantization2774\n1381\nFigure 1 . Hierarchical rank aggregation of estimators\nbased on audio features. Global features are ﬁrst aggre-\ngated using the mean rule , identifying 1,381 tracks in the\ntop-100, out of 5,464 tracks sampled from the Second\nHand Song dataset. The resulting composite estimator\nis then aggregated with four remaining features using the\nminimum rule , identifying 2,774 tracks in the top-100.\nthe songs in the database. Chroma vectors describe the\nharmony of the songs and are robust to changes in instru-\nmentation and timbre, which makes them quite popular for\nthe task. While chromas are the most used features in the\nliterature, other works investigate the use of different fea-\ntures, such as timbral features [23] or cognition based fea-\ntures [4].\nIn recent work [16], we established that combining mul-\ntiple audio features improves the performance of cover song\nidentiﬁcation systems: designing several classiﬁers based\non different features and combining them through proba-\nbilistic rules or rank aggregation techniques improves the\nperformance. In light of this, it seems important to study\nhow state-of-the-art features perform when they are com-\nbined for cover song identiﬁcation. In this paper, we im-\nprove upon previous work by considering a total of nine\nfeatures, including four state-of-the-art ones. These fea-\ntures cover a wide range of audio characteristics, from low-\ndimensional ones such as the tempo or the duration of the\nsongs, to higher level characteristics such as chromas and\ntimbral sequences. We build similarity estimators for each\nfeature, using supervised machine learning for some of\nthem, and combine them in a hierarchical way to design\na new combination method. In this method, we ﬁrst ag-\ngregate ﬁve estimators that are based on global features:136tempo, duration, loudness, beats, and averaged chroma.\nThese global features are computed for the entire song,\nrather than for individual chunks of audio. We show that\ncombining such estimators using rank aggregation meth-\nods improves the performance, compared to probabilistic\nfusion [16]. We then take the resulting aggregated estima-\ntor and combine it with four state-of-the-art methods us-\ning a different aggregation rule, as shown in Figure 1. We\nfurther achieve a higher performance by applying a reﬁne-\nment step called “local Kemenization” [7, 8]. We found\nthat this reﬁnement step signiﬁcantly increases the number\nof queries identiﬁed immediately (Top-1).\n2. METHOD OVERVIEW\nIdentifying cover songs with respect to an audio query in-\nvolves comparing the query to a set of tracks in a refer-\nence database using a similarity function. Considering two\ninput tracks, the function should return a score indicating\nwhether the tracks are considered as being similar or not.\nOur approach follows the combining method that we pro-\nposed in [16]. As there exist several effective features in\nthe literature, the idea is to take advantage of all of them\nby combining them. We therefore design several pairwise\ncomparison functions, called similarity estimators, based\non different audio features. We ﬁrst consider the same\nset of estimators as the one used in [16]. We make use\nof global low-dimensional features such as the tempo, the\nduration, the number of beats, the overall loudness, and\nthe average chroma vector of a song, learning a probabilis-\ntic model to predict the similarity. We also include three\nestimators based on chromas features. The ﬁrst and sec-\nond ones were used in previous works and are respectively\nbased on the quantization of chroma features [11, 16] and\nthe cross-correlation of entire chroma sequences [9]. We\nadd a third chroma estimator based on an efﬁcient large-\nscale method proposed by Bertin-Mahieux et al. [2]. Fi-\nnally, to take into account timbral information, we include\nan estimator based on MFCC features. This method, intro-\nduced by Tralie et al. [23], showed that some covers could\nbe identiﬁed based on timbre only.\n2.1 Weak estimators\nIn previous work [16], we demonstrated that global low-\ndimensional features (tempo, duration, etc.) bring infor-\nmation that helps in the identiﬁcation process. However,\nusing only such features for identifying cover songs is not\nenough to achieve good performance. Indeed, such fea-\ntures are considered as weak because they only slightly im-\nprove a classiﬁer with respect to a purely random classiﬁer.\nWhile we combined these features using probabilistic com-\nbination rules, we innovate by combining them using rank\naggregation techniques. For each feature, we build a prob-\nabilistic estimator, using supervised machine learning. To\ndetermine the similarity of candidates with respect to the\nquery, we perform pairwise comparisons using the learned\nprobabilistic models to predict probabilities of similarities.\nEach query is compared to the database using each esti-mator. We then aggregate the rankings produced by each\nestimator to build an improved list of results.\n2.2 Chroma estimators\n2.2.1 Cross-correlation\nWe design the same cross-correlation estimator as the one\nused in [16]. This estimator is useful to take into account\ntemporal information. It computes two dimensional cross-\ncorrelations between high-pass ﬁltered chroma sequences\nof the tracks to be compared. The similarity score between\ntwo songs is computed as the reciprocal of the peak value\nof the cross-correlated signal. We refer the reader to the\noriginal work [9] for details.\n2.2.2 Quantization\nTo take into account the harmonic distribution of the songs,\nwe make use of an estimator based on the quantization of\nchroma features [11, 17]. For each track, chroma vec-\ntors are mapped to speciﬁc codewords. Codewords are\ndetermined using a K-Means clustering of 200,000 chro-\nmas vectors. We retain 100 clusters for the feature, re-\nsulting in a 100-dimensional feature vector. The similar-\nity score is computed as the cosine similarity between two\n100-dimensional vectors. To account for key transposition,\nwe make use of the optimal transposition index [21] (OTI)\ntechnique, as it has been used in other works [1, 20].\n2.2.3 2D Fourier transform magnitude coefﬁcients\nThis method was ﬁrst introduced by Bertin-Mahieux et\nal.[2] and was designed as a fast and accurate feature for\ncover song identiﬁcation. The idea is to encode harmonic\ninformation in a compact representation, to make it invari-\nant to local tempo changes and pitch shifts. First we ex-\ntract patches of 75 consecutive chromas with an overlap of\n1. We then compute the 2D FFT magnitude coefﬁcients\nfor each patch. Next, we aggregate all the patches point-\nwise using a median rule. Finally, we project the resulting\n900-dimensional representation on a 50 dimensional PCA\nsub-space. Each track is therefore represented by a 50-\ndimensional vector. The ﬁnal score between two tracks is\ncomputed as the cosine similarity between two projections.\n2.3 Timbre estimator\nIn our base set of estimators, we also include a method pro-\nposed by Tralie et al. [23], that takes into account the rela-\ntive evolution of timbre over time. Using careful centering\nand normalization, the authors were able to design features\nthat are approximately invariant to cover. The features are\nbased on self-similarity matrices of MFCC coefﬁcients and\ncan be used to identify cover songs. Being based on tim-\nbre rather than harmony, this feature demonstrates that if\nthe pitch is blurred and obscured, cover song identiﬁcation\nshould still be possible (see the original paper [23] for a\ndetailed explanation). We designed an estimator based on\nfeatures that were kindly computed for us by the authors\nof the method. The similarity score is computed using the\nSmith-Waterman alignment algorithm.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 1373. HIERARCHICAL RANK AGGREGATION\n3.1 Rank aggregation techniques\nTo take advantage of all the features that we use, we need\na way to combine them. One way of doing that is through\nprobabilistic combination rules. Under the hypothesis that\nall estimators return probabilities, we can experiment sev-\neral rules such as the probabilistic product, sum or median\nrules [5, 6]. The problem is that not all of our estimators\nreturn a probability. Some estimators return probabilities,\nwhile others return different kinds of scores, for example\na cosine similarity or a cross-correlation peak value. One\nsolution for using such rules would be to map scores to\nprobabilities, but there is no straightforward way of doing\nthat. Furthermore, an independent dataset is often manda-\ntory for such a mapping.\nAnother solution is to combine estimators through rank\naggregation techniques, as we proposed in [16]. As a sin-\ngle queryqis compared to the entire database using Nes-\ntimators, we obtain Ndifferent orderings of the database.\nEach track of the database can be found at different posi-\ntions in the resulting orderings. Based on the positions of\nthe tracks, rank aggregation techniques compute a new po-\nsition by applying simple rules such as computing the new\nrank as the mean of the ranks of each track in the initial\norderings. Other rules include the minimum, maximum\nor median rules. Rank aggregation techniques are popular\nin the web literature [7]. Such techniques are interesting\ncompared to score-based combination because they are in-\ntrinsically calibrated and scale-insensitive [19].\nIn this paper, we aggregate features at different levels.\nWe ﬁrst aggregate weak features, experimenting with mul-\ntiple rules. We next use the resulting ranking as a new\ninput for another aggregation rule, by considering our four\nremaining estimators. We therefore build a hierarchy of\ntwo aggregated classiﬁers (Figure 1) and achieve improved\nperformance compared to previous results [16].\n3.2 Optimizing rank aggregation\nAfter several input rankings r1, r2, ..., r khave been ag-\ngregated into one ﬁnal ranking µusing one of the rules\nproposed before, we can apply a reﬁnement step called lo-\ncal Kemenization [8] to further improve the ranking µ. An\naggregated ranking is locally Kemeny optimal if there are\nno pairwise swaps of items in the list that will reduce the\nsum of Kendall τ[8] measures between each input ranking\nriandµ, wherei= 1, ..., k . The sum of the Kendall τ\nmeasures with respect to each initial ranking is called “ ag-\ngregated Kendall measure” . The Kendall τmeasure deter-\nmines the correlation between two rankings of equal size.\nIt measures the degree to which one list agrees with an-\nother [15]. In practice, one way of computing it is to count\nthe number of swaps needed by a bubble sort algorithm\nto permute one list to the other. Formally, the Kendall τ\ndistance is deﬁned by\nτ=nc−nd\nn(n−1)/2, (1)wherencis the number of concordant pairs and ndis the\nnumber of discordant pairs. The denominator corresponds\nto the total number of pairs of nitems in the lists. A pair\nof tracks (i,j)isconcordant ifiis ranked above jin both\nlists, and discordant otherwise.\nBased on this distance measure between rankings, the\nlocal Kemenization procedure considers each pair of adja-\ncent tracks in µand veriﬁes whether a swap will improve\nthe aggregated Kendall measure. In practice, for two adja-\ncent tracks (i,j)inµ, withiranked above j, the procedure\nchecks whether track jis ranked above iin the majority\nof the input rankings. If yes, it swaps the two items as it\nreﬁnes the aggregated list with a reduced Kendall distance.\nThe procedure starts from the beginning of the list, and is\nrepeated iteratively for all pairs of tracks, requiring n−1\nchecks for an aggregated list of length n. Note that the\nconsecutive swaps of the Kemenization process take into\naccount the inclusion of earlier swaps. For implementa-\ntion details, we refer the reader to our own implementation\nof several rank aggregation rules with local Kemenization\nin a C++ library1. We used that code to produce results\nfor this paper.\nFor our task of cover song identiﬁcation, we apply the\nlocal Kemenization step to our ﬁnal aggregations to im-\nprove the overall performance. Detailed results are given\nin Section 4.\n4. EXPERIMENTS AND RESULTS\n4.1 Experimental setup\n4.1.1 Evaluation database\nWe evaluate our method on the Second Hand Song dataset2\n(SHS), a subset of the Million Song Dataset (MSD) [3].\nThe SHS is structured in 5,854 cliques , which are groups\nof 3 cover songs on average, for a total of 18,196 tracks.\nThe dataset does not provide any audio data. Rather than\nthat, it proposes a set of pre-computed features. Since we\nneed independent learning and test sets for learning prob-\nabilistic models, we split all tracks in a learning set (LS)\ncontaining 70 % of the tracks, and a test set (TS) contain-\ning the 30 % remaining tracks. We learn our models on\nthe LS, and evaluate our ﬁnal system on the TS, containing\n5,464 tracks. Following the procedure explained in [16],\nwe get rid of duplicate tracks in the SHS, thus reducing the\nnumber of cliques to 5,828.\n4.1.2 Estimators settings\nFor each estimator, we use the pre-computed features in the\nSHS. As the chroma features provided in the dataset are\naligned on onsets rather than the beats, we re-align them\non the beats to account for tempo variations within covers,\nas done in other works [2, 13].\nFor our weak estimators, we learn probabilistic models\nusing the ExtraTrees algorithm [12], to estimate probabili-\nties of similarity. The models are learned with 1,000 trees\n1https://github.com/osmju/librag\n2http://labrosa.ee.columbia.edu/millionsong/\nsecondhand138 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016010203040506070top−1\nimprov. of 146%\n050100150200250300350top−10\nimprov. of 111%\n02004006008001000120014001600top−100\nimprov. of 32%\n020040060080010001200MR\nimprov. of 10%\n00.0050.010.0150.020.0250.030.035MRR\nimprov. of 88%\n00.0020.0040.0060.0080.010.0120.0140.016MAP\nimprov. of 67%Figure 2 . Comparison of the performance of rank aggregation methods for combining estimators based on weak features.\nThe baseline is the probabilistic fusion of weak features proposed in [16]. The mean rank aggregation rule signiﬁcantly\noutperforms the baseline, especially in the top-1 with an improvement of 146 % . The red arrows quantify the improvement\ncompared to the baseline. /squaresolidbaseline, /squaresolidmean rank rule, /squaresolidmean rank rule with local Kemenization, /squaresolidminimum rank rule,\n/squaresolidminimim rank rule with local Kemenization, /squaresolidmedian rank rule, /squaresolidmedian rank rule with local Kemenization.\nto reduce the variance, and a maximum depth of 20. The\ntrees are not completely developped to avoid over-ﬁtting.\nThe implementation we use is the Python Scikit-Learn li-\nbrary [18].\nTo account for key transpositions in our estimator based\non the average chroma and in the quantization estimator,\nwe use the optimal transposition index (OTI) technique [21].\nFor the 2D-FTM estimator, we closely follow the orig-\ninal implementation. We wrote our own C++ implemen-\ntation, based on the Python code3provided by Humphrey\net al. [13]. We use the FFTW library for computing the\n2D-FFT of chroma patches.\nFinally, our timbre estimator is close to the original im-\nplementation [23], as the features were computed by the\nauthor itself for us. The only difference in the implementa-\ntion comes from the fact that the MFCC sequence of a song\nin the SHS does not have the same resolution than in the\noriginal implementation. We implemented our own ver-\nsion of the sequence alignment Smith-Waterman to com-\npute the ﬁnal score.\n4.2 Aggregation of weak features\nOur ﬁrst experiment consists in aggregating weak features,\nexperimenting with several fusion rules. We take estima-\ntors based on the tempo, the duration, the number of beats,\nthe average chroma vectors, the loudness, and aggregate\nthem. We compare the performance to the baseline re-\nsults obtained in our previous work [16]. We evaluate the\nperformance of the system using standard information re-\ntrieval statistics, such as the Mean Rank of the ﬁrst iden-\ntiﬁed track (MR), the Mean Reciprocal Rank (MRR), and\nthe Mean Average Precision (MAP). Note that the lower\nthe MR is, the better it is, while the goal is to maximize\n3https://github.com/urinieto/\nLargeScaleCoverSongIdBaseline Mean Kemeny Increase\nTop-1 26 58 64 + 146 %\nTop-10 158 307 333 + 111 %\nTop-100 1,044 1,379 1,381 + 32 %\nTop-1000 3,729 3,911 3,911 + 5 %\nMR 977.6 876.2 875 +10 %\nMRR 0.016 0.029 0.03 + 88 %\nMAP 0.009 0.014 0.015 + 67 %\nTable 1 . Performance achieved with weak estimators when\napplying the mean rank aggregation rule (“mean” column),\nand applying the reﬁnement step (“Kemeny” column) to\nimprove the performance.\nthe MAP and the MRR. We also evaluate the number of\nqueries for which a match is identiﬁed in the Top- K,K\nbeing a parameter. We present results for the number of\ntracks identiﬁed in the top-1, 10 and 100.\nFigure 2 displays the performance of three aggregation\nrules for the weak estimators with respect to all the met-\nrics. We can notice immediately that the mean aggrega-\ntion rule outperforms all other combinations. Without lo-\ncal Kemenization, the mean rule provides an improvement\nof123 % compared to the baseline, with 58 tracks iden-\ntiﬁed in the top-1 (against 26 in the baseline). That re-\nsult shows that rank aggregation of these features outper-\nforms the probabilistic rules proposed in previous work.\nImprovements in terms of the other metrics are also sig-\nniﬁcant and demonstrate the strength of the method. Ap-\nplying the reﬁnement local Kemenization step, we further\nimprove the performance to 64 tracks identiﬁed in the top-\n1, which corresponds to an increase of 146 % compared to\nthe baseline. Note that the reﬁnement provides surprisingly\ngood improvement, especially for the minimum aggrega-\ntion rule. Without optimization, we identify 7 tracks in theProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 139Baseline Mean Min Median\nTop-1 328 832 1010 839\nTop-10 1015 1479 1785 1499\nTop-100 2015 2669 2774 2681\nTop-1000 4158 4456 4416 4385\nMR 726.4 563 582 595\nMRR 0.107 0.194 0.234 0.198\nMAP 0.055 0.105 0.132 0.106\nTable 2 . Hierarchical aggregation of all features, with lo-\ncal Kemenization. Best performance is achieved with the\nminimum rule.\ntop-1. This number jumps to 42 tracks, without changing\nanything to the base estimators, simply by applying the al-\ngorithm presented in Section 3.2. Table 1 quantiﬁes the\nperformance and improvement compared to the baseline\nfor the mean aggregation rule, as it provides the best per-\nformance. Note that it is interesting to realize that using\nsuch weak features, we still can identify cover songs much\nbetter than random guessing.\n4.3 Hierarchical aggregation\nAs the mean rule produces the best experimental results\nwith weak estimators, we consider that resulting aggre-\ngated ranking as a single estimator by itself, and combine\nit with the four remaining estimators based on chroma and\ntimbral features. We experiment the hierarchical combi-\nnation with the mean, minimum and median rules, as for\nthe weak estimators. Figure 3 displays the performance of\neach rule, with the local Kemenization step applied. It is\nstraightforward to notice that the minimum rule with Ke-\nmenization signiﬁcantly outperforms the other rules, es-\npecially for the top-1, with 1,010 tracks identiﬁed in the\ntop-1. For the top-1 metric, we achieve the best perfor-\nmance so far on the subset we use for evaluation, with a\nMAP set to 0.132and a MRR set to 0.234. Table 2 quan-\ntiﬁes the metrics for all rules. The minimum rule achieves\nan impressive performance, especially for the top-1 met-\nric, with an improvement of 208 % , and for the MRR and\nthe MAP, with an improvement of respectively 119 % and\n140 % . Note that as we used a signiﬁcant subset of the SHS\n(70 %) as a learning set for our probabilistic models, it is\ndifﬁcult to compare our results with other works. There-\nfore, the baseline here corresponds to the best combination\nresults proposed in our previous work [16]. The baselines\nin Figures 2 and 3 are different because they respectively\ncorrespond to the combination of weak features, as done\nin [16], and the combination of weak features and chroma\nbased estimators, also as proposed in [16]. Figure 4 shows\nthe performance curves of the aggregations corresponding\nto the bars in Figure 3. The horizontal axis corresponds\nto the top-k cutoff, that is the proportion of tracks that are\nrejected from the ﬁnal set (the tracks ranked below k). The\nvertical axis corresponds to the loss, that is the proportion\nof queries for which no matches at all have been found in\nthe top-k. If at least one corresponding track matches the\nquery, then the loss is set to zero for that query. The sec-Mean Min Median\nTop-1 503 784 519\nTop-10 1187 1577 1106\nTop-100 2435 2535 2299\nTop-1000 4423 4290 4309\nMR 593 651 650\nMRR 0.13 0.19 0.13\nMAP 0.07 0.1 0.07\nTable 3 . Performance of single aggregation rules without\nhierarchization, with local Kemenization. Performance is\nnot as good as with hierarchization.\nond and third charts correspond to zooms in the lower left\ncorner and in the upper right corner. From the performance\ncurves, we clearly observe the improvement compared to\nthe baseline. We also observe that the ﬁnal curve (mini-\nmum rule, green) ﬁts very closely the upper right part of\nthe chart, corresponding to a very high cutoff value. We\ncan reasonably tell that approximately half of the input\nqueries are identiﬁed in the top-1% of the returned rank-\ning. Note however how the mean curve (blue) takes the\nbest position at low cutoff values.\n4.4 Single aggregation of all features\nTo quantify the beneﬁt of using hierarchical rank aggre-\ngation rather than running a single combination, we com-\nbined all the features with the three aggregation rules, and\napplied the reﬁnement step. Aggregating all features in a\nsingle run corresponds to setting equal weights to all fea-\ntures. On the other hand, aggregating the results in a hi-\nerarchical way corresponds to set different weights to the\nfeatures. Table 3 gives the performance of all aggregation\nrules with local Kemenization without any hierarchization.\nThe best performing rule in terms of Top-{1, 10, 100} is\nagain the minimum rule. Similar conclusions yield for the\nMRR and MAP metrics. For the Top-1000 and the MR,\nthe best rule is the mean aggregation. Overall, the results\nare worse than using hierarchical aggregation. For the top-\n1 metric, the number of identiﬁed tracks drops by 22 % for\nthe minimum rule, which is quite signiﬁcant. For the MRR\nand the MAP respectively, the performance is decreased by\n19 % and24 % for the minimum rule. This demonstrates\nthat attributing different weights to the estimators allows to\nachieve better performance. Avoiding the hierarchization\nwould lead to a decreased performance, as indicated by the\nresults in Table 3, compared to Table 2.\n5. CONCLUSION\nIn this paper, we improve cover song identiﬁcation by eval-\nuating multiple rank aggregation rules. Based on previous\nwork, we ﬁrst construct probabilistic estimators based on\nmultiple weak features such as tempo, duration, loudness,\nnumber of beats, and average chroma vectors. We use su-\npervised machine learning to learn models predicting prob-\nabilities of similarity. Then, rather than combining the\nestimators through probabilistic rules, we have evaluated140 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016020040060080010001200top−1\nimprov. of 208%\n0200400600800100012001400160018002000top−10\nimprov. of 76%\n050010001500200025003000top−100\nimprov. of 38%\n0100200300400500600700800MR\nimprov. of 22%\n00.050.10.150.20.25MRR\nimprov. of 119%\n00.050.10.15MAP\nimprov. of 140%Figure 3 . Hierarchical aggregation of all estimators with local Kemenization. The weak estimators are ﬁrst aggregated\nusing the mean rank aggregation rule. The ﬁgure shows how the performance vary when considering different top-level\naggregation rules. The red arrows quantify the improvement compared to the baseline. /squaresolidbaseline [16], /squaresolidmean rank rule\nwith local Kemenization, /squaresolidminimum rank rule with local Kemenization, /squaresolidmedian rank rule with local Kemenization.\n0 0.2 0.4 0.6 0.8 100.10.20.30.40.50.60.70.80.91\nTop 100Top 1000\nCutoffLoss\n  \nBaseline\nMean\nMedian\nMin\nRandom\n0 0.05 0.1 0.15 0.2 0.2500.0020.0040.0060.0080.010.0120.0140.0160.0180.02\nCutoffLoss\n  \nBaseline\nMean\nMedian\nMin\nRandom\n0.99 0.992 0.994 0.996 0.998 10.550.60.650.70.750.80.850.90.951\nCutoffLoss\n  \nBaseline\nMean\nMedian\nMin\nRandom\nFigure 4 . Performance curves of the reﬁned hierarchical aggregation using all features. The x-axis corresponds to the\nproportion of tracks considered dissimilar by the method and the y-axis corresponds to the proportion of lost queries, that is\nthe proportion of queries for which no matches at all have been found. The second and third charts correspond respectively\nto a zoom in the lower left part of the ﬁrst chart, and to a zoom in the upper right corner of the ﬁrst chart.\nseveral rank aggregation rules, and prove that the mean\naggregation rule provides improved results, compared to\nthe baseline. Considering the resulting combined estima-\ntor, we further aggregate it with four estimators based on\nfour state-of-the-art features. The selected features take\ninto account harmonic information through chroma fea-\ntures, and timbral information through self-similarity ma-\ntrices of MFCC coefﬁcients. We further introduce an op-\ntimization step, called local Kemenization, that builds an\nimproved aggregated ranking by swapping tracks to the top\nof the list, with respect to the agreement with each base es-\ntimator. To combine the estimators, we aggregate them all\nin a hierarchical way, evaluating several hierarchical rank\naggregation rules. To highlight the gain of using hierarchi-\ncak rank aggregation, we also aggregate all nine features\nthrough a single aggregation rule, thus allocating an iden-\ntical weight to all features. We show that such a combi-\nnation degrades the performance. Our method is evaluated\non the Second Hand Song dataset, displaying the perfor-\nmance in terms of standard statistics such as the mean rankof the ﬁrst identiﬁed query, the mean reciprocal rank, the\nmean average precision and the number of tracks identi-\nﬁed at the top-k cutoff. Best results are achieved with the\nminimum aggregation rule with local Kemenization. In-\ndeed, we are able to identify 1,010 tracks at the ﬁrst po-\nsition, which corresponds to 18 % of the database. In the\nﬁrst 10 tracks returned, we identify 1,785 tracks ( 32% of\nthe database), which is a signiﬁcant improvement over pre-\nvious work. Compared to previous work on combination,\nwe improve the results by 208 % in terms of the number\nof tracks identiﬁed in the top-1. In terms of mean recipro-\ncal rank and mean average precision, we achieve improved\nperformance with a value of 0.234for the MRR and 0.132\nfor the MAP. The results show that aggregating multiple\nfeatures, and therefore taking into account multiple sources\nof musical information, leads to signiﬁcant improvements\nin the ﬁeld of cover song identiﬁcation. Our method takes\nadvantage of all the best from the literature in that ﬁeld,\nand suggests that following that direction of research might\neventually lead to an even better performance.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 1416. REFERENCES\n[1] T. Ahonen. Compression-Based Clustering of Chroma-\ngram Data : New Method and Representations. In In-\nternational Symposium on Computer Music Multidis-\nciplinary Research , pages 474–481, 2012.\n[2] T. Bertin-Mahieux and D. Ellis. Large-scale cover song\nrecognition using the 2d fourier transform magnitude.\nInProceedings of the 13th International Society for\nMusic Information Retrieval (ISMIR) , pages 241–246,\n2012.\n[3] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian\nWhitman, and Paul Lamere. The million song dataset.\nInProceedings of the 12th International Society for\nMusic Information Retrieval (ISMIR) , pages 591–596,\n2011.\n[4] S. Downie, H. Xiao, Y . Zhu, J. Zhu, and N. Chen.\nModiﬁed perceptual linear prediction liftered cepstrum\n(mplplc) model for pop cover song recognition. In 16th\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pages 598–604, Malaga, 2015.\n[5] R. Duin. The combining classiﬁer: to train or not\nto train? 16th International Conference on Pattern\nRecognition , 2:765–770, 2002.\n[6] R. Duin and D. Tax. Experiments with classiﬁer com-\nbining rules. Lecture Notes in Computer Science ,\n31(15):16–29, 2000.\n[7] C. Dwork, R. Kumar, M. Naor, and D. Sivakumar.\nRank aggregation methods for the Web. In Proceedings\nof the 10th international conference on World Wide\nWeb, pages 613–622, 2001.\n[8] C. Dwork, R. Kumar, M. Naor, and D. Sivaku-\nmar. Rank Aggregation Revisited. Systems Research ,\n13(2):86–93, 2001.\n[9] D. Ellis, C. Cotton, and M. Mandel. Cross-correlation\nof beat-synchronous representations for music similar-\nity. In International conference on acoustics, speech\nand signal processing (ICASSP) , pages 57–60, Las Ve-\ngas, 2008. IEEE.\n[10] D. Ellis and G. Poliner. Identifying Cover Songs with-\nchroma featires and dynamic beat tracking. In IEEE,\neditor, IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , pages 1–16,\nNew York, 2007. IEEE.\n[11] P. Foster, S. Dixon, and A. Klapuri. Identifying Cover\nSongs Using Information-Theoretic Measures of Sim-\nilarity. IEEE Transactions on Audio, Speech and Lan-\nguage Processing , 23(6):993–1005, 2015.\n[12] P. Geurts, D. Ernst, and L. Wehenkel. Extremely ran-\ndomized trees. Machine Learning , 63(1):3–42, 2006.[13] E. Humphrey, O. Nieto, and J. Bello. Data Driven and\nDiscriminative Projections for Large-scale Cover Song\nIdentiﬁcation. In Proceedings of the 14th International\nSociety for Music Information Retrieval Conference\n(ISMIR) , pages 4–9, 2013.\n[14] M. Khadkevich and M. Omologo. Large-Scale Cover\nSong Identiﬁcation Using Chord Proﬁles. In Proceed-\nings of the 14th International Society for Music In-\nformation Retrieval Conference (ISMIR) , pages 5–10,\n2013.\n[15] A. Langville and C. Meyer. The Science of Rating and\nRanking - Who’s #1? Princeton University Press, 2012.\n[16] J. Osmalskyj, P. Foster, S. Dixon, and J.J. Embrechts.\nCombining features for cover song identiﬁcation. In\n16th International Society for Music Information Re-\ntrieval Conference (ISMIR) , pages 462–468, Malaga,\n2015.\n[17] J. Osmalskyj, S. Pierard, M. Van Droogenbroeck, and\nJ.J. Embrechts. Efﬁcient database pruning for large-\nscale cover song recognition. In International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) , pages 714–718, Vancouver, BC, 2013.\n[18] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duches-\nnay. Scikit-learn: Machine learning in Python. Journal\nof Machine Learning Research , 12:2825–2830, 2011.\n[19] R. Prati. Combining feature ranking algorithms\nthrough rank aggregation. In Proceedings of the Inter-\nnational Joint Conference on Neural Networks , pages\n10–15, 2012.\n[20] J. Serrà. Identiﬁcation of Versions of the Same Musical\nComposition by Processing Audio Descriptions . PhD\nthesis, 2011.\n[21] J. Serrà, E. Gómez, and P. Herrera. Transposing\nChroma Representations to a Common Key. In IEEE\nConference on The Use of Symbols to Represent Music\nand Multimedia Objects , pages 45–48, 2008.\n[22] J. Serrà, E. Gomez, P. Herrera, and X. Serra. Chroma\nbinary similarity and local alignment applied to cover\nsong identiﬁcation. IEEE Transactions on Audio,\nSpeech and Language Processing , 16(6):1138–1151,\n2008.\n[23] C. Tralie and P. Bendich. Cover song identiﬁcation\nwith timbral shape sequences. In 16th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , pages 38–44, Malaga, 2015.142 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Structural Segmentation and Visualization of Sitar and Sarod Concert Audio.",
        "author": [
            "Vinutha T. P.",
            "Suryanarayana Sankagiri",
            "Kaustuv Kanti Ganguli",
            "Preeti Rao"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1414924",
        "url": "https://doi.org/10.5281/zenodo.1414924",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/151_Paper.pdf",
        "abstract": "Hindustani classical instrumental concerts follow an episodic development that, musicologically, is described via changes in the rhythmic structure. Uncovering this structure in a musically relevant form can provide for pow- erful visual representations of the concert audio that is of potential value in music appreciation and pedagogy. We in- vestigate the structural analysis of the metered section (gat) of concerts of two plucked string instruments, the sitar and sarod. A prominent aspect of the gat is the interplay be- tween the melody soloist and the accompanying drummer (tabla). The tempo as provided by the tabla together with the rhythmic density of the sitar/sarod plucks serve as the main dimensions that predict the transition between con- cert sections. We present methods to access the stream of tabla onsets separately from the sitar/sarod onsets, ad- dressing challenges that arise in the instrument separation. Further, the robust detection of tempo and the estimation of rhythmic density of sitar/sarod plucks are discussed. A case study of a fully annotated concert is presented, and is followed by results of achieved segmentation accuracy on a database of sitar and sarod gats across artists.",
        "zenodo_id": 1414924,
        "dblp_key": "conf/ismir/PSGR16",
        "content": "STRUCTURAL SEGMENTATION AND VISUALIZATION OF SITAR AND\nSAROD CONCERT AUDIO\nVinutha T.P. Suryanarayana Sankagiri Kaustuv Kanti Ganguli Preeti Rao\nDepartment of Electrical Engineering, IIT Bombay, India\nprao@ee.iitb.ac.in\nABSTRACT\nHindustani classical instrumental concerts follow an\nepisodic development that, musicologically, is described\nvia changes in the rhythmic structure. Uncovering this\nstructure in a musically relevant form can provide for pow-\nerful visual representations of the concert audio that is of\npotential value in music appreciation and pedagogy. We in-\nvestigate the structural analysis of the metered section (gat)\nof concerts of two plucked string instruments, the sitar and\nsarod. A prominent aspect of the gat is the interplay be-\ntween the melody soloist and the accompanying drummer\n(tabla). The tempo as provided by the tabla together with\nthe rhythmic density of the sitar/sarod plucks serve as the\nmain dimensions that predict the transition between con-\ncert sections. We present methods to access the stream\nof tabla onsets separately from the sitar/sarod onsets, ad-\ndressing challenges that arise in the instrument separation.\nFurther, the robust detection of tempo and the estimation\nof rhythmic density of sitar/sarod plucks are discussed. A\ncase study of a fully annotated concert is presented, and is\nfollowed by results of achieved segmentation accuracy on\na database of sitar and sarod gats across artists.\n1. INTRODUCTION\nThe repertoire of North Indian (Hindustani) classical music\nis characterized by a wide variety of solo instruments, play-\ning styles and melodic material in the form of ragas and\ncompositions. However, across all these, there is a striking\nuniversality in the concert structure, i.e., the way in which\nthe music is organized in time. The temporal evolution\nof a concert can be described via changes in the rhythm\nof the music, with homogenous sections having identical\nrhythmic characteristics. The metric tempo and the sur-\nface rhythm, two important aspects of rhythm, characterize\nthe individual sections. Obtaining these rhythm features as\nthey vary with time gives us a rich transcription for mu-\nsic appreciation and pedagogy. It also allows rhythm-base\nsegmentation with potential applications in concert sum-\nc/circlecopyrtVinutha T.P., Suryanarayana Sankagiri, Kaustuv Kanti\nGanguli, Preeti Rao. Licensed under a Creative Commons Attribution\n4.0 International License (CC BY 4.0). Attribution: Vinutha T.P.,\nSuryanarayana Sankagiri, Kaustuv Kanti Ganguli, Preeti Rao. “STRUC-\nTURAL SEGMENTATION AND VISUALIZATION OF SITAR AND\nSAROD CONCERT AUDIO”, 17th International Society for Music In-\nformation Retrieval Conference, 2016.marization, music navigation. This provides a strong mo-\ntivation for the rhythmic analysis of Hindustani classical\nconcert audio.\nRhythmic analyses of audio has been widely used for\nmusic classiﬁcation and tempo detection [1–3]. It has also\nbeen applied to music segmentation [4,5] although timbre-\nand harmony-based segmentation are more common. Re-\ncently, computational descriptions of rhythm were studied\nfor Indian and Turkish music [6]. Beat detection and cy-\ncle length annotation were identiﬁed as musically relevant\ntasks that could beneﬁt from the computational methods.\nIn this paper, we focus on the Hindustani classical in-\nstrumental concert which follows an established structure\nvia a speciﬁed sequence of sections, viz. alap-jod-jhala-\ngat[7]. The ﬁrst three are improvised sections where the\nmelody instrumentalist (sarod/sitar) plays solo, and are of-\nten together called the “ alap”. The gator composed sec-\ntion is marked by the entry of the tabla. The gatis further\nsubdivided into episodes as discussed later. The structure\noriginated in the ancient style of dhrupad singing where\naraga performance is subdivided unequally into the men-\ntioned temporally ordered sections.\nIn the present work, we consider concerts of two\nplucked string instruments, sitar and sarod, which are ma-\njor components of Indian instrumental music. The two\nmelodic instruments share common origins and represent\nthe fretted and unfretted plucked monochords respectively.\nVerma et. al. [8] have worked on the segmentation of the\nunmetered section ( alap) of such concerts into alap-jod-\njhala based purely on the tempo and its salience. They\nuse the fact that an increase in regularity and pluck density\nmarked the beginning of jod. Higher pluck density was\ncaptured via increases in the energy and in the estimated\ntempo. The transition to jhala was marked by a further rise\nin tempo and additionally distinguished by the presence of\nthe “ chikari ” strings.\nIn this paper, we focus on the rhythmic analysis and\nsegmentation of the gat, or the tabla-accompaniment re-\ngion, into its sections. Owing to differences in the rhyth-\nmic structure of the alap and the gat, the challenges in-\nvolved in this task are different from those addressed in\n[8]. In the gat, the tabla provides a deﬁnite meter to the\nconcert by playing a certain tala. The tempo, as set by\nthe tabla, is also called the metric tempo. The tempo of\nthe concert increases gradually with time, with occasional\njumps. While the tabla provides the basic beats ( theka ), the\nmelody instrumentalist plays the composition interspersed232with raga-based improvisation (“ vistaar ”). A prominent\naspect of instrumental concerts is that the gatis charac-\nterized by an interplay between the melody instrumentalist\nand the drummer, in which they alternate between the roles\nof soloist and timekeeper [7, 9]. The melody instrument\ncan switch to fast rhythmic play (“ layakari ”) over several\ncycles of the tabla. Then there are interludes where the\ntabla player is in the foreground (“tabla solo”), improvising\nat a fast rhythm, while the melody instrumentalist plays the\nrole of the timekeeper by playing the melodic refrain of the\ncomposition cyclically. Although both these sections have\nhigh surface rhythm, the term “rhythmic density” refers to\nthe stroke density of the sarod/sitar [10], and therefore is\nhigh only during the layakari sections. The values of the\nconcert tempo and the rhythmic density as they evolve in\ntime can thus provide an informative visual representation\nof the concert, as shown in [10].\nIn order to compute the rhythmic quantities of interest,\nwe follow the general strategy of obtaining an onset detec-\ntion function (ODF) and then computing the tempo from it\n[11]. To obtain the surface rhythm, we need an ODF sensi-\ntive to all onsets. However, to calculate the metric tempo,\nas well as to identify sections of high surface rhythm as\noriginating from the tabla or sarod/sitar, we must discrim-\ninate the tabla and sitar/sarod stroke onsets. Both the sitar\nand the sarod are melodic instruments but share the per-\ncussive nature of the tabla near the pluck onset. The tabla\nitself is characterized by a wide variety of strokes, some\nof which are diffused in time and have decaying harmonic\npartials. This makes the discrimination of onsets particu-\nlarly challenging.\nOur new contributions are the (i) proposal of a tabla-\nspeciﬁc onset detection method, (ii) computation of the\nmetric tempo and rhythmic density of the gatover a con-\ncert to obtain a rhythmic description which matches with\none provided by a musician, (iii) segmentation of the gat\ninto episodes based on the rhythm analysis. These meth-\nods are demonstrated on a case study of a sarod gatby a\nfamous artist, and are further tested for segmentation accu-\nracy on a manually labeled set of sitar and sarod gats.\nIn section 2, we present the proposed tabla-sensitive\nODF and test its effectiveness in selectively detecting tabla\nonsets from a dataset of labeled onsets drawn from a few\nsitar and sarod concerts. In section 3, we discuss the esti-\nmation of tempo and rhythmic density from the periodicity\nof the onset sequences and present the results on a manu-\nally annotated sarod gat. Finally, we present the results of\nsegmentation on a test set of sitar and sarod gats.\n2. ONSET DETECTION\nA computationally simple and effective method of on-\nset detection is the spectral ﬂux which involves the time\nderivative of the short-time energy [12]. The onsets of both\nthe percussive as well as the string instrument lead to a sud-\nden increase in energy, and are therefore detected well by\nthis method. A slight modiﬁcation involves using a bipha-\nsic ﬁlter to compute the derivative [13]. This enhances the\ndetection of sarod/sitar onsets, which have a slow decayin energy, and leads to a better ODF. Taking the logarithm\nof the energy before differencing enhances the sensitivity\nto weaker onsets. We hereafter refer to this ODF as the\nspectral ﬂux-ODF (SF-ODF), and is given by Eq. 1. ( h[n]\ndenotes the biphasic ﬁlter as in [13] )\nSF-ODF [n] =h[n]∗log(N/2/summationdisplay\nk=0|X[n, k]|) (1)\nFigure 1, which contains a sarod concert excerpt, illustrates\nthe fact that SF-ODF is sensitive to both sarod and tabla on-\nsets. In this example, and in all subsequent cases, we com-\npute the spectrum by using a 40ms Hamming window on\naudio sampled at 16 kHz. The spectrum (and therefore the\nODF) is computed at 5 ms intervals. Fig. 1(a) shows the\naudio waveform where onsets can be identiﬁed by peaks in\nthe waveform envelope. Onsets can also be seen as vertical\nstriations in the spectrogram (Fig. 1(b)). SF-ODF is shown\nin Fig. 1(c). Clearly, SF-ODF is not tabla-selective.\nIn order to obtain a tabla-sensitive ODF, we need to ex-\nploit some difference between tabla and sarod/sitar onsets.\nOne salient difference is that in the case of a tabla onset,\nthe energy decays very quickly ( <0.1 s). In contrast, the\nenergy of a sitar/sarod pluck decays at a much slower rate\n(>0.5 s). This difference is captured in the ODF that we\npropose, hereafter called as P-ODF. This ODF counts the\nnumber of bins in a spectral frame where the energy in-\ncreases from the previous frame, and is given by Eq. 2.\nThis method is similar in computation to the spectral ﬂux\nmethod in [12]; we take the 0-norm of the half-wave recti-\nﬁed energy differences, instead of the 2-norm [12] or 1-\nnorm [14]. However, the principle on which this ODF\noperates is different from the spectral ﬂux ODF. P-ODF\ndetects only those onsets that are characterised by a wide-\nband event, i.e., onsets that are pecussive in nature. Unlike\nthe spectral ﬂux ODF, it does not rely on the magnitude of\nenergy change. In our work, this proves to be an advantage\nas it detects weak onsets of any instrument better, provided\nthey are wide-band events.\nP-ODF [n] =N/2/summationdisplay\nk=01{|X[n, k]|>|X[n−1, k]|}(2)\nFrom Fig. 1(d), we see that P-ODF peaks at the onset of\na tabla stroke, as would be expected due to the wide-band\nnature of these onsets. It also peaks for sarod onsets, as\nthese onsets have a percussive character. Thus, it is sen-\nsitive to all onsets of interest, and can be potentially used\nas generic ODF in place of SF-ODF, for sitar/sarod audio.\nWhat is of more interest is the fact that in the region im-\nmediately following a tabla onset, this count falls rapidly\nwhile such a pattern is not observed for sarod onsets (see\nFig. 1(d)). This feature is seen because of the rapid de-\ncrease in energy after a tabla onset. In the absence of any\nactivity, the value of the ODF is equal to half the number\nof bins as the energy changes from frame to frame in a bin\ndue to small random perturbations.\nThe sharp downward lobe in P-ODF is a striking feature\nof tabla onsets, and can be used to obtain a tabla-sensitiveProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 233Figure 1 :(a)Audio waveform, (b)Spectrogram, (c)SF-\nODF, (d)P-ODF and (e)P-T-ODF of an excerpt of a sarod\nconcert. All ODFs normalised. Tabla onsets marked in\nblue solid lines; sarod onsets marked in red dashed lines\nODF. We normalize the mean-removed function to [-1,1]\nand consider only the negative peaks of magnitude that ex-\nceed the empirically chosen threshold of 0.3. We call our\nproposed tabla-sensitive ODF as P-T-ODF. An example is\nshown in Fig. 1(e).\nWe wish to establish that the P-T-ODF performs better\nas a tabla-sensitive ODF than other existing methods. The\nspectral ﬂux method is known to be sensitive to both on-\nsets, and performs poorly as a tabla-sensitive ODF. How-\never, one could hope to obtain better results by computing\nthe ODF on a percussion-enhanced audio. Fitzgerald [15]\nproposes a median-ﬁlter based method for percussion en-\nhancement that exploits the relatively high spectral vari-\nability of the melodic component of a music signal to sup-\npress it relative to the more repetitive percussion. We used\nthis method to preprocess our gataudio to obtain what we\ncall the enhanced audio signal (tabla is enhanced), and test\nthe SF-ODF on it. With this as the baseline, we compare\nour P-T-ODF applied to the original audio. In parallel, we\nwish to justify our claim that the P-ODF is a suitable ODF\nfor detecting sarod/sitar as well as tabla onsets.\nWe evaluate our ODFs on a dataset of 930 labeled on-\nsets comprising 158 sitar, 239 sarod and 533 tabla strokes\ndrawn from different sections of 6 different concert gats.\nOnsets were marked by two of the authors, by carefully\nlistening to the audio, and precisely locating the onset in-\nstant with the aid of the waveform and the spectrogram.\nWe evaluate P-ODF and SF-ODF, derived from the origi-\nnal audio, for detection of all onsets, with SF-ODF serving\nas a baseline. The obtained ROC is shown in Fig. 2(a).\nWe also evaluate P-T-ODF, derived from the original audio\nFigure 2 :(a)All-onsets ROC for SF-ODF (blue dia-\nmonds) and P-ODF (green circles); (b)Tabla-onsets ROC\nfor SF-ODF on enhanced audio (blue diamonds), and P-T-\nODF on original audio (green circles)\nand compare it with SF-ODF from enhanced audio, for de-\ntection of tabla onsets. The corresponding ROC is shown\nin Fig. 2(b).\nWe observe that the spectral ﬂux and the P-ODF per-\nform similarly in the all-onsets ROC of Fig. 2(a). A close\nexamination of performance on the sitar and sarod gats\nseparately revealed that the P-ODF performed marginally\nbetter than SF-ODF on sarod gats, while the performance\nof the spectral ﬂux ODF was better than the P-ODF on the\nsitar strokes. In the following sections, we use the P-ODF\nto detect all onsets in sarod gats and the spectral ﬂux-ODF\non the sitar gats. We also note from Fig. 2(b) that the P-\nT-ODF fares signiﬁcantly better than the SF-ODF applied\non tabla-enhanced signal. The ineffectiveness of Fitzger-\nald’s percussion enhancement is explained by the percus-\nsive nature of both instruments as well as the high variation\n(intended and unintended) of tabla strokes in performance.\nWe observed that the median ﬁltering did a good job of\nsuppressing the sarod/sitar harmonics in but not their on-\nsets. The P-T-ODF is established as an effective way to\ndetect tabla onsets exclusively in both sarod and sitar gats.\n3. RHYTHMOGRAMS AND TEMPO\nESTIMATION: A CASE STUDY\nA rhythm representation of a gatcan be obtained from the\nonset detection function by periodicity analysis via the au-\ntocorrelation function (ACF) or the DFT. A rhythmogram\nuses the ACF to represent the rhythmic structure as it varies\nin time [16]. Abrupt changes in the rhythmic structure can\nbe detected for concert section boundaries. The dominant\nperiodicity at any time can serve as an estimate of the per-\nceived tempo [5, 11]. Our goal is to meaningfully link the\noutcomes of such a computational analysis to the musico-\nlogical description of the concert.\nIn this section, we present the musicological and cor-\nresponding computational analyses of a commercially\nrecorded sarod gat(Raga Bahar, Madhyalaya, Jhaptal ) by\nlegendary sarodist Ustad Amjad Ali Khan. The musico-\nlogical description was prepared by a trained musician on\nlines similar to the sitar gatcase study by Clayton [17] and\nis presented next. The computational analysis involved ap-\nplying the onset detection methods to obtain a rhythm rep-234 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016resentation that facilitates the detection of the metric tempo\nand rhythmic density as well as the segmentation of the\ngat.\n3.1 Annotation by a Trained Musician\nA musician with over 15 years of training in Hindustani\nclassical music made a few passes listening to the audio\n(duration 14 min) to annotate the gatat three levels. The\nﬁrst was to segment and label the sequence of distinct\nepisodes as shown in Table 1. These labels reﬂect the per-\nformers’ (i.e. the sarod and tabla players) intentions as per-\nceived by a trained listener. The next two annotation lev-\nels involved marking the time-varying metric tempo and a\nmeasure of the sarod rhythmic density. The metric tempo\nwas measured by tapping to the tabla strokes that deﬁne\nthetheka (i.e. the 10 beats of the Jhaptal cycle) and com-\nputing the average BPM per cycle with the aid of the Sonic\nVisualizer interface [18]. The metric tempo is constant or\nslowly increasing across the concert with three observed\ninstants of abrupt change.\nThe rhythmic density, on the other hand, was obtained\nby tapping to the sarod strokes and similarly obtaining a\nBPM per cycle over the duration of the gat. Figure 3\nshows the obtained curves with the episode boundaries in\nthe background. We note that the section boundaries co-\nincide with abrupt changes in the rhythmic density. The\nmetric tempo is constant or slowly increasing across the\nconcert with three observed instants of abrupt change.\nThe rhythmic density corresponds to the sarod strokes and\nswitches between being once/twice the tempo in the vis-\ntaar to four times in the layakari (rhythmic improvisation\nby the melody soloist). Although the rhythmic density is\nhigh between cycles 20-40, this was due to fast melodic\nphrases occupying part of the rhythmic cycle during the\nvistaar improvisation. Since this is not a systematic change\nin the surface rhythm, it was not labeled layakari by our\nmusician. In the tabla solo section, although the surface\nrhythm increases, it is not due to the sarod. Therefore, the\ntabla solo section does not appear distinctive in the musi-\ncian’s markings in Figure 3.\nFigure 3 : Musicians annotation of tempo and rhythmic\ndensity attributes across the gat. Dashed lines indicate sec-\ntion boundariesSec. No. Cycles Time (s) Label\n1 1-61 0-301 Vistaar *\n2 62-73 302-356 Layakari\n3 74-77 357-374 Vistaar\n4 78-86 375-414 Layakari\n5 87-91 415-441 Vistaar\n6 92-103 442-490 Tabla solo\n7 104-120 491-568 Vistaar\n8 121-140 567-643 Layakari\n9 141-160 644-728 Vistaar #\n10 161-178 729-797 Layakari\n11 179-190 798-839 Vistaar\nTable 1 : Labeled sections for the sarod case study.\n*Tempo increases at 67s & 127s; # also at 657s\n3.2 Computational Analysis\n3.2.1 Rhythmogram\nThe onset detection methods of Section 2 are applied over\nthe duration of the concert. We conﬁne our study to two\nODFs based on insights obtained from the ROCs of Fig. 2.\nThese are the P-ODF for all onsets and the P-T-ODF for\ntabla-onsets. Although the P-ODF was marginally worse\nthan spectral ﬂux in Fig. 2(a), it was found to detect weak\nsarod strokes better while the false alarms were irregularly\ndistributed in time. This property is expected to help us\ntrack the sarod rhythmic density better.\nThe autocorrelation function of the ODFs is computed\nframe-wise, with a window length of 3 seconds and a hop\nof 0.5 seconds up to a lag of 1.5 seconds, and is normal-\nized to have a maximum value of 1 in each frame. To im-\nprove the representation of peaks across the dynamic range\nin the rhythmogram, we perform a non-linear scaling of\nthe amplitude of the ACF. For the tabla-centric rhythmo-\ngram (from P-T-ODF), we take the logarithm of the ACF\nbetween 0.1 and 1; for the generic rhythmogram (from P-\nODF), the logarithm is taken between 0.01 and 1 due to its\ninherently wider dynamic range for peaks. The ACF val-\nues below this range are capped to a minimum of -10. This\nis followed by smoothing in the lag and time axes by mov-\ning average ﬁlters to length 3 and 10 respectively bringing\nin short-time continuity.\nWe thus obtain the two rhythmograms shown in Figures\n4 and 5. We note that the P-ODF all-onsets rhythmogram\n(Figure 4) captures the homogenous rhythmic structure of\neach episode of vistaar ,layakari and tabla solo, showing\nabrupt changes at the boundaries. Each section itself ap-\npears homogenous except for some spottiness in the se-\nquence of low amplitude ACF peaks at submultiple lags\n(such as near 0.1s in the region until 300 s).\nThe tabla-centric rhythmogram (Figure 5), on the other\nhand, with its more prominent peaks appearing at lags near\n0.5s and multiples, is indicative of a metric (base) tempo of\naround 120 BPM. We clearly distinguish from this rhyth-\nmogram, the tabla solo segment (where the tabla surface\nrhythm shoots up to 8 times the metric tempo). We ob-\nserve, as expected, that the sarod layakari sections areProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 235Figure 4 : All-onsets rhythmogram from P-ODF\nFigure 5 : Tabla centric rhythmogram from P-T-ODF\ncompletely absent from the tabla-centric rhythmogram.\n3.2.2 Tempo and surface rhythm estimation\nThe rhythmograms provide interesting visual representa-\ntions of the rhythmic structure. However a visual repre-\nsentation that is more amenable to immediate interpreta-\ntion by musicians and listeners would have to parallel the\nmusician’s annotation of Fig. 3. We therefore must process\nthe rhythmograms further to extract the relevant attributes\nof metric tempo and sarod rhythmic density. We present\nnext the frame-wise estimation of these from the ACF vec-\ntors of the smoothened rhythmograms of Figs. 4 and 5.\nThe basic or metric tempo is obtained from the tabla\nrhythmogram (Fig. 5) by maximizing the mean of the\npeaks at candidate lags and corresponding lag multi-\nples over the lag range of 50ms to 750ms (1200BPM\nto 80BPM). The estimated time-varying metric tempo is\nshown in Fig. 6(a) superposed on the ground-truth annota-\ntion (x-axis converted to time from cycles as in Fig. 3).\nWe observe a near perfect match between the two with\nthe exception of the tabla-solo region, where the surface\nrhythm was tracked. We use our knowledge that the sur-\nface rhythm would be a multiple of the metric tempo. Di-\nviding each tempo value by that multiple that maintains\ncontinuity of the tempo gave us the detected contour of\nFig. 6(a).\nThe rhythmic density of the sarod is the second mu-\nsical attribute required to complete the visual representa-tion. This is estimated from the generic (P-ODF) rhyth-\nmogram of Fig. 4 in a manner similar to that used on\nthe table-centric version. The single difference is that we\napply a bias favouring lower lags in the maximum likeli-\nhood tempo estimation. A weighting factor proportional to\nthe inverse of the lag is applied. The biasing is motivated\nby our stated objective of uncovering the surface rhythmic\ndensity (equivalent to the smallest inter-onset interval).\nThe obtained rhythmic density estimates are shown in\nFig. 6(b), again in comparison with the ground truth\nmarked by the musician. The ground-truth markings have\nbeen converted to the time axis while smoothening lightly\nto remove the abrupt cycle-to-cycle variations in Fig. 3.\nWe note that the correct tempo corresponding to the sarod\nsurface rhythm is captured for the most part. The layakari\nsections are distinguished from the vistaar by the doubling\nof the rhythmic density. Obvious differences between the\nground-truth and estimated rhythmic density appear in (i)\nthe table solo region due to the high surface rhythm con-\ntributed by tabla strokes. Since P-ODF captures both the\ninstrument onsets, this is expected. Another step based on\nthe comparison of the two rhythmograms would easily en-\nable us to correct this; (ii) intermittent regions in the 0-300s\nregion of the gat. This is due to the low amplitude ACF\npeaks arising from the fast rhythmic phrases discussed in\nSec. 3.1.\nFigure 6 :(a)Estimated metric tempo with musician’s\nmarked tempo. (b)Estimated rhythmic density with musi-\ncians marked rhythmic density\n4. SEGMENTATION PERFORMANCE\nThe all-onsets rhythmogram provides a clear visual repre-\nsentation of abrupt rhythmic structure changes at the sec-\ntion boundaries speciﬁed by the ground-truth labels. In\norder to algorithmically detect the segment boundaries,\nwe resort to the method of the similarity distance matrix\n(SDM) where peaks in the novelty function derived from\ndiagonal kernel convolution can help identify instants of236 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016change [19]. We treat the ACF at each time frame as a\nfeature vector that contains the information of the local\nrhythmic structure. We compute the correlation distance\nbetween the ACF of every pair of frames across the con-\ncert to obtain the SDM. The diagonal of the SDM is then\nconvolved with a checker-board kernel of 25s ×25s to\ncompute the novelty function. Local maxima in the nov-\nelty function are suitably thresholded to locate instants of\nchange in the rhythmic structure. Figure 7 shows the SDM\nand novelty function computed on the rhythmogram of\nFigure 5 corresponding to the case study sarod gat. We\nobserve that all the known boundaries coincide with sharp\npeaks in the novelty function. The layakari -vistaar bound-\nary at 644s is subsumed by the sudden tempo change at\n657s due to the minimum time resolution imposed by the\nSDM kernel dimensions. We next present results for per-\nformance of our system on segment boundary detection\nacross a small dataset of sitar and sarod gats.\nFigure 7 : SDM and novelty curve for the case study sarod\ngat(whose rhythmogram appears in Figure 5). The blue\ndashed lines indicate ground-truth section boundaries as\nin Table 1. The red dashed lines indicate ground-truth in-\nstants of metric tempo jump.\n4.1 Dataset\nOur dataset for structural segmentation analysis consists of\nthree sitar and three sarod gats, by four renowned artists.\nWe have a total of 47 min of sarod audio (including the\ncase study gat) and 64 min of sitar audio. Just like the case-\nstudy gat, each gat has multiple sections which have been\nlabelled as vistaar ,layakari and tabla solo. Overall we\nhave 37 vistaar sections, 21 layakari sections and 25 tabla\nsolo sections. Boundaries have been manually marked by\nnoting rhythm changes upon listening to the audio. Mini-\nmum duration of any section is found to be 10s.Gat. Dur Method Hit False\nNo. (min) Used rate Alarms\n1 14 P-ODF 13/13 0\n2 24 P-ODF 14/14 1\n3 9 P-ODF 20/20 2\n4 16 SF-ODF 17/17 2\n5 21 SF-ODF 11/12 1\n6 27 SF-ODF 14/14 4\nTable 2 : Boundary detection results for 6 gats\n4.2 Boundary Detection Performance\nFor each concert, the novelty function was normalised to\n[0,1] range and peaks above a threshold of 0.3 were taken\nto indicate boundary instants. We consider the detected\nboundary as a hit if it lies within 12.5 s of a marked bound-\nary considering our kernel dimension of 25 s. We expect\nto detect instants where there is either a change in surface\nrhythm or an abrupt change in the metric tempo. Consis-\ntent with our onsets detection ROC study of Section 2, we\nobserved that the P-ODF method gave better segmentation\nresults than the spectral ﬂux for sarod gats, while the re-\nverse was true for sitar gats. Table 2 shows the correspond-\ning segmentation performance for the sarod (1-3) and sitar\n(4-6) gats. We observe a nearly 100% boundary detection\nrate with a few false detections in each concert. The false\nalarms were found to be triggered by instances of tabla im-\nprovisation (change in stroke pattern) without a change in\nthe metric tempo or basic theka .\n5. CONCLUSION\nMotivated by a compelling visual depiction of the rhythmic\nstructure of a Hindustani classical sitar concert [10], we set\nabout an effort to reproduce automatically, with MIR meth-\nods, the manual annotation created by expert musicians.\nA novel onset detection function that exploited the stroke\ncharacteristics of the melodic and percussive instrument,\nand additionally discriminated the two, proved effective in\nobtaining rhythm representations that separately captured\nthe structural contributions of the tabla and the sitar/sarod.\nTempo detection on the separate rhythm vectors provided\nestimates of the metric tempo and rhythmic density of the\nsitar/sarod. Segmentation using an SDM on the rhythm\nvectors provided section boundary estimates with high ac-\ncuracy. The system now needs to be tested on a large and\ndiverse database of sitar and sarod concerts. Further, given\nthat the rhythmogram contains more information than we\nhave exploited in the current work, we propose to develop\nmethods for section labeling and other relevant musical de-\nscriptors.\nAcknowledgement: This work received partial funding\nfrom the European Research Council under the Euro-\npean Union’s Seventh Framework Programme (FP7/2007-\n2013)/ERC grant agreement 267583 (CompMusic). Also,\npart of the work was supported by Bharti Centre for Com-\nmunication in IIT Bombay.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 2376. REFERENCES\n[1] Geoffroy Peeters. Rhythm Classiﬁcation Using Spec-\ntral Rhythm Patterns. In Proceedings of the Inter-\nnational Symposium on Music Information Retrieval ,\npages 644–647, 2005.\n[2] Fabien Gouyon, Simon Dixon, Elias Pampalk, and\nGerhard Widmer. Evaluating rhythmic descriptors for\nmusical genre classiﬁcation. In Proceedings of the AES\n25th International Conference , pages 196–204, 2004.\n[3] Klaus Seyerlehner, Gerhard Widmer, and Dominik\nSchnitzer. From Rhythm Patterns to Perceived Tempo.\nInProceedings of the International Symposium on Mu-\nsic Information Retrieval , pages 519–524, 2007.\n[4] Kristoffer Jensen, Jieping Xu, and Martin Zachariasen.\nRhythm-Based Segmentation of Popular Chinese Mu-\nsic. In Proceedings of the International Symposium on\nMusic Information Retrieval , pages 374–380, 2005.\n[5] Peter Grosche, Meinard M ¨uller, and Frank Kurth.\nCyclic tempogram-a mid-level tempo representation\nfor music signals. In IEEE International Conference on\nAcoustics Speech and Signal Processing , pages 5522–\n5525, 2010.\n[6] Ajay Srinivasamurthy, Andr ´e Holzapfel, and Xavier\nSerra. In search of automatic rhythm analysis methods\nfor Turkish and Indian art music. Journal of New Music\nResearch , 43(1):94–114, 2014.\n[7] Bonnie C Wade. Music in India: The classical tradi-\ntions , chapter 7: Performance Genres of Hindustani\nMusic. Manohar Publishers, 2001.\n[8] Prateek Verma, T. P. Vinutha, Parthe Pandit, and Preeti\nRao. Structural segmentation of Hindustani concert au-\ndio with posterior features. In IEEE International Con-\nference on Acoustics Speech and Signal Processing ,\npages 136–140, 2015.\n[9] Sandeep Bagchee. Nad: Understanding Raga Music .\nBusiness Publications Inc., India, 1998.\n[10] Martin Clayton. Time in Indian Music: Rhythm, Metre,\nand Form in North Indian Rag Performance , chapter\n11: A case study in rhythmic analysis. Oxford Univer-\nsity Press, UK, 2001.\n[11] Geoffroy Peeters. Template-based estimation of time-\nvarying tempo. EURASIP Journal on Applied Signal\nProcessing , 2007(1):158–171, 2007.\n[12] Juan Pablo Bello, Laurent Daudet, Samer Abdal-\nlah, Chris Duxbury, Mike Davies, and Mark B San-\ndler. A tutorial on onset detection in music signals.\nIEEE Transactions on Speech and Audio Processing ,\n13(5):1035–1047, 2005.\n[13] Dik J Hermes. V owel-onset detection. Journal of the\nAcoustical Society of America , 87(2):866–873, 1990.[14] Simon Dixon. Onset detection revisited. In Proceed-\nings of the 9th International Conference on Digital Au-\ndio Effects , volume 120, pages 133–137, 2006.\n[15] Derry FitzGerald. V ocal separation using nearest\nneighbours and median ﬁltering. In IET Irish Signals\nand Systems Conference (ISSC 2012) , pages 1–5, 2012.\n[16] Kristoffer Jensen. Multiple scale music segmentation\nusing rhythm, timbre, and harmony. EURASIP Jour-\nnal on Advances in Signal Processing , 2006(1):1–11,\n2006.\n[17] Martin Clayton. Two gat forms for the sit ¯ar: a case\nstudy in the rhythmic analysis of north indian music.\nBritish Journal of Ethnomusicology , 2(1):75–98, 1993.\n[18] Chris Cannam, Christian Landone, and Mark Sandler.\nSonic visualiser: An open source application for view-\ning, analysing, and annotating music audio ﬁles. In\nProceedings of the 18th ACM international conference\non Multimedia , pages 1467–1468, 2010.\n[19] Jonathan Foote. Automatic audio segmentation using a\nmeasure of audio novelty. In IEEE International Con-\nference on Multimedia and Expo , volume 1, pages\n452–455, 2000.238 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Learning a Feature Space for Similarity in World Music.",
        "author": [
            "Maria Panteli",
            "Emmanouil Benetos",
            "Simon Dixon"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1415216",
        "url": "https://doi.org/10.5281/zenodo.1415216",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/166_Paper.pdf",
        "abstract": "In this study we investigate computational methods for as- sessing music similarity in world music. We use state-of- the-art audio features to describe musical content in world music recordings. Our music collection is a subset of the Smithsonian Folkways Recordings with audio examples from 31 countries from around the world. Using super- vised and unsupervised dimensionality reduction techniques we learn feature representations for music similarity. We evaluate how well music styles separate in this learned space with a classification experiment. We obtained moderate performance classifying the recordings by country. Analy- sis of misclassifications revealed cases of geographical or cultural proximity. We further evaluate the learned space by detecting outliers, i.e. identifying recordings that stand out in the collection. We use a data mining technique based on Mahalanobis distances to detect outliers and perform a listening experiment in the ‘odd one out’ style to evaluate our findings. We are able to detect, amongst others, record- ings of non-musical content as outliers as well as music with distinct timbral and harmonic content. The listening experiment reveals moderate agreement between subjects’ ratings and our outlier estimation.",
        "zenodo_id": 1415216,
        "dblp_key": "conf/ismir/PanteliBD16",
        "content": "LEARNING A FEATURE SPACE FOR SIMILARITY IN WORLD MUSIC\nMaria Panteli, Emmanouil Benetos, Simon Dixon\nCentre for Digital Music, Queen Mary University of London, United Kingdom\n{m.panteli, emmanouil.benetos, s.e.dixon }@qmul.ac.uk\nABSTRACT\nIn this study we investigate computational methods for as-\nsessing music similarity in world music. We use state-of-\nthe-art audio features to describe musical content in world\nmusic recordings. Our music collection is a subset of the\nSmithsonian Folkways Recordings with audio examples\nfrom 31 countries from around the world. Using super-\nvised and unsupervised dimensionality reduction techniques\nwe learn feature representations for music similarity. We\nevaluate how well music styles separate in this learned space\nwith a classiﬁcation experiment. We obtained moderate\nperformance classifying the recordings by country. Analy-\nsis of misclassiﬁcations revealed cases of geographical or\ncultural proximity. We further evaluate the learned space\nby detecting outliers, i.e. identifying recordings that stand\nout in the collection. We use a data mining technique based\non Mahalanobis distances to detect outliers and perform a\nlistening experiment in the ‘odd one out’ style to evaluate\nour ﬁndings. We are able to detect, amongst others, record-\nings of non-musical content as outliers as well as music\nwith distinct timbral and harmonic content. The listening\nexperiment reveals moderate agreement between subjects’\nratings and our outlier estimation.\n1. INTRODUCTION\nThe analysis, systematic annotation and comparison of\nworld music styles has been of interest to many research\nstudies in the ﬁelds of ethnomusicology [5, 14, 20] and\nMusic Information Retrieval (MIR) [7, 12, 28]. The for-\nmer studies rely on manually annotating musical attributes\nof world music recordings and investigating similarity via\nseveral clustering techniques. The latter studies rely on au-\ntomatically extracting features to describe musical content\nof recordings and investigating music style similarity via\nclassiﬁcation methods. We focus on research studies that\nprovide a systematic way of annotating music; a method\nthat often disregards speciﬁc characteristics of a music cul-\nture but makes an across-culture comparison feasible. We\nare interested in the latter and follow a computational ap-\nproach to describe musical content of world music record-\nings and investigate similarity across music cultures.\nc/circlecopyrtMaria Panteli, Emmanouil Benetos, Simon Dixon. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Maria Panteli, Emmanouil Benetos, Simon\nDixon. “Learning a feature space for similarity in world music”, 17th\nInternational Society for Music Information Retrieval Conference, 2016.This study falls under the general scope of music cor-\npus analysis. While several studies have focused on pop-\nular (mainly Eurogenetic) music corpus analysis, for ex-\nample, the use of modes in American popular music [21],\npitch, loudness and timbre in contemporary Western popu-\nlar music [23], harmonic and timbral aspects in USA popu-\nlar music [16], only a few studies have considered world or\nfolk music genres, for example, the use of scales in African\nmusic [17]. Research projects have focused on the devel-\nopment of MIR tools for world music analysis1, but no\nstudy, to the best of our knowledge, has applied such com-\nputational methods to investigate similarity in a world mu-\nsic corpus.\nWhile the notion of world music is ambiguous, often\nmixing folk, popular, and classical musics from around the\nworld and from different eras [4], it has been used to study\nstylistic similarity between various music cultures. We fo-\ncus on a collection of folk recordings from countries from\naround the world, and use these to investigate music style\nsimilarity. Here we adopt the notion of music style by [19],\n‘style can be recognized by characteristic uses of form,\ntexture, harmony, melody, and rhythm’. Similarly, we de-\nscribe music recordings by features that capture aspects of\ntimbral, rhythmic, melodic, and harmonic content2.\nThe goal of this work is to infer similarity in collections\nof world music recordings. From low-level audio descrip-\ntors we are interested to learn high-level representations\nthat project data to a music similarity space. We compare\nthree feature learning methods and assess music similarity\nwith a classiﬁcation experiment and outlier detection. The\nformer evaluates recordings that are expected to cluster to-\ngether according to some ground truth label and helps us\nunderstand better the notion of ‘similarity’. The latter eval-\nuates examples that are different from the rest of the corpus\nand is useful to understand ‘dissimilarity’. Outlier detec-\ntion in large music collections can also be applied to ﬁlter\nout irrelevant audio or discover music with unique char-\nacteristics. We use an outlier detection method based on\nMahalanobis distances, a common technique for detecting\noutliers in multivariate data [1]. To evaluate our ﬁndings\nwe perform a listening test in the ‘odd one out’ framework\nwhere subjects are asked to listen to three audio excerpts\nand select the one that is most different [27].\nAmongst the main contributions of this paper is a set\n1Digital Music Lab ( http://dml.city.ac.uk ), CompMusic\n(http://compmusic.upf.edu/node/1 ), Telemata ( https://\nparisson.github.io/Telemeta/ )\n2The use of form is ignored in this study as our music collection is\nrestricted to 30-second audio excerpts.538of low-level features to represent musical content in world\nmusic recordings and a method to assess music style sim-\nilarity. Our results reveal similarity in music cultures with\ngeographical or cultural proximity and identify recordings\nwith possibly unique musical content. These ﬁndings can\nbe used in subsequent musicological analyses to track in-\nﬂuence and cultural exchange in world music. We per-\nformed a listening test with the purpose of collecting simi-\nlarity ratings to evaluate our outlier detection method. In a\nsimilar way, ratings can be collected for larger collections\nand used as a reference for ground truth similarity. The\ndata and code for extracting audio features, detecting out-\nliers and running classiﬁcation experiments as described in\nthis study are made publicly available3.\nThe paper is structured as follows. First a detailed de-\nscription of the low-level features used in this study is pre-\nsented in Section 2. Details of the size, type, and spatio-\ntemporal spread of our world music collection are pre-\nsented in Section 3. Section 4 presents the feature learning\nmethods with speciﬁcations of the models and Section 5\ndescribes the two evaluation methods, namely, classiﬁca-\ntion and outlier detection. In Section 5.2 we provide details\nof the listening test designed to assess the outlier detection\naccuracy. Results are presented in Section 6 and ﬁnally a\ndiscussion and concluding remarks are summarised in Sec-\ntion 7 and 8 respectively.\n2. FEATURES\nOver the years several toolboxes have been developed for\nmusic content description and have been applied for tasks\nof automatic classiﬁcation and retrieval [13, 18, 25]. For\ncontent description of world music styles, mainly tim-\nbral, rhythmic and tonal features have been used such\nas roughness, spectral centroid, pitch histograms, equal-\ntempered deviation, tempo and inter-onset interval distri-\nbutions [7,12,28]. We are interested in world music analy-\nsis and add to this list the requirement of melodic descrip-\ntors.\nWe focus on state-of-the-art descriptors (and adapta-\ntions of them) that aim at capturing relevant rhythmic,\nmelodic, harmonic, and timbral content. In particular, we\nextract onset patterns with the scale transform [10] for\nrhythm, pitch bihistograms [26] for melody, average chro-\nmagrams [3] for harmony, and Mel frequency cepstrum co-\nefﬁcients [2] for timbre content description. We choose\nthese descriptors because they deﬁne low-level representa-\ntions of the musical content, i.e. less abstract representa-\ntions but ones that are more likely to be robust with respect\nto the diversity of the music styles we consider. In addition,\nthese features have achieved state-of-the-art performances\nin relevant classiﬁcation or retrieval tasks, for example, on-\nset patterns with scale transform perform best in classify-\ning Western and non-Western rhythms [9, 15] and pitch\nbihistograms have been used successfully in cover song\n(pitch content-based) recognition [26]. The low-level de-\n3https://code.soundsoftware.ac.uk/projects/feature-space-world-\nmusicscriptors are later used to learn high-level representations\nusing various feature learning methods (Section 4).\nThe audio features used in this study are computed with\nthe following speciﬁcations. For all features we ﬁx the\nsampling rate at 44100 Hz and compute the (ﬁrst) frame\ndecomposition using a window size of 40ms and hop\nsize of 5ms. We use a second frame decomposition to\nsummarise descriptors over 8-second windows with 0.5-\nsecond hop size. This is particularly useful for rhythmic\nand melodic descriptors since rhythm and melody are per-\nceived over longer time frames. For consistency, the tim-\nbral and harmonic descriptors considered in this study are\nsummarised by their mean and standard deviation over this\nsecond frame decomposition.\nRhythm and Timbre. For rhythm and timbre features\nwe compute a Mel spectrogram with 40Mel bands up\nto8000 Hz using Librosa4. To describe rhythmic con-\ntent we extract onset strength envelopes for each Mel band\nand compute rhythmic periodicities using a second Fourier\ntransform with window size of 8seconds and hop size of\n0.5seconds. We then apply the Mellin transform to achieve\ntempo invariance [9] and output rhythmic periodicities up\nto960bpm. The output is averaged across low and high\nfrequency Mel bands with cutoff at 1758 Hz. Timbral as-\npects are characterised by 20Mel Frequency Cepstrum Co-\nefﬁcients (MFCCs) and 20ﬁrst-order delta coefﬁcients [2].\nWe take the mean and standard deviation of these coefﬁ-\ncients over 8-second windows with 0.5-second hop size.\nHarmony and Melody. To describe melodic and har-\nmonic content we compute chromagrams using variable-\nQtransforms [22] with 5ms hop size and 20-cent pitch\nresolution to allow for microtonality. Chromagrams are\naligned to the pitch class of maximum magnitude for key\ninvariance. Harmonic content is described by the mean and\nstandard deviation of chroma vectors using 8-second win-\ndows with 0.5-second hop size. Melodic aspects are cap-\ntured via pitch bihistograms which denote counts of transi-\ntions of pitch classes [26]. We use a window d= 0.5sec-\nonds to look for pitch class transitions in the chromagram.\nThe resulting pitch bihistogram matrix is decomposed us-\ning non-negative matrix factorization [24] and we keep 2\nbasis vectors with their corresponding activations to rep-\nresent melodic content. Pitch bihistograms are computed\nagain over 8-second windows with 0.5-second hop size.\n3. DATASET\nOur dataset is a subset of the Smithsonian Folkways\nRecordings, a collection of documents of “people’s mu-\nsic”, spoken word, instruction, and sounds from around\nthe world5. We use the publicly available 30-second audio\npreviews and from available metadata we choose the coun-\ntry of the recording as a proxy for music style. We choose a\nminimum number of N= 50 recordings for each country\nto capture adequate variability of its style-speciﬁc charac-\nteristics. For evaluation purposes we further require the\n4https://bmcfee.github.io/librosa/\n5http://www.folkways.si.eduProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 539dataset to have the same number of recordings per coun-\ntry. By manually sub-setting the data we observe that an\noptimal number of recordings is obtained for N= 70 , re-\nsulting in a total of 2170 recordings, 70recordings chosen\nat random from each of 31countries from North Amer-\nica, Europe, Asia, Africa and Australia. According to the\nmetadata these recordings belong to the genre ‘world’ and\nhave been recorded between 1949 and2009 .\n4. FEATURE LEARNING\nFor the low-level descriptors presented in Section 2 and\nthe music dataset in Section 3, we aim to learn feature\nrepresentations that best characterise music style similar-\nity. Feature learning is also appropriate for reducing di-\nmensionality, an essential step for the amount of data we\ncurrently analyse. In our analysis we approximate style\nby the country label of a recording and use this for super-\nvised training and cross-validating our methods. We learn\nfeature representations from the 8-second frame-based de-\nscriptors.\nThe audio features described in Section 2 are standard-\nised using z-scores and aggregated to a single feature vec-\ntor for each 8-second frame of a recording. A recording\nconsists of multiple 8-second frame feature vectors, each\nannotated with the country label of the recording. Fea-\nture representations are learned using Principal Compo-\nnent Analysis (PCA), Non-Negative Matrix Factorisation\n(NMF) and Linear Discriminant Analysis (LDA) meth-\nods [24]. PCA and NMF are unsupervised methods and\ntry to extract components that account for the most vari-\nance in the data. LDA is a supervised method and tries to\nidentify attributes that account for the most variance be-\ntween classes (in this case country labels).\nWe split the 2170 recordings of our collection into train-\ning ( 60%), validation ( 20%), and testing ( 20%) sets. We\ntrain and test our models on the frame-based descriptors;\nthis results in a dataset of 57282 ,19104 , and 19104 frames\nfor training, validation, and testing, respectively. Frames\nused for training do not belong to the same recordings as\nframes used for testing or validation and vice versa as this\nwould bias results. We use the training set to train the PCA,\nNMF, and LDA models and the validation set to optimise\nthe number of components. We investigate performance\naccuracy of the models when the number of components\nranges between 5and the maximum number of classes. We\nuse the testing set to evaluate the learned space by classiﬁ-\ncation and outlier detection tasks as explained below.\n5. EV ALUATION\n5.1 Objective Evaluation\nTo evaluate whether we have learned a meaningful feature\nspace we perform two experiments. One experiment aims\nat assessing similarity between recordings from the same\ncountry (which we expect to have related styles) via a clas-\nsiﬁcation task, i.e. validating recordings that lie close to\neach other in the learned feature space. The second experi-\nment aims at assessing dissimilarity between recordings bydetecting ‘outliers’, i.e. recordings that lie far apart in the\nlearned feature space.\nClassiﬁcation. For the classiﬁcation experiment we use\nthree classiﬁers: K-Nearest Neighbors (KNN) with K= 3\nand Euclidean distance metric, Linear Discriminant Anal-\nysis (LDA), and Support Vector Machines (SVM) with a\nRadial Basis Function kernel. We report results on the\naccuracy of the predicted frame labels and the predicted\nrecording labels. To predict the label of the recording we\nconsider the vote of its frame labels and select the most\npopular label.\nOutlier Detection. The second experiment uses a\nmethod based on squared Mahalanobis distances to detect\noutliers in multivariate data [1,8]. We use the best perform-\ning feature learning method, as indicated by the classiﬁca-\ntion experiment, to transform all frame-based features of\nour dataset. For each recording we calculare the average of\nits transformed feature vectors and use this to compute its\nMahalanobis distance from the set of all recordings. Using\nMahalanobis, an n-dimensional feature vector is expressed\nas the distance to the mean of the distribution in standard\ndeviation units. Data points that lie beyond a threshold,\nhere set to the 99.5%quantile of the chi-square distribution\nwithndegrees of freedom [6], are considered outliers.\n5.2 Subjective Evaluation\nTo evaluate the detected outliers we perform a listening\nexperiment in the ‘odd one out’ fashion [27]. A listener is\nasked to evaluate triads of audio excerpts by selecting the\none that is most different from the other two, in terms of its\nmusical characteristics. For the purpose of evaluating out-\nliers, a triad consists of one outlier excerpt and two inliers\nas estimated by their Mahalanobis distance from the set of\nall recordings.\nTo distinguish outliers from inliers (the most typical ex-\namples) and other excerpts which are neither outliers nor\ninliers, we set two thresholds for the Mahalanobis distance.\nDistances above the upper threshold identify outliers, and\ndistances below the lower threshold identify inliers. The\nthresholds are selected such that the majority of excerpts\nare neither outliers nor inliers. We randomly select 60out-\nliers and for each of these outliers we randomly select 10\ninliers, in order to construct 300triads ( 5triads for each of\n60outliers), which we split into 10sets of 30triads. Each\nparticipant rates one randomly selected set of 30triads.\nThe triads of outlier-inlier examples are presented in\nrandom order to the participant and we additionally in-\nclude 2control triads to assess the reliability of the partici-\npant. A control triad consists of two audio excerpts (the in-\nliers) extracted from the ﬁrst and second half, respectively,\nof the same recording and exhibiting very similar musi-\ncal attributes, and one excerpt (the outlier) from a different\nrecording exhibiting very different musical attributes. At\nthe end of the experiment we include a questionnaire for\ndemographic purposes.\nWe report results on the level of agreement between the\ncomputational outliers and the audio excerpts selected as\nthe odd ones by the participants of the experiment. We540 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Figure 1 . Classiﬁcation accuracy for different numbers of\ncomponents for PCA, NMF, and LDA methods (random\nbaseline is 0.03for31classes).\nfocus on two metrics; ﬁrst, we measure the average ac-\ncuracy between detected and rated outlier across all 300\ntriads used in the experiment, and second, we measure the\naverage accuracy for each outlier, i.e. for each of 60out-\nliers we compute the average accuracy of its corresponding\nrated triads. Further analysis such as how the music culture\nand music education of the participant inﬂuences the simi-\nlarity ratings is left for future work.\n6. RESULTS\nIn this section we present results from the feature learning\nmethods, their evaluation and the listening test as described\nin Sections 4 and 5.\n6.1 Number of Components\nFirst we present a comparison of classiﬁcation perfor-\nmance when the number of components for PCA, NMF\nand LDA methods ranges between 5and 30. For each\nnumber of components we train a PCA, NMF and LDA\ntransformer and report classiﬁcation accuracies on the val-\nidation set. The accuracies correspond to predictions of the\nlabel as estimated by a vote count of its predicted frame la-\nbels. We use the KNN classiﬁer with K= 3 neighbors\nand Euclidean distance metric. Results are shown in Fig-\nure 1. We observe that the best feature learning method is\nLDA and achieves its best performance when the number\nof components is 26. PCA and NMF achieve optimal re-\nsults when the number of components is 30and29respec-\ntively. We ﬁx the number of components to 30as this gave\ngood average classiﬁcation accuracies for all methods.\n6.2 Classiﬁcation\nUsing 30components we compute classiﬁcation accura-\ncies for the PCA, NMF and LDA transformed testing set.\nWe also compute classiﬁcation accuracies for the non-\ntransformed testing set. In Table 1 we report accuraciesClassiﬁer Transform. Frame Recording\nMethod Accuracy Accuracy\nKNN – 0.175 0.281\nPCA 0.177 0.279\nNMF 0.139 0.214\nLDA 0.258 0.406\nLDA – 0.300 0.401\nPCA 0.230 0.283\nNMF 0.032 0.032\nLDA 0.300 0.401\nSVM – 0.038 0.035\nPCA 0.046 0.044\nNMF 0.152 0.177\nLDA 0.277 0.350\nTable 1 . Classiﬁcation accuracies for the predicted frame\nlabels and the predicted recording labels based on a vote\ncount (– denotes no transformation).\nArmenia\nAustralia\nAustria\nBotswana\nCanada\nChina\nFrance\nGermany\nGhana\nGreece\nHungary\nIndia\nIndonesia\nItaly\nJapan\nKenya\nNigeria\nNorway\nPapua N Guinea\nPhilippines\nPoland\nPortugal\nRussia\nSouth Africa\nSpain\nSweden\nUganda\nUkraine\nUnit. Kingdom\nUnit. S America\nVietnamArmenia\nAustralia\nAustria\nBotswana\nCanada\nChina\nFrance\nGermany\nGhana\nGreece\nHungary\nIndia\nIndonesia\nItaly\nJapan\nKenya\nNigeria\nNorway\nPapua N Guinea\nPhilippines\nPoland\nPortugal\nRusshia\nSouth Africa\nSpain\nSweden\nUganda\nUkraine\nUnit. Kingdom\nUnit. S America\nVietnam\nFigure 2 . Confusion matrix for the best performing classi-\nﬁer, KNN with LDA transform (Table 1).\nfor the predicted frame labels and the predicted record-\ning labels as estimated from a vote count (Section 4). The\nKNN classiﬁer with the LDA transform method achieved\nthe highest accuracy, 0.406, for the predicted recording la-\nbels. For the predicted frame labels the LDA classiﬁer and\ntransform was best with an accuracy of 0.300. In subse-\nquent analysis we use the LDA transform as it was shown\nto achieve optimal results for our data.\nFor the highest classiﬁcation accuracy achieved with the\nKNN classiﬁer and the LDA transformation method (Ta-\nble 1), we compute the confusion matrix shown in Fig-\nure 2. From this we note that China is the most accurate\nclass and Russia and Philippines the least accurate classes.\nAnalysing the misclassiﬁcations we observe the following:\nVietnam is often confused with China and Japan, United\nStates of America is often confused with Austria, France\nand Germany, Russia is confused with Hungary, and South\nAfrica is confused with Botswana. These cases are char-\nacterised by a certain degree of geographical or cultural\nproximity which could explain the observed confusion.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 5410 500 1000 1500 2000\nRecording Number100\n0100200300400500Mahalanobis Distanceoutliers\nnon-outliersFigure 3 . Mahalanobis distances and outliers at the 99.5%\nquantile of chi-square distribution.\n6.3 Outlier Detection\nThe second experiment to evaluate the learned space aims\nat detecting outliers in the dataset. In this experiment we\nare not interested in how close music recordings of the\nsame country are to each other, but we are rather interested\nin recordings that are very different from the rest. We use\nthe LDA method as found optimal in the classiﬁcation ex-\nperiment (Section 6.2) to transform all frame-based feature\nvectors in our collection. Each recording is characterised\nby the average of its transformed frame-based descriptors.\nFrom our collection of 2170 recordings ( 70recordings\nfor each of 31countries), 557 recordings (around 26%)\nare detected as outliers at the chi-square 99.5%quantile\nthreshold. In Figure 3 we plot the Mahalanobis distances\nfor all samples in our dataset and indicate the ones that\nhave been identiﬁed as outliers. The three recordings with\nmaximum distances, i.e. standing out the most from the\ncorpus, are identiﬁed as follows (in order of high to low\nMahalanobis distance): 1) A recording of the dav dav in-\nstrument from the culture group ‘Khmu’ from Vietnam6,\n2) a rather non-musical example of bells from Greece7, 3)\nan example of the angklung instrument from Indonesia8.\nThese recordings can be characterised by distinct timbral\nand harmonic aspects or, in the case of the second example,\nby a distinct combination of all style attributes considered.\nWe plot the number of detected outliers per country on a\nworld map (Figure 4) to get an overview of the spatial dis-\ntribution of outliers in our music collection. We observe\nthat Germany was the only country without any outliers ( 0\noutliers out of 70recordings) and Uganda was the country\nwith the most outliers ( 39outliers out of 70recordings).\nOther countries with high number of outliers were Nigeria\n(34outliers out of 70recordings), Indonesia and Botswana\n(each with 31outliers out of 70recordings). We note that\nBotswana and Spain had achieved a relatively high classi-\nﬁcation accuracy in the previous evaluation (Section 6.2)\nand were also detected with a relatively high number of\n6http://s.si.edu/1RuJfuu\n7http://s.si.edu/21DgzP7\n8http://s.si.edu/22yz0qPoutliers ( 31and26outliers, respectively). This could indi-\ncate that recordings from these two countries are consistent\nin their music characteristics but also stand out in compar-\nison with other recordings of our world music collection.\n6.4 Listening Test\nThe listening test described in Section 5.2 aimed at evalu-\nating the outlier detection method. A total of 23subjects\nparticipated in the experiment. There were 15male and 8\nfemale participants and the majority ( 83%) aged between\n26and35years old. A small number of participants ( 5) re-\nported they are very familiar with world music genres and\na similar number ( 6) reported they are quite familiar. The\nremaining participants reported they are not so familiar ( 10\nof23) and not at all familiar ( 2) with world music genres.\nFollowing the speciﬁcations described in Section 5.2,\nparticipant’s reliability was assessed with two control tri-\nads and results showed that all participants rated both these\ntriads correctly. From the data collected, each of the 300\ntriads ( 5triads for each of 60detected outliers) was rated\na minimum of 1and maximum of 5times. Each of the\n60outliers was rated a minimum of 9and maximum of 14\ntimes with an average of 11.5.\nWe received a total of 690ratings ( 23participants rat-\ning30triads each). For each rating we assign an accuracy\nvalue of 1if the odd sample selected by the participant\nmatches the ‘outlier’ detected by our algorithm versus the\ntwo ‘inliers’ of the triad, and an accuracy of 0otherwise.\nThe average accuracy from 690ratings was 0.53. A sec-\nond measure aimed to evaluate the accuracy per outlier.\nFor this, the 690ratings were grouped per outlier, and an\naverage accuracy was estimated for each outlier. Results\nshowed that each outlier achieved an average accuracy of\n0.54with standard deviation of 0.25. One particular outlier\nwas never rated as the odd one by the participants (average\naccuracy of 0from a total of 14ratings). Conversely, four\noutliers were always in agreement with the subjects’ rat-\nings (average accuracy of 1for about 10ratings for each\noutlier). Overall, there was agreement well above the ran-\ndom baseline of 33% between the automatic outlier detec-\ntion and the odd one selections made by the participants.\n7. DISCUSSION\nSeveral steps in the overall methodology could be imple-\nmented differently and audio excerpts and features could\nbe expanded and improved. Here we discuss a few critical\nremarks and point directions for future improvement.\nNumerous audio features exist in the literature suitable\nto describe musical content in sound recordings depend-\ning on the application. Instead of starting with a large set\nof features and narrowing it down to the ones that give\nbest performance, we chose to start with a small set of\nfeatures selected upon their state-of-the-art performance\nand relevance and expand the set gradually in future work.\nThis way we can have more control of what the contri-\nbution is from each feature and each music dimension,\ntimbre, rhythm, melody or harmony, as considered in this542 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 201604568101113141516171820222325262729313439Figure 4 . Number of outliers for each of the 31countries in our world music collection (grey areas denote missing data).\nstudy. The choice of features and implementation parame-\nters could be improved, for example, in this study we have\nassumed descriptor summaries over 8-second windows but\nthe optimal window size could be investigated further.\nWe used feature learning methods to learn higher-level\nrepresentations from our low-level descriptors. We have\nonly tested three methods, namely PCA, NMF, LDA, and\ndid not exhaustively optimise parameters. Depending on\nthe data and application, more advanced methods could be\nemployed to learn meaningful feature representations [11].\nSimilarly, the classiﬁcation and outlier detection methods\ncould be tuned to give better accuracies.\nThe bigger aim of this work is to investigate similar-\nity in a large collection of world music recordings. Here\nwe have used a small dataset to assess similarity as esti-\nmated by classiﬁcation and outlier detection tasks. It is\ndifﬁcult to gather representative samples of ‘all’ music of\nthe world but at least a larger and better geographically\n(and temporally) spread dataset than the one used in this\nstudy could be considered. In addition, more metadata can\nbe incorporated to deﬁne ground truth similarity of music\nrecordings; in this study we have used country labels but\nother attributes more suitable to describe the music style\nor cultural proximity can be considered. An unweighted\ncombination of features was used to assess music similar-\nity. Performance accuracies can be improved by exploring\nfeature weights. What is more, analysing each feature sep-\narately can reveal which music attributes characterise most\neach country or which countries share aspects of rhythm,\ntimbre, melody or harmony.\nWhether a music example is selected as the odd one out\ndepends vastly on what it is compared with. Our outlier de-\ntection algorithm compares a single recording to all other\nrecordings in the collection ( 1versus 2169 samples) but\na human listener could not do this with similar efﬁciency.\nLikewise, we could only evaluate a limited set of 60out-\nliers from the total of 557outliers detected due to time lim-\nitations of our subjects. We evaluated comparisons from\nsets of three recordings and we used computational meth-\nods to create ‘easy’ triads, i.e. select three recordings fromwhich one is as different as possible compared to the other\ntwo. However in some cases, as also reported by some\nof the participants, the three recordings were very differ-\nent from each other which made it difﬁcult to select the\nodd one out. In future work this could be improved by\nrestricting the genre of the triad, i.e. selecting three audio\nexamples from the same music style or culture. In addition\nthe selection criteria could be made more speciﬁc; in our\nexperiment we let participants decide on ‘general’ music\nsimilarity but in some cases it is beneﬁcial to focus on, for\nexample, only rhythm or only melody.\n8. CONCLUSION\nIn this study we analysed a world music corpus by ex-\ntracting audio descriptors and assessing music similarity.\nWe used feature learning techniques to transform low-level\nfeature representations. We evaluated the learned space in\na classiﬁcation manner to check how well recordings of\nthe same country cluster together. In addition, we used\nthe learned space to detect outliers and identify recordings\nthat are different from the rest of the corpus. A listening\ntest was conducted to evaluate our ﬁndings and moderate\nagreement was found between computational and human\njudgement of odd samples in the collection.\nWe believe there is a lot for MIR research to learn from\nand to contribute to the analysis of world music recordings,\ndealing with challenges of the signal processing tools, data\nmining techniques, and ground truth annotation procedures\nfor large data collections. This line of research makes a\nlarge scale comparison of recorded music possible, a sig-\nniﬁcant contribution for ethnomusicology, and one we be-\nlieve will help us understand better the music cultures of\nthe world.\n9. ACKNOWLEDGEMENTS\nEB is supported by a RAEng Research Fellowship\n(RF/128). MP is supported by a Queen Mary Princi-\npal’s research studentship and the EPSRC-funded Platform\nGrant: Digital Music (EP/K009559/1).Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 54310. REFERENCES\n[1] C. C. Aggarwal and P. S. Yu. Outlier detection for high\ndimensional data. In International Conference on Man-\nagement of Data (ACM SIGMOD) , pages 37–46, 2001.\n[2] J. J. Aucouturier, F. Pachet, and M. Sandler. ”The way\nit sounds”: Timbre models for analysis and retrieval\nof music signals. IEEE Transactions on Multimedia ,\n7(6):1028–1035, 2005.\n[3] M. A. Bartsch and G.H. Wakeﬁeld. Audio thumbnail-\ning of popular music using chroma-based representa-\ntions. IEEE Transactions on Multimedia , 7(1):96–104,\n2005.\n[4] P. V . Bohlman. World Music: A Very Short Introduc-\ntion. Oxford University Press, 2002.\n[5] S. Brown and J. Jordania. Universals in the world’s mu-\nsics. Psychology of Music , 41(2):229–248, 2011.\n[6] P. Filzmoser. A Multivariate Outlier Detection Method.\nInInternational Conference on Computer Data Analy-\nsis and Modeling , pages 18–22, 2004.\n[7] E. G ´omez, M. Haro, and P. Herrera. Music and geog-\nraphy: Content description of musical audio from dif-\nferent parts of theworld. In Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence, pages 753–758, 2009.\n[8] V . Hodge and J. Austin. A Survey of Outlier De-\ntection Methodologies. Artiﬁcial Intelligence Review ,\n22(2):85–126, 2004.\n[9] A. Holzapfel, A. Flexer, and G. Widmer. Improv-\ning tempo-sensitive and tempo-robust descriptors for\nrhythmic similarity. In Proceedings of the Sound and\nMusic Computing Conference , pages 247–252, 2011.\n[10] A. Holzapfel and Y . Stylianou. Scale Transform in\nRhythmic Similarity of Music. IEEE Transactions on\nAudio, Speech, and Language Processing , 19(1):176–\n185, 2011.\n[11] E. J. Humphrey, A. P. Glennon, and J. P. Bello. Non-\nlinear semantic embedding for organizing large instru-\nment sample libraries. In Proceedings of the Interna-\ntional Conference on Machine Learning and Applica-\ntions , volume 2, pages 142–147, 2011.\n[12] A. Kruspe, H. Lukashevich, J. Abeßer, H. Großmann,\nand C. Dittmar. Automatic Classiﬁcation of Musical\nPieces Into Global Cultural Areas. In AES 42nd Inter-\nnational Conference , pages 1–10, 2011.\n[13] O. Lartillot and P. Toiviainen. A Matlab Toolbox for\nMusical Feature Extraction From Audio. In Interna-\ntional Conference on Digital Audio Effects , pages 237–\n244, 2007.\n[14] A. Lomax. Folk song style and culture . American As-\nsociation for the Advancement of Science, 1968.[15] U. Marchand and G. Peeters. The modulation scale\nspectrum and its application to rhythm-content descrip-\ntion. In International Conference on Diﬁtal Audio Ef-\nfects, pages 167–172, 2014.\n[16] M. Mauch, R. M. MacCallum, M. Levy, and A. M.\nLeroi. The evolution of popular music: USA 1960-\n2010. Royal Society Open Science , 2(5):150081, 2015.\n[17] D. Moelants, O. Cornelis, and M. Leman. Exploring\nAfrican Tone Scales. In Proccedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence, pages 489–494, 2009.\n[18] G. Peeters. A large set of audio features for\nsound description (similarity and classiﬁcation) in the\nCUIDADO project. Technical Report. IRCAM , 2004.\n[19] S. Sadie, J. Tyrrell, and M. Levy. The New Grove Dic-\ntionary of Music and Musicians . Oxford University\nPress, 2001.\n[20] P. E. Savage, S. Brown, E. Sakai, and T. E. Currie. Sta-\ntistical universals reveal the structures and functions of\nhuman music. Proceedings of the National Academy\nof Sciences of the United States of America (PNAS) ,\n112(29):8987–8992, 2015.\n[21] E. G. Schellenberg and C. von Scheve. Emotional cues\nin American popular music: Five decades of the Top\n40.Psychology of Aesthetics, Creativity, and the Arts ,\n6(3):196–203, 2012.\n[22] C. Sch ¨orkhuber, A. Klapuri, N. Holighaus, and\nM. D ¨orﬂer. A Matlab Toolbox for Efﬁcient Perfect\nReconstruction Time-Frequency Transforms with Log-\nFrequency Resolution. In AES 53rd Conference on Se-\nmantic Audio , pages 1–8, 2014.\n[23] J. Serr `a,´A. Corral, M. Bogu ˜n´a, M. Haro, and J. L. Ar-\ncos. Measuring the Evolution of Contemporary West-\nern Popular Music. Scientiﬁc Reports , 2, 2012.\n[24] L. Sun, S. Ji, and J. Ye. Multi-Label Dimensionality\nReduction . CRC Press, Taylor & Francis Group, 2013.\n[25] G. Tzanetakis and P. Cook. MARSYAS: a framework\nfor audio analysis. Organised Sound , 4(3):169–175,\n2000.\n[26] J. Van Balen, D. Bountouridis, F. Wiering, and\nR. Veltkamp. Cognition-inspired Descriptors for Scal-\nable Cover Song Retrieval. In Proceedings of the Inter-\nnational Society for Music Information Retrieval Con-\nference , pages 379–384, 2014.\n[27] D. Wolff and T. Weyde. Adapting Metrics for Music\nSimilarity Using Comparative Ratings. In Proceedings\nof the International Society for Music Information Re-\ntrieval Conference , pages 73–78, 2011.\n[28] F. Zhou, Q. Claire, and R. D. King. Predicting the Geo-\ngraphical Origin of Music. In IEEE International Con-\nference on Data Mining , pages 1115–1120, 2014.544 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "On the Evaluation of Rhythmic and Melodic Descriptors for Music Similarity.",
        "author": [
            "Maria Panteli",
            "Simon Dixon"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417555",
        "url": "https://doi.org/10.5281/zenodo.1417555",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/161_Paper.pdf",
        "abstract": "In exploratory studies of large music collections where of- ten no ground truth is available, it is essential to evaluate the suitability of the underlying methods prior to drawing any conclusions. In this study we focus on the evaluation of audio features that can be used for rhythmic and melodic content description and similarity estimation. We select a set of state-of-the-art rhythmic and melodic descriptors and assess their invariance with respect to transformations of timbre, recording quality, tempo and pitch. We create a dataset of synthesised audio and investigate which fea- tures are invariant to the aforementioned transformations and whether invariance is affected by characteristics of the music style and the monophonic or polyphonic character of the audio recording. From the descriptors tested, the scale transform performed best for rhythm classification and re- trieval and pitch bihistogram performed best for melody. The proposed evaluation strategy can inform decisions in the feature design process leading to significant improve- ment in the reliability of the features.",
        "zenodo_id": 1417555,
        "dblp_key": "conf/ismir/PanteliD16",
        "content": "ON THE EVALUATION OF RHYTHMIC AND MELODIC DESCRIPTORS\nFOR MUSIC SIMILARITY\nMaria Panteli, Simon Dixon\nCentre for Digital Music, Queen Mary University of London, United Kingdom\n{m.panteli, s.e.dixon }@qmul.ac.uk\nABSTRACT\nIn exploratory studies of large music collections where of-\nten no ground truth is available, it is essential to evaluate\nthe suitability of the underlying methods prior to drawing\nany conclusions. In this study we focus on the evaluation\nof audio features that can be used for rhythmic and melodic\ncontent description and similarity estimation. We select\na set of state-of-the-art rhythmic and melodic descriptors\nand assess their invariance with respect to transformations\nof timbre, recording quality, tempo and pitch. We create\na dataset of synthesised audio and investigate which fea-\ntures are invariant to the aforementioned transformations\nand whether invariance is affected by characteristics of the\nmusic style and the monophonic or polyphonic character of\nthe audio recording. From the descriptors tested, the scale\ntransform performed best for rhythm classiﬁcation and re-\ntrieval and pitch bihistogram performed best for melody.\nThe proposed evaluation strategy can inform decisions in\nthe feature design process leading to signiﬁcant improve-\nment in the reliability of the features.\n1. INTRODUCTION\nWith the signiﬁcant number of music information retrieval\ntechniques and large audio collections now available it is\npossible to explore general trends in musical style evo-\nlution [11, 16]. Such exploratory studies often have no\nground truth to compare to and therefore any conclusions\nare subject to the validity of the underlying tools. In music\ncontent-based systems this often translates to the ability of\nthe audio descriptors to correctly and sufﬁciently represent\nthe music-speciﬁc characteristics.\nIn this study we focus on the evaluation of audio fea-\ntures that can be used for rhythmic and melodic content\ndescription and similarity estimation. We are particularly\ninterested in audio features that can be used to describe\nworld music recordings. We propose an evaluation frame-\nwork which aims to simulate the challenges faced in the\nanalysis of recorded world music collections, such as pro-\ncessing noisy recordings or audio samples exhibiting a va-\nriety of music style characteristics. In particular, we deﬁne\nc/circlecopyrtMaria Panteli, Simon Dixon. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: Maria Panteli, Simon Dixon. “On the evaluation of rhythmic and\nmelodic descriptors for music similarity”, 17th International Society for\nMusic Information Retrieval Conference, 2016.transformations with respect to timbre, recording quality,\ntempo and key and assess the invariance of a set of state-\nof-the-art rhythmic and melodic descriptors.\nA number of studies have dealt with the evaluation of\naudio features and speciﬁcally of rhythmic and melodic\ndescriptors. Robustness is usually addressed in the design\nprocess where certain decisions ensure tempo or key in-\nvariance of the features. For example, rhythmic descriptors\nhave been designed to achieve partial [5,6] or complete [7]\ntempo invariance, and melodic descriptors exist which are\ntempo and/or key invariant [1,22,24]. Robustness to audio\nquality has been also addressed for MFCC and chroma fea-\ntures (describing timbre and harmony respectively) [21].\nThe relevance of the features is not guaranteed even if a\nclassiﬁcation task seems successful. For example, unbal-\nanced datasets can lead to high accuracies in genre classi-\nﬁcation tasks [18], or high rhythm classiﬁcation accuracies\ncan be achieved with (only) tempo information [4, 6] indi-\ncating that other audio features used had limited relevant\ncontribution to the task.\nTo be perceptually valid, and useful in real-world col-\nlections, the representations need to be invariant to subtle\nchanges in tempo, key (or reference pitch), recording qual-\nity and timbre. Additionally, to be usable in cross-cultural\nstudies, the features need to be agnostic to properties of\nparticular music cultures. For instance, pitch representa-\ntions should not depend on the 12-tone equal temperament\ntuning, and rhythm representations should not depend on\nspeciﬁc Western metric structures such as4\n4metre.\nWe examine a selection of audio features for rhythm\nand melody to assess their suitability for scientiﬁc studies\nof world music corpora subject to the above constraints.\nTo achieve this we test classiﬁcation and retrieval perfor-\nmance of multiple rhythm and melody features on a con-\ntrolled dataset, which allows us to systematically vary tim-\nbre, tempo, pitch and audio quality. The main contribu-\ntions of the paper are the controlled dataset, which we\nmake freely available, and the proposed evaluation strat-\negy to assess robustness and facilitate the feature design\nand selection process.\n2. FEATURES\nWe present details of three descriptors from each category\n(rhythm and melody), chosen from the literature based\non their performance on related classiﬁcation and retrieval\ntasks. In the paragraphs below we provide a short sum-468mary of these features and discuss further considerations\nof their design. Our implementations of the features follow\nthe speciﬁcations published in the corresponding research\npapers but are not necessarily exact replicas.\n2.1 Rhythm\nWe start our investigation with state-of-the-art rhythmic\ndescriptors that have been used in similarity tasks includ-\ning genre and rhythm classiﬁcation [5, 7, 12]. The rhyth-\nmic descriptors we use here share the general process-\ning pipeline of two consecutive frequency analyses [15].\nFirst, a spectrogram representation is calculated, usually\nwith frequencies on the mel scale. The ﬂuctuations in its\n“rows”, i.e. the frequency bands, are then analysed for their\nrhythmic frequency content over larger windows. This ba-\nsic process has multiple variations, which we explain be-\nlow.\nFor comparison purposes we ﬁx the sampling rate at\n44100 Hz for all features. Likewise, the spectrogram frame\nsize is 40 ms with a hop size of 5 ms. All frequency bins\nare mapped to the mel scale. The rhythmic periodicities\nare calculated on 8-second windows with a hop size of 0.5\nseconds. In the second step we compute the periodicities\nwithin each mel band then average across all bands, and ﬁ-\nnally summarise a recording by taking the mean across all\nframes.\nOnset Patterns (OP). The deﬁning characteristic of Onset\nPatterns is that the mel frequency magnitude spectrogram\nis post-processed by computing the ﬁrst-order difference\nin each frequency band and then subtracting the mean and\nhalf-wave rectifying the result. The resulting onset func-\ntion is then frequency-analysed using the discrete Fourier\ntransform [5, 6, 14]. We omit the post-processing step of\ntransforming the resulting linear ﬂuctuation frequencies to\nlog2-spaced frequencies. The second frame decomposition\nresults in an F×POmatrix with F= 40 mel bands and\nPO= 200 periodicities linearly spaced up to 20Hz.\nFluctuation Patterns (FP). Fluctuation patterns differ\nfrom onset patterns by using a log-magnitude mel spectro-\ngram, and by the additional application of psychoacous-\ntic models (e.g. loudness and ﬂuctuation resonance mod-\nels) to weight perceptually relevant periodicities [12]. We\nuse the MIRToolbox [8] implementation of ﬂuctuation pat-\nterns with the parameters speciﬁed at the beginning of Sec-\ntion 2.1. Here, we obtain an F×PFmatrix with F= 40\nmel bands and PF= 1025 periodicities of up to 10Hz.\nScale Transform (ST). The scale transform [7], is a spe-\ncial case of the Mellin transform, a scale-invariant transfor-\nmation of the signal. Here, the scale invariance property is\nexploited to provide tempo invariance. When ﬁrst intro-\nduced, the scale transform was applied to the autocorrela-\ntion of onset strength envelopes spanning the mel scale [7].\nOnset strength envelopes here differ from the onset func-\ntion implemented in OP by the steps of post-processing\nthe spectrogram. In our implementation we apply the scale\ntransform to the onset patterns deﬁned above.2.2 Melody\nMelodic descriptors selected for this study are based on\nintervals of adjacent pitches or 2-dimensional periodicities\nof the chromagram. We use a chromagram representation\nderived from an NMF-based approximate transcription.\nFor comparison purposes we ﬁx the following parame-\nters in the design of the features: sampling rate at 44100\nHz, variable-Q transform with 3ms hop size and pitch res-\nolution at 60bins per octave (to account for microtonality),\nsecondary frame decomposition (where appropriate) using\nan8-second window and 0.5-second hop size, and ﬁnally\naveraging the outcome across all frames in time.\nPitch Bihistogram (PB). The pitch bihistogram [22] de-\nscribes how often pairs of pitch classes occur within a win-\ndowdof time. It can be represented as an n-by-nmatrix\nPwherenis the number of pitch classes and element pij\ndenotes the count of co-occurrences of pitch classes iand\nj. In our implementation, the pitch content is wrapped to a\nsingle octave to form a chromagram with 60 discrete bins\nand the window length is set to d= 0.5seconds. The fea-\nture values are normalised to the range [0,1]. To approx-\nimate key invariance the bihistogram is circularly shifted\ntopi−ˆi,j−ˆiwherepˆiˆjdenotes the bin of maximum mag-\nnitude. This does not strictly represent tonal structure but\nrather relative prominence of the pitch bigrams.\n2D Fourier Transform Magnitudes (FTM). The magni-\ntudes of the 2-dimensional Fourier transform of the chro-\nmagram describe periodicities in both frequency and time\naxes. This feature renders the chromagram key-invariant,\nbut still carries pitch content information, and has accord-\ningly been used in cover song recognition [1,9]. In our im-\nplementation, chromagrams are computed with 60bins per\noctave and no beat-synchronisation. The FTM is applied\nwith the frame decomposition parameters stated above. We\nselect only the ﬁrst 50frequency bins which correspond to\nperiodicities up to 16Hz.\nIntervalgram (IG). The intervalgram [24] is a represen-\ntation of chroma vectors averaged over different windows\nin time and cross-correlated with a local reference chroma\nvector. In the implementation we use,we reduce this to one\nwindow size d= 0.5, and cross-correlation is computed\non every pair of chroma vectors from successive windows.\nIn this study we place the emphasis on the evaluation\nframework and provide a baseline performance of (only) a\nsmall set of features. The study could be extended to in-\nclude more audio descriptors and performance accuracies\ncould be compared in order to choose the best descriptor\nfor a given application.\n3. DATA\nFor our experiments we compiled a dataset of synthesised\naudio, which allowed us to control transformations under\nwhich ideal rhythmic and melodic descriptors should be\ninvariant. In the sections below we present the dataset of\nselected rhythms and melodies and detailed description of\ntheir transformations.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 469Melody Rhythm\nDescription No. Description No.\nDutch Folk (M) 5 Afro-American (M) 5\nClassical (M) 5 North-Indian (M) 5\nByzantine (M) 5 African (M) 5\nPop (M) 5 Classical (M) 5\nClassical (P) 5 EDM (P) 5\nPop (P) 5 Latin-Brazilian (P) 5\nTable 1 : The dataset of rhythms and melodies transformed\nfor feature robustness evaluation. (M) is monophonic and\n(P) polyphonic as described in Section 3.1.\n3.1 Material\nWe compiled 30melodies and 30rhythms extracted from\na variety of musical styles with both monophonic and\npolyphonic structure (Table 1). In particular, we collect\nMIDI monophonic melodies of classical music used in\nthe MIREX 2013: Discovery of Repeated Themes and\nSections task1, MIDI monophonic melodies of Dutch\nfolk music from the Meertens Tune Collections [23], fun-\ndamental frequency (F0) estimates of monophonic pop\nmelodies from the MedleyDB dataset [2], fundamental\nfrequency (F0) estimates of monophonic Byzantine reli-\ngious music [13], MIDI polyphonic melodies of classical\nmusic from the MIREX 2013 dataset, and fundamental\nfrequency (F0) estimates of polyphonic pop music from\nthe MedleyDB dataset. These styles exhibit differences\nin the melodic pitch range, for example, classical pieces\nspan multiple octaves whereas Dutch folk and Byzantine\nmelodies are usually limited to a single octave range. Pitch\nfrom fundamental frequency estimates allows us to also\ntake into account vibrato and microtonal intervals. This\nis essential for microtonal tuning systems such as Byzan-\ntine religious music, and for melodies with ornamentation\nsuch as recordings of the singing voice in the Dutch folk\nand pop music collections.\nFor rhythm we collect rhythmic sequences common\nin Western classical music traditions [17], African mu-\nsic traditions [20], North-Indian and Afro-American tradi-\ntions [19], Electronic Dance Music (EDM) [3], and Latin-\nBrazilian traditions.2These rhythms span different me-\ntres such as118in North-Indian,128in African,44in EDM,\nand68in Latin-Brazilian styles. The rhythms for Western,\nAfrican, North-Indian, Afro-American traditions are con-\nstructed from single rhythmic patterns whereas EDM and\nLatin-Brazilian rhythms are constructed with multiple pat-\nterns overlapping in time. We refer to the use of a single\npattern as ‘monophonic’ and of multiple patterns as ‘poly-\nphonic’ for consistency with the melodic dataset.\n1http://www.tomcollinsresearch.net/\nmirex-pattern-discovery-task.html\n2http://www.formedia.ca/rhythms/5drumset.html3.2 Transformations\nIntuitively, melodies and rhythms retain their character\neven if the music is transposed to a different tonality,\nplayed at a (slightly) different tempo or under different\nrecording conditions. These are variations that we expect\nto ﬁnd in real-world corpora, and to which audio features\nshould be reasonably invariant. Indeed, the cover song\nidentiﬁcation literature suggests that invariance of features\nin terms of key transpositions and tempo shifts is desir-\nable [1, 22, 24]; for rhythm description, the existing liter-\nature mainly focuses on tempo invariance and robustness\nagainst recording quality [5, 6]. We add to this list the re-\nquirement of invariance to slight changes in timbre for both\nmelody and rhythm description3. Overall, we test our fea-\ntures for robustness in tempo, pitch, timbre and recording\nquality by systematically varying these parameters to pro-\nduce multiple versions of each melody and rhythm (Ta-\nble 2). We apply only one transformation at a time while\nkeeping the other factors constant. The ‘default’ version\nof a rhythm or melody is computed using one of the 25\ntimbres available, ﬁxing the tempo at 120 bpm, and, for\nmelody, keeping the original key as expressed in the MIDI\nor F0 values. The dataset is made available online4.\nTimbre (Timb) : For a given sequence of MIDI notes or\nfundamental frequency estimates we synthesise audio us-\ning sine waves with time-varying parameters. The synthe-\nsised timbres vary from harmonic to inharmonic sounds\nand from low to high frequency range. For a given set of\nrhythm sequences we synthesise audio using samples of\ndifferent (mainly percussive) instruments5. Beyond the\ntypical drum set sounds (kick, snare, hi-hat), we include\npercussive instruments of different music traditions such\nas the Indian mridangam, the Arabic daf, the Turkish dar-\nbuka, and the Brazilian pandeiro. Overall, we use 25dif-\nferent timbres for each melody and rhythm in the dataset.\nRecording Quality (RecQ) : Large music archives usually\ncontain material recorded under a variety of recording con-\nditions, and are preserved to different degrees of ﬁdelity.\nWe use the Audio Degradation Toolbox [10] to create 25\naudio degradations that we expect to be representative of\nwhat is found in such archives. Amongst the degradations\nwe consider are effects of prominent reverb (live record-\nings), overlaid random noise (old equipment), added ran-\ndom sounds including speech, birds, cars (ﬁeld recording),\nstrong compression (MP3), wow sampling, high or low\npass ﬁltering (vinyl or low quality microphone).\nGlobal tempo shifts (GTemp) : We deﬁne ‘small’ vari-\nations the tempo changes of up to 20% of the original\ntempo (in this case centred at 120bpm), which we assume\nwill leave the character of melodies and rhythms intact. In\nparticular, we use 25tempo shifts distributed in the range\n[−20,20](excluding 0) percent slower or faster than the\noriginal speed.\n3The timbre transformations we consider are not expected to vastly\nalter the perception of a rhythm or melody.\n4https://code.soundsoftware.ac.uk/projects/rhythm-melody-feature-\nevaluation\n5http://www.freesound.org470 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Transformations Values\nTimbre 25distinct timbres (similar frequency\nrange and instrument)\nRec. Quality 25degradations including reverb, com-\npression, wow, speech, noise\nGlobal Tempo 25values in [−20,20]percent deviation\nfrom original tempo\nKey Transp. 25values in [−10,10]semitones devia-\ntion from original key\nLocal Tempo 25values in [−20,20]percent deviation\nfrom original tempo\nTable 2 : Transformations for assessing feature invariance.\nKey transpositions/Local tempo shifts (KeyT/LTemp) :\nFor melodic descriptor robustness we consider transposing\nthe audio with respect to 25key transpositions in the range\n[−10,10](excluding 0) semitones from the original key.\nThese shifts include microtonal intervals e.g. a transposi-\ntion of 1.5semitones up as one expects to ﬁnd in world\nmusic singing examples. For rhythmic descriptor robust-\nness we consider instead small step changes of the tempo.\nWe introduce a local tempo change for a duration of 2(out\nof8) seconds centred around the middle of the recording.\nThis is common in, for example, performances of amateur\nmusicians where they might unintentionally speed up or\nslow down the music. Similar to global tempo transforma-\ntion we use 25shifts in the range [−20,20]percent slower\nor faster than the original speed.\nWhile the above transformations do not deﬁne an ex-\nhaustive list of effects and variations found in world music\ncorpora they provide a starting point for assessing feature\nrobustness. The dataset can be expanded in future work to\ninclude more transformations and parameter values. For\nthis study we restrict to the abovementioned 4transforma-\ntions with 25values each (Table 2). For our dataset of\n30rhythms and 30melodies this results in a total of 3000\ntransformed rhythms and 3000 transformed melodies.\n4. EVALUATION STRATEGY\nWith the proposed evaluation strategy we would like to\nassess feature robustness with respect to the transforma-\ntions and transformation values presented above in Sec-\ntion 3.2. Additionally we would like to check whether\nthe performance of the features relates to particularities\nof the music style for the styles presented in Section 3.1.\nLastly, since our dataset consists of monophonic and poly-\nphonic melodies and rhythms, we would also like to check\nwhether the features are inﬂuenced by the monophonic or\npolyphonic character of the audio signal.\nRobustness evaluation is performed on the dataset of\n3000 transformed rhythms and 3000 transformed melodies\n(Section 3.1). Considering the variety of MIR tasks and\ncorresponding MIR models, we choose to assess feature\nperformance accuracy in both classiﬁcation and retrieval\nexperiments as explained below. In our experiments we in-\nclude a variety of classiﬁers and distance metrics to cover\na wide range of audio feature similarity methods.We ﬁrst verify the power of the features to classify dif-\nferent melodies and rhythms. To do so we employ four\nclassiﬁers: K-Nearest Neighbors (KNN) with 1 neighbor\nand Euclidean distance metric, Support Vector Machine\n(SVM) with a linear kernel, Linear Discriminant Analysis\n(LDA) with 20 components, and Gaussian Naive Bayes.\nWe use 5-fold cross-validation for all classiﬁcation exper-\niments. In each case the prediction target is one of the 30\nrhythm or melody ‘families’. For each of the 3000 trans-\nformed rhythms or melodies we output the classiﬁcation\naccuracy as a binary value, 1 if the rhythm or melody was\nclassiﬁed correctly and 0 otherwise.\nAs reassuring as good classiﬁcation performance is, it\ndoes not imply that a melody or rhythm and its transfor-\nmations cluster closely in the original feature space. Ac-\ncordingly, we choose to use a similarity-based retrieval\nparadigm that more directly reﬂects the feature represen-\ntations. For each of the 30 rhythms or melodies we choose\none of the 25 timbres as the default version of the rhythm\nor melody, which we use as the query. We rank the 2999\ncandidates based on their distance to the query and assess\nthe recall rate of its 99 transformations. Each transformed\nrhythm or melody is assigned a score of 1 if it was retrieved\nin the topK= 99 results of its corresponding query and\n0 otherwise. We compare four distance metrics, namely\nEuclidean, cosine, correlation and Mahalanobis.\nFor an overview of the performance of the features we\ncompute the mean accuracy across all recordings for each\nclassiﬁcation or retrieval experiment and each feature. To\nbetter understand why a descriptor is successfull or not in\nthe corresponding classiﬁcation or retrieval task we further\nanalyse the performance accuracies with respect to the dif-\nferent transformations, transformation values, music style\nand monophonic versus polyphonic character. To achieve\nthis we group recordings by, for example, transformation,\nand compute the mean accuracy for each transformation.\nWe discuss results in the section below.\n5. RESULTS\nThe mean performance accuracy of each feature and each\nclassiﬁcation or retrieval experiment is shown in Table 3.\nOverall, the features with the highest mean classiﬁcation\nand retrieval accuracies are the scale transform (ST) for\nrhythm and the pitch bihistogram (PB) for melody.\n5.1 Transformation\nWe consider four transformations for rhythm and four for\nmelody. We compute the mean accuracy per transforma-\ntion by averaging accuracies of recordings from the same\ntransformation. Results for rhythm are shown in Table 4\nand for melody in Table 5. Due to space limitations we\npresent results for only the best, on average, classiﬁer\n(KNN) and similarity metric (Mahalanobis) as obtained in\nTable 3. We observe that onset patterns and ﬂuctuation pat-\nterns show, on average, lower accuracies for transforma-\ntions based on global tempo deviations. This is expected\nas the aforementioned descriptors are not tempo invariant.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 471Rhythm Melody\nMetric ST OP FP PB IG FTM\nClassiﬁcation\nKNN 0.86 0.71 0.68 0.88 0.83 0.86\nLDA 0.82 0.66 0.59 0.83 0.82 0.82\nNB 0.80 0.62 0.58 0.84 0.76 0.81\nSVM 0.87 0.66 0.59 0.86 0.86 0.87\nRetrieval\nEuclidean 0.65 0.47 0.42 0.80 0.56 0.67\nCosine 0.66 0.47 0.42 0.80 0.55 0.68\nCorrelation 0.66 0.47 0.42 0.80 0.54 0.67\nMahalanobis 0.61 0.48 0.40 0.81 0.60 0.72\nTable 3 : Mean accuracy of the rhythmic and melodic de-\nscriptors for the classiﬁcation and retrieval experiments.\nMetric Feature Timb GTemp RecQ LTemp\nClassiﬁcation\nKNN ST 0.98 0.90 0.93 0.62\nKNN OP 0.97 0.20 0.92 0.75\nKNN FP 0.91 0.18 0.92 0.71\nRetrieval\nMahalan. ST 0.95 0.36 0.91 0.25\nMahalan. OP 0.94 0.00 0.88 0.13\nMahalan. FP 0.62 0.01 0.87 0.09\nTable 4 : Mean accuracies of the rhythmic descriptors un-\nder four transformations (Section 3.1).\nIn the rhythm classiﬁcation task, the performance of the\nscale transform is highest for global tempo deviations but\nit is lowest for local tempo deviations. We believe this is\ndue to the scale transform assumption of a constant peri-\nodicity over the 8-second frame, an assumption that is vio-\nlated when local tempo deviations are introduced. We also\nnote that ﬂuctuation patterns show lower performance ac-\ncuracies for transformations of the timbre compared to the\nonset patterns and scale transform descriptors.\n5.2 Transformation Value\nWe also investigate whether speciﬁc transformation values\naffect the performance of the rhythmic and melodic de-\nscriptors. To analyse this we compute mean classiﬁcation\naccuracies averaged across recordings of the same trans-\nformation value (there are 25values for each of 4trans-\nformations so 100mean accuracies in total). Due to space\nlimitations we omit the table of results and report only a\nsummary of our observations.\nOnset patterns and ﬂuctuation patterns exhibit low clas-\nsiﬁcation accuracies for almost all global tempo devia-\ntions whereas scale transform only shows a slight perfor-\nmance degradation on global tempo deviations of around\n±20%. For local tempo deviations, scale transform\nperforms poorly at large local deviations (magnitude >\n15%) whereas onset patterns and ﬂuctuation patterns show\nhigher accuracies for these particular parameters. All de-\nscriptors seem to be robust to degradations of the recordingMetric Feature Timb GTemp RecQ KeyT\nClassiﬁcation\nKNN PB 0.97 0.99 0.78 0.76\nKNN IG 0.95 0.99 0.62 0.77\nKNN FTM 0.98 0.96 0.71 0.79\nRetrieval\nMahalan. PB 0.94 0.98 0.78 0.53\nMahalan. IG 0.70 0.91 0.33 0.46\nMahalan. FTM 0.87 0.88 0.57 0.57\nTable 5 : Mean accuracies of the melodic descriptors under\nfour transformations (Section 3.1).\nquality with the exception of a wow effect that causes all\nrhythmic descriptors to perform poorly. Onset patterns and\nﬂuctuation patterns perform poorly also in the degradation\nof a radio-broadcast compression.\nFor melody classiﬁcation, all features perform poorly\non key transpositions of more than 6semitones up amd a\nwow effect degradation. Pitch bihistogram also performs\npoorly in transpositions between 2.5−5semitones down.\nIntervalgram and Fourier transform magnitudes perform\nbadly also for reverb effect degradations and noisy record-\nings with overlaid wind, applause, or speech sound effects.\n5.3 Music Style\nOur dataset consists of rhythms and melodies from differ-\nent music styles and we would like to test whether the ro-\nbustness of the features is affected by the style. To achieve\nthis we average classiﬁcation accuracies across recordings\nof the same style. We have 6styles for rhythm with 500\nrecordings in each style and likewise for melody. This\ngives us 6mean accuracies for each feature and each clas-\nsiﬁcation experiment. We summarise results in a boxplot\nas shown in Figure 1. We also perform two sets of multiple\npaired t-tests with Bonferroni correction, one for rhythmic\nand one for melodic descriptors, to test whether mean clas-\nsiﬁcation accuracies per style are signiﬁcantly different.\nUsing the paired t-tests with multiple comparison cor-\nrection we observe that the majority of pairs of styles are\nsigniﬁcantly different at the Bonferroni signiﬁcance level\nalpha = 0.003 for both the rhythmic and melodic de-\nscriptors. In particular the accuracies for classiﬁcation\nand retrieval of African rhythms are signiﬁcantly different\nfrom all other styles. Western classical rhythms are sig-\nniﬁcantly different from all other styles except the EDM\nrhythms, and North-Indian rhythms are signiﬁcantly dif-\nferent from all other styles except the EDM and Latin-\nBrazilian rhythms. For melody, the accuracies for the\nByzantine and polyphonic pop styles are signiﬁcantly dif-\nferent from all other styles. The descriptors that perform\nparticularly badly with respect to these styles are the ﬂuctu-\nation patterns for rhythm and the intervalgram for melody.\nWe use our current results as an indication of which styles\nmight possibly affect the performance of the features but\nleave the analysis of the intra-style similarity for future\nwork.472 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Afro-AmericanNorth-IndianAfricanClassicalEDM\nLatin-Brazilian\nMusic Style0.00.20.40.60.81.0Classification Accuracy(a) Rhythm\nDutch FolkClassical (M)ByzantinePop (M)\nClassical (P)Pop (P)\nMusic Style0.00.20.40.60.81.0Classification Accuracy\n(b) Melody\nFigure 1 : Box plot of classiﬁcation accuracies of the rhyth-\nmic (top) and melodic (bottom) descriptors for each style.\n5.4 Monophonic versus Polyphonic\nOur dataset consists of monophonic and polyphonic\nmelodies and rhythms and we would like to test whether\nthe performance of the features is affected by the mono-\nphonic or polyphonic character. Similar to the preced-\ning analysis, we average performance accuracies across all\nmonophonic recordings and across all polyphonic record-\nings. We perform two paired t-tests, one for rhythmic and\none for melodic descriptors, to test whether mean classi-\nﬁcation accuracies of monophonic recordings are drawn\nfrom a distribution with the same mean as the polyphonic\nrecordings distribution. At the α= 0.05signiﬁcance level\nthe null hypothesis is not rejected for rhythm, p= 0.25, but\nis rejected for melody, p < 0.001. The melodic descrip-\ntors achieve on average higher classiﬁcation accuracies for\npolyphonic ( M= 0.88,SD= 0.02) than monophonic\nrecordings (M= 0.82,SD= 0.04).\n6. DISCUSSION\nWe have analysed the performance accuracy of the fea-\ntures under different transformations, transformation val-\nues, music styles, and monophonic versus polyphonic\nstructure. Scale transform achieved the highest accuracyfor rhythm classiﬁcation and retrieval, and pitch bihis-\ntogram for melody. The scale transform is less invariant\nto transformations of the local tempo, and the pitch bihis-\ntogram to transformations of the key. We observed that the\ndescriptors are not invariant to music style characteristics\nand that the performance of melodic descriptors depends\non the pitch content being monophonic or polyphonic.\nWe have performed this evaluation on a dataset of syn-\nthesised audio. While this is ideal for adjusting degra-\ndation parameters and performing controlled experiments\nlike the ones presented in this study, it may not be represen-\ntative of the analysis of real-world music recordings. The\nlatter involve many challenges, one of which is the mix\nof different instruments which results in a more complex\naudio signal. In this case rhythmic or melodic elements\nmay get lost in the polyphonic mixture and further pre-\nprocessing of the spectrum is needed to be able to detect\nand isolate the relevant information.\nOur results are based on the analysis of success rates\non classiﬁcation or retrieval tasks. This enabled us to have\nan overview of the performances of different audio fea-\ntures across several factors: transformation, transformation\nvalue, style, monophonic or polyphonic structure. A more\ndetailed analysis could involve a ﬁxed effects model where\nthe contribution of each factor to the performance accuracy\nof each feature is tested individually.\nIn this evaluation we used a wide range of standard clas-\nsiﬁers and distance metrics with default settings. We have\nnot tried to optimise parameters nor use more advanced\nmodels since we wanted the evaluation to be as indepen-\ndent of the application as possible. However, depending\non the application different models could be trained to\nbe more robust to certain transformations than others and\nhigher performance accuracies could be achieved.\n7. CONCLUSION\nWe have investigated the invariance of audio features for\nrhythmic and melodic content description of diverse music\nstyles. A dataset of synthesised audio was designed to test\ninvariance against a broad range of transformations in tim-\nbre, recording quality, tempo and pitch. Considering the\ncriteria and analyses in this study the most robust rhythmic\ndescriptor is the scale transform and melodic descriptor the\npitch bihistogram. Results indicated that the descriptors\nare not completely invariant to characteristics of the music\nstyle and lower accuracies were particularly obtained for\nAfrican and EDM rhythms and Byzantine melodies. The\nperformance of the melodic features was slightly better for\npolyphonic than monophonic content. The proposed evalu-\nation framework can inform decisions in the feature design\nprocess leading to signiﬁcant improvement in the reliabil-\nity of the features.\n8. ACKNOWLEDGEMENTS\nMP is supported by a Queen Mary Principal’s research stu-\ndentship and the EPSRC-funded Platform Grant: Digital\nMusic (EP/K009559/1).Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 4739. REFERENCES\n[1] T. Bertin-Mahieux and D. P. W. Ellis. Large-scale cover\nsong recognition using the 2D Fourier transform mag-\nnitude. In Proceedings of the International Society for\nMusic Information Retrieval Conference , pages 241–\n246, 2012.\n[2] R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Can-\nnam, and J. P. Bello. MedleyDB: A Multitrack Dataset\nfor Annotation-Intensive MIR Research. In Proceed-\nings of the International Society for Music Information\nRetrieval Conference , pages 155–160, 2014.\n[3] M. J. Butler. Unlocking the Groove . Indiana University\nPress, Bloomington and Indianapolis, 2006.\n[4] S. Dixon, F. Gouyon, and G. Widmer. Towards Char-\nacterisation of Music via Rhythmic Patterns. In Pro-\nceedings of the International Symposium on Music In-\nformation Retrieval , pages 509–516, 2004.\n[5] T. M. Esparza, J. P. Bello, and E. J. Humphrey. From\nGenre Classiﬁcation to Rhythm Similarity: Computa-\ntional and Musicological Insights. Journal of New Mu-\nsic Research , 44(1):39–57, 2014.\n[6] A. Holzapfel, A. Flexer, and G. Widmer. Improv-\ning tempo-sensitive and tempo-robust descriptors for\nrhythmic similarity. In Proceedings of the 8th Sound\nand Music Computing Conference , pages 247–252,\n2011.\n[7] A. Holzapfel and Y . Stylianou. Scale Transform in\nRhythmic Similarity of Music. IEEE Transactions on\nAudio, Speech, and Language Processing , 19(1):176–\n185, 2011.\n[8] O. Lartillot and P. Toiviainen. A Matlab Toolbox for\nMusical Feature Extraction From Audio. In Interna-\ntional Conference on Digital Audio Effects , pages 237–\n244, 2007.\n[9] M. Marolt. A mid-level representation for melody-\nbased retrieval in audio collections. IEEE Transactions\non Multimedia , 10(8):1617–1625, 2008.\n[10] M. Mauch and S. Ewert. The Audio Degradation Tool-\nbox and Its Application to Robustness Evaluation. In\nProceedings of the International Society for Music In-\nformation Retrieval Conference , pages 83–88, 2013.\n[11] M. Mauch, R. M. MacCallum, M. Levy, and\nA. M. Leroi. The evolution of popular music: USA\n19602010. Royal Society Open Science , 2(5):150081,\n2015.\n[12] E. Pampalk, A. Flexer, and G. Widmer. Improvements\nof Audio-Based Music Similarity and Genre Classiﬁ-\ncation. In Proceedings of the International Society for\nMusic Information Retrieval Conference , pages 634–\n637, 2005.[13] M. Panteli and H. Purwins. A Quantitative Comparison\nof Chrysanthine Theory and Performance Practice of\nScale Tuning, Steps, and Prominence of the Octoechos\nin Byzantine Chant. Journal of New Music Research ,\n42(3):205–221, 2013.\n[14] T. Pohle, D. Schnitzer, M. Schedl, P. Knees, and\nG. Widmer. On rhythm and general music similarity. In\nProceedings of the International Society for Music In-\nformation Retrieval Conference , pages 525–530, 2009.\n[15] E. D. Scheirer. Tempo and beat analysis of acoustic\nmusical signals. The Journal of the Acoustical Society\nof America , 103(1):588–601, 1998.\n[16] J. Serr `a,´A. Corral, M. Bogu ˜n´a, M. Haro, and J. L. Ar-\ncos. Measuring the Evolution of Contemporary West-\nern Popular Music. Scientiﬁc Reports , 2, 2012.\n[17] S. Stober, D. J. Cameron, and J. A. Grahn. Classifying\nEEG recordings of rhythm perception. In Proceedings\nof the International Society for Music Information Re-\ntrieval Conference , pages 649–654, 2014.\n[18] B. L. Sturm. Classiﬁcation accuracy is not enough.\nJournal of Intelligent Information Systems , 41(3):371–\n406, 2013.\n[19] E. Thul and G. T. Toussaint. A Comparative\nPhylogenetic-Tree Analysis of African Timelines and\nNorth Indian Talas. In Bridges Leeuwarden: Mathe-\nmatics, Music, Art, Architecture, Culture , pages 187–\n194, 2008.\n[20] G. Toussaint. Classiﬁcation and phylogenetic analysis\nof African ternary rhythm timelines. In Meeting Al-\nhambra, ISAMA-BRIDGES Conference , pages 25–36,\n2003.\n[21] J. Urbano, D. Bogdanov, P. Herrera, E. G ´omez, and\nX. Serra. What is the effect of audio quality on the ro-\nbustness of MFCCs and chroma features. In Proceed-\nings of the International Society for Music Information\nRetrieval Conference , pages 573–578, 2014.\n[22] J. van Balen, D. Bountouridis, F. Wiering, and\nR. Veltkamp. Cognition-inspired Descriptors for Scal-\nable Cover Song Retrieval. In Proceedings of the Inter-\nnational Society for Music Information Retrieval Con-\nference , pages 379–384, 2014.\n[23] P. Van Kranenburg, M. de Bruin, L. P. Grijp, and\nF. Wiering. The Meertens Tune Collections . Meertens\nInstitute, Amsterdam, no. 1 edition, 2014.\n[24] T. C. Walters, D. A. Ross, and R. F. Lyon. The Inter-\nvalgram: An Audio Feature for Large-scale Melody\nRecognition. In 9th International Conference on Com-\nputer Music Modeling and Retrieval , pages 19–22,\n2012.474 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Extracting Ground-Truth Information from MIDI Files: A MIDIfesto.",
        "author": [
            "Colin Raffel",
            "Daniel P. W. Ellis"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1418233",
        "url": "https://doi.org/10.5281/zenodo.1418233",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/105_Paper.pdf",
        "abstract": "MIDI files abound and provide a bounty of information for music informatics. We enumerate the types of infor- mation available in MIDI files and describe the steps nec- essary for utilizing them. We also quantify the reliability of this data by comparing it to human-annotated ground truth. The results suggest that developing better methods to leverage information present in MIDI files will facili- tate the creation of MIDI-derived ground truth for audio content-based MIR.",
        "zenodo_id": 1418233,
        "dblp_key": "conf/ismir/RaffelE16",
        "content": "EXTRACTING GROUND TRUTH INFORMATION FROM MIDI FILES:\nA MIDIFESTO\nColin Raffel and Daniel P. W. Ellis\nLabROSA\nDepartment of Electrical Engineering\nColumbia University\nNew York, NY\nABSTRACT\nMIDI ﬁles abound and provide a bounty of information\nfor music informatics. We enumerate the types of infor-\nmation available in MIDI ﬁles and describe the steps nec-\nessary for utilizing them. We also quantify the reliability\nof this data by comparing it to human-annotated ground\ntruth. The results suggest that developing better methods\nto leverage information present in MIDI ﬁles will facili-\ntate the creation of MIDI-derived ground truth for audio\ncontent-based MIR.\n1. MIDI FILES\nMIDI (Music Instrument Digital Interface) is a hardware\nand software standard for communicating musical events.\nFirst proposed in 1983 [1], MIDI remains a highly per-\nvasive standard both for storing musical scores and com-\nmunicating information between digital music devices. Its\nuse is perhaps in spite of its crudeness, which has been\nlamented since MIDI’s early days [21]; most control values\nare quantized as 7-bit integers and information is transmit-\nted at the relatively slow (by today’s standards) 31,250 bits\nper second. Nevertheless, its efﬁciency and well-designed\nspeciﬁcation make it a convenient way of formatting digi-\ntal music information.\nIn the present work, we will focus on MIDI ﬁles, which\nin a simplistic view can be considered a compact way of\nstoring a musical score. MIDI ﬁles are speciﬁed by an ex-\ntension to the MIDI standard [2] and consist of a sequence\nof MIDI messages organized in a speciﬁc format. A typical\nMIDI ﬁle contains timing and meter information in addi-\ntion to a collection of one or more “tracks”, each of which\ncontains a sequence of notes and control messages. The\nGeneral MIDI standard [3] further speciﬁes a collection of\n128 instruments on which the notes can be played, which\nstandardizes the playback of MIDI ﬁles and has therefore\nbeen widely adopted.\nWhen paired with a General MIDI synthesizer, MIDI\nﬁles have been used as a sort of semantic audio codec,\nc/circlecopyrtColin Raffel and Daniel P. W. Ellis. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Colin Raffel and Daniel P. W. Ellis. “Extracting Ground\nTruth Information from MIDI Files: A MIDIfesto”, 17th International\nSociety for Music Information Retrieval Conference, 2016.with entire songs only requiring a few kilobytes of stor-\nage. The early availability of this “coding method”, com-\nbined with the expense of digital storage in the 90s, made\nMIDI ﬁles a highly pervasive method of storing and play-\ning back songs before the advent of the MP3. Even af-\nter high-quality perceptual audio codecs were developed\nand storage prices plummeted, MIDI ﬁles remained in use\nin resource-scarce settings such as karaoke machines and\ncell phone ringtones. As a result, there is an abundance of\nMIDI ﬁle transcriptions of music available today; through\na large-scale web scrape, we obtained 178,561 MIDI ﬁles\nwith unique MD5 checksums. Given their wide availabil-\nity, we believe that MIDI ﬁles are underutilized in the Mu-\nsic Information Retrieval community.\nIn this paper, we start by outlining the various sources\nof information present in MIDI ﬁles and reference rele-\nvant works which utilize them in Section 2. In Section\n3, we discuss the steps needed to leverage MIDI-derived\ninformation as ground truth for content-based MIR. We\nthen establish a baseline for the reliability of MIDI-derived\nground truth by comparing it to handmade annotations in\nSection 4. Finally, in Section 5, we argue that improving\nthe process of extracting information from MIDI ﬁles is a\nviable path for creating large amounts of ground truth data\nfor MIR.\n2. INFORMATION A VAILABLE IN MIDI FILES\nWhile various aspects of MIDI ﬁles have been used in\nMIR research, to our knowledge there has been no uni-\nﬁed overview of the information they provide, nor a dis-\ncussion of the availability and reliability of this informa-\ntion in MIDI transcriptions found “in the wild”. We there-\nfore present an enumeration of the different information\nsources in a typical MIDI ﬁle and discuss their applicabil-\nity to different MIR tasks. Because not all MIDI ﬁles are\ncreated equal, we also computed statistics about the pres-\nence and quantity of each information source across our\ncollection of 178,561 MIDI ﬁles; the results can be seen in\nFigure 1 and will be discussed in the following sections.\n2.1 Transcription\nMIDI ﬁles are speciﬁed as a collection of “tracks”, where\neach track consists of a sequence of MIDI events on one\nof 16 channels. Commonly used MIDI events are note-\non and note-off messages, which together specify the start796Figure 1 : Statistics about sources of information in 178,561 unique MIDI ﬁles scraped from the internet. Histograms in\nthe top row show the number of MIDI ﬁles which had a given number of events for different event types; in the bottom row,\nwe show distributions of the different values set by these events across all MIDI ﬁles. All counts are reported in thousands.\nFor example, about 125,000 MIDI ﬁles had a single time signature change event, and about 210,000 4/4 time signature\nchanges were found in all of our MIDI ﬁles.\nand end time of notes played at a given pitch on a given\nchannel. Various control events also exist, such as pitch\nbends, which allow for ﬁner control of the playback of\nthe MIDI ﬁle. Program change events determine which in-\nstrument these events are sent to. The General MIDI stan-\ndard deﬁnes a correspondence between program numbers\nand a predeﬁned list of 128 instruments. General MIDI\nalso speciﬁes that all notes occurring on MIDI channel 10\nplay on a separate percussion instrument, which allows for\ndrum tracks to be transcribed. The distribution of the to-\ntal number of program change events (corresponding to\nthe number of instruments) across the MIDI ﬁles in our\ncollection and the distribution of these program numbers\nare shown in Figures 1(a) and 1(e) respectively. The four\nmost common program numbers (shown as the four tallest\nbars in Figure 1(e)) were 0 (“Acoustic Grand Piano”), 48\n(“String Ensemble 1”), 33 (“Electric Bass (ﬁnger)”), and\n25 (“Acoustic Guitar (steel)”).\nThis speciﬁcation makes MIDI ﬁles naturally suited to\nbe used as transcriptions of pieces of music, due to the\nfact that they can be considered a sequence of notes played\nat different “velocities” (intensities) on a collection of in-\nstruments. As a result, many MIDI ﬁles are transcriptions\nand are thus commonly used as training data for automatic\ntranscription systems (see [32] for an early example). This\ntype of data also beneﬁts score-informed source separa-\ntion methods, which utilize the score as a prior to improve\nsource separation quality [15]. An additional natural use\nof this information is for “instrument activity detection”,\ni.e. determining when certain instruments are being played\nover the course of a piece of music. Finally, the enumera-\ntion of note start times lends itself naturally to onset detec-\ntion, and so MIDI data has been used for this task [4].\n2.2 Music-Theoretic Features\nBecause many MIDI ﬁles are transcriptions of music, they\ncan also be used to compute high-level musicological char-\nacteristics of a given piece. Towards this end, the soft-ware library jSymbolic [20] includes functionality to\nextract a wide variety of features, including instrumenta-\ntion, rhythm, and pitch statistics. Similarly, music21 [9]\nprovides a general-purpose framework for analyzing col-\nlections of digital scores (including MIDI ﬁles). Comput-\ning these features on a collection of MIDI transcriptions\nis valuable for computational musicology and can enable\ndata-driven corpus studies. For example, [10] discusses\nthe use of music21 andjSymbolic to extract features\nfrom scores and uses them to distinguish music from dif-\nferent composers and musical traditions.\n2.3 Meter\nTiming in MIDI ﬁles is determined by two factors:\nThe MIDI ﬁle’s speciﬁed “resolution” and tempo change\nevents. Each event within the MIDI ﬁle speciﬁes the num-\nber of “ticks” between it and the preceding event. The res-\nolution, which is stored in the MIDI ﬁle’s header, sets the\nnumber of ticks which correspond to a single beat. The\namount of time spanned by each tick is then determined\naccording to the current tempo, as set by tempo change\nevents. For example, if a MIDI ﬁle has a resolution of\n220 ticks per beat and the current tempo is 120 beats per\nminute,1each tick would correspond to 60/(120 ∗220) =\n0.00227seconds. If a MIDI event in this ﬁle is speciﬁed\nto occur 330 ticks after the previous event, then it would\noccur 330∗0.00227 = .75seconds later.\nThe timing in a MIDI ﬁle can vary over time by includ-\ning many tempo change events. In practice, as shown in\nFigure 1(b), most MIDI ﬁles only contain a single tempo\nchange and are therefore transcribed at a ﬁxed tempo.\nHowever, there are many MIDI ﬁles in our collection\nwhich have a large number of tempo change events (as\nindicated by the rightmost bars in Figure 1(b)). We have\nfound that this is a common practice for making the tim-\ning of a MIDI transcription closely match that of an audio\n1Actually, tempo change events specify the number of microseconds\nper quarter beat, but this can be readily converted to beats per minute.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 797recording of the same song. Despite the fact that the de-\nfault tempo for a MIDI ﬁle is 120 beats per minute, Figure\n1(f) demonstrates that a wide range of tempos are used.\nIn practice, we ﬁnd that this is due to the fact that even\nwhen a single tempo event is used, it is often set so that the\nMIDI transcription’s tempo approximates that of an audio\nrecording of the same song.\nTime signature change events further augment MIDI\nﬁles with the ability to specify time signatures, and are\nalso used to indicate the start of a measure. By convention,\nMIDI ﬁles have a time signature change at the ﬁrst tick,\nalthough this is not a requirement. Because time signature\nchanges are relatively rare in western popular music, the\nvast majority of the MIDI ﬁles in our collection contain a\nsingle time signature change, as seen in Figure 1(c). De-\nspite the fact that 4/4 is the default time signature for MIDI\nﬁles and is pervasive in western popular music, a substan-\ntial portion (about half) of the time signature changes were\nnot 4/4, as shown in Figure 1(g).\nBecause MIDI ﬁles are required to include tempo infor-\nmation in order to specify their timing, it is straightforward\nto extract beat locations from a MIDI ﬁle. By convention,\nthe ﬁrst (down)beat in a MIDI transcription occurs at the\nﬁrst tick. Determining the beat locations in a MIDI ﬁle\ntherefore involves computing beat locations starting from\nthe ﬁrst tick and adjusting the tempo and time signature\naccording to any tempo change or time signature change\nevents found. Despite this capability, to our knowledge\nMIDI ﬁles have not been used as ground truth for beat\ntracking algorithms. However, [19] utilized a large dataset\nof MIDI ﬁles to study drum patterns using natural language\nprocessing techniques.\n2.4 Key\nAn additional useful event in MIDI ﬁles is the key change\nevent. Any of the 24 major or minor keys may be speciﬁed.\nKey changes simply give a suggestion as to the tonal con-\ntent and do not affect playback, and so are a completely\noptional meta-event. As seen in Figure 1(d), this results\nin many MIDI ﬁles omitting key change events altogether.\nA further complication is that a disproportionate number\n(about half) of the key changes in the MIDI ﬁles in our\ncollection were C major, as shown in Figure 1(h). This\ndisagrees with corpus studies of popular music, e.g. [8]\nwhich found that only about 26% of songs from the Bill-\nboard 100 were in C major. We believe this is because\nmany MIDI transcription software packages automatically\ninsert a C major key change at the beginning of the ﬁle.\n2.5 Lyrics\nLyrics can be added to MIDI transcriptions by the use of\nlyrics meta-events, which allow for timestamped text to\nbe included over the course of the song. This capabil-\nity enables the common use of MIDI ﬁles for karaoke; in\nfact, a separate ﬁle extension “.kar” is often used for MIDI\nﬁles which include lyrics meta-events. Occasionally, the\ngeneric text meta-event is also used for lyrics, but this is\nnot its intended use. In our collection, we found 23,801\nMIDI ﬁles (about 13.3%) which had at least one lyrics\nmeta-event.2.6 What’s Missing\nDespite the wide variety of information sources available\nin MIDI ﬁles outlined in the previous sections, there are\nvarious types of information which are not possible (or not\ncommon) to store in MIDI ﬁles. While the General MIDI\nspeciﬁcation includes the vocal instruments “Choir Aahs”,\n“V oice Oohs”, “Synth Choir”, “Lead 6 (voice)” and “Pad\n4 (choir)”, in practice there is no speciﬁc program number\n(or numbers) which is consistently used to transcribe vo-\ncals. As a result, in a given MIDI ﬁle there is no reliable\nway of determining which instrument is a transcription of\nthe vocals in a song. Furthermore, because a substantial\nportion of MIDI ﬁles were designed for karaoke, the vo-\ncals may not be transcribed at all.\nWhile the MIDI speciﬁcation does include “track\nname”, “program name”, and “instrument name” meta-\nevents, they are not standardized and so are not used con-\nsistently. It follows that there is no simple way to retrieve\nthe “melody” from a MIDI transcription, although the fact\nthat all instruments are transcribed separately can make its\nestimation more straightforward than for audio ﬁles. For\nexample, [31] explores the use of simple features such as\nthe average velocity and note range within a track to pre-\ndict whether it is a melody, and also ﬁnds that in a small\ndataset the track name reliably indicates a melody track\n44.3% of the time. Similarly, [23] uses heuristic features\nand a random forest classiﬁer to predict with high accuracy\nwhether a track is a melody.\nThere is also no explicit way for MIDI ﬁles to include\nchord labels or structural segmentation boundaries (e.g.\n“verse”, “chorus”, “solo”). While this would in princi-\nple be possible thanks to the generic MIDI “text” meta-\nevent, we have yet to ﬁnd any MIDI ﬁles which store this\ninformation. Nevertheless, estimating chords in particu-\nlar is greatly facilitated by the presence of a ground truth\ntranscription. Both music21 [9] and melisma [30] in-\nclude functionality for estimating chord sequences from\nsymbolic data. Rhodes et al. [29] also proposed a symbolic\nchord estimation method using Bayesian Model Selection,\nwhich was shown to outperform melisma on a dataset of\nBeatles MIDI ﬁles in [14].\nWhile text meta-events could also be used to store song-\nlevel metadata (song title, artist name, etc.) in a MIDI ﬁle,\nwe seldom encountered this. There is no standardized way\nto store this metadata in a MIDI ﬁle, although we found\nthat a minority of the ﬁlenames in our collection indicated\nthe song title and occasionally the artist name. The lack\nof a metadata speciﬁcation also inhibits the attribution of\nMIDI transcriptions to the person who transcribed them.\n3. UTILIZING MIDI FILES AS GROUND TRUTH\nUtilizing MIDI ﬁles as ground truth information for au-\ndio content-based MIR tasks requires the following: First,\nthe compact low-level binary format used by MIDI ﬁles\nmust be parsed so that the information can be readily ex-\ntracted. Second, the artist and song of a MIDI ﬁle must be\ndetermined so it can be paired with a corresponding audio\nrecording. Finally, for many uses, the MIDI ﬁle must be\naligned in time with its matching audio.798 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20163.1 Extracting Information\nThe information sources enumerated in Section 2 are not\nreadily available from MIDI ﬁles due to fact that they\nfollow a low-level binary protocol. For example, in or-\nder to extract the time (in seconds) of all onsets from a\ngiven instrument in a MIDI ﬁle, note events which oc-\ncur on the same track and channel as program change\nevents for the instrument must be collected and their tim-\ning must be computed from their relative ticks using the\nglobal tempo change events. Fortunately, various soft-\nware libraries have been created to facilitate this process.\npretty_midi [24] simpliﬁes the extraction of useful in-\nformation from MIDI transcriptions by taking care of most\nof the low-level parsing needed to convert the information\nto a more human-friendly format. It contains functions for\nretrieving beats, onsets, and note lists from speciﬁc instru-\nments, and the times and values of key, tempo, and time\nsignature changes. It also can be used to modify MIDI\nﬁles, as well as to convert them to synthesized audio or a\nspectrogram-like piano roll representation. The aforemen-\ntioned jSymbolic contains an extensive collection of\nroutines for computing musicological features from MIDI\nﬁles. Finally, both music21 andmelisma are capable\nof inferring high-level music information from symbolic\ndata of various types, including MIDI.\n3.2 Matching\nApart from metadata-agnostic corpus studies such as [19],\ndetermining the song a given MIDI ﬁle represents is usu-\nally required. Matching a given MIDI ﬁle to, for example,\na corresponding entry in the Million Song Dataset [5] can\nbe beneﬁcial even in experiments solely involving sym-\nbolic data analysis because it can provide additional meta-\ndata for the track including its year, genre, and user-applied\ntags. Utilizing information in a MIDI ﬁle for ground truth\nin audio content-based MIR tasks further requires that it be\nmatched to an audio recording of the song, but this is made\ndifﬁcult by the lack of a standardized method for storing\nsong-level metadata in MIDI ﬁles (as discussed in Sec-\ntion 2.6). Content-based matching offers a solution; for\nexample, early work by Hu et al. [17] assigned matches\nby ﬁnding the smallest dynamic time warp (DTW) dis-\ntance between spectrograms of MIDI syntheses and au-\ndio ﬁles across a corpus. This approach is prohibitively\nslow for very large collections of MIDI and/or audio ﬁles,\nso [25] explored learning a mapping from spectrograms to\ndownsampled sequences of binary vectors, which greatly\naccelerates DTW. [27] provided further speed-up by map-\nping entire spectrograms to ﬁxed-length vectors in a Eu-\nclidean space where similar songs are mapped close to-\ngether. These methods make it feasible to match a MIDI\nﬁle against an extremely large corpus of music audio.\n3.3 Aligning\nThere is no guarantee that a MIDI transcription for a given\nsong was transcribed so that its timing matches an audio\nrecording of a performance of the song. For the many types\nof ground truth data that depend on timing (e.g. beats, note\ntranscription, or lyrics), the MIDI ﬁle must therefore haveits timing adjusted so that it matches that of the perfor-\nmance. Fortunately, score-to-audio alignment, of which\nMIDI-to-audio alignment is a special “ofﬂine” case, has\nreceived substantial research attention. A common method\nis to use DTW or another edit-distance measure to ﬁnd the\nbest alignment between spectrograms of the synthesized\nMIDI and the audio recording; see [26] or [14] for surveys.\nIn practice, audio-to-MIDI alignment systems can fail\nwhen there are overwhelming differences in timing or de-\nﬁciencies in the transcription, e.g. missing or incorrect\nnotes or instruments. Ideally, the alignment and match-\ning processes would automatically report the success of the\nalignment and the quality of the MIDI transcription. [26]\nexplores the ability of DTW-based alignment systems to\nreport a “conﬁdence” score indicating the success of the\nalignment. We do not know of any research into automati-\ncally determining the quality of a MIDI transcription.\n4. MEASURING A BASELINE OF RELIABILITY\nFOR MIDI-DERIVED INFORMATION\nGiven the potential availability of ground truth information\nin MIDI transcriptions, we wish to measure the reliability\nof MIDI transcriptions found “in the wild”. A straightfor-\nward way to evaluate the quality of MIDI-derived annota-\ntions is to compare them with hand-made annotations for\nthe same songs. Given a MIDI transcription and human-\ngenerated ground truth data, we can extract correspond-\ning information from the MIDI ﬁle and compare using\nthe evaluation metrics employed in the Music Information\nRetrieval Evaluation eXchange (MIREX) [12]. We there-\nfore leveraged the Isophonics Beatles annotations [18] as\na source of ground truth to compare against MIDI-derived\ninformation. MIDI transcriptions of these songs are readily\navailable due to The Beatles’ popularity.\nOur choice in tasks depends on the overlap in sources of\ninformation in the Isophonics annotations and MIDI ﬁles.\nIsophonics includes beat times, song-level key informa-\ntion, chord changes, and structural segmentation. As noted\nin Section 2, beat times and key changes may be included\nin MIDI ﬁles but there is no standard way to include chord\nchange or structural segmentation information. We there-\nfore performed experiments to evaluate the quality of key\nlabels and beat times available in MIDI ﬁles. Fortuitously,\nthese two experiments give us an insight into both song-\nlevel timing-agnostic information (key) and alignment-\ndependent timing-critical information (beats). To carry out\nthese experiments, we ﬁrst manually identiﬁed 545 MIDI\nﬁles from our collection which had ﬁlenames indicating\nthat they were transcriptions of one of the 179 songs in the\nIsophonics Beatles collection; we found MIDI transcrip-\ntions for all but 11. The median number of MIDI tran-\nscriptions per song was 2; the song “Eleanor Rigby” had\nthe most, with 14 unique transcriptions.\n4.1 Key Experiment\nIn our ﬁrst experiment, we evaluated the reliability of key\nchange events in MIDI ﬁles. We followed the MIREX\nmethodology for comparing keys [13], which proceeds as\nfollows: Each song may only have a single key. All keysProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 799Source Score Comparisons\nMIDI, all keys 0.400 223\nMIDI, C major only 0.167 146\nMIDI, non-C major 0.842 77\nQM Key Detector 0.687 151\nwhatkeyisitin.com 0.857 145\nTable 1 : Mean scores achieved, and the number of com-\nparisons made, by different datasets compared to Isophon-\nics Beatles key annotations.\nmust be either major or minor, e.g. “C# Major” and “E\nminor” are allowed but “D Mixolydian” is not. An esti-\nmated key is given a score of 1.0 when it exactly matches\na ground truth key, 0.5 when it is a perfect ﬁfth above the\nground truth key, 0.3 when it is a relative major or minor,\n0.2 when it is a parallel major or minor, and 0.0 otherwise.\nWe utilized the evaluation library mir_eval [28] to com-\npute this score.\nThe Isophonics annotations mostly follow this format,\nexcept that 21 songs contained multiple key annotations\nand 7 others contained non-major/minor keys. To simplify\nevaluation, we discarded these songs, leaving 151 ground\ntruth key annotations. Of our 545 Beatles MIDIs, 221 had\nno key change event and 5 had more than one, which we\nalso omitted from evaluation. This left 223 MIDI ﬁles for\nwhich we extracted key annotations and compared them to\nvalid Isophonics annotations. Because of the preponder-\nance of C major key change events noted in Section 2.4,\nwe also evaluated MIDI-derived C Major and non-C major\ninstances separately to see whether they were less reliable.\nAs a baseline, we also extracted keys using the QM\nVamp Key Detector plugin [7] whose underlying algorithm\nis based on [22] which ﬁnds the key proﬁle best corre-\nlated with the chromagram of a given song. This plu-\ngin achieved the highest score in MIREX 2013, and has\nbeen the only key detection algorithm submitted in 2014\nand 2015. This gives us a reasonable expectation for a\ngood audio content-based key estimator. To determine\nthe extent to which human annotators agree on key labels,\nwe also collected key annotations for Beatles’ songs from\nwhatkeyisitin.com . As with the Isophonics key an-\nnotations, some songs had multiple and/or modal key la-\nbels; we discarded these and ended up with 145 labels for\nsongs in the Isophonics dataset.\nThe mean scores resulting from comparing each dataset\nto the Isophonics annotations can be seen in Table 1. At\nﬁrst glance, the mean score of 0.4achieved by MIDI key\nchange messages is discouraging. However, by omitting\nall MIDI ﬁles with C major key events (which achieved a\nmean score of 0.167), the mean score jumps to 0.842. This\nis comparable to the human baseline, and is substantially\nhigher than the algorithmically estimated score. We there-\nfore propose that non-C major MIDI key change events are\nas reliable as hand-annotated labels, but that C major key\nannotations in MIDI ﬁles are effectively useless.4.2 Beat Experiment\nUtilizing many of the sources of information in MIDI ﬁles\ndepends on the precise alignment of a given MIDI ﬁle to\nan audio recording of a performance of the same song. We\ntherefore performed an additional experiment to evaluate\nthe quality of MIDI-derived beat annotations, which are\nevaluated on the scale of tens of milliseconds. Producing\nvalid beat annotations from a MIDI ﬁle requires not only\nthat the ﬁle’s meter information is correct, but also that it\nhas been aligned with high precision.\nTo align our Beatles MIDI ﬁles to corresponding audio\nrecordings, we used the scheme proposed in [26], which\nwas found by a large-scale search over common DTW-\nbased audio-to-MIDI alignment systems. We give an out-\nline of this method below; for a full description, see [26].\nFirst, the MIDI ﬁle is synthesized using the fluidsynth\nprogram. Log-magnitude, constant-Q spectrograms of the\nsynthesized MIDI and audio recording are extracted and\ntheir pairwise cosine distance matrix is computed. The\nlowest-cost path through the distance matrix is then found\nusing DTW, with the constraint that the path must span at\nleast 96% of the shorter of the two spectrograms. In ad-\ndition, all paths are penalized by adding the median value\nof the distance matrix each time a frame in one spectro-\ngram is mapped to multiple frames in the other. Finally, a\n“conﬁdence score” is computed as the mean pairwise dis-\ntance along the lowest-cost path, normalized by mean of\nthe submatrix spanned by the path.\nWe followed [26] exactly, except for the following\nchanges: First, instead of computing spectrograms with a\nhop size of 46 ms, we used 23 ms. This ﬁner timescale\nis more appropriate for the beat evaluation metrics we\nwill use, which have tolerances measured in tens of mil-\nliseconds. Second, the conﬁdence scores computed using\nthe method of [26] lie in the range [0.5,1.0]where 0.5\ncorresponds to “highly conﬁdent” and 1.0corresponds to\n“likely wrong”; we mapped this linearly to a more easily-\ninterpretable range of [0.0,1.0]where higher scores mean\nhigher conﬁdence.\nWe used pretty_midi ’sget_beats method to ex-\ntract beat times from our 545 Beatles MIDI ﬁles, and\nadjusted each beat’s timing according to the MIDI ﬁle’s\nalignment to corresponding audio recordings. For eval-\nuation, we used the F-measure ,Any Metric Level Total ,\nandInformation Gain metrics described in [11], as im-\nplemented in mir_eval . As a baseline, we also com-\nputed beat locations using the DBNBeatTracker from\nthemadmom software library,2which is based on the\nalgorithm from [6]. This represents a state-of-the-art\ngeneral-purpose beat tracker which, on the Beatles data,\ncan reliably produce high-quality annotations. If MIDI-\nderived beat annotations are to be taken as ground truth,\nthey must achieve scores similar to or higher than the\nDBNBeatTracker .\nWe visualize the resulting scores in Figure 2. Because\nwe don’t expect beats to be extracted accurately from MIDI\nﬁles that are poor transcriptions or when alignment failed,\nwe plotted each MIDI ﬁle as a single point whose x coor-\n2https://github.com/CPJKU/madmom800 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Figure 2 : Beat evaluation metric scores (compared to Isophonics beat annotations) and alignment conﬁdence scores\nachieved by different audio-to-MIDI alignments of Beatles MIDI ﬁles, with each shown as a blue dot. Mean scores for\neach metric achieved by the DBNBeatTracker [6] are shown as dashed lines.\ndinate corresponds to the alignment conﬁdence score and\nwhose y coordinate is the resulting evaluation metric score\nachieved. Ideally, all points in these plots would be clus-\ntered in the bottom left (corresponding to failed alignments\nwith low conﬁdence scores) or top right (corresponding\nto a successful alignment and beat annotation extraction\nwith a high conﬁdence score). For reference, we plot\nthe mean score achieved by the DBNBeatTracker as\ndotted lines for each metric. From these plots, we can\nsee that in many cases, MIDI-derived annotations achieve\nnear-perfect scores, particularly for the F-Measure andAny\nMetric Level Total metrics. However, there is no reliable\ncorrespondence between high conﬁdence scores and high\nevaluation metric scores. For example, while it appears\nthat a prerequisite for an accurate MIDI-derived beat an-\nnotation is a conﬁdence score above .5, there are many\nMIDI ﬁles which had high conﬁdence scores but low met-\nric scores (appearing in the bottom-right corner of the plots\nin Figure 2).\nWe found that this undesirable behavior was primar-\nily caused by a few issues: First, it is common that the\nalignment system would produce alignments which were\nslightly “sloppy”, i.e. were off by one or two frames (cor-\nresponding to 23 milliseconds each) in places. This had\nless of an effect on the F-measure andAny Metric Level\nTotal metrics, which are invariant to small temporal errors\nup to a certain threshold, but deﬂated the Information Gain\nscores because this metric rewards consistency even for\nﬁne-timing errors. Second, many MIDI ﬁles had tempos\nwhich were at a different metric level than the annotations\n(e.g. double, half, or a third of the tempo). This affected\ntheAny Metric Level Total scores the least because it is in-\nvariant to these issues, except for the handful of ﬁles which\nwere transcribed at a third of the tempo. Finally, we found\nthat the conﬁdence score produced by the alignment sys-\ntem is most reliable at producing a low score in the event\nof a total failure (indicated by points in the bottom left of\nthe plots in Figure 2), but was otherwise insensitive to the\nmore minor issues that can cause beat evaluation metrics\nto produce low scores.5. DISCUSSION\nOur results suggest that while MIDI ﬁles have the poten-\ntial to be valuable sources of ground truth information,\ntheir usage may come with a variety of caveats. However,\ndue to the enormous number of MIDI transcriptions avail-\nable, we believe that developing better methods to lever-\nage information present in MIDI ﬁles is a tantalizing av-\nenue for obtaining more ground truth data for music infor-\nmation retrieval. For example, while C major key annota-\ntions cannot be trusted, developing a highly reliable C ma-\njor vs. non-C major classiﬁcation algorithm for symbolic\ndata (which would ostensibly be much more tractable than\ncreating a perfect general-purpose audio content-based key\nestimation algorithm) would enable the reliable usage of\nall key change messages in MIDI ﬁles. Further work into\nrobust audio-to-MIDI alignment is also warranted in or-\nder to leverage timing-critical information, as is the ne-\nglected problem of alignment conﬁdence score reporting.\nNovel questions such as determining whether all instru-\nments have been transcribed in a given MIDI ﬁle would\nalso facilitate their use as ground truth transcriptions. For-\ntunately, all of these tasks are made easier by the fact\nthat MIDI ﬁles are speciﬁed in a format from which it is\nstraightforward to extract pitch information. Any tech-\nniques developed towards this end could also be applied\nto other ubiquitous symbolic digital music formats such as\nMusicXML [16].\nTo facilitate further investigation, all 178,561 of the\nMIDI ﬁles we obtained in our web scrape (including our\ncollection of 545 Beatles MIDIs) are available online,3\nas well as all of the code used in the experiments in this\npaper.4We hope this data and discussion facilitates a\ngroundswell of MIDI utilization in the MIR community.\n6. ACKNOWLEDGMENTS\nWe thank Eric J. Humphrey and Hunter McCurry for dis-\ncussion about key evaluation, Rafael Valle for investiga-\ntions into MIDI beat tracking, and our anonymous review-\ners for their suggestions.\n3http://colinraffel.com/projects/lmd\n4https://github.com/craffel/midi-ground-truthProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 8017. REFERENCES\n[1] International MIDI Association. MIDI musical instrument\ndigital interface speciﬁcation 1.0. 1983.\n[2] International MIDI Association. Standard MIDI ﬁles. 1988.\n[3] International MIDI Association. General MIDI level 1 speci-\nﬁcation. 1991.\n[4] Juan Pablo Bello, Laurent Daudet, Samer Abdallah, Chris\nDuxbury, Mike Davies, and Mark B. Sandler. A tutorial\non onset detection in music signals. IEEE Transactions on\nSpeech and Audio Processing , 13(5):1035–1047, 2005.\n[5] Thierry Bertin-Mahieux, Daniel P. W. Ellis, Brian Whitman,\nand Paul Lamere. The million song dataset. In Proceedings\nof the 12th International Society for Music Information Re-\ntrieval Conference , pages 591–596, 2011.\n[6] Sebastian B ¨ock, Florian Krebs, and Gerhard Widmer. A\nmulti-model approach to beat tracking considering heteroge-\nneous music styles. In Proceedings of the 15th International\nSociety for Music Information Retrieval Conference , pages\n603–608, 2014.\n[7] Chris Cannam, Emmanouil Benetos, Matthias Mauch,\nMatthew E. P. Davies, Simon Dixon, Christian Landone,\nKaty Noland, and Dan Stowell. MIREX 2015 entry: Vamp\nplugins from the centre for digital music. In 11th Music In-\nformation Retrieval Evaluation eXchange , 2015.\n[8] Dave Carlton. I analyzed the chords of 1300 pop-\nular songs for patterns. This is what I found.\nhttp://www.hooktheory.com/blog/ , June 2012.\n[9] Michael Scott Cuthbert and Christopher Ariza. music21 : A\ntoolkit for computer-aided musicology and symbolic music\ndata. In Proceedings of the 11th International Society for Mu-\nsic Information Retrieval Conference , pages 637–642, 2010.\n[10] Michael Scott Cuthbert, Christopher Ariza, and Lisa Fried-\nland. Feature extraction and machine learning on symbolic\nmusic using the music21 toolkit. In Proceedings of the 12th\nInternational Society for Music Information Retrieval Con-\nference , pages 387–392, 2011.\n[11] Matthew E. P. Davies, Norberto Degara, and Mark D. Plumb-\nley. Evaluation methods for musical audio beat tracking al-\ngorithms. Technical Report C4DM-TR-09-06, Queen Mary\nUniversity of London, 2009.\n[12] J. Stephen Downie. The music information retrieval evalua-\ntion exchange (2005-2007): A window into music informa-\ntion retrieval research. Acoustical Science and Technology ,\n29(4):247–255, 2008.\n[13] Andreas Ehmann, Kris West, Mert Bay, Kahyun Choi,\nand Yun Hao. MIREX task: Audio key detection.\nhttp://music-ir.org/mirex/wiki/2016:\nAudio_Key_Detection , 2016.\n[14] Sebastian Ewert, Meinard M ¨uller, Verena Konz, Daniel\nM¨ullensiefen, and Geraint A. Wiggins. Towards cross-\nversion harmonic analysis of music. IEEE Transactions on\nMultimedia , 14(3):770–782, 2012.\n[15] Sebastian Ewert, Bryan Pardo, Meinard M ¨uller, and Mark\nPlumbley. Score-informed source separation for musical au-\ndio recordings: An overview. IEEE Signal Processing Maga-\nzine, 31(3):116–124, 2014.\n[16] Michael Good. MusicXML for notation and analysis. In Wal-\nter B. Hewelett and Eleanor Selfridge-Field, editors, The vir-\ntual score: representation, retrieval, restoration , volume 12,\npages 113–124. MIT Press, 2001.[17] Ning Hu, Roger B. Dannenberg, and George Tzanetakis.\nPolyphonic audio matching and alignment for music retrieval.\nInIEEE Workshop on Applications of Signal Processing to\nAudio and Acoustics , pages 185–188, 2003.\n[18] Matthias Mauch, Chris Cannam, Matthew Davies, Simon\nDixon, Christopher Harte, Sefki Kolozali, Dan Tidhar, and\nMark Sandler. OMRAS2 metadata project 2009. In 10th In-\nternational Society for Music Information Retrieval Confer-\nence Late Breaking and Demo Papers , 2009.\n[19] Matthias Mauch and Simon Dixon. A corpus-based study\nof rhythm patterns. In Proceedings of the 13th International\nSociety for Music Information Retrieval Conference , pages\n163–168, 2012.\n[20] Cory McKay and Ichiro Fujinaga. jSymbolic : A feature\nextractor for MIDI ﬁles. In Proceedings of the International\nComputer Music Conference , pages 302–5, 2006.\n[21] F. Richard Moore. The dysfunctions of MIDI. Computer mu-\nsic journal , 12(1):19–28, 1988.\n[22] Katy Noland and Mark Sandler. Signal processing parameters\nfor tonality estimation. In Audio Engineering Society Conven-\ntion 122 , 2007.\n[23] Pedro Jos ´e Ponce de Le ´on Amador, Jos ´e Manuel\nI˜nesta Quereda, and David Rizo Valero. Mining digital\nmusic score collections: melody extraction and genre\nrecognition. In Peng-Yeng Yin, editor, Pattern Recognition\nTechniques, Technology and Applications , chapter 25, pages\n559–590. InTech, 2008.\n[24] Colin Raffel and Daniel P. W. Ellis. Intuitive analysis, cre-\nation and manipulation of MIDI data with pretty_midi .\nIn15th International Society for Music Information Retrieval\nConference Late Breaking and Demo Papers , 2014.\n[25] Colin Raffel and Daniel P. W. Ellis. Large-scale content-\nbased matching of MIDI and audio ﬁles. In Proceedings\nof the 16th International Society for Music Information Re-\ntrieval Conference , pages 234–240, 2015.\n[26] Colin Raffel and Daniel P. W. Ellis. Optimizing DTW-based\naudio-to-MIDI alignment and matching. In 41st IEEE Inter-\nnational Conference on Acoustics, Speech, and Signal Pro-\ncessing , pages 81–85, 2016.\n[27] Colin Raffel and Daniel P. W. Ellis. Pruning subsequence\nsearch with attention-based embedding. In 41st IEEE Inter-\nnational Conference on Acoustics, Speech, and Signal Pro-\ncessing , pages 554–558, 2016.\n[28] Colin Raffel, Brian McFee, Eric J. Humphrey, Justin Sala-\nmon, Oriol Nieto, Dawen Liang, and Daniel P. W. Ellis.\nmir_eval : A transparent implementation of common MIR\nmetrics. In Proeedings of the 15th International Society for\nMusic Information Retrieval Conference , pages 376–372,\n2014.\n[29] Christophe Rhodes, David Lewis, and Daniel M ¨ullensiefen.\nBayesian model selection for harmonic labelling. In Mathe-\nmatics and Computation in Music , pages 107–116. Springer,\n2007.\n[30] Daniel Sleator and David Temper-\nley. The melisma music analyzer.\nhttp://www.link.cs.cmu.edu/melisma , 2001.\n[31] Michael Tang, Yip Chi Lap, and Ben Kao. Selection of\nmelody lines for music databases. In Proceedings of the 24th\nInternational Computer Software and Applications Confer-\nence, pages 243–248, 2000.\n[32] Robert J. Turetsky and Daniel P. W. Ellis. Ground-truth tran-\nscriptions of real music from force-aligned MIDI syntheses.\nInProceedings of the 4th International Society for Music In-\nformation Retrieval Conference , pages 135–141, 2003.802 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Singing Voice Melody Transcription Using Deep Neural Networks.",
        "author": [
            "François Rigaud",
            "Mathieu Radenen"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1418051",
        "url": "https://doi.org/10.5281/zenodo.1418051",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/163_Paper.pdf",
        "abstract": "This paper presents a system for the transcription of singing voice melodies in polyphonic music signals based on Deep Neural Network (DNN) models. In particular, a new DNN system is introduced for performing the f0 es- timation of the melody, and another DNN, inspired from recent studies, is learned for segmenting vocal sequences. Preparation of the data and learning configurations related to the specificity of both tasks are described. The perfor- mance of the melody f0 estimation system is compared with a state-of-the-art method and exhibits highest accu- racy through a better generalization on two different music databases. Insights into the global functioning of this DNN are proposed. Finally, an evaluation of the global system combining the two DNNs for singing voice melody tran- scription is presented.",
        "zenodo_id": 1418051,
        "dblp_key": "conf/ismir/RigaudR16",
        "content": "SINGING VOICE MELODY TRANSCRIPTION USING DEEP NEURAL\nNETWORKS\nFranc ¸ois Rigaud and Mathieu Radenen\nAudionamix R&D\n171 quai de Valmy, 75010 Paris, France\n<firstname>.<lastname>@audionamix.com\nABSTRACT\nThis paper presents a system for the transcription of\nsinging voice melodies in polyphonic music signals based\non Deep Neural Network (DNN) models. In particular, a\nnew DNN system is introduced for performing the f0es-\ntimation of the melody, and another DNN, inspired from\nrecent studies, is learned for segmenting vocal sequences.\nPreparation of the data and learning conﬁgurations related\nto the speciﬁcity of both tasks are described. The perfor-\nmance of the melody f0estimation system is compared\nwith a state-of-the-art method and exhibits highest accu-\nracy through a better generalization on two different music\ndatabases. Insights into the global functioning of this DNN\nare proposed. Finally, an evaluation of the global system\ncombining the two DNNs for singing voice melody tran-\nscription is presented.\n1. INTRODUCTION\nThe automatic transcription of the main melody from poly-\nphonic music signals is a major task of Music Information\nRetrieval (MIR) research [19]. Indeed, besides applica-\ntions to musicological analysis or music practice, the use\nof the main melody as prior information has been shown\nuseful in various types of higher-level tasks such as music\ngenre classiﬁcation [20], music retrieval [21], music de-\nsoloing [4, 18] or lyrics alignment [15, 23]. From a sig-\nnal processing perspective, the main melody can be repre-\nsented by sequences of fundamental frequency ( f0) deﬁned\non voicing instants, i.e.on portions where the instrument\nproducing the melody is active. Hence, main melody tran-\nscription algorithms usually follow two main processing\nsteps. First, a representation emphasizing the most likely\nf0s over time is computed, e.g.under the form of a salience\nmatrix [19], a vocal source activation matrix [4] or an en-\nhanced spectrogram [22]. Second, a binary classiﬁcation\nof the selected f0s between melodic and background con-\ntent is performed using melodic contour detection/tracking\nand voicing detection.\nc/circlecopyrtFranc ¸ois Rigaud and Mathieu Radenen. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Franc ¸ois Rigaud and Mathieu Radenen. “Singing V oice\nMelody Transcription using Deep Neural Networks”, 17th International\nSociety for Music Information Retrieval Conference, 2016.In this paper we propose to tackle the melody transcrip-\ntion task as a supervised classiﬁcation problem where each\ntime frame of signal has to be assigned into a pitch class\nwhen a melody is present and an ‘unvoiced’ class when it is\nnot. Such approach has been proposed in [5] where melody\ntranscription is performed applying Support Vector Ma-\nchine on input features composed of Short-Time Fourier\nTransforms (STFT). Similarly for noisy speech signals,\nf0estimation algorithms based on Deep Neural Networks\n(DNN) have been introduced in [9, 12].\nFollowing such fully data driven approaches we intro-\nduce a singing voice melody transcription system com-\nposed of two DNN models respectively used to perform\nthef0estimation task and the V oice Activity Detection\n(V AD) task. The main contribution of this paper is to\npresent a DNN architecture able to discriminate the differ-\nentf0s from low-level features, namely spectrogram data.\nCompared to a well-known state-of-the-art method [19],\nit shows signiﬁcant improvements in terms of f0accu-\nracy through an increase of robustness with regard to mu-\nsical genre and a reduction of octave-related errors. By\nanalyzing the weights of the network, the DNN is shown\nsomehow equivalent to a simple harmonic-sum method for\nwhich the parameters usually set empirically are here auto-\nmatically learned from the data and where the succession\nof non-linear layers likely increases the power of discrim-\nination of harmonically-related f0. For the task of V AD,\nanother DNN model, inspired from [13] is learned. For\nboth models, special care is taken to prevent over-ﬁtting\nissues by using different databases and perturbing the data\nwith audio degradations. Performance of the whole system\nis ﬁnally evaluated and shows promising results.\nThe rest of the paper is organized as follows. Section 2\npresents an overview of the whole system. Sections 3 and\n4 introduce the DNN models and detail the learning con-\nﬁgurations respectively for the V AD and the f0estimation\ntask. Then, Section 5 presents an evaluation of the system\nand Section 6 concludes the study.\n2. SYSTEM OVERVIEW\n2.1 Global architecture\nThe proposed system, displayed on Figure 1, is composed\nof two independent parallel DNN blocks that perform re-\nspectively the f0melody estimation and the V AD.737\u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\t\n\u000b\f\r\u000b\u0001\u0002\u000e\u0002\u000b\u0004\n\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0002\u0003\u0004\n\u000f\u0007\u0010\t\u0005\u000e\u0011\u0012\t\u0007\n\u000b\f\r\u0011\u000e\u0005\u000e\u0002\u000b\u0004\n\u000f\u0007\u0004\t\u000e\u0013\u000b\u0012\u0014\u0007\r\u0012\u000b\r\u0005\u0003\u0005\u000e\u0002\u000b\u0004\n\u000f\u0007\r\u000b\u0001\u000e\u0015\r\u0012\u000b\n\t\u0001\u0001\u0002\u0004\u0003\u0001\u0001\u0001\u0001\u0001\u0005\u0006\u0001\u0007\b\t\n\u000b\f\t\n\r\u000e\n\u000f\u0007\u0010\t\u0005\u000e\u0011\u0012\t\u0007\n\u000b\f\r\u0011\u000e\u0005\u000e\u0002\u000b\u0004\n\u000f\u0007\u0004\t\u000e\u0013\u000b\u0012\u0014\u0007\r\u0012\u000b\r\u0005\u0003\u0005\u000e\u0002\u000b\u0004\n\u000f\u0007\r\u000b\u0001\u000e\u0015\r\u0012\u000b\n\t\u0001\u0001\u0002\u0004\u0003\n\r\u000b\u0001\u000e\u0015\r\u0012\u000b\n\t\u0001\u0001\u0002\u0004\u0003\u0005\u0011\b\u0002\u000b\u0007\u0001\u0002\u0003\u0004\u0005\u0006\n\u0016\u000b\n\u0005\u0006\u0007\f\t\u0006\u000b\b\u0017Figure 1 : Architecture of the proposed system for singing\nvoice melody transcription.\nIn contrast with [9,12] that propose a single DNN model\nto perform both tasks, we did not ﬁnd such uniﬁed func-\ntional architecture able to discriminate successfully a time\nframe between quantiﬁed f0s and ‘unvoiced’ classes. In-\ndeed the models presented in these studies are designed\nfor speech signals mixed with background noise for which\nthe discrimination between a frame of noise and a frame\nof speech is very likely related to the presence or absence\nof a pitched structure, which is also probably the kind of\ninformation on which the system relies to estimate the f0.\nConversely, with music signals both the melody and the\naccompaniment exhibit harmonic structures and the voic-\ning discrimination usually requires different levels of in-\nformation, e.g.under the form of timbral features such as\nMel-Frequency Cepstral Coefﬁcients.\nAnother characteristic of the proposed system is the par-\nallel architecture that allows considering different types of\ninput data for the two DNNs and which arises from the\napplication restricted to vocal melodies. Indeed, unlike\ngeneric systems dealing with main melody transcription of\ndifferent instruments (often within a same piece of music)\nwhich usually process the f0estimation and the voicing de-\ntection sequentially, the focus on singing voice here hardly\nallows for a voicing detection relying only on the distri-\nbution and statistics of the candidate pitch contours and/or\ntheir energy [2, 19]. Thus, this constraint requires to build\na speciﬁc V AD system that should learn to discriminate\nthe timbre of a vocal melody from an instrumental melody,\nsuch as for example played by a saxophone.\n2.2 Signal decomposition\nAs shown on Figure 1, both DNN models are preceded by\na signal decomposition. At the input of the global system,\naudio signals are ﬁrst converted to mono and re-sampled to\n16 kHz. Then, following [13], it is proposed to provide the\nDNNs with a set of pre-decomposed signals obtained by\napplying a double-stage Harmonic/Percussive Source Sep-\naration (HPSS) [6,22] on the input mixture signal. The key\nidea behind double-stage HPSS is to consider that within a\nmix, melodic signals are usually less stable/stationary than\nthe background ‘harmonic’ instruments (such as a bass or\na piano), but more than the percussive instruments (such\nas the drums). Thus, according to the frequency reso-lution that is used to compute a STFT, applying a har-\nmonic/percussive decomposition on a mixture spectrogram\nlead to a rough separation where the melody is mainly ex-\ntracted either in the harmonic or in the percussive content.\nUsing such pre-processing, 4 different signals are ob-\ntained. First, the input signal sis decomposed into the sum\nofh1andp1using a high-frequency resolution STFT (typ-\nically with a window of about 300 ms) where p1should\nmainly contain the melody and the drums, and h1the\nremaining stable instrument signals. Second, p1is fur-\nther decomposed into the sum of h2andp2using a low-\nfrequency resolution STFT (typically with a window of\nabout 30 ms), where h2mainly contains the melody, and\np2the drums. As presented latter in Sections 3 and 4, dif-\nferent types of these 4 signals or combinations of them will\nbe used to experimentally determine optimal DNN models.\n2.3 Learning data\nSeveral annotated databases composed of polyphonic mu-\nsic with transcribed melodies are used for building the\ntrain, validation and test datasets used for the learning ( cf.\nSections 3 and 4) and the evaluation ( cf.Section 5) of the\nDNNs. In particular, a subset of RWC Popular Music and\nRoyalty Free Music [7] and MIR-1k [10] databases are\nused for the train dataset, and the recent databases Med-\nleyDB [1] and iKala [3] are split between train, validation\nand test datasets. Note that for iKala the vocal and instru-\nmental tracks are mixed with a relative gain of 0 dB.\nAlso, in order to minimize over-ﬁtting issues and to in-\ncrease the robustness of the system with respect to audio\nequalization and encoding degradations, we use the Audio\nDegradation Toolbox [14]. Thus, several ﬁles composing\nthe train and validation datasets (50% for the V AD task and\n25% for the f0estimation task) are duplicated with one de-\ngraded version, the degradation type being randomly cho-\nsen amongst those available preserving the alignment be-\ntween the audio and the annotation ( e.g. not producing\ntime/pitch warping or too long reverberation effects).\n3. VOICE ACTIVITY DETECTION WITH DEEP\nNEURAL NETWORKS\nThis section brieﬂy describes the process for learning the\nDNN used to perform the V AD. It is largely inspired from\na previous study presented in more detail in [13]. A similar\narchitecture of deep recurrent neural network composed of\nBidirectional Long Short-Term Memory (BLSTM) [8] is\nused. In our case the architecture is arbitrarily ﬁxed to 3\nBLSTM layers of 50 units each and a ﬁnal feed-forward lo-\ngistic output layer with one unit. As in [13], different types\nof combination of the pre-decomposed signals ( cf.Section\n2.2) are considered to determine an optimal network: s,\np1,h2,h1p1,h2p2andh1h2p2. For each of these pre-\ndecomposed signals, timbral features are computed under\nthe form of mel-frequency spectrograms obtained using a\nSTFT with 32 ms long Hamming windows and 75 % of\noverlap, and 40 triangular ﬁlters distributed on a mel scale\nbetween 0 and 8000 Hz. Then, each feature of the input738 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 201650 \nBLSTM units...\n50 \nBLSTM units\n1\nlogistic unit\ntime frame indexDNN input feature index\n50 100 150 200 250 300 35020406080100120\n0 50 100 150 200 250 300 35000.20.40.60.81\ntime frame indexDNN output\n...\n...50 \nBLSTM unitsFigure 2 : V AD network illustration.\ndata is normalized using the mean and variance computed\nover the train dataset. Contrary to [13] the learning is per-\nformed in a single step, i.e.without adopting a layer by\nlayer training.\nFinally, the best architecture is obtained for the combi-\nnation of h1,h2andp2signals, thus for an input of size\n120, which corresponds to a use of the whole information\npresent in the original signal ( s=h1+h2+p2). An\nillustration of this network is presented in Figure 2.\nA simple post-processing of the DNN output consisting\nin a threshold of 0.5 is ﬁnally applied to take the binary\ndecision of voicing frame activation.\n4.F0ESTIMATION WITH DEEP NEURAL\nNETWORKS\nThis section presents in detail the learning conﬁguration\nfor the DNN used for performing the f0estimation task.\nAn interpretation of the network functioning is ﬁnally pre-\nsented.\n4.1 Preparation of learning data\nAs proposed in [5] we decide to keep low level features\nto feed the DNN model. Compared to [12] and [9] which\nuse as input pre-computed representations known for high-\nlighting the periodicity of pitched sounds (respectivelybased on an auto-correlation and a harmonic ﬁltering), we\nexpect here the network to be able to learn an optimal\ntransformation automatically from spectrogram data. Thus\nthe set of selected features consists of log-spectrograms\n(logarithm of the modulus of the STFT) computed from a\nHamming window of duration 64 ms (1024 samples for a\nsampling frequency of 16000 Hz) with an overlap of 0.75,\nand from which frequencies below 50 Hz and above 4000\nHz are discarded. For each music excerpt the correspond-\ning log-spectrogram is rescaled between 0 and 1. Since, as\ndescribed in Section 2.1, the V AD is performed by a sec-\nond independent system, all time frames for which no vo-\ncal melody is present are removed from the dataset. These\nfeatures are computed independently for 3 different types\nof input signal for which the melody should be more or less\nemphasized: s,p1andh2(cf.Section 2.2).\nFor the output, the f0s are quantiﬁed between C#2\n(f0/similarequal69.29 Hz) and C#6 ( f0/similarequal1108.73 Hz) with a spac-\ning of an eighth of tone, thus leading to a total of 193\nclasses.\nThe train and validation datasets including audio de-\ngraded versions are ﬁnally composed of, respectively,\n22877 melodic sequences (resp. 3394) for a total duration\nof about 220 minutes (resp. 29 min).\n4.2 Training\nSeveral experiments have been run to determine a func-\ntional DNN architecture. In particular, two types of neuron\nunits have been considered: the standard feed-forward sig-\nmoid unit and the Bidirectional Long Short-Term Memory\n(BLSTM) recurrent unit [8].\nFor each test, the weights of the network are initialized\nrandomly according to a Gaussian distribution with 0 mean\nand a standard deviation of 0.1, and optimized to mini-\nmize the cross-entropy error function. The learning is then\nperformed by means of a stochastic gradient descent with\nshufﬂed mini-batches composed of 30 melodic sequences,\na learning rate of 10−7and a momentum of 0.9. The op-\ntimization is run for a maximum of 10000 epochs and an\nearly stopping is applied if no decrease is observed on the\nvalidation set error during 100consecutive epochs. In ad-\ndition to the use of audio degradations during the prepa-\nration of the data for preventing over-ﬁtting ( cf.Section\n2.3), the training examples are slightly corrupted during\nthe learning by adding a Gaussian noise with variance 0.05\nat each epoch.\nAmong the different architectures tested, the best clas-\nsiﬁcation performance is obtained for the input signal p1\n(slightly better than for s,i.e.without pre-separation) by\na 2-hidden layer feed-forward network with 500 sigmoid\nunits each, and a 193 output softmax layer. An illustration\nof this network is presented in Figure 3. Interestingly, for\nthat conﬁguration the learning did not suffered from over-\nﬁtting so that it ended at the maximum number of epochs,\nthus without early stopping.\nWhile the temporal continuity of the f0along time-\nframes should provide valuable information, the use of\nBLSTM recurrent layers (alone or in combination withProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 739500 \nlogistic units...\n...500 \nlogistic units\n...193 \nsoftmax units\ntime frame indexDNN input feature index\n260 280 300 320 340 360 380 400 420 440100200300400500600700\ntime frame indexDNN output class index\n260 280 300 320 340 360 380 400 420 44020406080100120140160180Figure 3 :f0estimation network illustration.\nfeed-forward sigmoid layers) did not lead to efﬁcient sys-\ntems. Further experiments should be conducted to enforce\nthe inclusion of such temporal context in a feed-forward\nDNN architecture, for instance by concatenating several\nconsecutive time frames in the input.\n4.3 Post-processing\nThe output layer of the DNN composed of softmax units\nreturns a f0probability distribution for each time frame\nthat can be seen for a full piece of music as a pitch ac-\ntivation matrix. In order to take a ﬁnal decision that ac-\ncount for the continuity of the f0along melodic sequences,\na Viterbi tracking is ﬁnally applied on the network out-\nput [5, 9, 12]. For that, the log-probability transition be-\ntween two consecutive time frames and two f0classes is\nsimply arbitrarily set inversely proportional to their abso-\nlute difference in semi-tones. For further improvement of\nthe system, such transition matrix could be learned from\nthe data [5], however this simple rule gives interesting per-\nformance gains (when compared to a simple ‘maximum\npicking’ post-processing without temporal context) while\npotentially reducing the risk of over-ﬁtting to a particular\nmusic style.\nlayer #1 unit indexDNN input feature index\n100 200 300 400 500100200300400500600700\nlayer #2 unit indexlayer #1 output index\n100 200 300 400 50050100150200250300350400450500\nlayer #2 output index\nsoftmax unit index50 100 15050100150200250300350400450500Figure 4 : Display of the weights for the two sigmoid feed-\nforward layers (top) and the softmax layer (down) of the\nDNN learned for the f0estimation task.\n4.4 Network weights interpretation\nWe propose here to have an insight into the network func-\ntioning for this speciﬁc task of f0estimation by analyzing\nthe weights of the DNN. The input is a short-time spec-\ntrum and the output corresponds to an activation vector for\nwhich a single element (the actual f0of the melody at that\ntime frame) should be predominant. In that case, it is rea-\nsonable to expect that the DNN somehow behaves like a\nharmonic-sum operator.\nWhile the visualization of the distribution of the hidden-\nlayer weights usually does not provide with straightfor-\nward cues to analyse a DNN functioning ( cf.Figure 4)\nwe consider a simpliﬁed network for which it is assumed\nthat each feed-forward logistic unit is working in the linear\nregime. Thus, removing the non-linear operations, the out-\nput of a feed-forward layer with index lcomposed of Nl\nunits writes\nxl=Wl·xl−1+bl, (1)\nwhere xl∈RNl(resp. xl−1∈RNl−1) corresponds to the\nouput vector of layer l(resp. l−1),Wl∈RNl×Nl−1is\nthe weight matrix and bl∈RNlthe bias vector. Using this\nexpression, the output of a layer with index Lexpressed as\nthe propagation of the input x0through the linear network\nalso writes\nxL=W·x0+b, (2)\nwhere W=/producttextL\nl=1Wlcorresponds to a global weight ma-\ntrix, and bto a global bias that depends on the set of pa-\nrameters{Wl, bl,∀l∈[1..L]}.\nAs mentioned above, in our case x0is a short-time spec-\ntrum and xLis af0activation vector. The global weight\nmatrix should thus present some characteristics of a pitch\ndetector. Indeed as displayed on Figure 5a, the matrix\nWfor the learned DNN (which is thus the product of the\n3 weight matrices depicted on Figure 4) exhibits an har-\nmonic structure for most output classes of f0s; except for740 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016DNN output class indexDNN input feature index\n20 40 60 80 100 120 140 160 180100200300400500600700(a)\n100 200 300 400 500 600 700−200204060\nDNN input feature index\n(b)\nFigure 5 : Linearized DNN illustration. (a) Visualization\nof the (transposed) weight matrix W. The x-axis corre-\nsponds to the output class indices (the f0s) and the y-axis\nrepresents the input feature indices (frequency channel of\nthe spectrum input). (b) Weights display for the f0output\nclass with index 100.\nsome f0s in the low and high frequency range for which no\nor too few examples are present in the learning data.\nMost approaches dealing with main melody transcrip-\ntion usually relies on such types of transformations to\ncompute a representation emphasizing f0candidates (or\nsalience function) and are usually partly based on hand-\ncrafted designs [11, 17, 19]. Interestingly, using a fully\ndata driven method as proposed, parameters of a compara-\nble weighted harmonic summation algorithm (such as the\nnumber of harmonics to consider for each note and their\nrespective weights) do not have to be deﬁned. This can be\nobserved in more details on Figure 5b which depicts the\nlinearized network weights for the class index 100 ( f0/similarequal\n289.43 Hz). Moreover, while this interpretation assumes a\nlinear network, one can expect that the non-linear opera-\ntions actually present in the network help in enhancing the\ndiscrimination between the different f0classes.\n5. EV ALUATION\n5.1 Experimental procedure\nTwo different test datasets composed of full music ex-\ncerpts ( i.e. vocal and non vocal portions) are used for\nthe evaluation. One is composed of 17 tracks from Med-\nleyDB (last songs comprising vocal melodies, from Mu-\nsicDelta Reggae toWolf DieBekherte , for a total of∼25.5\nmin of vocal portions) and the other is composed of 63\ntracks from iKala (from 54223 chorus to90587 verse fora total of∼21 min of vocal portions).\nThe evaluation is conducted in two steps. First the per-\nformance of the f0estimation DNN taken alone (thus with-\nout voicing detection) is compared with the state-of the\nart system melodia [19] using f0accuracy metrics. Sec-\nond, the performance of our complete singing voice tran-\nscription system (V AD and f0estimation) is evaluated on\nthe same datasets. Since our system is restricted to the\ntranscription of vocal melodies and that, to our knowledge\nall available state-of-the-art systems are designed to target\nmain melody, this ﬁnal evaluation presents the results for\nour system without comparisons with a reference.\nFor all tasks and systems, the evaluation metrics are\ncomputed using the mireval library [16]. For Section\n5.3, some additional metrics related to voicing detection,\nnamely precision, f-measure and voicing accuracy, were\nnot present in the original mireval code and thus were\nadded for our experiments.\n5.2f0estimation task\nThe performance of the DNN performing the f0estima-\ntion task is ﬁrst compared to melodia system [19] using\nthe plug-in implementation with f0search range limits set\nequal to those of our system (69.29-1108.73 Hz, cf.Sec.\n4.1) and with remaining parameters left to default values.\nFor each system and each music track the performance is\nevaluated in terms of raw pitch accuracy (RPA) and raw\nchroma accuracy (RCA). These metrics are computed on\nvocal segments ( i.e.without accounting for potential voic-\ning detection errors) for a f0tolerance of 50 cents.\nThe results are presented on Figure 6 under the form\nof a box plot where, for each metric and dataset, the ends\nof the dashed vertical bars delimit the lowest and highest\nscores obtained, the 3 vertical bars composing each center\nbox respectively correspond to the ﬁrst quartile, the median\nand the third quartile of the distribution, and ﬁnally the\nstar markers represent the mean. Both systems are char-\nacterized by more widespread distributions for MedleyDB\nthan for iKala. This reﬂects the fact that MedleyDB is\nmore heterogeneous in musical genres and recording con-\nditions than iKala. On iKala, the DNN performs slightly\nbetter than melodia when comparing the means. On Med-\nleyDB, the gap between the two systems increases signif-\nicantly. The DNN system seems much less affected by\nthe variability of the music examples and clearly improve\nthe mean RPA by 20% (62.13% for melodia and 82.48%\nfor the DNN). Additionally, while exhibiting more similar\ndistributions of RPA and RCA, the DNN tends to produce\nless octave detection errors. It should be noted that this re-\nsult does not take into account the recent post-processing\nimprovement proposed for melodia [2], yet it shows the\ninterest of using such DNN approach to compute an en-\nhanced pitch salience matrix which, simply combined with\na Viterbi post-processing, achieves good performance.\n5.3 Singing voice transcription task\nThe evaluation of the global system is ﬁnally performed\non the two same test datasets. The results are displayed asProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 741RPA RCA RPA RCA00.10.20.30.40.50.60.70.80.91MedleyDB                                          iKala\n  \nNN\nmelodiaFigure 6 : Comparative evaluation of the proposed DNN\n(in black) and melodia (in gray) on MedleyDB (left) and\niKala (right) test sets for a f0vocal melody estimation task.\nboxplots ( cf.description Section 5.2) on Figures 7a and 7b\nrespectively for the iKala and the MedleyDB datasets. Five\nmetrics are computed to evaluate the voicing detection,\nnamely the precision (P), the recall (R), the f-measure (F),\nthe false alarm rate (FA) and the voicing accuracy (V A). A\nsixth metric of overall accuracy (OA) is also presented for\nassessing the global performance of the complete singing\nvoice melody transcription system.\nIn accordance with the previous evaluation, the results\non MedleyDB are characterized by much more variance\nthan on iKala. In particular, the voicing precision of the\nsystem ( i.e. it’s ability to provide correct detections, no\nmatter the number of forgotten voiced frames) is signif-\nicantly degraded on MedleyDB. Conversely, the voicing\nrecall which evaluate the ability of the system to detect all\nvoiced portions actually present no matter the number of\nfalse alarm, remains relatively good on MedleyDB. Com-\nbining both metrics, a mean f-measure of 93.15 % and\n79.19 % are respectively obtained on iKala and MedleyDB\ntest datasets.\nFinally, the mean scores of overall accuracy obtained\nfor the global system are equal to 85.06 % and 75.03 %\nrespectively for iKala and MedleyDB databases.\n6. CONCLUSION\nThis paper introduced a system for the transcription of\nsinging voice melodies composed of two DNN models. In\nparticular a new system able to learn a representation em-\nphasizing melodic lines from low level data composed of\nspectrograms has been proposed for the estimation of the\nf0. For this DNN, the performance evaluation shows a rel-\natively good generalization (when compared to a reference\nsystem) on two different test datasets and an increase of ro-\nbustness to western music recordings that tend to be repre-\nsentative of the current music industry productions. While\nfor these experiments the systems have been learned from\na relatively low amount of data, the robustness, particu-\nlarly for the task of V AD, could very likely be improved\nby increasing the number of training examples.\nP R F FA VA OA00.10.20.30.40.50.60.70.80.91(a) iKala test dataset\nP R F FA VA OA00.10.20.30.40.50.60.70.80.91\n(b) MedleyDB test dataset\nFigure 7 : V oicing detection and overall performance of the\nproposed system for iKala and MedleyDB test datasets.\n7. REFERENCES\n[1] R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Can-\nnam, and J. Bello. MedleyDB: A multitrack dataset for\nannotation-intensive MIR research. In Proc. of the 15th\nInt. Society for Music Information Retrieval (ISMIR)\nConference , October 2014.\n[2] R. M. Bittner, J. Salamon, S. Essid, and J. P. Bello.\nMelody extraction by contour classiﬁcation. In Proc.\nof the 16th Int. Society for Music Information Retrieval\n(ISMIR) Conference , October 2015.\n[3] T.-S. Chan, T.-C. Yeh, Z.-C. Fan, H.-W. Chen, L. Su,\nY .-H. Yang, and R. Jang. V ocal activity informed\nsinging voice separation with the ikala dataset. In Proc.\nof IEEE Int. Conf. on Acoustics, Speech and Signal\nProcessing (ICASSP) , pages 718–722, April 2015.\n[4] J.-L. Durrieu, G. Richard, and B. David. An iterative\napproach to monaural musical mixture de-soloing. In\nProc. of IEEE Int. Conf. on Acoustics, Speech and Sig-\nnal Processing (ICASSP) , pages 105–108, April 2009.\n[5] D. P. W. Ellis and G. E. Poliner. Classiﬁcation-based\nmelody transcription. Machine Learning , 65(2):439–\n456, 2006.\n[6] D. FitzGerald and M. Gainza. Single channel vo-\ncal separation using median ﬁltering and factorisa-\ntion techniques. ISAST Trans. on Electronic and Signal\nProcessing , 4(1):62–73, 2010.742 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016[7] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRwc music database: Popular, classical, and jazz music\ndatabases. In Proc. of the 3rd Int. Society for Music\nInformation Retrieval (ISMIR) Conference , pages 287–\n288, October 2002.\n[8] A. Graves, A.-R. Mohamed, and G. Hinton. Speech\nrecognition with deep recurrent neural networks. In\nProc. of IEEE Int. Conf. on Acoustics, Speech and\nSignal Processing (ICASSP) , pages 6645–6649, May\n2013.\n[9] K. Han and DL. Wang. Neural network based\npitch tracking in very noisy speech. IEEE/ACM\nTrans. on Audio, Speech, and Language Processing ,\n22(12):2158–2168, October 2014.\n[10] C.-L. Hsu and J.-S. R. Jang. On the improvement of\nsinging voice separation for monaural recordings using\nthe mir-1k dataset. IEEE Trans. on Audio, Speech, and\nLanguage Processing , 18(2):310–319, 2010.\n[11] S. Jo, S. Joo, and C. D. Yoo. Melody pitch estima-\ntion based on range estimation and candidate extrac-\ntion using harmonic structure model. In Proc. of IN-\nTERSPEECH , pages 2902–2905, 2010.\n[12] B. S. Lee and D. P. W. Ellis. Noise robust pitch tracking\nby subband autocorrelation classiﬁcation. In Proc. of\nINTERSPEECH , 2012.\n[13] S. Leglaive, R. Hennequin, and R. Badeau. Singing\nvoice detection with deep recurrent neural networks. In\nProc. of IEEE Int. Conf. on Acoustics, Speech and Sig-\nnal Processing (ICASSP) , pages 121–125, April 2015.\n[14] M. Mauch and S. Ewert. The audio degradation tool-\nbox and its application to robustness evaluation. In\nProc. of the 14th Int. Society for Music Information Re-\ntrieval (ISMIR) Conference , November 2013.\n[15] A. Mesaros and T. Virtanen. Automatic alignment of\nmusic audio and lyrics. In Proc. of 11th Int. Conf. on\nDigital Audio Effects (DAFx) , September 2008.\n[16] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon,\nO. Nieto, D. Liang, and D. P. W. Ellis. mir eval: a\ntransparent implementation of common MIR metrics.\nInProc. of the 15th Int. Society for Music Information\nRetrieval (ISMIR) Conference , October 2014.\n[17] M. Ryyn ¨anen and A. P. Klapuri. Automatic transcrip-\ntion of melody, bass line, and chords in polyphonic mu-\nsic.Computer Music Journal , 32(3):72–86, 2008.\n[18] M. Ryyn ¨anen, T. Virtanen, J. Paulus, and A. Kla-\npuri. Accompaniment separation and karaoke applica-\ntion based on automatic melody transcription. In Proc.\nof the IEEE Int. Conf. on Multimedia and Expo , pages\n1417–1420, April 2008.[19] J. Salamon, E. G ´omez, D. P. W. Ellis, and G. Richard.\nMelody extraction from polyphonic music signals. Ap-\nproaches, applications, and challenges. IEEE Signal\nProcessing Magazine , 31(2):118–134, March 2014.\n[20] J. Salamon, B. Rocha, and E. G ´omez. Musical genre\nclassiﬁcation using melody features extracted from\npolyphonic music. In Proc. of IEEE Int. Conf. on\nAcoustics, Speech and Signal Processing (ICASSP) ,\npages 81–84, March 2012.\n[21] J. Salamon, J. Serr `a, and E. G ´omez. Tonal representa-\ntions for music retrieval: From version identiﬁcation to\nquery-by-humming. Int. Jour. of Multimedia Informa-\ntion Retrieval, special issue on Hybrid Music Informa-\ntion Retrieval , 2(1):45–58, 2013.\n[22] H. Tachibana, T. Ono, N. Ono, and S. Sagayama.\nMelody line estimation in homophonic music au-\ndio signals based on temporal-variability of melodic\nsource. In Proc. of IEEE Int. Conf. on Acoustics,\nSpeech and Signal Processing (ICASSP) , pages 425–\n428, March 2010.\n[23] C. H. Wong, W. M. Szeto, and K. H. Wong. Automatic\nlyrics alignment for Cantonese popular music. Multi-\nmedia Systems , 4-5(12):307–323, 2007.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 743"
    },
    {
        "title": "Analysing Scattering-Based Music Content Analysis Systems: Where&apos;s the Music?.",
        "author": [
            "Francisco Rodríguez-Algarra",
            "Bob L. Sturm",
            "Hugo Maruri-Aguilar"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1414724",
        "url": "https://doi.org/10.5281/zenodo.1414724",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/146_Paper.pdf",
        "abstract": "Music content analysis (MCA) systems built using scat- tering transform features are reported quite successful in the GTZAN benchmark music dataset. In this paper, we seek to answer why. We first analyse the feature extraction and classification components of scattering-based MCA systems. This guides us to perform intervention experi- ments on three factors: train/test partition, classifier and recording spectrum. The partition intervention shows a de- crease in the amount of reproduced ground truth by the resulting systems. We then replace the learning algorithm with a binary decision tree, and identify the impact of spe- cific feature dimensions. We finally alter the spectral con- tent related to such dimensions, which reveals that these scattering-based systems exploit acoustic information be- low 20 Hz to reproduce GTZAN ground truth. The source code to reproduce our experiments is available online. 1",
        "zenodo_id": 1414724,
        "dblp_key": "conf/ismir/Rodriguez-Algarra16",
        "content": "ANALYSING SCATTERING-BASED MUSIC CONTENT ANALYSIS\nSYSTEMS: WHERE’S THE MUSIC?\nFrancisco Rodr ´ıguez-Algarra\nCentre for Digital MusicBob L. Sturm\nCentre for Digital MusicHugo Maruri-Aguilar\nSchool of Mathematical Sciences\nQueen Mary University of London, U.K.\n{f.rodriguezalgarra, b.sturm, h.maruri-aguilar }@qmul.ac.uk\nABSTRACT\nMusic content analysis (MCA) systems built using scat-\ntering transform features are reported quite successful in\ntheGTZAN benchmark music dataset. In this paper, we\nseek to answer why. We ﬁrst analyse the feature extraction\nand classiﬁcation components of scattering-based MCA\nsystems. This guides us to perform intervention experi-\nments on three factors: train/test partition, classiﬁer and\nrecording spectrum. The partition intervention shows a de-\ncrease in the amount of reproduced ground truth by the\nresulting systems. We then replace the learning algorithm\nwith a binary decision tree, and identify the impact of spe-\nciﬁc feature dimensions. We ﬁnally alter the spectral con-\ntent related to such dimensions, which reveals that these\nscattering-based systems exploit acoustic information be-\nlow 20 Hz to reproduce GTZAN ground truth. The source\ncode to reproduce our experiments is available online.1\n1. INTRODUCTION\nMusic content analysis (MCA) systems trained and tested\nin [2] reproduce a large amount of the ground truth of\nthe benchmark music dataset GTZAN [18], and are among\nthe “best” reported in the literature [14]. They use sup-\nport vector machines (SVM) classiﬁers trained on fea-\ntures extracted from audio by the scattering transform , a\nnon-linear spectrotemporal modulation representation us-\ning a cascade of wavelet transforms [8]. The mathemati-\ncal derivation of the scattering transform enforces invari-\nances to local time and frequency shifts, which is a desir-\nable property for music classiﬁcation tasks. Scattering fea-\ntures are considered to have perceptual relevance [2], and\ncan be related to modulation features [5]. Such features\nare potentially useful for timbre-related music classiﬁca-\ntion tasks, such as instrument recognition [11], or genre\nrecognition [7].\nReproducing the ground truth of a dataset does not nec-\nessarily reﬂect the ability of a system to address a particu-\n1https://code.soundsoftware.ac.uk/projects/scatter-analysis\nc/circlecopyrtFrancisco Rodr ´ıguez-Algarra, Bob L. Sturm, Hugo\nMaruri-Aguilar. Licensed under a Creative Commons Attribution 4.0 In-\nternational License (CC BY 4.0). Attribution: Francisco Rodr ´ıguez-\nAlgarra, Bob L. Sturm, Hugo Maruri-Aguilar. “Analysing Scattering-\nbased Music Content Analysis Systems: Where’s the Music?”, 17th In-\nternational Society for Music Information Retrieval Conference, 2016.\nω\"\"\n•Acoustic information below 20 Hz (outside the lim-its of human hearing) both without and with modu-lations (feature b and c vectors)–enormous difference of FoM between trainingregimens–it is likely that the faults of the dataset providesigniﬁcant amount of identifying informationto most of the labels.–(“rock” particularly relevant?)–there appears to be little additional discimina-tive information using the 1.6 Hz modulationof rectiﬁed FB1 bands.•Acoustic information above 4186 Hz–without using the modulation, the FoM are notas good as those from using acoustic informa-tion below 20 Hz.–including the modulations makes a remarkabledifference in the FoM.–the FoM for “rock” appears to have becomevery poor now.•Combining all feature dimensions from acoustic in-formation below 20 Hz and above 4186 Hz.–(Rock recovers partly)3.3 Filtering InterventionAn oracle intervention attempts to make a systemSbe-have in certain ways for a test datasetD, by applying atransformationTto the input signals. In our case,Sisa Support Vector Machines classiﬁer trained with differentscattering feature sets (a - f, as deﬁned in Table 1) com-puted from the excerpts in the train partition ofGTZANunder condition (ii) of Section 3.1.D, thus, is the testpartition ofGTZANunder the same condition. The trans-formationTwe apply is a time-invariant ﬁltering basedon near-perfect reconstruction ﬁlters [3].Indeﬂationmode, the oracle attempts to make the sys-tem’s performance consistent with a system randomly se-lecting labels in a procedure as follows [3]:1.Identify all recordings inDthatSmaps “cor-rectly”.2.Create a transformationT.3.ApplyTto all recordings found in (1).4.HaveSmap transformed recordings.5.Find the recordings thatSmaps “correctly”.6.For each recording in (1) thatSystemnow maps“correctly” in (5), replace it inDwith its trans-formed version.7.Return to (1); repeat until performance ofSis con-sistent with random Figures-of-Merit, or a maxi-mum number of iterations is reached.If we see the number of errorsSmakes increases notably,we conclude that its performance strongly relies upon thefactors transformed byT.Figure 3 shows our results from 30 deﬂation steps forfeature sets (a-f). The ﬁrst step corresponds to the erroron the “original” test set. We observe two different be-haviours. Error rates in feature sets (a, b) increase rapidlyin a similar fashion, while scattering-based feature setswith more than one layer (c-f) show more robustness tothe effects of these ﬁlters. Table 4 summarises the over-all changes. These results suggest that features generatedby deeper layers of the scattering transform are capturingcharacteristics other than the short-term spectrum of theaudio signal.\n15101520253000.10.20.30.40.50.60.70.80.91\nDeflation StepError RateabecdfFigure 3. Error rate over 30 steps of ﬁltering-based oracledeﬂation for feature sets (a - f).Set Features Orig. ER Final ERa\u0000-MFCC, 740 ms 0.220 0.784b Time Scattering,l=1 0.208 0.684c Time Scattering,l=2 0.120 0.416d Time & Freq Scattering,l=2 0.128 0.368e Time & Freq Scattering,l=2, Adap. Q10.144 0.440f Time Scattering,l=3 0.164 0.360Table 4. Overall changes in computed error rate (ER) in30 steps of ﬁltering-based oracle deﬂation for feature sets(a - f) (see Fig. 3).We also check how the performance of these systems isaffected by the attenuation of the transformationTfor twoparticularGTZANclasses:discoandmetal. We ﬁrstdeﬁne a mean attenuation level for each ﬁlter with stepsof 0.5 dB, from 0.5 dB to 9 dB within±0.01 dB. Then,for each feature set and mean attenuation level, we per-form 10 iterations of a deﬂation process (30 steps each). Inthis way, we attempt to minimise the effect of potentiallydestructive ﬁlters (e.g., completely removing low frequen-cies) in the average performance for each feature set andattenuation level. Figure 4 shows the results we obtain forfeature sets (a - f) considering bothdiscoandmetalclasses. We observe that systems using feature sets (a, b)behave similarly, increasing their error rates rapidly whenwe augment the attenuation levels. This suggests that SVMmusic classiﬁcation systems with a single layer of scatter-ing transform features (l=1) are highly reliant onspec-trato reproduce the ground truth ofGTZAN. Behaviour ofdeeper layers (l>1), on the other hand, is more robust toﬁltering transformations.Figure 1 . Schematic representation of a music content\nanalysis (MCA) system [16].\nlar task [12–15]. In this paper, we analyse scattering-based\nMCA systems to determine why they reproduce so much\nGTZAN ground truth. Our approach involves system anal-\nysis and experimental interventions. System analysis in-\nvolves decomposing an MCA system into its components\nto understand how each contributes to its overall behaviour.\nOur system analysis of scattering-based MCA systems in\nSec. 2 shows that they use some information from inaudi-\nble frequencies, i.e., below 20 Hz [4]. Experimental in-\nterventions, on the other hand, involve testing hypotheses\nabout what a system is actually doing by altering some fac-\ntor to see how system behaviour changes. In Sec. 3, we per-\nform intervention experiments to conﬁrm that scattering-\nbased MCA systems exploit information below 20 Hz to\nreproduce GTZAN ground truth. When we attenuate that\ninformation, ground truth reproduction decreases.\nWe conceive our work here as a case study within the\ndevelopment of an improved systematic methodology for\nevaluating MCA systems. This is one challenge posed in\nthe Music Information Retrieval (MIR) Roadmap [10], and\nexempliﬁes the pipeline in [15]. In Sec. 4 we discuss the\nimplications of our results, and suggest how they might be\nintegrated in a general evaluation framework.\n2. SYSTEM ANALYSIS\nUsing the formalism of [16], an MCA system Smaps\na recording universe RΩ— a particular realisation of an\nintangible music universe Ω— to a description universe\nSV,A. As shown in Fig. 1, this mapping is decomposed\ninto two stages. First, a feature extractor EmapsRΩto a\nfeature universeSF,A/prime; then, a classiﬁer CmapsSF,A/primeto\nSV,A.\nThe environment and deﬁnition of the MCA systems\nin [2] are as follows. RΩconsists of time-domain sig-344ID F Feature Description\na R252Mel-frequency spectrogram (84 coefﬁcients, 740-\nms frames, 50% overlap), concatenated with ﬁrst-\nand second-order time derivatives over the se-\nquence of feature vectors2\nb R85First-order ( l= 1) time-scattering features (effec-\ntive sampling rate 2.7 Hz)\nc R747Second-order ( l= 2) time scattering features (ef-\nfective sampling rate 2.7 Hz)\ndR1574First-order time-frequency scattering features\neR1907First-order time-frequency-adaptive scattering\nfeatures\nfR2769Third-order ( l= 3) time scattering features (ef-\nfective sampling rate 2.7 Hz)\nTable 1 . Description of SF,A/prime(feature universes) used\nin [2].A/primepermits only vector sequences of length 80.\nnals of duration about 30seconds uniformly sampled at\nFs= 22050 Hz (the sampling rate of GTZAN ).SV,Ais the\nset of the 10 GTZAN labels.SF,A/primeis a space consisting\nof sequences of 80 elements of a vector vocabulary F. All\nMCA systems trained in [2] use the same SV,A, the same\nlearning method to build the classiﬁers, but different SF,A/prime.\nMore speciﬁcally, the semantic rules A/primeare the same for all\nsystems (sequences of length 80), with only a difference in\nthe feature vocabulary, F. Table 1 describes the six differ-\nentSF,A/primeappearing in [2].\nWe now analyse the two components of the systems\nbuilt using ﬁrst- (“ bvectors”) and second-layer (“ cvec-\ntors”) time scattering features (see Table 1). Systems built\nusing fvectors can be understood as a further iteration\nof the process described here. In addition to that, the in-\nclusion of frequency-scattering features ( dandevectors),\ndoes not affect our conclusions.\n2.1 Feature extractors of bandc(EbandEc)\nThe feature extraction procedure begins by ﬁrst extending\na recording to be of length 221= 2097152 samples us-\ning what is referred to in the code as “padding” by “sym-\nmetric boundary condition with half-sample symmetry”:\ntheN≈5·217samples ofr∈ R Ωare concatenated\nwith the same but time-reversed, then concatenated with\nits ﬁrst∼50000 samples, and its last ∼50000 samples,\nand ﬁnally the time-reversed samples again. This “padded”\nsignal is then transformed into the frequency domain by\nthe FFT. The complex spectrum is then multiplied by the\nmagnitude response of each of 85 ﬁlters of a ﬁlterbank\ndesigned using a scaling function and dilations of a one-\ndimensional Gabor mother wavelet with 8 wavelets per oc-\ntave up to a maximum dilation of 273/8. (The bandwidth\nof the lowest 11 bands are made constant.) Figure 2(a)\nshows the magnitude responses of the bands of this ﬁl-\nterbank (FB1). Each spectrum product is then reshaped\n— equivalent to decimation in the time-domain —, trans-\nformed to the time domain by the inverse FFT, and then\nwindowed to the portion corresponding to rin the padded\nsequences.\nNext, the time-series output of each band of FB1 is rec-\ntiﬁed, padded using the same padding method as above,\n2[2] does not actually compute ∆- and ∆-∆-MFCCs, but instead\ncyclically time-shifts the sequence of MFCCs ahead and behind by one\nframe so that the classiﬁer has ﬂexibility in learning a transformation.\nFrequency (Hz)1 5 10 20 50 100 200 500 1k 2k 5k 10kMagnitude\n00.20.40.60.811.21.4(a) Filterbank 1 (FB1)\nFrequency (Hz)1 5 10 20 50 100 200 500 1k 2k 5k 10kMagnitude\n00.20.40.60.811.21.4\n(b) Filterbank 2 (FB2)\nFigure 2 . Magnitude responses of the bands in the ﬁlter-\nbanks for scattering feature IDs bandc.\ntransformed into the frequency domain by the FFT, and\nthen multiplied by the magnitude response of each of 25\nﬁlters of a ﬁlterbank designed with a scaling function\nand dilations of a one-dimensional Morlet mother wavelet,\nwith 2 wavelets per octave up to a maximum dilation of\n223/2. Figure 2(b) shows the magnitude responses of the\nbands of this ﬁlterbank (FB2). Each FB2 spectrum prod-\nuct is then reshaped — again, equivalent to decimation in\nthe time-domain —, transformed to the time domain by\nthe inverse FFT, and then windowed corresponding to the\noriginal forward-going sequence in the padded sequences\n(length 80). Finally, Ebretains only those values related to\nthe DC ﬁlter of FB2, and computes the natural log of all\nvalues (added with a small positive value). This results in\n80bvectors. For creating 80 cvectors, Ectakes those FB2\ntime-series outputs with non-negligible energy,3“renor-\nmalises” each non-zero frequency band (to account for en-\nergy captured in the ﬁrst layer of scattering coefﬁcients),\nand takes the natural log of all values (added with a small\npositive value).\nFigure 3 shows the relationship between the dimensions\nofbandcvectors and the centre frequencies of FB1 and\nFB2 bands. For display purposes, the bottom-most row is\nfrom the scaling function of FB2. The 85 dimensions of a\nbvector are at bottom, with dimensions [1, 75:85] coming\nfrom FB1 bands with centre frequencies below 20 Hz. Di-\nmensions [1, 75:85, 737:747] of a cvector come from such\nbands. Dimensions [2:12] of a bvector, and [2:12, 86:268]\nof acvector, are from FB1 bands with centre frequencies\nabove 4186 Hz (pitch C8).\n3In fact, not every rectiﬁed FB1 band output is ﬁltered by all FB2\nbands because ﬁltering by FB1 will remove all frequencies outside its\nband.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 345FB 1 band center frequency (Hz)1 5 10 20 50 100 200 500 1k 2k 5k 10kFB 2 band center frequency (Hz)151020501002005001k\n1 2345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182 83 84 8586\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513514\n515\n516\n517\n518\n519\n520\n521\n522\n523524\n525\n526\n527\n528\n529\n530\n531\n532\n533534\n535\n536\n537\n538\n539\n540\n541\n542\n543544\n545\n546\n547\n548\n549\n550\n551\n552\n553554\n555\n556\n557\n558\n559\n560\n561\n562563\n564\n565\n566\n567\n568\n569\n570\n571572\n573\n574\n575\n576\n577\n578\n579\n580581\n582\n583\n584\n585\n586\n587\n588\n589590\n591\n592\n593\n594\n595\n596\n597598\n599\n600\n601\n602\n603\n604\n605606\n607\n608\n609\n610\n611\n612\n613614\n615\n616\n617\n618\n619\n620\n621622\n623\n624\n625\n626\n627\n628629\n630\n631\n632\n633\n634\n635636\n637\n638\n639\n640\n641\n642643\n644\n645\n646\n647\n648\n649650\n651\n652\n653\n654\n655656\n657\n658\n659\n660\n661662\n663\n664\n665\n666\n667668\n669\n670\n671\n672\n673674\n675\n676\n677\n678679\n680\n681\n682\n683684\n685\n686\n687\n688689\n690\n691\n692\n693694\n695\n696\n697698\n699\n700\n701702\n703\n704\n705706\n707\n708\n709710\n711\n712713\n714\n715716\n717\n718719\n720\n721722\n723724\n725726\n727728\n729730731732733734735736737738739740741742743744 745 746 747[1, 75:85, 737:747] \nb vector [1:85]Figure 3 . Relationship of bandcvector dimensions to FB1 and FB2 band centre frequencies. Dimensions [1, 75:85] of b\nvectors, and [1, 75:85, 737:747] of cvectors, are from bands with centre frequencies below 20 Hz.\n2.2 Classiﬁer C\nDeﬁne the number of support vectors of a trained SVM as\n|SV|. Classiﬁers Cof the MCA systems in [2] are charac-\nterised by a set of support vectors V∈F|SV|, a Gaussian\nkernel parameter γ, a weight matrix W∈R|SV|×45, and\na bias vector ρ∈R45. (45 is the number of pair-wise\ncombinations of the 10 elements in SV,A, i.e., label 1 vs.\nlabel 2, label 1 vs. label 3, etc.) CmapsSF,A/primetoSV,A\nby majority vote from the individual mappings of all ele-\nmentsfj∈Fof a sequence from r∈R Ωby an SVM\nclassiﬁer C/prime.C/prime, thus, maps FtoSV,Aby computing 45\npair-wise decisions by means of sign(WTe−γK(f)−ρ),\nwhereK(f)is a vector of squared Euclidean norm of dif-\nferences between fand allvj∈V.C/primethen mapsfto\nSV,Aby majority vote from the 45 pair-wise decisions.\nThe authors of [2] use LibSVM4to build C/primeusing a\nGaussian kernel with a subset of the feature vectors (down-\nsampled by 2). They optimise the SVM parameters by grid\nsearch and 5-fold cross-validation on a training set. Lib-\nSVM uses a 1 vs. 1 strategy to deal with multiclass classi-\nﬁcation problems, so each support vector receives a weight\nfor each of the nine possible pair-wise decisions involving\nthe class associated with the support vector. The matrix W\ncontains weights associated with all possible 45 pair-wise\ndecisions. The training of the SVM also generates the vec-\ntorρcontaining a bias term for each pair-wise decision.\n3. SEARCHING FOR THE MUSIC\nWe now report three intervention experiments we design\nto answer our question: how are scattering-based MCA\nsystems reproducing so much GTZAN ground truth? We\nadapt the code used for the experiments in [2] (available\nonline5). The experiments performed in [2] do not con-\nsider the known faults of GTZAN [14], so in Sec. 3.1 we\nreproduce them using two different train-test partitioning\nconditions. We observe a decrease in performance, but\nnot as dramatic as seen in past re-evaluations [14]. In\nSec. 3.2, we replace the classiﬁer Cwith a binary deci-\nsion tree (BDT) trained with different subsets of scattering\n4https://www.csie.ntu.edu.tw/ ˜cjlin/libsvm/\n5http://www.di.ens.fr/data/softwarefeatures. This leads us to identify the impact of speciﬁc\nfeature dimensions. The analysis of the feature extractor in\nSec. 2.1 allows us to relate such dimensions with spectral\nbands of the audio signal. In Sec. 3.3, we alter the spec-\ntral content of the test recordings and observe how GTZAN\nground truth reproduction changes. This reveals that these\nscattering-based MCA systems exploit acoustic informa-\ntion below 20 Hz.\n3.1 Partitioning intervention\nThe benchmark music dataset GTZAN contains faults (e.g.,\nrepetitions) that can affect the amount of ground truth that\nan MCA system reproduces [14]. This amount often de-\ncreases when we train and test it using a “fault-ﬁltered”\npartition of GTZAN , as done in [6, 14]. This suggests that\nthe faults in the dataset are related to the amount of ground\ntruth reproduced by a system.\nWhile [2] evaluates the performance of the scattering-\nbased MCA systems using 10-fold stratiﬁed cross-\nvalidation, we employ two different hold-out train-test par-\ntitioning conditions. The ﬁrst is RANDOM , which mimics\nthe train-test procedure in [2]: we randomly select 75%\nof the recordings of each label for the training set, leav-\ning the remaining 25% for the testing set. The second\nisFAULT , which is the “fault-ﬁltered” partitioning proce-\ndure in [6], with the training and validation sets merged.\nThis partitioning condition considers various problems of\nthe dataset: we remove 70 replicated or distorted record-\nings [14]; we then assign by hand 640 recordings to the\ntraining set and the remaining 290 to the testing set, avoid-\ning repetition of artists across partitions [9]. Due to mem-\nory constraints, we decrease by a factor of 4 the number of\nscattering features in the pre-computation of the Gaussian\nkernel of the SVM. This reduces the computational cost\nwithout sacriﬁcing much performance.6\nTable 2 shows the normalised accuracies (mean recalls)\nof our systems along with those reported in [2] for the six\nfeatures described in Table 1. We see the differences be-\ntween the results in [2] and ours in RANDOM are small,\nand most of them within reason considering the standard\n6We acknowledge Joakim And ´en and Vincent Lostanlen for their valu-\nable advice.346 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Original GTZAN recordings Attenuated [0, 20] Hz\nIDReported in [2] RANDOM FAULT RANDOM FAULT\na 82.0±4.2 78.00 53.29 39.20 30.09\nb 80.9±4.5 79.20 54.96 31.60 22.42\nc 89.3±3.1 88.00 66.46 50.80 44.47\nd 90.7±2.4 87.20 68.49 62.40 55.11\ne 91.4±2.2 85.60 68.61 64.80 44.52\nf 89.4±2.5 83.60 68.32 64.80 53.16\nTable 2 . Normalised accuracies (mean recall) in GTZAN\ndataset obtained by scattering-based MCA systems in [2]\nand our systems using RANDOM andFAULT partition-\ning conditions, trained and tested with the original GTZAN\nrecordings (left) and versions ones with information below\n20 Hz (right) attenuated (see Sec. 3.3).\nNRANDOM FAULT\n1 52.99 51.82\n2 65.05 65.27\n3 71.42 71.62\n4 75.53 75.80\n5 79.48 79.74\nTable 3 . Cumulative percentage of variance captured by\nthe N highest principal components of bvectors in the\ntraining sets of RANDOM andFAULT partitioning condi-\ntions.\ndeviations reported in [2]. In RANDOM , we see an in-\ncrease of accuracy when we include second-order scatter-\ning features, i.e., btoc. We ﬁnd that adding depth to the\nfeatures, however, does not increase further the amount of\nground truth reproduced, and even decreases it when we\ninclude third-order features ( ctof), contrary to what is\nreported in [2]. Most importantly, we observe a consider-\nable decrease in the amount of ground truth reproduced by\nall systems between RANDOM andFAULT . Figures 4(a)\nand 4(b) show the ﬁgure-of-merit (FoM) of the systems\ntrained and tested in RANDOM andFAULT withbvec-\ntors, respectively. We see recalls and F-measures of every\nlabel decrease except for “classical”, which increase.\nFigure 5 shows the eigenvectors of the ﬁrst ﬁve prin-\ncipal components of ﬁrst-layer scattering features ( bvec-\ntors) in the training sets of RANDOM andFAULT . (Table 3\nshows the percentage of variance captured by the ﬁrst N\nprincipal components.) We see large changes in the low-\nest and highest dimensions of the fourth component. This\nsuggests that these dimensions of the scattering features\ncapture information that differs in both training sets, which\nmay play a role in the performance differences we observe.\nThe characteristics of CandC/prime, however, make it difﬁcult\nto determine the inﬂuence that each individual feature di-\nmension (or subset of dimensions) has in the overall per-\nformance of a system. For this reason, in Section 3.2 we\nreplace the SVM by a binary decision tree (BDT) classi-\nﬁer, which allows an easier interpretation of SF,A/primeand its\nrelationship withSV,A.\n3.2 Classiﬁer intervention\nSVM classiﬁers generate decision boundaries in multi-\ndimensional spaces. While this can beneﬁt prediction, it\nhampers their interpretability. In our case, this implies that\n(a)RANDOM\n(b)FAULT\n(c)RANDOM , information below 20 Hz attenuated\nFigure 4 . Figure-of-merit (FoM) obtained with bvec-\ntors by SVM systems trained and tested in (a) RANDOM\nand (b) FAULT (Sec. 3.1), as well as (c) SVM trained in\nRANDOM and tested in recordings with content below 20\nHz attenuated (Sec. 3.3). Column is ground truth, row is\nprediction. Far-right column is precision, diagonal is re-\ncall, bottom row is F-score, lower right-hand corner is nor-\nmalised accuracy. Off-diagonals are confusions.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 347−0.8−0.40.00.40.8\n020406080Feature DimensionWeight5 4 3 2 1 (a)RANDOM\n−0.8−0.40.00.40.8\n020406080Feature DimensionWeight5 4 3 2 1 \n(b)FAULT\nFigure 5 . Eigenvectors of the ﬁrst ﬁve principal compo-\nnents (labelled) of bvectors in the training sets of (a) RAN-\nDOM (79.74% of variance captured) and (b) FAULT\n(79.48% of variance captured) partitioning conditions.\nID RANDOM FAULT\na 72.80 45.70\nb 71.60 42.35\nc 80.00 49.91\nd 79.20 46.81\ne 79.60 44.77\nf 79.20 46.48\nTable 4 . Normalised accuracies (mean recall) in GTZAN\nfor MCA systems built using binary decision tree classi-\nﬁers using features described in Table 1, trained and tested\nwith RANDOM andFAULT partitioning conditions.\nthe relevance of each individual dimension of the scatter-\ning feature vectors gets blurred. We now replace the SVM\nclassiﬁers used in [2] by BDT, consisting of a set of rules\ndeﬁned by linear splits of the feature space one dimension\nat a time. BDT are considered to be among the easiest\nlearning methods to construct and understand [1], at the\ncost of potentially less accuracy.\nTable 4 shows the normalised accuracies we obtain with\nMATLAB’s BDT classiﬁer,7for the two partitioning con-\nditions deﬁned in Sec. 3.1, using the different feature vec-\ntors described in Table 1. Clearly, there exists a major\ndifference between the two training conditions, similar to\nwhat Table 2 shows for SVM. We see a decrease of around\n8 percentage points in the amount of ground truth repro-\nduced by each of the BDT systems in RANDOM compared\nto the SVM systems in Table 2. On the other hand, when\ntraining the BDT systems in FAULT , we observe falls in\nperformance with respect to RANDOM at least as large as\nthose reported in Table 2. This suggests that the amount of\n7http://uk.mathworks.com/help/stats/classificationtree-class.\nhtml\n0.10.20.3\n0 20 40 60 80\nFeature DimensionReproduced Ground −Truth\nCondition FAULT RANDOMFigure 6 . Proportion of ground truth reproduced by BDT\nclassiﬁers trained with single dimensions of bvectors in\nRANDOM andFAULT partitioning conditions.\nground truth reproduced by the systems in both conditions\ndiffer due to distinct information being captured by the fea-\nture extractors, and not necessarily as an effect of the clas-\nsiﬁcation algorithm. The training of the SVM classiﬁer,\nif anything, appears to mitigate the potential performance\ndecrease in c-fvectors.\nWe now explore differences in reproduced GTZAN\nground truth between partitioning conditions by each di-\nmension of the scattering features individually. We thus\ntrain and test BDTs with each of the 85 dimensions of b\nvectors in both RANDOM andFAULT . Figure 6 shows the\nclassiﬁcation accuracies we obtain. We see clear differ-\nences between conditions, especially in dimensions iden-\ntiﬁed in Sec. 2.1 as belonging to bands close to or outside\nthe limits of normal human hearing (namely [1, 70:85]).\nWe also explore how much ground truth BDT systems\ncan reproduce using solely information below 20 Hz. BDT\nsystems trained with dimensions 1 and 75:85 of bvec-\ntors achieve a 60.40% of normalised accuracy in RAN-\nDOM , which is close to the performance originally re-\nported in [18] for the GTZAN dataset. In FAULT , how-\never, the normalised accuracy drops to 22.47%. Adding di-\nmensions 737:747 from cvectors (modulations from FB2\nof information below 20 Hz) only marginally increases\nthe performance in both conditions. These results suggest\nthat our scattering-based MCA systems could be exploiting\nacoustic information from below 20 Hz. We next perform\ninterventions to test this hypothesis.\n3.3 Filtering intervention\nWe now see how the amount of ground truth reproduced\nby a scattering-based an MCA system changes when we\nattenuate acoustic information below 20 Hz. We thus ap-\nply a ﬁfth-order Butterworth high-pass ﬁlter to attenuate\nall frequencies below 20 Hz by at least 30 dB to the test\nrecordings in both RANDOM andFAULT . We check that\nwe do not perceive differences between ﬁltered and non-\nﬁltered versions. We then use the SVM systems in Sec. 3.1\nto predict labels in the ﬁltered test recordings. The two\nright-most columns of Table 2 show the normalised accu-\nracies we obtain. We clearly see that the ﬁgures drop from\nthose reported in Sec. 3.1. In particular, the decrease of\naccuracy using bvectors in RANDOM is close to 50 per-\ncentage points, while that using features generated from348 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016deeper scattering layers is smaller but still notable. Sys-\ntems trained in FAULT also suffer in the performance mea-\nsured.\nFigure 4(c) shows the FoM obtained by an SVM trained\ninFAULT withbvectors and tested in high-pass ﬁltered\nrecordings. We note that the changes in FoM between\nFigs. 4(a) and 4(c) do not always match those reported in\nSec. 3.1 between Figs. 4(a) and 4(b). More precisely, re-\ncall and F-measure decrease instead of increase in “classi-\ncal”, and increase instead of decrease in “country”. This\nsuggests that partitioning and information below 20 Hz are\ndistinct factors affecting the amount of ground truth sys-\ntems reproduce, notwithstanding an interaction between\nthem as suggested by Figs. 5 and 6. Our results allow\nus to conclude that the scattering-based MCA systems\ntrained and tested in [2] beneﬁt from partitioning and ex-\nploit acoustic information below 20 Hz to reproduce a large\namount of GTZAN ground truth.\n4. DISCUSSION\nOur analysis in Sec. 2.1 shows how ﬁrst- and second-layer\ntime-scattering features relate to acoustic information. We\nsee that several dimensions of such features capture infor-\nmation at frequencies below 20 Hz, which is inaudible to\nhumans [4].\nWe ﬁnd in the intervention experiments in Secs. 3.1\nand 3.2 that partitioning affects the amount of GTZAN\nground truth scattering-based systems reproduce. Remov-\ning the known faults of the dataset and avoiding artist repli-\ncation across folds leads to a decrease in the FoM we ob-\ntain, but to a lesser extent than previous re-evaluations of\nother MCA systems [6, 14]. We also note that differences\nbetween the ﬁrst principal components of ﬁrst-layer time-\nscattering features lay mainly within the dimensions corre-\nsponding to frequency bands below 20 Hz.\nWhen we replace the SVM classiﬁer with a BDT\n(Sec. 3.2), we see differences in the amount of reproduced\nground truth similar to those we ﬁnd for SVM systems be-\ntween partitioning conditions. This suggests that the dis-\ntinct acoustic information the scattering features capture\ncauses differences in performance, regardless of the par-\nticular learning algorithm employed. Furthermore, we ﬁnd\nthat BDT systems trained with individual dimensions of\nﬁrst-layer time-scattering features reproduce an amount of\nGTZAN ground truth larger than that expected when se-\nlecting randomly. Again, we see differences between par-\ntitioning conditions, especially in the dimensions captur-\ning information below 20 Hz. Moreover, we reproduce\nalmost as much GTZAN ground truth as the one origi-\nnally reported in [18] by using a BDT trained in RANDOM\nwith only information below 20 Hz. This result suggests\nthat acoustic information below 20 Hz present in GTZAN\nrecordings may inﬂate the performance of MCA systems\ntrained and tested in the benchmark music dataset.\nOur system analysis in Sec. 2 and intervention experi-\nments in Sec. 3.1 and 3.2 point toward information present\nin frequencies below 20 Hz playing an important role in\nthe apparent success of the scattering-based MCA systemswe examine. The results of our experiments in Sec. 3.3\nclearly reveal that the amount of GTZAN ground truth\nSVM scattering-based systems reproduce decreases when\nwe attenuate that information in test recordings. This im-\nplies these systems are using inaudible information. We\nconclude that the scattering-based MCA systems in [2]\nexploit acoustic information not controlled by partition-\ning and below 20 Hz to reproduce a large amount of\nGTZAN ground truth. Machine music listening is an ex-\nciting prospect as it complements and even extends human\nabilities, but we dispute the relevance of acoustic infor-\nmation below 20 Hz to address the problem intended by\nGTZAN [18].\nThe results of our three intervention experiments sug-\ngest a complex relationship between the accuracy mea-\nsured of a system, the contribution of its feature extraction\nand machine learning components, and the conditions of\nthe training and testing dataset. We already know that the\nfaults and partitioning of GTZAN can have signiﬁcant ef-\nfects on an outcome, and that there is an interaction with\nthe components of a system [14,17]. Our experiments here\nshow for the systems we examine that acoustic information\nbelow 20 Hz can greatly affect an outcome, and that this\ninteracts with the components of a system and the dataset\npartitioning. This thus calls into question the interpretation\nof the results reported in [2] (column 2 of Table 2) as unbi-\nased estimates of system success. In future work, we will\nspecify more complex measurement models, e.g., [17].\nUnderstanding how and why a system works is essential\nto determine its suitability for a speciﬁc task, not to men-\ntion its improvement. Our work here demonstrates the use\nof system analysis and the intervention experiment to ad-\ndress this problem. For instance, our conclusions suggest\nmodifying the FB1 ﬁlterbank in the scattering features ex-\ntractor to avoid capturing information below 20 Hz. They\nalso suggest removing information below 20 Hz from any\nelement ofRΩas a pre-processing step before training an\nMCA system, if relevant.\n5. CONCLUSION\nIn this paper, we report several steps we followed to de-\ntermine what the scattering-based MCA systems reported\nin [2] have actually learned to do in order to reproduce\nthe ground truth of GTZAN . We show how performing sys-\ntem analysis guides our design of appropriate intervention\nexperiments. The results lead us to conclude that these\nMCA systems beneﬁt not only from the partitioning of the\ndataset, but also from acoustic information below 20 Hz.\nOur work here constitutes steps toward a holistic analy-\nsis of MCA systems — an action point for MIR evaluation\nidentiﬁed in [10]. Our ultimate goal is to help develop a\ngeneral MIR research pipeline that integrates system anal-\nysis and interventions, and is grounded in formal princi-\nples of statistical design of experiments, e.g., [3]. Such\na pipeline will provide a solid empirical foundation upon\nwhich to build machine music listening systems and tech-\nnologies [15].Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 3496. REFERENCES\n[1] E. Alpaydin. Introduction to Machine Learning . The\nMIT Press, Cambridge, MA, USA, 3rd edition, 2014.\n[2] J. And ´en and S. Mallat. Deep Scattering Spec-\ntrum. IEEE Transactions on Signal Processing ,\n62(16):4114–4128, 2014.\n[3] R. A. Bailey. Design of Comparative Experiments .\nCambridge University Press, 2008.\n[4] A. Chaudhuri. Fundamentals of Sensory Perception .\nOxford University Press, 2011.\n[5] T. Chi, P. Ru, and S. A. Shamma. Multiresolution Spec-\ntrotemporal Analysis of Complex Sounds. Journal of\nthe Acoustical Society of America , 118(2):887–906,\n2005.\n[6] C. Kereliuk, B. L. Sturm, and J. Larsen. Deep Learning\nand Music Adversaries. IEEE Transactions on Multi-\nmedia , 17(11):2059–2071, 2015.\n[7] C. Lee, J. Shih, K. Yu, and H. Lin. Automatic Mu-\nsic Genre Classiﬁcation Based on Modulation Spec-\ntral Analysis of Spectral and Cepstral Features. IEEE\nTransactions on Multimedia Multimedia , 11(4):670–\n682, June 2009.\n[8] S. Mallat. Group Invariant Scattering. Communications\non Pure and Applied Mathematics , LXV:1331–1398,\n2012.\n[9] E. Pampalk, A. Flexer, and G. Widmer. Improvements\nof Audio-Based Similarity and Genre Classiﬁcation.\nInProc. 6th International Society for Music Informa-\ntion Retrieval Conference (ISMIR’05) , pages 628–633,\nLondon, UK, September 2005.\n[10] X. Serra, M. Magas, E. Benetos, M. Chudy, S. Dixon,\nA. Flexer, E. G ´omez, F. Gouyon, P. Herrera, S. Jord ´a,\nO. Paytuvi, G. Peeters, H. V . Schluter, and G. Widmer.\nRoadmap for Music Information Research . The MIReS\nConsortium, 2013.\n[11] K. Siedenburg, I. Fujinaga, and S. McAdams. A Com-\nparison of Approaches to Timbre Descriptors in Music\nInformation Retrieval and Music Psychology. Journal\nof New Music Research , 45(1):27–41, January 2016.\n[12] B. L. Sturm. Classiﬁcation Accuracy Is Not Enough.\nJournal of Intelligent Information Systems , 41(3):371–\n406, 2013.\n[13] B. L. Sturm. A Simple Method to Determine if a Mu-\nsic Information Retrieval System Is a “Horse”. IEEE\nTransactions on Multimedia , 16(6):1636–1644, 2014.\n[14] B. L. Sturm. The State of the Art Ten Years After a\nState of the Art: Future Research in Music Information\nRetrieval. Journal of New Music Research , 43(2):147–\n172, 2014.[15] B. L. Sturm. Revisiting Priorities: Improving MIR\nEvaluation Practices. In Proc. 17th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR’16) , New York, NY , USA, August 2016.\n[16] B. L. Sturm, R. Bardeli, T. Langlois, and V . Emiya.\nFormalizing the Problem of Music Description. In\nProc. 15th International Society for Music Information\nRetrieval Conference (ISMIR’14) , pages 89–94, Taipei,\nTaiwan, October 2014.\n[17] B. L. Sturm, H. Maruri-Aguilar, B. Parker, and\nH. Grossman. The Scientiﬁc Evaluation of Music Con-\ntent Analysis Systems: Valid Empirical Foundations\nfor Future Real-World Impact. In Proc. ICML Machine\nLearning for Music Discovery Workshop , Lille, France,\nJuly 2015.\n[18] G. Tzanetakis and P. Cook. Musical Genre Classiﬁca-\ntion of Audio Signals. IEEE Transactions on Speech\nand Audio Processing , 10(5):293–301, 2002.350 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "An Analysis of Agreement in Classical Music Perception and its Relationship to Listener Characteristics.",
        "author": [
            "Markus Schedl",
            "Hamid Eghbal-Zadeh",
            "Emilia Gómez",
            "Marko Tkalcic"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417559",
        "url": "https://doi.org/10.5281/zenodo.1417559",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/260_Paper.pdf",
        "abstract": "We present a study, carried out on 241 participants, which investigates on classical music material the agreement of listeners on perceptual music aspects (related to emotion, tempo, complexity, and instrumentation) and the relation- ship between listener characteristics and these aspects. For the currently popular task of music emotion recognition, the former question is particularly important when defin- ing a ground truth of emotions perceived in a given music collection. We characterize listeners via a range of factors, including demographics, musical inclination, experience, and education, and personality traits. Participants rate the music material under investigation, i.e., 15 expert-defined segments of Beethoven’s 3rd symphony, “Eroica”, in terms of 10 emotions, perceived tempo, complexity, and num- ber of instrument groups. Our study indicates only slight agreement on most perceptual aspects, but significant cor- relations between several listener characteristics and per- ceptual qualities.",
        "zenodo_id": 1417559,
        "dblp_key": "conf/ismir/SchedlEGT16",
        "content": "AN ANALYSIS OF AGREEMENT IN CLASSICAL MUSIC PERCEPTION\nAND ITS RELATIONSHIP TO LISTENER CHARACTERISTICS\nMarkus Schedl, Hamid Eghbal-Zadeh\nJohannes Kepler University\nLinz, Austria\nfirstname.lastname@jku.atEmilia G ´omez\nUniversitat Pompeu Fabra\nBarcelona, Spain\nemilia.gomez@upf.eduMarko Tkal ˇciˇc\nFree University of Bozen–Bolzano\nItaly\nmarko.tkalcic@unibz.it\nABSTRACT\nWe present a study, carried out on 241 participants, which\ninvestigates on classical music material the agreement of\nlisteners on perceptual music aspects (related to emotion,\ntempo, complexity, and instrumentation) and the relation-\nship between listener characteristics and these aspects. For\nthe currently popular task of music emotion recognition,\nthe former question is particularly important when deﬁn-\ning a ground truth of emotions perceived in a given music\ncollection. We characterize listeners via a range of factors,\nincluding demographics, musical inclination, experience,\nand education, and personality traits. Participants rate the\nmusic material under investigation, i.e., 15 expert-deﬁned\nsegments of Beethoven’s 3rdsymphony, “Eroica”, in terms\nof 10 emotions, perceived tempo, complexity, and num-\nber of instrument groups. Our study indicates only slight\nagreement on most perceptual aspects, but signiﬁcant cor-\nrelations between several listener characteristics and per-\nceptual qualities.\n1. INTRODUCTION\nMusic has always been closely related to human emotion.\nIt can express emotions and humans can perceive and expe-\nrience emotions when listening to music, e.g., [10, 22, 29].\nIn a uses and gratiﬁcation analysis of why people listen\nto music [20], Lonsdale and North even identify emotion\nregulation as the main reason why people actively listen to\nmusic.\nHowever, little is known about the inﬂuence of individ-\nual listener characteristics on music perception (emotion\nand other aspects) and whether listeners agree on such as-\npects at all. The aim of this paper is therefore to gain a\nbetter understanding of the agreement on perceptual music\naspects and the relationship between perceptual music as-\npects and personal characteristics . To approach these two\nquestions, we present and analyze results of a web-based\nuser study involving 241 participants. We characterize lis-\nc/circlecopyrtMarkus Schedl, Hamid Eghbal-Zadeh, Emilia G ´omez,\nMarko Tkal ˇciˇc. Licensed under a Creative Commons Attribution 4.0 In-\nternational License (CC BY 4.0). Attribution: Markus Schedl, Hamid\nEghbal-Zadeh, Emilia G ´omez, Marko Tkal ˇciˇc. “An Analysis of Agree-\nment in Classical Music Perception and Its Relationship to Listener Char-\nacteristics”, 17th International Society for Music Information Retrieval\nConference, 2016.teners by demographics, music knowledge and experience,\nand personality. For our study, we focus on classical music,\nthe repertoire under investigation being Beethoven’s 3rd\nsymphony, “Eroica”. Responses of the listeners to the mu-\nsic are recorded via ratings of perceived emotions, tempo,\ncomplexity, and instrumentation.\nIn Section 2, we position our contribution within ex-\nisting literature. Details on data acquisition and setup of\nthe user study are provided in Section 3. Subsequently,\nwe present and discuss the ﬁndings of our analysis on the\nagreement on perceptual aspects (Section 4) and on the re-\nlationship between these aspects and listener characteris-\ntics (Section 5). We round off by concluding remarks and\na brief outlook to future research directions in Section 6.\n2. RELATED WORK\nThis work connects to other investigations of music per-\nception, to studies on personality in music, and to music\nemotion recognition.\nPrevious analyses on music perception have suggested\nthat certain musical parameters especially inﬂuence the\ncontent of emotional responses, notably timbre, orches-\ntration, acoustics, rhythm, melody, harmony, and struc-\nture [14]. For instance, Laurier created mappings be-\ntween musical descriptors and emotion categories [19],\nbut these emotion categories are limited to the ﬁve emo-\ntions happiness, sadness, anger, fear, and tenderness [3].\nRentfrow et al. identiﬁed ﬁve genre-free latent factors that\nreﬂect the affective response of listeners to music [25].\nThey named them “mellow”, “urban”, “sophisticated”, “in-\ntense”, and “campestral” music preference factors, yield-\ning the acronym MUSIC . Not much research has been de-\nvoted to how listeners of different demographic, personal-\nity, and musical background experience different percep-\ntual aspects of the same music. While there do exist sev-\neral cross-cultural studies on music and perceived emo-\ntion [1, 8, 12, 15, 28], these studies tend to focus on greatly\ndifferent cultures, rather than on more subtle differences\nsuch as age, gender, and musical experience or exposure.\nPersonality has been related to music preferences in a\nnumber of studies. Rentfrow and Gosling showed that per-\nsonality traits are related to four preference dimensions:\nreﬂective and complex, intense and rebellious, upbeat and\nconventional, and energetic and rhythmic [26]. Further-\nmore, they found that personality-based stereotypes are578strongly correlated with music genre preferences [24]. Per-\nhaps the most commonly used model of personality is the\nﬁve factor model (FFM), which is composed of the fac-\ntors openness, conscientiousness, extraversion, agreeable-\nness, and neuroticism [21]. Employing this model in their\nstudy, Chamorro-Premuzic and Furnham found that peo-\nple who score high on openness tend to consume music\nin a more rational way, while people who score high on\nneuroticism and those who score low on extraversion and\nconscientiousness tend to consume music to regulate their\nemotions [2]. Similarly, Ferwerda et al. showed that per-\nsonality accounts for individual differences in mood regu-\nlation [6]. Personality has also been linked to how users\ntend to perceive and organize music [7].\nOur work also connects to music emotion recognition\n(MER) at large, which has lately become a hot research\ntopic [4, 11, 13, 18, 27, 30, 32]. It aims at automatically\nlearning relationships between music audio or web features\nand emotion terms. However, common MER approaches\nassume that such a relationship exists, irrespective of a par-\nticular listener. In the study at hand, we take one step back\nand approach the question of whether listeners at all agree\non certain emotions and other perceptive aspects when lis-\ntening to classical music.\n3. MATERIALS AND USER STUDY\n3.1 Music Material\nIn our study, we focused on classical music and se-\nlected one particular piece, namely Beethoven’s 3rdsym-\nphony, “Eroica” , from which we extracted 15 coherent ex-\ncerpts [31]. This symphony is a well-known piece, also to\nmany who are not much into classical music. We had to\nmake this restriction to one piece to compare results be-\ntween participants and keep them engaged throughout the\nquestionnaire. Furthermore, this symphony was selected\nbecause of its focus in the PHENICX project,1the work\nat hand emerged from. Beethoven’s “Eroica” is generally\nagreed on as a key composition of the symphonic reper-\ntoire, constituting a paradigm of formal complexity, as\nevidenced by the vast literature analyzing the symphony.\nWe considered a performance by the Royal Concertge-\nbouw Orchestra, Amsterdam. The 15 excerpts we used in\nthe study were carefully selected by the authors, some of\nwhich are trained in music theory and performance, then\nreviewed by a musicologist. To this end, every section was\nﬁrst labeled with one of the nine emotions according to\nthe Geneva Emotional Music Scale (GEMS) [33], judged\nbased on musical elements. Then, the six emotions that ap-\npeared most frequently among the labels were identiﬁed.\nThree excerpts each for peacefulness, power, and tension,\nand two excerpts each for transcendence, joyful activation,\nand sadness, were ﬁnally selected. In this ﬁnal selection\nstep, we ensured that the segments covered a variety of\nmusical characteristics, lasted the duration of a complete\nmusical phrase, and strongly represented one of the above\nsix emotions.\n1http://phenicx.upf.eduFor the sake of reproducibility, interested readers can\ndownload the excerpts from http://mtg.upf.edu/\ndownload/datasets/phenicx-emotion .\n3.2 Study Design\nThe study was conducted as online survey, accessible via\na web interface. Participants were recruited by mass mail\nto all students of the Johannes Kepler University Linz and\nby posting to several research mailing lists. Announce-\nments were also made on various social media platforms\nthe authors are active on. In the survey, we ﬁrst asked\nparticipants a range of questions, related to demograph-\nics, music education and experience, inclination to music\nand to classical music in particular, and familiarity with\nBeethoven’s “Eroica”. Subsequently, participants had to\nﬁll in a personality questionnaire, i.e., the standardized Ten\nItem Personality Instrument (TIPI) [9]. After having pro-\nvided this personal information, we asked participants to\nlisten to each of the 15 excerpts and provide ratings of\nperceptual qualities (emotions, tempo, complexity, and in-\nstrumentation). We ensured that participants actually lis-\ntened to the excerpts by measuring the time they played\neach piece in the web browser. To describe emotions, we\nused the six emotions of the GEMS model most dominant\nin the music material (see above) and added ﬁve basic hu-\nman emotions identiﬁed in psychological literature [5,23]:\ntranscendence, peacefulness, power, joyful activation, ten-\nsion, sadness; anger, disgust, fear, surprise, tenderness. We\nadded these additional emotions to complement the GEMS\nmodel with basic emotions not speciﬁcally targeted at mu-\nsic perception. The options available to participants for\neach answer, as well as their numeric coding for the fol-\nlowing analysis, are provided in Table 1. Note that we\nare interested in perceived music qualities. Therefore, the\nquestions were formulated according to the scheme “I per-\nceive the music as ...”.\n3.3 Statistics of Participants\nThe survey was completed by 241 participants, taking\nthem around 40 minutes on average. We had 123 male\nand 118 female participants. The vast majority of 217 par-\nticipants were Austrians; other participants were Germans,\nItalians, Russians, Englishmen, and Spaniards. A limita-\ntion of the study is that participation was biased towards\nyounger people, the median age of participants being 25\nyears. This can be explained by the large number of stu-\ndents among participants. However, the youngest partici-\npants were only 16, while the eldest one was 67. As for\nparticipants’ music taste and listening frequency, on aver-\nage subjects listen to classical music 2.6 hours per week,\nand to other genres 11 hours per week. Interestingly, the\nmedian for listening to classical music (1 hour per week)\nis much lower than the median of listening to other gen-\nres (8 hours per week). It thus seems that participants\neither love classical music and devote a lot of time to it,\nor do not listen to it at all. Less than half of the partici-\npants play an instrument (median of 0 hours per week), but\nmost had some form of musical education, on average 6.8Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 579Aspect Options Numeric encoding\nAge free form years\nGender male or female —\nCountry list selection from 193 countries —\nListening classical free form hours per week\nListening non-classical free form hours per week\nPlaying instrument free form hours per week\nMusical education free form years\nConcerts classical free form attendances per year\nConcerts non-classical free form attendances per year\nFamiliar with “Eroica” unfamiliar, somewhat familiar, very\nfamiliar0–2\nAll personality traits strongly disagree–strongly agree 1–7\nAll emotions strongly disagree, disagree, neither\nagree nor disagree, agree, strongly\nagree, don’t know0–4, -1\nPerceived tempo slow, fast, don’t know 0, 1, -1\nPerceived complexity very low–very high, don’t know 0–4, -1\nKinds of instruments 1, 2, 3, 4, more, don’t know 1, 2, 3, 4, 5, -1\nDescription of the excerpt free form —\nTable 1 . Options available to participants and numerical encoding of the answers using for analysis.\nAspect µσmed min max\nAge 27.35 8.47 25 16 67\nListening classical (hrs/week) 2.56 5.20 1 040\nListening non-classical 11.16 11.86 8 070\nPlaying instrument 1.93 4.23 0 040\nMusical education 6.77 6.39 5 033\nConcerts classical 2.43 5.28 1 040\nConcerts non-classical 3.93 6.70 2 070\nFamiliar with “Eroica” 0.83 0.64 1 0 2\nTable 2 . Basic statistics of the participants. µ= mean,\nσ= standard deviation, med = median\nyears. Participants attend on average 2 classical and 4 non-\nclassical concerts per year, but the median values are again\nsmaller (1 and 2 concerts, respectively). Many participants\ndo not attend concerts at all: 39% do not attend a single\nclassical concert, 22% do not attend a single concert of\nanother genre per year. Most participants were not (72 or\n30%) or somewhat (137 or 57%) familiar with Beethoven’s\n“Eroica”. Only 32 (14%) indicated to be very familiar with\nthe piece. Analyzing the personality traits, shown in Ta-\nble 3, we observe that subjects tend to regard themselves\nas open to new experiences, sympathetic, calm, but also de-\npendable (average and median ratings are at least “agree a\nlittle”). On the other hand, they negate being disorganized,\nconventional, and anxious (average and median ratings are\nat most “disagree a little”).\n4. LISTENER AGREEMENT\nWe compute the agreement on all perceptive aspects un-\nder investigation. To this end, we use Krippendorff’s α\nscore for inter-rater agreement [16], computed on the rat-\nings given by participants for each segment separately andPersonality trait µσmed min max\nExtraverted 4.27 1.88 5 1 7\nCritical 4.54 1.68 5 1 7\nDependable 5.27 1.43 6 1 7\nAnxious 3.17 1.64 3 1 7\nOpen to new experiences 5.59 1.27 6 2 7\nReserved 4.41 1.81 5 1 7\nSympathetic 5.39 1.32 6 1 7\nDisorganized 2.83 1.69 2 1 7\nCalm 5.01 1.56 6 1 7\nConventional 2.84 1.63 2 1 7\nTable 3 . Personality statistics of participants. µ= mean,\nσ= standard deviation, med = median\nsubsequently averaged. We excluded from the calculations\n“don’t know” answers, i.e., treated them as missing values.\nTable 4 shows the overall mean ratings, standard devi-\nations, and agreement scores among participants for each\ninvestigated aspect, macro-averaged over all segments. We\nobserve that participants give highest average ratings (col-\numnµin Table 4) to the aspects of power and tension,\nfollowed by transcendence and joyful activation. Lowest\nratings are given to fear, sadness, anger, and — much be-\nlow — disgust. Overall, it seems that the aspects ranging\nin the lower arousal range (sadness, peacefulness, etc.) are\nperceived to a smaller degree in the music material under\nconsideration. Tempo is, on average, neither perceived as\nparticularly low nor high. So is complexity. As for instru-\nmentation, overall, most participants could distinguish 4\nkinds of instruments.\nAs for agreement, the study evidences a low to mod-\nerate agreement for most aspects, according to Krippen-\ndorff’sα. Participants do not (0.00–0.20) or at most\nslightly (0.21–0.40) agree on most perceptual aspects.580 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Aspect Scaleµσα\nTranscendence 0–4 2.215 1.095 0.010\nPeacefulness 0–4 1.812 0.986 0.450\nPower 0–4 2.477 0.937 0.450\nJoyful activation 0–4 2.048 1.059 0.320\nTension 0–4 2.318 1.121 0.222\nSadness 0–4 1.233 0.979 0.298\nAnger 0–4 1.204 1.008 0.300\nDisgust 0–4 0.808 0.941 0.128\nFear 0–4 1.292 1.084 0.276\nSurprise 0–4 1.790 1.162 0.054\nTenderness 0–4 1.687 1.046 0.366\nTempo 0–1 0.460 0.337 0.513\nComplexity 0–4 2.240 0.864 0.116\nInstrument kinds 1–5 3.899 0.980 0.077\nTable 4 . Meanµ, standard deviation σ, and agreement\nscore (Krippendorff’s α) for investigated aspects of music\nperception. Italic font is used to indicate slight agreement.\nBold face is used to denote moderate agreement.\nThe values indicating moderate agreement (0.41–0.60) ac-\ncording to [17] are printed in bold in Table 4, whereas\nslight agreement is indicated by italics. Highest agreement\namong the emotion aspects is found for peacefulness and\npower, while tempo shows the highest agreement among\nall investigated aspects. Slight agreement can be observed\nfor joyful activation, tension, sadness, anger, fear, and ten-\nderness. No relevant agreement is observed for transcen-\ndence, disgust, surprise, as well as perceived complexity\nand number of instrument groups. Perceived complexity\nis presumably a highly subjective aspect. Furthermore, it\nseems that there is a discrepancy between listeners with\nregard to their ability to distinguish different instrumenta-\ntions, which is presumably due to different music knowl-\nedge and expertise levels.\n5. LISTENER CHARACTERISTICS AND\nPERCEPTUAL ASPECTS\nWe investigate whether there exists a signiﬁcant relation-\nship between listener characteristics and the perceptual as-\npects under investigation. To this end, we calculate Pear-\nson’s correlation coefﬁcient between the respective numer-\nically encoded factors, according to Table 1, treating each\nuser–segment pair as one observation. Table 5 shows the\ncorrelation values for all listener characteristics (rows) and\nperceptual aspects (columns). While most correlations are\nnot very pronounced, several are signiﬁcant (at p< 0.05),\nwherepvalues are the probability of observing by chance\na correlation as large as the observed one, when the true\ncorrelation is 0.\nReviewing the results, a remarkable observation is the\nsigniﬁcant correlations between factors of musical back-\nground and knowledge (listening to classical music, play-\ning an instrument, musical education, concert attendances,\nfamiliarity with the piece) and perceived number of instru-\nment groups. Participants with a stronger musical back-ground therefore seem to be able to distinguish more in-\nstruments. While participants scoring high on convention-\nalism show negative correlation with the perceived num-\nber of instrument groups, the opposite it true for people\nwho are open to new experiences. As for participants’ age,\nolder people tend to perceive the music as more joyful and\nless fearsome. Frequent listeners of classical music tend\nto perceive the piece as more powerful, transcendent, and\ntender, but less fearsome. On the other hand, listeners of\nother genres perceive more anger and surprise. Playing an\ninstrument, extensive musical education, and frequent clas-\nsical concert attendances show a positive correlation with\nperceived power and tension, while attending non-classical\nconcerts seem to have no inﬂuence on emotion perception.\nParticipants who are familiar with the “Eroica” overall tend\nto perceive it as more transcendent, powerful, joyful, and\ntender.\nAmong the personality traits, most show little correla-\ntion with the perceptual aspects. However, openness to\nnew experiences is signiﬁcantly correlated with the emo-\ntion categories transcendence, peacefulness, joyful activa-\ntion, and tenderness, as well as tempo and instrumenta-\ntion. Disorganized people tend to rate the piece higher on\nsadness, anger, disgust, but also on tenderness. Sympa-\nthetic subjects on average give higher ratings to aspects\nof peacefulness, tenderness, tempo, and number of instru-\nment groups. Calm participants perceive the music as more\npeaceful, joyful, and tender than others, while convention-\nalists perceive it as less transcendent and tense.\n6. CONCLUSIONS AND FUTURE DIRECTIONS\nIn the presented study, we addressed two research ques-\ntions. First, we investigated whether listeners agree on\na range of perceptual aspects (emotions, tempo, com-\nplexity, and instrumentation) in classical music material,\nrepresented by excerpts from Beethoven’s 3rdsymphony,\n“Eroica”. Only for the perceived emotions peacefulness\nand power as well as for the perceived tempo, moder-\nate agreement was found. On other aspects, participants\nagreed only slightly or not at all. The second question we\napproached in the study is the relationship between listener\ncharacteristics (demographics, musical background, and\npersonality traits) and ratings given to the perceptual as-\npects. Among others, we found signiﬁcant correlations be-\ntween musical knowledge and perceived number of instru-\nment groups, which might not be too surprising. We fur-\nther identiﬁed slight, but signiﬁcant positive correlations\nbetween aspects of musical inclination or knowledge and\nperceived power and tension. Several correlations were\nalso found for the personality traits open to new experi-\nences, sympathetic, disorganized, calm, and conventional.\nAs part of future work, we plan to assess to which extent\nthe investigated perceptual aspects can be related to music\naudio descriptors. We would further like to analyze the im-\npact of listener characteristics on the agreement scores of\nperceptual aspects. Furthermore, a cross-correlation anal-\nysis between ratings of the perceived qualities could re-\nveal which emotions (or other investigated aspects) areProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 581Trans. Peace. Power Joyful. Tension Sadness Anger Disgust Fear Surprise Tender Tempo Compl. Instr.\nAge 0.155 0.040 0.102 0.261 0.075 -0.081 -0.110 -0.002 -0.186 -0.015 0.104 -0.031 -0.019 -0.026\nListening classical 0.203 0.112 0.212 0.078 0.019 -0.082 -0.090 -0.105 -0.190 -0.029 0.148 0.028 0.123 0.192\nListening non-classical 0.085 0.092 0.121 0.007 0.033 0.028 0.139 0.042 0.078 0.149 0.054 0.122 0.064 -0.036\nPlaying instrument 0.085 -0.016 0.133 0.010 0.190 0.077 0.113 0.073 0.050 0.042 0.014 0.061 0.012 0.259\nMusical education 0.140 -0.073 0.143 0.007 0.170 0.029 0.101 0.085 0.008 -0.064 0.007 0.077 0.076 0.418\nConcerts classical 0.170 0.065 0.175 0.108 0.192 -0.015 -0.033 -0.028 -0.065 -0.046 0.076 0.017 0.086 0.243\nConcerts non-classical 0.114 -0.004 0.048 -0.008 0.099 0.080 0.079 0.061 0.091 0.069 -0.003 0.106 0.045 0.153\nFamiliar with “Eroica” 0.141 0.118 0.211 0.184 0.116 -0.045 0.057 0.026 -0.018 0.004 0.149 0.056 0.096 0.242\nExtraverted 0.045 0.024 0.120 0.065 0.022 0.031 -0.014 -0.027 0.007 0.041 0.166 0.112 0.059 0.066\nCritical 0.010 0.031 0.094 0.081 0.049 0.037 -0.035 -0.041 -0.011 -0.141 0.043 0.066 0.075 0.049\nDependable 0.054 -0.098 -0.074 -0.098 0.009 -0.049 -0.065 -0.035 0.011 -0.018 0.007 -0.023 -0.075 0.033\nAnxious -0.084 -0.054 -0.108 -0.114 -0.108 -0.003 0.017 0.064 0.055 0.023 -0.089 -0.072 -0.054 -0.087\nOpen to new exp. 0.159 0.139 0.108 0.181 0.054 0.053 0.010 0.005 -0.003 0.009 0.222 0.173 0.006 0.201\nReserved -0.049 0.033 -0.112 -0.057 -0.095 -0.038 -0.033 -0.014 -0.045 -0.042 -0.084 -0.026 -0.054 -0.061\nSympathetic 0.077 0.147 0.098 0.107 0.059 -0.031 -0.012 0.020 0.026 0.078 0.166 0.148 0.015 0.134\nDisorganized 0.076 0.120 0.032 0.083 0.114 0.167 0.157 0.146 0.116 0.111 0.129 0.130 -0.014 -0.069\nCalm 0.076 0.142 -0.002 0.153 -0.032 -0.023 -0.044 -0.060 0.031 -0.063 0.132 0.069 0.153 0.135\nConventional -0.145 0.099 -0.048 0.012 -0.135 0.050 0.087 0.070 0.102 0.008 -0.058 -0.040 -0.002 -0.129\nTable 5 . Correlation between demographics, music expertise, and personality traits on the one hand, and aspects of music\nperception on the other. Signiﬁcant results ( p<0.05) are depicted in bold face.\nfrequently perceived together. Finally, we would like to\nperform a larger study, involving on the one hand a larger\ngenre repertoire and on the other an audience more diverse\nin terms of cultural background.\n7. ACKNOWLEDGMENTS\nThis research is supported by the Austrian Science Fund\n(FWF): P25655 and was made possible by the EU FP7\nproject no. 601166 (“PHENICX”).\n8. REFERENCES\n[1] L.L. Balkwill and W.F. Thompson. A cross-cultural in-\nvestigation of tlhe perception of emotion in music. Mu-\nsic Perception , 17(1):43–64, 1999.\n[2] Tomas Chamorro-Premuzic and Adrian Furnham. Per-\nsonality and music: can traits explain how people use\nmusic in everyday life? British Journal of Psychology ,\n98:175–85, May 2007.\n[3] T. Eerola and J.K. Vuoskoski. A comparison of the\ndiscrete and dimensional models of emotion in music.\nPsychology of Music , 39(1):18–49, 2011.\n[4] Tuomas Eerola, Olivier Lartillot, and Petri Toivi-\nainen. Prediction of Multidimensional Emotional Rat-\nings in Music from Audio Using Multivariate Regres-\nsion Models. In Proceedings of the 10th International\nSociety for Music Information Retrieval Conference\n(ISMIR) , Kobe, Japan, October 2009.\n[5] Paul Ekman. Basic Emotions , pages 45–60. John Wiley\n& Sons Ltd., New York, NY , USA, 1999.\n[6] Bruce Ferwerda, Markus Schedl, and Marko Tkalcic.\nPersonality & Emotional States : Understanding UsersMusic Listening Needs. In Alexandra Cristea, Ju-\ndith Masthoff, Alan Said, and Nava Tintarev, editors,\nUMAP 2015 Extended Proceedings , 2015.\n[7] Bruce Ferwerda, Emily Yang, Markus Schedl, and\nMarko Tkal ˇciˇc. Personality Traits Predict Music Tax-\nonomy Preferences. In Proceedings of the 33rd Annual\nACM Conference Extended Abstracts on Human Fac-\ntors in Computing Systems - CHI EA ’15 , pages 2241–\n2246, 2015.\n[8] T. Fritz, S. Jentschke, N. Gosselin, D. Sammler,\nI. Peretz, R. Turner, A.D. Friederici, and S. Koelsch.\nUniversal recognition of three basic emotions in mu-\nsic.Current Biology , 19(7):573–576, 2009.\n[9] Samuel D. Gosling, Peter J. Rentfrow, and William\nB. Swann Jr. A very brief measure of the Big-Five per-\nsonality domains. Journal of Research in Personality ,\n37(6):504–528, December 2003.\n[10] K. Hevner. Expression in Music: A Discussion of Ex-\nperimental Studies and Theories. Psychological Re-\nview, 42, March 1935.\n[11] Xiao Hu, J. Stephen Downie, and Andreas F. Ehmann.\nLyric Text Mining in Music Mood Classiﬁcation. In\nProceedings of the 10th International Society for Mu-\nsic Information Retrieval Conference (ISMIR) , Kobe,\nJapan, October 2009.\n[12] Xiao Hu and Jin Ha Lee. A Cross-cultural Study of\nMusic Mood Perception Between American and Chi-\nnese Listeners. In Proceedings of the 13th Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , Porto, Portugal, October 2012.\n[13] A Huq, J.P. Bello, and R. Rowe. Automated Music\nEmotion Recognition: A Systematic Evaluation. Jour-\nnal of New Music Research , 39(3):227–244, November\n2010.582 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016[14] P.N. Juslin and P. Laukka. Expression, perception, and\ninduction of musical emotions: A review and a ques-\ntionnaire study of everyday listening. Journal of New\nMusic Research , 33(2):217–238, 2004.\n[15] Katerina Kosta, Yading Song, Gyorgy Fazekas, and\nMark B. Sandler. A Study of Cultural Dependence of\nPerceived Mood in Greek Music. In Proceedings of the\n14th International Society for Music Information Re-\ntrieval Conference (ISMIR) , Curitiba, Brazil, Novem-\nber 2013.\n[16] Klaus Krippendorff. Content Analysis – An Introduc-\ntion to Its Methodology . SAGE, 3rd edition, 2013.\n[17] J.R. Landis and G.G. Koch. The measurement of\nobserver agreement for categorical data. Biometrics ,\n33:159–174, 1977.\n[18] C. Laurier, J. Grivolla, and P. Herrera. Multimodal Mu-\nsic Mood Classiﬁcation Using Audio and Lyrics. In\nProceedings of the 7th International Conference on\nMachine Learning and Applications (ICMLA) , pages\n688–693, December 2008.\n[19] Cyril Laurier. Automatic Classiﬁcation of Music Mood\nby Content-based Analysis . PhD thesis, Universitat\nPompeu Fabra, Barcelona, Spain, 2011.\n[20] Adam J Lonsdale and Adrian C North. Why do we lis-\nten to music? A uses and gratiﬁcations analysis. British\nJournal of Psychology , 102(1):108–34, February 2011.\n[21] Robert R McCrae and Oliver P John. An Introduction\nto the Five-Factor Model and its Applications. Journal\nof Personality , 60(2):175–215, 1992.\n[22] A. Pike. A phenomenological analysis of emotional ex-\nperience in music. Journal of Research in Music Edu-\ncation , 20:262–267, 1972.\n[23] Robert Plutchik. The Nature of Emotions. American\nScientist , 89(4):344–350, 2001.\n[24] P. J. Rentfrow and S. D. Gosling. The content and va-\nlidity of music-genre stereotypes among college stu-\ndents. Psychology of Music , 35(2):306–326, February\n2007.[25] Peter J. Rentfrow, Lewis R. Goldberg, and Daniel J.\nLevitin. The structure of musical preferences: A ﬁve-\nfactor model. Journal of Personality and Social Psy-\nchology , 100(6):1139–1157, 2011.\n[26] Peter J. Rentfrow and Samuel D. Gosling. The do re\nmi’s of everyday life: The structure and personality\ncorrelates of music preferences. Journal of Personal-\nity and Social Psychology , 84(6):1236–1256, 2003.\n[27] Erik M. Schmidt and Youngmoo E. Kim. Projection\nof Acoustic Features to Continuous Valence-Arousal\nMood Labels via Regression. In Proceedings of the\n10th International Society for Music Information Re-\ntrieval Conference (ISMIR) , Kobe, Japan, October\n2009.\n[28] Abhishek Singhi and Daniel G. Brown. On Cultural,\nTextual and Experiential Aspects of Music Mood. In\nProceedings of the 15th International Society for Mu-\nsic Information Retrieval Conference (ISMIR) , Taipei,\nTaiwan, October 2014.\n[29] Justin Sloboda and Patrick Juslin. Music and Emotion:\nTheory and Research . Oxford University Press, 2001.\n[30] Yading Song, Simon Dixon, and Marcus Pearce. Eval-\nuation of Musical Features for Emotion Classiﬁca-\ntion. In Proceedings of the 13th International Society\nfor Music Information Retrieval Conference (ISMIR) ,\nPorto, Portugal, October 2012.\n[31] Erika Trent and Emilia G ´omez. Correlations Be-\ntween Musical Descriptors and Emotions Recognized\nin Beethoven’s Eroica. In Proceedings of the 9th Trien-\nnial Conference of the European Society for the Cog-\nnitive Sciences of Music (ESCOM) , Manchester, UK,\nAugust 2015.\n[32] Yi-Hsuan Yang and Homer H. Chen. Machine recog-\nnition of music emotion: A review. Transactions on\nIntelligent Systems and Technology , 3(3), May 2013.\n[33] M. Zenter, D. Grandjean, and K.R. Scherer. Emotions\nevoked by the sound of music: Characterization, clas-\nsiﬁcation, and measurement. Emotion , 8:494, 2008.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 583"
    },
    {
        "title": "Learning to Pinpoint Singing Voice from Weakly Labeled Examples.",
        "author": [
            "Jan Schlüter"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417651",
        "url": "https://doi.org/10.5281/zenodo.1417651",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/315_Paper.pdf",
        "abstract": "Building an instrument detector usually requires tempo- rally accurate ground truth that is expensive to create. However, song-wise information on the presence of in- struments is often easily available. In this work, we in- vestigate how well we can train a singing voice detec- tion system merely from song-wise annotations of vocal presence. Using convolutional neural networks, multiple- instance learning and saliency maps, we can not only de- tect singing voice in a test signal with a temporal accuracy close to the state-of-the-art, but also localize the spectral bins with precision and recall close to a recent source sep- aration method. Our recipe may provide a basis for other sequence labeling tasks, for improving source separation or for inspecting neural networks trained on auditory spec- trograms.",
        "zenodo_id": 1417651,
        "dblp_key": "conf/ismir/Schluter16",
        "content": "LEARNING TO PINPOINT SINGING VOICE\nFROM WEAKLY LABELED EXAMPLES\nJan Schlüter\nAustrian Research Institute for Artiﬁcial Intelligence, Vienna\njan.schlueter@ofai.at\nABSTRACT\nBuilding an instrument detector usually requires tempo-\nrally accurate ground truth that is expensive to create.\nHowever, song-wise information on the presence of in-\nstruments is often easily available. In this work, we in-\nvestigate how well we can train a singing voice detec-\ntion system merely from song-wise annotations of vocal\npresence. Using convolutional neural networks, multiple-\ninstance learning and saliency maps, we can not only de-\ntect singing voice in a test signal with a temporal accuracy\nclose to the state-of-the-art, but also localize the spectral\nbins with precision and recall close to a recent source sep-\naration method. Our recipe may provide a basis for other\nsequence labeling tasks, for improving source separation\nor for inspecting neural networks trained on auditory spec-\ntrograms.\n1. INTRODUCTION\nA fundamental step in automated music understanding is\nto detect which instruments are present in a music audio\nrecording, and at what time they are active. Traditionally,\ndeveloping a system detecting and localizing a particular\ninstrument requires a set of music pieces annotated at the\nsame granularity as expected to be output by the system –\nno matter if the system is constructed by hand or by ma-\nchine learning algorithms.\nAnnotating music pieces at high temporal accuracy re-\nquires skilled annotators and a lot of time. On the other\nhand, instrument annotations at a song level are often eas-\nily available online, as part of the tags given by users of\nstreaming services, or descriptions or credits by the pub-\nlisher. Even if not, collecting or cleaning song-wise anno-\ntations requires very little effort and low skill compared to\ncurating annotations with sub-second granularity.\nAs a step towards tapping into such resources, in this\nwork, we explore how to obtain high-granularity vocal de-\ntection results from low-granularity annotations. Speciﬁ-\ncally, we train a Convolutional Neural Network (CNN) on\n10,000 30-second song snippets annotated as to whether\nc/circlecopyrtJan Schlüter. Licensed under a Creative Commons Attri-\nbution 4.0 International License (CC BY 4.0). Attribution: Jan Schlüter.\n“Learning to Pinpoint Singing V oice From Weakly Labeled Examples”,\n17th International Society for Music Information Retrieval Conference,\n2016.they contain singing voice anywhere within, and subse-\nquently use it to detect the presence of singing voice with\nsub-second granularity. As the main contribution of our\nwork, we develop a recipe to improve initial results us-\ning multiple-instance learning and saliency maps. Finally,\nwe investigate how well the system can even pinpoint\nthe spectral bins containing singing voice, instead of the\ntime frames only. While we constrain our experiments to\nsinging voice detection as a special case of instrument de-\ntection (and possibly the easiest), we do not assume any\nprior knowledge about the content to be detected, and thus\nexpect the recipe to carry over to other instruments.\nThe next section will provide a review of related work\non learning from weakly-annotated data both outside and\nwithin of the music domain, and on singing voice detec-\ntion. Section 3 explains the methods we combined in this\npaper, Section 4 describes how we combined them, and\nSection 5 evaluates the resulting system on four datasets.\nFinally, Section 6 discusses what we achieved and what is\nstill open, highlights avenues for future research and points\nout alternative uses for some of our ﬁndings.\n2. RELATED WORK\nThe idea of training on weakly-labeled data is far from\nnew, since coarse labels are almost always easier to ob-\ntain than ﬁne ones. The general framework for this setting\nis Multiple-Instance Learning, which we will return to in\nSection 3.2. As one of the ﬁrst instances, Keeler et al. [8]\ntrain a CNN to recognize and localize two hand-written\ndigits in an input image of about 36×36pixels, giving\nonly the identities of the two digits as training targets. As a\nrecent work closer to our setting, Hou et al. [6] train a CNN\nto detect and classify brain tumors in gigapixel resolution\ntissue images. As such images are too large to be processed\nas a whole, they propose to train on patches, still using\nimage-level labels only. To account for the fact that not all\npatches in a tumor image show tumorous tissue, Hou et al.\nemploy an expectation maximization algorithm that itera-\ntively prunes non-discriminative patches from the training\nset based on the CNN’s predictions. As far as we are aware,\nthe only work in music information retrieval aiming to pro-\nduce ﬁne-grained predictions from coarse training data is\nthat of Mandel and Ellis [14]: They train special variants\nof SVMs on song, album or artist labels to predict tags on\na granularity of 10-second clips. In contrast, we aim for\nsub-second granularity, and for identifying spectral bins.44Recent approaches for singing voice detection [9,10,18]\nare all based on machine learning from temporally accurate\nlabels. The current state of the art is our previous work\n[18], a CNN trained on mel spectrogram excerpts. We will\nuse it both as a starting point and for comparing our results\nagainst, and describe it in more detail in Section 3.1.\nSinging voice detection does not entail identifying the\nspectral bins. The closest task to this is singing voice ex-\ntraction, which aims to extract a purely vocal signal from\na single-channel audio recording and thus has to estimate\nits spectral extends. It differs from general blind source\nseparation in that it can leverage prior knowledge about\nthe two signals to be separated – vocals and background\nmusic. As an improvement over Nonnegative Matrix Fac-\ntorization (NMF) [20], which can only encode such prior\nknowledge in the form of spectral templates, REPET [16]\nuses the fact that background music is repetitive while vo-\ncals are not. Kernel-Additive Modeling [11] generalizes\nthis method and uses a set of assumptions on local regular-\nities of vocals and background music to perform singing\nvoice extraction. We will use it as a mark to compare our\nresults to.\n3. INGREDIENTS\nOur recipe combines a few methods that we shall intro-\nduce up front: a singing voice detector based on CNNs,\nmultiple-instance learning, and saliency maps.\n3.1 CNN-based Singing Voice Detection\nThe base of our system is a CNN trained to predict whether\na short spectrogram excerpt contains singing voice at its\ncenter. We mostly follow Schlüter et al. [18], and will limit\nthe description here to what is needed for understanding\nthe paper, as well as how we deviated from that approach.\nInput signals are converted to 22.05 kHz mono and pro-\ncessed by a Short-Time Fourier Transform with 70 1024-\nsample frames per second. Phases are discarded, magni-\ntudes are scaled by log(1 +x), the spectrogram is cropped\nabove 8 kHz (keeping 372 bins) and each frequency band\nis normalized to zero mean and unit variance over the train-\ning set. We skip the mel-scaling step of Schlüter et al. to\nenable more accurate spectral localization of singing voice.\nAs in [18], the network architecture starts with 64 and\n323×3convolutions, 3×3max-pooling, 128 and 64 3×3\nconvolutions. At this point, the 372 frequency bands have\nbeen reduced to 118. We add 128 3×115 convolutions\nand1×4max-pooling. This way, the network learns spec-\ntrotemporal patterns spanning almost the full frequency\nrange, applies them at four different frequency offsets and\nkeeps the maximum activation of those four, effectively\nintroducing some pitch invariance. We ﬁnish with three\ndense layers of 256, 64 and 1 unit, respectively. Except\nfor the ﬁnal layer, each convolution and dense layer is fol-\nlowed by batch normalization [7] and the leaky rectiﬁer\nmax(x/100,x)[13]. The ﬁnal layer uses the sigmoid ac-\ntivation function.\nTraining is done on excerpts of 115 frames (about1.6 sec) paired with a binary label for the excerpt’s central\nframe. We follow the training protocol described in [18,\nSec. 3.3], augmenting inputs with random pitch-shifting\nand time-stretching of ±30% and frequency ﬁltering of\n±10dB using the code accompanying [18].\nWe arrived at this system by selectively modifying our\nprevious work [18] to work well with linear-frequency\nspectrograms, on an internal dataset with ﬁne-grained an-\nnotations. It slightly outperforms [18] on this dataset.\n3.2 Multiple-Instance Learning\nWhile we train our network on short spectrogram excerpts,\nwe actually only have a single label per 30-second clip.\nIn the Multiple-Instance Learning (MIL) framework, each\nexplicitly labeled 30-second clip is called a bag, and the\nexcerpts we train on are called instances . In our setting,\na bag is labeled positively if and only if at least one of\nthe instances contained within are positive (referred to as\nthestandard MI assumption [4]). This gives an interesting\nasymmetry: If a 30-second clip is labeled as “no vocals”,\nwe can infer that neither of its excerpts contains vocals. If\na clip contains vocals, we only know that some excerpts\nwill contain vocals, but neither which ones nor how many.\nOne approach for training a neural network in this set-\nting is based on the observation that the label of a bag is\nsimply the maximum over its instance labels. If we deﬁne\nthe network’s prediction for a bag to be the maximum over\nthe predictions for its instances, and the objective function\nto measure the discrepancy between this bag-wise predic-\ntion and the true label, minimizing it by gradient descent\ndirectly results in the following algorithm (BP-MIP, [24]):\nPropagate all instances of a bag through the network, pick\nthe instance that gives the highest output, and update the\nnetwork weights to minimize its discrepancy with the bag\nlabel. Unfortunately, this scheme is very costly: It com-\nputes predictions for all instances of a bag, then performs\nan update for a single instance only. Furthermore, it is\neasy to overﬁt: It is enough for the network to produce\na strongly positive output for a single chosen instance per\nbag and negative outputs for all others. For 10,000 30-\nsecond clips, it would require merely learning 10,000 short\nexcerpts by heart.\nA different approach is to present all instances from\nnegative bags as negative examples (we know they are neg-\native) and all instances from positive bags as positive ex-\namples (they could be positive), and use a classiﬁer that\ncan underﬁt the training data, i.e., that may deviate from\nthe training labels for some examples. This naive idea\nalone can produce good results, but it is also the basis\nfor algorithms iteratively reﬁning this starting point: The\nmi-SVM algorithm [1] uses the predictions of the initial\nclassiﬁer to re-label some instances from positive bags to\nbecome negative, and alternates between re-training and\nre-labeling until convergence. A variant proposed by Hou\net al. [6] is to prune instances from positive bags that are\nnot clearly positive, and also iterate until convergence. We\nwill ﬁnd that for our task, the idea of improving initial re-\nsults by relabeling instances is important as well.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 45(a)\n (b)\n (c)\n (d)\nFigure 1 : Demonstration of saliency mapping: Network\ninput (a), gradient (b), guided backpropagation (c) and its\npositive values (d). Best viewed on screen.\n3.3 Saliency Mapping\nSaliency maps for neural networks have been popularized\nby Zeiler et al. [23] as a means of inspecting how a trained\nneural network forms its decisions. One of the most ele-\ngant forms computes the saliency map as the gradient of\nthe network’s output1with respect to its input [19]. For a\nsingle data point, this tells how and in which direction each\ninput feature inﬂuences the prediction for that data point.\nIn our case, the input is a spectrogram excerpt and the gra-\ndient shows for each spectrogram bin how an inﬁnitesimal\nincrease would affect the probability of predicting singing\nvoice (demonstrated in Figure 1a and 1b, respectively).\nUnfortunately, for a deep neural network, an input feature\ncan inﬂuence the output in convoluted ways: Some input\nmay increase the output by decreasing activities in hidden\nlayers that are negatively connected to the output unit.\nTo get a clearer picture, Springenberg et al. [21] propose\nguided backpropagation : At each layer, only propagate the\npositive gradient values to the previous layer. This limits\nthe saliency map to showing how input features affect the\noutput by a chain of changes in the same direction. Fig-\nure 1c demonstrates this for our example. A positive value\n(displayed in red) for a bin means increasing this bin will\nincrease the output by increasing activities in all layers in\nbetween . Likewise, a negative value (displayed in blue) for\na bin means that increasing it will decrease the output.\nNote that the negative saliencies are not very useful:\nThey form “halos” around the positive saliencies, indicat-\ning that the network hinges on the local contrast, and they\nare much less sharply localized. Assuming the hidden lay-\ners show a similar picture, this explains why ignoring neg-\native gradients in guided backpropagation gives a sharper\nsaliency map. To obtain a map of spectrogram bins corre-\nsponding to what the network used to detect singing voice,\nwe keep the positive saliencies only (Figure 1d).\n1Precisely, the pre-activation of the output unit, before applying the\nnonlinearity, as the sigmoid would dampen gradients at high activations.4. RECIPE\nHaving the basic ingredients in place, we will now describe\nour recipe. We begin by showing how to use a naively\ntrained network for temporal detection and spectral local-\nization, and then highlight an observation about saliency\nmaps that enables higher precision. Finally, we give a\nthree-step training procedure that further improves results.\n4.1 Naive Training\nThe easiest solution to dealing with the problem of incom-\nplete labels – and the starting point of our recipe – is to\npretend the labels were complete. We train an initial net-\nwork by presenting all excerpts from instrumental songs\nas negative, and all excerpts from vocal songs as positive.\nThis already works quite well: even on the training data, it\nproduces lower output for excerpts of vocal songs that do\nnot contain voice than for those that do.\nTo obtain a temporal detection curve for a test song,\nwe pass overlapping spectrogram excerpts of 115 frames\nthrough the network (with a hop size of 1 frame), recording\neach prediction. This way, for each spectrogram frame, we\nobtain a probability of singing voice being present in the\nsurrounding ±57frames. As in [18], we post-process this\ncurve by a sliding median ﬁlter of 56 frames (800 ms).\nFor spectral localization, we also pass overlapping spec-\ntrogram excerpts through the network, each time comput-\ning the 115×372-pixels saliency map for the excerpt. To\ncombine these into a single map for a test song, we con-\ncatenate the central frames of the saliency maps. This gives\na much sharper picture than an overlap-add of the excerpt\nmaps. When used for vocal extraction, we apply two post-\nprocessing steps: As the saliency maps are very sparse,\nwe apply Gaussian blurring with a standard dev. of 1 bin.\nAnd as the saliency values are very low, we scale them to a\nrange comparable to the spectrogram and take the element-\nwise minimum of the scaled map and spectrogram.\n4.2 Overshoot Correction\nWhen examining the predictions of the initial, naively\ntrained network, we observe that it regularly overshoots\nat the boundaries of segments of singing voice. Figure 2\nillustrates the problem for a training example: Predictions\nonly decline far away from vocal parts, and short pauses\nare glossed over. This is an artifact from training on weak\nlabels: The network predicts singing voice whenever its\n1.6-second input excerpt contains vocals, even if only at\nthe edge, because such excerpts have never been presented\nas negative examples during training.\nThe saliency map provides additional information,\nthough. By computing the saliency of the central frame\nof each excerpt (Figure 2e), we can check whether it was\nimportant for that excerpt’s prediction. Summing up the\nsaliency map over frequencies (Figure 2f) gives an alterna-\ntive prediction curve that can be used to improve the preci-\nsion of temporal detection. In the next section, we will use\nthis observation to improve the network.46 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016(a) spectrogram of a 30-second training clip containing vocals\n0.00.51.0\n(b) weak labels for training (all positive)\n0.00.51.0\n(c) actual ground truth\n0.00.51.0\n(d) network predictions\n(e) network saliency map\n0.00.10.2\n(f) network saliency map summarized over frequencies\nFigure 2 : Network predictions (d) overshoot vocal seg-\nments (c) because input windows only partially contain-\ning vocals were always presented as positive examples (b).\nSummarizing the saliency map (e) over frequencies (f) al-\nlows to correct such overshoots.\n4.3 Self-Improvement\nA basic idea we described in Section 3.2 is that the naively\ntrained network could be used to relabel the positive train-\ning instances, which necessarily contain false positives\n(compare Figure 2b to 2c). The intuition is that correct-\ning even just a few of those and re-training should improve\nresults.\nWe tried several variants of this idea: Relabeling by\nthresholding the initial network’s predictions (using a low\nthreshold to only relabel very certainly false positives),\nweighting positive examples by the initial network’s pre-\ndictions (so conﬁdently positive examples would affect\ntraining more than others), or removing positive examples\nthe initial network is not conﬁdent about. However, the\nonly effect was that the bias of the re-trained network was\nlower, and iterating such a scheme lead to a network pre-\ndicting “no” all the time. In hindsight, this is not surpris-\ning: The only positive instances we relabel this way are\nthose the network got correct with naive training already,\nso it will not learn anything new when re-training.\nWe found a single scheme that does not deteriorate re-\nsults: Training a second network to output the temporally\nsmoothed predictions of the initial network for positive in-\nstances, and the actual labels for negative instances. It does\nnot result in better predictions either, but in much clearer\nsaliency maps with less noise in non-vocal sections. Us-\ning the technique of Section 4.2, we ﬁnd that for such asecond network, the summarized saliency maps alone ac-\ntually provide a better temporal detection curve than the\nnetwork output, as it does not suffer from overshoot.\nIterating the latter scheme by re-training on the second\nnetwork’s predictions does not help. However, we can train\na third network to output the temporally smoothed sum-\nmarized saliency maps of the second network for positive\ninstances, again keeping negative instances at their true la-\nbels. To bring them to a suitable range for training (i.e.,\nbetween 0 and 1), we scale them by 10 and apply the tanh\nfunction. Finally, this third network gives predictions that\ndo not need any overshoot correction. It can be seen as\nﬁnding a more efﬁcient way of computing the summarized\nsaliency map of the second network.\nPut together, our recipe consists of: (1) Training a ﬁrst\nnetwork (subsequently called CNN- α) on the weak in-\nstance labels, (2) training a second network (CNN- β) on\nthe predictions of CNN- α, (3) training a third network\n(CNN-γ) on the summarized saliency maps of CNN- β.\n5. EXPERIMENTS\nTo test our approach, we train networks on a dataset of\n10,000 weakly-annotated 30-second snippets, then evalu-\nate them on two public datasets for temporal detection and\ntwo public datasets for spectral localization.\n5.1 Datasets\n5.1.1 Training and Development\nWe collected 10,000 30-second song snippets of 10,000\nartists. Using a custom web interface, 5 annotators sorted\n2,000 snippets each into vocal and non-vocal ones, where\n“vocal” was deﬁned to be “any use of human vocal\nchords”. Annotators were allowed to skip examples they\nwere very unsure about or that only contained voice-like\nsound effects, leaving 9751 annotated clips, 6772 of which\ncontain vocals. To check inter-annotator agreement, we\nhad 100 clips be labeled by 6 annotators. Of those, anno-\ntators agreed on 97 clips, the remaining 3 were skipped by\nat least one annotator and disagreed on by others.\nWe annotate the multiply-annotated clips with sub-\nsecond granularity to be used for testing, and keep the re-\nmaining clips for training. We further annotate 100 of the\ntraining clips ﬁnely to train a network equivalent to the one\nof Schlüter et al. [18] (dataset In-House A in that work).\nThe ﬁnely annotated positive clips feature vocals for\n70% of the running time (between 13% and 99% per clip).\nExtrapolating, we thus expect CNN- αto be presented with\n30% false positives among its positive training instances.\n5.1.2 Testing\nFor testing temporal detection, we use two public datasets\nﬁnely annotated with singing voice presence:\n–RWC , collected by Goto et al. [5] and annotated by\nMauch et al. [15], contains 100 pop songs.\n–Jamendo , curated by Ramona et al. [17], contains 93\nsongs. It comes with a predeﬁned train/test split, but\nwe use all songs for testing.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 47internal RWC Jamendo\nAU-\nROCmax\nacc.AU-\nROCmax\nacc.AU-\nROCmax\nacc.\nCNN-αpred. .911 .888 .879 .856 .913 .865\nsal. .955 .896 .912 .843 .930 .849\nCNN-βpred. .922 .888 .890 .861 .923 .875\nsal. .970 .916 .936 .883 .955 .894\nCNN-γpred. .970 .915 .939 .887 .960 .901\nsal. .965 .914 .931 .884 .950 .898\nCNN- pred. .979 .930 .947 .882 .951 .880\nﬁne sal. .969 .909 .937 .883 .948 .885\nTable 1 : Temporal detection results for the three steps of\nself-improvement on weak labels (Section 4.3) as well as a\nnetwork trained on ﬁne labels, for three datasets.2\nFor spectral localization, we use two public datasets that\ncome with separated vocal tracks:\n–ccMixter , collected by Liutkus et al. [11], consists of 50\nrecordings from ccmixter.org .\n–MedleyDB , compiled by Bittner et al. [2], contains 174\nsongs. We only use the 52 songs that feature vocals\n(singer or rapper) and do not have bleed between tracks.\nDownmixes are provided, we obtain the corresponding\nvocal tracks by mixing all vocal stems per song.\n5.2 Temporal Detection Results\nSinging voice detection is usually evaluated via the classi-\nﬁcation error at a particular threshold [9, 10, 15, 18]. How-\never, different tasks may require different thresholds tuned\ntowards higher precision or recall. Furthermore, tuning the\nthreshold towards some goal requires a ﬁnely-annotated\nvalidation set, which we assume is not available in our set-\nting. We therefore opt to assess the quality of the detection\ncurves, rather than hard classiﬁcations. Speciﬁcally, we\ncompute two measures: The Area Under the Receiver Op-\nerating Characteristic Curve (AUROC), and the classiﬁca-\ntion accuracy at the optimal threshold. The former gives\nthe probability that a randomly drawn positive example\ngets a higher prediction than a randomly drawn negative\nexample, and the latter gives a lower bound on the error.\nTable 1 shows the results. We compare four CNNs:\nCNN-αto CNN-γfrom the three-step self-improvement\nscheme of Section 4.3, and CNN-ﬁne, a network of the\nsame architecture trained on 100 ﬁnely-annotated clips,\nas a reimplementation of the state-of-the-art by Schlüter\net al. [18]. For each network, we assess the quality of\nits predictions and of its summarized saliency map, on the\nheld-out part of our internal dataset as well as the two pub-\nlic datasets.2\nWe can see that for CNN- α, summarized saliencies are\nbetter than its direct predictions in terms of AUROC, but\nworse in terms of optimal classiﬁcation error. CNN- β,\nwhich is trained on the predictions of CNN- α, performs\nstrictly better than CNN- αand gains a lot when using\n2Note that we did not train on the public datasets and used the full\ndatasets for evaluation, so results are not directly comparable to literature.ccMixter MedleyDB\nprec. rec. F1prec. rec. F1\nbaseline .324 .947 .473 .247 .955 .361\nKAML [12] .597 .681 .627 .416 .739 .484\nCNN-α .575 .651 .603 .467 .637 .497\nCNN-β .565 .763 .643 .522 .618 .529\nCNN- ﬁne .552 .795 .646 .494 .669 .528\nTable 2 : Spectral localization results for the baseline of\njust predicting the spectrogram of the mix, a voice/music\nseparation method, and saliency maps of three networks.\nsummarized saliencies instead of predictions. CNN- γis\ntrained to predict the saliencies of CNN- βand matches or\neven outperforms those. Its saliency maps do not provide\nany beneﬁt. Figure 3 provides a qualitative comparison of\nthe predictions for the three networks.\nComparing results to CNN-ﬁne, we see that it performs\ncomparable to CNN- γ: It is strictly better on the internal\ntest set (which is taken from the same source as the train-\ning data), but not on the public test sets, indicating that\nCNN-γproﬁts from its larger training set despite the weak\nlabels. The summarized saliencies of CNN-ﬁne do not pro-\nvide any improvement, which was to be expected: There is\nno overshoot they could correct as in Section 4.2.\n5.3 Spectral Localization Results\nSpectral localization of singing voice, i.e., identifying the\nspectrogram bins containing vocals, is not an established\ntask. The closest to this is singing voice extraction, which\nis evaluated with standard source separation measures\n(Source to Distortion Ration, Source to Interference Ra-\ntio). However, developing our method into a full-ﬂedged\nsource separator is out of scope for this work. We therefore\nresort to comparing the saliency map produced by the net-\nwork for a mixed signal to the spectrogram of the known\npure-vocal signal. Speciﬁcally, we compute a general-\nization of precision, recall and F1-measure towards real-\nvalued (instead of binary) targets: For a predicted saliency\nmapPijand pure-vocal spectrogram Tij, we deﬁne the to-\ntal amount of true positives as t=/summationtext\ni,jmin(Pij,Tij).\nWe then obtain precision as p=t//summationtext\ni,jPij, recall as\nr=t//summationtext\ni,jTij, andF1-measure as f= 2pr/(p+r).\nResults are shown in Table 2. We ﬁrst compute a sim-\nple baseline: Using the spectrogram of the mix as our vo-\ncal predictions. In theory, this should give 100% recall,\nbut since the songs are mastered, the mix spectrogram is\nsometimes lower than the spectrogram of the vocal track,\nleaving a gap. We then obtain results for KAML [12], a re-\nﬁned implementation of KAM [11] described in Section 2.\nAs the vocal prediction Pij, we compute the spectrogram\nof the vocal signal it extracts, clipped to 8 kHz to be com-\nparable to our network’s saliency maps. Finally, we eval-\nuate the saliency maps of CNN- α, CNN-βand CNN-ﬁne,\npost-processing them as described in Section 4.1. We omit\nCNN-γas it is tailored for temporal detection and will not\nproduce better saliencies than CNN- β.48 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016We ﬁnd that all methods are comparably far from the\nbaseline, with the CNNs obtaining higher F1-measure than\nKAML. As in temporal detection, CNN- βhas an edge over\nCNN-α, and is close to CNN-ﬁne. While these results look\npromising, it should be noted that the saliency maps will\nnot necessarily be better for source separation: A lot of\nthe improvement in F1-score hinges on the fact that the\nsaliency maps are often perfectly silent in passages that\ndo not contain vocals, while KAML still extracts parts of\nbackground instruments. To obtain high recall, for pas-\nsages that do contain vocals, the post-processed saliency\nmaps include more instrumental interference than KAML.\n6. DISCUSSION\nWe have explored how to train CNNs for singing voice de-\ntection on coarsely annotated training data and still obtain\ntemporally accurate predictions, closely matching perfor-\nmance of a network trained on ﬁnely annotated data. Fur-\nthermore, we have investigated a method for localizing the\nspectral bins that contain singing voice, without requiring\naccording ground truth for training. We expect the recipe\nto carry over from human voice to musical instruments,\nif good contrasting examples are available – training on\nweakly-annotated data can only learn to distinguish instru-\nments that occur independently from one another in differ-\nent music pieces.\nWhile our results are promising, there are a few short-\ncomings that provide opportunities for further research.\nFor one, our networks produce good prediction curves, but\nwhen used for binary classiﬁcation, we still need to choose\na suitable threshold (e.g., optimizing accuracy, or a preci-\nsion/recall tradeoff). We did not ﬁnd good heuristics for se-\nlecting such a threshold solely based on weakly labeled ex-\namples. Secondly, comparing training on 10,000 weakly-\nlabeled clips against 100 ﬁnely-labeled clips is clearly ar-\nbitrary. The main purpose in this work was to show that\ntraining from weakly-labeled clips can give high tempo-\nral accuracy at all , which is useful if such weak labels are\neasy to obtain. For future work, it would be interesting to\ncompare the two methods on more even grounds. Specif-\nically, we could investigate if weak labeling, ﬁne labeling\nor a combination of both provides the best value for a given\nbudget of annotator time.\nThe spectral localization results could be a starting point\nfor instrument-speciﬁc source separation. But as for tem-\nporal detection, this route would ﬁrst have to be com-\npared on even grounds to learning from ﬁnely-annotated\nor source-separated training data, and its viability depends\non how easily these types of data are obtainable. And in\ncontrast to temporal detection, it requires further work to\nbe turned into a source separation method.\nFinally, the kind of saliency maps explored in this work\ncould be used for other purposes: For example, it can be\nused to visualize and auralize precisely which content in a\nspectrogram was responsible for a particular false positive\ngiven by a network, and thus give a hint on how to enrich\nthe training data to improve results.\n(a) spectrogram of a 30-second test clip containing vocals\n0.00.51.0\n(b) corresponding ground truth\n(c) spectrogram of corresponding vocal track\n0.00.51.0\n(d) predictions of CNN- α(trained on weak labels)\n(e) saliency map of CNN- α\n0.00.10.2\n(f) summarized saliency map of CNN- α\n0.00.51.0\n(g) predictions of CNN- β(trained on predictions of CNN- α)\n(h) saliency map of CNN- β\n0.00.20.4\n(i) summarized saliency map of CNN- β\n0.00.51.0\n(j) predictions of CNN- γ(trained on tanh squashed sal. of CNN- β)\nFigure 3 : Qualitative demonstration of the self-\nimprovement recipe in Section 4.3 for a single test clip\n(0:24 to 0:54 of “Vermont” by “The Districts”, part of\nthe MedleyDB dataset [2]). Visit http://ofai.at/\n~jan.schlueter/pubs/2016_ismir/ for an in-\nteractive version.\n7. ACKNOWLEDGMENTS\nThe author would like to thank the anonymous review-\ners for their valuable comments and suggestions. This\nresearch is funded by the Federal Ministry for Transport,\nInnovation & Technology (BMVIT) and the Austrian Sci-\nence Fund (FWF): TRP 307-N23 and Z159. We also grate-\nfully acknowledge the support of NVIDIA Corporation\nwith the donation of a Tesla K40 GPU used for this re-\nsearch. Finally, we thank the authors and co-developers of\nTheano [22] and Lasagne [3] the experiments build on.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 498. REFERENCES\n[1] S. Andrews, I. Tsochantaridis, and T. Hofmann. Sup-\nport vector machines for multiple-instance learning. In\nAdvances in Neural Information Processing Systems\n15, pages 577–584. 2003.\n[2] R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Can-\nnam, and J. P. Bello. MedleyDB: A multitrack dataset\nfor annotation-intensive MIR research. In Proc. of the\n15th Int. Soc. for Music Information Retrieval Conf.\n(ISMIR) , Taipei, Taiwan, Oct 2014.\n[3] S. Dieleman, J. Schlüter, C. Raffel, E. Olson, S. K.\nSønderby, D. Nouri, et al. Lasagne: First release., Aug\n2015.\n[4] J. Foulds and E. Frank. A review of multi-instance\nlearning assumptions. Knowledge Engineering Review ,\n25(1):1–25, 2010.\n[5] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: Popular, classical, and jazz mu-\nsic databases. In Proc. of the 3rd Int. Conf. on Mu-\nsic Information Retrieval (ISMIR) , pages 287–288, Oct\n2002.\n[6] L. Hou, D. Samaras, T. M. Kurç, Y . Gao, J. E. Davis,\nand J. H. Saltz. Efﬁcient multiple instance convolu-\ntional neural networks for gigapixel resolution image\nclassiﬁcation. CoRR , abs/1504.07947v3, 2015.\n[7] S. Ioffe and C. Szegedy. Batch normalization: Accel-\nerating deep network training by reducing internal co-\nvariate shift. In Proc. of the 32nd Int. Conf. on Machine\nLearning (ICML) , 2015.\n[8] J. D. Keeler, D. E. Rumelhart, and W. K. Leow. In-\ntegrated segmentation and recognition of hand-printed\nnumerals. In Advances in Neural Information Process-\ning Systems 3 , pages 557–563. 1991.\n[9] S. Leglaive, R. Hennequin, and R. Badeau. Singing\nvoice detection with deep recurrent neural networks. In\nProc. of the 2015 IEEE Int. Conf. on Acoustics, Speech\nand Signal Processing (ICASSP) , Brisbane, Australia,\nApr 2015.\n[10] B. Lehner, G. Widmer, and S. Böck. A low-latency,\nreal-time-capable singing voice detection method with\nLSTM recurrent neural networks. In Proc. of the 23th\nEuropean Signal Processing Conf. (EUSIPCO) , Nice,\nFrance, 2015.\n[11] A. Liutkus, D. Fitzgerald, Z. Raﬁi, B. Pardo, and\nL. Daudet. Kernel additive models for source sep-\naration. IEEE Transactions on Signal Processing ,\n62(16):4298–4310, Aug 2014.\n[12] A. Liutkus, D. Fitzgerald, and Z. Raﬁi. Scalable au-\ndio separation with light kernel additive modelling. In\nProc. of the 2015 IEEE Int. Conf. on Acoustics, Speech\nand Signal Processing (ICASSP) , Brisbane, Australia,\nApr 2015.[13] A. L. Maas, A. Y . Hannun, and A. Y . Ng. Rectiﬁer\nnonlinearities improve neural network acoustic mod-\nels. In Proc. of the 30th Int. Conf. on Machine Learning\n(ICML) , 2013.\n[14] M. I. Mandel and D. P. W. Ellis. Multiple-instance\nlearning for music information retrieval. In Proc. of the\n9th Int. Soc. for Music Information Retrieval Conf. (IS-\nMIR) , Philadelphia, USA, 2008.\n[15] M. Mauch, H. Fujihara, K. Yoshii, and M. Goto. Tim-\nbre and melody features for the recognition of vocal\nactivity and instrumental solos in polyphonic music. In\nProc. of the 12th Int. Soc. for Music Information Re-\ntrieval Conf. (ISMIR) , 2011.\n[16] Z. Raﬁi and B. Pardo. REpeating Pattern Extraction\nTechnique (REPET): A simple method for music/voice\nseparation. IEEE Transactions on Audio, Speech, and\nLanguage Processing , 21(1):73–84, Jan 2013.\n[17] M. Ramona, G. Richard, and B. David. V ocal detection\nin music with support vector machines. In Proc. of the\n2008 IEEE Int. Conf. on Acoustics, Speech, and Signal\nProcessing (ICASSP) , pages 1885–1888, 2008.\n[18] J. Schlüter and T. Grill. Exploring data augmentation\nfor improved singing voice detection with neural net-\nworks. In Proc of the 16th Int. Soc. for Music Informa-\ntion Retrieval Conf. (ISMIR) , Malaga, Spain, 2015.\n[19] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep in-\nside convolutional networks: Visualising image clas-\nsiﬁcation models and saliency maps. In Workshop of\nthe 2nd Int. Conf. on Learning Representations (ICLR) ,\nBanff, Canada, 2014.\n[20] P. Smaragdis and J. C. Brown. Non-negative matrix\nfactorization for polyphonic music transcription. In Ap-\nplications of Signal Processing to Audio and Acoustics,\n2003 IEEE Workshop on. , pages 177–180, Oct 2003.\n[21] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M.\nRiedmiller. Striving for simplicity: The all convolu-\ntional net. In Workshop of the 3rd Int. Conf. on Learn-\ning Representations (ICLR) , San Diego, USA, 2015.\n[22] Theano Development Team. Theano: A Python frame-\nwork for fast computation of mathematical expres-\nsions. arXiv e-prints , abs/1605.02688, May 2016.\n[23] M. D. Zeiler and R. Fergus. Visualizing and under-\nstanding convolutional networks. In Proc. of the 13th\nEuropean Conf. on Computer Vision (ECCV) , pages\n818–833, Zürich, Switzerland, 2014.\n[24] Z.-H. Zhou and M.-L. Zhang. Neural networks for\nmulti-instance learning. Technical report, Nanjing Uni-\nversity, China, Aug 2002.50 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Cross Task Study on MIREX Recent Results: An Index for Evolution Measurement and Some Stagnation Hypotheses.",
        "author": [
            "Ricardo E. P. Scholz",
            "Geber L. Ramalho",
            "Giordano Cabral 0001"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416898",
        "url": "https://doi.org/10.5281/zenodo.1416898",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/054_Paper.pdf",
        "abstract": "In the last 20 years, Music Information Retrieval (MIR) has been an expanding research field, and the MIREX competition has become the main evaluation venue in MIR field. Analyzing recent results for various tasks of MIREX (MIR Evaluation eXchange), we observed that the evolution of task solutions follows two different patterns: for some tasks, the results apparently hit stagnation, whereas for others, they seem getting better over time. In this paper, (a) we compile the MIREX results of the last 6 years, (b) we propose a configurable quantitative index for evolution trend measurement of MIREX tasks, and (c) we discuss possible explanations or hypotheses for the stagnation phenomena hitting some of them. This paper hopes to incite a debate in the MIR research community about the progress in the field and how to adequately measure evolution trends.",
        "zenodo_id": 1416898,
        "dblp_key": "conf/ismir/ScholzRC16",
        "content": "CROSS TASK STUDY ON MIREX RECENT RESULTS: AN \nINDEX FOR EVOLUTION MEASUREMENT AND SOME \nSTAGNATION HYPOTHESES  \nRicardo Scholz  Geber Ramalho  Giordano Cabral  \nUniversidade Federal de  Pernambuco   \nRecife, PE, Brasil  \nreps@cin.ufpe.br  glr@cin.ufpe.br  grec@cin.ufpe.br  \nABSTRACT  \nIn the last 20 years, Music Information Retrieval (MIR) \nhas been an expanding research field, and the MIREX \ncompetition has become the main evaluation venue in \nMIR field. Analyzing recent results for various tasks of \nMIREX (MIR Evaluation eXchange), we obs erved that \nthe evolution of task solutions follows two different \npatterns: for some tasks, the results apparently hit \nstagnation, whereas for others, they seem getting better \nover time. In this paper, (a) we compile the MIREX \nresults of the last 6 years, ( b) we propose a configurable \nquantitative index for evolution trend measurement of \nMIREX tasks, and (c) we discuss possible explanations \nor hypotheses for the stagnation phenomena hitting some \nof them. This paper hopes to incite a debate in the MIR \nresearc h community about the progress in the field and \nhow to adequately measure evolution trends . \n1. INTRODUCTION  \nIn the last 20 years, mainly due to growth of audio data \navailable in the Internet, Music Information Retrieval \n(MIR) has been an expanding field of re search. It \nencompasses various problems or tasks, whose solutions \nhave impact in music market. Since 2005, the MIR \nEvaluation eXchange (MIREX) [7] is the main evaluation \n“arena” in MIR field, proposing datasets, tasks and \nmetrics to compare MIR solutions. A shallow analysis of \nits results shows they are continuously evolving for some \ntasks, whereas the y seem stagnated for other ones . \nThere are several MIR and MIREX meta -analysis pa -\npers [6][7][23][24]. However, to our knowledge, a trans -\nversal study over st agnation of results on MIR tasks is \nlacking, as well as an index for evolution trend measure -\nment. Also, stagnation phenomenon on many of these \ntasks is not yet being deeply discussed by the communit y. \nBoth the existence of common reasons and task \nspecific reasons for stagnation on MIR tasks are very \nprobable. Therefore, a deep study of stagnation \nphenomena is task -dependent, and demands the analysis of techniques, datasets and metrics used in recent yea rs. \nThen, it is out of the scope of this paper to perform a deep \nanalysis on the reasons of stagnation for each one of the \nMIREX tasks. This paper intends, instead, to provoke \nresearchers involved with MIREX tasks (stagnated or \nnot) to test some general hy potheses we suggest, and to \npropose their own task specific hypotheses.  \nUnderstanding of stagnation phenomena may be \nimproved by objective evolution trends measurement. \nComparing evolution trends between different datasets or \nmetrics, for a given task, pos sibly help to identify how \nmetrics and datasets bias observable results, or how each \nsub-problem of the task is more or less developed. In \naddition, evolution trends comparison between different \ntasks provide an overall picture of evolution in MIR \nresearch , drawing attention to what kind of methods and \nstrategies are being used on developing tasks that could \nbe adapted for stagnated ones.  \nThis paper presents an accurate empirical analysis of \nMIREX recent results. It also proposes a configurable \nquantitative  index for evolution trends measurement. \nFinally, it raises some hypotheses and questions that \ncould possibly explain stagnation phenomena and/or \nhopefully help MIR research community to exchange \nmore information about it in order to move forward.  \nSection 2 presents method used to analyze data. We \nexplain and formalize a  configurable  index for evolution \nmeasurement on Section 3. Section 4 raises hypotheses \nabout possible causes of stagnation. Section 5 draws \nsome general conclusions on the performed analysi s. \nFinally, future works are listed on Section 6.  \n2. METHOD  \nMIREX is the MIR competition that became the main \nevaluation venue in MIR field. It has been running since \n2005. According to MIREX 2015 final results’ poster, \n107 researchers from 64 teams participa ted in the last \nedition and submitted algorithms for 21 active tasks, \nresulting in 402 runs, over 47 different datasets [11].  \nMIREX contributions to the MIR community are \nevident. Influential MIR researchers have identified four \nkey contributions of MIREX : “training and induction into \nMIR”, “dissemination of new research”, “dissemination \nof data” and “benchmarking and evaluation” [7].  \nIn order to evaluate research progress in MIR tasks, \nwe could have tried to compare results published in  © Ricardo Scholz , Geber Ramalho , Giordano Cabral . \nLicensed under a Creative Commons Attribution 4.0 International \nLicense (CC BY 4.0). Attribution:  Ricardo Scholz , Geber Ramalho , \nGiordano Cabral . “Cross -task Study on MIREX Recent Results: an \nIndex for Evolution Measurement and Some Stagnation Hypotheses ”, \n17th International Society for Music Information Retrieval Conference, \n2016.  \n372  \n \nrecent years. Howe ver, we decided to focus analysis in \nMIREX because it can be more systematic , since: (1) \nMIREX tasks are well defined, and (2) submissions from \ndifferent years to a given task/subtask run over the same \ndatasets and (3) the results are evaluated using the s ame \nmetrics. We do acknowledge the limitations of this \nmethodological choice, since not all MIR algorithms \ndeveloped have been evaluated in MIREX competition. \nBut, for the sake of comparison precision and \nextensiveness, it seemed to be the best choice . \nA timeline of tasks, subtasks and datasets used for \neach task or subtask was constructed with data collected \nfrom MIREX results between 2010 and 2015 [11]. In \norder to analyze tendencies, it is necessary to consider a \nrelevant time frame, as well as to guaran tee comparisons \nover time are consistent. As inclusion criterion, only tasks \nfor which there was at least one dataset used for at least \nfive editions since 2010 were admitted. The rationale of \nthis choice is that observing a unique dataset ensures \nconsiste ncy and comparability of results, whereas \nconsidering at least five editions provides a reliable time \nwindow for trend analysis. Though, from all 28 tasks \nproposed between the first edition in 2005 and the last in \n2015, 4 tasks were discontinued until 2008 , other 6 tasks \nwere considered very recent (started in 2013 or later), \nwhereas the remaining 18 tasks were analyzed in this \nstudy, including 3 active tasks in 2014 which did n ot run \nin 2015 . \nWe assumed datasets and methods for metrics \ncomputation did not change, except when explicitly \ndocumented on the task’s MIREX official wiki or results’ \npages [11]. Among the remaining candidates, one dataset \nfor each task or subtask was chosen to collect data for \nanalysis. When more than one dataset was available, \nolder datasets were preferred, to allow future researches \nto extend this work by comparing backwards. For Audio \nGenre Classification task, two datasets were equally \nolder: Mixed Set and Latin. Mixed Set was then chosen, \nas a more generic set tends to provide a  more realistic \npicture of the state of the art.  \nAmong 18 analyzed tasks, 3 tasks presented more \nthan one subtask. “MF0 Estimation & Tracking” is \ndivided into “MF0 Estimation” and “Note Tracking”. \nActually, we believe that they could be two different \ntasks  themselves, due to the different nature of their \nobjectives. Then, both subtasks were analyzed. For \n“Query -by-tapping” (QBT), two subtasks are available: \n“QBT with symbolic input” (subtask 1) and “QBT with wave input” (subtask 2). We analyzed subtask 1, s ince \nonset files allow participants to concentrate on similarity \nmatching, which is the main objective of the task, instead \nof onset detection. Finally, “Query -by-Singing/  \nHumming” (QBSH) presented two subtasks: “Classic \nQBSH evaluation” and “Variants QBSH  evaluation”. \nClassic evaluation (subtask 1) was chosen, since the \nvariants evaluation adds constraints to the original \nproblem – for instance, considering queries as variants of \n“ground -truth” midi.  \nEach task has several metrics computed. As our \nanalysis needs to rank results, for the sake of comparison, \none metric for each task or subtask was chosen. As this \nanalysis aims to understand evolution of the state of the \nart on each task, more general metrics were assumed to \nprovide a more realistic picture of each task’s \nperformance. Then, metrics often used in MIREX Overall \nResults Posters [11] and metrics measuring overall \nperformance were chosen, at the expense of those \nmeasuring a given characteristic of the algorithms. For \ninstance, F -Measure was preferred  when tasks also \ncompute Precision and Recall, as Precision and Recall \ncompute specific performances whereas F -Measure \nrelates to both Precision and Recall.  \nConsidering the chosen metrics, top results for each \ntask were analyzed. We then noticed that two g roups \nemerged: “tasks presenting stagnated results” and “tasks \npresenting evolving results”. The first group included \ntasks which presented no significant improvement on \nresults in the last years of the competition. And the \nsecond group included tasks whos e results’ evolution is \nnoticeable in the last six years. Of course, there is a high \nlevel of subjectivity on deciding when a given task is \nevolving (and at which pace), or stagnated. For a \nsystematic analysis, a quantitative index for evolution \nmeasuremen t is necessary.  \n3. AN INDEX FOR EVOLUTI ON MEASUREMENT  \nTo perform our study, we needed a quantitative index for \nmeasuring results’ evolution trends, in order to \ndistinguish stagnated resu lts from evolving ones. In this  \nsection  we introduce what we called “Wei ghted \nEvolution Measurement Index” (WEMI), and we discuss \nits semantics.  \nMeasuring stagnation phenomena by just looking to \nevolution graphs  has limitations , as similar graphs may \n \nFigure 1. Examples of different evolution trends: (a) stagnation trend; (b) continuous evolution trend; and (c) recovery \nfrom recent stagnation trend.  \nProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 373  \n \n \nFigure 2. Some tasks results evolution plots (3 top results per year) and respective WEMI values  (w=0.6  and \nc=0.0713 ): (A) “Audio Music Mood Classification ”; (B) “Music Structure Segmentation ”; (C) “Audio Chord Estim a-\ntion”; ( D) “Audio Melody Extraction ”; (E) Query -by-Singing/Humming”; and ( F) “Score Following”; top historical r e-\nsults ( squares) were used for trend analysis.  \nbe hard to distinguish. For a state of the art analysis of \ntrends, we must be  able to objectively  differentiate \ncontinuous  from intermittent  evolution, as well as \nmeasure how evolution occurred over time  and \nconsistently compare evolution of different tasks . For \ninstance, consider Figure 1, which shows different \nhypothetical evolution scenarios. In all cases, results \nevolve from 0.2 to 0.8, so the first and the last result of all \nseries coincide (overall error drop was exactly the same). \nHowever, the way evolution occurred is different in each \ncase, so evolution trends are not the same. First series (a) \nshows a clear stagnation trend , as no recent improvement \noccurred after a huge improvement in the past. Second \nseries (b) shows a continuous evolution  of results, with \nsmall improvements every year. Finally, third series (c) \nshows a huge recovery from a recent stagnation period , \nas recent improvements occurred after many years \nwithout any improvement. Theref ore, it is interesting that \nan index for evolution trend measurement can be properly \nbalanced to differentiate these scenarios. In add ition, such \nan index must be consistent in scenarios of complete \nstagnation (i.e., no evolution since the beginning of the  \nseries) . \nConsidering that we are interested in state of the art \nevolution, it does make sense to discard results which did \nnot overcome the top result achieved so far. Then, the \nproposed index considers evolution as a monotonically \nincreasing function. Fi gure 2 shows various examples of \nactual top results per year, and results selected for trend \nanalysis.  \nThe index we propose considers a series of results \nfrom year i to year f (in this study, i = 2010  and f = \n2015 ). According to the chosen metric and state  of development of each task, bias may occur if we observe \ntop results directly. In order to avoid it, we consider \nrelative error drop rate from one year to the next.  \nError, in a given year y, such that i ≤ y ≤ f, is defined \nas: \n                                       (1) \n \nWhere rj is the top result achieved in year j. The error \ndrop rates are then computed for each pair of successive \nyears ( y-1 and y, such that  i+1 ≤  y  ≤  f ), as:  \n \n                                                       \n \nRecent evolution  is reinforced by higher weights of \nerror drop rates for recent years, so that recent \nimprovements tend to push WEMI up more than results \nachieved many years ago, even if the error drop rate in \nboth cases was the same.  \nContinuous evolution  is reinforced with a direct \nproportionality between WEMI and the number of actual \nimprovements within the time frame. This way, WEMI \ntends to be higher when continuous evolutions are \nachieved each year, in comparison with the situation in \nwhich the same ov erall evolution is achieved from one \nyear to the next, at once. Then, WEMI is defined as:  \n \n                     \n     \n          \n         \n             \n \nThe number of improv ements over previous top result  \nbetween i+1 and f (i.e., the number of times ∆ej is large r \n374 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016  \n \nTask  Dataset  Used  Metric  Observed  YHTR  q OEDR  WEMI  \nAudio Music Sim. and Retr . Default  Av. Fine Score Human Eval.  2011  1 0.04 0.0188  \nAudio Music Mood Classif.  MIREX 2007  Normalized Class. Accuracy  2011  1 0.14 0.0202  \nMusic Structure Segment.  MIREX 2009  Frame Pair Clust. F -Measure  2012  1 0.10 0.0216  \nAudio Tag Classification  Maj/Min Tag  Tag Classification Accuracy  2011  1 0.11 0.0254  \nMFFE&T – MF0 Estimat.  MIREX 2009  Chroma Precision  2011  1 0.36 0.0324  \nAudio Class. Comp. Ident.  MIREX 2009  Normalized Class. Accuracy  2011  1 0.39 0.0341  \nSymbolic Melodic Simil.  Essen Col.  \"Fine\" score1 2013  2 0.12 0.0398  \nAudio Key Detection  MIREX 2005  Weighted Key Score  2013  1 0.26 0.0540  \nAudio Chord Estimation  MIREX 20092 Weigh. Chord Symbol Recall  2011  1 0.87 0.0608  \nClassic Query -by-Tapping  Roger Jang  Simple Count  2012  1 0.29 0.0738  \nAudio Onset Detection  MIREX 2005  Average F -Measure  2013  3 0.40 0.0801  \nAudio Genre Classification  Mixed Popular3 Normalized Class. Accuracy  2014  2 0.37 0.0829  \nAudio Melody Extraction  MIREX 2005  Overall Accuracy  2014  2 0.33 0.0901  \nAudio Tempo Estimation  MIREX 2006  Average P -Score  2015  2 0.18 0.1014  \nAudio Beat Tracking  MCK  F-Measure  2015  4 0.20 0.1038  \nMFFE&T – Note Tracking  MIREX 2009  Average F -Measure4 2014  3 0.58 0.1731  \nQuery -by-Singing/Humm.  Roger Jang  Simple Count  2015  1 0.51 0.2353  \nScore Following5 Not identified6 Total Precision  2015  4 0.83 0.4225  \nAudio Cover Song Identif . Mixed Collec.  Total num. of cov. id. in top 107 2013  1 N/A N/A \n1 Sum of fine -grained human similarity decisions. | 2 Major/minor triads classification. | 3 Also known as US Pop Music. | \n4 For onset only over chroma . | 5 Also  known as “Real Time Audio to Score Alignment”. | 6 MIREX result pages me n-\ntions 3 datasets, but we could not identify which one was considered for the results in provided tables. | 7 Metric “mean \nnumber of covers identified in top 10 (average performance)”  would be preferable, but is not available for all years.  \nTable 1.  Analyzed tasks’ general information ; WEMI computed for w = 0.6 and c = 0.0713; YHTR stands for “Year of \nHistorical Top Result”; OEDR stands for “Overall Error Drop Rate”, computed as OEDR =  1 – (e2015/e2010). \nthan zero, for i+1 ≤   j ≤  f) is called q (if no improvement \noccurred, WEMI must be zero). Two configurable \nconstants, w and c, are defined such that 0 < w ≤ 1  and c \n> 0. Clearly, the  closer w is to zero, the greatest the \nweight of recent  improvements  on final index, whereas \nthe closer it is to 1, more equalized weights are used. \nAlso, the closer c is to zero, the lowest the weight of \nconstant evolution  on final WEMI value. In this study, \nwe computed WEMI for a variety of w and c values  \n(results are available  at https://goo.gl/bxwrDy ). Balancing \nw and c depends mainly on the importance one gives to \nrecent  against continuous  evolution. Therefore , we \nbelieve a discussion on MIR community about this trade \noff would lead to more appropriate  balancing of the index \nfor MIREX tasks , considering the goal of identifying \nbottlenecks of evolution and/or evaluation . \nThe “Audio Cover Song Identification” task could not \nhave WEMI computed, as expected “total number of \ncovers identified in top 10” (T10) is not available. \nHowever, results almost doubled T10 from 908 in 2010 \nto 1714 in 2013, regardless of the absence of \nimprovements in other MIREX editions.  \nA total of 18 tasks were an alyzed, with “MF0 \nEstimation & Tracking” comprising two subtasks. This \nresulted in 19 task/subtasks . Table 1 shows a summary of \nthe analysis , with examples of output  for w = 0.6 and c = 0.0713  (the average weighted sum of error drop rates of \nall tasks , considering w = 0.6). \nWEMI is a first proposal and a provocation for a \nbroader discussion about evolution measurement indexes \nfor MIR tasks, especially on MIREX competition.  \nObjective and early identification of stagnation trends \nmay raise earlier discussions  in the community about the \nappropriateness of metrics, datasets or current methods \nfor a given task, probably helping to shorten future \nstagnation periods or to improve current metrics and/or \ndatasets . Evidently, computing WEMI for a single metric \nof a ta sk may be misleading. On the other hand, \ncomputing it for many metrics of a task will probably \nlead to a greater understanding of specific bottlenecks in \ntask evaluation and/or evolution.  \n4. SOME STAGNATION HYPO THESES  \nStagnation on most MIR task results is al ready \nacknowledged by MIR community, as in the case of \nsinger identification in polyphonic audio [13], music  \ntranscription [2],  emotion and genre classification \n[14][19], music similarity [8][18], and so on. In spite of \nthis acknowledgement, there is not  much discussion \nabout possible hypothesis which could explain the \nphenomena.  Sturm [22], among others [23][24], have Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 375  \n \nrecently raised questions about the experimental validity \nin MIR evaluation, stating reliable evaluation remains \nneglected in MIR researc h. Even though, data put \ntogether in this research inspire some questions. This \npaper intends to provoke this kind of questions,  and its \nexplanation hypotheses . \nIn order to encourage this discussion, identifying \nwhether MIREX manages to satisfactorily measure \nimprovements of performance for its various tasks is \nnecessary. If this is true, why so many of them are \nstagnated? Temporary solution stagnation phenomena are \na normal stage in scientific development. However, some \nmechanisms co uld be employed to shorten them . \nAs we said before, since the explanations for \nstagnation may be task -dependent, it is difficult to \nprovide general explanations for stagnation, and then \nhints on how to overcome it. Nevertheless, from some \ndiscussion on the literature of spec ific tasks, coupled with \nour own experience in MIR, we have formulated two \nhypotheses that may possibly help researchers to move \nforward. These hypotheses are not meant to be correct, \nbut rather to start a transversal discussion among \nstagnated tasks.  \nThe first hypothesis is: MIR approaches should \nperhaps be more musical knowledge -intensive.  \nAccording to Downie [6], in 2008, community members \nwere becoming aware of the limitations of MIR “generic \napproaches”, i.e. the application of information retrieval \nsolutions for music, without relying on musically \nmeaningful features. However, since then, most of works \nin MIREX seems to still rely on more generic IR \ntechniques than on an in -depth use of specific music \nknowledge. It is true that embedding music expert \nknowledge pawn generality of the approaches, but \nperhaps this could be path to move away from stagnation.  \nLet’s take “Audio Chord Estimation” as an example. \nChord estimation is apparently stuck into a kind of glass \nceiling. Very often, approaches are agnos tic, neglecting \ncontextual information or musical knowledge after \nfeature selection. The most successful approaches in \nMIREX often use probabilistic machine learning \ntechniques, mostly through neural networks, as HMM, \nMLN or Bayesian [3][17][20][25]. A few  approaches \nmake use of specialist knowledge, applying it on the \nlower levels of symbolic information, in order to improve \nfeature vector quality [4][12]. A deeper study on “Audio \nChord Estimation” state of the art, performed by McVicar \net al. [16] , observ ed advances in feature extraction and \nmodeling stage, as well as expert musical knowledge use \nfor model training, but no musical knowledge use for \npost-processing , for instance . \nMusical creation process is essentially artistic. Then, \nperplexity of harmonic  sequences in real world tends to \nbe high, implying less predictability. In fact, one cannot \ntalk about a correct or wrong chord sequence, as in most \nclassification problems . A composer not only is free to create novel chord sequences, but he or she tends to look \nfor them. Therefore, purely probabilistic approaches are \nlimited by the predictability of analyzed corpus, meaning \nthat uncommon (artistically novel) chord sequences may \nbe misrecognized. In addition, other variables may \ninterfere in harmonic seque nces, such as genre (jazz \nharmony differs strongly from rock harmony) or style (a \ngiven musicia n tends to prefer some chord se quences).  \nThere are evidences that musical knowledge can \nimprove chord estimation [21][1][5][15]. Therefore, we \nbelieve that impro vements can be achieved using musical \nknowledge on higher levels of information and contextual  \ninformation to decide what chord is represented by a \ngiven feature vector. By higher levels of information we \nbasically mean musical theory applied over symbolic  \ninformation. For instance, the use of functional harmonic \nanalysis, which has been proved to add relevant \ninformation to chord sequences [21][5], to chose, among \ncandidate chords, the ones which lead to more \nmeaningful chord sequences, even when their fea ture \nvector are not the first options provided by a feature \nvector based classifier.  \nMusic structure information has also been shown to \nadd relevant contextual information for chord estimation \n[15]. For instance, the classification of “easy” chords first \nand the use of this information to help classification of \n“harder” ones, according to the harmonic meaning of the \nsequence they would lead to, or using harmonically \nsimilar pieces already classified of the same song \n(sometimes with better conditions to feat ure extraction \nand classification, such as less noise, transients, arpeggios \nor ornamentation) may lead to improvement on current \nresults.  \nThe second hypothesis is: the number of techniques \nemployed by the MIREX community is perhaps too \nlimited. It is very  difficult to prove that a particular \ntechnique is not used by the community, as failed \nattempts are rarely published. But observing recent \nISMIR publications, we noticed that each task presents a \nsmall set of often used techniques. For instance, chord \nestimation tends to rely mostly on HMM, but also on \nMLN or Bayesian networks, for classification.  \nTo reinforce our hypothesis, we analyze the impact of \na specific technique in MIREX results, showing how the \nuse of a new technique can affect results. Chosen \ntechnique was Deep Learning, which dates back to the \nNeo-cognition introduced by K. Fukushima in 1980 [9], \nbut only a few years ago have been found promising for \nMIR.  \nIn 2012, Humphrey warned about the lack of deep \nlearning approaches in MIR research [10]. A nalyzing the \ntop results from 2010 to 2015, among the 19 \ntasks/subtasks considered in this work, we can notice \nthat, in the last 3 years, 11 tasks had their results \nimproved, but only 3 of the improvements came from \napproaches using deep learning technique s, according to 376 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016  \n \nthe technical reports submitted to MIREX. This may \nsuggest that (1) deep learning is not being extensively \nexplored yet in MIR and (2) if deep learning could \nimprove results in three of the tasks, it is fair to consider \nthere are possibly o ther techniques, yet not explored, with \nsimilar potential.  \nRegarding the first assertion, it might be due to the \nlack of enough labeled data, meaning that some tasks are \nnot even eligible for Deep Learning yet. In this case, it \nwould be fair to consider th e creation of new datasets or \nenlargement of existing ones as a possible path to \novercome stagnation on these tasks.  \nOf course, other hypotheses could be deeper \ninvestigated . For instance, the lowest WEMI values  (even \nfor several different w and c values)  belong to tasks \nwhich use human generated ground truth data. Further \ninvestigation of this relation could lead to relevant \ninformation.  Unsuitability or limitation of the datasets \nand metrics of the stagnated tasks  are worth to \ninvestigate . However many o f these hypotheses are task \ndependent and such an investigation would be better \nperformed by specialists on each task.  \n5. CONCLUSIONS  \nTrying to understand recent practical advances in MIR \nresearch, a compilation of the last 6 years of MIREX \nresults for 18 tasks (one of them comprised of two sub -\ntasks) was performed. Aiming to encourage discussion on \nhow to measure progress in MIR, w e propose a \nconfigurable quantitative index of improvement, the \n“Weighted Evolution Measurement Index” (WEMI), in \norder to objectively measure trends on each task, in a \ncomparable way, reinforcing recent  and continuous  \nadvances. We believe such an index may help \nunderstanding bottlenecks of evolution or measurement \nissues, by comparing different datasets and metrics f or a \ngiven task (intra -task analysis), as well as helping to \novercome stagnation, by task comparisons, observing \nwhether methods and strategies of evolving tasks are \nbeing applied to stagnated ones (inter -task analysis).  The \nindex can be balanced, accordin g to the community’s \nunderstanding of what is most relevant: continuous  or \nrecent  evolution.  Also, we raise hypotheses and questions \nabout stagnation affecting many of MIR tasks and we \npoint some possible insights on this matter. We believe \nthat a deeper d iscussion in MIR community about \nstagnation phenomena affecting many of MIR tasks may \nhelp to find general mechanisms or strategies which will \nallow overcoming it, as well as improvi ng MIREX \ninterest and relevance . \n6. FUTURE WORK  \nIt would be possible to obtai n a more detailed overview \nof MIREX tasks’ trends with a number of additional \ninformation, for instance: (1) comparing WEMI computed for other metrics or other datasets, in a given \ntask, will probably help understanding if metrics or \ndatasets are biasing o bservable evolution trends at first \nsight; and/or (2) a deeper study on discrepant results (for \ninstance, “Classical Composer Identification, 2011” and \n“Chord Estimation, 2011”, as seen in Figure 2) in order to \nidentify overfitting or other distortions of top result will \ncertainly improve accuracy. Another interesting \nimprovement would be adaptation of WEMI so that total \nweights sum is normalized by the time window, as this \nwould allow more consistent comparisons between ti me \nwindows of different lengths , if this makes sense in a \ngiven context . Finally, a formal  evaluation of the  index  is \nstill missing . \n7. REFERENCES  \n[1] Bello, J. P.; Pickens, J. 2005. A Robust Mid -level \nRepresentation for Harmonic Content in Music \nSignals. Proc . of the 6th International Society for \nMusic Information Retrieval Conference  (London, \nUnited Kingdom, September 11 – 15), ISMIR’05. \n304-311. \n[2] Benetos, E., et al. 2013. Automatic music \ntranscription: challenges and future directions. \nJournal of Intelligent Information Systems , 41, 3 \n(July 2013), 407 -434. DOI=10.1007/s10844 -013-\n0258 -3. \n[3] Chen, R.; Shen, W.; Srinivasamurthy, A.; Chordia, \nP. 2012. Chord recognition using duration -explicit \nhidden Markov models. Proc.  of the 13th \nInternational Society for Music Inf ormation \nRetrieval Conference  (Porto, Portugal, October 8 – \n12, 2012), ISMIR’12. 445 -450. \n[4] Cho, T.; Bello, J. P. 2011. A feature smoothing \nmethod for chord recognition using recurrence plots. \nProc.  of the 12th International Society for Music \nInformation Ret rieval Conference  (Miami, USA, \nOctober 24 – 28, 2011), ISMIR’11. 651 -656. \n[5] De Haas, W. B.; Rodrigues Magalhães, J. P. and \nWiering, F. 2012. Improving Audio Chord \nTranscription by Exploiting Harmonic and Metric \nKnowledge.  Proc.  of the 13th International Soci ety \nfor Music Information Retrieval Conference  (Porto, \nPortugal, October 8 – 12, 2012), ISMIR’12. 295-\n300. \n[6] Downie, J. 2008. The music information retrieval \nevaluation exchange (2005 –2007): A window into \nmusic information retrieval research. Acoustical \nScience and Technology , 29, 4, 247 -255. \nDOI=10.1250/ast.29.247.  \n[7] Downie, J., et al. 2014. Ten years of MIREX: \nreflections, challenges and opportunities. In Proc.  of \nthe 15th International Society for Music Information Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 377  \n \nRetrieval Conference  (Taipei, Taiwan, Octob er 27 – \n31, 2014). ISMIR’14. 657 -662. \n[8] Flexer, A. 2014. On Inter -rater Agreement in Audio \nMusic Similarity. In Proc.  of the 15th International \nSociety for Music Information Retrieval Conference  \n(Taipei, Taiwan, October 27 – 31, 2014 ), ISMIR’14. \n245-250. \n[9] Fukushima, K. 1980. Neocognitron: A self -\norganizing neural network model for a mechanism \nof pattern recognition unaffected by shift in \nposition.  Biological Cybernetics , 36, 4, 193 –\n202. DOI = 10.1007/bf00344251 . \n[10] Humphrey, E. J.; Bello, J. P.; Lecun, Y. 2012. \nMoving Beyond Feature Design: Deep Architectures \nand Automatic Feature Learning in Music \nInformatics. Proc.  of the 13th International Society \nfor Music Information Retrieval Conference  (Porto, \nPortugal, October 8 – 12, 2012), ISMIR’12. 403-\n408. \n[11] ISMIRSEL. 2 016, January 8. MIREX Home \n[Online] . Available : http://www.music -ir.org/mirex/  \nwiki/MIREX_ HOME.  \n[12] Khadkevich, M.; Omologo, M. 2011. Time -\nfrequency reassigned features for automatic chord \nrecognition. Proc.  of the 36th IEEE International \nConference on Acoustics, Speech and Signal \nProcessing (Prague, Czech Republic, May 22 – 27, \n2011), ICASSP’11. 181 -184. \n[13] Lagrange, M., Ozerov, A., and Vincent, E.. 2012. \nRobust singer identification in polyphonic music \nusing melody enhanc ement and uncertainty -based \nlearning. Proc.  of the 13th International Society for \nMusic Information Retrieval Conference  (Porto, \nPortugal, October 8 – 12, 2012), ISMIR’12. 595 -\n600. \n[14] Lidy, T., et al. Improving Genre Classification by \nCombination of Audio and  Symbolic Descriptors \nUsing a Transcription System. Proc.  of the 8th \nInternational Society for Music Information \nRetrieval Conference  (Viena, Austria, September 23 \n– 27, 2007), ISMIR’07. 61 -66. \n[15] Mauch, M.; Noland, K. and Dixon, S. 2009. Using \nMusical Struct ure to Enhance Automatic Chord \nTranscription.  Proc.  of the 10th International \nSociety for Music Information Retrieval Conference  \n(Kobe , Japan , October  26 – 30, 200 9), ISMIR’ 10. \n231-236. \n[16] McVicar, M.; Santos -Rodríguez, R.; Ni, Y. and De \nBie, T. 2014. Automat ic Chord Estimation from \nAudio: A Review of the State of the Art . Proc.  of \nIEEE/ACM Transactions on Audio, Speech, and \nLanguage Pro cessing , 22, 2, 556-575. DOI = 10.1109/TASLP.2013.229458.  \n[17] Ni, Y.; McVicar, M.; Santos -Rodríguez, R.; De Bie, \nT. 2012. An end -to-end machine learning system for \nharmonic analysis of music. Proc.  of the IEEE \nTransactions on Audio, Speech and Language \nProcessing , 20, 6, 1771 -1783. DOI = 10.1109/ \nTASL.2012.2188516.  \n[18] Pachet, F. and Aucouturier, J. 2004. Improving \ntimbre similar ity: How high is the sky?. Journal of \nNegative Results in Speech and Audio Sciences , 1, 1, \n1-13. \n[19] Panda, R., et al. 2013. Multi -modal music emotion \nrecognition: A new dataset, methodology and \ncomparative analysis. Proc.  of the 10th International \nSymp.  on Co mputer Music Multidisciplinary \nResearch  (Marseille, France, October 15 – 18, \n2013), CMMR’13. 570 -582. \n[20] Papadopoulos, H.; Tzanetakis, G. 2012. Modeling \nchord and key structure with Markov logic.  Proc.  of \nthe 13th International Society for Music Information \nRetrieval Conference  (Porto, Portugal, October 8 – \n12, 2012), ISMIR’12. 127 -132. \n[21] Scholz, R.; Vincent, E.; Bimbot, F. 2009. Robust \nmodeling of musical chord sequences using \nprobabilistic N -grams.  Proc.  of the 34th IEEE \nInternational Conference on Acoustics, Speech and \nSignal Processing (Taipei, Taiwan, April 19 – 24, \n2009), ICASSP’09. 53–56. \n[22] Sturm, B. L. A Simple Method to Determine if a \nMusic Information Retrieval System is a \"Horse\". \n2014. In IEEE Transactions on  Multimedia , 16, 6 \n(July 2014), 1636 -1644. DOI  = 10.1109/TMM.2014.  \n2330697.  \n[23] Urbano, J. 2011. Information Retrieval Meta -\nevaluation: Challenges and Opportunities in the \nMusic Domain. In Proc.  of the 12th International \nSociety for Music Information Retrieval  Conference  \n(Miami, USA, October 24 – 28, 2011), ISMIR’11. \n609-614. \n[24] Urbano, J., Schedl, M. and Serra, X. 2013. \nEvaluation in Music Information Retrieval. Journal \nof Intelligent Information Systems , 41, 3 (December \n2013), 345 -369. DOI= 10.1007/s10844 -013-0249-4. \n[25] Yoshii, K.; Mauch, M.; Goto, M. 2011. A unified \nprobabilistic model of note combinations and chord \nprogressions. Workshop Book of Neural Information \nProcessing Systems, 4th International Workshop on \nMachine Learning and Music: Learning from \nMusical Structure  (Sierra Nevada, USA, December \n17, 2011), MML’11. 46.  378 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Genre Ontology Learning: Comparing Curated with Crowd-Sourced Ontologies.",
        "author": [
            "Hendrik Schreiber 0001"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417479",
        "url": "https://doi.org/10.5281/zenodo.1417479",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/074_Paper.pdf",
        "abstract": "The Semantic Web has made it possible to automati- cally find meaningful connections between musical pieces which can be used to infer their degree of similarity. Simi- larity in turn, can be used by recommender systems driving music discovery or playlist generation. One useful facet of knowledge for this purpose are fine-grained genres and their inter-relationships. In this paper we present a method for learning genre ontologies from crowd-sourced genre labels, exploiting genre co-occurrence rates. Using both lexical and con- ceptual similarity measures, we show that the quality of such learned ontologies is comparable with manually cre- ated ones. In the process, we document properties of cur- rent reference genre ontologies, in particular a high degree of disconnectivity. Further, motivated by shortcomings of the established taxonomic precision measure, we define a novel measure for highly disconnected ontologies.",
        "zenodo_id": 1417479,
        "dblp_key": "conf/ismir/Schreiber16",
        "content": "GENRE ONTOLOGY LEARNING:\nCOMPARING CURATED WITH CROWD-SOURCED ONTOLOGIES\nHendrik Schreiber\ntagtraum industries incorporated\nhs@tagtraum.com\nABSTRACT\nThe Semantic Web has made it possible to automati-\ncally ﬁnd meaningful connections between musical pieces\nwhich can be used to infer their degree of similarity. Simi-\nlarity in turn, can be used by recommender systems driving\nmusic discovery or playlist generation. One useful facet\nof knowledge for this purpose are ﬁne-grained genres and\ntheir inter-relationships.\nIn this paper we present a method for learning genre\nontologies from crowd-sourced genre labels, exploiting\ngenre co-occurrence rates. Using both lexical and con-\nceptual similarity measures, we show that the quality of\nsuch learned ontologies is comparable with manually cre-\nated ones. In the process, we document properties of cur-\nrent reference genre ontologies, in particular a high degree\nof disconnectivity. Further, motivated by shortcomings of\nthe established taxonomic precision measure, we deﬁne a\nnovel measure for highly disconnected ontologies.\n1. INTRODUCTION\nIn the 15 years since Tim Berners-Lee’s article about the\nSemantic Web [2], the Linking Open Data Community\nProject1has successfully connected hundreds of datasets,\ncreating a universe of structured data with DBpedia2at\nits center [1, 3]. In this universe, the de facto standard for\ndescribing music, artists, the production workﬂow etc. is\nThe Music Ontology [15]. Examples for datasets using it\nare MusicBrainz/LinkedBrainz3 , 4and DBTune5. While\nin practice taking advantage of Linked Open Data (LOD)\nis not always easy [9], semantic data has been used suc-\ncessfully, e.g. to build recommender systems. Passant et\nal. outlined how to use LOD to recommend musical con-\ntent [14]. An implementation of this concept can be found\n1http://www.w3.org/wiki/SweoIG/TaskForces/\nCommunityProjects/LinkingOpenData\n2http://wiki.dbpedia.org/\n3http://musicbrainz.org/\n4http://linkedbrainz.org/\n5http://dbtune.org/\nc/circlecopyrtHendrik Schreiber. Licensed under a Creative Commons\nAttribution 4.0 International License (CC BY 4.0). Attribution: Hen-\ndrik Schreiber. “Genre Ontology Learning:\nComparing curated with crowd-sourced ontologies”, 17th International\nSociety for Music Information Retrieval Conference, 2016.in [13]. Tatlı et al. created a context-based music rec-\nommendation system, using genre and instrumentation in-\nformation from DBpedia [18]. Di Noia et al. proposed a\nmovie recommender based on LOD from DBpedia, Free-\nbase6, and LinkedMDB7[8]. And recently, Oramas et\nal. created a system for judging artist similarity based on\nbiographies linked to entities in LOD-space [11]. Many of\nthese approaches are trying to solve problems found in rec-\nommender systems relying on collaborative ﬁltering, like\ncold start or popularity bias [4].\nAmong other data, genre ontologies are a basis for these\nsystems. They allow the determination of degree of simi-\nlarity for musical pieces (e.g. via the length of the shortest\nconnecting path in the ontology graph), even if we have\nno other information available. Surprisingly, we know lit-\ntle about the genre ontologies contained in repositories like\nDBpedia. How large and deep are they? How well do they\nrepresent genre knowledge? Are they culturally biased?\nHow interconnected are genres in these ontologies?\nWhile editors of LOD ontologies often follow estab-\nlished rules, it is an inherent property of any ontology that\nits quality is subjective. An alternative are learned ontolo-\ngies. Naturally, they do not represent objective truth either,\nbut instead of relying on design principles, they use empir-\nical data. An interesting question is: How do curated genre\nontologies compare with learned ontologies?\nIn the following we are attempting to answer some of\nthese questions. Section 2 starts with proposing a method\nfor building a genre ontology from user-submitted genre\ntags. In Section 3, we describe the existing genre ontolo-\ngies DBpedia and WikiData as well as two new ontologies\ncreated with the method from Section 2. In Section 4, we\ndescribe evaluation measures loaned from the ﬁeld of on-\ntology learning. Our results are discussed in Section 5, and\nour conclusions are presented in Section 6.\n2. BUILDING THE GENRE GRAPH\nAs shown in [16], it is possible to create genre taxonomy\ntrees from user-submitted genre labels. These trees have\nbeen proven useful for inferring a single top-level genre\nfor a given sub-genre. Unfortunately, taxonomy trees are\ninsufﬁcient when attempting to model the complex inter-\ngenre relations found in the real world. The concept of a\nfusion-genre for example, i.e. a connection between two\n6http://www.freebase.com/ — to be shut-down soon.\n7http://www.linkedmdb.org/400otherwise separate taxonomy trees, is impossible to repre-\nsent. Therefore, an ontology is commonly regarded as the\npreferred structure to model genres and their relations.\nSimilar to [5], we deﬁne a genre ontology as a struc-\ntureO= (C,root,≤C)consisting of a set of concepts\nC, a designated root concept and the partial order ≤Con\nC∪{root}. This partial order is called concept hierarchy.\nThe equation∀c∈C :c≤Croot holds for this concept\nhierarchy. For the sake of simplicity, we treat the relation\nbetween genre names and genre concepts as a bijection,\ni.e. we assume that each genre name corresponds to ex-\nactly one genre concept and vice versa.\nTo construct a genre ontology based on suitably normal-\nized labels, we ﬁrst create a genre co-occurrence matrix M\nas described in [16]. The set C={c1,c2,...,c n}contains\nngenres. Each user submission is represented by a sparse\nvectoru∈Nnwith\nui=/braceleftbigg1,ifci=user-submitted genre\n0,otherwise.(1)\nEach song is represented by a vector s∈Rn. Eachs\nis deﬁned as the arithmetic mean of all user submissions u\nassociated with a given song. Thus sidescribes the relative\nstrength of genre ci. Co-occurrence rates for a given genre\nciwith all other genres can be computed by element-wise\naveraging all sfor whichsi/negationslash= 0is true:\nMi=s,∀swithsi/negationslash= 0;M∈Rn×n(2)\nUnlike [16], we normalize the co-occurrence rates from\nMso that the maximum co-occurrence rate of one genre\nwith another is 1. This normalized co-occurrence matrix\nis calledN. Just likeM,Nis asymmetric. For example,\nalternative strongly co-occurs with rock , butrock\nco-occurs not as strongly with alternative . We take\nadvantage of this by deﬁning a rule that helps us ﬁnd sub-\ngenres: If a genre cico-occurs with another genre cjmore\nthan a minimum threshold τ,cjco-occurs with cimore\nthan a minimum threshold υ, andcico-occurs with cjmore\nthan the other way around, then we assume that ciis a sub-\ngenre ofcj. More formally:\n∀ci,cj∈C:ci<Ccjiff\nci/negationslash=cj∧Ni,j>τ∧Nj,i>υ∧Ni,j>Nj,i(3)\nNote, that this rule allows one genre to be the sub-genre\nof multiple other genres. τcontrols the co-occurrence rate\nit takes to be recognized as sub-genre. A low τleads to\nmore sub-genres and fewer top-level genres. υensures that\nthe relationship is not entirely one-sided. As an extreme\nexample, a negative υwould require no co-occurrence of\ngenrecjwithci, butcicould still be a sub-genre of cj.\nApplying (3) makes it easy to ﬁnd top-level genres, but\nthe resulting hierarchy is rather ﬂat. If a genre is more than\none node away from root, the rule does not perform well,\nwhen it comes to deciding whether a genre is either a sub-\ngenre or a sibling. The reason lies in the ﬁxed parameters\nτandυ, which are suitably chosen to ﬁnd top-level genres,\nbut not sub-genres two or more levels deep. To better de-\ntermine deep sub-genre relationships starting from a giventop-level genre, we apply (3) recursively on each hierarchi-\ncal sub-structure. So if C/prime⊂C is the set of sub-genres for\nack∈C, then the co-occurrence matrix N/primeforC/primecan be\ncomputed just like N. BecauseN/primeis normalized, the same\nτandυare suitable to ﬁnd C/prime’s top-level genres, i.e. ck’s\ndirect children. Recursion stops, when the sub-structure\nconsists of at most one genre.\n3. ONTOLOGIES\nIn order to evaluate learned ontologies, we need at least one\nontology that serves as reference. This is different from a\nground truth, as it is well known that a single truth does\nnot exist for ontologies: Different people create different\nontologies, when asked to model the same domain [6, 10,\n12, 17]. We chose DBpedia and WikiData as references,\nwhich are described in Sections 3.1 and 3.2. Using the\ndeﬁnitions and rules from Section 2, we constructed two\nontologies. One based on submissions by English speaking\nusers and another based on submissions by international\nusers. They are described in Sections 3.3 and 3.4.\n3.1 DBpedia Genre Ontology\nDBpedia is the suggested genre extension for The Music\nOntology and therefore a natural choice for a reference on-\ntology.8The part of DBpedia related to musical genres is\ncreated by extracting Wikipedia’s genre infoboxes. For this\nto succeed, the DBpedia creation process requires that such\ninfoboxes exist, and that there is a deﬁned mapping from\nlocalized infobox to ontology properties. Informally we\nfound that for the English edition of Wikipedia both con-\nditions are usually met. This is not always true for other\nlanguage editions, e.g. German.\nWikipedia’s guidelines9deﬁne three possible hierar-\nchical relations between genres:\n•Sub-genre :heavy metal <thrash metal ,\nblack metal ,death metal , etc.\n•Fusion :industrial <industrial metal\n∧heavy metal <industrial metal .\n•Derivative : post punk < house ,\nalternative rock ,dark wave , etc.\nThe derivative relation differs from sub-genre andfu-\nsion in that derivative genres are considered “separate or\ndeveloped enough musicologically to be considered par-\nent/root genres in their own right”. As the relation does\nnot ﬁt the general concept of sub-genre or sub-class, we\nexcluded it when building the ontology. Further, we were\nunable to ﬁnd a formal deﬁnition for the DBpedia rela-\ntionstylistic origin . Based on sample data we interpreted\nit as the inverse of derivative . As such it was also ex-\ncluded. While this made sense for most genres, it did not\nfor some. The hip hop infobox for example, lists East\n8As source for this work, we used DBpedia Live, http://live.\ndbpedia.org .\n9https://en.wikipedia.org/wiki/Wikipedia:\nWikiProject_Music/Music_genres_task_force/\nGuidelines#GenreboxProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 4010 200 400 600 800 1,000 1,200DBpedia\nWikiData\nEng\nIntl522\n271\n791\n807629\n276\n209\n234\nNumber of GenresDisconnected Connected\nFigure 1 . Connected vs. disconnected genres in the four\nused ontologies. Parameters for generated ontologies: τ=\n0.17,υ= 0.0001 ,|CEng|= 1000 ,|CIntl|= 1041 .\nCoast hip hop andWest Coast hip hop as re-\ngional scenes, but not as sub-genres or derivatives. Un-\nfortunately, in DBpedia, regional scene is not deﬁned as a\nspecial genre relation, like sub-genre, but just as a plain\nproperty. In contrast, both Wikipedia articles on East\nCoast hip hop andWest Coast hip hop start\nwith assuring a sub-genre relationship to hip hop . Also,\nboth DBpedia entries list hip hop as the stylistic ori-\ngin. We found similar issues with techno andDetroit\ntechno , and other genres.\nAt the time of writing, the DBpedia-based ontology,\ncreated as described above, consisted of 1151 genres with\na maximum hierarchy depth of 6.629genres ( 54.6%) did\nnot have any super- or sub-genres (Figure 1). We will refer\nto it asODBpedia . In order to increase the chances of ﬁnd-\ning corresponding genres in other ontologies, we normal-\nized the raw genre names as well as their aliases found via\nDBpedia wikiPageRedirects (Wikipedia entries for concep-\ntually identical terms).\nLoaning from graph theory, we call genres without\nsuper- or sub-genres disconnected . Ontologies consisting\nexclusively of disconnected genres we call trivial .\n3.2 WikiData Genre Ontology\nUnlike DBpedia, WikiData is not a parsed version of\nWikipedia, but an independent database of structured data\nfor anyone to edit. Currently, WikiData deﬁnes just one\nrelation between musical genres: sub-class .\nIn an informal evaluation, we found that, with regard to\ngenres, WikiData is still evolving. While East Coast\nhip hop for example is listed as a sub-genre of hip\nhop,West Coast hip hop had no parent at the time\nof writing. Another example is techno andDetroit\ntechno .Detroit techno existed as an entity, but\nwas not of type music genre, and techno was not con-\nnected to it in any way. On the plus side, translations of\ngenre names are easily accessible via localized labels for\neach genre. For matching we used normalized versions of\nthese labels.\nAt the time of writing, the WikiData-based genre on-\ntology consisted of 547genres, 276(50.5%) genres were\ndisconnected, and the hierarchy-depth was 5. We will refer\nto this ontology as OWikiData .3.3 English Language Ontology\nUsing the rules deﬁned in Section 2, we constructed an on-\ntology based on the top ngenre labels submitted by users\nto the central database of beaTunes10, a consumer music\napplication [16]. Given the relevance of English in Western\npop-culture and the fact that our reference ODBpedia offers\ndata based on the English edition of Wikipedia, we only\nconsidered submissions by users with English as their sys-\ntem language. We will refer to this ontology as OEng. Nat-\nurally,OEngis strongly biased towards English culture and\ncontains English genre names almost exclusively. Also, as\nit is generated from user submitted labels, it contains noise.\nUsingτ= 0.17andυ= 0.0001 for the top 1000 En-\nglish genres, we found 209(20.9%) disconnected genres\nand the maximum hierarchy-depth was 4.\nBecause we mentioned hip hop andtechno as\nproblematic examples before, here is what we found for\nOEng: While neither East Coast hip hop norWest\nCoast hip hop occur in the top 1000 English genres,\nEast Coast rap andWest Coast rap do. They\nboth have rap as a parent, which in turn is a child of\nhip hop .Techno does occur as genre, but Detroit\ntechno is not in the top 1000 (rank 1557 ). When using\nthe top 1600 genres as source, Detroit techno has\ntechno andelectronica as parents.\n3.4 International Ontology\nIn addition toOEng, we generated an international ontol-\nogy namedOIntlbased on submissions by users with the\nsystem languages French, German, Spanish, Dutch, or En-\nglish. These are the ﬁve languages with the most submis-\nsions in the beaTunes database. The ontology was created\nwith the goal of being less anglocentric.\nBecause, the database contains different numbers of\nsubmissions per language, we normalized each submis-\nsion’s weight on a per language basis to ensure equal in-\nﬂuence. To represent the chosen languages in the selec-\ntion of the most used genres, we used the intersection of\nthe topnlanguage-speciﬁc genres. For n= 400 this re-\nsulted in a set of 1041 genres, 534of which also occur in\nthe English top 1000 genres. The sub-set of non-English\ngenre names mostly consists of genuinely new additions\nlikeK¨olsch andDeutsch Pop , and translations like\nKindermuziek andpsychedelische Rock . The\nsituation regarding hip hop andtechno is similar to\nOEng. Usingτ= 0.17andυ= 0.0001 we found that\n234(22.5%) genres were disconnected and the maximum\nhierarchy-depth was 5.\n4. EVALUATION MEASURES\nOntologies can be compared on different levels. In\nthe following, we are going to concentrate on lexical\n(Section 4.1) and conceptual (Section 4.2) aspects. For\nboth viewpoints measures have been established in the on-\ntology learning community (see e.g. [7, 19]).\n10http://www.beatunes.com/402 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20164.1 Lexical Measures\nLetORdenote a reference ontology, and OCan ontology\nwe wish to evaluate. Correspondingly, CRis the set of con-\ncepts contained in OR, andCCthe concepts inOC. As\nwe assume a bijective relation between lexical terms and\nconcepts, lexical precision (LP) is deﬁned as the ratio be-\ntween the number of concepts in both ontologies and the\nnumber of concepts in OC:\nLP(OC,OR) =|CC∩CR|\n|CC|(4)\nLexical recall (LR) is deﬁned as the ratio between the\nnumber of concepts in both ontologies and the number of\nconcepts inOR[5]:\nLR(OC,OR) =|CC∩CR|\n|CR|(5)\nFinally, the lexical F-measure (LF) is deﬁned by:\nLF(OC,OR) =2·LP·LR\nLP+LR(6)\n4.2 Conceptual Measures\nThe similarity of two concepts ci∈CCandcj∈CRcan\nbe measured by comparing their semantic cotopies [10].\nA basic semantic cotopy is deﬁned as the set containing\nall super- and sub-concepts for a given concept including\nitself. The common semantic cotopy (csc) is similar, but\nonly takes concepts into account that are members of both\nontologies we wish to compare. Additionally, the concept\nfor which we are building the cotopy is excluded ( <Cin-\nstead of≤C). Both modiﬁcations are intended to minimize\nthe inﬂuence of lexical similarity [5]:\ncsc(ci,OC,OR)\n={cj∈CC∩CR|cj<CCci∨ci<CCcj}(7)\nThelocal taxonomic precision (tpcsc) is deﬁned as the\nratio between the size of the intersection of the cotopies for\ntwo concepts, and the size of the cotopy of just the concept\nto evaluate:\ntpcsc(ci,cj,OC,OR)\n=|csc(ci,OC,OR)∩csc(cj,OC,OR)|\n|csc(ci,OC,OR)|(8)\ntpcscis undeﬁned for|csc(ci,OC,OR)|= 0 (division\nby zero). In the spirit of [5], i.e. to avoid unjustiﬁably high\nvalues for trivial ontologies, we deﬁne tpcsc= 0 for this\ncase. Based on the local tpcsc, we deﬁne a global taxo-\nnomic precision (TPcsc) as the mean tpcscfor all concepts\ninCC∩CR[7]:\nTPcsc(OC,OR)\n=1\n|CC∩CR|/summationdisplay\nc∈CC∩CRtpcsc(c,c,OC,OR)(9)\nTable 1 nEng/DBpediaLPEng/WikiDataLPIntl/DBpediaLPIntl/WikiDataLPEng/DBpediaEng/DBpediaEng/WikiDataEng/WikiDataIntl/DBpediaIntl/DBpediaIntl/WikiDataIntl/WikiDataWikiData/DBpediaDBpedia/WikiDataWikiData/DBpedia 118.688097306689834E-41.00.0018348623853211011.0210.00173761946133796691.00.0036697247706422021.010.010.020.020.00.3060.6460.6420.30450.415285714285714 220.00173761946133796691.00.0036697247706422021.0220.00173761946133796691.00.0036697247706422021.020.020.020.020.0200000.415285714285714 330.00260642919200695071.00.0055045871559633031.0630.00347523892267593380.66666666666666660.0110091743119266061.030.030.060.0069144338807260160.0 440.00260642919200695070.750.0055045871559633030.75940.0052128583840139010.66666666666666660.0128440366972477070.777777777777777840.005194805194805240.010928961748633990.010344827586206990.0252707581227437 550.00347523892267593380.80.0073394495412844040.81150.0060816681146828850.63636363636363640.0146788990825688080.727272727272727350.0069204152249134950.0145454545454545110.0120481927710843110.0287769784172662 660.0043440486533449180.83333333333333340.0091743119266055050.83333333333333341260.00695047784535186750.66666666666666660.016513761467889910.7560.0086430423509075260.0181488203266788120.0137575236457438120.0323159784560144 770.0052128583840139010.85714285714285710.0110091743119266060.85714285714285711570.0095569070373588190.73333333333333330.0220183486238532120.870.010362694300518170.0217391304347826150.0188679245283019150.0428571428571429 880.0052128583840139010.750.0128440366972477070.8751580.0095569070373588190.73333333333333330.0220183486238532120.880.010353753235547980.0253164556962025150.0188679245283019150.0428571428571429 990.0060816681146828850.77777777777777780.0146788990825688080.88888888888888881690.0095569070373588190.68750.0238532110091743130.812590.012068965517241490.0288808664259928160.0188517566409597160.0463458110516934 10100.0060816681146828850.70.016513761467889910.917100.0095569070373588190.64705882352941180.0256880733944954140.8235294117647058100.0120585701981051100.0324324324324324170.0188356164383562170.0498220640569395 12120.0060816681146828850.58333333333333340.020183486238532110.916666666666666619120.0095569070373588190.57894736842105270.0256880733944954140.7368421052631579120.0120378331900258120.0394973070017953190.0188034188034188190.049645390070922 14140.0078192875760208520.64285714285714290.0238532110091743130.928571428571428621140.0112945264986967860.61904761904761910.0293577981651376160.7619047619047619140.015450643776824140.0465116279069768210.0221843003412969210.0565371024734982 16160.0086880973066898360.6250.0275229357798165150.937525160.0130321459600347520.60.034862385321100920.76160.0171379605826907160.053475935828877250.0255102040816327250.0666666666666667 18180.0104257167680278030.66666666666666660.0311926605504587170.944444444444444432180.0147697654213727190.531250.040366972477064220.6875180.0205303678357571180.0603907637655417320.0287404902789518320.0762564991334489 20200.0112945264986967860.650.033027522935779820.936200.0156385751520417040.50.042201834862385320.6388888888888888200.022203245089667200.063716814159292360.0303285593934288360.0791738382099828 22220.012163336229365770.63636363636363640.033027522935779820.818181818181818240220.0156385751520417040.450.042201834862385320.575220.0238704177323103220.0634920634920635400.0302267002518892400.0786324786324786 24240.0139009556907037350.66666666666666660.036697247706422020.833333333333333444240.017376194613379670.454545454545454530.0477064220183486270.5909090909090909240.0272340425531915240.070298769771529440.0334728033472803440.0882852292020374 26260.0139009556907037350.61538461538461540.040366972477064220.846153846153846147260.0191138140747176380.468085106382978730.053211009174311930.6170212765957447260.0271877655055225260.0770577933450088470.0367278797996661470.097972972972973 28280.0156385751520417040.64285714285714290.042201834862385320.821428571428571450280.019982623805386620.460.053211009174311930.58280.0305343511450382280.0802792321116929500.0383014154870941500.0974789915966386 30300.017376194613379670.66666666666666660.0440366972477064240.853300.0217202432667245870.47169811320754720.055045871559633030.5660377358490566300.0338696020321761300.0834782608695652530.0415282392026578530.100334448160535 35350.0191138140747176380.62857142857142860.049541284403669730.771428571428571565350.025195482189400520.44615384615384620.066055045871559640.5538461538461539350.0370994940978078350.0931034482758621650.0476973684210526650.118032786885246 40400.0217202432667245870.6250.055045871559633030.7571400.0286707211120764550.46478873239436620.073394495412844040.5633802816901409400.0419815281276238400.102564102564103710.0540098199672668710.12987012987013 45450.025195482189400520.64444444444444450.0623853211009174350.755555555555555582450.033014769765421370.46341463414634150.084403669724770650.5609756097560976450.048494983277592450.115254237288136820.0616382806163828820.146730462519936 50500.026933101650738490.620.066055045871559640.7292500.034752389226759340.434782608695652160.088073394495412850.5217391304347826500.051623646960866500.121008403361345920.0643604183427192920.150706436420722 55550.0295395308427454370.61818181818181820.071559633027522940.7090909090909091102550.037358818418766290.42156862745098040.095412844036697250.5098039215686274550.0563847429519071550.131020.0686352753391861020.160741885625966 60600.031277150304083410.60.078899082568807340.7166666666666667113600.040834057341442220.4159292035398230.106422018348623860.5132743362831859600.0594549958711808600.1421487603305791130.07436708860759491130.17629179331307 65650.034752389226759340.61538461538461540.086238532110091750.7230769230769231123650.0443092962641181560.41463414634146340.113761467889908260.5040650406504065650.0657894736842105650.1540983606557381230.08006279434850861230.18562874251497 70700.0382276281494352760.62857142857142860.093577981651376150.7285714285714285133700.047784535186794090.413533834586466140.122935779816513770.5037593984962406700.0720720720720721700.1658536585365851330.08566978193146421330.1976401179941 75750.040834057341442220.62666666666666670.099082568807339460.72149750.0512597741094700240.39597315436241610.133944954128440370.4899328859060403750.0766721044045677750.1741935483870971490.09076923076923081490.210374639769452 80800.042571676802780190.61250.102752293577981660.7165800.054735013032145960.381818181818181830.144954128440366980.47878787878787876800.0796100731112916800.17921650.09574468085106381650.222535211267606 85850.0451781059947871450.6117647058823530.108256880733944960.6941176470588235175850.057341442224152910.377142857142857170.148623853211009180.46285714285714286850.0841423948220065850.1873015873015871750.09954751131221721750.225 90900.047784535186794090.61111111111111120.113761467889908260.6888888888888889189900.0590790616854908750.359788359788359770.150458715596330280.43386243386243384900.088638195004029900.1952755905511811890.1014925373134331890.223433242506812 95950.048653344917463080.58947368421052630.119266055045871570.6842105263157895204950.061685490877497830.34803921568627450.15779816513761470.4215686274509804950.0898876404494382950.2031252040.104797047970482040.229639519359146 1001000.0512597741094700240.590.124770642201834870.682201000.065160729800173760.34090909090909090.16697247706422020.413636363636363641000.09432454036770581000.2108527131782952200.1094091903719912200.237908496732026 1101100.054735013032145960.57272727272727280.135779816513761480.67272727272727272841250.078192875760208520.316901408450704250.20183486238532110.38732394366197181100.09992069785884221100.2259541984732822840.1254355400696862840.265379975874548 1201200.062554300608166820.60.150458715596330280.68333333333333333451500.090356211989574290.301449275362318860.227522935779816530.359420289855072461200.1132966168371361200.2466165413533833450.139037433155083450.278651685393258 1301300.065160729800173760.57692307692307690.15963302752293580.66923076923076924111750.103388357949609030.28953771289537710.245871559633027540.32603406326034061300.1170960187353631300.2577777777777784110.1523687580025614110.280334728033473 1401400.070373588184187670.57857142857142860.17064220183486240.66428571428571434842000.113814074717636840.27066115702479340.262385321100917450.295454545454545471400.1254841208365611400.2715328467153284840.1602446483180434840.277939747327502 1501500.072980017376194610.560.17981651376146790.65333333333333335512250.122502172024326670.25589836660617060.284403669724770660.28130671506352091500.129131437355881500.2820143884892095510.1656874265569925510.282846715328467 1601600.076455256298870550.550.18715596330275230.63756212500.12684622067767160.235104669887278570.295412844036697260.259259259259259241600.1342486651411141600.289361702127666210.1647855530474046210.276157804459691 1701700.080799304952215460.54705882352941180.19449541284403670.62352941176470596982750.13727193744569940.226361031518624630.31192660550458720.243553008595988551700.1408024224072671700.2965034965034966980.1709031909140086980.273531777956557 1801800.081668114682884440.52222222222222230.19633027522935780.59444444444444447703000.143353605560382260.214285714285714270.33211009174311930.235064935064935061800.1412471825694971800.2951724137931037700.171785528370647700.275285171102662 1901900.08427454387489140.51052631578947370.20183486238532110.57894736842105278403250.149435273675065150.204761904761904760.35045871559633030.227380952380952381900.1446681580909771900.2993197278911568400.172777498744358400.275812274368231 2002000.088618592528236310.510.209174311926605520.579083500.1572545612510860.199339207048458140.37064220183486240.222466960352422922000.150999259807552000.3060402684563769080.1758135016998549080.278045423262216 2102100.093831450912250220.51428571428571420.216513761467889920.56190476190476199733750.161598609904430920.191161356628982540.37798165137614680.21171634121274412100.1587068332108742100.3125827814569549730.1751412429378539730.271409749670619 2202200.098175499565595140.51363636363636370.223853211009174320.554545454545454610414000.165073848827106850.182516810758885680.38532110091743120.20172910662824212200.1648431801604672200.31895424836601310410.17335766423357710410.264817150063052 2302300.09991311902693310.50.225688073394495430.534782608695652211844500.17376194613379670.168918918918918910.40917431192660550.18834459459459462300.1665459811730632300.3174193548387111840.17130620985010711840.257952573742047 2402400.101650738488271070.48750.229357798165137630.520833333333333413325000.18331885317115550.158408408408408420.429357798165137640.175675675675675692400.1682242990654212400.31847133757961813320.1699556987515113320.249334043686734 2502500.105125977410947010.4840.234862385321100930.51214845500.192875760208514320.14959568733153640.449541284403669750.16509433962264152500.1727337615988582500.32201257861635214840.16850094876660314840.241498275012321 3003000.120764552562988710.46333333333333330.271559633027522950.4933333333333333516376000.201563857515204170.14172266340867440.462385321100917460.153940134392180823000.1915920055134393000.35029585798816616370.16642754662840716370.230980751604033 3503500.130321459600347520.428571428571428550.297247706422018370.4628571428571428617946500.212858384013900950.136566332218506140.480733944954128470.146042363433667783500.1998667554963363500.36201117318435817940.16638370118845517940.224027362120564 4004000.13640312771503040.39250.31376146788990830.427519477000.22067767158992180.130457113507960960.488073394495412870.136620441705187474000.2024500322372664000.36190476190476219470.16397675919948419470.213483146067416 4504500.144222415291051260.36888888888888890.33394495412844040.4044444444444444421127500.228496959165942650.124526515151515150.49908256880733950.128787878787878784500.207370393504064500.36582914572864321120.16120134845234421120.204742190440346 5005000.156385751520417030.360.35412844036697250.38622638000.238922675933970460.121520106053910740.51192660550458710.12328767123287675000.2180496668685655000.36937799043062222630.16110134739308722630.198717948717949 5505500.164205039096437880.343636363636363630.36880733944954130.365454545454545525629000.25282363162467420.113583138173302110.54128440366972470.115144418423106955500.2222222222222225500.36712328767123325620.15674656611904125620.189893788220148 6006000.169417897480451770.3250.38532110091743120.35288710000.259774109470026050.103567717353654320.55779816513761470.105299618981641856000.2227298686464886000.36681222707423628870.14809311540366528870.177156177156177 6506500.180712423979148580.320.40.3353846153846154355012000.28410078192875760.092112676056338020.58899082568807340.09042253521126766500.2309827873403666500.36485355648535635500.13911933631142335500.156776556776557 7007000.187662901824500430.308571428571428550.414678899082568830.32285714285714284422014000.30234578627280630.082464454976303320.6183486238532110.079857819905213287000.2333873581847657000.36305220883534142200.12958480729845542200.141448058761805 7507500.192875760208514320.2960.422018348623853230.30666666666666664488616000.32145960034752390.07572656569791240.64220183486238540.071633237822349577500.2335612835349827500.35521235521235548860.12257743912539348860.128889707236236 8008000.203301476976542130.29250.433027522935779840.295555018000.33275412684622070.069009009009009010.66238532110091740.065045045045045058000.2398769861609438000.3509293680297455500.1143112968213755500.118457752255947 8508500.207645525629887060.28117647058823530.440366972477064240.2823529411764706624020000.344917463075586460.063621794871794870.67706422018348620.059134615384615388500.238880559720148500.34408602150537662400.10742795291570862400.108769344141489 9009000.21546481320590790.275555555555555550.458715596330275250.2777777777777778972630000.387489139878366640.045856467201316060.72110091743119260.040407156076495999000.241833252072169000.34602076124567597260.082007906591891197260.0765261415636257 9509500.22154648132059080.268421052631578950.471559633027522960.270526315789473671336040000.41442224152910510.035703592814371260.74311926605504590.030314371257485039500.2427415516420759500.3438127090301133600.0657432292743436133600.058252427184466 100010000.225021720243266720.2590.475229357798165160.2591702850000.440486533449174660.0297744890768146570.76513761467889910.0244890768146582110000.24081822408182210000.335275080906149170280.055778645690082170280.0474591703181016 110011000.23718505647263250.248181818181818170.49724770642201840.246363636363636372074660000.46307558644656820.0256916996047430840.79082568807339450.02077508917381663811000.24255886272767711000.329483282674772207460.0486824679179796207460.0404865905781786 120012000.241529105125977420.231666666666666660.50275229357798170.2283333333333333312000.23649510846448312000.3140401146131800 130013000.248479582971329270.220.51743119266055050.216923076923076913000.23337413300693613000.30569105691056900 140014000.25543006081668110.210.53211009174311930.2071428571428571614000.23049784398275214000.29820051413881700 150015000.260642919200695040.20.53944954128440370.19615000.226329686910615000.28753056234718800 160016000.266724587315377960.1918750.55596330275229360.18937516000.22319156670301716000.28251748251748200 170017000.27454387489139880.185882352941176470.5633027522935780.1805882352941176617000.22167660470010517000.27349665924276200 180018000.27888792354474370.178333333333333340.57614678899082570.1744444444444444618000.21755337173839418000.26780383795309200 190019000.28670721112076450.17368421052631580.58899082568807340.1689473684210526219000.21632251720747319000.26257668711656400 200020000.291920069504778450.1680.60.163520000.21326562995874320000.25697445972495100 250025000.312771503040834060.1440.6183486238532110.134825000.19720624486442125000.22134646962233200 300030000.33709817549956560.129333333333333330.64954128440366980.11830000.18694290532401830000.19971791255289100 350035000.34839270199826240.114571428571428570.65871559633027520.1025714285714285835000.17243603526123435000.17750309023485800 400040000.35794960903562120.1030.67522935779816520.09240000.15996893807027840000.16193619361936200 450045000.3692441355343180.094444444444444440.6917431192660550.0837777777777777845000.15041585560077945000.14945490584737400 500050000.375325803649000870.08640.69357798165137620.075650000.14046496504633450000.1363390441839500 600060000.39357080799304950.07550.71376146788990820.0648333333333333460000.12669556705355960000.11886936592818900 700070000.406602953953084270.066857142857142850.72293577981651380.05628571428571428670000.11483253588516770000.10444002650762100 800080000.418766290182450040.060250.73577981651376150.05012580000.10534367828652680000.093856056173200700 10000100000.439617723718505660.05060.75412844036697250.0411100000.0907541924491077100000.077951635846372700 15000150000.470026064291920060.0360666666666666640.79082568807339450.028733333333333333150000.0669927558665098150000.055451913798649100 20000200000.496959165942658540.02860.81100917431192670.0221200000.0540872771972956200000.043027500608420500Lexical Precision LP0.000.250.500.751.00\nLexical Recall LR0.000.250.500.751.00\nEng/DBpedia\nEng/WikiData\nIntl/DBpedia\nIntl/WikiDataWikiData/DBpediaDBpedia/WikiData\nLexical F-Measure LF0.0000.1250.2500.3750.500\nNumber of Genres101001,00010,000\nEng/DBpedia\nEng/WikiData\nIntl/DBpedia\nIntl/WikiData\nWikiData/DBpedia\u00003Figure 2 . Lexical precision LPand recallLRfor learned\nontologiesOEngandOIntlbased on different genre num-\nbers.ODBpedia andOWikiData serve as reference ontolo-\ngies (with a ﬁxed number of genres).\nOCOWikiDataOEngOIntl\nLP 0.644 0.260 0.183\nLR 0.306 0.226 0.166\nLF 0.415 0.242 0.174\nTPcsc 0.098 0.187 0.193\nTRcsc 0.114 0.220 0.212\nTFcsc 0.105 0.202 0.202\nTPcon 0.266 0.237 0.240\nTRcon 0.319 0.278 0.268\nTFcon 0.290 0.256 0.253\nTable 1 . Results forOR=ODBpedia ,τ= 0.17,υ=\n0.0001 ,|CEng|= 1000 ,|CIntl|= 1041 .\nThetaxonomic recall (TRcsc) is:\nTRcsc(OC,OR) =TPcsc(OR,OC) (10)\nFinally, the taxonomic F-measure (TFcsc) is deﬁned by:\nTFcsc(OC,OR) =2·TPcsc·TRcsc\nTPcsc+TRcsc(11)\n5. RESULTS AND DISCUSSION\nWe measured the similarity of all four ontologies using\nvarying parameters for the learned ones. Section 5.1 re-\nports lexical results, Section 5.2 conceptual results. In\nSection 5.3 we discuss our ﬁndings.\n5.1 Lexical Results\nHow similar are the ontologies on the lexical level? For\nthe reference ontologies ODBpedia andOWikiData this is\neasy to answer: LP/LR/LF (OWikiData,ODBpedia ) =\n0.64/0.31/0.42(Table 1). Given their respective sizes, the\nhighest possible values for this pairing are 1.00/0.48/0.64\n(ifCWikiData⊂CDBpedia ).Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 403OCODBpediaOEngOIntl\nLP 0.303 0.259 0.202\nLR 0.638 0.473 0.384\nLF 0.411 0.335 0.264\nTPcsc 0.114 0.174 0.181\nTRcsc 0.098 0.151 0.149\nTFcsc 0.105 0.162 0.163\nTPcon 0.319 0.305 0.357\nTRcon 0.266 0.274 0.303\nTFcon 0.290 0.288 0.328\nTable 2 . Results forOR=OWikiData ,τ= 0.17,υ=\n0.0001 ,|CEng|= 1000 ,|CIntl|= 1041 .\nTable 1 nEng/DBpediaLPEng/WikiDataLPIntl/DBpediaLPIntl/WikiDataLPEng/DBpediaEng/DBpediaEng/WikiDataEng/WikiDataIntl/DBpediaIntl/DBpediaIntl/WikiDataIntl/WikiDataWikiData/DBpediaDBpedia/WikiDataWikiData/DBpedia 118.688097306689834E-41.00.0018348623853211011.0210.00173761946133796691.00.0036697247706422021.010.010.020.020.00.3060.6460.6420.30450.415285714285714 220.00173761946133796691.00.0036697247706422021.0220.00173761946133796691.00.0036697247706422021.020.020.020.020.0200000.415285714285714 330.00260642919200695071.00.0055045871559633031.0630.00347523892267593380.66666666666666660.0110091743119266061.030.030.060.0069144338807260160.0 440.00260642919200695070.750.0055045871559633030.75940.0052128583840139010.66666666666666660.0128440366972477070.777777777777777840.005194805194805240.010928961748633990.010344827586206990.0252707581227437 550.00347523892267593380.80.0073394495412844040.81150.0060816681146828850.63636363636363640.0146788990825688080.727272727272727350.0069204152249134950.0145454545454545110.0120481927710843110.0287769784172662 660.0043440486533449180.83333333333333340.0091743119266055050.83333333333333341260.00695047784535186750.66666666666666660.016513761467889910.7560.0086430423509075260.0181488203266788120.0137575236457438120.0323159784560144 770.0052128583840139010.85714285714285710.0110091743119266060.85714285714285711570.0095569070373588190.73333333333333330.0220183486238532120.870.010362694300518170.0217391304347826150.0188679245283019150.0428571428571429 880.0052128583840139010.750.0128440366972477070.8751580.0095569070373588190.73333333333333330.0220183486238532120.880.010353753235547980.0253164556962025150.0188679245283019150.0428571428571429 990.0060816681146828850.77777777777777780.0146788990825688080.88888888888888881690.0095569070373588190.68750.0238532110091743130.812590.012068965517241490.0288808664259928160.0188517566409597160.0463458110516934 10100.0060816681146828850.70.016513761467889910.917100.0095569070373588190.64705882352941180.0256880733944954140.8235294117647058100.0120585701981051100.0324324324324324170.0188356164383562170.0498220640569395 12120.0060816681146828850.58333333333333340.020183486238532110.916666666666666619120.0095569070373588190.57894736842105270.0256880733944954140.7368421052631579120.0120378331900258120.0394973070017953190.0188034188034188190.049645390070922 14140.0078192875760208520.64285714285714290.0238532110091743130.928571428571428621140.0112945264986967860.61904761904761910.0293577981651376160.7619047619047619140.015450643776824140.0465116279069768210.0221843003412969210.0565371024734982 16160.0086880973066898360.6250.0275229357798165150.937525160.0130321459600347520.60.034862385321100920.76160.0171379605826907160.053475935828877250.0255102040816327250.0666666666666667 18180.0104257167680278030.66666666666666660.0311926605504587170.944444444444444432180.0147697654213727190.531250.040366972477064220.6875180.0205303678357571180.0603907637655417320.0287404902789518320.0762564991334489 20200.0112945264986967860.650.033027522935779820.936200.0156385751520417040.50.042201834862385320.6388888888888888200.022203245089667200.063716814159292360.0303285593934288360.0791738382099828 22220.012163336229365770.63636363636363640.033027522935779820.818181818181818240220.0156385751520417040.450.042201834862385320.575220.0238704177323103220.0634920634920635400.0302267002518892400.0786324786324786 24240.0139009556907037350.66666666666666660.036697247706422020.833333333333333444240.017376194613379670.454545454545454530.0477064220183486270.5909090909090909240.0272340425531915240.070298769771529440.0334728033472803440.0882852292020374 26260.0139009556907037350.61538461538461540.040366972477064220.846153846153846147260.0191138140747176380.468085106382978730.053211009174311930.6170212765957447260.0271877655055225260.0770577933450088470.0367278797996661470.097972972972973 28280.0156385751520417040.64285714285714290.042201834862385320.821428571428571450280.019982623805386620.460.053211009174311930.58280.0305343511450382280.0802792321116929500.0383014154870941500.0974789915966386 30300.017376194613379670.66666666666666660.0440366972477064240.853300.0217202432667245870.47169811320754720.055045871559633030.5660377358490566300.0338696020321761300.0834782608695652530.0415282392026578530.100334448160535 35350.0191138140747176380.62857142857142860.049541284403669730.771428571428571565350.025195482189400520.44615384615384620.066055045871559640.5538461538461539350.0370994940978078350.0931034482758621650.0476973684210526650.118032786885246 40400.0217202432667245870.6250.055045871559633030.7571400.0286707211120764550.46478873239436620.073394495412844040.5633802816901409400.0419815281276238400.102564102564103710.0540098199672668710.12987012987013 45450.025195482189400520.64444444444444450.0623853211009174350.755555555555555582450.033014769765421370.46341463414634150.084403669724770650.5609756097560976450.048494983277592450.115254237288136820.0616382806163828820.146730462519936 50500.026933101650738490.620.066055045871559640.7292500.034752389226759340.434782608695652160.088073394495412850.5217391304347826500.051623646960866500.121008403361345920.0643604183427192920.150706436420722 55550.0295395308427454370.61818181818181820.071559633027522940.7090909090909091102550.037358818418766290.42156862745098040.095412844036697250.5098039215686274550.0563847429519071550.131020.0686352753391861020.160741885625966 60600.031277150304083410.60.078899082568807340.7166666666666667113600.040834057341442220.4159292035398230.106422018348623860.5132743362831859600.0594549958711808600.1421487603305791130.07436708860759491130.17629179331307 65650.034752389226759340.61538461538461540.086238532110091750.7230769230769231123650.0443092962641181560.41463414634146340.113761467889908260.5040650406504065650.0657894736842105650.1540983606557381230.08006279434850861230.18562874251497 70700.0382276281494352760.62857142857142860.093577981651376150.7285714285714285133700.047784535186794090.413533834586466140.122935779816513770.5037593984962406700.0720720720720721700.1658536585365851330.08566978193146421330.1976401179941 75750.040834057341442220.62666666666666670.099082568807339460.72149750.0512597741094700240.39597315436241610.133944954128440370.4899328859060403750.0766721044045677750.1741935483870971490.09076923076923081490.210374639769452 80800.042571676802780190.61250.102752293577981660.7165800.054735013032145960.381818181818181830.144954128440366980.47878787878787876800.0796100731112916800.17921650.09574468085106381650.222535211267606 85850.0451781059947871450.6117647058823530.108256880733944960.6941176470588235175850.057341442224152910.377142857142857170.148623853211009180.46285714285714286850.0841423948220065850.1873015873015871750.09954751131221721750.225 90900.047784535186794090.61111111111111120.113761467889908260.6888888888888889189900.0590790616854908750.359788359788359770.150458715596330280.43386243386243384900.088638195004029900.1952755905511811890.1014925373134331890.223433242506812 95950.048653344917463080.58947368421052630.119266055045871570.6842105263157895204950.061685490877497830.34803921568627450.15779816513761470.4215686274509804950.0898876404494382950.2031252040.104797047970482040.229639519359146 1001000.0512597741094700240.590.124770642201834870.682201000.065160729800173760.34090909090909090.16697247706422020.413636363636363641000.09432454036770581000.2108527131782952200.1094091903719912200.237908496732026 1101100.054735013032145960.57272727272727280.135779816513761480.67272727272727272841250.078192875760208520.316901408450704250.20183486238532110.38732394366197181100.09992069785884221100.2259541984732822840.1254355400696862840.265379975874548 1201200.062554300608166820.60.150458715596330280.68333333333333333451500.090356211989574290.301449275362318860.227522935779816530.359420289855072461200.1132966168371361200.2466165413533833450.139037433155083450.278651685393258 1301300.065160729800173760.57692307692307690.15963302752293580.66923076923076924111750.103388357949609030.28953771289537710.245871559633027540.32603406326034061300.1170960187353631300.2577777777777784110.1523687580025614110.280334728033473 1401400.070373588184187670.57857142857142860.17064220183486240.66428571428571434842000.113814074717636840.27066115702479340.262385321100917450.295454545454545471400.1254841208365611400.2715328467153284840.1602446483180434840.277939747327502 1501500.072980017376194610.560.17981651376146790.65333333333333335512250.122502172024326670.25589836660617060.284403669724770660.28130671506352091500.129131437355881500.2820143884892095510.1656874265569925510.282846715328467 1601600.076455256298870550.550.18715596330275230.63756212500.12684622067767160.235104669887278570.295412844036697260.259259259259259241600.1342486651411141600.289361702127666210.1647855530474046210.276157804459691 1701700.080799304952215460.54705882352941180.19449541284403670.62352941176470596982750.13727193744569940.226361031518624630.31192660550458720.243553008595988551700.1408024224072671700.2965034965034966980.1709031909140086980.273531777956557 1801800.081668114682884440.52222222222222230.19633027522935780.59444444444444447703000.143353605560382260.214285714285714270.33211009174311930.235064935064935061800.1412471825694971800.2951724137931037700.171785528370647700.275285171102662 1901900.08427454387489140.51052631578947370.20183486238532110.57894736842105278403250.149435273675065150.204761904761904760.35045871559633030.227380952380952381900.1446681580909771900.2993197278911568400.172777498744358400.275812274368231 2002000.088618592528236310.510.209174311926605520.579083500.1572545612510860.199339207048458140.37064220183486240.222466960352422922000.150999259807552000.3060402684563769080.1758135016998549080.278045423262216 2102100.093831450912250220.51428571428571420.216513761467889920.56190476190476199733750.161598609904430920.191161356628982540.37798165137614680.21171634121274412100.1587068332108742100.3125827814569549730.1751412429378539730.271409749670619 2202200.098175499565595140.51363636363636370.223853211009174320.554545454545454610414000.165073848827106850.182516810758885680.38532110091743120.20172910662824212200.1648431801604672200.31895424836601310410.17335766423357710410.264817150063052 2302300.09991311902693310.50.225688073394495430.534782608695652211844500.17376194613379670.168918918918918910.40917431192660550.18834459459459462300.1665459811730632300.3174193548387111840.17130620985010711840.257952573742047 2402400.101650738488271070.48750.229357798165137630.520833333333333413325000.18331885317115550.158408408408408420.429357798165137640.175675675675675692400.1682242990654212400.31847133757961813320.1699556987515113320.249334043686734 2502500.105125977410947010.4840.234862385321100930.51214845500.192875760208514320.14959568733153640.449541284403669750.16509433962264152500.1727337615988582500.32201257861635214840.16850094876660314840.241498275012321 3003000.120764552562988710.46333333333333330.271559633027522950.4933333333333333516376000.201563857515204170.14172266340867440.462385321100917460.153940134392180823000.1915920055134393000.35029585798816616370.16642754662840716370.230980751604033 3503500.130321459600347520.428571428571428550.297247706422018370.4628571428571428617946500.212858384013900950.136566332218506140.480733944954128470.146042363433667783500.1998667554963363500.36201117318435817940.16638370118845517940.224027362120564 4004000.13640312771503040.39250.31376146788990830.427519477000.22067767158992180.130457113507960960.488073394495412870.136620441705187474000.2024500322372664000.36190476190476219470.16397675919948419470.213483146067416 4504500.144222415291051260.36888888888888890.33394495412844040.4044444444444444421127500.228496959165942650.124526515151515150.49908256880733950.128787878787878784500.207370393504064500.36582914572864321120.16120134845234421120.204742190440346 5005000.156385751520417030.360.35412844036697250.38622638000.238922675933970460.121520106053910740.51192660550458710.12328767123287675000.2180496668685655000.36937799043062222630.16110134739308722630.198717948717949 5505500.164205039096437880.343636363636363630.36880733944954130.365454545454545525629000.25282363162467420.113583138173302110.54128440366972470.115144418423106955500.2222222222222225500.36712328767123325620.15674656611904125620.189893788220148 6006000.169417897480451770.3250.38532110091743120.35288710000.259774109470026050.103567717353654320.55779816513761470.105299618981641856000.2227298686464886000.36681222707423628870.14809311540366528870.177156177156177 6506500.180712423979148580.320.40.3353846153846154355012000.28410078192875760.092112676056338020.58899082568807340.09042253521126766500.2309827873403666500.36485355648535635500.13911933631142335500.156776556776557 7007000.187662901824500430.308571428571428550.414678899082568830.32285714285714284422014000.30234578627280630.082464454976303320.6183486238532110.079857819905213287000.2333873581847657000.36305220883534142200.12958480729845542200.141448058761805 7507500.192875760208514320.2960.422018348623853230.30666666666666664488616000.32145960034752390.07572656569791240.64220183486238540.071633237822349577500.2335612835349827500.35521235521235548860.12257743912539348860.128889707236236 8008000.203301476976542130.29250.433027522935779840.295555018000.33275412684622070.069009009009009010.66238532110091740.065045045045045058000.2398769861609438000.3509293680297455500.1143112968213755500.118457752255947 8508500.207645525629887060.28117647058823530.440366972477064240.2823529411764706624020000.344917463075586460.063621794871794870.67706422018348620.059134615384615388500.238880559720148500.34408602150537662400.10742795291570862400.108769344141489 9009000.21546481320590790.275555555555555550.458715596330275250.2777777777777778972630000.387489139878366640.045856467201316060.72110091743119260.040407156076495999000.241833252072169000.34602076124567597260.082007906591891197260.0765261415636257 9509500.22154648132059080.268421052631578950.471559633027522960.270526315789473671336040000.41442224152910510.035703592814371260.74311926605504590.030314371257485039500.2427415516420759500.3438127090301133600.0657432292743436133600.058252427184466 100010000.225021720243266720.2590.475229357798165160.2591702850000.440486533449174660.0297744890768146570.76513761467889910.0244890768146582110000.24081822408182210000.335275080906149170280.055778645690082170280.0474591703181016 110011000.23718505647263250.248181818181818170.49724770642201840.246363636363636372074660000.46307558644656820.0256916996047430840.79082568807339450.02077508917381663811000.24255886272767711000.329483282674772207460.0486824679179796207460.0404865905781786 120012000.241529105125977420.231666666666666660.50275229357798170.2283333333333333312000.23649510846448312000.3140401146131800 130013000.248479582971329270.220.51743119266055050.216923076923076913000.23337413300693613000.30569105691056900 140014000.25543006081668110.210.53211009174311930.2071428571428571614000.23049784398275214000.29820051413881700 150015000.260642919200695040.20.53944954128440370.19615000.226329686910615000.28753056234718800 160016000.266724587315377960.1918750.55596330275229360.18937516000.22319156670301716000.28251748251748200 170017000.27454387489139880.185882352941176470.5633027522935780.1805882352941176617000.22167660470010517000.27349665924276200 180018000.27888792354474370.178333333333333340.57614678899082570.1744444444444444618000.21755337173839418000.26780383795309200 190019000.28670721112076450.17368421052631580.58899082568807340.1689473684210526219000.21632251720747319000.26257668711656400 200020000.291920069504778450.1680.60.163520000.21326562995874320000.25697445972495100 250025000.312771503040834060.1440.6183486238532110.134825000.19720624486442125000.22134646962233200 300030000.33709817549956560.129333333333333330.64954128440366980.11830000.18694290532401830000.19971791255289100 350035000.34839270199826240.114571428571428570.65871559633027520.1025714285714285835000.17243603526123435000.17750309023485800 400040000.35794960903562120.1030.67522935779816520.09240000.15996893807027840000.16193619361936200 450045000.3692441355343180.094444444444444440.6917431192660550.0837777777777777845000.15041585560077945000.14945490584737400 500050000.375325803649000870.08640.69357798165137620.075650000.14046496504633450000.1363390441839500 600060000.39357080799304950.07550.71376146788990820.0648333333333333460000.12669556705355960000.11886936592818900 700070000.406602953953084270.066857142857142850.72293577981651380.05628571428571428670000.11483253588516770000.10444002650762100 800080000.418766290182450040.060250.73577981651376150.05012580000.10534367828652680000.093856056173200700 10000100000.439617723718505660.05060.75412844036697250.0411100000.0907541924491077100000.077951635846372700 15000150000.470026064291920060.0360666666666666640.79082568807339450.028733333333333333150000.0669927558665098150000.055451913798649100 20000200000.496959165942658540.02860.81100917431192670.0221200000.0540872771972956200000.043027500608420500Lexical Precision LP0.000.250.500.751.00\nLexical Recall LR0.000.250.500.751.00Eng/DBpediaEng/WikiDataIntl/DBpediaIntl/WikiDataWikiData/DBpediaDBpedia/WikiData\nLexical F-Measure LF0.0000.1250.2500.3750.500\nNumber of Genres101001,00010,000\nEng/DBpedia\nEng/WikiData\nIntl/DBpedia\nIntl/WikiData\nWikiData/DBpedia\u00003LFmax\nFigure 3 . Lexical F-measure LF for learned ontolo-\ngiesOEngandOIntlbased on different genre numbers.\nODBpedia andOWikiData serve as reference ontologies\n(with a ﬁxed number of genres).\nFor the learned ontologies, the answer depends on the\nnumber of genres used during generation. Not surpris-\ningly, we observed that recall increases with the num-\nber of genres, while precision decreases. When com-\nparing precision/recall values for the learned ontologies\nwithODBpedia andOWikiData , values forOWikiData\nare predominantly higher, indicating a greater similarity\nwith the learned ontologies (dashed lines in Figure 2).\nThis is also reﬂected in the lexical F-measure shown\nin Figure 3. While LFmax(OEng,ODBpedia )is only\n0.24,LFmax(OEng,OWikiData )is0.37—just 0.05be-\nlowLF(OWikiData,ODBpedia ), shown as dotted line.\nForOIntl, theLFmax values are lower than their\nOEngcounterparts: LFmax(OIntl,ODBpedia )is0.18and\nLFmax(OIntl,OWikiData )is0.28. In all cases, the number\nof genres needed to achieve LFmaxapproximately equals\nthe number of genres in the reference ontology.\nWhen generated for very few genres, both learned on-\ntologies reach LP= 1.0for either reference ontology,\nas they all contain the top genres rock ,pop, etc. The\nachievableLRvalues however, differ signiﬁcantly. At a\nvery low precision level, both learned ontologies reach no\nmore thanLR= 0.5withODBpedia as reference. In con-\ntrast, at the same precision level, with OWikiData as refer-\nence,LRis greater than 0.74(Figure 2). We investigated\nwhat might be the reason for the low recall for ODBpedia\nand came to the conclusion that it contains many genresthat are unlikely to be found in standard genre tags, e.g.\nMusic of Innsbruck orMusic of Guangxi .\n5.2 Conceptual Results\nJust like the lexical results, conceptual results depend on\nthe number of genres considered and of course the refer-\nence used. Additionally, τandυinﬂuence the outcome.\nWe found that values for υ≤0.0001 hardly affect\nTP/TR/TF results, when the learned ontology is com-\npared withODBpedia orOWikiData . However, inspection\nof the learned ontologies shows, that a very low υcauses\nsome genres to have signiﬁcantly more parents than the\naverage genre. Consequently, they connect unrelated parts\nof the ontology. Examples for this are canadian and\nseventies . We argue that neither is really a musical\ngenre, but rather an orthogonal concept—a region and an\nera, respectively. This also explains why TP/TR/TF are\nunaffected, as by deﬁnition they are only inﬂuenced by\ngenres that appear in both the learned and the reference on-\ntology. Being orthogonal to the genre concept, they never\noccur in a reference ontology. We further observed, that υ\nvalues greater than 0.0001 affectTP/TR/TF negatively.\nThe following data are therefore based on υ= 0.0001 .\nWe investigated how τinﬂuencesTP/TR/TF by\ncalculating TFcscforOEng (|CEng|=1000 ) andOIntl\n(|CIntl|=1041 ) withODBpedia andOWikiData as reference\nontologies. Based on Figure 4, we chose τ= 0.17as a\nvalue reasonably suited for all ontologies.\nKeepingτandυconstant, how are taxo-\nnomic results inﬂuenced by the number of genres?\nTFcsc(OEng,ODBpedia )peaks around 160 gen-\nres withTFmax = 0.31. The same is true for\nTFcsc(OEng,OWikiData )withTFmax = 0.32. For\nTFcsc(OIntl,ODBpedia )we foundTFmaxaround 285gen-\nres with a value of 0.26and forTFcsc(OIntl,OWikiData )\naround 411 genres with 0.28(Figure 5a). In all cases,\nTFcscpeaks for genre numbers that are well below the\nnumber of genres in the reference ontology. This makes\nsense as all ontologies, to a large degree, consist of\ndisconnected genres that cannot contribute to a higher\nTFcsc. But even for most non- TFmax genre numbers,\nTFcscvalues involving the learned ontologies are higher\nthanTFcsc(OWikiData,ODBpedia ) = 0.12, depicted as\nthe dotted line in Figure 5a. It appears, as if both OEng\nandOIntlare taxonomically more similar to ODBpedia\nandOWikiData thanODBpedia toOWikiData or the other\nway around. Upon closer inspection, we attributed this\nto the greater intersection of disconnected genres from\nODBpedia andOWikiData .47.6%of the genres in the\nlexical intersection CDBpedia∩CWikiData are disconnected\nin at least one of the two ontologies. But only 36.9%of\ntheOWikiData intersection withOEngand38.6%of the\nintersection withOIntlare disconnected. Even lower are\nthe values forODBpedia : Just 17.7%of the intersection\nwithOEngand16.7%of the intersection with OIntlare\ndisconnected. As deﬁned in Section 4.2, disconnected\ngenres lead to zero tpcscvalues. This signiﬁcantly\ncontributes toODBpedia andOWikiData achieving lower404 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016ττEng/DBpediaEng/WikiDataIntl/DBpediaIntl/WikiData0.050.050.177552182531139720.122839626689891230.16408170317516240.107461842437224690.060.0600000020.18501756534506210.129473907227816510.176241657502009930.113992819326547410.070.070.198813803025714640.142841059911087060.18539730325068050.122589182451599730.080.080.20372158853598580.15372986912731510.199109695270348940.133379437251925360.090.0899999960.20495259842491880.1587815901730990.199656991850777230.137536005523076430.10.0999999940.205751809565080880.167159218480197460.197787276117395660.140779667352395360.110.109999990.206023382918003180.16958536305173770.204238977501330770.145128147713464220.120.119999990.20686352313558040.1765052842697420.201030227762460880.15303516091972630.130.130.206593916552826130.181307784676309540.21553014175920560.162601152963671420.140.140.2050794698848550.184046375929699520.21362334967075460.163215614253124170.150.150.203520461354298550.186200866226835770.21538012250832770.163213848133147460.160.160000010.203450988141503280.191504096044928930.203998840231615690.164660567633743260.170.170000020.20080034112711930.193889992893745120.195089902812525420.160280620083250140.180.180000020.199663569687492880.189549791353941120.18991612004383580.162887222896999560.190.190000030.196385586785663840.188789073430273320.188812295665638170.164222900830075870.20.200000030.192786044065576920.18741705097725150.185561973790631380.16384680707314380.210.210000040.187025059872932410.186871489669075320.182723240469443940.164549403367231730.220.220000040.18245416334371280.184712138848956760.179504195561715620.16486809180852450.230.230000050.177122472923929960.179781583606427760.179609933267203040.163969689736048750.240.240000050.175057600682272160.177421188200721860.177556476666819260.16394770515360790.250.250000060.173481450022883840.17822774624828630.174827550839002420.162509296037429580.260.260000050.17392102480788710.17864698658051860.175915097406036140.15975189827983490.270.270000040.169044390137584380.17030771233869890.17313334762897780.162094717252260520.280.280000030.16658923000129010.166435766726525880.171856105016008060.160639145736504980.290.290000020.165510994060966870.16528285374305980.16776280377717740.154046467667523780.30.30.162692496707240230.168027462332317550.166191473041458040.15155583062968670.310.310.16303769753055460.167526879282271860.162047848513927780.148179180159161260.320.320.162158713372742870.166449785587475420.161109231930601140.147635471237974550.330.329999980.151527824372683920.157348996664102260.156226948007481960.141641507935853170.340.339999970.15087964258646310.155787201860506740.151094217268608940.1365555367444990.350.349999960.148219665877363520.151710404930790940.151730539482994140.136924880983679860.360.359999950.14807643740082470.150702373652130040.14965451297843410.137043338650841620.370.369999950.147500690671041240.15053005826215460.142683870085390440.132392113711204070.380.379999940.146333149943700920.15053005826215460.136092324530143440.127019715396170160.390.389999930.142493150456002750.15041301671718880.134084228296439430.127501286694791240.40.399999920.140192156398740360.148515909621747880.132301951899054080.12358835799939903\nTaxonomic F-Measure TF csc0.00.10.20.3\nτ 0.050.100.150.200.250.300.350.40\nEng/DBpedia\nEng/WikiData\nIntl/DBpedia\nIntl/WikiData\n\u00001Figure 4 . Taxonomic F-measure TFcscforOEng\n(|CEng|=1000 ) andOIntl(|CIntl|=1040 ) compared with\nODBpedia andOWikiData for varyingτvalues.\nTaxonomic F-Measure TF con0.00.10.20.30.40.50.6\nNumber of Genres101001,00010,000\nTaxonomic F-Measure TF csc0.00.10.20.30.40.50.6\n101001,00010,000\nEng/DBpedia\nEng/WikiData\nIntl/DBpedia\nIntl/WikiData\nWikiData/DBpedia(a)\n(b)\n\u00006\nFigure 5 . Taxonomic F-measures TFcscandTFconfor\nlearned ontologies and different genre numbers compared\nwithODBpedia andOWikiData .τ= 0.17,υ= 0.0001 .\nTP/TR/TF values, when compared with each other,\nthan a pairing that does not have as many disconnected\ngenres in common. By removing all disconnected genres\nfromCC∩CRbefore calculating TPcsc, we calculated\ntheconnected taxonomic precision (TPcon), which results\nin higher values for all pairings, and especially for\n(ODBpedia ,OWikiData ) (Figure 5b). The problem with\ngenre ontologies is, that from a taxonomic point of view,\nthe reference ontologies are, to a large degree, trivial.\nTPconattempts to work around the problem by accepting\nthat there are disconnected genres and ignores them when\ncalculatingTP.\n5.3 Discussion\nThe results show that, using the proposed method, it\nis possible to create an ontology that is almost as sim-\nilar toOWikiData as the alternative reference ontology\nODBpedia —on both the lexical and conceptual level. When\ncomparing learned ontologies with the more comprehen-\nsiveODBpedia , the results are not quite as good: while\nit is possible to generate an ontology that is as similar to\nODBpedia asOWikiData on the conceptual level, it was notChildren’s Music\nChildren Children’s Kindermusik\nFigure 6 . Declinations and translations in OIntl.\npossible on the lexical level due to the many uncommon\ngenres contained in DBpedia.\nSourcing genre tags from international instead of just\nEnglish users has proven detrimental to lexical similar-\nity, when comparing with either ODBpedia orOWikiData .\nWhen inspectingOIntl, we noted translations and decli-\nnations of genre names. They are often close relatives in\nthe generated hierarchy (e.g. Figure 6). On one hand, this\nclearly contributed to worse lexical results. On the other\nhand, we see this as a potentially useful property. Differ-\nent crowd-sourced notations in a reference ontology sim-\nplify lookups, because there is no mismatch between the\nnames that are really being used and the names that occur\nin the ontology. Furthermore, it allows easy measurement\nof semantic similarity for unknown notations or transla-\ntions, e.g. via the length of the shortest connecting path. It\nalso adds a cultural dimension, as children’s music\nandKindermusik are clearly the same genre, but a par-\nent looking for music may prefer music from its own cul-\nture and chooses one genre over the other.\nAll differences put aside, one must not forget that the\nmentioned ontologies can be linked and thus complement\neach other. A missing connection in one ontology, may\nbe made through another one. The generated ontolo-\ngies can be found at http://www.tagtraum.com/\nlearned_ontologies.html and contain sameAs -\nrelations with WikiData and DBpedia.\n6. CONCLUSION AND FUTURE WORK\nDBpedia and WikiData both consist of two parts: The ﬁrst\npart contains disconnected genres that have neither parents\nnor sub-genres. It has little value in a taxonomic sense, but\ncan still serve as linkable data in LOD-space. The second\npart is an imperfect, but rich, interconnected hierarchy of\nrelatively popular genres that can be used for similarity es-\ntimation and therefore recommender systems. Because of\nthe way DBpedia is created, not all language editions are\nrepresented equally well.\nBy exploiting co-occurrence rates of user submitted\ngenre labels, we were able to learn new genre ontologies.\nUsing established lexical and conceptual similarity mea-\nsures, we successfully demonstrated the validity of the pro-\nposed learning method. Further, to improve conceptual\nsimilarity measures with largely trivial reference ontolo-\ngies, we proposed an additional measure, the connected\ntaxonomic precision.\nFuture work may add translation recognition and im-\nprove genre name normalization. Taking advantage of\nlearned genre ontologies may lead to interesting new music\ninformation retrieval applications.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 4057. REFERENCES\n[1] S ¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens\nLehmann, Richard Cyganiak, and Zachary Ives. DBpe-\ndia: A nucleus for a web of open data . Springer, 2007.\n[2] Tim Berners-Lee, James Hendler, and Ora Lassila.\nThe semantic web. Scientiﬁc American , 284(5):28–37,\n2001.\n[3] Christian Bizer, Jens Lehmann, Georgi Kobilarov,\nS¨oren Auer, Christian Becker, Richard Cyganiak, and\nSebastian Hellmann. DBpedia–a crystallization point\nfor the web of data. Web Semantics: science, ser-\nvices and agents on the world wide web , 7(3):154–165,\n2009.\n[4]`Oscar Celma and Pedro Cano. From hits to niches?\nor how popular artists can bias music recommendation\nand discovery. In Proceedings of the 2nd KDD Work-\nshop on Large-Scale Recommender Systems and the\nNetﬂix Prize Competition , page 5. ACM, 2008.\n[5] Philipp Cimiano, Andreas Hotho, and Steffen Staab.\nLearning concept hierarchies from text corpora us-\ning formal concept analysis. Journal of AI Research\n(JAIR) , 24:305–339, 2005.\n[6] Philipp Cimiano, Alexander M ¨adche, Steffen Staab,\nand Johanna V ¨olker. Ontology learning. In Handbook\non ontologies , pages 245–267. Springer, 2009.\n[7] Klaas Dellschaft and Steffen Staab. On how to perform\na gold standard based evaluation of ontology learn-\ning. In The Semantic Web-ISWC 2006 , pages 228–241.\nSpringer, 2006.\n[8] Tommaso Di Noia, Roberto Mirizzi, Vito Claudio Os-\ntuni, Davide Romito, and Markus Zanker. Linked open\ndata to support content-based recommender systems.\nInProceedings of the 8th International Conference on\nSemantic Systems , pages 1–8. ACM, 2012.\n[9] Ben Fields, Sam Phippen, and Brad Cohen. A case\nstudy in pragmatism: exploring the practical failure\nmodes of linked data as applied to classical music cat-\nalogues. In Proceedings of the 2nd International Work-\nshop on Digital Libraries for Musicology , pages 21–\n24. ACM, 2015.\n[10] Alexander M ¨adche and Steffen Staab. Measuring sim-\nilarity between ontologies. In Knowledge engineering\nand knowledge management: Ontologies and the se-\nmantic web , pages 251–263. Springer, 2002.\n[11] Sergio Oramas, Mohamed Sordo, Luis Espinosa-Anke,\nand Xavier Serra. A semantic-based approach for artist\nsimilarity. In Proceedings of the 16th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , pages 100–106, M ´alaga, Spain, 2015.\n[12] Franc ¸ois Pachet and Daniel Cazaly. A taxonomy of\nmusical genres. In Proceedings of the Content-basedMultimedia Information Access Conference (RIAO) ,\npages 1238–1245, Paris, France, 2000.\n[13] Alexandre Passant. dbrec–music recommendations us-\ning DBpedia. In The Semantic Web–ISWC 2010 , pages\n209–224. Springer, 2010.\n[14] Alexandre Passant and Yves Raimond. Combining so-\ncial music and semantic web for music-related rec-\nommender systems. In The 7th International Semantic\nWeb Conference , volume 19, 2008.\n[15] Yves Raimond, Samer Abdallah, Mark Sandler, and\nFrederick Giasson. The music ontology. In Proceed-\nings of the 8th International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , pages 417–422,\nVienna, Austria, 2007.\n[16] Hendrik Schreiber. Improving genre annotations for\nthe million song dataset. In Proceedings of the 16th\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pages 241–247, M ´alaga, Spain,\n2015.\n[17] Mohamed Sordo, `Oscar Celma, Mart ´ın Blech, and En-\nric Guaus. The quest for musical genres: Do the ex-\nperts and the wisdom of crowds agree? In Proceedings\nof the 9th International Society for Music Information\nRetrieval Conference (ISMIR) , pages 255–260, 2008.\n[18] ˙Ipek Tatlı and Ays ¸enur Birt ¨urk. Using semantic re-\nlations in context-based music recommendations. In\nWorkshop on Music Recommendation and Discovery\n(WOMRAD), ACM Conference on Recommender Sys-\ntems. , pages 14–17, Chicago, IL, USA, 2011.\n[19] Wilson Wong, Wei Liu, and Mohammed Bennamoun.\nOntology learning from text: A look back and into\nthe future. ACM Computing Surveys (CSUR) , 44(4):20,\n2012.406 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Simultaneous Separation and Segmentation in Layered Music.",
        "author": [
            "Prem Seetharaman",
            "Bryan Pardo"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417987",
        "url": "https://doi.org/10.5281/zenodo.1417987",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/057_Paper.pdf",
        "abstract": "In many pieces of music, the composer signals how in- dividual sonic elements (samples, loops, the trumpet sec- tion) should be grouped by introducing sources or groups in a layered manner. We propose to discover and lever- age the layering structure and use it for both structural segmentation and source separation. We use reconstruc- tion error from non-negative matrix factorization (NMF) to guide structure discovery. Reconstruction error spikes at moments of significant sonic change. This guides segmen- tation and also lets us group basis sets for NMF. The num- ber of sources, the types of sources, and when the sources are active are not known in advance. The only informa- tion is a specific type of layering structure. There is no separate training phase to learn a good basis set. No prior seeding of the NMF matrices is required. Unlike standard approaches to NMF there is no need for a post-processor to partition the learned basis functions by group. Source groups are learned automatically from the data. We eval- uate our method on mixtures consisting of looping source groups. This separation approach outperforms a standard clustering NMF source separation approach on such mix- tures. We find our segmentation approach is competitive with state-of-the-art segmentation methods on this dataset.",
        "zenodo_id": 1417987,
        "dblp_key": "conf/ismir/SeetharamanP16",
        "content": "SIMULTANEOUS SEPARATION AND SEGMENTATION IN LAYERED\nMUSIC\nPrem Seetharaman\nNorthwestern University\nprem@u.northwestern.eduBryan Pardo\nNorthwestern University\npardo@northwestern.edu\nABSTRACT\nIn many pieces of music, the composer signals how in-\ndividual sonic elements (samples, loops, the trumpet sec-\ntion) should be grouped by introducing sources or groups\nin a layered manner. We propose to discover and lever-\nage the layering structure and use it for both structural\nsegmentation and source separation. We use reconstruc-\ntion error from non-negative matrix factorization (NMF)\nto guide structure discovery. Reconstruction error spikes at\nmoments of signiﬁcant sonic change. This guides segmen-\ntation and also lets us group basis sets for NMF. The num-\nber of sources, the types of sources, and when the sources\nare active are not known in advance. The only informa-\ntion is a speciﬁc type of layering structure. There is no\nseparate training phase to learn a good basis set. No prior\nseeding of the NMF matrices is required. Unlike standard\napproaches to NMF there is no need for a post-processor\nto partition the learned basis functions by group. Source\ngroups are learned automatically from the data. We eval-\nuate our method on mixtures consisting of looping source\ngroups. This separation approach outperforms a standard\nclustering NMF source separation approach on such mix-\ntures. We ﬁnd our segmentation approach is competitive\nwith state-of-the-art segmentation methods on this dataset.\n1. INTRODUCTION\nAudio source separation, an open problem in signal pro-\ncessing, is the act of isolating sound producing sources (or\ngroups of sources) in an audio scene. Examples include\nisolating a single person’s voice from a crowd of speakers,\nthe saxophone section from a recording of a jazz big band,\nor the drums from a musical recording [13].\nA system that can understand and separate musical sig-\nnals into meaningful constituent parts (e.g. melody, back-\ning chords, percussion) would have many useful applica-\ntions in music information retrieval and signal process-\ning. These include melody transcription [18], audio remix-\ning [28], karaoke [21], and instrument identiﬁcation [8].\nMany approaches have been taken to audio source sep-\naration, some of which take into account salient aspects\nc/circlecopyrtPrem Seetharaman, Bryan Pardo. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: Prem Seetharaman, Bryan Pardo. “Simultaneous separation\nand segmentation in layered music”, 17th International Society for Music\nInformation Retrieval Conference, 2016.\nFigure 1 . An exemplar layering structure in classical mu-\nsic - String Quartet No. 1, Op. 27, Mvt IV , Measures 1-5.\nbyEdvard Grieg . The instruments enter one at a time in a\nlayering structure, guiding the ear to both the content and\nthe different sources.\nof musical structure, such as musical scores, or pitch (see\nSection 2). Few algorithms have explicitly learned musical\nstructure from the audio recording (using no prior learning\nand no musical score) and used it to guide source discov-\nery and separation. Our approach is designed to leverage\ncompositional structures that introduce important musical\nelements one by one in layers . In our approach, separa-\ntion alternates with segmentation, simultaneously discov-\nering the layering structure and the functional groupings of\nsounds.\nIn a layered composition, the composer signals how in-\ndividual sound sources (clarinet, cello) or sonic elements\n(samples, loops, sets of instruments) should be grouped.\nFor example, often a song will start by introducing sources\nindividually (e.g. drums, then guitar, then vocals, etc) or\nin groups (the trumpet section). Similarly, in many songs,\nthere will be a “breakdown”, where most of the mixture\nis stripped away, and built back up one element at a time.\nIn this way, the composer communicates to the listener the\nfunctional musical groups (where each group may consist\nof more than one source) in the mixture. This layering\nstructure is widely found in modern music, especially in\nthe pop and electronic genres (Figure 2), as well as classi-\ncal works (Figure 1).\nWe propose a separation approach that engages with the\ncomposer’s intent, as expressed in a layered musical struc-\nture, and separates the audio scene using discovered func-\ntional elements. This approach links the learning of the\nsegmentation of music to source separation.\nWe identify the layering structure in an unsupervised\nmanner. We use reconstruction error from non-negative\nmatrix factorization (NMF) to guide structure discovery.495Reconstruction error spikes at moments of signiﬁcant sonic\nchange. This guides segmentation and also lets us know\nwhere to learn a new basis set.\nOur approach assumes nothing beyond a layering struc-\nture. The number of sources, the types of sources, and\nwhen the sources are active are not known a priori . In\nparallel with discovering the musical elements, the algo-\nrithm temporally segments the original music mixture at\nmoments of signiﬁcant change. There is no separate train-\ning phase to learn a good basis set. No prior seeding of\nthe NMF matrices is required. Unlike standard NMF there\nis no need for a post-processor that groups the learned ba-\nsis functions by source or element [9] [25]. Groupings are\nlearned automatically from the data by leveraging informa-\ntion the composer put there for a listener to ﬁnd.\nOur system produces two kinds of output: a tempo-\nral segmentation of the original audio at points of signif-\nicant change, and a separation of the audio into the con-\nstituent sonic elements that were introduced at these points\nof change. These elements may be individual sources, or\nmay be groups (eg. stems, orchestra sections).\nWe test our method on a dataset of music built from\ncommercial musical loops, which are placed in a layering\nstructure. We evaluate the algorithm based on separation\nquality, as well as segmentation accuracy. We compare our\nsource separation method to standard NMF, paired with a\npost processer that clusters the learned basis set into groups\nin a standard way. We compare our segmentation method\nto the algorithms included in the Musical Structure Analy-\nsis Framework (MSAF) [16].\nThe structure of this paper is as follows. First, we de-\nscribe related work in audio source separation and mu-\nsic segmentation. Then, we give an overview of our pro-\nposed separation/segmentation method, illustrated with a\nreal-world example. We then evaluate our method on our\ndataset. Finally, we consider future work and conclude.\n2. RELATED WORK\n2.1 Music segmentation\nA good music segmentation reports perceptually relevant\nstructural temporal boundaries in a piece of music (e.g.\nverse, chorus, bridge, an instrument change, a new source\nentering the mixture).\nA standard approach for music segmentation is to lever-\nage the self-similarity matrix [7]. A novelty curve is ex-\ntracted along the diagonal of the matrix using a checker-\nboard kernel. Peak picking on this novelty curve results in\na music segmentation. The relevance of this segmentation\nis tied to the relevance of the similarity measure.\n[12] describes a method of segmenting music where\nframes of audio are labeled as belonging to different states\nin a hidden Markov model, according to a hierarchical la-\nbeling of spectral features. [10] takes the self-similarity\nmatrix and uses NMF to ﬁnd repeating patterns/clusters\nwithin it. These patterns are then used to segment the au-\ndio. [17] expands on this work by adding a convex con-\nstraint to NMF.[22] infers the structural properties of music based on\nstructure features that capture both local and global prop-\nerties of a time series, with similarity features.\nThe most similar work to ours is [27], which uses shift\ninvariant probabilistic latent component analysis to extract\nmusical riffs and repeated patterns from a piece of music.\nThe activation of these recurring temporal patterns is used\nto then segment the audio. Our approach takes into account\ntemporal groupings when ﬁnding these patterns, whereas\ntheir approach does not.\nOur proposed method uses the reconstruction error of a\nsource model over time in a musical signal in order to ﬁnd\nstructural boundaries. We explicitly connect the problem\nof music segmentation with the problem of audio source\nseparation and provide an alternative to existing approaches\nto ﬁnding points of signiﬁcant change from the audio.\n2.2 Source separation\nThere are several source separation methods that leverage\nhigh-level musical information in order to perform audio\nsource separation.\nSeparation from repeating patterns: REPET [21] sep-\narates the repeating background structure (e.g. bass, back-\ning chords, rhythm from the guitar, and drums in a band)\nfrom a non-repeating foreground (e.g. lead singer with a\nmelody) by detecting a periodic pattern (e.g. a sequence of\nchords that repeats every four bars). While REPET models\nthe repeating background structure as a whole, our pro-\nposed method models the individual musical elements im-\nplicit in the composer’s presentation of the material and\ndoes not require a ﬁxed periodic pattern.\nIn [20], source models are built using a similarity ma-\ntrix. This work looks for similar time frames anywhere in\nthe signal, using a similarity measure (cosine similarity)\nthat does not take musical grouping or temporal structure\ninto account. Our method leverages the temporal group-\nings created by the composer’s layering of sonic elements.\nInformed separation: [4] incorporates outside infor-\nmation about the musical signal. A musical score gives\ninformation about the pitch and timing of events in the au-\ndio and is commonly used for informed musical separation.\nFirst, [4] ﬁnds an alignment between the low-level audio\nsignal and the high-level musical score (in MIDI form).\nThe pitch and timing of the event are then used to per-\nform audio separation. These score-informed approaches\nare elaborated on in [6]. Our approach, does not require\na score. Musical elements are discovered, modelled, and\nseparated from the mixture using only the mixture, itself.\nNon-negative matrix factorization (NMF) . Our work\nuses NMF which was ﬁrst proposed for audio source sep-\naration in [24]. Probabilistic latent component analysis\n(PLCA) can be seen as a probabilistic formulation of NMF,\nand is also used for source separation [23].\nNMF ﬁnds a factorization of an input matrix X(the\nspectrogram) into two matrices, often referred to as spec-\ntral templates Wand activations H. Straightforward NMF\nhas two weaknesses when used for source separation that\nwill be elaborated on in Section 3: (1) there is no guar-496 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 20160:00 0:15 0:30 0:46 1:01\nTime (s)04521098219611025Frequency (Hz)Spectrogram for One More Time - Daft Punk\n0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240\nTime (beats)0.00.20.40.60.81.0Reconstruction errorNormalized reconstruction error over time for sampled layer\n0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240\nTime (beats)0.00.20.40.60.81.0Reconstruction errorNormalized reconstruction error over time for full discovered layerFigure 2 . The top graph shows the spectrogram of 0:00 to 1:01 of One More Time , byDaft Punk . Ground truth segmentation\nis shown by the solid vertical black lines, where each line signals a new source starting. The middle graph shows the\nbehavior of the reconstruction error of a sampled source layer over time ( e). When new layers begin, reconstruction error\nnoticeably spikes and changes behavior. The bottom graph shows the reconstruction error over time for a full model of the\nﬁrst layer. Beats are shown by the vertical dashed black lines.\nantee that an individual template (a column of W) corre-\nsponds to only one source and (2) spectral templates are\nnot grouped by source. Until one knows which templates\ncorrespond to a particular source or element of interest, one\ncannot separate out that element from the audio.\nOne may solve these problems by using prior training\ndata to learn templates, or meta-data, such as musical scores\n[6], to seed matrices with approximately-correct templates\nand activations. User guidance to select the portions of the\naudio to learn from has also been used [2].\nTo group spectral templates by source without user guid-\nance, researchers typically apply timbre-based clustering\n[9] [25]. This does not consider temporal grouping of sources.\nThere are many cases where sound sources with dissimilar\nspectra (e.g. a high squeak and a tom drum, as in Work-\ning in a Coal Mine by DEVO) are temporally grouped as a\nsingle functional element by the composer. Such elements\nwill not be grouped together with timbre-based clustering.\nA non-negative Hidden Markov model (NHMM) [15]\nhas been used to separate individual spoken voices from\nmixtures. Here, multiple sets of spectral templates are\nlearned from prior training data and the system dynam-\nically switches between template sets based on the esti-\nmated current state in the NHMM Markov model. A sim-\nilar idea is exploited in [3], where a classiﬁcation system\nis employed to determine whether a spectral frame is de-\nscribed by a learned dictionary for speech.\nOur approach leverages temporal grouping created by\ncomposers in layered music. This lets us appropriately\nlearn and group spectral templates without the need for\nprior training, user input or extra information from a musi-\ncal score, or post-processing.\n3. NON-NEGATIVE MATRIX FACTORIZATION\nWe now provide a brief overview of non-negative matrix\nfactorization (NMF). NMF is a method to factorize a non-negative matrix Xas the product of two matrices Wand\nH. In audio source separation, Xis the power spectrogram\nof the audio signal, which is given as input. Wis inter-\npreted as a set of spectral templates (e.g. individual notes,\nthe spectrum of a snare hit, etc.). His interpreted as an\nactivation matrix indicating when the spectral templates of\nWare active in the mixture. The goal is to learn this dic-\ntionary of spectral templates and activation functions. To\nﬁndWandH, some initial pair of WandHare created\nwith (possibly random) initial values. Then, a gradient de-\nscent algorithm is employed [11] to update WandHat\neach step, using an objective function such as:\nargmin W,H||X−WH||2\nF (1)\nwhere||·||2\nFrefers to the Frobenius norm. Once the differ-\nence between WH andXfalls below an error tolerance,\nthe factorization is complete.\nThere are typically many approximate solutions that fall\nbelow any given error bound. If one varies the initial W\nandHand restarts, a different decomposition is likely to\noccur. Many of these will not have the property that each\nspectral template (each column of W) represents exactly\none element of interest. For example, it is common for a\nsingle spectral template to contain audio from two or more\nelements of interest (e.g. a mixture of piano and voice in\none template). Since these templates are the atomic units\nof separation with NMF, mixed templates preclude suc-\ncessful source separation. Therefore, something must be\ndone to ensure that, after gradient descent is complete,\neach spectral template belongs to precisely one group or\nsource of interest. An additional issue is that, to perform\nmeaningful source separation, one must partition these spec-\ntral templates into groups of interest for separation. For\nexample, if the goal is to separate piano from drums in a\nmixture of piano and drums, all the templates modeling the\ndrums should be grouped together.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 497One can solve these problems by using prior training\ndata, running the algorithm on audio containing an iso-\nlated element of interest to learn a restricted set of W. One\ncan repeat this for multiple elements of interest to separate\naudio from a mixture using these prior learned templates.\nThis avoids the issues caused from learning the spectral\ntemplates directly from the mixture: one template having\nportions of two sources, and not knowing which templates\nbelong to the same musical element. One may also use\nprior knowledge (e.g. a musical score) to seed the Wand\nHmatrices with values close to the desired ﬁnal goal. We\npropose an alternative way of grouping spectral templates\nthat does not require prior seeding of matrices [6] or user\nsegmentation of audio to learn the basis set for each desired\ngroup [2], nor post-processing to cluster templates.\n4. PROPOSED APPROACH\nOur approach has four stages: estimation, segmentation,\nmodeling, and separation. We cycle through these four\nstages in that order until all elements and structure have\nbeen found.\nEstimation: We assume the composer is applying the\ncompositional technique of layering. This means that es-\ntimating the source model from the ﬁrst few audio frames\nwill give us an initial model of the ﬁrst layer present in the\nrecording. Note that in our implementation, we beat track\nthe audio [14] [5]. Beat tracking reduces the search space\nfor a plausible segmentation, but is not integral to our ap-\nproach.\nWe use the frames from the ﬁrst four beats to learn the\ninitial spectral dictionary. Consider two time segments in\nthe audio, with i,jandkas temporal boundaries: X=\n[Xi:j−1,Xj:k]. To build this model, we use NMF on a\nsegment of Xi:j−1to ﬁnd spectral templates West.\nSegmentation: Once an estimated dictionary Westis\nfound, we measure how well it models the mixture over\ntime. Keeping Westﬁxed, learn the activation matrix H\nfor the second portion. The reconstruction error for this is:\nerror (WestH,Xj:k) =||Xj:k−WestH||2\nF (2)\nEquation 2 measures how well the templates in West\nmodel the input X. For example, assume Westwas con-\nstructed a spectrogram of snare drum hits in segment Xi:j−1.\nIf a guitar and bass are added to the mixture somewhere in\nthe range j:k,then the reconstruction error on Xj:kwill\nbe greater than the reconstruction error on Xi:j−1. We use\nreconstruction error as a signal for segmentation. We slide\nthe boundaries j, kover the the mixture, and calculate er-\nror for each of these time segments, as shown in Figure 2.\nThis gives us e, a vector of reconstruction errors for each\ntime segment.\nIn the middle graph in Figure 2, we show reconstruction\nerror over time, quantized at the beat level. Reconstruc-\ntion error spikes on beats where the audio contains new\nsounds not modeled in West. As layers are introduced\nby the artist, reconstruction error of the initial model risesAlgorithm 1 Method for ﬁnding level changes in recon-\nstruction error over time, where ⊙is the element-wise\nproduct. eis a vector where e(t)is the reconstruction er-\nror for Westat time step t.lagis the size of a smoothing\nwindow for e.pandqaffect how sensitive the algorithm is\nwhen ﬁnding boundary points. We use lag= 16 ,p= 5.5\nandq=.25in our implementation. These values were\nfound when training on a different dataset, containing 5\nmixtures, than the one in Section 5.\nlag, p, q←initialize, tunable parameters\ne←reconstruction error over time for West\ne←(e⊙e) ⊿Element-wise product\nd←max(∆e/∆t)\nforifromlagto length( e)do ⊿Indexing e\nwindow←ei−lag:i−1\nm←median(abs(window - median(window))\nifabs(ei−median(window))≥p∗mthen\nifabs(ei−ei−1)≥q∗dthen\nreturn i ⊿ Boundary frame in X\nend if\nend if\nend for\nreturn length( e) ⊿Last frame in X\nconsiderably. Identifying sections of signiﬁcant change in\nreconstruction error gives a segmentation on the music. In\nFigure 2, the segmentation is shown by solid vertical lines.\nAt each solid line, a new layer is introduced into the mix-\nture. Our method for identifying these sections uses a mov-\ning median absolute deviation, and is detailed in Algorithm\n1.\nModeling: once the segmentation is found, we learn a\nmodel using NMF on the entire ﬁrst segment. This gives us\nWfull, which is different from West, which was learned\nfrom just ﬁrst four beats of the signal. In Figure 2, the ﬁrst\nsegment is the ﬁrst half of the audio. Wfullis the ﬁnal\nmodel used for separating the ﬁrst layer from the mixture.\nAs can be seen in the bottom graph of Figure 2, once the\nfull model is learned, the reconstruction error of the ﬁrst\nlayer drops.\nSeparation: once the full model Wfullis learned, we\nuse it for separation. To perform separation, we construct\na binary mask using NMF. Wfullis kept ﬁxed, and His\ninitialized randomly for the entire mixture. The objective\nfunction described in Section 3 is minimized only over H.\nOnceHis found, WfullHtells us when the elements of\nWfullare active. We use a binary mask for separation,\nobtained via:\nM=round (WfullH/circledividemax(WfullH, abs(X)))\nwhere/circledivideindicates element-wise division and ⊙is element-\nwise multiplication. We reconstruct the layer using:\nXlayer =M⊙X (3)\nXresidual = (1−M)⊙X (4)498 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Loop ALoop ALoop ALoop BLoop BLoop C   ++\nAA+B A+B+Ctime\nmixtureFigure 3 . Construction of a single mixture using a layering\nstructure in our dataset, from 3randomly selected loops\neach from 3setsA,B, andC.\nwhere⊙indicates element-wise product. Xresidual is the\nmixture without the layer. We restart at the estimation\nstage above, this time using Xresidual as the input, and set-\nting the start point to the segmentation boundary found in\nthesegmentation stage above. Taking the inverse Fourier\ntransform of Xlayer gives us the audio signal of the sepa-\nrated layer.\nTermination: ifXresidual is empty (no source groups\nremain in the mixture), we terminate.\n5. EVALUATION\n5.1 Dataset\nWe evaluate our approach in two ways: separation qual-\nity, and segmentation accuracy. To do this, we construct\na dataset where ground truth is known for separation and\nsegmentation. As our approach looks for a layering struc-\nture, we devise mixtures where this layering occurs. We\nobtain source audio from Looperman [1], an online re-\nsource for musicians and composers looking for loops and\nsamples to use in their creative work. Each loop from\nLooperman is intended by its contributor to represent a sin-\ngle source. Each loop can consist of a single sound produc-\ning source (e.g. solo piano) or a complex group of sources\nworking together (e.g. a highly varied drumkit).\nFrom Looperman , we downloaded 15of these loops,\neach 8seconds long at 120beats per minute. These loops\nare divided into three sets of 5loops each. Set Acontained\n5loops of rhythmic material (drum-kit based loops mixed\nwith electronics), set Bcontained 5loops of harmonic and\nrhythmic material performed on guitars, and set Ccon-\ntained 5loops of piano. We arranged these loops to create\nmixtures that had layering structure, as seen in Figure 3.\nWe start with a random loop from set A, then add a ran-\ndom loop from B, then add a random loop from C, for a\ntotal length of 24seconds. We produce 125of these mix-\ntures.\nGround truth segmentation boundaries are at 8seconds\n(when the second loop comes in), and at 16seconds (when\nthe third loop comes in). In Figure 3, each row is ground\ntruth for separation.\nSDR SIR SAR\nSeparation quality measure15\n10\n5\n0510152025Measure (dB)-3.302.674.83\n0.305.00 5.404.0612.89\n5.8113.0119.07\n14.47better valuesSeparation quality results\nNMF (Clustered MFCC, K = 24)\nNMF (Clustered MFCC, K = 100)\nProposed\nIdeal binary maskFigure 4 . Separation performance of current and proposed\nalgorithms, and an ideal binary mask. Higher numbers are\nbetter. The ideal binary mask is an upper bound on sepa-\nration performance. The error bars indicate standard devi-\nations above and below the mean.\nProposed CNMF SF Foote\nSegmentation approach0.00.51.01.52.02.53.03.54.0Average median deviation (s)0.53 s 0.53 s0.97 s3.30 sbetter valuesDeviation between est. and ref. boundaries\nFigure 5 . Segmentation performance of current and pro-\nposed algorithms. Est. and ref. refers to the median de-\nviation in seconds between a ground truth boundary and a\nboundary estimated by the algorithm. Lower numbers are\nbetter. The error bars indicate standard deviations above\nand below the mean.\n5.2 Methods for comparison\nFor separation, we compare our approach to a separation\nmethod in [25]. In this method, they use NMF on the en-\ntire mixture spectrogram, and then cluster the components\ninto sources using MFCCs. Each cluster of components is\nthen used to reconstruct a single source in the mixture. In\nour approach, the number of components ( K) was ﬁxed at\nK= 8, giving a total of K= 24 components for the entire\nmixture. For direct comparison, we give the method in [25]\nK= 24 components. We also look at the case where [25]\nis given K= 100 components.\nFor segmentation, we compare our approach with [22],\n[17], and [7].Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 499Approach Median deviation (s) Avg # of segments\nCNMF [17] .53 6.152\nSF [22] .97 4.024\nFoote [7] 3.3 3.048\nProposed .53 3.216\nTable 1 . Segmentation results for various approaches. In\nthe dataset, an accurate segmentation reports 3segments.\nWhile CNMF reports similar average median deviation\nfrom estimated to reference boundaries to the proposed\nmethod, it ﬁnds almost twice the number of boundaries.\nFoote ﬁnds a number of segments closer to ground truth,\nbut the boundaries are in the wrong place.\n5.3 Results\n5.3.1 Separation\nTo measure separation quality, we use the BSS Eval tool-\nbox [26] as implemented in [19], which reports Source-to-\nDistortion (SDR), Source-to-Interference (SIR), and Source-\nto-Artifact (SAR) ratios. For all of these, we compare our\nproposed approach to an NMF clustering approach based\non MFCCs in [25]. This clustering approach was given\nthe number of sources to ﬁnd in the mixture. This is in\ncontrast to our algorithm, where the number of sources is\nunknown, and instead is discovered. We also compare to\nan ideal binary mask. Results are in Figure 4, which shows\nmean SDR, SIR, and SAR for different source separation\nmethods.\nAs seen in Figure 4, our approach found sources that\ncorrelated with the target sources, giving SDR and SIR\nmore comparable to the ideal binary mask. This is in con-\ntrast to the clustering approach, which found sources that\npoorly correlated with the actual target sources, resulting in\nlow values for SDR and SIR, even when using more com-\nponents than our approach ( K= 100 vs.K= 24 ). The\nclustering mechanism in [25] leverages MFCCs, and ﬁnds\nsources that are related in terms of resonant characteristics\n(e.g. instrument types) but fails to model sources that have\nmultiple distinct timbres working together.\nOur results indicate that separation based on NMF re-\nconstruction error is a useful signal to guide the grouping\nof spectral templates for NMF, and boost separation qual-\nity on layered mixtures.\n5.3.2 Segmentation\nTo measure segmentation accuracy, we use the median ab-\nsolute time difference from a reference boundary to its near-\nest estimated boundary, and vice versa. For both of these\nmeasures, we compare our proposed approach with [22],\n[17], and [7], implemented in MSAF [16], as shown in Fig-\nure 5.\nWe ﬁnd that our approach is as accurate as existing\nstate-of-the-art, as can be seen in Figure 5 and Table 1.\nOur results indicate that, when ﬁnding a segmentation of\na mixture, in which segment boundaries are dictated by\nsources entering the mixture, current approaches are not\nsufﬁcient. Our approach, because it uses reconstruction er-ror of source models to drive the segmentation, ﬁnds more\naccurate segment boundaries.\n6. CONCLUSIONS\nWe have presented a method for source separation and mu-\nsic segmentation which uses reconstruction error in non-\nnegative matrix factorization to ﬁnd and model groups of\nsources according to discovered layered structure. Our\nmethod does not require pre-processing of the mixture or\npost-processing of the basis sets. It requires no user input,\nor pre-trained external data. It bootstraps an understand-\ning of both the segmentation and the separation from the\nmixture alone. It is a step towards a framework in which\nseparation and segmentation algorithms can inform one an-\nother, for mutual beneﬁt. It makes no assumptions on what\na source actually is, but rather ﬁnds functional sources im-\nplied by a speciﬁc type of musical structure.\nWe showed that tracking reconstruction error of a source\nmodel over time in a mixture is a helpful approach to ﬁnd-\ning structural boundary points in the mixture. These struc-\ntural boundary points can be used to guide NMF. This sep-\naration approach outperforms NMF that clusters spectral\ntemplates via heuristics. This work demonstrates a clear,\nnovel, and useful relationship between the problems of sep-\naration and segmentation.\nThe principles behind this approach can be expanded to\nother source separation approaches. Since source separa-\ntion algorithms rely on speciﬁc cues (e.g. repetition like\nin REPET, or a spectral model like in NMF), the temporal\nfailure points of source separation algorithms (e.g. the re-\npeating period has failed, or the model found by NMF has\nfailed to reconstruct the mixture) may be a useful cue for\nmusic segmentation.\nThe approach presented here exploits the compositional\ntechnique of layering employed in many musical works.\nFor future approaches, we would like to build separation\ntechniques which leverage other compositional techniques\nand musical structures, perhaps integrating our work with\nexisting work in segmentation.\n7. ACKNOWLEDGEMENTS\nThis work is supported by National Science Foundation\nGrant 1420971.\n8. REFERENCES\n[1] Looperman. http://www.looperman.com.\n[2] Nicholas J Bryan, Gautham J Mysore, and Ge Wang.\nIsse: An interactive source separation editor. In Pro-\nceedings of the SIGCHI Conference on Human Factors\nin Computing Systems , pages 257–266. ACM, 2014.\n[3] Zhiyao Duan, Gautham J Mysore, and Paris\nSmaragdis. Online plca for real-time semi-supervised\nsource separation. In Latent Variable Analysis and\nSignal Separation , pages 34–41. Springer, 2012.500 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016[4] Zhiyao Duan and Bryan Pardo. Soundprism: An online\nsystem for score-informed source separation of mu-\nsic audio. Selected Topics in Signal Processing, IEEE\nJournal of , 5(6):1205–1215, 2011.\n[5] Daniel PW Ellis. Beat tracking by dynamic program-\nming. Journal of New Music Research , 36(1):51–60,\n2007.\n[6] Sebastian Ewert, Bryan Pardo, Mathias Muller, and\nMark D Plumbley. Score-informed source separation\nfor musical audio recordings: An overview. Signal\nProcessing Magazine, IEEE , 31(3):116–124, 2014.\n[7] Jonathan Foote. Automatic audio segmentation using\na measure of audio novelty. In Multimedia and Expo,\n2000. ICME 2000. 2000 IEEE International Confer-\nence on , volume 1, pages 452–455. IEEE, 2000.\n[8] Toni Heittola, Anssi Klapuri, and Tuomas Virtanen.\nMusical instrument recognition in polyphonic audio\nusing source-ﬁlter model for sound separation. In IS-\nMIR, pages 327–332, 2009.\n[9] Rajesh Jaiswal, Derry FitzGerald, Dan Barry, Eugene\nCoyle, and Scott Rickard. Clustering nmf basis func-\ntions using shifted nmf for monaural sound source\nseparation. In Acoustics, Speech and Signal Process-\ning (ICASSP), 2011 IEEE International Conference on ,\npages 245–248. IEEE, 2011.\n[10] Florian Kaiser and Thomas Sikora. Music structure\ndiscovery in popular music using non-negative matrix\nfactorization. In ISMIR , pages 429–434, 2010.\n[11] Daniel D Lee and H Sebastian Seung. Algorithms for\nnon-negative matrix factorization. In Advances in neu-\nral information processing systems , pages 556–562,\n2001.\n[12] Mark Levy and Mark Sandler. Structural segmenta-\ntion of musical audio by constrained clustering. Audio,\nSpeech, and Language Processing, IEEE Transactions\non, 16(2):318–326, 2008.\n[13] Josh H McDermott. The cocktail party problem. Cur-\nrent Biology , 19(22):R1024–R1027, 2009.\n[14] Brian McFee, Colin Raffel, Dawen Liang, Daniel PW\nEllis, Matt McVicar, Eric Battenberg, and Oriol Nieto.\nlibrosa: Audio and music signal analysis in python. In\nProceedings of the 14th Python in Science Conference ,\n2015.\n[15] Gautham J Mysore, Paris Smaragdis, and Bhiksha\nRaj. Non-negative hidden markov modeling of audio\nwith application to source separation. In Latent vari-\nable analysis and signal separation , pages 140–148.\nSpringer, 2010.\n[16] O. Nieto and J. P. Bello. Msaf: Music structure analytis\nframework. In The 16th International Society for Music\nInformation Retrieval Conference , 2015.[17] Oriol Nieto and Tristan Jehan. Convex non-negative\nmatrix factorization for automatic music structure\nidentiﬁcation. In Acoustics, Speech and Signal Pro-\ncessing (ICASSP), 2013 IEEE International Confer-\nence on , pages 236–240. IEEE, 2013.\n[18] Mark D Plumbley, Samer A Abdallah, Juan Pablo\nBello, Mike E Davies, Giuliano Monti, and Mark B\nSandler. Automatic music transcription and audio\nsource separation. Cybernetics &Systems , 33(6):603–\n627, 2002.\n[19] Colin Raffel, Brian McFee, Eric J Humphrey, Justin\nSalamon, Oriol Nieto, Dawen Liang, Daniel PW El-\nlis, and C Colin Raffel. mir eval: A transparent imple-\nmentation of common mir metrics. Proc. of the 15th\nInternational Society for Music Information Retrieval\nConference , 2014.\n[20] Zafar Raﬁi and Bryan Pardo. Music/voice separation\nusing the similarity matrix. In ISMIR , pages 583–588,\n2012.\n[21] Zafar Raﬁi and Bryan Pardo. Repeating pattern ex-\ntraction technique (repet): A simple method for mu-\nsic/voice separation. Audio, Speech, and Language\nProcessing, IEEE Transactions on , 21(1):73–84, 2013.\n[22] Jean Serra, Mathias Muller, Peter Grosche, and\nJosep Ll Arcos. Unsupervised music structure annota-\ntion by time series structure features and segment simi-\nlarity. Multimedia, IEEE Transactions on , 16(5):1229–\n1240, 2014.\n[23] Madhusudana Shashanka, Bhiksha Raj, and Paris\nSmaragdis. Probabilistic latent variable models as\nnonnegative factorizations. Computational intelligence\nand neuroscience , 2008, 2008.\n[24] Paris Smaragdis. Non-negative matrix factor decon-\nvolution; extraction of multiple sound sources from\nmonophonic inputs. In Independent Component Anal-\nysis and Blind Signal Separation , pages 494–499.\nSpringer, 2004.\n[25] Martin Spiertz and V olker Gnann. Source-ﬁlter based\nclustering for monaural blind source separation. In\nProceedings of International Conference on Digital\nAudio Effects DAFx09 , 2009.\n[26] Emmanuel Vincent, R ´emi Gribonval, and C ´edric\nF´evotte. Performance measurement in blind audio\nsource separation. Audio, Speech, and Language Pro-\ncessing, IEEE Transactions on , 14(4):1462–1469,\n2006.\n[27] Ron J Weiss and Juan P Bello. Unsupervised discov-\nery of temporal structure in music. Selected Topics in\nSignal Processing, IEEE Journal of , 5(6):1240–1251,\n2011.\n[28] John F Woodruff, Bryan Pardo, and Roger B Dan-\nnenberg. Remixing stereo music with score-informed\nsource separation. In ISMIR , pages 314–319, 2006.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 501"
    },
    {
        "title": "Mining Musical Traits of Social Functions in Native American Music.",
        "author": [
            "Daniel Shanahan",
            "Kerstin Neubarth",
            "Darrell Conklin"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416408",
        "url": "https://doi.org/10.5281/zenodo.1416408",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/167_Paper.pdf",
        "abstract": "Native American music is perhaps one of the most doc- umented repertoires of indigenous folk music, being the subject of empirical ethnomusicological analyses for sig- nificant portions of the early 20th century. However, it has been largely neglected in more recent computational research, partly due to a lack of encoded data. In this pa- per we use the symbolic encoding of Frances Densmore’s collection of over 2000 songs, digitized between 1998 and 2014, to examine the relationship between internal musical features and social function. More specifically, this paper applies contrast data mining to discover global feature pat- terns that describe generalized social functions. Extracted patterns are discussed with reference to early ethnomusi- cological work and recent approaches to music, emotion, and ethology. A more general aim of this paper is to pro- vide a methodology in which contrast data mining can be used to further examine the interactions between musical features and external factors such as social function, geog- raphy, language, and emotion.",
        "zenodo_id": 1416408,
        "dblp_key": "conf/ismir/ShanahanNC16",
        "content": "MINING MUSICAL TRAITS OF SOCIAL FUNCTIONS IN NATIVE\nAMERICAN MUSIC\nDaniel Shanahan1Kerstin Neubarth2Darrell Conklin3,4\n1Louisiana State University, Baton Rouge, LA, USA\n2Canterbury Christ Church University, United Kingdom\n3University of the Basque Country UPV/EHU, San Sebastian, Spain\n4IKERBASQUE, Basque Foundation for Science, Bilbao, Spain\nABSTRACT\nNative American music is perhaps one of the most doc-\numented repertoires of indigenous folk music, being the\nsubject of empirical ethnomusicological analyses for sig-\nniﬁcant portions of the early 20th century. However, it\nhas been largely neglected in more recent computational\nresearch, partly due to a lack of encoded data. In this pa-\nper we use the symbolic encoding of Frances Densmore’s\ncollection of over 2000 songs, digitized between 1998 and\n2014, to examine the relationship between internal musical\nfeatures and social function. More speciﬁcally, this paper\napplies contrast data mining to discover global feature pat-\nterns that describe generalized social functions. Extracted\npatterns are discussed with reference to early ethnomusi-\ncological work and recent approaches to music, emotion,\nand ethology. A more general aim of this paper is to pro-\nvide a methodology in which contrast data mining can be\nused to further examine the interactions between musical\nfeatures and external factors such as social function, geog-\nraphy, language, and emotion.\n1. INTRODUCTION\nStudying “musical universals” in the context of contem-\nporary theories of music evolution, Savage et al. [23] ar-\ngue that many of the most common features across musi-\ncal cultures serve as a way of facilitating social cohesion\nand group bonding (see also [2, 18]). The focus of their\nanalysis, however, is on comparing geographical regions\nwithout systematically differentiating between social con-\ntexts and functions of music making. Across these regions,\nthe authors look for links and elements of “sameness”. The\napplication and methodology presented here can be viewed\nas complementary to the earlier study [23]. Firstly, we fo-\ncus on the relationship between internal musical features\n(such as pitch range, melodic or rhythmic variability) and\nthe speciﬁc social function ascribed to songs rather than\nfeature distributions across geographic regions. Secondly,\nc/circlecopyrtDaniel Shanahan, Kerstin Neubarth, Darrell Conklin.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Daniel Shanahan, Kerstin Neubarth,\nDarrell Conklin. “Mining Musical Traits of Social Functions in Native\nAmerican Music”, 17th International Society for Music Information Re-\ntrieval Conference, 2016.using computational techniques we study features that can\ncontrast between different social functions within a cul-\nture, rather than those that are potentially universal in mu-\nsic.\nThe folk songs of Native American groups provide a\nconvenient starting point for the analysis of social function\nand musical features: a large number of pieces has been\nrecorded by (relatively few) individuals who often anno-\ntated the music with an explicit social function. Nettl com-\nmented in 1954 that “more musical material [was] avail-\nable from this large area [...] than from any other of similar\nsize” [20, p. 45]. The collection created by Frances Dens-\nmore [25] covers repertoires from ﬁve out of the six mu-\nsical areas postulated by Nettl. Densmore collected songs\nby Native American groups (see Table 1), and recorded\nthe social usage of songs, ranging from the general (e.g.\nwar songs) to the speciﬁc (e.g. songs of the corn dance).\nBuilding on Densmore’s work, Herzog [10] discussed four\ncategories of social function in music of the North Ameri-\ncan Plains, speciﬁcally love songs, songs of hiding games,\nghost dance songs, and songs in animal stories. Employing\nquantitative analysis, Gundlach also compared songs used\nin different situations, e.g. war songs or healing songs;\ngroups of songs were taken as proxies for studying mood,\nspeciﬁcally asking if “objective characteristics of a piece of\nmusic form the basis for the mood which it may arouse” [7,\npp. 134-135]. Interestingly, Gundlach found a diversity in\nthe treatment of some musical features to convey emotion\nacross indigenous groups, such as larger intervals mainly\nassociated with “sad” love songs among the Chippewa and\nOjibway but with “happy” love songs among the Teton-\nSioux [7, p. 139].\nThis paper builds upon Gundlach’s work, exploring\nquantitative analysis to identify musical traits of songs as-\nsociated with different social functions. More speciﬁcally,\nwe adopt contrast data mining [1, 5, 21], a type of descrip-\ntive supervised data mining. In the context of music infor-\nmation retrieval, supervised data analysis has been largely\ndominated by predictive classiﬁcation, i.e. building mod-\nels that discriminate labeled groups in data and predict the\ngroup label of unseen data instances. Classiﬁers are gen-\nerally treated as a black box, and results tend to focus on\npredictive accuracy. By comparison, contrast data mining\naims to discover distinctive patterns which offer an under-\nstandable symbolic description of a group.681Discovered patterns are discussed both in light of eth-\nnomusicological writings such as those by Densmore, Her-\nzog and Gundlach, and in the context of research into mu-\nsic and emotion. The music information retrieval commu-\nnity has engaged with models and classiﬁers of emotion\nin music from many different perspectives and utilizing a\nwide range of approaches. For example, Han et al. [8] im-\nplemented support vector regression to determine musical\nemotion, and found their model to correlate quite strongly\nwith a two-dimensional model of emotion. Schmidt and\nKim used conditional random ﬁelds to model a dynamic\nemotional response to music [24]. For a thorough review of\nemotional models in music information retrieval, see Kim\net al. [14], and for an evaluation and taxonomy of the many\nemotional approaches to music cognition, see Eerola and\nVuoskoski [6]. Unlike these studies, the current study does\nnot attempt to model emotion or provide a method of emo-\ntion classiﬁcation, but considers ﬁndings from emotion and\nethological research in discussing the mining results.\n2. THE DENSMORE CORPUS\nFrances Densmore’s transcriptions of Native American\nfolksongs provide an invaluable dataset with which we\nmight examine issues pertaining to geography, language,\nand culture. As Nettl points out, many of the earlier\nrecordings were conducted in the very early days of ﬁeld\nrecording, and contain performances from elderly individ-\nuals who had little contact and inﬂuence from the Western\nmusical tradition [20]. The fact that this collection was\ntranscribed by a single individual, covers such a large geo-\ngraphic area, and focuses on cultures with disparate social\nand linguistic norms, makes it immensely useful for studies\nof large-scale relationships between music and language,\ngeography, and social function.\nInterest in digitally encoding Frances Densmore’s col-\nlection of Native American songs began in the late 1990s,\nwhen Paul von Hippel encoded excerpts of the ﬁrst book\nof Chippewa songs in 1998 into Humdrum’s **kern for-\nmat. David Huron encoded the Pawnee and Mandan books\nin 2000, and Craig Sapp encoded the lengthy Teton Sioux\nbook in 2002. In 2014, Eva and Daniel Shanahan encoded\nthe remaining books into **kern format [25]. The dig-\nitized collection contains 2,083 folksongs from 16 books\n(Table 1), collected between 1907 and 1958.1\nThe Densmore volumes provide a rich source of infor-\nmation because they not only give transcriptions of all the\ncollected songs, but also additional information – includ-\ning the associated social function – and musical analy-\nses. Densmore’s annotations were integrated as metadata\ninto the digital collection. As exact phrasings and anno-\ntation criteria vary across the chronological span of Dens-\nmore’s writing, the metadata vocabulary was prepared by\ncleaning and generalizing social function terms: ﬁrstly, in-\nconsistent phrasings were harmonized, e.g. “hand game\nsongs” (Northern Ute book) and “songs of the hand game”\n(Cheyenne and Arapaho book). Secondly, functions were\n1The corpus is available at musiccog.lsu.edu/densmoreBook Year published\nChippewa I 1910\nChippewa II 1913\nTeton Sioux 1918\nNorthern Ute 1922\nMandan and Hidatsa 1923\nPapago 1929\nPawnee 1929\nMenominee 1932\nYuman and Yaqui 1932\nCheyenne and Arapaho 1936\nNootka and Quileute 1939\nIndians of British Columbia 1943\nChoctaw 1943\nSeminole 1956\nAcoma, Isleta, Cochiti, and Zu ˜ni Pueblos 1957\nMaidu 1958\nTable 1 . Collections included in the Densmore corpus.\n...animal\ndance\ngamebird dance songsong received from \nanimal\nbear dance song\ncorn dance song\nhand game song\nmoccasin game \nsong\nball game song......\nFigure 1 . Excerpt of the social functions ontology.\nmerged to create generalized functions, e.g. different game\nsongs such as hand game songs and moccasin game songs\nwere collated into one group “game songs” (see Fig. 1).\nSongs which Densmore listed as “uncategorized” or\n“miscellaneous” were not considered. The resulting on-\ntology reduces the 223 distinct terms used by Densmore to\n31 generalized functions. Note that songs can be assigned\nmore than one function, e.g. bird dance songs are anno-\ntated as both “animal” and “dance” (see Fig. 1).\n3. CONTRAST DATA MINING\nContrast data mining [1, 5] refers to a range of methods\nwhich identify and describe differences between groups in\na dataset, and has been applied with success to several folk\nsong corpora [21]. In the current study with the Dens-\nmore corpus, groups are deﬁned by songs associated with\ndifferent social functions. Following several other earlier\nworks on contrast data mining in folk music analysis, in682 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Attribute Deﬁnition High (H)\nAverageMelodicInterval average melodic interval in semitones ≥ 1.676\nAverageNoteDuration average duration of notes in seconds ≥ 0.347\nDirectionofMotion fraction of melodic intervals that are rising rather than falling ≥ 0.388\nDuration total duration of piece in seconds ≥ 22.294\nDurationofMelodicArcs average number of notes that separate melodic peaks and troughs ≥ 1.704\nPitchVariety number of pitches used at least once ≥ 6.583\nPrimaryRegister average MIDI pitch ≥ 55.578\nRange difference between highest and lowest MIDI pitches ≥ 13.084\nRepeatedNotes fraction of notes that are repeated melodically ≥ 0.462\nSizeofMelodicArcs average melodic interval separating the top note of melodic peaks and\nbottom note of melodic troughs≥ 4.899\nStepwiseMotion fraction of melodic intervals corresponding to a minor or major second ≥ 0.250\nVariabilityofNoteDuration standard deviation of note durations in seconds ≥ 0.224\nDcontRedundancy duration contour relative redundancy ≥ 0.749\nDurRedundancy note duration relative redundancy ≥ 0.667\nIntRedundancy melodic interval relative redundancy ≥ 0.606\nMlRedundancy metric level relative redundancy ≥ 0.681\nPcontRedundancy melodic pitch contour relative redundancy ≥ 0.751\nPitchRedundancy pitch relative redundancy ≥ 0.603\nTable 2 . A selection of attributes used in this study. Top: jSymbolic attributes [17]. Bottom: information-theoretic\nattributes. The rightmost column indicates the value range for the discretisation bin High.\nthis study songs are described by global features which\nare attribute-value pairs each describing a song by a single\nvalue. Global features have been used productively in com-\nputational folk music analysis in the areas of classiﬁcation\n(e.g. [11, 17, 27]) and descriptive mining (e.g. [16, 26]).\nIt is important to highlight the distinction between at-\ntribute selection and contrast data mining. Whereas the\nformer is the process of selecting informative attributes ,\nusually for the purposes of classiﬁer construction, contrast\ndata mining is used to discover particular attribute-value\npairs (features) that have signiﬁcantly different supports in\ndifferent groups.\n3.1 Global feature representation\nAll songs in the corpus were converted to a MIDI for-\nmat, ignoring percussion tracks and extracting one sin-\ngle melodic spine for each song. Since only a fraction\nof the songs in the corpus were annotated with tempo in\nthe **kern ﬁles, all songs were standardized to a tempo of\n♩= 60 . This was followed by computing 18 global at-\ntributes: twelve attributes from the jSymbolic set [17] and\nsix newly implemented information-theoretic attributes.\nAfter discarding attributes not applicable to the current\nstudy such as those related to instrumentation, dynamics,\nor polyphonic texture, the twelve jSymbolic attributes were\nselected manually, informed by Densmore’s own writings,\nadditional ethnomusicological studies of Native American\nmusic [7, 10, 20] and research into music and emotions\n[6, 13, 22]. The six information-theoretic attributes mea-\nsure the relative redundancy within a piece of a particular\nevent attribute (pitch, duration, interval, pitch contour, du-\nration contour, and metric level). The features are deﬁnedas1−H/H maxwhereHis the entropy of the event at-\ntribute in the piece and the maximum entropy Hmaxis the\nlogarithm of the number of distinct values of the attribute\nin the piece. The value of relative redundancy therefore\nranges from 0 (low redundancy, i.e. high variability) to 1\n(high redundancy, i.e. low variability) of the particular at-\ntribute. Numeric features were discretized into categorical\nvalues, with a split point at the mean: the value Low cov-\ners attribute values below the average across the complete\ndataset, the value High covers attribute values at the av-\nerage or above (cf. [26]). Table 2 gives deﬁnitions for the\nattributes which contribute to the contrast patterns reported\nin Section 4.\n3.2 Contrast data mining method\nGlobal features are assessed as candidate contrast patterns\nby evaluating the difference in pattern support between dif-\nferent groups (e.g. [1, 5]). A feature (attribute-value pair)\nissupported by a song if the value of the attribute is true\nfor the song. Then the support n(X∧G)of a feature X\nin a groupGis the number of songs in group Gwhich\nsupport feature X. A feature is a contrast pattern for a\ncertain group if its support in the group, n(X∧G), is\nsigniﬁcantly higher or lower than in the remaining groups\ntaken together, n(X∧¬G). This is known as a one-vs.-\nall strategy for contrast mining [5, 21] as it contrasts one\ngroup against the combined set of other groups rather than\ncontrasting groups in pairs. The signiﬁcance of a pattern,\nthat is, how surprising is the under- or over-representation\nofXinG, can be quantiﬁed using the hypergeometric\ndistribution (equivalent to Fisher’s exact test ). This uses\na2×2contingency table (see Table 3) which gives theProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 683G¬G\nX n(X∧G) n(X∧¬G)n(X)\n¬X n(¬X∧G)n(¬X∧¬G)n(¬X)\nn(G) n(¬G) N\nTable 3 . Contingency table showing the occurrence of a\npatternXand its complement ¬Xin a target group Gand\nin the background¬G. The highlighted area is the support\nof the putative contrast pattern X∧G. For the Densmore\ncorpusN= 2083 .\nprobability of sampling n(X)pieces, and ﬁnding exactly\nn(X∧G)successes (instances of group G). Thus the left\nor right tails of the hypergeometric distribution give the\ntwo desired p-values: the probability of observing at most\nor at leastn(X∧G)instances in a single random sample\nofn(X)instances. A low p-value, less than some speci-\nﬁed signiﬁcance level α, indicates a statistically signiﬁcant\ncontrast pattern which is assumed to be interesting for fur-\nther exploration [3].\nFollowing the extraction of features as described in Sec-\ntion 3.1, each song in the corpus is represented by a set of\nglobal features together with a set of group labels (func-\ntions) of the song. Note that, as mentioned above, more\nthan one function can be assigned to a song. From this\ninput dataset candidate patterns are generated as the cross-\nproduct for all occurring pairs of features Xand groupsG.\nFor eachX∧Gpair its support and a p-value for each tail\nare computed and the results processed to form a matrix of\nfunction/feature patterns.\n4. RESULTS AND DISCUSSION\nA total of 17 social function groups (uncategorized and\nmiscellaneous songs and groups supported by less than ten\nsongs were not considered) were mined for contrast pairs\nwith 18 attributes. The 17 groups together cover most of\nthe corpus: 1891 of the 2083 songs. Regarding the at-\ntributes for global features, though each has two possi-\nble values High (H) and Low (L), if one is signiﬁcantly\nover-represented the other must be signiﬁcantly under-\nrepresented, therefore in this study only the High value was\nconsidered during mining. Table 4 presents the results of\nthe contrast data mining. Each cell in the matrix shows\nthe distribution of a particular feature in a particular group.\nWhite indicates presence of the feature in the group (with\narean(X∧G)) and black absence (with area n(¬X∧G)).\nThus the total area covered by a cell in a row indexed by\ngroupGisn(G). The rows and columns in the table are\nordered by the geometric mean of the p-value to all other\nfunctions or features in that particular row or column.\nStatistical signiﬁcance of each contrast pattern was\nevaluated using the hypergeometric distribution as de-\nscribed above, with signiﬁcance level α= 0.05adjusted\nusing a Bonferroni multiple testing correction factor of\n306 = 17×18, representing the number of contrast pat-\nterns tested for signiﬁcance. Using the adjusted signiﬁ-\ncance level of 0.05/306 = 1.6e-4, green areas in Table 4indicate signiﬁcant over-representation, and red areas sig-\nniﬁcant under-representation of a feature in a group. A to-\ntal of 56signiﬁcant patterns were found (colored patterns\nof Table 4).\nAs a statistical control, a permutation method was used\nto estimate the false discovery rate, assuming that most\ncontrast patterns found in randomized data would be arti-\nfactual. Social function labels were randomly redistributed\nover songs, while maintaining the overall function counts,\nthen the number of signiﬁcant (left or right tail p-value ≤\n1.6e-4) contrast patterns using the 306 possible pairs was\ncounted. Repeated 1000 times, this produced a mean of\njust1.14signiﬁcant contrast patterns per iteration, suggest-\ning that there are few false discoveries to be expected in the\ncolored patterns of Table 4.\nBearing in mind that the exact data samples and feature\ndeﬁnitions differ, the results seem to conﬁrm – and gen-\neralize to a larger dataset – several observations presented\nin earlier studies. The signiﬁcance of PrimaryRegister :H\nfor love songs recalls Gundlach’s ﬁnding that “love songs\ntend to be high” [7, p. 138]. Herzog describes the “melodic\nmake-up” of love songs as “spacious” [10, p. 28]: in our\nanalysis we ﬁnd that love songs generally have larger aver-\nage melodic intervals and a wider range than other songs.\nThe over-representation of AverageNoteDuration :Hin love\nsongs may reﬂect characterisations of love songs as slow\n[7, 10]. For hiding game songs of the Plains, Herzog no-\ntices that they are comparatively short with a very often\nlimited range [10, p. 29]; game songs in the current cor-\npus – including 90 out of the 143 game songs explicitly\nassociated with hiding games such as moccasin, hand and\nhiding-stick or hiding-bones games – show a signiﬁcant\nunder-representation of Duration :HandRange :H. The\nnarrow range that Gundlach observed in healing songs [7,\npp. 138,140] is also reﬂected in the results in Table 4, but\nin the current analysis is not statistically signiﬁcant. Gund-\nlach compared healing songs speciﬁcally against war and\nlove songs: considering only those two groups as the back-\nground does indeed lead to a lower p-value (left-tail) for\nRange :Hin healing songs (6.8e-9 instead of 2.6e-3). To-\ngether with other traits which are common in healing songs\nbut not distinctive from other song types — e.g. a high\nproportion of repeated notes and low variability in pitch,\nintervals or duration — a comparatively narrow range may\ncontribute to a soothing character of many healing songs,\nintended “to remove discomfort” [4, p. 565].\nThe information-theoretic features are particularly char-\nacteristic of songs labelled as “nature”, which show an\nover-representation of redundancy values above the aver-\nage for all six considered event attributes. The group con-\ntains 13 Yuman lightning songs, which trace the journey of\nWhite Cloud who controls lightning, thunder and storms.\nMore generally, Yuman songs tend to be of comparatively\nsmall range, the melodic movement on the whole mainly\ndescending, the rhythm dominated by few duration val-\nues and isometric organisation more common than in other\nrepertoires [9,20]; structurally, Yuman music is often based\non repeated motifs [9]. The example of the Yuman light-684 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016totaldancelovewarspiritualceremonialanimalsocietyhuntingnaturehealingchildrenlegendsgamestoriessocialharvestwomen\nPitchRedundancy :H\nRange :H\nPitchVariety :H\nPrimaryRegister :H\nDirectionofMotion :H\nVariabilityofNoteDuration :H\nDurRedundancy :H\nDurationofMelodicArcs :H\nAverageMelodicInterval :H\nRepeatedNotes :H\nIntRedundancy :H\nAverageNoteDuration :H\nPcontRedundancy :H\nStepwiseMotion :H\nDuration :H\nSizeofMelodicArcs :H\nDcontRedundancy :H\nMlRedundancy :HTable 4 . Pie charts for contrast patterns showing the distribution of social function groups (rows) against features (columns).\nWhite indicates presence and black absence of the corresponding feature. Green/red (light/dark gray in grayscale) indicate\nsigniﬁcant over/under-representation of a feature in a group.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 685ning songs opens avenues for future analysis, such as con-\nsidering also sequential pattern mining [3], and encourages\napplying data mining to questions left unanswered in ear-\nlier work, such as exploring the stylistic unity of songs\nforming a series related to a myth or ritual [9, p. 184].\nIt can be productive to discuss the mining results in the\ncontext of recent work that takes an “ethological” approach\nto music and emotion. This approach argues that pitch\nheight, tempo, dynamics, and variability convey levels of\nboth arousal and valence, and many of these relationships\nare innate, cross-cultural, and cross-species [12, 13, 19].\nSimilarly to Gundlach’s earlier work [7], we ﬁnd that war\nsongs and love songs exhibit several salient musical traits.\nIn the Densmore collection, war songs are distinguished\nfrom other song types by signiﬁcant over-representation\nof a wider than average range, higher than average regis-\nter, and higher variability in both pitch and duration (over-\nrepresentation of PitchVariety :Hand under-representation\nofPitchRedundancy :HandDurRedundancy :H). Interest-\ningly, dance songs also show signiﬁcant contrasts in these\nfeatures, but consistently in the opposite direction com-\npared to war songs. War songs and dance songs might\nboth be thought of as “high arousal”, but on opposite ends\nof the valence spectrum on Russell’s Circumplex model\n[22]. This hypothesis invites further inspection of war and\ndance songs in the corpus. Signiﬁcant features shared be-\ntween dance and animal songs ( Range :H,PitchVariety :H,\nPrimaryRegister :HandVariabilityofNoteDuration :Hbeing\nunder-represented) reﬂect the fact that many of the sup-\nporting songs – e.g. bird, bear or deer dance songs – are\nannotated with both “dance” and “animal” (see also Fig. 1).\nIn love songs, the over-representation of higher pitch\nregisters, observed both by Gundlach and in the current\nstudy, seems in line with Huron’s acoustic ethological\nmodel [13], according to which higher pitches (alongside\nquiet dynamics) connote afﬁliation. For a Pawnee love\nsong Densmore relates her informant’s explanation that\nin this song a married couple for the ﬁrst time openly\nexpressed affection for each other. Both Densmore and\nGundlach characterize many love songs as “sad”, asso-\nciated with departure, loss, longing or disappointment,\nwhich might be reﬂected in the relatively slow movement\nof many love songs (see above). Remarkably, though,\nat ﬁrst inspection other contrast patterns describing love\nsongs (e.g. under-representation of IntRedundancy :Hor\nover-representation of PrimaryRegister :H) seem at odds\nwith ﬁndings on e.g. sad speech which contains markers of\nlow arousal such as weak intervallic variability and lower\npitch [15]. However, when comparing observations across\nstudies, their speciﬁc feature deﬁnitions and analysis meth-\nods need to be taken into account. In the current study,\nsigniﬁcant contrast features are discovered relative to the\nfeature distributions in the dataset, both in terms of feature\nvalues and thus the mean value in the corpus (used in dis-\ncretizing global features into values LowandHigh), and oc-\ncurrence across groups (used in evaluating signiﬁcant over-\nor under-representation during contrast mining).5. CONCLUSIONS\nThis paper has presented the use of descriptive contrast pat-\ntern mining to identify features which distinguish between\nNative American songs associated with different social\nfunctions. Descriptive mining is often used for explorative\nanalysis, as opposed to statistical hypothesis testing or pre-\ndictive classiﬁcation. Illustrating contrast pattern mining\nin an application to the Densmore collection, results sug-\ngest musical traits which describe contrasts between mu-\nsics in different social contexts. Different from studies\nfocusing on putative musical universals [23], which test\ngeneralized features with disjunctive values (e.g. two- or\nthree-beat subdivisions), and from attribute selection stud-\nies [27], which do not specify distinctive values, global-\nfeature contrast patterns make explicit an attribute and\nvalue pair which is distinctive for a certain song type. In\nthis case study, mining results conﬁrm ﬁndings of earlier\nethnomusicological research based on smaller samples, but\nalso generate questions for further investigation.\nThe Densmore corpus of Native American music pro-\nvides a rich resource for studying relations between inter-\nnal musical features and contextual aspects of songs, in-\ncluding not only their social function but also e.g. lan-\nguages and language families [25], geographical or mu-\nsical areas [20]. Thus, contrast mining of the Densmore\ncollection could be extended to other groupings. Regard-\ning social functions, the ontology used here possibly could\nbe linked to anthropological taxonomies on functions of\nmusical behaviour (e.g. [2, 18]), whose categories on their\nown are too broad for the purposes of contrast pattern min-\ning but could open additional interpretations if integrated\ninto hierarchical, multi-level, mining. Regarding pattern\nrepresentations, the method of contrast data mining is very\ngeneral and in theory any logical predicate can be used to\ndescribe groups of songs. For future work we intend to\nexplore the use of sequential melodic patterns to describe\nsocial functions in the Densmore corpus, and also to apply\nthe methods to other large folk song collections.\n6. ACKNOWLEDGMENTS\nThis research is partially supported by the project\nLrn2Cre8 which is funded by the Future and Emerg-\ning Technologies (FET) programme within the Seventh\nFramework Programme for Research of the European\nCommission, under FET grant number 610859. The au-\nthors would like to thank Olivia Barrow, Eva Shanahan,\nPaul von Hippel, Craig Sapp, and David Huron for encod-\ning data and assistance with the project.\n7. REFERENCES\n[1] Stephen Bay and Michael Pazzani. Detecting group\ndifferences: Mining contrast sets. Data Mining and\nKnowledge Discovery , 5(3):213–246, 2001.\n[2] Martin Clayton. The social and personal functions of\nmusic in cross-cultural perspective. In Susan Hallam,\nIan Cross, and Michael Thaut, editors, The Oxford686 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Handbook of Music Psychology , pages 35–44. Oxford\nUniversity Press, 2008.\n[3] Darrell Conklin. Antipattern discovery in folk tunes.\nJournal of New Music Research , 42(2):161–169, 2013.\n[4] Frances Densmore. The use of music in the treatment\nof the sick by American Indians. The Musical Quar-\nterly, 13(4):555–565, 1927.\n[5] Guozhu Dong and Jinyan Li. Efﬁcient mining of\nemerging patterns: discovering trends and differences.\nInProceedings of the 5th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining\n(KDD-99) , pages 43–52, San Diego, CA, USA, 1999.\n[6] Tuomas Eerola and Jonna K. Vuoskoski. A review\nof music and emotion studies: approaches, emotion\nmodels, and stimuli. Music Perception: An Interdisci-\nplinary Journal , 30(3):307–340, 2013.\n[7] Ralph H. Gundlach. A quantitative analysis of In-\ndian music. The American Journal of Psychology ,\n44(1):133–145, 1932.\n[8] Byeong-Jun Han, Seungmin Rho, Roger B. Dannen-\nberg, and Eenjun Hwang. SMERS: Music emotion\nrecognition using support vector regression. In Pro-\nceedings of the 10th International Society for Music\nInformation Retrieval Conference (ISMIR 2009) , pages\n651–656, Kobe, Japan, 2009.\n[9] George Herzog. The Yuman musical style. The Journal\nof American Folklore , 41(160):183–231, 1928.\n[10] George Herzog. Special song types in North American\nIndian music. Zeitschrift f ¨ur vergleichende Musikwis-\nsenschaft , 3:23–33, 1935.\n[11] Ruben Hillewaere, Bernard Manderick, and Darrell\nConklin. Global feature versus event models for folk\nsong classiﬁcation. In Proceedings of the 10th Inter-\nnational Society for Music Information Retrieval Con-\nference (ISMIR 2009) , pages 729–733, Kobe, Japan,\n2009.\n[12] Leanne Hinton, Johanna Nichols, and John Ohala, ed-\nitors. Sound Symbolism . Cambridge University Press,\n1995.\n[13] David Huron. Understanding music-related emotion:\nLessons from ethology. In Proceedings of the 12th\nInternational Conference on Music Perception and\nCognition (ICMPC 2012) , pages 23–28, Thessaloniki,\nGreece, 2012.\n[14] Youngmoo E. Kim, Erik M. Schmidt, Raymond\nMigneco, Brandon G. Morton, Patrick Richardson, Jef-\nfrey Scott, Jacquelin A. Speck, and Douglas Turnbull.\nMusic emotion recognition: A state of the art review. In\nProceedings of the 11th International Society for Mu-\nsic Information Retrieval Conference (ISMIR 2010) ,\npages 255–266, Utrecht, The Netherlands, 2010.[15] Emile Kraepelin. Psychiatrie. Ein Lehrbuch f ¨ur\nStudierende und ¨Arzte, ed. 2 . Edinburgh: E.& S. Liv-\ningstone, 1921.\n[16] Mario L.G. Martins and Carlos N. Silla Jr. Irish tra-\nditional ethnomusicology analysis using decision trees\nand high level symbolic features. In Proceedings of the\nSound and Music Computing Conference (SMC 2015) ,\npages 455–462, Maynooth, Ireland, 2015.\n[17] Cory McKay. Automatic music classiﬁcation with\njMIR . PhD thesis, McGill University, Canada, 2010.\n[18] Alan P. Merriam and Valerie Merriam. The Anthropol-\nogy of Music . Northwestern University Press, 1964.\n[19] Eugene S. Morton. On the occurrence and signiﬁcance\nof motivation-structural rules in some bird and mam-\nmal sounds. American Naturalist , 111(981):855–869,\n1977.\n[20] Bruno Nettl. North American Indian musical styles.\nThe Journal of American Folklore , 67(263,265,266),\npages 44–56,297–307,351–368, 1954.\n[21] Kerstin Neubarth and Darrell Conklin. Contrast pattern\nmining in folk music analysis. In David Meredith, ed-\nitor, Computational Music Analysis , pages 393–424.\nSpringer, 2016.\n[22] James A. Russell. Core affect and the psychologi-\ncal construction of emotion. Psychological Review ,\n110(1):145–172, 2003.\n[23] Patrick E. Savage, Steven Brown, Emi Sakai, and\nThomas E. Currie. Statistical universals reveal the\nstructures and functions of human music. Proceedings\nof the National Academy of Sciences , 112(29):8987–\n8992, 2015.\n[24] Erik M. Schmidt and Youngmoo E. Kim. Modeling\nmusical emotion dynamics with conditional random\nﬁelds. In Proceedings of the 12th International Soci-\nety for Music Information Retrieval Conference (IS-\nMIR 2011) , pages 777–782, Miami, FL, USA, 2011.\n[25] Daniel Shanahan and Eva Shanahan. The Densmore\ncollection of Native American songs: A new corpus for\nstudies of effects of geography and social function in\nmusic. In Proceedings for the 13th International Con-\nference for Music Perception and Cognition (ICMPC\n2014) , pages 206–209, Seoul, Korea, 2014.\n[26] Jonatan Taminau, Ruben Hillewaere, Steijn Meganck,\nDarrell Conklin, Ann Now ´e, and Bernard Manderick.\nDescriptive subgroup mining of folk music. In 2nd In-\nternational Workshop on Music and Machine Learning\nat ECML/PKDD 2009 (MML 2009) , Bled, Slovenia,\n2009.\n[27] P. van Kranenburg, A. V olk, and F. Wiering. A com-\nparison between global and local features for computa-\ntional classiﬁcation of folk song melodies. Journal of\nNew Music Research , 42(1):1–18, 2013.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 687"
    },
    {
        "title": "SiMPle: Assessing Music Similarity Using Subsequences Joins.",
        "author": [
            "Diego Furtado Silva",
            "Chin-Chia Michael Yeh",
            "Gustavo E. A. P. A. Batista",
            "Eamonn J. Keogh"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1415012",
        "url": "https://doi.org/10.5281/zenodo.1415012",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/099_Paper.pdf",
        "abstract": "Most algorithms for music information retrieval are based on the analysis of the similarity between feature sets ex- tracted from the raw audio. A common approach to as- sessing similarities within or between recordings is by creating similarity matrices. However, this approach re- quires quadratic space for each comparison and typically requires a costly post-processing of the matrix. In this work, we propose a simple and efficient representation based on a subsequence similarity join, which may be used in several music information retrieval tasks. We ap- ply our method to the cover song recognition problem and demonstrate that it is superior to state-of-the-art algo- rithms. In addition, we demonstrate how the proposed representation can be exploited for multiple applications in music processing.",
        "zenodo_id": 1415012,
        "dblp_key": "conf/ismir/SilvaYBK16",
        "content": "SIMPLE : ASSESSING MUSIC SIMILARITY USING \nSUBSEQUENCES JOINS  \nDiego F . Silva1,2, Chin -Chia M . Yeh2,Gustavo E. A. P. A. Batista1, Eamonn Keogh2 \n1 Instituto de Ciências Matemáticas e de Computação , Universidade de São Paulo, Brazil  \n2 Department of Computer Science and Engineering, University of California, Riverside, USA  \ndiegofsilva@icmc.usp.br,myeh003@ucr.edu,gbatista@icmc.usp.br,eamonn@ucr.edu  \nABSTRACT  \nMost algorithms for music information retrieval are based \non the analysis of the similarity between feature sets ex-\ntracted from the raw audio. A  common approach to as-\nsessing similarities within or between recordings is by \ncreating similarity matrices. However, this approach re-\nquires quadratic space for each comparison and typically  \nrequires a costly post -processing of the matrix. In this \nwork, we propose a simple and efficient representation \nbased on a subsequence similarity join , which may be \nused in several music information retrieval  tasks. We ap-\nply our method to the cover song recognition problem \nand demonstrate that it is superior to state -of-the-art algo-\nrithms. In addition, we demonstrate how the proposed \nrepresentation can be exploited for multiple applications \nin music processing.  \n1. INTRODUCTION  \nWith the grow ing interest in applications related to music \nprocessing, the area of music informatio n retrieval (MIR) \nhas attracted huge attention in both academia and indus-\ntry. However , the analysis of audio recordings remains a  \nsignificant challenge . Most algorithms for content -based \nmusic retrieval have at their cores  some  similarity or dis-\ntance function . For this reason, a wide range of applica-\ntions  rely on some technique  to assess the similarity be-\ntween music objects.  Such applications  includ e segmen-\ntation  [8], audio -to-score alignment  [4], cover song \nrecognition  [15], and visualization  [23]. \n A common approach to assess ing similarity in music \nrecordings  is achieved by utilizing  a self-similarity matrix \n(SSM) [5]. This representation reveals the relation ship \nbetween  each “snippet”  of a track  to all the other seg-\nments in the same recording.  This idea has been general-\nized to  measur e the relation ships  between  subsequences \nof different  songs , as in  the application of  cross -\nrecurrence analysis for cover song recognition [16]. \n The main advantage of similarity matrices is the fact \nthat they  simultaneously reveal both the  global  and the  \nlocal  structure of music recordings. However , this repre-\nsentation requires quadratic space in relation to the length \nof the feature vector used to describe the audio. For this \nreason, most methods to find patterns in the similarity matrix are (at least) quadrati c in time complexity. In spite \nof this , most information contained in similarity matrices \nis irrelevant or has little  impact in its analysis. This ob-\nservation suggest s the need for a  more space and time ef-\nficient  representation of music recordings . \n In this work , we extend the subsequences all-pairs -\nsimilarity -search , also known as similarity join , in order  \nto assess the similarity between audio recordings for  MIR \ntasks . As with  the common similarity matrices, represent-\ning the entire  subsequence  join require s a quadratic \nspace, and also  has a high time complexity , which is de-\npendent on the length of the subsequences to be joined.  \n However, in this work we show that we can exploit a \nnew data structure call ed matrix profile  which allows a  \nspace efficient representation  of the similarity  join matrix \nbetween subsequences . Moreover, we can leverage recent \noptimizations in FFT -based all -neighbor search that allow \nthe matrix profile to be computed efficiently [10]. For \nclarity , we refer to the representation presented in this \npaper as Similarity Matrix Profi LE (SiMPle). \n Figure 1 illustrates  an example of two matrices repre-\nsenting the dissimilarities within and between recordings  \nand their relative SiMPle , which  correspond to the mini-\nmum value of each column of the similarity matrices.  \n050 100 150 200 250 3002468\n50100150200250300\n510152025\n50 100 150 200 250 300 350\n50100150200250300\n05101520\n50 100 150 200 250 300\n50 100 150 200 250 300 35005101520\n \nFigure 1. Similarity matrix within ( left) and between different \nsongs ( right ) and their respective SiMPle.  \n In summary, o ur method  has the following ad-\nvantages /features : \n It is a novel approach to assess the audio similarity \nand can be used in several  MIR algorithm s; \n We exploit  the fastest known subsequence similarity \nsearch technique in the literature  [10], which makes \nour method fast and exact;  \n It is simple and only requires a single  parameter , \nwhich is intuitive  to set  for MIR applications;  \n It is space efficient, requiring the storage of  only \nO(n) values ; \n Once we calculate the similarity profile for a dataset \nit can be efficiently updated, which has implications \nfor streaming audio processing .  © Diego F. Silva, Chin -Chia M. Yeh, Gustavo E. A. P. A. \nBatista, Eamonn Keogh. Licensed under a Creative Commons Attribu-\ntion 4.0 International License (CC BY 4.0). Attribution:  Diego F. Sil-\nva, Chin -Chia M. Yeh, Gustavo E. A. P. A. Batista, Eamonn Keogh.  \n“SiMPle: Assessing Music Similarity Using Subsequences Joins”, 16th \nInternational Society for Music Information Retrieval Conference, 2016.  \n23  \n \n2. SiMPle : SIMILARITY  MATRIX PROFILE  \nWe begin by describing the operation for produci ng the \nmatrix profile,  a similarity join . For clarity , we use the \nterm time series  to refer to the ordered set of features that \ndescribe a whole recording and subsequence  to define \nany continuous subset of features  from  the time series.  \nDefinition 1:  Similarity join : given two time series A \nand B with the desired subsequence length m, the sim-\nilarity join identifies  the nearest neighbor of each sub-\nsequence (with length m) in A from all the possible \nsubsequence set of B. \n Through  such a  similarity  join, we can gather two \npieces of information about each subsequence in A, \nwhich are: 1) the  Euclidean distance to its nearest neigh-\nbor in B and 2 ) the position of its nearest neighbor in B. \nSuch information can be compactly stored in vectors , re-\nferred as  similarity matrix profile  (SiMPle) and similarity \nmatrix pr ofile index  (SiMPle index) respectively.  \n One special case of similarity join is when both input \ntime series refer to  the same recording . We define the op-\neration that handles this specific case self-similarity join . \nDefinition 2:  Self-similarity join : given a time series A \nwith the desired subsequence length m, the self -\nsimilarity join identifies the non -trivial nearest neigh-\nbor of each subsequence (with length m) in A from all \nthe possible subseque nce set of A. \n The only major difference between self -similarity join \n(Definition 2 ) and similarity join ( Definition 1 ) is the \nexclusion of trivial matched pairs when identifying the \nnearest neighbor. The exclusion of trivial match es is cru-\ncial as matching a subsequence with itself (or slightly \nshifted version of itself) produces no useful  information .  \n We describe our method  to calculate SiMPle  in Algo-\nrithm 1 . In line 1, we record  the length of B. In line 2, we \nallocate memory and initial ize SiMPle  PAB and SiMPle  \nindex IAB. From line  3 to line 6, we calculate the distance \nprofile  vector D which contains the distance s between a \ngiven subsequence in time series B and each subsequence \nin time series A. The particular function we used to com-\npute D is MASS (Mueen’s Algorithm for Similarity \nSearch) , which is the most efficient algorithm  known for \ndistance vector computation  [10]. We then  perform the  \npairwise minimum for each element in D with the paired \nelement in PAB (i.e., min( D[i], PAB[i]) for i = 0 to \nlength( D) - 1.) We also update IAB[i] with idx when D[i] ≤ \nPAB[i] as we perform the pairwise minimum operation. \nFinally, we return the result PAB and IAB in line 7.  \nAlgorithm 1. Procedure to c alculate SiMPle and SiMPle  index  \nInput:  Two user provided time series, A and B, and the desired  subse-\nquence length m \nOutput:  The SiMPle  PAB and the associated SiMPle  index IAB  \n1 \n2 \n3 \n4 \n5 \n6 \n7 nB ← Length( B) \nPAB ← infs, IAB ← zeros, idxes  ← 1: nB-m+1 \nfor each  idx in idxes  \n          D ← MASS (B[idx:idx+m -1], TA) // c.f. [10] \n          PAB, IAB ← ElementWiseMin( PAB, IAB, D, idx) \nend for  \nreturn PAB, IAB \n Note that the Algorithm 1  computes SiMPle  for the \ngeneral similarity join. To modify  it to compute the self -similarity join SiMPle  of a time series A, we simply re-\nplace B by A in line s 1 and  4 and ignore trivial match es in \nD when performing ElementWiseMin  in line 5.  \n The method MASS  (used in line 4) is important to \nspeed -up the similarity calculatio ns. This algorithm has a \ntime complexity of O(n log n ). For brevity , we refer the \nreader  interested in details of this method  to [10].  \n In this work , we focus on demonstr ating the utility of \nSiMPle  on the cover song recognition task.  Given that the \ncover song recognition is a specialization of the “query -\nby-similarity ” task, we believe that it is the best  scenario \nto evaluate a similarity method . Specifically, we propose \na SiMPle -based  distance measure  between a query and its \npotential original version . \n3. COVER SONG RECOGNITI ON \n“Cover song ” is the generic term used to denote a new \nperformance of a previously recorded track . For example , \na cover song may refer to a live performance, a remix or \nan interpretation in a different  music style . The automatic \nidentification of covers has several applications, such as \ncopyright management, collection organization , and \nsearch by content.   \n In order to identify different versions of the same \nsong, most algorithms search  for global ly [20] or local ly \n[15][18] conserved structure(s) . A well -known and wide-\nly applied algorithm for measur ing the global similarit y \nbetween tracks is Dynamic Time Warping (DTW)  [11]. \nIn spite of its utility in other domains , DTW  is not gener-\nally robust to differences in  structure between the record-\nings. A potential solution would be segment ing the song \nbefore apply ing the DTW similarity estimation. However, \naudio segmentation  itself  is also an open problem , and the \nerror on boundaries detection can cause a domino effect  \n(compounded errors)  in the whole identification process.  \n In addition, the complexity of the algorithm to calcu-\nlate DTW is O(n2). Although methods to fast approxi-\nmate the DTW have been proposed  [13], there is no error \nbound  for such approximations. In other words, it is not \npossible to set a maximum error in the value obtained by \nit in relation to the actual DTW.  \n Algorithms that search  for local similarities have been \nsuccessfully used to provide structural  invariance to the \ncover song identification task. A widely used  method for \nmusic similarity proposes the use of a binary distance  \nfunction to compare chroma -based  features followed by a \ndynamic programming local alignment [15]. Despite its \ndemonstrated utility  to recognize cover recordings, this \nmethod has several parameters , that are unintuitive to \ntune, and is slow.  Specifically , the local alignment is es-\ntimated by an  algorithm with similar complexity to DTW . \nPlus,  the binary distance between chroma features used in \neach step of the algorithm relies on multiple  shifts of the \nchroma vectors under comparison.  \n3.1 SiMPle -Based  Cover Son g Recognition  \nIn this work , we propose to use  SiMPle  to measure the \ndistance between recordings in order to identify cover \nsongs. In essence we exploit the fact that the  global  rela-\ntion between the tracks  is composed of many  local  simi-24 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016  \n \nlarities. In this way , we are able to  simultaneously take \nadvantage  of both local and global pattern  matching.  \n Intuitively , we should  expect that the SiMPle obtained \nby comparing a cover song to its original version  is com-\nposed mostly  of low values. In contrast,  two completely \ndifferent songs will result in a SiMPle constituted mainly \nby high values.  For this reason, we adopted the median \nvalue of the SiMPle  as a global distance estimation. For-\nmally, the distance between a query B and a candidate  \noriginal recordi ng A is defined in Equation 1.  \n \n dist(A,B)=median (SiMPle(B,A)) (1) \n \n Note that several  other measures of statistics could  be \nused instead of the median. However, the median is ro-\nbust to outliers  in the matrix profile. Such distortions may \nappear when a performer decide s, for instance,  to add a \nnew segment  (e.g., an improvisation or drum solo ) to the \nsong. The robustness of our method to this situation, as \nwell as other changes in structure, is di scussed in the next \nsection.  \n3.2 On the Structural Invariance  \nThe structural variance is a critical concern when  com-\nparing different songs. Changes in structure may occur by \ninsertion or deletion  of segments, as well as chang es in \nthe order that different exce rpts are played. From a high -\nlevel  point of view, SiMPle  describes  a global similarity \noutline between songs by providing information of local \ncomparisons.  This fact has several implications in our \ndistance estimation , which makes it largely  invariant to \nstructural variations : \n If two performances are virtually identical, except for \nthe order and the number of repetitions of each rep-\nresentative excerpt (i.e. , chorus, verse, bridge, etc.), \nall the values that compose SiMPle are close to zero;  \n If a segment of  the original version is deleted  in the \ncover song, this will cause virtually no changes in \nthe SiMPle ; \n If a new feature is inserted  into a cover, this will \nhave as consequence a peak in the SiMPle that will \ncause only a slight  increase  in its median value . \n4. EXPERIMENTAL EVALUAT ION  \nThe evaluation of  different  choices of features  sets is not \nthe main focus of this paper . For this reason, we fix the \nuse of chroma -based features in our experiments, as it is \nthe most popular feature set to analyze music data. In or-\nder to provide local tempo invariance, we used the chro-\nma energy normalized statistics  (CENS) [12]. Specif ical-\nly, for the cover song recognition task, we adopted  the \nrate of two CENS per second of audio.  \n In addition, we preprocessed the feature sets in each \ncomparison to provide key invariance. B efore calculating  \nthe similarity between songs, we transpose on e of them in \norder to have the same key using the optimal transposi-\ntion index (OTI) [14].  \n We notice that we are committed to the reproducibil-\nity of our results, a nd we encourage researchers and prac-\ntitioners to extend our ideas and evaluate the use of the SiMPle in different MIR tasks. To this end, we created a \nwebsite [19] with the complete source code used in our \nexperiments and videos highlighting some of the results \npresented in this work.  \n4.1 Datasets  \nWe evaluate our method in different scenarios regarding \nmusic styles and size of the databases. Specifically, we \ntested  the proposed distance measure ’s utility  for as-\nsessing both popular and classical recordings.  \n The first database considered  is the YouTube Covers  \n[18], composed of 50 different compositions, each one \ncontaining 7 different recordings  obtained from YouTube \nvideos . The data was originally split in to training and \ntesting partitions, in which  the training set is composed of \nthe origina l recording in studio and a live version per-\nformed by the same artist. To allow comparisons to the \nliterature , we follow the same configuration.  \n The second dataset  we consider  is the widely used \ncollection of Chopin’s Mazurkas. The set of Mazurkas \nused in  this work  contains 2 ,919 record ings of 49 pieces \nfor piano. The number of recordings of each song varies \nfrom 41 to 95 . \n4.2 Results  and Discussion  \nIn order to assess the performance  of our method , we \nused three commonly applied evaluation measures: mean \navera ge precision (MAP), precision at 10 (P@10) , and \nmean rank of first correctly identified cover (MR1).  Note \nthat for MR1, smaller values are  better.  \n For both the  YouTube Covers and Mazurkas  datasets , \nwe compared our algorithm using results previously pre-\nsented in the literature. For the former  case, in addition to \ncompar ing to the results presented in the paper for which \nthe dataset was created  [18], we carefully implemented \nthe algorithm for local alignments based on  the chroma \nbinary distance [15]. Table 1 shows the results.  \n \nTable 1. Mean average precision (MAP), precision at 10 \n(P@10), and mean rank of first correctly identified cover \n(MR1) on the YouTube Covers dataset.  Given that this dataset \nhas only two recordings per song i n the training set, the maxi-\nmum value to P@10 is 0.2.  \n Our method achieved the best results in this experi-\nment. In addition, we note that our method is notably \nfaster than the second best (Serr à et al. ). Specifically, \nwhile our method took 1.3 hours , the other  method  took \napproximately one week to run on the same computer1. \n                                                           \n1 In our experiments, we used a n 8-core Intel® Core ™ i7 -6700K CPU \n@ 4.00GHz with 32GB of RAM memory running Windows 1 0®. All \nour codes were implemented and executed using Matlab R2014a®.  Algorithm  MAP  P@10  MR1  \nDTW  0.425 0.114 11.69  \nSilva et al. [18] 0.478  0.126  8.49 \nSerrà et al. [15] 0.525 0.132 9.43 \nSiMPle  0.591 0.140 7.91 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 25  \n \nWe acknowledge  that we did not invest a lot of effort op-\ntimizing the competing  method. However, w e do not be-\nlieve that any code optimization is capable of significant-\nly reducing  the performance gap . \n We also consider  the Mazurkas dataset. In addition to \nthe results achieved by DTW, we report MAP results \ndocumented in the literature , which were  achieved by re-\ntrieving the recordings by structural similarit y strategies  \nusing  this data . Specifically, the subset of mazurkas  used \nin this work is  exactly the same as the used in [2] and \n[17] and ha s only minor  differences to the dataset used in  \n[6]. Although [15] is considered the state -of-the-art for \ncover song recognition, we d o not include its results due \nto the high time complexity.  Table 2 shows the results.  \nAlgorithm  MAP  P@10  MR1  \nDTW  0.882 0.949 4.05 \nBello [2] 0.767  - - \nSilva et al. [17] 0.795  - - \nGrosche et  al. [6]  0.819  - - \nSiMPle  0.880 0.952 2.33 \nTable 2. Mean average precision (MAP), precision at 10 \n(P@10), and mean rank of first correctly identified cover \n(MR1) on the Mazurk as dataset.  \n The structures of the pieces  on this dataset are re-\nspected in most of the recordings. In this case, DTW per-\nforms similar  than our algorithm. However, our method  is \nfaster (approximately two times in our experiments) and \nhas several advantages over  DTW , such as its  incre men-\ntal property , discussed in the next section . \n4.3 Stream ing Cover Song  Recognition  \nReal-time audio matching has attracted the attention of \nthe community in the last years. In this scenario, the input \nis a stream of audio and the output is a sorted list of simi-\nlar objects in a database.  \n In this section, we evaluate our algorithm in an online \ncover song recognition scenario. For concreteness , con-\nsider  that a TV station  is broadcasting  a live concert. In \norder to automatically present the name of the song to the \nviewers  or to synchronize th e concert  with a second \nscreen app, we would like  to take the streaming audio as \ninput to our algorithm and be able to recognize what song \nthe band is playing as soon as possible. To accomplish \nthis task, we need to match the input to a set of (previous-\nly processed) recordings.  \n In addition to allowing the fast calculat ion of  all the \ndistances of a  subsequence  to a whole song , the proposed \nalgorithm has an incremental  property  that can be ex-\nploited to estimate cover song  similarity  in a stream ing \nfashion. If we have a previously calculated SiMPle , then,  \nwhen we extract a new vector of (chroma) features , we \ndo not need to recalculate the whole SiMPle from the be-\nginning. Instead,  just two quick steps are required :  Calculation of the distance profile to the new subse-\nquence , i.e., the distance of the last observed subse-\nquence (including the new feature vector)  to all the \nsubsequences of the origi nal song ; \n Update of SiMPle by selecting  the minimum value \nbetween the new distance profile a nd the previous \nSiMPle for each subsequence.  \n These steps are done  by the Algorithm 2.  \nAlgorithm 2. Procedure to incrementally update SiMPle and SiMPle \nindex   \nInput:  The current time series A and B, the new chroma vector c, the \ndesired subsequence length m, and the current SiMPle PAB and SiMPle \nindex IAB \nOutput:  The updated SiMPle PAB,new  and the associated SiMPle index \nIAB,new   \n1 \n2 \n3 \n4 \n5 \n6 newB  ← Concatenate( B,c), nB ← Length( newB ) \nD ← MASS( newB [nB-m+1: nB], A) // c.f. [10] \nPAB, IAB ← ElementWiseMin( PAB, IAB, D, nB-m+1) \nPAB,last, IAB,last ← FindMin( D) \nPAB,new← [PAB, PAB,last], IAB,new  ← [IAB, IAB,last] \nreturn PAB,new, IAB,new \n To evaluate the ability of our method for streaming \nrecognition , we performed a simple experiment  simulat-\ning the previously described scenario . First, we extracted \nfeatures from each track in the dataset of original  record-\nings. For clarity, we will refer t o this database as the \ntraining set . Then, we randomly chose another recording \nas our query and processed it according  to the following \nsteps. We begin extracting features from the first three  \nseconds of the query in order to  calculat e the first dis-\ntance estimation to each training object. After this initial \nstep, for each second of the query , we repeat the process \nof extract ing features and re -estimating  the distance \nmeasure to the training set.  \n In this experiment, we used the Mazurk as dataset with \ntwo CENS per second. The training set is composed of \nthe first recording (in alphabetical order) of each piece. \nWe used a performance with approximately 275 seconds  \nas a query , and we were able to maintain the process fast-\ner than real -time. Specifically, the update s took approxi-\nmately 0. 7 seconds to extract the features, update  SiMPle, \nand recalculate the distance for all the training objects.   \n Figure 2 visualizes  the changes in distance estimation \nin an au dio streaming query session . In this case, we used \na recording of the “ Mazurka in F major, Op. 68, No. 3” \nas query. In the first estimati on, its training version ap-\npears as the sixth nearest neighbor. However as we see \nmore evidence,  it quickly  becomes the b est match.  \n0 4 8 12Op. 68, No. 3\nOp. 24, No. 4\nOp. 33, No. 3\nOp. 56, No. 3\nOp. 24, No. 1\nOp. 27, No. 1\n0 4 8 12Op. 24, No. 2\nOp. 33, No. 3\nOp. 68, No. 1\nOp. 24, No. 3\nOp. 41, No. 3\nOp. 68, No. 3\n0 4 8 12Op. 56, No. 3\nOp. 17, No. 4\nOp. 68, No. 3\nOp. 45, No. 4\nOp. 24, No. 2\nOp. 24, No. 2\nDistance to the training recording\n \nFigure 2. Changes  in the distance when querying a recording \nof the “ Mazurka in F major, Op. 68, No. 3 ” in a streaming \nfashion. The graphs represent the top 6 matches  after pro-\ncessing 3 ( left), 5 ( middle ), and 10 ( right ) seconds of the audio.  26 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016  \n \n Another strategy that can be used in this scenario is an \namnesic sliding window , in order to forget old values and \nfurther speedup the  matching of new subsequences . For a \ngiven window length w, we can maintain just the last w \nvalues in the SiMPle. In this way, a change in the distri-\nbution of the distance estimates may assist in the identifi-\ncation of the ending and beginning of a song. At the same \ntime, the positions of the most recently matched section s \ncan be used as estimation of the moment in the current  \nsong.  These ideas  may help to identify songs  being  se-\nquentially played in a random  or unknown order . \n5. EXPANDING THE RANGE OF APPLICATIONS \nOF SiMPle  \nIn this work , we focus on  assess ing music similarity by \njoining subsequences. While  we evaluate  our method on \nthe cover song recognition  task, we claim that the SiMPle \nis a powerful tool for other music processing  tasks . To \nreinforce this argument , we present insights on  how to \nuse SiMPle in different application domains , as well \nsome initial result s. The methods presented in this section \nhave room for improvements, but they are simple yet ef-\nfective.  We intend to further explore and evaluate SiMPle \nin (at least) the tasks listed below.  \n In contrast to  the previous experiments, when we use \nthe self -similarity join to highlight  points of interest in a \nrecording, we apply ten CENS per second.   \n5.1 Motifs and Discords  \nThe SiMPle  from a self -similarity join  has several  ex-\nploitable  properties. For example,  the lowest points cor-\nrespond to the locations of the most faithfully repeated \nsection (i.e. , the chorus  or refrain ). Between several defi-\nnitions of motifs  in the literature, such as  harmonic or me-\nlodic motifs , its simplest definition is the closest pair of \nsubsequences . As noted  in the  time series literatur e, given \nthe best motif pair, other definition s of motifs  can be \nsolved by minor additional calculations [9]. \n On th e other hand, the highest point on the SiMPle \ncorresponds to the “most unique” snippet from the re-\ncording. The procedure to search for such a subsequence \nthat is the furthest from any other , known as discord dis-\ncovery , can be used in music processing to find interest-\ning segments in recordings. For example , it can be used \nto identify a solo , improvisation  segments or the bridge . \n For example , consider  the song “ Let It Be ” by The \nBeatles.  Figure 3. shows the SiMPle obtained for this \ntrack and points to their discord and pair of best motifs.  \n0 30 60 90 120 150 180 2100204060\nTime (s)SiMPleBest motifs at 3m9s \nand 3m23sDiscord at 1m54s\n \nFigure 3. The pair of best motifs in a recording is determine d \nby the subsequences starting at the positions of the minimum \nvalues of its SiMPle. At the same time, the position of the \nhighest value points to the beginning of the discord excerpt.  \n While the motifs point to refrains, the discord in-\ncludes bridge and the beginning of the guitar solo.  5.2 Audio Thumbnailing  \nAudio thumbnail s are short representative excerpt s of au-\ndio recording s. Thumbnails have several applications in \nmusic information retrieval. For example , they can be \nused as  the snippet  show n the result  of a search to the us-\ner. In a commercial application, they can be used as the \npreview to a potential costumer in an online music store.   \n There is a consensus in the MIR community that the \n“ideal” music thumbnail is the most repeated excerpt , \nsuch as the chorus [1]. Using this assumption, the appli-\ncation  of SiMPle  to identify a thumbnail  is direct. Con-\nsider the SiMPle index  obtained by the self -join proce-\ndure. The thumbnail is given by the subsequence starting \nin the position that is most used as a nearest  neighbor. In \nother words, the beginning of the thumbnail is given by \nthe position related to the mode of SiMPle index.  \n To illustrate  this idea, we considered the song “New \nYork, New York ” by Frank Sinatra . Looking for  a 30 sec-\nonds thumbnail , we found an excerpt that is comprised o f \nthe last refrain, as well as the famous (brass) instrumental \nbasis of the song . Figure 4 shows the histogram of the \nSiMPle ind ex found in this experiment.  \nThe position of the mode corresponds to \napproximately 2 minutes and 49 seconds\n0123456\n0 30 60 90 120 150\nTime (s)\n \nFigure 4. Histogram of SiMPle index for the song “ New York, \nNew York ”. Each bar counts how many times the sub sequence \nstarting at that point was considered the nearest neighbor of any \nother. We consider the subsequence represented by the most \nprominent peak as the thumbnail for this recording.  \n5.3 Visualization  \nThe visualization of music structure aids the understand-\ning of the music content. Introduced in  [21], the arc dia-\ngram is a powerful tool to visualize repetitive segments in \nMIDI files  [22] and audio recordings [23]. This approach \nrepresents a song by plotting  arcs linking repeated  seg-\nments.  \n All t he information required to creat e such arcs are \ncompletely comprised on the SiMPle and the SiMPle in-\ndex obta ined by a self -join. Specifically,  SiMPle provides \nthe distances between subsequences, which can be used \nto determine if they are similar enough to exist a link be-\ntween them  and to define the color or transparency of \neach arc . The SiMPle index can be used to define  both \nthe positions and width of the arcs.  \n Figure 5 shows the scatter plot of  the SiMPle index  \nfor “Hotel California ” by Eagles . In this figure, there is a \npoint (x,y) only if y is the nearest neighbor of x. The clear \ndiagonals on this plot represent  region s of n points such \nthat the nearest neighbors of [x,x+1,…,x+n -1] are approx-\nimately [y,y+1,…,y+n -1]. If the distance between su ch \nexcerpts  is low, then these regions may have a link be-\ntween them.  For this  example , we define d the mean value \nof the SiMPle in that region as the distance  threshold  be-\ntween the segments , in order to resolve if they should Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 27  \n \nstablish a link . Such threshold  has direct impact on the \nnumber of arcs plotted.  \n0 60 120 180 240 300 360060120180240300360\n \nFigure 5. Scatter plot of the SiMPle index for the song “ Hotel \nCalifornia ”. The (light) gray area indicates a possible link, but \nonly the values in the (dark) green area represent subsequences \nwith distance lower than the threshold.  \n By using a simple algorithm to spot such diagonals, \nwe only need to define a threshold of dist ance and mini-\nmum length of the linkages. We set t he width of the links \nin our experiment to be  greater than or equal to 5 sec-\nonds. Figure 6 shows the r esulting arc plot for the exam-\nple show n in Figure 5. \n0 30 60 90 120 150 180 210 240 270 300 330 360 3900 30 60 90 120 150 180 210 240 270 300 330 360 390\nTime (s)\n \nFigure 6. Arc plot for the song “ Hotel California ”. These plots \nshow the difference between using the mean value of SiMPle \nas distance threshold ( above ) and no distance threshold at all \n(below ). The color of the arcs are relate d to their relevance, i.e., \nas darker the arc, closer the subsequences linked by it.  \n5.4 Endless Reproduction  \nConsider a music excerpt  s1, which starts  at the time  t1 of \na specific song, has a small distance to its nearest neigh-\nbor s2, which  starts  at time t2. When the reproduction of \nthis song arrives t1, we can make a random decision to \n“skip” the reproduction to t2. Given  that s1 and s2 are sim-\nilar, this jump may be imperceptible to the listener.  By \ncreating several points of skip, we are able to define  a se-\nquence of jumps that creates  an endless reproduction of \nthe song. A well-known deployed  example of this kind of \nplayer is the  Infinite Ju kebox  [7]. \n The distanc e values obtained by the self -join repre-\nsent how similar each subsequence is to its nearest neigh-\nbor in another region of the song. Adopting a small \nthreshold to the distance  between subsequences , we can \nuse SiMPle to define the jumps . These characteristic s \nmay be explored in order to  create a player for endless \nreproduction. We refer the interested reader to the sup-\nporting website [19] for examples  of this functionality . \n5.5 Sampling Identification  \nIn addition to provid ing a global  distance estimation be-\ntween different songs, SiMPle is also powerful to exam-\nine local  similaritie s. An interesting application that may \nexploit this ability  is the automatic identification of sam-\nples. Sampling is the act of “borrow ing” the instrumental basis or main melody from another song. This is a com-\nmon approach in electronic and hip -hop music.  \n In contrast to  cover version s, sampling is used as a \nvirtual “ instrument ” to compose new songs.  However, \nalgorithms that look  only for local patterns to identify \nversions of the same track may classify a recording using \nsamples as a cover song. Using SiMPle, we can discover  \nthat the sampling excerpts have small dis tance values. In \ncontrast,  the segments related  to the new song have sig-\nnificantly higher values.  \n Figure 7 shows an example of the us age of SiMPle to \nspot sampling.  In this case, we compare the song “ Under \nPressure ” by Queen and David Bowie with “ Ice Ice Ba-\nby” by Vanilla Ice. Most of the continuous regions with \nvalues lower than the mean refe r to the sampling of the \nfamous bass line of the former song.  \n0 25 50 75 100 125 150 175 200 225 250Mean value\nSampling excerpts\nTime (s)SiMPle\n \nFigure 7. SiMPle  (in blue) obtained between the songs “ Ice Ice  \nBaby ” and “ Under Pressure ”. The continuous regions below the \nmean value (in red) represent the excerpts sampled by Vanilla \nIce from Queen’s song.  \n6. CONCLUSION S \nIn this paper, we introduced a technique to exploit  subse-\nquences joins to assess  similarity in mu sic. The presented \nmethod is very fast and requires only  one parame ter that \nis intuitively set in  music applications.  \n While  we focused our evaluation on the cover song \nrecognition, we have shown that our approach has the po-\ntential for applications in diff erent MIR tasks. We intend \nto further investigate the use of matrix profiles in the \ntasks discussed in Section 5 and the effects of different \nfeatur es in the process.  \n The main limitation of the proposed method is that \nthe use of only one nearest neighbor may be sensitive to \nhubs, i.e., subsequences that are considered the nearest \nneighbor of many other snippets . In addition, SiMPle \ncannot be directly used to identify regions  where several \nsubsequences are next to each other, composing a dense \nregion . For this reason , we intend to measure the impact \nof the reduction in the amount of information in different  \ntasks. Given that, we plan to explore  how to incorporate \nadditional information to SiMPle with no loss of time and \nspace efficiency.  \n We have encourage d the community to confirm our \nresults and explore or extend our ideas by making the \ncode freely available  [19]. \n \nAcknowledgements: The authors would like to thank \nFAPESP by the grants #2013/26151 -5 and #2015/07628 -\n0 and CNPq by the grants #303083/2013 -1 and \n#446330/2014 -0, and NSF by the  grant IIS 1510741.  28 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016  \n \n7. REFERENCES  \n[1] M. A. Bartsch  and G. H. Wakefield . “Audio \nthumbnailing of popular music using chroma -based \nrepresentations”. IEEE Transactions on Multimedia , \nVol. 7, No. 1 , pp 96 –104, 2005.  \n[2] J. P. Bello. “Measuring Structural Similarity in \nMusic”. IEEE Transactions on Audio, Speech, and \nLanguage Processing, Vol. 9, No. 7, pp. 2013 –2025, \n2011.  \n[3] AHRC Research - Centre for the History and \nAnalysis of Recorded Music . “Mazurka project”. \nurl: www.maz urka.org.uk/  (accessed 24 May, 2016)  \n[4] J. J. Carabias -Orti, F.  J. Rodriguez -Serrano, P. Vera -\nCandeas, N. Ruiz -Reyes, and F.  J. Canadas -\nQuesada. “An audio to score ali gnment framework \nusing spectral factorization and dynamic time \nwarping ”. International Socie ty for Music \nInformation Retrieval Conference , pp. 742 –748, \n2015.  \n[5] J. Foote. “Visualizing music and audio using self -\nsimilarity”. ACM International Conference on \nMultimedia , pp. 77 –80, 1999.  \n[6] P. Grosche, J. Serr à, M. M üller, and J. L. Arcos. \n“Structure -based  audio fingerprinting for music \nretrieval”. International Society for Music \nInformation Retrieval Conference , pp. 55 –60, 2012.  \n[7] P. Lamere. “The Infinite Jukebox”, url: \nwww.infinitejuke.com/  (accessed 24 May, 2016) . \n[8] B. McFee and  D. P. W. Ellis. “Analyzing song \nstructure with spectral clustering ”, International \nSociety for Music Information Retrieval Conference , \npp. 405 –410, 2014.  \n[9] A. Mueen, E. Keogh, Q. Zhu, S. Cash, and M. B. \nWestover. “Exact discovery of time series motifs”, \nSIAM International Conference on Data Mining , pp. \n473–484, 2009.  \n[10] A. Mueen, K. Viswanathan, C. K. Gupta and E . \nKeogh . “The fastest similarity search algorithm for \ntime series subsequences under Euclidean distance ”, \nurl:  \nwww.cs.unm.edu/~mueen/FastestSimilaritySearch.html  \n(accessed 24 May, 2016 ). \n[11] M. Müller . “Dynamic time warping”.  Information \nretrieval for music and motion , pp. 69-84, Springer, \n2007 . \n[12] M. Müller, F. Kurth, and M. Clausen. “Audio \nmatching via chroma -based statistical features”. \nInternational Society for Music Information \nRetrieval Conference , pp. 288 –295, 2005.  [13] S. Salvador and  P. Chan. “Toward accurate dynamic \ntime warping in linear time and space ”. Intelligent \nData Analysis , Vol. 11, No. 5, pp 561–580, 2007 . \n[14] J. Serrà, E. Gómez, and P. Herrera. “Transposing \nchroma representations to a common key”. CS \nConference on The Use of Symbols to Represent \nMusic and Multimedia Objects , pp. 45 –48, 200 8. \n[15] J. Serrà, E. Gómez, P. Herrera , and X. Serra. \n“Chroma binary similarity and local alignment \napplied to cover song identification ”. IEEE \nTransactions on Audio, Speech, and Language \nProcessing . Vol. 16, No. 6, pp. 1138 –1151, 2008.  \n[16] J. Serrà , X. Serr a, and R. G. Andrzejak. “Cross \nrecurrence quantification for cover song \nidentification”. New Journal of Physics , Vol. 11, No.  \n9, pp. 093017, 2009.  \n[17] D. F. Silva, H. Papadopoulos, G. E. A. P. A. Batista, \nand D. P. W. Ellis. “A video compression -based \napproac h to measure music structural similarity”. \nInternational Society for Music Information \nRetrieval Conference , pp. 95 –10, 2014.  \n[18] D. F. Silva, V. M. A. Souza, and G. E. A. P. A. \nBatista. “Music shapelets for fast cover song \nrecognition” . International Society for Music \nInformation Retrieval Conference , pp. 441 –447, \n2015.  \n[19] D. F. Silva, C. -C. M. Yeh, G. E. A. P. A. Batista, E. \nKeogh.  “Supporting website for this work”, url:   \nhttp://sites.google.com/site /ismir2016simple/  \n(accessed 24 May, 2016).  \n[20] W.-H. Tsai, H.-M. Yu, and H. -M. Wang . “Using the \nsimilarity of main melodies to identify cover \nversions of popular songs for music document \nretrieval ”. Journal of Information Science and \nEngineering , vol. 24, No. 6, pp. 1669 –1687 , 2008 . \n[21] M. Wattenberg . “Arc diagrams: Visualizing \nstructure in strings ”. IEEE Symposium on \nInformation Visualization , pp 110–116, 2002 . \n[22] M. Wattenberg . “The Shape of Song”, url: \nwww.turbulence.org/Works/song/  (accessed 24 May, \n2016).  \n[23] H. H. Wu, J. P. Bello. “ Audio -based music \nvisualization for music structure analysis ”. Sound \nand Music Computing Conference , pp. 1 –6, 2010.  Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 29"
    },
    {
        "title": "Using Priors to Improve Estimates of Music Structure.",
        "author": [
            "Jordan B. L. Smith",
            "Masataka Goto"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416916",
        "url": "https://doi.org/10.5281/zenodo.1416916",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/135_Paper.pdf",
        "abstract": "Existing collections of annotations of musical structure possess many strong regularities: for example, the lengths of segments are approximately log-normally distributed, as is the number of segments per annotation; and the lengths of two adjacent segments are highly likely to have an inte- ger ratio. Since many aspects of structural annotations are highly regular, but few of these regularities are taken into account by current algorithms, we propose several meth- ods of improving predictions of musical structure by using their likelihood according to prior distributions. We test the use of priors to improve a committee of basic segmentation algorithms, and to improve a committee of cutting-edge approaches submitted to MIREX. In both cases, we are unable to improve on the best committee member, mean- ing that our proposed approach is outperformed by sim- ple parameter tuning. The same negative result was found despite incorporating the priors in multiple ways. To ex- plain the result, we show that although there is a correla- tion overall between output accuracy and prior likelihood, the weakness of the correlation in the high-likelihood re- gion makes the proposed method infeasible. We suggest that to improve on the state of the art using prior likeli- hoods, these ought to be incorporated at a deeper level of the algorithm.",
        "zenodo_id": 1416916,
        "dblp_key": "conf/ismir/SmithG16",
        "content": "USING PRIORS TO IMPROVE ESTIMATES OF MUSIC STRUCTURE\nJordan B. L. Smith Masataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST), Japan\njordan.smith@aist.go.jp, m.goto@aist.go.jp\nABSTRACT\nExisting collections of annotations of musical structure\npossess many strong regularities: for example, the lengths\nof segments are approximately log-normally distributed, as\nis the number of segments per annotation; and the lengths\nof two adjacent segments are highly likely to have an inte-\nger ratio. Since many aspects of structural annotations are\nhighly regular, but few of these regularities are taken into\naccount by current algorithms, we propose several meth-\nods of improving predictions of musical structure by using\ntheir likelihood according to prior distributions. We test the\nuse of priors to improve a committee of basic segmentation\nalgorithms, and to improve a committee of cutting-edge\napproaches submitted to MIREX. In both cases, we are\nunable to improve on the best committee member, mean-\ning that our proposed approach is outperformed by sim-\nple parameter tuning. The same negative result was found\ndespite incorporating the priors in multiple ways. To ex-\nplain the result, we show that although there is a correla-\ntion overall between output accuracy and prior likelihood,\nthe weakness of the correlation in the high-likelihood re-\ngion makes the proposed method infeasible. We suggest\nthat to improve on the state of the art using prior likeli-\nhoods, these ought to be incorporated at a deeper level of\nthe algorithm.\n1. INTRODUCTION\nOne reason that the perception of structure in music is such\na complex and compelling phenomenon is that it is a com-\nbination of ‘bottom-up’ and ‘top-down’ processes. It is\nbottom-up in the sense that a listener ﬁrst performs group-\ning on short timescales before understanding the grouping\nat large timescales, but it is top-down in the sense that one\nhas global expectations that can affect the way one per-\nceives the music. For example, when hearing a new pop\nsong for the ﬁrst time, we expect there to be a chorus; even\non our ﬁrst hearing, we may identify the chorus partway\nthrough a song and already expect it to repeat later. After\nhearing a verse and a chorus, each 32 beats long, we may\nexpect the bridge to be the same length when it starts.\nc/circlecopyrtJordan B. L. Smith, Masataka Goto. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Jordan B. L. Smith, Masataka Goto. “Using Priors To\nImprove Estimates of Music Structure”, 17th International Society for\nMusic Information Retrieval Conference, 2016.\nFigure 1 . Proposed system overview.\nThis is important to recognize, since structure is am-\nbiguous: for any piece, there are often multiple ways of\ninterpreting it. As a result, we might never expect a purely\nbottom-up approach to be 100% correct; we need to also\nmodel the top-down inﬂuence, or what the listener ‘brings’\nto the analysis.\nFor instance, consider the following analysis of a piece\nof music, with the A sections each 10 seconds long, and B\n200 seconds long:\n[-A-|-A-|-A-|------------B------------]\nEven without knowing the piece of music, we can tell this\nis an unlikely analysis; it seems wrong to have the seg-\nments of the piece sized so asymmetrically. This example\nhints that the space of plausible analyses is limited (even\nif it is huge), and that listeners’ intuitions about these lim-\nits inform the annotation process. Is there a way to embed\nsuch intuitions into music structure analysis algorithms?\nCan we employ a kind of ‘top-down’ critic to assess the\nlikelihood of a given analysis?\nWe propose a system to accomplish this, illustrated in\nFigure 1. The inputs to the system are: (a) a song to ana-\nlyze, and (b) a set of probability density functions (PDFs)\nestimated from a corpus of annotations. The input song\nis analyzed by a set of algorithms (step 1); the prior likeli-\nhood of each output is computed (step 2); and the estimated\ndescription with the highest prior likelihood is chosen (step\n3). In contrast to the usual parameter tuning approach, in\nwhich a single parameter setting is ﬁxed after evaluating\nperformance over a corpus of songs, in our approach pa-\nrameters can be tuned for each song, on the basis of prior\nlikelihood.5541.1 Related work\nMost algorithms do at least some domain knowledge-based\ntuning, by putting a lower and/or upper bound on the length\nof segments, or by ﬁltering features to reduce variations at\ncertain timescales. These are important steps because, al-\nthough musical structure is hierarchical, algorithms rarely\nattempt to predict this hierarchy and are evaluated only at a\nsingle level. (This status quo has been challenged by [5].)\nHowever, a few algorithms have made greater use of\ndomain knowledge, and their success has been notewor-\nthy. Among the ﬁrst optimization-based approaches to\nstructure analyis was [7], who explicitly sought to deﬁne\n(in a top-down way) what constitutes a “good” analysis\n(i.e., one more likely to be in the space of plausible so-\nlutions). Later, [10] estimated the median segment length\nof a piece and used this as the preferred segment length in\nits search for an optimal segmentation; at the time, their\nalgorithm outperformed the leader at MIREX. The Auto-\nMashUpper system also uses a cost-based approach, re-\nwarding solutions with “good” segment durations of 2, 4,\nor 8 downbeats, and penalizing ones deemed less likely,\nlike 7 or 9 [2]. In the symbolic domain, [9] also used a\ncost-minimization approach, with costs increased for seg-\nments of unlikely duration or unlikely melodic contour; on\none dataset, the approach outperformed a pack of leading\nmelodic-segmentation algorithms.\nThe most direct way to use domain knowledge is to\nuse supervised learning. Two examples include [14, 15],\nwho each used machine learning to classify short excerpts\nas boundaries or not based on their resemblance to other\nshort excerpts known to be boundaries. The performance\nof [15] exceeded the best MIREX result by nearly 10%\nf-measure for both 0.5- and 3-second thresholds, an enor-\nmous achievement.\nOur intuition about what constrains the space of plausi-\nble analyses, as well as the success of previous algorithms\nin using domain knowledge and priors learned from cor-\npora, suggest that taking full advantage of this prior knowl-\nedge is essential to designing effective algorithms.\nIn the next section, we survey some of these regularities,\nand explore the extent to which prior algorithms adhere to\nthem. We detail our proposed algorithms and report our\nexperimental results in Section 3. Alas, despite the solid\nfoundations, no approach will be found to work. The sig-\nniﬁcance of this negative result, and possible explanations\nfor it, are discussed in Section 4.\n2. REGULARITIES\nIn this section we brieﬂy survey some regularities found in\nthe SALAMI corpus of annotations [12], and describe the\nrelationship between these regularities and algorithms that\nhave participated in MIREX campaigns from 2012–14.\nAlthough the time scale of the SALAMI annotations\nwas not explicitly constrained in the Annotator’s Guide1,\nthe length of annotated segments in the SALAMI corpus\n1Available at http://ddmal.music.mcgill.ca/\nresearch/salami/annotations .(a) log segment length\n(b) log number of segments\n(c) log ratio to median segment length\nFigure 2 . Estimated PDFs of three properties for anno-\ntations in SALAMI corpus; x-axis gives the value of the\nproperty, y-axis gives its relative probability. PDFs from\nlarge-scale segments shown in black, from small-scale seg-\nments in gray. In (c), the vertical axis is clipped to show\ndetail; the gray line extends upwards to just over 0.1.\nis roughly log-normally distributed, for both hierarchical\nlevels. Figure 2a shows the PDF of the log segment length\n(P(logLi)) for the large and small hierarchical levels in\nSALAMI; this and all other PDFs in this paper were found\nusing kernel density estimation (KDE). The number of seg-\nments within a piece ( N) is also log-normally distributed\n(Figure 2b). If we take the log ratio of each segment’s\nlength to the median length of segments within that piece\n(log(Li/Lmed)), we obtain a PDF strongly concentrated at\nlog (1) = 0 (see Figure 2c), with additional spikes near\n±0.693, orlog (2) andlog ( 1/2), for segments of twice\nor half the median length. There is even more detail if\nwe look at the log length ratio between adjacent segments\n(log(Li/Li+1)), a histogram of which is shown in Fig-\nure 3. Note that all the prominent peaks occur at ratios\nof small numbers. This makes sense if we consider that\nsegments are usually a whole number of measures long.\nThese properties are not speciﬁc to SALAMI annotations;\nsimilar distributions were reported by [1] for a completely\ndifferent corpus of annotations.\nHow closely do algorithms model these properties of\nthe annotations? We looked at three years of participants\nin the MIREX Structural Segmentation task, 2012–14, and\nestimated PDFs for the same properties. Some examples\nare shown in Figure 4. Figure 4a shows PDFs for segment\nlength estimated from each algorithm individually: some\nhew closely to the ground truth, but the majority underes-\ntimate the mean segment length. (Since precision is harder\nto achieve than recall, oversegmentation usually leads toProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 555Figure 3 . Histogram of log(Li/Li+1)estimated from\nSALAMI annotations ( y-axis truncated at 1/3 maximum).\nAs shown, nearly every spike represents an integer ratio of\nsegment lengths.\nbetter evaluation scores. [13])\nIf segment length seems like a weak prior, consider\ninstead Figure 4b, which compares PDFs for the log ra-\ntio of adjacent segment length. The characteristic side-\nlobes representing the high frequency of half- and double-\nlength segments are prominent in only two of the algo-\nrithms, RBH1 and RBH3 (2013). This is likely because\nthe algorithm [8] expects boundaries to occur on an 8-\nmeasure metrical grid, and snaps estimated boundaries to\nthis grid. Performance (evaluated with f-measure and\n3-second threshold) was mixed: RBH1 was close to the\nstate of the art in 2013, while RBH3 was below-average.2\nOn the other hand, the next-strongest side-lobes belong to\nSUG1, the second-best algorithm overall. SUG1 uses a\nconvolutional neural network to classify short excerpts as\ncontaining boundaries or not; the method ends by pick-\ning peaks from a boundary-likelihood curve, without post-\nprocessing [15]. Although SUG1 obviously learns from\nannotated data, it learns from low-level features (a mel\nspectrogram) rather than high-level attributes like segment\nlength ratios.\nDoes the ﬁtness of the algorithms to the SALAMI-\nderived priors actually have an impact on their perfor-\nmance? We found this to be true by looking at the correla-\ntion between algorithm performance and prior likelihood.\nWe took the output of the 18 unique segmentation algo-\nrithms that participated in MIREX from 2012–143, and\nfor each algorithm, computed the average log-likelihood\nof its estimated segments based on the KDE-derived PDFs\nfrom SALAMI. We also took the average performance of\nthe algorithms on the three boundary retrieval metrics ( f-\nmeasure, precision, and recall) with a threshold of 3 sec-\nonds. Figure 5 shows the correlation between the mean\nlog-likelihoods (of various segment properties) and the\nevaluation metrics. There is a weak to moderate correla-\n2Evaluation results in this paper differ from those reported at MIREX,\nsince we re-evaluated the algorithm output with a 5-second ‘warm-up’\napplied: boundaries within the ﬁrst and ﬁnal 5 seconds of pieces were\nignored. This leads to lower results overall but better differentiation be-\ntween algorithms.\n3Of the 24 participants in these years, 5 used the same segmentation\nalgorithm as another, and the data for one (FK2) were posted later than\nthe others, and were excluded.(a) PDFs of segment duration (in seconds)\n(b) PDFs of log ratio of length of adjacent segments\nFigure 4 . PDFs of properties estimated from SALAMI\nannotations (black dotted line) and from the output of\nMIREX algorithms (each algorithm in a different colour).\ntion between likelihood and f-measure for each of these\nproperties, usually attributable to a strong correlation be-\ntween likelihood and either precision or recall.\nWe have seen that most algorithms deviate substantially\nfrom the corpus; the algorithms’ descriptions simply don’t\n‘look’ like the ground truth. Also, there is some evidence\nthat an algorithm’s accuracy is related to the prior likeli-\nhood of its output. Hence, it seems reasonable to ask: can\nwe improve on these algorithms, or any set of algorithms,\nby maximizing their ﬁtness to the priors?\n3. USING PRIORS TO IMPROVE A COMMITTEE\nOur proposed system is simple: for a single audio ﬁle,\n(1) run several existing structural analysis algorithms, (2)\ncompute the log-likelihood of each prediction with respect\nto a corpus, and (3) choose the output that maximizes this.\n(See Figure 1.) For each of these steps, there are many\nways to proceed.\n3.1 Assembling a committee\nWe assembled two committees of algorithms: a commit-\ntee of multiple parameterizations of two basic approaches\n(Foote [3] and Serra et al. [11]), and a committee of ap-\nproaches drawn from MIREX. In the ﬁrst case, we test556 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Figure 5 . Scatter plots of mean log likelihood of algo-\nrithm output ( x-axis) and f-measure, precision and recall\n(y-axis), for 18 MIREX segmentation algorithms, 2012–\n14. Correlations (Pearson’s R) and lines of best ﬁt are\nshown.\nwhether a set of more strictly bottom-up segmentation ap-\nproaches can be improved with the top-down likelihood-\nmaximizing process; in the second case, we test whether a\nset of already-optimized algorithms can also be improved.\nFoote uses a checkerboard kernel to identify discontinu-\nities between homogeneous regions in a self-similarity ma-\ntrix (SSM), and his remains a classic approach to segmen-\ntation. Although surpassed in evaluations such as MIREX,\nthe simplicity and effectiveness of the algorithm means it\nis still commonly used as a model to improve upon (e.g.,\nsee [4]). In contrast to Foote, Serra et al. [11] aim to use\nboth repetition and novelty to locate boundaries. In prac-\ntice, both algorithms require several design choices: which\naudio features to use, what amount of smoothing to apply,\netc. We ran each algorithm with a small factorial range\nof settings, including three features (HPCP, MFCCs, and\ntempograms), for a total of 40 unique settings—hence, 40\ncommittee members. We ran the algorithms on 773 songs\nwithin the public SALAMI corpus (version 1.9). Fea-\nture extraction and algorithm computation were both easily\nhandled using MSAF [6].\nThe output of the algorithms that participated in\nMIREX is publically available, so we simply assembledit, along with the reported algorithm performance, for a\nMIREX committee of 23 members. We restricted our-\nselves to the SALAMI portion of the MIREX evalua-\ntion, which overlaps signiﬁcantly with the public half of\nSALAMI but is not identical.\n3.2 Computing likelihoods\nWe looked at the distribution of several attributes of the\nSALAMI corpus, listed below. Of these, A1−4are es-\ntimated on a per-segment basis and A5−9are global at-\ntributes of a description.\n•A1Segment length ( Li)\n•A2Fractional segment length ( Li/ song length)\n•A3Ratio of Lito median segment length\n•A4Ratio of adjacent segment lengths ( Li/Li+1)\n•A5Median segment length (median of Li)\n•A6Number of segments\n•A7Minimum segment length\n•A8Maximum segment length\n•A9Standard deviation of segment length\nFor attributes A1−4, we took the average across seg-\nments. Although log likelihoods are designed to be\nsummed, taking the sum of logP(Li)would punish de-\nscriptions with more boundaries, regardless of how proba-\nble the segments are. (In fact, we did test taking the sum\ninstead of the mean across segments, and the results were\ngenerally much poorer.)\n3.3 Electing a winner\nOnce we have computed all of the log likelihoods, how\nshould they be combined, and how should we use these\nvalues to elect an answer? Without any a priori reason to\nprefer one over another, we tested multiple approaches:\n•choose the description that maximizes the likelihood\nof attribute Ai;\n•choose the description that maximizes a summary\nstatistic over all attributes;\n•use a linear model to predict f-measure based on the\nlikelihoods;\n•use a linear model with interactions;\n•use a quadratic model.\nAs two summary statistics, we used the sum and the\nminimum of the log likelihoods of Ai. Using the sum op-\ntimizes the general ﬁtness; using the minimum penalizes\ndescriptions with any unlikely attributes.\n3.4 Experiment and results\nWith 5-fold cross-validation, we tested all versions of the\nalgorithm, using both the Foote/Serra and MIREX com-\nmittees. For each fold, the prior PDFs were estimated only\nusing annotations from the training set.\nAs a baseline, we used simple parameter tuning: i.e.,\nsimply pick the committee member with the greatest aver-\nage success on the training set. For reference, we also com-\nputed the mean f-measure of all committee members, andProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 557Attribute f(3 sec) f(0.5 sec)\nA1(mean) 0.4230 0.1051\nA2(mean) 0.4156 0.0958\nA3(mean) 0.4176 0.1140\nA4(mean) 0.4194 0.1072\nA5 0.3597 0.0863\nA6 0.3781 0.0991\nA7 0.0603 0.0124\nA8 0.3907 0.0961\nA9 0.3956 0.0950/summationtextAi 0.4260 0.1093\nMinAi 0.4206 0.1046\nLinear model 0.4399 0.0845\nInteractions model 0.4451 0.0688\nQuadratic model 0.4494 0.0739\nBaseline 0.4439 0.1151\nCommittee mean 0.2826 0.0691\nTheoretical max 0.6015 0.2572\nTable 1 . Average f-measure (at two thresholds) achieved\nby different decision criteria for Foote-and-Serra commit-\ntee.\nthe theoretical maximum—i.e., the average of the highest-\nscoring estimates for each song.\nThe results are shown in Tables 1 and 2. Among all\nthe variations of the proposed method, there were only two\ninstances that surpassed the baseline: the quadratic and in-\nteractions models for the Foote-Serra committee, with a\n3-second tolerance level. They surpassed the baseline f-\nmeasure by 0.0055 and 0.0012, respectively. Given the\nnumber of trials conducted, this small amount of success\ncould easily have come by chance.\n4. DISCUSSION\nNegative results are not normally conclusive: in this case,\nthe reader may suspect that with a small twist, our pro-\nposed method may yet succeed. For example, what if\nwe examined subsets by genre, or considered conditional\nprobabilities? In fact, this process of tweaking is how our\nexperiments came about. Our ﬁrst effort to solve the prob-\nlem used a small committee of solely Foote-based algo-\nrithms, and a set of four log likelihoods. When tests with\nthis initial system gave us a negative result, we tried vary-\ning each of the parts of the system—adding more mem-\nbers to the committee, including more PDFs, using in-\ncreasingly sophisticated regression approaches—until we\nhad assembled the large-scale experiment reported here.\nAnd we conducted several more informal tests—looking\nat subsets of the data, varying the method of characterizing\nthe PDFs (instead of with KDE, they can be modelled with\nplain histograms, or normal curves can be ﬁtted to some\ndistributions), looking at subcommittees (e.g., removing\ntop-performing and low-performing outlier members) and\ncomputing two-dimensional priors (to model, for example,\nthe fact that segment length is not independent of when a\nsegment begins)—all to no avail.Attribute f(3 sec) f(0.5 sec)\nA1(mean) 0.6273 0.2733\nA2(mean) 0.3487 0.0996\nA3(mean) 0.3487 0.0996\nA4(mean) 0.3487 0.0996\nA5 0.3916 0.1385\nA6 0.3768 0.1594\nA7 0.3487 0.0996\nA8 0.4662 0.1356\nA9 0.4233 0.1514/summationtextAi 0.6273 0.2733\nMinAi 0.6273 0.2733\nLinear model 0.5591 0.4005\nInteractions model 0.6273 0.4005\nQuadratic model 0.6273 0.4005\nBaseline 0.6273 0.4005\nCommittee mean 0.2826 0.0691\nTheoretical max 0.7345 0.5157\nTable 2 . Average f-measure (at two thresholds) achieved\nby different decision criteria for MIREX committee.\nThe consistency of the negative result—only two trials\nout of 56 exceeded the baseline, and only by the slimmest\nof margins—suggests a dead end. But in order to draw\nconclusions from this negative result, we must try to un-\nderstand whythe approach failed.\nEarlier, in Figure 5, we saw that algorithm performance\ncould, over many trials, correlate with the prior likelihood\nof their output. But what happens when we dig deeper\nand look at the relationship between each individual out-\nput’s correctness and its likelihood, as in Figure 6? On the\none hand, there is a clear positive trend overall, since there\nare no examples in the upper-left corner—that is, there are\nno predictions that have low likelihood but that are close\nto correct. And the examples with the highest f-measure\nare also among those with the highest likelihoods. Thanks\nto this relationship, the committees can, despite the noise,\ngenerally choose an output that is at least, or slightly bet-\nter than, the committee’s average; that’s why, in Tables 1\nand 2, nearly all of the algorithms exceeded the average\nresult of the committee.\nOn the other hand, trying to ﬁnd the high- f-measure\npredictions based on their prior likelihood is clearly futile\nwhen we consider only the rightmost region of the plot, a\nzoomed-in portion of which is shown in the lower part of\nFigure 6. Even these predictions, with the greatest ﬁt to the\npriors, range widely in accuracy: there are plenty above the\nbaseline (0.44), but also plenty below it, including a large\nnumber of predictions that contain zero correct boundaries.\nFigure 6 shows that having a high log-likelihood is a nec-\nessary but not sufﬁcient condition to be correct, and it is a\ncondition that most algorithms already achieve.\nThe uppermost points in Figure 6 represent a few lucky,\nperfect estimates of the true structure. Their distribution\nreveals another important point: that although the prior\nPDFs derive from the ground truth, the prior likelihood of558 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Figure 6 . Above: scatter plot of/summationtextlogP(Ai)(x-axis)\nvs.f-measure ( y-axis, 3-second threshold) of algorithm\noutputs for Foote/Serra committee on all data. (The heavy\nhorizontal lines are caused by the fact that f-measure is\noften the product of common fractions.) Below: inset of\nplot indicated by rectangle.\nmany annotations is moderate. The fat tails of the PDFs in\nFigure 2 represent a large set of descriptions that are un-\nlikely to ever be predicted by a prior likelihood-based ap-\nproach. For example, consider the analyses shown in Fig-\nure 7.4One algorithm achieved a perfect f-measure (with\n3-second threshold), and the likelihood of the description\n(measured with respect to attribute A4) was close to that\nof the annotated description. But a second estimate had a\nslightly higher prior likelihood thanks to its more consis-\ntent segment lengths, and a very poor f-measure.\nTo sum up the factors that appear to limit the effective-\nness of our approach:\n1. Although a high f-measure tends to come with\na higher prior likelihood, the reverse is not true:\nplenty of highly probable descriptions are very poor.\n2. The moderate correlation between algorithm suc-\ncess and prior likelihood is irrelevant, since we\nare interested only in the high-likelihood region of\nestimated descriptions.\n4Although the MIREX data are anonymized, many songs can be iden-\ntiﬁed by comparing the ground truth to known datasets. [13]\nFigure 7 . Two algorithmic estimates, compared to the\nground truth (middle). The estimates differ somewhat in\nthe likelihood of A4(adjacent segment length), but dras-\ntically in f-measure. The song is “Rock With You” by\nMichael Jackson, SALAMI ID 1616.\n3. Among high-likelihood descriptions, the correlation\nbetween success and likelihood is much weaker:\nmany likely descriptions are poor, and many anno-\ntations have low likelihood.\n5. CONCLUSION AND FUTURE WORK\nWe proposed and tested a novel committee-based approach\nto structural analysis. We motivated the approach by dis-\ncussing the strong regularities displayed by annotations of\nmusic structure. But after a long stretch of negative results,\nwe have concluded that the approach seems unviable: the\nrelationship between a description’s prior likelihood and\nits evaluated score seems to be too weak, especially in the\nhigh-likelihood region we are interested in.\nWe began the article by pointing out some mismatches\nbetween the properties of algorithmic estimates of struc-\nture and the ground truth, and we suggested that this\nmay be because algorithms do not model top-down fac-\ntors in perception. For a listener, top-down factors interact\nwith bottom-up factors; in contrast, our algorithm applies\nbottom-up considerations ﬁrst (by collecting the commit-\ntee of estimates), and then applies the top-down considera-\ntions post hoc . This may be the central weakness of our al-\ngorithm. Perhaps, if the top-down inﬂuence were modelled\nearlier on, an estimate like the top one in Figure 7 could be\nﬁne-tuned, the boundaries shifted slightly to give a more\nprobable output, rather than rejected early on because of\nits low likelihood. One algorithm that is ready to test this\nas future work is the optimization approach of [10]. Al-\nthough the authors model only a few basic priors, it could\nbe improved by including more.\n6. ACKNOWLEDGEMENTS\nA version of this work was shown to participants at the\nDagstuhl Workshop on Computational Music Structure\nAnalysis. We are very grateful to the attendees for their\nhelpful suggestions, and to the workshop organizers and\nthe Leibniz Center for Informatics for hosting us. This\nwork was supported in part by OngaCREST, CREST, JST.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 5597. REFERENCES\n[1] Fr ´ed´eric Bimbot, Gabriel Sargent, Emmanuel Deruty,\nCorentin Guichaoua, and Emmanuel Vincent. Semiotic\ndescription of music structure: An introduction to the\nQuaero/Metiss structural annotations. In Proceedings\nof the AES 53rd Conference on Semantic Audio , 2014.\n[2] Matthew E. P. Davies, Philippe Hamel, Kazuyoshi\nYoshii, and Masataka Goto. AutoMashUpper: Auto-\nmatic creation of multi-song music mashups. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 22(12):1726–1737, 2014.\n[3] Jonathan Foote. Automatic audio segmentation us-\ning a measure of audio novelty. In Proceedings of\nthe IEEE International Conference on Multimedia &\nExpo , pages 452–455, 2000.\n[4] Florian Kaiser and Geoffroy Peeters. A simple fusion\nmethod of state and sequence segmentation for mu-\nsic structure discovery. In Proceedings of ISMIR , pages\n257–262, Curitiba, Brazil, 2013.\n[5] Brian McFee, Oriol Nieto, and Juan Pablo Bello. Hier-\narchical evaluation of segment boundary detection. In\nProceedings of ISMIR , M´alaga, Spain, 2015.\n[6] Oriol Nieto and Juan Pablo Bello. Systematic explo-\nration of computational music structure research. In\nProceedings of ISMIR , New York, NY , USA, 2016.\n[7] Jouni Paulus and Anssi Klapuri. Music structure ana-\nlysis using a probabilistic ﬁtness measure and a greedy\nsearch algorithm. IEEE Transactions on Audio, Speech\n& Language Processing , 17(6):1159–1170, 2009.\n[8] Bruno Rocha, Niels Bogaards, and Aline Honingh.\nSegmentation and timbre similarity in electronic dance\nmusic. In Proceedings of the Sound and Music Com-\nputing Conference , pages 754–761, Stockholm, Swe-\nden, 2013.\n[9] Marcelo Rodr ´ıguez-L ´opez, Anja V olk, and Dimi-\ntrios Bountouridis. Multi-strategy segmentation of\nmelodies. In Proceedings of ISMIR , pages 207–212,\nTaipei, Taiwan, November 2014.\n[10] Gabriel Sargent, Fr ´ed´eric Bimbot, and Emmanuel Vin-\ncent. A regularity-constrained Viterbi algorithm and\nits application to the structural segmentation of songs.\nInProceedings of ISMIR , pages 483–488, Miami, FL,\nUSA, 2011.\n[11] Joan Serr `a, Meinard M ¨uller, Peter Grosche, and\nJosep Ll. Arcos. Unsupervised detection of music\nboundaries by time series structure features. In Pro-\nceedings of the AAAI International Conference on Arti-\nﬁcial Intelligence , pages 1613–1619, Toronto, Canada,\n2012.\n[12] Jordan B. L. Smith, J. Ashley Burgoyne, Ichiro Fuji-\nnaga, David De Roure, and J. Stephen Downie. De-\nsign and creation of a large-scale database of structuralannotations. In Proceedings of ISMIR , pages 555–560,\nMiami, FL, USA, 2011.\n[13] Jordan B. L. Smith and Elaine Chew. A meta-analysis\nof the MIREX Structure Segmentation task. In Pro-\nceedings of ISMIR , pages 251–256, Curitiba, Brazil,\n2013.\n[14] Douglas Turnbull, Gert Lanckriet, Elias Pampalk, and\nMasataka Goto. A supervised approach for detect-\ning boundaries in music using difference features and\nboosting. In Proceedings of ISMIR , pages 51–54, Vi-\nenna, Austria, 2007.\n[15] Karen Ullrich, Jan Schl ¨uter, and Thomas Grill. Bound-\nary detection in music structure analysis using convolu-\ntional neural networks. In Proceedings of ISMIR , pages\n417–422, Taipei, Taiwan, November 2014.560 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016"
    },
    {
        "title": "Landmark-Based Audio Fingerprinting for DJ Mix Monitoring.",
        "author": [
            "Reinhard Sonnleitner",
            "Andreas Arzt",
            "Gerhard Widmer"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1417008",
        "url": "https://doi.org/10.5281/zenodo.1417008",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/187_Paper.pdf",
        "abstract": "Recently, the media monitoring industry shows increased interest in applying automated audio identification systems for revenue distribution of DJ performances played in dis- cotheques. DJ mixes incorporate a wide variety of signal modifications, e.g. pitch shifting, tempo modifications, cross-fading and beat-matching. These signal modifica- tions are expected to be more severe than what is usually encountered in the monitoring of radio and TV broadcasts. The monitoring of DJ mixes presents a hard challenge for automated music identification systems, which need to be robust to various signal modifications while maintaining a high level of specificity to avoid false revenue assignment. In this work we assess the fitness of three landmark-based audio fingerprinting systems with different properties on real-world data – DJ mixes that were performed in dis- cotheques. To enable the research community to evaluate systems on DJ mixes, we also create and publish a freely available, creative-commons licensed dataset of DJ mixes along with their reference tracks and song-border annota- tions. Experiments on these datasets reveal that a recent quad-based method achieves considerably higher perfor- mance on this task than the other methods.",
        "zenodo_id": 1417008,
        "dblp_key": "conf/ismir/SonnleitnerAW16",
        "content": "LANDMARK-BASED AUDIO FINGERPRINTING FOR DJ MIX\nMONITORING\nReinhard Sonnleitner1, Andreas Arzt1, Gerhard Widmer1,2\nDepartment of Computational Perception, Johannes Kepler University, Linz, Austria1\nAustrian Research Institute for Artiﬁcial Intelligence (OFAI), Vienna, Austria2\nreinhard.sonnleitner@jku.at\nABSTRACT\nRecently, the media monitoring industry shows increased\ninterest in applying automated audio identiﬁcation systems\nfor revenue distribution of DJ performances played in dis-\ncotheques. DJ mixes incorporate a wide variety of signal\nmodiﬁcations, e.g. pitch shifting, tempo modiﬁcations,\ncross-fading and beat-matching. These signal modiﬁca-\ntions are expected to be more severe than what is usually\nencountered in the monitoring of radio and TV broadcasts.\nThe monitoring of DJ mixes presents a hard challenge for\nautomated music identiﬁcation systems, which need to be\nrobust to various signal modiﬁcations while maintaining a\nhigh level of speciﬁcity to avoid false revenue assignment.\nIn this work we assess the ﬁtness of three landmark-based\naudio ﬁngerprinting systems with different properties on\nreal-world data – DJ mixes that were performed in dis-\ncotheques. To enable the research community to evaluate\nsystems on DJ mixes, we also create and publish a freely\navailable, creative-commons licensed dataset of DJ mixes\nalong with their reference tracks and song-border annota-\ntions. Experiments on these datasets reveal that a recent\nquad-based method achieves considerably higher perfor-\nmance on this task than the other methods.\n1. INTRODUCTION\nAutomated audio identiﬁcation systems, also referred to as\naudio ﬁngerprinters, identify a piece of query audio from\na collection of known reference audio pieces. In general,\nsuch systems search for characteristic features in the query\naudio, which are then compared to features of known audio\npieces. The features are the so-called ﬁngerprints, which\nshould embody a favourable trade-off in storage demands,\ncomputation complexity, comparability, speciﬁcity, and ro-\nbustness. The importance of the individual properties of\nthe ﬁngerprints is dictated by the use case. The indus-\ntry uses audio identiﬁcations systems to monitor radio and\nTV broadcast channels to create detailed lists of the spe-\nciﬁc content that was played at any given time. In addi-\nc/circlecopyrtReinhard Sonnleitner1, Andreas Arzt1, Gerhard\nWidmer1,2. Licensed under a Creative Commons Attribution 4.0 Inter-\nnational License (CC BY 4.0). Attribution: Reinhard Sonnleitner1,\nAndreas Arzt1, Gerhard Widmer1,2. “Landmark-based Audio Finger-\nprinting for DJ Mix Monitoring”, 17th International Society for Music\nInformation Retrieval Conference, 2016.tion to radio and TV broadcast monitoring, performance\nrights organizations show interest in monitoring music per-\nformances, for example in discotheques. Without using au-\ntomated identiﬁcation systems, royalty collection depends\non the broadcasters who are expected to create detailed\nlists of played content.\nMusical content that is played in discotheques is usu-\nally performed by DJs, who can introduce severe signal\nmodiﬁcations by mixing sets of songs in a homogeneous\nfashion. This frequently involves temporally changing the\npitch or tempo of the audio to achieve a smooth transition\nfrom one track to the other, and often DJs will add effects\nin response to the mood or atmosphere in the club.\nSignal content that is modiﬁed by DJs arguably puts\nenormous robustness demands on automated systems. It\nseems hard to quantify the type and severity of signal ma-\nnipulations that can be introduced by DJs, as several effects\ncan be applied in combination. For the same reason we be-\nlieve it is hard to manually create meaningful test cases that\nreﬂect the possible modiﬁcations for system evaluation.\nIn this work, we investigate the ﬁtness and performance\nof systems that belong to the class of so-called landmark-\nbased audio ﬁngerprinting methods. Landmark-based sys-\ntems extract highly robust feature points, i.e. local energy\nmaxima, from the two dimensional time-frequency repre-\nsentation of the audio signal, and combine groups of these\nlandmarks to form the individual ﬁngerprints.\nWe show via experiments that it is hard to achieve ac-\ncurate results on DJ mixes. To do this, we test three im-\nplementations with different robustness properties, and re-\nport on their abilities to correctly identify known audio\npieces while correctly abstaining from reporting a match\nif the correct song is not contained in the given reference\ndatabase.\nWhile the algorithmic approaches that we use in this\nwork are extensively evaluated in the literature, we show\nthat the application to DJ mixes indeed unveils shortcom-\nings, speciﬁcally in the ability to prevent false detections.\nIn the context of media monitoring, falsely detecting a\nsong can lead to incorrect royalty management.\nWe contribute a new dataset which poses difﬁculties to\nautomated identiﬁcation systems, and investigate the dif-\nferent properties of three landmark based systems via ex-\nperiments on these datasets.\nThe paper is organized as follows. Section 2 discusses\nprior and related work, in Section 3 we introduce the185datasets that are the basis for the experiments and analysis\nand interpretation of results. Section 4 gives an overview\nof the methods we test in this work. Then, in Section 5 we\ndescribe the setup of experiments and their evaluation. An\nanalysis of the different properties of the tested methods is\ngiven in Section 6. Finally, in Section 7 we conclude our\nwork.\n2. RELATED WORK\nThe ﬁeld of audio ﬁngerprinting enjoys high research ac-\ntivity and numerous systems are described in the literature\nthat approach the task [6, 7, 10, 12, 14, 15, 18]. Excellent\nreviews of earlier systems are presented in [2, 3].\nThe system described in [13] achieves pitch scale\nchange robustness to small scaling factors by describing\ncontent based on short term band energies. In addition, the\nsystem is robust to small time scale modiﬁcations.\nThe basic algorithm of the Shazam system, a well\nknown representative method for landmark-based audio\nﬁngerprinting, is described in [18]. It pairs spectral peaks\n(i.e. the so-called landmarks) that are extracted from the\naudio to obtain compact hashes, which are used as index\ninto a hashtable to search for matches. The ﬁngerprints are\nhighly robust to environmental noise and signal degrada-\ntions that result from digital-analog conversions of audio.\nAnother system that achieves a certain amount of ro-\nbustness to changes in playback speed is described in [4].\nAs the change of the playback speed of audio inﬂuences\nthe pitch scale, a system is described that can mitigate this\neffect by ﬁrst searching for common pitch offsets of query\nand reference pieces, and then rescaling the query accord-\ningly. This system also is a member of landmark based\nidentiﬁcation methods.\nThe work described in [1, 19, 20] incorporates tech-\nniques from the domain of computer vision to audio iden-\ntiﬁcation. The authors of [1] apply a wavelet transform to\nsignals and create compact bit vector descriptors of content\nthat can be efﬁciently indexed via the Min-Hash algorithm.\nThe approach shown in [20] uses the image retrieval and\nsimilarity approach by applying the SIFT [9] method on\nlogarithmically scaled audio spectrograms, and later pro-\npose a matching method using LSH [5] in [19].\nThe concept of extracting features based on time-\nchroma patches from the time frequency representation of\nthe audio to describe robust features for audio identiﬁca-\ntion is discussed in [11].\nWe proposed to perform audio identiﬁcation using com-\npact scale invariant quad descriptors that are robust to time,\npitch or speed modiﬁcations in [16], and later reﬁned and\nextended that approach in [17].\nThe systems we use for the experiments in this work are\ndescribed in Section 4.\n3. DATA SETS\nWe perform experiments on two different datasets, called\ndisco set, and mixotic set. In the following we introduce\nthese datasets, and summarize their properties in Table 1.Disco tracks ref. +[s]−[s]\nset0 25 18 5661 2179\nset1 12 12 3760 0\nset2 12 11 3206 294\nset3 11 4 1054 2006\nset20 19 17 3123 457\nset35 20 7 324 996\nset36 28 13 872 768\nset37 21 10 720 720\ntotal: 8 148 92 18 720 7420\nMixotic tracks ref. +[s]−[s]\nset044 14 14 4640 0\nset123 12 12 3320 0\nset222 18 11 3543 2097\nset230 9 7 2560 780\nset275 17 11 3398 1622\nset278 12 11 3576 284\nset281 18 15 3300 280\nset282 14 8 2200 1740\nset285 15 15 4540 0\nset286 14 14 3140 0\ntotal: 10 143 118 34 217 6803\nTable 1 : Data set properties of the disco set (top) and\nthe mixotic set (bottom). The column “tracks” gives the\nnumber of played tracks in the DJ mix, “ref” denotes the\nnumber of these tracks that are present in the reference\ndatabase, and the columns “ +[s],−[s]” hold the number\nof seconds of referenced audio and not-referenced audio\nfor the individual DJ mixes.\nThe ﬁrst dataset, the disco set , contains eight mixes that\nwere performed in discotheques, and digitally recorded\nfrom the DJ mixing desk. The duration of the mixes is\napproximately 7hours and 16minutes. For this dataset we\nhave296reference tracks, only some of which are actually\nplayed in the mixes. The genres of the mixes include pop\nand rock, electronic music and German folk.\nBecause of copyright reasons, we cannot make the\ndisco set publicly available, therefore we compile a sec-\nond dataset, called mixotic set . We created this dataset\nfrom free, CC-licensed DJ mixes that were published on\nthe mixotic netlabel1, and collected their respective refer-\nence songs, which are available under the same license.\nThe mixotic set consists of 10mixes with a total dura-\ntion of 11hours and 23minutes. For this dataset we col-\nlected a set of 723reference tracks, 118of which are actu-\nally played in the mixes. According to the artists, this set\ncontains genres like Techno, Chicago House, Deep-Tech,\nDub-Techno, Tech-House, and the like. To be able to eval-\nuate the ﬁngerprinting results, we annotated the song bor-\nders of the tracks that are played in the individual mixes.\nDue to the long fading regions and sometimes very ho-\nmogeneous track transitions, these annotations cannot be\nexact. We tried to mark the positions in time where the\n1Mixotic is accessible via http://www.mixotic.net.186 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016previous track is fully faded out.\nWe think that the mixotic set may be useful to the re-\nsearch community, and could help to design well balanced\nidentiﬁcation systems and to uncover speciﬁc strengths and\npotential shortcomings of various methods, therefore we\npublish the mixotic set along with the annotations2.\n4. METHOD OVERVIEW\nWe use the datasets that we described in the previous\nsection to experiment with the following three methods:\nAudfprint ,Panako and the quad based audio ﬁngerprinter,\nhenceforth referred to as simply Qfp.\nAudfprint Audfprint is a MIT-licensed implementa-\ntion3of a landmark-based audio identiﬁcation algorithm\nbased on the method described in [18]. The published al-\ngorithm utilizes quantized hash ﬁngerprints that represent\npairs of spectral peaks. The hashes are described by the\ntime-frequency position of the ﬁrst peak and its distance\nin time and frequency to the second peak. The hashes that\nare computed from a snippet of query audio are used as\nthe keys into a suitable reference data structure, e.g. a hash\ntable, to retrieve reference hashes with the same key. For\neach query hash, a lookup is performed and the result sets\nare collected. Matched query and reference hashes which\nhappen to have a constant time offset in their individual\npeak-time identify the reference audio, along with its posi-\ntion in which the query snippet could be located.\nPanako Panako [15], available4under the GNU Affero\nGeneral Public License is a free audio identiﬁcation sys-\ntem. It transforms the time domain audio signal into a\ntwo dimensional time frequency representation using the\nConstant Q transform, from which it extracts event coordi-\nnates. Instead of peak pairs, the method uses triples, which\nallows for a hash representation that is robust to small time\nand pitch scale modiﬁcations of the query audio. Thus, the\nsystem can also report the scale change factors of the query\naudio with respect to the identiﬁed reference. The system\nis evaluated on queries against a database of 30 000 full\nlength songs, and on this data set achieves perfect speci-\nﬁcity while being able to detect queries that were changed\nin time or frequency scale of up to around 8%. In this work\nwe use Version 1.4of Panako.\nQfp The Qfp method [16, 17] is a landmark based\nmethod that is robust to time and pitch scale changes of\nquery audio. Its evaluation shows high average accuracy of\nmore than 95% and an average precision of 99% on queries\nthat are modiﬁed in pitch and/or time scale by up to ±30%.\nThe evaluation is performed on a reference data base con-\nsisting of 100 000 full length songs. The average query run\ntime is under two seconds for query snippets of 20seconds\n2Available on http://www.cp.jku.at/datasets/ﬁngerprinting/\n3Audfprint is available on https://github.com/dpwe/audfprint.\n4Panako is available on http://www.panako.be/.in length. The system also correctly uncovers any underly-\ning scale changes of query audio. While some robust au-\ndio identiﬁcation systems are using methods from the ﬁeld\nof computer vision (c.f. Section 2), Qfp is inspired by a\nmethod used in astronomy [8], which proposes to use n-\ntuples (with n >2) of two dimensional point coordinates\nto describe continuous feature descriptors that are invariant\nto rotation and isotropic scaling. The Qfp method adapts\nthe described ﬁndings to represent non-isotropic -scale in-\nvariant features that allow for robust and efﬁcient audio\nidentiﬁcation. The system uses range queries against a spa-\ntial data structure, and a subsequent veriﬁcation stage to\nreliably discard false matches. The veriﬁcation process ac-\ncepts matches within individual match sequences if spec-\ntral peaks in a region around the candidate match in the\nreference audio are also present in the query audio excerpt.\nEvaluation results of the Qfp method along with a param-\neter study and resulting run times are given in [17].\nThese methods are well performing identiﬁcation sys-\ntems. An evaluation of experiments using Audfprint and\nPanako is given in [15]. While all three methods are\nlandmark-based, the systems employ different inner mech-\nanisms and thus are expected to perform differently on the\ndatasets used in this work. Note that we use Audfprint and\nPanako as published, without tuning to the task at hand.\nWe do this because we believe that the methods are pub-\nlished with a set of standard parameters that turned out to\nbe well suited for general use cases according to experi-\nmentation performed by their authors. Likewise, we also\nuse the same set of parameters for Qfp, as they are de-\nscribed in [17]. We incorporated improvements for run-\ntime, but these do not have any impact on the identiﬁcation\nresults at all. For the task at hand, we want to investigate\nthe ﬁtness of the underlying algorithms of the methods,\nrather than discussing their speciﬁc implementations.\n5. EXPERIMENT SETUP\nExperiments are performed individually on the datasets we\ndescribed in Section 3. The general experimental setup is\nas follows. The mixes are split into non-overlapping query\nsnippets of 20seconds in length. To create query snippets\nfrom the DJ mix we use the tool SoX5along with switches\nto prevent clipping, and convert the snippets into .wav ﬁles.\nThe methods process each query snippet and store the\nresults. The implementations of the three tested systems\nbehave differently in answering a query: if the query ex-\ncerpt could be matched, Audfprint and Panako by default\nreport the whole query duration as matched sequence. Qfp\ngives a more detailed answer and reports the start time and\nend time of the matched portion within the query excerpt.\nLikewise, as Qfp, Audfprint allows to report the exact part\nof the query that it could actually match (using the option\n--find-time-range ), but for Panako we did not ﬁnd\nsuch an option. For best comparability of the evaluation\nresults, for all of the three methods we assign the reported\nmatch ﬁle ID to its whole query of 20seconds.\n5SoX is available on http://sox.sourceforge.net/.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 187Dataset Referenced not ref.\n+[s]−[s]M. TP FP FN acc. prec. TN FP spec.\nA 7838 7440 3442 0 .419 0 .513 3611 3809 0 .487\nDisco. 18720 7420 P 4624 5596 8500 0 .247 0 .452 5539 1881 0 .746\nQ13879 1253 3588 0.741 0 .917 6996 424 0.942\nQv14316 1523 2881 0 .765 0 .904 6587 833 0 .888\nQ/epsilon13423 152 15145 0 .183 0 .957 7413 7 0 .999\nA21783 10233 2201 0 .637 0 .680 1735 5068 0 .255\nMixotic 34 217 6803 P12326 16181 5710 0 .360 0 .432 2371 4432 0 .349\nQ29985 1262 2970 0.876 0 .959 6304 499 0.927\nQv30445 1680 2092 0 .889 0 .948 4395 2408 0 .647\nQ/epsilon119497 349 14371 0 .570 0 .982 6715 88 0 .987\nTable 2 : Evaluation results for the data sets. The column “ +” shows the number of seconds of the DJ mix, for which a\nreference is present. The column “ −” likewise gives the number of seconds for which no reference track is present in the\ndatabase. The methods (M.) Audfprint, Panako and Qfp are abbreviated as “ A”, “P” and “ Q”. The column “ Qv” shows\nQfp results without the veriﬁcation stage, and “ Q/epsilon1” shows the results for reduced search neighbourhood. “acc.” is the\naccuracy, “prec.” is the precision and “spec.” is the speciﬁcity. The experiment setup and the meaning of the measures is\ndeﬁned in Section 5. Because of space constraints we omit showing the individual statistics of each DJ mix that is contained\nin the dataset, and directly present the overall values. Detailed results are available in the published dataset.\nIt is important to note that we do not perform smoothing\nover time on the individual results but rather test the raw\nidentiﬁcation performance of each method based on each\nindividual query.\nWe compare the ﬁngerprinting results to the ground\ntruth on a one second basis, i.e. for each second of the DJ\nmix we check whether the corresponding query result is\ncorrect.\nHere we distinguish the following two cases: Case 1\n(C1)identiﬁable , and Case 2 (C2)not identiﬁable por-\ntions of the mixes. We investigate how the systems perform\nin cases where a song is identiﬁable, because it is present\nin the reference database (C1), and how well behaving a\nsystem is in not producing a match result in cases where\nthis is correct, i.e. because the track is in fact not present in\nthe reference (C2).\nFor all cases (C1), we count the number of seconds of\ntrue positives ( TP), false positives ( FP) and false nega-\ntives ( FN). True positives are cases in which the system\ncorrectly identiﬁed a track from a query. The false posi-\ntives denote situations in which the wrong track is claimed\nto be present, and the false negatives are cases in which\nthe system did not report a result at all. For this evaluation\nthere exist no true negatives, i.e. TP+FP+FN=N. For\nthis case (C1)we deﬁne the following two performance\nmeasures.\nAccuracy, as the proportion of correctly identiﬁed\nqueries:\nAccuracy =TP\nTP+FP+FN=TP\nN(1)\nPrecision, as the proportion of cases in which the sys-\ntem reports an identiﬁcation and this claim is correct, i.e.\na system that operates with high precision produces a low\nproportion of false positives:\nPrecision =TP\nTP+FP(2)To assess system performance for cases (C2), in which\nthe reference track is unknown, i.e. not present in the\ndatabase, we compute a third evaluation measure, the\nspeciﬁcity:\nSpeciﬁcity =TN\nTN+FP(3)\nHere, TN denotes the number of seconds in which no re-\nsult was produced, and at the same time the reference track\nis absent. The number of FPare the cases where the sys-\ntem reports a match despite the fact that there is no refer-\nence. Speciﬁcity expresses the capability of a system to\navoid false predictions by not reporting a result.\nThe identiﬁcation performance of all three methods is\nlisted in Table 2. We will discuss the results in the Section\nbelow, and analyze the properties and differences of the\nmethods.\n6. DISCUSSION OF RESULTS\nTable 2 summarizes the results of each method on the disco\nset and the mixotic set (rows QvandQ/epsilon1become relevant\nat a later point of this section). For the disco set, the accu-\nracy shows that just between 25% and74% of detectable\nseconds were assigned to the correct reference track. This\nreveals that DJ mix track identiﬁcation indeed is a tough\nproblem. The precision values show that Audfprint and\nPanako claim a wrong track in around 50% of the cases\nwhere the correct track should be identiﬁable. The speci-\nﬁcity of the systems shows that Audfprint correctly ab-\nstains from claiming a match in roughly 50% of the cases\nwhere no track can be found because it is not referenced\nin the database. Panako shows higher speciﬁcity at around\n75%. Qfp manages to correctly treat TN in94% of the\ncases.\nThe results obtained from the experiment on the mixotic\nset show better accuracy for all three methods, and Audf-\nprint and Qfp operate with higher precision than on the\ndisco set. For the mixotic set, all three systems show lower188 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016speciﬁcity than for the disco set. We believe that this is\na result of the larger reference database ( 723songs rather\nthan296in the disco set) and the highly repetitive tracks\nin the mixotic set. In total, Qfp performs at higher accu-\nracy, precision and speciﬁcity than Audfprint and Panako.\nPanako shows higher speciﬁcity than Audfprint on both\ndatasets.\nThe low speciﬁcity of the algorithm that is implemented\nin Audfprint indicates that its ﬁngerprints are too general.\nPanako uses triples of peaks, which inherently capture\nmore speciﬁc information of the local signal. Indeed, its\nspeciﬁcity on the disco set is considerably higher than that\nof Audfprint, i.e. its ﬁngerprint descriptors are less general,\nwhich may be the reason for it to correctly refuse to make\na claim in around 75% of the cases on the disco set, and in\nroughly 35% of the cases on the mixotic set.\nAnalysis Qfp performs best on the tested datasets. To\nﬁnd out which properties of the system are responsible for\nthat, we perform two additional experiments. The ﬁrst ex-\nperiment is intended to investigate the impact of the veri-\nﬁcation process, and the second experiment highlights the\neffect of the range query for Qfp. For a detailed explana-\ntion on the parameters that are mentioned in this section,\nwe ask the reader to consult [17].\nFirst, we want to ﬁnd out if it is the veriﬁcation process\nthat allows it to maintain high performance.\nIf we switch off the veriﬁcation6and run the experi-\nments, this results in an overall accuracy of 0.76, a pre-\ncision of 0.90, and a speciﬁcity of 0.89on the disco set.\nFor the mixotic set this results in the accuracy of 0.89, pre-\ncision of 0.95and a speciﬁcity of 0.65(c.f. Table 2, row\nQv). In terms of accuracy and precision, the results for\nboth datasets are comparable to those with active veriﬁca-\ntion. The speciﬁcity on the mixotic set, however, is notably\nlower.\nWe now investigate the performance of the Qfp method\nusing a reduced neighbourhood for the range queries.\nWe argue, that this loosely translates to using quantized\nhashes, i.e. if a query peak moves with respect to the oth-\ners, the corresponding reference hash cannot be retrieved.\nThis neighbourhood is speciﬁed as distance in the continu-\nous hash space of the quad descriptor. For this experiment\nwe reduce this distance from 0.0035,0.012for pitch and\ntime to 0.001,0.001for pitch and time. For the disco set,\nthis results in a low accuracy of 0.18, precision of 0.96\nand speciﬁcity of 0.99. On the mixotic set, the small range\nquery neighbourhoods result in an accuracy and precision\nof0.57and0.98, and speciﬁcity of 0.99(c.f. Table 2, Q/epsilon1).\nExtended Database We now add the reference tracks\nof both, the disco set and the mixotic set to a reference\ndatabase that consists of 430 000 full length tracks (this\ncaptures almost the entire Jamendo corpus7), and inspect\n6Strictly speaking, the implementation does not allow to switch off the\nveriﬁcation. Therefore we instead relax the veriﬁcation constraints such\nthat no candidate can be rejected.\n7Jamendo is accessible via https://www.jamendo.com.how the Qfp method responds to that amount of additional\ntracks. The overall result for the disco set (with standard\nsettings for the range query and veriﬁcation) is 0.69for\naccuracy and 0.80for precision. The speciﬁcity is 0.71.\nOn the mixotic set, the results are as follows: Accuracy\n0.83, precision 0.87and speciﬁcity 0.56. The low speci-\nﬁcity here is also impacted by a song duplicate in the DJ\nmixes and Jamendo corpus, i.e. in the case of mixotic set\n282, Qfp could correctly identify the track “Akusmatic -\nScamos” within the additional 430 000 songs, but the eval-\nuation treats this as FP, because according to the ground\ntruth this track is not present. The issue with song du-\nplicates does not inﬂuence any other experiments in this\nwork, since we use the extended reference database only\nwith the Qfp method.\nThe experiment shows that there is a certain negative\nimpact, causing more FPwhen trying to identify tracks in\nDJ mixes on larger databases. Note that these results also\ndepend on the experiment setup as deﬁned in Section 5,\nwhere we chose to assign the identiﬁed track ID to the\nwhole query of 20seconds in length. If we respect the\nreported start and end time of identiﬁed queries, the results\non the disco set give an accuracy of 0.60, precision of 0.88,\nand a speciﬁcity of 0.89. For the mixotic set the accuracy\nthen is 0.76, precision is 0.93and the speciﬁcity results in\n0.80.\nQfp turns out to maintain – what we think is – accept-\nable performance, on a database with 430 000 full length\nsongs. According to precision and speciﬁcity, the other\nmethods tested in this work seem to get distracted by 723\nreference songs. This leads us to suggest that the moni-\ntoring of DJ mixes via automated ﬁngerprinting systems\nindeed is a challenging task.\nVisual analysis The different behaviour of the systems\ncan be conveyed visually. In Figure 1 we show an excerpt\nof the mixotic set mix-ID 2228, from second 1500 to4300 .\nVertical lines represent song borders. The ﬁgure shows the\nscattered query identiﬁcation results, where the x-axis po-\nsition is the query time, and the y-axis position locates the\nquery within the reference song that the system could iden-\ntify. Thus, scattered positions of songs that are correctly\nidentiﬁed over several successive queries usually take the\nshape of a sawtooth function. In DJ mixes this will not al-\nways be the case, as the DJ can loop content. The different\ntrack names are encoded as markers, to be able to see if a\nsystem tends to confuse the same two tracks, or whether\nit reports many different tracks for a portion that it fails\nto identify correctly. The larger markers shown on top, be-\ntween song borders, are the reference. A missing reference\nmarker means that the song is not present in the database.\nNote that the evaluation does not consider whether the pre-\ndicted position within the reference is correct, as this is not\nmeaningful for highly repetitive musical content.\n8The mix-IDs are listed and explained in the published dataset.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 189Audfp.\nRef. pos[s]Panako\nRef. pos[s]\n1500 2000 2500 3000 3500 4000\nQuery position [s]Qfp\nRef. pos[s]Figure 1 : Query visualisation of an excerpt of mixotic set-ID 222. The rows show the results of individual, non-overlapping\n20squeries without smoothing of predictions for Audfprint (top), Panako (middle) and Qfp (bottom). The vertical lines are\nthe annotated song borders. The identiﬁcation claims of the systems are encoded in the shown markers, where each marker\nrepresents a reference track. The x-axis position shows the query excerpt position, and y-axis the location of the matched\nquery within the identiﬁed reference track. A missing large marker between song borders means that the reference song is\nnot present in the database. The ﬁgures show a bar at the bottom, which represents the confusions. TP(green) and TN\n(blue) are shown on top of the horizontal line, FP(red) and FN(yellow) are shown below.\n7. CONCLUSIONS\nThe results obtained from the experiments shown in this\nwork support the intuition that automated audio identiﬁca-\ntion on DJ mixes is a challenging problem. We observe that\nthe Qfp method performs best on the tested datasets, and\nbelieve that it constitutes a well suited method to further\ninvestigate the analysis of DJ mixes via audio ﬁngerprint-\ning.\nFor future work and experiments we strive to collect\nDJ mixes with accurate annotations and timestamps, that\nare exported from the speciﬁc software or the midi con-\ntroller used by the DJ. This would allow to gain insight on\nwhat kinds of effects and combinations thereof prevent au-\ntomated identiﬁcation systems from correctly identifying\ncertain portions of query audio.\n8. ACKNOWLEDGEMENTS\nThis work is supported by the Austrian Science Fund FWF\nunder projects TRP307 and Z159, and by the European\nResearch Council (ERC Grant Agreement 670035, project\nCON ESPRESSIONE).\n9. REFERENCES\n[1] Shumeet Baluja and Michele Covell. Audio ﬁnger-\nprinting: Combining computer vision & data stream\nprocessing. In Acoustics, Speech and Signal Process-\ning (ICASSP), 2007. IEEE International Conference\non, volume 2, pages II–213. IEEE, 2007.\n[2] Pedro Cano, Eloi Batlle, Ton Kalker, and Jaap Haitsma.\nA review of algorithms for audio ﬁngerprinting. InMultimedia Signal Processing, 2002 IEEE Workshop\non, pages 169–173. IEEE, 2002.\n[3] Pedro Cano, Eloi Batlle, Ton Kalker, and Jaap Haitsma.\nA review of audio ﬁngerprinting. Journal of VLSI sig-\nnal processing systems for signal, image and video\ntechnology , 41(3):271–284, 2005.\n[4] Elsa Dupraz and Ga ¨el Richard. Robust frequency-\nbased audio ﬁngerprinting. In Acoustics Speech and\nSignal Processing (ICASSP), 2010 IEEE International\nConference on , pages 281–284. IEEE, 2010.\n[5] Piotr Indyk and Rajeev Motwani. Approximate nearest\nneighbors: towards removing the curse of dimensional-\nity. In Proceedings of the thirtieth annual ACM sympo-\nsium on Theory of computing , pages 604–613. ACM,\n1998.\n[6] Frank Kurth, Thorsten Gehrmann, and Meinard\nM¨uller. The cyclic beat spectrum: Tempo-related audio\nfeatures for time-scale invariant audio identiﬁcation. In\nISMIR , pages 35–40, 2006.\n[7] Frank Kurth, Andreas Ribbrock, and Michael Clausen.\nIdentiﬁcation of highly distorted audio material for\nquerying large scale data bases. In Audio Engineering\nSociety Convention 112 . Audio Engineering Society,\n2002.\n[8] Dustin Lang, David W Hogg, Keir Mierle, Michael\nBlanton, and Sam Roweis. Astrometry.net: Blind as-\ntrometric calibration of arbitrary astronomical images.\nThe Astronomical Journal , 137:1782–2800, 2010.\narXiv:0910.2233.190 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016[9] David G Lowe. Object recognition from local scale-\ninvariant features. In Computer vision, 1999. The pro-\nceedings of the seventh IEEE international conference\non, volume 2, pages 1150–1157. Ieee, 1999.\n[10] Chun-Shim Lu. Audio ﬁngerprinting based on analyz-\ning time-frequency localization of signals. In Multime-\ndia Signal Processing, 2002 IEEE Workshop on , pages\n174–177. IEEE, 2002.\n[11] Mani Malekesmaeili and Rabab K Ward. A local ﬁn-\ngerprinting approach for audio copy detection. Signal\nProcessing , 98:308–321, 2014.\n[12] Meinard M ¨uller, Frank Kurth, and Michael Clausen.\nAudio matching via chroma-based statistical features.\nInISMIR , volume 2005, page 6th, 2005.\n[13] Mathieu Ramona and Geoffroy Peeters. Audioprint:\nAn efﬁcient audio ﬁngerprint system based on a novel\ncost-less synchronization scheme. In Acoustics, Speech\nand Signal Processing (ICASSP), 2013 IEEE Interna-\ntional Conference on , pages 818–822. IEEE, 2013.\n[14] Joren Six and Olmo Cornelis. A robust audio ﬁnger-\nprinter based on pitch class histograms applications for\nethnic music archives. In Proceedings of the Folk Mu-\nsic Analysis conference (FMA 2012) , 2012.\n[15] Joren Six and Marc Leman. Panako - a scalable acous-\ntic ﬁngerprinting system handling time-scale and pitch\nmodiﬁcation. In ISMIR , pages 259–264, 2014.\n[16] Reinhard Sonnleitner and Gerhard Widmer. Quad-\nbased audio ﬁngerprinting robust to time and frequency\nscaling. In Proceedings of the 17th International Con-\nference on Digital Audio Effects, DAFx-14, Erlangen,\nGermany, September 1-5, 2014 , pages 173–180, 2014.\n[17] Reinhard Sonnleitner and Gerhard Widmer. Robust\nquad-based audio ﬁngerprinting. IEEE/ACM Trans.\nAudio, Speech & Language Processing , 24(3):409–\n421, 2016.\n[18] Avery L Wang. An industrial-strength audio search al-\ngorithm. In ISMIR , pages 7–13, 2003.\n[19] Xiu Zhang, Bilei Zhu, Linwei Li, Wei Li, Xiao-\nqiang Li, Wei Wang, Peizhong Lu, and Wenqiang\nZhang. Sift-based local spectrogram image descrip-\ntor: a novel feature for robust music identiﬁcation.\nEURASIP Journal on Audio, Speech, and Music Pro-\ncessing , 2015(1):1–15, 2015.\n[20] Bilei Zhu, Wei Li, Zhurong Wang, and Xiangyang\nXue. A novel audio ﬁngerprinting method robust to\ntime scale modiﬁcation and pitch shifting. In Proceed-\nings of the 18th ACM international conference on Mul-\ntimedia , pages 987–990. ACM, 2010.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 191"
    },
    {
        "title": "Automatic Drum Transcription Using Bi-Directional Recurrent Neural Networks.",
        "author": [
            "Carl Southall",
            "Ryan Stables",
            "Jason Hockman"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1416244",
        "url": "https://doi.org/10.5281/zenodo.1416244",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/217_Paper.pdf",
        "abstract": "Automatic drum transcription (ADT) systems attempt to generate a symbolic music notation for percussive in- struments in audio recordings. Neural networks have al- ready been shown to perform well in fields related to ADT such as source separation and onset detection due to their utilisation of time-series data in classification. We pro- pose the use of neural networks for ADT in order to ex- ploit their ability to capture a complex configuration of fea- tures associated with individual or combined drum classes. In this paper we present a bi-directional recurrent neu- ral network for offline detection of percussive onsets from specified drum classes and a recurrent neural network suit- able for online operation. In both systems, a separate net- work is trained to identify onsets for each drum class under observation—that is, kick drum, snare drum, hi-hats, and combinations thereof. We perform four evaluations utilis- ing the IDMT-SMT-Drums and ENST minus one datasets, which cover solo percussion and polyphonic audio respec- tively. The results demonstrate the effectiveness of the pre- sented methods for solo percussion and a capacity for iden- tifying snare drums, which are historically the most diffi- cult drum class to detect.",
        "zenodo_id": 1416244,
        "dblp_key": "conf/ismir/SouthallSH16",
        "content": "AUTOMATIC DRUM TRANSCRIPTION USING BI-DIRECTIONAL\nRECURRENT NEURAL NETWORKS\nCarl Southall, Ryan Stables, Jason Hockman\nDigital Media Technology Laboratory (DMT Lab)\nBirmingham City University\nBirmingham, United Kingdom\n{carl.southall, ryan.stables, jason.hockman }@bcu.ac.uk\nABSTRACT\nAutomatic drum transcription (ADT) systems attempt\nto generate a symbolic music notation for percussive in-\nstruments in audio recordings. Neural networks have al-\nready been shown to perform well in ﬁelds related to ADT\nsuch as source separation and onset detection due to their\nutilisation of time-series data in classiﬁcation. We pro-\npose the use of neural networks for ADT in order to ex-\nploit their ability to capture a complex conﬁguration of fea-\ntures associated with individual or combined drum classes.\nIn this paper we present a bi-directional recurrent neu-\nral network for ofﬂine detection of percussive onsets from\nspeciﬁed drum classes and a recurrent neural network suit-\nable for online operation. In both systems, a separate net-\nwork is trained to identify onsets for each drum class under\nobservation—that is, kick drum, snare drum, hi-hats, and\ncombinations thereof. We perform four evaluations utilis-\ning the IDMT-SMT-Drums and ENST minus one datasets,\nwhich cover solo percussion and polyphonic audio respec-\ntively. The results demonstrate the effectiveness of the pre-\nsented methods for solo percussion and a capacity for iden-\ntifying snare drums, which are historically the most difﬁ-\ncult drum class to detect.\n1. INTRODUCTION\nWithin the ﬁeld of music information retrieval, automatic\nmusic transcription systems seek to produce a symbolic\nnotation for the instruments in an audio recording. There\nare a variety of areas in the educational, analytical and cre-\native industries that would beneﬁt from high quality mu-\nsic transcription. To date, the majority of such systems\nfocus on transcription of pitched instruments, with rela-\ntively few systems intended for the extraction of drum no-\ntation. Automatic drum transcription (ADT) is useful in\ndetermining the rhythm and groove inherent in recordings\nconsisting of either drum solos or polyphonic instrument\nmixtures. While high classiﬁcation accuracies have been\nc/circlecopyrtCarl Southall, Ryan Stables, Jason Hockman. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Carl Southall, Ryan Stables, Jason Hockman. “Au-\ntomatic Drum Transcription using Bi-directional Recurrent Neural Net-\nworks”, 17th International Society for Music Information Retrieval Con-\nference, 2016.demonstrated for isolated drum hits [9], the task of clas-\nsiﬁcation becomes more difﬁcult when multiple different\ndrum instrument hits occur at the same time [10], and is\nfurther complicated when other instrumentation is intro-\nduced creating a polyphonic mixture.\n1.1 Background\nUsing the categorisation presented in [7], the majority of\nprevious ADT systems can be understood as either seg-\nment and classify ,match and adapt , orseparate and detect .\nSegment and classify methods [2,6,17] ﬁrst divide record-\nings into regions using either onset detection or a metrical\ngrid derived from beat tracking; second, extract features\nfrom the segments; and third perform classiﬁcation to de-\ntermine the drum instruments in the segments. Match and\nadapt methods [21, 22] ﬁrst associate instruments to pre-\ndetermined templates then iteratively update the templates\nto reﬂect the spectral character of the recording. Sepa-\nrate and detect methods [5, 13, 14, 16] attempt to sepa-\nrate the music signal into the drum sources that make up\nthe mixture prior to identifying the onsets of each source.\nTo date, the most effective separate and detect method for\nADT has been non-negative matrix factorisation (NMF),\nan algorithm that divides a recording into a number of ba-\nsis functions and corresponding time variant gains. Sys-\ntems have been proposed for both ofﬂine and online ap-\nplications. Dittmar and G ¨artner [3] proposed three types\nof NMF—ﬁxed, adaptive and semi-adaptive—which can\nbe used in online situations taking each frame as its own\nNMF instance. For polyphonic audio, Wu and Lerch [20]\nused harmonic basis functions to separate the drums under\nobservation from the mixtures and improved on standard\nNMF by introducing new iterative update methods.\nIn addition to the above methods, ADT systems have\nbeen proposed that do not ﬁt in the above categorisation.\nPaulus incorporated hidden Markov models to identify the\nprobability of drum events based on previous information\n[15]. Thompson used support vector machines (SVM) with\na large dictionary of possible rhythmic conﬁgurations to\nclassify automatically detected bars [18].\n1.2 Motivation\nWith the exception of [15, 18] the majority of recent ADT\nsystems rely on single basis functions for each instrument.591Neural \nNetworks \nKick Snare Hi-hat Activation \nFunctions \nInput \nFeatures Drum \nOnsets Figure 1 : Overview of proposed method. Features are in-\nput to individual neural networks for each instrument, re-\nsulting in activation functions. Drum onsets are found by\npeak-picking the activation functions.\nThis has the potential to overﬁt to a speciﬁc playing tech-\nnique associated with an individual instrument and fails\nto recognise more subtle usage. The instrument with the\nmost varied playing techniques in the standard drum kit is\nthe snare drum (e.g., ﬂam, rolls, ghost notes), which not\nsurprisingly is the most difﬁcult to reliably detect. In ad-\ndition, spectral overlap between basis functions may pro-\nduce crosstalk between instruments such as snare drums\nand hi-hats, which can result in noisy time-variant gains,\nultimately making peak picking more difﬁcult.\nNeural networks are capable of associating complex\nconﬁgurations of features with both individual or com-\nbined classes. They have also demonstrated excellent per-\nformance in ﬁelds related to ADT, such as source separa-\ntion [8, 11, 12] and onset detection [1, 4]. Other supervised\nlearning techniques such as SVMs have been incorporated\nin ADT systems [14, 18, 19], however neural networks are\ncapable of capturing the association of class labels with\ntime-series data and producing clean activation functions\nfor the subsequent peak-picking stage. We therefore pro-\npose to extend the use of neural networks to ADT in or-\nder to exploit their well-known prowess for class separabil-\nity and their ability to capture the variety of playing tech-\nniques associated with each instrument class under obser-\nvation.\nThe remainder of this paper is structured as follows:\nSection 2 outlines our proposed methods for ADT. The\nevaluation and results are outlined in Section 3 and Sec-\ntion 4 presents a discussion of these results. Conclusions\nand possible future work are provided in Section 5.\n2. METHOD\nAn overview of our proposed method for ADT is presented\nin Figure 1. For each percussive instrument under ob-\nservation, features obtained from the audio recording are\ninput into the pre-trained neural networks iteratively by\nframe. We then select the peaks from the resulting acti-\nvation functions to determine the location of onsets for the\ncorresponding instruments.2.1 Neural Networks\nRecurrent neural networks ( RNN) incorporate information\nfrom previous time steps that allow for temporal infor-\nmation to be understood. Bi-directional neural networks\n(BDRNN ) include information from future time steps by\ncombining two RNNs: the ﬁrst is a standard backwards di-\nrectional RNN that incorporates present and previous time\ninformation; the second RNNis instead positioned to incor-\nporate information from present and future time positions,\nachieved by reversing the order of the input time steps. As\nBDRNN s are unsuitable for online applications, we propose\ntwo separate models: an RNNfor online usage and a BDRNN\nfor applications that can operate ofﬂine. An overview of\nboth neural networks is given in Figure 2.\n2.1.1 Recurrent Neural Network\nTheRNNarchitecture is represented in Figure 2 by the solid\nlines. For an RNNwithLlayers, the equation for each layer\nlis:\nal\n0(t) =fl(al−1\n0(t)Wl\n0+β(al\n0(t−1)Ul\n0) +bl\n0),(1)\nwhereβ= 0forl=L, and 1 otherwise. With layer lout-\nputa, the weight matrices WandUand the bias matrices\nb. The transfer function is determined by the layer, and is\ndeﬁned as:\nfl(x) =/braceleftbigg2/(1 +e−2x)−1, l/negationslash=L\ny=ex/(/summationtextex), l =L.(2)\n2.1.2 Bi-directional Recurrent Neural Network\nThe additional BDRNN connections are represented by\ndashed lines in Figure 2. For a BDRNN withLlayers, the\nequation for each hidden layer lis:\nal\nn(t) =fl(al−1\nn(t)Wl\nn+al\nn(t−1)Ul\nn+al−1\n(1−n)Zl−1\n(1−n)+bl\nn)(3)\nwhere the layer is deﬁned as forward directional when\nn= 0 and backwards directional when n= 1.Zis an\nadditional weight matrix. The output layer for time tcan\nthen be deﬁned as:\naL\n0(t) =fL(aL−1\n0(t)WL\n0+aL−1\n1(t)WL\n1+bL) (4)\n2.2 Input Features\nFollowing the approach in [20], input audio (mono .wav\nﬁles sampled at 44.1 kHz with 16-bit resolution) is trans-\nformed into a 1024 xnspectrogram representation using\nthe short-time Fourier transform (STFT), in which nis the\nnumbers of frames. The STFT is calculated using a Han-\nning window with a window length of 2048 samples and a\nhop size of 512 samples.592 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Input Features (t)a01(t-1) a02(t-1)\na02(t)\na12(t)\na12(t-1)a01(t)\na11(t)\na11(t-1)a03(t)\na13(t)a04(t)a03(t-1)\na13(t-1)Output (t)\nb11b13b12b04b03b02b01\nW02\nW01\nW04W03\nW12W11 W14\nW13\nU11U12U13U1U02U03\nZ11Z12Z01Z02Figure 2 : Overview of the proposed bi-directional recurrent neural network ( BDRNN ) and recurrent neural network ( RNN).\nSolid lines represent the RNNconnections and dashed lines are additional BDRNN connections. Tan sigmoid layers are shown\nas curved rectangles and soft max layers are represented by circles. The weight matrices are denoted as W,UandZand\nthe biases as b. The output of layer two of the backwards directional recurrent neural network is denoted by a2\n1.\n2.3 Architecture\nThe neural network architectures in each instrument are\nidentical, consisting of three dense hidden layers of 50\nneurons. This conﬁguration was chosen as it achieved the\nhighest results in preliminary testing. The neural networks\nare trained using target activation function representations\ncreated from training data annotations. The ﬁrst row of the\nactivation function frames in which onsets occur are set to\none and all other frames are set to zero. The networks are\ntrained with a learning rate of 0.05 using truncated back\npropagation through time, which updates the weights and\nbiases iteratively using the output errors. A maximum it-\neration limit is set to 1000, and the weights and biases are\ninitialised to random non-zero values between ±1ensuring\nthat training commences correctly. To prevent overﬁtting,\na validation set is created from 10% of the training data. If\nno improvement is demonstrated on the validation set over\n100 iterations, then training is stopped. The performance\nmeasure used is cross entropy combined with a softmax\noutput layer, as this proved to be the most effective conﬁg-\nuration.\n2.4 Onset Detection\nOnce a drum activation function θhas been generated for\neach drum class under observation, the onsets must be\ntemporally located from within θ. We adopt the method\nfrom [4] for onset detection in the BDRNN . To calculate on-\nset positions, a threshold is ﬁrst determined using the mean\nof all frames and a constant λ:\nT=mean (θ)∗λ. (5)\nIf the current frame nis determined to be both a peak and\nabove the threshold Tthen it is accepted as an onset Γ:Γ(n) =/braceleftbigg1, θ(n−1)<θ(n)≥θ(n+ 1) &T <θ (n)\n0, otherwise.\n(6)\nFor online applications using the RNN, where future infor-\nmation can not be used within the peak picking process,\nthe threshold is determined by taking the mean of the cur-\nrent frame and the previous ρframes with an onset being\naccepted if the current frame is greater than the threshold\nand the previous frame. We selected ρ= 9after initial in-\nformal testing. Due to the iterative classiﬁcation of each\nframe, onsets may be detected in adjacent frames. We\ntherefore disregard onsets detected within 50 ms of each\nother to ensure false positives are not obtained for a drum\nevent that has already been detected.\n3. EVALUATION\nWe conduct four evaluations intended to test the presented\nsystems in a variety of different contexts in which an ADT\nsystem could be used. The ﬁrst evaluation, termed auto-\nmatic , aims to demonstrate system performance on drum\nsolo recordings in a general purpose way where no prior in-\nformation about the test track is known. Following [3], the\nsecond evaluation allows information from the test tracks\nto be used to aid in transcription of drum solo recordings\nin asemi-automatic manner. This scenario could be used\nfor compositional or educational purposes either for iden-\ntifying an arrangement of a speciﬁed drum solo that has\nbeen resequenced in another recording, or in a studio situ-\nation in which a single drum kit is being used. The third\nand fourth evaluations aim to evaluate the systems in poly-\nphonic mixtures, where instruments other than the drums\nunder observation are found. Mixtures containing other\ndrums (e.g., ﬂoor toms, ride cymbal) are used in the third\nevaluation and additional harmonic accompaniment (e.g.,\nguitars, keyboards) is found in the fourth. These evalua-Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 593Precision Recall F-measure\nkick snare hi-hat mean kick snare hi-hat mean kick snare hi-hat mean\nBDRNN 0.912 0.834 0.795 0.847 0.929 0.901 0.729 0.853 0.909 0.852 0.738 0.833\nRNN 0.890 0.856 0.741 0.829 0.909 0.851 0.788 0.849 0.884 0.833 0.729 0.816\nPFNMF 0.934 0.633 0.939 0.835 0.931 0.889 0.743 0.854 0.926 0.699 0.811 0.812\nAM1 0.934 0.633 0.939 0.835 0.931 0.889 0.743 0.854 0.926 0.699 0.811 0.812\nAM2 0.937 0.651 0.893 0.827 0.934 0.886 0.786 0.868 0.929 0.713 0.805 0.816\nTable 1 : Precision, recall and F-measure results for the automatic evaluation using the IDMT-SMT-Drums dataset, includ-\ning the PFNMF ,AM1andAM2systems and the proposed BDRNN andRNNsystems. The highest accuracy achieved in each of\nthe categories is highlighted bold.\ntions are termed percussive mixtures andmulti-instrument\nmixtures respectively.\n3.1 Evaluation Methodology\nStandard precision, recall, and F-measure scores are used\nto measure system performance. Precision and recall are\ndetermined from detected drum instrument onsets, with\ncandidate onsets determined as correct if found within 50\nms of annotations. Only kick drum, snare drum and hi-hat\nonsets are taken into consideration. The mean F-measure\nis calculated by taking the average of the individual instru-\nment F-measures. We set the λparameter used in neural\nnetwork peak picking using grid-search across the dataset.\n3.1.1 Automatic Evaluation\nTo test the generalisation of the proposed system, we un-\ndertake the automatic evaluation, using the IDMT-SMT-\nDrums dataset [3]. This dataset consists of 95 tracks (14\nreal drum tracks, 11 techno drum tracks, and 70 wave drum\ntracks) with individual kick drum, snare drum and hi-hat\nrecordings. The average track length is 15 seconds, and\nin total there are 3471 onsets. Using three-fold cross val-\nidation the dataset is split into training and testing data,\nresulting in approximately 89,000 and 37,000 frames, re-\nspectively. Mean precision, recall and F-measure scores\nare taken across tested folds for each system under eval-\nuation. The proposed neural network systems are evalu-\nated alongside the three methods proposed in [20]: PFNMF ,\nwhich uses ﬁxed percussive basis functions in conjunction\nwith harmonic basis functions within a NMF framework;\nAM1, which iteratively updates the percussive basis func-\ntions of PFNMF ; and AM2, which updates the PFNMF basis\nfunctions and activation functions in an alternating fash-\nion. Each of the NMF systems are initialised by taking\nthe mean of each of the basis functions derived from the\nindividual tracks.\n3.1.2 Semi-automatic Evaluation\nIn order to test the systems ability to adapt to a speciﬁc sit-\nuation, we undertake the semi-automatic evaluation. We\nagain utilise the IDMT-SMT-Drums dataset, however in\nthis context we provide the systems exclusively with in-\ndividual drum hits that are used in the overall track un-\nder analysis. For a performance comparison in this evalua-\ntion, we also test the worth of training the neural networks\nusing mixed drum hits (e.g., kick drum and hi-hat playedtogether). The proposed methods are evaluated alongside\nthe semi-adaptive online NMF technique CDas presented\nin [3]. As the evaluation procedures herein are identical to\nthose in [3] the results from this work have been incorpo-\nrated for comparison.\n3.1.3 Percussion and Multi-instrument Evaluations\nTo test how well the proposed system can identify drums\nwithin various types of mixtures, we perform the percus-\nsion and multi-instrument evaluations, using the same pro-\ncedure as in the automatic evaluation. For these evalua-\ntions, we use the ENST minus one dataset as it contains\ndrum tracks with additional drum instruments (e.g., ﬂoor\ntom, ride cymbal) and techniques (e.g., ghost notes, ﬂams,\nrolls) as well as accompaniment tracks. The ENST mi-\nnus one dataset contains 64 recordings performed by three\ndrummers; two drummers performed 21 tracks each and\nthe third drummer performed 22 tracks. The BDRNN and\nRNN are provided recordings of two of the drummers as\ntraining, while testing on the third. The average track\nlength is 55 seconds with a total of 22,410 kick drum, snare\ndrum and hi-hat onsets, resulting in 210,000 and 105,000\nframes for training and testing respectively in each fold.\nWe mix the accompaniment and drum recordings in the\ndataset using the same ratios (1\n3and2\n3, respectively) as\nin [7, 15, 20]. The evaluation procedures in these two eval-\nuations are identical to those in [7, 15, 20], and as such the\nresults from these studies have been used for comparison\nherein.\nMethod Mean F-measure\nRNN(individual drums) 0.634\nBDRNN (individual drums) 0.700\nRNN(mixed drums) 0.955\nBDRNN (mixed drums) 0.961\nCD 0.950\nTable 2 : Mean F-measure results for the semi-automatic\nevaluation. BDRNN andRNN systems are trained on indi-\nvidual drum hits (individual drums) or mixtures of drum\nhits (mixed drums) and are compared with that of the CD\nmethod in [3].594 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016     BDRNN   RNN    PFNMF   AM1     AM2      Gillet     Paulus 0.5 0.6 0.7 0.8 0.9 1.0 F-measure Percussion Mixture \n0.5 0.6 0.7 0.8 0.9 1.0 F-measure Multi-instrument Mixture \nKick Snare Hi-hat Mean Instrument      BDRNN   RNN    PFNMF   AM1     AM2      Gillet     Paulus Figure 3 : Kick drum, snare drum, hi-hat and mean instrument F-measure results for the BDRNN andRNN. Results for\npercussion mixture (left) and multi-instrument mixture (right) evaluation scenarios are compared to those obtained in [7,15,\n20].\n3.2 Results\n3.2.1 Automatic Results\nThe proposed BDRNN achieved a higher mean instrument F-\nmeasure than existing ADT methods in the ﬁrst evaluation,\nwhich focuses on drum solos. Table 1 demonstrates that\nthe neural network approaches achieved the highest scores\nin six of the twelve categories, with the largest relative im-\nprovement being the snare F-measure and precision. As\nexpected, the RNN achieved lower F-measure scores than\ntheBDRNN for all instruments, however the results matched\nthose of the other evaluated systems. The three methods\nproposed in [20] achieved similar results as previously ob-\ntained on the IDMT-SMT-Drums dataset. Initial tests re-\nvealed that the best performance for all three algorithms\nwas achieved with the rank parameter set to 1 and an offset\ncoefﬁcient of 0.2.AM1showed no improvement on PFNMF\nin this instance, however AM2did slightly improve.\n3.2.2 Semi-Automatic Results\nTable 2 shows the results of the semi-automatic evaluation\nscenario with both systems compared to the results of the\nCDsystem obtained in [3]. Training the neural networks\nusing individual drum hits alone resulted in low accura-\ncies, however when training includes mixed drum instru-\nment signals (e.g., kick drums and hi-hats playing at the\nsame time) both the BDRNN andRNN achieve the highest\nresults of the tested systems.\n3.2.3 Percussion and Multi-Instrument Mixture Results\nFigure 3 shows the results of the BDRNN andRNN meth-\nods as compared to those achieved by the Gillet [7] and\nPaulus [15] systems, as well as the results of the PFNMF ,\nAM1, and AM2 systems in [20]. The results are shown for\nboth scenarios: percussive mixtures (left ﬁgure) and multi-\ninstrument mixtures (right ﬁgure). In both evaluations, the\nneural network approaches achieve high snare F-measures\nrelative to the other systems, and the BDRNN achieves the\nhighest snare F-measure for the multi-instrument mixtureevaluation. Figure 4 shows the mean precision and re-\ncall scores of the neural network systems in comparison\nto the other evaluated systems. The highest recall scores\nare achieved by the BDRNN andRNN for both percussion\nand multi-instrument mixtures. While the neural networks\nachieved lower mean F-measure scores, this high recall\ndemonstrates the potential worth of the clean activation\nfunctions.\n4. DISCUSSION\nThe results show that the proposed neural network systems\nachieve higher results for a solo drum dataset in ofﬂine and\nonline situations in both automatic and and semi-automatic\nevaluations. The ofﬂine bi-directional recurrent neural net-\nwork architecture outperformed the online recurrent neural\nnetwork architecture in all evaluations, demonstrating the\nworth of additional future information for applications that\nallow it. The high results for the snare drum class achieved\nthroughout the evaluation indicate the ability of the neural\nnetworks to associate multiple different frequency bases to\nthe same class making them well suited to detect a variety\nBDRNN     RNN      PFNMF       AM1         AM2       Gillet        Paulus                                0.5 0.6 0.7 0.8 0.9 1.0 Score Percussion Mixture Precision \nPercussion Mixture Recall \nMulti-Instrument Mixture Precision \nMulti-Instrument Mixture Recall \nFigure 4 : Mean Precision and Recall results from the per-\ncussion and multi-instrument mixture evaluations.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 5950.0          0.5         1.0          1.5          2.0          2.5          3.0          3.5         4.0          4.5 \nTime (s) 2x10 4\n2x10 3\n2x10 2\n2x10 1Log Spectrum (Hz) \n0.0        0.5           1.0         1.5          2 .0          2.5          3.0          3.5          4.0          4.5 \nTime (s) 0.5 1.0 Normalised Magnitude \n      0.0         0.5          1.0          1.5          2.0          2.5         3.0           3.5         4.0          4.5 \nTime (s) 0.5 Normalised Magnitude                \n1.0 Figure 5 : Comparison between neural network activa-\ntion function and PFNMF time-variant gain: ( top) Mixed\nspectrogram containing kick drum, snare drum and hi-hat;\n(middle )BDRNN snare drum activation function and found\nonsets; ( bottom )PFNMF snare drum time-variant gain and\nfound onsets.\nof playing techniques for a given instrument (e.g., ﬂams,\nrolls, ghost notes). An example of the instrument-speciﬁc\nfocus achievable by neural networks is shown in Figure 5,\nwhere spectral overlap exists between the snare drum and\nhi-hats. While the PFNMF output for this particular example\nshows the effect of crosstalk, the BDRNN is able to achieve\na less noisy activation function.\nAlthough high F-measures for the kick drum and hi-\nhat were achieved by both the RNN andBDRNN methods,\nthey had lower scores than other techniques for all situa-\ntions other than the semi-automatic drum transcription test.\nThe precision scores for the kick drum and hi-hat were the\nmain factor in the lower F-measure score. As shown in Ta-\nble 1 and Figure 4, the BDRNN and the RNN achieved the\nhighest mean recall scores for all tests using the ENST mi-\nnus one dataset, which indicates that the methods beneﬁt\nfrom a simpliﬁed peak picking process due to clean ac-\ntivation functions. However, F-measures scores for these\ntests indicate that the BDRNN andRNNwere not as success-\nful as other systems—a somewhat expected result as this\ndataset contains polyphonic mixtures. The addition of a\npre-processing stage similar to [20] could remove these\nsources prior to ADT and potentially improve results for\ntheBDRNN andRNN methods. Another area for possible\nimprovement would be to evaluate the worth of different\ninput features such as MFCCs, which have already been\ndemonstrated to be successful in conjunction with neural\nnetworks in the related task of onset detection [1].\n5. CONCLUSIONS AND FUTURE WORK\nWe have presented two neural network based approaches\nfor ADT: the BDRNN for off-line usage and the RNN on-\nline applications. Results from the conducted evaluationsdemonstrate that the proposed methods are capable of out-\nperforming existing ADT systems on drum solo recordings\nin both automatic and semi-automatic situations. The abil-\nity to learn a rich representation of drum classes enables\nthe neural networks to detect multiple playing techniques\nwithin the same class. Evaluations were also carried out on\npolyphonic mixtures in which the neural network achieved\nhigh snare F-measures relative to existing approaches. To\nimprove performance of the proposed methods for poly-\nphonic audio, an additional pre-processing source separa-\ntion stage could be introduced into the system to separate\nthe desired drums from additional instrumentation prior to\nADT. Furthermore, additional time step connections to ad-\nditional previous and future time steps may potentially in-\ncrease the accuracy of the system. One method for doing\nthis is by using long short-term memory cells within the\nneural network architecture which have already proven to\nbe effective for onset detection [4]. Further evaluation will\nbe carried out to determine performance when additional\ndrum classes are present, as well as testing other input fea-\ntures.\n6. ACKNOWLEDGEMENTS\nThe authors would like to thank Chih-Wei Wu and Chris-\ntian Dittmar for their help in providing code for implemen-\ntations of their methods, as well as the reviewers for their\nvaluable feedback in this paper.\n7. REFERENCES\n[1] S. B ¨ock, A. Arzt, F. Krebs, and M. Schedl. Online real-\ntime onset detection with recurrent neural networks. In\nProc. of the 15th International Conference on Digital\nAudio Effects (DAFx-12) , 2012.\n[2] C. Dittmar. Drum detection from polyphonic audio\nvia detailed analysis of the time frequency domain. In\nProc. of the Music Information Retrieval Evaluation\neXchange (MIREX) , 2005.\n[3] C. Dittmar and D. G ¨artner. Real-time transcription and\nseparation of drum recordings based on nmf decompo-\nsition. In Proc. of the 17th International Conference on\nDigital Audio Effects (DAFX) , 2014.\n[4] F. Eyben, S. B ¨ock, B. Schuller, and A. Graves. Univer-\nsal onset detection with bidirectional long short-term\nmemory neural networks. In Proc. of the 11th Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR) , pages 589–594, 2010.\n[5] D. Fitzgerald. Automatic drum transcription and\nsource separation . PhD thesis, Dublin Institute of\nTechnology, 2004.\n[6] O. Gillet and G. Richard. Automatic transcription of\ndrum loops. In Proc. of the 2004 IEEE International\nConference on Acoustics, Speech, and Signal Process-\ning (ICASSP) , pages 269–272, 2004.596 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016[7] O. Gillet and G. Richard. Transcription and separation\nof drum signals from polyphonic music. IEEE Trans-\nactions on Audio, Speech and Language Processing ,\n16(3):529–540, 2008.\n[8] E. M. Grais, M. U. Sen, and H. Erdogan. Deep neural\nnetworks for single channel source separation. In Proc.\nof the 2014 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , pages\n3734–3738, 2014.\n[9] P. Herrera, A. Yeterian, and F. Gouyon. Automatic\nclassiﬁcation of drum sounds: A comparison of fea-\nture selection methods and classiﬁcation techniques. In\nProc. of the International Conference on Music and Ar-\ntiﬁcial Intelligence , pages 69–80, 2002.\n[10] P. Herrera-Boyer, G. Peeters, and S. Dubnov. Au-\ntomatic classiﬁcation of musical instrument sounds.\nJournal of New Music Research , 32(1):3–21, 2003.\n[11] P.-S. Huang, M. Kim, M. Hasegawa-Johnson, and\nP. Smaragdis. Deep learning for monaural speech sep-\naration. In Proc. of the 2014 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) , pages 1562–1566, 2014.\n[12] J. Le Roux, J. R. Hershey, and F. Weninger. Deep NMF\nfor speech separation. In Proc. of the 2015 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , pages 66–70, 2015.\n[13] H. Lindsay-Smith, S. McDonald, and M. Sandler.\nDrumkit transcription via convolutive NMF. In Proc.\nof the 15th International Conference on Digital Audio\nEffects (DAFx) , 2012.\n[14] A. Moreau and A. Flexer. Drum transcription in poly-\nphonic music using non-negative matrix factorization.\npages 353–354, 2007.\n[15] J. Paulus and A. Klapuri. Drum sound detection\nin polyphonic music with hidden Markov models.\nEURASIP Journal on Audio, Speech, and Music Pro-\ncessing , 2009(1):1–9, 2009.\n[16] J. Paulus and T. Virtanen. Drum transcription with non-\nnegative spectrogram factorization. In Proc. of the 13th\nEuropean Signal Processing Conference (EUSIPCO) ,\npages 1–4, 2005.\n[17] K. Tanghe, S. Degroeve, and B. De Baets. An algo-\nrithm for detecting and labeling drum events in poly-\nphonic music. pages 11–15, 2005.\n[18] L. Thompson, M. Mauch, and S. Dixon. Drum tran-\nscription via classiﬁcation of bar-level rhythmic pat-\nterns. In Proc. of the International Conference on\nMusic Information Retrieval (ISMIR) , pages 187–192,\n2014.[19] D. van Steelant and K. Tanghe. Classiﬁcation of per-\ncussive sounds using support vector machines. In Proc.\nof the 2004 Machine Learning Conference of Belgium\nand The Netherlands , pages 146–152, 2004.\n[20] C.-W. Wu and A. Lerch. Drum transcription using par-\ntially ﬁxed non-negative matrix factorization with tem-\nplate adaptation. In Proc. of the 16th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , pages 257–263, 2015.\n[21] K. Yoshii, M. Goto, and H. G. Okuno. Automatic drum\nsound description for eeal-world music using template\nadaptation and matching methods. In Proc. of the 5th\nInternational Conference on Music Information Re-\ntrieval , pages 184–191, 2004.\n[22] K. Yoshii, M. Goto, and H. G. Okuno. Drum sound\nrecognition for polyphonic audio signals by adapta-\ntion and matching of spectrogram templates with har-\nmonic structure suppression. IEEE Transactions on\nAudio, Speech and Language Processing , 15(1):333–\n345, 2007.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 597"
    },
    {
        "title": "DTV-Based Melody Cutting for DTW-Based Melody Search and Indexing in QbH Systems.",
        "author": [
            "Bartlomiej Stasiak"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1415704",
        "url": "https://doi.org/10.5281/zenodo.1415704",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/089_Paper.pdf",
        "abstract": "Melody analysis is an important processing step in several areas of Music Information Retrieval (MIR). Processing the pitch values extracted from raw input audio signal may be computationally complex as it requires substantial effort to reduce the uncertainty resulting i.a. from tempo vari- ability and transpositions. A typical example is the melody matching problem in Query-by-Humming (QbH) systems, where Dynamic Time Warping (DTW) and note-based ap- proaches are typically applied. In this work we present a new, simple and efficient method of investigating the melody content which may be used for approximate, preliminary matching of melodies irrespective of their tempo and length. The proposed so- lution is based on Discrete Total Variation (DTV) of the melody pitch vector, which may be computed in linear time. We demonstrate its practical application for finding the appropriate melody cutting points in the R∗-tree-based DTW indexing framework. The experimental validation is based on a dataset of 4431 queries and over 4000 tem- plate melodies, constructed specially for testing Query-by- Humming systems.",
        "zenodo_id": 1415704,
        "dblp_key": "conf/ismir/Stasiak16",
        "content": "DTV-BASED MELODY CUTTING FOR DTW-BASED MELODY SEARCH\nAND INDEXING IN QBH SYSTEMS\nBartłomiej Stasiak\nInstitute of Information Technology, Lodz University of Technology,\nbartlomiej.stasiak@p.lodz.pl\nABSTRACT\nMelody analysis is an important processing step in several\nareas of Music Information Retrieval (MIR). Processing\nthe pitch values extracted from raw input audio signal may\nbe computationally complex as it requires substantial effort\nto reduce the uncertainty resulting i.a. from tempo vari-\nability and transpositions. A typical example is the melody\nmatching problem in Query-by-Humming (QbH) systems,\nwhere Dynamic Time Warping (DTW) and note-based ap-\nproaches are typically applied.\nIn this work we present a new, simple and efﬁcient\nmethod of investigating the melody content which may be\nused for approximate, preliminary matching of melodies\nirrespective of their tempo and length. The proposed so-\nlution is based on Discrete Total Variation (DTV) of the\nmelody pitch vector, which may be computed in linear\ntime. We demonstrate its practical application for ﬁnding\nthe appropriate melody cutting points in the R∗-tree-based\nDTW indexing framework. The experimental validation\nis based on a dataset of 4431 queries and over 4000 tem-\nplate melodies, constructed specially for testing Query-by-\nHumming systems.\n1. INTRODUCTION\nContent-based search and retrieval is becoming a popular\nand attractive way to locate relevant resources in the ever-\ngrowing multimedia collections and databases. For Mu-\nsic Information Retrieval (MIR) several important appli-\ncation areas have been deﬁned over the last decades, with\nAudio Fingerprinting , and Query by Humming (QbH) be-\ning perhaps the most prominent examples. The latter one\nis speciﬁc as it is exclusively based on the user-generated\nsound signal and it depends mostly on a single parameter\nof this signal – the pitch of the consecutive notes, forming\nthe melody sung by the user.\nIn a typical QbH system the query in the form of\nraw audio data is subjected to a pitch-tracking procedure,\nwhich yields a sequence of pitch values in consecutive time\nframes, often referred to as a pitch vector . The music re-\nc/circlecopyrtBartłomiej Stasiak. Licensed under a Creative Com-\nmons Attribution 4.0 International License (CC BY 4.0). Attribution:\nBartłomiej Stasiak. “DTV-based melody cutting for DTW-based melody\nsearch and indexing in QbH systems”, 17th International Society for Mu-\nsic Information Retrieval Conference, 2016.sources in the database are represented by templates , hav-\ning the similar form, so that the search is essentially based\non simply comparing the pitch vectors in order to ﬁnd the\ntemplate melody best matching the query melody.\nAn additional step of note segmentation may be used\nto obtain symbolic representation, explicitly deﬁning the\npitch and length of separate notes. In this case, several\nefﬁcient methods based on e.g. transportation distance or\nstring matching algorithms may be used. This approach\nenables fast searching, although it is difﬁcult to obtain high\nprecision due to unavoidable ambiguities of the note seg-\nmentation step. Comparing the pitch vectors directly usu-\nally yields higher-quality results but on the cost of the in-\ncreased computational complexity, as the local tempo vari-\nations require to use tools for automatic alignment between\nthe compared melodies.\nDynamic Time Warping (DTW) is a standard method\napplied for comparing data sequences, generated by pro-\ncesses that may exhibit substantial, yet semantically ir-\nrelevant local decelerations and accelerations. The exam-\nples include e.g. handwritten signature recognition or gait\nrecognition for biometric applications, sign language anal-\nysis, spoken word recognition and many other problems\ninvolving temporal patterns analysis. It is also a method of\nchoice for direct comparison of pitch vectors in the Query\nby Humming task.\n2. PREVIOUS WORK\nEarly works on the QbH systems date back to the 1990’s,\nwith some background concepts and techniques being in-\ntroduced much earlier [21, 24]. Initially, the symbolic,\nnote-based approach was used [6, 20, 30], often in the\nsimpliﬁed form comprising only melody direction change\n(U/D/S - Up/Down/the Same) [6, 21]. In the following\ndecade the direct pitch sequence matching with Hidden\nMarkov Models (HMM) [26] and Dynamic Time Warp-\ning [11, 19] was proposed and extensively used, in paral-\nlel to note-based approaches employing transportation dis-\ntances, such as Earth Mover’s Distance (EMD) [28,29]. In\nmany practical QbH systems, such as those presented in\nthe annual MIREX competition [1, 27], a multi-stage ap-\nproach is applied involving the application of the EMD\nto ﬁlter out most of the non-matching candidate tem-\nplates, leaving only the most promising ones for the ac-\ncurate, but more computationally expensive DTW-based\nsearch [31, 32, 34].619Another possibility of search time optimization is to\naccelerate the computation of DTW itself. For this pur-\npose several methods have been proposed, including iter-\native deepening [2], Windowed Time Warping [18], Fast-\nDTW [25] and SparseDTW [3].\nYet another approach, which is of special interest to\nus, is to apply efﬁcient DTW indexing techniques, based\non lower bounding functions [13]. These methods re-\nduce computational complexity by limiting the number of\ntimes the DTW algorithm must be run, but – unlike the\naforementioned EMD-based multi-stage systems – they\nare not domain speciﬁc. Introduced by Keogh [13, 16] as\na general-purpose method for time-series matching, they\nhave also been successfully applied for the Query by Hum-\nming task [17, 35].\n2.1 Indexing the dynamic time warping\nDTW indexing is based on a more general approach\nto indexing time series, known as GEMINI framework\n(GEneric Multimedia INdexIng) [5, 14]. In this approach\nthe sequences are indexed with R-trees, R*-trees or other\nSpatial Access Methods (SAM) [7] after being subjected\nto dimensionality reduction transformation. The typical\nSAMs require that the data are represented in a not more\nthan 12-16–dimensional index space I[14, 15]. Searching\nin the index space is guaranteed to yield a superset of all the\nrelevant templates (i.e. it will produce no false rejections),\nprovided that a proper distance measure ρIis deﬁned in I.\nLetNdenote the length of the original time series and let\nM/lessmuchNbe the number of dimensions of the index space.\nIt may be shown [5] that when the distance ρXbetween el-\nementsTN,QNof the input space Xis properly bounded\nfrom below by the distance between their low-dimensional\nrepresentations TM,QMin the index space, i.e. when:\nρI(TM,QM)≤ρX(TN,QN), (1)\nthen it is possible to construct an indexing mechanism\nwhich guarantees no false dismissals. The efﬁcient, SAM-\noptimized query in the index space may only return some\nfalse positives which are then eliminated by direct match-\ning of the time series in the original, input space X. De-\npending on the tightness of the lower bound (Eq. 1), the\nnumber of times the matching must be done in Xmay be\nreduced even by orders of magnitude. The detailed de-\nscription of the appropriate algorithms for k-nearest neigh-\nbor search and range queries may be found in [5, 14].\nThe generality of the GEMINI framework enables\nits application with many dimensionality reducing trans-\nforms, based on e.g. discrete Fourier transform (DFT) [5],\nHaar Wavelet Transform [12] or piecewise aggregate ap-\nproximation (PAA) [14, 33]. However, comparing se-\nquences under dynamic time warping differs quite signif-\nicantly from the case of Euclidean spaces, i.a. because\nDTW is not – strictly speaking – a metric (it does not sat-\nisfy the triangle inequality). It has been however shown\nthat it is possible to deﬁne a valid lower-bounding dis-\ntance measure [13] when proper global constraints, such\nas Sakoe and Chiba band [24] or Itakura parallelogram [9]are used. The dimensionality reduction may be obtained\nby the simple PAA algorithm, as demonstrated by Keogh\nin [13]. A more general approach, based on properties of\ncontainer-invariant time series envelopes, was introduced\nby Zhu and Shasha, who extended the lower-bounding cri-\nterion to the whole class of linear transforms [35].\nThe aforementioned techniques of DTW indexing may\nbe successfully applied to accelerate the melody matching\nin Query by Humming systems, as demonstrated in [35]\non an example of a small music database of 50 Beatles\nsongs. However, in real-life, large-scale systems some\npractical problems are likely to occur, especially when het-\nerogeneous audio material is used as input for querying the\ndatabase.\nOne of these problems is that the actual length and\ntempo of the query, with respect to the potentially match-\ning template, are not known in advance. As we will\ndemonstrate in the following section, this uncertainty\nmakes the use of the global constraints of the DTW dif-\nﬁcult, which in turn put in doubt the practical applicability\nof the DTW indexing schemes in the Query by Humming\ntask.\nAs a remedy, we propose a novel solution, based on\ncomputation of Discrete Total Variation (DTV) of the pitch\nvector, which enables to assess the optimal cutting point of\nthe query with respect to the template (Sect. 4). In this\nway, the DTW indexing becomes feasible even for diver-\nsiﬁed input data, containing the queries of varied length\nand tempo. Moreover, the analysis of the DTV may appear\nbeneﬁcial also for ﬁxed-length queries. This conclusion\nwill be supported by the experimental results presented in\nSection 5.\n3. PROBLEM SETTING\nThe implicit assumption underlying the efﬁcient DTW in-\ndexing methods introduced in [13] is that the beginning\nand the end of both compared sequences coincide. Un-\nfortunately, in the query-by-humming task, this condition\nis rarely met, especially with respect to the ending point.\nThe beginning is less problematic because most users typi-\ncally sing from the beginning of a phrase [8]. The length of\nthe query, on the other hand, is often unknown in advance,\nboth in terms of absolute time units and with relation to\nthe template. It is possible to control the absolute length of\nthe query in the acquisition module, e.g. by stopping the\nrecording session after xseconds. Even then, however, the\nassessment of the exact number of notes or phrases sung\nis impossible, mainly due to tempo differences between\nusers. The query may therefore end anywhere within the\ncourse of the template, as illustrated in Fig. 1.\nFortunately, the DTW may deal with this case quite eas-\nily, as the endpoint of the warping path may effectively\nbe searched for along the last row of the DTW matrix (or\nalong the last column, if we also expect queries longer than\ntemplates). The real problem is, however, that it is now\nimpossible to effectively use the global constraints, such\nas Sakoe and Chiba band (Fig. 1b), which are the sine qua\nnoncondition in the DTW indexing techniques [13, 35].620 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Figure 1 . Optimal warping path in the DTW matrix\n(with Sakoe and Chiba band shown) for ﬁxed-length query,\nwhere the query is much shorter than the template: a) The\n“fast singer” case – both the beginning and the ending\npoints coincide; b) The “normal singer” case – the query\nends in the middle of the template melody.\nRelaxing the constraints (e.g. increasing the radius of\nthe band) may help to incorporate more queries deviat-\ning from the diagonal, but on the cost of making the in-\ndexing less efﬁcient. Hence, although in theory the lower\nbounding techniques guarantee no false dismissals, we ar-\nrive again at the trade-off between time-efﬁciency and re-\ntrieval rate. In fact, setting the proper constraints is always\na matter of a compromise, but here the problem is much\nmore severe, as even moderate tempo mismatch may lead\nto signiﬁcant accumulation of deviations at the end of a\nquery.\nA real-life example – a template and the matching\nqueries from Jang’s dataset [10] – is presented in Fig. 2\nwhere the ending point of each query, with respect to the\ntemplate, has been determined on the basis of the optimal\nwarping path in the DTW matrix. Fig. 2 reveals, that al-\nthough within the ﬁxed time of 8s most users managed to\nsing between two and three two-bar motifs (out of all four),\nthere were also some “lazy singers” that did not manage to\ncomplete two motifs and some “fast singers” who com-\npleted the whole or almost the whole melody. With in-\ndexing, these queries may be easily lost, unless some ex-\ntremely loose constraints were applied.\nLet us note that the problem occurs even for optimal\ntemplate length (e.g. from Fig. 2 we may conclude that\nthis particular template is actually too long). In fact, in\nmany datasets the templates tend to be much longer than\nthe queries. For example, in Jang’s [10] collection the\ntemplate length varies from ca. 12 seconds up to over 5\nminutes. Hence, determining the reasonable cutting point\nfor the templates, prior to indexing, becomes a necessary\npreliminary step of processing.\nIt is important to note that this step cannot be done reli-\nably without some form of melodic content analysis. Nat-\nurally we might try – for ﬁxed query length of xseconds –\nto cut the templates to the same xseconds, assuming that\nthe tempo of the template roughly corresponds to the mean\ntempo of the queries. However, we have no guarantee that\nthis assumption is correct, which may obviously lead tosuboptimal indexing results.\nIn the following section we present an automatic\nmethod for determining the cutting point of the template\nmelody. The same method is also applied to each query to\ncut it at the point corresponding to the cutting point of the\ntemplate.\nAlthough it may seem not obvious, we should note that\nwe also touch the problem of query transposition here. A\nuser may sing the melody in any key, so it must be trans-\nposed to a reference key before matching, which is typi-\ncally done by mean subtraction. However, the mean pitch\nof a melody obviously depends on the location of its end-\ning point, which gives an additional motivation for trying\nto agree on a common cutting point among all the poten-\ntially matching melodies. The proposed method is based\non a simple content-based ﬁlter, which enables the prelim-\ninary match of the lengths of the compared melodies and –\nin consequence – the practical use of the efﬁcient indexing\nalgorithms.\n4. THE DISCRETE TOTAL V ARIATION\nAn intuitive solution to our problem might be formulated\nas follows: given a perfect note segmentation of the audio\nﬁles we could cut every melody after a ﬁxed number of\nnotes (the same for all melodies – templates and queries).\nThis would guarantee the endpoint match for efﬁcient in-\ndexing and the consecutive DTW would successfully deal\nwith potential rhythm and tempo discrepancies. Unfor-\ntunately, while it is straightforward for MIDI-based tem-\nplates, it is not so for the sung queries. The singer’s im-\nprecision on one hand and the speciﬁcity of a given pitch\ntracking algorithm on the other hand may lead to note seg-\nmentation errors that will make this approach unusable.\nIn our approach, instead of a crisp note segmentation we\nprefer to construct a soft measure of pitch value variability\nin time. In continuous case, for p(τ)representing the pitch\nvalue at time τ, we would deﬁne the following functional:\nTV(t) =t/integraldisplay\n0/vextendsingle/vextendsingle/vextendsingle/vextendsingled\ndτp(τ)/vextendsingle/vextendsingle/vextendsingle/vextendsingledτ . (2)\nWe may note that this deﬁnition, corresponding to the\nL1norm of the pitch signal derivative, may be seen as a\none-dimensional version of Total Variation (TV) as intro-\nduced by Rudin [22, 23] in the image analysis and noise\nremoval context. The one-dimensional Discrete Total Vari-\nationDTV may be deﬁned as:\nDTV (n) =n/summationdisplay\nk=1|p(k)−p(k−1)|, (3)\nwherep(k)denotes thek-th time frame of the pitch vector.\nThe fundamental property of DTV is that it accumu-\nlates pitch changes in the course of the melody, irrespec-\ntive of the actual direction of the changes (Fig. 3). We\nmay therefore set a threshold TDTV for the accumulated\npitch changes and cut all the compared melodies when theyProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 621Figure 2 . Example template from Jang’s [10] collection (top) and the ending points of all the 170 matching queries\n(bottom).\nFigure 3 . Pitch vector representation of the melody from\nFig. 2 (top, light-grey) and the corresponding DTV se-\nquence (bottom, dark-grey).\nreachTDTV , as follows:\npc= [p(0),p(1),...,p (nc)], (4)\nwherepcdenotes the pitch vector reduced to the ﬁrst nc+1\nvalues and where:\nnc= min{n∈N;DTV (n)≥TDTV}. (5)\nThe proposed DTV -based cutting scheme has some fur-\nther properties that are relevant to the considered problem:\n1. TheDTV sequence is monotonically nondecreasing\nand it stays constant only within segments of ﬁxed\npitch. The latter fact means that the note lengths are\nbasically ignored – only the number of notes and the\nspan of the consecutive musical intervals (pitch dif-\nference) inﬂuence the increase of the value of DTV .\n2. Ignoring the direction of the pitch changes means\nthat theDTV is not unique. For example, ascend-\ning chromatic scale will yield the same DTV se-\nquence as the descending one, assuming the same\ntempo and the same number of notes (diatonic scale\nwould produce different results due to different or-\ndering of whole steps and half steps).\n3.DTV -based cutting leads to obtaining the melody\nrepresentation robust to glissandi , occurring fre-\nquently in sung queries, where the pitch changes are\n“spread” over several consecutive time frames.\n4.DTV -based cutting leads to obtaining the melody\nrepresentation which is not robust to jitter and vi-\nbrato, which may be present within single notes, i.e.\nin segments of – otherwise constant – pitch.Property 1 implies a fundamental fact that two versions\nof a melody, consisting of identical pitch sequences but\nwith different rhythm and tempo will yield the same DTV\nsequence for corresponding notes. In particular, their rep-\nresentations obtained with Eq. 4 may have different num-\nber of frames, but they will basically represent the same\nmelodic content .\nProperty 2 means that the DTV may be interpreted as\na hash function which may occasionally return equal val-\nues for dissimilar input data. In fact, what we need is the\nopposite implication: the results for similar input must be\nalso similar and – fortunately – this condition is generally\nfulﬁlled.\nProperty 3 is connected to an important advantage of the\nproposed method to ignore the slope of the pitch changes.\nWhen singing a musical interval, the second note is often\nreached after several frames of intermediate pitch values,\nas opposed to MIDI-based signals where the changes are\ninstantaneous. This difference is well visible in Fig. 4 (top\nplots). It can be observed that despite the fuzzy note tran-\nsitions in frames 30–33 and 51–59 of the query (plot a)),\nthe obtained DTV sequences (Fig. 4, the bottom plots) are\nindeed similar. Therefore setting a given threshold value\nTDTV in Eq. 5 would allow to obtain the similar melody\nsections both for the query and for the template, irrespec-\ntive of the signiﬁcant tempo discrepancy between the two.\nFor example, for TDTV = 5 both sequences would be cut\nat the onset of the 4th note.\nProperty 4 indicates a potential weakness of the pro-\nposed solution as jitter and vibrato are ubiquitous in pitch\nvectors of sung melodies. However, a popular and simple\nmedian ﬁlter, which is often used to pre-process the pitch\nvectors prior to further melody analysis, may be effectively\napplied here to suppress the minor pitch ﬂuctuations. The\nexample in Fig. 4 had been already ﬁltered with median ﬁl-\nter of 9thorder which had appropriately smoothed almost\nthe whole signal, except of the small artefact in frames\n86–88. As a comparison, Fig. 5 a)presents the original\nquery. The dramatic distortion of the obtained DTV se-\nquence may be assessed even better on the right plot ( b)),\nwhere we see over twofold increase of the accumulated\npitch changes for the unﬁltered query with respect to the\nﬁltered one.\n5. EXPERIMENTAL V ALIDATION\nIn order to evaluate the usefulness of the proposed method\nin melody indexing, we performed tests on the well-622 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016Figure 4 . The ﬁrst motif of the melody from Fig. 2: a)query; b)template. Top plots present the original pitch vectors\n(after transposition by 3 octaves down, for visualization purposes); the bottom plots show the DTV sequences.\nFigure 5 .a)the same melody as in Fig. 4, but without the median ﬁlter; b)comparison of the obtained DTV sequences:\nquery without median ﬁlter (light grey, top); template (black, middle); query after median ﬁlter (dark grey, bottom).\nknown, publicly available dataset, consisting of a collec-\ntion of 48 popular songs (in the form of ground-truth MIDI\nﬁles) to be matched against 4431 queries sung by about\n200 users [10]. In order to increase the difﬁculty of the\nproblem, the set of templates was artiﬁcially expanded, so\nthat it contained – apart from the 48 ground-truth ﬁles –\n4225 additional noise midi ﬁles from Essen collection [4].\nPiecewise aggregate approximation was applied as a di-\nmensionality reduction technique. Each template melody\nwas represented as a point in 16-dimensional index space\nwhereR∗-tree was used as the spatial index. For each\nquery melody the minimal bounding rectangle (MBR) was\ncomputed and used for k-NN search, according to [15].\nTwo quantities were measured during the tests: the CPU\ntime of computation and the number of correctly recog-\nnized queries. The baseline results obtained by a non-\nindexing system, computing the DTW match between ev-\nery query and every template, were: 4211 out of 4431\nqueries recognized (95.03%) in 48h 55m 28s.\nFor the indexing tests several melody cutting strategies\nwere applied and the corresponding results have been pre-\nsented in Fig. 6. All the queries in the test database are\nof the same length of 8s. Our ﬁrst attempt was therefore\nto apply the straightforward approach based on cutting the\ntemplates to the same 8s (250 frames with a step of 32 ms)\nprior to indexing. This in fact yields the optimal template\nlength also in terms of the melody content because, as we\nhave found, there is no bias towards faster or slower queries\nin the test database, i.e. the mean tempo of each query is\nbasically the same as the tempo of the corresponding tem-plate.\nHowever, it appears that even in this optimal setting,\nour DTV-based cutting scheme may increase the recogni-\ntion rate with respect to the ﬁxed template cutting point\n(for the same number of nearest neighbors). For example,\nthe recognition rate obtained for the ﬁxed-length templates\nwith 1500 nearest neighbors could be obtained for the tem-\nplates cut on the basis of their DTV with 1100 nearest\nneighbors, which means ca. 25% gain in the computation\ntime (Fig. 6).\nThe key point in the effective application of the DTV is\nsetting the proper threshold TDTV . Too low value leads\n(Fig. 6,TDTV = 20 ) to extracting and indexing very\nshort melody fragments, which means that they contain\nfew notes and hence many templates may even become\nindistinguishable from each other. Moreover, for short\nqueries the lower bounding measure often happens to be\nzero which prevents establishing the right order of the re-\nsults.\nOn the other hand, too high TDTV threshold means that\nmany queries will not reach it before their “hard” cut point\n(8s in our case). This problem will not occur in the case\nof the templates, because they are usually much longer. As\na result, after the cutting procedure the templates will be\ngenerally longer than the queries and they will also contain\nmuch more melodic material, which will make the index-\ning ineffective.\nAs a partial remedy, we propose to use a simple con-\ndition, limiting the absolute length of the templates to theProceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 623Figure 6 . Top-ten results for various cutting point setting\nabsolute length of the queries:\nˆnc= min{nc,|q|} (6)\nwherencis given by Eq. 5 and |q|is the ﬁxed length of\nthe queries. In this way the template lengths are limited\nsimilarly as the queries. We should note that the only prob-\nlem which may occur here is when some monotonous, not-\nmuch-varied melodies are sung by a “fast singer”, because\nthese queries will contain more melodic material than –\nthen prematurely cut – template. This problem generally\ncannot be avoided without a priori knowledge on the cor-\nrect classiﬁcation of the query, but it appears not to have\nmuch impact on the recognition rate. The results in Fig. 6\nhave been obtained with template length limiting (Eq. 6)\nand as we may see they are stable in a quite broad range\nof the threshold values ( TDTV = 30 ,TDTV = 40 ). Going\nbeyond this optimal range ( TDTV = 50 ) deteriorates the\nrecognition but still the obtained results are only slightly\nworse that the ﬁxed cutting point approach.\n6. CONCLUSION\nIn this work we have introduced a new method of deter-\nmining the optimal cutting point for melody comparisons,\nbased on Discrete Total Variation of the melody pitch vec-\ntor. Our solution is fast to compute and it yields useful\ninformation about the melody, which enables to effectively\napply DTW indexing strategies, introduced in [13]. We\ndemonstrated the usefulness of the proposed solution for\nthe indexing task on a known database, designed for testing\nQuery-by-Humming systems. It should be noted, however,\nthat the method has potentially much broader application\narea. In particular, much more signiﬁcant gain may be ex-\npected in less constrained, on-line QbH systems, especially\nwhen more relaxed limits of the query length are used,and/or when greater singing tempo discrepancies may be\nexpected. The ability to ﬁnd the appropriate melody length\nin a fast way, without detailed note-based analysis and\nwithout computationally expensive DTW is an advantage\nwhich may simplify and accelerate many content-based\nmusic information retrieval tasks.\n7. REFERENCES\n[1] http://www.music-ir.org/mirex.\n[2] N. Adams, D. Marquez, and G. Wakeﬁeld. Iterative\nDeepening for Melody Alignment and Retrieval. In\n6th Int. Conf. on Music Information Retrieval, ISMIR\n2005 , pages 199–206, 2005.\n[3] G. Al-Naymat, S. Chawla, and J. Taheri. SparseDTW:\nA Novel Approach to Speed Up Dynamic Time Warp-\ning. In Proc. of the 8th Australasian Data Mining\nConf. , pages 117–127, 2009.\n[4] http://www.esac-data.org, 2009.\n[5] C. Faloutsos, M. Ranganathan, and Y . Manolopoulos.\nFast subsequence matching in time-series databases. In\nSIGMOD1994 , pages 419–429. ACM, 1994.\n[6] A. Ghias, J. Logan, D. Chamberlin, and B. C. Smith.\nQuery By Humming – Musical Information Retrieval\nin an Audio Database. In Proc. of the 3rd ACM Int.\nConf. on Multimedia , pages 231–236, 1995.\n[7] A. Guttman. R-Trees: A Dynamic Index Structure for\nSpatial Searching. In SIGMOD 1984 , pages 47–57.\nACM Press, 1984.\n[8] S. Huang, L. Wang, S. Hu, H. Jiang, and B. Xu. Query\nby humming via multiscale transportation distance in624 Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016random query occurrence context. In IEEE Int. Conf.\non Multimedia and Expo , pages 1225–1228, 2008.\n[9] F. Itakura. Minimum Prediction Residual Principle Ap-\nplied to Speech Recognition. IEEE Trans. on Acous-\ntics, Speech, and Signal Processing , 23(1):67–72,\n1975.\n[10] http://mirlab.org/dataSet/public/MIR-QBSH-\ncorpus.rar, 2009.\n[11] J.-S. R. Jang and H.-R. Lee. Hierarchical ﬁltering\nmethod for content-based music retrieval via acoustic\ninput. In Proc. of the 9th ACM Int. Conf. on Multime-\ndia, pages 401–410, 2001.\n[12] F. K.-P. Chan, A. W.-C. Fu, and C. Yu. Haar Wavelets\nfor Efﬁcient Similarity Search of Time-Series: With\nand Without Time Warping. IEEE Trans. on Knowl.\nand Data Eng. , 15(3):686–705, 2003.\n[13] E. Keogh. Exact indexing of dynamic time warping. In\nProc. of the 28th Int. Conf. on Very Large Data Bases ,\nVLDB ’02, pages 406–417, 2002.\n[14] E. Keogh, K. Chakrabarti, M. Pazzani, and S. Mehro-\ntra. Dimensionality reduction for fast similarity search\nin large time series databases. Knowledge and informa-\ntion Systems , 3(3):263–286, 2001.\n[15] E. Keogh and C. A. Ratanamahatana. Exact Index-\ning of Dynamic Time Warping. Knowl. Inf. Syst. ,\n7(3):358–386, 2005.\n[16] E. J. Keogh. Efﬁciently Finding Arbitrarily Scaled Pat-\nterns in Massive Time Series Databases. In PKDD , vol-\nume 2838 of Lecture Notes in Computer Science , pages\n253–265. Springer, 2003.\n[17] E. Lau, A. Ding, and J. Calvin. MusicDB: A Query\nby Humming System. Final Project Report, Mas-\nsachusetts Institute of Technology, USA, 2005.\n[18] R. Macrae and S. Dixon. Accurate Real-time Win-\ndowed Time Warping. In J. S. Downie and R. C.\nVeltkamp, editors, Proc. of the 11th Int. Society for\nMusic Information Retrieval Conf., ISMIR 2010 , pages\n423–428, 2010.\n[19] D. Mazzoni and R. B. Dannenberg. Melody Matching\nDirectly From Audio. In 2nd Annual Int. Symposium\non Music Information Retrieval, ISMIR 2001 , pages\n73–82, 2001.\n[20] R. J. McNab, L. A. Smith, I. H. Witten, C. L. Hender-\nson, and S. J. Cunningham. Towards the digital music\nlibrary: tune retrieval from acoustic input. In Proc. of\nthe 1st ACM Int. Conf. on Digital Libraries , DL ’96,\npages 11–18, 1996.\n[21] D. Parsons. The Directory of Tunes and Musical\nThemes . S. Brown, 1975.[22] L. A. Rudin. Images, Numerical Analysis of Singulari-\nties and Shock Filters . PhD thesis, 1987.\n[23] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear Total\nVariation Based Noise Removal Algorithms. Phys. D ,\n60(1-4):259–268, November 1992.\n[24] H. Sakoe and S. Chiba. Dynamic programming algo-\nrithm optimization for spoken word recognition. IEEE\nTrans. on Acoustics, Speech, and Signal Processing ,\n26(1):43–49, 1978.\n[25] S. Salvador and P. Chan. FastDTW: Toward accurate\ndynamic time warping in linear time and space. In 3rd\nWorkshop on Mining Temporal and Sequential Data ,\n2004.\n[26] J. Shifrin, B. Pardo, and W. Birmingham. HMM-Based\nMusical Query Retrieval. In Joint Conf. on Digital Li-\nbraries. 2002. Portland, Oregon , pages 295–330, 2002.\n[27] B. Stasiak. Follow That Tune - Adaptive Approach to\nDTW-based Query-by-Humming System. Archives of\nAcoustics , 39(4):467 – 476, 2014.\n[28] R. Typke, P. Giannopoulos, R. C. Veltkamp, F. Wier-\ning, and R. van Oostrum. Using transportation dis-\ntances for measuring melodic similarity. In ISMIR\n2003 , pages 107–114, 2003.\n[29] R. Typke, F. Wiering, and R. C. Veltkamp. Transporta-\ntion distances and human perception of melodic simi-\nlarity. Musicae Scientiae , pages 153–181, 2007.\n[30] A. L. Uitdenbogerd and J. Zobel. Manipulation of Mu-\nsic for Melody Matching. In Proc. of the 6th ACM Int.\nConf. on Multimedia , pages 235–240. ACM, 1998.\n[31] L. Wang, S. Huang, S. Hu, J. Laing, and B. Xu. An\neffective and efﬁcient method for query by humming\nsystem based on multi-similarity measurement fusion.\nInInt. Conf. on Audio, Language and Image Process-\ning, pages 471–475, 2008.\n[32] L. Wang, Sh. Huang, Sh. Hu, J. Liang, and B. Xu.\nImproving searching speed and accuracy of query by\nhumming system based on three methods: feature fu-\nsion, candidates set reduction and multiple similar-\nity measurement rescoring. In INTERSPEECH , pages\n2024–2027. ISCA, 2008.\n[33] B.-K. Yi and C. Faloutsos. Fast Time Sequence Index-\ning for Arbitrary Lp Norms. In Proc. of the 26th Int.\nConf. on Very Large Data Bases , VLDB ’00, pages\n385–394, San Francisco, CA, USA, 2000. Morgan\nKaufmann Publishers Inc.\n[34] B. Zhu and H. Liu. MIREX 2015 QBSH task: Tencent\nBestimage’s solution. pages 1–2, 2015.\n[35] Y . Zhu and D. Shasha. Warping indexes with envelope\ntransforms for query by humming. In Proc. of the 2003\nACM SIGMOD Int. Conf. on Management of Data ,\npages 181–192, 2003.Proceedings of the 17th ISMIR Conference, New York City, USA, August 7-11, 2016 625"
    },
    {
        "title": "Brain Beats: Tempo Extraction from EEG Data.",
        "author": [
            "Sebastian Stober",
            "Thomas Prätzlich",
            "Meinard Müller"
        ],
        "year": "2016",
        "doi": "10.5281/zenodo.1044300",
        "url": "https://doi.org/10.5281/zenodo.1044300",
        "ee": "https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/022_Paper.pdf",
        "abstract": "ABSTRACT\n\nThe idea that brain activity could be used as a communication channel has rapidly developed. Indeed, Electroencephalography (EEG) is the most common technique to measure the brain activity on the scalp and in real-time. In this study we examine the use of EEG signals in Brain Computer Interface (BCI). This approach consists of combining the Empirical Mode Decomposition (EMD) and band power (BP) for the extraction of EEG signals in order to classify motor imagery (MI). This new feature extraction approach is intended for non-stationary and non-linear characteristics MI EEG. The EMD method is proposed to decompose the EEG signal into a set of stationary time series called Intrinsic Mode Functions (IMF). These IMFs are analyzed with the bandpower (BP) to detect the characteristics of sensorimotor rhythms (mu and beta) when a subject imagines a left or right hand movement. Finally, the data were just reconstructed with the specific IMFs and the bandpower is applied on the new database. Once the new feature vector is rebuilt, the classification of MI is performed using two types of classifiers: generative and discriminant. The results obtained show that the EMD allows the most reliable features to be extracted from EEG and that the classification rate obtained is higher and better than using the direct BP approach only. Such a system is a promising communication channel for people suffering from severe paralysis, for instance, people with myopathic diseases or muscular dystrophy (MD) in order to help them move a joystick to a desired direction corresponding to the specific motor imagery",
        "zenodo_id": 1044300,
        "dblp_key": "conf/ismir/StoberPM16",
        "content": 