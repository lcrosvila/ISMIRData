[
    {
        "title": "A multiple feature model for musical similarity retrieval.",
        "author": [
            "E. Allamanche",
            "Jürgen Herre",
            "Oliver Hellmuth",
            "Thorsten Kastner",
            "C. Ertel"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1416684",
        "url": "https://doi.org/10.5281/zenodo.1416684",
        "ee": "https://zenodo.org/records/1416684/files/AllamancheHHKE03.pdf",
        "abstract": "Despite the “fuzzy” nature of musical similarity, which varies from one person to another, perceptual low level features combined with appropriate classi- fication schemes have proven to perform satisfacto- rily for this task. Since a single feature only captures some selective characteristics of an audio signal, this information may, in some cases, not be sufficient to properly identify similarities between songs. This pa- per presents a system which combines a set of acous- tic features for the task of retrieving similar sounding songs. The methodology for optimum feature selec- tion and combination is explained, and the system’s performance is assessed by means of a subjective lis- tening test. 1",
        "zenodo_id": 1416684,
        "dblp_key": "conf/ismir/AllamancheHHKE03",
        "keywords": [
            "musical similarity",
            "fuzzy nature",
            "perceptual low level features",
            "classification schemes",
            "satisfactorily",
            "audio signal",
            "song similarities",
            "system",
            "acoustic features",
            "subjective listening test"
        ],
        "content": "A Multiple Feature Model for Musical Similarity Retrieval\nEric Allamanche\nalm@iis.fhg.deJ¨urgen Herre\nhrr@iis.fhg.deOliver Hellmuth\nhel@iis.fhg.de\nFraunhofer Institut IntegrierteSchaltungen, IIS\nAm Wolfsmantel 33\nD-91058 Erlangen\nGermanyThorsten Kastner\nksr@iis.fhg.deChristian Ertel\nertelcn@iis.fhg.de\nAbstract\nDespite the “fuzzy” nature of musical similarity,\nwhich varies from one person to another, perceptual\nlow level features combined with appropriate classi-\nﬁcation schemes have proven to perform satisfacto-\nrily for this task. Since a single feature only captures\nsome selective characteristics of an audio signal, this\ninformation may, in some cases, not be sufﬁcient to\nproperlyidentifysimilaritiesbetweensongs. Thispa-\nperpresentsa systemwhichcombinesaset ofacous-\nticfeaturesforthetaskofretrievingsimilarsounding\nsongs. The methodology for optimum feature selec-\ntion and combination is explained, and the system’s\nperformance is assessed by means of a subjective lis-\ntening test.\n1 Introduction\nUsually, human listeners have a well-developed feeling for\n“whether two songs sound similar” or whether they don’t.\nWhile this type of judgment is generally based on both lis-\ntening to the music material itself and considerable amount of\nbackground knowledge (the listener’s “world model”), an emu-\nlation of this capability within reasonable bounds of complex-\nity can only be based on the music material itself and the fea-\ntures extracted from the audio material. Clearly, when trying to\nmodel certain aspects of human behavior, a careful assessment\nofthemodel’sperformanceisnecessaryinordertocomparethe\nachieved results with theresponse of human listeners.\n2 Related Work\nWhile calculation of a subjective musical similarity measure is\ndifferent from many other well-known tasks in the ﬁeld of mu-\nsic information retrieval (MIR), it deﬁnitely touches upon re-\nlatedwork. Asanexample,thenotionof“content-basedsearch\nandclassiﬁcation”waspioneeredbyWoldetall.(1996),where\na set of acoustical features was proposed. Weare and Tanner\nPermission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided that\ncopies are not made or distributed for proﬁt or commercial advan-\ntage and that copies bear this notice and the full citation on the ﬁrst\npage.c\r2003 Johns Hopkins University.(2001) focused on the modeling of certain basic semantic as-\npects relevant to the human perception of music, which then in\ncombinationcanbeusedtoachieveamorecomprehensivechar-\nacterization of a higher level. Evaluating the signal’s low-level\nacousticfeaturesonly,AucouturierandPachet(2002)proposed\nan MFCC-based systemfor similarity search.\n3 Experimental Setup\n3.1 Feature Candidates\nA number of low-level acoustic features were included into the\ninvestigationbased on theirknown merits in MIR tasks:\nNormalizedLoudness isabandwisemeasureofperceivedsound\nintensity(ZwickerandFastl,1990)dividedbytheoverallloud-\nness,Delta Log-Loudness is the bandwise difference over time\nof the logarithm of speciﬁc loudness, Spectral Flatness Mea-\nsure(SFM) and Spectral Crest Factor (SCF) indicate how ﬂat\nor “peaky” the power spectral density is in a given subbband\n(Herreetal.,2001), RealCepstralCoefﬁcients (RCC)(Rabiner\nand Juang, 1993) have been found to be an efﬁcient means of\nrepresenting a signal’s spectral envelope shape, Mel Frequency\nCepstral Coefﬁcients (MFCC) further extend the concept of\nRCCs by incorporating perceptual aspects (Rabiner and Juang,\n1993),Spectral Tilt andSharpness (Zwicker and Fastl, 1990)\nand are indicators of the overall slope of the frequency enve-\nlope and Zero Crossing Rate (ZCR) gives the number of sign\nchanges which occur withina frame.\nThese features were extracted using a common front end con-\nsisting of a windowed discrete Fourier transform. To further\nreduce the amount of feature data, the short term means and\nvariances were calculated for short time segments comprising\nbetween 2 and 32 values (subjectedto optimization).\n3.2 Optimization Methodology\nThe task of a model of musical similarity is to produce a dis-\ntance between any two musical excerpts. It was postulated that\nagoodmodelshouldprovidesmalldistancemeasurevaluesbe-\ntweensimilarsoundingitems. Asmallsetof21referenceitems\nwasselectedcontainingitemsofratherdifferentmusicalstyles,\nsuch as pop, jazz, classical, rap, rhythm & blues. Each of these\n21referenceitemshadone(known)veryclosestylisticcounter-\npartwithinafurthersetof30musicaltestitems(i.e.,therewere\n9additionalitemswhichdidnotexhibitveryclosesimilarityto\nanyofthereferenceitems). Foreachmodelunderexamination,\nthe following evaluation steps were carried out: Features were\nextractedfromall30itemsinthetestsetandsubsequentlyclus-tered using a k-meansalgorithm into 16 centroids. From each\nreference item, features were extracted from typical 10-second\nexcerpts. The (accumulated) distance between these features\nandanyoftheclusteredtestitemfeatureswasdeterminedusing\nanearest neighbor (NN) classiﬁer type procedure. These dis-\ntance values are intended to correspond to the subjective sim-\nilarity between the compared items. The distances between a\nreference item and all items in the test set were used to order\nthe list of test items according to their similarity with respect\nto any particular reference item. Thus, the entry at the ﬁrst list\nposition would denote the most similar test item found for a\nreference item. For each of the reference items, the list posi-\ntionofitsknownstylisticcounterpartwasdeterminedandaver-\nagedacrossallreferenceitems,resultinginanaveragelistposi-\ntion value. An average list position of one would show that the\nproperstylisticcounterpartwasalwaysconsideredmostsimilar\nto the corresponding referenceitem.\nUsing the average list position as an overall ﬁgure of merit for\na similarity model, the goal of the development process was\nthus reduced to an automatic procedure. Note that in this sce-\nnario, the meaning of “test item” versus “reference item” ap-\npearsswappedascomparedtostandardterminologywhereref-\nerence items are commonly used to train a recognition system.\nItseemed,however,appropriatetousetheterm“referenceitem”\ninorder todescribethemusicitems onwhichthecalibration of\nthe whole optimization processis based.\nAfter evaluating simple similarity models which made use of\nonly one feature at a time over a set of 1,000 test items, the set\nof the most promising subband-based candidate features where\nretained. Based on both the minimization of the average posi-\ntion list ﬁgure of merit and the desire for a balanced behavior\nacross all items, optimized combinations of the candidate fea-\ntures were determined. Finally, a model using a combination\nof SFM, SCF, Normalized Loudness, MFCCs and Delta Log-\nLoudnessemergedasthebestandachievedanaverageposition\nvalueof20withinthe1,000itemslist. Itneedstobementioned\nthat the “average position” criterion is certainly an imperfect\ncriterionforoptimizationsincenomanualselectionofthe1,000\nitems was undertaken to ensure that there were no other items\nin the database which would also exhibit a very close musical\nsimilarity to the referenceitems.\n4 Assessing the Model’s Performance\nTheperformanceofthemodelusingtheoptimizedfeaturecom-\nbination was evaluated through a subjective listening test by\n10 subjects with various musical backgrounds and preferences.\nThe training set was increased to 15,000 songs. From this set\nof reference songs, 10 representative seconds of 20 randomly\nchosen items were selected as test excerpts. The similarity sys-\ntem was then queried with these excerpts and for each one, the\n5 songs considered most similar were retained for the listening\ntest, as well as the song rated most dissimilar. For comparison\nto chance, an additional song was randomly selected from the\nreference set but not ranked by the system. Thus, for each test\nexcerpt, the subjects were presented 7 candidates whose simi-\nlarities to the excerpt were to be ranked on a scale from 0 (very\ndissimilar) to 100 (very similar).\nThe rankings of the listening subjects over all 20 test items\nwere collected and evaluated statistically. The average similar-ityscoresoveralllisteningsubjectsanditemsaregiveninTable\n1. The results show that the pairwise musical similarities rated\nbythelisteningsubjectsareconsistentwiththeclosestmatches\ngiven by the system. More speciﬁcally, it was found that the\nscores of the 4 most similar songs range within a small inter-\nval reﬂecting the high similarities between these songs and the\nreference, that the listeners’ scores were in agreement with the\npredictions of the model for the most dissimilar items, and, ﬁ-\nnally,thatthescoresoftherandomlyselecteditemsarebetween\nthe scores of the most similar and dissimilar songs (as could be\nexpected statistically).\nsuggested by the similaritymodel\n1234dissimilar random\n64.563.867.864.0 11.9 23.8\nTable 1: Average similarity scores over all listening subjects\nand over all 20 items (0=mostdissimilar, 100=most similar)\n5 Conclusions\nThis paper focused on algorithmic modeling the musical sim-\nilarity, as perceived by humans. The investigated models rely\non a set of low-level acoustic features which can be extracted\nefﬁciently from audio data. In order to minimize subjective\ntest effort during the development process, the optimization of\nsingle feature and feature combination based models was con-\nducted with a new method enabling an automatic assessment\nof the model’s ﬁtness. In a subjective listening test, the model\nresultingfromtheoptimizationprocesswasassessedinitscon-\nsistencywithhumanperceptionandhasdemonstratedtodeliver\npromising results with adata base of 15,000 musical items.\nReferences\nAllamanche,E.&HerreJ.&HellmuthO.&Fr ¨oba,B.&Kast-\nner,T.&Cremer,M.(2001). Contentbasedidentiﬁcationofau-\ndio material using MPEG-7 low level description. Proceedings\n2nd International Conference on Music Information Retrieval ,\n(pp. 197–204). Bloomington,IN.\nAucouturier,J.J.&Pachet,F.(2002). Findingsongsthatsound\nthe same. Proceedings IEEE Workshop on Model Based Pro-\ncessing and Coding of Audio , (pp. 91–98), Leuven, Belgium.\nHerre, J. & Allamanche, E. & Hellmuth, O. (2001). Robust\nMatching of Audio Signals Using Spectral Flatness Features.\nIEEE Workshop on Applications of Signal Processing to Audio\nand Acoustics , (pp. 127–130), New Paltz,NY.\nRabiner, L. & Juang, B. H. (1993). Fundamentals of Speech\nRecognition . Englewood Cliffs: PrenticeHall\nWeare,C.&Tanner,T.C.(2001). InSearchofaMappingfrom\nParameter Space to Perceptual Space. AES 18th International\nConference , Burlingame.\nWold, E. & Blum, T. & Keislar, D. & Wheaton, J. (1996).\nContent-Based Classiﬁcation, Search, and Retrieval of Audio.\nIEEE Multimedia , 3(3), 27–36.\nZwicker, E. & Fastl, H. (1990). Psychoacoustics - Facts and\nModels. Berlin, Heidelberg: Springer"
    },
    {
        "title": "Automatic synchronization of music data in score-, MIDI- and PCM-format.",
        "author": [
            "Vlora Arifi",
            "Michael Clausen",
            "Frank Kurth",
            "Meinard Müller"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417848",
        "url": "https://doi.org/10.5281/zenodo.1417848",
        "ee": "https://zenodo.org/records/1417848/files/ArifiCKM03.pdf",
        "abstract": "In this paper we present algorithms for the auto- matic time-synchronizationof score-, MIDI- or PCM- data streams representing the same polyphonic piano piece. 1",
        "zenodo_id": 1417848,
        "dblp_key": "conf/ismir/ArifiCKM03",
        "keywords": [
            "auto-matic",
            "time-synchronization",
            "score-",
            "MIDI-",
            "PCM-",
            "data",
            "streams",
            "polyphonic",
            "piano",
            "piece"
        ],
        "content": "AutomaticSynchronization ofMusicDatainScore-,MIDI-andPCM-Format\nVlora Ariﬁ, Michael Clausen, Frank Kurth, andMeinard M¨uller\nUniversit¨atBonn, Institut f¨urInformatik III\nR¨omerstr .164, D-53117 Bonn, German y\nfvlora,clausen, kurth,meinard g@cs.uni-bonn.de\nAbstract\nInthis paper wepresent algorithms fortheauto-\nmatic time-synchronization ofscore-, MIDI- orPCM-\ndata streams representing thesame polyphonic piano\npiece.\n1Introduction\nModern digital music libraries consist oflargecollections of\ndocuments containing music data ofdiversecharacteristics and\nformats. Forexample, foroneandthesame piece ofmusic, the\nlibrary may contain thecorresponding score inCapella orScore\nformat, some MIDI-ﬁles, andseveralinterpretations inform of\nCDrecordings. Inhomogeneity andcomple xityofsuch music\ndata makecontent-based browsing andretrie valindigital mu-\nsiclibraries adifﬁcult task with manyyetunsolv edproblems.\nOne important steptowardsasolution aresynchronization algo-\nrithms which automatically link data streams ofdifferent data\nformats representing asimilar kind ofinformation. Inpartic-\nular,intheframe workofaudio bysynchronization wemean\nsome procedure which, foragivenposition insome represen-\ntation ofagivenpiece ofmusic (e.g., giveninscore format),\ndetermines thecorresponding position within some other repre-\nsentation (e.g., giveninPCM-format).\nSuch synchronization algorithms haveapplications inmanydif-\nferent scenarios: follo wing some score-based music retrie val,\nlinking structures canbeused toaccess some suitable audio\nCDaccurately tolisten tothedesired partoftheinterpretation.\nAfurther application istheautomatic annotation ofapiece of\nmusic indifferent data formats asabasis forcontent-based re-\ntrieval.Asanother example, musicologists canusesynchro-\nnization algorithms fortheinvestigation ofagogic andtempo\nstudies. Furthermore, temporal linking ofscore andaudio data\ncanbeuseful forautomatic tracking ofthescore positions dur-\ningaperformance.\nInourwork, weconcentrate onthree representati vedata for-\nmats used formusic data: thesymbolic scoreformat ,thephys-\nicalPCM-format andtheMIDI-format which may bethought\nPermission tomakedigitalorhardcopiesofallorpartofthiswork\nforpersonalorclassroom useisgrantedwithoutfeeprovidedthat\ncopiesarenotmadeordistributedforpro®torcommercial advan-\ntageandthatcopiesbearthisnoticeandthefullcitationonthe®rst\npage.c\r2003JohnsHopkinsUniversity.ofasahybrid ofthelasttwodata formats. Wehavedevel-\noped synchronization algorithms fortwodata streams inanyof\nthese three data formats which willbereferred toasScore-to-\nMIDI (SM) synchronization, Score-to-PCM (SP) synchroniza-\ntion, andMIDI-to-PCM (MP) synchronization. Especially ,SP-\nandMP-synchronization constitute adifﬁcult problem since the\nwaveform-based PCM-format does notcontain anyexplicit in-\nformation onthenotes. Therefore, inthese cases some score-\nlikeparameters such asonset times andpitches havetobeex-\ntracted from thePCM-data prior totheactual synchronization.\nDue tospace limitations, wearenotable togiveanovervie wof\ntherelated work. Links totherelevantliterature canbefound\ninthefullpaper version Ariﬁ etal.(2003a) available onour\nwebsite orinAriﬁ etal.(2003b).\n2FeatureExtraction\nInthissection wesummarize oursystem forextracting note pa-\nrameters from thePCM-data stream. Using severalestablished\ntools from audio signal processing, ourmain contrib utions are\nareﬁned template matching algorithm forpolyphonic pitch ex-\ntraction andatwo-step algorithm fornote onset detection.\nAudio\nGenerate note\npitches and\nonset positionsparameters fromSubband\nanalysisComputation of\nenergy vectorsPitch extraction\nvia template\nmatching\nnovelity curve:\nonset detectionMap onset positions\nvia LPC methodRefined onset detectionto subband positions\nComputation of\nobjectsnote onsetspitches\ndata baseTemplatesignal\nFigure 1:Diagram ofthefeature extraction algorithm.\nFigure 1showstheoverall feature extraction algorithm. Similar\ntoBobrek etal.(1998) aninput PCM-signal istransformed toa\nsubband representation using amultirate ﬁlter bank. Simultane-\nously ,atwo-stage peak-picking algorithm detects probable note\nonset positions. According tothose onset positions, thesubband\nrepresentation issplit intotime interv als.Foreach interv al,we\ncalculate anenergyvector with components corresponding to\nthesubbands: each component contains thetotal energywithin\ntheinterv aloftherespecti vesubband. Then, foreach energy\nvector apitch extraction based onatemplate matching algo-\nrithm isperformed. The pitch extraction yields asetofnotes\nforthecorresponding time interv al.The feature extraction al-\ngorithm outputs anote object foreach note ineach time interv al,where anote object consists ofanonset time andapitch infor -\nmation. Forimplementation details oftheoverall system we\nrefer toAriﬁ (2002).\n3Synchronization Algorithms\nInthis section wedescribe theactual synchronization algo-\nrithms. Here, weonly consider thecase ofascore- andaPCM-\ndata stream (SP-synchronization). Theother cases such asSM-\norMP-synchronization areeveneasier orcanbedone inasim-\nilarfashion (seeAriﬁ (2002)).\nWeﬁrst preprocess thescore data stream where wedistin-\nguish between twokinds ofnote objects:explicit andimplicit\nnotes. Forexplicit objects allnote parameters such asmeasure,\nbeat, duration, andpitch aregivenexplicitly .Inviewofthe\nsynchronization algorithm weonly usethemusical onset time\nandpitch. Werepresent each explicit note object byatuple\n(e;p)2Q\u0002[0:127],where therational number eencodes\ntheonset time andtheintegerpthepitch ofthecorresponding\nnote. Byanimplicit note object weunderstand notes oragroup\nofnotes with some additional speciﬁcation such asatrill, an\narpeggio orgrace notes. Implicit objects allowdifferent real-\nizations. Togetthisambiguity under control weintroduce the\nconcept ofafuzzynote,which isdeﬁned tobeatuple(e;H)\nconsisting ofsome onset timee2Qandsome setofalternati ve\npitches H\u001a[0:127].Inthismodel, thepreprocessed score is\ngivenbysome subset S\u001aQ\u00022[0:127]\u00022[0:127],where 2[0:127]\ndenotes thesetofallsubsets of[0:127].Here, inatriple\n(e;H0;H1)2Sthesubset H0\u001a2[0:127]consists ofallpitches\nofexplicit note objects having musical onset timeeandsimi-\nlarly thesubset H1\u001a2[0:127]consists ofallpitches ofimplicit\nnote objects.\nNext,theextracted note parameters from thePCM data stream\n(seeSection 2)arefurther preprocessed byquantizing theonset\ntime candidates with some suitable chosen quantizer resolution\n\u0001>0.Theresulting data stream isdenoted byP\u0001which can\nberegarded asasubset ofQ\u00022[0:127].Note thatinthePCM-\ncase there areonly explicit note objects.\nAltogether ,wemay assume that the score and the\u0001-\nquantized extracted PCM-data are givenby the sets\nS=[(s1;S01;S11);:::;(ss;S0s;S1s)]andP\u0001=\n[(p1;P01);:::;(pp;P0p)]:Here, thesi,1\u0014i\u0014s,denote\nthemusical onset times andthepj,1\u0014j\u0014p,thequantized\nphysical onset times. Furthermore, S0i;S1i;P0j\u001a[0:127]\naretherespecti vesets ofpitches fortheexplicit andimplicit\nobjects.\nWenowaccomplish theSP-synchronization bymatching the\nsetsSandP\u0001inthefollo wing sense.\nDeﬁnition 3.1. Ascore-PCM-matc h(SP-match) ofSandP\u0001is\ndeﬁned tobeapartial map\u0016:[1:s]![1:p],which isstrictly\nmonotonously increasing onitsdomain satisfying (S0i[S1i)\\\nP0\u0016(i)6=;foralli2Domain (\u0016).\nThis deﬁnition needs some explanations. Thefactthatobjects\ninSorP\u0001may nothaveacounterpart intheother data stream\nismodeled bytherequirement that\u0016isonly apartial function\nandnotatotal one. Themonoton yof\u0016reﬂects therequirement\noffaithful timing: ifanote inSprecedes asecond onethis\nalso should hold forthe\u0016-images ofthese notes. Finally ,the\nrequirement (S0i[S1i)\\P0\u0016(i)6=;preventsthatonset timesarelinkedwhich arecompletely unrelated with respect totheir\npitches.\nObviously ,there aremanypossible SP-matches between S\nandP\u0001.However,introducing acost function makesdiffer-\nentmatches comparable. Foranexplicit formula ofourcost\nfunction, which iscrucial inviewofthequality ofthesolu-\ntionforthesynchronization problems, wehavetorefer toAr-\niﬁetal.(2003a). Summarizing, ourcost function penalizes\nnon-matched explicit andimplicit note objects inS(where the\nweighting ofthepenalty depends onthetype ofnote objects),\nnon-matched note-objects inP\u0001,aswell aslargerelati veonset\ntime deviations thus preventing largeglobal deviations inthe\nsynchronization.\nThe solution oftheSP-synchronization isthen realized byan\noptimal SP-match minimizing thecost function. This optimal\nmatch canbeefﬁciently computed bymeans ofdynamic pro-\ngramming.\n4Experimental ResultsandConclusions\nWehaveimplemented aprototype oftheextraction algorithms\nfrom Section 2andthesynchronization algorithms intheMAT-\nLAB programming language andtested ouralgorithms forSM-,\nSP-, andMP-synchronization onavariety ofclassical poly-\nphonic piano pieces ofdifferent comple xityandlength (ranging\nfrom 10to60seconds) played onvarious instruments. Fur-\nthermore, wehavesystematically generated alibrary ofmore\nthan onehundred testpieces both inMIDI- andPCM-format\nplayed onaMIDI-piano, aSteinw aygrand piano, andaSchim-\nmelpiano. Insome ofthose pieces ourperformer hasdelib-\nerately builtinexcessi veaccelerandi, ritartandi, rhythmic dis-\ntortions, andwrong notes. Eveninthese extreme situations,\nwhere one unsurprisingly hasmany“erroneously” extracted\nnote objects which considerably differfrom thescore-data, our\nSP-synchronization algorithm resulted ingood overall global\nmatches which aresufﬁcient fortheapplications mentioned in\ntheintroduction. Evenmore, incase ofrather accurate extracted\nnote parameters oursynchronization algorithms could resolv e\nsubtle local time variations insome interpreted version ofthe\npiano piece. Forfurther details andresults ofourexperiments\nwerefer toAriﬁ (2002).\nRefer ences\nAriﬁ, V.Algorithmen zurSynchronisation vonMusikdaten\nimParitur-,MIDI-undPCM-Format .PhD thesis, Uni-\nversit¨atBonn, Institut f¨urInformatik (2002). Retrie v-\nable fromhttp://hss.ulb.uni-bonn.de:90/ulb bonn/\ndissonline/math natfak/2002/arifi vlora/\nAriﬁ, V.,Clausen, M., Kurth, F.,M¨uller,M.(2003a).\nAutomatic Synchronization ofMusicDatainScore-,\nMIDI-, andPCM-Format . Technical Report, Univer-\nsit¨atBonn, Institut f¨urInformatik. Retrie vable from\nhttp://www-mmdb.iai.uni-bonn.de/eng-pub lic.html\nAriﬁ, V.,Clausen, M.,Kurth, F.,M¨uller,M.(2003b). Auto-\nmatic Synchronization ofMusic Data. Toappear inComputing\ninMusicology,MIT Press.\nBobrek, M.,Koch, D.(1998). Music Signal Segmentation Us-\ningTree-Structured Filter Banks.JournalofAudioEngineering\nSociety ,46(5), 412–427."
    },
    {
        "title": "Analysis of queries to a Wizard-of-Oz MIR system: Challenging assumptions about what people really want.",
        "author": [
            "David Bainbridge 0001",
            "Sally Jo Cunningham",
            "J. Stephen Downie"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1416850",
        "url": "https://doi.org/10.5281/zenodo.1416850",
        "ee": "https://zenodo.org/records/1416850/files/BainbridgeCD03.pdf",
        "abstract": "How do users of music information retrieval (MIR) systems express their needs? Using a Wizard of Oz approach to system evaluation, combined with a grounded theory analysis of 502 real-world music queries posted to Google Answers, this paper addresses this pivotal question. 1",
        "zenodo_id": 1416850,
        "dblp_key": "conf/ismir/BainbridgeCD03",
        "keywords": [
            "music information retrieval",
            "system evaluation",
            "grounded theory",
            "Google Answers",
            "pivotal question",
            "users",
            "express their needs",
            "Wizard of Oz approach",
            "real-world music queries",
            "system"
        ],
        "content": "How People Describe Their Music Information Needs:\nA Grounded Theory Analysis Of Music Queries\nDavid Bainbridge\nDepartment of Computer Science\nUniversity of Waikato\nHamilton, New Zealand\ndavidb@cs.waikato.ac.nzSally Jo Cunningham\nDepartment of Computer Science\nUniversity of Waikato\nHamilton, New Zealand\nsallyjo@cs.waikato.ac.nzJ. Stephen Downie\nGraduate School of Library and\n Information Science\nUniversity of Illinois at Urbana-Champaign\nChampaign, Illinois, USA\njdownie@uiuc.edu\nAbstract\nHow do users of music information retrieval (MIR)\nsystems express their needs? Using a Wizard of Oz\napproach to system evaluation, combined with a\ngrounded theory analysis of 502 real-world music\nqueries posted to Google Answers, this paper\naddresses this pivotal question.\n1 Introduction\nIn this paper we study the issue of what facilities people really\nwant when performing a music information retrieval (MIR)\ntask by analysing the questions and answers posted to the\nmusic category of Google’s “ask an expert” service, Google\nAnswers. This work builds on a previous study that analysed\npostings to a Usenet newsgroup for a specific genre of music:\n‘old time’ country (Downie and Cunningham, 2002). The set\nof music queries analysed in this present work is considerably\nlarger (approximately four fold), and the scope of questions\nmore broadly based.\nGoogle Answers is essentially a reference service affiliated\nwith, but not restricted to, the Web; the music category can be\nbroadly seen as an MIR system in which the ‘interface’\ninvolves submission of natural language queries to be\n‘processed’ by human experts. The queries posed to Google\nAnswers express authentic music information needs, not\nconstrained by necessity to use an artificial query language.\nWe argue this adds up to a “Wizard of Oz” system for music\ninformation retrieval—that is, the Google Answers experts\nsimulate the processing of an MIR system. The Wizard of Oz\ntechnique is commonly used in computer application design to\nevaluate an approach before it is implemented, in an attempt\nto gain insights that can help evolve the design before\nirrecoverable coding decisions are made (see, for example, an\noverview in Dahlback et al, 1993). We use the approach here\nto see what sort of capabilities an MIR needs to serve a\ndiverse group of users.2 Data collection\nGoogle Answers makes an interesting case study since it has a\nsizable number of users and experts, has been running a\nreasonable length of time (over 1 year) and music questions in\nparticular seem a popular preoccupation: music is large sub-\ncategory within Arts and Entertainment. 626 postings were\nretrieved from the music sub-category, spanning 10 April\n2002–1 April 2003. Note that it is the poster who determines\nthe subject category associated with the question, so it is the\nusers themselves who have decided that these are fitting music\nqueries. However, 5 postings in our analysis were discarded as\nbeing off-topic (for example, “How do you find investors?”).\nAn additional 119 postings were related to the music industry\nrather than to music itself, and so these postings are not further\nanalyzed in this paper.\nThe 502 remaining postings in the music category were\nanalysed using a grounded theory approach (Glaser and\nStrauss, 1967). With this technique researchers attempt to\napproach the data without prior assumptions, and to generate\ntheory from the data. The queries were coded to characterize\nthe types of details users are able to offer when describing\ntheir information need.\n3 How do people describe what they want?\nUnfortunately, space limitations preclude a detailed\nexplication of the analytic categories emerging from the\nqueries. Table 1 shows that for the majority of music queries\nanalysed (slightly over 80%), users are able to provide some\nform of Bibliographic  metadata when describing their\ninformation request. By far the most common bibliographic\nattributes (Table 2) supplied are the name of the Performer (s)\nand the Title of a work—certainly an MIR system should\ninclude these metadata, as a bare minimum. A small\nproportion of queries included a URL Link to a webpage\nproviding additional bibliographic metadata. Orchestration\nmight include instrument, vocal range of singers, or (most\ncommonly) the gender of the singers. The default Language\nfor lyrics appears to be English, and language was only\nmentioned in the query if the desired work(s) had non-English\nlyrics. Similarly, the Nationality of the performer (Table 2)\nwas primarily mentioned when that person was not from thePermission to make digital or hard copies of all or part of this work for\npersonal of classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear\nthis notice and the full citation on the first page.   2003 The Johns Hopkins\nUniversity.US or the UK. From these categories, it appears that quality\nbibliographic metadata may be crucial for use in searching.\nUsers experienced difficulty in coming up with crisp\ndescriptions for several of the categories, indicating a need to\nsupport ‘fuzzy’ or imprecise metadata values for searches. For\nexample, the Date of composition or recording for the desired\nmusical items is rarely specified precisely by year. More\ntypically, the decade will be given, or other somewhat\nnebulous date indications such as “recent” or “old”. The user\nmay be uncertain as to the accuracy of the lyrics that they\nrecall (“the chorus went something like…”). Some Lyric\nelements may not be dictionary words (“bling bling”), which\nposes problems for the user in trying to come up with the\n‘correct’ spelling; it is also not clear how to transcribe\nrepetitions (“ Money money money moooooooneeeeeeyy.....\nMO-NEY!” ). Sometimes users were uncertain as to the\naccuracy of their descriptions: “female singer, African-\nAmerican likely (but i could be wrong..heck  it might just be a\nmale with a female-sounding voice)”.\nGenre descriptions ranged from standard (but not crisply\ndefined) characterizations (“jazz”, “pop”) to the highly\nidiosyncratic (“Sort of that teenie bop bitter angst genre”). The\nlack of consensus over genre categories and descriptions\nsuggests that it may be appropriate to support (or replace) agenre label with examples of music falling within the genre;\nperhaps a more productive way of locating new pieces within\na genre is to allow the user to ask for ‘more things like this’ by\nproviding music query-by-example facilities\nFew queries included an audio Example , yet it would be\npremature to conclude that query-by-humming MIR systems\nhave no potential user-base. Some queries suggest that users\nwould like to include a musical representation but are stymied\nby the form-based interface to Google Answers: “The only\nother information that I have is what the song sounds like, but\nit is hard to express that via the web.”\nReferences\nDahlback, N., Jonsson, A., & Ahrenberg, L. (1993). Wizard of\nOZ studies—why and how. In Proceedings Intelligent User\nInterfaces ’93  (pp. 193–200).\nDownie, J.S., & Cunningham, S.J. (2202). Toward a theory of\nmusic information retrieval queries: System design\nimplications. In Michael Fingerhut  (Editor) Proceedings of the\nThird International Conference on Music Information\nRetrieval  (ISMIR) (pp. 299–300). Paris: IRCAM.\nGlaser, B., and Strauss, A. (1967). The discovery of grounded\ntheory: Strategies for qualitative research . Chicago: Aldine de\nGruyter.Category Description of information need includes… Count %\nBIBLIOGRAPHIC metadata (see Table 2 for breakdown) 40881.3\nGENRE Description of the genre or style 16432.7\nLYRIC FRAGMENT some or (rarely) all of “the words” 14528.9\nWHERE HEARD circumstances surrounding a remembered performance or broadcast of a song 12124.1\nNATIONALITY Nationality of performer or origin of song 6312.5\nSIMILAR reference(s) to known work(s) or performer(s) used to defined attributes upon which\ndescribe the unknown item(s); 234.6\nEXAMPLE representation(s) of the desired work(s) (e.g., links to MP3 or midi files) 224.4\nLYRIC STORY storyline of song 132.6\nAFFECT  ‘mood’ or emotional state induced  (e.g.,  “funny”, “silly” , “plaintive”) 122.4\nTEMPO speed and/or rhythm of work(s) 122.4\nNote #1: % calculated against the set of 502 queries seeking music work(s) or music work information\nTable 1. Categories of need description types\nCategory Description Count %\nPerformer performer or group who created a particular recording 240 58.8\nTitle Name (or approximation) of work(s) 176 43.1\nDate date that a recording was produced, or that a song was composed 160 39.2\nOrchestration Name of instrument(s) and/or vocal range(s) and/or genders (male/female) 68 16.7\nCollection title Name of album, LP, CD, audiotape, etc 61 15.0\nComposer Name of composer 36 8.8\nLabel Name of organization which produced recording(s) 27 6.6\nLink URL providing a link to further bibliographic data 12 2.9\nLanguage specifies a particular language (other than English) for lyrics 10 2.5\nOther bibliographic data falling outside the above categories 36 8.8\nNote #1: % calculated against the set of 408 Bibliographic  queries\nTable 2. Breakdown of BIBLIOGRAPHIC category from Table 1."
    },
    {
        "title": "A large-scale evalutation of acoustic and subjective music similarity measures.",
        "author": [
            "Adam Berenzweig",
            "Beth Logan",
            "Daniel P. W. Ellis",
            "Brian Whitman"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417010",
        "url": "https://doi.org/10.5281/zenodo.1417010",
        "ee": "https://zenodo.org/records/1417010/files/BerenzweigLEW03.pdf",
        "abstract": "Subjective similarity between musical pieces and artists is an elusive concept, but one that must be pur- sued in support of applications to provide automatic organization of large music collections. In this paper, we examine both acoustic and subjective approaches for calculating similarity between artists, comparing their performance on a common database of 400 pop- ular artists. Specifically, we evaluate acoustic tech- niques based on Mel-frequency cepstral coefficients and an intermediate ‘anchor space’ of genre classifi- cation, and subjective techniques which use data from The All Music Guide, from a survey, from playlists and personal collections, and from web-text mining. We find the following: (1) Acoustic-based measures can achieve agreement with ground truth data that is at least comparable to the internal agreement between different subjective sources. However, we observe significant differences between superficially similar distribution modeling and comparison techniques. (2) Subjective measures from diverse sources show rea- sonable agreement, with the measure derived from co-occurrence in personal music collections being the most reliable overall. (3) Our methodology for large- scale cross-site music similarity evaluations is prac- tical and convenient, yielding directly comparable numbers for different approaches. In particular, we hope that our information-retrieval-based approach to scoring similarity measures, our paradigm of sharing common feature representations, and even our partic- ular dataset of features for 400 artists, will be useful to other researchers. Keywords: Music similarity, acoustic measures, evaluation, ground-truth. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advan- tage and that copies bear this notice and the full citation on the first page. c⃝2003 Johns Hopkins University. 1",
        "zenodo_id": 1417010,
        "dblp_key": "conf/ismir/BerenzweigLEW03",
        "keywords": [
            "subjective similarity",
            "musical pieces",
            "artists",
            "automatic organization",
            "large music collections",
            "acoustic approaches",
            "genre classification",
            "subjective techniques",
            "The All Music Guide",
            "survey"
        ],
        "content": "ALarge-Scale Evaluationof Acoustic and Subjective Music Similarity Measures\nAdam Berenzweig\nLabROSA\nColumbiaUniversity\nNewYorkNY U.S.A.\nalb63@columbia.eduBethLogan\nHPLabs\nOneCambri dgeCenter\nCambridgeMAU. S.A.\nbeth.logan@hp.comDaniel P.W.Ellis\nLabROSA\nColumbiaUniversity\nNewYorkNY U.S.A.\ndpwe@ee.columbia.eduBrian Whitman\nMusicMind&MachineGroup\nMITMediaLab\nCambridgeMAU. S.A.\nbwhitman@media.mit.edu\nAbstract\nSubjective similarity between musical pieces and\nartistsisanelusiveconcept,butonethatmustbepur-\nsued in support of applications to provide automatic\norganizationoflargemusic collections. Inthispaper,we examin ebothacousticandsubjective approaches\nfor calculatin gs i milarity between artists, comparing\ntheirperformanceonacommondatabaseof400pop-\nular artists. Speciﬁcally, we evaluate acoustic tech-\nniques based on Mel-frequency cepstral coefﬁcients\nand an intermediate ‘anchor space’ of genre classiﬁ-cation,andsubjectivetechniqueswhichusedatafrom\nThe All Music Guide, from a survey, from playlists\nandpersonalcollections,andfromweb-textmining.\nWe ﬁnd the fo llowing: (1) Acoustic-based measures\ncan achieve agreement with ground truth data that is\natleastcomparabletotheinternalagreementbetween\ndifferent subjective sources. However, we observe\nsigniﬁcant differences between superﬁcially similar\ndistributionmodelingandcomparisontechniques. (2)Subjective measures from diverse sources show rea-\nsonable agreement, with the measure derived from\nco-occurrenceinpersonalmusiccollectionsbeingthemostreliableoverall. (3)Ourmethodologyforlarge-\nscale cross-site music similarity evaluations is prac-\ntical and convenient, yiel ding directly comparable\nnumbers for different approaches. In particular, we\nhopethatourinformation-retrieval-basedapproachto\nscoring similarity measures ,our paradigm of sharing\ncommonfeaturerepresentations,andevenourpartic-\nular dataset of features for 400 artists, will be useful\ntootherresearchers.\nKeywords:Music similarity, acoustic measures,\nevaluation,ground-truth.\nPermission to make digital or har dcopies of all or part of this work\nforpersonal or classroom use is granted without fee provided that\ncopies are not made or distributed for proﬁt or commercial advan-tage and that copies bear this notice and the full citation on the ﬁrstpage.c/circlecopyrt2003 Johns Hopkins University.\n1I n t r oduction\nTechniques to automatically determine music similarity have\nattracted much attention in r ecent years (Ghias et al., 1995;\nFoote,1997;Tzanetakis,2002;LoganandSalomon,2001;Au-\ncouturier and Pachet, 2002; Ellis et al., 2002). Similarity is at\nthe core of the classiﬁcation a nd ranking algorithms needed to\norganize and recommend music. Such algorithms will be usedinfuturesystemstoindexvasta udiorepositories,andthusmust\nrelyonau tomaticanalysis.\nHowever,for the researcher or system builder looking to use\nsimilaritytechniques,itisdifﬁculttodecidewhichisbestsuited\nfor the task at hand. Few authors perform comparisons across\nmultiple techniques, not least because there is no agreed-upon\ndatabase for the community. Furthermore, even if a commondatabase were available, it would still be a challenge to estab-\nlishanassociatedgroundtruth,giventheintrinsicallysubjective\nnatureofmusi csimilarity.\nThe work reportedin this paper started with a simple question:\nHow do two existing audio-based music-similarity measurescompare? This led us in several directions. Firstly, there are\nmultiple aspects of each acoustic measure: the basic features\nused, the way that feature distributions are modeled, and the\nmethodsforcalculatingsimilaritybetweendistributionmodels.\nIn this paper, we investigate t he inﬂuence of each of these fac-\ntors.\nTo do that, how ever, we n eeded to be able to calculate a mean-\ningful performance score for each possible variant. This basic\nquestion of e valuation brings us back to our earlier question of\nwhere to get ground truth (Ellis et al., 2002), and then how to\nusethisgroundtruthtoscoreaspeciﬁcacousticmeasure. Here,\nweconsider ﬁve differentsources of groundtruth, all collected\nvia the Web one way or another, and look at several different\nways to sc ore measures against them. We also compare them\nwithonea notherinanefforttoidentifywhichmeasureis‘best’\ninthe senseof approachinga consensus.\nAﬁ n a la s p ect of this work touches the question of sharing\ncommonevaluationstandards,andcomputingcomparablemea-\nsures across different sites. Although common in ﬁelds such\nas speech recognition, we believe this is one of the ﬁrst and\nlargest cross-site evaluations in music information retrieval.\nOurworkwasconductedintwoindependentlabs(LabROSAat\nColumbia, and HP Labs in Camb ridge), yet by carefully spec-\nifying our evaluation metrics, and by sharing evaluation data\nin the form of derived features (which presents little threat tocopyright holders), we were able to make ﬁne distinctions be-\ntweenalgorithmsrunningateachsite. Weseethisasapowerful\nparadigm that we would like to encourage other researchers to\nuse.\nThis paper is organizedas follows. First we review prior work\ninmusicsimilarity. Wethendescribethevariousalgorithmsand\ndata sources used in this paper. Next we describe our database\nandevaluationmethodologiesindetail. InSection6wediscussourexperimentsandresults. Finallywepresentconclusionsand\nsuggestionsforfuturedirections.\n2P r i o r Work\nPriorworkinmusicsimilarityha sfocusedononeofthreeareas:\nsymbolicrepresenta tions,acousticproperties,andsubjectiveor\n‘cultural’information. We de scribe each of these below noting\ninparticulartheirsuitabilityforautomaticsystems.\nManyresearchershavestudiedthemusicsimilarityproblemby\nanalyzing symbolic representa tions such as MIDI music data,\nmusicalscores,andthelike. Arelatedtechniqueistousepitch-\ntracking to ﬁnd a ‘melody contour’ for each piece of music.\nString matc hing techniques are then used to compare the tran-\nscriptions for each song e.g. (G hias et al., 1995). However,\ntechniques based on MIDI or scores are limited to music for\nwhich this data exists in elect ronic form, since only limited\nsuccess has been achieved for pitc h-tracking of arbitrary poly-\nphonicmusic.\nAcoustic approaches analyze t he music content directly and\nthus can be applied to any music for which one has the au-\ndio. Blum et al. present an indexing system based on match-\ningfeatures such as pitch, loudness or Mel-frequency cepstral\ncoefﬁcients (MFCCs) (Blum et al., 1999). Foote has designedam u s i cindexing system based on histograms of MFCC fea-\ntures derived from a discriminatively trained vector quantizer\n(Foote, 1997). Tzanetakis (2002) extracts a variety of featuresrepresentingthe spectrum, rhythmand chordchangesand con-\ncatenates them into a single vector to determinesimilarity. Lo-\ngan and Salomon (2001) and Aucouturier and Pachet (2002)model songs using local clustering of MFCC features, deter-\nmining similar ity by comparing the models. Berenzweig et al.\n(2003)usesa suite of patternclassiﬁers to mapMFCCs into an\n“anchor space”, in which probability models are ﬁt and com-\npared.\nWith the growth of the Web, t echniques based on publicly-\navailable data haveemerged(Cohenand Fan,2000;Ellis et al.,\n2002). These use text analysis and collaborative ﬁltering tech-\nniques to combine data from many users to determine similar-\nity. Since they are based on hum an opinion, these approaches\ncapture many cultural and other intangible factors that are un-\nlikely to be obt ained from audio. The disadvantage of these\ntechniqueshoweveristhattheyareonlyapplicabletomusicfor\nwhichareasonable amount of reliable Web data is available.\nFornewor undiscoveredartists, an audio-basedtechniquemay\nbemoresuitable.\n3A c o u s t i c S imilarity\nTo determ ine similarity based solel yo nt h ea udio content of\nthe music, we use ourprevioustechniqueswhichﬁt a paramet-\nric probability model to point sinanaudio-derivedinput space(Loganand Salomon, 2001;Berenzweig et al., 2003). We then\ncompute similarity using a measure that compares the models\nfor two artists. The results of each measure are summarized\nin asimilarity matrix ,asquare matrix where each entry gives\nthe similarity between a particular pair of artists. The leading\ndiagonalis, bydeﬁnition,1,wh ichisthelargestvalue.\nThe techniquesstudied are char acterized by the features, mod-\nelsanddistancemeasuresused.\n3.1 FeatureSpaces\nThefeaturespaceshouldcompactlyrepresenttheaudio,distill-\ning musically important informationand throwing away irrele-\nvantnoise. Althoughmanyfeatureshavebeenproposed,inthis\npaper we concentrate on features derived from Mel-frequencycepstralcoefﬁcients(MFCCs). Thesefeatureshavebeenshown\ntogive good performance for a variety of audio classiﬁcation\ntasksand are favored by a numberof groupsworking on audio\nsimilarity (Blum et al., 1999; Foote, 1997; Tzanetakis, 2002;\nLogan, 2000; Logan and Salomon, 2001; Aucouturier and Pa-\nchet,2002;Berenzweigetal.,2003).\nMel-cepstracapturetheshort-tim espectralshape,whichcarries\nimportantinformationaboutthemusic’sinstrumentationandits\ntimbres, the quality of a singer’s voice, and productioneffects.\nHowever, as a purely local feature calculated over a windowof tens of millis econds, they do not capture information about\nmelody,rhythmorlong-termsongstructure.\nWealso examine features in an ‘anchor space’ derived from\nMFCC features. The anchor space technique is inspired by afolk-wisdom approach to music s imilarity in which people de-\nscribe artists by statements such as, “Jeff Buckley sounds like\nVan Morrison meets Led Zeppelin, but more folky”. Here,\nmusically-meaningfulcategoriesandwell-knownanchorartists\nserve as conve nient reference points for describing salient fea-\ntures of the music. This approach is mirrored in the anchor\nspace techniquewith classiﬁers trainedtorecognizemusically-\nmeaningful categories. Music is “described” in terms of thesecategoriesbyrunningtheaudiot hrougheachclassiﬁer,withthe\noutputsformingtheactivationorlikelihoodofthecategory.\nFor this pap er, we used neural networks as anchor model pat-\ntern classiﬁers. Speciﬁcally, we trained a 12-class network to\ndiscriminate between 12 genres and two two-class networks torecognize these supplementalclasses: Male/Female (gender of\nthevocalist),andLo/Hiﬁdelity .Furtherdetailsaboutthechoice\nof anchors and the training technique are available in (Beren-\nzweig et al., 2003). An important point to note is that the in-\nput tothe classiﬁers is a large vector consisting of 5 frames of\nMFCC vectors plus deltas. This gives the network some time-\ndependent information from which it can learn about rhythm\nandtempo,atleastonthescal eofafewhundredm illiseconds.\n3.2 Modelinga nd ComparingDistributions\nBecause feature vectors are computed from short segments of\naudio,anentiresonginducesac loudofpointsinfeaturespace.\nThecloudcanbethoughtofassamplesfromadistributionthat\ncharacterizesthesong,andwecanmodelthatdistributionusingstatisticaltechnique s. Extendingthisidea, wecanconceiveofa\ndistribution in feature space that characterizes th ee ntire reper-\ntoireofeachartist.\nWe use Gau ssian Mixt ure Models (GMMs) to model thesedistributions, similar to previous work (Logan and Salomon,\n2001). Two methods of training the models were used: (1)\nsimple K-means clustering of t he data points to form clusters\nthat were then each ﬁt with a Gaussian component, to make aGaussianmi xturemodel(GMM),and(2)standardExpectation-\nMaximization(EM) re-estimation of the GMM parametersini-\ntializedfromtheK-meansclustering. Althoughunconventional,\nthe use of K-means to train GMMs withoutasubsequentstage\nofEMre-estimationwasdiscoveredtobebothefﬁcientanduse-\nfulforsong-levelsimilaritymeasurementinpreviouswork(Lo-\nganandSalomon,2001).\nThe parameters for these models are the mean, covariance and\nweight of each cluster. In some e xperiments, we used a single\ncovariance to describe all the cl usters. This is sometimes re-\nferred to as a “pooled” covariance in the ﬁeld of speech recog-\nnition; an “independent” covariance model estimates separate\ncovariance matrices for each cluster, allowing each to take on\nanindividual‘shape’infeatur espace,butrequiringmanymore\nparametersto beestimatedfromthedata.\nHaving ﬁt models to the data, we calculate similarity by com-\nparingthemodels. TheKullback-Leiblerdivergenceorrelativeentropy is the natural way to deﬁne distance between proba-\nbility distributions. However, for GMMs, no closed form for\nthe KL-divergence is known. We explore several alternativesand approximations: the “centroid distance” (Euclidean dis-\ntance between the overall means), the Earth-Mover’s distance\n(EMD)(Rubneretal.,1998)(whichcalculatesthecostof‘mov-\ning’ probability mass between mixture components to make\nthem equivalent), and the Asymptotic Likelihood Approxima-\ntion (ALA) to the KL-divergence between GMMs (Vasconce-\nlos,2001) (which segments feature space and assumes only\none Gaussian component dominates in each region). Anotherpossibility would be t ocompute the likelihood of one model\ngivenpointssampledfromthesecond(AucouturierandPachet,\n2002), but as this is very computationally expensive for largedatasets it was not attempted. Computationally, the centroid\ndistance is the cheapest of our methodsand the EMD the most\nexpensive.\n4Subjective similaritymeasures\nAn alternativeapproachto music similarity is to use sourcesof\nhumanopinion,forinstancebyminingtheWeb. Althoughthese\nmethods cannot easily be used on new music because they re-quire observations of humans interacting with the music, they\ncan uncover subtle relationships that may be difﬁcult to detect\nfromthe audiosignal. Subjectivemeasuresarealso valuableasgroundtruthagainstwhichtoevaluateacousticmeasures—even\nasparse ground truth can help validate a more comprehensive\nacoustic measure. Like the acoustic measures, subjective simi-larityinformationcanalsoberepresentedasasimilaritymatrix,\nwherethevaluesineachrowgivetherelativesimilaritybetween\nevery artist andonetarget.\n4.1Survey\nThemoststraightforwardway to gatherhumansimilarity judg-\nmentsistoexplicitlyask forit in a survey. We have previously\nconstructedawebsite,musicseer.com,toconductsuchasurvey(Ellisetal.,2002). Wedeﬁnedasetofsome400popularartists\n(described in section 5.3 below), then presented subjects with\nalist of 10 artists (a\n1,..a10),a n das i ngle target artistat,a n dasked“Whichoftheseartistsismostsimilartothetargetartist?”\nWeinterpret each response to mean that the chosen artist acis\nmore similar to t he target artistatthan any of the other artists\ninthelist ifthoseartistsareknown tothesubject,whichwecan\ninferby seeing if the subject has everselected the artists in any\ncontext.\nIdeally, the survey would provide enough data to derive a full\nsimilarity matrix, for example by counting how many timesusers selected artist a\nibeing most sim ilar to artistaj.H o w -\never, evenwith the 22,000responsescollected, the coverageof\nour modest artist set is relatively sparse: only around 7.5% ofall our artist pairs were directly compared, and only 1.7% of\nartist pairs were ever chosen as most similar. We constructed\nthis sparse similarity matr ix by populating each row with the\nnumber of times a given artist was chosen as most similar to a\ntarget as a proportion of the trials in which it could have been\nchosen. Althoughheuristic,thisworkedquitewellforourdata.\n4.2 ExpertOpinionRather than surveying the ma sses, we can ask a few experts.\nSeveralmusic-relatedonlineservicescontainmusictaxonomies\nand articles containing similarity data. The All Music Guide(www.allmusic.com)issuchaserviceinwhichprofessionaled-\nitorswritebriefdescriptionsofalargenumberofpopularmusi-\ncal artists, often includinga list of similar artists. We extractedthe “similar artists” lists from the All Music Guide for the 400\nartists in our set, discardinganyartists fromoutside the set, re-\nsulting in an average of 5.4 similar artists per list (so 1.35% of\nartist pairs had direct links). 26 of our artists had no neighbors\nfromwith intheset.\nAs in (Ellis et al., 2002) we convert these descriptions of the\nimmediate neighborhoodof each artist into a similarity matrixby computing the path length between each artist in the graph\nwherenodesareartistsandthereisanedgebetweentwo artists\niftheAllMusiceditorsconsiderthemsimilar. Ourconstruction\nissymmetric,since linksbetweenartists weretreated asnondi-\nrectional. We call this the Erd¨ os measure, after the technique\nused among mathematiciansto gauge their relationship to Paul\nErd¨os . This extends the similarity measure to cover 87.4% of\nartist pairs.\n4.3 PlaylistCo-occurrenceAnother source of human opinion about music similarity is\nhuman-authored playlists. We assume that such playlists con-\ntainsimilar music, which, though crude, proves useful. We\ngathered around 29,000 playlists from “The Art of the Mix”\n(www.artofthemix.org),awebs itethatservesasarepositoryand\ncommunitycenterforplaylisthobbyists.\nTo convert this data into a similarity matrix, we start with the\nnormalized playlist co-occurrence matrix, where entry (i,j)\nrepresentsthe jointprobabilitythat artist a\niandajoccurin the\nsameplaylist. However,thisprobabilityisinﬂuencedbyoverall\nartist popularity which should not affect a similarity measure.\nTherefore, we use a normalized conditional probability matrix\ninstead: Entry (i,j)ofthe normalized conditional probability\nmatrixCis the conditional probability p(ai|aj)divided by the\npriorprobabilityp(ai).S i n c e\ncij=p(ai|aj)\np(ai)=p(ai,aj)\np(ai)p(aj), (1)this is an appropriate normalization of the joint probability.\nNote that the expected log of thi sm e a s u r ei st h e mutualinfor-\nmationI(ai;aj)betweenartistaiandaj.\nUsing the playlists gathered from Art of the Mix, we con-\nstructed a similarity matrix with 51.4% coverage for our artistset (i.e. morethanhalfofthematrixcellswerenonzero).\n4.4 UserCo llections\nSimilar to user-authored playlis ts, individual music collections\nare another source of music similarity often available on the\nWeb.Mirroringtheideasthatunderlycollaborativeﬁltering,we\nassume that artists co-occurring in someone’s collection have\nab etter-than-average chance of being similar, which increases\nwiththe numbe rofco-occurrencesobserved.\nWeretrievedusercollectiondatafromOpenNap,apopularmu-\nsicsharingservice,althoughwewerecarefulnotdownloadany\naudioﬁles. After discardingartists notin ourdata set, we were\nleftwithabout176,000user-to-artistrelationsfromabout3,200\nuser collections. To turn this dat ai n t oas i m ilarity matrix, we\nusethesame normalized conditional probability technique as\nfor playlists as described a bove. This returneda similarity ma-\ntrixnonzerovalue sfor95.6%oftheartistpairs.\n4.5 Webtext\nAr i c hsource of informatio nresides in text documentsthat de-\nscribeor discuss music. Using techniques from the Informa-\ntion Retrieval (IR) community,we derive artist similarity mea-suresfrom documents returned fro mW e bsearches (Whitman\nand Lawrence, 2002). The best-performing similarity matrix\nfromthatstudy,derivedfrombigramphrases,isusedhere. Thismatrixhasessentiallyfullcoverage.\n5E v a l u ationMethods\nIn thissection, we describeour evaluationmethodology,which\nrelies on some kind of ground truth against which to compare\ncandidate measures; we expect the subjective data describedabovetobeagoodsourceofgroundtruthsincetheyarederived\nfrom human choices. In this section we present several ways\ntousethisdatatoevaluateouracoustic-basedmodels,although\nthe techniques can be used to evaluate any measure expressed\nas a similarity matrix. The ﬁrst technique is a general method\nby which one can use one simila rity matrix as a reference to\nevaluateany other, whereasthe other techniquesare speciﬁc to\noursurveydata.\n5.1 Evaluationagainstareferencesimilaritymatrix\nIf we can establish one similarity metric as ground truth, how\ncanwecalculatetheagreementach ievedbyothersimilarityma-\ntrices? Weuseanapproachinspiredbypracticeintextinforma-\ntionretrieval (Breese et al., 1998): Each matrix row is sorted\ninto decreasing similarity, and treated as the results of a query\nfor the corresponding target artist. The top N‘hits’ from the\nreference matrix deﬁne the ground truth, with exponentially-decaying weights so that the top hit has weight 1, the second\nhithas weightα\nr,t h enextα2\nretc. (We consideronly Nhits to\nminimize issues aris ing from similarity information sparsity.)\nThecandidatematrix‘query’isscoredbysummingtheweights\nof the hits by another exponentially-decaying factor, so that a\nground-truth hit placed at rank ris scaled byαr\nc.Thus this“top-Nrankingagreementscore” siforrowiis\nsi=N/summationdisplay\nr=1αr\nrαkr\nc (2)\nwherekris the ranking according to the candidate measure of\ntherth-ranked hit under the ground truth. αcandαrgovern\nhow sensitive the metric is to orderingunder the candidate andreference measures respectively. With N=1 0,α\nr=0.51/3\nandαc=α2\nr(the values we used, biased to emphasize when\nthetop few ground-truth hits appear somewhere near the top\nof the candidate response), the best possible score of 0.999 is\nachieved when the top 10 ground truth hits are returned in thesame order by the candidate matrix. F inally, the overall score\nfor the experimental similarity measure is the average of the\nnormalized row scores S=\n1\nN/summationtextN\nisi/smax,w h e r esmaxis\nthe best possible score. Thus a larger rank agreement score is\nbetter,with 1.0indicatingperfectagreement.\nOne issue with this measure arises from the handling of ties.\nBecausemuchofthesubjectiveinformationisbasedoncounts,\nranking ties are not uncommon (an extreme case being the 26\n‘disconnected’ artis ts in the “expert” m easure, who must be\ntreated as uniformly dissimilar to all artists). We handle thisbycalculatinganaveragescore overmultiplerandompermuta-\ntionsoftheequivalently-rankedentities; becauseoftheinterac-\ntionwiththetop-Nselection,aclosed-formsolutionhaseludedus.Thenumberofrepetitionswas based on empiricalobserva-\ntions of the variation in successive estimates in order to obtain\nastableestimate ofthe underlyingmean.\n5.2 Evaluatingagainstsurveydata\nThesimilaritydatacollectedusingourWeb-basedsurveycanbe\narguedto be a goodindependentmeasure of groundtruth artist\nsimilarity since users were explicitly asked to indicate similar-\nity. However,thecoverageofthesimilaritymatrixderivedfromthesurveydataisonlyaround1.7%,whichmakesitsuspectfor\nuseasagroundtruthreferenceasdescribedinsection5.1above.\nInstead,wecancomparetheindividualuserjudgmentsfromthe\nsurvey directly to the metric that we wish to evaluate. That is,\nwe ask the similarity metric the same questions that we askedtheusersandcomputeanaverageagreementscore.\nWeused two variants of this idea. The ﬁrst, “average response\nrank”, determines the average rank of the artists chosen from\nthe list of 10 presented in the survey according to the experi-\nmental metric. For example, if the experimental metric agrees\nperfectly with the human subject, then the ranking of the cho-\nsenartist will be 1 in every case, while a random ordering of\ntheartistswouldproduceanaveragerankingof5.5. Inpractice,\nthe ideal score of 1.0 is not possible because survey subjects\ndid notalways agree aboutartist similarity; therefore, a ceilingexistscorresponding to the single, consistent metric that opti-\nmallymatchesthesurveydata. Forourdata,thiswasestimated\ntogiveascoreof2.13.\nThe second approach is simply to count how many times the\nsimilarity measure agrees with the user about the ﬁrst-place\n(most sim ilar) artist from the list. This “ﬁrst place agreement”\nproportionhastheadvantagethatitcanbeviewedastheaverageofasetofindependentbinomial(binary-valued)trials,meaning\nthatwecanuseastandardstatisticalsigniﬁcancetesttoconﬁrm\nthatcertainvariationsinvalues forthismeasurearisefromgen-uine differences in performance, rather than random variations\nin the measure. Our estimate o ft h ebest possible ﬁrst place\nagreementwiththesurveydatawas53.5%.\n5.3 Evaluationdatabase\nIn order to conduct experiments we have compiled a large\ndataset from audio and Web sources. The dataset covers 400artists chosen to have the maximal overlap of the user collec-\ntion (OpenNap) and playlist (Art of the Mix) data. We had\npreviously purchased audio corresponding to the most popular\nOpenNapartists and had also used these artists to construct the\nsurvey data. For each artist, our database contains audio, sur-\nveyresponses, expert opinions from All Music Guide, playlist\ninformation,OpenNapcollec tiondata,andwebtextdata.\nThe audio data consists of 707 albums and 8772 songs for an\naverage of 22 songs per artist. Because our audio experiments\nwere conducted at two sites, a level of discipline was required\nwhen setting up the data. We shared MFCC features rather\nthan raw a udio, both to save bandwidth and to avoid copyright\nproblems. This had the added advantageof ensuring both sites\nstarted with the same features when conducting experiments.\nWe believe that this technique of establishing common feature\ncalculation tools, then sharing c ommon feature sets, could be\nusefulfor futurecross-groupcollaborationsand shouldbeseri-\nously considered by those proposing audio music evaluations,andwewouldbeinterestedinsharingourderivedfeatures. Du-\nplicated tests on a small subset of the data were used to verify\ntheequivale nceofourprocessingandscoringschemes.\nThe speciﬁc track listings for thi sd a tabase, which we refer to\nas“uspop2002”,areavailableat\nhttp://www.ee.columbia.\nedu/˜dpwe/research/musicsim/ .\n6E x periments andResults\nAnumber of experiments were conducted to answer the fol-\nlowing questions about acoustic- and subjective-based similar-\nitymeasures:\n1. Isanchorspacebetterform easuringsimilaritythanMFCC\nspace?\n2. Which method of modeling and comparing feature distri-\nbutionsisbest?\n3. Which subjective similarity measure provides the best\nground truth, e.g. in terms of agreeing best, on average,\nwiththe othermeasures?\nAlthough it risks circularity to deﬁne the best ground truth as\nthe measure which agrees best with the others, we argue that\nsince the various measures are constructed from diverse data\nsources and methods, any correlation between them should re-\nﬂecta trueunderlyingconsensusamongthepeoplewho gener-\nated the data. A measureconsistentwith allthese sourcesmustreﬂectthe‘real’groundtruth.\n6.1 Acousticsim ilaritymeasures\nWe ﬁrst comp are the acoustic-based similarity measures, ex-\namining artist models trained on MFCC and anchor space fea-tures. Each model is trained us ing features calculated from\nthe available audio for that artist. Our MFCC features are 20-\ndimensional and are computed using 32 ms frames overlappedby16ms. Theanchorspacefeatureshave14dimensionswhere\neach dimension represents the posterior probability of a pre-\nlearned acoustic class given the observedaudio as describedin\nSection3.1.\nInapreliminary experiment, we performed dimensionality re-\nduction on the MFCC space by taking the ﬁrst 14 dimensions\nof a PCA analysis and compared results with the original 20-\ndimensionalMFCCspace. Therewasnoappreciabledifferencein results, conﬁrming that any di fference between the anchor-\nbased and MFCC-based models is not due to the difference in\ndimensionality.\nTable 1 shows res ults for similarity measures based on MFCC\nspace, in which we compare the effect of varying the distri-\nbutionmodels and the distribution similarity method. For the\nGMMdistributionmodels,wevarythenumberofmixtures,usepooled or independent variance models, and train using either\nplainK-means,orK-meansfollo wedbyEMre-estimation. Dis-\ntributions are c ompared using centroid distance, ALA or EMD\n(as described in section 3.2). We also compare the effect of\nincluding or excluding the ﬁrst cepstral coefﬁcient, c\n0,w h i c h\nmeasures the overall intensity of a signa l. Table 1 shows the\naverage response rank and ﬁrst place agreementpercentagefor\neachapproach.\nFrom this table, we se et h a tthe different training techniques\nfor GMMs give comparable performance and that more mix-ture components help up to a point. Pooling the data to train\nthe covariance matrices is useful as has been shown in speech\nrecognition since it allows for more robust covariance parame-\nter estimates. Omitting the ﬁrst cepstral coefﬁcient givesbetter\nresults, possibly because similar ity is more related to spectral\nshape than overall signal energy, although this improvementis\nlesspronounced when pooled covariances are used. The best\nsystem is one which uses pooled covariances and ignores c\n0.\nModels trained with the simpler K-means procedure appear to\nsuffernoloss, andt husarepreferred.\nAsimilar table was constructed for anchor-space-based meth-\nods, which revealed that full, independentcovariance using all\n14 dimensions was the best-performing method. Curiously,whiletheALA distance measure performed poorly on MFCC-\nbasedmodels,itperformedcompetitivelywith EMDonanchor\nspace models. We are still investigatingthe cause; perhapsit isbecause the assumptions behind the asymptotic likelihood ap-\nproximationdonotholdinMFCCspace.\nThe comparison of the best-performing MFCC and anchor\nspacemodelsisshowninTable2. Weseethatbothhavesimilarperformanceunder these metrics, despite the prior information\nencodedin theanchors.\n6.2 Comparinggro undtruthmeasures\nNow we turn to a comp arison of the acoustic and subjective\nmeasures. We takethebest-pe rformingapproachesineachfea-\nture space class (MFCC and anchor space, limiting both to 16\nGMM components for parity) and evaluate them against eachof the subjectivemeasures. At thesame time, we evaluateeach\nof the subjective measures against each other. The results are\npresentedinTable3. Rowsrepres entsimilaritymeasuresbeing\nevaluated,andthecolumnsgiveresultstreatingeachofourﬁve\nsubjective similarity metrics as ground truth. Top-N ranking\nagreementScoresarecomputed asdescribedinsection5.1.The mean down each column, excluding the self-reference di-\nagonal,are also shown(denoted“mean*”). The columnmeans\ncanbetakenasameasureofhowwelleachmeasureapproaches\ngroundtruthbyagreeingwithallthedata. Bythisstandard,thesurvey-derivedsimilaritymatrixis best,butitsverysparsecov-\nerage makes it less useful. The user collection (opennap) data\nhas the second-highest “mean*”, i ncluding particularly high\nagreement with the survey met ric, as can be seen when the\ntop-N ranking agreements are p lotted as an image in ﬁgure 1.\nThus, we consider the user collections as the best source of a\nground truth similarity matrix based on this evidence, with the\nsurvey(and hence the ﬁrst place ag reement metric) also pro-\nvidingreliabledata. (Interestingly,the collectiondatadoesless\nwell agreeing with the survey data when measured by the ﬁrst\nplace agreement percentage; we i nfer that it is doing better at\nmatchingfurtherdowntherankings).\nWe menti onedthatakeyadvantageofthe ﬁrstplaceagreement\nmeasurewasthatitallowedtheus eofestab lishedstatisticalsig-\nniﬁcancetests. Usingaone-tailedtestunderabinomialassump-tion,ﬁrstplaceagreementsdifferingbymorethanabout1%are\nsigniﬁcant at the 5% level for this data (10,884 trials). Thus\nall the subjective measures are s howing signiﬁcantly different\nresults, although differences among the variants in modeling\nschemesfromtables1and2areattheedgeofsigniﬁcance.\n7C o n c l u s i o ns and Future Work\nReturning to the three questi ons posed in the previous section,\nbasedontheresultsjustshown,we conclude:\n1. MFCCandanchorspaceachieve comparableresultsonthe\nsurveydata.\n2. K-means training is comparable to EM training. Us-\ningpooled, diagonal covariance matrices is beneﬁcial for\nMFCC space, but in general the best modeling scheme\nand comparison method depend on the feature space be-ingmodeled.\n3. The measure derived from co-occurrencein personal mu-\nsiccollections is the most useful ground truth, although\nsome way of com bining the information from different\nsource warrantsinvestigations ince theyare p rovidingdif-\nferentinformation.\nThe work covered by this paper suggests many directions for\nfuture research. Although the acoustic measures achieved re-\nspectable performance, there is still much room for improve-ment. Oneglaringweaknessofourcurrentfeaturesistheirfail-\nure to capture any temporal structure information, although it\nis interesting to see how far we can get based on this limitedrepresentation.\nBasedonourcross-s iteexperience,wefeelthatthisworkpoints\nthe way to practical music similarity system evaluations that\ncan even be carried out on the same database, and that theserious obstacles to sharing or distributing large music col-\nlections can be av oided by transferring only derived features\n(which should also reduce bandwidth requirements). To thisend, we haveset up aweb site givingfulldetails of our ground\ntruth and evaluation data,\nhttp://www.ee.columbia.edu/\n˜dpwe/research/musicsim/ .W ewillalsosharetheMFCCfeatures for the 8772 tracks we used in this work by burning\nDVDs to send to interested researchers. We are also interested\ninproposals for other features that it would be valuable to cal-\nculateforthisdataset.\nAcknowledgments\nWeare grateful for support for this work received from NEC\nLaboratories America, Inc. We also thank the anonymous re-\nviewersfortheirusefulcomments.\nMuch of the contentof this paper also appearsin ourwhite pa-\nperpresentedattheWorkshopontheEvaluationofMusicInfor-\nmation Retrieval (MIR) Systems at SIGIR-03, Ottawa, August2003.\nReferences\nAucouturier,J.-J. and Pachet, F. (2002). Music similarity mea-\nsures: What’s the use? In International Symposium on Music\nInformationRetrieval .\nBerenzweig, A., Ellis, D. P. W., and Lawrence, S. (2003). An-\nchorspaceforclassiﬁcationandsimilaritymeasurementofmu-sic. InICME2003 .\nBlum, T. L., Keislar, D. F., W heaton, J. A., and Wold, E. H.\n(1999).Method and article of manufacture for content-based\nanalysis,storage,retrieval,andsegmentationofaudioinforma-tion.U.S.Patent5,918,223.\nBreese, J. S., Heckerman, D., and Kadie, C. (1998). Empiri-\ncal analysis of predictive algorithms for collaborative ﬁltering.\nInFourteenth Annual Conference on Uncertainty in Artiﬁcial\nIntelligence ,pages43–52.\nCohen, W. W. and Fan, W. (2000). Web-collaborativeﬁltering:\nrecommendingmusicbycrawlingtheweb. WWW9 /Computer\nNetworks ,33(1-6):685–698.\nEllis, D. P., Whitman, B., Berenzweig, A., and Lawrence, S.\n(2002). Thequestforgroundtruthinmusicalartistsimilarity.\nFoote,J.T.(1997). Content-basedretrievalofmusicandaudio.\nInSPIE,pages138–147.\nGhias,A.,Logan, J., Chamberlin, D., and Smith, B. (1995).\nQueryby humming. In ACMMultimedia.\nLogan,B.(2000). Melfrequencycepstralcoefﬁcientsformusic\nmodeling. In International Symposium on Music Information\nRetrieval.\nLogan,B.andSalomon,A.(2001). Amusicsimilarityfunction\nbasedonsignalanalysis. In ICME2001 ,T okyo,Japan.\nRubner,Y.,Tomasi,C.,andGuibas,L.(1998).Ametricfordis-\ntributionswithapplicationstoimagedatabases. In Proc.ICCV.\nTzanetakis, G. (2002). Manipulation, Analysis, and Retrieval\nSystemsforAudioSignals .P h Dt h e s is, PrincetonUniversity.\nVasconcelos,N.(2001). Onthecomplexityofprobabilisticim-\nageretrieval. In ICCV’01.V ancouver.\nWhitman, B. and Lawrence, S. (2002). Inferring descriptions\nand similarity for music from community metadata. In Pro-\nceedings of the 2002 Internati onal Computer Music Confer-\nence.Sweden.Independent\n Pooled\n#mix\nc0?\n ALA\n EMD\n ALA\n Cntrd\n EMD\nEM\n 8\ny\n4.76 /16%\n 4.46 /20%\n 4.72 / 17%\n 4.66 / 20%\n 4.30 / 21%\n8\nn\n -\n4.37 /22%\n -\n -\n4.23 / 22%\n16\nn\n -\n4.37 /22%\n -\n -\n4.21 / 21%\nK-means\n 8\ny\n -\n4.64 /18%\n -\n -\n4.30 / 22%\n8\nn\n4.70 /16%\n 4.30 /22%\n 4.76 / 17%\n 4.37 / 20%\n 4.28 / 21%\n16\ny\n -\n4.75 /18%\n -\n -\n4.25 / 22%\n16\nn\n4.58 /18%\n 4.25 /22%\n 4.75 / 17%\n 4.37 / 20%\n 4.20 / 22%\n32\nn\n -\n -\n4.73 / 17%\n 4.37 / 20%\n 4.15 / 23%\n64\nn\n -\n -\n4.73 / 17%\n 4.37 / 20%\n 4.14 / 23%\nOptimal\n 2.13 / 53.5%\nRandom\n 5.50 / 11.4%\nTable 1: Averageresponse rank / ﬁrst place agreementpercentagefor varioussimilarity schemes based on MFCC features. Lower\nvaluesarebetterforaverageresponserank,andlargerpercentagesarebetterforﬁrstplaceagreement.\nMFCC\n Anchor\n#mix\n EMD\n ALA\n8\n4.28 / 21.3%\n 4.25 / 20.2%\n16\n4.20 / 22.2%\n 4.20 / 19.8%\nTable2: Best-in-classcomparisonofanchorvs. MFCC-basedmeasures(averageresponserank/ﬁrstplaceagreementpercentage).\nMFCC system uses K-means training, pooled diagonal covariance matrices, and excludes c0. Anchor space system uses EMtraining,independentfullcovariancematrices,andincludesc0.\n1st place\n survey\n expert\nplaylist\ncollection\nwebtext\nRandom\n 11.8%\n 0.015 0.020 0.015 0.017 0.012\nAnchor\n 19.8%\n 0.092 0.095 0.117 0.097 0.041\nMFCC\n 22.2%\n 0.112 0.099 0.142 0.116 0.046\nSurvey\n 53.5%\n 0.874 0.249 0.204 0.331 0.121\nExpert\n 27.9%\n 0.267\n 0.710 0.193 0.182 0.077\nPlaylist\n 26.5%\n 0.222 0.186\n 0.985 0.226 0.075\nCollection\n 23.2%\n 0.355 0.179 0.224\n 0.993 0.083\nWebtext\n 18.5%\n 0.131 0.082 0.077 0.087\n 0.997\nmean*\n 0.197 0.148 0.160 0.173 0.074\nTable 3: First place agreementpercentages(with surveydata) and top-Nrankingagreementscore s(againsteach candidate ground\ntruth) for acoustic and subjective similar ity measures. “mean*” is the mean of each gr ound-truth column, ex cluding the shaded\n“cheating”diagonalandthe“random”row.\nsrv exp ply clc wtxrnd\nank\nmfc\nsrv\nexp\nply\nclc\nwtx\nmn*\n0.20.40.60.8\nFigure1: Top-Nrankingagreementscoresfro mtable3p lottedasagrayscaleimage."
    },
    {
        "title": "Determining context-defining windows: Pitch spelling using the spiral array.",
        "author": [
            "Elaine Chew",
            "Yun-Ching Chen"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1418037",
        "url": "https://doi.org/10.5281/zenodo.1418037",
        "ee": "https://zenodo.org/records/1418037/files/ChewC03.pdf",
        "abstract": "This paper presents algorithms for pitch spelling us- ing the Spiral Array model. Accurate pitch spelling, assigning contextually consistent letter names to pitch numbers (for example, MIDI), is a critical component of music transcription and analysis systems. The local context is found to be more important than the global, but a combination of both achieves the best results. Keywords: pitch spelling, music analysis, algorithm design. 1 Pitch Spelling Pitch spelling is a critical first step in any content-based mu- sic processing system. This paper presents three real-time pitch spelling algorithms based on context-defining windows. The al- gorithms summarize music information in windows of varying sizes to determining local and long-term tonal contexts using the Spiral Array model (Chew, 2000). The Spiral Array is a ge- ometric model for tonality that clusters closely-related pitches and summarizes note content as spatial points in the interior of the structure. These interior points, called centers of effect, serve as proxies for the key context in pitch spelling. The ap- propriate letter name is assigned to each pitch through a nearest neighbor search in the Spiral Array space. The problem of pitch spelling is an artefact of equal tempera- ment tuning in western tonal music −several pitches are ap- proximated by the same frequency (these pitches are said to be enharmonically equivalent). In a MIDI file, enharmonically equivalent pitches are represented by the same numerical value indicating its frequency and not its letter name. The name of the pitch determines the notation and serves as a clue to the key context. The local key context determines the pitch spelling in a ∗Funded in part by the Integrated Media Systems Center, an NSF ERC, Cooperative Agreement No. EEC-9529152. Any Opinions, find- ings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the Na- tional Science Foundation. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advan- tage and that copies bear this notice and the full citation on the first page. c⃝2003 Johns Hopkins University. musical excerpt. The problem of pitch spelling is to assign ap- propriate pitch names that are consistent with the key context. 2 The Algorithms Each algorithm in this section generates pitch spelling assign- ments based on tonal contexts of varying ranges. The notes in each window generate a center of effect (CE) in the Spiral Ar- ray. Suppose the music is divided into chunks. The CE for chunks a through b is given by ca,b = Pb i=a P j pij.dij/Da,b where (pij, dij) are the pitch and duration of the j-th note in the i-th chunk and Da,b = Pb i=a P j dij. We use the evolving CE as a proxy for the key context. A cu- mulative window generates a CE that represents the long-term tonal context. A sliding window CE represents the local tonal context. For the ij-th note, suppose that indexij is the spelling whose index in the Spiral Array is closest to 0. This choice will bias the notation towards fewer sharps (♯)and flats (♭). The most probable pitch name assignments are then given by the triplet: Iij = {indexij −12, indexij, indexij + 12}. Of the three plausible indices, we choose the one corresponding to a pitch position that is closest to the CE.",
        "zenodo_id": 1418037,
        "dblp_key": "conf/ismir/ChewC03",
        "keywords": [
            "pitch spelling",
            "music transcription",
            "algorithm design",
            "Spiral Array model",
            "context-defining windows",
            "local and long-term tonal contexts",
            "enharmonically equivalent pitches",
            "nearest neighbor search",
            "MIDI file",
            "key context"
        ],
        "content": "Determining Context-Deﬁning Windows: PitchSpelling usin g the Spiral Array\nElaine Chew∗and Yun-Ching Chen\nIntegratedMediaSystemsCenterandD.J. EpsteinDepofIndu strial&SystemsEngineering\nUniversityofSouthernCalifornia,LosAngeles,Californi a,USA.\n[echew,yunchinc]@usc.edu\nAbstract\nThis paper presents algorithms for pitch spelling us-\ning the Spiral Array model. Accurate pitch spelling,\nassigningcontextuallyconsistentletternamestopitch\nnumbers(forexample,MIDI),isacriticalcomponent\nofmusictranscriptionandanalysissystems. Thelocal\ncontextisfoundtobemoreimportantthantheglobal,\nbuta combinationofbothachievesthebest results.\nKeywords: pitchspelling,musicanalysis,algorithmdesig n.\n1 Pitch Spelling\nPitch spelling is a critical ﬁrst step in any content-based m u-\nsicprocessingsystem. Thispaperpresentsthreereal-time pitch\nspellingalgorithmsbasedoncontext-deﬁningwindows. The al-\ngorithms summarize music information in windows of varying\nsizes to determining local and long-term tonal contexts usi ng\ntheSpiralArray model(Chew,2000). TheSpiralArrayisage-\nometric model for tonality that clusters closely-related p itches\nand summarizes note content as spatial points in the interio r\nof the structure. These interior points, called centers of effect ,\nserve as proxies for the key context in pitch spelling. The ap -\npropriateletternameisassignedtoeachpitchthroughanea rest\nneighborsearchintheSpiralArrayspace.\nThe problem of pitch spelling is an artefact of equal tempera -\nment tuning in western tonal music −several pitches are ap-\nproximated by the same frequency (these pitches are said to\nbe enharmonicallyequivalent). In a MIDI ﬁle, enharmonical ly\nequivalentpitchesare representedby the same numericalva lue\nindicating its frequency and not its letter name. The name of\nthepitchdeterminesthenotationandservesasacluetothek ey\ncontext. Thelocalkeycontextdeterminesthepitchspellin gina\n∗Funded in part by the Integrated Media Systems Center, an NSF\nERC,CooperativeAgreementNo. EEC-9529152. AnyOpinions, ﬁnd-\nings and conclusions or recommendations expressed in this m aterial\nare those of the authors and do not necessarily reﬂect those o f the Na-\ntional Science Foundation.\nPermission to make digital or hard copies of all or part of thi s work\nfor personal or classroom use is granted without fee provide d that\ncopies are not made or distributed for proﬁt or commercial ad van-\ntage and that copies bear this notice and the full citation on the ﬁrst\npage.c/circlecopyrt2003 Johns Hopkins University.musical excerpt. The problem of pitch spelling is to assign a p-\npropriatepitchnamesthatare consistentwith thekeyconte xt.\n2 The Algorithms\nEach algorithm in this section generates pitch spelling ass ign-\nments based on tonal contexts of varying ranges. The notes in\neach window generate a center of effect (CE) in the Spiral Ar-\nray. Suppose the music is divided into chunks. The CE for\nchunks athrough bis given by ca,b=/summationtextb\ni=a/summationtext\njpij.dij/Da,b\nwhere (pij, dij)arethepitchanddurationofthe j-thnoteinthe\ni-thchunkand Da,b=/summationtextb\ni=a/summationtext\njdij.\nWe use the evolving CE as a proxy for the key context. A cu-\nmulative window generates a CE that represents the long-ter m\ntonal context. A sliding window CE represents the local tona l\ncontext. For the ij-thnote, supposethat index ijisthe spelling\nwhoseindexintheSpiralArrayisclosestto0. Thischoicewi ll\nbiasthenotationtowardsfewersharps( /sharp)andﬂats( /flat). Themost\nprobable pitch name assignments are then given by the triple t:\nIij={index ij−12, index ij, index ij+ 12 }. Of the three\nplausible indices, we choose the one corresponding to a pitc h\npositionthat isclosestto theCE.\n2.1 Algorithm1: Cumulative CE\nOur original algorithm (Chew & Chen, 2003) used a cumula-\ntivewindowtosummarizethetonalcontext. Thealgorithmad -\nvanced one chunk at a time. At time t, we examine and assign\npitchnamestothenotesinthe t-thchunk. Thenotesinthepre-\nvioust−1chunksareusedtogenerateaCE: ˆct=c0,t−1. This\nmethod had an error rate of 1/1375 (that is to say, 99.93% cor-\nrect) for Beethoven’s Piano Sonata Op.79 Mvt.3 and 73/1516\n(95.18%correct)in themorecomplexSonataOp.109Mvt.1.\n2.2 Algorithm2: Sliding Window\nWe propose a sliding window algorithm that is more sensitive\nto changes in local tonal contexts. Figure 1 shows the slidin g\nwindow method for a window of size 4. For a window of size\nw, the CE is deﬁned as: ˆct=ct−w,t−1. The sliding window\nmethod eliminated several of the spelling errors in the orig inal\nalgorithm. However, it was not able to detect quickly enough\nthe sudden changes to distant keys in the exposition and the\nrecapitulationoftheBeethovenOp.109Mvt.1.\n2.3 Algorithm3: Two-phase AssignmentMethod\nWe propose another sliding window method where the algo-\nrithm is allowed to re-visit previousdecisions in a second w in-\u0000\u0001\u0000\u0001\n\u0002\u0003\u0003\n\u0003\u0003\u0004\u0004\n\u0004\n\u0005\u0004\u0004\u0004\u0005\u0004\u0004\n\u0004\n\u0005\u0004\u0004\u0004\u0005\u0004\u0004\n\u0004\n\u0005\u0004\u0004\u0004\u0005\n\u0004\u0004\n\u0004\n\u0004\u0005\n\u0004\u0004\n\u0004\n\u0004\u0005\n\u0006\u0007\n\u0003\u0003\n\u0003\u0003\b\n\u0004\n\u0004\b\n\u0004\u0004\n\b\u0004\n\u0004\b\n\u0004\u0004\n\b\u0004\n\u0004\b\n\u0004\u0004\n\b\u0004\n\u0004\n\b\n\u0004\u0004\n\u0006\t\n \u000b \f \u000b \r \u000e\n\u000f\u000b\n\u0010\u0011\n\u0012\u0011\n\u0013\t\u0014 \u0015 \u000b\n\u0016 \u0016\u0013\u0000\u0001\u0000\u0001\n\u0002\u0003\u0003\n\u0003\u0003\u0004\u0004\n\u0004\n\u0005\u0004\u0004\u0004\u0005\u0004\u0004\n\u0004\n\u0005\u0004\u0004\u0004\u0005\u0004\u0004\n\u0004\n\u0005\u0004\u0004\u0004\u0005\n\u0004\u0004\n\u0004\n\u0004\u0005\n\u0004\u0004\n\u0004\n\u0004\u0005\n\u0006\u0007\n\u0003\u0003\n\u0003\u0003\b\n\u0004\n\u0004\b\n\u0004\u0004\n\b\u0004\n\u0004\b\n\u0004\u0004\n\b\u0004\n\u0004\b\n\u0004\u0004\n\b\u0004\n\u0004\n\b\n\u0004\u0004\n\u0006\t\u0014 \u0015 \u000b\n\u0016 \u0016\u0013\n\u0017\u000f\u000b \u0015\u0011\n\u0018\u0017\u000f\u000b \u0015\u0011\n\u0019\t\n \u000b \f \u000b \r \u000e\n\u000f\u000b\n\u0010\u0011\n\u0012\u0011\n\u0013\nFigure1: SlidingWindow( w= 4).\u001a\u001b\u001a\u001b\n\u001c\u001d\u001d\n\u001d\u001d\u001e\u001e\n\u001e\n\u001f\u001e\u001e\u001e\u001f\u001e\u001e\n\u001e\n\u001f\u001e\u001e\u001e\u001f\u001e\u001e\n\u001e\n\u001f\u001e\u001e\u001e\u001f\n\u001e\u001e\n\u001e\n\u001e\u001f\n\u001e\u001e\n\u001e\n\u001e\u001f\n !\n\u001d\u001d\n\u001d\u001d\"\n\u001e\n\u001e\"\n\u001e\u001e\n\"\u001e\n\u001e\"\n\u001e\u001e\n\"\u001e\n\u001e\"\n\u001e\u001e\n\"\u001e\n\u001e\n\"\n\u001e\u001e\n #$ % & % ' (\n)%\n*+\n,+\n-#. / %\n0 0-\u001a\u001b\u001a\u001b\n\u001c\u001d\u001d\n\u001d\u001d\u001e\u001e\n\u001e\n\u001f\u001e\u001e\u001e\u001f\u001e\u001e\n\u001e\n\u001f\u001e\u001e\u001e\u001f\u001e\u001e\n\u001e\n\u001f\u001e\u001e\u001e\u001f\n\u001e\u001e\n\u001e\n\u001e\u001f\n\u001e\u001e\n\u001e\n\u001e\u001f\n !\n\u001d\u001d\n\u001d\u001d\"\n\u001e\n\u001e\"\n\u001e\u001e\n\"\u001e\n\u001e\"\n\u001e\u001e\n\"\u001e\n\u001e\"\n\u001e\u001e\n\"\u001e\n\u001e\n\"\n\u001e\u001e\n #. / %\n0 0-\n1)% /+\n21)% /+\n3#$ % & % ' (\n)%\n*+\n,+\n-\n#. %\n0\n45\n6% 5\n7-#. %\n0\n45\n6% 5\n7-\nFigure2: Two-phaseassignmentmethod( w= 4, wr= 2).\ndow. The additional step ensures local consistency in spell ing\nand heightens the sensitivity to abrupt key changes. Figure 2\ndepicts this new procedure. After assigning pitch names as b e-\nfore, the two-phase assignment method re-visits the notes i n a\nsecond window (denotedby dotted lines) that includesthe cu r-\nrentchunk. Bothwindowsadvancesimultaneouslyovertime.\nPhase 1: The deﬁnition of the CE and the ﬁrst pass at pitch\nnameassignmentsproceedasinthepreviousalgorithms.\nPhase 2: Let the second local window be of size wr, call\nthis the self-referential window. A second CE is generated:\n˜ct=f·ct−wr,t+ (1 −f)·c1,t, where fis the weight of the\nlocal context relative to the global tonal context. Spellin g as-\nsignmentsinthedottedboxarere-visitedandre-assignedw hen\nnecessaryto makethemconsistentwith thissecondCE.\n2.4 OtherAlgorithms\nPriortothetwo-phasemethoddescribedinSection2.3,wetr ied\natwo-phasemethodwithoutthecumulativeCE.Thisalgorith m\ndidnottakeintoaccounttheglobalkeycontext. Theresultw as\nthat an erroneous spelling upon a return to the original tona l\ncontextcan tip the spelling fromonewith mostly sharpsto on e\nwith manyﬂats. Theresult maybelocallyconsistent,but glo b-\nallydisastrous. Inthiscase,asmallperturbationcanseta patho-\nlogicalcourseforerror-ﬁlledspelling.\nWealsotestedadynamicwindowalgorithminspiredbythehy-\npothesisthatagrowingdistancefromtheclosestkeyporten dsa\nkey change and decreasing distance indicates stability. Th e al-\ngorithmcanbesummarizedasfollows: whenthedistancefrom\nthe CE to the closest key exceeds fd(some fraction fof the\nminimum distance between any two keys d) by a given thresh-old,w=1\n2w; when it is less than fdby the same amount,\nw= 2w. Because theSpiralArrayisconﬁguredin a3D space,\nthereislittle correlationbetweenthe secondordereffect ofEu-\nclidean distance and changing keys. This algorithm was no\nmoreeffectivethantheslidingwindowalgorithm.\n3 Results and Conclusions\nThe algorithms were tested on the ﬁrst movement of\nBeethoven’s PianoSonataNo.30inEMajor ,Op.109. Thislate\nBeethovensonatacontainsmanysuddenkeychangesandposes\na challengeto pitch spellingalgorithms. Thecomputationa lre-\nsultsareshowninFigure3. Theslidingwindowalgorithmcon -\nsistentlyproducesbetterresultsthanthecumulativeCEme thod.\nThebestresultsareshownforwindowsize4,with31errorsou t\nof 1516. The two-phase assignment method outperforms the\nsliding window algorithm. For window size 4, using wr= 3\nandf= 0.8or 0.7, we get only 27 errors. The same result\nis reported for window size 8, using wr= 6andf= 0.9.\nFor window size 16, the best results (30 errors) are seen when\nwr= 6andf= 0.8.\nMethod Parameters Errors (1516 notes) % Correct \nCumulative  73 95.18 \n4 31 98.00 \n8 47 96.90 Sliding Window \n(w)16 40 97.36 \n4 2 0.6 28 98.15 \n4 3 0.8 27 98.22\n4 3 0.7 27 98.22\n8 2 0.9 31 97.96 \n8 4 0.9 40 97.36 \n8 6 0.9 27 98.22\n16 4 0.8 40 97.36 \n16 6 0.8 30 98.02Two-Phase\n(w, w r, f) \n16 8 0.9 37 97.56 \nFigure3: ComputationalResults.\nWe conclude that the local context is more important than the\nglobal context in pitch spelling. However, we achieve the lo w-\nest errorrateswhenwecombinebothshort-termandlong-ter m\ntonal contexts. Hence, pitch spelling is a functionof both l ocal\nandglobaltonalcontexts.\nFuture work includes developing a database suitable for pit ch\nspellingbenchmarkpurposesandtestingthealgorithmsaga inst\nthosebyCambouropoulos(2001)andMeredith(2003).\nReferences\nE. Cambouropoulos. Automatic pitch spelling: From numbers\ntosharpsandﬂats. In ProceedingsoftheVIIIBraziliansympo-\nsium onComputerMusic , Fortaleza,Brazil,2001.\nE. Chew. Towards a Mathematical Model of Tonality . PhD\nthesis, MIT,Cambridge,MA,2000.\nE. Chew. Modeling tonality: Applications to music cognitio n.\nInThe 23rd Annual Meeting of the Cognitive Science Society ,\nEdinburgh,Scotland,August2001.\nE. Chew and Y.-C. Chen. Mapping MIDI to the Spiral Array:\nDisambiguating Pitch Spellings. In The 8th INFORMS Com-\nputerSocietyConference(ICS) ,Chandler,AZ,2003.\nD. Meredith. Pitch spelling algorithms. In Proceedings of the\n5thTriennialESCOM Conference ,Hanover,Germany,2003."
    },
    {
        "title": "The MUSART testbed for query-by-humming evaluation.",
        "author": [
            "Roger B. Dannenberg",
            "William P. Birmingham",
            "George Tzanetakis",
            "Colin Meek",
            "Ning Hu",
            "Bryan Pardo"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1415978",
        "url": "https://doi.org/10.5281/zenodo.1415978",
        "ee": "https://zenodo.org/records/1415978/files/DannenbergBTMHP03.pdf",
        "abstract": "Evaluating music information retrieval systems is acknowledged to be a difficult problem. We have created a database and a software testbed for the systematic evaluation of various query-by-humming (QBH) search systems. As might be expected, different queries and different databases lead to wide variations in observed search precision. “Natural” queries from two sources led to lower performance than that typically reported in the QBH literature. These results point out the importance of careful measurement and objective comparisons to study retrieval algorithms. This study compares search algorithms based on note-interval matching with dynamic programming, fixed-frame melodic contour matching with dynamic time warping, and a hidden Markov model. An examination of scaling trends is encouraging: precision falls off very slowly as the database size increases. This trend is simple to compute and could be useful to predict performance on larger databases. 1",
        "zenodo_id": 1415978,
        "dblp_key": "conf/ismir/DannenbergBTMHP03",
        "keywords": [
            "music information retrieval systems",
            "query-by-humming (QBH)",
            "database",
            "software testbed",
            "systematic evaluation",
            "query-by-humming search systems",
            "different queries",
            "different databases",
            "search precision",
            "search algorithms"
        ],
        "content": "The M USART  Testbed for Query-By-Humming Evaluation \nRoger B. Dannenberg, William P. Birmingham, George Tzanetakis, Colin Meek, Ning Hu, Bryan Pardo \nSchool of Computer Science \nCarnegie Mellon University \nPittsburgh, PA 15213 USA \n+1-412-268-3827 \nrbd@cs.cmu.edu \n Department of Electrical Engineering and  \nComputer Science \nUniversity of Michigan \nAnn Arbor, MI 48109-2110 \n+1-734-936-1590  \nwpb@eecs.umich.edu \n \nAbstract \nEvaluating music information retrieval systems is \nacknowledged to be a difficult problem. We have \ncreated a database and a software testbed for the \nsystematic evaluation of various query-by-humming \n(QBH) search systems. As might be expected, \ndifferent queries and different databases lead to w ide \nvariations in observed search precision. “Natural” \nqueries from two sources led to lower performance \nthan that typically reported in the QBH literature.  \nThese results point out the importance of careful \nmeasurement and objective comparisons to study \nretrieval algorithms. This study compares search \nalgorithms based on note-interval matching with \ndynamic programming, fixed-frame melodic contour \nmatching with dynamic time warping, and a hidden \nMarkov model. An examination of scaling trends is \nencouraging: precision falls off very slowly as the  \ndatabase size increases. This trend is simple to \ncompute and could be useful to predict performance \non larger databases. \n1 Introduction \nThe M USART project is a collaboration between the University \nof Michigan and Carnegie Mellon University. Togethe r, we \nhave been exploring the design of query-by-humming systems \n(Birmingham, et al., 2001; Hu & Dannenberg, 2002; M eek & \nBirmingham, 2002; Pardo & Birmingham, 2002; Shifrin , et al., \n2002). We have developed a variety of algorithms ba sed on \nMarkov models, hidden Markov models, and contour \nmatching. In addition, we have implemented several versions \nof note sequence matching algorithms using dynamic \nprogramming. \nAs our research progressed, it became expedient for  project \nmembers to adopt their own data and methods. As we \ndeveloped and implemented search algorithms, we als o \ncreated new signal-analysis software, collected new  queries, added files to our databases, and improved our them e-\nextraction software. With so many variables, it was  simplest to \nhold constant a collection of data and programs in order to \nfocus on one or two experimental variables. \nAfter following these procedures for a year or two,  we found it \nincreasingly difficult to compare systems. They had  simply \nbecome incompatible. We feel that this state of aff airs in our \nmicrocosm mirrors the state of the field in general . (Downie, \n2002; Futrelle & Downie, 2002)  Many results are pu blished \n(Ghias, et al., 1995; McNab, et al., 1996; Pauws, 2 002), but \nevaluation is difficult, and results are not compar able. \nTo remedy this situation, at least in our own resea rch project, \nwe created a general testbed that is capable of hos ting all our \nwork on content-based retrieval. The testbed includ es \ncollections of queries, target data, analysis softw are, and \nsearch algorithms. We have integrated several of ou r research \nsystems into this testbed and are able to compare t he systems \nobjectively. Some of our data can be shared, and we  can also \nevaluate algorithms for other researchers using our  testbed. \nIn the next section, we describe the architecture o f our testbed. \nThen, in Section 3, we describe three different sea rch systems \nwe have studied. In Section 4 we present some resul ts of our \nalgorithm comparisons. Section 5 discusses the gene ral \nsources of error we observed. In Section 6, we disc uss the \nissue of search performance as we scale to larger d atabases. \nSection 7 presents some discussion and conclusions.  \n2 The Testbed Architecture \nThe M USART testbed is hosted on a Linux server and relies on \nscripts written in Python to conduct experiments. T he use of \nPython makes it easily portable to other operating systems. \nOur goal is that complete tests should run from sta rt to finish \nwithout manual intervention. A typical test starts with a \ncollection of audio queries, a database of target M IDI files, \nand a variety of programs to process audio, process  MIDI, and \nsearch the database. The output of a test includes statistical \ninformation about the search results in text and gr aphical plot \nformats. All input and output data can be viewed us ing a web \nbrowser so that researchers (currently in Pennsylva nia, \nMichigan, and Washington) can have convenient acces s to all \nresults from all tests. \nIn order to support different systems, including va rious \npreprocessing stages, we adopted the model shown in  Figure 1. Permission to make digital or hard copies of all or  part of this work for \npersonal of classroom use is granted without fee pr ovided that copies are not \nmade or distributed for profit or commercial advant age and that copies bear \nthis notice and the full citation on the first page .   2003 The Johns Hopkins \nUniversity. In this model, the input to the system consists of “queries” \n(generally an audio recording of someone singing a melody) \nand “targets” (generally MIDI files to be searched) . We have a \nnumber of collections of queries and targets, which  we store in \na hierarchical directory structure. For any given t est run, we \ndescribe the queries and targets of interest as lis ts of filenames. \nThis allows us to reproduce our results even if new  files are \nadded to the database. \nQueries Targets \nQuery \nRepr.Target \nRepr.\nSearch \nResults preprocess preprocess \nCompare Query \nCollections \n(lists) Target \nCollections \n(lists) \nSearch \nAlgorithms Query-To-Target \nAnswer Key \n \nFigure 1: Architecture of the Musart Testbed. \nIn addition to queries and targets, we have interme diate \nrepresentations. For example, most search systems c onvert \nqueries to transcriptions stored as MIDI files or t o pitch \ncontours stored as data in text files. We usually u se the \n“Thematic Extractor” to obtain themes from target M IDI files \nand search the themes rather than the full MIDI tar get file. Our \nscripts will automatically generate these intermedi ate \nrepresentations if possible. It is also possible to  import \nintermediate representations as files when their co nstruction is \nnot fully automated. \nOur system needs the correct target(s) for each que ry to \nevaluate search performance. We keep a file for eac h query \nthat lists all the correct targets, since there may  be several \nversions of a song in the database, appearing as se veral \ndifferent targets. When reporting rank order, we re port the \nlowest ranking correct target. \nTests of search systems are saved in a “results” di rectory. \nSearch programs take one query and a list of target s, and \ngenerate “match scores” indicating how well the que ry \nmatches the target. The test script collects the re sults and sorts \nthem to calculate the rank order of the correct tar get. The \nscript produces an easy-to-parse text output summar y for \nfurther analysis. The output includes: \n• The name of the queries collection file \n• The name of the targets collection file \n• The search algorithm and any command line options u sed \n• The query preprocessor and any command line options  \nused \n• For each query: the match score for each target and  the \nrank order of the correct target. \n• Statistics: mean rank, average deviation, standard \ndeviation, and a histogram of ranks. The output format assumes full searches in which th e query is \ncompared to every target in the database, but it wo uld be \nrelatively simple to change this assumption and ret urn less \ninformation. \n3 Description of search systems \nThe primary goal of our testbed is to enable object ive \ncomparisons between different search methods. We ha ve \nfocused on our three best-performing algorithms. Th e first \napplies dynamic programming string-matching algorit hms to \nmatch sequences of pitch intervals and IOI ratios. The second \napplies dynamic time warping algorithms to compare melodic \ncontours. The third uses a hidden Markov model to a ccount \nfor differences between queries and targets. We rep ort results \nfrom the best configurations of our algorithms. Wit h two \nquery transcription systems, two theme finders, and  many \nvariations in the search algorithms, the space of p ossibilities is \nquite large. \n3.1  Note-Interval, Dynamic Programming Search \nThe Note-Interval system relies on a query transcri ber to \nestimate note onset times and pitches in audio quer ies. Both \ntargets and queries are then transformed into seque nces of \nnote-intervals, each of which consists of a pitch i nterval and a \nrhythmic interval. Pitch intervals are quantized to  the half-step \nand range from –12 to +12 half steps. Rhythmic inte rvals are \nrepresented as one of five log-spaced Inter Onset I nterval \nRatio (IOIr) values (Pardo & Birmingham, 2002). Thi s \nencoding is both tempo-invariant and transposition- invariant. \nOnce encoded, targets are ranked by similarity to t he query. \nSimilarity is given by the minimum cost of transfor ming a \ntarget into the query using three editing operation s: insert a \nnote-interval, delete a note-interval, and substitu te a note-\ninterval in the query for a corresponding one in th e target. \n(Pardo & Birmingham, 2002; Pardo, Birmingham, & Shi frin, \n2003) Both insertion and deletion are fixed-cost op erations. \nThe reward (or cost) of substituting a query note-i nterval for a \ntarget note-interval is based on the similarity of the note \nintervals. Reward decreases exponentially with dist ance in \neither IOIr or pitch-interval. \n3.2  Melodic-Contour, Dynamic Time Warping Search \nThe melodic-contour matcher is based on the idea th at while \npitch estimation is not too difficult, segmentation  into notes is \nvery difficult and error prone. A segmentation erro r \ncorresponds to a note insertion or deletion in note -based \napproaches, and at least in some cases this seems t o be a major \nsource of errors. In the melodic-contour approach ( Mazzoni & \nDannenberg, 2001), time is divided into equal-lengt h frames \nand the fundamental frequency of the query is estim ated in \neach frame. Similarly, the target melody is split i nto equal-\nlength frames, ignoring note boundaries. Dynamic ti me-\nwarping is used to find a good alignment of the que ry to the \ntarget. Transposition is handled by folding all pit ches into one \noctave and running each search with 24 different qu arter-step \ntranspositions. The primary difference between this  matcher \nand the Note-Interval matcher is that this one alig ns equal-\nduration frames rather than notes. Furthermore, the  contour \nrepresentation is not invariant to transposition or  tempo \nchange. 3.3  Hidden Markov Model Matching \nJohnny Can’t Sing (JCS) (Meek & Birmingham, 2002a; Meek \n& Birmingham, 2002b) is a hidden Markov model match er \nthat uses a distributed state representation to mod el both \n“cumulative” and “local” error. This means that, li ke the note-\ninterval approach, JCS explicitly models changes in  tempo \nand pitch-center, and like the melodic-contour appr oach, \nmodels errors that have a purely local effect on th e pitch and \nrhythm of the query. A note-based approach, we inco rporate \nthe notion of fragmentation and consolidation (Mong eau & \nSankoff, 1990), but the state model also supports a rbitrary \ngaps in the query and target with low probability. \n4 Results of Comparisons \nWe have conducted tests with different sets of quer ies and \ndatabases. In all of the work reported here, there were no \nspecial instructions for singers (such as singing “ ta ta ta”) and \nall targets are fully polyphonic MIDI files which a re \nautomatically processed to extract themes. The firs t set of \nqueries is relatively high in quality, meaning that  the queries \nfollow the melody and rhythm of the target song, an d the \nrecordings are of good quality (i.e., no drop outs or extraneous \nnoise). We have found in previous studies that our algorithms \nperform quite well when the queries are high qualit y. We also \ncollected new, larger sets of queries of lower qual ity, and \nfound that, with these, the search performance of a ll \nalgorithms was much worse. Below, we compare these sets of \nqueries. We then compare our three algorithms. \nAll of our algorithms return an ordered list of tar gets, from \nbest match to worst. The rank of the correct answer  within the \nlist is also computed. To summarize performance, we  count \nthe percentage of answers at rank = 1, rank ≤≤ ≤≤ 2, or rank ≤ 3. \nWe also compute the MRR (mean reciprocal rank). The  MRR \nis the average value of 1/rank, a value in the rang e 0 to 1, with \nhigher numbers indicating better performance. To si mplify \nreporting, we scale the MRR to the range 0 to 100. \n4.1  The “High-Quality” Queries \nFive queriers, two musically trained, sang controll ed excerpts \nfrom ten well-known folk songs, yielding a database  of 160 \nqueries. The HMM search system was tested against a  massive \ndatabase of 10,000 synthetically generated targets with a mean \nlength of 40 notes (plus the ten folk-song targets used in the \nqueries) in order to test scalability, given querie s collected in \nideal circumstances. The singers were – for the mos t part – \nfamiliar with the folk songs, and sang only contigu ous \nportions of those songs. Using the full HMM model, 59 out of \n80 queries (the other 80 were used for training) re turned \ncorrect targets ranked first, with an MRR value of 76. The \ndistribution of ranks is shown in Figure 2. For the  remaining \ndata sets, JCS is used with default parameters, wit h no training. \nThe point of this test is to establish that good pe rformance can \nbe obtained under reasonable conditions, namely tha t queries \nare fairly in-tune sub-sequences of the targets. In  the next \nsection, we will see that performance is highly dep endent \nupon queries and databases. This is one of the reas ons that our \ntestbed is so important for our research. 5% 9% 5% 8% \n74% 1\n2-10 \n11-100 \n101-1000 \n1001-10000 \n \nFigure 2: Distribution of ranks for the HMM search algorithm \non \"high quality queries”. \n4.2  The “Ordinary” Queries \nWe have two more collections of queries that turn o ut to be \nmore difficult than the folk song queries. Query Se t 1 was \ncollected from 10 subjects with no vocal training w ho were \npresented 10 Beatles songs. After hearing a song on ce, each \nsinger was asked to sing the “most memorable” porti on of the \npiece. No instructions were given as to whether the y should \nsing lyrics, and subjects varied in this respect. S ubjects were \nfree to try again, if they felt their first attempt  was bad for \nsome reason. In many cases, subjects made more than  one \nattempt, so there are 131 queries in all. While mos t of the \nqueries are recognizable, many of them do not corre spond \nvery well to the actual songs (as judged by the aut hors \nlistening to the queries). Subjects often skipped f rom one \nsection to another, creating melodic sequences that  do not \nexist in the actual song. It is interesting to note  that these \nfabricated sequences are often completely convincin g and do \nnot seem to confuse human listeners. Many singers h ave mild \nto severe intonation problems and many added expres sive \npitch bends to their singing, which complicates not e \nidentification. Some queries contain noise caused b y touching \nthe microphone, and some contain bits of self-consc ious \nlaughter and other sounds. \nQuery Set 2 was collected from a larger number of s ubjects. \nAs a class project, students were recruited to reco rd 10 queries \neach from volunteers, resulting in a collection of 165 usable \nqueries. These are all sung from memory and suffer from \nmany of the same problems as Query Set 1. \nA preprocessing step (Meek & Birmingham, 2001) extr acts \napproximately 11 short “themes” from each target so ng in the \ndatabase. In all of our systems, search is performe d by \ncomparing the query to each theme from a song. The \nsimilarity rating of the best match is reported as the similarity \nrating of the song. These ratings are then sorted t o compute \nthe rank order of the correct song.  \nTable 1 shows the results of running Query Set 1 ag ainst a \ncollection of 258 Beatles songs, for which there ar e a total of \n2844 themes. It can be seen that the matchers are s ignificantly \ndifferent in terms of search quality. At least with  these queries, \nit seems that better melodic similarity and error m odels give \nbetter search performance.  \nTable 2 shows the results of running Query Set 2 ag ainst a \ncollection of 868 popular songs. The total number o f themes \nin this database is 8926. All three algorithms perf ormed better \non this data than with Query Set 1, even though the re are many more themes. Unlike in Table 1, where the algo rithms \nseem to be significantly different, all three algor ithms in this \ntest have similar performance, with an MRR of about  30. The \nNote-Interval algorithm is about 100 times faster t han the \nother two, so at least in this test, it seems to be  the best, even if \nits MRR is slightly lower. \n \nSearch Algorithm = 1 ≤≤ ≤≤ 2 ≤≤ ≤≤ 3 MRR  \nNote-Interval 8.4%  12.2  13.0  13.4  \nMelodic-Contour  15.3  19.1  21.4  21.0 \nHidden Markov Model  20.6  26.7  29.0  27.0  \nTable 1: Percentage of correct targets returned at or below \nranks 1, 2 and 3, and Mean Reciprocal Rank (MRR) fo r Query \nSet 1. MRR is reported on a scale from 0 to 100. \n \nSearch Algorithm = 1 ≤≤ ≤≤ 2 ≤≤ ≤≤ 3 MRR  \nNote-Interval 21.3%  27.1  31.6  28.2  \nMelodic-Contour  27.7 32.3  32.9  32.9  \nHidden Markov Model  25.8 30.3  32.9  31.0  \nTable 2: Percentage of correct targets returned at or below \nranks 1, 2, and 3, and Mean Reciprocal Rank (MRR) f or \nQuery Set 2. \nThe fact that the Note-Interval algorithm works wel l in this \ntest deserves some comment. In previous work, we co mpared \nnote-by-note matchers to contour- or frame-based ma tchers \nand concluded that the melodic-contour approach was  \nsignificantly better in terms of precision and reca ll (Mazzoni \n& Dannenberg, 2001). For that work, we experimented  with \nvarious note-matching algorithms, but we did not fi nd one that \nperforms as well as the contour matcher. Apparently , the note-\nmatching approach is sensitive to the relative weig hts given to \nduration versus pitch, and matching scores are also  sensitive to \nthe assigned edit penalties. Perhaps also this set of queries \nfavors matchers that use local information (interva ls and ratios) \nover those that use more global information (entire  contours). \n5 Sources of error \nWe have studied where errors arise in these search algorithms. \nAs mentioned, the major problem is that many melodi es \npresented in the queries are simply not present in the original \nsongs. In Set 1, only about half were judged to mat ch the \ncorrect target in the database in the sense that th e notes of the \nmelody and the notes of the target were in correspo ndence. \n(See Figure 3.) About a fifth of the queries partia lly matched a \ntarget, and a few did not match at all. Interesting ly, about one \nfourth of the queries matched material in the corre ct target, but \nthe query contained extra repetitions or out-of-ord er phrases. \nAn example is where subjects alternately hum a melo dy and a \ncountermelody, even when these do not appear as any  single \nvoice in the original song. Another example is wher e subjects \nsing two phrases in succession that did not occur t hat way in \nthe original song. Sometimes subjects repeat phrase s that were \nnot repeated in the original. Ultimately, query-by- humming assumes reasonably good queries, and more work is n eeded to \nhelp the average user create better queries. \nGood Match \nPartial Match \nOut-of-order or \nrepetition \nNo Match \nFigure 3: Distribution of query problems. We judged  only \nabout half the queries to have a direct corresponde nce to the \ncorrect target. \n6 Scaling to larger databases \nOur experimental algorithms are computationally dem anding, \nso we have limited our studies to medium-sized data bases. \nThe Beatles database used with Query Set 1 has 2844  themes \nextracted from 258 songs. The database used with Qu ery Set 2 \nhas 8926 themes extracted from 868 songs. Themes ha ve an \naverage of about 41 notes. \nRegardless of the algorithm, an interesting questio n is always: \nHow do the results scale as the database grows larg er? One \nway to explore this question is to use the similari ty scores to \nsimulate databases of different sizes without actua lly re-\nrunning the search.  \nLet us assume we have a table of melodic distance s cores for \nQ queries and T targets: S(q,t) (where 0 ≤ q < Q, and  0 ≤ t < \nT) is the distance of the best match of query q to target t. We \nalso have a list of correct targets C(q) for each query. Now, \nsuppose we want to simulate a database of size N < T for \nsome query q. We construct a “random” database by inserting \nthe correct target C(q) and N −1 random choices from the set \n{0…T −1} −{C(q)}.  We can compute the rank of the correct \ntarget in such a random database R by counting how many \nentries in the database have a lower score than the  score for \nthe correct target: \nrank = 1 + |{ x: x ∈ R and S(q,x) < S(q,C(q))}| \nThis gives us the rank for a particular random data base R, and \nwe would need to run this simulation many times to estimate \nthe expected rank. \nIn practice, we want to consider all queries (not j ust the single \nquery q) and we want results for all sizes of databases in  order \nto study the trend. To accomplish this, we “grow” t he random \ndatabase for each query. Initially, each query’s da tabase has \nonly the correct target. Then we grow each database  by one \ntarget selected randomly from the targets not yet i ncluded. \nEach time we grow the database, we compute the numb er of \ncorrect targets at rank 1, rank 2, etc. These numbe rs can then \nbe plotted as a function of database size as in Fig ure 4, which \nis based on Query Set 1 and the Melodic-Contour sea rch. Retrieval Rates vs. Database Size \nRank<=1 Rank<=3 \n020 40 60 80 100 \n0 50 100 150 200 250 300 \nDatabase Size Retrieval Rate Rank<=1 \nRank<=2 \nRank<=3 \nMRR \n \nFigure 4: Number of targets ranked 1 (bottom curve) , 2 or less, \n3 or less (top curve), and MRR (triangles) as a fun ction of \ndatabase size. \nNote that the function becomes flat, indicating tha t the number \nof correct answers does not fall off rapidly as the  database size \nincreases. Various functions could be used to appro ximate and \nextrapolate the observed data. Figure 5 shows the R ank 1 \ncurve together with various candidate models of sea rch \nperformance as a function of database size. \nThe simplest model is that the search procedure sim ply returns \na random guess. The expected number of correct resu lts at \nrank 1 is y = Q/N, where y is the number of correct targets \nreturned with rank 1, Q is the number of queries, a nd N is the \ndatabase size. This rapidly converges to zero and i s a poor fit \nto the data (as one would hope!). A slight modifica tion to this \nhas a set of queries where the search is perfect, r egardless of \ndatabase size, combined with another set of queries  where the \nsearch is ineffective and returns a random guess. T he \ncorresponding equation is y = c +(Q −c)/N, for some constant c. \nNote that in this model, the searches that really “ work” are \nindependent of the database size. This model, label ed \n“Constant+Random” in Figure 5 converges more rapidl y to \nthe constant c than does the observed data. Another  possible \nmodel is a power-law model: y = N −p. The corresponding \ncurve (labeled “Power Law”) is not as “flat” as the  observed \ndata. A function that flattens quickly is the logar ithm, so we \ntried two forms based on the log(N). The equation y  = \nQ(1 −c⋅log (N)) does not conform to the observations (see the \n“1 −Log” curve), but the equation y = c 1/log (c 2⋅N) fits the data \nreasonably well, albeit with two parameters (see th e “1/Log” \ncurve). Of course, there is no proof that we can ex trapolate \nthis function to predict behavior with larger datab ases, but it is \nencouraging that the function decreases slowly. For  example, \nto reduce the number of correct results at rank 1 i n this model \nby half, the database size must be squared.   Scaling Models \n020 40 60 80 100 \n0 200 400 600 800 1000 \nDatabase Size Rank 1 Retrieval Rate 1-Log \nPower Law \n1/Log \nRank 1 \nConstant+Random \nRandom Guess 1-Log \nPower Law  \n1 / Log Constant+Random \nRandom Guess \n \nFigure 5: Various models of database scaling with o bserved \ndata for Rank 1. \n \nFigure 6 is similar to Figure 4, but it plots the M RR from all \nthree search systems using Query Set 2. This data s eems to \nconfirm the general 1/log(N) scaling trend. At leas t in our \nlimited examples, the scaling trend seems to be ind ependent of \nthe set of queries, the database, and the search al gorithm. \nThis data shows that mean rank is not a good measur e of \nperformance. For example, a ranker that returns the  correct \nanswer ranked 1 st  half the time and ranked 100 th  half the time \nis a better performer that one that returns the rig ht answer \nrandomly between 1 st  and 100 th , even though their mean rank \nis the same. Figure 7 shows a histogram of ranks re turned \nfrom Query Set 1. There is some significant fractio n of \n“correctly matched” results with very low ranks, bu t the rest \n(queries for which no good match was found) are alm ost \nrandomly distributed. The mean rank is the “center of gravity” \nof this histogram, and it will obviously grow with the database \nsize. On the other hand, the number of low-ranking correct \ntargets will remain nearly constant as shown in Fig ure 4. In \nthese tests, the MRR seems to be highly correlated with the \nproportion of correct answers ranked in the top two  or three. \n \nMRR \n020 40 60 80 100 \n0 200 400 600 800 1000 \nDatabase Size MRR (percent) \n Melodic Contour HMM \nNote-Interval  \n \nFigure 6: MRR as a function of database size for th ree \ndifferent search algorithms. All three follow the s ame general \n1/Log trend. Frequency (Rank 1 to 15) \n010 20 30 40 50 \n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \nRank \nFrequency (Bin Size = 100) \n020 40 60 80 100 \n100 200 300 400 500 600 700 800 900 \nRank Note-Interval\nMelodic-Contour \nHMM \nFigure 7: Histograms showing how correct targets ar e ranked \nfor each of the three systems using Query Set 2. Th e upper \nhistogram shows details of the first 15 ranks. The lower \nhistogram includes all of the data, but with large bins of 100 \nranks each. \n7 Summary and Conclusions \nIt is widely understood and agreed that better eval uation tools \nare needed in the field of Music-Information Retrie val. We \nhave constructed a Query-By-Humming testbed to eval uate \nand compare different search techniques. The testbe d helps us \nto organize experiments by providing explicit repre sentations \nand standard formats for queries, targets, collecti ons (subsets \nof queries or targets), preprocessing stages, searc h algorithms, \nand result reporting. A single command can run a co mplete \ntest, including the preprocessing of data, searchin g for a set of \nqueries, and generating reports. Most of our testbe d including \nsome of the databases is available to other researc hers, and we \ncan also collaborate with other researchers by addi ng new \nsearch systems into our testbed. Please contact the  authors for \ninformation about formats and APIs. \nWe have compared three algorithms for music search that \nhave been reported previously, but never compared i n a “head-\nto-head” fashion. The Note-Interval algorithm treat s music as \nsequences of pitch intervals and IOI ratios, and se arches for an \nalignment that minimizes a distance function. The C ontour-\nMatching algorithm, a variation of string matching,  does not \nsegment the query into notes, but uses dynamic time -warping \nto find the best match to melodic contour. The HMM \napproach matches notes using a probabilistic error model \nintended to account for the kinds of errors observe d in queries.   \nThe contour-matching and HMM algorithms are extreme ly \nslow, taking on the order of 2 seconds of computati on time per \nentry in the database, which translates into days o f runtime for many of our tests. While this may be impractical fo r many \ntasks, we believe it is important to discover the b est search \ntechniques possible in terms of precision and recal l. Until \nquite recently, these algorithms seemed to out-perf orm all \nfaster approaches. However, at least on Query Set 2 , our \ncurrent Note-Interval system delivers similar searc h quality \nwith a run time of about 0.02 seconds per entry in the database, \nmaking it the clear winner in our comparison. Inter estingly, \nthe HMM and contour-matching approaches do not make  the \nsame mistakes, so returning the top choice of each is superior \nto returning the top two choices of either algorith m. \nOur work shows a wide range of performance accordin g to the \nquality of queries. When queries contain a reasonab ly long \nsequence of well-sung pitches, search algorithms ca n be very \neffective. On the other hand, when we collected que ries from \ngeneral university populations (which, if anything,  might be \nexpected to produce better queries than the overall  population), \nwe found many queries that were very difficult to m atch. The \nwide range in performance of our systems on differe nt query \nsets should serve as a warning to researchers: perf ormance is \nhighly dependent on queries, so no comparison is po ssible \nwithout controlling the query set. \nFinally, we propose that the issue of scaling with database size \ncan be studied by simulation. Given distance or sim ilarity \nestimates between queries and targets, we can plot the \nexpected number of queries whose correct targets wi ll be \nranked 1 (or in general, less than some rank k). For our \nalgorithms, we found that a 1/log(N) model gives a reasonable \nfit to the observed data. This is encouraging becau se this \nfunction becomes flat as the database size increase s. \nAcknowledgements \nWe gratefully acknowledge the support of the Nation al \nScience Foundation under grant IIS-0085945. The opi nions in \nthis paper are solely those of the authors and do n ot \nnecessarily reflect the opinions of the funding age ncies. \nThanks to Dominic Mazzoni and Mark Bartsch for init ial \nimplementation of some testbed components. Thanks t o Chee \nKiat, Crystal Fong, and David Murray for help with data \ncollection and analysis. \nReferences \nBirmingham, W. P., Dannenberg, R. B., Wakefield, G.  H., \nBartsch, M., Bykowski, D., Mazzoni, D., Meek, C., \nMellody, M., & Rand, W. (2001). \"MUSART: Music \nRetrieval Via Aural Queries.\" International Symposium \non Music Information Retrieval . pp. 73-81. \nDownie, J. S. (2002). \"Panel in Music Information R etrieval \nEvaluation Frameworks.\" ISMIR 2002 Conference \nProceedings . IRCAM, pp. 303-304. \nFutrelle, J., & Downie, J. S. (2002). \"Interdiscipl inary \nCommunities and Research Issues in Music Informatio n \nRetrieval.\" ISMIR 2002 Conference Proceedings . IRCAM, \npp. 215-221. \nGhias, A., Logan, J., Chamberlin, D., & Smith, B. C . (1995). \n\"Query by humming - musical information retrieval i n an \naudio database.\" Proceedings of ACM Multimedia 95 . pp. \n231-236. Hu, N., & Dannenberg, R. B. (2002). \"A Comparison o f \nMelodic Database Retrieval Techniques Using Sung \nQueries.\" Joint Conference on Digital Libraries . \nAssociation for Computing Machinery. \nMazzoni, D., & Dannenberg, R. B. (2001). \"Melody Ma tching \nDirectly From Audio.\" 2nd Annual International \nSymposium on Music Information Retrieval . Bloomington: \nIndiana University, pp. 17-18. \nMcNab, R. J., Smith, L. A., Witten, I. H., Henderso n, C. L., & \nCunningham, S. J. (1996). \"Towards the digital musi c \nlibrary: Tune retrieval from acoustic input.\" Proceedings \nof Digital Libraries '96 . ACM. \nMeek, C., & Birmingham, W. P. (2001). \"Thematic \nExtractor.\" 2nd Annual International Symposium on \nMusic Information Retrieval . Bloomington: Indiana \nUniversity, pp. 119-128. \nMeek, C., & Birmingham, W. P. (2002a). \"Johnny Can' t Sing: \nA Comprehensive Error Model for Sung Music Queries. \" \nISMIR 2002 Conference Proceedings . IRCAM, pp. 124-\n132. \nMeek, C., & Birmingham, W. P. (2002b). Johnny Can't Sing: \nA Comprehensive Error Model for Sung Music Queries  \n(CSE-TR-471-02): University of Michigan. Mongeau, M., & Sankoff, D. (1990). Comparison of Mu sical \nSequences. In W. Hewlett , et al.  (Eds.), Melodic \nSimilarity Concepts, Procedures, and Applications  (Vol. \n11). Cambridge: MIT Press.  \nPardo, B., & Birmingham, W. P. (2002, , October 13- 17). \n\"Encoding Timing Information for Musical Query \nMatching.\" ISMIR 2002, 3rd International Conference on \nMusic Information Retrieval . IRCAM, pp. 267-268. \nPardo, B., & Birmingham, W. P. (2002). \"Improved Sc ore \nFollowing for Acoustic Performances.\" Proceedings of \nthe 2002 International Computer Music Conference . San \nFrancisco: International Computer Music Association . \nPardo, B., Birmingham, W. P., & Shifrin, J. (2003).  \"Name \nthat Tune: A Pilot Studying in Finding a Melody fro m a \nSung Query.\" Journal of the American Society for \nInformation Science and Technology , (in review).  \nPauws., S. (2002). \"CubyHum: A Fully Operational Qu ery-by-\nHumming System.\" ISMIR 2002 Conference Proceedings . \nIRCAM, pp. 187-196. \nShifrin, J., Pardo, B., Meek, C., & Birmingham, W. P. (2002). \n\"HMM-Based Musical Query Retrieval.\" Joint \nConference on Digital Libraries . Association for \nComputing Machinery, pp. 295-300."
    },
    {
        "title": "The Sheet Music Consortium: A Specialized Open Archives Initiative harvester project.",
        "author": [
            "Stephen Davison"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417731",
        "url": "https://doi.org/10.5281/zenodo.1417731",
        "ee": "https://zenodo.org/records/1417731/files/Davison03.pdf",
        "abstract": "The Open Archives Initiative (OAI) Sheet Music Project is a consortium of institutions building OAI- compliant data providers, a metadata harvester, and a web-based service provider for digital sheet music collections. The project aims to test the viability of the OAI standard for providing access to sheet music collections on the web, and to build a permanent and increasingly participatory service for the discovery of digital sheet music. The service provider design has been informed by detailed usability testing, and by limitations imposed by the variations in metadata harvested from the different participating collections. Advanced services in addition to basic searching and browsing have been developed, including the ability to save and share subsets across participating collections. Harvesting and searching strategies for overcoming metadata limitations are being developed. The consortium is seeking additional participants with digital sheet music collections, and is exploring the possibility of incorporating scores and audio into the project. Digital sheet music collections were among the earliest substantial music collections to appear on the web. Most significantly, digital sheet music collections have been mounted by the Library of Congress, Johns Hopkins University, and Duke University. There are many other important collections, resulting in a rich distributed research resource for music. Sheet music collections have become the focus of digitization projects for a numb er of reasons relating to the publication format, its component parts, and the resulting problems in providing access, either through traditional cataloging systems, or by other means. A piece of sheet music generally consists of a number of different components, brought together for a specific publication. These include the music itself, usually consisting of between two and eight pages; a cover page, that often includes graphic artwork and/or a photographic reproduction; and advertisements, either for additional sheet music from the same publisher, or from other vendors. The most common sheet music genre is popular song, and the text of these songs comprises another important element of the publication. It seems likely that providing access to the cover art of sheet music has been the prime motivation for many sheet music projects. The graphic artwork found on published sheet music is often very decorative, and provides information of interest to a wide variety of scholars, including historians of art, cultural historians, sociologists, and so on. Much as sound recordings are today, sheet music of the C19th and early C20th documents taste, attitudes, and societal concerns, across time, and in different geographical locations. Sheet music may also reflect the concerns of specific groups of people (e.g. political publications), or entire nations (e.g. nationalistic publications). Songs that become perennial favorites were often republished with changes in text (e.g. taking out insulting epithets; or perhaps writing completely new words for a favorite tune), new cover graphic arts; or simply changes in the advertising. All of these changes document changing attitudes both musical and non-musical. Traditional cataloging schemes have had a difficult time capturing the disparate elements that make up a single piece of sheet music, let alone the relationships between repeated publications of a single work. In addition sheet music has usually been published as simply music, with little recorded about the cover art or the text. For example, an AACR2 catalog record for a piece of sheet music would typically record the genre (e.g. “Piano music” or “Songs with piano”); include transcriptions of title, attribution, and publication information; and provide access points to composer and lyricist. Information about the cover art and subject of the song—other than that obvious from the title —typically would be absent. These difficulties, both the physical structure and the cataloging problem, along with the recognition that sheet music has unique research potential, have meant that sheet music collections have often found a home among institutional “Special Collections, ” rather than in the music library. These problems, along with interest from a wide variety of users, have encouraged the creation of digital sheet Permission to make digital or hard copies of all or part of this work for personal of classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.   2003 The Jo hns Hopkins University. music collections. These allow for a wide variety of keyword searching, as well as browsing of images and advertisements, which are impossible using traditional means of access. Although the capture of song texts as text rather than graphics has not usually been part of these projects, there has been recognition that the capture of these texts is a very desirable feature (e.g. the Johns Hopkins projects to develop both music and text recognition software). The metadata describing sheet music collections vary greatly in both detail and structure. For example brief AACR2/MARC records have been used by the Library of Congress for American Memory; Duke University uses Encoded Archival Description for the Historic American Sheet Music collection; and UCLA uses an AACR2-flavored Dublin Core schema for the Digital Archive of Popular American Music. One challenge to establishing efficient access to these distributed collections lies in these different metadata encoding schemes. The Sheet Music Consortium was established to develop unified access to these sheet music collections using the Open Archives Initiative Protocol for Metadata Harvesting (OAI- PMH). There are currently four active participants, all with functioning data providers—Indiana University, Johns Hopkins University, and UCLA —with the Library of Congress involved passively as a data provider. Duke University and Brown University have also been involved in planning and usability testing, and will be joining as data providers in the near future. The aims of the project are: • To demonstrate the viability of a specialized OAI harvester • To develop a specialized service provider that will provide searching and browsing capabilities, along with more advanced services such as the ability to save and share subsets across the various collections • To establish data mapping guidelines for participating collections • To establish data creation guidelines for new participants, who either have established digital collections or plan to do so • To demonstrate a collaborative development model that includes both the current active participants and other potential participants The project has been guided by a steering group drawn from the three active participants, with additional teams to work on technical development (both data providers and metadata harvester), data mapping, and development of the service provider interface. To date three data providers have been built (Indiana, Johns Hopkins, UCLA, in addition to the already existing LC data provider), along with a service provider. The service provider is available at: http://digital.library.ucla.edu/sheetmusic/ This prototype service was used in a usability study funded by the Mellon Foundation. Users from all five active participants in the project participated in both focus groups and one-on- one interviews designed to assess interest in and need for a sheet music service, and to provide feedback to inform the future design of the interface. The study resulted in a set of recommendations for improvement in the search interface, including the addition of an advanced search screen, additional browsing options, a more consistent and comprehensible layout, the ability to protect, email, and save virtual collections, and the ability to more easily pick records from a results list. The Consortium has adopted unqualified Dublin Core as the metadata standard for initial phase of the project. While acknowledging the advanced services that a more detailed schema (e.g. qualified Dublin Core, MARC) might provide, we recognize the advantages that more basic requirements provide: lower barriers to participation, and a shorter development timeline. A variety of metadata issues limit the services that can be provided by the sheet music service provider. For instance, although some subset of the Anglo-American Cataloging Rules, 2nd edition (AACR2) is most often used to guide the type and format of the data collected, there are significant variations in their implementation. For instance, one collection may record a statement of responsibility (“music by George Gershwin; lyrics by Ira Gershwin”) but not create a table of names in inverted form (“Gershwin, George”; “Gershwin, Ira”), whereas another collection may do the opposite: create a table of names, but not record an accurate statement of responsibility. The result of this variance in metadata is that retrieval by name is limited to searching, and that browsing of names is difficult, if not impossible, to achieve in the service provider. In addition, some collections may impose authority control on certain data elements (e.g. names, publishers), while others will simply transcribe names as they appear on the published item. The consortium is discussing the possibility of providing improved access to collections by mapping data from the various native formats to qualified Dublin Core elements. For instance, it may then be possible to distinguish between composers and lyricists in searches, to provide access to various descriptive elements, such as plate and publisher numbers, or to distinguish between different types of dates that may be present in the metadata record. However, although many desired service improvements may theoretically be possible through the implementation of qualified Dublin Core, the metadata available through the harvesting process may not support them. Future service enhancements will be implemented only to the extent that they can be supported by the metadata. The sheet music consortium is seeking additional participants in the OAI Sheet Music Project from sheet music collections worldwide. The consortium is also exploring the possibility of expanding to scope of the project to include other musical formats, such as musical scores and parts, and sound recordings. Expansion of the project to include other musical formats (e.g. digital audio, encoded music, music notation files, etc.) is also under discussion. Although the sheet music project has demonstrated the viability of the OAI protocol for metadata harvesting in establishing a specialized service, there is a danger in generalizing the service into to areas that may be better served by other means of discovery. Acknowledgements Our thanks to the other members of the project steering team, Stephen Schwartz and Curtis Fornadley of the UCLA Library.",
        "zenodo_id": 1417731,
        "dblp_key": "conf/ismir/Davison03",
        "keywords": [
            "Open Archives Initiative (OAI)",
            "Sheet Music Project",
            "consortium of institutions",
            "OAI-compliant data providers",
            "metadata harvester",
            "web-based service provider",
            "digital sheet music collections",
            "OAI standard",
            "viability testing",
            "permanent and increasingly participatory service"
        ],
        "content": "A Specialized Open Archives Initiative Harvester for Sheet Music:  \nA Project Report and Examination of Issues  \nStephen Davison  \nUniversity of California, Los Angeles  \nMusic Library  \n1102 Schoenberg Music Building  \nLos Angeles, CA 90095 -1490  \nsdavison@library.ucla .edu Cynthia Requardt  \nThe Johns Hopkins University  \nSpecial Collections , Eisenhower Library  \n3400 N orth Charles Street  \nBaltimore, MD 21218  \nCynthia.Requardt@jhu.edu  Kristine Brancolini  \nIndiana University  \nDigital L ibrary Program  \n1320 East 10th Street , Rm. E170  \nBloomington, IN 47405  \nbrancoli@indiana.edu  \n Abstract  \nThe Open Archives Initiative (OAI) Sheet Music \nProject is a consortium of institutions building OAI -\ncompliant data providers, a metadata harvester, and a \nweb-based service provider for digital sheet music \ncollections. The p roject aims to test the viability of \nthe OAI standard for providing access to sheet music \ncollections on the web, and to build a permanent and \nincreasingly participatory service for the discovery of \ndigital sheet music. The service provider design has \nbeen  informed by detailed usability testing, and by \nlimitations imposed by the variations in metadata \nharvested from the different participating collections. \nAdvanced services in addition to basic searching and \nbrowsing have been developed, including the abili ty \nto save and share subsets across participating \ncollections. Harvesting and searching strategies for \novercoming metadata limitations are being \ndeveloped. The consortium is seeking additional \nparticipants with digital sheet music collections, and \nis explo ring the possibility of incorporating scores \nand audio into the project.  \nDigital sheet music collections were among the earliest \nsubstantial music collections to appear on the web. Most \nsignificantly, digital sheet music collections have been \nmounted by th e Library of Congress, Johns Hopkins \nUniversity, and Duke University. There are many other \nimportant collections, resulting in a rich distributed research \nresource for music.  \nSheet music collections have become the focus of digitization \nprojects for a numb er of reasons relating to the publication \nformat, its component parts, and the resulting problems in \nproviding access, either through traditional cataloging \nsystems, or by other means.  \nA piece of sheet music generally consists of a number of \ndifferent com ponents, brought together for a specific publication. These include the music itself, usually consisting \nof between two and eight pages; a cover page, that often \nincludes graphic artwork and/or a photographic reproduction; \nand advertisements, either for ad ditional sheet music from the \nsame publisher, or from other vendors.  \nThe most common sheet music genre is popular song, and the \ntext of these songs comprises another important element of the \npublication. It seems likely that providing access to the cover \nart of sheet music has been the prime motivation for many \nsheet music projects. The graphic artwork found on published \nsheet music is often very decorative, and provides information \nof interest to a wide variety of scholars, including historians of \nart, cu ltural historians, sociologists, and so on. Much as sound \nrecordings are today, sheet music of the C19th and early \nC20th documents taste, attitudes, and societal concerns, across \ntime, and in different geographical locations. Sheet music may \nalso reflect t he concerns of specific groups of people (e.g. \npolitical publications), or entire nations (e.g. nationalistic \npublications). Songs that become perennial favorites were \noften republished with changes in text (e.g. taking out \ninsulting epithets; or perhaps w riting completely new words \nfor a favorite tune), new cover graphic arts; or simply changes \nin the advertising. All of these changes document changing \nattitudes both musical and non -musical.  \nTraditional cataloging schemes have had a difficult time \ncapturin g the disparate elements that make up a single piece of \nsheet music, let alone the relationships between repeated \npublications of a single work. In addition sheet music has \nusually been published as simply music, with little recorded \nabout the cover art or  the text. For example, an AACR2 \ncatalog record for a piece of sheet music would typically \nrecord the genre (e.g. “Piano music” or “Songs with piano”); \ninclude transcriptions of title, attribution, and publication \ninformation; and provide access points to composer and \nlyricist. Information about the cover art and subject of the \nsong —other than that obvious from the title —typically would \nbe absent.  \nThese difficulties, both the physical structure and the \ncataloging problem, along with the recognition that sh eet \nmusic has unique research potential, have meant that sheet \nmusic collections have often found a home among \ninstitutional “Special Collections, ” rather than in the music \nlibrary. These problems, along with interest from a wide \nvariety of users, have en couraged the creation of digital sheet Permission to make digital or hard copi es of all or part of this work for \npersonal of classroom use is granted without fee provided that copies are not \nmade or distributed for profit or commercial advantage and that copies bear \nthis notice and the full citation on the first page.   2003 The Jo hns Hopkins \nUniversity.  music collections. These allow for a wide variety of keyword \nsearching, as well as browsing of images and advertisements, \nwhich are impossible using traditional means of access. \nAlthough the capture of song texts as t ext rather than graphics \nhas not usually been part of these projects, there has been \nrecognition that the capture of these texts is a very desirable \nfeature (e.g. the Johns Hopkins projects to develop both music \nand text recognition software).  \nThe metadata  describing sheet music collections vary greatly \nin both detail and structure. For example brief AACR2/MARC \nrecords have been used by the Library of Congress for \nAmerican Memory; Duke University uses Encoded Archival \nDescription for the Historic American S heet Music collection; \nand UCLA uses an AACR2 -flavored Dublin Core schema for \nthe Digital Archive of Popular American Music. One \nchallenge to establishing efficient access to these distributed \ncollections lies in these different metadata encoding schemes.  \nThe Sheet Music Consortium was established to develop \nunified access to these sheet music collections using the Open \nArchives Initiative Protocol for Metadata Harvesting (OAI -\nPMH). There are currently four active participants, all with \nfunctioning data pro viders —Indiana University, Johns \nHopkins University, and UCLA —with the Library of \nCongress involved passively as a data provider. Duke \nUniversity and Brown University have also been involved in \nplanning and usability testing, and will be joining as data \nproviders in the near future.  \nThe aims of the project are:  \n• To demonstrate the viability of a specialized OAI harvester  \n• To develop a specialized service provider that will provide \nsearching and browsing capabilities, along with more \nadvanced services such as the ability to save and share subsets \nacross the various collections  \n• To establish data mapping guidelines for participating \ncollections  \n• To establish data creation guidelines for new participants, \nwho either have established digital collections or plan to d o so \n• To demonstrate a collaborative development model that \nincludes both the current active participants and other \npotential participants  \nThe project has been guided by a steering group drawn from \nthe three active participants, with additional teams to wor k on \ntechnical development (both data providers and metadata \nharvester), data mapping, and development of the service \nprovider interface. To date three data providers have been \nbuilt (Indiana, Johns Hopkins, UCLA, in addition to the \nalready existing LC dat a provider), along with a service \nprovider. The service provider is available at:  \nhttp://digital.library.ucla.edu/sheetmusic/  \nThis prototype service was used in a usability study funded by \nthe Mellon Foundation. Users from all five active participants \nin the project participated in both focus groups and one -on-\none interviews designed to assess interest in and need for a \nsheet music service, and to provide feedback to inform the \nfuture design of the interface. The study resulted in a set of \nrecommendations f or improvement in the search interface, \nincluding the addition of an advanced search screen, additional browsing options, a more consistent and \ncomprehensible layout, the ability to protect, email, and save \nvirtual collections, and the ability to more easi ly pick records \nfrom a results list.  \nThe Consortium has adopted unqualified Dublin Core as the \nmetadata standard for initial phase of the project. While \nacknowledging the advanced services that a more detailed \nschema (e.g. qualified Dublin Core, MARC) migh t provide, \nwe recognize the advantages that more basic requirements \nprovide: lower barriers to participation, and a shorter \ndevelopment timeline.  \nA variety of metadata issues limit the services that can be \nprovided by the sheet music service provider. For instance, \nalthough some subset of the Anglo -American Cataloging \nRules, 2nd edition (AACR2) is most often used to guide the \ntype and format of the data collected, there are significant \nvariations in their implementation. For instance, one collection \nmay rec ord a statement of responsibility (“music by George \nGershwin; lyrics by Ira Gershwin”) but not create a table of \nnames in inverted form (“Gershwin, George”; “Gershwin, \nIra”), whereas another collection may do the opposite: create a \ntable of names, but not record an accurate statement of \nresponsibility. The result of this variance in metadata is that \nretrieval by name is limited to searching, and that browsing of \nnames is difficult, if not impossible, to achieve in the service \nprovider. In addition, some col lections may impose authority \ncontrol on certain data elements (e.g. names, publishers), \nwhile others will simply transcribe names as they appear on \nthe published item.  \nThe consortium is discussing the possibility of providing \nimproved access to collection s by mapping data from the \nvarious native formats to qualified Dublin Core elements. For \ninstance, it may then be possible to distinguish between \ncomposers and lyricists in searches, to provide access to \nvarious descriptive elements, such as plate and publ isher \nnumbers, or to distinguish between different types of dates \nthat may be present in the metadata record. However, although \nmany desired service improvements may theoretically be \npossible through the implementation of qualified Dublin Core, \nthe metadat a available through the harvesting process may not \nsupport them. Future service enhancements will be \nimplemented only to the extent that they can be supported by \nthe metadata.  \nThe sheet music consortium is seeking additional participants \nin the OAI Sheet M usic Project from sheet music collections \nworldwide. The consortium is also exploring the possibility of \nexpanding to scope of the project to include other musical \nformats, such as musical scores and parts, and sound \nrecordings. Expansion of the project to  include other musical \nformats (e.g. digital audio, encoded music, music notation \nfiles, etc.) is also under discussion. Although the sheet music \nproject has demonstrated the viability of the OAI protocol for \nmetadata harvesting in establishing a specializ ed service, there \nis a danger in generalizing the service into to areas that may be \nbetter served by other means of discovery.  \nAcknowledgements  \nOur thanks to the other members of the project steering team, \nStephen Schwartz and Curtis Fornadley of the UCLA Library."
    },
    {
        "title": "Classification of dance music by periodicity patterns.",
        "author": [
            "Simon Dixon",
            "Elias Pampalk",
            "Gerhard Widmer"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1414936",
        "url": "https://doi.org/10.5281/zenodo.1414936",
        "ee": "https://zenodo.org/records/1414936/files/DixonPW03.pdf",
        "abstract": "This paper addresses the genre classification prob- lem for a specific subset of music, standard and Latin ballroom dance music, using a classification method based only on timing information. We compare two methods of extracting periodicities from audio recordings in order to find the metrical hierarchy and timing patterns by which the style of the music can be recognised: the first method performs onset detec- tion and clustering of inter-onset intervals; the sec- ond uses autocorrelation on the amplitude envelopes of band-limited versions of the signal as its method of periodicity detection. The relationships between periodicities are then used to find the metrical hierar- chy and to estimate the tempo at the beat and measure levels of the hierarchy. The periodicities are then in- terpreted as musical note values, and the estimated tempo, meter and the distribution of periodicities are used to predict the style of music using a simple set of rules. The methods are evaluated with a test set of standard and Latin dance music, for which the style and tempo are given on the CD cover, providing a “ground truth” by which the automatic classification can be measured. 1",
        "zenodo_id": 1414936,
        "dblp_key": "conf/ismir/DixonPW03",
        "keywords": [
            "genre classification",
            "music",
            "periodicities",
            "audio recordings",
            "metrical hierarchy",
            "tempo estimation",
            "simple rules",
            "ground truth",
            "automatic classification",
            "CD cover"
        ],
        "content": "Classiﬁcation of Dance Music by Periodicity Patterns\nSimon Dixon\nAustrian Research Institutefor AI\nFreyung 6/6, Vienna 1010, Austria\nsimon@oefai.atElias Pampalk\nAustrian Research Institutefor AI\nFreyung 6/6, Vienna 1010, Austria\nelias@oefai.atGerhard Widmer\nAustrian Research Institutefor AI\nFreyung 6/6, Vienna 1010, Austria\nand\nDepartment of Medical Cybernetics and AI\nUniversity of Vienna\ngerhard@oefai.at\nAbstract\nThis paper addresses the genre classiﬁcation prob-\nlemforaspeciﬁcsubsetofmusic,standardandLatin\nballroom dance music, using a classiﬁcation method\nbased only on timing information. We compare\ntwo methods of extracting periodicities from audio\nrecordings in order to ﬁnd the metrical hierarchy and\ntiming patterns by which the style of the music can\nberecognised: theﬁrstmethodperformsonsetdetec-\ntion and clustering of inter-onset intervals; the sec-\nond uses autocorrelation on the amplitude envelopes\nof band-limited versions of the signal as its method\nof periodicity detection. The relationships between\nperiodicities are then used to ﬁnd the metrical hierar-\nchyandtoestimatethetempoatthebeatandmeasure\nlevels of the hierarchy. The periodicities are then in-\nterpreted as musical note values, and the estimated\ntempo, meter and the distribution of periodicities are\nused to predict the style of music using a simple set\nof rules. The methods are evaluated with a test set of\nstandard and Latin dance music, for which the style\nand tempo are given on the CD cover, providing a\n“ground truth” by which the automatic classiﬁcation\ncan be measured.\n1 Introduction\nGenre classiﬁcation is an important problem in music informa-\ntionretrieval. Automaticclassiﬁcationatacoarselevel,suchas\ndistinguishingclassicalfromrockmusic,isnotadifﬁcultprob-\nlem, but more ﬁne-grained distinctions amongst pieces sharing\nsimilarcharacteristicsaremoredifﬁculttoestablish(Tzanetakis\nand Cook, 2002). In this paper we consider the recognition of\ngenre within the various styles of standard and Latin ballroom\ndance music. These styles have certain common characteris-\ntics (for example, a strong beat and a mostly constant tempo),\nbutatthesametimehaveclearlyrecognisabledifferences(con-\nsider tango, waltz and jive), which humans are usually able to\nPermission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided that\ncopies are not made or distributed for proﬁt or commercial advan-\ntage and that copies bear this notice and the full citation on the ﬁrst\npage.c/circlecopyrt2003 Johns Hopkins University.distinguish with minimal training. Since the major feature of\ndance music is rhythm, this paper focusses entirely on classi-\nﬁcation based on temporal features of the music, although we\nrecognise that other features (such as instrumentation and ar-\nticulation) are also important in helping dancers to choose the\nappropriate dance style for aparticular piece of music.\nWe compare two methods of generating a ranked list of period-\nicities from audio recordings in order to ﬁnd the metrical hier-\narchy and timing patterns by which the style of the music can\nbe recognised. The ﬁrst method is based on an onset detection\nalgorithm taken from a performance analysis and visualisation\nsystem(Dixonetal.,2002),whichprocessestheaudiosignalby\ndetecting onsets of musical notes, calculates the time intervals\nbetween pairs of onsets, and uses a clustering algorithm to ﬁnd\nthe signiﬁcant periodicities in the music. The second method is\nbasedonasystemforcalculatingthesimilarityofrhythmicpat-\nterns (Paulus and Klapuri, 2002), which splits the audio signal\nintoanumberoffrequencybands,smoothseachonetoproduce\nasetofamplitudeenvelopes,andﬁndsperiodicitiesineachfre-\nquency band as peaks in the autocorrelation function.\nTheperiodicitiesarethenprocessedtoﬁndthebest-ﬁttingmet-\nrical hierarchy, by assigning each periodicity to a musical note\nvalue, expressed as a simple integer fraction representing the\nnumber of beats. The distribution of note values and their\nweights, as well as the rounding errors, are used in determin-\ning the most likely metrical structure, which in turn determines\nthe tempo and meter of the music. Finally, a simple rule-based\nsystemisusedtoclassifythepiecebydancestyle,basedonthe\ntempo, meter, patterns of periodicities and their strengths.\nIt is not clear that periodicity patterns provide sufﬁcient infor-\nmationtocorrectlyclassifyalldancemusic. Notestshavebeen\nmade with human subjects to compare their performance on\nsuch a task. As it stands, the greatest source of error is in the\nselection of the metrical hierarchy. Once this is correctly deter-\nmined, classiﬁcation accuracy compares favourably with other\nsystems. The results from a test set of over 100 standard and\nLatin ballroom dance pieces indicate that when the tempo and\nmeter are correctly estimated, the style recognition rules attain\nup to 80% accuracy.\nThe following 2 sections describe the two periodicity detection\nmethods respectively, and then in section 4, the algorithm for\ndetermining tempo, meter and ﬁnally dance style is presented.\nSection 5 contains the results of testing on a set of dance CDs,\nand the paper concludes with a discussion of the results and an\noutline of planned future work.2 Inter-Onset Interval Clustering\nMost rhythmic information is conveyed by the timing of the\nbeginnings (onsets) of notes. For this reason, many tempo in-\nductionandbeattrackingsystemswhichworkwithaudioinput\nstart by estimating the onset times of musical notes and/or per-\ncussivesounds(e.g.LargeandKolen,1994;GotoandMuraoka,\n1995; Dixon, 2001). Othersystems workwith MIDIinput (e.g.\nRosenthal, 1992) and necessarily use the onset times in their\nprocessing. The subsequent processing is then performed sym-\nbolically, without further reference to the audio data, in which\ncase onset detection can be seen as a preprocessing step for an\nalgorithm that is not audio-based. Tempo information is then\nderived from the time durations between pairs of onsets, which\ncorrespondtovariousrhythmicunits,suchasquarternotes,half\nnotesanddottedquarternotes. Thesedurationsarecalledinter-\nonset intervals (IOIs), referring to the time intervals between\nboth consecutive and non-consecutive pairs of onsets. Assum-\ning the tempo does not vary greatly during the analysis pe-\nriod, clustering of similar IOIs will reveal the main periodici-\nties in the music and ﬁlter out most spuriously detected onsets.\nThis section describes a periodicity detection algorithm based\non Dixon et al. (2002).\n2.1 Audio Processing\nThe audio input is read in linear PCM format (after being con-\nverted from a compressed format if necessary). If the input has\nmorethanonechannel,asinglechannelsignaliscreatedbyav-\neragingallchannels. Theaudiodataisprocessedinblocksbya\nsmoothing ﬁlter which calculates the RMS amplitude for 40ms\nblocks of data, using a 10ms hop size. Onsets are detected us-\ning a simple time-domain method, which ﬁnds local peaks in\nthe slope of this smoothed amplitude envelope (see ﬁgure 1),\nwhere the slope is calculated using a 4-point linear regression.\nThresholds in amplitude and slope are used to delete spurious\npeaks, and the remaining peaks are taken to be the note onset\ntimes. Although a relatively simple algorithm is used for event\ndetection, it has been shown that it works sufﬁciently well for\nthe successful extraction oftempo.\n2.2 Clustering\nThe onset times are used by the clustering algorithm given in\nﬁgure 2 to ﬁnd signiﬁcant periodicities in the data. The clus-\ntering algorithm begins by calculating all IOIs between pairs\nof onsets up to 5 seconds apart, weighting the intervals by the\ngeometric mean of the amplitudes of the onsets, and summing\nacross equally spaced onset pairs, to give a weighted IOI his-\ntogram, as shown in ﬁgure 3.\nThe IOIs are then clustered using an iterative best-ﬁrst algo-\nrithm which sequentially ﬁnds the cluster with the greatest av-\nerage amplitude, marks its IOIs as used, and continues search-\ning for the next cluster. The width of a cluster is adjusted ac-\ncording to the IOI duration, so that clusters representing longer\ndurations allow greater variation in the IOIs. Each cluster Ci\nis ranked by its weight Si, which is calculated as the average\nweight wjof its component IOIs, and the centroid Tiof each\nclusteriscalculatedastheweightedaverageoftheIOIs tjthem-\nselves. The next best clusters and their weights are calculated\nbymarkingtheIOIswhichhavebeenusedinapreviouscluster\nand repeating the above calculations ignoring the marked IOIs.\nItisusuallythecaseintraditionalWesternmusicthattimeinter-\n0 0.5 1 1.5 2−0.04−0.03−0.02−0.0100.010.020.030.04\nTime (s)AmplitudeFigure 1: Onset detection method, showing the audio signal\nwith the smoothed amplitude envelope overlaid in bold and the\npeaks in the slope marked bydashed lines.\n 0.5  1.0  1.5  2.0\nTime (s) 2.5  3.0  3.5  4.0  4.5 3.5\n 3.0\n 2.5\n 2.0\n 1.5Cluster Weight\n 1.0\n 0.5\nFigure 3: Example showing weighted IOI histogram for a\nsambapieceat56measuresperminute(224BPM).The3high-\nest peaks correspond to 4 beats (one measure), 8 beats and 1\nbeat, respectively.For times tifrom 0.1s to 5.0s in 0.01s steps\nFind all pairs of onsets whichare tiapart\nWeight wi= sum of the mean amplitude of the onset pairs\nWhile there are unmarked IOIs\nFor times tifrom 0.1s to 5.0s in 0.01s steps\nCluster width si= 0.01⌊ti\n0.3+ 8⌋\nFind average amplitude of unmarked IOIs in window [ti, ti+si]\nFind tMwhich gives maximum average amplitude\nCreate a cluster containing the IOIs in the range [tM, tM+sM]\nMark the IOIs in the cluster as used\nFor each cluster\nFind related clusters (multiples or divisors)\nAdjust related clusters usingweighted average\nFigure 2: Algorithm for clustering of inter-onset intervals\nvals are approximately related by small integer ratios; the clus-\nter centroids also tend to reﬂect this property. In other words,\ntheclustercentroidsarenotindependent;theyrepresentrelated\nmusical units such as quarter notes and half notes. Since the\nclustersarepartofasinglemetricalhierarchy,thecentroidscan\nbe used to correct each other, since we expect them to exhibit\nsimple integer fraction ratios. Thus an error in a single cluster\ncan be corrected by reference to the other clusters. This is the\nﬁnal step of periodicity detection, where the cluster centroids\nand weights are adjusted based on the combined information\ngiven by all of their related clusters. The cluster centres and\ntheirweightsdeﬁnearankedlistofperiodicitieswhicharethen\nused in determining tempo, meter and style.\n3 Periodicity Detection with Autocorrelation\nAn alternative approach to periodicity detection uses autocor-\nrelation. This method has been used for detecting the me-\nter of musical scores by Brown (1993), and for pulse tracking\nby Scheirer (1997), using the Meddis and Hewitt pitch model\nwithmuchlargertimewindows. Webasethisworkonthemore\nrecentresearchofPaulusandKlapuri(2002),whichwasdevel-\noped for measuring the similarity of rhythmic patterns.\n3.1 Audio Processing\nThe second method of periodicity detection was implemented\nby converting the audio input data to the mid-level representa-\ntion advocated by Paulus and Klapuri. The suggested prepro-\ncessing step which removes sinusoids from the signal was not\nperformed,sincewecouldnotguaranteetheexistenceofdrums\ninallpiecesinourdataset. Theaimoftheaudioprocessingstep\nwas to reduce the audio signal to a set of amplitude envelopes\nfrom which temporal information could be derived.\nThe audio data was taken from audio CDs, converted to a sin-\nglechannelbyaveraging,andthenpassedthroughan8channel\nﬁlter bank, with the ﬁrst band up to 100 Hz and the remaining\nbands equally spaced (logarithmically) at just over an octave\nwide to cover the full frequency range of the signal. Then for\neachofthe8frequencybands,thesignalwasrectiﬁed,squared,\ndecimated to a sampling rate of 980Hz, and smoothed with\na 20Hz low-pass ﬁlter. Finally the dynamic range was com-\npressed using a logarithmicfunction.3.2 Periodicity Calculation\nPeriodicities are found in each frequency band by examining\nthe peaks of the autocorrelation function for time lags between\n0 and 5 seconds. After normalising the autocorrelation by the\nmagnitude of the lag 0 value, this peak is discarded, and the\nthree highest peaks are collected from each frequency band.\nFigure 4 shows an example of these peaks for a samba piece\nat 224 beats per minute. The periodicities corresponding to the\nbeat (268ms) and the measure (1057 ms) are prominent in sev-\neral frequency bands.\nRatherthansummingtheautocorrelationresultsacrossthevari-\nousfrequencybands,wematchpeaks(periodicities)indifferent\nfrequency bands which differ by less than 20ms. Matched sets\nofperiodicitiesarecombinedbyaveragingtheperiod. Aweight\nSifor each resulting periodicity Tiis calculated as the sum of\nthe mean autocorrelation value and the number of matched pe-\nriodicities in the set. The weights Siare used to rank the el-\nements in the list of periodicities; these values are used in the\nsubsequent estimation of tempo, meter and style.\n4 Determining Tempo, Meter and Style\nBoth of the methods discussed in the previous sections are\ngenerally successful in ﬁnding peaks at the periodicities corre-\nspondingtothebeatandmeasurelevelofthemetricalhierarchy.\nThedifﬁcultyisthatpeaksalsooccurathigher,lowerandinter-\nvening metrical levels, as well as at commonly occurring note\ndurations which are not directly part of the metrical hierarchy,\nfor example the dotted quarter note in a samba rhythm.\nWe use an exhaustive approach to evaluate the suitability of\neach periodicity as the measure level of the metrical hierarchy.\nForeachperiodicity Tk,theratio rioftheotherperiodicities Ti\ntoTkis expressed as a simple integer fractionpi\nqi, attempting to\nkeep piandqismall while minimising the error |pi−qiri|for\neach i. Formally, the constraints areas follows for each i:\npi<16 (for qi>1)\nqi∈ {1,2,3,4,6,8,12,16}\nGCD( pi, qi) = 1\n¬∃p/prime, q/primesuch that |p/prime−q/prime∗ri|<|pi−qi∗ri|0.00.51.01.52.02.53.03.54.04.55.00.900.951.00\nAutocorrelation Lag (seconds)0−100 Hz0.00.51.01.52.02.53.03.54.04.55.00.900.951.00100−216 Hz0.00.51.01.52.02.53.03.54.04.55.00.940.96216−467 Hz0.00.51.01.52.02.53.03.54.04.55.00.900.951.00467−1010 Hz0.00.51.01.52.02.53.03.54.04.55.00.900.951.001010−2183 Hz0.00.51.01.52.02.53.03.54.04.55.00.900.951.002183−4719 Hz0.00.51.01.52.02.53.03.54.04.55.00.850.900.954719−10201 Hz0.00.51.01.52.02.53.03.54.04.55.00.800.901.0010201−22050 HzSamba de tres notas (224 BPM)Figure 4: Autocorrelation of the amplitude envelopes for each\nfrequency band, showing lags up to 5 seconds. The top three\npeaks(excludingthelag0peak)aremarkedbyadashedvertical\nline. The input is the same piece as used in ﬁgure 3.Tempo Range\nStyle Actual Suggested Met.#\nMinMaxMinMax\nBlues 2020--42\nRumba 26292626411\nTango 28333032410\nSlow fox 2930283045\nDisco 3030--42\nSlow waltz 2930283039\nCha cha 30333232427\nJive 32444444421\nRock and roll 4242--41\nBoogie 4446--42\nFoxtrot 4450--43\nQuickstep 5052505247\nSamba 5063505049\nMambo 5656--44\nViennese waltz 43162586039\nPaso doble 6065--26\nPolka 7676--41\nMiscellaneous ----432\nFigure 5: The distribution of styles in the test set, including\nthe range of tempos (where given), the suggested tempo ranges\nforeachstylestakenfromBallroomdancers.com(2002)(where\navailable), and the meter andnumber of pieces in that style.\nThe next step involves calculating a weighted sum of the peri-\nodicity weights Sidescribed in previous sections. This is com-\nputed separately for even and odd meters, since the distribution\npatterns vary depending on the meter. For periodicity Tiwith\nweight Si, the weighted sums S∗\ni,eandS∗\ni,oare given by:\nS∗\ni,e=/summationdisplay\ni/negationslash=kwe(pi, qi)Siu(Ti)v(Ei)\nS∗\ni,o=/summationdisplay\ni/negationslash=kwo(pi, qi)Siu(Ti)v(Ei)\nwhere weand woare empirically determined matrices of\nweights for even and odd meters respectively, uis a tempo\nweightingfunctionwhichrestrictstherangeofallowedtempos,\nvis an error weighting function which penalises periodicities\nwhich deviate from the supposed simple integer fraction rela-\ntionship, and Ei=|pi−qiri|represents the error in the ratio-\nnal approximationpi\nqi. The maximum value of S∗\nj,kdetermines\ntheperiodicityofthemeasurelevelandthemeter. Wecurrently\nassumethemeteriseither3\n4or4\n4,sothequarternote(beat)level\nof the metrical hierarchy isalso determined by this step.\nThe ﬁnal step is to choose the style, which we do using a sim-\nple rule-based approach, which combines information from the\ntempo, meter and periodicity distribution to make its decision.\nThe allowed tempo range and the meter of the dances, given in\nﬁgure 5, are used as constraints, and in cases where multiple\ndancesarepossibleforagiventempoandmeter,theperiodicity\ndistribution is used to distinguish between genres.\nThe simplest rules are for the pieces in triple meter, since there\nare only two genres, slow waltz and Viennese waltz, and these\n1TheViennesewaltzat43MPMisanoutlier;allotherinstancesof\nthis style are at least 60 MPM.are separated by a large tempo difference. The following 2\nrules, expressed in Prolog-like notation, ensure correct classi-\nﬁcation of instances of theseclasses:\nviennesewaltz(Meter, Tempo) :-\nMeter = 3,\nTempo > 40.\nslowwaltz(Meter, Tempo) :-\nMeter = 3,\nTempo <= 40.\nThe remaining rules are used for duple and quadruple meters,\nconceptuallybyﬁrstdividingthepiecesintotemporanges,and\nthen using the strengths of various periodicities to choose the\nmost likely genre. For ease of reading, we express each of the\nrules independently; the implementation is otherwise.\npolka(Meter, Tempo) :-\nMeter = 4,\nTempo > 70.\npasodoble(Meter, Tempo) :-\nMeter = 4,\nTempo <= 70,\nTempo > 58,\nweight(3/8) <= 3.\nquickstep(Meter, Tempo) :-\nMeter = 4,\nTempo <= 54,\nTempo > 48,\nweight(3/8) <= 3.\nsamba(Meter, Tempo) :-\nMeter = 4,\n(\n( Tempo <= 70,\nTempo > 48,\nweight(3/8) > 3\n);\n( Tempo <= 58,\nTempo > 54\n)\n).\njive(Meter, Tempo) :-\nMeter = 4,\nTempo <= 48,\nTempo > 35.\nslowfox(Meter, Tempo) :-\nMeter = 4,\nTempo <= 35,\nTempo > 29,\nmaxWeightAt(1/2).\nchacha(Meter, Tempo) :-\nMeter = 4,\nTempo <= 35,\nTempo > 29,\n!maxWeightAt(1/2),\nweight(_/8) > 4.\ntango(Meter, Tempo) :-\nMeter = 4,\nTempo <= 35,\nTempo > 29,\n!maxWeightAt(1/2),\nweight(_/8) <= 4.rumba(Meter, Tempo) :-\nMeter = 4,\nTempo <= 29,\nTempo > 25.\nblues(Meter, Tempo) :-\nMeter = 4,\nTempo <= 25.\nWe brieﬂy explain the two most complex rules, those for the\nsamba and cha cha. The tempo of the samba has quite a wide\nrangeofpossiblevalues,sothatitoverlapsthepasodobleatthe\nhigher end and the quickstep at the lower end. To distinguish\nthese dances in the overlapping parts of the tempo range, we\ntakeadvantageofthefactthatthesambarhythmtendstohavea\nstrong periodicity at the dotted quarter note level (see ﬁgures 3\nand 4) which is not present in the other dances which overlap\nits tempo range. The weight Siof this periodicity is compared\nwith a threshold, and if higher, the piece is classiﬁed as samba,\notherwise, the tempo determines the classiﬁcation.\nThe tempo range of the cha cha coincides with that of the slow\nfox and the tango. In this case, the weight of the half, quarter\nandeighthnotesareusedtoclassifythepiece. Iftheperiodicity\nwith maximum weight is the half note level, then piece is clas-\nsiﬁedasslowfox,otherwisetheweightofthesumoftheeighth\nnoteperiodicities(i.e.1\n8,3\n8,5\n8, ...)determinesthegenre,witha\nhigh value indicating cha chaand a low value indicating tango.\nThe current rule set was constructed in an ad hoc fashion, as\na proof of concept, that a reasonable level of classiﬁcation can\nbe obtained based only on periodicity information. The danger\nwith such a small set of data is that the rules overﬁt the data set\nandgeneralisepoorly. Forthisreason,weomitteddevelopment\nof rules for some of the styles which have very few instances,\nandwedidnottryamoreprincipledapproachtorulegeneration\nusingmachinelearning. Ifweareabletoobtainmoredata,this\nis an interesting avenue for further work.\n5 Tests and Results\nOneofthemajordifﬁcultiesinevaluatingsystemsthatdealwith\nmusic similarity and style is that there is no ground truth, that\nis, noobjectiveevaluation criteria orstandard testsets. Instead,\ncategory deﬁnitions are subjective, they change over time, and\nmostmusicconsistsofelementsfromamixtureofdifferentcat-\negories. Since the current work focusses on temporal features,\nwe chose a test set of music where rhythm is an important ele-\nment, and for which somewhat objective evaluation criteria are\navailable.\nThetestsetconsistsofstandardandLatindancemusic,whichis\nsubdivided into various styles such as tango, slow waltz, Vien-\nnese waltz, foxtrot, quickstep, jive, cha cha, samba and rumba\n(seeﬁgure5). Eachofthesestylesarereasonablywell-deﬁned,\ninthatdancersaregenerallyabletoidentifyandagreeuponthe\nstyle of such a piece within the ﬁrst few bars of music, as evi-\ndencedbythegeneraluniformityofthechosendancestylesata\nball. Furthermore, the test set consisted of CDs where the style\nofdanceand/ortempoareprintedontheCDcover,providingan\nindependentmeansofevaluation. Butdespitetheapparentclar-\nity, there is known to be some degree of overlap between the\nvarious dance styles, such that some pieces of music ﬁt more\nthan one type of dance, so perfect results are never to be ex-\npected. The test set consists of 161 pieces for which the styleIOI-Clustering Correlation\nTempo 53/96 65/96\nMeter 142/161 150/161\nStyle 36/52 52/65\nFigure 7: Summary results for recognition of tempo, meter and\nstyle.\nIOI-Clustering Correlation\nHalf tempo 4 10\nDouble tempo 24 16\nWrong meter 14 5\nOther 1 0\nFigure 8: Counts of each type oftempo error.\nis given, and 96 for which the tempo is given. There are 17\ndifferent styles for which the tempo is given (but not for all in-\nstances), plus a further 9 styles for which the tempo is never\ngiven (the row marked “miscellaneous” in ﬁgure 5).\nThe ﬁrst results deal with the periodicity detection algorithms.\nBoth algorithms produce a ranked list of periodicities, and we\ncomparethesewiththetempoandstyleprintedontheCDcover.\nWecalltheperiodicitiescorrespondingtothemeasureandbeat\nlevelsofthemetricalhierarchythemeasureperiodandthebeat\nperiodrespectively. Figure6shows,foreachpositionfrom1to\n10, the number of songs for which the measure period (respec-\ntively the beat period) was ranked at this position. From these\nresults it appears that the correlation method ranks the impor-\ntant periodicities higher, but it is also the case that the correla-\ntion method produces shorter lists of periodicities, which may\nexplain both the higher ranking and the higher number of un-\nranked beat and measure periods.\nThe main results are shown in ﬁgure 7. The ﬁrst row shows the\nnumberofsongsforwhichthecalculatedmeasureperiodagrees\nwith the given tempo on the CD (plus or minus 3 measures per\nminute). We use measures per minute for tempo instead of the\nmore usual beats per minute, because this is the format of the\ndetails printed on the CD liner notes. This value will be wrong\nif either the beats per minute or the meter is wrong.\nWeexaminethenatureofthetempoerrorsinﬁgure8. Thema-\njority of errors are due to selecting the wrong metrical level,\nthat is choosing a measure period which is half or double the\ncorrect (i.e. notated) value. In other words, the system chose a\nmusically plausible solution which didn’t correspond with the\nintention of the musicians. This is a common problem which is\nreported in tempo induction systems (Dixon, 2001) and a phe-\nnomenon that also occurs in tapping experiments with human\nsubjects (Dixon and Goebl, 2002).\nAllothererrorsexceptonewereduetoselectingthewrongme-\nter, so that even if the beat period were correct, the measure\nperiod would be wrong because it contains the wrong number\nof beats. The remaining error occurred on a piece that contains\nmanytriplets,andthesystemchosethetripletsasthebeatlevel,\nbut (surprisingly) also chose a binary grouping of these triplets\nasthemeasurelevel. Itisunlikelythatpeoplewouldmakethese\ntypes of errors.\nThe second row of results (ﬁgure 7) shows the meter recogni-tion results, which appear to be very good. However, there are\nonly 18 pieces in3\n4time, so the IOI-clustering results would be\nimproved (marginally) by replacing this part of the system by\none that always predicts4\n4time! More data is required to deter-\nminehowwellthispartofthesystemreallyfunctions. Inrecent\nwork, Gouyon and Herrera (2003) report over 90% accuracy in\ndistinguishing duple and triple meters.\nThe style recognition results range from 69% (for the IOI-\nclusteringdata)to80%(fortheautocorrelationdata),assuming\nthe data set is restricted to pieces for which the tempo is cor-\nrectly recognised. (Since none of the dance genres has a tempo\nrange wide enough to accommodate a factor of two error, it is\nimpossible for the system to predict style correctly once it has\nthe wrong tempo. The wrong meter also guarantees failure in\nstyle recognition, but these were not deleted from the style re-\nsults.) The confusion matrix (ﬁgure 9) shows the nature of the\nclassiﬁcation errors. Some errors, such as the confusion of cha\nchawithtangoandslowfox,showaweaknessintheclassiﬁca-\ntion rules, whereas others, such as classifying boogie and rock\nandrollasjive,aretobeexpected,asthegenresareveryclosely\nrelated. In fact, since there are no rules for boogie or rock and\nroll,thesepiecescouldnotbecorrectlyclassiﬁedbythepresent\nsystem.\n6 Conclusion\nWe presented a comparison of two methods of generating a\nranked list of periodicities from an audio ﬁle, and found that\nan autocorrelation-based approach gave better results than one\nbased on processing discretely detected onsets. The periodicity\npatterns were used to predict tempo, meter and genre of differ-\nent types of dance music with some success. The major source\nof error was in choosing the periodicity which corresponds to\nthemeasurelevelofthemusic. Whenthiswascorrectlychosen,\nthe classiﬁcation of autocorrelation-based periodicities reached\n80% success. This is particularly surprising when one consid-\ners that no rhythmic patterns (i.e. sequences of durations) were\nused, nor timbral, nor melodic, nor harmonic features. Tzane-\ntakisandCook(2002)reporta61%successrateforclassifying\nmusic into 10 (non-similar) genres, using features representing\ntimbre, rhythm and pitch. The overall success rate in this work\n(includingtempodetectionerrors)isperhapslower,butitisim-\npossible to make a meaningful comparison due to the different\nnature of the tasks, methodsand data.\nThecurrentworkislimitedbythesmalltestsetandtheaccom-\npanyingdangerofoverﬁtting. Infutureworkwehopetobuilda\nlargertestsetandinvestigatetheuseofautomaticclassiﬁcation\ntechniques.\nPeriodicitiesgiveinformationaboutthemetricalstructureofthe\nmusic,butnottherhythmicstructure,whicharisesfromtherel-\nativetimingofonsetswithinandbetweenthevariousfrequency\nbands. A fruitful area for further work would be to extract and\nencode commonly occurring rhythmic patterns, which is (in-\ntuitively at least) a better way of identifying genres of dance\nmusic. (Notethatthisisverydifferenttoexaminingperiodicity\ndistributions.) As a starting point, there is a large body of lit-\nerature on beat tracking involving the analysis of sequences of\ntemporal events in order to estimate tempo, meter and metrical\nboundaries (see Dixon, 2001, for an overview).Rank: 12345678910none\nMethod 1: Measure 13161691113133200\nIOI-Clustering Beat 11161815912101103\nMethod 2: Measure 1920251013110007\nCorrelation Beat 302520740000010\nFigure 6: Position of the periodicities corresponding to the bar and measure levels in the ranked lists of IOI clusters and autocorre-\nlation peaks.\nPDSATASFQURRRUSWCHBOWWFOJIMA\nPD5-------------\nSA -3-----------1\nTA --6-----1-----\nSF ---4----1---1-\nQU -3--6---------\nRR --------------\nRU --1---4-------\nSW -------6------\nCH --------------\nBO --------------\nWW ----------2---\nFO --------------\nJI -----1---2-216-\nMA --------------\nFigure 9: Confusion matrix for correlation-based classiﬁcation. The columns refer to the actual dance style, and the rows the\npredicted style. The abbreviations for the dance styles are: paso doble (PD), samba (SA), tango (TA), slow fox (SF), quickstep\n(QU), rock and roll (RR), rumba (RU), slow waltz (SW), cha cha (CH), boogie (BO), Viennese waltz (WW), foxtrot (FO), jive (JI)\nand mambo (MA).\nAcknowledgements\nThis research is part of the project Y99-INF, sponsored by\nthe Austrian Federal Ministry of Education, Science and Cul-\nture (BMBWK) in the form of a START Research Prize. The\nBMBWK also provides ﬁnancial support to the Austrian Re-\nsearch Institute for Artiﬁcial Intelligence. Thanks also to the\nanonymous reviewers of this paper for their insightful com-\nments.\nReferences\nBallroomdancers.com (2002). Learn the dances. Retrieved 24\nApril, 2003, from:\nhttp://www.ballroomdancers.com/Dances/ .\nBrown,J.(1993). Determinationofthemeterofmusicalscores\nby autocorrelation. Journal of the Acoustical Society of Amer-\nica, 94(4):1953–1957.\nDixon, S. (2001). Automatic extraction of tempo and beat\nfromexpressiveperformances. JournalofNewMusicResearch ,\n30(1):39–58.\nDixon, S. and Goebl, W. (2002). Pinpointing the beat: Tapping\nto expressive performances. In 7th International Conference\nonMusicPerceptionandCognition(ICMPC7) ,pages617–620,\nSydney, Australia.\nDixon, S., Goebl, W., and Widmer, G. (2002). Real time track-\ning and visualisation of musical expression. In Music and\nArtiﬁcial Intelligence: Second International Conference, IC-\nMAI2002 , pages 58–68, Edinburgh, Scotland. Springer.Goto, M. and Muraoka, Y. (1995). A real-time beat tracking\nsystem for audio signals. In Proceedings of the International\nComputer Music Conference , pages 171–174, San Francisco\nCA. International Computer Music Association.\nGouyon, F. and Herrera, P. (2003). Determination of the meter\nof musical audio signals: Seeking recurrences in beat segment\ndescriptors. In Presented at the 114th Convention of the Audio\nEngineering Society , Amsterdam, Netherlands.\nLarge, E. and Kolen, J. (1994). Resonance and the perception\nof musical meter. Connection Science , 6:177–208.\nPaulus, J. and Klapuri, A. (2002). Measuring the similarity\nof rhythmic patterns. In Proceedings of the 3rd International\nConference on Musical Information Retrieval . IRCAM Centre\nPompidou.\nRosenthal, D. (1992). Emulation of human rhythm perception.\nComputer Music Journal , 16(1):64–76.\nScheirer, E. (1997). Pulse tracking with a pitch tracker. In\nProceedings of IEEE Workshop on Applications of Signal Pro-\ncessing to Audio and Acoustics , Mohonk, NY.\nTzanetakis, G. and Cook, P. (2002). Musical genre classiﬁca-\ntion of audio signals. IEEE Transactions on Speech and Audio\nProcessing , 10(5):293–302."
    },
    {
        "title": "Position Indexing of Adjacent and Concurrent N-Grams for Polyphonic Music Retrieval.",
        "author": [
            "Shyamala Doraisamy",
            "Stefan M. Rüger"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417919",
        "url": "https://doi.org/10.5281/zenodo.1417919",
        "ee": "https://zenodo.org/records/1417919/files/DoraisamyR03.pdf",
        "abstract": "In this paper we examine the retrieval performance of adjacent and concurrent n-grams generated from polyphonic music data.  We deploy a method to index polyphonic music using a word position indexer with the n-gram approach.  Using all possible combinations of monophonic sequences from polyphonic music data, “overlaying” word locations within a document are obtained, such as needed with polyphony (i.e. where more than one word can assume the same word position).  The feasibility in utilising the position information of polyphonic ‘musical words’ is investigated using various proximity-based and structured query operators available with text retrieval system.  Our experiments show that nested phrase operators improve the retrieval performance and we present the results of our comparative study on a collection of 5456 polyphonic pieces encoded in the MIDI format. 1",
        "zenodo_id": 1417919,
        "dblp_key": "conf/ismir/DoraisamyR03",
        "keywords": [
            "polyphonic music data",
            "n-gram approach",
            "word position indexer",
            "monophonic sequences",
            "polyphony",
            "text retrieval system",
            "proximity-based",
            "structured query operators",
            "nested phrase operators",
            "comparative study"
        ],
        "content": "Position Indexing of Adjacent and Concurrent N-Gram s for Polyphonic \nMusic Retrieval \nShyamala Doraisamy and Stefan Rüger \nDepartment of Computing, South Kensington Campus \nImperial College London \nLondon SW7 2AZ, England \n{sd3,s.rueger}@imperial.ac.uk \n \nAbstract \nIn this paper we examine the retrieval performance of \nadjacent and concurrent n-grams generated from \npolyphonic music data.  We deploy a method to \nindex polyphonic music using a word position \nindexer with the n-gram approach.  Using all possib le \ncombinations of monophonic sequences from \npolyphonic music data, “overlaying” word locations \nwithin a document are obtained, such as needed with  \npolyphony (i.e. where more than one word can \nassume the same word position).  The feasibility in  \nutilising the position information of polyphonic \n‘musical words’ is investigated using various \nproximity-based and structured query operators \navailable with text retrieval system.  Our experime nts \nshow that nested phrase operators improve the \nretrieval performance and we present the results of  \nour comparative study on a collection of 5456 \npolyphonic pieces encoded in the MIDI format. \n1 Introduction \nThe robustness of the use of n-grams have been show n in \nsome studies, both text and music (Harding et al, 1 997; \nDoraisamy and Rüger, 2003).  In this paper we study  the \nimprovement of the n-gram approach to polyphonic mu sic \nretrieval with the use of term adjacency.  In gener al, proximity \ninformation can be quite effective in improving pre cision of \ntext searches.  We outline a method to obtain adjac ent and \nconcurrent term locations from polyphonic music.  S tructured \nquery formulations of music queries are used in inv estigating \nthe retrieval performance of position indexing of a djacent and \nconcurrent ‘musical words’.  The following sections  contain \ndescriptions of retrieval performance experiments u sing \nvarious structured query formulations. \n2 Position indexing \n2.1 Pattern extraction and encoding \nWith the n-gram approach to full-music indexing of polyphonic music data, a “bag of terms” is generate d for the \nindexing process.  A polyphonic piece is encoded as  an \nordered pair of onset times (in milliseconds) and p itch (in \nMIDI semitone numbers), and these are sorted based on the \nonsets times.  There may possibly be a few differen t pitches \ncorresponding to one particular onset time.  The pi tches with \nsimilar onset times are grouped together as musical  events.  \nUsing the gliding window approach, this sequence of  events is \ndivided into overlapping subsequences of n different adjacent \nevents, each characterised by a unique onset time.  For each \nwindow, all possible monophonic pitch sequences are  \nextracted and corresponding musical words are const ructed.     \nN-grams are generated from all the possible monopho nic \nsequences using pitch differences and ratios of ons et time \ndifferences.  A sequence of n onset times generates  n-1 pitch \nintervals and n-2 time ratios, all of which are quantised and \nencoded as a word with 2 n-3 letters.  For an in-depth \ndiscussion, see Doraisamy and Rüger (2003). \n2.2 Adjacent and concurrent n-grams \nIn utilising the time-dependent element of polyphon ic music \ndata where the concurrency and sequencing informati on of the \ndata is considered, a ‘polyphonic musical word posi tion \nindexer’ is needed.  This is briefly illustrated wi th the example \nbelow.  The first five onsets of the music excerpt given in \nFigure 1 would generate a text document as shown at  the \nbottom of Figure 1, with the corresponding position s of the \nadjacent and concurrent ‘musical words’. \n \n \n1 bYaYA 2 aYAYl 2 aYAYC 3 AYlFG 3 AYCFh 3 AYlFC 3 A YCFl 4 \nlFGYJ 4 CFhYJ 4 lFCYN 4 CFlYN 4 lFGY8 4 CFhY8 4 lFC YD 4 CFlYD 4 \nlFGYd 4 CFhYd 4 lFCY8 4 CFlY8 5 GYJfb 5 hYJfb 5 CYN fb 5 lYNfb 5 \nGY8fH 5 hY8fH 5 CYDfH 5 lYDfH 5 GYdfL 5 hYdfL 5 CY8 fL 5 lY8fL  \nFigure 1 Adjacent and concurrent ‘musical words’ wi th their \npositions from an excerpt of Mozart’s Alla Turca. \n2.3 Query formulation \nThe Lemur Toolkit ( http://www2.cs.cmu.edu/~lemur ) was \nmodified to index the ‘polyphonic musical words’.  The \nstructured and proximity-based operators available were used Permission to make digital or hard copies of all or  part of this work for \npersonal of classroom use is granted without fee pr ovided that copies are not \nmade or distributed for profit or commercial advant age and that copies bear \nthis notice and the full citation on the first page .   2003 The Johns Hopkins \nUniversity. to investigate the retrieval performance.  Definiti ons of \noperators adopted for our study as given by Lemur a re listed \nin Figure 2. \n \nSum operator : #sum(T 1…T n) \nThe terms or nodes contained in the sum operator ar e treated as having equal \ninfluence on the final result.  The belief values p rovided by the arguments of \nthe sum are averaged to produce the belief value of  the #sum node. \nOrdered Distance Operator : #odN (T 1…T n) \nThe terms within an ODN operator must be found in a ny order within a \nwindow of N words of each other in the text in orde r to contribute to the \ndocument’s belief value.  \nAnd  operator : #and (T 1…T n) \nThe more terms contained in the AND operator which are found in the \ndocument , the higher the belief value of the docum ent. \nOr operator: #or (T 1…T n) \nOne of the terms within the OR operator must be fou nd in a document for that \ndocument to get credit for this operator.    \n \nFigure 2: Structured and proximity-based operators.  \nBased on the monophonic study by Pickens (2000), us ing \nnested phrase operators in a manner that attempts t o recapture \nthe original sequentiality of the songs produces mo re precise \nresults.  We use this query formulation towards our  \ninvestigation of querying monophonic queries agains t a \npolyphonically encoded collection.  A monophonic th eme \nextracted from Figure 1 is encoded as: \nbYaYA aYAYC AYCIB CIBib BibYa bYaYA aYAYD AYDIA DIA ia \nAiaYa aYaYA aYAYG AYGYb GYbYa bYaYA aYAYB AYBYb BYb Ya \nbYaYA aYAYC \n \nWe arbitrarily selected two contiguous musical word s for the \nformation of smaller phrases within a longer query.   The query \nwould then be reformulated as: \n#SUM ( #ODN3(bYaYA aYAYC) \n#ODN3(AYCIB CIBib) \n  …  \n#ODN3(bYaYA aYAYC)) \n3 Results \nA collection of 5456 polyphonic music pieces in the  MIDI \nformat and 25 monophonic excerpts extracted from th is \ncollection were used as experimental monophonic que ries.  \nFor each query, there were several performances fro m the \ncollection that was considered similar.  The pitch and rhythm \ndimensions with the value n=4 was used in generating musical \nn-grams and indexed as PR4, and with position infor mation as \nPPR4.  In reducing the terms indexed, the upper and  lower \nenvelopes of the polyphonic data were used and inde xed as \nPR4ENV and PPR4ENV respectively. The probabilistic \nmodel supported by Lemur using the Okapi BM25 funct ion \nfor weighting was adopted for retrieval using PR4 a nd \nPR4ENV.  The relevance assumptions and indexing str ategies \nare discussed in Doraisamy and Rüger (2003).   \nThe precision-at-15 measure was used for evaluation .  The \nweighted average (W.A.) of this precision was used - the \nretrieval performance of the 25 queries averaged, w eighted by \nthe number of relevant documents for each query.  B ased on \nthe rank position of the relevant document, the Mea n Reciprocal Rank (MRR) measure was used.  Although \nnormally used with the known-item search, the MRR m easure \nwas used in this context based on the best rank of the relevant \ndocuments retrieved at precision-at-15.  From Table  1, it is \nclear that the nested phrase operators improve the precision \nwhere a MRR measure of 0.70 was obtained.   \n  W.A. MRR \nPR4  52 0.63 \nPR4ENV  62 0.66 \nSum 48 0.57 \nNested 58 0.70 \nAND 49 0.56 PPR4 \nOR 62 0.61 \nSum 62 0.62 \nNested 62 0.66 \nAND 56 0.65 PPR4ENV \nOR 56 0.62 \nTable 1: Retrieval measures \n4 Conclusion and Future Work \nWe have shown the feasibility of position indexing of \npolyphonic music and the results show that using ne sted \nphrase operators is promising. Although, the improv ement in \nthe use of phrase operators has not significantly i mproved the \nprecision, we continue to work on formulating a sim ilarity \nmeasure in retrieving adjacent and concurrent ‘poly phonic \nmusical words’.   \nN-grams generated from monophonic queries would pos e a \nproblem when querying against its polyphonic versio n due to \nintercepting accompanying onsets.  Another problem with the \nretrieval could also be due to erroneous queries, w hich are \nhighly probable with music queries such as QBH. A \nproximity-based operator more specific to music ret rieval is \nclearly needed.  We are currently working on a ‘mus ical \nordered distance operator’ whereby this difference between n-\ngrams generated from the query and the relevant pol yphonic \ndocument indexed is given by a similarity measure.   \nAcknowledgements \nThis work is partially supported by the EPSRC, UK.   \nReferences \nDoraisamy, S & Rüger, S. (2003). Robust Polyphonic Music \nRetrieval with N-grams. Journal of Intelligent Information \nSystems, Kluwer Academic Publishers, 21(1), 53 −70. \n \nHarding, S.M, Croft, W.B. & Weir, C. (1997), Probab ilistic \nRetrieval of OCR Degraded Text Using N-Grams, In \nResearch and Advanced Technology for Digital Librar ies , \nCarol Peters and Constantino Thanos, Editors, pp 34 5-359. \n \nPickens, J. (2000). A Comparison of Language Modeli ng and \nProbabilistic Text Information Retrieval Approaches  to \nMonophonic Music Retrieval, ISMIR 2000 , U.S.A."
    },
    {
        "title": "Toward the scientific evaluation of music information retrieval systems.",
        "author": [
            "J. Stephen Downie"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417121",
        "url": "https://doi.org/10.5281/zenodo.1417121",
        "ee": "https://zenodo.org/records/1417121/files/Downie03.pdf",
        "abstract": "This paper outlines the findings-to-date of a project to assist in the efforts being made to establish a TREC-like evaluation paradigm within the Music Information Retrieval (MIR) research community. The findings and recommendations are based upon expert opinion garnered from members of the Information Retrieval (IR), Music Digital Library (MDL) and MIR communities with regard to the construction and implementation of scientifically valid evaluation frameworks. Proposed recommendations include the creation of data-rich query records that are both grounded in real-world requirements and neutral with respect to retrieval technique(s) being examined; adoption, and subsequent validation, of a “reasonable person” approach to “relevance” assessment; and, the development of a secure, yet accessible, research environment that allows researchers to remotely access the large-scale testbed collection. 1",
        "zenodo_id": 1417121,
        "dblp_key": "conf/ismir/Downie03",
        "keywords": [
            "Music Information Retrieval",
            "TREC-like evaluation paradigm",
            "establishing",
            "MIR research community",
            "expert opinion",
            "scientifically valid evaluation frameworks",
            "data-rich query records",
            "neutral with respect to retrieval technique",
            "reasonable person approach",
            "research environment"
        ],
        "content": "Toward the Scientific Evaluation of  Music Information Retrieval Systems\nJ. Stephen Downie\nGraduate School of Library and Information Science\nUniversity of Illinois at Urbana-Champaign\njdownie@uiuc.edu\nABSTRACT\nThis paper outlines the findings-to-date of a project\nto assist in the efforts being made to establish aTREC-like evaluation paradigm within the MusicInformation Retrieval (MIR) research community.The findings and recommendations are based uponexpert opinion garnered from members of theInformation Retrieval (IR), Music Digital Library(MDL) and MIR communities with regard to theconstruction and implementation of scientificallyvalid evaluation frameworks. Proposedrecommendations include the creation of data-richquery records that are both grounded in real-worldrequirements and neutral with respect to retrievaltechnique(s) being examined; adoption, andsubsequent validation, of a “reasonable person”approach to “relevan ce” assessment; and, the\ndevelopment of a secure, yet accessible, researchenvironment that allows researchers to remotely\naccess the large-scale testbed collection.\n1 INTRODUCTION\nMusic Information Retrieval (MIR) is a multidisciplinary\nresearch endeavor that strives to develop innovative content-based searching schemes, novel interfaces, and evolving\nnetworked delivery mechanisms in an effort to make theworld’s vast store of music accessible to all. Some teams aredeveloping “Query-by-Singing” systems (e.g., Haus andPollastri (2001), Birmingham et al. (2001)), some “Query-by-Note” systems (e.g., Doraisamy and Rüger (2002), Pickens(2000)), some “Query-by-Example” systems (e.g., Haitsmaand Kalker (2002), Harb and Chen (2003)), somecomprehensive music recommendation and distributionsystems (e.g., Pauws and Eggen (2002), Logan (2002)), somemusical analysis systems (e.g., Kornstädt (2001), Barthélemyand Bonardi (2001)), and so on. Good overviews of MIR’sinterdisciplinary research areas can be found in Downie(2003), Byrd and Crawford (2002), Futrelle and Downie(2002).\nIn this paper, Section 1 outlines the current scientific problemfacing MIR research. Sections 2-3 report upon the findings-to-\ndate of the “MIR/MDL Evaluation Project,” with issuessurrounding the creation of a TREC-like evaluation paradigmfor MIR as the central focus. Section 4 highlights the progressbeing made concerning the establishment of the necessary testcollection(s). Section 5 concludes with a summary andoutlines some of the key challenges uncovered that requirefurther investigation.\n1.1 Current Scientific Problem\nNotwithstanding the promising technological advancements\nbeing made by the various research teams, MIR research hasbeen plagued by one overarching difficulty: There has been noway for research teams to scie ntifically compare and contrast\ntheir various approaches. This is because there has existed:\n1. no standard collection of music against which each\nteam could test its techniques;\n2. no standardized sets of performance tasks; and,\n3. no standardized evaluation metrics.\nThe MIR community has long recognized the need for a more\nrigorous and comprehensive evaluation paradigm. A formalresolution expressing this need was passed, 16 October 2001,by the attendees of the Second International Symposium on\nMusic Information Retrieval  (ISMIR 2001). (See\nhttp://music-ir.org/mirbib2/resolution  for the list of\nsignatories.)  \nOver a decade ago, the Nationa l Institute of Standards and\nTechnology developed a testing and evaluation paradigm for\nthe text retrieval community, called TREC ( Text REtrieval\nConference ; http://trec.nist.org/overview.html ). Under\nthis paradigm, each text retrieval team is given access to:\n1. a standardized, large-scale test collection of text;\n2. a standardized set of test queries; and,\n3. a standardized evaluation of the results each team\ngenerates.\nBecause of the strong overlap  between the MIR and the\ntraditional IR communities, many  informally suggested that\nMIR researchers should explore the TREC model as a keycomponent of MIR evaluation. In July 2002, the author\nsecured funding from the Andrew W. Mellon Foundation tobegin exploratory work on the “Establishing MusicInformation Retrieval (MIR) and Music Digital Libraries(MDL) Evaluation Frameworks Project.” The mandate of the“MIR/MDL Evaluation Project” is “…to establish theinfrastructural foundation for the formation of meaningful and\nPermission to make digital or hard copi es of all or part of this work for\npersonal of classroom use is granted w ithout fee provided that copies are not\nmade or distributed for profit or comme rcial advantage and that copies bear\nthis notice and the full citation on the first page.   2003 The Johns Hopkins\nUniversity.comprehensive MIR/MDL evaluation through the\nidentification and/or creation of standardized test collections,retrieval tasks and performance metrics…”(Downie, 2002).\n2 Data Collection Method\nThe Delphi method (Linstone and Turoff, 1975) of data\ncollection forms the basis of the analytic modality employedby the “MIR/MDL Evaluation Project.” The Delphi approachis an iterative method wherein initial prompting questions areput before a community of experts and their opinionssolicited. These opinions are then brought together and trendsuncovered. The resultant data then is fed back to thecommunity for further input and refinement. The goal of thisapproach is to allow consensus on the uncovered trends toemerge naturally from these learned opinions. There are nineprompting questions used in this study providing specificcontexts for participants (Dow nie, 2002). In addition to the\naforementioned nine detailed/sp ecific questions, each of the\nparticipants is presented with the four, more basic, questionsthat represent the intellectual underpinnings of the project(Downie, 2002):\n1. How do we determine, and then appropriately classify,\nthe tasks that should make up the legitimate purviewsof the MIR/MDL domains?\n2. What do we mean by “success”? What do we mean by\n“failure”?\n3. How will we decide whether one MIR/MDL approach\nworks better than another?\n4. How do we best decide which MIR/MDL approach is\nbest suited for a particular task?\nThree rounds of input are planned for the “MIR/MDL\nEvaluation Project.” Two of  these have already been\nconcluded. The third, and final, round will close in August2003. The input rounds consist of a formal solicitation forWhite Papers from the MIR, MDL and IR communities withthe prompting and primary questions as the basis fordiscussion. Each of the completed rounds culminated in theconvening a special meeting wherein the participants wereable to expound upon their White Paper opinions andexchange ideas. The White Papers from each round are beingcollected in successive editions of The MIR/MDL Evaluation\nWhite Paper Collection . See \nhttp://music-\nir.org/evaluation  for the most recent edition. Information\nabout each of the first two input rounds follows.\n3 Emergent Themes and Commentary\nRound #1 Meeting : “The Workshop on the Creation of\nStandardized Test Collections, Tasks, and Metrics for Music\nInformation Retrieval (MIR) and Music Digital Library(MDL) Evaluation” was held at the Second Joint Conference\non Digital Libraries  (JCDL 2002) in July of 2002\n(\nhttp://www.ohsu.edu/jcdl ). Dr. Ellen Voorhees, Project\nManager of the National Institute of Standards andTechnology's Text REtrieval  Conference  (TREC)\n(\nhttp://trec.nist.gov ), presented the keynote address\n(Voorhees, 2002). Her White Paper presentation focussed onthe potential applicability of the TREC evaluation paradigm tothe needs of the MI R/MDL community. Fifteen other authors,\npresenting eleven White Papers, also participated in Round#1. The creation of a TREC-like evaluation model was thecentral theme played out by the participants. “TREC-like” is\nused here deliberately, as attendees made it clear thatMIR/MDL systems, because they  deal with music, are not\ndirectly analogous to text retrieval systems. Issues raised formore detailed examination included the successful integrationof multiple formats (i.e., a udio—(Reiss and Sandler, 2002a;\nPardo, Meek, and Birmingham, 2002), symbolicrepresentations—(Bainbridge, 2002; Montalvo, 2002),metadata and scores—(MacMillan, 2002)), analysis of real-world queries (i.e., needs and uses (Cunningham, 2002;Futrelle, 2002)), and the set of tasks to be examined (Melucciand Orio, 2002), including recreational uses, educational uses,scholarly uses (Issacson, 2002), etc.. In short, the consensuswas that work should pro ceed on developing TREC-like\nevaluations with the provisos that:\n1. any TREC-like approach developed be centered on\nthe unique nature of music information and not“artificially imposed” on MIR/MDL systems simplybecause of the perceived “convenience” of theapproach;\n2. the integration of music metadata not be overlooked;\nand,\n3. the TREC-like approach not  become the sole means\nof evaluating the performance of MIR/MDL systems.\nRound #2 Meeting : “The Panel on Music Information\nRetrieval Evaluation Frameworks” was held as part of ISMIR2002. Dr. Edie Rasmussen, (Prof., University of Pittsburgh)delivered the keynote White Paper (Rasmussen, 2002) whichfurther developed the TREC-like evaluation theme byproviding insights on the strengths and weaknesses of theTREC paradigm. Twelve authors also contributed eight Round#2 White Papers. Almost every paper addressed issuessurrounding the requisite components of the large-scale testcollections needed for TREC- like evaluations (e.g., Herrera-\nBoyer (2002), Rüger (2002), Richard (2002)). One paperextended the large-scale test collection notion to encompass\nmultiple test collections housed  in multiple locations and\ninterconnected via a Music GRID (Dovey, 2002). Theimportance of delineating the nature of music-specificretrieval tasks — and their related queries — to be used inevaluation testing was another significant theme (e.g., Meek,Birmingham, and Pardo (2002), Södring and Smeaton (2002),Reiss and Sandler (2002b)). The idea that the TREC-likeevaluation scenario not be the sole evaluation approach usedwas iterated in Reiss and Sandler (2002b). Notwithstandingthe caveats expressed by Reiss and Sandler (2002b), sostrongly did the TREC leitmotif run through the White Papersof Round #2 that it is safe to summarize the consensus as“How do we move forward on making a TREC-likeevaluation scenario for MIR/MDL a reality?”\n3.1 Commentary on Emergent Themes\nGiven the overwhelming consensus on the establishment of a\nTREC-like evaluation paradigm, why is it that a TREC-likeapproach has not been adopted already? Participantsconsistently touched upon four problem areas that will provide\nsome insight into this question:\n1. the complexity of music information;\n2. the complexity of music queries;\n3. the nature of relevance within the context of MIR and\nthe applicability of precision and recall  as evaluation\nmetrics (terms defined in Section 3.1.3) and,\n4. the lack of access to music collections brought about\nby intellectual property law as practiced by the musicindustry.\nThe ordering of first three is significant. The complexity of\nmusic information can be seen as the cause of the complexity\nfound in real-world music queries. Query complexity, in turn,contributes to the difficulties associated with the assessmentof relevance (and thus the app licability of precision and recall\nas evaluation metrics).\n3.1.1 Problem #1: The complexity of music\nMusic information is inherently more complex than text\ninformation. Music information is a multifaceted amalgam ofpitch, tempo, rhythmic, harmonic, timbral, textual (i.e., lyricsand librettti ), editorial, praxis, and bibliographic elements.\nMusic can be represented as scores, MIDI files and otherdiscrete encodings, and in any number of analogue and digitalaudio formats (e.g., LPs, tapes, MP3s, CDs, etc.). Unlike mosttext, music is extremely plastic; that is, a given piece of musiccan be transposed, have its rhythms altered, its harmoniesreset, its orchestration recast, its lyrics changed, and so on, yetsomehow it is still perceived to be the “same” piece of music.\nThe interaction of music's complexity and plasticity make the\nselection of possible retrieval elements extraordinarilyproblematic. This, in turn, leads to difficulties on four fronts:\n1. Until such time as there is a “universal” music repository,\nthe determination of the most “representative” versions(and formats) of music objects for use in building testcollections remains an open problem. Given the problemsoutlined in Section 3.1.4, consensus is that the MIRcommunity will “make do” with whatever it will befortunate to acquire so long as efforts are made to expandthe collection over time. Section 4 discusses progressbeing made to alleviate this problem\n2. Test collection size is real concern. Because of the need\nfor multiple instances of symbolic, audio and metadatainformation for each piece in  the collection, a MIR\ntestbed will approach, if not  exceed, the storage limits of\nmost research facilities. That audio files tend to be large,relative to their symbolic counterparts, also contributessignificantly to this problem. A large-scale, multi-formatmusic test collection requires storage in the terabyterange: approximately two to three orders of magnitudegreater than the gigabyte-range text databases used in thead hoc  TREC evaluations (Voorhees, 2002). A solution to\nthe large dataset problem is discussed in Section 4.\n3. Establishing and maintaining workable linkages between\nthe various manifestations of each work (i.e., linkagesbetween and among a given pi ece’s audio, symbolic and\nmetadata information) is a non-trivial research problem(Dunn, Davidson and Isaacson, 2001; Smiraglia, 2001).Much more work needs to be done on this problem inorder that one retrieval method is not “privileged” overanother. This leads to the  notion of “retrieval neutrality”discussed in Section 3.1.2.\n4. Music queries — being themselves a kind of music\ninformation — are also plastic, complex and multifaceted.This implies that the formalized encapsulation of queriesin the “query records” for use in TREC-like testing (i.e.,“topic statements”) must, from the outset, be designed toreflect this fact. More about the “query problem” next.\n3.1.2 Problem #2: The complexity of music queries\nThere is a much-lamented paucity of formal literature\nreporting upon the analyses of the real-world informationneeds and uses of MIR/MDL users (Downie, 2003; Byrd andCrawford, 2002; Futrelle and Downie, 2002). In fairness, thispaucity is partially caused by the non-existence of MIR/MDLsystems containing music that users actually want. However,when such studies are attempted (e.g., Downie (1994), Itoh,(2000), Kim and Belkin (2002), Downie and Cunningham,(2002)), the disconnect between assumptions commonly madeby MIR researchers concerning the nature of music queries(i.e., simple hummed melodies, retrieval of known-items,identification of songs users have in-hand, etc.) and the real-world situation, is remarkable. To illustrate this point,compare Fig. 2 (a TREC topic statement (Voorhees, 2002))with Fig. 1 (a real-world music query (Cunningham, 2002)),both presented on the next page. Table 1 also illustrates thewide variety of information types contained in real-worldmusic queries along with the wide variety of intended uses forthe sought-after music.\n1\nThe consensus opinion among community members is thatgreat care must be taken in developing the TREC-like queryrecords, for their use will have significant scientificramifications, especially with regard to the validity of theresultant evaluation experiments. While there is much workyet to be done on finalizing the specific form of the TREC-like query records, a set of firs t principles is emerging. The\nquery records developed must:\n1. be grounded in real-world needs and uses;\n2. be representative of the complexity of real-world\nqueries (see Table 1);\n3. be neutral with regard to the retrieval method\nemployed; and,\n4. be data-rich so realistic and meaningful “relevance”\njudgements can be made. (Discussed in Section3.1.3.)\n                                                          \n1 The percentage values, which are most  likely idiosyncratic to the population\nexamined, are less important th an the categories themselves.The “retrieval neutrality” principle requires some explication.\nThe MIR community can be divided roughly into two camps:1) those engaged in symbolic retrieval research; and, 2) thoseexploring audio- and signal-processing techniques. Given thatno data exist on the comparative strengths and weaknesses ofthe techniques employed across the two camps, the consensus\nis that the TREC-like evaluation paradigm — at least in itsearly stages — must provide a means to make informedassessments on the relative merits of the two approaches. The\nidea of “symbol-only” and “audio-only” tracks is therefore notan attractive initial option. Related to this matter, the notion oftask-specific tracks, analogous to the video, interactive,natural language processing, etc. tracks in TREC, has beendiscussed. However, the apparent consensus is that earlyimplementations of the TREC-like evaluation scenario shouldbe conducted with a singular, unified collection of queriesuntil such time as participants feel comfortable with theprocess.\nSynthesizing from the suggestions made by the expert\nparticipants, it thus appears that a minimal TREC-like queryrecord needs to include th e following basic elements:\n1. High quality audio representation(s)\n2. Verbose Metadata:\ni. About the “user”\nii. About the “need”\niii. About the “use”\n3.   Symbolic representation(s) of the music presented\nFigure 2. A TREC to pic statement from Voorhees (2002 ).From: XXXXXXXXX\nSubject: Early 80's - Please identify this song! (it's *very* difficult, though)\nNewsgroups: alt.music.lyricsDate: 2000-12-14 09:42:24 PSTHi, thiis is so difficult because I only remember those damn FRAGMENTS of it, which can (in combination\nwith possible errors) make it VERY difficult to identify this song!\nBut I'll try my best to make myself clear as possible.This song MUST be from the period 1979-1984, most likely 1981 or 1982.Tempo: about 120 bpmSounds VERY close to a SAGA or Asia tune (maybe it is SAGA even! ;)OK here I go...(gonna add the chords for you guitarists out there ;)[verse 1]F    C                  Bb            Bb C\nCrazy ................ onto the ..... café\n     F         C            Bb\nI'm drinking coffee, she came away\n      F      C              Bb      Bb         C\nShe ordered .............. precious sum of money ???\nF          C        Bb\ndeedeedeedeedeedeedeedee ....<remaining text deleted>\nOhohohoo\n[(instrumental) F C Bb Bb C F C Bb][verse 2] [...]\n[chorus]\nFigure 1. A real-world information request posted to alt.music.lyrics as presented in Cunningham (2002).\n Information need description  % of Queries  Category of intended use  % of Queries\n BIBLIOGRAPHIC  75.2%  LOCATE (e.g., “Where can I find…”)  49.7%\n LYRICS  14.3%  RESEARCH (i.e., background information, etc.)  19.3%\n GENRE  9.9%  PERFORM (i.e., play pi ece(s) on instrument)  18.6%\n SIMILAR WORKS  9.9%  COLLECTION  BUILDING (i..e., add to pre-existing collection similar items)  18.0%\n AFFECT (i.e., description of mood)  7.5%  LISTEN (i.e., as opposed to perform)  6.8%\n LYRIC STORY  6.8%   \n TEMPO  2.5%   \n EXAMPLE  1.8%   \nTable 1. Categorization of real-world query and intended use el ements as developed and descri bed in Downie and Cunningham (2002 ).           <num> Number: 409\n<title> legal, Pan Am, 103<desc>Description:What legal actions have resulted from the destruction of Pan Am\nFlight 102  over Lockerbie, Scotland, on December 21, 1988?\n<narr> Narrative:Documents describing any charges, claims, or fines presented to or\nimposed  by any court or tribunal are relevant, but documents thatdiscuss charges made in diplom atic jousting are not relevant.One is struck by how these requirements are less like a\ntraditional TREC topic statement (Fig. 2) and more like thekind of information garnered in a traditional, well-conducted,reference interview (Dewdney and Michell, 1997; TheReference Interview, 2001). This suggests that theinvolvement of professional music librarians in thedevelopment of the TREC-like music query records is veryimportant — perhaps even critical.\n3.1.3 Problem #3: Whither relevance, precision and\nrecall?\nThe text IR community has had a set of standardized\nperformance evaluation metrics for last four decades. Sincethe Cranfield experiments of the early 1960's (Cleverdon,Mills and Keen, 1966), two metrics have predominated:precision  (i.e., the ratio of relevant documents retrieved to the\nnumber of documents retrieved); and, recall  (i.e., the ratio of\nrelevant documents retrieved to the number of relevantdocuments present in the system ). These metrics are the heart\nof the TREC evaluation paradigm. The key determinant in theuse of precision and recall as metrics is the apprehension ofthose documents deemed “relev ant” to a particular query.\nWhile there have been ongoing debates about the nature of“relevance” (see Schamber (1994)), its meaning has beenstable enough to make the TREC evaluations possible. Simplyput, a “document” is deemed to be “relevant” to a given queryif the document is “about” the same subject matter as thequery (i.e., there is an in tersection of “meaning” or\n“aboutness” between query and document).\nWithin the context of MIR eval uation, however, this meaning-\nbased approach to relevance a ssessment is clearly inadequate.\nFor example, what do Beethoven’s Piano Sonatas, orHendrix’s guitar solos, actua lly “mean”? The MIR community\nrecognizes this important shortc oming. In fact, the definition\nof “relevance” within the MIR context has been so\nproblematic that the precision and recall metrics are rarelyfound in the MIR literature. Studi es by Downie (1999), Foote\n(1997), Uitdenbogerd and  Zobel (1999), Södring andSmeaton (2002) are among the few that employ thesemeasures. Notwithstanding th is absence of a community\ntradition of use, the consensus opinion holds that the MIRcommunity should not shy away from creating a means toassess MIR systems within the TREC-like paradigm and thusshould continue to examine precision and recall as coremetrics.\nTo this end, it is hoped that by making the query records as\ndata-rich as possible, that a “reasonable person” standardcould emerge as the criterion for the judging the relevance ofreturned items. That is, there should be enough informationcontained within the query records that reasonable personswould concur as to whether or not a given returned item\nsatisfied the intention  of the query. The validity of the\n“reasonable person” assumption would, of course, be subjectto empirical verification.3.1.4 Problem #4: Collection building and intellectual\nproperty law\nMusic is expensive. In the cu rrent Post-Napster era, music\nrights-holders are notoriously  litigious. Recent changes to\ncopyright law in the United States have put into question thevery existence of “public domain” sources of audio recordings(see Downie (2003)). These three facts, when taken together,have effectively stopped the development of any large-scale,community-accessible, test collections comprising the\nnecessary audio, symbolic and metadata representations. Someprivate research institutions have acquired substantialcollections of audio files. However, these collections areintended for their in-house use only. Collection holders do notmake them accessible to others in the community for fear of\nbecoming the objects of expensive civil and criminallitigation.\nNotwithstanding these very r eal difficulties, some recent\ndevelopments have made it po ssible to begin construction of\nthe much-needed test collection database. The key here hasbeen convincing select rights-holders that MIR researcherscan be trusted to respect their property. This has meantdeveloping mechanisms whereby the intellectual propertyassets of the right-holders can be shown to be secure fromunlicensed access and distribution.\n4 Building a TREC-like Test  Collection: Important\nFirst Steps\nThe author and colleagues have begun to construct the world’s\nfirst-and-only, internationally -accessible, large-scale MIR\ntesting and development database . This will be housed at the\nUniversity of Illinois’s National Center for SupercomputingApplications (NCSA) (Fig. 3). Formal transfer and useagreements are being finalized with HNH Hong KongInternational, Ltd. (\nhttp://www.naxos.com ), the owner of the\nNaxos  and Marco Polo  recording labels. This will afford the\nMIR community research access to HNH’s entire  catalogue of\nClassical, Jazz, and Asian dig ital recordings. This generous\ngesture on the part of HNH re presents approximately 30,000\naudio tracks or about 3 terabytes of digital audio musicinformation. All Media Guide (\nhttp://www.allmusic.com )\nhas also agreed to follow HNH’s lead, enabling UIUC/NCSA\nto incorporate its vast database of music metadata within thesame test collection . All Media’s  dataset includes descriptive\ncatalogue records, discographies, and recordingclassifications.\n4.1 Test Collection Data base: System Overview\nGiven the unique opportunity that these rights-holders have\nafforded the MIR community, it is important that the MIRtesting and evaluation database  be constructed with three\ncentral features in mind:\n1. security for the property of the rights-holders,\nespecially important if we are to convince otherrights-holders to participate in the future;\n2. accessibility for both internal, domestic, and\ninternational researchers; and,3. sufficient computing and storage infrastructure to\nsupport the computationally- and data-intensivetechniques being investigated by the various researchteams.\nTo these ends, we are exploiting the expertise and resources of\nNCSA and its Automated Learning Group (ALG), headed byProf. Michael Welge. NCSA's systems have been designed tobe secure. Certificate-based auth entication for all users as well\nas means for encrypting data and data transfers arefundamental to NCSA’s security protocols.\nThe ALG has developed a data-to-knowledge system, D2K,\nwhich supports all phases of the data mining process . D2K\nwas originally designed to provide data mining professionalswith a flexible “sandbox” for developing and evaluating theperformance of a range of supercomputing techniques on avariety of data sets. Using the D2K technology as a startingpoint, we are creating a secure “Virtual Research Lab” (VRL)\nfor each participating research team. These VRLs will providesecure access to the test collection and the resources necessaryto conduct large-scale MIR evaluation experiments. Simplyput, we enhance the security of  the valuable music data by\nbringing the research teams to  the collection, rather than\ndistributing the collection willy-nilly around the globe.\nFor the transfer of the MIR TREC-like environment to the\ninternational, domestic and internal research teams, we areincorporating another ALG application, D2K-SL. D2K-SLbuilds upon current D2K modules to provide a set of pre-defined applications that guide users through thesupercomputing process. These tools will be instrumental insupporting the multidisciplinary na ture of MIR research and\nevaluation. Their relative ease-of-use should also help retainand encourage the participation in MIR research of such non-computer experts as librarian s, musicologists, Arts and\nHumanities students and educators, and business executives.In addition, we hope that these D2K-SL applications can beused to address other related re search thrusts, such as new\nMIR techniques, new interface designs and the developmentof protocols to make the pr oposed MIR GRID a viable entity\n(Dovey, 2002).\n5 Summary and Future Research\nThis paper has outlined the efforts being made to establish a\nscientifically valid TREC-like evaluation paradigm for MIRresearch. Expert opinion on the implementation of MIR/MDLevaluation frameworks was solicited, analyzed, and thensummarized. Major issues raised by participating expertsinclude addressing the complex nature of music information;adequately capturing the complex nature of music queries;recognition of the MIR “relev ance” problem; and, overcoming\nthe intellectual property hurdles to collection building.\nProposed solutions include the creation of data-rich query\nrecords that are both grounded in real-world requirements andneutral with respect to retrieval technique; adoption of a“reasonable person” approach to “relevance” assessment; and,\nthe establishment of TREC-like evaluation protocols. Finally,the development of a secure, yet accessible, researchenvironment at NCSA — one that allows researchers toremotely participate in the secure  use of the large-scale testbed\ncollection — represents a signi ficant first step forward in\nsurmounting the intellectual property hurdles plaguing MIRresearch and evaluation.\nSome of these proposed solutions will require further\ninvestigation and effort. In particular, we must work on the:\n1. explicit capturing and analysis of a wide variety real-\nworld music queries upon which to base the creation ofthe query records;\n2. development of formal requirements for the necessary\nelements (and their constituent data types) to be used inthe query records;\n3. validation of the “reasonable person” relevance\njudgement assumption through inter-rater reliabilitystudies; and,Figure 3. Schematic of the secure, yet accessible, test collection environment.\n4. continued acquisition of more music information (audio,\nsymbolic, and metadata) with a special effort to acquire“top hits” popular music and more non-Western musics tomake real-world, real-time, user studies a possibility. Theacquisition of non-Western musics is particularlyimportant as there is a strongly-perceived bias towardWestern music within current MIR research (Futrelle and\nDownie, 2002).\nAcknowledgements\nThe keynote speakers, contributors and meeting participants\nare all to be thanked. Drs. Don Waters and Suzanne Lodato,both of the Andrew W. Mellon Foundation, are thanked fortheir moral and financial support. Karen Medina, Joe Futrelleand Mike Welge are also thanked for their valuablecontributions and suggestions throughout the project.\nReferences\nBainbridge, D. (2002). Towards a workbench for symbolic\nmusic information retrieval. In J. S. Downie (Ed.), The\nMIR/MDL Evaluation Project White Paper Collection (2nd\ned., pp. 14-16). Champaign, IL: GSLIS.\nBarthélemy, J., & Bonardi, A. (2001). Figured bass and\ntonality recognition. In J. S. Do wnie and D. Bainbridge (Eds.),\nProceedings of the Second Inte rnational Symposium on Music\nInformation Retrieval  (ISMIR 2001 ) (pp. 129-136).\nBloomington, IN: Indiana University.\nBirmingham, W., Dannenberg, R. B., Wakefield, G. H.,\nBartsch, M., Bykowski, D., M azzoni, D., Meek, C., Mellody,\nM., & Rand, W. (2001). Musart: Music retrieval via auralqueries. In Second International Symposium on Music\nInformation Retrieval  (ISMIR 2001 ) (pp. 73-81).\nByrd, D., & Crawford, T. C. (2002). Problems of music\ninformation retrieval in the real 2orld. Information Processing\nand Management , 38, 249-272.\nCleverdon, C., Mills, J., & Keen, M. (1966). Factors\ndetermining the performance of indexing systems . Cranfield,\nUK: ASLIB Cranfield Research Project, College ofAeronautics.\nCunningham, S. J. (2002). User studies: A first step in\ndesigning an MIR testbed. In The MIR/MDL Evaluation\nProject White Paper Collection  (2nd ed., pp. 17-19).\nDewdney, P., & Michell, G. (1997). Asking \"why\" questions\nin the reference Interview: A theoretical Justification. Library\nQuarterly , 67, 50-71.\nDoraisamy, S., & Rüger, S. M. (2002). A comparative and\nfault-tolerance study of the use of n-grams with polyphonicmusic. In M. Fingerhut (Ed.), Proceedings of the Third\nInternational Conference on Music Information Retrieval(ISMIR 2002)  (pp. 101-106). Paris: IRCAM.Dovey, M. J. (2002). Music GRID: A collaborative virtual\norganization for music information retrieval collaboration andevaluation. In The MIR/MDL Evaluation Project White Paper\nCollection (2nd ed., pp. 50-52).\nDownie, J. S. (2002). Establishing music information retrieval\n(MIR) and music digital library (MDL) evaluationframeworks: Preliminary foundations and infrastructures. InThe MIR/MDL Evaluation Project White Paper Collection(2nd ed., pp. 3-6).\nDownie, J. S. (1994). The Musifind musical information\nretrieval project, phase ii: User assessment survey. InProceedings of the 22nd Annual  Conference of the Canadian\nAssociation for Information Science  (pp. 149-166). Toronto:\nCAIS.\nDownie, J. S. (1999). Evaluating a simple approach to music\ninformation retrieval: Conceiving melodic n-grams as text.Unpublished doctoral dissertation, University of WesternOntario, London, Ontario.\nDownie, J. S. (2003). Music information retrieval.  Annual\nReview of Information Science and Technology , 37, 295-340.\nDownie, J. S., & Cunningham, S. J. (2002). Toward a theory\nof music information retrieval queries: System designimplications. In Proceedings of the Third International\nConference on Music Information Retrieval  (ISMIR 2002)\n(pp. 299-300).\nDunn, J. W., Davidson, M. W., & Isaacson, E. J. (2001).\nIndiana University digital music library project: An update. InSecond International Symposium on Music InformationRetrieval  (ISMIR 2001 ) (pp. 137-138).\nFoote, J. (1997). Content-based retrieval of music and audio.\nIn SPIE  Vol. 3229. Multimedia Storage and Archiving\nSystems II . Bellingham, WA: SPIE Press.\nFutrelle, J. (2002). Three criteria for the evaluation of music\ninformation retrieval techniques against collections of musicalmaterial. In The MIR/MDL Evaluation Project White Paper\nCollection (2nd ed., pp. 20-22).\nFutrelle, J., & Downie, J. S. (2002). Interdisciplinary\ncommunities and research issues in music informationretrieval. In Third International Conference on Music\nInformation Retrieval (ISMIR 2002)  (pp. 215-221).\nHaitsma, J., & Kalker, T. (2002). A highly robust audio\nfingerprinting system. In Third International Conference on\nMusic Information Retrieval  (ISMIR 2002)  (pp. 107-115).\nHaus, G., & Pollastri, E. (2001). An audio front end for query-\nby-humming systems. In Second International Symposium on\nMusic Information Retrieval  (ISMIR 2001 ) (pp. 65-72).Herrera-Boyer, P. (2002). Setti ng up an audio database for\nmusic information retrieval benchmarking. In The MIR/MDL\nEvaluation Project White Paper Collection (2nd ed., pp. 53-\n55).\nIssacson, E. J. (2002). Music IR for music theory. In The\nMIR/MDL Evaluation Project White Paper Collection (2nd\ned., pp. 23-26).Harb, H., & Chen, L. (2003). A query by example music\nretrieval algorithm. In Proceedings of the 4th European\nWorkshop on Image Analysis for Multimedia InteractiveServices  (WIAMIS03)  (pp 122-128).\nItoh, M. (2000). Subject search for music: Quantitativeanalysis of access point selection.  In D. Byrd and J. S. Downie\n(Eds.), Proceedings of the  International Symposium on Music\nInformation Retrieval (ISMIR 2001) . Amherst, MA:\nUniversity of Massachusetts at Amherst.\nKim, J.-Y., & Belkin, N. J. (2002). Categories of music\ndescription and search terms and phrases used by non-musicexperts. In Third International Conference on Music\nInformation Retrieval (ISMIR 2002)  (pp. 209-214).\nKornstädt, A. (2001). The JRing system for computer-assistedmusicological analysis. In Second International Symposium\non Music Information Retrieval  (ISMIR 2001 ) (pp. 93-98).\nLinstone, H., & Turoff, M. (1975). The Delphi method:\nTechniques and applications . Boston, MA: Addison-Wesley.\nLogan, B. (2002). Content-based playlist generation:Exploratory experiments. In Third International Conference\non Music Information Retrieval  (ISMIR 2002)  (pp. 295-296).\nMacMillan, K. (2002). Common music notation as a sourcefor music information retrieval. In The MIR/MDL Evaluation\nProject White Paper Collection (2nd ed., pp. 27-28).\nMeek, C., Birmingham, W. P., & Pardo, B. (2002). What is asung query? In The MIR/MDL Evaluation Project White\nPaper Collection (2nd ed., pp. 56-57).\nMelucci, M., & Orio, N. (2002). A task-oriented approach forthe development of a test collection for music informationretrieval. In The MIR/MDL Evaluation Project White Paper\nCollection (2nd ed., pp. 29-31).\nMontalvo, J. (2002). A MIDI track for music informationretrieval. In The MIR/MDL Evaluation Project White Paper\nCollection (2nd ed., pp. 32-32).\nPardo, B., Meek, C., & Birmingham, B. (2002). Comparingaural music information retrieval systems. In The MIR/MDL\nEvaluation Project White Paper Collection (2nd ed., pp. 34-\n36).Pauws, S., & Eggen, B. (2002). PATS: Realization and userevaluation of an automatic playlist generator. In Third\nInternational Conference on Music Information Retrieval(ISMIR 2002)  (pp. 222-230).\nPickens, J. (2000). A comparison of language modeling andprobabilistic text information retrieval approaches tomonophonic music retrieval. In International Symposium on\nMusic Information Retrieval (ISMIR 2000).\nRasmussen, E. (2002). Evaluation in information retrieval. In\nThe MIR/MDL Evaluation Project White Paper Collection(2nd ed., pp. 45-39).\nReiss, J., & Sandler, M. (2002a). Benchmarking music\ninformation retrieval systems. In The MIR/MDL Evaluation\nProject White Paper Collection (2nd ed., pp. 37-42).\nReiss, J., & Sandler, M. ( 2002b). Beyond recall and precision:\nA full framework for MIR system evaluation. In The\nMIR/MDL Evaluation Project  White  Paper Collection (2nd\ned., pp. 58-63).\nRichard, G. (2002). Towards large databases for music\ninformation retr ieval  systems development  and evaluation. In\nThe MIR/MDL Evaluation Project White Paper Collection(2nd ed., pp. 64-67).\nRüger, S. (2002). A Framework for the evaluation of content-\nbased music information retr ieval  using the TREC  Paradigm.\nIn The MIR/MDL Evaluation Project White Paper Collection\n(2nd ed., pp. 68-70).\nSchamber, L. (1994). Relevance and information behavior.\nAnnual Review  of Information Science and Technology , 29, 3-\n48.\nSmiraglia, R. P. (2001). Musical  works as information\nretrieval entities: Epistemological perspectives. In Second\nInternational Symposium on Music Information Retrieval(ISMIR 2001) (pp. 85-91).\nSödring, T., & Smeaton, A. F. (2002). Evaluating a musicinformation retrieval system: TREC style. In The MIR/MDL\nEvaluation Project White Paper Collection (pp. 71-78).\nThe Reference Interview. (2001). In R. E. Bopp, & L. C.Smith, Reference and information services : An introduction\n(3rd ed., pp. 47-68). Englewood, CO: Libraries Unlimited.\nUitdenbogerd, A. L., & Zobel, J. (1999).  Matching\ntechniques for large music databases. In Proceedings of the\n7th ACM International Multimedia Conference (pp. 57-66).\nVoorhees, E. M. (2002). Whither music IR evaluationinfrastructure: Lessons to be learned from TREC. In The\nMIR/MDL Evaluation Project White Paper Collection (2nd\ned., pp. 7-13)."
    },
    {
        "title": "Application of missing feature theory to the recognition of musical instruments in polyphonic audio.",
        "author": [
            "Jana Eggink",
            "Guy J. Brown"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1416262",
        "url": "https://doi.org/10.5281/zenodo.1416262",
        "ee": "https://zenodo.org/records/1416262/files/EgginkB03.pdf",
        "abstract": "A system for musical instrument recognition based on a Gaussian Mixture Model (GMM) classifier is introduced. To enable instrument recognition when more than one sound is present at the same time, ideas from missing feature theory are incorporated. Specifically, frequency regions that are dominated by energy from an interfering tone are marked as unreliable and excluded from the classification process. The approach has been evaluated on clean and noisy monophonic recordings, and on combinations of two instrument sounds. These included random chords made from two isolated notes and combinations of two realistic phrases taken from commercially available compact discs. Classification results were generally good, not only when the decision between reliable and unreliable features was based on the knowledge of the clean signal, but also when it was solely based on the harmonic overtone series of the interfering sound. 1     Introduction Music transcription describes the process of finding a symbolic representation for a piece of music based on an audio recording or possibly a live performance. A symbolic representation in this context generally means some kind of musical score, with information for every tone about its fundamental frequency (F0), its onset time and duration, the instrument on which the tone was played, and possibly loudness and other expressive gestures. Transcription is a task that is currently almost exclusively performed by trained musicians; computer based automatic transcription remains a challenging problem. In the present study we focus on one part of the automatic music transcription problem - instrument recognition from an audio recording. Realistic sound recordings from commercially available compact discs (CDs) have been successfully used in systems limited to monophonic sound recognition. Martin (1999) used a number of features related to both temporal and spectral characteristics of instrument sounds in a hierarchical classification scheme. Generally, the performance of his system was comparable to human performance, although humans outperformed the computer system in instrument family differentiation. Using 27 different instruments, the system achieved a recognition accuracy of 57% for realistic monophonic examples and 39% for isolated tones with the best possible parameter settings. Reducing the number of instruments to 6 improved results up to 82% for monophonic phrases. Brown et al. (2001) described a classifier based on Gaussian mixture models (GMMs), and compared the influence of different features on classification accuracy. Test material consisted of realistic monophonic phrases from four different woodwinds. Both cepstral features and features related to spectral smoothness performed well. With these features they achieved an average recognition accuracy of around 60%, reaching 80% for the best possible parameter combination and choice of training material. Marques and Moreno (1999) compared the performance of classifiers based on Gaussian mixture models and support vector machines (SVMs). Cepstral features performed better than linear prediction based features; and mel-frequency scaled cepstral features performed again better than linearly scaled ones. Using realistic recordings of 8 different instruments, they achieved recognition accuracies of 63% for the GMM-based classifier and a slightly improved result of 70% for SVMs, but the influence of the choice of features seemed to be higher than that of the classification method. Only very few studies have attempted instrument recognition for polyphonic music, and the systems were mostly tested on very limited and artificial examples. Kashino and Murase (1999) used a template-based time domain approach. For each note of each possible instrument an example waveform was stored. As a first step, the sound file was divided according to onsets. For every part the most prominent instrument tone was then determined by comparing the mixture with the phase- adjusted example waveforms. In an iterative processing cycle, the energy of the corresponding waveform was subtracted to find the next most prominent instrument tone. Using only three different instruments (flute, violin and piano) and specially arranged ensemble recordings they achieved 68% correct instrument identifications with both the true F0s and the onsets supplied to the algorithm. With the inclusion of higher level Permission to make digital or hard copies of all or part of this work for personal of classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2003 The Johns Hopkins University. Application Of Missing Feature Theory To The Recognition Of Musical Instruments In Polyphonic Audio Jana Eggink and Guy J. Brown Department of Computer Science, University of Sheffield Regent Court, 211 Portobello Street, Sheffield S1 4DP, UK {j.eggink, g.brown}@dcs.shef.ac.uk musical knowledge, most importantly voice leading rules, recognition accuracy improved to 88%. A frequency domain approach was proposed by Kinoshita et al. (1999), using features related to the sharpness of onsets and the spectral distribution of partials. F0s were extracted prior to the instrument classification process to determine where partials from more than one F0 would coincide. Corresponding feature values were either completely ignored or used only after an average value corresponding to the first identified instrument was subtracted. Using random two-tone combinations from three different instruments (clarinet, violin, piano), they obtained recognition accuracies between 66% and 75% (73%- 81% if the correct F0s were provided), depending on the interval between the two notes. In this paper, we propose an approach based on missing feature (or missing data) theory to enable instrument recognition in situations where multiple tones may overlap in time. The general idea is to use only the parts of the signal which are dominated by the target sound, and ignore features that are dominated by background noise or interfering tones. This approach is motivated by a model of auditory perception which postulates a similar process in listeners; since target sounds are often partially masked by an interfering sound, it can be inferred that listeners are able to recognize sound sources from an incomplete acoustic representation (Cooke et al., 2001). The missing feature approach has previously been successfully applied in the fields of robust speech recognition (Cooke et al., 2001) and speaker identification (Drygajlo and El-Maliki, 1998), the latter task beeing one which is closely related to musical instrument identification. In polyphonic music, partials of one tone often overlap with those of another tone. As a consequence, the energy values of these partials no longer correspond to those of either instrument, and most existing instrument recognition techniques will fail. Within a missing feature approach, these corrupted features will be excluded from the recognition process. The remaining information will therefore be incomplete, but feature values will mainly contain information about one sound source only. The hope is that this remaining information is still sufficient to enable robust instrument classification. The main requirement for the actual classifier is its robustness towards incomplete feature sets. Classifiers based on Gaussian mixture models (GMMs) can be easily adapted to work with incomplete data (Drygajlo and El-Maliki, 1998). They have also been successfully employed for instrument classification in monophonic music (Brown et al., 2001; Marques and Moreno, 1999) and are therefore a promising choice for a system attempting instrument classification for polyphonic music. 2     System Description A schematic view of our system is shown in Figure 1. The first stage is a frequency analysis of the sampled audio signal. Subsequently, the F0s of all tones are extracted and frequency regions where partials of a non-target tone are found are marked as unreliable. Hence, a binary ‘mask’ is derived, that indicates the features which should be employed by a GMM classifier.",
        "zenodo_id": 1416262,
        "dblp_key": "conf/ismir/EgginkB03",
        "keywords": [
            "system",
            "frequency analysis",
            "F0 extraction",
            "masking",
            "Gaussian Mixture Model (GMM)",
            "classifier",
            "missing feature theory",
            "auditory perception",
            "robust speech recognition",
            "speaker identification"
        ],
        "content": "Abstract\nA system for musical instrument recognition based on\na Gaussian Mixture Model (GMM) classifier isintroduced. To enable instrument recognition whenmore than one sound is present at the same time, ideasfrom missing feature theory are incorporated.Specifically, frequency regions that are dominated byenergy from an interfering tone are marked asunreliable and excluded from the classificationprocess. The approach has been evaluated on clean andnoisy monophonic recordings, and on combinations oftwo instrument sounds. These included random chordsmade from two isolated notes and combinations of tworealistic phrases taken from commercially availablecompact discs. Classification results were generallygood, not only when the decision between reliable andunreliable features was based on the knowledge of theclean signal, but also when it was solely based on theharmonic overtone series of the interfering sound.\n1     Introduction\nMusic transcription describes the process of finding a symbolicrepresentation for a piece of music based on an audio recordingor possibly a live performance. A symbolic representation inthis context generally means some kind of musical score, withinformation for every tone about its fundamental frequency(F0), its onset time and duration, the instrument on which thetone was played, and possibly loudness and other expressivegestures. Transcription is a task that is currently almostexclusively performed by trained musicians; computer basedautomatic transcription remains a challenging problem. In thepresent study we focus on one part of the automatic musictranscription problem - instrument recognition from an audiorecording.\nRealistic sound recordings from commercially available\ncompact discs (CDs) have been successfully used in systemslimited to monophonic sound recognition. Martin (1999) used anumber of features related to both temporal and spectralcharacteristics of instrument sounds in a hierarchical\nclassification scheme. Generally, the performance of his systemwas comparable to human performance, although humansoutperformed the computer system in instrument familydifferentiation. Using 27 different instruments, the systemachieved a recognition accuracy of 57% for realisticmonophonic examples and 39% for isolated tones with the bestpossible parameter settings. Reducing the number ofinstruments to 6 improved results up to 82% for monophonicphrases.\nBrown et al.  (2001) described a classifier based on Gaussian\nmixture models (GMMs), and compared the influence ofdifferent features on classification accuracy. Test materialconsisted of realistic monophonic phrases from four differentwoodwinds. Both cepstral features and features related tospectral smoothness performed well. With these features theyachieved an average recognition accuracy of around 60%,reaching 80% for the best possible parameter combination andchoice of training material.\nMarques and Moreno (1999) compared the performance of\nclassifiers based on Gaussian mixture models and supportvector machines (SVMs). Cepstral features performed betterthan linear prediction based features; and mel-frequency scaledcepstral features performed again better than linearly scaledones. Using realistic recordings of 8 different instruments, theyachieved recognition accuracies of 63% for the GMM-basedclassifier and a slightly improved result of 70% for SVMs, butthe influence of the choice of features seemed to be higher thanthat of the classification method.\nOnly very few studies have attempted instrument recognition\nfor polyphonic music, and the systems were mostly tested onvery limited and artificial examples. Kashino and Murase(1999) used a template-based time domain approach. For eachnote of each possible instrument an example waveform wasstored. As a first step, the sound file was divided according toonsets. For every part the most prominent instrument tone wasthen determined by comparing the mixture with the phase-adjusted example waveforms. In an iterative processing cycle,the energy of the corresponding waveform was subtracted tofind the next most prominent instrument tone. Using only threedifferent instruments (flute, violin and piano) and speciallyarranged ensemble recordings they achieved 68% correctinstrument identifications with both the true F0s and the onsetssupplied to the algorithm. With the inclusion of higher level\nPermission to make digital or hard copies of all or part of this work for\npersonal of classroom use is granted without fee provided that copies are notmade or distributed for profit or commercial advantage and that copies bear\nthis notice and the full citation on the first page. © 2003 The Johns Hopkins\nUniversity.Application Of Missing Feature Theory To The Recognition\nOf Musical Instruments In Polyphonic Audio\nJana Eggink  and Guy J. Brown\nDepartment of Computer Science, University of Sheffield\nRegent Court, 211 Portobello Street, Sheffield S1 4DP, UK\n{j.eggink, g.brown}@dcs.shef.ac.ukmusical knowledge, most importantly voice leading rules,\nrecognition accuracy improved to 88%.\nA frequency domain approach was proposed by Kinoshita et al.\n(1999), using features related to the sharpness of onsets and thespectral distribution of partials. F0s were extracted prior to theinstrument classification process to determine where partialsfrom more than one F0 would coincide. Corresponding featurevalues were either completely ignored or used only after anaverage value corresponding to the first identified instrumentwas subtracted. Using random two-tone combinations fromthree different instruments (clarinet, violin, piano), theyobtained recognition accuracies between 66% and 75% (73%-81% if the correct F0s were provided), depending on theinterval between the two notes. \nIn this paper, we propose an approach based on missing feature\n(or missing data) theory to enable instrument recognition insituations where multiple tones may overlap in time. Thegeneral idea is to use only the parts of the signal which aredominated by the target sound, and ignore features that aredominated by background noise or interfering tones. Thisapproach is motivated by a model of auditory perception whichpostulates a similar process in listeners; since target sounds areoften partially masked by an interfering sound, it can be inferredthat listeners are able to recognize sound sources from anincomplete acoustic representation (Cooke et al. , 2001). The\nmissing feature approach has previously been successfullyapplied in the fields of robust speech recognition (Cooke et al. ,\n2001) and speaker identification (Drygajlo and El-Maliki,1998), the latter task beeing one which is closely related tomusical instrument identification. \nIn polyphonic music, partials of one tone often overlap with\nthose of another tone. As a consequence, the energy values ofthese partials no longer correspond to those of eitherinstrument, and most existing instrument recognitiontechniques will fail. Within a missing feature approach, thesecorrupted features will be excluded from the recognitionprocess. The remaining information will therefore beincomplete, but feature values will mainly contain informationabout one sound source only. The hope is that this remaininginformation is still sufficient to enable robust instrumentclassification.\nThe main requirement for the actual classifier is its robustness\ntowards incomplete feature sets. Classifiers based on Gaussianmixture models (GMMs) can be easily adapted to work withincomplete data (Drygajlo and El-Maliki, 1998). They havealso been successfully employed for instrument classification inmonophonic music (Brown et al. , 2001; Marques and Moreno,\n1999) and are therefore a promising choice for a systemattempting instrument classification for polyphonic music.\n2     System Description\nA schematic view of our system is shown in Figure 1. The firststage is a frequency analysis of the sampled audio signal.Subsequently, the F0s of all tones are extracted and frequencyregions where partials of a non-target tone are found are markedas unreliable. Hence, a binary ‘mask’ is derived, that indicates\nthe features which should be employed by a GMM classifier.\n2.1     Acoustic Features\nThe choice of acoustic features is very important for any\nclassification system. While cepstral features, especially whenmel-frequency scaled, have been proven to give good results formusical instrument classification systems (see section 1), theydo not easily fit within a missing feature approach. The idea ofthe missing feature approach is to exclude frequency regionsdominated by energy from an interfering sound source. Aspecific frequency region does not have a clear correspondencein the cepstral domain, so that a distinction between featuresdominated by the target tone and those dominated by aninterfering tone cannot be made. Therefore local spectralfeatures are required for the missing feature approach.\nFrom these considerations, we chose linearly scaled features\nover a quasi-logarithmic scaling which would be closer tohuman hearing. The harmonic overtone series of musical tonesis approximately evenly spaced on a linear scale, and an equallylinear scaling of features makes it easier to block out the energyof such an interfering harmonic series. \nThe employed features can basically be described as a coarse\nspectrogram. Sampled audio recordings were divided intoframes 40 ms in length with a 20 ms overlap. Each frame wasmultiplied with a Hanning window, and a fast Fourier transform(FFT) was computed. The resulting spectra were logcompressed and normalised to a standard maximum value. Eachfeature consists of the spectral energy within a 60 Hz widefrequency band. The features span a region between 50 Hz and6 kHz, with 10 Hz overlap between adjacent features, resultingin a total of 120 features per time-frame. The overall frequencyrange includes all possible F0s of the instruments used, andtheir formant regions.\n2.2     Fundamental Frequency Detection\nThe system described here depends on the estimation of F0s\nprior to any instrument identification. To evaluate the accuracyof the instrument classification independent from a pitchdetection system, we decided to circumvent the problem byInstrument classSampled audio signal\nF0 analysisFourier\nanalysisAcoustic\nfeatures\nFeature\nmaskGMM\nclassifier\nFigure 1: Schematic of the instrument classification system.using either a priori masks (see section 2.3.1) or isolated tones\nwith a known F0 that could be manually supplied to the system.\nFinding multiple F0s in polyphonic music is known to be a non-\ntrivial problem, and a growing number of publications isfocusing on its various aspects (e.g. see Klapuri, 2001; Raphael,2002), with encouraging results if the number of concurrentvoices is low. In an earlier publication (Eggink and Brown,2003), we presented an iterative pattern matching approachbased on ‘harmonic sieves’. While no extensive tests werecarried out, the results were generally good for two-voicedmusic. The advantage of this approach lies in its explicitidentification of spectral peaks which belong to the harmonicovertone series of the different F0s. If the real frequencylocation of partials is known, the assumption of an exactlyharmonic overtone spcectrum can be dropped and moreaccurate missing feature masks can be derived.\n2.3     Feature Masks\n2.3.1     A priori MasksFeature masks are used to indicate which features should be\nused for the classification process. The identification of thesereliable features is often one of the hardest problems in missingfeature systems. Commonly, a priori  masks are used to\nestablish an upper performance limit. If the clean signal (i.e. themonophonic target signal alone without any interfering noise orany other sound sources) is known, it can be compared with themixture, and only those parts where the mixture is similar to thetarget sound alone are used for recognition. Feature values werecomputed for the target sound alone and for the mixtureconsisting of the target and the interfering sound. They weremarked as reliable only when then the feature value of themixture was within a range of ±3dB compared to thecorresponding feature value of the clean signal. The thresholdof ±3 dB is somewhat arbitrary, but led to good results in initialstudies, and a lower threshold of ±1 dB gave generally similarresults.\n2.3.2     Pitch-based Masks\nWhile a priori  masks provide a good tool to assess a best\npossible performance, they are not very realistic, as the cleansignal is not normally available. A more realistic way to\ngenerate the missing feature masks is based on the F0 of theinterfering tone (or possibly tones). The energy of harmonictones is concentrated in their partials, whose positions can beapproximated once the F0 is known. If a partial from the non-target tone falls within the frequency range of a feature, thefeature is marked as unreliable and not used for recognition.This approach obviously depends on the harmonic structure ofthe interfering tone, and is therefore suitable for most musicalinstrument tones, but does not work for percussion and otherinharmonic sounds. In such cases, other cues (such as e.g. stereoposition) could be used.\n2.4     Gaussian Mixture Model Classifier with Missing \nFeatures\nA GMM models the probability density function (pdf) of\nobserved features by a multivariate Gaussian mixture density:\n(1)\nwhere x is a D-dimensional feature vector and N is the number\nof Gaussian densities , each of which has a mean vector ,covariance matrix  and mixing coefficient p\ni. Here, we\nassume a diagonal covariance matrix; although this embodiesan assumption which is incorrect (independence of features) itis a widely used simplification (e.g., see Brown et al. , 2001).\nAccordingly, (1) can be rewritten as:\n(2)\nwhere m\nij and represent the mean and variance respectively\nof a univariate Gaussian pdf. Now, consider the case in whichsome components of x are missing or unreliable, as indicated by\na binary mask M. In this case, it can be shown (Drygajlo and El-\nMaliki, 1998) that the pdf (2) can be computed from partial dataonly, and takes the form:\n(3)timefrequencya) target tone d) mixture with mask\nFigure 2: Example for missing feature masks. Simplified spectra of a) the target tone, b) the interfering tone, and c) the\nmixture of both tones. Energy values which, due to overlapping partials, do not correspond to those of either tone aloneare shown in dark grey. In d) the mixture is overlaid with the mask, represented by hatched bars. b) interfering tone c) mixture\ntimefrequency\ntimefrequency\ntimefrequency\npx() piΦixµiΣi,,()\ni1=N\n∑=\nΦiµi\nΣi\npx() piΦixjmijσij2,,()\nj1=D\n∏\ni1=N\n∑=\nσij2\npxr() piΦixjmijσij2,,()\njM '∈∏\ni1=N\n∑=where M’ is the subset of reliable features xr in M. Hence,\nmissing features are effectively eliminated from thecomputation of the pdf.\n2.5     Bounded Marginalisation\nWith the binary masks described so far, all information from the\nfeatures marked as unreliable is completely discarded. But thefeatures still hold some information, as the observed energyvalue represents an upper boundary for the possible value of thetarget sound (Cooke et al. , 2001). Instead of ignoring the\nunreliable features, the pdf can be approximated as a product ofthe activation based on the reliable features x\nr and an integration\nover all possible values of the unreliable features xu:\n(4)\nIf the upper and lower bounds ( xhigh, xlow) of the unreliable\nfeatures are known, for diagonal covariance matrices theintegral can be evaluated as a vector difference of multivariateerror functions (Cooke et al. , 2001). Since no specific\nknowledge exists for the lower boundary, it is always assumedto be zero and the corresponding error function is subsequentlyignored. The integral in (4) can then be computed as:\n(5)\nwhere x\nhigh,u  represents the upper bound of the unreliable\nfeature xu, and µu,i and σ2\nu,i the mean and variance respectively\nof the unreliable feature of centre i.\n2.6     Training\nIndividual GMMs were trained for five different instruments\n(flute, oboe, clarinet, violin and cello). To make the models asrobust as possible they were trained with different recordingsfor each instrument, using both monophonic musical phrasesand single tone recordings. After an initial clustering using a K-means algorithm, the parameters of the GMMs were trained bythe expectation-maximisation (EM) algorithm. The number ofGaussian densities, N, was set to 120 after some\nexperimentation; a further increase gave no improvement.\n3     Evaluation\nBoth realistic phrases from commercially available CDs andisolated samples were used for evaluation purposes. Theadvantage of the former is that it is closer to realisticapplications, and likely to include a range of acoustic propertiesthat can pose additional difficulties to a recognition system, likee.g. reverberation and a wide range of tempo and dynamicdifferences. Isolated samples on the other hand make it possibleto evaluate a system independent of a pitch extractor, as the F0sare known beforehand. It also allows systematic testing ofspecific chord combinations.3.1     Monophonic Sounds\nTo establish an upper limit on performance with missing\nfeatures, tests were carried out with monophonic recordings.Test material was taken from recordings which were notincluded in the training material, consisting of chromatic scalesfrom the McGill master samples CD (Opolko and Wapnick,1987), the Ircam studio online collection, and the Iowa musicalinstrument samples. Different models were trained by a leave-one-out cross validation scheme, each using only two of thementioned sample collections, but the same realisticmonophonic phrases from commercially available classicalmusic CDs. To avoid cues based solely on the different pitchrange of the instruments, only tones from one octave (C4-C5)were used for testing, although the models were always trainedon the full pitch range of the instruments. Where necessary,chromatic scales were manually cut into single tones.\nClassification decisions were made for each frame\nindependently and the model which accumulated the most‘wins’ over the tone or phrase duration was taken as the overallclassification for that example. Average instrument recognitionaccuracy was 66% for the McGill samples, 70% for the Ircamsamples and 62% for the Iowa samples (the last one excludingthe violin, for which no recordings were available). A confusionmatrix averaing across all three conditions is shown in Table 1.\nIdentification performance was also assessed on monophonic\nphrases from a number of classical music CDs, which were notused for training. For every instrument 5 different recordings ofvarying length from 2-10 seconds were used, with classificationdecisions made for each sound file separately. Results werevery similar for the 3 sets of models, with an averagerecognition accuracy of 88%. All flute, clarinet and violinexamples were correctly classified, while up to 2 oboeexamples were mistaken for flutes and up to 2 cello exampleswere confused with clarinets.\nThe main reason for the better classification of realistic phrases\nseems to lie in their generally longer duration and highervariability. If one tone of a certain F0 gets misclassified, this ismore likely to be evened out by a majority of correctlyclassified tones. If the results are compared on a frame by framebasis, the system performs equally well on realistic phrases andsingle notes, with an average of 60% correctly classifiedframes.pxrxu,() piΦixrµiΣi,,() ΦixuµiΣi,,() xud ∫\ni1=N\n∑=\nΦixuµiΣi,,() xud ∫1\n2---erfxhigh , uµui,–\n2σu, i2-------------------------------- -\n=Flute Clarinet Oboe Violin Cello\nFlute 67% 8% 0% 15% 8%\nClarinet 23% 59% 5% 8% 5%\nOboe 0% 10% 85% 3% 3%\nViolin 4% 4% 8% 65% 19%\nCello 3% 10% 15% 15% 56%\nTable 1: Confusion matrix for mean instrument recognition \nof single notes.stimulusresponse3.2     A Priori  Masks\nA priori masks provide a good tool to accurately select reliable\nand unreliable features if the clean signal is available. Theperformance achieved can be regarded as an upper limit, and astarting point for more realistic methods of distinguishingbetween reliable and unreliable features.\n3.2.1     Noise\nThe system was shown earlier to be very robust against random\ndeletions of features (Eggink and Brown, 2003). Here we test itsrobustness towards artificially added noise with and withoutmissing feature masks. Aside from providing a good evaluationmethod for the missing features approach, noise robustness maybe relevant in cases where low quality recordings aretranscribed, such as live performances or old analog records.\nSingle notes from the three sample collections and realistic\nmonophonic phrases were mixed with white noise at differentsignal-to-noise ratios (SNRs). The results were very similar forboth isolated notes and realistic phrases. With SNRs of 0 to 10dB, almost all examples were classified as flutes; with a furtherdecrease of noise to a SNR of 15 dB, a few violin tones werealso correctly identified. This bias towards the flute model doesnot seem to be caused by a similarity between the noise andflute tones, as the noise alone resulted in such smallprobabilities from all models that they came within roundingerror of 0. Recognition accuracy was only above chance at veryhigh SNRs of 20dB, although with around 40% correctlyidentified examples the results were still well below thoseobtained for clean signals.\nMaking use of the missing feature approach based on a priori\nmasks improved results significantly at all SNR levels.Averaging over all tested conditions, the use of missing featuremasks improved recognition accuracy by 27% (Figure 3).\n3.2.2     Two Concurrent Instrument Sounds\nA priori  masks were also used to identify the instruments in\ncombinations of two independent monophonic examples,which were always played by different instruments. For eachsample collection, a test set was derived by taking all possiblecombinations of two tones within one octave (C4-C5),excluding intervals in which both tones had the same F0 (3120combinations per sample collection). Before mixing, the tones\nwere normalised to have equal root-mean-square (rms) power.The length of each sound was determined by the shorter of thetwo tones to ensure that two instruments were present for thewhole mixture. Average recognition accuracy was 59% for theMcGill samples, 63% for the Ircam samples and 65% for theIowa sample collection; representing an average drop inperformance of less then 5% compared to the monophoniccontrol condition. A confusion matrix averaging across thethree examples is shown in Table 2.\nThe same approach was also tested with mixtures of realistic\nmonophonic recordings. Using the same examples as for themonophonic condition, with all possible mixtures of twodifferent instruments (500 different combinations), averagerecognition accuracy was 74%, a drop of 14% compared to themonophonic control condition. Generally the confusions forrealistic phrases are very similar to those of isolated tonecombinations. The main difference lies in the lower level ofconfusions between violin and cello for the realistic phrases.This is most likely due to the fact that for the isolated notes allexamples were taken from the same octave, while the realisticphrases span the whole natural pitch range of the instruments. \nAn additional factor that could influence recognition accuracy\nis the interval relationship between the two notes. Criticalintervals could be octaves and fifths, because the amount ofoverlap between partials from the target and the non-target toneis high. Recognition results for octaves were indeed about 10%below average, while no drop in performance occurred forfifths. Other intervals that could pose additional problems areseconds, where the F0s of the two notes are very close, and theindividual partials might not be separated by the spectralfeatures. Again, no drop in performance occurred, so the systemproved to be quite robust towards the actual intervalrelationship of the notes.\n3.3     Pitch-based Masks\nAs a next step towards realistic performance, we used\ncombinations of two notes with masks based on the F0s of thenon-target tone, which were in this case manually supplied tothe algorithm. Since the system was shown earlier to be quiterobust towards missing features, it seemed preferable to excludetoo many features than to risk including corrupted features.Some preliminary tests supported this approach, as recognitionusing missing feature masks based on ‘broadened’ harmonics20 40 60\n5 dB 20dB 15dB 0dB 10dBa priori  masks\nno missing feature masks\nchance level\nclean80\nSignal-to-Noise Ratio (SNR)\nFigure 3: Recognition accuracy in the presence of white noise \nat various SNRs, with and without missing feature masks.Recognition\nAccuracy in %Flute Clarinet Oboe Violin Cello\nFlute 73% 3% 3% 12% 8%\nClarinet 22% 47% 10% 14% 7%\nOboe 1% 6% 73% 9% 11%\nViolin 0% 3% 8% 68% 21%\nCello 3% 2% 15% 27% 51%\nTable 2: Confusion matrix for mean instrument recognition of \ntwo concurrent notes using a priori masks.stimulusresponseimproved recognition accuracy by up to 10%. The exact amount\nof deletions did not have a strong influence; a relativebroadening of ±2.5% (slightly less than ± a quarter tone)worked well and was subsequently used for furtherexperiments. \nRecognition accuracy was 49% for the McGill samples, 43%\nfor the Iowa and 48% for the Ircam samples. A confusion matrixaveraging across the three conditions is shown in Table 3. Allinstruments were correctly identified in the majority of cases,except for the cello which was often mistaken for a violin. Asall test tones were from the same octave and therefore relativelyhigh for a cello, this confusion is not very surprising. Informallistening tests confirmed that low violin and high cello toneswere hard to distinguish for humans, even in the cleanmonophonic condition were the system performed relativelywell. \n3.4     Bounded Marginalisation\nFor combinations of two instrument sounds, often more than\nhalf of the features are marked as unreliable and subsequentlyexcluded from the recognition process. We now tested if theinclusion of the values of these unreliable features as upperbounds for the corresponding feature values could be used toimprove recognition accuracy. \nHowever, when tested with combinations of two isolated notes\nor monophonic phrases using a priori  or pitch-based masks, no\nsignificant improvement was found. This result is at first ratherunexpected, as bounded marginalisation can improve resultssignificantly for speech recognition in the presence of noise(Cooke et al. , 2001). However, most noises used to test robust\nspeech recognition systems are mainly inharmonic andtherefore quite different from musical instrument tones. To seeif the lack of improvement was due to this difference, we testedbounded marginalisation on monophonic sounds mixed withwhite noise at various SNRs. In these cases, the use of boundsdid improve results considerably. For all mixtures with an SNRlevel between 0dB and 20dB, recognition accuracy using a\npriori masks and bounded marginalisation was on average as\ngood as with clean monophonic signals.\nThe reason why the use of upper bounds proved only to be\nuseful with random noise, but not with a harmonicallystructured tone, can probably be explained in terms of thedifferent distribution of energy. With a musical tone, the energyis high at the frequencies where a harmonic overtone is present,and low otherwise. Frequency regions that are not excited by apartial of the interfering tone are therefore less likely to be\nmarked as unreliable, while the energy in frequency regionswhere an interfering parital is present is likely to be well abovethe energy caused by the target tone alone. \nUpper bounds appear to be useful only when the difference\nbetween observed energy (feature values) and energy caused bythe target sound is relatively small, but in these cases boundedmarginalisation improves the noise robustness of the recogniserconsiderably. Even though the white noise used in ourexperiments is an extreme case due to its complete spectralflatness, the use of bounded marginalisation could prove to bevery useful for instrument recognition in noisy recordings.While pitch-based missing feature masks are not usable in thesecases, various other noise estimation algorithms have beendeveloped in the context of robust speech recogntion. They canbe easily integrated with a missing feature approach and havebeen shown to lead to good results for speech mixed withvarious inharmonic noise sources (Cooke et al. , 2001).\n4     Conclusions and Future Work\nA system for the identification of musical instrument tones\nbased on missing feature theory and a GMM classifier has beendescribed. It generalises well, giving good results on single noterecordings and on realistic musical phrases. Especially for thelatter, results are well comparable to those of other systemsdirectly designed for the identification of monophonicexamples. Importantly, the system introduced here is notlimited to identification of instruments in monophonic music.Rather, by using missing feature masks, the system is able toidentify two different instruments playing concurrently. Theuse of missing feature masks also aids the recognition ofmonophonic instrument sounds in noisy conditions.\nThe system was primarily evaluated using a priori  masks for\ncombinations of isolated tones and independent monophonicphrases. Using pitch-based masks for isolated notecombinations, performance was still good, but about 15% lowerthan with the use of a priori  masks. This indicates that a more\ndetailed analysis of which features are dominated by whichsource could lead to an improvement for the estimation of thepitch-based masks. Nevertheless, the system has been shownearlier (Eggink and Brown, 2003) to be able to reliable identifythe instruments in a duet recording taken from a commerciallyavailable CD, using missing feature masks based on the F0sestimated by the system.\nTo be a useful tool for instrument classification tasks in the\ncontext of automatic transcription or musical informationretrieval, not necessarily every note has to be correctlyidentified, as higher musical knowledge can help to suppress afew random errors. The integration of such higher levelknowledge into our system will form part of our future work.Also, we intend to test how the system performs when morethan two concurrent instruments are present. But the resultsachieved so far are encouraging, and it seems that our eventualgoal - an automatic transcription system for audio recordings ofclassical chamber music played by small ensembles - isachievable.Flute Clarinet Oboe Violin Cello\nFlute 54% 4% 5% 27% 9%\nClarinet 25% 44% 7% 17% 7%\nOboe 17% 8% 48% 18% 8%\nViolin 7% 3% 5% 64% 20%\nCello 16% 4% 15% 34% 31%\nTable 3: Confusion matrix for mean instrument recognition \nof two concurrent notes with pitch based masks.\nstimulusresponseAcknowledgments\nJE is supported by the IHP HOARSE project. GJB is supported\nby EPSRC grant GR/R47400/01 and the MOSART IHPnetwork.\nReferences\nBrown, J.C., Houix, O. & McAdams, S. (2001). Feature\ndependence in the automatic identification of musicalwoodwind instruments. Journal of the Acoustical Sociecty\nof America , 109(3), 1064-1072\nCooke, M., Green, P., Josifovski, L. & Vizinho, A. (2001).\nRobust automatic speech recognition with missing andunreliable acoustic data. Speech Communication  34, 267-\n285\nDrygajlo, A. & El-Maliki, M. (1998). Speaker verification in\nnoisy environments with combined spectral subtraction andmissing feature theory. Proceedings of the IEEE\nInternational Conference on Acoustics, Speech and SignalProcessing, ICASSP-98 , 121-124\nEggink, J. & Brown, G.J. (2003). A missing feature approach to\ninstrument identification in polyphonic music. Proceedings\nof the IEEE International Conference on Acoustics, Speechand Signal Processing, ICASSP-03 , 553-556\nIowa Musical Instrument Samples, \nhttp://theremin.music.uiowa.edu\nIrcam Studio Online (SOL), http://www.ircam.fr\nKashino, K. & Murase, H. (1999). A sound source\nidentification system for ensemble music based on templateadaptation and music stream extraction. Speech\nCommunication  27, 337-349\nKinoshita, T., Sakai, S. & Tanaka, H. (1999). Musical sound\nsource identification based on frequency componentadaptation. Proceedings IJCAI-99 Workshop on\nComputational Auditory Scene Analysis , Stockholm,\nSweden\nKlapuri, A. (2001). Multipitch estimation and sound separation\nby the spectral smoothness principle. Proceedings of the\nIEEE International Conference on Acoustics, Speech andSignal Processing, ICASSP-01 , 3381-3384\nMarques, J. & Moreno, P. (1999). A study of musical\ninstrument classification using Gaussian mixture modelsand support vector machines. Cambridge Research\nLaboratory Technical Report Series CRL/4.\nMartin, K. (1999) Sound-source recognition: A theory and\ncomputational model . PhD Thesis, MIT.\nOpolko, F. & Wapnick, J. (1987).  McGill University master\nsamples (CD), Montreal, Quebec: McGill University\nRaphael, C. (2002). Automatic transciption of piano music.\nProceedings of the International Conference on MusicInformation Retrieval, ISMIR-02"
    },
    {
        "title": "Automatic labeling of tabla signals.",
        "author": [
            "Olivier Gillet",
            "Gaël Richard"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1418281",
        "url": "https://doi.org/10.5281/zenodo.1418281",
        "ee": "https://zenodo.org/records/1418281/files/GilletR03.pdf",
        "abstract": "Most of the recent developments in the field of music indexing and music information retrieval are focused on western music. In this paper, we present an auto- matic music transcription system dedicated to Tabla - a North Indian percussion instrument. Our approach is based on three main steps: firstly, the audio signal is segmented in adjacent segments where each segment represents a single stroke. Secondly, rhythmic infor- mation such as relative durations are calculated using beat detection techniques. Finally, the transcription (recognition of the strokes) is performed by means of a statistical model based on Hidden Markov Model (HMM). The structure of this model is designed in or- der to represent the time dependencies between suc- cessives strokes and to take into account the specifici- ties of the tabla score notation (transcription symbols may be context dependent). Realtime transcription of Tabla soli (or performances) with an error rate of",
        "zenodo_id": 1418281,
        "dblp_key": "conf/ismir/GilletR03",
        "keywords": [
            "audio signal segmentation",
            "rhythmic information calculation",
            "HMM-based transcription",
            "Tabla-specific notation",
            "real-time transcription",
            "error rate",
            "North Indian percussion instrument",
            "hidden Markov Model",
            "Tabla score notation",
            "context-dependent transcription"
        ],
        "content": "Automatic Labelling ofTabla Signals\nOlivier K.GILLET\nGET-ENST (TELECOM Paris)\n46,rueBarrault\n75013 Paris, France\nolivier.gillet@enst.frGa¨elRICHARD\nGET-ENST (TELECOM Paris)\n46,rueBarrault\n75013 Paris, France\ngael.richard@enst.fr\nAbstract\nMost oftherecent developments inthe\u0002eld ofmusic\nindexing andmusic information retrie valarefocused\nonwestern music. Inthispaper ,wepresent anauto-\nmatic music transcription system dedicated toTabla -\naNorth Indian percussion instrument. Ourapproach\nisbased onthree main steps: \u0002rstly ,theaudio signal is\nsegmented inadjacent segments where each segment\nrepresents asingle strok e.Secondly ,rhythmic infor -\nmation such asrelati vedurations arecalculated using\nbeat detection techniques. Finally ,thetranscription\n(recognition ofthestrok es)isperformed bymeans of\nastatistical model based onHidden Mark ovModel\n(HMM). Thestructure ofthismodel isdesigned inor-\ndertorepresent thetime dependencies between suc-\ncessi vesstrok esandtotakeintoaccount thespeci\u0002ci-\ntiesofthetabla score notation (transcription symbols\nmay beconte xtdependent). Realtime transcription\nofTabla soli(orperformances) with anerror rateof\n6.5% ismade possible with thistranscriber .Thetran-\nscription system, along with some additional features\nsuch assound synthesis orphrase correction, areinte-\ngrated inauser-friendly environment called Tablas-\ncope .\n1Introduction\nDue totheexponential growthofavailable digital information,\nthere isaneed fortechniques thatwould makethisinformation\nmore readily accessible totheuser.Asaconsequence, auto-\nmatic indexing andretrie valofinformation based oncontent is\nbecoming more andmore important andrepresent verychal-\nlenging research areas. Automatic indexing ofdigital informa-\ntionpermits toextract atextual description ofthisinformation\n(i.e. meta data). Intheconte xtofmusic signals, ifsuch ade-\nscription would ultimately beacomplete transcription (forex-\nample intheform ofmusic scores), itshould atleast targetto\nextract twotypes ofdescriptors from theaudio signal:\nPermission tomakedigital orhard copies ofallorpartofthiswork\nforpersonal orclassroom useisgranted without feeprovided that\ncopies arenotmade ordistrib uted forpro\u0002t orcommercial advan-\ntage andthatcopies bear thisnotice andthefullcitation onthe\u0002rst\npage. c\r2003 Johns Hopkins University .\u000fasetofgeneral descriptors such asgenre, style, musicians\nandinstruments ofamusic piece.\n\u000fasetofmore speci\u0002c descriptors such asbeat, notes,\nchords ornuances.\nAutomatic labelling ofinstrument sounds therefore represents\noneofthecomponent ofacomplete indexing system. Pre-\nvious efforts inautomatic labelling ofinstruments sounds\nhavemainly been dedicated tosounds with de\u0002nite pitch\n([Kaminsk yj,2001 ],[Martin, 1999 ]or[Herrera etal.,2000 ]for\nacritical review). Percussi vesounds isaspeci\u0002c class ofin-\nstrument sounds. Theycanbepitched (asforxylophone or\nmarimba) orunpitched (asfordrum orcong as).There ismuch\ninterest indevoting more efforttothelatter class since there is\nanumber ofspeci\u0002c applications forthisclass ofsounds (auto-\nmatic drum loop recognition andretrie val,virtual dancer ,drum-\ncontrolled synthesizers ...).\nInfact, more attention isnowgiven tothe class\nof unpitched percussi vesounds (see for example\n[Herrera etal.,2003 ],[McDonald andTsang, 1997 ],\n[Gouyon andHerrera, 2001 ])), butthese efforts are still\nlimited towestern music and formost studies only isolated\nsounds areconsidered.\nWepropose inthispaper astudy ontheautomatic transcrip-\ntionoftabla performances. Thetabla isapercussion instrument\nwidely used in(North) Indian classical andsemi-classical mu-\nsic,andwhich hasgained more andmore recognition among\nfusion musicians. Asforanumber ofpercussi veinstruments\n(such ascong as,drum), thecharacteristics oftheaudio signal\nproduced israther impulsi veandsuccessi veeventsaretherefore\nrather easily detected. Another advantage forautomatic analy-\nsisisthatsuch instruments produce only limited sound mixtures\n(1or2eventsatthesame time where inpolyphonic music over\n50notes canbeplayed simultaneously). However,thetranscrip-\ntionoftabla signals isquite challenging because there isstrong\ntime dependencies thatareessential totakeinto account fora\nsuccessful transcription. There aretwotypes oftime dependen-\ncies:\u0002rstly ,theinstrument canproduce long, resonant strok es\nwhich willoverlap andalter thetimbre ofthefollowing strok es.\nSecondly ,thesymbol used torepresent astrok ecandepend on\ntheconte xtinwhich itisused. Prior works related tocomputeri-\nzation oftheTabla include aphrase retrie valsystem using MIDI\ninput [Roh andWilcox, 1995 ]orspeci\u0002c controller andsynthe-\nsismodels [A.Kapur andCook, 2002 ],buttoourknowledge no\ntranscription system oftabla performances havebeen reported.Figure 1:The tabla isapair ofdrums :ametallic bass drum\n(thebayan )andawooden treble drum :thedayan\nThepaper isorganized asfollows.Thenextsection presents the\ninstrument andthespeci\u0002c transcription thatiscommonly used\ntodescribe tabla performances. Section 3isdedicated tothe\ndescription ofthetranscription system including adetailed sec-\ntiononthemodel proposed. Section 4presents some evaluation\nresults onrealtabla performances. Finally ,section 5suggests\nsome conclusions.\n2Presentation ofthetabla\nThetabla (see\u0002gure 1)isapairofdrums traditionally used for\ntheaccompaniment ofIndian classical orsemi-classical music.\nThe bayan isthemetallic bass drum, played bythelefthand.\nItisused toproduce aloud resonant sound oradamped non-\nresonant one. Bysliding thepalm ofthehand onthedrum head\nwhile striking, thepercei vedpitch canbemodi\u0002ed. Thedayan\nisthewooden treble drum, played bytheright hand. Alarger\nvariety ofsounds isproduced onthisdrum. Asfortimbals for\nexample, thedrum canbetuned according totheaccompanied\nvoice orinstrument. Itisalso important tonote thatforthis\nclass ofinstruments themain resonance (that corresponds tothe\nmain percei vedpitch) ismuch abovetheresonance oftheother\nharmonics. Further elements onthephysics oftheTabla canbe\nfound in[Fletcher andRossing, 1998 ]orintheearly workof\nRaman [Raman, 1934 ].\nAmnemonic syllable orbolisassociated toeach ofthese\nstrok es.Since themusical tradition inIndia ismostly oral, com-\npositions areoften transmitted from masters tostudents assung\nbols.Common bols are:Ge,Ke(bayan bols),Na,Tin,Tun,\nTi,Te(dayan bols).Strok esontheBayan anddayan canbe\ncombined, likeinthebols :Dha (Na+Ge),Dhin (Tin+Ge),\nDhun (Tun+Ge)...Because ofthese characteristics, evenif\ntwodrums areplayed simultaneously ,thetranscription canal-\nwaysbeconsidered asmonophonic -asingle symbol isused\nevenifthecorresponding strok eiscompound.\nAspeci\u0002city ofthisnotation system isthattwodifferent bols\ncansound verysimilar .Forexample, TiandTenearly sound\nthesame, butareplayed with different \u0002ngers. Though, itis\npossible to\u0002gure outwhich bolisused, since afastsequence of\nthese strok esisplayed byalternating between TiandTe.An-\nother characteristic istheexistence ofwords made ofseveral\nbols concatenated instereotyped groups which areused asa\nwhole, e.g.TiReKiT eorGeReNaGe .Such words areused as\ncompositional units, andmaketherecitation andmemorization\nofapiece easier .Atabla sequence orrhythm iscomposed ofsuccessi vebols of\ndifferent durations (note, half-note, quarter note,...) howeverthe\nrhythmic structure canbequite comple x.The basic rhythmic\nstructures (called taal)canhavealargevariety ofbeats (for\nexample 6,7,8, 10,12,16,..) which aregrouped inmeasures.\nThesuccessi vesmeasures may havedifferent number ofbeats\n(forexample Dhin NajDhin Dhin NajTinNajDhin Dhin\nNawhere thevertical bars symbolise themeasure boundaries).\nThegrouping characteristics areinasense similar tothose ob-\nservedinspok enorwritten languages where words canbe\ngrouped insentences. These properties canbemodelled by\nmeans ofaLangua gemodel thatwillprovide anestimation of\ntheprobability ofagivensequence ofwords. Forthetabla,\nsince theassociation between asound andthecorresponding\nbolcanbeconte xt-sensiti ve(i.e. depends onprevious bols),it\nseems quite appropriate touselanguage modelling approaches\ntomodel these dependencies.\nInthiswork, thetranscription islimited totherecognition of\nthesuccessi vesbols with their corresponding relati veduration\n(note, half-note etc...) butdoes notintend toextract thecomple x\nrhythmic structure (i.ethemeasures).\n3Transcription ofTabla phrases\n3.1 Architectur eofthesystem\nThearchitecture ofoursystem isorganized inseveralmodules:\n\u000fParametric representation thesignal parametric repre-\nsentation isobtained bymeans oftwomain sub-modules:\ntheonset detection which permits tosegment thesignal\ninto individual strok esandthefeatur esextraction which\nprovides anacoustic vector from thesegment previously\nextracted.\n\u000fSequence Modelling: thesequence offeature vectors\n(one vector perstrok e)isthen modelled byaclassi\u0002cation\ntechnique such asknearest neighbors (k-NN) orHidden\nMark ovModels (HMM).\n\u000fTranscription: The\u0002nal transcription isgivenintheform\nofasuccession ofbolsorwords accompanied with arhyth-\nmicnotation obtained from thetempo detection module\nFigure 2provides thegeneral architecture ofthesystem includ-\ningthetranscription modules (inbold line), andthetabla se-\nquence generator/synthesizer (indotted line) thatisfurther ex-\nplained insection 5.2.Itisimportant tonote thattheinput tothe\nsystem caneither beanaudio \u0002le(forexample in.wav format)\noraudio streams thatareprocessed inrealtime.\n3.2 Parametric representation\n3.2.1 Segmentation instrokes\nAsformanytranscription system, the\u0002rst step ofourtran-\nscriber consists inperforming asegmentation oftheincoming\nstream inseparate strok es.Anumber ofapproaches havebeen\nproposed intheliterature forsuch asegmentation (see forex-\nample [Klapuri, 1999 ]).Inthecase oftabla signals, most ofthe\nstrok eshaveafastdecaying exponential envelope. Asacon-\nsequence, asimple envelope/threshold based approach seems\nadequate andisdetailed below:Audio file\nRealtime audio input\nTranscription\nUser input\nAudio outputOnset detection\nTempo detection\nPhrase correctionFeatures extraction\nSynthesisHMM modelAnnotated corpus\n64 phrases\n5715 bolsTranscription\nsystem\nAdditional\nfeatures\nFigure 2:Architecture ofthesystem: thetranscription system isrepresented bybold lines andthetabla sequence generator or\nsynthesis isrepresented bydotted lines\n\u000fSampling: Theoriginal input signal issampled (orresam-\npled) at44.1 kHz.\n\u000fEnvelope extraction: Theenvelope, env(t),isobtained in\ntwosteps. First, alow-frequenc yenvelope signal (sampled\nat220.5 Hz),env1(t),iscalculated bytaking theabsolute\nvalue ofthemaximum in400samples windo ws.Second, a\nlocal normalisation isperformed bydividing theprevious\nenvelope byanother recti\u0002ed very-lo w-frequenc yenvelope\nsignal, n(t),(sampled at1Hz)inorder tocompensate the\ndynamic variations oftheinput signal. Note thatn(t)is\nestimated bytaking theabsolute value ofthemaximum in\n44100 samples windo ws.\n\u000fOnset detection :Anonset isdetected atlocations where\nthedifference between twosuccessi vesamples oftherec-\nti\u0002ed envelope signal exceeds agiventhreshold while an-\nother onset wasn'tdetected during thelast100ms. Note\nthatduetothenormalisation approach, thisthreshold does\nnotdepend ontheinitial signal energy.\n3.2.2 Duration andtempo extraction\nTempo extraction hasalso recei vedmuch interest intherecent\nyears (see [Scheirer ,1998 ]and[Alonso etal.,2003 ]forexam-\nple). Due totheimpulsi veness ofthetabla signals, asimple\napproach isalso preferred inthiswork. Alocal tempo attime\nTisestimated by\u0002nding apeak intheautocorrelation func-\ntionofenv(t);t2[T\u000010s;T+10s]intherange [60, 240]\nBPM (Beats perminute ).Events arethen aligned andquan-\ntized tothecorresponding time grid. Wekeepforeach strok e\nitsrelati veduration asafraction ofthetempo (each events is\nthen rhythmically described asanote, half-note, quarter noter\netc....).\n3.3 Featur esextraction\nEarly percussion instruments classi\u0002cation systems used en-\nergyinasetofwell-chosen frequenc ybands asfeatures.\nRecent systems uses more comple xfeatures sets, including\ntemporal, spectral orcepstral features ([Herrera etal.,2003 ],\n[McDonald andTsang, 1997 ],[Sillanp ¨a¨aetal.,2000 ]).\nThe power spectra ofvarious bols areplotted in\u0002gure 3.It\ncanbeseen that thediscrimination between thebols canbe\nachie vedaccording totheposition, relati veweight andwidth\nofthespectral peaks. Itisthen appropriate toconsider thatthepowerspectra ofeach strok ecanberepresented byaprobability\ndistrib ution andthatitcanbeapproximated byaweighted sum\nofNGaussian distrib utions. Fortabla signals, N=4repre-\nsents anappropriate choice. Thefour Gaussian distrib utions are\nobtained asfollows:\n\u000fAninitial estimate isobtained bycomputing theem-\npirical mean and variance infour pre-de\u0002ned fre-\nquenc ybands B1[0;150]Hz;B2=[150;220]Hz;B3=\n[220;380]Hz;B4=[700;900].Despite that theband-\nwidth ofthese bands aresuf\u0002cient tocope with signi\u0002-\ncant tuning variation oftheDayan, itwould benecessary\ntoadapt thisfront-end toobtain atruly tuning-independent\nsystem. Forexample, MelFrequenc yCepstral Coe\u0002cients\n(MFCC) which havebetter generalization properties have\nalso been tested buthavelead toslightly degraded perfor -\nmances onourcorpus.\n\u000fThen, animpro vedestimation isobtained byiterating the\nExpectation-Maximisation (EM) algorithm onatraining\ndata set.Inpractice, convergence isobtained inafewsteps\n(typically in4iterations).\nInsummary ,thefeature vectors Fareconstituted with 12ele-\nments F=f1;::;12(themean, variance, andrelati veweight of\neach ofthe4Gaussians).\n3.4 Lear ning andclassi\u0002cation ofbols\nDue tothetime dependencies discussed above,classifying each\nstrok eindependently ofitsconte xtleads tounsatisf actory re-\nsults. Anef\u0002cient approach that integrates conte xt(ortime)\ndependencies isgivenbytheHidden Mark ovModel (HMM).\nThis class ofmodels isparticularly suitable formodelling\nshort term time-dependencies andithasbeen successfully used\nforawide variety ofproblems ranging from speech recogni-\ntion ([Rabiner andJuang, 1993 ])topiano music transcription\n([Raphael, 2002 ]).Insuch aframe work, thesequence offea-\nturevectors Otisrepresented astheoutput ofaHidden Mark ov\nModel. The recognition isperformed bysearching themost\nlikelystates sequence, giventheoutput sequence offeature vec-\ntors.0 500 100000.0050.010.0150.020.0250.030.035\nfrequencynormalized amplitudeDha\n0 500 100000.020.040.060.080.1\nfrequencynormalized amplitudeGe\n0 500 100000.0050.010.0150.020.025\nfrequencynormalized amplitudeNa\n0 500 100000.511.522.533.54x 10-3\nfrequencynormalized amplitudeTi\n0 500 100000.0020.0040.0060.0080.01\nfrequencynormalized amplitudeKe\nFigure 3:Spectra ofcommon bols (from lefttoright Dha, Ge,Na,Ti,Ke).\n3.4.1 Description ofthemodel\nAlthough manyimplementation ofHMM arenowavailable, it\nisfeltimportant torecall themain stages ofsuch amodel and\ntodescribe howitisapplied totheparticular problem oftabla\nsequence recognition. The model includes states, transitions\nbetween states andemission probabilities.\nStates Acouple ofbolsB1B2isassociated toeach stateqt\nofthemodel. Inother words,qtrepresents B2inthe\nconte xtofB1attimet.Forexample, forthesequence\nofbolsb=(\u0003start\u0003;\u0003start\u0003;Dha;Dhin;Dhin;Dha),\nthecorresponding sequence ofstates isq0=[\u0003start\u0003:\u0003\nstart\u0003];q1=[\u0003start\u0003:Dha];q2=[Dha:Dhin];q3=\n[Dhin:D hin];q4=[Dhin:D ha].\nTransitions Ifthestateiislabelled byB1B2;andjbyB2B3,\nthen thetransition from stateqt\u00001=itostateqt=jis\ngivenby:\naij=p(qt=jjqt\u00001=i)\n=p(bt=B3jbt\u00001=B2;bt\u00002=B1)\nwhere p(bt=B3)istheprobability density ofobserving\nthebolB3attimet.\nInitial probabilities Since twodummy states*start* are\ninserted atthebeginning ofeach sequence, the\u0002rst state\nisalwaysq0=[\u0003start\u0003:\u0003start\u0003].\nEmission probabilities Each stateilabelled byB1B2emits a\nfeature vector according toadistrib utionbi(x)character -\nistic ofthebolB2preceded byB1:\nbi(x)=p(Ot=xjqt=i)\n=p(Ot=xjbt=B2;bt\u00001=B1)\nbi(x)represents theprobability ofobserving xknowing\nthatthestate attimetisi.Inthisworkbi(x)iseither mod-\nelled byasingle mixture (aGaussian vector distrib ution\nwith diagonal covariance matrix) oramixture oftwoGaus-\nsiandistrib utions. Forexample, inthesingle mixture case,\nthefeature vectors aremodelled with asingle vector distri-\nbution of4Gaussians distrib utions (where each Gaussian\ncharacterizes themean, variance andrelati veweights of\nthefrequenc ycontent ineach pre-de\u0002ned frequenc ybands,\nseesection 3.3).The HMM model described aboveisatrigrams model (3-\ngrams) since theparameters ofthismodel only depends onthe\ntwoprevious events. Inthispaper ,4-grams models (the three\nprevious events areused toestimate thevarious probabilities)\narealsoused.\n3.4.2 Training\nTransition probabilities areestimated bycounting occurrences\ninthetraining database :Ifthestateiislabelled byB1B2,jby\nB2B3,andifC(s) isthenumber ofoccurrences ofsubsequence\nsintheannotated corpus, then :\naij=p(bt=B3jbt\u00001=B2;bt\u00002=B1)=C(B1B2B3)\nC(B1B2)\nIfiislabelled byB1B2,emission probabilities forthisstate are\nestimated with :\n\u000fempirical mean andvariance estimators onthesetoffea-\nturevectors :Ai=fxt=bt=B2;bt\u00001=B1g-inthecase\nofasimple Gaussian model\n\u000f8iterations oftheEMalgorithm after arandom affectation\ntomixtures inthecase ofamixture model.\n3.4.3 Reco gnition\nRecognition isperformed through theclassical iterati veViterbi\nalgorithm. After aninitialisation step withvq0(0)=0and\nvq6=q0(0)=\u00001,thealgorithm includes:\nIteration\nvl(t+1)=logbl(ft+1)+max\nq2Qfvq(t)+log(aql)g\nbacktrackl(t)=argmax q2Qfvq(t)+log(aql)g\nBacktracking The most likelybols sequence b\u0003istheone\nwhich maximizes thelog-lik elihood andiscalculated by:\nq(T)=argmax q2Qfvq(T)g\nq(t)=backtrackq(t+1)(t)and b(t)=Etq(q(t))\nwhere Etq(q(t))represents thelabel (i.ethebol)ofstateq(t).Implementation issues Inpractice, itisneeded tousethe\nlog-lik ehood version ofthisalgorithm which permits toavoid\nthescaling problems. Another practical problem isthein-\nability ofthemodel torecognize some bols sequences which\nwere notpresent inthetraining set. Toauthorize such se-\nquences toberecognized (i.e. topermit alltransitions:\naij6=0),theiteration step oftheViterbi algorithm has\nbeen slightly modi\u0002ed to:vl(t+1)=logbl(ft+1)+\nmax q2Qfvq(t)+log((1\u0000\")aql+\")g\n4Experimental results\n4.1 Database andevaluation protocol\n4.1.1 Thedatabase\nThedatabase used forthisstudy ismade of64phrases with a\ntotal of5715 bols.Some ofthese phrases arelong compositions\nwith themes /variations (kaida ),some others areshorter pieces\n(tukra)orbasic taals .This database issub-di vided inthree sets,\neach onecorresponding toaspeci\u0002c model oftabla:\n\u000fTabla #1: recorded using acheap quality tabla whose\nstrok esaremuch less resonant than professional instru-\nments. TheDayan ofthetabla wastuned inC#3 andthe\nrecording wasmade using good home-studio equipment\n(Sennheiser e825s microphone), inlowreverberation and\nlownoise conditions.\n\u000fTabla #2: recorded using ahigh quality instrument and\nwith aDayan tuned inD3. Therecording wasmade with\ngood home-studio equipment (Sennheiser e825s micro-\nphone), inlowreverberation andlownoise conditions.\n\u000fTabla #3recorded using another high quality instrument\nandwith aDayan tuned inD3. This setwasrecorded in\nanoisier environment andwith agreater distance between\nthemicrophone andthemusician.\n4.1.2 Evaluation Protocol 1\nForevaluation, theusual cross-v alidation approach wasfol-\nlowed (often called ten-fold procedure inthe literature\n[Herrera etal.,2003 ]). Itconsists insplitting the whole\ndatabase in10subsets randomly selected andinusing nine of\nthem fortraining andthelastsubset (i.e. 10%ofthedata) for\ntesting. Theprocedure isthen iterated byrotating the10subsets\nused fortraining andtesting. The results arecomputed asthe\naverage values forthetenruns. Inthisprotocol, oneiteration\nthus consists inusing 90%ofeach data set(90%ofTabla #1,\nTabla #2andTabla #3)andtousetheremaining data (coming\nfrom each set)fortesting. Inthisprotocol, itisimportant to\nnote thatallinstruments andconditions areknowninthetrain-\ningphase.\n4.1.3 Evaluation Protocol 2\nToevaluate thegeneralization properties ofouralgorithm, a\nsecond protocol isused where training setsandtesting setcor-\nrespond todifferent instruments andrecording conditions. In\nthisprotocol, thetraining setconsists ofallsignals from 2sets\n(for example 100 %ofTabla #1andTabla #2)andthetest-\ningsetconsists ofallsignal from thethird set(i.e. 100%of\nTabla #3).This protocol therefore allowstoevaluate thegener -\nalization properties ofourapproach since thetraining setdoesnotinclude signals thatshare thesame environment acoustical\nproperties andinstrument than those ofthetesting set.\n4.2 Results\nThe results obtained forournovelapproach based onHMM\nlanguage model iscompared tosimpler classi\u0002ers such asKer-\nneldensity estimator ,k-NN (knearest neighbors) andanaive\nBayesian approach. These simple classi\u0002ers arebrie\u0003y de-\nscribed below:\nk-Near estNeighbors (k-NN) The k-NN approach isapop-\nular technique and has already been used insev-\neral studies onmusical instruments (see forexample\n[Herrera etal.,2003 ],[Kaminsk yj,2001 ]). Itconsists in\nbuilding asetofcouples associating afeature vectorF\nwith thecorresponding bolBj.Ifk=1,astrok eBwith\nfeature vectorFisthen identi\u0002ed asthebolBjwhose fea-\nturevectorFj=f1;::;12istheclosest tothefeature vector\nF.Forkgreater than 1,thedecision istakenonthemost\nrepresented bolBjamongst theknearest feature vectors\nFj.Thedistance used isaclassical Euclidian distance on\nthenormalised feature vectors:\nfi=fi\u0000\u0016fi\n\u001bfi\nwhere \u0016fiand\u001bfirespecti velycorrespond tothemean and\nstandard deviation oftheparameter fionthetraining set.\nNaiveBayesThis approach consists inbuilding aGaussian\nmodel foreach bolBj.Alltraining data foragivenbol\narethen used tobuildaGaussian model foreach element\nfiofthefeature vector leading toasetof24parameters per\nbolBj(each element fiofagivenbolBkisthen charac-\nterized byitsmean andvariance, respecti vely\u0016(fi;Bk)and\n\u001b(fi;Bk().The chosen bolBkisthen theonethatmax-\nimises thelikelihood knowing theobserv edfeature vector\nF:\np(B=Bkjo=F)=12Y\ni=11\n\u001b(fi;Bk)p\n2\u0019:e\u0000(fi\u0000\u0016(fi;Bk))2\n2\u001b2\n(fi;Bk)\nKernelDensity estimator This approach does notgenerate a\nreal abstract model ofthedata. Itmerely approximates\nthedistrib ution ofthedata byasum offunctions called the\nkernel. This Kernel canhavedifferent properties andcan\nforexample lead toverysmooth approximation ofthedis-\ntribution. Note thatoneoftheadvantage ofthisapproach\nisthatitcandescribe nonGaussian distrib utions.\nForallthree classi\u0002ers described abovetheimplementation pro-\nposed inWekaisused ([Waikato, 2003 ]).\nAsmentioned insection 4.1,twoevaluation protocols areused.\nThetranscription results obtained using these twoprotocols are\nrespecti velysummarized inTable 1andTable 2.\nFirst, itisimportant tonote that duetothesound quality of\nthedatabase, thesegmentation isveryef\u0002cient andresulted in\n98:3%correct segment detection (only 97errors -70strok esnot\ndetected, 27wrongly detected onsets) onthewhole database.Database All Tabla #1 Tabla #2 Tabla #3\n#ofbols 5715 1678 2216 1821\nClassi\u0002cation using only featur esofstroken\nKernel density estimator 81.7% 81.8% 82.4% 85.2%\n5-NN 83.0% 81.7% 83.3% 85.6%\nNaiveBayes 76.6% 79.4% 78.6% 78.5%\nClassi\u0002cation using featur esofstroken;n\u00001;n\u00002\nKernel density estimator 86.8% 86.0% 88.7% 92.0%\n5-NN 88.9% 87.2% 88.4% 90.6%\nNaiveBayes 81.8% 86.5% 83.8% 85.8%\nClassi\u0002cation using language modelling\nHMM, 3-grams, 1mixture 88.0% 90.6% 89.9% 92.6%\nHMM, 4-grams, 2mixtur es 93.6% 92.0% 91.9% 93.4%\nTable 1:Recognition scores using evaluation protocol 1\nInTable 1,itcanbeobserv edthatthesimple classi\u0002ers (Kernel\ndensity estimator ,k-NN, NaiveBayes) already obtain satisf ac-\ntoryresults andtherefore indicate thatthebasic parametric rep-\nresentation chosen isvaluable. However,these models cannot\ndeal with time dependencies. Asimple wayofmodelling these\ndependencies istoextend thefeature vectors with thefeatures\nofthe2previous bols.Inthese experiments, thefeature vectors\naresimply theconcatenation ofthefeature vector with those\nofthetwoprevious strok es.Clearly ,thissimple approach suc-\nceeds tomodel some ofthetime dependencies andrelevantly\nimpro vestherecognition score, especially fornon-parametric\nmodels. However,thebest results areachie vedwith theHMM\nlanguage model approach thatappears tobeaveryappropriate\nsolution forthetranscription ofTabla signals.\nTheTable 2summarizes theresults obtained with theprotocol\n2where thetraining setsandtestsetarerecorded ondifferent\ninstruments andindifferent conditions. Inother words thecon-\ndition andspeci\u0002cities ofthetestsetarenotknowninthetrain-\ningphase. Inthiscondition, theclassi\u0002er which gavethebest\nrecognition scores with protocol 1failed toproperly generalize\nandtoadapt toother instruments orrecording conditions. This\nisclearly another advantage oftheHMM approach where per-\nformance remain ataveryhigh levelevenwith signal recorded\nindifferent conditions. Though, itisinteresting tonote thatin\nthiscase thesimpler HMM model leads tothebestresults. This\nmay beexplained bythefactthatusing more comple xmixture\nmodels asdistrib utions fortheacoustic features may lead toan\nover\u0002tting ofthetraining data.\nInfurther analyzing theerrors ofouralgorithms, itcanbeno-\nticed that most ofthem occur with similar strok es. Ifbols\naregrouped in5categories corresponding toasimilar produc-\ntion mechanism (see Table 3),onecanobserv ethat most of\ntherecognition errors happen within thesame strok ecategory\n(non-r esonant bayan strokes).\nItisworth tonote thattheoverall category recognition rateis\nfairly high since itreaches 95.7% with thebest model (HMM,\n4-grams, 2mixtures models).\n5Tablascope :afully integrated environment\n5.1 Description\nThetabla transcription algorithm hasbeen embedded inafully\nintegrated environment called Tablascope. This system hasbeen developed following aclient/serv erapproach which pro-\nvides agreat levelof\u0003exibility andportability .Allthesignal\nprocessing /recognition modules areimplemented inaserver\nwritten inC++, while thegraphical client isaJavaapplication.\nAprotocol based ontheserialization ofobjects asXML mes-\nsages hasbeen chosen forthecommunication between thetwo\nmodules. Thedatabase andthetranscriptions arealsostored us-\ningtheXML format. Tablascope also integrates thepossibility\ntoexport transcription asCsound scores. Anovervie wofthe\nenvironment ispresented in\u0002gure 4.\n5.2 Applications\nSince therecognition algorithm runs inrealtime, anumber of\nother applications arepossible including:\nTabla sequence generation orsynthesis Inthat conte xt,the\nuser types inthedesired bols sequence andtheoverall\ntempo. Then, theHMM sequence model cancheck the\nphrase input bytheuser,i.e. checks whether itcon-\ntains unkno wnbols orforbidden bols sequences. Inthis\ncase, Tablascope suggests thenearest correct phrase ac-\ncording toaneditdistance. Synthesis isthen performed by\nanoverlap-add procedure slightly adapted topermit pitch\nshifting.\nTabla-contr olled synthesizer Tablascope canalsobeused asa\nMIDI controller ,thatistocontrol asynthesizer with input\ncoming from aTabla without using anyother sensor than\namicrophone. Insuch anapplication, agivenbolcanbe\nassociated toaspeci\u0002c instrument ofthesynthesizer giving\ntheTabla player awider range ofperformance capabilities.\n6Conclusion\nMost ofthemusic transcription systems arefocused onmelody\nandwestern music. Inthispaper ,wepresented atranscription\nsystem forTabla -anorth Indian percussion instrument -based\nonastatistical machine learning approach. Theidenti\u0002cation of\nTabla bols with alow(6.5%) error rateisachie vedthrough the\nuseofaHMM model inasimilar waythatlanguage models are\nexploited inlargevocabulary speech recognition systems. Ad-\nditional functions such astabla sequence correction andgenera-\ntion(orsynthesis) areintegrated inauser friendly environment\ncalled Tablascope .Although thisstudy isconducted onTabla\nsignals, itcaneasily begeneralized toother types ofpercus-Training set Tabla #1&Tabla #2 Tabla #2&Tabla #3\nTestset Tabla #3(noisy rec.) Tabla #1(cheap quality)\n5-NN 79.8 % 78.2 %\nHMM, 3-grams, 1mixtur e 90.2 % 88.4 %\nHMM, 4-grams, 2mixtures 84.5 % 85.0 %\nTable 2:Generalization testusing evaluation protocol 2\na b c d e¡-classi\u0002ed as\n1241 22 2 7 8a:resonant dayan strok es(Tin,Na,Tun...)\n20 1076 2 1 5b:bayan +dayan strok es(Dhin, Dha...)\n1 3766 5 20 c:resonant bayan strok es(Ge, Gi...)\n8 2 2448 61 d:non-resonant bayan strok es(Ke,Ki...)\n11 7 6 50 1938 e:non-resonant dayan strok es(Te,Ti,Tek...)\nTable 3:Confusion matrix bybolcategory (with HMM, 4-grams, 2mixtures classi\u0002er)\nFigure 4:Anovervie woftheTablascope environment :phrase edition (BolP ad),audio annotation, realtime transcription, corpus\nmanagement andtuning windo wssivesignals such aspercussion ordrum loops which also show\nstrong time dependencies. Asamatter offact,except forthe\nsignal parametric representation (i.etheacoustic front-end) that\nisrather speci\u0002c toTabla, allother modules arefairly generic.\n7Ackno wledgements\nTheauthors wish tothank Sanjay Pandit andUday Deshpande\nfortheir precious advice about Tabla theory ,Claire Waast-\nRichard forfruitful discussions oncurrent speech recognition\ntechnologies andNicolas Lelandais forthetabla performances\nused inthiswork.\nRefer ences\n[Alonso etal.,2003] Alonso, M.,Badeau, R.,David, B.,and\nRichard, G.(2003). Musical tempo estimation using noise\nsubspace projections. IEEE Workshop onApplications of\nSignal Processing toAudio andAcoustics (WASPAA'03).\n[A.Kapur andCook, 2002] A.Kapur ,G.Essl, P.D.andCook,\nP.R.(2002). The electronic tabla controller .InProceed-\nings ofthe2002 Confer ence onNewInstruments forMusical\nExpr ession (NIME-02) .\n[Fletcher andRossing, 1998] Fletcher ,N.and Rossing, T.\n(1998). ThePhysics ofMusical Instruments .Springer verlag,\n2ndedition edition.\n[Gouyon andHerrera, 2001] Gouyon, F.and Herrera, P.\n(2001). Exploration oftechniques forautomatic labeling of\naudio drum tracks. InProceedings ofMOSART :Workshop\nonCurr entDirections inComputer Music .\n[Herrera etal.,2000] Herrera, P.,Amatriain, X.,Batlle, E.,and\nSerra, X.(2000). Towards instrument segmentation formu-\nsiccontent description: Acritical reviewofautomatic musi-\ncalinstrument classi\u0002cation. InInProc.ofISMIR2000 .\n[Herrera etal.,2003] Herrera, P.,Dehamel, A.,andGouyon, F.\n(2003). Automatic labeling ofunpitched percussion sounds.\nIn114th AES Convention ,Amsterdam, TheNetherlands.\n[Kaminsk yj,2001] Kaminsk yj,I.(2001). Multi feature musical\ninstrument sound classi\u0002er .InProceedings ofAsutr alasian\nComputer Music Confer ence.\n[Klapuri, 1999] Klapuri, A.(1999). Sound onset detection\nbyapplying psychoacoustic knowledge. InIEEE Interna-\ntional Confer ence onAcoustics, Speec handSignal Process-\ning,Phoenix, Arizona.\n[Martin, 1999] Martin, K.(1999). Sound SourceReco gnition:\nATheory andComputational Model .Ph.d., Massachussetts\nInstitute ofTechnology .\n[McDonald andTsang, 1997] McDonald, S.and Tsang, C.\n(1997). Percussi vesound identi\u0002cation using spectral centre\ntrajectories. InProceedings of1997 Postgr aduate Resear ch\nConfer ence.\n[Rabiner andJuang, 1993] Rabiner ,L.andJuang, B.(1993).\nFundamentals ofspeec hrecognition .Engle woodCliffs,NJ.[Raman, 1934] Raman, C.(1934). The indian musical drum.\nInScience, P.I.A.,editor ,Reprinted inMusical Acoustics:\nselected reprints, ed.T.D.Rossing Am. Assn. Phys. Tech.,\nColle gePark,MD, 1988 .\n[Raphael, 2002] Raphael, C.(2002). Automatic transcription\nofpiano music. InProceedings ofISMIR2002 ,pages 1516.\n[Roh andWilcox, 1995] Roh, J.andWilcox, L.(1995). Ex-\nploring tabla drumming using rhythmic input. InProceed-\nings oftheCHI '95,pages 310311.\n[Scheirer ,1998] Scheirer ,E.D.(1998). Tempo andbeat anal-\nysis ofacoustic musical signals. Journal oftheAcoustical\nSociety ofAmerica ,103(1):588601.\n[Sillanp ¨a¨aetal.,2000] Sillanp ¨a¨a,J.,Klapuri, A.,Sepp ¨anen,\nJ.,andVirtanen, T.(2000). Recognition ofacoustic noise\nmixtures bycombined bottom-up andtop-do wnapproach.\nInProceedings ofEuropean Signal Processing Confer ence,\nEUSIPCO-2000 .\n[Waikato, 2003] Waikato (2003). Weka3:Machine learning\nsoftw areinjava.Inhttp:/ /www .cs.waikato.ac.nz/ml /weka/ ."
    },
    {
        "title": "Music scene description project: Toward audio-based real-time music understanding.",
        "author": [
            "Masataka Goto"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1415684",
        "url": "https://doi.org/10.5281/zenodo.1415684",
        "ee": "https://zenodo.org/records/1415684/files/Goto03.pdf",
        "abstract": "This paper reports a research project intended to build a real-time music-understanding system producing intuitively meaningful descriptions of real-world mu- sical audio signals, such as the melody lines and cho- rus sections. This paper also introduces our efforts to add correct descriptions (metadata) to the pieces in a music database. 1",
        "zenodo_id": 1415684,
        "dblp_key": "conf/ismir/Goto03",
        "keywords": [
            "real-time music-understanding system",
            "intuitively meaningful descriptions",
            "real-world musical audio signals",
            "melody lines",
            "chorus sections",
            "correct descriptions",
            "music database",
            "metadata",
            "efforts",
            "introduction"
        ],
        "content": "Music Scene Description Project:\nToward Audio-based Real-time Music Understanding\nMasataka Goto\n“Information and Human Activity,” PRESTO, JST / National Institute of Advanced Industrial Science and Technology (AIST)\nIT, AIST, 1-1-1 Umezono, Tsukuba, Ibaraki 305-8568, Japan\nm.goto@aist.go.jp\nAbstract\nThis paper reports a research project intended to build\na real-time music-understanding system producing\nintuitively meaningful descriptions of real-world mu-sical audio signals, such as the melody lines and cho-\nrus sections. This paper also introduces our efforts to\nadd correct descriptions (metadata) to the pieces in a\nmusic database.\n1 Introduction\nOur goal is to build a computer system that can understand mu-sical audio signals in a human-like fashion. People listening to\nmusic (especially popular music) can easily hum the melody,\nnotice a phrase being repeated, and ﬁnd chorus sections. The\nbrain mechanisms underlying these abilities, however, have notbeen understood. It has also been difﬁcult to implement these\nabilities on a computer system, although a system with them\nwould be useful in various applications such as music informa-tion retrieval and music production/editing. We therefore want\nto build a real-time system that can understand complex real-\nworld monaural music signals like those recorded on commer-cially distributed compact discs (CDs).\nTwo popular approaches are to build a sound source separation\nsystem (Casey and Westner, 2000) or an automatic music tran-\nscription system (Katayose and Inokuchi, 1989; Klapuri et al.,2001). Although these technologies are valuable from an engi-\nneering viewpoint, neither separation nor transcription is neces-\nsary or sufﬁcient for understanding music. The fact that humanlisteners understand various properties of audio signals is not\nnecessarily evidence that the human auditory system extracts\neach individual audio signal: even if a mixture of two compo-nents cannot be separated, that the mixture includes them can\nbe understood from their salient features. Indeed, as pointed\nout by Goto and Muraoka (1999) and Scheirer (2000), untrainedlisteners understand music to some extent without mentally rep-\nresenting audio signals as musical scores: music transcription is\na skill mastered only by trained musicians. Furthermore, even\nif we could derive separated signals and musical notes, it would\nstill not be easy to obtain high-level music descriptions like\nPermission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided thatcopies are not made or distributed for proﬁt or commercial advan-tage and that copies bear this notice and the full citation on the ﬁrstpage. c/circlecopyrt2003 Johns Hopkins University.\nmelody lines and chorus sections.\nWe have therefore been trying to construct a real-time music-\nscene-description system that obtains descriptions intuitively\nmeaningful to untrained listeners. By considering what is to\nbe achieved to understand music, we have proposed the follow-ing descriptions: hierarchical beat structure, melody line, bass\nline, repeated sections, and chorus sections (Figure 1). The fol-\nlowing sections introduce our methods for producing these de-scriptions and report our efforts to provide the songs in a music\ndatabase with correct descriptions by using a metadata editor\nwe developed.\n2 Real-time Methods for Obtaining Music\nScene Descriptions Automatically\nWe have proposed and implemented the following methods that\nproduce, in real time, the descriptions shown in Figure 1 of real-\nworld audio signals containing simultaneous sounds of variousinstruments (with or without drum-sounds).\n(1) An audio-based real-time beat-tracking method (Goto and\nMuraoka, 1999; Goto, 2001a)\nThis method recognizes, in audio signals sampled from\npopular-music CDs, a hierarchical beat structure compris-\ning the quarter-note and measure levels. Its main advan-\ntage is that it can track beats above the quarter-note levelby using three kinds of musical knowledge: onset times,\nchord changes, and drum patterns.\n(2) A predominant-F0 estimation method for detecting\nmelody and bass lines (PreFEst) (Goto, 2001b, 2003b)\nThe PreFEst (Pre\ndominant-F\n 0 Est\n imation Method) esti-\nmates the fundamental frequency (F0) of the melody and\nbass lines. To do this without assuming the number ofsound sources, it considers every possible F0 at the same\ntime and estimates a probability density function of the F0\n(relative dominance of each possible F0) by using the MAP(maximum a posteriori probability) estimation and the EM\n(expectation-maximization) algorithm.\n(3) A chorus-section detection method (RefraiD) (Goto,\n2003a,c)\nTheRefraiD (Refrai\nnD\netecting Method) detects sections\nbeing repeated and identiﬁes the chorus (refrain) sections\nof songs in popular-music CDs. Most previous methodsdetected as a chorus a repeated section of a given length\n(Logan and Chu, 2000; Cooper and Foote, 2002) and had\ndifﬁculty identifying both ends of a chorus section andCompact disc (CD)\nMusical\n  audio signalsRepeated\n  sections\nChorus\n  sections\nGlobal descriptions Local descriptionstimeHierarchical  beat structure\nMelody line\nBass linesame\nchorus\nchorus chorus...same\n...\nFigure 1: Descriptions in our music-scene-description system.\ndealing with modulations (key changes) (Peeters et al.,\n2002; Dannenberg and Hu, 2002). By analyzing relation-\nships between various repeated sections, RefraiD can de-\ntect all the chorus sections in a song and identify bothends of each section. It can also detect modulated chorus\nsections by introducing a similarity measure that enables\nmodulated repetition to be judged correctly.\n3 Hand-Labeling the Music Scene Descriptions\non a Metadata Editor\nTo evaluate automatic music-scene-description methods, we\nhave been working on labeling the pieces in a music database\nwith their correct descriptions (metadata). We have therefore\ndeveloped a multipurpose music-scene labeling editor (meta-data editor) that enables a user to hand-label a musical piece\nwith descriptions such as hierarchical beat structure, melody\nand bass lines, and chorus sections. It can deal with both au-dio ﬁles and standard MIDI ﬁles and it supports interactive\naudio/MIDI playback while editing. Along a wave or MIDI-\npiano-roll display it shows subwindows in which any selecteddescriptions can be displayed and edited. It also supports prac-\ntical editing aids such as a magnifying-glass function, a region-\nbased cut-and-paste operation, and cursor movement betweencontext-dependent grid points.\nThe editor has been ported on several operating systems (Linux,\nSGI IRIX, and Microsoft Windows). To facilitate the support of\nother descriptions in the future, its architecture is based on aplug-in system in which an external module for editing each\ndescription is installed as plug-in software.\nUsing the editor, we hand-labeled the chorus sections of all 100\nsongs of the RWC Music Database: Popular Music (Goto et al.,\n2002) and evaluated the RefraiD. By comparing the output of\nthe RefraiD with those hand-labeled chorus sections, we found\nthat the correct chorus sections had been detected in 80 of the100 songs. We are also working on labeling the songs in the\ndatabase with other descriptions.\n4 Conclusion\nWe have described the Music Scene Description Project in\nwhich we are building a music-scene-description system that\nunderstands real-world musical audio signals without deriving\nmusical scores or separating signals and are also developing a\nmetadata editor that enables a user to hand-label audio ﬁles andstandard MIDI ﬁles with descriptions of the music in those ﬁles.\nWe have already implemented real-time methods for tracking\nbeats, detecting melody and bass lines, and ﬁnding chorus sec-tions. We have also hand-labeled the popular songs in the RWC\nMusic Database with their chorus sections and used them to\nevaluate our chorus-section detection method.Because our automatic descrip-\ntion methods are useful for ob-\ntaining various metadata for mu-\nsic information retrieval, we planto build a retrieval system based\non such metadata. Other fu-\nture work will include improvingthe performance of the automatic\ndescription methods, supporting\nother music-scene descriptions,\nand hand-labeling the RWC Mu-\nsic Database with other kinds of metadata.\nAcknowledgments\nThis project has been funded by “Information and Human Ac-\ntivity,” Precursory Research for Embryonic Science and Tech-\nnology (PRESTO), Japan Science and Technology Corporation\n(JST) during October 2000 and September 2003.\nReferences\nCasey, M. A. and Westner, A. (2000). Separation of mixed au-\ndio sources by independent subspace analysis. In Proc. of ICMC\n2000 , pp. 154–161.\nCooper, M. and Foote, J. (2002). Automatic music summariza-\ntion via similarity analysis. In Proc. of ISMIR 2002 , pp. 81–85.\nDannenberg, R. B. and Hu, N. (2002). Pattern discovery tech-\nniques for music audio. In Proc. of ISMIR 2002 , pp. 63–70.\nGoto, M. (2001a). An audio-based real-time beat tracking sys-\ntem for music with or without drum-sounds. Journal of New\nMusic Research , 30(2):159–171.\nGoto, M. (2001b). A predominant-F0 estimation method for CD\nrecordings: MAP estimation using EM algorithm for adaptive\ntone models. In Proc. of ICASSP 2001 , pp. V–3365–3368.\nGoto, M. (2003a). A chorus-section detecting method for mu-\nsical audio signals. In Proc. of ICASSP 2003 , pp. V–437–440.\nGoto, M. (2003b). A real-time music scene description system:\nPredominant-f0 estimation for detecting melody and bass lines\nin real-world audio signals. Speech Communication .(accepted)\nGoto, M. (2003c). SmartMusicKIOSK: Music listening station\nwith chorus-search function. In Proc. of UIST 2003 .(accepted)\nGoto, M., Hashiguchi, H., Nishimura, T., and Oka, R. (2002).\nRWC music database: Popular, classical, and jazz music\ndatabases. In Proc. of ISMIR 2002 , pp. 287–288.\nGoto, M. and Muraoka, Y . (1999). Real-time beat tracking for\ndrumless audio signals: Chord change detection for musical de-\ncisions. Speech Communication , 27(3–4):311–335.\nKatayose, H. and Inokuchi, S. (1989). The Kansei music sys-\ntem. Computer Music Journal , 13(4):72–77.\nKlapuri, A., Virtanen, T., Eronen, A., and Sepp ¨anen, J. (2001).\nAutomatic transcription of musical recordings. In Proc. of\nCRAC-2001 .\nLogan, B. and Chu, S. (2000). Music summarization using key\nphrases. In Proc. of ICASSP 2000 , pp. II–749–752.\nPeeters, G., Burthe, A. L., and Rodet, X. (2002). Toward auto-\nmatic music audio summary generation from signal analysis. In\nProc. of ISMIR 2002 , pp. 94–100.\nScheirer, E. D. (2000). Music-Listening Systems . PhD thesis,\nMIT."
    },
    {
        "title": "RWC Music Database: Music genre database and musical instrument sound database.",
        "author": [
            "Masataka Goto",
            "Hiroki Hashiguchi",
            "Takuichi Nishimura",
            "Ryuichi Oka"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1415536",
        "url": "https://doi.org/10.5281/zenodo.1415536",
        "ee": "https://zenodo.org/records/1415536/files/GotoHNO03.pdf",
        "abstract": "This paper describes the design policy and specifications of the RWC Music Database, a copyright-cleared mu- sic database (DB) compiled specifically for research pur- poses. Shared DBs are common in other research fields and have made significant contributions to progress in those fields. The field of music information processing, however, has lacked a common DB of musical pieces or a large-scale DB of musical instrument sounds. We therefore recently constructed the RWC Music Database comprising four original component DBs: Popular Mu- sic Database (100 pieces), Royalty-Free Music Database (15 pieces), Classical Music Database (50 pieces), and Jazz Music Database (50 pieces). In this paper we re- port the construction of two additional component DBs: Music Genre Database (100 pieces) and Musical Instru- ment Sound Database (50 instruments). It is our hope that our DB will make a significant contribution to future advances in the field of music information processing. 1",
        "zenodo_id": 1415536,
        "dblp_key": "conf/ismir/GotoHNO03",
        "keywords": [
            "RWC Music Database",
            "Copyright-cleared music database",
            "Research purposes",
            "Shared DBs",
            "Music information processing",
            "Musical pieces",
            "Musical instrument sounds",
            "Additional component DBs",
            "Music Genre Database",
            "Musical Instrument Sound Database"
        ],
        "content": "RWC Music Database:\nMusic Genre Database and Musical Instrument Sound Database\nMasataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST)\n/ “Information and Human Activity”, PRESTO, JST\nIT, AIST, 1-1-1 Umezono, Tsukuba, Ibaraki 305-8568, Japan\nm.goto@aist.go.jpHiroki Hashiguchi\nMejiro University\n4-31-1 Naka-Ochiai, Shinjuku-ku,\nTokyo 161-8539, Japan\nhiro@mejiro.ac.jp\nTakuichi Nishimura\nNational Institute of Advanced Industrial Science and Technology (AIST)\nCARC, AIST, 2-41-6 Aomi, Koto-ku, Tokyo 135-0064, Japan\ntaku@ni.aist.go.jpRyuichi Oka\nUniversity of Aizu\nAizu-Wakamatsu, Fukushima 965-8580, Japan\noka@u-aizu.ac.jp\nAbstract\nThis paper describes the design policy and speciﬁcations\nof the RWC Music Database , a copyright-cleared mu-\nsic database (DB) compiled speciﬁcally for research pur-\nposes. Shared DBs are common in other research ﬁeldsand have made signiﬁcant contributions to progress in\nthose ﬁelds. The ﬁeld of music information processing,\nhowever, has lacked a common DB of musical pieces\nor a large-scale DB of musical instrument sounds. We\ntherefore recently constructed the RWC Music Database\ncomprising four original component DBs: Popular Mu-\nsic Database (100 pieces), Royalty-Free Music Database\n(15 pieces), Classical Music Database (50 pieces), and\nJazz Music Database (50 pieces). In this paper we re-\nport the construction of two additional component DBs:\nMusic Genre Database (100 pieces) and Musical Instru-\nment Sound Database (50 instruments). It is our hope\nthat our DB will make a signiﬁcant contribution to future\nadvances in the ﬁeld of music information processing.\n1 Introduction\nWith the aim of promoting further advances in the ﬁeld of mu-sic information processing, we constructed a copyright-cleared\nmusic DB that can be used in common by researchers (Goto\net al., 2002). This common DB will provide a benchmark en-abling researchers to compare and evaluate their various sys-\ntems and methods against a common standard. It can also be\nused to stimulate research in corpus-oriented approaches thatuse statistical methods and learning techniques. In all cases, re-\nsearchers can use this copyright-cleared DB for research publi-\ncations and presentations (including publications in conferenceCD-ROMs). Since copyright restrictions prevent commercially\ndistributed musical pieces from being easily used in such pre-\nsentations, the availability of a common DB should play an im-\nportant role in promoting substantial advances in this research\nﬁeld.\nVarious commonly available DBs have been built in other re-\nsearch ﬁelds such as speech and image processing since the im-portance and signiﬁcance of such DBs have been widely recog-\nnized. The ﬁeld of music information processing, however, has\nPermission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided thatcopies are not made or distributed for proﬁt or commercial advan-tage and that copies bear this notice and the full citation on the ﬁrstpage. c/circlecopyrt2003 Johns Hopkins University.\nlong lacked a copyright-cleared DB of musical pieces that could\nbe used as a common foundation for research. We therefore re-\ncently constructed a music DB — the RWC (Real World Com-\nputing) Music Database (Goto et al., 2002) — that researchers\ncan obtain and use freely in common for research purposes.\nThis music DB features four component DBs: the Popular Mu-\nsic Database ,Royalty-Free Music Database ,Classical Music\nDatabase , and Jazz Music Database . Meanwhile, for musical\ninstrument sounds, there are the McGill University Master Sam-\nples (Opolko and Wapnick, 1987) and the University of Iowa\nMusical Instrument Samples released for public use, and privatecollections assembled within companies (Kashino and Murase,\n1999). Nevertheless, there is still a need for a large-scale com-\nmon DB containing the sounds of many musical instrumentsplayed in various styles.\nIn this paper we report the construction of two additional com-\nponent DBs, Music Genre Database andMusical Instrument\nSound Database , that follow the basic concept of the four DBs\nalready used in the RWC Music Database . The following sec-\ntions describe the design policy and an overview of each of the\ntwo new DBs.\n2 Design Policy\nWe decided to include musical instrument sounds as well asmusical pieces and took into consideration the following points\nwhen designing these two additional DBs.\nContent\nWith respect to musical pieces, we constructed a DB of mu-\nsic genres complementing the four DBs already in the RWC\nMusic Database . This Music Genre Database contains 100\nmusical pieces in a variety of genres. In producing these\npieces, we included as much variety as our production re-source constraints allowed.\nWith respect to musical instrument sounds, we constructed\na DB covering 50 musical instruments and, for each in-\nstrument, recorded individual sounds at half-tone intervals.Furthermore, to provide variety in playing style, dynamics\n(stress), instrument manufacturers, and musicians, we made\nmany recordings for each type of instrument.\nCopyrights of musical pieces\nTo make our DB available to researchers around the world,\nwe had to obtain all the necessary copyrights and associatedlegal interests related to this DB. Accordingly, for some mu-\nsic genres (classical, marches, vocals, and some traditional\nJapanese music) we used 27 public-domain pieces, and forTable 1: List of music CDs and DVD-ROMs for distributing the Music Genre Database and Musical Instrument Sound Database.\nContent (Version)\n Quantity\n CD/DVD Catalog Number\n Piece/Instrument Nos.\nMusic G\n enre Database (Original Version: M\n ixed)\n 9 CDs\n RWC-MDB-G-2001-M01 ∼M09\n Nos. 1–100\nMusical I\n nstrument Sound Database (W\n ave Files)\n 12 DVDs\n RWC-MDB-I-2001-W01 ∼W12\n Nos. 01–50\nCatalog number: RWC-MDB-[Content]-[Year]-[Version][Volume No.], Content: The underlined letter , Year: Made in 2001\nthe others we used original pieces composed and arranged\nfor this DB. All 100 musical pieces were newly performed,sung, and recorded. Note that our DB is not copyright-free\neven though it is available for free for research purposes.\nStandard MIDI ﬁles (SMFs) and lyrics text ﬁles\nWe prepared transcribed SMFs for all musical pieces. These\nSMFs are valuable because they can be freely used for re-\nsearch purposes and can also be used as effective substitutesfor scores. Song lyrics are provided as separate text ﬁles.\nTable 1 lists the music compact discs (CDs) and DVD-ROMs\nwe prepared for distributing audio signals to researchers. Each\nof the musical pieces and musical instruments has a unique“piece/instrument number” (sequential within each DB) that\nshould be referred to for research use and publication (e.g.,\nRWC-MDB-G-2001 No. 53).\n3 Music Genre Database\nThis DB consists of 100 musical pieces, three for each of 33\ngenres and one for a cappella. It is divided into 10 main cat-\negories (popular, rock, dance, jazz, Latin, classical, marches,\nworld, vocals, and traditional Japanese music) and 33 subcate-\ngories (popular, ballads, rock, heavy metal, rap/hip-hop, house,\ntechno, funk, soul/R&B, big band, modern Jazz, fusion, bossanova, samba, reggae, tango, baroque, classic, romantic, mod-\nern, brass band, blues, folk, country, gospel, African, Indian,\nﬂamenco, chanson, canzone, traditional-style Japanese popu-lar music Enka , Japanese folk music Min’you , and ancient\nJapanese court music Gagaku ). Note that this does not mean to\nimply that all music can be categorized in this way: these cat-egories were used simply for convenience when recording the\npieces. All 100 pieces are original recordings, 73 being original\ncompositions and 27 being existing public-domain pieces. Atotal of 280 people participated in their production.\n4 Musical Instrument Sound Database\nThis DB covers 50 musical instruments and provides, in princi-ple, three variations for each instrument. It thus comprises per-formances of about 150 instrument bodies. To provide a wide\nvariety of sounds, we took the following approach.\nVariations (3 instrument manufacturers, 3 musicians): Each\nvariation featured, in principle, an instrument from a differ-\nent manufacturer played by a different musician.\nPlaying style (instrument dependent): Within the range pos-\nsible for each instrument, we recorded many playing styles.\nPitch (total range): For each playing style, the musician\nplayed individual sounds at half-tone intervals over the entire\nrange of tones that could be produced by that instrument.\nDynamics (3 dynamic levels): We also recorded each play-\ning style at three levels of dynamics (forte, mezzo, piano)\nspanning the total range of the instrument.In producing\nRWC-MDB-I-2001 No. 01 “Piano”, for exam-\nple, we used three pianos from three different manufacturers(Yamaha, B¨ osendorfer, and Steinway) and recorded, for each of\n88 keys, four different playing styles (normal, staccato, pedal,\nand repeated playing of same sound) at three dynamic levels\n(forte, mezzo, and piano). In other words, we recorded a total\nof 3168 (3 ×88×4×3) individual sounds for this DB item.\nThese sounds were assembled into 36 (3 ×4×3) ﬁles, each\nholding 88 keys’ worth of sounds.\nThe sounds of these 50 instruments were recorded at 16 bit /\n44.1 kHz and stored in 3544 monaural sound ﬁles having a total\nsize of about 29.1 Gbytes and a total playback time (includ-ing mute intervals) of about 91.6 hours. Each ﬁle, in principle,\nholds a collection of individual sounds in the order of ascending\npitch across the total range of an instrument. In addition, about\nﬁve color photographs of each individual instrument were also\ntaken and assembled into 948 ﬁles (about 703.1 Mbytes).\n5 Conclusion\nWe have described the construction of the Music Genre Data-\nbase andMusical Instrument Sound Database complementing\nthe four original DBs (Goto et al., 2002) in the RWC Mu-\nsic Database . The RWC Music Database built in ﬁscal 2000\nand 2001 by the RWC Music Database Sub-Working Group\n(chair: Masataka Goto) in the Real World Computing Partner-\nship (RWCP) of Japan is now complete with these six compo-nent DBs. It is available to researchers around the world at a\nnominal cost to cover only duplication, shipping, and handling\ncharges (i.e., it is practically free). While it was built for gen-eral purposes related to music information processing, we hope\nthat our DB is a ﬁrst step toward the construction of various mu-\nsic DBs for specialized purposes and will accelerate progress in\nthis ﬁeld of research.\nAcknowledgmentsWe thank everyone who has made this DB project possible. We\nreceived considerable cooperation from Yuzuru Hiraga, Keiji\nHirata, Satoru Hayamizu, Hideki Asoh, and Katunobu Itou.\nThis project was supported by many parties involved in the\nRWC project. The C MUSIC Corporation carried out the pro-\nduction of all the musical pieces and the recording of the instru-ment sounds.\nReferences\nGoto, M., Hashiguchi, H., Nishimura, T., and Oka, R. (2002).\nRWC music database: Popular, classical, and jazz music\ndatabases. In Proc. of ISMIR 2002 , pp. 287–288.\nKashino, K. and Murase, H. (1999). A sound source identiﬁ-\ncation system for ensemble music based on template adaptation\nand music stream extraction. Speech Communication , 27(3–\n4):337–349.\nOpolko, F. and Wapnick, J. (1987). McGill University master\nsamples (CDs)."
    },
    {
        "title": "Automatic segmentation, learning and retrieval of melodies using a self-organizing neural network.",
        "author": [
            "S. Harford"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1416182",
        "url": "https://doi.org/10.5281/zenodo.1416182",
        "ee": "https://zenodo.org/records/1416182/files/Harford03.pdf",
        "abstract": "We introduce a neural network, known as SONNET- MAP, capable of automatic segmentation, learning and retrieval of melodies. SONNET-MAP is a syn- thesis of the SONNET (Self-Organizing Neural NET- work) architecture (Nigrin, 1993) and an associa- tive map derived from ARTMAP (Carpenter, Gross- berg, and Reynolds, 1991). SONNET-MAP automat- ically segments a melody based on pitch and rhythmic grouping cues. Separate SONNET modules represent the pitch and rhythm dimensions of each segmented phrase independently, with two associative maps fus- ing these representations at the phrase level. Further SONNET modules aggregate these phrases forming a hierarchical memory structure that encompasses the entire melody. In addition, melodic queries may be used to retrieve any encoded melody. As far as we are aware, SONNET-MAP is the first self-organizing neural network architecture capable of automatically segmenting and retrieving melodies based on both pitch and rhythm. 1",
        "zenodo_id": 1416182,
        "dblp_key": "conf/ismir/Harford03",
        "keywords": [
            "SONNET-MAP",
            "melody segmentation",
            "pitch and rhythmic grouping cues",
            "associative map",
            "melodic queries",
            "melody retrieval",
            "self-organizing neural network architecture",
            "pitch and rhythm",
            "melodic structure",
            "melodic representation"
        ],
        "content": "Automatic Segmentation, Learning andRetrieval ofMelodie s Using A\nSelf-Organizing Neural Network\nStevenHarford\nSchoolofComputing\nDublinCity University\nDublin9\nIreland\nsharford@computing.dcu.ie\nAbstract\nWe introduce a neural network, known as SONNET-\nMAP, capable of automatic segmentation, learning\nand retrieval of melodies. SONNET-MAP is a syn-\nthesisoftheSONNET(Self-OrganizingNeuralNET-\nwork) architecture (Nigrin, 1993) and an associa-\ntive map derived from ARTMAP (Carpenter, Gross-\nberg,andReynolds,1991). SONNET-MAP automat-\nicallysegmentsamelodybasedonpitchandrhythmic\ngroupingcues. SeparateSONNETmodulesrepresent\nthe pitch and rhythm dimensions of each segmented\nphraseindependently,with two associativemapsfus-\ning these representations at the phrase level. Further\nSONNETmodulesaggregatethesephrasesforminga\nhierarchical memory structure that encompasses the\nentire melody. In addition, melodic queries may be\nused to retrieve any encoded melody. As far as we\nare aware, SONNET-MAP is the ﬁrst self-organizing\nneural network architecture capable of automatically\nsegmenting and retrieving melodies based on both\npitchandrhythm.\n1 Introduction\nTypically,music informationretrieval(MIR) systems repr esent\nmelodies as strings of symbols drawn from a ﬁnite alphabet,\nthereby reducing the retrieval process to the task of approx i-\nmate string matching. We introduce a self-organizing neura l\nnetwork architecture, known as SONNET-MAP, that offers an\nalternative approach to MIR. SONNET-MAP does not repre-\nsent a melody as a string of symbols but, instead, as a learned\nhierarchy of automatically segmented melodic phrases that are\nencodedinlong-termmemory(LTM)byadjustableconnection\nweights. Furthermore,the retrieval process is accomplish edby\npropagating cell activities through the network rather tha n by\ntheuse oftraditionalstringmatchingalgorithms.\nSONNET-MAP is primarily based on the SONNET architec-\nture. Recent work has applied SONNET to the problem of\nPermission to make digital or hard copies of all or part of thi s work\nfor personal or classroom use is granted without fee provide d that\ncopies are not made or distributed for proﬁt or commercial ad van-\ntage and that copies bear this notice and the full citation on the ﬁrst\npage.c/circlecopyrt2003 Johns Hopkins University.learning pitch sequences (Page, 1993) and to the interpreta tion\nof rhythmic structure (Roberts, 1996). We expand on this re-\nsearch by applyingSONNET-MAP to the automaticsegmenta-\ntionandretrievalofmelodiesbasedonbothpitchandrhythm .\n2 SONNET-MAP\n2.1 Architecture\nSONNET-MAP is composed of two parallel SONNET mod-\nules; onewhichrepresentspitchsequencesandtheotherwhi ch\nrepresents rhythms (see Figure 1). These modules are con-\nnected via two associative maps, derived from ARTMAP, that\nbind pitch sequence representations with their correspond ing\nrhythmrepresentations,therebyformingrigid,two-dimen sional\nrepresentations of melodic phrases. An additional SONNET\nmodule aggregates these phrases forming melodic phrase se-\nquences which, in turn, are aggregated by further SONNET\nmodules forming a hierarchical memory structure that encom -\npasses the entire melody. With such an organization, a singl e\nclassiﬁcationcellatthemostabstractlevelinthehierarc hyrep-\nresentsthe wholemelody.\nFigure1: TheSONNET-MAPArchitecture\n2.2 MelodyRepresentation\nMelodiesare presentedto SONNET-MAP’sevent ﬁeld as a se-\nquence of {Invariant Pitch-Class, Onset Time }duples. Withtheinvariantpitch-classrepresentation(Bharucha,1991 ),allse-\nquences are normalized into a common set of invariant pitch\ncategoriesbycodingthem relativeto a tonalcenter. In the c ase\nof Western tonal music employing equal-tempered tuning, ev -\nery input cell of the pitch sequence module represents one of\ntwelve chromaticpitch classes. Rhythmsare representedus ing\ninter-onset intervals (IOIs). By allowing the input cells o f the\nrhythmic module to represent onset times and by allowing the\ncellactivationstocontinuouslychange,therelativeacti vitylev-\nels associated with consecutive onsets represent IOIs (Rob erts,\n1996).\n2.3 Segmentation\nSONNET-MAPsegmentsmelodiesusinganumberofgrouping\nmechanismsthatarebasedonaspectsofhumanmelodypercep-\ntion (Roberts, 1996). Some of these mechanismsare describe d\nbelow:\n•An extendedresponse time is affordedto the ﬁrst items in\na sequence. Consequently, a primacy effect is exhibited\nwhereby the ﬁrst pattern in a sequence is generally more\nlikelytobe encodedthanlaterpatterns.\n•The limited capacity and depth of STM restricts the size\nanddurationofmelodicsegments.\n•Bottom-up weight initialization biases the network to-\nwardsgroupingbytemporalproximity.\n•Generally, relatively long IOIs tend to indicate phrase\nboundaries. Such IOIs providean extendedresponse time\nallowing SONNET-MAP to segment patterns ending with\nrelativelylongIOIswithoutdifﬁculty.\n•Frequently occurring musical patterns tend to represent\nsigniﬁcant melodic phrases. Repetitions of such phrases\nallowthemtobeeasily segmentedbySONNET-MAP.\nMelucci and Orio (2002) argue that automatic segmentation\nbasedonmelodicfeaturesismoreeffectivethanalgorithms that\ndo not use such information. The approach outlined above is\nconsistent with this argument and preliminary SONNET-MAP\nsimulationsproducedsegmentationscomparabletomanuals eg-\nmentation.\n2.4 Retrieval\nThe parallel and hierarchical organization of SONNET-MAP\nprovides practical beneﬁts when applied to content-based r e-\ntrieval (CBR). The initial separation of pitch sequences an d\nrhythmsallows retrievalto be based on either pitch, or rhyt hm,\nor both. SONNET-MAP also provides a set of vigilance pa-\nrametersthat may be used to allow either exact or approximat e\nmelody matching. Unlike other CBR systems, SONNET-MAP\ndoes not retrieve melodies using traditional string matchi ng al-\ngorithms. Instead, by presenting melodic queries to SONNET -\nMAP, previously encoded melodies are retrieved by the prop-\nagation of cell activities throughout the network. The most\nhighly activated cell assemblies in the most abstract SONNE T\nmoduleareusedtoforma rankedlist ofcandidatemelodies.\n3 Preliminary Simulations\nPreliminary simulations of SONNET-MAP were tested using\na set of contemporary melodies. The segmentations formedwereconsistentwiththegroupingmechanismsdescribedabo ve\nand were comparable to manual segmentations, which we per-\nformed. SONNET-MAP was then applied to the task of CBR.\nMelodic querieswere presentedto SONNET-MAP andin each\ncase SONNET-MAP retrieved the desired melody. Generally,\nmelodies were retrieved using fewer notes when the melodic\nqueries were aligned with the segmentations performed by\nSONNET-MAP. This further demonstrates the value of utiliz-\ningsegmentationbasedonmelodicfeatures.\n4 Conclusions and Future Work\nWe introduced a system, known as SONNET-MAP, capa-\nble of automatic segmentation, learning and retrieval of\nmelodies. Preliminary testing revealed promising results\nregarding SONNET-MAP’s ability to segment and retrieve\nmelodies. However,these simulationswerebasedonexactpa t-\ntern matching using perfect queries. SONNET-MAP needs to\nbe tested using more realistic queries, such as those genera ted\nbyquery-by-humming(QBH)systems,inordertoevaluatehow\nwell it copeswith imperfectinput.\nRoberts (1996) shows how a beat-tracking signal can be trans -\nmitted to SONNET in order to achieve tempo invariant pattern\nrecognition. No beat-trackingsignal is providedin the net work\nsimulationsdescribedabove;insteadthetempoofeachmelo dy\nis suppliedin advance(in the formof a tactus-span)in order to\nachievetempoinvariance. Thismethodhasthe disadvantage of\npreventingSONNET-MAP from operatingin real-time. There-\nfore,futureversionsofSONNET-MAPshouldbeprovidedwith\na beat-trackingsignal.\nSONNET-MAP was designed in a manner that allows the dif-\nferent dimensions of melody to be independently processed\nand subsequently combined at more abstract levels. The ad-\ndition of other parallel SONNET modules representing other\nmelodicdimensions(e.g. duration,intensity,timbre)wou ldfur-\nther enhance SONNET-MAP’s ability to segment and retrieve\nmelodies.\nReferences\nBharucha,J. J. (1991). Pitch,harmony,andneuralnets: A ps y-\nchological perspective. In Todd, P. M. & Loy, D. G. (Eds.),\nMusic andConnectionism .TheMITPress.\nCarpenter, G. A., Grossberg, S., & Reynolds, J. H. (1991).\nArtmap: Supervised real-time learning and classiﬁcation o f\nnonstationary data by a self-organizing neural network. Neu-\nralNetworks , 4(5).\nMelucci, M. & Orio, N. (2002). A comparison of manual and\nautomatic melody segmentation. In Proceedings of the 3rd In-\nternationalConferenceonMusic InformationRetrieval .\nNigrin, A. (1993). Neural Networks for Pattern Recognition .\nTheMITPress.\nPage, M. P. A. (1993). Modelling Aspects of Music Perception\nUsingSelf-OrganizingNeuralNetworks . PhDthesis,University\nofWales.\nRoberts, S. C. (1996). Interpreting Rhythmic Structures Using\nArtiﬁcialNeuralNetworks . PhD thesis,UniversityofWales."
    },
    {
        "title": "Three-dimensional continuous DP algorithm for multiple pitch candidates in a music information retrieval system.",
        "author": [
            "Sung-Phil Heo",
            "Motoyuki Suzuki",
            "Akinori Ito",
            "Shozo Makino"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1416862",
        "url": "https://doi.org/10.5281/zenodo.1416862",
        "ee": "https://zenodo.org/records/1416862/files/HeoSIM03.pdf",
        "abstract": "This paper treats theoretical and practical issues that implement a music information retrieval system based on query by humming. In order to extract accuracy features from the user’s humming, we propose a new retrieval method based on multiple pitch candidates. Extracted multiple pitches have shown to be very important parameters in determining melodic similarity, but it is also clear that the confidence measures feature which are obtained from the power are important as well. Furthermore, we propose extending the traditional DP algorithm to three dimensions so that multiple pitch candidates can be treated. Simultaneously, at the melody representation technique, we propose the DP paths are changed dynamically to be able to take relative values so that they can respond to insert or omit notes. 1",
        "zenodo_id": 1416862,
        "dblp_key": "conf/ismir/HeoSIM03",
        "keywords": [
            "music information retrieval",
            "query by humming",
            "pitch candidates",
            "melodic similarity",
            "confidence measures",
            "traditional DP algorithm",
            "melody representation",
            "DP paths",
            "dynamic changes",
            "notes"
        ],
        "content": "Three Dimensional Continuous DP Algorithm for Multiple Pitch Candidates \nin Music Information Retrieval System \nSung-Phil HEO†          Motoyuki SUZUKI‡          Akinori ITO‡          Shozo MAKINO‡ \n†Graduate School of Information Sciences Tohoku University \n‡Graduate School of Engineering Tohoku University \n05 Aza-Aoba, Aramaki, Aoba-ku Sendai, 980-8579 Japan \n{hsphil, moto, aito, makino}@makino.ecei.tohoku.ac.jp \n \nAbstract \nThis paper treats theoretical and practical issues that \nimplement a music information retrieval system based on query by humming. In order to extract accuracy features from the user’s humming, we propose a new retrieval method based on multiple pitch candidates. Extracted multiple pitches have shown to be very important parameters in determining melodic similarity, but it is also clear that the confidence measures feature which are obtained from the power are important as well. Furthermore, we propose extending the \ntraditional DP algorithm to three dimensions so that multiple \npitch candidates can be treated. Simultaneously, at the melody representation technique, we propose the DP paths are changed dynamically to be able to take relative values so that they can respond to insert or omit notes. \n1 Introduction \nIn researching MIRS (Music Information Retrieval System), \nretrieval keys have been made to not only using text information, such as a singer’s name, the composer, the title of the piece of music, or the lyrics of a song, but also the use of music information has been accomplished. \nThere are several important issues in building MIRS. One is \nthat an input query may have various errors due to uncertainty of the user’s memory or the user’s singing ability. Next problem, it is still difficult to implement a 100% accurate system for transcribing the hummed signals into musical symbols, which are needed for melody matching in the next step (Kosugi,  N. et al.  2000, Pauws, S. 2002).To tolerate these \nerrors, we need an effective representation of the hummed \nmelody and a musically reasonable approximate matching method. Therefore, we can consider the following problems: the feature extraction problem such as harmonic frequencies was extracted, the melody representation problem when insertion or omission of notes occurred and the melody matching problem connected with such problems. We believe the above problems are the key points for complete, robust and efficient MIRS. 2 Extraction of multiple pitch candidates \nPitch is a perception that is defined as the characteristic of a \nsound that gives the sensation of being high or being low. A pitch extraction algorithm is applied to the humming input, but it often captures incorrect frequency. \nThe accuracy of the pitch extraction greatly affects the \nsystem’s performance. Therefore, we consider multiple pitch candidates to enhance the performance of retrieval. \nThe pitch extraction accuracy was calculated by comparing \nthe true pitch value with the extraction result. The references were labeled by human. The pitch extraction accuracies when \n1st rank, within the 2nd rank and within the 3rd rank obtained \n88.4%, 96.3% and 99.7%, respectively. Harmonic frequencies (double or half pitch of the true pitch) were extracted at the most frames with incorrect results. This result shows that three pitch candidates are enough for the following processing. \nThe pitch extraction is based on cepstral analysis. Multiple \npitch candidates are passed to the query engine without \nchoosing one candidate in the feature extraction part. The \nconfidence measure is calculated as the cepstral value of the peak divided by that of the top candidate. \n3 Melody representation \nWhen a user inputs humming, its key and tempo may differ \nfrom that of the corresponding music in the database. Therefore humming data needs to be normalized. The traditional melody representation method normalizes the humming data by calculating relative pitch ratio and relative span ratio against the successive note (Sonoda,  T. et al. 1998). \nHowever there are several problems in the traditional melody \nrepresentation method. When insertion or omission of notes \noccurs, relative values are changed. \nTherefore we propose a new melody representation strategy to \ntreat this kind of mismatch in Figure 1. When a note is matched in an assumption that the previous note in humming is separation, the relative pitch of the current note in humming \nshould be calculated against the second note before the current \nnote. \nThis means deciding dynamically the relative value to match. \nBesides in omission, the determination method of the relative pitches is dynamically decided by the same manner. \nPermission to make digital or hard copies of all or part of this work for \npersonal of classroom use is granted without fee provided that copies are not \nmade or distributed for profit or commercial advantage and that copies bear \nthis notice and the full citation on the first page.   2003 The Johns Hopkins \nUniversity. SOL\nMI\nRE\n200 300\nHumming\n300\nRE MI SOL\nDatabaseDatabase Humming\n300300\nseparationseparation200 200SOL\n200d=0\n \nFigure 1: Relative pitch values conversion method when \nseparation is occurred. (Unit is cent) \n4 Three dimensional continuous DP algorithm \nThe features obtained from humming and the features of the \nmusical piece in the database are matched using continuous DP (Paulus, J. et al.  2002). However, the query engine must \nbe extended so that multiple pitch candidates along with \nconfidence measure can be utilized. Furthermore, the DP \nequation must be changed. The relative notes should be dynamically calculated considering the insert or omit of notes. Therefore, the DP algorithm is extended into three dimensions shown as equation (1). Here, g(i,j[k])  is the accumulation \ndistance of the k-th pitch candidate value in the j-th humming \nnote and the i-th musical piece note. \n\n\n\n+ − −+ − −+ − −\n=\n])}.[],[,(2])[2,1({min])}[],[,( ])[1,1({min])}[],[,( ])[1,2({min\nmin])[,(\n321\nljkjid l j igljkjid lj igljkjid lj ig\nkjig\nlll         (1) \n() ).,() 1()},( 1),,({ ),,(1yxt zyc zyxp zyxde e e e β α αβ −+ −+ =−      (2) \nThis score is a weighted sum of pitches with confidence \nmeasure and span scores, pe(x, y, z) , ce(y, z)  and te(x, y) , the \nformer being the weight assigned for a particular distance in pitch, the next being the confidence measure obtained from the power value, and the last being the weight assigned for the distance in duration. Moreover, d\ne(e=1,2,3)  equation (2) \ncorresponds to local path constraint, respectively (Heo, Sung-Phil et al.  2003). The factors α, β can be varied to reflect the \nrelative contribution of pitch, confidence measure and span. \nThe humming is matched with the database at the DP plane \nextended to three dimensions. Therefore the mechanism of the matching algorithm extended to three dimensions. When considering matching the humming to the database, the proposed algorithm calculates the combination in all candidate points. Finally, the algorithm will determine the optimal candidate points and paths. \n5 Experimental conditions and results \nThe music database consists of children’s songs, and 155 \npieces of music from Japan and foreign countries. We also \nused the query corpus which is 5 subjects hummed, totally 320 \nqueries. All subjects were inexperienced singer. They were allowed to use a headset microphone and start humming any part of a song with free key and free tempo. \nEvaluation measure is the most important factor to evaluate \nthe performance of a retrieval method. The performances of \nthe systems are compared using ‘ retrieval accuracy ’.  \n Table 1: Comparisons of the accuracy by various features (%) \nFeatures 1st \nrank Ranked in \nthe top 10Weight values\nSpan + Pitch 73.9 91.1 α=1.0, β=0.6 \nCoarse-to-Fine 78.4 89.6 none \nCategory 27 81.6 91.5 none \nSpan + Pitch(3) + CM 86.5 94.1 α=0.5, β=0.7 \nThe experimental results for music retrieval are shown in \nTable 1. To a performance evaluation here, the Coarse-to-Fine \nand Category 27 method are used (Sonoda,  T. et al. 1998).  \nPitch  (m) refers to the use of m multiple pitch candidates. CM \nstands for confidence measure. By using multiple pitch candidates, especially the accuracy of the 1st rank was improved, and thus the validity could be conformed. Retrieval accuracies are improved by using multiple pitch candidates. When we used three pitch candidates the first rank was 86.5%.  \nEven if we compared Category 27 and the proposed method \n(Span + Pitch (3) + CM), the retrieval accuracy is improved from 81.6% to 86.5% at the 1st rank. \n6 Conclusion \nIn this paper, we implemented a practical query-by-humming \nsystem, which can find a piece of music in the database based on a few hummed notes. Moreover, the retrieval engine was extended by the three dimensional continuous dynamic programming so that multiple pitch candidates could be treated. Actually, the influence by individual character, such as a difference in key and a difference in tempo, an error like \ninsertion or omission of notes, etc. are contained in a user's \nhumming. Considering these problems, we represented the notes by taking relative ratio against the optimum neighbor note dynamically determined according to the DP paths.  \nFurthermore, even if the hummed queries are perfect, it is still \ndifficult to retrieve the pitch perfectly from the hummed \nqueries. To consider the pitch extraction errors, we proposed \nto use multiple pitch candidates. Using the proposed method, 99.7% of pitch extraction accuracy was shown within the third rank. Moreover the similarity measuring algorithm extended the search space of DP plane into three dimensions for robust matching against pitch extraction errors in the query processing. \nReferences \nHeo, Sung-Phil et al.  (2003). Multiple Pitch Candidate based \nMusic Information Retrieval Method for Query-by-Humming. In International Workshop on AMR2003 . \nKosugi,  N. et al.  (2000). A Practical Query-By-Humming \nSystem for a Large Music Database. ACM Multimedia 2000 , \n(pp. 333-342). \nPauws. S. (2002). CubyHum: A Fully Operational Query by \nHumming System. In Proc. 3rd ISMIR2002 . \nPaulus, J. et al.  (2002). Measuring the Similarity of Rhythmic \nPatterns. In Proc. 3rd ISMIR2002 . \nSonoda,  T. et al. (1998). WWW-based Music Retrieval System. \nIn Proc. ICMC’98  (pp. 343-352)."
    },
    {
        "title": "Discovering musical pattern through perceptual heuristics.",
        "author": [
            "Olivier Lartillot"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417285",
        "url": "https://doi.org/10.5281/zenodo.1417285",
        "ee": "https://zenodo.org/records/1417285/files/Lartillot03.pdf",
        "abstract": "This paper defends the view that the intricate difficulties challenging the emerging domain of Musical Pattern Discovery, which is dedicated to the automation of motivic analysis, will be overcome only through a thorough taking into account of the specificity of music as a perceptive object. Actual musical patterns, although constantly transformed, are nevertheless perceived by the listener as musical identities. Such dynamical properties of human perception, not reducible to geometrical models, will only be explained with the notions of contexts and expectations. This paper sketches the general principles of a new approach that attempts to build such a general perceptual system. On a sub-cognitive level, patterns are discovered through the detection, by an associative memory, of local similarities. On a cognitive level, patterns are managed by a general logical framework that avoids irrelevant inferences and combinatorial explosion. In this way, actual musical patterns that convey musical significance are discovered. This approach, offering promising results, is a first step toward a complete system of automated music analysis and an explicit modeling of basic mechanisms for music understanding. 1",
        "zenodo_id": 1417285,
        "dblp_key": "conf/ismir/Lartillot03",
        "keywords": [
            "Musical Pattern Discovery",
            "Automation of motivic analysis",
            "Perception of music",
            "Dynamical properties",
            "Contexts and expectations",
            "Perceptual system",
            "Local similarities",
            "General logical framework",
            "Music understanding",
            "Automated music analysis"
        ],
        "content": "Discovering Musical Patterns through Perceptive HeuristicsOlivier LartillotIRCAM -/nobreakspaceCentre Pompidou4, Place Igor-Stravinsky,75004 Paris, FranceOlivier.Lartillot@ircam.frAbstractThis paper defends the view that the intricatedifficulties challenging the emerging domain ofMusical Pattern Discovery, which is dedicated to theautomation of motivic analysis, will be overcomeonly through a thorough taking into account of thespecificity of music as a perceptive object. Actualmusical patterns, although constantly transformed,are nevertheless perceived by the listener as musicalidentities. Such dynamical properties of humanperception, not reducible to geometrical models, willonly be explained with the notions of contexts andexpectations. This paper sketches the generalprinciples of a new approach that attempts to buildsuch a general perceptual system. On a sub-cognitivelevel, patterns are discovered through the detection,by an associative memory, of local similarities. On acognitive level, patterns are managed by a generallogical framework that avoids irrelevant inferencesand combinatorial explosion. In this way, actualmusical patterns that convey musical significance arediscovered. This approach, offering promisingresults, is a first step toward a complete system ofautomated music analysis and an explicit modeling ofbasic mechanisms for music understanding.1 IntroductionMusical Pattern Discovery (MPD) is an emerging discipline,which aims at offering automated motivic analyses of musicalscores. Nowadays, however, no computer system is able tocomplete in a relevant way the demanding task of MPD. Inthis paper, we suggest a new approach that may offer ananswer to fundamental problems arisen in this discipline, andthat could therefore solve a certain number of difficulties.We introduce a general methodology whose intuition stemsfrom a mimicking of human perception. We will defend sucha position through a critical overview of current approaches inMPD. We suggest a characterization of musical pattern thattakes into account the fundamental notions of repetition,sequence and similarity in the musical context of temporalperception. Following Hofstadter’s propositions (1995), wepropose to discriminate between sub-cognitive and cognitivemechanisms. We sketch the general principles of acomputational model that attempts to pursue suchmethodology. The model has been implemented and offersfirst promising results.2 Pattern Discovery in Music Analysis2.1 The Dimensions of Music AnalysisMusic may be considered as a particularly complex“language” that conveys information or meaning alongdifferent dimensions.Melody is one of its most salient aspects, since it istraditionally ruled by mnemonic constraints that helprecognition and learning. Accompaniment too features salient“syntactic” or motivic aspects. Motivic analysis may thereforefocus on all these aspects.Rhythm is another essential aspect of music, that may beconsidered separately, but that also play an important part inmelodies and motives.Metric is a global vision of temporal structures that is usuallydependent on stylistic knowledge.Moreover, music is fundamentally ruled by the “grammatical”rules of harmony. Such rules participate more or less in thecomprehension of music by average listeners, but only in anrather implicit way. This higher-level symbolic discourse,which can be objectified by educated listeners, also offerssalient aspects, such as cadenza. Its analysis may thereforeshare some similar aspects with motivic analysis, such as theinductive methods of patterns discovery and recognition.Finally, the construction of the musical work ends up in aglobal formal structure, that can be considered in a veryschematic way — such as A-B-A — or more carefully. But asthe formal structure is a simple consequence of the materialconstruction, its detailed description may be simply revealedby a thorough motivic analysis.Our approach will focus on the design of an automated systemfor motivic analysis, without any reference to harmony orstyle.Permission to make digital or hard copies of all or part of this work forpersonal of classroom use is granted without fee provided that copies are notmade or distributed for profit or commercial advantage and that copies bearthis notice and the full citation on the first page.  ” 2003 The Johns HopkinsUniversity.2.2 Music GroupingsThe significant structures discovered by music analysisnaturally consist of sets of notes. Hence analyzing meansgrouping local notes in the score, and relating these groupsone to others and to external concepts. These groupings aregenerally guided by three principal criteria:2.2.1 Style-Based GroupingsMetrical and harmonic descriptions of music may induce aspecific way of segmenting the musical discourse, especiallywith the help of stylistic norms, which may for instanceconstrain pattern length (Lerdahl and Jackendoff, 1983). Asour approach will not rely on harmonic and stylisticknowledge, such heuristics will not be considered.2.2.2 Local BoundariesThe musical surface may be segmented by localdiscontinuities, in particular between notes that areparticularly distant in time or in pitch dimensions (Lerdahl andJackendoff, 1983), or when the melodic contour changes sign,i.e. when melody first grows and then decreases, for instance.Discontinuities along other parameters1 such as dynamics,accents or instrumentation may contribute to these localsegmentations (Cambouropoulos, 1998).2.2.3 RepetitionFinally, a musical motive is traditionally defined as a set ofnotes that is repeated several times throughout the score(Nattiez, 1990). Such criterion conflicts with the previous one.Indeed, a motive may feature contrastive steps. In Figure 1,the first leap between the first two notes, although segmentingthe local discourse, is the important element that characterizesthe beginning of the motive itself. Instead of unifying what isconstant and alike, a motive rather unifies what is contrastiveand different, or more precisely: what is similarly contrastivein different places of the work.\nFigure 1: A motive (solid lines) may feature contrastive stepsand therefore contradicts local segmentations (dashed lines).Our approach will uniquely focus on repetition, since theconcept of repeated motives seems more developed intraditional music theories than the notion of local boundaries.Nevertheless, the process of repetition discovery itself is ruledby local constraints too. Indeed, repeated patterns are detectedonly when their successive notes are sufficiently close, andwhen their borders contrasts sufficiently with the outerenvironment, in particular because of temporal or pitchdiscontinuities. For instance, a pattern that is strictly includedinside an ascending movement (first dashed-lined grouping inFigure 2) cannot be related to its exact repetition (seconddashed-lined grouping) since it has not been pre-segmented bylocal boundaries first.                                                            1 In a first approach, these parameters will not be considered.\nFigure 2: Exactly or approximately repeated pre-segmentedpatterns (solid lines) are perceived, whereas exactly repeatedbut non-pre-segmented ones (dashed lines) are not perceived.2.3 Towards Conceptual Pattern DiscoveryThe previous illustrated discussion implies that repetition ofsuccessions — that may be discovered through stringalgorithms (Smith and Medina, 2001) —, although the corecharacteristic of musical motives, is not a sufficient conditionfor their exhaustive determination. Other aspects contribute tothis idea. For instance, a series of successive repetitions of asingle elementary pattern features numerous geometricalrepetitions that are not relevant.\nFigure 3: Irrelevant geometrical patterns induced by thesuccessive repetition of an elementary pattern.Cambouropoulos (1998) proposes to solve such paradox byselecting classes of patterns where overlapping is minimal.This means that all possible patterns are computed first, andthat certain classes of patterns are then selected according toglobal factors, with no respect to each particular case. Such astatistical approach, which may offer satisfying results forsimple examples, cannot be applied to actual score whereclasses of patterns may be relevant at some places and not atothers.This difficulty may be solved by considering the fact thatpatterns are conceptually inferred during the incrementallistening of the piece. For each new note, the set of inferencescurrently in process constitutes a context, which inducesconstraints upon the candidates for new inferences. In thisway, irrelevant inferences described in previous examples maybe avoided.2.4 Approximate RepetitionsMusical motives are commonly subject to numerous kinds oftransformations, such as transposition (pitch translation), timestretching, distortions due to scale constraints, and thematicdevelopments.2.4.1 Related WorksOne popular approach for detecting patterns repetition isfounded on self-similarity matrix (Dannenberg, 2002). Musicis decomposed into local segments s = 1, …, N; similaritydistances are measured between all possible pairs (i, j) of localsegments and stored in a matrix at position (i, j). Then patternsmay be discovered along lines of the matrix that are(approximately) parallel to the first diagonal. Such approach,by implicitly assuming that one necessary condition forpattern similarity is the similarity of their elementaryconstituents at each respective position, cannot identifytransformed patterns, even simply transposed ones. Actually,what characterizes a pattern is less its intrinsic compositionthan the formal properties that are shared by its differentrepetitions. One particular property is the local relationshipsbetween notes, which is not actually taken into account inmethods based on self-similarity matrix.In order to handle such music plasticity, multiple parallel“viewpoints” (Conklin and Anagnostopoulou,, 2001) or “levelof abstraction” (Dannenberg, 2002) of the pattern areconsidered. For instance, transposed patterns may beidentified by expressing pitch parameters relatively to areference point associated to each pattern, such as theirrespective first note. A simpler heuristics consists ofpreferring pitch intervals between successive notes to absolutepitches. However, such methods cannot handle local distortioninside patterns. In this case, more relative viewpoints maythen be considered. One commonly used description is thecontour, which describes the direction of interval betweensuccessive notes (downward, upward, or constant). Thetrouble is, contour-based repeated pattern discoveryalgorithms easily produce irrelevant results, since lots of falsepositive patterns may be discovered.2.4.2 Memories and RepresentationsDowling and Harwood (1986) have suggested that contour isthe minimal degree of similarity between patterns, but alsothat contour description is relevant for short-term memory(STM ) retrieval whereas pitch interval description is necessaryfor long-term memory (LTM). Indeed, the very small scale ofSTM and its very active presence in our mind makes it easy torelate its content to present perception in very loose terms,such as contour representation. On the contrary, due to itslarge size, LTM should be searched through precise queries,such as pitch interval representations.That is why it may be interesting to divide the patterndiscovery process into two subtasks, one for STM and one forLTM. This paper will only focus on the LTM part. We suggestin paragraph 3 an improvement of Dowling’s suggestion. Itmay be interesting to limit his constraint on the queryexactness to only a prefix of it. Indeed, if a prefix of the queryis sufficient for the retrieval of one particular context in LTM,the remaining of the retrieved pattern may simply becompared to the remaining of the query along contourrepresentation.Other factors may influence pattern discovery. For instance,patterns that are highly expected, for instance after having justbeen successively repeated several times before, as will beshown in paragraph 4.6, can be detected even when highlydistorted. Moreover, patterns featuring a particular structuremay be retrieved each time such structure is discovered again.All this supports the claim that pattern discovery needsperceptual considerations, related in particular to time andmemory (Lartillot and Saint-James, 2003).3 Subcognitive Mechanisms of PatternDiscovery3.1 Local contexts3.1.1 Associative MemoryIf a pattern induction algorithm has to mimic humancapabilities, their basic principles need first to be described. Inparticular, we need to understand how a human listener, oncebeginning to hear the second occurrence of a pattern, is able tosuddenly remember that what is being played has alreadyappeared previously — exactly or similarly —, even whensuch pattern was not explicitly defined before.Such cognitive capabilities seem to rely on the generalcharacteristics of associative memory, where knowledge isindexed by their content instead of being referenced accordingto any arbitrarily defined memory address.As pattern may be recalled even before being explicitlydiscovered, there exists a reproductive memory that associateslocal succession of notes that are similar one to the other.Patterns may be defined as successions of local similarities.3.1.2 Interval DistancesThese local similarities are progressively built from singleintervals. Distances are computed first between singleintervals, then between succession of intervals, or patterns.Let n1, n2, n’1 and n’2 be four notes whose respective pitchesare p1, p2, p’1 and p’2 and respective time onset o1, o2, o’1 ando’2.We propose, in a first approach, to formalize theperceptual distance between the two intervals (n1, n2) and (n’1,n’2) as a weighted product of a pitch interval distance and aninter-onset distance:D2((n1, n2), (n’1, n’2)) = (abs [(p2– p1) – (p’2 – p’1)] + 1)* (max [(o2– o1) / (o’2 – o’1), (o’2 – o’1) / (o2– o1)])0.7We may remark that in music, pitch intervals are subtracted,whereas inter-onsets are divided.3.2 Initiation Phase: Discovering similar contextsEach successive local interval has to be memorized in anassociative memory that is able to retrieve any interval similarto a query. For this purpose, a hash-table associates for eachpitch interval value the set of its associated occurrences in thepart of the score that has already been analyzed. For anycurrent local interval (n1, n2) of pitch intervals (p2– p1), anysimilar old local interval (n’1, n’2) belongs to those stored inthe hash-table at a hash value (p’2 – p’1) equal or similar to(p2– p1), that is:abs [(p’2 – p’1)— (p2 – p1)]  < dpSimilar hash-tables could be added for other intervalparameters such as inter-onset values, in order to retrieve inparticular similar rhythmic patterns.For each similar old interval (n’1, n’2), its previous interval(n’0, n’1) is now considered, and compared to the previousinterval (n0, n1) of (n1, n2). If (n’0, n’1) and (n0, n1) are alsosimilar, then (n0, n1, n2) and (n’0, n’1, n’2) are considered assimilar patterns.\nFigure 4: Similarity between intervals is found through thehash-table of local intervals (up). When two intervals (n1, n2)and (n’1, n’2) are similar and preceding intervals (n0, n1) and(n’0, n’1) are similar too, then a pattern is inferred (down).Moreover, when (n1, n2) and (n’1, n’2) are first compared, ifadditional facts contribute to their equivalence — for instancep1 = p’1 —, then the inference of the repetition of patterns (n1,n2) and (n’1, n’2) may be immediately inferred withoutconsidering their previous patterns. If on the contrarysimilarity between (n0, n1, n2) and (n’0, n’1, n’2) is not highenough, maybe previous intervals can be considered onceagain in a recursive way.3.3 Extension Phase: Prolonging contextsOnce the similarity between the beginnings of two patternshave been detected, the inference of the similarity between thecontinuations of each pattern is far easier to compute, sincethere is no need to discover new contexts any more. Therefore,continuations can be compared only along their contourparameters.\nFigure 5: Once a pattern is inferred, respective followingintervals can be easily compared along their contour.3.4 Local Pre-SegmentationAs explained in paragraph 2.2.3, repeated pattern can bediscovered only when they sufficiently contrast to theirsurrounding at their borders. Their should be a discontinuity atthe beginning of current candidate pattern (n0, n1, … ) and atthe beginning (or at the ending) of past candidate pattern (n’0,n’1, … ). We propose to sketch roughly such idea as follows:there is a discontinuity between the note n— preceding thepattern (n0, n1, … ) and the pattern itself when either:- the contour changes between intervals (n—, n0) and(n0, n1), from up to down, from constant to up, etc.- there is a significant temporal gap before the pattern:(o0 — o—) / (o1 — o0) > go- there is a significant pitch interval before the pattern:(p0 — p—) / (p1 — p0) > gpIn current implementation, go and gp are fixed to 2, but thevalue should perhaps be lower and dependent on context.4 Logical Rules of Pattern Processing4.1 Incremental Conceptual Building on the ScoreThe discovery of single patterns, fundamental mechanisms ofwhich have been proposed in previous section, though oneessential characteristic of musical pattern perception, does notsuffice to take into account human capabilities. Logicalprinciples have to be added in order to make sure that thecognitive system infer relevant knowledge and do not facecombinatorial explosion.As we need to design a system that incrementally scans thescore and that progressively produces inferences that shoulddepend on the score and on what has been already inferred, itmay be convenient to construct these inferences directly onthe score.The score is progressively scanned from the beginning to theend, each new note triggers the calling of a main routine thatcarries out a determinate set of analysis operations.4.2 Pattern Classes and Pattern OccurrencesWhen a pattern is repeated several times, each occurrenceshould not be related to each other, since this would lead to anundesired complex relationship network. What has to be takeninto account here is the concept of pattern classes (PC) andpattern occurrences (PO), also called “clustering”(Dannenberg, 2002). Multiple repetitions of a pattern shouldtherefore simply be considered as occurrences of a single PC.A pattern is an approximately repeated succession of notes.Therefore a PO is a set of notes in the score, whose successionis similar to the succession defined in the PC characteristics.Such property may be formalized through graph theory.We propose to model a pattern as a chain of states, wheresuccessive states correspond to successive notes of the pattern,as in Figure 6. In this way, a PC is a chain of states, calledpattern class chain (PCC), where each state represents theshared characteristic of the associated note, and where eachtransition between two successive states represents an intervalbetween two successive notes.Each PO is also a chain of states, named pattern occurrencechain (POC), where each state interfaces a particular note inthe score with its corresponding state in the associated PC.n0n2n1n’0n’2n’1Figure 6: The two POCs (black circles) interface notes in thescore with the corresponding states in the PCC (white circles).4.3 Discovering Further OccurrencesOnce a PC has been discovered, its further POs should all besubsumed under the same PC. The distinction between contextdiscovery and context prolonging still prevails.During initiation phase, when a similarity has been discoveredbetween two different contexts, and before deciding to createany new PC, we have to make sure that such context does notalready exist in the beginning of one of the set of alldiscovered PCs. If there does exist such pattern class, a newPO simply associates the new discovered context with theretrieved PC.The extension phase of PO discovery significantly differsfrom PC discovery. Since the beginning of currentlydiscovered PO is already associated to a PC, each of itssuccessive candidate continuations may simply be comparedto the successive continuations along the PC. In this case,current PO does not need to be compared to old occurrences.4.4 Avoiding RedundancyWe have to constantly check that each new pattern occurrencediscovery was not already induced through a previousmechanism that led to the same result. The incremental andlogical thinking that builds human perception of music is ruledby fundamental principles, which are necessary for a coherentprocess. For example, every time a sequence is considered asan occurrence of a pattern, every suffix could itself beconsidered as occurrence of other pattern class, for simplemathematical reasons. But cognitively speaking, suchinferences are irrelevant, since they do not correspond toinference human makes when listening to music. This is dueto the fact that the first longest pattern was sufficient toexplain the phenomenon, and that further inferences ofsuffixes would only infringe a clear analysis of the score. Thatis why suffix of pattern or subsequences of sequences shouldnot be explicitly represented. (Dannenberg, 2002; Pienimäki,2002). Our formalization through graph theory enables toeasily implement such rules.\nFigure 7: Suffixes (bottom POCs and PCC) cannot beconsidered as new PCs, since the associated notes of each ofits POCs already belongs to the POCs of the whole pattern(top POCs and PCC).4.5 Parallel Memory vs. Sequential ComputerThe whole configuration of relationships between notes, PCs,and POs, forms a particularly complex and interdependentsystem. Such vision is largely influenced by Holland et al.’s(1989) cognitive model of induction. All the operationsundertaken during the analysis are in fact triggered orinhibited by the configuration of the network and the priorrules. The trouble is, the implementation of such a parallelsystem has to cope with the sequential architecture ofcomputers. For this purpose, the operations has to be veryprecisely ordered, else numerous artifacts may produceirrelevant results and combinatorial explosions.For each new note considered by the main routine, eachpossible PO concluded by previous note is a candidate forthree successive operations:4.5.1 Pattern Occurrence ExtensionCurrent new note may be associated to one possiblecontinuation of the PO, such that this continuation is alreadydescribed in the PC. Candidates are considered in a decreasingorder of similarity. Minor candidates that are considered asnegligible compared to the most salient ones are discarded. Inthis way, current note may induce poorly perceptible patternsonly when it cannot be related to particularly salient patterns.4.5.2 Pattern Class ExtensionIf previous condition does not occur, the eventual extension ofthe PO with current note into a new PC is checked. Thisextension should not be already inferred by currentconfiguration. As previously, negligible candidates arediscarded.4.5.3 Pattern Class InitiationFinally, in any case, new pattern initiations are attempted. Asfor pattern extension, these new patterns should not be alreadydeduced by the previously inferred POs. In particular, asshown in paragraph 4.4, simple suffixes of patterns cannot beconsidered as new patterns.If the past candidate (n’0, n’1, n’2) already initiate another PC,then current candidate (n0, n1, n2) is simply associated to a newPO of this PC.Finally, for all three trials, it is also necessary to make surethat every PC or PO in process of creation does not alreadyexist. The inference of duplicate concepts may indeed provokedisastrous effects such as combinatorial explosions.4.6 Pattern AssociationMusic features multiple levels of pattern descriptions. Notesmay indeed belong to several possible patterns in parallel.When discovering another PO of a PC that was associated toanother PC, we may expect to retrieve the same associationbetween these PCs. Therefore, pattern association may inducepattern expectation.For this purpose, the association between patterns may bedirectly represented on the PCs themselves. Every time a noteof a PO is associated to another PO, on the corresponding statein each PC is associated a new pattern associated to the otherPC. Such pattern association discovery induces a patternassociation expectation rule, stating that every time a new POof such PC is discovered, possible associated PCs are alsoexpected. Such expectation may be formalized through theinitiation of new hypothetical PO where only the first state isrepresented, waiting for further extension.For instance, a pattern may include another sub-pattern. InFigure 8, the 8-note PO features a repetition of two 3-notePOs. Therefore, on the 8-note PC itself is added two 3-notePOs.\nFigure 8: The 3-note POCs (up) that are included inside the 8-note POC are represented directly on the 8-note PCC (down).In this way, during the discovery of a new 8-note POC,corresponding 3-note POCs are also expected (right).4.7 Successive RepetitionsMusic usually features successive repetitions of a same PC. Asexplained in paragraph 2.3, if no new mechanisms wereadded, the system would consider each possible concatenationof these successive patterns as a new pattern. Theseinferences, not corresponding to human judgments and leadingto combinatory explosion, should be forbidden.It should be remarked that such pattern repetition is a specialcase of pattern association. If each pattern is extended with thefirst note of the succeeding pattern, then this last note of suchextended pattern may be associated to the first note of thesame PC. This means that, in the extended PC, the last state islinked to the first state. The idea of pattern cycling is thereforeexplicitly represented.The first note of each new PO, as soon as it appears, isimmediately associated to a new POC. An additionalmechanism prevents any pattern, whose first note is also thelast note of another occurrence of the same pattern, to beextended any further.\nFigure 9: The last note of the 9-note pattern is linked to itsfirst one.4.8 Meta-Pattern of PatternsThe succession of POs may be examined in exactly the sameway that the previously considered succession of simplesnotes, provided that these POs belong to the same PC. For thispurpose, a concept of interval between two successive POs,that generalizes the traditional notion of interval between twosuccessive notes, is defined. The parameters of an intervalbetween notes were pitch intervals and inter-onset. In a similarway, we define the parameters of an interval between POs as:- the pitch intervals and inter-onsets between therespective last notes of each PO. In this way, theinterval between notes of previous positions in eachPO can be obtained by considering the intervalbetween POs prefixes.- the difference between pitch intervals between therespective last two notes of each PO. Idem for inter-onsets. Such parameter, that may be considered as ameta-interval between intervals, represent theevolution of the interval value between the two POs.\nFigure 10: In a generalized interval between patternoccurrences, notes of same rank are compared.As intervals between notes were stored in a hash-table,intervals between POs are stored in a new hash-tablededicated to the associated PC. Each time a new PO is inferred(be it the initiation of a new pattern or the extension of aprevious one), the meta-interval between the previous PO ofsame PC and current PO is stored in the PC hash-table.In this way, the meta-pattern discovery process is astraightforward generalization of pattern discovery. Instead ofwriting new functions dedicated to meta-pattern discovery, wepropose to directly consider simple note as a degenerate caseof PO, and the concept of note as a special PC. Hence themain routine that is called for each new note is alsorecursively called by sub-routines of the routine itself, eachtime a new PO is inferred.When two intervals between POs of same PC are similar, ameta-PC is initiated, with the two corresponding meta-POs.\nFigure 11: Initiation of a new meta-PC.The main difficulty of the meta-generalization is that theelements of a meta-pattern may themselves be extended. Ifcurrent PO is extended, and if previous PO of current meta-POand previous meta-PO(s) can also be extended, then thesemeta-POs will contain these extended POs instead of theinitial POs.\nFigure 12: Extension of the element of the meta-PC.Meta-POs may extend their POs even when intervals betweenextended POs are no more similar. For instance in Figure 12,although the second note of successive POs (E-F-F-E) do notform any pattern (or maybe a mirrored one), the initial 1-notePOs are extended to 2-note POs, since the overall successionof these POs are actually perceived as meta-patterns.On the contrary, intervals between extended POs maysometimes contribute to the initial meta-pattern. For instance,the third note of POs (G-G-G-G) also form a same pattern (G-G, G-G).Exact repetition of POs may be considered as generalized“unison”. Moreover, this meta generalization may be operatedrecursively and meta-pattern of meta-pattern may beconsidered too!5 Current Results5.1 ImplementationThese algorithms have been implemented in Common Lisplanguage, as a library of OpenMusic (Assayag et al., 1999), agraphical programming language dedicated to the computationof symbolic representations of music. In current version of thislibrary, called OMkanthus, these results are displayed as a listof texts that is not easy to understand. That is why this libraryis provided with some basic tools for selecting and displayinglongest patterns, most frequent patterns, or most significantpatterns, where pertinence is a product of length andfrequency.5.2 ResultsThe analysis of Bach’s Prelude in C, BWV 846, generates allthe occurrence of the 8-note pattern. The whole Prelude(except the last two bars) is represented as a perpetualrepetition of a meta-pattern of exact repetition of two 8-notepattern occurrences. Meta-pattern of meta-pattern are alsodiscovered, such as transpositions of sequences of two andfour bars at several different places.\nFigure 13: Each bar of the Bach’s Prelude is a meta-pattern ofan exact repetition of two 8-note patterns.\nFigure 14: Two transposed occurrences of a meta-patterns oftwo meta-patterns of 8-note patterns.\nFigure 15: Two transposed occurrences of a meta-patterns offour meta-patterns of 8-note patterns.The last bars of the Prelude feature the first two notes of theconstantly expected 8-note patterns, but with a newcontinuation that constitutes a new pattern class.\nFigure 16: The last bars feature two occurrences of a newpattern, which is developed from the first two notes of 8-notepatterns.Irrelevant patterns may sometimes be found too. This modelshould be considered more as a very experimental prototypethat attempts to simulate some aspects of pattern perception,than as a complete and robust automated music analyzer.6 Future Works6.1 Integrating Contour RepresentationAs explained previously, the contour representation is highlyrelevant for short-term memory retrieval. A specific patterndiscovery module dedicated to contour representation wouldenable to easily represent local relationships between similargestures.6.2 Large-Scale IntervalsEach successive note of the currently heard repetition istentatively related with a possible successive note of thepreviously heard occurrence. In a pattern, each note may bedisposed relatively to the position of its preceding note, or alsorelatively to the position of the first note of the pattern, andsometimes even relatively to the position of another particularprevious note of the pattern. In Figure 17, the similaritybetween the third note of each pattern is explained by itsrelative position with respect to the first note, and not thesecond note, since the two intervals between the second andthe third note of each pattern are particularly different,whereas the two intervals between the first and the third noteof each pattern are less dissimilar. On the contrary, thesimilarity between the fourth and fifth notes of each patternmay simply be explained by their position relatively to theirpreceding notes.\nFigure 17: Local referential relationships between patternnotes.That is why the position of each note in a pattern may beconsidered relatively to each possible previous note in thepattern, in order to find the minimum dissimilarity.6.3 Towards PolyphonyThere may be, inside patterns, “enclaves” of foreign notes notreally belonging to these patterns. Patterns may also featurestransitory states, such as passing notes or appoggiatura.More generally, patterns may be included inside a polyphonicflow. If all this flow is represented like a single totally orderedsequence, patterns representations, here also, feature enclaves.Such problem has already been tackled by Meredith,Lemström and Wiggins (2002) but for exact repetition.Chords should also be taken into consideration and patterns ofchords should be discovered.7 ConclusionIn this paper, we have proposed a new approach of musicalpattern discovery based on a modeling of cognitivemechanisms of music perception. This strategy leads topromising results. The discovered structures correspond tobasic patterns effectively perceived by human listeners. Withsome improvements, this algorithm should also be able todetect more subtle patterns that are less easily discriminatedby human listeners, but that participate to the complex flow ofimplicit reasoning.Then an interface has to be designed, enabling a browsinginside the score and the discovered structures. In a long term,such approach may go beyond pattern and catch higher-levelconcepts. A project of automatic music theory discovery mayalso be envisaged.AcknowledgementsMy doctorate project is supervised by Emmanuel Saint-James(LIP6, Paris 6) and Gérard Assayag (Musical RepresentationTeam, Ircam). Lots of ideas arise from very stimulatingdiscussion with my colleague Benoit Meudic. Up-to-dateinformation about this project is available at followingwebpage address : www.ircam.fr/equipes/repmus/lartillotReferencesAssayag, G., Rueda, C., Laurson, M., Agon C. & Delerue, O.(1999). Computer Assisted Composition at Ircam : PatchWorkand OpenMusic. Computer Music Journal, 23(3), 59-72.Cambouropoulos, E. (1998). A Generative Theory of TonalMusic. Ph.D. dissertation, Edinburgh Univ.Conklin, D. & Anagnostopoulou, C. (2001). Representationand Discovery of Mutiple Viewpoint Patterns. In Proceedingsof the International Computer Music Conference,(pp./nobreakspace479-485). San-Francisco: ICMA.Dannenberg, R. (2002). Listening to “Naima”: An AutomatedStructural Analysis of Music from Recorded Audio. InProceedings of the International Computer Music Conference,(pp./nobreakspace28-34). San-Francisco: ICMA.Dowling, W. J. & Harwood, D. L. (1986). Music Cognition.Orlando, FL: Academic Press.Hofstadter, D. (1995). Fluid Concepts and CreativeAnalogies: Computer Models of the Fundamental Mechanismsof Thought. New-York: Basic Books.Holland, J. H., Holyoak, K. J., Nisbett, R. E. & Thagard, P. R.(1989). Induction: Processes of Inference, Learning, andDiscovery, Cambridge, MA: MIT Press.Lartillot, O. & Saint-James, E. (2003). Automating MotivicAnalysis through the Application of Perceptual Rules.Computing in Musicology, 13, to appear.Lerdahl, F. & Jackendoff, R. (1983). A Generative Theory ofTonal Music. Cambridge, MA: MIT Press.Meredith, D., Lemstrom, K. & Wiggins, G. A. (2002).Algorithms for discovering repeated patterns inmultidimensional representations of polyphonic music.Journal of New Music Research, (31)4, 321-345.Nattiez. J.-J. (1990). Music and Discourse: Towards aSemiology of Music. Princeton, NJ: Princeton University.Pienimäki, A. (2002). Indexing Music Databases UsingAutomatic Extraction of Frequent Phrases. In Proceedings ofthe International Conference on Music Information Retrieval,(pp./nobreakspace25-30). Paris: Ircam.Smith, L. & Medina, R. (2001). Discovering Themes by ExactPattern Matching. In Proceedings of the InternationalConference on Music Information Retrieval, Bloomington, IN."
    },
    {
        "title": "The C-BRAHMS project.",
        "author": [
            "Kjell Lemström",
            "Veli Mäkinen",
            "Anna Pienimäki",
            "Mika Turkia",
            "Esko Ukkonen"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417551",
        "url": "https://doi.org/10.5281/zenodo.1417551",
        "ee": "https://zenodo.org/records/1417551/files/LemstromMPTU03.pdf",
        "abstract": "The C-BRAHMS project develops computational",
        "zenodo_id": 1417551,
        "dblp_key": "conf/ismir/LemstromMPTU03",
        "keywords": [
            "C-BRAHMS project",
            "Content-based music retrieval (CBMR)",
            "Query-by-humming",
            "Polyphonic music",
            "String matching algorithms",
            "Geometric methods",
            "Music data analysis",
            "MIDI files",
            "Transposition invariance",
            "Music search engine"
        ],
        "content": "The C-BRAHMS Project∗\nKjell Lemstr ¨om, Veli M ¨akinen, Anna Pienim ¨aki, Mika Turkia, Esko Ukkonen\nDepartment of Computer Science, University of Helsinki\nPO Box 26 (Teollisuuskatu 23)\nFIN-00014 University of Helsinki, Finland\n{klemstro,vmakinen,apimmone,turkia,ukkonen }@cs.Helsinki.FI\nAbstract\nThe C-BRAHMS project develops computational\nmethods for content-based retrieval and analysis of\nmusicdata. Asummaryoftherecentalgorithmicand\nexperimental developments of the project is given.\nThe search engine developed by the project is avail-\nable athttp://www.cs.helsinki.ﬁ/group/cbrahms .\n1 Introduction\nContent-Based Music Retrieval, or CBMR for short, is a re-\nsearch topic studied rather extensively during the last half\ndecade. One of its famous instances is the so-called “query by\nhumming” or WYHIWYG (What You Hum Is What You Get)\napplication. Given a large database of music called the source,\nthetaskistoﬁndexcerptsinthedatabasethatresemblethemost\n(in a musical way) the hummed query pattern .\nThis paper introduces our CBMR project called C-BRAHMS\n(Content-Based Retrieval and Analysis of Harmony and other\nMusic Structures) and its output, the C-BRAHMS engine. The\nproject aims at designing and developing efﬁcient methods for\ncomputationalproblemsarisingfrommusiccomparison,analy-\nsis, data mining and retrieval. Currently the project has a focus\non retrieving polyphonic music in large scale music databases\nof symbolically encoded music.\nThe C-BRAHMS project was formally established in January,\n2002. C-BRAHMS is part of the From Data to Knowledge\n(FDK) research unit hosted by the Department of Computer\nScience at University of Helsinki. FDK has been selected as\na centre of excellence funded by the Academy of Finland. The\ngroupcollaborateswithseveralresearchersandresearchgroups\nabroad.\n2 Music retrieval algorithms\nSymbolic music data can be seen as strings of symbols, and\nstring-matching-based methods have been applied to CBMR\n∗Supported by the Academy of Finland (grant 201560).\nPermission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided that\ncopies are not made or distributed for proﬁt or commercial advan-\ntage and that copies bear this notice and the full citation on the ﬁrst\npage.c/circlecopyrt2003 Johns Hopkins University.problems. These methods are designed for handling one-\ndimensional data, and hence they do not apply on polyphonic\nmusic without modiﬁcations.\nTheC-BRAHMSprojecthasdevelopedsomegeneralizedstring\nmatching algorithms that can deal with polyphonic music. For\ninstance the algorithm by Lemstr ¨om & Tarhio (2003) uses bit-\nparallelism and precomputed ofﬂine data structure containing\npitch interval classes for each chord. This structure is then\nscanned to ﬁlter out match candidates, which are checked with\nanother, slower, algorithm.\nOur other string-matching-based algorithms allow efﬁcient\ntransposition-invariant approximate searching (M ¨akinen,\nNavarro & Ukkonen, 2003; Lemstr ¨om & Navarro, 2003); here\napproximation means insertions and deletions of notes but not\nsmall variations of pitch levels. The efﬁciency is achieved by\nusingsparsedynamicprogrammingandbit-paralleltechniques.\nSimilar techniques are used for ﬁnding the minimum splitting\nof a pattern in a multi-track musical work (Lemstr ¨om &\nM¨akinen, 2003).\nThe project has put effort in developing a recent methodology\ninterpreting music data as geometric objects in an Euclidean\nspace (Wiggins, Lemstr ¨om & Meredith, 2002; Ukkonen, Lem-\nstr¨om & M¨akinen, 2003). In geometric representation both the\nquery pattern and the source are represented by objects in a\nmultidimensional space. For instance, in a 2-dimensional time-\npitch space, point objects give the onset time and the pitch,\nwhileline segment objects (the well-known piano-roll repre-\nsentation) give the duration of the associated notes, as well.\nTo include additional note parameters the dimensionality can\nbe increased without the need to modify algorithms. The ap-\nproach is inherently transposition-invariant, and dealing with\nmonophonic and polyphonic music is equally straightforward.\nMoreover, musical decorations, such as ornamentations for in-\nstance, do not deteriorate itsworking.\nOne ofthe basicideas ofthe geometricapproach isto calculate\ndifferencevectorsbetweeneachsourcepointobjectandpattern\npoint object, sort the vectors, and calculate the frequencies of\nall such difference vectors. There is a complete match if the\nfrequencyofsomedifferencevectorequalsthenumberofpoint\nobjects in the pattern. Then the onset times must match, which\nmeansthatrhythmicinformationistakenintoaccountinthege-\nometric approach; note that the string matching approach usu-\nally loses this because only the relative order of notes is taken\ninto account. A more efﬁcient version sorts the difference vec-\ntorsonlinebyusingahashtable(Wiggins,Lemstr ¨om&Mered-ith, 2002), and a subsequent improvement uses a pointer array\nand a priority queue (Ukkonen,Lemstr ¨om & M¨akinen, 2003).\nSmall deviations of note onset times inherent in MIDI data\ngenerated by playing with a MIDI keyboard confuse the basic\ngeometric approach described above. This problem is solved\nby another algorithm working on line segment objects, which\nﬁnds the maximal overlap (common duration) between the line\nsegments of the pattern and the data (Ukkonen, Lemstr ¨om &\nM¨akinen, 2003).\n3 C-BRAHMS engine\nWehaveimplementedaCBMRenginewithaWWWinterface1\nfor testing and illustrating the algorithms that have been and\nwillbedevelopedintheproject. Theengineisdividedintotwo\nseparate parts, i.e., a public demo engine and a private search\nengine. The developed algorithms are straightforwardly em-\nbedded in the private engine and, thus, the system serves as a\nvaluable tool and testbed for comparing various algorithms as\nregards their performances and their results to similar queries\non a given database.\nTo the public demo engine, new algorithms are added after a\ntestingprocedureconductedwiththeprivateengine. Thepublic\ndemoenginecontainsmostofthealgorithmsmentionedabove.\nWe provide text-based and WWW based query interfaces. The\nWWWinterfaceincludesapianokeyboardforrecording,play-\ning back and reﬁning the query pattern before executing the\nquery. Italsoallows forchoosingthe queryalgorithm,and tun-\ningtheerrorthresholdandthenumberofqueryresultsshownto\ntheuser. Moreover,theusermayselectwhethertheenginegives\nonly the best match or all matches for one database document.\nThe query results are given in a decreasing order of similarity,\nwhichmaybebasedone.g. theamountoftranspositionandthe\nnumber of errors in an approximate query task.\nThe query results include metadata, such as name of the com-\nposer; title; opus number; date and genre of the music piece,\nand content data, such as score ﬁles in PostScript and PDF for-\nmats; approximate bar number of the match; pitch class names\n(and octaves) of the matching notes; and the amount of trans-\nposition required. The interface allows the matched part or the\nwholemusicpiecetobeplayed,andtoviewvarioushistograms\nof the note data.\nThe algorithms have been implemented as C language exten-\nsions to a distributed server implemented in Ruby language.\nThemixingofinterpretedandcompiledcodeallowsforgreater\nproductivity in implementing parts that are not essential as re-\ngards the performance. In practice, the overhead of using inter-\npreted language is negligible. A reduced number of code lines\nallows for greater ﬂexibilityand better maintainability.\nThe demo engine uses MIDI ﬁles from the Mutopia project2.\nFiles are released either as public domain or under a Mutopi-\naBSD license. Each MIDI ﬁle is converted to a string contain-\ning variable-length chords which, subsequently, contain notes.\nThis string is included in a object representing the music piece,\nand searches are performed separately for each piece. Accord-\ningtoourexperiments,thisprocedureminimizestheamountof\nused working memory in contrast to the approach of merging\n1http://www.cs.helsinki.fi/group/cbrahms\n2http://www.mutopiaproject.orgall data into one large string and performing a single search on\nit(thelatterapproachwouldalsorequireremovingmatchesthat\ngo beyond music piece boundaries). This is an essential issue\nwith algorithms requiring a large amount of space, when work-\ning on large databases and large amounts of concurrent queries\nare allowed.\nAccordingtoourexperiments,theformerarrangementalsoim-\nproves the performance of memory-allocation-intensive algo-\nrithmsoverthelatterapproach. However,algorithmsthatdonot\nneed memory allocation during search, such as our bit-parallel\nalgorithms, suffer minor performance degradation.\n4 Future Directions\nIneveryculture,musicisanimportantpartofhumancommuni-\ncation,andmusicologistshavebeenanalyzingwrittenmusicfor\ncenturies. During the last decades some musical analysis tools\nhave been developed, which might be further formalized to de-\nscribe them as computational problems. Thus, suitable com-\nputer programs could be developed to replace the tedious man-\nual work. Moreover, some work on music psychology about\nwhat makes a musical work pleasing to listen to have been de-\nscribed precisely enough to be applicable in computerized mu-\nsicanalysis. Inthefuture,theprojectwillfurtherattempttouse\nﬁndings in musicology and music psychology to achieve better\ncomputational methods and results. Data mining methods such\nas in Pienim ¨aki (2002) will also be used.\nReferences\nLemstr¨om, K. & M ¨akinen, V. (2003). On ﬁnding minimum\nsplitting of pattern in multi-track string matching. In Proceed-\nings of the 14th Annual Symposium on Combinatorial Pattern\nMatching . Springer-Verlag LNCS 2676, (pp. 237-253).\nLemstr¨om, K. & Navarro, G. (2003). Flexible and efﬁcient\nbit-parallel techniques for transposition invariant approximate\nmatching in music retrieval. To appear in Proceedings of the\n10th International Symposium on String Processing and Infor-\nmation Retrieval .\nLemstr¨om,K.,Tarhio,J.(2003). Transpositioninvariantpattern\nmatching for multi-track strings. To appear in Nordic Journal\nof Computing .\nM¨akinen, V., Navarro, G. & Ukkonen, E. (2003). Algorithms\nfor transposition invariant string matching. In Proceedings of\nthe 20th International Symposium on Theoretical Aspects of\nComputerScience .Springer-VerlagLNCS2607,(pp.191-202).\nPienim¨aki, A. (2002). Indexing music databases using au-\ntomatic extraction of frequent phrases. In Proceedings of\nthe Third International Conference on Music Information Re-\ntrieval, (pp. 25-30).\nUkkonen, E., Lemstr ¨om, K. & and M ¨akinen, V. (2003). Geo-\nmetricalgorithmsfortranspositioninvariantcontent-basedmu-\nsic retrieval. To appear in Proceedings of the 4th International\nSymposium on Music Information Retrieval .\nWiggins, G. A., Lemstr ¨om, K. & Meredith, D. (2002).\nSIA(M)ESE: an algorithm for transposition invariant, poly-\nphonic content-based music retrieval. In Proceedings of the\nThird International Conference on Music Information Re-\ntrieval, (pp. 283-284)."
    },
    {
        "title": "The MAMI query-by-voice experiment: collecting and annotating vocal queries for music information retrieval.",
        "author": [
            "Micheline Lesaffre",
            "Koen Tanghe",
            "Gaëtan Martens",
            "Dirk Moelants",
            "Marc Leman",
            "Bernard De Baets",
            "Hans E. De Meyer",
            "Jean-Pierre Martens"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1418133",
        "url": "https://doi.org/10.5281/zenodo.1418133",
        "ee": "https://zenodo.org/records/1418133/files/LesaffreTMMLBMM03.pdf",
        "abstract": "The MIR research community requires coordinated strategies in dealing with databases for system development and experimentation. Manually annotated files can accelerate the development of accurate analysis tools for music information retrieval. This paper presents background information on an annotated database of vocal queries that is freely available on the Internet. First we outline the design and set up of the experiment through which the vocal queries were generated. Then attention is drawn to the manual annotation of the vocal queries. 1",
        "zenodo_id": 1418133,
        "dblp_key": "conf/ismir/LesaffreTMMLBMM03",
        "keywords": [
            "system development",
            "experimentation",
            "database",
            "music information retrieval",
            "manually annotated files",
            "background information",
            "freely available",
            "vocal queries",
            "design and set up",
            "manual annotation"
        ],
        "content": "The MAMI Query-By-Voice Experiment : Collecting and annotating vocal\nqueries for music information retrieval\nMicheline Lesaffre1, Koen Tanghe1, Gaëtan Martens2, Dirk Moelants1, Marc Leman1, Bernard De Baets3,\nHans De Meyer2 and Jean- Pierre Martens4\n1 IPEM: Department of Musicology, Ghent University, Blandijnberg 2, 9000-Ghent, Belgium\nMicheline.Lesaffre@UGent.be\n2 Department of Applied Mathematics and Computer Science, Ghent University\n3 Department of Applied mathematics, Biometrics and Process Control, Ghent University\n4 Department of Electronics and Inform ation Systems (ELIS), Ghent University\nAbstract\nThe MIR research community requires coordinated\nstrategies in dealing with databases for system\ndevelopment and experimentation. Manually\nannotated files can accelerate the development of\naccurate analysis tools for music informationretrieval. This paper presents background information\non an annotated database of vocal queries that is\nfreely available on the Internet. First we outline thedesign and set up of the experiment through which\nthe vocal queries were generated. Then attention is\ndrawn to the manual annotation of the vocal queries.\n1 Introduction\nQuery by vocal input is a paradigm for the specification of\nmusical audio content in the context of music information\nretrieval (MIR). Such systems typically make a distinction\nbetween the query part (specification of musical content byusers) and the target part (dat abase of audio files). The MAMI\nproject ( http://www.ipem.UGent.be/MAMI/ ) aims at building\na music retrieval system using advanced audio-mining\ntechniques (Leman et al., 200 2). Paying attention to user-\nfriendly interaction with the quer y part is a prerequisite for\nfuture successful app lications of audio-based MIR. Little is\nknown about the capabilities and behavior of users dealing\nwith audio-based MIR, such as the impact of memory recall,human inaccuracy in the performance of vocal queries and\ndistinction between different user profiles. In context of this\nresearch a query-by-voice experiment was set up thatgenerated a query file database of around 1500 queries. By\n‘query-by voice’ we mean quer ies produced by the voice and\nthe vocal organs, notably singing lyrics, singing syllables,humming or whistling. This query file database was manuallyannotated in view of analysis of spontaneous user behavior\nand evaluation of tool effectiveness.\nA database of manually annotated vocal queries is useful for\nmultiple reasons. The major motivations are that it leads to a\nbetter understanding of the user’s querying behavior, it is a\nreference for automated annotation development and it serves\nas a benchmark for testing MIR systems. This paperintroduces the MAMI (Musical Audio Mining) query-by-voice\nexperiment. The aim of the e xperiment was to collect vocal\nqueries for detailed study of spontaneous user behavior andfor setting up a database that can be used for developing and\ntesting query-by-voice based MIR systems. For this\nexperiment, 30 pieces of music were selected, from a larger\nMAMI target database. The selection contains popular music,\nranging from chanson to heavy metal, well-known Flemish\nchildren songs and classical music. Thus it reflects theheterogeneous musical landscape of today (for a detailed list\nsee the MAMI website). Seventy-two subjects were involved\nin the experiment. We start with a brief description of how theexperiment was set up. Then the methods used for manual\nannotation are discussed and some results of the statistical\nanalysis are presented.\n2 Experiment setup\nThis section contains a brief description of the experiment. A\nuse case description gives an overview of the course of the\nexperiment. It is followed by a summary of the input andoutput files and some notes about the software itself.\n2.1 Use case description\n2.1.1 Physical setup\nA PC running Windows98SE is installed on a table in a small,\nclosed room with no special sound isolation (environmental\nnoise and sounds are still slightly hearable). This is a\ncompromise between recording in a natural environment and\ngiving the user some feeling of privacy. A low-budget\nmicrophone typically shipped with a new multimedia PC (in\nour case a Labtec Verse 514) is connected to the microphonePermission to make digital or hard copi es of all or part of this work for\npersonal of classroom use is granted w ithout fee provided that copies are not\nmade or distributed for profit or comm ercial advantage and that copies bear\nthis notice and the full citation on the first page.   2003 The Johns Hopkins\nUniversity.input of the computer's sound ca rd (a standard sound card, in\nour case a Yamaha DS1 PCI). Headphones are connected to\nthe sound card output. The subject is sitting in a chair in front\nof the computer monitor and the microphone, which is fixed\non top of the monitor keeping a distance of about 30centimeters of the subject's h ead. The experiment takes about\n35 minutes.\n2.1.2 Sound level check\nThe program can be told to perform a sound level check first.\nThis should be done at least once to check whether the\nrecording settings of the microphone/sound card are (still)\nOK. The sound card settings should be adjusted so that thelevel is \"just high enough without causing clipping for loud\nsinging or clapping\".\n2.1.3 Collecting info on the subject\nEach subject is automatically given a unique ID (a counter\nincreased by 1 each time the program is run). This unique ID\nis used for labeling the generated information files. Then a\nbasic profile of the user is obtained by asking the followingquestions: \"What is your age?\" \"Are you male or female?\"\n\"How many hours a week do you actively listen to music?\"\n\"Do you play a musical instrument (yes/no)? If so, how many\nhours a week?\" \"What is the highest level of musical\neducation you’ve had\". This information is written to a profilefile, together with the name of th is file and the name of the file\nthat is used as a log of the experiment for this subject.\n2.1.4 Collecting info on the subject's knowledge of the\nmusical pieces\nThis phase of the experiment collects information about the\nsubject's knowledge of the various pieces used in the\nexperiment. All pieces are specified in the configuration fileand gathered in a set called Set1. We used 30 pieces for Set1\nand for each subject this same set of 30 pieces is reorganized\nat random. The 30 pieces were selected from the MAMI targetdatabase that contains 160 en tire pieces of music stored in\nWAV format. Starting with the first piece of Set1, the title is\nshown together with (between brackets) its composer,performer or other brief specification and the subject is asked\nwhether he/she would be able to imitate a fragment of that\npiece. If the answer is \"yes\", the piece is added to a set of\"known, imitable\" pieces (Set3). If the answer is \"no\", the\nsubject is asked why not. He/she can then choose between: \"I\ndon't know it\" (piece is added to  Set4K), \"(I think) I know it,\nbut I just can't remember how it sounds\" (piece is added to\nSet4R) or \"I really do know it, bu t I can't imitate it\" (piece is\nadded to Set6). Then the second piece is presented etc… This\niteration over the pieces in Set1 stops if all pieces are handled,\nor as soon as Set3, Set4R and Set4K all contain enoughpieces. The results of this categorization are also written to the\nlog file where each of the above 4 sets shows the ID's of the\npieces that were added to it.\nIn order to obtain enough diversity in the subject's knowledge\nabout the pieces, we chose to us e 30 pieces for Set1 and aim to\nget 10 pieces for Set3 (the final number depends on the\nsubject's knowledge of  the pieces). This is a tradeoff between\ngetting enough recordings of  the same piece by differentsubjects and getting recordings for enough different pieces. To\nreduce the total time of the experiment, we chose to use only 2pieces from Set4R and 2 from Set4K (see Experiment part 2 ).\nAlso note that there is no Set2 (for historical reasons). An\noverview of the different sets of pieces is given in Table 1.\nSet1 fixed set of pieces from MAMI target database\nSet3 known and imitable\nSet4K not known\nSet4R thought to be known, but not remembered\nSet5 fixed fragment to be imitated in different ways\nSet6 known, but not imitable\nTable 1: Overview of the di fferent sets of musical pieces\n2.1.5 Experiment part 1\nThis part is focussed on the reproduction of known pieces\nfrom long-term memory. It gives information on how people\nprefer to (or would like to) imitate musical pieces and which\nparts are imitated without having heard the piece.\nThe pieces that ended up in Set3 (\"known pieces\") are\npresented one by one as described above (only title +\ncomposer, performer or other brief specification, no sound).\nThe subject is asked to im itate the piece vocally. The\nfollowing methods are proposed (but not imposed): humming,\nsinging the text, singing using a syllable, whistling, or any mixof these methods. The subject is  free to choose the fragment or\nvoice/instrument he/she wishes to imitate. Subjects can choose\nwhen to start and stop the recording by pressing the enter key,the maximum duration is 30 seconds. After the recording, the\nsubject is given the choice to make a second recording. This\ncan be done when the subject is  not satisfied with the first\nrecording, if he/she wants to use another method, or wants to\nperform another fragment from the same piece. After the vocal\nimitation(s), the subject is asked whether he/she wants todescribe the piece in another way. The choices are the\nfollowing: make a recording (using a method other than the\nprevious ones), provide a text ual description of the piece or\ndescribe an alternative query method using typed text. Each of\nthese choices can be made at most once.\nThe iteration over the pieces in  Set3 stops if a predefined\nnumber of pieces (we used 10) were handled (or if all pieceswere handled if there are not enough pieces in Set3 to reach\nthis number). All recordings are stored in WAV files (44.1\nkHz, 16-bit mono) and the textual inputs are stored in the log\nfile. A list of the pieces that were presented, the names of the\nfiles that were recorded for each piece and the choices madeby the user are also stored in  the log file. Remark: If the\nsubject did not specify any piece in the first part as \"known,\nimitable\" (Set3 is empty), this entire part has to be skippedand the experiment immediately continues with part 2.\nHowever, in our experiment, a number of pieces that were\nsupposed to be familiar to a large majority of the participantshad been included. This almost guarantees Set3 to contain a\nsufficient number of queries to obtain relevant results.2.1.6 Experiment part 2\nThis part is focussed on imitations from short-term memory,\nafter listening to the piece. It is  used to investigate differences\nwith imitations from long term memory, and to get an idea of\nwhich parts of a piece tend to \"stick\" after just hearing the\nentire piece.\nThe subject has to listen to a number of entire pieces (we used\n4), one by one. Immediately after listening to a piece he/she is\nasked whether it sounds familiar or not and then to imitate a\nfragment from it by vocal query. Again, the subject has the\noption to make a second recording if he/she wants to. The\npieces in this experiment are se lected as follows . We start with\nthe pieces from Set4K (\"not known\") until we have had apredefined number of them (we used 2) or until we run out of\nthem. Then the pieces of Set4R (\"known, but not\nremembered\") are used, again until we had a predefinednumber of them (we used 2) or until we run out of them. If we\nstill have not presented enough  entire pieces, the remaining\npieces are selected from (in th is order): Set3, Set4K, Set4R\nand Set6. Again, recordings are written to WAV files and all\nused pieces and the responses are logged in the log file.\n2.1.7 Experiment part 3\nThis last part is meant to give some information on differences\nin performances of the same melody by various subjects using\ndifferent query methods (male/female differences, pitchfluctuations, use of vibrato, accuracy of imitation, number of\npeople that can whistle a melody…).\nA short musical fragment (Set5) is played back (same\nfragment for all subjects). The subject can listen to it up tothree times (the first time he/she is asked whether he/she\nknows it or not). Then the subject is asked to imitate it in 4\ndifferent ways: sing it with words (the text is shown on thescreen), sing it using \"tatata\", hum it and whistle it (if he/she\ncan whistle, of course).\n2.1.8 The end\nAt the end of the experiment, a word of gratitude for\nparticipating in the experiment is shown on the screen, a final\nmessage is played back and a possible response from the\nsubject is recorded (could be used as an example of a non-query or noise recording). After that, the subject receives a\nsmall reward from one of the collaborators (a cinema ticket).\n2.1.9 Overview\nIn what follows an overview is given of the major steps in the\nexperiment.2.2 Input and output files\n2.2.1 Input files\nThe configuration file  specifies the setup of the experiment. It\nis supplied as a parameter when running the experimentprogram and contains directory paths and 2 sets of entries for\nmusical pieces. An example of a configuration file, using\npaths relative to the position where the executable is locatedcan be found on the MAMI webs ite (see below ‘Access’). The\nexample shows the configuration file used in the experiment\ndescribed here, so all queries in our database are fragments\nthat are related to these pieces.\nEach entry for a piece contains the following information: a 3-\ndigit ID of the piece (PID), na mes or words related to the\ncomposer, performer or another description, the title of the\npiece, the name of the sound file  and the name of the text file\ncontaining the lyrics for the fragment (only used for Set5).\nThe test sound files  for Set1 should be recordings of complete\npieces and can be in any format supported by the used\nlibraries. The names of the sound files are specified in the\nconfiguration file. We digitally  recorded the pieces from the\nCD's in 44.1 kHz 16-bit ster eo PCM WAV format and simply\nused the 3-digit piece ID's in the sound file names like this:\nPID.wav.\nFor the single fragment in Set5, a lyrics file  is specified in the\nconfiguration file. This lyrics file contains the exact lines of\ntext that are sung in the fragment.\n2.2.2 Output files\nProfile file\nThis file is created for each su bject and contains the following\ninformation: a unique ID for the subject, age and gender, the\nnumber of hours a week the subject actively listens to music,\nwhether the subject plays an instrument or not (if so, the\nnumber of hours a week is added), the highest level of musical\neducation (no musical education, music academy or musicconservatory) and paths to the log file and profile file.\nLog fileThe log file keeps track of the course of an experiment for a\nspecific subject (the subject ID is stored in the file name).\nEach log file consists of 4 parts, corresponding to the flow of1. collecting info on the subject\n2. collecting info on the subject's knowledge of the\nmusical pieces\n3. imitating known pieces without hearing them first\n(vocally, by whistling, or in any other possibleway)\n4. imitating pieces after hearing them in their entirety\nfirst (vocally or by whistling)\n5. imitating a fixed fragment in 4 different ways\n(singing  lyrics, singing \"tatata\", humming and\nwhistling)the experiment as described in the use case above. The\nfollowing describes the output into the log file of each of theseparts:\n• Preparatory stage\nThe log section for the pr eparatory stage of the\nexperiment shows a division of pieces into 4 categories.\nPieces were eventually classified as either \"I known it andI can imitate it\", \"I don't know it\", \"(I think) I know it, but\nI don't remember how it sounds\" or \"I really do know it,\nbut I can't imitate it\".\n• Experiment part 1\nThe log section for experiment part 1 has multiple entries,\none for each presented piece (specified by the piece ID).\nEach entry shows the file name(s) of the recorded vocal\nimitation(s) and is followed by an \"other query\" section.The latter can possibly contain any (or none) of the\nfollowing in any order: a verbal query, a description of\nanother query method, or the filename of an extra\nrecording. There can be less than 10 log entries like this if\nthe subject didn't specify enough pieces as \"known\".\n• Experiment part 2\nAgain, the log section has multiple entries, one for each\npresented piece (specified by the piece ID). Each entry\nshows whether the subject knew the piece after having\nlistened to it, followed by an indication if this answerdiffers from the one in the pr eparatory stage. After that,\nthe file name of the recorded imitation is shown, possibly\nfollowed by the file name of a second recording on thenext line. There are always lo g entries like this for exactly\n4 pieces.\n• Experiment part 3\nThe log section for this part shows an indication of\nwhether the subject knew the fragment after having\nlistened to it, followed by the number of times the subject\nlistened to the fragment. Then, the file names of therecorded fragments are shown. Fragments are always\nrecorded for singing with words, singing with syllables\nand humming. A fragment for whistling is only recordedif the subject indicated he/she could do that.\nQuery sound filesThese are the files that are analyzed to investigate user\npreferences and that can be used for the evaluation of audio\nfeature extraction algorithms. A ll generated query sound files\nare named consistently in a way that allows identifying the\nstage of the experiment where they were generated. There are\nsound files for the different query-by-voice trials in part 1 ofthe experiment (reproducing known pieces from long-term\nmemory), the first recording and possibly one or two extra\nrecordings (see above) and similarly for the 2nd part(producing queries after listening  to the piece). In part 3, all\nsubjects are asked to imitate a fragment heard using several\nmethods: singing words, singing syllables, humming and\nwhistling (not available if the subject indicated that she/he\ncould not whistle it). This gives an extra 3 or 4 sound files persubject. A last sound file contains some spontaneouscomments (if any) of the su bject, recorded after the\nexperiment has ended (mainly noise or laughter).\nSound level check fileThis sound file is generated when a sound level check is\nperformed (i.e. when running the program with the -L flag). It\njust contains the sound that wa s recorded the last time a sound\nlevel check was performed.\nCounter fileThis file always contains a single number representing the last\nused unique subject ID. Each time the program is run, this ID\nis incremented. The unique ID is used throughout the program\nfor labeling the output files.\n2.3 About the software itself\nThe application used for conducting this experiment was\ndeveloped as a Win32 console application using MSVC++ 6.0\nand was tested on Windows98SE and Windows2000 systems(we ran it on a Windows98SE machine for the experiment).\nStandard C++ was used as much as possible and the used\nlibraries PortAudio (Bencina et al., 2001-) and libsndfile (deCastro Lopo, 1999-) are cross platform so it should be\npossible to build it on other platforms as well (possibly with\nminor modifications).\n3 Database annotation\n3.1 Annotators\nMusicologists working at IPEM, Dept. of Musicology, Ghent\nUniversity, annotated the MAMI query database. They are all\nexperienced in computer assisted annotation using software\ntools such as Cool Edit, Pure Data and Matlab for assistance\nand verification.\n3.2 Annotation strategy\nThe annotation strategy was focused on two objectives, one\nuser-oriented, and one modeling-oriented. The user-orientedapproach aimed at providing content about the spontaneous\nbehavior of users taking part in the vocal query experiment.\nThe model-oriented approach aimed at providing detaileddescriptions of queries in order to build a referentialframework for testing automatic transcription models.\nUser-oriented annotations were carried out for a set of 1148\nqueries taken from the user responses to the 30 songs used inthe experiment. The user-oriented annotation focused on the\nanalysis of long-term versus s hort-term memory effects, the\ndifferent vocal methods used and the differences between\nsubjects and their relation to musical education, age and\ngender.\nThe model-oriented annotations were carried out for 32\nqueries, excerpts of the popular songs “Blowin’ in the wind”,\n“Walk on the wild side”, “S unday Bloody Sunday” and “My\nway”. These annotations provide detailed descriptions of lowand mid level acoustical features, as described in the nextsection.3.3 Annotated features\n3.3.1 User-oriented annotation\nThe user-oriented annotation was performed on the following\nfeatures: timing, query method, performance style, target\nsimilarity, and syllabic structure. In the field of timing, thetotal length of the recording, the start, end and length of the\nactual query were collected. Vocal queries may contain a mix\nof different query methods such as humming, singingsyllables, singing lyrics and whistling. In addition to those\nmethods percussion (e.g. tapping along with the drum) and\ncomments (spoken comments made by the subjects) are found.Studying these six methods in more detail required\nsegmentation into homogeneous parts. Segmentation in\ntemporal units according to the methods used resulted in a setof 2114 segments.\nSegment annotations at the temporal level indicate the starting\nand ending time as well as the total duration of the segment.\nThe number of segments in a query (according to the used\nmethods) is counted. Furthermore, distinction is made\nbetween three different perf ormance styles accentuating on\nmelodic, rhythmic or intermediate interpretations. A\nperformance style is considered  to be melodic when a clear\nsuccession of different pitches, or melodic intervals isobserved. A segment is annotated  as rhythmic when no clear\npitch intervals are noticed (as in a spoken text or a percussive\nsequence). An intermediate category is used to classify\nsegments where a sense of pitch is present, but without a clear\nmelody (e.g. using a reciting tone). Then, to each of the\nsegments, a relative similarity rating is given on a six-pointscale, ranging from not recognizable (0) to sounding similar\n(5) to the target song (presented textually in part 1 and aurally\nin part 2). The estimation of the degree of similarity betweenquery segment and target is focused on melodic and rhythmic\nproperties. Aspects of timbre or  use of lyrics are neglected.\nThis estimation, obviously, is  subjective and therefore the\nsimilarity measures are only used to compare large sets of\ndata. That is to compare the efficiency of the different\nmethods, the performance of different groups of users and theeffects of differences in memory recall.\nAs 766 segments out of 2114 have a syllabic content, a major\npart of the annotation work is related to the analysis of\nsyllabic queries. Syllables are considered as non-semantic\nvocal events, containing a vowel, which is preceded and/or\nfollowed by a consonant or a complex of consonants.Syllables are analyzed according to their structural\ncomponents: the onset (initia l consonant or complex of\nconsonants), the nucleus (v owel) and the coda (final\nconsonant). The 766 syllabic segments in the database contain\na total number of 14748 syllabic units.\n3.3.2 Model-oriented annotation\nThe model-oriented annotation is based on a set of\nhomogeneous query segments containing singing syllables or\nwhistling, and a set of heterogeneous queries containing amixture of methods. The features  investigated are event, onset,\nfrequency, pitch stability, method and sung words or syllables.At the local temporal level, ev ents are determined. An event or\nobject is characterized by its du ration. It starts at the moment\nin time defined by the beginning of an onset and ends at the\nmoment in time where the onset of  the next event or non-event\nbegins. From a conceptual point of view, an onset isconsidered a moment in time defined by the beginning of an\nevent. This definition acc ounts for successive events\npertaining to monophonic files. The point of onset is based onboth auditory (listening with headphones) and visual (looking\nat the waveform) perception. Moreover, a sureness quotation\nis given (0, 1) which distinguishes between clear onsets (1)and less pronounced onsets (0). An onset is considered less\npronounced when successive notes with the same or different\npitch are performed smoothly with no explicit separation (e.g.legato performance).\nFor pitch annotation, frequency wa s assigned in Hertz with a\nresolution of one Hertz. Taking into account that the query\nfiles in the data set are real audio and not MIDI encoded aresolution greater than a semitone makes sense. It facilitates\nadaptation to users who sing too high or too low with\nfrequency deviations that do not coincide with a semitone.\n3.4 Method of model-oriented annotation\nFor model-oriented annota tion the PRAAT program for\nspeech analysis (Boersma & Weenink, 1996) is used. Labeling\nand segmentation of the sound files is stored in a TextGrid\nobject that is separated fro m the sound. Boundaries are\nmarked by the places in time wh ere an event, a non-event or a\npause for breath begins. The TextGrid object is written into a\nformatted ASCII-text file. Time points are labeled on multipletiers that are time synchronized  for different annotation levels\nas shown in figure 1.\nFigure 1: Vocal query annotation using PRAAT. The tiers\nfrom top to bottom show (1) the sound waveform, (2) the pitch\ncontour of a sound as a function of time, (3) used lyrics or\nquery method, (4) sureness quotation and frequency and (5)\npitch stability.Tier objects representing non-events such as breathing or a\nclapping door are labeled with an ‘x’. The query method islabeled with h (humming), p (percussion), w (whistling) or c\n(comment). For sung queries the lyrics or nonsense syllables\nhave been annotated. Frequency estimation was done aurally,directly comparing the sung t ones with pure tones generated\nby a frequency generator implemented in Pure Data (Puckette,\nno date). Then a final check is done, reproducing theannotated frequencies simultaneously with the original\nsounds, using a Matlab script. To each event, an extra\nsegmentation descriptor has been added defining pitchdynamics in terms of stability. This feature refers to the steady\npart of a note where all the harmonics become stable and\nclearly marked in the spectrum. Stability annotation isconforming to four semantic labeled categories: up, down,\nstable and fluctuating. A pitch is considered stable when the\ndistance between the lowest a nd highest frequency within an\nevent is equal to or smaller than 5 Hz. As sung melodies,\nespecially when produced by un trained voices, move within a\nlimited frequency range, this  threshold rarely exceeds a\nquartertone.\nThe model-oriented database was used as a reference\nframework for testing pitch to MIDI models. Results of thisinvestigation are discussed by Clarisse et al. (2002).\n4 Overview of the user-oriented annotation\nStatistical analysis of the user-oriented annotation provided\ninsight in the structure of a que ry-by-voice search. Some basic\ncharacteristics of vocal queries and several user categoriescould be distinguished (Lesaffre, Moelants & Leman, in press;Lesaffre et al., in press).\n4.1 Timing\nAnalysis of the timing characteristics of the queries shows a\nmean query length of around 14 seconds while the actual\nquery starts 634 ms (average) after the start of the recording.\nBut the between-subjects va riance is considerable.\n4.2 Query methods\nSix query methods are distinguished, most common are\nsinging lyrics and singing syllables. Whistling is the third\nmost popular method, while actual humming, percussion andcomments occupy only a small sh are of the whole. In 40% of\nthe queries a mixture of at least two methods is used. Also the\nuse of certain methods is shown to be user dependent. Five\nuser categories have been distinguished that (1) show\npreference for singing text, (2) for singing syllables, (3)alternate between two methods, (4) mix several methods, and\n(5) show preference for whistlin g. Around 75% of the queries\nare performed in a melodic way.\n4.3 Syllable structure\nIn the detailed analysis of the syllabic queries a total of 23\ndifferent onsets and 37 rhymes is found, but 99.3% is\norganized in 11 onsets and 19 rhymes. The ten most commonsyllables are (in order of decreasing importance) [na], [n@],\n[la], [t@],  [da], [di], [d@], [ta], [tu] and [ti], of which [t@],\n[na] and [d@] belong to the syllable repertoire of the largestnumber of subjects. Syllables are transcribed using SAMPA\n(http://www.phon.ucl.ac.uk/home/sampa/home.htm ) that is a\nmachine-readable variant on the International Phonetic\nAlphabet. Analysis of the type, spread and clustering of the\nsyllables also distinguished between different user categories.For onset as well as for nucleus different user groups were\nfound. Users may use specific ons ets ([n] and [l]) or nuclei\n([a] and [@]). Some users clearly prefer the onset cluster [d-t-\nr] while others go for the alternation of [a] and [@] nuclei.\n4.4 Effects of age, gend er, musical experience\nSignificant effects of age, ge nder and musical backgrounds\nwere found. It is shown that younger people tend to start their\nqueries sooner and have a better similarity score. Men tend tostart their queries later than women do and use a larger variety\nof syllables with a larger share of onset [t] and a smaller\namount of [a] nuclei. Musicians make longer queries than non-musicians and use less text (in favor of syllables and\nwhistling) than non-musicians. The use of [a] nuclei and [l]\nonsets and of comment as a query method increases with age.\n4.5 Memory\nDifferences were also found between the reproduction of\nunfamiliar melodies from short-term memory and the recall ofknown melodies from long-term memory. The timing of the\nqueries as well as their similarity with the target is dependent\non the type of memory used  by the subjects. When known\nsongs are ‘refreshed’ by presen ting them aurally, the queries\nstart sooner and last longer. The reproduction of unfamiliar\nmelodies from short-term memo ry has less quality than recall\nof known melodies even if the query only relies on long-term\nmemory. The lesser degree of acquaintance is also reflected in\nthe larger share of syllabic segments and the increasedimportance of rhythmic and intermediate performances.\n5 Access\nThe database can be accessed from the public section of the\nMAMI project website: http://www.ipem.ugent.be/MAMI .\nThe website also provides more detailed descriptions of the\nsetup, the input and output files, the queries and annotations,its availability and links to electronic versions of related\npapers.\n6 Conclusion\nAnnotation of music collections is often seen as a necessary\nstep for the development of MIR systems. Byrd (2003) has for\nexample described candidate musi c IR test collections, their\ncharacteristics and availability. However none of thesecollections contain human produced queries stored as audio.\nResearchers tend to work with databases containing small sets\nof queries with tunes obtained from small-scale experimentalstudies. Query sets with sung t unes, are far from elaborate and\nthere is a need for thorough studies related to the user’s\nsinging preferences and habits (Downie, 2003) . This paper\npresented an elaborate experiment that generated a query\ndatabase with melodies that re present all possible vocal query\nmethods. Investigations concerning user behavior and systemdevelopment show that the use of manually annotated files\nyields a better conceptual understanding of some of therecurring issues encountered in an undertaking of musical\ncontent retrieval. The availability of the MAMI vocal query\ndatabase and its annotations provides basic material for MIRresearch.\nAcknowledgements\nThe authors wish to thank Jelle Dierickx and Liesbeth De\nVoogdt for their assistance in annotating the query files. We\nthank Johannes Taelman for hi s assistance in working with\nPure Data. This research has been conducted in the framework\nof the MAMI project for audio recognition at IPEM,Department of Musicology at Ghent University. The Flemish\nInstitute for the Promotion of Scientific and Technical\nResearch in Industry gives financial support for this project.\nReferences\nBencina, R., Burk, P., Dannenberg, R., McNab, D., Eldridge,\nB., et al. (2001-). PortAudio, a free, cross platform, open-\nsource, audio I/O library . Retrieved July 31, 2003, from\nhttp://www.portaudio.com .\nBoersma, P., & Weenink,  D. (1996). Praat. A system for\ndoing phonetics by computer . Amsterdam: Institute of\nPhonetic Sciences of the Univer sity of Amsterdam. Retrieved\nJuly 31, 2003, from http://www.praat.org .\nByrd, D. (2003). Candidate Music IR Test Collections\nRetrieved July 31, 2003, fromhttp://php.indiana.edu/~donbyrd/MusicTestCollections.HTML\nClarisse, L. P., Martens, J.-P.,  Lesaffre, M., De Baets, B., De\nMeyer, H., & Leman, M. (2002). An Auditory Model Based\nTranscriber of singing sequences. In M. Fingerhut (Ed.) ,\nProceedings of the Third International Conference on Music\nInformation Retrieval  (pp. 116-123). Paris: IRCAM – Centre\nPompidou.\nde Castro Lopo, E. (1999-). libsndfile, a C library for reading\nand writing files containing sampled sound through one\nstandard library . Retrieved July 31, 2003, from\nhttp://www.zip.com.au/~erikd/libsndfile .\nDownie, S. J. (2003). Music information retrieval. Annual\nReview of Information Science and Technology , 37, 295-340.\nRetrieved July 31, 2003, from http://music-\nir.org/downie_mir_arist37.pdf .\nLeman, M., Clarisse, L. P., De Baets, B., De Meyer, H.,\nLesaffre, M., Martens, G., Martens, J.-P., & Van Steelant, D.\n(2002). Tendencies, Perspectives, and Opportunities ofMusical Audio-Mining. Journal Revista de Acústica ,\nXXXIII (3-4),   abstract p. 79 , full text: CD-ROM.\nLesaffre, M., Moelants, D., & Leman, M. (in press).\nSpontaneous user behavior in “vocal” queries for music-information retrieval. Computing in musicology .\nLesaffre, M., Moelants, D., Leman, M., De Baets, B., De\nMeyer, H., Martens, G. , & Martens, J. -P.\n (in press). User\nbehavior in the spontaneous reproduction of musical pieces byvocal query. In: Proceedings of ESCOM5. Hannover .Puckette, M. S. (no date). Pure Data. Retrieved July 31, 2003,\nfrom http://www.pure-data.org/ ."
    },
    {
        "title": "Detecting emotion in music.",
        "author": [
            "Tao Li 0001",
            "Mitsunori Ogihara"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417293",
        "url": "https://doi.org/10.5281/zenodo.1417293",
        "ee": "https://zenodo.org/records/1417293/files/LiM03.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1417293,
        "dblp_key": "conf/ismir/LiM03",
        "content": "Detecting EmotioninMusic\nTaoLi\nDepartment ofComputer Science\nUniversity ofRochester\nRochester ,NY14627\nemail: taoli@cs.rochester .eduMitsunori Ogihara\nDepartment ofComputer Science\nUniversity ofRochester\nRochester ,NY14627\nemail: ogihara@cs.rochester .edu\n1Introduction\nMusic isnotonly forentertainment andforpleasure, buthas\nbeen used forawide range ofpurposes duetoitssocial and\nphysiological effects. Traditionally musical information has\nbeen retrie vedand/or classiﬁed based onstandard reference in-\nformation, such asthename ofthecomposer andthetitleofthe\nworketc.These basic pieces information willremain essential,\nbutinformation retrie valbased onthese arefarfrom satisf ac-\ntory.Huron points outthatsince thepreeminent functions of\nmusic aresocial andpsychological, themost useful characteri-\nzation would bebased onfour types ofinformation: thestyle,\nemotion, genre, andsimilarity [Huron,2000 ].\nThe relation between musical sounds and their inﬂuence on\nthelistener’ semotion hasbeen well studied. The celebrated\npaper ofHevner [Hevner,1936 ]studied this relation through\nexperiments inwhich thelisteners areaskedtowrite adjec-\ntivesthat came totheir minds asthemost descripti veofthe\nmusic played. The experiments substantiated ahypothesis\nthatmusic inherently carries emotional meaning. Hevner dis-\ncovered theexistence ofclusters ofdescripti veadjecti vesand\nlaid them out(there were eight ofthem) inacircle. She\nalso disco vered that thelabeling isconsistent within agroup\nhaving asimilar cultural background. The Hevner adjec-\ntiveswere reﬁned andregrouped into tenadjecti vegroups by\nFarnsw orth [Farnsw orth,1958 ].Wehypothesize that emotion\ndetection inmusic canbemade byanalyzing music signals. We\napproach totheproblem using multi-label classiﬁcation.\nWecasttheemotion detection problem asamulti-label classiﬁ-\ncation problem ,where themusic sounds areclassiﬁed intomul-\ntiple classes simultaneously .That isasingle music sound may\nbecharacterized bymore than onelabel, e.g. both “dreamy”\nand“cheerful. ”Wedivide theprocess ofemotion detection in\nmusic into twosteps: featur eextraction andmulti-label clas-\nsiﬁcation .Inthefeature extraction step, weextract from the\nmusic signals information representing themusic. Thefeatures\nextract should becompr ehensive (representing themusic very\nwell), compact (requiring asmall amount ofstorage), andeffec-\ntive(not requiring much computation forextraction). Tomeet\nPermission tomakedigital orhard copies ofallorpartofthiswork\nforpersonal orclassroom useisgranted without feeprovided that\ncopies arenotmade ordistrib uted forproﬁt orcommercial advan-\ntage andthatcopies bear thisnotice andthefullcitation ontheﬁrst\npage. c\n\u00002003 Johns Hopkins University .A cheerful,gay ,happ y H dramatic, emphatic\nB fanciful, light I agitated, exciting\nC delicate,graceful J frustrated\nD dreamy ,leisurely K mysterious, spook y\nE longing, pathetic L passionate\nF dark, depressing M bluesy\nG sacred, spiritual\nTable 1:The adjecti vegroups. The ﬁrst tenofthem arethe\nFarnsw orthgroups andthelastthree aretheadditions.\ntheﬁrstrequirement thedesign hastobemade sothattheboth\nlow-levelandhigh-le velinformation ofthemusic isincluded.\nInthesecond step, webuildamechanism (analgorithm and/or\namathematical model) foridentifying thelabels from therep-\nresentation ofthemusic sounds with respect totheir features.\n2The Music Data Used andTheir Emotional\nLabels\nAcollection of499sound ﬁles wascreated from 128music al-\nbums asfollo ws:From each albumtheﬁrst four music tracks\nwere chosen (three tracks from albums with only three music\ntracks). Then from each music track thesound signals overa\nperiod of30seconds after theinitial 30seconds were extracted\ninMP3. Thecollection covered four major music types, Ambi-\nent(120 ﬁles), Classical (164 ﬁles), Fusion (135 ﬁles), andJazz\n(100 ﬁles).\nThe 499ﬁles were labeled byasubject (a39year old,male).\nThetenadjecti vegroups ofFarnsw orth[Farnsw orth,1958 ]were\nused forthelabeling. The subject wasinstructed toselect for\neach track alladjecti vegroups that match thesound with no\nlimit tothenumber ofgroups chosen. Also, thesubject was\ninstructed tosuggest anewadjecti vegroup ifnecessary .The\nsubject added three newgroups: mysterious, spook y;passion-\nate;andbluesy ,thereby increasing thetotal number ofgroups to\nthirteen. Thesubject wasalsoaskedtogroup thethirteen groups\ninto “super groups. ”Heformed thesixsuper groups. Table 1\nshowsﬁrstfewadjecti vesofeach group. Thesixsuper groups\nare \u0001\u0003\u0002\u0005\u0004\u0007\u0006\t\b\n\u0004\u000b\u0001\u0003\f\r\u0004\u0007\u000e\u000f\b\n\u0004\u0010\u0001\u0003\u0011\u0012\u0004\u0007\u0013\u0014\b\n\u0004\u000b\u0001\u0016\u0015\u0017\u0004\u0019\u0018\u001a\u0004\u001c\u001b\u001d\b\u001e\u0004\u0010\u0001\u0003\u001f \u0004\u0007!\"\b and \u0001\u0003#$\u0004\u0019%&\b .\n3The Classiﬁcation Method\nInthetraditional classiﬁcation problem, classes aremutually\nexclusi vebydeﬁnition. Inemotion detection inmusic, how-ever,thedisjointness ofthelabels isnolonger valid, inthesense\nthatasingle music sound may beclassiﬁed intomultiple emo-\ntional categories. This stipulation seems tomaketheproblem\nsigniﬁcantly more complicated. Unfortunately ,thearea isyet\ntobeexplored. Thesparse literature onthissubject isprimar -\nilygeared towardtextclassiﬁcation and, toourknowledge, no\nprior workexists inthemusic information retrie valdomain.\nWeresort tothescarcity ofliterature inmulti-label classiﬁca-\ntion bydecomposing theproblem into asetofbinary classiﬁ-\ncation problems. Inthisapproach, foreach binary problem a\nclassiﬁer isdeveloped using theprojection ofthetraining data\ntothebinary problem. Todetermine labels ofatestdata, the\nbinary classiﬁers thus develop arerunindividually onthedata\nandeverylabel forwhich theoutput oftheclassiﬁer exceeds a\npredetermined threshold isselected asalabel ofthedata. To\nbuild classiﬁers weused Support Vector Machines (SVM for\nshort) andourimplementation isbased ontheLIBSVM1.\nThe SVMs were trained using features extracted\nfrom the sounds. Toextract features we used\nMARSY AS [Tzanetakis andCook,2000 ]. The extracted\nfeatures aredivided into three different categories: timbral\ntexture features, rhythmic content features, andpitch content\nfeatures. Thedimension oftheﬁnal feature vector is30.\nThe accurac yoftheclassiﬁers ismeasured using precision ,\nrecall ,break-e venpoint and F1-measur e.Since thepreci-\nsion andtherecall canbeaveraged overtheclassiﬁers with\norwithout weighting, weuseboth micro-aver agedprecision\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t,micro-aver agedrecall \u000b\n\u0001\u0004\u0003\f\u0005\b\u0007\r\t,macr o-aver agedpreci-\nsion\n\u0000\u000e\u0001\u0004\u000f\u0010\u0005\b\u0007\n\tandthemacr o-aver agedrecall \u000b\n\u0001\u0011\u000f\u0012\u0005\b\u0007\r\t.Inaddi-\ntion, wealsocompute theHamming accurac y(denoted by \u0013\u0015\u0014),\nwhich isdeﬁned tobethesimple unweighted accurac y,thatis\ntheunweighted ratio ofthetotal correct tothetotal input size.\nThese aretheperformance measures thatarewidely used inin-\nformation retrie valliterature [YangandLiu,1999 ].\n4Experiments\nOur SVM-based multi-label classiﬁcation method wastested\nfortwoproblems: classiﬁcation into thethirteen adjecti ve\ngroups andclassiﬁcation into thesixsuper groups. There was\nsigniﬁcant difference inthedistrib ution ofthepositi vedata for\nsome oftheadjecti vegroups (e.g., “bluesy” notappearing inthe\nclassical category). Weconstructed thesuper group classiﬁer for\neach ofthefour styles. Due tothespace limitation, weonly in-\nclude theresults ofallthethirteen adjecti vegroups onallfour\nstyles. Wedivided the499sounds intotraining data andtesting\ndata byarandom \u0016\u0018\u0017\u001a\u0019\u001c\u001b\u001d\u0016\u0018\u0017\u001a\u0019 split.\nTheaccurac ymeasures oneach ofthethirteen classes areshown\ninTable 2.The overall accurac yforthetwoexperiments are\nshowninTable 3.Thebreak evenpoint, i.e.thehalf-w aypoint\nbetween theprecision andtherecall, was46% inmicro-a vering\nand43% inmacro-a vering. Inoursix-super group experiment\nthebreak evenpoint was50% inmicro-a vering and 49% in\nmacro-a veraging, sotheoverall accurac ywasimpro vedwhen\nthenumber ofcategories isreduced.\nThe overall lowperformance canbeattrib uted tothefactthat\nthere were numerous borderline cases forwhich thelabeler\nfound itdifﬁcult tomakedecision. Also, thefrequenc yofthe\n1Available athttp://www .csie.ntu.edu.tw/˜cjlin/libsvmGroup \u001e\n\u0000\u001e\u0011\u001f  \n\u0000 !\u001f\n\u0000\u000b \u0013\u0015\u0014\nA 12 132 81 22 0.1290 0.3529 0.5830\nB 3189 44 11 0.0638 0.2143 0.7773\nC 96 70 45 36 0.6809 0.7273 0.6721\nD 53 106 64 24 0.4530 0.6883 0.6437\nE 46 81 61 59 0.4299 0.4381 0.5142\nF 43 102 73 29 0.3707 0.5972 0.5870\nG 26 127 78 16 0.2500 0.6190 0.6194\nH 28 156 40 23 0.4118 0.5490 0.7449\nI 56 135 41 15 0.5773 0.7887 0.7733\nJ 10 178 47 12 0.1754 0.4545 0.7611\nK 15 161 51 20 0.2273 0.4286 0.7126\nL 13 144 70 20 0.1566 0.3939 0.6356\nM 18 181 43 50.2951 0.7826 0.8057\nTable 2:Accurac ymeasures onadjecti vegroup classiﬁcation.\nMeasure\n\u0000\u000e\u0001\u0004\u0003\f\u0005\b\u0007\r\t\u000b\n\u0001\u0004\u0003\f\u0005\b\u0007\r\t \"\u0004\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t \n\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\nValues 0.3621 0.5893 0.4757 0.4486\nMeasure\n\u0000\n\u0001\u0011\u000f\u0012\u0005\b\u0007\r\t\u000b\n\u0001\u0011\u000f\u0012\u0005\b\u0007\r\t\"\n\u0001\u0011\u000f\u0012\u0005\b\u0007\n\t \n\u0001\u0011\u000f\u0012\u0005\b\u0007\r\t\nValues 0.3247 0.5411 0.4329 0.4058\nTable 3:Overall accurac ymeasures.\nlabels wasnotequal across music types. Weactually carried out\nanother setofexperiments, emotion detection forsuper groups\nwithin each music type. Weobserv edimpro vements especially ,\nPerformance stood outonsuper group 2forclassical andsu-\npergroup 4forfusion. This may suggest thattheuseofgenre\ninformation might impro veemotion detection.\nOur experiments showthatemotion detection isarather difﬁ-\ncultproblem andimpro vement ofperformance istheimmediate\nissue. This canberesolv edby:expanding thesound data sets,\ncollecting labeling inmultiple rounds toensure conﬁdence in\nlabeling, using different setsofadjecti ves,incorporating style\nandgenre information, andusing different types offeatures.\nAckno wledgments\nThe authors thank Diane Cass forhelping usinﬁnding ref-\nerences. This workissupported inpart byNSF grants EIA-\n0080124, DUE-9980943, andEIA-0205061, andinpartbyNIH\ngrants RO1-A G18231 (5-25589) andP30-A G18254.\nRefer ences\n[Farnsw orth,1958] PaulR.Farnsw orth. Thesocial psychology\nofmusic .TheDryden Press, 1958.\n[Hevner,1936] Kate Hevner.Experimental studies oftheele-\nments ofexpression inmusic. American Journal ofPsyc hol-\nogy,48:246–268, 1936.\n[Huron,2000] D.Huron. Perceptual andcogniti veapplications\ninmusic information retrie val.InInternational Symposium\nonMusic Information Retrie val,2000.\n[Tzanetakis andCook,2000] Geor geTzanetakis and Perry\nCook. Marsyas: Aframe workforaudio analysis. Organized\nSound ,4(3):169–175, 2000.\n[YangandLiu,1999] Y.YangandX.Liu. Are-examination of\ntextcategorization methods. InSIGIR ,1999."
    },
    {
        "title": "Automatic mood detection from acoustic music data.",
        "author": [
            "Dan Liu 0001",
            "Lie Lu",
            "HongJiang Zhang"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1418335",
        "url": "https://doi.org/10.5281/zenodo.1418335",
        "ee": "https://zenodo.org/records/1418335/files/LiuLZ03.pdf",
        "abstract": "Music mood describes the inherent emotional meaning of a music clip.  It is helpful in music understanding, music search and some music-related applications.  In this paper, a hierarchical framework is presented to automate the task of mood detection from acoustic music data, by following some music psychological theories in western cultures.  Three feature sets , intensity, timbre and rhythm, are extracted to represent the characteristics of a music clip.  Moreover, a mood tracking approach is also presented for a whole piece of music.  Experimental evaluations indicate that the proposed algorithms produce satisfactory results. 1",
        "zenodo_id": 1418335,
        "dblp_key": "conf/ismir/LiuLZ03",
        "keywords": [
            "Mood detection",
            "Acoustic music data",
            "Feature extraction",
            "Intensity",
            "Timbre",
            "Rhythm",
            "Music psychological theories",
            "Mood tracking",
            "Cultural context",
            "Emotion in music"
        ],
        "content": "Automatic  Mood Detection  from  Acoustic Music Data1 \nDan Liu  \nDepartment of Automation  \n Tsinghua University  \nBeijing 100084 China   \naryaya00@mails.tsinghua.edu.cn  Lie Lu  \nMicrosoft Research Asia  \nSigma Center , Haidian District  \n Beijing 100080 China  \nllu@microsoft.com  Hong -Jiang Zhang  \nMicrosoft Research Asia  \nSigma Center , Haidian District  \n Beijing 100080 China  \nhjzhang@microsoft.com  \n \nAbstract  \nMusic mood describes the i nherent emotional \nmeaning of a music clip.  It is helpful in music \nunderstand ing, music search  and some music -related \napplications .  In this paper, a hierarchical framework \nis presented to automate the task of mood detection \nfrom acoustic music data, by fo llowing some music \npsychological theories in western cultures.  Three  \nfeature sets , intensity, timbre and rhythm, are \nextracted to represent the characteristics of a music \nclip.  Moreover, a mood tracking approach is also \npresented for a whole piece of mus ic.  E xperiment al \nevaluations indicate that the proposed  algorithms \nproduce satisfactory results.  \n1 Introduction  \nAs there are more and more music databases in personal \ncomputer and the Internet at present, people start to realize the \nimportance of creating m etadata that allow users to access \nmusical works easily .  Although traditional information such \nas the name of the artist or the title of the work remains \nimportant, these tags have limited applicability in many \nmusic -related queries.  Nowadays, users expe ct more semantic \nmetadata to archive music, such as similarity, style  and mood \n(Huron , 2000) .  However, compared to the first two, few  \nworks have focused on mood detection .   \nOne common opinion objecting to  mood detection is that the \nemotional meaning of music is subjective  and it depends on \nmany factors including culture.  Music psychologists now \nagree that culture is of great importance in people ’s mood \nresponse to music, as well as other factors including  education \nand previous experiences.  However, it is also found that, \nwithin a given cultural context, there is agreement among \nindividuals as to the mood elicited by music (Radoc y and  \nBoyle , 1988) .  Krumhansl  (Krumhansl, 2002)  also pointed out \nthat musical sounds might  inherently have emotional meaning.  \nFor example, some music patterns represent contentment or \nrelaxing, while some others make an individual feel a nxious or frantic.  Therefore , it is possible to build a mood detection \nsystem in a concrete environment, for example, for classical \nmusic in western culture.  \nFew works have touched this field.  Liu (Liu, Zhan g and Zhu , \n2003)  has presented a mood recognition system, where a fuzzy \nclassifier was adopted to classify the mood of Johann \nStrauss’s waltz centos into five clusters.  In this system, tempo, \nloudness, pitch change, note density and timbre were extracted \nfrom MIDI file and used as the primitives to recognize the \nmood of music.   Kata yose (Katayose , Imai  and Inokuchi , 1988)  \nalso presented a sentiment extraction system  for pop music, \nwhere monophonic acoustic da ta was firstly transcribed into \nmusic codes.   Then, primitives of music such as melody, \nrhythm, harmony and form were extracted from these music \ncodes.   These works have led to some impressive results, but \nthey both concentrated  on MIDI or symbolic repres entations, \ndue to the difficulty of extracting useful features from acoustic \ndata.  However, most music in real world is not in symbolic \nform and there is no existing transcription system that can \ntranslate it into symbolic representations well (Scheirer , 2000) .  \nTherefore, it is necessary to deal with the acoustic data \ndirectly.   \nIn this paper, we present a mood detection algorithm for \nclassical music from acoustic data.  \n1.1 Mood Taxonomy  \nOne issue of mood detection is on mood taxonomy .  In music \npsychology, the traditional approach to describing mood \nresponse is using adjective descriptors, such as pathetic, \nhopeful  and gloomy .  However, these adjectives varied quite \nfreely  in different research es (Liu, Zhang  and  Zhu , 2003; \nKatayose , Imai  and Inokuchi , 1988) .  There is not a standard \nmood taxonomy  system  accepted by all currently .  Hevner’s \nadjective checklist  (Hevner, 1935)  presented in 1930s has \nserved as the basis for some subsequent  research on mood \nresponse to music .  This checklist is composed of 67 \nadjectives  from  eight clusters , which include Sober, Gloomy, \nLonging, Lyrical, Sprightly, Joyous, Restless and Robust .  \nHowever, since adjectives in the same cluster are actually of \napproximately the same meaning , it is very difficult to \ndiscriminate one from others.  Th is ambiguity  makes it Permission to make digital or hard copies of all or part of this work for \npersonal of classroom us e is granted without fee provided that copies are not \nmade or distributed for profit or commercial advantage and that copies bear \nthis notice and the full citation on the first page.   2003 The Johns Hopkins \nUniversity.   \n1 This work was perfor med in Microsoft Research Asia  difficult to obtain the “ground truth ”.  Meanwhile, it doesn ’t \nindicate any underlying stimulus that influences these \nresponses, which  will be of great importance for \ncomputational modeling.  In the late 1990 s, Thayer (Thayer, \n1989)  proposed a two -dimensional mood model .  Unlike \nHevner ’s checklist that uses individual adjectives which \ncollectively  form a mood pattern, this dimensional approach \nadopt s the theory that mood is entailed from t wo factors: \nStress (happy/anxious) and Energy (calm/ energetic ), and \ndivides music mood into four clusters: Contentment , \nDepression, Exuberance and Anxious/Frantic as shown in Fig. \n1.  \n \n In Fig. 1,  Contentment refers to happy and  calm music, such \nas Bach ’s “Jesus , Joy of Man ’s Desiring ”; Depression refers to \ncalm and anxious music, such as the opening of Stravinsky ’s \n“Firebird ”; Exuberance refers to happy and energetic music \nsuch as Rossini ’s “William Tell Overture ”; and \nAnxious/F rantic refers to anxious and energetic music, such as \nBerg ’s “Lulu ”.  Such definition s of the four clusters are \nexplicit  and d iscriminat able, and the two-dimensional  \nstructure  also gives importance cues for computational \nmodeling.  Therefore, it is applied  in our mood detection \nsystem.  \n1.2 Hierarchical Framework  \n \n \n \n \n \n \n \n \n \n Little  Big \n \nFeature B  Intensity  Group  \n  1 Music \nClip \nFeature C  Group  \n  2 \nContentmen\nt Depression  Exuberance  \n Anxious  \n  \nFigure 2: The hieratical mood detection framework  \n \nBased on  Thayer ’s hierarchical model of mood (Thayer, 1989) , \na hierarchica l framework is proposed for mood detection, as \nillustrated in Fig. 2.   Huron  (Huron, 1992 ) pointed out that of \nthe two factors in Thayer ’s model of mood, Energy is more \ncomputationally  tractable an d can be estimated using simple \namplitude -based measures.  In fact, energy for Contentment  \nand Depression  is usually much less than that of Exuberance \nand Anxious.  Therefore, features representing energy are \nfirstly used to classify all these four mood cl usters into two \ngroups.  If the energy is little, it will be classified  into Group 1 \n(Contentment and Depression); otherwise, it is classified into Group 2 (Exuberance and Anxious).  Then, other features are \nused to determine  which exactly the mood type is .  This \nframework is accordant to the music psychological theory.  \nMeanwhile, since the performance of different features is not \nthe same in discriminating different mood clusters  pairs , this \nframework is advantaged  in making it possible to use the most \nsuitable  features  for different tasks.  Moreover, like other \nhierarchical  methods, it can make better use of sparse training  \ndata than its non -hierarchical  counterparts  (McCallum , 1998 ).   \nThis paper is struct ured as follows.  Section 2 describes the \nextraction of feature s.  Detailed mood detection process is \npresented in Section 3.  In Section 4, an automatic  \nsegmentation approach is presented for mood tracking in a \nwhole piece of music.  Section 5 deals with empirical \nexperiments  and performance evaluations of the proposed \nalgorithms and  Section 6 with conclusions and future \ndirections.  \n2 Feature  Extraction  \nIt was indicated that mode, intensity, timbre and rhythm are of \ngreat significance in arousing different m usic moods ( Hevner, \n1935 ; Radocy , and Boyle , 1988; Krumhansl, 2002 ).  For \nexample, major keys are consistently  associated with positive \nemotions, whereas minor ones are associated with negative \nemotions.  However, mode is very difficult to obtain from \nacoustic data (Hinn, 1996 ).  Therefore, only the rest three \nfeatures a re extracted and used in our mood detection system.  \nCompared to the two dimensions in Thayer ’s model of mood, \nintensity is corresponding to “energy ”, while both timbre and \nrhythm  are corresponding to  “stress ”.  \nEach input music clip is first down -sampled into a uniform \nformat: 16000Hz, 16 bits, mono channel , and divided into \nnon-overlapping 32ms -long frames.  In each frame, an octave -\nscale filter -bank is used to divide the frequency domain into \nseveral sub-bands:  \n]2,2),...[2,2[),2,0[10\n20\n10 0 0w w w w w\n−n n n                            (1)  \nwhere w 0 refers to the sampling rate  and n is the number of \nsub-band filters.  In real implementation, 7  sub-bands  are used.  \nThen, timbre features and intensity  features  are extracted from \neach frame .  Their m eans and variances are calculated across \nthe music file and thus make up of  timbre and intensity \nfeatures sets .  Meanwhile,  rhythm  features are also extracted \ndirectly from the music clip.  In order to remove the relativity \namong these raw features , Karhunen -Loeve transform is \nperformed  on each feature set .  After K -L transform, each of \nthe three feature vectors is mapped into an orthogonal space, \nand each covariance  matrix also becomes diagonal in the new \nfeature space.  This procedure helps to achieve a better \nclassification  performance  with GMM classifier  later.  \nDetailed feature extractions are as follows . \n2.1 Timbre Features  \nMany existing  results show that  the timbre of sound is \ndetermined primarily by the spectral information  in different \nsub-bands  (Zhang  and Kuo , 1998 ).  In this paper, both spectral \nshape features and spectral contrast features are used.  The  \nEnergy   Exuberance  Anxious/Frantic  \nContentment  Depression  \nStress  \nFigure 1: Thayer’s model of mood  detail features used are listed in Table 1.  Spectral shape \nfeatures, which include centroid, bandwidth, roll off and \nspectral flux, are widely used to represent the characteristics \nof mus ic signals (Tzanetakis and Cook, 2002).  They are also \nimportant for mood detection.  For example, centroid for \nmusic of Exuberance is usually higher everywhere than that \nof Depression, since Exuberance is generally associated with \na high pitch whereas Dep ression is with a low pitch. \nMeanwhile,  octave -based spectral contrast features are also \nused to represent relative spectral distributions, due to their \ngood properties in music genre recognition (Jiang, Lu, Zhang, \nTao and Cai, 2002).   \n2.2 Intensity Features  \nIntensity is approximated by the signal ’s root mean -square \n(RMS) level in decibels (Erling, 1996 ).  It is essential  for \nmood detection, because  intensity in music of Contentment \nand Depressio n is usually little, but that of Exuberance and \nAnxious is usually big.  In this system, intensity in each sub -\nband and the sum of them are used.    \n2.3 Rhythm Features  \nThree aspects of rhythm are closely related with people ’s \nmood response: strength , regulari ty and tempo.  For example, \nin the Exuberance  cluster, the rhythm is usually strong, steady \nand the tempo is fast; while in Depression, music is usually \nslow and with no distinct rhythm pattern.  Therefore, these \nthree features  are extracted accordingly.  Since drum or some \nbass instruments are  the most important component s to \nrepresent rhythm, and they show their properties mainly in the \nlower sub -band s, in our system , only the lowest sub -band is \nused to extract rhythm features.  \nAfter amplitude envelope is extracted from this sub-band  by \nusing a half hamming (raise cosine) window , a Canny  \nestimator is used to estimate its difference  curve, which  is \nused to represent the rhythm information.   The peaks above \nsome threshold in such a rhythm curve are detecte d as bass \ninstrumental onsets.  Then, three features are extracted as \nfollows:   \n Average Strength: the average strength of bass instrumental \nonsets.  \n Average Correlation Peak: the average of the maximum \nthree peaks in the auto -correlation curve.  The more r egular  \nthe rhythm is, the higher the value is.  \n Average Tempo: the common divisor of the peaks of  the \nauto-correlation curve.  \n3 Mood Detection  \nBased on the three feature sets extracted in Section 2, the \nmood detection process is performed through a hierarchic al \nframework, as illustrated in Fig. 3.  Compared to its non -\nhierarchical counterpart as shown in Fig. 4, such  hierarchical  \nframework can stress on different features for different \nclassification  tasks, it can also make better use of sparse \ntraining  data (McCallum , 1998 ).   \nIn our system, Gaussian Mixture Model (GMM) is utilized to \nmodel each feature set.  In constructing each GMM, t he \nExpectation Maximization (EM) algorithm is used to estimate \nthe parameters of the Gaussian component and mixture \nweights.  The initialization is performed using the K -means \nalgorithm.   \nFor a given music clip X, it is firstly classified into Group 1 \n(Contentment and Depression) or Group 2 (Exuberance and \nAnxious) based on its int ensity information; and then \nclassification is performed in each group based on timbre and \nrhythm features, as the Fig. 3  illustrates.  \nTo classify the music clip into differnet groups, simple \nBayesian criteria is employed, as  \n                       \n\n<≥\n21\n21\n,1,1\n)|()|(\nG SelectG Select\nIGPIGP                         (2)  \nwhere Gi represent s different mood group, I represents the \nintensity feature set.  \nIn each group, the probability  of being an exact mood given \ntimber and rhythm features can be calculated as  Feature Name  Definition  \nCentroid  Mean of the short -time Fourier amplitude  spectrum.  \nBandwidth  Amplitude  weighte d average of the differences between the spectral \ncomponents and the centroid.  \nRoll off 95th percentile of the spectral  distribution.  Spectral  \nShape  \nFeatures  \nSpectral Flux 2-Norm distance of the frame -to-frame spectral amplitude difference . \nSub-band Peak  Average value in a small neighborhood around maximum  amplitude \nvalues of spectral components  in each sub -band.  \nSub-band Valley  Average value in a small neighborhood around m inimum amplitude \nvalues of spectral components  in each sub -band.  Spectral \nContrast \nFeatures  \nSub-band \nAverage  Average amplitude of all the spectral components  in each sub -\nband . \nTable 1: Definition  of Timbre Features  4,3 ) ( ) 1() ( ),, (2,1 ) ( ) 1() ( ),, (\n2 2 21 1 1\n= ×−+ ×== ×−+ ×=\nj RMP TMP RTGMPj RMP TMP RTGMP\nj j jj j j\nl ll l     (3)          \nwhere Mi is the mood cluster, T and R represent timbre and \nrhythm features respectively; 1l and 2l are two weighting  \nfactors to emphasize different features for the mood detection  \nin differe nt mood groups.  \nActually, in the Group 1, the tempo of both mood  cluster s is \nusually slow and the rhythm pattern is generally not steady, \nwhile the timbre of Contentment is usually much brighter and \nmore harmonic than that of Depression.  Therefore, the timbre \nfeatures are more important than the rhythm features in the \nclassificatio n in Group 1.  On the contrary, in Group 2, \nrhythm features are more important.  Exuberance usually has \na more distinguished and steady rhythm than Anxious, while \ntheir timbre features are similar, since the instruments of both \nmood  cluster s are mainly bra ss.  Based on these facts, 1l  is \nusually set as larger than 0.5, while 2l  is less than 0.5.  Their \ndetailed values are given in the experiments in Section 5.1. \nAfter each probability is obtained, Bayesian  criteri on, similar  \nto Equation 2 , is again employed to classify the music into \nexact mood  cluster . \n4 Mood Tracking  \nIn the previous sections, we present an algorithm on mood \ndetection in a given music clip, where the mood type is \nconsistent.  However, since the mood  is usually changeable  in \na whole piece of classical  music (Kamien, 1992 ), it is not appropriate  to detect the mood in the range of the whole song.  \nIn fact, i t is necessary to divide the music  into several \nindependent segments , each of which contains a constant \nmood, and then to detect the mood type in each segment  \nrespectively.  In this way , mood is tracked in a whole piece of \nmusic.  \nSince changes in intensity and timbre are main cues for new \nsound event and  are therefore important for segmentation \n(Tzanetakis  and Cook , 1999 ), both of the features are used to \ncomplement each other to improve the performance of \nsegmentation in this method.  \nAccording to music theo ry (Kamien, 1992 ), one paragraph is \nusually of 16 bars and a very fast tempo is about 1 bar/second \nin classical music.  Therefore, we assume the minimum \nsegment length is 16 seconds, and set the basic process ing \nunit as 16 seconds window with 1-second  temporal resolution.  \nTo find the segment boundary, divergence shape (Campbell, \n1997)  is used to measure the dissimilarity between two \ncontiguous windows , supposing both the featur es are \nGaussian distributed,  \n                    )] )( [(21 1 1 − −− − = i j j iC C C Ctr D                 (4) \nwhere Ci and Cj is the estimated covariance  matrix of ith and \n(i+1)th window, respectively.  \nBase on the dissimilarity measure, a confidence of being a \nboundary i s defined as   \nRhythm  \nIntensity  GMM 1 Timbre  \nGMM 2 \nGMM 3 \nTimb re GMM 4 \nRhythm  GMM 5 Group \n1 Contentment  \nDepression  \nExuberance  \nAnxious  Group \n2 Music  \nClip X  Mood \n1 \nMood \n2 \nMood \n3 \nMood \n4 \nLayer 1  Layer 2  Layer 3   \nFigure 3: The hierarchical  mood detection framework  \n \nRhythm  Intensity  \nTimbre  GMM  \n Contentment  \nDepression  \nExuberance  \nAnxious  Music  \nClip X  \n \nFigure 4: The non-hierarchical  mood detection framework  ) exp(1), exp(1\nTT T\nTT\nII I\nIID\nAConfD\nAConf\nsm\nsm −=−=   (5)                       \nwhere Im and Is are the mean and variance  of intensity \ndissimilarity between two contiguous windows , Tm and Ts \nare the mean and variance  of timbre dissimilarity between two \ncontiguous windows , AI and AT are used for normalization .  \nThus, the total confidence is  \nT IConf Conf Conf ×−+ ×= ) 1(a a                 (6)               \nwhere a is a weigh ting factor and we set 5.0=a  in real \nimplementation.  \nA potential  chance boundary is found between ith and ( i+1)th \nwindow, if the following conditions are satisfied:  \niTh ii Confi i Conf ii Confi i Conf ii Conf\n>+− >+++ >+\n)1,( )3),1( )1,( )2)2,1( )1,( )1                            (7) \nwhere Conf(i, j)  is the confidence that the segment  boundary is \nat between ith and jth window, Thi is a threshold .  The first two \nconditions guarantee that a local peak exists, and the last \ncondition can prevent very low peaks from being detected.  \nThe threshold is adaptive ly set according to its context as:  \n∑−=−−−\n××=N\nN nini ni Conf\nNTh ) ,1 (\n21a           (8)             \nwhere N is the number of the previous and succeeding  \ndistances to predict threshold, and a is an amplifier.  In our \nalgorithm, we set N= 8, 5.1=a  to obtain optimal  result.  That \nis, threshold is automatically  set according to neighborhood  of \n16 second, which is assumed to be the minimum length for \none segment .   \nThe threshold works well in the whole song, but we still need \nto refine the boundaries if more than one potential boundaries  \nexit in 16 second, since it contravenes  our assumption  on the \nminimum length of a segment.  In this case, the distances \nbetween the current segment and its two neighbor segments \nare compared , and  then co mbined with the more similar one.  \n5 Experiments  \nTwo experiment s are presented in this section to evaluate the \nproposed mood detection system.  The first experiment  shows \nthe performance on the selected music clips, inside of which \nthe mood type is consistent .  In the second experiment , the \nmood tracking method is evaluated with some famous music \nworks.  \n5.1 Mood Detection on 20s Music Clips  \nOur database contains about 250 pieces of music, composed \nmainly in the classical period and romantic period.  Choir, \norchest ra, piano and string quartet are all included to ensure \nthe diversity of music style in the database.  Three music \nexperts participated in selecting and annotating 200 \nrepresentative music clips of 20 seconds long from the \ndatabase for each of the four moo d clusters: Contentment, Depression, Exuberance and Anxious.   All these 800 music \nclips are used in the evaluation.  \nAmong the se four clusters , clips in the Contentment cluster \nare mainly selected from  Christian music and Serenade , while \nExuberance clips are mainly from  Overture, March and \nDancing music .   As for  Depression and Anxious  clusters , \nmusic is selected from much broader music genre s, since  \nthere are no dominant genres  as in the Contentment and \nExuberance  clusters .   Since the mood is usually changeable  \nin a whole piece of classical  music a s we mentioned before, \neach music clip is of 20 seconds long,  and selected carefully \nto ensure the perceived mood  is consistent  and representative .  \nOne example  is Suppe ’s “Light Cavalry ”, which contains  two \nclips in Depression and three clips in Exuberance .     \nThe classification results are calculated using a cross -\nvalidation evaluation where the dataset to be evaluated is \nrandomly partitioned so the 25% is used for testing and 75% \nis used for training.  T he process is iterated with different \nrandom partitions and the results are averaged (for Table 2 \nand Table 3, 10 iterations were performed).  It ensures that the \ncalculated accuracy will not be biased because of a particular \npartitioning of training and t esting.  The ± part shows the \nstandard deviation of classification accuracy.  \nIn order to emphasize the importance of timbre and rhythm \nfeatures in different mood groups, we used different \nweighting factors in Equation 3 and achieved the optimal \naverage acc uracy when 8.01=l , 4.02=l .  It confirms that \ntimbre features are more important to classify Contentment \nand Depression in Group 1, and rhythm features are more \nimportant to discriminate Exuberance and Anxious in Group 2.  \n \n Contentment  Depression  Exuberance  Anxious  \nContentment 76.6±7.6 21.8±7.2 0.5±0.8 1.2±1.2 \nDepression  4.0±3.5 94.5±3.4 0±0 1.5±2.5 \nExuberance  0±0 0.8±1.3 85.5±3.2 13.7±4.8 \nAnxious  0±0 0±0 11.5±6.7 88.5±6.7 \nTable 2: Mood detection confusion matrix based on \nhierarchical  framework  \n \n Contentment  Depression  Exuberan ce Anxious  \nContentment 75.0±11.8 25.0±11.8 0±0 0±0 \nDepression  5.8±2.6 94.2±2.6 0±0 0±0 \nExuberance  1.5±2.6 0.7±1.3 64.7±20.5 33.0±18.3 \nAnxious  0±0 0±0 11.7±7.9 88.3±7.9 \nTable 3: Mood detection confusion matrix based on non-\nhierarchical  framework  \n Table  2 shows the detailed  results in the form of confusion \nmatrix , where each row correspond s to the actual mood \ncluster and each  column  to the predicted cluster .  As can be \nseen  from Table 2 , only 1.6% music in Group 1 ( Contentment  \nand Depression) is classifi ed into Group 2 (Exuberance and \nAnxious), while only 0.4% music in Group 2 is classified  into \nGroup 1.   That is, the accuracy  rate reaches about 99% in the \nfirst step, when intensity features are used to classify all \nmusic clips into two groups.  This resu lt confirms the good \nperformance of intensity features in discriminating the two \ngroups  of mood clusters , which severs as the basis for further \nclassification by timbre and rhythm features.  \nIn order to compare the performance of above  hierarchical \nframewor k and its non -hierarchical counterpart, a \ncomparative experiment  is also performed  on the framework \nshown in Fig. 4, which  integrat es the three feature sets and \ncarries on classifi cation  directly on it.  The corresponding \nresults are shown in Table 3.   Compared Table 2  and Table 3, \nit can be seen that the overall classification  accuracy  for the \nproposed hierarchical  framework is up to 86.3%, about 5.7% \nbetter than the non -hierarchical framework .  Meanwhile, the \nstandard deviation  of classification  accuracy  decreases from \n10.7% to 5.2% , which indicates that our framework is more \nconstant .  It can be  also seen , by adopting  the proposed \nhieratical  framework, the classification  accuracies for  all of \nthe four clusters  are improved, especially for Exuberance.  In \nnon-hierarchical framework, about 33.0% Exuberance clips \nare classified into Anxious, whi le it is decreased by more than \n50% after  using our hierarchical framework.   These \nexperiment al results show that the proposed hierarchical  \nframework has a better perfo rmance than it s non -hierarchical  \ncounterpart , by using the most efficient features for different \nmood clusters.   \n5.2 Mood Tracking  \nThe proposed mood tracking method is a lso evaluated on \nseveral pieces of classical music and achieved satisfactory \nresult.  For example, it can correctly detect that Haydn’s \n“Serenade” is constantly Contentment; and the second \nmovement of Beethoven’s “Symphony No. 3” is mainly \nDepression.  Fig. 5 shows the results of mood tracking for a part of “1812 \nOverture” composed by Tchaikovsky  (from 361s – 661s).  \nThe figure also shows the potential boundaries and refined \nboundaries.  It can be seen that almost all of the correct \nboundaries are recalled, although there exist some false \nalarms. This ensures that the mood inside one segment is \nconsistent.  Compared the mood tracking result s based on our \napproach, every 20s clips and the “ground truth”, it can be \nalso seen that since our approach can detect t he boundar ies \nwell, the resulting  mood trac king performance is better than \nthat of detecting  mood every 20 seconds.  \n6 Conclusion  \nIn this paper, we present a mood detection approach for \nclassical music from acoustic data.  Thayer ’s model of mood \nis adopted fo r mood taxonomy , and three  efficient f eature sets  \nare extracted directly from acoustic data  represent ing intensity, \ntimbre and rhythm respectively .  A hierarchical framework is \nused to detect the mood in a music clip.  In order to detect the \nmood in a whol e piece of music, a segmentation  scheme is \npresented for mood tracking.  This algorithm achieves \nsatisfactory accuracy in the e xperiment al evaluations.  \nThere are m any future improvements in the proposed \nalgorithm.   We will work on extracting more powerful \nfeatures to better represent music primitives in music \nperception.   Furthermore, we will try more efficient ways for \nmood tracking.   Finally, we will extent this mood detection \nalgorithm  to other style s such as pop music.  \n \nReferences  \n[1] Campbell, J. P. (1997) . Speaker recognition: a tutorial. \nProceeding of the IEEE , 85 (9), 1437 -1462.  \n[2] Erling, W., et al (1996). Content -based c lassification , \nsearch, and retrieval of audio. IEEE Trans. Multimedia , 3, \n27-36. \n[3] Hevner, K. (1935). Expression in music: a discussion of \nexperimental studies and theories. Psychological Review , \n42, 186 -204. \n[4] Hinn, D. M. (1996). The effect of the major and minor \nmode in music as a mood induction procedure . Master  \n361 391 421 451 481 511 541 571 601 631 661 1 2 3 4 \nTime Index (s)  Mood Cluster  Mood Tracking (ground truth)  \nMood Tracking (segment)  \nMood Tracking (20s clip)  \nPotential Boundary  \nRefined Boundary  \n \nFigure 5: Mood tracking results on a part of “1812 Overture ” Thesis, Virginia Polytechnic Institute.  \n[5] Huron, D. (1992). The ramp archetype and  the \nmaintenance of auditory attention. Music Perception , 10 \n(1), 83 -92. \n[6] Huron , D. (2000).  Perceptual and cognitive applications \nin music information retrieval . International Symposium \non Music Information Retrieval  (ISMIR) 2000.  \n[7] Jiang , D. N., Lu , L., Zhan g, H. J. , Tao, J. H. & Cai, L. H. \n(2002) . Music type classification by spectral contrast \nfeatures . Proc eeding of Int. Conf. Multimedia Expo . \n[8] Kamien, R. (1992). Music: an appreciation (5th Edition). \nMcGraw -Hill Inc.  \n[9] Katayose , H., Imai , M. & Inokuchi , S. (1988). Sentiment \nextraction in music. Proc eeding of Int. Conf. Pattern \nRecognition , 2, (pp. 1083 -1087). \n[10] Krumhansl, C. L. (2002). Music: a link between \ncognition and emotion. Current Directions in Psychological \nScience , 11(2), 45 -50. \n[11] Liu, D., Zhang , N. Y. & Zhu, H. C. (2003) . Form and \nmood recognition of Johann Strauss’s waltz centos . \nChinese Journal of Electronics , 3. (in press)  [12] McCallum , A., et al (1998). Improving text classification \nby shrinkage in a hierarchy of classes . Proceeding of. Int. \nConf. Machine Learning , (pp. 359 -367).  \n[13] Radocy , E. & Boyle , J. D.  (1988) . Psychological \nfoundations of musical behavior . Illinois : Charles C \nThomas.  \n[14] Scheirer , E. D.  (2000) . Music -listening systems . Ph. D. \nThesis, MIT Media Lab.  \n[15] Thayer, R. E. (1989). The biopsychology of mood and \narousal . Oxford University Press.  \n[16] Tzanetakis , G. & Cook , P. (1999) . Multifeature audio \nsegmentation for browsing and annotation. IEEE \nWorkshop on Applications  of Signal Processing to Audio \nand Acoustics , (pp. 17 -20). \n[17] Tzanetakis , G. & Cook , P. (200 2). Music genre \nclassification of audio signals . IEEE Trans. Speech Audio \nProcessing , 10 (5), 293 -302. \n[18] Zhang , T. & Kuo , J. (1998 ). Hierarchical system for \ncontent -based audio classification and retrieval . \nProc eeding  of SPIE's Conference on Multimedia Stora ge \nand Archiving Systems III , 3527, (pp. 398-409)."
    },
    {
        "title": "The importance of cross database evaluation in musical instrument sound classification: A critical approach.",
        "author": [
            "Arie Livshin",
            "Xavier Rodet"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417771",
        "url": "https://doi.org/10.5281/zenodo.1417771",
        "ee": "https://zenodo.org/records/1417771/files/LivshinR03.pdf",
        "abstract": "In numerous articles (Martin and Kim, 1998; Fraser and Fujinaga, 1999; and many others) sound classification algorithms are evaluated using \"self classification\" - the learning and test groups are randomly selected out of the same sound database. We will show that \"self classification\" is not necessarily a good statistic for the ability of a classification algorithm to learn, generalize or classify well. We introduce the alternative \"Minus-1 DB\" evaluation method and demonstrate that it does not have the shortcomings of \"self classification\". 1 Testing Platform The importance of cross database evaluation will be demonstrated through a variety of classification experiments.",
        "zenodo_id": 1417771,
        "dblp_key": "conf/ismir/LivshinR03",
        "keywords": [
            "sound classification algorithms",
            "self classification",
            "statistic for learning",
            "ability to generalize",
            "classification well",
            "Minus-1 DB",
            "alternative evaluation method",
            "shortcomings",
            "cross database evaluation",
            "classification experiments"
        ],
        "content": "The Importance of Cross Database Evaluation in Sound Classification \nArie Livshin \nlivshin@ircam.fr Xavier Rodet \nrod@ircam.fr \nIRCAM Centre Pompidou, 1 place Igor -Stravinsky, 75004  Paris, France \n \nAbstract \nIn numerous articles (Martin and Kim, 1998; Fraser \nand Fujinaga, 1999; and many others) sound \nclassification algorithms are evaluated using \"self \nclassification\" - the learning and test groups are \nrandomly selected out of the same sound database. \nWe will show that \"self classification\" is not necessarily a good statistic for the ability of a \nclassification algorithm to  learn, generalize or \nclassify well. We introduce the alternative \"Minus-1 DB\" evaluation method and demonstrate that it does \nnot have the shortcomings of \"self classification\". \n1 Testing Platform \nThe importance of cross database evaluation will be \ndemonstrated through a variety of classification experiments. \n1.1 The Test Set \nThe Sounds.   In order to demonstrate well the claims in the \npaper, we extracted out of 5 sound databases, recorded  in \nvarious acoustic conditions and different equipment, the samples of 7 instruments common to them, played with a \n\"standard\" playing technique. The instruments are: Bassoon, \nContrabass, Clarinet, French horn, Flute, Oboe, and Cello.  The number of samples extracted out of each database, is: \nIrcam Studio Online (SOL) - 581, University of Iowa Musical \nInstrument Samples (IOWA) -  1289, McGill\n University \nMaster Samples (McGill) - 85, Pro collection - 158, Vi \ncollection - 249. The samples are remixed in mono, 44.1Khz, \n16bit and clipped to 2 seconds. \n \nThe Feature Descriptors . We use 162 different sound \ndescriptors (Peeters, 2002), normalized to the range 0 - 1. \n1.2 The Classification Algorithms  \n\"LDA+KNN\".  The classified data goes through Linear \nDiscriminant Analysis (McLachlan, 1992) then classified \nusing the K-nearest neighbors al gorithm. Instead of selecting \nconstant values for K, we estimate the best K by using the \nLeave-One-Out Cross Validation method on the learning set \nwith K's in the range of 1 - 20. \n\"BP80\" .  A back propagation neural network with a single hidden layer of 80 neurons, using \"tansig\" functions in all the \nlayers, is trained using Conjugate Gradient with Powell/Beale \nRestarts until a Mean Square Error of 0.004 is reached. \n1.3 Evaluation methods \n\"Self Classification\".   A single database is used for evaluation \nof the classification process. The training set consists of 2/3 of \nthe samples from each instrument  class, which are randomly \nselected. To minimize the fluctuations of this random \nselection, each result shown is the mean of 20/50 tests \n(depending on the classifica tion algorithm), so the 95% \nconfidence interval is not more  than 1% around the mean.  \n\"Mutual Classification\".   A single complete database is used \nto classify another single and complete database. \n\"Minus-1 DB\".  Several databases are used, each one \nclassified by the rest joined together.  \n2 Disadvantages of Se lf Classification \n2.1 Claim 1: evaluation using Self Classification is not \nnecessarily a good measure for the generalization \nabilities of the classification process \nIn this section we demonstrate several points: \n1. Evaluation results using a single database are not \nnecessarily an indication of the generalization abilities of the classification process and its suitability for practical \napplications of sound classification of musical instruments.  \n2. Self Classification results do not reflect the classifier \nability, after learning the specific database, to deal with new \nsounds, and thus its performance as a Concept Classifier\n1. \n3. We shall demonstrate the intuitive claim that enriching the \nlearning database with diverse samples from other databases \nimproves the generalization power of the classifier and makes \nit more suited for classification of new sounds. \nTable 1 mostly consists of the classification success \npercentage (recognition rate) of classifying each database by \nevery other one. The Minus-1 DB column shows the success \nresults of classifying a database by all the other databases put \ntogether. The diagonal shows the mean results of 50 Self \nClassification rounds for each database, using a learning group \nof 2/3 of the samples. All classifications are performed using \nthe LDA+KNN classification process. \n                                                           \n1 The ultimate goal of sound classificati on is to obtain a \"Concept Classifier\" - \nsuch classifier could recognize which instruments are playing regardless of \nspecific recording conditions, a specific performer or a specific instrument. \n Permission to make digital or hard copi es of all or part of this work for \npersonal of classroom use is granted w ithout fee provided that copies are not \nmade or distributed for profit or comme rcial advantage and that copies bear \nthis notice and the full citation on the first page.   2003 The Johns Hopkins \nUniversity.  SOL IOWA McGill Pro Vi Minus 1 DB\nSOL classified by  (98.24) 39.93 20.14 21.51 58.17 68.5 \nIOWA by 51.43 (97.75) 35.22 29.17 58.42 65.79 \nMcGill by     51.76 51. 76 (60.78) 23. 53 48.23 77.65 \nPro by        54.43 41.77 26.58 (48.04) 58.86 75.32 \nVi by          63.45 48.59 30.12 20.88 (64.42) 75.9 \nTable 1: Self Classificati on, Mutual Classification \nand Minus-1 DB results using LDA+KNN  \nTable 1 demonstrates that the results of Self Classification of a \ndatabase are very different from the results of classifying another database by it. This shows that Self Classification \nresults do not predict how a classifier which is trained on one \ndatabase will classify new samples. Point 2 demonstrated . \nComparing the Minus-1 DB column to the Self Classification \nresults, we see that enriching th e learning database by samples \nout of other databases helps the classifier to generalize better and thus get closer to recognizing the Concept Instruments. \nEven relatively small databases, which do not even contain \nenough samples for good Self Classification using LDA+KNN (McGill, Pro and Vi), when they are added to the learning set, \nconsiderably improve the generalization ability of the \nclassifier. For example, wh en SOL (a relatively large \ndatabase) is classified by IOWA (the largest one), still the \nresults are considerably improv ed, from 39.93% to 68.5%, \nwhen the 3 small databases are added to IOWA (Minus-1 DB \nclassification of SOL). Point 3 demonstrated . \nBy comparing Self Classifica tion and Mutual Classification \nresults, it is possible to evaluate for each database its self \ncontainment vs. its diversity, thus concluding how well the \ndatabase is suited for generalized classification, e.g. when examining Table 1, we can see that Vi, while not appearing to \nbe very self contained, seems to be diverse enough and \ncomparatively suited for classifi cation of the other databases. \nLet us now examine Table 2, which contains mean success \npercents of 20 Self Classification and Minus-1 DB \nclassifications using the BP80 neural network: \n Self Classification  Minus 1 DB \nSOL  (97.93) 87.78 \nIOWA  (99.35) 74.71 \nMcGill (77.86) 80 \nPro  (87.55) 84.18 \nVi  (92.84) 89.16 \nTable 2: Self Classification and Minus-1 DB results  \nusing BP80  \nIf we compare the \"Minus-1 DB\" columns in Tables 1 and 2, \nwe see that the neural networ k generalizes much better than \nLDA+KNN, probably due to its  ability to perform nonlinear \nanalysis. We see that by using this net we can get much closer to a Concept Classifier than with LDA+KNN. Yet, if we \ncompare the Self Classification results of the SOL and IOWA \ndatabases in Tables 1 and 2, the results are very similar. \nFollowing from that, if we would compare LDA+KNN and \nBP80 just by using Self Classification of a single large \ndatabase, we could conclude that there is no considerable difference in the capabilities of these algorithms and that both \nperform very well. Point 1 demonstrated . \n2.2 Claim 2: Evaluation using Self Classification of a \nclassification process where  specific instruments are \nbeing classified, does no t necessarily reflect the \nsuitability of the feature de scriptors being used, for \ngeneral classification of these instruments \nIn this section we shall see th at a feature selection algorithm \nmight choose different features for classification of the same \ninstrument types, depending on the sound database being used. \nThis also means that evaluating features using a single \ndatabase and Self Classification will not necessarily show the suitability of these features for a Concept Classifier. \nFor feature selection we shall use our GDE (Gradual \nDescriptor Elimination) algorithm, which repeatedly performs LDA and removes the least important descriptor, until a \ndesired number of the most important descriptors is left.  \nDemonstrating Claim 2.  Using GDE we have (apparently) \nchosen the best 8 feature desc riptors out of 162, for each \nsound database. Table 3 shows which features were selected \nusing each database. The upper row contains the indices of all \nthe feature descriptors that were chosen. The asterisks indicate \nthe selected features. \nDesc.#      18 19 42 44 45 46 47 48 49 50 51 52 73 134 135 136 137 138  140  \nSOL *   *                             *                       *    *            *     *     * IOWA *              *   *   *  *                                   *                          *      * \nMcGill           *              *  *                  *    *                       *     *     * \nPro *                        *  *        *                         *            *     *     * Vi *                        *  *             *                    *    *      *     *                .\n \nTotal: 4   1   1    1   1  4   4   1   1   1   1   1    1    4    1      4     4     4     1  .  \nAll DB's merged       *        *  *   *                              *            *     *     *         .  \nTable 3:  8 best feature descriptors provided by GDE  \nWe see that different featur es are selected for each DB2, thus \nevaluation of features using a single database does not \nnecessarily demonstrate the usef ulness of these features for a \nConcept Classifier. Claim 2 demonstrated . \nAcknowledgement \nMany thanks to Geoffroy Peeters for contributing his \ndescriptors computation routines and sharing his knowledge. \nReferences \nFraser, A. & Fujinaga., I. (1999). Toward real-time \nrecognition of acoustic musical instruments. In  Proceedings of \nthe ICMC, 1999 , (pp. 175-177).  \nMartin, K. D. & Kim, Y. E. (1998). Musical instrument \nidentification: A pattern-recogn ition approach. Paper read at \nthe 136'th meeting of the Acoustical Society of America ,   \nMcLachlan, G. J. (1992) Discriminant Analysis and Statistical \nPattern Recognition.  New York, NY: Wiley Interscience. \nPeeters, G. (2002). WP2.1 Preliminary Audio Descriptors . \nProject CUIDADO - Audio Feature Extraction . \n                                                           \n2 We see 7 descriptor indices that figu re a lot. They are Sharpness, Specific \nLoudness -19, Specific Loudness -20, Spectral Centroid, Spectral Skewness, \nSpectral Kurtosis and Spectral Slope. Explanation is found in Peeters (2002)."
    },
    {
        "title": "An SVM-based classification approach to musical audio.",
        "author": [
            "Namunu Chinthaka Maddage",
            "Changsheng Xu",
            "Ye Wang 0007"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1415610",
        "url": "https://doi.org/10.5281/zenodo.1415610",
        "ee": "https://zenodo.org/records/1415610/files/MaddageXW03.pdf",
        "abstract": "This paper describes an automatic hierarchical music classification approach based on support vector machines (SVM). Based on the proposed method, the music is classified into coursed classes such as vocal, instrumental or vocal mixed with instrumental music. These main classes are further sub-classed according to gender and instrument type. A novel method, Correction Algorithm for Music Sequence (CAMS) has been developed to improve the classification efficiency. 1",
        "zenodo_id": 1415610,
        "dblp_key": "conf/ismir/MaddageXW03",
        "keywords": [
            "automatic",
            "hierarchical",
            "music",
            "classification",
            "support",
            "vector",
            "machines",
            "SVM",
            "music",
            "classification"
        ],
        "content": "A SVM-Based Classification Appr oach to Musical Audio \nNamunu Chinthaka Maddage \nInstitute for Inforcomm Research  \n21, Heng Muikeng, Terrace \nSingapore 119613 \nmaddage@i 2r.a-st ar.edu.sg Changsheng X u \nInstitute for Inforcomm Research  \n21, Heng Muikeng, Terrace \nSingapore 119613 \nxucs@i 2r.a-st ar.edu.sg Ye Wang \nSchool  of C omputing \nNational University of Si ngapore \nSingapore 117543 \nwangy e@com p.nus.edu.sg \n \nAbstract \nThis paper describes an automatic hierarc hical music \nclassification approach bas ed on support vector  \nmach ines (SVM) . Based on t he proposed meth od, the \nmusic is c lassified int o cours ed classes s uch as voc al, \ninstrumental or vocal  mixed with instrumental music. \nThese m ain classes are further sub-classed according \nto gender and instrument type. A novel method, \nCorrection Algorithm for M usic Sequence (CAMS) \nhas been  developed to improve the classification \nefficiency.  \n1 Introduction \nWith the growing nee d for multimedia appl ications, a udio \nanalysis has becom e an i mportant issue in  the si gnal \nprocessi ng area. Content-based audio retrieval depe nds on \nclassificatio n of intrinsic pro perties of the aud io. Automatic \nmusic tran scription is an other important application , which  \ndepe nds upon a method of audio analysis and is related to post \nprocessi ng and  editing ph ases of act ual reco rdings (Eron en, \net.al., 2000). \nThe goals of this resea rch are:   (1) to explain whether there \nexist sig nifican t statistical d ifferences between vocal m elody \nstructure, m usic instrum ents (string  type-acou stic gu itar, \nblowing type-harmonica) and mixture o f vocal and \ninstrumental m usic wi thout tak ing tim e d ependent \ncharacteristics in to con sideratio n; (2) to study how supp ort \nvector m achines (SVM) perform s for m usic classification as a \ntime series analysis proble m; and (3) to com pare t he \nclassificatio n perform ance with  multilayer neural n etworks \n(MNN)and  Gaussi an m ixture m odel (GM M).  \n2 Musical Audio Features \nWe consi der features that are often use d in audio / speech \nanalysis in cluding linear prediction c oefficients (LPC ), LPC \nderived c epstrum s (LP CC), m el-freque ncy ce pstral  \nCoefficients (MFCC), spectral powe r (SP), s hort tim e energy  \n(STE), and zero crossi ng rat es (ZC ) (Rabiner, et .al, 1993). \nThe di fferent features have differe nt strengths distinguishing one class from  other class  of m usic. MFCCs are more \neffectiv e in iden tifyin g differen t vocal st ructures  as well as \ninstrum ental music. T he SP a nd ZC features  perform  better in \nidentifying vocal related m usic and b lowing ty pe of \ninstrumental music. The LPC and L PCC ar e highly correlated \nwith eac h other and perform ance wise LPCCs are  much better  \nin identifying v ocal m usic. The sel ective fre quency band \nLPCC can improve the perform ance over full band L PPCs \n(Maddage, et .al., 2002).  \n3 Experimental Setup \nWe have  recorded 10 Sri Lankan songs (2~3 minutes long), \nsung in middle scal e with major chords composition, by  both \nmale and female singers at different time peri ods with a stereo \n16-bit wave format and a 4 4.1 KHz sam pling frequency. In \norder to gene rate testing an d training samples, we m ixed vocal \ntrack s (female and male) with  instrumental track s (acoustic  \nguitar and harm onica) without distorting t he m elody \ncharacter istics of  the songs, as sho wn in Figur e 1 (the \npositions of time T1 , T2 and  T3 are chang ed in generating \naudio sam ples). \n \n \n  \nFigure 1: Song Com position  \nIn Figure 2, the classificati on ste ps of musical audio are  \nshown. Here  we use six SV M classifiers (SVM 1~6) and all  \nthe classifie rs are  trai ned for 2-class cl assification. T he \ntraining and testin g data sets are shown in  table 1.  Initially  the \nmusical audi o is segm ented into 20ms fram es with variable ∇ \npercentage overlapping. Else where (Xu , et.al., 20 02) we have \nshown the ∇ =70% for training and ∇ = 20% fo r testing  work  \nwell  co mpared with  other values of ∇ (i.e. 0 < ∇ <100).  \n \n \nFigur e 2: Classification Steps  \n \n \n \n \n \n \nPerm ission to make digital or  hard copi es of all or  part of this wor k for \npersonal of classr oom use is granted without fee pr ovided that copies ar e not \nmade or  distr ibuted for  profit or com mercial advantage and that copies bear  \nthis notice and the full citation on the first page.    2003 T he Johns Hopkins  \nUniversity.  \n \nIn order to fi nd effective orders of LPCCs,  MFCCs and SPs, \nwe v ary the order of the feature and note down the  \nclassification accuracies respective. T hen select the orde r according to best classification accuracy. B oth effective  orders  \nand t he classifi cation acc uraci es of L PCCs a re noted i n row 1 \nand row 2 of Table 1, respectiv ely. Th e jo int featu re \nefficiencies a re noted in last four rows. Since  thi s \nclassification task (Fi gure 2) is non-linear (Xu, et.al., 2002), \nwe t rain radial basi s ker nel function (Vapnik, 1998) with \ndifferent variable setting which varies wi th vect or dimension \nand t he common le ngth scale  constant (CL SC). T he values  for \nCLSCs are selected via cross validation (Table 1). \nKern el -RBF Main Cla sses Detailed Individual Classes Training \nSet(minutes) Evaluation \nSet (minutes) Classifie r CLSC \nMale Vocals  (a1) 5.12 7.54 SVM 1 13 Vocal  m usic (a) \nFemale Vocals  (a2) 6.36 8.23 SVM 2 45 \nAcoustic Guitar  (b1) 4.58 6.11 SVM 3 24 Instrumental music  (b) \nHarmonicas (b2) 5.17 6.89 SVM 4 18 \nMale or F emale Vocals  (a1/ a2) 15.78 17.57 SVM 5 21 Vocal m ixed instrumental \nmusic  (c) Acoustic Guitar or H armonica  (b1/ b2) 13.19 15.24 SVM 6 18 \nTable1: T raining and E valuation Date Set \nc             C lasses \n Featu res  Featu res a-bc b-c a1-a2 b1-b2 \na1-a2 b1-b2 \n22 15 19 20 21 25 LPCC \n90.57 84.55 87.52 88.34 88.69 87.43 \n12 23 24 22 25 12 MFCC \n89.80 86.21 88.77 87.97 86.75 34.17 \n17 25 12 12 12 13 SP \n76.12 79.81 51.75 79.94 80.52 85.40 \nSTE 18.34 61.09 52.35 64.07 80.13 34.17 \nZC 26.78 69.90 85.22 85.62 73.76 66.64 \nMFCC+ LPC 92.86 88.34 90.22 90.57 90.04  \nLPC+SP     89.03 90.34 \nMFCC+ SP     88.38  \nMFCC+ ZC   89.29 88.24   \nTable 2: Feature Analysis \n3.1 Correction Algorithm for Music Sequence (CAMS) \nSince the tem poral feat ures of the m usic signals are not taken  \ninto consideration while making feature vectors, it is noticed \nthat SVM m isclassifies th e class b oundaries o f different \nmusic, whi ch is more pr onounced in separat ing cl ass b and c. \nWe have developed an algorithm that expl oits the rhythm of \nthe musical score i n order t o do bet ter classification. \nThe prominent periodicities o f the melody of many types of \nmusic may be extracted by using rhythm  of the m usic. The \nmain beat frequency  (1/ rhythm  period) of t he musical score is  \ncalculated usi ng beat  histogram described in (Tza netakis, \net.al., 2001 ) and the assumption we made is th at m inimum \nduration of a class o f music is more than the 1/2 of the rhythm \nperiod.  The key  points of t he CAMS are sum marized bel ow. \n \n• (nx, nx) , n y, and n z  are number of  feature vectors (i.e.- feature frames ) in  \nclasses C x, Cy, and C z defined by the SVM classifier . \n• Cx, Cy, and Cz are the mean vectors in the classes; C x, Cy, and C z  \n• f()is the frame index \n• N is the total number of fram es in the musical score. \n• Nth is pre- define in teger and it depen ds on the beat /rhythm  of the musical \nscale N th = (1/beat frequency) *0.5 \n \nSince number of frames (  ny) in class C y is less than N th ,  we merge those \nframes with either class C x or class C z according to two cases describe below.   \nny<N th<< ( nx, nx, nz) ≤ N    n x nx ny nz ≠ 0   \n \nCase 1 : [{f(i+j+1)~f(i+j+n x)}&{f (i-nx)~f(i)}]∈Cx  f(i+j) ∈Cy  j= 1 …ny ≤Nth \n         Th en   f(i+j)∈Cx     \n \nCase 2 : {f(i-n x)~f(i)}∈Cx, {f(i+j+1)~f (i+j+n z)}∈ Cz, f(i+j) ∈Cy, j= 1 …ny ≤Nth \nIf {eudist(Cy-Cx) ≥ eudist( Cy-Cz)} ;  \n    Th en f(i+j) ∈ Cz \nElse \n    f(i+j) ∈Cx \n \n3.2 Compari son \nTo furth er illu strate th e adv antage of the propo sed approach , \nwe c ompare the per formance of  the SVM  method with other \nmethods  including MNN (Haykin, 1998) and GM M (Bilmes. 1998). For MNN, we use 6 hidden layers with 32 nodes in \neach layer.  The classification results in Table 3 prove  that \nhierarch ical classificatio n (Figure 2), is i deal fo r multi cl ass \nclassification problem and C AMS improves post classification \nefficiency of SVM, MNN and GMM) by (2~4) % .  The SVM \nperform s better in  all th e classificatio ns than MNN and  GMM. \nBoth gender (a1-a2) classification a nd instruments (b1-b2) \nclassification in vocal m ixed instrum ental m usic (c) are  \ndifficu lt tasks compared with other classes. This is beca use of \nthe complexity o f vocal stru cture and it is more pronou nced  \nwhen fem ale vocals  are  mixed with instrum ental m usic \n(fem ale vocal s have higher order harm onics than m ale vocal s). \nC                           C lasses \n    Cla ssifie rs a-bc b-c a1-a2 b1-b2 \na1-a2 b1-b2 \nSVM 92.86 88.34 90.22 90.57 90.34 90.34 \nMNN 87.22 84.19 85.78 82.35 79.56 82.87 Classifie rs \nwithout CA MS\nGMM 88.56 82.26 80.45 87.21 81.24 83.11 \n        \nSVM 95.78 91.21 94.10 93.59 94.22 94.72 \nMNN 91.45 87.81 89.25 86.76 81.77 86.58 Classifie rs with \nCAMS \nGMM 91.98 88.56 83.18 90.13 83.46 88.95 \nTable 3: Com parison Results \n4 Conclusion and Future Work \nAlthough the test data sets we used in our experiments are not \nsufficient to general ize the very high performance o f both the \nfeature s and the SVM  classi fier, it ca n be seen that  musical \nscore is statistically sep arable with  good perform ance (ov er 85 \n%); specially main 3 classes (i.e . a, b & c). The classific ation \ncomplexity can be reduced by hierarchical classification steps. \nBy introducing CAMS we could be able to in crease th e overall \nperform ance by (3 ~4) %. One of the dra wbacks of this sys tem \nis high c omputational com plexity in calculating different \nfeature orders for differe nt classification steps. \nSeveral pro blems need to b e tack led in  the fu ture. Th e \nreduction of fe ature di mension wi th good overall performance \nand developing uncorrelated feat ures are challenging tasks. \nThe other direction is to improve t he CAMS.  By taking \nmutual in formatio n between fram es in  to consideration, we \ncan improve stability o f the CAMS. \nReferences \nBilmes, J. A. (A pril 199 8). A Gen tle Tuto rial of th e EM \nAlgo rithm and its App licatio n to Param eter Estim ation for \nGaus sian Mixture and Hidden Markov Models.  International \nCom puter Science Institute Berkeley CA, 94704 .  \n \nEronen, A. & K lapuri A. (2 000). Musical I nstrument \nReco gnition Using  Cep stral Co efficien ts and  Temporal \nFeatures, Proc. IC ASSP.   \n \nHaykin, S. (1999).  Neural Networks: 2nd edition, Pren tice \nHall.  \n \nMaddage, N.C., Xu, C.S, LEE,  C.H., Kankan halli, M. S. & \nTian, Q. (2002).  Statistical Analysis o f Musical Instruments. \n3rd IEEE PCM, Taiwan (pp 581-588).  \n \nRabiner, L. R . & J uang, B. H. (1993). Fundamentals of \nSpeech Recognition, Prentice-Hall.  Euclidean distance between \nmean vectors in class C x Cy Cz \n \nTzanetakis , G., Essl, G. & C ook, P. (2001). Automatic Music \nGenre C lassification of Audi o Signals. ISMIR. \n \nVapni k, V. (1998). Statistical Learni ng Theory : Wiley. \n \nXu, C.S, Madd age, N.C. & Tian, Q. (20 02). Support vecto r \nMachine  Learning for Musi c Disc rimination. 3rd IEEE  PCM, \nTaiwan (pp 928-935)."
    },
    {
        "title": "Features for audio and music classification.",
        "author": [
            "Martin F. McKinney",
            "Jeroen Breebaart"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1415026",
        "url": "https://doi.org/10.5281/zenodo.1415026",
        "ee": "https://zenodo.org/records/1415026/files/McKinneyB03.pdf",
        "abstract": "Four audio feature sets are evaluated in their ability to classify five general audio classes and seven pop- ular music genres. The feature sets include low-level signal properties, mel-frequency spectral coefficients, and two new sets based on perceptual models of hear- ing. The temporal behavior of the features is ana- lyzed and parameterized and these parameters are in- cluded as additional features. Using a standard Gaus- sian framework for classification, results show that the temporal behavior of features is important for both music and audio classification. In addition, classifica- tion is better, on average, if based on features from models of auditory perception rather than on standard features. 1",
        "zenodo_id": 1415026,
        "dblp_key": "conf/ismir/McKinneyB03",
        "keywords": [
            "audio feature sets",
            "classification",
            "audio classes",
            "music genres",
            "low-level signal properties",
            "mel-frequency spectral coefficients",
            "perceptual models",
            "hear",
            "temporal behavior",
            "additional features"
        ],
        "content": "Features for Audio and Music Classiﬁcation\nMartin F. McKinney\nPhilips Research Laboratories\nProf. Holstlaan 4 (WY82)\n5656 AA Eindhoven, The Netherlands\nmartin.mckinney@philips.comJeroen Breebaart\nPhilips Research Laboratories\nProf. Holstlaan 4 (WY82)\n5656 AA Eindhoven, The Netherlands\njeroen.breebaart@philips.com\nAbstract\nFour audio feature sets are evaluated in their ability\nto classify ﬁve general audio classes and seven pop-\nular music genres. The feature sets include low-level\nsignalproperties,mel-frequencyspectralcoefﬁcients,\nandtwonewsetsbasedonperceptualmodelsofhear-\ning. The temporal behavior of the features is ana-\nlyzed and parameterized and these parameters are in-\ncludedasadditionalfeatures. UsingastandardGaus-\nsian framework for classiﬁcation, results show that\nthetemporalbehavioroffeaturesisimportantforboth\nmusicandaudioclassiﬁcation. Inaddition,classiﬁca-\ntion is better, on average, if based on features from\nmodelsofauditoryperceptionratherthanonstandard\nfeatures.\n1 Introduction\nDevelopments in Internet and broadcast technology enable\nusers to enjoy large amounts of multimedia content. With\nthis rapidly increasing amount of data, users require automatic\nmethods to ﬁlter, process and store incoming data. Some of\nthese functions will be aided by attached metadata, which pro-\nvide information about the content. However, due to the fact\nthat metadata are not always provided, and because local pro-\ncessing power has increased tremendously, interest in localau-\ntomatic multimedia analysis has increased. A major challenge\nin this ﬁeld is the automatic classiﬁcation of audio and mu-\nsic (Wold et al., 1996; Spina & Zue, 1997; Scheirer & Slaney,\n1997; Scheirer, 1998; Wang et al., 2000; Zhang & Kuo, 2001;\nLi et al., 2001; Tzanetakis &Cook, 2002).\nMost audio classiﬁcation systems combine two processing\nstages: feature extraction followed by classiﬁcation. A vari-\nety of signal features have been used for this purpose, includ-\ning low-level parameters such as the zero-crossing rate, signal\nbandwidth, spectral centroid, and signal energy. Another set\nof features used, inherited from automatic speech recognizers,\nis the set mel-frequency cepstral coefﬁcients (MFCC). Typi-\nPermission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided that\ncopies are not made or distributed for proﬁt or commercial advan-\ntage and that copies bear this notice and the full citation on the ﬁrst\npage.c/circlecopyrt2003 Johns Hopkins University.cal performance of these feature sets in speech/music discrim-\nination tasks is around 95% (Toonen Dekkers & Aarts, 1995;\nScheirer&Slaney,1997;Lu&Hankinson,1998)butdecreases\nas the number of audio classes increases (Zhang & Kuo, 1998,\n2001).\nTherehasalsobeensomerecentworkonautomaticmusicgenre\ndetection. Tzanetakis&Cook(2002)combinestandardfeatures\nwithrepresentationsofrhythmandpitchcontentandshowclas-\nsiﬁcation performance in therange of 60%.\nSeveral different classiﬁcation strategies have been employed\ninthesestudies,includingmultivariateGaussianmodels,Gaus-\nsian mixture models, self-organizing maps, neural networks, k-\nnearest neighbor schemes and hidden Markov models. In some\ncases, the the classiﬁcation scheme does not inﬂuence the clas-\nsiﬁcation accuracy (Scheirer & Slaney, 1997; Golub, 2000),\nsuggesting that the topology of the feature space is relatively\nsimple. An important implication of these ﬁndings is that, per-\nhaps further advances could be made by developing more pow-\nerful features or at least understanding the feature space, rather\nthan building new classiﬁcation schemes.\nThus, our focus here is on features for classifying audio and\nmusic. We compare the two feature sets most commonly used,\nlow-level signal properties and the MFCC, with two new fea-\nture sets and evaluate their performance in the classiﬁcation of\nasetofgeneralaudioclassesandasetofpopularmusicgenres.\nWe also examine how the characterization of features’ tempo-\nral behavior can inﬂuence classiﬁcation. The two new feature\nsets, described in detail below, are based on perceptual models\nof auditory processing.\n2 Method\nWe compare four distinct feature extraction stages to evaluate\ntheirrelativeperformancewhileusingthesameclassiﬁerstage,\naGaussian-basedquadraticdiscriminantanalysis(QDA)(Duda\n& Hart, 1973). The feature sets (described below) are: (1) low-\nlevel signal properties; (2) MFCC; (3) psychoacoustic features\nincluding roughness, loudness and sharpness; and (4) an au-\nditory model representation of temporal envelope ﬂuctuations.\nThetwonewfeaturesetsintroducedinSecs.2.1.3and2.1.4are\nbased on models of human auditory processing. Each begins\nwith a bank of bandpass ﬁlters which represent the frequency\nresolution of the peripheral human auditory system. These ﬁl-\nters, termed critical band ﬁlters, reﬂect the channeling property\nof the auditory system, i.e., signals that are passed through dif-General Audio Class Classical Music Popular Music Speech NoiseCrowd Noise\nNumber of Files 35 188 31 25 31\nPopular Music Class JazzFolkElectronica R&BRockReggae Vocal\nNumber of Files 3823 27 4337 11 9\nTable 1: Audio database byclass: number of audio ﬁles in each class.\nferent critical bands are, to a large extent, processed indepen-\ndently (Glasberg & Moore, 1990).\nPreviousstudieshaveshownthat,forspeech-musicdiscrimina-\ntion,2nd-orderstatisticsoffeatures(overtime)provideabetter\nbasisforclassiﬁcationthanthefeaturesthemselves(Scheirer&\nSlaney, 1997). In addition, Peeters et al. (2002) showed that\n“dynamic features” provide a good basis for music summariza-\ntionpurposes. Hereweapplythistechniquetoaudioandmusic\ngenre classiﬁcation and include parameterized analyses of fea-\ntures’ temporal ﬂuctuationsas additional features.\nTwotypesofclassiﬁcationwereperformed,oneonasetofﬁve\ngeneral audio classes and a second on a set of popular mu-\nsic genres. The general audio classes were: classical music,\npopularmusic(non-classicalgenres),speech(maleandfemale,\nEnglish, Dutch, German and French), crowd noise (applaud-\ningandcheering),andnoise(backgroundnoisesincludingtraf-\nﬁc, fan, restaurant, nature, etc. noises). The popular music\nclass contained music from seven genres: Jazz, Folk, Electron-\nica, R&B, Rock, Reggae, and Vocal. The database used in the\ncurrent study is a “quintessential” subset of a larger database.\nTwo subjects assigned each song a genre and rated the song as\nto how well it typiﬁes the genre. Songs were selected for the\nquintessentialdatabasebasedontheoverallrating. Thenumber\nof ﬁles in each class is givenin Table 1.\n2.1 Features\n74 3 - m s  an al y s i s  f r a m e\n2 3 - m s  s u bf r am es\nF ea t ur e ex t r ac t i o n\nS u bf r am e\nf ea t ur e v e c t o r s\nF ea t ur e s pe c t r a l  an al y s i s\nF ea t ur e s el ec t i o n ( 9 be s t )S pe c t r a l  s um m ar i z a t i on  \nof  f ea t u r e s\nF i na l  f ea t ur e\nv ec t or\nFigure 1: Feature extraction method\nThe feature extraction process, illustrated in Fig. 1, includes\na summarized temporal analysis of features. Individual fea-\ntures are calculated from 23-msec half-overlapping subframes\nof audio. A power spectrum is then calculated for each feature,\nacross 64 consecutive subframe values, resulting in an overall\nanalysisframeof743msec. Thepowerspectrumisnormalized\nby the DC value and summarized by calculating the energy in\nfour bands: 1) 0 Hz (average across observations), 2) 1-2 Hz(on the order of musical beat rates), 3) 3-15 Hz (on the order\nof speech syllabic rates), and 4) 20-43 Hz (in the lower range\nof modulations contributing to perceptual roughness). The top\nnine values (determined by a separate ranking procedure for\neachfeatureset-seebelow)ofthisspectralsummarizationwere\nthen selected and used in the classiﬁcation process. This entire\nprocess was performed separately for each feature set.\n2.1.1 Low-level signal parameters\nThis feature set, based on standard low-level (SLL) signal pa-\nrameters,includes: (1)root-mean-square(RMS)level,(2)spec-\ntral centroid, (3) bandwidth, (4) zero-crossing rate, (5) spec-\ntral roll-off frequency, (6) band energy ratio, (7) delta spectrum\nmagnitude, (8) pitch1, and (9) pitch strength. This set of fea-\ntures is based on a recent paper by Li et al. (2001). [See the\npaper for mathematical details.]\nThe ﬁnal SLL feature vector consists of 36 features:\n1-9: DC values of the SLL feature set\n10-18: 1-2 Hz modulation energy of the SLL feature set\n19-27: 3-15 Hz modulation energy of the SLL feature set\n28-36: 20-43 Hz modulation energy of the SLL feature set\n2.1.2 MFCC\nThe second feature set is based on the ﬁrst 13 MFCCs (Slaney,\n1998). The ﬁnal feature vectorconsists of 52 features:\n1-13: DC values of the MFCC coefﬁcients\n14-26: 1-2 Hz modulation energy of the MFCC coefﬁcients\n27-39: 3-15 Hz modulation energy of the MFCC coefﬁcients\n40-52: 20-43 modulation energy of the MFCC coefﬁcients\n2.1.3 Psychoacoustic features\nThethirdfeaturesetisbasedonestimatesoftheperceptsrough-\nness, loudness and sharpness. Roughness is the perception of\ntemporalenvelopemodulationsintherangeofabout20-150Hz,\nmaximal at 70 Hz, and is generally thought to be a primary\ncomponent of musical dissonance (Plomp & Levelt, 1965; Ter-\nhardt, 1974). Loudness is the sensation of signal strength and\nsharpness is a perception related to the spectral density and the\nrelative strength of high-frequency energy. Estimates of these\npercepts were calculated based on current models (Zwicker &\nFastl,1990;Daniel&Weber,1997;Bismarck,1974). Temporal\nanalyses of loudness and sharpness were calculated using the\nsubframe process described above. However, because rough-\nness is based on mid-rate temporal envelope modulations, an\naccurateestimatecanonlybeobtainedforrelativelylongaudio\nframes ( >∼180msec). Thus, the temporal variation of rough-\nnesswithin anaudioframe isrepresentedby itsmeanand stan-\ndarddeviationover186-msecsubframeswith93-msecoverlap.\n1The term pitchis used here to describe an estimate of the pitch\npercept derived using an autocorrelation-based method (see Li et al.,\n2001, for details).0.97\n±0.02\n0.76\n±0.02\n0.87\n±0.07\n0.41\n±0.15\n0.96\n±0.03Real classStatic Features\nClas Pop Spch Nse CrwdClas\nPop \nSpch\nNse \nCrwd0.98\n±0.02\n0.83\n±0.03\n0.94\n±0.04\n0.6\n±0.12\n0.97\n±0.02Static and Temporal Features\nClas Pop Spch Nse CrwdClas\nPop \nSpch\nNse \nCrwd\n0.54\n±0.1\n0.84\n±0.07\n0.48\n±0.13\n0.46\n±0.11\n0.73\n±0.06\n0.33\n±0.24\n0.45\n±0.23\nClassification resultJazzFolkElctR&B RockReggVoclJazz\nFolk\nElct\nR&B \nRock\nRegg\nVocl0.64\n±0.1\n0.8\n±0.09\n0.51\n±0.15\n0.49\n±0.08\n0.76\n±0.07\n0.57\n±0.17\n0.52\n±0.22\nJazzFolkElctR&B RockReggVoclJazz\nFolk\nElct\nR&B \nRock\nRegg\nVoclFigure 2: Classiﬁcation performance using standard low-level features. Confusion matrices for classiﬁcation based on the best 9\nstaticfeatures(left)andbest9overallfeatures,includingstaticandtemporalfeatures(right). Thetoptwopanelsshowresultsforthe\ngeneralaudioclasses,thebottompanelsforthemusicgenreclasses. Thenumbersintheboxesindicatetheprobability( ±standard\nerror) that the class on the left axis is classiﬁed as the class on the bottom axis. Where numbers are not present, the shade indicates\nthe probability: white = 0, black = 1.\nTheﬁnalpsychoacoustic(PA)featurevectorconsistsof10fea-\ntures:\n1: average roughness\n2: standard deviation of roughness\n3: average loudness\n4: average sharpness\n5: 1-2 Hz loudness modulation energy\n6: 1-2 Hz sharpness modulation energy\n7: 3-15 Hz loudness modulation energy\n8: 3-15 Hz sharpness modulation energy\n9: 20-43 Hz loudness modulation energy\n10: 20-43 Hz sharpness modulation energy\n2.1.4 Auditory ﬁlterbank temporal envelopes\nThefourthfeaturesetisbasedonamodelrepresentationoftem-\nporal envelope processing by the human auditory system. Each\naudio frame is processed in two stages: (1) it is passed through\na bank of 18 4th-order bandpass GammaTone ﬁlters (Glasberg\n& Moore, 1990; Hartmann, 1997, chap. 10) spaced logarithmi-\ncally from 26 to 9795 Hz; and (2) the modulation spectrum of\nthe temporal envelope is calculated for each ﬁlter output. The\nspectrum of each ﬁlter is then summarized by summing the en-\nergy in four bands: 0 Hz (DC), 3-15 Hz, 20-150 Hz, and 150-\n1000Hz. Theparameterizedsummaryofhighmodulationrates\nis not calculated for some low-frequency ﬁlters: a modulation\nratesummaryvalueisonlycomputedforacriticalbandﬁlterif\ntheﬁlter’scenterfrequencyisgreaterthanthemaximumrateofthe band. This process yields 62 features describing the audi-\ntory ﬁlterbank temporal envelopes (AFTE):\n1-18: DC envelope values of ﬁlters 1-18\n19-36: 3-15 Hz envelope modulation energy of ﬁlters 1-18\n37-52: 20-150 Hz envelope modulation energy of ﬁlters 3-18\n53-62: 150-1000 Hz envelope modulation energy of ﬁlters 9-18\n2.2 Classiﬁcation\nClassiﬁcation of audio ﬁles was performed using quadratic dis-\ncriminate analysis (see Duda & Hart, 1973), which provided\nbetterpreliminaryresultsthanlineardiscriminateanalysis. Fea-\ntureswerecalculatedfromeachﬁleon10consecutive743-msec\nframes with a 558-msec hop-size. The feature vectors were\ngroupedintoclassesbasedonthetypeofaudioandwereusedto\nparameterize an N-dimensional Gaussian mixture model (one\nGaussianwithitsownmeanandvarianceforeachclass),where\nNis the length of the feature vector. Training and cross-\nvalidationweredoneusingthe .632+bootstrap method,anim-\nproved version of the leave-one-out bootstrap (Efron & Tibshi-\nrani, 1997, 1993). This method has been shown to provide es-\ntimates of prediction error with less variance than standard k-\nfoldcross-validationtechniques,especiallyforsmalldatabases.\nBootstrapreplicationswereperformed500timesforeachclass.\nClassiﬁcation was done per audio ﬁle and was assigned based\non the majority of 10 consecutive audio frame classiﬁcations.0.74\n±0.09\n0.9\n±0.01\n0.97\n±0.02\n0.79\n±0.07\n0.94\n±0.02Real classStatic Features\nClas Pop Spch Nse CrwdClas\nPop \nSpch\nNse \nCrwd0.89\n±0.05\n0.92\n±0.01\n0.97\n±0.02\n0.82\n±0.07\n0.97\n±0.02Static and Temporal Features\nClas Pop Spch Nse CrwdClas\nPop \nSpch\nNse \nCrwd\n0.68\n±0.08\n0.69\n±0.1\n0.44\n±0.14\n0.43\n±0.1\n0.65\n±0.08\n0.37\n±0.22\n0.73\n±0.2\nClassification resultJazzFolkElctR&B RockReggVoclJazz\nFolk\nElct\nR&B \nRock\nRegg\nVocl0.68\n±0.08\n0.83\n±0.07\n0.53\n±0.13\n0.46\n±0.09\n0.78\n±0.05\n0.54\n±0.16\n0.73\n±0.2\nJazzFolkElctR&B RockReggVoclJazz\nFolk\nElct\nR&B \nRock\nRegg\nVoclFigure 3: Classiﬁcation performance using mel-frequency cepstral coefﬁcients (MFCC): Same format as Fig. 2.\nAlthough the size of the feature sets differ, we performed clas-\nsiﬁcation using the same number of features from each set. We\nchosethebestnine2featuresfromeachsetfollowinganiterative\nranking procedure: for each feature, we estimated classiﬁca-\ntionerrorfromthe Bhattacharyyadistances (see,e.g.,Papoulis,\n1991) between classes and designated the top ranked feature as\nthat which gave the lowest error; we repeated this for 2, 3, ... 9\nfeatures.\n3 Results\n3.1 SLL feature set\nThe ranking results for the SLL feature set are shown in Ta-\nble 2. Different rankings are shown for each combination of\naudio-class set (general audio or music genre) and feature type\n(static or static-and-temporal). For general audio classiﬁcation,\nfeatures 5 (spectral roll-off frequency), 6 (band-energy ratio),\nand 1 (RMS level) rank the highest. When temporal features\nare included, features 24 (3-15 Hz modulations of band-energy\nratio), 28 (20-43 Hz modulations of RMS level), and 26 (3-15\nHzmodulationsofpitch)areincludedinthetopnine. Formusic\ngenre classiﬁcation, the top ranked features are slightly differ-\nent: feature 3 (spectral bandwidth) is the top static feature and\nfeature 19 (3-15 Hz modulations of RMS level) is included in\nthe top nine. It is clear from these results that, when available,\ntemporalmodulations(overarangeofrates)offeaturesareim-\nportant for classiﬁcation.\nClassiﬁcationresultsfortheSLLfeaturesetareshowninFig.2.\n2We limited all feature sets to their top nine features because the\nstandard low-level feature set consists of only nine basic features.Class Feature Feature Rank\nSet Type 123456789\nGeneral Static 561839247\nAudio Stat. & Temp. 561248328269\nMusic Static 351894627\nGenre Stat. & Temp. 245148631928\nTable 2: Feature ranking for the standard low level feature\nset. Feature numbers correspond to the features described in\nSec. 2.1.1.\nEach panel shows a confusion matrix that indicates the proba-\nbility( ±standarderror) ofeach audioor musicclass (leftaxis)\nbeing classiﬁed as each class in the group (bottom axis). The\ntoppanelsshowresultsforgeneralaudioclassiﬁcation,thebot-\ntom panels for music genre classiﬁcation; the left panels show\nresultsforclassiﬁcationbasedonstaticfeatures;therightpanels\nfor classiﬁcation based on both static and temporal features.\nIngeneralclassiﬁcationisbetterwhentemporalfeaturesarein-\ncluded. Although only one audio class (popular music) shows\na signiﬁcant improvement, there is only one class (folk music)\nfor which performance decreased slightly. With temporal and\nstaticfeatures,overallclassiﬁcationperformanceis 86±4%for\nthe general audio classes and 61±11% for the music genres.\nFor the general audio classes, classiﬁcation is best for classical\nmusic ( 98±2%) and worst for background noise ( 60±12%),\nwhich is confused with crowd noise and classical music. For\nthe music genres, classiﬁcation is best for folk ( 80±9%) and\nworst for R&B ( 49±8%) which is confused with electronica\nand reggae.0.69\n±0.07\n0.73\n±0.03\n0.84\n±0.06\n0.6\n±0.11\n0.94\n±0.03Real classStatic Features\nClas Pop Spch Nse CrwdClas\nPop \nSpch\nNse \nCrwd0.94\n±0.02\n0.85\n±0.02\n1\n±0\n0.89\n±0.05\n0.9\n±0.03Static and Temporal Features\nClas Pop Spch Nse CrwdClas\nPop \nSpch\nNse \nCrwd\n0.38\n±0.11\n0.67\n±0.09\n0.37\n±0.16\n0.08\n±0.1\n0.4\n±0.12\n0.69\n±0.15\n0.27\n±0.31\nClassification resultJazzFolkElctR&B RockReggVoclJazz\nFolk\nElct\nR&B \nRock\nRegg\nVocl0.63\n±0.08\n0.72\n±0.09\n0.71\n±0.09\n0.52\n±0.09\n0.69\n±0.08\n0.55\n±0.18\n0.5\n±0.2\nJazzFolkElctR&B RockReggVoclJazz\nFolk\nElct\nR&B \nRock\nRegg\nVoclFigure 4: Classiﬁcation performance using psychoacoustic features. Same format as previous ﬁgures. Results shown on the left\npanels were computed with only three static features: average loudness, roughness, and sharpness.\n3.2 MFCC feature set\nTable 3 shows the top ranked MFCC features for each audio-\nclass set and feature-type combination. For general audio clas-\nsiﬁcation, the ﬁrst three MFCCs are highly ranked. When tem-\nporal features are included, features 27 (3-15 Hz modulations\nof MFCC 1) and 49 (20-43 Hz modulations of MFCC 10) are\nincluded in the top nine. For classiﬁcation of music genres,\ntherankingsareslightlydifferent,buttheﬁrstfewMFCCsalso\nseem to be the most important. When temporal features are in-\ncluded,feature40(20-43HzmodulationsofMFCC1)isranked\nthe highest. These results show that temporal modulations of\nMFCCs are also importantfor classiﬁcation.\nClass Feature Feature Rank\nSet Type 123456789\nGeneral Static 231911512107\nAudio Stat. & Temp. 227149359117\nMusic Static 14263711913\nGenre Stat. & Temp. 4014262731113\nTable 3: Feature ranking for the MFCC feature set. Feature\nnumbers correspond to the features described in Sec. 2.1.2.\nFigure 3 shows the classiﬁcation results for the MFCC feature\nset. As with the SLL feature set, classiﬁcation is better when\nbased on the top nine static-and-temporal features, rather than\nthetopninestaticfeatures. Overallclassiﬁcationbasedonboth\nfeature types (right panels) is 92±3% for the general audio\nclasses and 65±10% for the music genres. Overall perfor-\nmance is slightly better than the SLL feature set for generalaudio classiﬁcation, largely helped by an increased ability to\nclassify background noise and popular music. Classiﬁcation of\nclassical music is, however, worse for the MFCC feature set\n(89±5% with MFCC vs. 98±2% with SLL). Overall classi-\nﬁcation performance of music genres is not signiﬁcantly better\nwithMFCCfeaturesthanSLLfeatures( 65±10%withMFCC\nvs.61±11%withSLL),althoughclassiﬁcationperformanceof\nrock music is signiﬁcantly increased with MFCC features and\nno music genres show a decrease in classiﬁcation performance.\n3.3 PA feature set\nClass Feature Feature Rank\nSet Type 123456789\nGeneral Static 413······\nAudio Stat. & Temp. 8413972106\nMusic Static 413······\nGenre Stat. & Temp. 4189376510\nTable 4: Feature ranking for the psychoacoustic feature\nset. Feature numbers correspond to the features described in\nSec. 2.1.3. Rankings 4-9 for static features are not applicable\nbecause there are only 3 static psychoacoustic features.\nTable 4 shows the top ranked psychoacoustic features for each\naudio-classsetandfeature-typecombination. Ofthethreestatic\nfeatures, feature 4 (average sharpness) is ranked highest, fol-\nlowed by features 1 (average roughness) and 3 (average loud-\nness) for both general audio and music genre classiﬁcation.\nWhen temporal features are included, the top ranked features\ninclude 8 (3-15 Hz modulations of sharpness) and 9 (20-43 Hz0.93\n±0.03\n0.89\n±0.02\n0.93\n±0.03\n0.8\n±0.07\n0.93\n±0.03Real classStatic Features\nClas Pop Spch Nse CrwdClas\nPop \nSpch\nNse \nCrwd0.94\n±0.01\n0.95\n±0.01\n0.97\n±0.02\n0.85\n±0.06\n0.91\n±0.03Static and Temporal Features\nClas Pop Spch Nse CrwdClas\nPop \nSpch\nNse \nCrwd\n0.66\n±0.07\n0.8\n±0.08\n0.65\n±0.11\n0.66\n±0.08\n0.7\n±0.07\n0.62\n±0.18\n0.81\n±0.13\nClassification resultJazzFolkElctR&B RockReggVoclJazz\nFolk\nElct\nR&B \nRock\nRegg\nVocl0.81\n±0.05\n0.84\n±0.06\n0.71\n±0.11\n0.68\n±0.07\n0.77\n±0.07\n0.61\n±0.17\n0.76\n±0.16\nJazzFolkElctR&B RockReggVoclJazz\nFolk\nElct\nR&B \nRock\nRegg\nVoclFigure 5: Classiﬁcation performance using the auditory ﬁlterbank temporal envelope. Same format as previous ﬁgures.\nmodulations of loudness).\nClassiﬁcation results for the PA feature set are shown in Fig. 4.\nAswiththepreviousfeaturesets,theinclusionoftemporalfea-\ntures increases overall classiﬁcation performance. (In this case,\nthe number of features used for classiﬁcation also increased.)\nThe overall classiﬁcation performance using both static and\ntemporal features is 92±3% for general audio classes and\n62±10%formusicgenres. Thesevaluesareroughlythe same\nas those for the MFCC feature set.\n3.4 AFTE feature set\nTable 5 shows the top ranked features from the set describing\ntheauditoryﬁltertemporalenvelope. Thestaticfeaturesarethe\nDC outputs (no modulations) of the 18 bandpass ﬁlters, which\ncover a range of center frequencies (260-9795 Hz). The top\nninestaticfeaturesforbothgeneralaudioandmusicgenreclas-\nsiﬁcation span the range of center frequencies. When temporal\nfeatures are included, several appear in the top nine rankings:\n62 (150-1000 Hz modulations of ﬁlter 18), 25 (3-15 Hz mod-\nulations of ﬁlter 7), 41 (20-150 Hz modulations of ﬁlter 5), 52\n(20-150 Hz modulations of ﬁlter 18), and 20 (3-15 Hz modu-\nlations of ﬁlter 2). As with all the other feature sets, temporal\nvariations of features are important for classiﬁcation.\nClassiﬁcation results for the AFTE feature vector are shown in\nFig. 5. Classiﬁcation based on temporal and static features is\nslightly better than on static features alone. Overall classiﬁ-\ncation performance (right panels, 93±2% for general audio\nclasses and 74±9% for music genres) is better than the other\nfeaturesets,althoughtheimprovementissigniﬁcantonlyforthe\nSLL feature set. For general audio classiﬁcation, the most dif-Class Feature Feature Rank\nSet Type 123456789\nGeneral Static 31811148765\nAudio Stat. & Temp. 36218925411443\nMusic Static 21831611217156\nGenre Stat. & Temp. 218352161201246\nTable5: FeaturerankingfortheAFTEfeatureset. Featurenum-\nbers correspond to the features described in Sec. 2.1.4.\nﬁcult classes are background noise ( 85±6%) and crowd noise\n(91±3%),whichareconfusedwitheachother. Formusicgenre\nclassiﬁcation, the most difﬁcult genre is Reggae ( 61±17%),\nwhich is most often confusedwith electronica, R&B, and rock.\nOverallclassiﬁcationresultsaresummarizedinTable6. Acom-\nparison across all feature sets shows that, given our choices\nof classes/genres, the AFTE features provide the highest mean\nclassiﬁcationratesforbothgeneralaudioandmusicgenreclas-\nsiﬁcation. Thedifferencesinoverallclassiﬁcationperformance,\nhowever, are only signiﬁcant in a few cases: AFTE-SLL for\ngeneral audio; and AFTE-PA if only static features are used. If\nwe compare classiﬁcation of speciﬁc classes, the AFTE is only\nsigniﬁcantly outperformed by other feature sets in a few cases:\nSLLfeaturesprovidebetterclassiﬁcationofclassicalmusicand\ncrowd noise; MFCC features provide better classiﬁcation of\ncrowd noise; and PA features provide better classiﬁcation of\nspeech. On the other hand, the AFTE feature set signiﬁcantly\noutperformsallotherfeaturesetsintheclassiﬁcationofpopular\nmusic (general audio class)and jazz and R&B (music genres).\nTable 6 also shows that, for every feature set, the inclusion ofStatic Features Static and Temporal Features\nSLL MFCC PA AFTE SLL MFCC PA AFTE\nGeneral Audio 80±5% 87±4% 76±5% 90±3% 86±4% 92±3% 92±3% 93±2%\nMusic Genre 55±12% 57±12% 41±14% 70±9% 61±11% 65±10% 62±10% 74±9%\nTable 6: Classiﬁcation Results Summary. Each entry gives the percent correct classiﬁcation ±standard error for the given set of\naudio classes (left column)and feature set (top rows).\ntemporal features increases the mean overall classiﬁcation per-\nformance. Thedifferencesarenotsigniﬁcantinoverallclassiﬁ-\ncation rates but they are for many individual classes.\n4 Discussion\nItiswellknownthat,foraudiosignals,temporalenvelopeﬂuc-\ntuations at speciﬁc rates play an important role in perception.\nWehaveshownherethattheexplicitinclusionofparametersde-\nscribing these modulations (not only in estimates of the tempo-\nral envelope but in other features as well) can increase the per-\nformance of audio and music classiﬁers. We have also shown\nthat a feature set based on a model of auditory perception out-\nperformsothercurrentstandardfeaturesetsintheclassiﬁcation\nof general audio and music genre.\nWhile the overall classiﬁcation performance of our general au-\ndio classes is quite high ( 93±2%), music genre classiﬁcation\nis far from perfect ( 74±9%). While this measure of perfor-\nmance may seem low, it should be pointed out that the classes\nof music genre do not always have distinct boundaries, which\nmakes their classiﬁcation a fuzzy problem. We have attempted,\nin our selection of audio ﬁles, to create an internally consis-\ntent database so that each music genre contains examples with\nsimilar audio qualities. In this manner we can evaluate which\nfeatures or properties of features are important for characteriz-\ningaudioqualitiesrelevanttomusicalgenre. Neverthelessthere\nare a number of other (non-acoustic) properties that contribute\ntolabelingapieceofmusicasaspeciﬁcgenre,includingartist,\nalbum, and record label. These aspects of genre labeling will\nnotlikelybeaccountedforinfeaturesextractedfromtheaudio.\nSo, while we are using the classiﬁcation of musical genre as a\nmeanstomeasurehowrelevantourfeaturesare,theymaynever\nbeabletodothejobperfectly. Incomparisontoresultsofother\nstudies of music genre classiﬁcation (Tzanetakis et al., 2001;\nTzanetakis & Cook, 2002), ourfeatures looks quite promising.\nSeveral limitations of the current study should be mentioned.\nOur audio database is far from complete. We have shown clear\nadvantages of particular feature sets operating on our database\nbut these methods should be performed on larger data sets for\nconﬁrmation. Inaddition,alargerdatabasewouldlikelyreduce\nvariance in our estimates of classiﬁcation performance, and al-\nlowmoreconclusivecomparisonsbetweenthedifferentfeature\nsets.\nOurassumptionofGaussian-shapedclustersinthefeaturespace\nmay not be valid. Based on reasonably favorable results, it ap-\npears that it is not a bad assumption but we have not analyzed\nthefeaturespacetothepointwherewecanquantitativelyevalu-\natethisassumption. Classiﬁcationperformancecouldbefurther\nimproved by such an analysis followed by the incorporation of\nperhaps more appropriate probability density functions.\nFurther improvements in classiﬁcation performance could alsocome from changes to the classiﬁer. For example, it is pos-\nsible that sequential classiﬁcation using fewer classes at each\nstage (i.e. grouping several classes initially) could result in\nimproved performance. One could use different features, per-\nhaps based on the Bhattacharyya distances between classes,\nfor each sequential stage. In addition, as more powerful fea-\nturesforclassdiscriminationaredeveloped,differentclassiﬁca-\ntion schemes (self-organizing maps, neural networks, k-nearest\nneighbor schemes and hidden Markov models) may begin to\nshow differences in performance.\nFinally, combinations of the best features from each set could\nalso lead to improvements in classiﬁcation performance. One\ncould rank the features across sets in the same manner that we\nrank features within each feature set, and then choose the com-\nbination that yields the bestperformance.\n5 Conclusions\nWehaveshownthataudioclassiﬁcationcanbeimprovedbyde-\nveloping and working with improved audio features. Our com-\nparison of current feature sets for this purpose shows that tem-\nporalmodulationsoffeaturesareimportantfortheclassiﬁcation\nof audio and music.\nOverall,wesawthattheAFTEfeaturesetisthemostpowerful.\nHowever, for a few particular audio classes, classiﬁcation was\nbetter with other feature sets (crowd noise: SLL and MFCC;\nclassical music: SLL; speech: PA).\nFuture work will involve the development of new features, fur-\nther analysis of the feature space to test the Gaussian assump-\ntion, examination of alternative classiﬁcation schemes, and the\nincorporation of more audio classes.\nAcknowledgments\nThe authors would like to thank Armin Kohlrausch of Philips\nResearch for helpful comments on this manuscript and Nick de\nJong and Fabio Vignoli of Philips Research for their assistance\nin building the audio database.\nReferences\nBismarck, G. von. (1974). Sharpness as an attribute of the\ntimbre of steady sounds. Acustica,30, 159-172.\nDaniel, P., & Weber, R. (1997). Psychoacoustical roughness:\nImplementationofanoptimizedmodel. Acustica ·actaacustica ,\n83, 113-123.\nDuda, R., & Hart, P. (1973). Pattern classiﬁcation and scene\nanalysis. New York: Wiley.\nEfron, B., & Tibshirani, R. (1997). Improvements on cross-\nvalidation: The .632+ bootstrap method. Journal of the Ameri-\ncan Statistical Association ,92(438), 548-560.Efron, B., & Tibshirani, R. J. (1993). An introduction to the\nbootstrap. New York: Chapman & Hall.\nGlasberg, B. R., & Moore, B. C. J. (1990). Derivation of audi-\ntory ﬁlter shapes from notched-noise data. Hearing Research ,\n47, 103–138.\nGolub, S. (2000). Classifying recorded music. Unpublished\nmaster’s thesis, University of Edinburgh. (Retrieved January\n12, 2003, from http://www.aigeek.com/aimsc/ )\nHartmann,W.M. (1997). Signals,sound,andsensation. Wood-\nbury, New York: American Institute of Physics Press.\nLi,D.,Sethi,I.K.,Dimitrova,N.,&McGee,T.(2001).Classiﬁ-\ncationofgeneralaudiodataforcontent-basedretrieval. Pattern\nRecognition Letters ,22, 533-544.\nLu, G., & Hankinson, T. (1998). A technique towards auto-\nmatic audio classiﬁcation and retrieval. In 4th International\nConference on Signal Processing. Beijing. (Retrieved Novem-\nber 3, 2002, from http://www.gscit.monash.edu.\nau/˜guojunl/icsp98-1.pdf )\nPapoulis, A. (1991). Probability, random variables and\nstochastic processes. New York: McGraw-Hill.\nPeeters, G., Burthe, A. L., & Rodet, X. (2002). Toward au-\ntomatic music audio summary generation from signal analysis.\nIn M. Fingerhut (Ed.), Proceedings of the Third International\nConference on Music Information Retrieval (p. 94-100). Paris,\nFrance.\nPlomp, R., & Levelt, W. J. M. (1965). Tonal consonance and\ncritical bandwidth. Journal of the Acoustical Society of Amer-\nica.,38(2), 548-560.\nScheirer,E.,&Slaney,M. (1997). Constructionandevaluation\nof a robust multifeature speech/music discriminator. In Pro-\nceedings of the ieee 22nd International Conference on Acous-\ntics, Speech and Signal processing (pp. 1331–1334). Munich,\nGermany. (RetrievedJanuary30,2002,from citeseer.nj.\nnec.com/scheirer97construction.html )\nScheirer, E. D. (1998). Tempo and beat analysis of acoustical\nmusical signals. Journal of the Acoustical Society of America ,\n103, 588–601.\nSlaney, M. (1998). Auditory toolbox (Tech. Rep. No.\n1998-010). Interval Research Corporation. (Retrieved 30\nJanuary, 2002 from http://rvl4.ecn.purdue.edu/\n˜malcolm/interval/1998-010 )\nSpina, M. S., & Zue, V. W. (1997). Automatic transcription of\ngeneral audio data: Preliminary analysis. In Proceedings of the\n4th International Conference on Spoken Language Processing.\nPhiladelphia, PA.\nTerhardt, E. (1974). On the perception of periodic sound ﬂuc-\ntuations (roughness). Acustica,30, 201-213.\nToonen Dekkers, R. T. J., & Aarts, R. M. (1995). On a very\nlow-cost speech-music discriminator (Tech. Rep. No. 124/95).\nEindhoven, Netherlands: Philips Research Nat.Lab. Technical\nNote.Tzanetakis, G., & Cook, P. (2002). Musical genre classiﬁca-\ntion of audio signals. IEEE Transactions on Speech and Audio\nProcessing ,10(5), 293-302.\nTzanetakis, G., Essl, G., & Cook, P. (2001). Automatic mu-\nsical genre classiﬁcation of audio signals. In Proceedings of\nthe2ndAnnualInternationalSymposiumforMusicInformation\nRetrieval. Princeton, NJ.\nWang, Y., Liu, Z., & Huang, J. C. (2000). Multimedia content\nanalysis using both audio and visual cues. IEEE Signal Pro-\ncessing Magazine ,17, 12–36.\nWold,E.,Blum,T.,Keislar,D.,&Wheaton,J. (1996). Content-\nbasedclassiﬁcation,search,andretrievalofaudio. IEEEMulti-\nmedia,Fall, 27–36.\nZhang, T., & Kuo, C. (1998). Content-based classiﬁcation and\nretrievalofaudio. In SPIE’s43rdAnnualMeeting-Conference\non Advanced Signal Processing Algorithms, Architectures, and\nImplementations VIII. San Diego.\nZhang, T., & Kuo, C. C. J. (2001). Audio content analysis for\nonline audiovisual data segmentation and classiﬁcation. IEEE\nTransactions on Speech and Audio Processing ,9, 441–457.\nZwicker, E., & Fastl, H. (1990). Psychoacoustics: Facts and\nmodels.Berlin: Springer."
    },
    {
        "title": "The dangers of parsimony in query-by-humming applications.",
        "author": [
            "Colin Meek",
            "William P. Birmingham"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1415828",
        "url": "https://doi.org/10.5281/zenodo.1415828",
        "ee": "https://zenodo.org/records/1415828/files/MeekB03.pdf",
        "abstract": "Query-by-humming systems attempt to address the needs of the non-expert user, for whom the most nat- ural query format – for the purposes of finding a tune, hook or melody of unknown providence – is to sing it. While human listeners are quite tolerant of error in these queries, a music retrieval mechanism must explicitly model such errors in order to perform its task. We will present a unifying view of existing mod- els, illuminating the assumptions underlying their re- spective designs, and demonstrating where such as- sumptions succeed and fail, through analysis and real- world experiments. 1",
        "zenodo_id": 1415828,
        "dblp_key": "conf/ismir/MeekB03",
        "keywords": [
            "query-by-humming",
            "non-expert user",
            "natural query format",
            "singing it",
            "human listeners",
            "error in queries",
            "music retrieval mechanism",
            "explicitly model",
            "perform its task",
            "unifying view"
        ],
        "content": "The dangers of parsimonyin query-by-humming applications\nColin Meek\nUniversityofMichigan\n1101BealAvenue\nAnnArborMI48109USA\nmeek@umich.eduWilliamP.Birmingham\nUniversityofMichigan\n1101BealAvenue\nAnnArborMI48109USA\nwpb@umich.edu\nAbstract\nQuery-by-humming systems attempt to address theneeds of the non-expertuser, for whomthe most nat-\nuralqueryformat–forthepurposesofﬁndingatune,hook or melody of unknown providence – is to sing\nit. While human listeners are quite tolerant of error\nin these queries, a music retrieval mechanism must\nexplicitly model such errors in order to perform its\ntask. Wewillpresentaunifyingviewofexistingmod-els, illuminating the assumptions underlyingtheir re-\nspective designs, and demonstrating where such as-\nsumptionssucceedandfail,throughanalysisandreal-worldexperiments.\n1 Introduction\nWhenauditingasungquery–orindeedanymusicalproduction\n– a trained ear can recognize certain problems: pitch drift, out\noftunenotes,rhythmerrors,unsteadytempo,andsoforth. It isquite natural for a music teacher to comment to a student that\n“the third note was ﬂat”, or “you’re speeding up in the third\nmeasure”. These two statements represent two fundamentallydifferent views of error: the ﬁrst indicates a belief that a single\nnote was “off”, and the second indicates a belief that a trend is\noccurring.\nThe two views are, however, reconcilable. The teacher could\nalso, at the expenseof clarity,assert that “youmodulateddown\nto/BY /AZmajoronthe thirdnote,andmodulatedbackto /BZmajor\non the fourth note” or “all of the notes after the third measurewere too short” respectively. Thus, it may seem reasonable in\nthe context of a query-by-humming(QBH) system to view er-\nrorsinoneoftwofundamentalways:/AFErroroccurslocally: anydiscrepancybetweenaqueryandits targetmustbeexplainedonanotebynote(orframeby\nframe)basis,thoughallowingforsomeoveralldifferences\nin register, key and tempo. This view reasonably models\nthe situation described by the ﬁrst statement (the note is\nPermission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided thatcopies are not made or distributed for proﬁt or commercial advan-tage and that copies bear this notice and the full citation on the ﬁrstpage.c/AD2003 Johns Hopkins University.\nﬂat), and accounts for the other situation indirectly (these\nnotesareall tooshort)./AFAn error is always “cumulative”: errors occur with re-spect to the context established by previous notes. This\nview reasonably models the second statement (the tempo\nincreases), and accounts for the other situation indirectly(seenas a modulationdown,thena modulationup).\nWith respect to pitch and rhythm, most existing QBH systems\nimplicitly make one or the other assumption. There are com-\npelling arguments in favor of such assumptions, particularlywith regardsto modelcomplexity. In addition,neitherassump-\ntion is fatal even if both types of error are prevalent, as the al-\nternateinterpretationsshownabovereveal.\nWhy is model parsimonydangerous? As the size of a database\nincreases, intelligently diagnosing error becomes more and\nmorecritical: ifwecanexplainaquerywithrespecttoitstarget\nin terms of one error rather than four, the group of songs thatappear“just as close” is muchsmaller. Of course,most models\ndo not simply count the number of errors in a match, but the\nintuition remains the same. In Section 5, we formalize a moregeneralformofthis observation.\n2 Errors\nA query model should be capable of expressing the following\nmusical–orun-musicalyoumightargue–transformations,rel-\nativetoa target:\n1. Insertions and deletions: adding or removing notes from\nthe target, respectively. These editsare frequently intro-\nducedbytranscriptiontoolsas well.\n2. Transposition: the querymay be sung in a different keyor\nregisterthanthetarget. Essentially,thequerymightsound\n“higher”or“lower”thanthetarget.\n3. Tempo: thequerymaybeslowerorfasterthanthetarget.\n4. Modulation: over the course of a query, the transposition\nmaychange.\n5. Tempochange: thesingermayspeeduporslowdowndur-\ninga query.\n6. Non-cumulative local error: the singer might sing a note\noff-pitchorwithpoorrhythm.2.1 Edit Errors\nInsertions and deletionsin music tend to inﬂuence surrounding\nevents (Mongeau and Sankoff, 1990). For instance, when aninsertion is made, the inserted event and its neighbor tend to\noccupy the temporal space of the original note: if an insertion\nis made and the duration of the neighbors is not modiﬁed, theunderlying rhythmic structure (the beat) is changed. Similarly,\ninsertionswilltendtomodifytheintervalliccontourofaphrase,\nto maintain the overall contour. Reﬂecting this process, we de-scribe the edit operations as “elaborations” and “joins” for in-\nsertionanddeletionrespectively,becausethe insertednotes are\nseenas embellishingthe originalparentnote,anddeletednotesresultin themergingofmultiplenotesintoa longerone.\nThis approach to edits reﬂects a natural musical interpretation.\nA pragmatic motivation for our “musical” deﬁnition of edit is\ntranscriber error. In this context, we clearly would not expecttheonsettimesorpitchesofsurroundingeventstobeinﬂuenced\nby a “false hit” insertion, or a missed note. The relationships\namongst successive events must thereforebe modiﬁed to avoidwarpingandmodulation. Reﬂecting this bias, we use the terms\n“join”and“elaboration”toreferto deletionsandinsertions,re-\nspectively.\n2.2 Transpositionand Tempo\ntimepitchScale= 1.5\nTrans=+4query\ntarget\noverlap\ntimepitch\nChange=1.5Modu=+ 2\ntimepitchRError=1.5\nPError=-1a)\nb) c)\nFigure1: Examplesoferrors\nWe account for the phenomenon of persons reproducing the\nsame“tune”atdifferentspeedsandindifferentregistersorkeys.Few people have the ability to remember and reproduce exact\npitches (Terhardt and Ward, 1982), an ability known as “ab-\nsolute” or “perfect” pitch. As such, transpositional invariance\nis a desirable feature of any query/retrieval model. The effect\nof transposition is simply to add a certain value to all pitches.Consider for example the transposition illustrated in Figure 1,\nSectiona,of/CC /D6 /CP/D2/D7= +4.\nTempo in this context is simply the translation of rhythm,\nwhich describes duration relationships, into actual time dura-\ntions. Again, it is difﬁcult to rememberand reproducean exact\ntempo. Moreover, it is very unlikely that two persons would\nchoosethesame metronomemarking,muchless unconstrainedbeat timing,foranypiece of music. This is a natural“musical”\ninterpretation. The effectofa temposcalingis simplyto multi-\nply all inter-onsetinterval(IOI) valuesby some amount,wheretheIOIisthetimebetweentheonsetsofsuccessivenotes. Thus,\nif the query is 50% slower than the target, we have a scaling\nvalueof/CC /CT/D1/D4/D3 /BP/BD /BM /BH,as showninFigure1,Sectiona.2.3 Modulationandtempochange\nThroughouta query, the degree of transposition or tempo scal-\ning can change, referred to as modulation andtempo change ,\nrespectively. Consideraquerybeginningwiththeidentitytrans-\nposition /CC /D6 /CP/D2/D7 /BP/BCandidentitytemposcaling /CC /CT/D1/D4/D3 /BP/BD,as\nin Figure 1, Section b. When a modulation or tempo change isintroduced, it is always with respect to the previous transposi-\ntion and tempo. For instance, on the third note of the example,\na modulationof/C5 /D3/CS/D9 /BP/B7 /BEoccurs. For the remainderof the\nquery,the transpositionis equal to /BC/B7/BE /BP /B7 /BE , from the start-\ningreferencetranspositionof0. Similarly,thetempochangeof/BV /CW/CP/D2/CV /CT /BP /BD /BM /BHon the second note means that all subsequent\neventsoccurata temposcalingof /BD /A1 /BD /BM /BH/BP/BD /BM /BH.\nOriginal (transposed)\nQuerymodulation\nlocal pitch error\nFigure 2: Portion of a query on the American National\nAnthem\n2.4 LocalPitchand IOI Errors\nIn addition to the “gross” errors we have discussed thus far,\nthere are frequently local errors in pitch and rhythm. Theseerrors are relative to the modiﬁcations described above. A lo-\ncal pitch error of/A1\n/B4 /C8 /B5simply adds some value to the “ideal”\npitch, where the ideal is determined by the relevant target noteand the current transposition. A local IOI error of/A1\n/B4 /CA /B5has a\nscalar effect on the ideal IOI, derived from the relevant target\nnote and the current tempo. Figure 1, Section c, shows exam-ples of each error. Note that these errors do not propagate to\nsubsequent events, and as such are termed non-cumulative or\nlocalerrors. Transposition and tempo change are examples of\ncumulative error.\nIn some cases, there are multiple interpretations for the source\noferrorinaquery. ConsiderforinstanceFigure2,whichshows\naspeciﬁcinterpretationofthreedisagreementsbetweenatargetand query. The second note in the query is treated as a local\npitch error of -1. The ﬁnal two notes, which are a semi-tone\nsharperthanexpected(+1),areexplainedas a modulation. Theerrormodel,describedinthenextsection,considersallpossible\ninterpretations, for instance considering the possibility that the\nerror in the second note is accounted for by two modulations(before and after), and the ﬁnal two notes by a pair of local\nerrors. Depending on our expectation that such things might\noccur,one or the otherinterpretationmight appear morelikely.\nIn general,we would preferto ﬁnd the most direct possible ex-\nplanations for queries, since an increased likelihood of error inthe model can be shown to reduce discrimination (Meek and\nBirmingham,2002a).\n3 Existing error models\nFor edits, we assume, like Mongeau and Sankoff (1990), thatoverallrhythmis maintained,and make the natural musical as-\nsumption that edits have a local impact on pitch. Many QBHapplications adopt this approach to rhythm (Mazzoni, 2001;\nMeek and Birmingham, 2002b; Pauws, 2002; McNab et al.,\n1997,1996).\nIn this study, we are concerned primarily with the distinctionbetween local and cumulative error. Far less is known about\nthis area. This is largely a matter of convenience: a particular\nmusical representationwill tend to favoroneapproachoverthe\nother. For instance, we can adopt a pitch- and tempo-invariantrepresentation, using pitch interval and inter-onset interval ra-\ntio(Pauws,2002;Shifrinetal.,2002). This relativerepresenta-\ntionestablishes a newtranspositionandtempocontextforeachnote,thusintroducingtheimplicitassumptionthatallerrorsare\ncumulative(PardoandBirmingham,2002). Pollastri(2001)de-\ntermined that cumulative error is in fact far less common than\nlocalerror,aconclusionsupportedbyourstudies.\nAnotherapproachto thedifferencesin transpositionandtempo\ncontextistoattemptmultiplepassesoveraﬁxedcontextmodel,\nand evaluate error rigidly within each pass by comparing thequery to various permutations of the target. Dynamic time-\nwarping approaches (Mazzoni, 2001) and non-distributed hid-\nden Markov model techniques (Sorsa, 2001) are well-suited to\nthisapproach. However,itisnotpossibletomodel,forinstance,\na modulation, using these methods, only local error. Prelimi-narywork(Wigginsetal.,2002)usesasimilarapproach,group-\ningtogether“transpositionvectors”connectingqueryandtarget\nnotes. Such approaches are amenable to extensions supportingcumulativeerroraswell,buthavenot–toourknowledge–been\nextendedin thisway.\nChai (2001) normalizes the tempo of the query by either au-\ntomated beat-tracking, a difﬁcult problem for short queries,\nor, more effectively, by giving the querier an audible beat to\nsing along with – a simple enough requirement for users with\nsome musical background. Again, there is an assumption thatthe transposition will not change during a query, but the beat-\ntrackercanadapttochangingtempi.\n3.1 AlternativeapproachesWe are concerned primarily with sequence based approaches\nto music retrieval. Shifrin et al. (2002) relax this assumption\nsomewhat,bytranslatingtargetsintoMarkovmodelswherethe\nstate is simply a characteristic relationship between consecu-tive notes, allowing for loops in the model. Downie (1999);\nTseng(1999)modelmusicas acollectionofnote/D2-grams,and\napply standard text retrieval algorithms. In query-by-hummingsystems, the user is searching for a song that “sounds like...”\nrather than a song that is “about” some short snippet of notes,\nif it makes sense to discuss music in these terms at all\n1.F o r\nthis reason, we believe that sequence-based methods can more\naccuratelyrepresentmusicinthis context.\n4 Johnny Can’t Sing (JCS): Aunifying model\nWe havedevelopedasystemsupportingthesimultaneousmod-elling of local and cumulative error known as “Johnny Can’t\nSing” (Meek and Birmingham, 2002b). This system providesa unique opportunityto examine the effectiveness of these two\napproaches, both in isolation and together. A detailed descrip-\ntion of the training and matching algorithms used by JCS canbefoundina technicalreport(MeekandBirmingham,2002a).\nJCS is essentially an extendedhiddenMarkov model (Rabiner,\n1989) (HMM), which associates the notes in a query with the\nnotes in a target through a sequence of hidden states . The\nfundamentalerrors(transpositionandtempodifference)recom-\n1Beethoven’s FifthSymphony isa notable exceptionmend a fairly detailed state deﬁnition to describe this relation-ship. Each alignment of target and query notes must be con-\nsidered in each of the possible tempo and transposition con-\ntexts. Consider for instance an octave-invariant representation(for instance, pitch-class): there are twelve possible transposi-\ntions, given semi-tone quantization. Further, we must model\ntempodifferences. Considerarhythmquantizationschemethatallows for nine tempo mappings. In a song with/D2notes, there\nare thus /BD/BE /A1 /BL /A1 /D2states, ignoringthe various alignment or edit\npermutations.\nIn Figure 3.A, the conventional HMM dependency structure is\nshown. The hidden states ( /CB), are each deﬁned by a tuple,/D7/CX\n/BP /CW /BX /CJ /CX /CL /BN/C3 /CJ /CX /CL /BN/CB\n/BC/CJ /CX /CL /CX,andaccordingtotheﬁrst-orderMarkov\nassumption,thecurrentstatedependsonlyonthepreviousstate./BX /CJ /CX /CLis the “Edit” type associated with the state, deﬁning the\nwayinwhichqueryandtargetnotes“lineup”. /C3 /CJ /CX /CListhe“Key”\ncomponent, or the transposition relating the pitch in the target\nto the pitch in the query. /CB\n/BC/CJ /CX /CLis the “Speed”, or the tempo\nmappingin thetransformation.\nObservations ( /C7) are assumed to depend only on the hidden\nstate, and are deﬁned by /D3/D8= /CW /C8 /CX/D8/CR/CW/BN /CA /CW/DD /D8/CW/D1 /CX= /CW /C8 /CJ /D8 /CL /BN/CA /CJ /D8 /CL /CX.\nGiventhis view ofthe queryworld,we needto determine– us-\ningmachinelearningtechniquesorbyarduoushand-labelling–\nthe probability of each combination of pitch and rhythm in thequery observation given each combination of alignment, trans-\npositionandtempoin thehiddenstate.\nIt quickly becomes infeasible to explicitly model each of these\nstates. Distributed state representations help control this com-plexity. The idea is to assume some degree of independence\nbetween the components of a model. The second view isolates\nthe componentsofa hiddenstate andthecomponentsofanob-servation (Figure 3.B), and illustrates a more reasonable inter-\npretationofthe dependenciesbetweenthese components. Only\nthe previous edit information (/BX) determines the likelihood of\nvariouslegalextensionstothealignment. Thetransposition( /C3)\ndepends on both the previoustransposition and the currentedittype,sincethedegreeofmodulationandthecurrentpositionin\nthetargetinﬂuencetheprobabilityofarrivingatsometransposi-\ntion level. A pitchobservation(/C8) dependsonlyon the current\nedit-typeandthecurrenttransposition,whichtelluswhichpitch\nweexpecttoobserve: the“emission”probabilityisthensimply\nthe probability of the resulting error, or discrepancy betweenwhatweexpectandwhatwesee. Thereisasimilarrelationship\nbetween the edit-type (/BX), tempo ( /CB\n/BC), and rhythmobservation\n( /CA).\nA simple example illustrates the musical meaning of these ele-\nments. Considerthe stateofthe modelwhere /BXrelates thejoin\noftheﬁrsttwotargetnotestoaquerynote, /C3isatransposition\nof +2 semitones, and /CB\n/BCis a tempo scaling of 1.25. The se-\nquence of transformations corresponding to these components\nof state is shown in Figure 4, starting from the original target\nnotes. The resulting transformed event is compared with thequeryevent(showninblack),whichissaidtohaveapitcherror\nof+1andarhythmerror,expressedas afactor,of0.8.\n5 Analysis\nTo maintaingeneralityin our discussion,and drawconclusionsnot speciﬁc to ourexperimentaldata orapproachto note repre-\nsentation, it is useful to analyze model entropy with respect toS:\nO:...E:\nK:S':P:R: ...A. B.\nFigure 3: A possible dependency scheme for a distributed\nstate representation.\nE=``join 2'' K=``tranpose +2'' S'=``tempo 1.25''\nFigure4: Interpretationofstate\ncumulativeandlocalerror. Intuitively,theentropymeasuresour\nuncertaintyaboutwhatwillhappennextinthequery. Formally,\nthe entropy value of a process is the mean amount of informa-tionrequiredtopredictitsoutcome. Whentheentropyishigher,\nwe will cast a wider net in retrieval, because our ability to an-\nticipatehowthesingerwill erris reduced.\nWhat happens if we assume cumulative error with respect to\npitch when local error is in fact the usual case? Consider the\nfollowingsimpliﬁed analysis: assume that two notes are gener-\nated with pitch error distributed according to a normal Gaus-sian distribution, where/CGis the random variable represent-\ning the error on the ﬁrst note, and /CHrepresents the second.\nTherefore we have: /CU/CG\n/B4 /DC /B5 /BP /C8 /B4 /CG /BP /DC /B5 /BP\n/BD\n/D4\n/BE /AP\n/CT\n/A0 /DC\n/BE\n/BEand/CU/CH\n/B4 /DD /B5/BP /C8 /B4 /CH /BP /DD /B5/BP\n/BD\n/D4\n/BE /AP\n/CT\n/A0 /DD\n/BE\n/BE. What is thedistributionover\ntheerroronthe interval?I f /CIis therandomvariablerepresent-\ning the interval error, we have: /CI /BP /CH /A0 /CG. Since /CU/CG\n/B4 /DC /B5is\nsymmetrical about /DC /BP/BC, where /A3is the convolutionoperator,\nwehave: /CU/CI\n/B4 /DE /B5/BP /CU/CG\n/A3 /CU/CH\n/B4 /DE /B5/BP\n/BD\n/D4\n/BG /AP\n/CT\n/A0 /DE\n/BE\n/BG,whichcorresponds\nto a Gaussian distribution with variance /AR\n/BE/BP/BE(as compared\nwithavarianceof /AR\n/BE/BP/BDforthelocalerrordistribution). Given\nthis analysis, the derivative entropy for local error is equal to/BD\n/BE\n/B4/D0/D3/CV /B4/BE /AP/AR\n/BE/B5/B7 /BD /B5 /AP /BD /BM /BG/BE, and the derivative entropy of the\ncorresponding cumulative error is roughly 1.77. The underly-ing distributions are shown in Figure 5. It is a natural intuition\nthat when we account for local error using cumulative error –\nas is implicitly done with intervallic pitch representations– we\nﬂattenthe errordistribution.\nWhileexperimentalresultsindicatethatlocalerrorismostcom-\nmon,sweepingcumulativeerrorundertherug canalso bedan-\ngerous,particularlywith longerqueries. When we use local er-rortoaccountforasequenceofnormallydistributedcumulative\nerrorsrepresentedbytherandomvariables/CG/BD\n/BN/CG/BE\n/BN/BM/BM/BM /BN/CG/D2,the\nlocal error ( /CI) must absorb the sum over all previous cumula-\ntive errors : /CI /BP\n/C8/D2/CX /BP/BD\n/CG/CX. For example, when a user sings\nfour consecutive notes cumulatively sharp one semi-tone, theﬁnal note will be, in the local view, four semi-tones sharp. If\ncumulative error is normally distributed with variance/AR\n/BE, the\nexpected distribution on local error after /D2notes is normally\ndistributedwith variance /D2/AR\n/BE(astandardresultforthe summa-\ntionofGaussian-distributedrandomvariables). Assuch,evena\nlow probability of cumulative error can substantially effect theperformanceofa purelylocalmodeloverlongerqueries.\nThe critical observation here is that each simplifying assump-\ntionresults inthe compoundingoferror. Unless theunderlying\nerror probability distribution corresponds to an impulse func-\ntion(implyingthatnoerroris expected),thesummationofran-dom variables always results in an increase of entropy. Thus,\nwecanviewtheseresultsasfundamentaltoanyretrievalmech-\nanism.\n6 Results\n6.1 Experimentalsetup\n160 queries were collected from ﬁve people – who will be de-\nscribedassubjectsA-E,noneinvolvedinMIRresearch. Subject\nA is a professional instrumental musician, and subject C hassome pre-college musical training, but the remaining subjects\nhavenoformalmusicalbackground. Eachsubjectwas askedto\nsing eight passages from well-known songs. We recorded fourversions of each passage for each subject, twice with reference\nonly to the lyrics of the passage. After these ﬁrst two attempts,\nthe subjects were allowed to listen to a MIDI playback of that\npassage – transposed to their vocal range – as many times as\nneeded to familiarize themselves with the tune, and sang thequeriestwomoretimes.\n6.2 Training\nJCS can be conﬁgured to support only certain kinds of er-\nror. For instance, it can be told to assume that only local er-\nror occurs, or only cumulative error. Regardless of the setup,JCS uses a training algorithm based on the Baum-Welch re-\nestimation approach(Baum andEagon,1970;Meek and Birm-\ningham,2002a). Thisapproachlearnsparametersthat maximize\nthe expectationof the training examples, which intuitively cor-\nrespondstoourgoalofﬁndingthemostdirectexplanationpos-\nsible for the errors that occur in a collection of queries. It canbe shown that the procedure converges to a distribution deter-\nmined by the frequency of the events being modelled, though\nthe “events” in the hidden layer can only be interpreted indi-\nrectly. Because of the multiple hypothesis problem in the hid-\nden layer, the optimization procedure converges to only localmaxima in the search space, but by appropriately seeding the\nalgorithm – for instance with data found by hand-labelling the\ntraining data, and with random restarts – we can ﬁnd a consis-tentandefﬁcientcharacterizationoferror.\nThe results of this training, for three versions of the model\nover the full set of 160 queries, are shown in Figure 6, which\nindicates the overall parameters for each model. For all ver-sions, Mongeau-Sankoff-styleconsolidationand fragmentation\nare employed and result in a similar distribution: the probabil-\nity of no edit is roughly 0.85, the probability of consolidationis 0.05 and the probability of fragmentation is 0.1. These val-\nues are related primarily to the behaviorof the underlyingnote\nsegmentationmechanism.\nIn one of the models, both local and cumulative error are con-\nsidered,labelled“Full”intheﬁgure. Constrainedversions,with\nthe expected assumptions, are labelled “Local” and “Cumula-\ntive” respectively. It should be apparent that the full modelpermits a tighter distribution overlocal error(rhythmerrorand\npitch error) than the simpliﬁed local model, and a tighter dis-\ntributionovercumulativeerror(tempochangeandmodulation)thanthesimpliﬁedcumulativemodel.\n−4 −3 −2 −1 0 1 2 3 400.050.10.150.20.250.30.350.4\nLocal error\nCumulative error\nFigure5: Assumingcumulativeerrorwhenerroris local\n−2 −1 0 1 200.10.20.30.40.50.60.7Rhythm error probs\nError (quant. units)ProbLocal\nFull\n−4 −2 0 2 400.20.40.60.81Tempo change probs\nChange (quant. units)ProbCumu.\nFull\n−5 0 5 1000.20.40.60.8Pitch error probs\nError (semitones)ProbLocal\nFull\n−5 0 500.20.40.60.81Modulation probs\nChange (semitones)ProbCumu.\nFull\nFigure6: Result oftraining\nWhen JCS has the luxury of considering both cumulative and\nlocal error, it converges to a state where cumulative error is\nnonetheless extremely unlikely (with probability 0.94 there is\nno change in tempo at each state, and with probability of 0.93there is no modulation), which strengthens the view espoused\nbyPollastri (2001)thatlocal erroris indeedthecriticalcompo-\nnent. This ﬂexibility however allows us to improve our ability\nto predictthe localerrorsproducedbysingers, as evidencedby\nthe sharper distribution as compared with the purely local ver-sion. Thepracticalresultisthatthefullmodelisableto explain\nthequeriesintermsofthefewesterrors ,andconvergestoastate\nwherethequerieshavethehighestexpectation.\n6.3 Retrievalperformance\nGiven the analysis in Section 5, it is interesting to consider the\neffects on retrieval performance when we assume that only lo-cal, only cumulative, or both types of error occur. To this end,\nwe generated a collection of 10000 synthetic database songs,\nbased on the statistical properties (pitch intervals and rhythmicrelationships)ofa300piececollectionofMIDIrepresentations\nofpopularandclassicalworks. Inourexperiments,wecompare\nseveralversionsofJCS:\n1. ‘Full’ model: this version of JCS models both local and\ncumulativeerror.\n2. ‘Restricted’model: a versionofthe full modelwhich lim-\nits the range of tempo changes and modulations (/A6 /BG/BC/B1\nand /A6 /BDsemitone respectively). This seems like a reason-\nable approachbecause trainingrevealsthat largercumula-\ntiveerrorsareextremelyinfrequent.3. ‘Local’model: onlylocalerroris modelled.\n4. ‘Cumulative’model: onlycumulativeerroris modelled.\nWe ﬁrst randomly divided our queries into two sets for train-\ning the models and testing respectively. After training each ofthe models on the 80 training queries, we evaluated retrieval\nperformance on the remaining 80 testing queries. In evaluat-\ning performance, we consider the rank of the correct target’smatch score, where the score is determined by the probability\nthat each database song would “generate” the query given our\nerrormodel. In case of ties in the match score, we measure theworst-case rank: the correct song is counted below all songs\nwith an equivalent score. In addition to the median and mean\nrank,weprovidethemeanreciprocalrank(MRR):thisisamet-\nricusedbyTREC(VoorheesandHarman,1997)tomeasuretext\nretrieval performance. If the ranks of the correct song for eachqueryinatestsetare/D6/BD\n/BN/D6/BE\n/BN/BM/BM/BM/BN/D6/D2,theMRRisequalto,asthe\nnamesuggests:\n/BD\n/D2\n/C8/D2/CX /BP/BD\n/BD\n/D6/CX.\nThe distribution of ranks is summarized in Figure 7. The rank\nstatistics areas follows:\nFull\nRestricted\n Local\nCumulative\nMRR\n 0.7778\n 0.7602\n 0.7483\n 0.3093\nMedian\n 1\n 1\n 1\n 68.5\nMean\n 490.6\n 422.9\n 379.5\n 1861\n1 2−10 11−100 101−1000 1001−10000 10001−100000010203040506070\nRankNumber of queriesFull\nRestrictedLocalCumulativeSimple HMM\nFigure7: Distributionofranksoverrealqueries\n<12 12−14 14−17 >1700.10.20.30.40.50.60.70.80.91\nQuery lengthMRR\nFull\nRestrictedLocalCumulative\nFigure 8: Retrieval performance as a function of query\nlength\nThe cumulative error model performs quite poorly in compar-\nison with the other approaches, owing to the prevalence of lo-\ncal error in our query collection. We see little evidence of thereverse phenomenon: notice that restricting or ignoring cumu-\nlative error does not have a notable impact on retrieval perfor-\nmance except on the longest queries, where MRR decreases as\nwe diminish the contribution of cumulative error. Figure 8 in-dicates this trend, where each group represents the aggregate\nstatistics for a roughly equally-sized subset of the test queries,\ngrouped by length. These results agree with the basic entropyanalysis,whichpredictsgreaterdifﬁcultyfor‘local’approaches\nonlongerqueries.\nIt is informative to examine where JCS fails. We identify two\nclasses offailure:/AFAlignment assumption failure: This is the most common\ntype of error. JCS assumes that the entire query is con-\ntained in the database. When the segmenter misclassiﬁesregionsbeforeandafterthequeryproperas notes,this sit-\nuation arises. JCS must to explain the entirequery in the\ncontext of each target, including these margins. JCS does\nhowever model such added notes withinthe query, using\ntheelaborationoperation./AFEntropy failure: errors are so prevalent in the query thatmany target to query mappings appear equally strong. In-terestingly, we achieve solid performance in many cases\nwhere the queries are – subjectively – pretty wildly off\nthe mark. While using a different underlying representa-tion might allow us to extract additional useful informa-\ntion fromqueries, this does notalter the fundamentalcon-\nclusions drawn about retrieval behavior with different ap-\nproachestoerror.\n7 Conclusions\nWe have demonstrated that various assumptions about the na-\nture of errors in retrieval models can have a serious impact onperformance, both in the general case through analysis, and in\nthe speciﬁc case of the query representation used by JCS. De-\nsigners of QBH systems should consider these important inter-actions.\nThealignmentassumptionfailure,whichwilllikelyprovemore\nserious in experiments with less strict controls, warrants a re-\nthinking of our assumptions about where queries come from,andsuggest a shift to local-alignmentapproaches,or variations\nthereof. In addition, it would be useful to broadenthe scope of\nthis work by examining the effects of various representations,for instance using un-quantized and un-segmented views of a\nquery.\nAcknowledgements\nWe gratefullyacknowledgethesupportof theNationalScienceFoundationundergrantIIS-0085945.Theopinionsinthispaper\naresolelythoseoftheauthorsanddonotnecessarilyreﬂectthe\nopinions of the funding agencies. We also thank Bryan Pardo\nandGregWakeﬁeld fortheircommentsandsuggestions.\nReferences\nBaum,L.E.andEagon,J.A.(1970).Amaximizationtechnique\noccurringin the statistical analysis of probabilistic functions\nofmarkovchains. Annalsof MathematicalStatistics ,41:164\n–171.Chai, W. (2001). Melody retrievalon the web. Master’s thesis,\nMassachussetts InstituteofTechnology.\nDownie, S. (1999). Evaluatinga simple approachto music in-\nformationretrieval: conceivingmelodicn-gramsastext .PhD\nthesis, UniversityofWesternOntario.\nMazzoni, D. (2001). Melody matching directly from audio. In\nProceedings of International Symposium on Music Informa-\ntionRetrieval .\nMcNab, R., Smith, L., Bainbridge, D., and Witten, I. (1997).\nThenewzealanddigitallibraryMELodyinDEX. D-LibMag-\nazine.\nMcNab,R.J.,Smith,L.A.,Witten,I.H.,Henderson,C.L.,and\nCunningham,S.J.(1996). Towardsthedigitalmusiclibrary:\nTuneretrievalfromacousticinput.In DigitalLibraries ,pages\n11–18.\nMeek, C. and Birmingham, W. (2002a). Johnny can’t sing.\nTechnicalReportCSE-TR-471-02,UniversityofMichigan.\nMeek, C. and Birmingham, W. (2002b). Johnny can’t sing: A\ncomprehensive error model for sung music queries. In Pro-\nceedings of International Symposium on Music Information\nRetrieval,pages124–132.\nMongeau, M. and Sankoff, D. (1990). Comparison of musical\nsequences. Computers andtheHumanities ,24:161–175.\nPardo, B. and Birmingham, W. (2002). Timing information\nformusical querymatching. In Proceedingsof International\nSymposiumonMusicInformationRetrieval .\nPauws,S.(2002).Cubyhum: afullyfunctional,“querybyhum-\nming” system. In Proceedings of International Symposium\nonMusicInformationRetrieval .\nPollastri, E.(2001). Anaudiofrontendforquery-by-humming\nsystems. In ProceedingsofInternationalSymposiumonMu-\nsic InformationRetrieval .\nRabiner,L.R.(1989). Atutorialonhiddenmarkovmodelsand\nselected applications in speech recognition. In Proceedings\nofIEEE,volume77(2),pages257–286.\nShifrin, J., Pardo, B., Meek, C., and Birmingham, W. (2002).\nHmm-basedmusical queryretrieval. In Proceedingsof Joint\nConferenceonDigitalLibraries .\nSorsa,T.(2001). Melodicresolutionin musicretrieval. In Pro-\nceedings of International Symposium on Music InformationRetrieval.\nTerhardt,E. andWard, W. (1982). Recognitionof musicalkey:\nExploratorystudy. JournaloftheAcousticalSocietyofAmer-\nica,72:26–33.\nTseng,Y.(1999). Content-basedretrievalformusiccollections.\nInACMSpecialInterestGrouponInformationRetrieval .\nVoorhees, E. M. and Harman, D. K. (1997). Overview of the\nﬁfth text retrieval conference. In The Fifth Text REtrieval\nConference .\nWiggins, G., Lemstrom, K., and Meredith, D. (2002).\nSia(m)ese: An algorithm for transposition invariant, poly-\nphonic content-based music retrieval. In Proceedings of In-\nternationalSymposiumonMusicInformationRetrieval ."
    },
    {
        "title": "An auditory model based transriber of vocal queries.",
        "author": [
            "Tom De Mulder",
            "Jean-Pierre Martens",
            "Micheline Lesaffre",
            "Marc Leman",
            "Bernard De Baets"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1416492",
        "url": "https://doi.org/10.5281/zenodo.1416492",
        "ee": "https://zenodo.org/records/1416492/files/MulderMLLB03.pdf",
        "abstract": "In this paper a new auditory model-based transcriber of vocal melodic queries is presented. Our experi- ments show that the new system can transcribe que- ries with an accuracy between 76 % (whistling) and 85 % (singing with syllables), and that it outperforms four state-of-the-art systems it was compared with. 1",
        "zenodo_id": 1416492,
        "dblp_key": "conf/ismir/MulderMLLB03",
        "keywords": [
            "auditory model-based transcriber",
            "vocal melodic queries",
            "accuracy between 76% and 85%",
            "state-of-the-art systems",
            "whistling",
            "singing with syllables",
            "transcription",
            "experiments",
            "outperforms",
            "system"
        ],
        "content": "AnAuditoryModelBasedTranscriber ofVocalQueries\nTomDeMulder,Jean-PierreMartens\nELIS, Ghent University\nSint-Pietersnieuwstraat 41\nB-9000 Gent (Belgium)\ntdmulder@elis.ugent.beMicheline Lesaffre,MarcLeman\nIPEM, Ghent University\nBlandijnber g2\nB-9000 Gent (Belgium)\nmarc.leman@ugent.beBernardDeBaets,HansDeMeyer\nKERMIT ,Ghent University\nCoupure Links 653\nB-9000 Gent (Belgium)\nbdebaets@ugent.be\nAbstract\nInthispaper anewauditory model-based transcriber\nofvocal melodic queries ispresented. Our experi-\nments showthatthenewsystem cantranscribe que-\nrieswith anaccurac ybetween 76%(whistling) and\n85%(singing with syllables), andthatitoutperforms\nfour state-of-the-art systems itwascompared with.\n1Introduction\nNowadays, allexisting QBH systems seem toconsist oftwo\nparts: (i)anacoustic front-end totranscribe theacoustic input\ninto anote sequence, and(ii)apattern matching back-end to\nsearch inadatabase forthemusical piece bestmatching thisse-\nquence. This paper focuses onthedevelopment andevaluation\nofanewacoustic front-end.\n2Anewacousticfront-end\nThe newfront-end isanextension oftheonepreviously des-\ncribed in(Clarisse 2002). Theembedded auditory model now\nhastwopitch extractors working inparallel: (i)AMPEX (see\nVanImmerseel 1992, Clarisse 2002) which performs atemporal\nanalysis oftheindividual auditory nervepatterns, and(ii)SHS\nwhich performs ananalysis oftheauditory spectrum andwhich\nisinspired bytheSub-Harmonic Summation theory ofTerhardt\netal.(Terhardt 1982). Per10msframe theauditory model gen-\nerates adiscrete auditory spectrum (40channels) plus AMPEX\nandSHS (pitch,e vidence) pairs.\n2.1Acombination oftwopitchextractors\nIfthepitch ofaperiodic signal issufﬁciently low,theauditory\nnervepatterns inmost auditory channels will exhibit periodic\npatterns emer ging from interactions between harmonics ofthis\npitch (VanImmerseel 1992). These patterns canbeanalyzed\ninthetime domain. However,ifthepitch gets higher ,fewer\nchannels exhibit periodic patterns originating from thepitch.\nOntheother hand, ifconsecuti veharmonics appear indifferent\nchannels, theygiverisetomaxima intheauditory spectrum, and\nPermission tomakedigital orhard copies ofallorpartofthiswork\nforpersonal orclassroom useisgranted without feeprovided that\ncopies arenotmade ordistrib uted forproﬁt orcommercial advan-\ntage andthatcopies bear thisnotice andthefullcitation ontheﬁrst\npage. c\n\u00002003 Johns Hopkins University .thepitch willemer gefrom thepositions ofthese maxima. The\nlatter calls forananalysis inthefrequenc ydomain.\nAteach frame \u0001,ourSHS pitch extractor generates aset\u0002\u0004\u0003\u0006\u0005\b\u0007\n\t\f\u000b\u000e\r\u0001\u0010\u000f\u0012\u0011\u0014\u0013\n\u000b\u0015\r\u0001\u0010\u000f\u0017\u0016(with \u0018\n\u0005\b\u0019\u0011\u001b\u001a \u001a \u0011\u0014\u001c\n\u0003)offrequencies and\namplitudes oftones that arepresumed tobepresent inthat\nframe. Itdoes sointhree steps: (a)search forsalient maxima\ninthediscrete auditory spectrum, (b)reﬁne thepositions of\nthese maxima byparabolic ﬁtting, (c)iftheauditory spectrum\ninthevicinity ofsuch aposition resembles that caused bya\npure tone, addthemaximum position (converted toHz) and\namplitude to\n\u0002\u0004\u0003.Once thetone sets\n\u0002\u001d\u0003\u000e\u001e\u001d\u001f\u0011\n\u0002\u0004\u0003and\n\u0002\u0004\u0003! \"\u001fare\navailable, theyareused tocompute apitch estimate forframe\u0001.This isaccomplished asfollo ws:\n1.Select each\n\t#\u000b$\r\u0001\u0010\u000fand itsﬁrst ﬁvesub-harmonics as\npotential pitch candidates (provided theyfallintherange\nfrom 350to4000 Hz).\n2.Foreach candidate\n\t%\r\u0001\u0010\u000f,compute itsevidence as\ntheweighted mean oftheamplitudes ofallthetones\nin\n\u0002\u0004\u0003\u000e\u001e\u001d\u001f\u001a \u001a\n\u0002\u0004\u0003! \"\u001fthat coincide with aharmonic of\t%\r\u0001\u0010\u000f:frequencies\n\t&\u001fand\n\t\"'aresaid tocoincide if(\t#\u001f*)+\t\"'\n( ,\u0015(\t#\u001f.-/\t\"'\n(#0\b1\u00172.Ifthere iscoincidence with\nharmonic 3theweight is 4\u001d5.\n3.Forthemost evident pitch candidate\n\t,recompute its\nfrequenc yasaweighted mean: each tone in\n\u0002\u0010\u0003\u000e\u001e\u001d\u001f\u001a \u001a\n\u0002\u0004\u0003! \"\u001f\nthatcoincides with aharmonic of\n\tcontrib utes with the\nappropriate sub-harmonic frequenc y,and with thetone\namplitude asaweight.\n4.Once thereﬁned pitch\n\tiscomputed, itsﬁnal evidence\niscomputed asthesum oftheamplitudes ofthetones in\u0002\u0004\u0003\u000e\u001e\u001d\u001f\u001a \u001a\n\u0002\u0004\u0003! \"\u001fthatcoincide with aharmonic of\n\t.\nWithrespect toresolution itshould benoted that since AM-\nPEX isonly searching forpitches below400Hz,itcanachie ve\naresolution of3%with cochlear channel outputs sampled at\n2.5kHz. Similarly ,SHS isonly looking forpitches above\n400Hzandcantherefore achie vethesame resolution with a\nsampling oftheauditory spectra atmultiples of0.5bark.\n2.2Animprovedsegmentation strategy\nInouroriginal front-end, thesegmentation ofaquery intonote\nsegments andwhite spaces wasmainly based onananalysis of\nthetotal energypattern 6\n\r\u0001\u0010\u000f.Nowwepropose amulti-stage\nsegmentation method thatincorporates thetwopitch extractors\nandthatcancope better with legato, vibrato andtremolo. The\ndifferent stages canbedescribed asfollo ws:Pre-segmentation .Inthisstage, fully described inClarisse\n2002, candidate boundaries aregenerated atclear minima in6\n\r\u0001\u0010\u000fandanywhere 6\n\r\u0001\u0010\u000fdrops belowawhite space threshold.\nSegment labeling .Everysegment islabeled asawhite\nspace (WS), alow-frequenc ynote (LF) orahigh-frequenc y\nnote (HF). Ifthemaximum AMPEX evidence exceeds some\nthreshold \u0000\u0002\u00015\n\u0003(see Clarisse 2002) thelabel isLF,else, ifthe\nmaximum SHS evidence exceeds some threshold \u0000\u0004\u0003\u0006\u0005\u0007\u0003 ,the\nlabel isHF,else itisWS. Once thelabeling isperformed, a\nsegmental pitch iscomputed foreach note segment.\nBoundary labeling.Candidate segment boundaries aremark ed\nasreliable ornot. Anunreliable boundary corresponds to\naweak energydipbetweentwonoteswiththesamelabel .\nThe dipischaracterized byadipfraction \b\u0006\t5 \n.Itistheratio\nbetween theenergyattheboundary andtheminimum ofthe\ntwosurrounding energymaxima. Aweak dipcorresponds toa\ndipfraction \b\u000b\t5 \n\r\f\n1\t5 \n.\nBoundary elimination .Unreliable boundaries aresubjected\ntoamore detailed analysis which takesinto account \b\u0006\t5 \nand\nthedifference \u000e\n\t\u0010\u000f(insemitones) between thesegmental\npitches ofthetwosurrounding segments. Boundaries with a\u000e\n\t\u0011\u000f\n0\u0013\u0012\b\u000b\t5 \n-\u0015\u0014areeliminated. Experimental data revealed\ndifferent best combinations\n\r\n\u0012\u0017\u0016\u00152\u0011\n\u0014\n\u0016\u00152\u000fand\n\r\n\u0012\u0005\n2\u0011\n\u0014\u0005\n2\u000ffor\ntheelimination ofLFnotes andHFnotes respecti vely.\nLegatoprocessing .Some note boundaries arenotmark edby\nanenergydip,butbyapitch shift only.Theyareoverlook edby\nthepre-se gmentation, buttheycanberecovered bymeans of\nthefollo wing procedure applied tolong (\f\n\u0018\u001a\u0019\u001a\u0019ms)notes:\u001bPitchstabilityanalysis .Determine foreach frame the\nmaximum interv altotheright inwhich theminimum\nand maximum pitch still coincide (asdeﬁned before).\nTheresult ofthisanalysis isastable interv allength pattern.\u001bStableintervaldetection .From lefttoright, search fora\nmaximum inthestable interv allength pattern. Ifitexceeds\n150ms,mark theinterv alstarting atthatmaximum asa\nstable pitch interv alandmovetotheposition right after\nthatinterv al.Repeat thisprocedure ontheremainder of\nthesegment until theendofthesegment isreached.\u001bLegatodecision .Incase ofmultiple stable interv als,\nconsider thecenters ofthegaps between them asnew\nboundaries andcompute thepitches ofthenewsegments.\n3Experimental results\nThemain goals ofourexperiments were: toassess theaccurac y\nofthenewfront-end (MAMI) bycomparing itstranscriptions\ntomanual transcriptions, andtocompare thisaccurac ywith\nthatofother state-of-the-art systems likeSolo Explorer (Rol-\nland 1999), EarAnalyzer (Heinz 2003) andAkoffComposer .\n3.1Freeparameters ofMAMI\nThe freeparameters ofMAMI were setonthebasis ofexper-\niments onadevelopment data set.This gavethefollo wing re-\nsults: 4\n\u0005\u0019\u001a \u001c\u000b\u001d,\n1\u00172\u0005\u0019\u001a\n\u0019\u0006\u001e\u001d, \u0000\u001f\u0003\u0006\u0005\u0007\u0003\n\u0005\u0019\u001a  times themaxi-\nmalSHS evidence observ edintheentire data set,\n1\t5 \n\u0005\u0019\u001a\n\u0018,\r\n\u0012!\u0016\u00152\u0011\n\u0014\n\u0016\u00152\u000f\n\u0005 \r\u0018\u0011\n) \u0019\u001a \u001d \u000fand\n\r\n\u0012\u0005\n2\u0011\n\u0014\u0005\n2\u000f\n\u0005 \r\u001e\u0011\n\u0019\u000f.Themost\ncritical parameters are \u0000\"\u0003\u0006\u0005\u0007\u0003 andthetwo\n\r\n\u0012\u0011\n\u0014\u000f-combinations.\n3.2Evaluationofdifferentfront-ends\nThe acoustic front-ends were tested onthree types ofqueries:\n(a)singing with syllables, (b)singing with words and (c)whistling. Themeasures ofdiscrepanc ybetween generated and\nmanual transcriptions arepercent ofnote deletions+insertions,\nandtotal error ,obtained byadding thepercent oftimes aMIDI-\nrounded note difference of2ormore semitones isobserv ed.\nTheresults arelisted inTable 1.TheMAMI front-end clearly\ndata set error Evaluated acoustic FE\ntype AkoffSolo Ear MAMI\nsyllables del+ins 82.1 20.1 15.9 10.4\n(414 notes) total error 97.8 23.0 20.5 15.5\nwords del+ins 54.3 24.3 48.5 15.4\n(657 notes) total error 72.3 33.0 61.8 21.2\nwhistling del+ins 73.1 24.7 35.0 20.8\n(283 notes) total error 79.8 28.9 37.5 23.6\nTable 1:Evaluation offour front-ends onthree testsets. The\nsizeofeach testset(innotes) ismentioned between brack ets.\noutperforms theother systems onallquery types, butparticu-\nlarly onsinging with words. Forwhistling, itisnotthatmuch\nbetter than Solo Explorer .\n4Conclusions\nThenewlypresented acoustic front-end cantranscribe alltypes\nofvocal queries with anaccurac yranging from 76%for\nwhistling to85% forsinging with syllables. Itclearly outper -\nforms allother tested systems onallquery types. Itisalsoclear\nthat most oftheerrors aresegmentation errors, meaning that\nback-ends must beable toaccommodate thistype oferrors.\n5Acknowledgments\nThis research wasperformed intheconte xtoftheMusical Au-\ndioMining project which isfunded bytheFlemish Institute for\nthePromotion oftheScientiﬁc andTechnical Research inIn-\ndustry (grant 010035-GBOU). P.Y.Rolland, G.Raskinis andT.\nHeinz areackno wledged forgranting permission topublish re-\nsults obtained with Solo Explorer andEarAnalyzer .\n6References\n1.AkoffMusic Composer 2.0. AkoffSound Lab.\nhttp://www .akoff.com.\n2.Clarisse L.,Martens J.P.,Lesaf freM.,DeBaets B.,De\nMeyerH.,Leman M.(2002). ”An auditory model based\ntranscriber ofsinging sequences”, Procs. ISMIR, 116-123.\n3.Heinz T.,Br¨uckmann A.(2003) ”Using aphysiological\nearmodel forautomatic melody transcription andsound\nsource recognition”, AES 114th Convention (Amsterdam)\n4.Rolland P.Y.,Raskinis G.,Ganascia J.(1999). ”Musical\ncontent-based retrie val:anovervie woftheMelodisco v\napproach andsystem”, Procs. ACMMultimedia, 81-84.\n5.Terhardt E.,Stoll G.,SeewannM.(1982). ”Algorithm for\nextraction ofpitch andpitch salience forcomple xtonal\nsignals”, J.Acoust. Soc. Am. 71,679-688.\n6.VanImmerseel L.,Martens J.P.(1992). ”Pitch and\nvoiced/unv oiced determination with anauditory model”,\nJ.Acoust. Soc. Am. 91,3511-3526."
    },
    {
        "title": "Building Chopin Early Editions [Presentation].",
        "author": [
            "Tod A. Olson"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417397",
        "url": "https://doi.org/10.5281/zenodo.1417397",
        "ee": "https://zenodo.org/records/1417397/files/OlsonD03.pdf",
        "abstract": "The University of Chicago Library has digitized a collection of 19th century music scores. The online collection is generated programmatically from the scanned images and human-created descriptive and structural metadata, encoded as METS objects, and delivered using the Greenstone Digital Library software. Use statistics are analyzed and possible future directions for the collection are discussed. 1",
        "zenodo_id": 1417397,
        "dblp_key": "conf/ismir/Olson03",
        "keywords": [
            "University of Chicago Library",
            "digitized",
            "19th century music scores",
            "online collection",
            "scanned images",
            "human-created descriptive and structural metadata",
            "METS objects",
            "Greenstone Digital Library software",
            "statistics",
            "possible future directions"
        ],
        "content": "Chopin Early Editions: Construction  and Usage of Online Digital Scores\nTod A. Olson\nThe University of Chicago Library\n1100 E. 57th Street, JRL 220\nChicago, IL 60637\ntod@uchicago.eduJ. Stephen Downie\nGraduate School of Library and Information Science\nUniversity of Illinois at Urbana-Champaign\n501 East Daniel St.\nChampaign, IL 61820\njdownie@uiuc.edu\nAbstract\nThe University of Chicag o Library has digitized a\ncollection of 19th century music scores. The online\ncollection is generated programmatically from thescanned images and human-created descriptive and\nstructural metadata, encoded as METS objects, and\ndelivered using the Greenstone Digital Library\nsoftware. Use statistics are analyzed and possible\nfuture directions for th e collection are discussed.\n1 Introduction\nThe University of Chicago Li brary is home to an active\ncollection of over 400 first and early edition music scores by\nthe composer Frédéric Chopin. Published between 1830 and\n1880, these scores provide an important resource for studyingChopin’s publishing history, and include many examples of\nworks published concurrently in different countries with\ntextual variations. To make this collection more accessible toscholars, the collection is being digitized and made available\nover the Web.\n2 Building the online collection\nThe Preservation Department at the University of Chicago\nLibrary is scanning these sc ores according to guidelines\npublished by the National Archives and Records\nAdministration (1998). Scores are scanned at a resolution of\n400dpi, which the department determined would capture the\nsmallest significant details. Digital images are stored as TIFF\nfiles in 24-bit RGB with no compression. The TIFF imagesare not edited by hand; flawed images are rescanned rather\nthan retouched. For web delivery, two JPEG derivatives are\nmade for each TIFF image, one 2000 pixels wide and the other700 pixels. The larger image shows greater detail, but the\nsmaller image is faster to download and easier to view on\ncommon monitors. Table 1 shows average and extreme imagefile sizes. The smallest JPEG files are for blank pages, the\nlargest are for pages of fine text. As of Aug. 12, 2003, 370\nscores have been scanned, and a total of 7031 TIFF images\ncreated, for an average of appr oximately 19 scans per score. A\ntotal of 13,030 derivatives have been produced, as producing\nderivatives lags behind scanning. Testing has begun to add\nderivatives in the DjVu format, which provides a variety of\ndesirable client-side features, such as page-turning, zooming,\nand scale-to-fit printing (B ottou, et al., 1998).\nFormat Avg. size Min. size Max. size\nTIFF 400dpi 82.4MB 56.1MB 96.8MB\nJPEG 2000px 411KB 272KB 1896KB\nJPEG 700px 70KB 40KB 344KB\nTable 1: Image file sizes\nDetailed bibliographic records in the UC library catalog were\ncreated or enhanced by a prof essional music cataloger. These\nMARC records provided the descriptive metadata for the\nonline collection. The records strictly follow AACR2, inaccordance with guidelines for submission to OCLC. Uniform\ntitles were created as needed, and are used to gather different\neditions of a score together.\nA computer program assembles the digital scores as XML\ndocuments conforming to the Metadata Encoding and\nTransmission Standard (METS). Structural and administrative\nmetadata for a score are recorded in a relational database at\nthe time of scanning. These data  are exported and used by the\nprogram to populate the METS document for the digitalversion of the score. The bibliographic metadata in MARC are\ntranslated into the Metadata  Object Description Schema\n(MODS) and embedded in the METS document. Thecompleted METS documents are the fundamental\nrepresentations of the digital sc ores for this project and can be\nrepurposed as appropriate.\nThe user interface to the collection is provided by Greenstone\n(Witten, Bainbridge & Boddie, 2001). The METS digital score\nobjects are transformed via XS LT into the Greenstone native\ndocument format. The flexibility inherent in Greenstonepermits construction of custom intra-document navigation\nmechanisms specific to this collection. Metadata in the\ndocuments programmatically massaged for improvedPermission to make digital or hard copi es of all or part of this work for\npersonal of classroom use is granted w ithout fee provided that copies are not\nmade or distributed for profit or comm ercial advantage and that copies bear\nthis notice and the full citation on the first page.   2003 The Johns Hopkins\nUniversity.retrieval. For example, a place na me thesaurus is simulated, so\na search for scores published in Leipzig will match the sixvariations of that city name found in the database.\n3 Usage analysis\nAnalysis of web server logs for the first 5 months of public\nuse yield some interesting and thought-provoking findings.\nUnless otherwise indicated, all numbers exclude on-campususe. The site averages about 100 hits (collection browsing or\nscore viewing operations) daily. Connections have been made\nfrom over 900 individual IP addresses. At least 30% of totaluse appears to be international.\nTable 2 shows the proportion of user time spent navigating\nand viewing scores, broken down by the search interface and\nthe four browse listings. The user interface permits keyword\nsearching of 11 indexes; browsing by Title, Uniform Title,\nGenre, or Dedicatee; and viewing the score images. The Title\nand Genre browse listings are the most-used ways ofaccessing the collection. This is  consistent with the informal\nuser testing conducted with musi c students at the bachelor and\ndoctoral levels. The View/Navigation ratio for the Genre\nbrowse listing suggests it is the most effective route by which\nusers access scores that they will examine in more detail.\nDuring score-viewing activity, ov er 17,000 score images were\ndownloaded. 79% of images downloaded were the 700 pixel\nwide JPEG files and 21% were the 2000 pixel wide JPEGfiles. Currently, the smaller image is displayed by default and\nthe user has to take an extra action to see the larger image.\nThese numbers suggest that users are very interested in thelarger images and that the in terface should make them more\nreadily accessible.\nTable 3 shows the frequency w ith which each keyword index\nhas been searched within th e search interface. Aside from\nTitle, which is the most prominent index, the frequencies bear\nno relation to the order of indexes presented on the search\npage. The popularity of the Opus keyword search is consistentwith user requests to improve access by opus number in other\nparts of the interface.\nAccess method Viewing\nscoresNavigating\ncollectionView/Nav.\nratio\nTitle 5592 1944 2.88\nGenre 2540 646 3.93\nKeyword Search 771 648 1.19\nUniform Title 996 639 1.56\nDedicatee 861 285 3.02\nTable 2: User activity by access method (search interface and\nbrowse listings)\n4 Future directions\nThe user interface to this coll ection will be refined based on\nfurther user studies, user feedback, and information collectedfrom web logs. User studies and feedback have already\nsuggested changes that are plan ned for a future revision. For\nexample, the organization of some of the browse listings, andthe mechanisms for seeing different versions of the score\nimages will change.\nThe descriptive metadata for the scores in this collection will\nbe made available for OAI harvesting.\nWe are exploring options to a dd content-based search features\nto the collection. We would like to provide the searching of\nthe musical symbolic content of the scores. Options beingexplored include use of optical music recognition to create\nsearchable representations and/ or acquisition of pre-existing\nsymbolic representations (i.e., MIDI, GUIDO, etc.). Addingaudio files to complement the scores is also under\nconsideration.\nThis may also be an appropriate test collection for various\ntechniques in music information retrieval. The varied musictypography and complexity of notation may be especially\nuseful for testing progress in optical music recognition. The\npresence of scores with multiple  variant texts and full feature\nbibliographic data provides opportunities for gathering and\ndistinguishing between variations of the same work.\nThe Chopin Early Editions collection is available at\nhttp://chopin.lib.uchicago.edu/ .\nKeyword Index Percentage Occurrences\nTitle 54% 348\nOpus No. 12% 80\nGeneral Keyword 10% 64\nDate 5% 33\nGenre 5% 31\nSubject 5% 30\nUniform Title 2% 16\nPublisher 2% 13\nPlace of Publication 2% 12\nPlate Number 2% 12\nDedicatee 1% 9\nTable 3: Searches by keyword index\nReferences\nBottou, L., Haffner P., Howard P., Simard P., Bengio, Y. &\nLeCun, Y. (1998). High quality document image compression\nwith `DjVu'. Journal of Electronic Imaging.  7(3), 410-425.\nMetadata Encoding and Transmission Standard (METS) .\nLibrary of Congress. Retrieved Aug 11, 2003 fromhttp://www.loc.gov/standards/mets/\nMetadata Object Description Schema (MODS).  Library of\nCongress. Retrieved Aug 11, 2003 fromhttp://www.loc.gov/standards/mods/\nNational Archives and Records Administration. (1998). NARA\nGuidelines for Digitizing Archival Materials for Electronic\nAccess . Retrieved August 10, 2003 from\nhttp://www.archives.gov/research_room/arc/arc_info/\nguidelines_for_digitizing_archival_materials.pdf\nWitten, I. H., Bainbridge, D. & Boddie, S. J. (2001).\nGreenstone: Open-Source Digital Library Software . D-Lib\nMagazine. 7(10). Retrieved August 10, 2003 from http://\nwww.dlib.org/dlib/october01/witten/10witten.html"
    },
    {
        "title": "Chopin early editions: The construction and usage of a collection of digital scores.",
        "author": [
            "T. Olson",
            "J. Stephen Downie"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417397",
        "url": "https://doi.org/10.5281/zenodo.1417397",
        "ee": "https://zenodo.org/records/1417397/files/OlsonD03.pdf",
        "abstract": "The University of Chicago Library has digitized a collection of 19th century music scores. The online collection is generated programmatically from the scanned images and human-created descriptive and structural metadata, encoded as METS objects, and delivered using the Greenstone Digital Library software. Use statistics are analyzed and possible future directions for the collection are discussed. 1",
        "zenodo_id": 1417397,
        "dblp_key": "conf/ismir/OlsonD03",
        "keywords": [
            "University of Chicago Library",
            "digitized",
            "19th century music scores",
            "online collection",
            "scanned images",
            "human-created descriptive and structural metadata",
            "METS objects",
            "Greenstone Digital Library software",
            "statistics",
            "possible future directions"
        ],
        "content": "Chopin Early Editions: Construction  and Usage of Online Digital Scores\nTod A. Olson\nThe University of Chicago Library\n1100 E. 57th Street, JRL 220\nChicago, IL 60637\ntod@uchicago.eduJ. Stephen Downie\nGraduate School of Library and Information Science\nUniversity of Illinois at Urbana-Champaign\n501 East Daniel St.\nChampaign, IL 61820\njdownie@uiuc.edu\nAbstract\nThe University of Chicag o Library has digitized a\ncollection of 19th century music scores. The online\ncollection is generated programmatically from thescanned images and human-created descriptive and\nstructural metadata, encoded as METS objects, and\ndelivered using the Greenstone Digital Library\nsoftware. Use statistics are analyzed and possible\nfuture directions for th e collection are discussed.\n1 Introduction\nThe University of Chicago Li brary is home to an active\ncollection of over 400 first and early edition music scores by\nthe composer Frédéric Chopin. Published between 1830 and\n1880, these scores provide an important resource for studyingChopin’s publishing history, and include many examples of\nworks published concurrently in different countries with\ntextual variations. To make this collection more accessible toscholars, the collection is being digitized and made available\nover the Web.\n2 Building the online collection\nThe Preservation Department at the University of Chicago\nLibrary is scanning these sc ores according to guidelines\npublished by the National Archives and Records\nAdministration (1998). Scores are scanned at a resolution of\n400dpi, which the department determined would capture the\nsmallest significant details. Digital images are stored as TIFF\nfiles in 24-bit RGB with no compression. The TIFF imagesare not edited by hand; flawed images are rescanned rather\nthan retouched. For web delivery, two JPEG derivatives are\nmade for each TIFF image, one 2000 pixels wide and the other700 pixels. The larger image shows greater detail, but the\nsmaller image is faster to download and easier to view on\ncommon monitors. Table 1 shows average and extreme imagefile sizes. The smallest JPEG files are for blank pages, the\nlargest are for pages of fine text. As of Aug. 12, 2003, 370\nscores have been scanned, and a total of 7031 TIFF images\ncreated, for an average of appr oximately 19 scans per score. A\ntotal of 13,030 derivatives have been produced, as producing\nderivatives lags behind scanning. Testing has begun to add\nderivatives in the DjVu format, which provides a variety of\ndesirable client-side features, such as page-turning, zooming,\nand scale-to-fit printing (B ottou, et al., 1998).\nFormat Avg. size Min. size Max. size\nTIFF 400dpi 82.4MB 56.1MB 96.8MB\nJPEG 2000px 411KB 272KB 1896KB\nJPEG 700px 70KB 40KB 344KB\nTable 1: Image file sizes\nDetailed bibliographic records in the UC library catalog were\ncreated or enhanced by a prof essional music cataloger. These\nMARC records provided the descriptive metadata for the\nonline collection. The records strictly follow AACR2, inaccordance with guidelines for submission to OCLC. Uniform\ntitles were created as needed, and are used to gather different\neditions of a score together.\nA computer program assembles the digital scores as XML\ndocuments conforming to the Metadata Encoding and\nTransmission Standard (METS). Structural and administrative\nmetadata for a score are recorded in a relational database at\nthe time of scanning. These data  are exported and used by the\nprogram to populate the METS document for the digitalversion of the score. The bibliographic metadata in MARC are\ntranslated into the Metadata  Object Description Schema\n(MODS) and embedded in the METS document. Thecompleted METS documents are the fundamental\nrepresentations of the digital sc ores for this project and can be\nrepurposed as appropriate.\nThe user interface to the collection is provided by Greenstone\n(Witten, Bainbridge & Boddie, 2001). The METS digital score\nobjects are transformed via XS LT into the Greenstone native\ndocument format. The flexibility inherent in Greenstonepermits construction of custom intra-document navigation\nmechanisms specific to this collection. Metadata in the\ndocuments programmatically massaged for improvedPermission to make digital or hard copi es of all or part of this work for\npersonal of classroom use is granted w ithout fee provided that copies are not\nmade or distributed for profit or comm ercial advantage and that copies bear\nthis notice and the full citation on the first page.   2003 The Johns Hopkins\nUniversity.retrieval. For example, a place na me thesaurus is simulated, so\na search for scores published in Leipzig will match the sixvariations of that city name found in the database.\n3 Usage analysis\nAnalysis of web server logs for the first 5 months of public\nuse yield some interesting and thought-provoking findings.\nUnless otherwise indicated, all numbers exclude on-campususe. The site averages about 100 hits (collection browsing or\nscore viewing operations) daily. Connections have been made\nfrom over 900 individual IP addresses. At least 30% of totaluse appears to be international.\nTable 2 shows the proportion of user time spent navigating\nand viewing scores, broken down by the search interface and\nthe four browse listings. The user interface permits keyword\nsearching of 11 indexes; browsing by Title, Uniform Title,\nGenre, or Dedicatee; and viewing the score images. The Title\nand Genre browse listings are the most-used ways ofaccessing the collection. This is  consistent with the informal\nuser testing conducted with musi c students at the bachelor and\ndoctoral levels. The View/Navigation ratio for the Genre\nbrowse listing suggests it is the most effective route by which\nusers access scores that they will examine in more detail.\nDuring score-viewing activity, ov er 17,000 score images were\ndownloaded. 79% of images downloaded were the 700 pixel\nwide JPEG files and 21% were the 2000 pixel wide JPEGfiles. Currently, the smaller image is displayed by default and\nthe user has to take an extra action to see the larger image.\nThese numbers suggest that users are very interested in thelarger images and that the in terface should make them more\nreadily accessible.\nTable 3 shows the frequency w ith which each keyword index\nhas been searched within th e search interface. Aside from\nTitle, which is the most prominent index, the frequencies bear\nno relation to the order of indexes presented on the search\npage. The popularity of the Opus keyword search is consistentwith user requests to improve access by opus number in other\nparts of the interface.\nAccess method Viewing\nscoresNavigating\ncollectionView/Nav.\nratio\nTitle 5592 1944 2.88\nGenre 2540 646 3.93\nKeyword Search 771 648 1.19\nUniform Title 996 639 1.56\nDedicatee 861 285 3.02\nTable 2: User activity by access method (search interface and\nbrowse listings)\n4 Future directions\nThe user interface to this coll ection will be refined based on\nfurther user studies, user feedback, and information collectedfrom web logs. User studies and feedback have already\nsuggested changes that are plan ned for a future revision. For\nexample, the organization of some of the browse listings, andthe mechanisms for seeing different versions of the score\nimages will change.\nThe descriptive metadata for the scores in this collection will\nbe made available for OAI harvesting.\nWe are exploring options to a dd content-based search features\nto the collection. We would like to provide the searching of\nthe musical symbolic content of the scores. Options beingexplored include use of optical music recognition to create\nsearchable representations and/ or acquisition of pre-existing\nsymbolic representations (i.e., MIDI, GUIDO, etc.). Addingaudio files to complement the scores is also under\nconsideration.\nThis may also be an appropriate test collection for various\ntechniques in music information retrieval. The varied musictypography and complexity of notation may be especially\nuseful for testing progress in optical music recognition. The\npresence of scores with multiple  variant texts and full feature\nbibliographic data provides opportunities for gathering and\ndistinguishing between variations of the same work.\nThe Chopin Early Editions collection is available at\nhttp://chopin.lib.uchicago.edu/ .\nKeyword Index Percentage Occurrences\nTitle 54% 348\nOpus No. 12% 80\nGeneral Keyword 10% 64\nDate 5% 33\nGenre 5% 31\nSubject 5% 30\nUniform Title 2% 16\nPublisher 2% 13\nPlace of Publication 2% 12\nPlate Number 2% 12\nDedicatee 1% 9\nTable 3: Searches by keyword index\nReferences\nBottou, L., Haffner P., Howard P., Simard P., Bengio, Y. &\nLeCun, Y. (1998). High quality document image compression\nwith `DjVu'. Journal of Electronic Imaging.  7(3), 410-425.\nMetadata Encoding and Transmission Standard (METS) .\nLibrary of Congress. Retrieved Aug 11, 2003 fromhttp://www.loc.gov/standards/mets/\nMetadata Object Description Schema (MODS).  Library of\nCongress. Retrieved Aug 11, 2003 fromhttp://www.loc.gov/standards/mods/\nNational Archives and Records Administration. (1998). NARA\nGuidelines for Digitizing Archival Materials for Electronic\nAccess . Retrieved August 10, 2003 from\nhttp://www.archives.gov/research_room/arc/arc_info/\nguidelines_for_digitizing_archival_materials.pdf\nWitten, I. H., Bainbridge, D. & Boddie, S. J. (2001).\nGreenstone: Open-Source Digital Library Software . D-Lib\nMagazine. 7(10). Retrieved August 10, 2003 from http://\nwww.dlib.org/dlib/october01/witten/10witten.html"
    },
    {
        "title": "An HMM-based pitch tracker for audio queries.",
        "author": [
            "Nicola Orio",
            "M. Sisti Sette"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417601",
        "url": "https://doi.org/10.5281/zenodo.1417601",
        "ee": "https://zenodo.org/records/1417601/files/OrioS03.pdf",
        "abstract": "In this paper we present an approach to the transcrip- tion of musical queries based on a hidden Markov model (HMM). The HMM is used to model the audio features related to the singing voice, and the transcrip- tion is obtained through Viterbi decoding. We report our preliminary work on evaluation of the system. 1",
        "zenodo_id": 1417601,
        "dblp_key": "conf/ismir/OrioS03",
        "keywords": [
            "HMM-based pitch tracker",
            "Music Information Retrieval",
            "Audio queries",
            "Query-by-humming",
            "Pitch tracking",
            "Viterbi decoding",
            "Musical transcription",
            "Singing voice",
            "MIDI-like music events",
            "Query normalization"
        ],
        "content": "A HMM-Based Pitch TrackerforAudioQueries\nNicola Orio andMatteo Sisti Sette\nDepartment of Information Engineering, Universityof Padova\nViaGradenigo, 6/B\n35131Padova,Italy\nforio, msistis g@dei.unipd.it\nAbstract\nIn this paper we present an approach to the transcrip-\ntion of musical queries based on a hidden Markov\nmodel(HMM).TheHMMisusedtomodeltheaudio\nfeaturesrelatedtothesingingvoice,andthetranscrip-\ntion is obtained through Viterbi decoding. We report\nourpreliminary workon evaluationof the system.\n1 Introduction\nPotential users of a Music Information Retrieval (MIR) system\nare likely to express their information needs through the query-\nby-humming paradigm. The user’s query usually needs to be\ntranscribed for segmenting the audio signal in a sequence of\neventsanddescribingeacheventwithitsfeatures,normallythe\npitch. The effectiveness of a MIR system depends also on the\nquality of the query transcription. Research on automatic pitch\ntracking has been extensively carried out for many years, ap-\nplying different methods, including autocorrelation (Cheveign ´e\nand Kawahara, 2002), auditory models (Clarisse et al., 2002),\nand classical frequency or mel-cepstrum analyses; moreover, a\nnumber of commercial pitch trackers is already available. Yet,\ntherearesomepeculiaritiesofqueriessungbynonexpertusers\nthat require more research on pitch tracking . The query can\nbe systematically or locally out of tune and notes can start with\nor be connected by glissandi and be recorded in a noisy en-\nvironment. Moreover, queries can be whistled, hummed with\ndifferent syllables, or sung with the lyrics. All these problems\nneedtobetakenintoaccountduringthedevelopmentofaMIR\nfront-end.\nThe presented approach has been developed as a part of an ex-\nistingMIRsystem(MelucciandOrio,1999),whichisbasedon\ntheextractionofrelevantmusicalphrasesascontentdescriptors\nofmusicdocuments. Differentlevelsofnormalizationsareused\ntodealwitherrorsinthesungqueries. TheoutputoftheHMM-\nbased pitch tracker is a sequence of MIDI-like music events,\nwhichisusedastheinputforthemusicalphraseextractionand\nnormalizationof musical queries.\nPermission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided that\ncopies are not made or distributed for proﬁt or commercial advan-\ntage and that copies bear this notice and the full citation on the ﬁrst\npage.c°2003 Johns Hopkins University.2 Segmentation and pitch tracking with a\nHidden MarkovModel\nHidden Markov models (HMMs) have been applied in many\ndifferent areas, from speech recognition and biological se-\nquence analysis, to text and music information retrieval. An\nextensivedescription of HMMs is givenin (Rabiner and Juang,\n1993). HMMs have been applied to the pitch tracking of\nspeech by Wu et al. (2001), while an application of HMMs to\nthe alignment of performances with scores was presented by\nRaphael (1999).\nFromastatisticalpointofview,asungquerycanbeconsidered\nas the observation of an unknown process, which is the melody\ntheuserhasinmind. Whatisobservedisasetofaudiofeatures,\nwhich has to be chosen. The process can be modeled with a\nHMM,wheretransitionprobabilitiesconcernboththeevolution\nof a given note (e.g., attack, sustain, release, and pauses) and\nthe possible sequences of notes (e.g., the probability that the\nuser sings a given interval), and emission probabilities refer to\nthe observed audio features. The transcription reduces then to\nthe discovery of the evolution along states in the HMM that\nmostlikelygeneratedthequery,whichcanbeobtainedthrough\nViterbidecoding.\nWeproposetheuseofatwo-levelHMM.The eventlevel isaset\nofstates,eachonelabeledwithadifferentpitchinthechromatic\nscale inside a given range. At this level a melody is modeled\nas a path among the states. The network is fully connected, as\nshowninFigure1(a),buttheprobabilitytogofromastatetoan-\nother one depends on the melodic interval between their labels.\nTransition probabilities can be set to the a priori probability of\na musical interval in a given repertoire. The audio level is a\nset of states connected with a left-to-right topology, with labels\nrelated to the evolution of a musical event. Figure 1(b) shows\na simple topology of a note event at the audio level, with three\nstatesrepresentingattack,sustain,andaﬁnalrestwhichmaybe\nskipped. This is the audio level topology we have used in our\npreliminary tests, but we believe that the great potential of the\nHMM-basedapproachliesinthepossibilitytoimplementmore\ncomplextopologies. InFigure2weproposeanextensiontothe\nmodel of Figure 1(b), with multiple parallel attack states (with\ndifferent emission probability densities) representing different\nkindsofattacks,e.g.vowelandconsonantattacks,fromsilence\nandfromthesteadystateofthepreviousnote. Multipleconsec-\nutive sustain states (possibly with identical emission densities)\nallow to impose a lower bound to the note duration. Additional\nstates, marked with ‘+’ and ‘-’ in the ﬁgure, are added in orderto model slight detunings during the sustain of a note. Other\nevents, such as glissandi, vibrati, or thrills, can be represented\nbyintroducing other topologies and states at both levels.\n¹¸º·¹¸º· ¹¸º·\n±N\nN\nAAA KA\nAA U\n©©© *©©© ¼PPPPPP qPPPPPP i...\nDo2Mi5Do#2\nl l l----NNN\n3a s r\n(a) ( b)\nFigure 1: The HMM topology. (a) The event level. (b) The\naudio level used in our experiments: a,sandrstand for attack,\nsustain, andrestrespectively;squares represent null states.\nXX zXX z\n»» :»» :\nlalalala\n»» :\n¡¡ µXX z@@ Rlsi+\n±°\ni¡NM-...-lsi+\n±°\ni¡NM-lr-\n-\nFigure 2: A topology for the audio level modeling different\nkinds of attacks, durations, and slight detunings; self transition\narcsare not shown.\nThe complete HMM is given by the deﬁnition of the emission\nprobabilities. These probabilities need to be set and computed\nonlyforstatesoftheaudiolevel,becausestatesattheeventlevel\nare simply formed of states at the audio level. The features we\nhaveconsideredandtestedsofarare: thelog-energyofthesig-\nnal, for discriminating between silences and sounds; the sum\nof the energy in the bands of the ﬁrst harmonics for each note\nin the melodic range, for distinguishing different pitches; the\nﬁrstderivativesofboththesefeatures,foramoreprecisedetec-\ntion of note attacks; the maximum difference between energies\nin adjacent harmonic bands, to reduce the common problem of\ndoublingand halving octaves.\nThechosenrepresentationofthespectrum,simplybasedonthe\noverall energy on the harmonic bands, helps dealing with prob-\nlemslikemissingharmonicsordistortionsduetopossibleuser’s\nlow-qualityaudioequipment;moreoverthesizeofeachbandis\nchosentomodeltemporarydetuningsofatmostaquarter-tone.\nSinceusersmaynothaveabsolutepitch,apreprocessingstepis\ncarried out to estimate the most probable distance between the\nuser’s reference and the typical 440Hz to set accordingly the\nharmonicbands. Weareworkingonaddingotherfeatures,such\nas the difference between the energies in the bands of contigu-\nous pitches in the chromatic scale, in order to model glissandi\nand small variations of pitch inside a note, and to improve the\nmodellingof attacks.\nAll the features are modeled with unilateral exponential proba-\nbilitydensityfunctions(pdfs);preliminarytestswithGaussians\ngave slightly worse results. The exponential pdfs are trained\nusing a statistical analysis on a set of labeled audio examples.\nStatesofthesamekind(e.g.,allsustainstates)belongingtodif-ferent notes share the same parameters; they differ in the way\nthe harmonic bands are computed.\nThe transcription can be obtained through decoding, consider-\ningthattheHMM hasatransitioneach timeanewaudio frame\nis analyzed and a new set of features is observed. The path\nalong states at the event level gives both the segmentation and\nthe pitch tracking, because each state is associated to an event\nthrough its label.\n3 Evaluation\nWe carried out a preliminary evaluation of our approach, using\nthe HMM shown in Figure 1. We tested the system with 18\naudio queries sung by untrained users, using a low quality mi-\ncrophoneandacommondigitalaudiointerface. Querieshadan\noverallnumber of 300notes.\nResults are summarized in Table 1. Errors are mainly due to\nglissandi,whichwerenottakenintoaccountaspossibleexpres-\nsive gestures and so were tracked by the system as chromatic\nscales preceding notes. Other common errors are the insertion\nor the wrong recognition of notes a halfstep from the correct\none. These results highlight some improvements that can be\napplied to the simple model, which will be part of our future\nwork.\nomitted total 4%\nnotes repetition of same note 3%\npitch errors total 10%\n(whole notes) §1semitone 8%\n>1ﬁfth 2%\nnotes inserted total 13%\nduring notes repetition of same note 3%\n1-2semitones fromcorrect note 8%\n>1ﬁfthfrom correct note 2%\nnotes inserted during silence 1%\nmultiple notes detected during attacks 27%\nTable 1: Preliminary test results. All percentages refer to the\ntotal number of notes.\nReferences\nCheveign ´e,A.&Kawahara,H.(2002). YIN,afundamentalfre-\nquencyestimatorforspeechandmusic. JournaloftheAcoustic\nSociety of America ,111(4), 1917–1930.\nClarisse,L.P.etal.(2002). Anauditorymodelbasedtranscriber\nof singing sequences. In Proceedings of the International Con-\nferenceon Music Information Retrieval ,(pp. 116–123).\nMelucci, M. & Orio, N. (1999). Musical Information Retrieval\nUsing Melodic Surface. In Proceedings of IV ACM Conference\non Digital Libraries ,(pp. 152–160).\nRabiner, L. & Juang, B.H. (1993). Fundamentals of speech\nrecognition . (pp. 321–389). Prentice Hall, EnglewoodCliffs.\nRaphael, C. (1999). Automatic segmentation of acoustic musi-\ncal signals using Hidden Markov Models. IEEE Transactions\non PatternAnalysis and MachineIntelligence ,21(4), 360–370.\nWu, M. et al. (2001). Pitch Tracking Based on Statistical An-\nticipation. In ProceedingsoftheInternetionalJointConference\non NeuralNetworks ,Volume2, (pp. 866–871). DC."
    },
    {
        "title": "Exploring music collections by browsing different views.",
        "author": [
            "Elias Pampalk",
            "Simon Dixon",
            "Gerhard Widmer"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1416876",
        "url": "https://doi.org/10.5281/zenodo.1416876",
        "ee": "https://zenodo.org/records/1416876/files/PampalkDW03.pdf",
        "abstract": "The availability of large music collections calls for ways to efficiently access and explore them. We present a new approach which combines descriptors derived from audio analysis with meta-information to create different views of a collection. Such views can have a focus on timbre, rhythm, artist, style or other aspects of music. For each view the pieces of mu- sic are organized on a map in such a way that similar pieces are located close to each other. The maps are visualized using an Islands of Music metaphor where islands represent groups of similar pieces. The maps are linked to each other using a new technique to align self-organizing maps. The user is able to browse the collection and explore different aspects by gradu- ally changing focus from one view to another. We demonstrate our approach on a small collection using a meta-information-based view and two views gener- ated from audio analysis, namely, beat periodicity as an aspect of rhythm and spectral information as an aspect of timbre. 1",
        "zenodo_id": 1416876,
        "dblp_key": "conf/ismir/PampalkDW03",
        "keywords": [
            "large music collections",
            "efficient access",
            "exploring collections",
            "audio analysis",
            "meta-information",
            "different views",
            "timbre",
            "rhythm",
            "artist",
            "style"
        ],
        "content": "Exploring Music Collections by Browsing Different Views\nElias Pampalk1, Simon Dixon1, Gerhard Widmer1,2\n1Austrian Research Insitute for Artiﬁcial Intelligence (OeFAI)\nFreyung 6/6, A-1010 Vienna, Austria\n2Department of Medical Cybernetics and Artiﬁcial Intelligence\nUniversity of Vienna, Austria\n{elias, simon, gerhard }@oefai.at\nAbstract\nThe availability of large music collections calls for\nways to efﬁciently access and explore them. We\npresent a new approach which combines descriptors\nderivedfromaudioanalysiswithmeta-informationto\ncreatedifferentviewsofacollection. Suchviewscan\nhave a focus on timbre, rhythm, artist, style or other\naspects of music. For each view the pieces of mu-\nsic are organized on a map in such a way that similar\npieces are located close to each other. The maps are\nvisualizedusinganIslandsofMusicmetaphorwhere\nislands represent groups of similar pieces. The maps\nare linked to each other using a new technique to\nalignself-organizingmaps. Theuserisabletobrowse\nthe collection and explore different aspects by gradu-\nally changing focus from one view to another. We\ndemonstrateourapproachonasmallcollectionusing\na meta-information-based view and two views gener-\nated from audio analysis, namely, beat periodicity as\nan aspect of rhythm and spectral information as an\naspect of timbre.\n1 Introduction\nTechnological advances with respect to Internet bandwidth and\nstoragemediahavemadelargemusiccollectionsprevalent. Ex-\nploration of such collections is usually limited to listings re-\nturned from, for example, artist-based queries or requires ad-\nditional information not readily available to the public such as\ncustomer proﬁles from electronic music distributors. In partic-\nular, content-based browsing of music according to the overall\nsound similarity has remained unsolved although recent work\nseems very promising (e.g. Tzanetakis and Cook, 2001; Au-\ncouturier and Pachet, 2002b; Cano et al., 2002; Pampalk et al.,\n2002a). Themaindifﬁcultyistoestimatetheperceivedsimilar-\nity given solely the audio signal.\nMusicsimilarityassuchmightappeartobearathersimplecon-\ncept. For example, it is no problem to distinguish classical mu-\nPermission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided that\ncopies are not made or distributed for proﬁt or commercial advan-\ntage and that copies bear this notice and the full citation on the ﬁrst\npage.c/circlecopyrt2003 Johns Hopkins University.sic from heavy metal. However, there are several aspects of\nsimilarity to consider. Some aspects have a very high level of\ndetail such as the difference between a Vladimir Horowitz and\na Daniel Barenboim interpretation of a Mozart sonata. Other\naspects are more apparent such as the noise level. It is ques-\ntionable if it will ever be possible to automatically analyze all\naspects of similarity directly from audio. But within limits,\nit is possible to analyze, for example, similarity in terms of\nrhythm (Foote et al., 2002; Paulus and Klapuri, 2002; Dixon\net al., 2003) or timbre (Logan and Salomon, 2001; Aucouturier\nand Pachet, 2002b).\nIn this paper we present a new approach to combine informa-\ntion extracted from audio with meta-information such as artist\norgenre. Inparticular,weextractspectrumandperiodicityhis-\ntograms to roughly describe timbre and rhythm respectively.\nFor each of these aspects of similarity the collection is orga-\nnized using a self-organizing map (Kohonen, 1982, 2001). The\nSOM arranges the pieces of music on a map such that simi-\nlar pieces are located close to each other. We use smoothed\ndata histograms to visualize the cluster structure and to create\nanIslands of Music metaphor where groups of similar pieces\nare visualized as islands (Pampalk et al., 2002a,b).\nFurthermore, we integrate a third type of organization which\nis not be derived by audio analysis but is of interest to the\nuser. This could be any type of organization based on meta-\ninformation. We align these 3 different views and interpolate\nbetween them using aligned-SOMs (Pampalk et al., 2003b) to\nenable the user to interactively explore how the organization\nchangesasthefocusisshiftedfromoneviewtoanother. Thisis\nsimilartotheideapresentedbyAucouturierandPachet(2002b)\nwho use an “Aha-Slider” to control the combination of meta-\ninformation with information derived from audio analysis. We\ndemonstrate our approach on asmall music collection.\nTheremainderofthispaperisorganizedasfollows. Inthenext\nsection we present the spectrum and periodicity histograms we\nuse to calculate similarities from the respective viewpoints. In\nsection3wereviewtheSOMandthealigned-SOMs. Insection\n4 we demonstrate the approach and in section 5 we conclude\nour work.\n2 Similarity Measures\nIn general it is not predictable when a human listener will con-\nsiderpiecestobesimilar. Piecesmightbesimilardependingon\nthe lyrics, instrumentation, melody, rhythm, artists, or vaguelyby the emotions they invoke. However, even relatively simple\nsimilarity measures can aid in handling large music collections\nmore efﬁciently. For example, Logan (2002) uses a spectrum-\nbased similarity measure to automatically create playlists of\nsimilar pieces. Aucouturier and Pachet (2002b) use a similar\nspectrum-based measure to ﬁnd unexpected similarities, e.g.,\nsimilarities between pieces from different genres. A rather dif-\nferent approach based on the psychoacoustic model of ﬂuctua-\ntion strength was presented by Pampalk et al. (2002a) to orga-\nnize and visualize music collections.\nUnlike previous approaches we do not try to model the overall\nperceivedsimilaritybutratherfocusondifferentaspectsandal-\nlow the user to interactively decide which combination of these\naspects is the most interesting. In the remainder of this sec-\ntion we ﬁrst review the psychoacoustic preprocessing we ap-\nply. Subsequently we present the periodicity and spectrum his-\ntogram which rely on the preprocessing.\n2.1 Psychoacoustic Preprocessing\nTheobjectiveofthepsychoacousticpreprocessingistoremove\ninformationintheaudiosignalwhichisnotcriticaltoourhear-\ningsensationwhileretainingtheimportantparts. Afterthepre-\nprocessing each piece of music is described in the dimensions\ntime ( fs= 86Hz), frequency (20 critical-bands with the unit\nbark), and loudness measured in sone. Similar preprocessing\nfor instrument and music similarity have been used, for exam-\nple,byFeitenandG ¨unzel(1994)andbyPampalketal.(2002a).\nFurthermore,similarapproachesformthecoreofperceptualau-\ndio quality measures (e.g. Thiede et al., 2000).\nPrior to analysis we downsample and downmix the audio to\n11kHz mono. It is important to notice that we are not trying to\nmeasuredifferencesbetween44kHzand11kHz,betweenmono\nand stereo, or between an MP3 encoded piece compared to the\nsame piece encoded with Ogg Vorbis or any other format. In\nparticular, a piece of music given in (uncompressed) CD qual-\nity should have a minimal distance to the same piece encoded,\nfor example, with MP3 at 56kbps. As long as the main charac-\nteristicssuchasstyle,tempo,ortimbreremainclearlyrecogniz-\nablebyahumanlisteneranyformofdatareductioncanonlybe\nbeneﬁcial in terms of robustness and computational speed-up.\nIn the next step we remove the ﬁrst and last 10 seconds of each\npiece to avoid lead-in and fade-out effects. Subsequently we\napply a STFT to obtain the spectrogram using 23ms windows\n(256 samples), weighted with a Hann function, and 12ms over-\nlap(128samples). Tomodelthefrequencyresponseoftheouter\nandmiddleearweusetheformulaproposedbyTerhardt(1979),\nAdB(fkHz) = (1)\n−3.64 (10−3f)−0.8+\n+6.5exp/parenleftbig\n−0.6(10−3f−3.3)2/parenrightbig\n+\n−10−3(10−3f)4.\nThe main characteristics of this weighting ﬁlter are that the in-\nﬂuence of very high and low frequencies is reduced while fre-\nquencies around 3–4kHz are emphasized (see Figure 1).\nSubsequently the frequency bins of the STFT are grouped into\n20 critical-bands according to Zwicker and Fastl (1999). The\nconversion between the bark and the linear frequency scale can\nFigure 1: The curve shows the response of Terhardt’s outer and\nmiddleearmodel. Thedottedlinesmarkthecenterfrequencies\nof the critical-bands. For ourwork we use the ﬁrst 20 bands.\nbe computed with,\nZbark(fkHz) = 13arctan (0.76f) + 3 .5arctan (f/7.5)2.(2)\nThemaincharacteristicofthebarkscaleisthatthewidthofthe\nthe critical-bands is 100Hz up to 500Hz and beyond 500Hz the\nwidth increases nearly exponentially (see Figure 1).\nWe calculate spectral masking effects according to Schroeder\net al. (1979) who suggest a spreading function optimized for\nintermediate speech levels. The spreading function has lower\nand upper skirts with slopes of +25dB and −10dB per critical-\nband. The main characteristic is that lower frequencies have\na stronger masking inﬂuence on higher frequencies than vice\nversa. The contribution of critical-band zitozjwith ∆z=\nzj−ziis attenuated by,\nBdB(∆zbark) = (3)\n+15.81 + 7 .5(∆z+ 0.474) +\n−17.5(1 + (∆ z+ 0.474)2)1/2.\nWe calculate the loudness in sone using the formula suggested\nby Bladon and Lindblom (1981),\nSsone(ldB-SPL ) =/braceleftBigg\n2(l−40)/10,ifl≥40dB,\n(l/40)2.642,otherwise.(4)\nFinally,wenormalizeeachpiecesothatthemaximumloudness\nvalue equals 1 sone.\n2.2 Periodicity Histogram\nTo obtain periodicity histograms we use an approach presented\nbyScheirer(1998)inthecontextofbeattracking. Asimilarap-\nproach was developed by Tzanetakis and Cook (2002) to clas-\nsify genres. There are two main differences to this previous\nwork. First, we extend the typical histograms to incorporate\ninformation on the variations over time which is valuable in-\nformation when considering similarity. Second, we use a reso-\nnance model proposed by Moelants (2002) for preferred tempo\ntoweighttheperiodicitiesandinparticulartoemphasizediffer-\nences in tempos around 120 beats per minute (bpm).\nWe start with the preprocessed data and further process it us-\ning a half wave rectiﬁed difference ﬁlter on each critical-band\nto emphasize percussive sounds. We then process 12 second\nwindows (1024 samples) with 6 second overlap (512 samples).\nEach window is weighted using a Hann window before a combﬁlter bank is applied to each critical-band with a 5bpm resolu-\ntion in the range from 40 to 240bpm. Then we apply the reso-\nnance model of Moelants (2002) with β= 4to the amplitudes\nobtained from the comb ﬁlter. To emphasize peaks we use a\nfull wave rectiﬁed difference ﬁlter before summing up the am-\nplitudes for each periodicityover all bands.\nThatgivesus,forevery6secondsofmusic,40valuesrepresent-\ningthestrengthofrecurringbeatswithtemposrangingfrom40\nto240bpm. Tosummarizethisinformationforawholepieceof\nmusicweusea2-dimensionalhistogramwith40equallyspaced\ncolumnsrepresentingdifferenttemposand50rowsrepresenting\nstrength levels. The histogram counts for each periodicity how\nmany times a level equal to or greater than a speciﬁc value was\nreached. This partially preserves information on the distribu-\ntion of the strength levels over time. The sum of the histogram\nis normalized to one, and the distance between two histograms\nis computed by interpreting them as 2000-dimensional vectors\nin a Euclidean space.\nExamples for periodicity histograms are given in Figure 4. The\nhistogramhasclearedgesifaparticularstrengthlevelisreached\nconstantly and the edges will be very blurry if there are strong\nvariations in the strength level. It is important to notice that\nthe beats of music with strong variations in tempo cannot be\ndescribed using this approach. Furthermore, not all 2000 di-\nmensionscontaininformation. Manyarehighlycorrelated,thus\nit makes sense to compress the representation using principal\ncomponent analysis. For the experiments presented in this pa-\nper we used the ﬁrst 60 principal components.\nA ﬁrst quantitative evaluation of the periodicity histograms in-\ndicated that they are not well suited to measure the similarity\nof genres or artists in contrast to measures which use spectrum\ninformation (Pampalk et al., 2003a). One reason might be that\nthepiecesofanartistmightbebetterdistinguishableintermsof\nrhythmthantimbre. However,itisalsoimportanttorealizethat\nusing periodicity histograms in this simple way (i.e., interpret-\ningthemasimagesandcomparingthempixel-wise)todescribe\nrhythm has severe limitations. For example, the distance be-\ntween two pieces with strong peaks at 60bpm and 200bpm is\nthesameasbetweenpieceswithpeaksat100bpmand120bpm.\n2.3 Spectrum Histogram\nTo model timbre it is necessary to take into account which fre-\nquency bands are active simultaneously – information we ig-\nnore in the periodicity histograms. A popular choice for de-\nscribingsimultaneousactivationsinacompressedformaremel\nfrequency cepstrum coefﬁcients. Successful applications have\nbeenreported,forexample,byFoote(1997);Logan(2000);Lo-\ngan and Salomon (2001); Aucouturier and Pachet (2002b).\nLogan and Salomon (2001) suggested an interesting approach\nwhere a piece of music is described by spectra which occur\nfrequently. Two pieces are compared using the earth mover’s\ndistance (Rubner et al., 1998) which is a relatively expensive\ncomputation compared to theEuclidean distance.\nAucouturier and Pachet (2002a,b) presented a similar approach\nusing Gaussian mixture models to summarize the distribution\nofspectrawithinapiece. Tocomparetwopiecesthelikelihood\nthat samples from one mixture were generated by another is\ncomputed.\nAlthough the approach presented by Foote (1997) offers a vec-torspaceinwhichprototypebasedclusteringcanbeperformed\nefﬁcientlytheapproachdoesnotcopewellwithnewpieceswith\nsigniﬁcantly different spectral characteristics compared to the\nones used for training.\nComparedtothesepreviousapproachesweusearelativelysim-\nple technique to model spectral characteristics. In particular,\nwe use the same technique introduced for the periodicity his-\ntograms to capture information on variations of the spectrum.\nThe2-dimensionalhistogramhas20rowsforthecritical-bands\nand 50 columns for the loudness resolution. The histogram\ncountshowmanytimesaspeciﬁcloudnessinaspeciﬁccritical-\nbandwasreachedorexceeded. Thesumofthehistogramisnor-\nmalizedto1. Inourexperimentswereducedthedimensionality\nofthe1000-dimensionalvectorsto30dimensionsusingprinci-\npal component analysis. Examples of spectrum histograms are\ngiven in Figure 4. It is important to note that the spectrum his-\ntogram does not model many important aspects of timbre such\nas the attack of an instrument.\nA ﬁrst quantitative evaluation (Pampalk et al., 2003a) of the\nspectrum histograms indicated that they are suited to describe\nsimilarities in terms of genres or artists and even outperformed\nmore complex spectrum-based approaches such the those sug-\ngestbyLoganandSalomon(2001)andAucouturierandPachet\n(2002b).\n3 Organization and Visualization\nThe spectrum and periodicity histograms give us orthogonal\nviews of the same data. In addition we combine these 2 views\nwith a meta-information-based view. This meta-information\nviewcouldbeanytypeofviewforwhichnovectorspacemight\nexist, for example an organization of pieces according to per-\nsonal taste, artists, genres. Generally any arbitrary view and\nresulting organization is applicable which can be laid out on a\nmap.\nWe use a new technique, called aligned-SOMs (Pampalk et al.,\n2003b; Pampalk, 2003), to integrate these different views and\npermit the user to explore the relationships between them. In\nthis section we review the SOM algorithm, the smoothed data\nhistogram visualization, and specify the aligned-SOM imple-\nmentationweuseforourdemonstration. Weillustratethetech-\nniques using a simple dataset of animals.\n3.1 Self-Organizing Maps\nThe self-organizing map (Kohonen, 1982, 2001) is an unsuper-\nvised neural network with applications in various domains in-\ncludingaudioanalysis(e.g.Cosietal.,1994;FeitenandG ¨unzel,\n1994; Spevak and Polfreman, 2001; Fr ¨uhwirth and Rauber,\n2001). Alternativesincludemulti-dimensionalscaling(Kruskal\nand Wish, 1978), Sammon’s mapping (Sammon, 1969), and\ngenerative topographic mapping (Bishop et al., 1998). The ap-\nproachwepresentcanbeimplementedusinganyofthese,how-\never,wehavechosentheSOMbecauseofitscomputationalef-\nﬁciency.\nThe objective of the SOM is to map high-dimensional data to\na 2-dimensional map in such a way that similar items are lo-\ncated close to each other. The SOM consists of an ordered\nset of units which are arranged in a 2-dimensional visualiza-\ntion space, referred to as the map. Common choices to arrange\nthe map units are rectangular or hexagonal grids. Each unit isassigned a model vector in the high-dimensional data space. A\ndata item is mapped to the best matching unit which is the unit\nwiththemostsimilarmodelvector. TheSOMcanbeinitialized\nrandomly, i.e., random vectors in the data space are assigned to\neach model vector. Alternatives include, for example, initializ-\ning the model vectors using the ﬁrst two principal components\nof the data (Kohonen, 2001).\nAfter initialization 2 steps are repeated iteratively until conver-\ngence. The ﬁrst step is to ﬁnd the best matching unit for each\ndata item. In the second step the model vectors are updated so\nthattheyﬁtthedatabetterundertheconstraintthatneighboring\nunits represent similar items. The neighborhood of each unit\nis deﬁned through a neighborhood function and decreases with\neach iteration.\nTo formalize the basic SOM algorithm we deﬁne the data ma-\ntrixD, the model vector matrix Mt, the distance matrix U, the\nneighborhoodmatrix Nt,thepartitionmatrix Pt,andthespread\nactivationmatrix St. Thedatamatrix Disofsize n×dwhere n\nis the number of data items and dis the number of dimensions.\nThe model vector matrix Mtis of size m×d, where mis the\nnumber of map units. The values of Mtare updated in each it-\neration t. Thesquareddistancematrix Uofsize m×mdeﬁnes\nthe distance between the units on the map. The neighborhood\nmatrixNtcan be calculated, for example, as\nNt=e−U/(2r2\nt), (5)\nwhere rtdeﬁnes the neighborhood radius and monotonically\ndecreaseswitheachiteration. Ntisofsize m×m,symmetrical,\nwith high values on the diagonal, and represents the inﬂuence\nof one unit on another. The sparse partition matrix Ptof size\nn×mis calculated given DandMt,\nPt(i, j) =/braceleftBigg\n1,if unit jis the best match for item i,\n0,otherwise.(6)\nThe spread activation matrix St, with size n×m, deﬁnes the\nresponsibility of each unit for each data item at iteration tand\nis calculated as\nSt=PtNt. (7)\nAttheendofeachloopthenewmodelvectors Mt+1arecalcu-\nlated as\nMt+1=S∗\ntD, (8)\nwhereS∗\ntdenotes the spread activation matrix which has been\nnormalizedsothatthesumoverallrowsineachcolumnequals\n1 except for units to which no items are mapped.\nThere are two main parameters for the SOM algorithm. One is\nthemapsize,theotheristheﬁnalneighborhoodradius. Alarger\nmap gives a higher resolution of the mapping but is computa-\ntionally more expensive. The ﬁnal neighborhood radius deﬁnes\nthesmoothnessofthemappingandshouldbeadjusteddepend-\ning on the noise level in the data.\nVarious methods to visualize clusters based on the SOM have\nbeen developed. We use smoothed data histograms (Pampalk\net al., 2002b) where each data item votes for the map units\nwhich represent it best based on some function of the distance\nto the respective model vectors. All votes are accumulated for\neachmapunitandtheresultingdistributionisvisualizedonthe\nmap. Arobustrankingfunctionisusedtogatherthevotes. Theunit closest to a data item gets npoints, the second n-1, the\nthird n-2 and so forth, for the nclosest map units. Basically\ntheSDHapproximatestheprobabilitydensityofthedataonthe\nmap, which is then visualized using a color code (see Figures 2\nand3). AMatlabtoolboxfortheSDHcanbedownloadedfrom\nhttp://www.oefai.at/˜elias/sdh/.\n3.2 Aligned-SOMs\nTheSOMisausefultoolforexploringadatasetaccordingtoa\ngiven similarity measure. However, when exploring music the\nconcept of similarity is not clearly deﬁned since there are sev-\neralaspectstoconsider. Aligned-SOMs(Pampalketal.,2003b;\nPampalk, 2003) are an extension to the basic SOM which al-\nlowforinteractivelyshiftingthefocusbetweendifferentaspects\nand exploring the resulting gradual changes in the organization\nof the data. The aligned-SOMs architecture consists of several\nmutually constrained SOMs stacked on top of each other. Each\nmap has the same number of units arranged in the same way\n(e.g. on a rectangular grid) and all maps represent the same\npieces of music, but organized with a different focus in terms\nof, for example, aspects of timbre or rhythm.\nTheindividualSOMsaretrainedsuchthateachlayermapssim-\nilar data items close to each other within the layer, and neigh-\nboring layers are further constrained to map the same items to\nsimilar locations. To that end, we deﬁne a distance between in-\ndividual SOM layers, which is made to depend on how similar\nthe respective views are. The information between layers and\ndifferentviewsofthesamelayerissharedbasedonthelocation\nof the pieces on the map. Thus, organizations from arbitrary\nsources can be aligned.\nWe formulate the aligned-SOMs training algorithm based on\nthe formulation of the batch-SOM in the previous section. To\ntrain the SOM layers we extend the squared distance matrix U\nto contain the distances between all units in all layers, thus the\nsize ofUisml×ml, where mis the number of units per layer\nandlis the total number of layers. The neighborhood matrix\nis calculated according to Equation 5. For each aspect of simi-\nlarity aa sparse partition matrix Patof size n×mlis needed.\nInthedemonstrationdiscussedinsection4thereare3different\naspects. Two are calculated from the spectrum and periodicity\nhistogramsandoneisbasedonmeta-information. Thepartition\nmatricesfortheﬁrsttwoaspectsarecalculatedusingEquation6\nwith the extension that the best matching unit for a data item is\nselected for each layer. Thus, the sum of each row equals the\nnumber of layers. The spread activation matrix Satfor each\naspect ais calculated as in Equation 7. For each aspect aand\nlayer i, mixing coefﬁcients waiare deﬁned with/summationtext\nawai= 1\nthat specify the relative strength of each aspect. The spread ac-\ntivation for each layer is calculated as\nSit=/summationdisplay\nawaiSait (9)\nFinally, for each layer iand aspect awith data Dathe updated\nmodel vectors Mait+1are calculated as\nMait+1=S∗\nitDa, (10)\nwhereS∗\nitdenotes the normalized columns of Sit.\nInourdemonstrationweinitializedthealigned-SOMsbasedon\nthe meta-information organization for which we assumed thata\n b\n c\n d\n e\nFigure 2: Aligned-SOMs trained with a small animal dataset showing changes in the organization, (a) ﬁrst layer with weighting\nratio1:0betweenappearanceandactivityfeatures,(b)ratio3:1,(c)ratio1:1,(d)ratio1:3,(e)lastlayerwithratio0:1. Theshadings\nrepresent the density calculated using SDH ( n= 2 with bicubic interpolation).\nonly the partition matrix is given. Thus, for the 2 views based\non vector spaces, ﬁrst the partition matrices are initialized then\nthe model vectors are calculated from these.\nThe necessary resources in terms of CPU time and memory\nincrease rapidly with the number of layers and depend on the\ncomplexity of the feature extraction parameters analyzed. The\noverall computational load is of a higher order of magnitude\nthantrainingasingleSOM.Forlargerdatasetsseveraloptimiza-\ntionsarepossible,inparticular,applyinganextendedversionof\nthefastwinnersearchproposedbyKaski(1999)wouldimprove\ntheefﬁciencydrastically,sincethereisahighredundancyinthe\nmultiple layer structure.\nToillustratethealigned-SOMsweuseasimpledatasetcontain-\ning 16 animals with 13 boolean features describing their ap-\npearance and activities such as size, number of legs, ability to\nswim, and so forth (Kohonen, 2001). We trained 31 layers of\nSOMs using the aligned-SOM algorithm. The ﬁrst layer uses a\nweighting ratio between the aspects of appearance and activity\nof1:0. The16thlayer,i.e.,thecenterlayer,weightsbothaspects\nequally. The last layer uses a weighting ratio of 0:1, focusing\nonly on activities. The weighting ratios of all other layers are\nlinearly interpolated.\nFive layers from the resulting aligned-SOMs are shown in Fig-\nure 2. For interactive exploration an HTML version with all\n31 layers is available on the Internet.1When the focus is only\non appearance all small birds are located together in the lower\nright corner of the map. The Eagle is an outlier because of its\nsize. On the other hand, all mammals are located in the up-\nper half of the map separating the medium sized ones on the\nleft from the large ones on the right. As the focus is gradually\nshifted to activity descriptors the organization changes. In par-\nticular, predators are now located on the left and others on the\nright. Although there are several signiﬁcant changes regarding\nindividuals,theoverallstructurehasremainedlargelythesame,\nenabling the user to easily identify similarities and differences\nbetween two different ways of viewing the same data.\n4 Demonstration\nTo demonstrate our approach on musical data we have imple-\nmented an HTML based interface. A screen-shot is depicted\nin Figure 3, an online demonstration is available.1For this\ndemonstration we use a small collection of 77 pieces from dif-\nferent genres which we have also used in previous demonstra-\n1http://www.oefai.at/˜elias/aligned-soms/tions(Pampalketal.,2002a). Althoughrealisticsizesformusic\ncollectionsaremuchlarger,webelievethatevensmallnumbers\ncan be of interest as they might occur, for example, in a result\nsetofaquerysuchasthetop100inthecharts. Thelimitationin\nsize is mainly induced by our simple HTML interface. Larger\ncollectionswouldrequireahierarchicalextensionthat,e.g.,rep-\nresentseachislandonlybythemosttypicalmemberandallows\nthe user to zoom in and out.\nThe user interface (see Figure 3) is divided into 4 parts: the\nnavigationunit,themap,andtwocodebookvisualizations. The\nnavigation unit has the shape of a triangle, where each corner\nrepresentsanorganizationaccordingtoaparticularaspect. The\nmeta-information view is located at the top, periodicity on the\nleft, and spectrum on the right. The user can navigate between\nthese views by moving the mouse over the intermediate nodes,\nwhich results in smooth changes of the map. In total there are\n73 different nodes the user can browse.\nThe meta-information view we use in this demonstration was\ncreatedmanuallybyplacingthepiecesonthemapaccordingto\npersonal taste. For example, all classical pieces in the collec-\ntionaremixedtogetherintheupperleft. Ontheotherhand,the\nisland in the upper right of the map represents pieces by Bom-\nfunk MCs . The island in the lower right contains a mixture of\ndifferent pieces by Papa Roach ,Limp Bizkit ,Guano Apes , and\nothers which are partly very aggressive. The other islands con-\ntain more or less arbitrary mixtures of pieces, although the one\nlocated closer to the Bomfunk MCs island contains music with\nstronger beats.\nThe current position in the triangle is indicated with a red\nmarker which is located in the top corner in the screen-shot.\nThus,thecurrentmapdisplaystheorganizationbasedonmeta-\ninformation.\nBelow the map are the two codebook visualizations, i.e., the\nmodelvectorsforeachunit. Thisallowsustointerpretthemap.\nThe codebooks explain why a particular piece is located in a\nspeciﬁc region and what the differences between regions are.\nIn particular, the codebook visualizations reveal that the user\ndeﬁned organization is not completely arbitrary with respect to\nthe features extracted from the audio. For example, the period-\nicityhistogramhasthehighestpeaksaroundtheBomfunkMCs\nisland and the spectrum histogram has a characteristic shape\naroundtheclassicalmusicisland. Thisshapeischaracteristicof\nmusic with little energy in high frequencies. The shadings are\na result of the high variations in the loudness, while the overall\nrelatively thin shape is due to the fact that the maximum levelFigure 3: Screenshot of the HTML-based user interface. The navigation unit is located in the upper left, the map is to its right,\nand beneath the map are the codebook visualizations, where each subplot represents a unit of the 10 ×5 SOM trained on 77 pieces\nof music. On the left are the periodicity histogram codebooks. The x-axis of each subplot represents the range from 40 (left) to\n240bpm(right)witharesolutionof5bpm. They-axisrepresentsthestrengthlevelofaperiodicbeatattherespectivefrequency. The\ncolor shadings correspond to the number of frames within a piece that reach or exceed the respective strength level at the speciﬁc\nperiodicity. On the right are the spectrum histogram codebooks. Each subplot represents a spectrum histogram mirrored on the\ny-axis. The y-axis represents the 20 critical-bands while the x-axis represents the loudness. The color shadings correspond to the\nnumber of frames within a piece that reach or exceed the respective loudness in the speciﬁc critical-band.\nof loudness is not constantlyexploited.\nThe codebooks of the extreme perspectives are shown in Fig-\nure 4. When the focus is only on one aspect (e.g., periodicity)\nthemodelvectorsoftheSOMcanbetteradapttovariationsbe-\ntween histograms and thus represent them with higher detail.\nAlso noticeable is how the organization of the model vectors\nchanges as the focus is shifted. For instance, the structure of\nthe spectrum codebook becomes more pronounced as the focus\nshifts to spectral aspects.\nAn important characteristic of aligned-SOMs is the global\nalignment of different views. This is conﬁrmed by investigat-\ning the codebooks. For instance, the user deﬁned organization\nforcestheperiodicitypatternsofmusicbyBomfunkMCstobe\nlocated in the upper right. If trained individually, these period-\nicity histograms are found in the lower right which is furthestfrom the upper left where pieces such as, e.g., F¨ur Eliseby\nBeethoven can be found.\nFigure 5 shows the shapes of the islands for the two extreme\nviews focusing only on spectrum or periodicity. When the fo-\ncus is on spectral features the island of classical music (upper\nleft) is split into two islands where Hrepresents piano pieces\nandIorchestra. Inspectingthecodebookrevealsthatthediffer-\nenceisthatorchestramusicusesabroaderfrequencyrange. On\ntheotherhand,whenthefocusisonperiodicityalargeislandis\nformed which accommodates all classical pieces on one island\nA.Thisislandisconnectedtoisland Gwherealsonon-classical\nmusic can be found such as the song Little Drummer Boy by\nCrosby & Bowie or Yesterday by the Beatles. Although there\nare several differences between the maps the global orientation\nremains the same. In particular the islands AandH/I;CandJ;\nD/EandK;GandMcontainlargelythesamepiecesandcorre-a\n b\nc\n d\nFigure 4: Codebooks depicting the underlying organization. (a) and (b) represent the codebooks of the aligned-SOM organized\naccording to periodicity histograms while (c) and (d) are organized according to the spectrum histograms. The visualization is the\nsame as in Figure 3 with a different color scale.\na\n b\nFigure 5: Two extreme views of the data and the resulting Islands of Music. On the left (a) the focus is solely on the periodicity\nhistograms, on the right (b) the focus is solely on spectrum histograms.\nspondtotheglobalorganizationbasedonthemeta-information.\n5 Conclusions\nWehavepresentedanewapproachtoexploremusiccollections\nby navigating through different views. We proposed two com-\nplementary similarity measures, namely, the spectrum and pe-\nriodicityhistogramswhichdescribetimbreandrhythm,respec-\ntively. Wecombinedthesetwoaspectsofsimilaritywithathird\nview which is not based on audio analysis. This third view can\nbe any organization based on arbitrary meta-information.\nUsingalignedself-organizingmapsweimplementedanHTML\ninterface were the user can gradually change focus from one\nview to another while exploring how the organization of the\ncollection changes smoothly. Preliminary results are very en-\ncouraging given the simple similarity measures we use.\nFuture work will address the two main limitations of our ap-\nproach. First of all, major quality improvements could be\nachieved by better models for perceived similarity. The ap-\nproach we presented is independent of the speciﬁc similarity\nmeasure. Eitherofthetwosuggestedmeasurescaneasilybere-placed. Furthermore, the number of different views is not lim-\nited. For example, to study expressive piano performances the\naligned-SOMswereappliedtointegrate5differentviews(Pam-\npalk et al., 2003b).\nAnother direction for future work is to incorporate new hierar-\nchical extensions into the user interface to efﬁciently explore\nlarge collections. Current hierarchical extensions to the self-\norganizing map (e.g. Dittenbach et al., 2002) cannot be applied\ndirectlytothealigned-SOMandsmootheddatahistogramvisu-\nalization approach. However, using smoothed data histograms\nto reveal hierarchical structures seems promising (Pampalk\net al., 2002b).\nAcknowledgements\nThis research was supported by the EU project HPRN-CT-\n2000-00115(MOSART)andtheprojectY99-INF,sponsoredby\nthe Austrian Federal Ministry of Education, Science and Cul-\nture (BMBWK) in the form of a START Research Prize. The\nBMBWK also provides ﬁnancial support to the Austrian Re-\nsearch Institute for Artiﬁcial Intelligence.References\nAucouturier, J.-J. and Pachet, F. (2002a). Finding songs that\nsound the same. In Proc IEEE Workshop on Model Based Pro-\ncessing and Coding of Audio .\nAucouturier,J.-J.andPachet,F.(2002b). Musicsimilaritymea-\nsures: What’s the use? In Proc Intl Conf on Music Information\nRetrieval, Paris.\nBishop, C. M., Svens ´en, M., and Williams, C. K. I. (1998).\nGTM: The generative topographic mapping. Neural Compu-\ntation, 10(1):215–234.\nBladon,R.andLindblom,B.(1981). Modelingthejudgmentof\nvowel quality differences. J Acoust Soc Am , 69(5):1414–1422.\nCano, P., Kaltenbrunner, M., Gouyon, F., and Batlle, E. (2002).\nOntheuseoffastmapforaudioretrievalandbrowsing. In Proc\nIntl Conf on Music Information Retrieval , Paris.\nCosi, P., Poli, G. D., and Lauzzana, G. (1994). Auditory mod-\neling and self-organizing neural networks for timbre classiﬁca-\ntion.Journal of New Music Research , 23:71–98.\nDittenbach M., Rauber A., Merkl D. (2002). Uncovering hi-\nerarchical structure in data using the growing hierarchical self-\norganizing map. Neurocomputing , 48(1-4):199–216.\nDixon, S., Pampalk, E., and Widmer, G. (2003). Classiﬁcation\nof dance music by periodicity patterns. In Proc Intl Conf on\nMusic Information Retrieval , Washington DC.\nFeiten, B. and G ¨unzel, S. (1994). Automatic indexing of a\nsound database using self-organizing neural nets. Computer\nMusic Journal , 18(3):53–65.\nFoote, J., Cooper, M., and Nam, U. (2002). Audio retrieval by\nrhythmic similarity. In Proc Intl Conf on Musical Information\nRetrieval, Paris.\nFoote,J.T.(1997). Content-basedretrievalofmusicandaudio.\nInProc SPIE Multimedia Storage and Archiving Systems II .\nFr¨uhwirth,M.andRauber,A.(2001). Self-organizingmapsfor\ncontent-based music clustering. In Proc Italian Workshop on\nNeural Nets , Vietri sul Mare, Italy. Springer.\nKaski,S.(1999).FastwinnersearchforSOM-basedmonitoring\nand retrieval of high-dimensional data. In Proc Intl Conf on\nArtiﬁcial Neural Networks , London.\nKohonen,T.(1982). Self-organizingformationoftopologically\ncorrect feature maps. Biological Cybernetics , 43:59–69.\nKohonen, T. (2001). Self-Organizing Maps . Springer, 3rd edi-\ntion.\nKruskal, J. B. and Wish, M. (1978). Multidimensional Scaling .\nSage Publications.\nLogan, B. (2000). Mel frequency cepstral coefﬁcients for mu-\nsic modeling. In Proc Intl Symposium on Music Information\nRetrieval (ISMIR’00) , Plymouth, MA.\nLogan, B. (2002). Content-based playlist generation: Ex-\nploratory experiments. In Proc Intl Conf on Music Information\nRetrieval, Paris.Logan,B.andSalomon,A.(2001). Amusicsimilarityfunction\nbasedonsignalanalysis. In ProcIEEEIntlConfonMultimedia\nand Expo , Tokyo.\nMoelants, D. (2002). Preferred tempo reconsidered. In Proc\nIntl Conf on Music Perception and Cognition , Sydney.\nPampalk, E. (2003). Aligned self-organizing maps. In Proc\nWorkshop on Self-OrganizingMaps , Kitakyushu, Japan.\nPampalk, E., Dixon, S., and Widmer, G. (2003a). On the eval-\nuationofperceptualsimilaritymeasuresformusic. In ProcIntl\nConf on Digital Audio Effects , London.\nPampalk, E., Goebl, W., and Widmer, G. (2003b). Visualizing\nchangesinthestructureofdataforexploratoryfeatureselection.\nInProc ACM SIGKDD Intl Conf on Knowledge Discovery and\nData Mining , Washington DC.\nPampalk,E.,Rauber,A.,andMerkl,D.(2002a). Content-based\norganization and visualization of music archives. In Proc ACM\nMultimedia , Juan les Pins, France.\nPampalk, E., Rauber, A., and Merkl, D. (2002b). Using\nsmoothed data histograms for cluster visualization in self-\norganizing maps. In Proc Intl Conf on Artiﬁcal Neural Net-\nworks, Madrid.\nPaulus, J. and Klapuri, A. (2002). Measuring the similarity of\nrhythmic patterns. In Proc Intl Conf of Music Information Re-\ntrieval, Paris.\nRubner,Y.,Tomasi,C.,andGuibas,L.(1998). Ametricfordis-\ntributions with applications to image databases. In Proc IEEE\nIntl Conf on Computer Vision .\nSammon, J. W. (1969). A nonlinear mapping for data structure\nanalysis. IEEE Transactions on Computers , 18:401–409.\nScheirer, E. D. (1998). Tempo and beat analysis of acoustic\nmusical signals. J Acoust Soc Am , 103(1):588–601.\nSchroeder,M.R.,Atal,B.S.,andHall,J.L.(1979). Optimizing\ndigital speech coders by exploiting masking properties of the\nhuman ear. J Acoust Soc Am , 66(6):1647–1652.\nSpevak, C. and Polfreman, R. (2001). Sound Spotting – A\nFrame-Based Approach. In Proc Intl Symposium of Music In-\nformation Retrieval , Bloomington, IN.\nTerhardt, E. (1979). Calculating virtual pitch. Hearing Re-\nsearch, 1:155–182.\nThiede, T., Treurniet, W. C., Bitto, R., Schmidmer, C., Sporer,\nT.,Beerends,J.G.,Colomes,C.,Keyhl,M.,Stoll,G.,Branden-\nburg, K., and Feiten, B. (2000). PEAQ – The ITU standard for\nobjective measurement of perceived audio quality. Journal of\nthe Audio Engineering Society , 48(1/2):3–27.\nTzanetakis,G.andCook,P.(2001). Aprototypeaudiobrowser-\neditor using a large scale immersive visual audio display. In\nProc Intl Conf on Auditory Display .\nTzanetakis, G. and Cook, P. (2002). Musical genre classiﬁca-\ntion of audio signals. IEEE Transactions on Speech and Audio\nProcessing , 10(5):293–302.\nZwicker, E. and Fastl, H. (1999). Psychoacoustics, Facts and\nModels. Springer, 2nd edition."
    },
    {
        "title": "Rhythmic similarity through elaboration.",
        "author": [
            "R. Mitchell Parry",
            "Irfan A. Essa"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1416738",
        "url": "https://doi.org/10.5281/zenodo.1416738",
        "ee": "https://zenodo.org/records/1416738/files/ParryE03.pdf",
        "abstract": "Rhythmic similarity techniques for audio tend to eval- uate how close to identical two rhythms are. This pa- per proposes a similarity metric based on rhythmic elaboration that matches rhythms that share the same beats regardless of tempo or identicalness. Elabora- tions can help an application decide where to transi- tion between songs. Potential applications include au- tomatically generating a non-stop music mix or soni- cally browsing a music library. 1",
        "zenodo_id": 1416738,
        "dblp_key": "conf/ismir/ParryE03",
        "keywords": [
            "rhythmic similarity",
            "evaluating how close to identical",
            "proposes a similarity metric",
            "based on rhythmic elaboration",
            "matches rhythms that share the same beats",
            "tempo or identicalness",
            "Elaborations can help an application decide",
            "transition between songs",
            "automatically generating a non-stop music mix",
            "sonically browsing a music library"
        ],
        "content": "Rhythmic Similarity through Elaboration\nMitchell Parry and IrfanEssa\nCollege of Computing / GVUCenter\nGeorgia Institute of Technology\nAtlanta, GA 30332-0280\n{parry,irfan }@cc.gatech.edu\nAbstract\nRhythmicsimilaritytechniquesforaudiotendtoeval-\nuate how close to identical two rhythms are. This pa-\nper proposes a similarity metric based on rhythmic\nelaboration that matches rhythms that share the same\nbeats regardless of tempo or identicalness. Elabora-\ntions can help an application decide where to transi-\ntionbetweensongs. Potentialapplicationsincludeau-\ntomatically generating a non-stop music mix or soni-\ncally browsing a music library.\n1 Introduction\nThispaperproposesasimilaritymetricbasedonrhythmicelab-\noration that matches rhythms that share the same beats re-\ngardless of tempo or identicalness. Using this metric, we can\nidentify relatively complex or elaborated rhythms and repeated\nrhythmic patterns within a song or collection. Rhythmic sim-\nilarity metrics can be applied at multiple levels. Some tech-\nniques (Tzanetakis and Cook, 2002; Foote et. al., 2002) de-\nrive a single representation for an entire song. These tech-\nniquesaidinsongretrievalandautomaticallyorderingsongsin\naplaylist. Othertechniques,suchasPaulusandKlapuri(2002),\ncompare rhythms at the measure level using low-level features.\nTheir technique allows approximate matches and small tempo\nchanges. FooteandCooper(2001)revealhighlevelrepetitionin\nmusicbyvisualizingaself-similaritymatrix. Tanguiane(1993)\nhas proposed symbolic music to compare rhythms with a dif-\nferent number of beats and equal total duration. He uses the\nconcept of elaboration to label rhythmic phrases. An elabora-\ntion of a rhythm is a rhythm that contains all the same onsets.\nHe expresses rhythms as bit vectors, Rk=< r 1, r2, ..., r n>,\nandidentiﬁeselaborationsbythesimplerelation: Riisanelab-\noration of RjifRi·Rj=Rj·Rj. Clearly, if RiandRjare\nelaborations of each other, they are identical. Elaborations ap-\nplied to audio help decide where to transition between songs.\nPotential applications include automatically generating a non-\nstop music mix or sonicallybrowsing a music library.\nPermission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided that\ncopies are not made or distributed for proﬁt or commercial advan-\ntage and that copies bear this notice and the full citation on the ﬁrst\npage.c/circlecopyrt2003 Johns Hopkins University.2 Methods\nWe use a variant on Scheirer’s (1998) beat analysis that uses\nthediscretewavelettransformasproposedbyTzanetakiset. al.\n(2001) instead of a ﬁlterbank. Starting with a 22 kHz mono\nsignal we output a 172 Hz beat envelope. We chose to parti-\ntion the signal into measure-length segments. Paulus and Kla-\npuri (2002) describe a probabilistic algorithm for determining\nthe measure length from an audio signal. Using this measure\nlength, we divide the song into non-overlapping adjacent seg-\nments.\n2.1 Similarity\nWe adapt Tanguiane’s (1993) method for ﬁnding elaborations\nto our amplitude envelope segments. To account for phase and\namplitudedifferences,eachpairofsegmentsareshiftedaccord-\ning to the peak in the cross-correlation and normalized by their\nmaximumvalue. Thenallpairsareevaluatedbytheasymmetric\nrelation,\nelab(si, sj) = 1 −min( si·sj, sj·sj)\nmax( si·sj, sj·sj).(1)\nSegment siisanelaborationofsegment sjifelab(si, sj)isnear\nzero. If siandsjare identical, elab(si, sj)andelab(sj, si)is\nzero. For comparison, we use the symmetric cosine distance,\ndcos(si, sj) = 1 −si·sj\n/bardblsi/bardbl/bardblsj/bardbl. (2)\nTheelaboration andcosine relationgeneratesa matrixthat rep-\nresents the entire song.\n2.2 Complexity\nComplexity can be evaluated on two levels: within a song and\nbetween songs. To evaluate the complexity of a rhythmic seg-\nment relative to the rest of the song, we can use the row and\ncolumnsumsinourelaborationmatrix. Thedifferenceofthese\ntwosumsispositiveforsegmentsthattendtohaveelaborations\nand negative for segments that tend to be elaborations. We use\nthe function,\ncomplexity (si) =/summationdisplay\njelab(si, sj)−elab(sj, si).(3)\nComplexity can also be evaluated between songs (i.e. the com-\nplexityintransitioningbetweentwosegments). Theapplication\nhere is to ﬁnd good transitions between songs. The best candi-\ndatesfortransitionsarethosethatareelaborationsofeachother.Figure 1: Cosine (left) and elaboration (right) similarity matrices\nFor this application, we do not care about the direction of the\nelaboration. We assume that it is equally pleasing to transition\ntoamoreelaboratedrhythmorasimplerrhythm. Thereforewe\nuse a transition rating whereratings near zero are ideal:\ntrans (si, sj) = min( elab(si, sj), elab (sj, si)).(4)\n3 Results\nBy visualizing the elaboration matrix within a song we can\nshow the added information it provides. Figure 1 shows the\ncosinesimilaritymatrix(left)andtheelaborationmatrix(right)\nfor the ﬁrst half of ”Down In It (Demo)” by Nine Inch Nails.\nTime runs down and to the right. White indicates high simi-\nlarity (value near zero). Repetition can be seen on both ma-\ntrices where white rectangles appear off the diagonal, for in-\nstance, segments 2-12 and 22-27. Another more subtle repeti-\ntion occurs in the checkerboard pattern during segments 14-21\nand 30-36. These regions are outlined in black for comparison\nbetween the matrices. The cosine similarity matrix is generally\ndark where repetition does not occur. The elaboration matrix\ncontains white rows and dark columns that indicate a relatively\nelaboratedpattern,andviceversaforrelativelysimplepatterns.\nFor instance, row 35 is bright and column 35 is dark. Instances\nwhere elab(si, sj)andelab(sj, si)is relatively dark, indicate\nthat these rhythms are wholly different (i.e. they each contain\nbeats that the other does not).\nIn order to compare rhythms between songs, we use a collec-\ntion of 38 songs from the Dave Matthews Band. We extract the\nﬁrst measure from each song and time stretch using commer-\ncialsoftwaresothatallrhythmshavethesamelength. Thenwe\ngenerate an elaboration matrix that compares every song to ev-\neryothersong. Figure2showsanexampleofagoodtransition,\ntrans = 0.01,(left)andabadtransition, trans = 0.53,(right).\nFor the good example, notice that all of the beats in the lower\nrhythm are contained in the top rhythm. Therefore, blending\nthese rhythms will provide a smooth transition. In the bad ex-\nample, notice that both rhythms contain beats that do not exist\nin the other. For instance, the top rhythm has a beat at 25 and\nthe bottom has a beat at 330.\n4 Conclusions\nThis paper proposes elaboration as a rhythmic similarity met-\nric. We show that it provides more information than the cosine\nFigure 2: A good transition, trans = 0.01, (left) and a bad\ntransition, trans= 0.53, (right)\ndistance metric. Rhythmic elaboration can be used to identify\nrhythms that share the same beats as a target rhythm. Elabo-\nrations could aid applications that automatically transition be-\ntween multiple audio sources, such as a non-stop music mix or\nsonic browser for music libraries.\nReferences\nFoote, J. & Cooper, M. (2001). Visualizing Musical Structure\nand Rhythm via Self-Similarity. In Schloss, A., Dannenberg,\nR.,&Diessen,P.(Eds.), ProceedingsoftheInternationalCom-\nputer Music Conference , 27, (pp. 419–422). San Francisco,\nCA: International ComputerMusic Association.\nFoote, J., Cooper, M., & Nam, U. (2002). Audio Retrieval by\nRhythmic Similarity. In Fingerhut, M. (Ed.), Proceedings of\nthe International Conference on Music Information Retrieval ,\n3, (pp. 265–266). Paris: IRCAM– Centre Pompidou.\nPaulus, J. & Klapuri, A. (2002). Measuring the Similarity of\nRhythmic Patterns. In Fingerhut, M. (Ed.), Proceedings of the\nInternational Conference on Music Information Retrieval , 3,\n(pp. 150–156). Paris: IRCAM– Centre Pompidou.\nScheirer, E. (1998). Tempo and Beat Analysis of Acoustic Mu-\nsical Signals. Journal of the Acoustical Society of America ,\n103(1), 588–601.\nTanguiane,A.(1993). ArtiﬁcialPerceptionandMusicRecogni-\ntion. New York: Springer-Verlag.\nTzanetakis, G. & Cook, P. (2002). Musical Genre Classiﬁca-\ntionofAudioSignals. IEEETransactionsonSpeechandAudio\nProcessing , 10(5), 293–302.\nTzanetakis, G., Essl, G., & Cook, P. (2001). Audio Analysis\nusingtheDiscreteWaveletTransform. In ProceedingsofWSES\nInternational Conference on Acoustics and Music: Theory and\nApplications ."
    },
    {
        "title": "Effects of song familiarity, singing training and recent song exposure on the singing of melodies.",
        "author": [
            "Steffen Pauws"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1414722",
        "url": "https://doi.org/10.5281/zenodo.1414722",
        "ee": "https://zenodo.org/records/1414722/files/Pauws03.pdf",
        "abstract": "Findings of a singing experiment are presented in which trained and untrained singers sang melodies of familiar and less familiar Beatles songs from memory and after listening to the original song on CD. Results showed that adopting the correct pitch of a melody was done better by trained singers, and only after listening to the song. Contours of melodies were equally well reproduced by both trained and untrained singers. In contrast, the intervals of a melody were sung more accurately by trained singers than by untrained singers. These findings demonstrate the dominance of contour for remembering melodies and the poorer interval encoding of melodies or the lack of essential singing skills by untrained singers. When singing from memory, almost two-third of the singing came reasonably close to the actual tempo on the CD. This improved to more than 70% after listening to the song on CD. In general, the singing of less familiar melodies improved after song listening. Implications for 'query by humming' applications are discussed.",
        "zenodo_id": 1414722,
        "dblp_key": "conf/ismir/Pauws03",
        "keywords": [
            "singing experiment",
            "trained and untrained singers",
            "memorizing melodies",
            "correct pitch",
            "melody contours",
            "melody intervals",
            "contour dominance",
            "interval encoding",
            "essential singing skills",
            "query by humming applications"
        ],
        "content": "\u0001\u0002\u0002\u0003\u0004\u0005\u0006\u0007\b\u0002\u0007\u0006\b\t\n\u0007\u0002\u000b\f\r\u000e\r\u000b\u000f\r\u0005\u0010\u0011\u0007\u0006\r\t\n\r\t\n\u0007\u0005\u000f\u000b\r\t\r\t\n\u0007\u000b\t\u0012\u0007\u000f\u0003\u0004\u0003\t\u0005\u0007\u0006\b\t\n\u0007\u0003\u0013\u0014\b\u0006\u0015\u000f\u0003\u0007\b\t\u0007\u0005\u0016\u0003\u0007 \n\u0006\r\t\n\r\t\n\u0007\b\u0002\u0007\f\u0003\u000e\b\u0012\r\u0003\u0006 \n\u0017\u0005\u0003\u0002\u0002\u0003\t\u0007\u0018\u000b\u0015\u0019\u0006 \n\u0001\u0002\u0003\u0004\u0003\u0005\u0006\u0007\b\t\u0006\t\n\u000b\f\u0002\u0007\r\n\u000e\u000f\u000b\n\u0010\u000f\u000b\u0003\t\u0006\u0007\u0011\u0003\u0012\u0013\u0002\u000f\u0014\t\u0012 \n\u0001\u000b\u000f\u0015\u0016\u0007\u0017\u000f\u0004\u0006\u0010\u0004\n\n\u0012\u0007\u0018 \n\u0019\u001a\u0019\u001a\u0007\u001b\u001b\u0007\u0011\u0003\u0012\u0013\u0002\u000f\u0014\t\u0012\u001c\u0007\u0010\u0002\t\u0007\u001d\t\u0010\u0002\t\u000b\u0004\n\u0012\u0013\u0006 \n\u0002\u0003\u0004\u0005\u0005\u0004\u0006\u0007\b\t\n\u000b\u0002\f\b\r\u000e\u000f\u000e\b\u0002\u0007\u0010\u0011\u0012 \n\u001a\u001b\u0006\u0005\u000f\u000b\u0004\u0005 \n\u001e\u0003\u0012\u0013\u0003\u0012\u001f\u0006\u0007 \u000f\u0015\u0007 \n\u0007 \u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007 \t \u0005\t\u000b\u0003!\t\u0012\u0010\u0007 \n\u000b\t\u0007 \u0005\u000b\t\u0006\t\u0012\u0010\t\u0013\u0007 \u0003\u0012\u0007 \n\"\u0002\u0003\f\u0002\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\n\u0012\u0013\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0006\n\u0012\u001f\u0007!\t\u0004\u000f\u0013\u0003\t\u0006\u0007\u000f\u0015\u0007 \u0015\n!\u0003\u0004\u0003\n\u000b\u0007\n\u0012\u0013\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007$\t\n\u0010\u0004\t\u0006\u0007\u0006\u000f\u0012\u001f\u0006\u0007\u0015\u000b\u000f!\u0007!\t!\u000f\u000b%\u0007 \n\n\u0012\u0013\u0007\n\u0015\u0010\t\u000b\u0007\u0004\u0003\u0006\u0010\t\u0012\u0003\u0012\u001f\u0007\u0010\u000f\u0007\u0010\u0002\t\u0007\u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007\u0006\u000f\u0012\u001f\u0007\u000f\u0012\u0007&'\u0016\u0007 \b\t\u0006#\u0004\u0010\u0006\u0007 \n\u0006\u0002\u000f\"\t\u0013\u0007\u0010\u0002\n\u0010\u0007\n\u0013\u000f\u0005\u0010\u0003\u0012\u001f\u0007\u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0005\u0003\u0010\f\u0002\u0007\u000f\u0015\u0007\n\u0007!\t\u0004\u000f\u0013%\u0007 \"\n\u0006\u0007\u0013\u000f\u0012\t\u0007\u000e\t\u0010\u0010\t\u000b\u0007\u000e%\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u001c\u0007\n\u0012\u0013\u0007\u000f\u0012\u0004%\u0007\n\u0015\u0010\t\u000b\u0007 \u0004\u0003\u0006\u0010\t\u0012\u0003\u0012\u001f\u0007 \u0010\u000f\u0007 \u0010\u0002\t\u0007 \u0006\u000f\u0012\u001f\u0016\u0007 &\u000f\u0012\u0010\u000f#\u000b\u0006\u0007 \u000f\u0015\u0007 !\t\u0004\u000f\u0013\u0003\t\u0006\u0007 \"\t\u000b\t\u0007 \t(#\n\u0004\u0004%\u0007 \"\t\u0004\u0004\u0007 \u000b\t\u0005\u000b\u000f\u0013#\f\t\u0013\u0007 \u000e%\u0007 \u000e\u000f\u0010\u0002\u0007 \u0010\u000b\n\u0003\u0012\t\u0013\u0007 \n\u0012\u0013\u0007 \n#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007 \u0006\u0003\u0012\u001f\t\u000b\u0006\u0016\u0007 )\u0012\u0007 \f\u000f\u0012\u0010\u000b\n\u0006\u0010\u001c\u0007 \u0010\u0002\t\u0007 \u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007 \u000f\u0015\u0007 \n\u0007 \n!\t \u0004\u000f\u0013%\u0007\"\t\u000b\t\u0007\u0006#\u0012\u001f\u0007!\u000f\u000b\t\u0007\n\f\f#\u000b\n\u0010\t\u0004%\u0007\u000e%\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007 \n\u0010\u0002\n\u0012\u0007 \u000e%\u0007 #\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007 \u0006\u0003\u0012\u001f\t\u000b\u0006\u0016\u0007 *\u0002\t\u0006\t\u0007 \u0015\u0003\u0012\u0013\u0003\u0012\u001f\u0006\u0007 \u0013\t!\u000f\u0012\u0006\u0010\u000b\n\u0010\t\u0007 \u0010\u0002\t\u0007 \u0013\u000f!\u0003\u0012\n\u0012\f\t\u0007 \u000f\u0015\u0007 \f\u000f\u0012\u0010\u000f#\u000b\u0007 \u0015\u000f\u000b\u0007 \u000b\t!\t!\u000e\t\u000b\u0003\u0012\u001f\u0007 !\t\u0004\u000f\u0013\u0003\t\u0006\u0007 \n\u0012\u0013\u0007 \u0010\u0002\t\u0007 \u0005\u000f\u000f\u000b\t\u000b\u0007 \u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0007 \t\u0012\f\u000f\u0013\u0003\u0012\u001f\u0007\u000f\u0015\u0007!\t\u0004\u000f\u0013\u0003\t\u0006\u0007\u000f\u000b\u0007\u0010\u0002\t\u0007\u0004\n\f+\u0007\u000f\u0015\u0007\t\u0006\u0006\t\u0012\u0010\u0003\n\u0004\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007 \n\u0006+\u0003\u0004\u0004\u0006\u0007 \u000e%\u0007 #\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007 \u0006\u0003\u0012\u001f \t\u000b\u0006\u0016\u0007 ,\u0002\t\u0012\u0007 \u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007 \u0015\u000b\u000f!\u0007 \n!\t!\u000f\u000b%\u001c\u0007 \n\u0004!\u000f\u0006\u0010\u0007 \u0010\"\u000f -\u0010\u0002\u0003\u000b\u0013\u0007 \u000f\u0015\u0007 \u0010\u0002\t\u0007 \u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007 \f\n!\t\u0007 \n\u000b\t\n\u0006\u000f\u0012\n\u000e\u0004%\u0007\f\u0004\u000f\u0006\t\u0007\u0010\u000f\u0007\u0010\u0002\t\u0007\n\f\u0010#\n\u0004\u0007\u0010\t!\u0005\u000f\u0007\u000f\u0012\u0007\u0010\u0002\t\u0007&'\u0016\u0007*\u0002\u0003\u0006\u0007 \u0003!\u0005\u000b\u000f\u0014\t\u0013\u0007 \u0010\u000f\u0007 !\u000f\u000b\t\u0007 \u0010\u0002\n\u0012\u0007 ./0\u0007 \n\u0015\u0010\t\u000b\u0007 \u0004\u0003\u0006\u0010\t\u0012\u0003\u0012\u001f\u0007 \u0010\u000f\u0007 \u0010\u0002\t\u0007 \u0006\u000f\u0012\u001f\u0007\u000f\u0012\u0007&'\u0016\u0007)\u0012\u0007\u001f\t\u0012\t\u000b\n\u0004\u001c\u0007\u0010\u0002\t\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u000f\u0015\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007 !\t\u0004\u000f\u0013\u0003\t\u0006\u0007\u0003!\u0005\u000b\u000f\u0014\t\u0013\u0007\n\u0015\u0010\t\u000b\u0007\u0006\u000f\u0012\u001f\u0007\u0004\u0003\u0006\u0010 \t\u0012\u0003\u0012\u001f\u0016\u0007)!\u0005\u0004\u0003\f\n\u0010\u0003\u000f\u0012\u0006\u0007 \n\u0015\u000f\u000b\u00071(#\t\u000b%\u0007\u000e%\u0007\u0002#!!\u0003\u0012\u001f2\u0007\n\u0005\u0005\u0004\u0003\f\n\u0010\u0003\u000f\u0012\u0006\u0007\n\u000b\t\u0007\u0013\u0003\u0006\f#\u0006\u0006\t\u0013\u0016 \n\u001c\u001d\t\u0005\u000f\b\u0012\u0015\u0004\u0005\r\b\t \n\u001b\u0004\u0004\u000f\"\u0003\u0012\u001f\u0007\u0005\t\u000f\u0005\u0004\t\u0007\u0010\u000f\u0007\u000b\t\u0010\u000b\u0003\t\u0014\t\u0007\n\u0007\u0006\u000f\u0012\u001f\u0007\u0015\u000b\u000f!\u0007\n\u0007\u0004\n\u000b\u001f\t\u0007\u0014\u000f\u0004#!\t\u0007\u000e%\u0007 \n\u0004\t\u0010\u0010\u0003\u0012\u001f\u0007\u0010\u0002\t!\u0007\u0006\u0003\u0012\u001f\u0007\n\u00071\u0002\u000f\u000f+ -\u0004\u0003\u0012\t2\u0007\u000f\u0015\u0007\u0010\u0002\n\u0010\u0007\u0006\u000f\u0012\u001f\u0007\u0003\u0006\u0007\u000f\u0015\u0010\t\u0012\u0007\u0005\u000b\t\u0006\t\u0012\u0010\t\u0013\u0007 \n\n\u0006\u0007\n\u0007\f\u000f!\u0005\t\u0004\u0004\u0003\u0012\u001f\u0007!\t\n\u0012\u0006\u0007\u0010\u000f\u0007\u0015\n\f\u0003\u0004\u0003\u0010\n\u0010\t\u0007!#\u0006\u0003\f\u0007\u0006\t\u0004\t\f\u0010\u0003\u000f \u0012\u0007\u0003\u0015\u0007\u000f\u0012\u0004%\u0007\n\u0007 \n\u0006\t\u0012\u0006\t\u0007 \u000f\u0015\u0007 \u0010\u0002\t\u0007 !\t\u0004\u000f\u0013%\u0007 \u0003\u0006\u0007 +\u0012\u000f\"\u0012\u001c\u0007 \u000e#\u0010\u0007 \u0012\u000f\u0007 \u0012\n!\t\u0007 \u000f\u0015\u0007 \u0010\u0002\t\u0007 \u0006\u000f\u0012\u001f\u0016\u0007 \u0017\u000f\"\t\u0014\t\u000b\u001c\u0007\u0010\u0002\t\u0006\t\u0007\u0006\u000f -\f\n\u0004\u0004\t\u0013\u00071(#\t\u000b%\u0007\u000e%\u0007\u0002#!!\u0003\u0012\u001f2\u0007\n\u0005\u0005\u0004\u0003\f\n\u0010\u0003\u000f\u0012\u0006\u0007\n\u000b\t\u0007 \n\f\u0002\n\u0004\u0004\t\u0012\u001f\t\u0013\u0007\u000e%\u0007\u0010\u0002\t\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u0005\t\u000b\u0015\u000f\u000b!\n\u0012\f\t\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u001f\t\u0012\t\u000b\n\u0004\u0007\u0005#\u000e\u0004\u0003\f\u001c\u0007\u0003\u0015\u0007 \n\u0010\u0002\t\u0007 \u0010\n\u000b\u001f\t\u0010\u0007 #\u0006\t\u000b\u0007 \u001f\u000b\u000f#\u0005\u0007 \u0003\u0006\u0007 \u0010\u0002\t\u0007 \u001f\t\u0012\t\u000b\n\u0004\u0007 \u0005#\u000e\u0004\u0003\f\u0016\u0007 3\u0012\u0015\u000f\u000b\u0010#\u0012\n\u0010\t\u0004%\u001c\u0007 \u0006\u0010#\u0013 \u0003\t\u0006\u0007\u000f\u0012\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u0005\t\u000b\u0015\u000f\u000b!\n\u0012\f\t\u0006\u0007\n\u000b\t\u0007\u0002\n\u000b\u0013\u0004%\u0007\n\u0014\n\u0003\u0004\n\u000e\u0004\t\u001c\u0007\u0010\u0002\u000f#\u001f\u0002\u0007 \n1(#\t\u000b%\u0007 \u000e%\u0007 \u0002#!!\u0003\u0012\u001f2\u0007 \n\u0005\u0005\u0004\u0003\f\n\u0010\u0003\u000f\u0012\u0006\u0007 \u0012\t\t\u0013\u0007 \u0010\u000f\u0007 \f\n\u0005\u0003\u0010\n\u0004\u00034\t\u0007 \u000f\u0012\u0007 +\u0012\u000f\"\u0004\t\u0013\u001f\t\u0007\n\u000e\u000f#\u0010\u0007\t\u0014\t\u000b%\u0013\n%\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0016 \n*\u0002\t\u000b\t\u0015\u000f\u000b\t\u001c\u0007 \u0010\u0002\t\u0007 \u001f\u000f\n\u0004\u0007 \u000f\u0015\u0007 \u0010\u0002\t\u0007 \f#\u000b\u000b\t\u0012\u0010\u0007 \u0006\u0010#\u0013%\u0007 \"\n\u0006\u0007 \u0010\u000f\u0007 \u0015\u000f\u000b!#\u0004\n\u0010\t\u0007 \n\u0002%\u0005\u000f\u0010\u0002\t\u0006\t\u0006\u0007 \u0015\u000b\u000f!\u0007\u0015\u0003\u0012\u0013\u0003\u0012\u001f\u0006\u0007\u000f\u0015\u0007\u0006\u000f!\t\u0007\u000b\t\u0004\t\u0014\n\u0012\u0010\u0007\t \u0003\u0006\u0010\u0003\u0012\u001f\u0007\u0006\u0010 #\u0013\u0003\t\u0006\u0007 \n\n\u0012\u0013\u0007\u0010\u000f\u0007\u0010\t\u0006\u0010\u0007\u0010\u0002\t\u0006\t\u0007\u0002%\u0005\u000f\u0010\u0002\t\u0006\t\u0006\u0007\u0003\u0012\u0007\n\u0007\f\u000f\u0012\u0010\u000b\u000f\u0004\u0004\t\u0013\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\t \u0005\t\u000b\u0003!\t\u0012\u0010\u0007 #\u0006\u0003\u0012\u001f\u0007\t\u0014\t\u000b%\u0013\n%\u0007!#\u0006\u0003\f\u0016\u0007*\u0002\t\u0007\t \u0005\t\u000b\u0003!\t\u0012\u0010\u0007\t \n!\u0003\u0012\t\u0013\u0007\u0010\u0002\t\u0007\t\u0015\u0015\t\f\u0010\u0006\u0007 \n\u000f\u0012\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007!\t\u0004\u000f\u0013\u0003\t\u0006\u0007\u000f\u0015\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\n\u0012\u0013\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\u000f\u0015\u00071*\u0002\t\u0007 \n$\t\n\u0010\u0004\t\u00062\u001c\u0007\t\u0015\u0015\t\f\u0010\u0006\u0007\u000f\u0015\u0007\u000e\t\u0003\u0012\u001f\u0007\n\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0007\u000f\u000b\u0007\u0012\u000f\u0010\u0007\n\u0012\u0013\u0007\t\u0015\u0015\t\f\u0010\u0006\u0007\u000f \u0015\u0007 \n\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u0015\u000b\u000f!\u0007!\t!\u000f\u000b%\u0007\u000f\u000b\u0007\n\u0015\u0010\t\u000b\u0007\u000b\t\f\t\u0012\u0010\u0007\u0006\u000f\u0012\u001f\u0007\t \u0005\u000f\u0006#\u000b\t\u00075\u0003\u0016\t\u0016\u001c\u0007&'\u0007 \u0004\u0003\u0006\u0010\t\u0012\u0003\u0012\u001f6\u0016\u0007 \n*\u000f\u0007\n\u0012\u0010\u0003\f\u0003\u0005\n\u0010\t\u0007\u0010\u0002\t\u0007\u000b\t\u0006#\u0004\u0010\u0006\u001c\u0007\u0003\u0010\u0007\"\n\u0006\u0007\u0015\u000f#\u0012\u0013\u0007\u0010\u0002\n\u0010\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\"\t\u000b\t\u0007 \n\u000e\t\u0010\u0010\t\u000b\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\t\u0013\u0007\u0010\u0002\n\u0012\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\u000e%\u0007\u000e\u000f\u0010\u0002\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\n\u0012\u0013\u0007 \n#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0016\u0007$\u000f\u0010\u0002\u0007\u001f\u000b\u000f#\u0005\u0006\u0007\u000f\u0015\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0002\n\u0013\u0007\u000e\n\u0013\u0007!\t! \u000f\u000b%\u0007\u0015\u000f\u000b\u0007 \n\u0010\u0002\t\u0007 \f\u000f\u000b\u000b\t\f\u0010\u0007 \u0005\u0003\u0010\f\u0002\u0007 \u000f\u0015\u0007 \n\u0007 \u0006\u000f\u0012\u001f7\u0007 \u0010\u000b\n\u0003\u0012\t\u0013\u0007 \u0006\u0003\u0012\u001f\t\u000b\u0006\u0007 \"\t\u000b\t\u0007 \u000e\t\u0010\u0010\t\u000b\u0007 \u0003\u0012\u0007 \n\u0013\u000f\u0005\u0010\u0003\u0012\u001f\u0007\u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0005\u0003\u0010\f\u0002\u0007\n\u0015\u0010\t\u000b\u0007\u0004\u0003\u0006\u0010\t\u0012\u0003\u0012\u001f\u0007\u0010\u000f\u0007\u0010\u0002\t\u0007\u0006\u000f\u0012\u001f\u0007\u000f\u0012\u0007&'\u0016\u0007 $\u000f\u0010\u0002\u0007\u001f\u000b\u000f#\u0005\u0006\u0007\u0013\u0003\u0013\u0007\u0002\n\u0014\t\u0007\u001f\u000f\u000f\u0013\u0007!\t!\u000f\u000b%\u0007\u0015\u000f\u000b\u0007\u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u001f\u0004\u000f\u000e\n\u0004\u0007 \u0010\t!\u0005\u000f\u0007\u000f\u0015\u0007\n\u0007\u0006\u000f\u0012\u001f\u0016\u0007$\u000f\u0010\u0002\u0007\u001f\u000b\u000f#\u0005\u0006\u0007\u0013\u0003\u0013\u0007\u0012\u000f\u0010\u0007\u0005\t\u000b\u0015\u000f\u000b!\u0007\u0013\u0003\u0015\u0015\t\u000b\t\u0012\u0010\u0004%\u0007\u0003\u0012\u0007 \n\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u0010\u0002 \t\u0007\u001f\u0004\u000f\u000e\n\u0004\u0007!\u000f\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007!\t\u0004\u000f\u0013%\u00075\u0003\u0016\t\u0016\u001c\u0007\u0010\u0002\t\u0007\f\u000f\u0012\u0010\u000f#\u000b6\u0007 \n\f\u000f\u000b\u000b\t\f\u0010\u0004%\u0016\u0007*\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\"\t\u000b\t\u0007!\u000f\u000b\t\u0007\n\f\f#\u000b\n\u0010\t\u0007\u0003\u0012\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u0010\u0002\t\u0007 !\u000f\u000b\t\u0007\u0013\t\u0010\n\u0003\u0004\t\u0013\u0007!\t\u0004\u000f\u0013\u0003\f\u0007!\u000f\u0010\u0003\u000f\u0012\u00075\u0003\u0016\t\u0016\u001c\u0007\u0010\u0002\t\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u00066\u0016\u0007$\u000f\u0010\u0002\u0007\u001f\u000b\u000f#\u0005\u0006\u0007 \u0003!\u0005\u000b\u000f\u0014\t\u0013\u0007 \u0010\u0002\t\u0003\u000b\u0007 \u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007 \n\u0015\u0010\t\u000b\u0007 \u0004\u0003\u0006\u0010\t\u0012\u0003\u0012\u001f\u0007 \u0010\u000f\u0007 \u0010\u0002\t\u0007 \u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007 \u0006\u000f\u0012\u001f\u0016\u0007 *\u0002\t\u0006\t\u0007 \u0015\u0003\u0012\u0013\u0003\u0012\u001f\u0006\u0007 \u0002\n\u0014\t\u0007 \u0003!\u0005\u0004\u0003\f\n\u0010\u0003\u000f\u0012\u0006\u0007 \u0015\u000f\u000b\u0007 1(#\t\u000b%\u0007 \u000e%\u0007 \u0002#!!\u0003\u0012\u001f2\u0007 \n\n\u0005\u0005\u0004\u0003\f\n\u0010\u0003\u000f\u0012\u0006\u0016 \n\u001c\u001e\u001c \u001f\u0003\f\b\u000f\u0010\u0007\u0002\b\u000f\u0007\f\u0003\u000e\b\u0012\r\u0003\u0006 \n,\u0002\n\u0010\u0007!\n+\t\u0006\u0007\u0003\u0010\u0007\u0010\u0002\n\u0010\u0007\"\t\u0007\f\n\u0012\u0007\u0003\u0013\t\u0012\u0010\u0003\u0015%\u0007\n\u0007!\t\u0004\u000f\u0013%\u0007\n\u0006\u00071\u0002\t\n\u000b\u0013 -\u000f\u0012\f\t -\n\u000e\t\u0015\u000f\u000b\t2\u001c\u0007\u000b\t\f\u000f\u001f\u0012\u00034\t\u0007\u0003\u0010\u001c\u0007\u0013\u0003\u0006\u0010\u0003\u0012\u001f#\u0003\u0006\u0002\u0007\u0003\u0010\u0007\u0015\u000b\u000f!\u0007\u000f\u0010\u0002\t\u000b\u0006\u001c\u0007\u000f\u000b\u0007\t\u0014\t\u0012\u0007\u000b\t\f\n\u0004\u0004\u0007 \n\u0012\u0013\u0007\u0006\u0003\u0012\u001f\u0007\u0003\u00108\u0007\u0011!\u0005\u0003\u000b\u0003\f\n\u0004\u0007\"\u000f\u000b+\u0007\u0010\u0002\n\u0010\u0007\u0010\u000b\u0003\t\u0006\u0007\u0010\u000f\u0007\u0015\u0003\u0012\u0013\u0007\n\u0012\u0006\"\t\u000b\u0006\u0007\u0015\u000f\u000b\u0007\u0010\u0002 \t\u0007 \n\u000b\t\f\u000f\u001f\u0012\u0003\u0010\u0003\u000f\u0012\u00075\u000e#\u0010\u0007\u0012\u000f\u0010\u0007\u0010\u0002\t\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u00126\u0007\u000f\u0015\u0007\u0006\u0002\u000f\u000b\u0010\u001c\u0007\u0003\u0006\u000f\f\u0002\u000b\u000f\u0012\u000f#\u0006\u0007 \n\u0012\u0013\u0007\u000b\n\u0010\u0002\t\u000b\u00071\u0012\u000f\u0012 -!#\u0006\u0003\f\n\u00042\u0007!\t\u0004\u000f\u0013\u0003\t\u0006\u0007\u0002\n\u0006\u0007\u000e\t\t\u0012\u0007\u0005#\u000e\u0004\u0003\u0006\u0002\t\u0013\u00075\u0006\t\t\u001c\u0007 \n\t\u0016\u001f\u0016\u001c\u0007 &\u000b\u000f\u000f\u0012\t\u0012\u001c\u0007 9::\u00187\u0007 '\u000f\"\u0004\u0003\u0012\u001f\u001c\u0007 9:.;7\u0007 \u0011\u0013\"\u000f\u000b\u0010\u0002%\u001c\u0007 9:;\u00196\u0016\u0007 )\u0012\u0007 \n\u0010\u0002\t\u0006\t\u0007 \u0006\u0010#\u0013\u0003\t\u0006\u001c\u0007 !\t\u0004\u000f\u0013\u0003\t\u0006\u0007 \n\u000b\t\u0007 \u0013\u0003\u0006\u0010\u0003\u0012\u001f#\u0003\u0006\u0002\t\u0013\u0007 \n\u0004\u000f\u0012\u001f\u0007 \u0010\u0002\t\u0003\u000b\u0007 \u0005\t\u000b\f\t\u0005\u0010#\n\u0004\u0007\u0005\u000b\u000f\u0005\t\u000b\u0010\u0003\t\u0006\u001c\u0007\u0006 #\f\u0002\u0007\n\u0006\u0007\u0005\u0003\u0010\f\u0002\u001c\u0007\u0004\u000f#\u0013\u0012\t\u0006\u0006\u001c\u0007\u0010\t!\u0005\u000f\u001c\u0007\u0010\u0003!\u000e\u000b\t\u001c\u0007 \n\f\u000f\u0012\u0010\u000f#\u000b\u0007\n\u0012\u0013\u0007\u000b\u0002%\u0010\u0002!\u0016\u0007 \n\u001e\u0003\u001f#\u000b\t\u00079<\u0007*\u0002\t\u0007\u0015\u0003\u000b\u0006\u0010\u0007\u0010\u0002\u000b\t\t\u0007!\t\n\u0006#\u000b\t\u0006\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0014\u000f\f\n\u0004\u0007!\t\u0004\u000f\u0013%\u0007\u000f\u0015\u0007 \n1=\t\u0006\u0010\t\u000b\u0013\n%2\u0007\u0003\u0012\u0007!#\u0006\u0003\f\n\u0004\u0007\u0006\f\u000f\u000b\t\u0007\u0012\u000f\u0010\n\u0010\u0003\u000f\u0012\u0016\u0007*\u0002\t\u0007!\t\u0004\u000f\u0013\u0003\f\u0007\f\u000f\u0012\u0010\u000f#\u000b\u0007 \n\n\u0012\u0013\u0007!#\u0006\u0003\f\n\u0004\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007\n\u000b\t\u0007\u0006\u0002\u000f\"\u0012\u0007#\u0012\u0013\t\u000b\u0012\t\n\u0010\u0002\u0016 \n\u0001\u0003\u0010\f\u0002\u001c\u0007\u0004\u000f#\u0013\u0012\t\u0006\u0006\u001c\u0007\u0010\t!\u0005\u000f\u0007\n\u0012\u0013\u0007\u0010\u0003!\u000e\u000b\t \u0007\n\u000b\t\u0007\u0002#!\n\u0012\u0007>#\u0013\u001f!\t\u0012\u0010\u0006\u0007\u000f\u0015\u0007 \n\u0002\u000f\"\u0007\u0002\u0003\u001f\u0002\u0007\u000f\u000b\u0007\u0004\u000f\"\u001c\u0007\u0002\u000f\"\u0007\u0004\u000f#\u0013\u0007\u000f\u000b\u0007\u0006\u000f\u0015\u0010\u001c\u0007\u0002\u000f\"\u0007\u0015\n\u0006\u0010\u0007\u000f\u000b\u0007\u0006\u0004\u000f\"\u0007\n\u0012\u0013\u0007\u0003\u0012\u0007 \"\u0002\n\u0010\u0007 (#\n\u0004\u0003\u0010%\u0007 \n\u0007 !#\u0006\u0003\f\n\u0004\u0007 \u0006\u000f#\u0012\u0013\u0007 \u0003\u0006\u0007 \u0002\t\n\u000b\u0013\u0016\u0007 \u001b\u0006\u0007 \n\u0007 \u0006\u0010\n\u0012\u0013\n\u000b\u0013\u0007 \u0015\u000f\u000b\u0007 \n\"\t\u0006\u0010\t\u000b\u0012\u0007!#\u0006\u0003\f\u001c\u0007\u0010\u0002\t\u0007\f\u000f\u0012\u0010\u0003\u0012#\u000f#\u0006\u0007\u0005\t\u000b\f\t\u0005\u0010\u0007\u000f\u0015\u0007\u0005\u0003\u0010\f\u0002\u0007\u0003\u0006\u0007\u0006#\u000e\u0013\u0003\u0014\u0003\u0013\t\u0013\u0007 \n\u0003\u0012\u0010\u000f\u0007 \n\u0007 \u0006\t\u0010\u0007 \u000f\u0015\u0007 \u0013\u0003\u0006\f\u000b\t\u0010\t\u0007 \u0004\u000f\u001f\n\u000b\u0003\u0010\u0002!\u0003\f\u0007 \u0006\u0010\t\u0005\u0006\u0007 \u0010\u0002\n\u0010\u0007 \u0002\u0003\u0012\u001f\t\u0006\u0007 \u000f\u0012 \u0007 \n\u0007 \n\u000b\t\u0015\t\u000b\t\u0012\f\t\u0007\u0010\u000f\u0012\t\u00075\u001b\u0018\u0007?\u0007\u0018\u0018/\u0007\u00174\u001c\u0007\u0010\u0002\t\u0007\f\u000f\u0012\f\t\u000b\u0010\u0007\u0005\u0003\u0010\f\u00026\u0016\u0007\u0011\n\f\u0002\u0007\u0006\u0010\t\u0005\u0007\u0003\u0006\u0007 \u0013\t\u0012\u000f\u0010\t\u0013\u0007\n\u0006\u0007\n\u0007\u0006\t!\u0003\u0010\u000f\u0012\t\u0016\u0007@#\u0006\u0003\f\n\u0004\u0007\u0012\u000f\u0010\t\u0006\u0007\n\u000b\t\u0007\n\u000b\u000b\n\u0012\u001f\t\u0013\u0007\n\u0004\u000f\u0012\u001f\u0007\u0010\u0002\t\u0006\t\u0007 \u0001\t\u000b!\u0003\u0006\u0006\u0003\u000f\u0012\u0007\u0010\u000f\u0007!\n+\t\u0007\u0013 \u0003\u001f\u0003\u0010\n\u0004\u0007\u000f\u000b\u0007\u0002\n\u000b\u0013\u0007\f\u000f\u0005\u0003\t\u0006\u0007\u000f\u0015\u0007\n\u0004\u0004\u0007\u000f\u000b\u0007\u0005\n\u000b\u0010\u0007\u000f\u0015\u0007\u0010\u0002\u0003\u0006\u0007\"\u000f\u000b+\u0007\u0015\u000f\u000b\u0007 \n\u0005\t\u000b\u0006\u000f\u0012\n\u0004\u0007\u000f\u0015\u0007\f\u0004\n\u0006\u0006\u000b\u000f\u000f!\u0007#\u0006\t\u0007\u0003\u0006\u0007\u001f\u000b\n\u0012\u0010\t\u0013\u0007\"\u0003\u0010\u0002\u000f#\u0010\u0007\u0015\t\t\u0007\u0005\u000b\u000f\u0014\u0003\u0013\t\u0013\u0007\u0010\u0002\n\u0010\u0007\f\u000f\u0005\u0003\t\u0006\u0007\n\u000b\t\u0007\u0012\u000f\u0010\u0007 \n!\n\u0013\t\u0007\u000f\u000b\u0007\u0013\u0003\u0006\u0010\u000b\u0003\u000e#\u0010\t\u0013\u0007\u0015\u000f\u000b\u0007\u0005\u000b\u000f\u0015\u0003\u0010\u0007\u000f\u000b\u0007\f\u000f!!\t\u000b\f\u0003\n\u0004\u0007\n\u0013\u0014\n\u0012\u0010\n\u001f\t\u0007\n\u0012\u0013\u0007\u0010\u0002\n\u0010\u0007\f\u000f\u0005\u0003\t\u0006\u0007\u000e\t\n\u000b\u0007 \n\u0010\u0002\u0003\u0006\u0007\u0012\u000f\u0010\u0003\f\t\u0007\n\u0012\u0013\u0007\u0010\u0002\t\u0007\u0015#\u0004\u0004\u0007\f\u0003\u0010\n\u0010\u0003\u000f\u0012\u0007\u000f\u0012\u0007\u0010\u0002\t\u0007\u0015\u0003\u000b\u0006\u0010\u0007\u0005 \n\u001f\t\u0016\u0007\u0007 \u0001\u0007A//B\u0007*\u0002\t\u0007C\u000f\u0002\u0012\u0006\u0007\u0017\u000f\u0005+\u0003\u0012\u0006\u0007 \n3\u0012\u0003\u0014\t\u000b\u0006\u0003\u0010%\u0016 \u0006\u0010\t\u0005\u0006\u0007\n\u0006\u0007!#\u0006\u0003\f\n\u0004\u0007\u0006\f\n\u0004\t\u0006\u0016\u0007*\u0002\t\u0007\u0013\u0003\u0006\u0010\n\u0012\f\t\u0007\u000e\t\u0010\"\t\t\u0012\u0007\n\u0012%\u0007\u0010\"\u000f\u0007\u0012\u000f\u0010\t\u0006\u0007\u0003\u0006\u0007 \n\f\n\u0004\u0004\t\u0013\u0007\n\u0007!#\u0006\u0003\f\n\u0004\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0007\n\u0012\u0013\u0007\u0003\u0006\u0007\t \u0005\u000b\t\u0006\u0006\t\u0013\u0007\u000e%\u0007\u0010\u0002\t\u0007\u0012#!\u000e\t\u000b\u0007\u000f\u0015\u0007 \n\u0006\t!\u0003\u0010\u000f \u0012\t\u0006\u0007 \u000e\t\u0010\"\t\t\u0012\u0007 \u0010\u0002\t!\u0016\u0007 \u001b\u0007 !\t\u0004\u000f\u0013\u0003\f\u0007 \f\u000f\u0012\u0010\u000f#\u000b\u0007 \u0003\u0006\u0007 \n\u0007 \f\u000b#\u0013\t\u0007 \n\u000b\t\u0005\u000b\t\u0006\t\u0012\u0010\n\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007\u000e\t\u0010\"\t\t\u0012\u0007\u0006#\f\f\t\u0006\u0006\u0003\u0014\t\u0007\u0012\u000f\u0010\t\u0006\u0007\u0003\u0012\u0007\n\u0007 !\t\u0004\u000f\u0013%\u0016\u0007)\u0010\u0007\u0003\u001f\u0012\u000f\u000b\t\u0006\u0007\u0010\u0002\t\u0007!\n\u001f\u0012\u0003\u0010#\u0013\t\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007\n\u0012\u0013\u0007\u000b\t\u0010\n\u0003\u0012\u0006\u0007 \u000f\u0012\u0004%\u0007\u0010\u0002\t\u0007\u0003\u0012\u0015\u000f\u000b!\n\u0010\u0003\u000f\u0012\u0007\"\u0002\t\u0010\u0002\t\u000b\u0007\u0010\u0002\t\u0007!\t\u0004\u000f\u0013%\u0007\u0003\u0006\u0007\u001f\u000f\u0003\u0012\u001f\u0007#\u0005\u001c\u0007\u0003\u0006\u0007\u001f\u000f\u0003\u0012\u001f\u0007 \u0013\u000f\"\u0012\u0007 \u000f\u000b\u0007 \u0013\u000f\t\u0006\u0007 \u0012\u000f\u0010\u0007 \f\u0002\n\u0012\u001f\t \u0007 \u0003\u0012\u0007 \u0005\u0003\u0010\f\u0002\u0007 5\u0006\t\t\u0007 \u001e\u0003\u001f#\u000b\t\u0007 96\u0016\u0007 \r\n\u0006\u0010\u0004%\u001c\u0007 \n\u000b\u0002%\u0010\u0002!\u0007 \u0003\u0006\u0007 \u0010\u0002\t\u0007 \u0005\t\u000b\f\t\u0005\u0010\u0003\u000f\u0012\u0007 \u000f\u0015\u0007 \u0010\t!\u0005\u000f\u000b\n\u0004\u0007 \u0005\n\u0010\u0010\t\u000b\u0012\u0006\u0007 \u0010\u0002\n\u0010\u0007 \n5\u0002\u0003\t\u000b\n\u000b\f\u0002\u0003\f\n\u0004\u0004%6\u0007 \u001f\u000b\u000f#\u0005\u0006\u0007 \n\u0007 \u0006\t(#\t\u0012\f\t\u0007 \u000f\u0015\u0007 \u0012\u000f\u0010\t\u0006\u0007 \n\u0004\u000f\u0012\u001f\u0007 \n\u0007 \u0006\u0010\t\n\u0013%\u0007 \n\u000e\t\n\u0010\u0016\u0007$\t\n\u0010\u0007\u0003\u0006\u0007+\u0012\u000f\"\u0012\u0007\n\u0006\u0007\u0010\u0002\t\u0007\u0005\t\u000b\f\t\u0003\u0014\t\u0013\u0007\u0005#\u0004\u0006\t\u0007\u0003\u0012\u0007!#\u0006\u0003\f\u0007!\n\u000b+\u0003\u0012\u001f\u0007 \u000f\u0015\u0015\u0007\t(#\n\u0004\u0007\u0010\u0003!\t\u0007#\u0012\u0003\u0010\u0006\u0007\u0003\u0012\u0007\n\u0007!\t\u0010\u000b\u0003\f\n\u0004\u0007\u0006\u0010\u000b#\f\u0010#\u000b\t7\u0007\u0010\u0002\t\u0007\u0004\n \u0010\u0010\t\u000b\u0007\u0013\t\u0015\u0003\u0012\t\u0006\u0007 \n\u0010\u0002\t\u0007 \u0005\t\u000b\u0003\u000f\u0013\u0003\f\u0007 \n\u0004\u0010\t\u000b\u0012\n\u0010\u0003\u000f\u0012\u0007 \u000f\u0015\u0007 \u0006\u0010\u000b\u000f\u0012\u001f\u0007 \n\u0012\u0013\u0007 \"\t\n+\u0007 \n\f\f\t\u0012\u0010\u0006\u0016\u0007 *\u0002\t\u0007 (#\t\u0006\u0010\u0003\u000f\u0012\u0007\n\u000b\u0003\u0006\t\u0006\u0007\u0012\u000f\"\u0007\"\u0002\n\u0010\u0007\u0005\t\u000b\f\t\u0005\u0010#\n\u0004\u0007\u0005\u000b\u000f\u0005\t\u000b\u0010\u0003\t\u0006\u0007\u000f\u0015\u0007\n\u0007!\t\u0004\u000f\u0013%\u0007 \n\u000b\t\u0007 !\u000f\u0006\u0010\u0007 \u0003!\u0005\u000f\u000b\u0010\n\u0012\u0010\u0007 \u0003\u0012\u0007 \n\u0004\u0004\u000f\"\u0003\u0012\u001f\u0007 #\u0006\u0007 \u0010\u000f\u0007 \t\u0012\f\u000f\u0013\t\u001c\u0007 \u000b\t\f\u000f\u001f\u0012\u00034\t\u001c\u0007 \n\u000b\t!\t!\u000e\t\u000b\u0007\n\u0012\u0013\u0007\t\u0014\t\u0012\u0010#\n\u0004\u0004%\u0007\u0006\u0003\u0012\u001f\u0007!\t\u0004\u000f\u0013\u0003\t\u0006\u0007\f\u000f\u000b\u000b\t\f\u0010\u0004%\u0016\u0007 \n@\n\u0012%\u0007\u0006\u0010#\u0013\u0003\t\u0006\u0007\u000b\t\u0005\u000f\u000b\u0010\u0007\u0010 \u0002\n\u0010\u0007\u0010\u0002\t\u0007\u0005\t\u000b\f\t\u0005\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u0007\n\u0007!\t\u0004\u000f\u0013%\u0007\f\u0002\n\u0012\u001f\t\u0006\u0007 \n\u000f\u0012\u0004%\u0007 \u0006\u0004\u0003\u001f\u0002\u0010\u0004%\u0007 #\u0012\u0013\t\u000b\u0007 !\u000f\u0013\t\u0006\u0010\u0007 \u0010\u000b\n\u0012\u0006\u0015\u000f\u000b!\n\u0010\u0003\u000f\u0012\u0006\u0007 \n\u0004\u000f\u0012\u001f\u0007 \u0005\u0003\u0010\f\u0002\u001c\u0007 \u0004\u000f#\u0013\u0012\t\u0006\u0006\u001c\u0007 \u0010\t!\u0005\u000f\u0007 \n\u0012\u0013\u0007 \u0010\u0003!\u000e\u000b\t\u0016\u0007 \u0011 \f\t\u0005\u0010\u0007 \u0015\u000f\u000b\u0007 \u0006\u000f!\t\u0007 \t \u0010\u000b\t!\t\u0007 \u0014\n\u000b\u0003\n\u0010\u0003\u000f\u0012\u0006\u001c\u0007\u0003\u0010\u0007\u0003\u0006\u0007\t\u0014\u0003\u0013\t\u0012\u0010\u0007\u0010\u0002\n\u0010\u0007\u0010\u0002\t\u0007\u0005\t\u000b\f\t\u0003\u0014\t\u0013\u0007!\t\u0004\u000f\u0013%\u0007\u0013\u000f\t\u0006\u0007\u0012\u000f\u0010\u0007 \f\u0002\n\u0012\u001f\t\u0007!#\f\u0002\u0007\u0003\u0015\u0007\u0003\u0010\u0007\u0003\u0006\u0007\u0005\u0004\n%\t\u0013\u0007\u0003\u0012\u0007\u0013\u0003\u0015\u0015\t\u000b\t\u0012\u0010\u0007+\t%\u0006\u001c\u0007\u0004 \u000f#\u0013\t\u000b\u0007\u000f\u000b\u0007\u0006\u000f\u0015\u0010\t\u000b\u001c\u0007 \n\u0015\n\u0006\u0010\t\u000b\u0007\u000f\u000b\u0007\u0006\u0004\u000f\"\t\u000b\u0007\u000f\u000b\u0007\u000e%\u0007\u0013\u0003\u0015\u0015\t\u000b\t\u0012\u0010\u0007!#\u0006\u0003\f\n\u0004\u0007\u0003\u0012\u0006\u0010\u000b#!\t\u0012\u0010\u0006\u0007\u000f\u000b\u0007\u0014\u000f\u0003\f\t\u0006\u0007 \n5'\u000f\"\u0004\u0003\u0012\u001f\u0007\n\u0012\u0013\u0007\u0017\n\u000b\"\u000f\u000f\u0013\u001c\u00079:;\u001a6\u0016\u0007 \n\u0017\u000f\"\t\u0014\t\u000b\u001c\u0007 \u0006\u0003\u0012\f\t\u0007 \u000b\u0002%\u0010\u0002!\u0007 \n\u0012\u0013\u0007 \u0005\u0003\u0010\f\u0002\u0007 \n\u000b\t\u0007 \u0002\n\u000b\u0013\u0007 \u0010\u000f\u0007 \n\u0010\u0010\t\u0012\u0013\u0007 \u0010\u000f\u0007 \n\u0006\t\u0004\t\f\u0010\u0003\u0014\t\u0004%\u001c\u0007 \u000b\t\f\u000f\u001f\u0012\u0003\u0010\u0003\u000f\u0012\u0007 \u000f\u0015\u0007 \n\u0007 +\u0012\u000f\"\u0012\u0007 !\t\u0004\u000f\u0013%\u0007 \u0005\u000b\t\u0006\t\u0012\u0010\t\u0013\u0007 \u0003\u0012\u0007 \n\u0012\u000f\u0010\u0002\t\u000b\u0007\u000b\u0002%\u0010\u0002!\u0007\u0003\u0006\u0007\u0013\u0003\u0015\u0015\u0003\f#\u0004\u0010\u00075C\u000f\u0012\t\u0006\u001c\u0007 D#!!\t\u000b\t\u0004\u0004\u0007\n\u0012\u0013\u0007@\n\u000b\u0006\u0002\u000e#\u000b\u0012\u001c\u0007 \n9:;.6\u0016\u0007 \b\u0002%\u0010\u0002!\u0003\f\u0007 \n\f\f\t\u0012\u0010\u0006\u0007 \u0005\u0004\n\f\t\u0013\u0007 \u000f\u0012\u0007 \u0006\n\u0004\u0003\t\u0012\u0010\u0007 !\t\u0004\u000f\u0013%\u0007 \t\u0014\t\u0012\u0010\u0006\u0007 \n\u0015\n\f\u0003\u0004\u0003\u0010\n\u0010\t\u0007 \u0010\u0002\t\u0007 \u000b\t!\t!\u000e\u000b\n\u0012\f\t\u0007 \u000f\u0015\u0007 !\t\u0004\u000f\u0013\u0003\t\u0006\u0007 5$\u000f\u0004\u00104\u0007 \n\u0012\u0013\u0007 C\u000f\u0012\t\u0006\u001c\u0007 9:;\u001a6\u0016\u0007*\u0002\u000f#\u001f\u0002\u0007\n\u0007\f\u000f\u0012\u0010\u000f#\u000b\u0007!\n\u0003\u0012\u0010\n\u0003\u0012\u0006\u0007\u000f\u0012\u0004%\u0007\u0010\u0002\t\u0007\u001f\u0004\u000f\u000e\n\u0004\u0007!\u000f\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u0007 1#\u0005\u00062\u0007\n\u0012\u0013\u00071\u0013\u000f\"\u0012\u00062\u0007\u0003\u0012\u0007\n\u0007!\t\u0004\u000f\u0013%\u001c\u0007\u0003\u0010\u0007\u0003\u0006\u0007\u000f\u0012\t\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007!\n\u0003\u0012\u0007\u0005\u0003\u0004\u0004\n \u000b\u0006\u0007\u0015\u000f\u000b\u0007 \n!\t!\u000f\u000b\u00034\u0003\u0012\u001f\u0007 !\t\u0004\u000f\u0013\u0003\t\u0006\u0016\u0007 &\u0002\u0003\u0004\u0013\u000b\t\u0012\u0007 5#\u0012\u0013\t\u000b\u0007 \u0010\u0002\t\u0007 \n\u001f\t\u0007 \u000f\u0015\u0007 \u0019\u0007 %\t\n\u000b\u00066\u0007 \u0002\n\u0014\t\u0007\u000f\u0012\u0004%\u0007\u0010\u0002\t\u0007\f\u000f\u0012\u0010\u000f#\u000b\u0007\n\u0006\u0007\n\u0007\u0013\t\u0014\u0003\f\t\u0007\u0010\u000f\u0007\u000b\t!\t!\u000e\t\u000b\u0007!\t\u0004\u000f\u0013\u0003\t\u0006\u0016\u0007$#\u0010\u0007 \n\n\u0004\u0006\u000f\u0007 \u0015\u000f\u000b\u0007 \n\u0013#\u0004\u0010\u0006\u001c\u0007 !\t\u0004\u000f\u0013\u0003\t\u0006\u0007 \"\u0003\u0010\u0002\u0007 \u0010\u0002\t\u0007 \u0006\n!\t\u0007 \f\u000f\u0012\u0010\u000f#\u000b\u0007 \n\u000b\t\u0007 \t\n\u0006\u0003\u0004%\u0007 \n\f\u000f\u0012\u0015#\u0006\t\u0013\u00075'\u000f\"\u0004\u0003\u0012\u001f\u001c\u00079:.;6\u0016\u0007,\t\u0004\u0004 -+\u0012\u000f\"\u0012\u0007!\t\u0004\u000f\u0013\u0003\t\u0006\u0007\u0003\u0012\u0007\"\u0002\u0003\f\u0002\u0007\u0010\u0002\t\u0007 \n\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007\n\u000b\t\u0007 \u0013\u0003\u0006\u0010\u000f\u000b\u0010\t\u0013\u0007\u000e#\u0010\u0007\u0010\u0002\t\u0007\f\u000f\u0012\u0010\u000f#\u000b\u0006\u0007\n\u000b\t\u0007\u0005\u000b\t\u0006\t\u000b\u0014\t\u0013\u0007\n\u000b\t\u0007\u0006\u0010\u0003\u0004\u0004\u0007 \n\t\n\u0006\u0003\u0004%\u0007 \u000b\t\f\u000f\u001f\u0012\u00034\t\u0013\u0007 5'\u000f\"\u0004\u0003\u0012\u001f\u0007 \n\u0012\u0013\u0007 \u0017\n\u000b\"\u000f\u000f\u0013\u001c\u0007 9:;\u001a6\u0016\u0007 \u001d\t\"\u0007 !\t\u0004\u000f\u0013\u0003\t\u0006\u0007\u000f\u000b\u0007!\t\u0004\u000f\u0013\u0003\t\u0006\u0007\u0010\u0002\n\u0010\u0007\"\t\u0007\u0002\n\u0014\t\u0007\u0002\t\n\u000b\u0013\u0007\u000f\u0012\u0004%\u0007\n\u0007\f\u000f#\u0005\u0004\t\u0007\u000f\u0015\u0007 \u0010\u0003!\t\u0006\u0007\n\u000b\t\u0007\u0004\n\u000b\u001f\t\u0004%\u0007\u000b\t!\t!\u000e\t\u000b\t\u0013\u0007\u000e%\u0007\u0010\u0002\t\u0003\u000b\u0007\f\u000f\u0012\u0010\u000f#\u000b\u0016\u0007*\u0002\t\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007 \n\u000f\u0015\u0007\n\u0007!\t\u0004\u000f\u0013%\u0007\u000e\t\f\u000f!\t\u0007\u000f\u0012\u0004%\u0007!\u000f\u000b\t\u0007\u0003!\u0005\u000f\u000b\u0010\n \u0012\u0010\u0007\"\u0003\u0010\u0002\u0007\u0003\u0012\f\u000b\t\n\u0006\u0003\u0012\u001f\u0007\u0006\u000f\u0012\u001f\u0007 \n\u0015\n!\u0003\u0004\u0003\n\u000b\u0003\u0010%\u001c\u0007 \n\u0015\u0010\t\u000b\u0007 \f\u0002\u0003\u0004\u0013\u0002\u000f\u000f\u0013\u0007 \n\u0012\u0013\u0007 \n\u0015\u0010\t\u000b\u0007 !#\u0006\u0003\f\n\u0004\u0007 \u0010\u000b\n\u0003\u0012\u0003\u0012\u001f\u0007 5'\n\u0014\u0003\u0013\u0006\u000f\u0012\u001c\u00079::\u00187\u0007'\u000f\"\u0004\u0003\u0012\u001f\u0007\n\u0012\u0013\u0007\u0017\n\u000b\"\u000f\u000f\u0013\u001c\u00079:;\u001a6\u0016 \n\u001c\u001e \u0017\r\t\n\r\t\n\u0007\f\u0003\u000e\b\u0012\r\u0003\u0006 \nD\u0003\u0012\u001f\u0003\u0012\u001f\u0007 !\t\n\u0012\u0006\u0007 \n\u000b\u0010\u0003\f#\u0004\n\u0010\u0003\u0012\u001f\u0007 \n\u0007 \u000b\t\f\n\u0004\u0004\t\u0013\u0007 !\t\u0004\u000f\u0013%\u0016\u0007 *\u0002\u0003\u0006\u0007 \u0010\n\u0006+\u0007 \u0003\u0006\u0007 \n!\u000f\u000b\t\u0007\f\u000f!\u0005\u0004\t \u0007\u0010\u0002\n\u0012\u0007\u0003\u0010\u0007\n\u0005\u0005\t\n\u000b\u0006\u0007\u0010\u000f\u0007\u000e\t\u0007\n\u0010\u0007\u0015\u0003\u000b\u0006\u0010\u0016\u0007$\t\u0006\u0003\u0013\t\u0006\u0007\u000b\t \f\n\u0004\u0004\u0003\u0012\u001f\u0007 \n\u0010\u0002\t\u0007\f\u000f\u0012\u0010\u000f#\u000b\u001c\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007\n\u0012\u0013\u0007\u000b\u0002%\u0010\u0002!\u0007\u000f\u0015\u0007\n\u0007!\t\u0004\u000f\u0013%\u001c\u0007\u0003\u0010\u0007\u0003\u0012\u0014\u000f\u0004\u0014\t\u0006\u0007\n\u0007 \u0013\t\u0004\u0003\f\n\u0010\t\u0007 !\n\u0006\u0010\t\u000b%\u0007 \u000f\u0015\u0007 \u0006\t\u0014\t\u000b\n\u0004\u0007 !#\u0006\f\u0004\t\u0007 \u0006%\u0006\u0010\t!\u0006\u0007 \f\u000f\u0012\u0010\u000b\u000f\u0004\u0004\t\u0013\u0007 \u000e%\u0007 \n#\u0013\u0003\u0010\u000f\u000b%\u0007\u0015\t\t\u0013\u000e\n\f+\u0016\u0007 \nE\u000e\u0014\u0003\u000f#\u0006\u0004%\u001c\u0007 \u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007 \n\u000e\u0003\u0004\u0003\u0010%\u0007 \n\u0012\u0013\u0007 \u0010\u000b\n\u0003\u0012\u0003\u0012\u001f \n9\u0007 \n\u000b\t\u0007 \u0013\t\t!\t\u0013\u0007 \u0010\u000f\u0007 \u000e\t\u0007 \n\u0003!\u0005\u000f\u000b\u0010\n\u0012\u0010\u0007\u0015\n\f\u0010\u000f\u000b\u0006\u0007\u0010\u000f\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\t\u0007\n\u0007!\t\u0004\u000f\u0013%\u0007\n\f\f#\u000b\n\u0010\t\u0004%\u0016\u0007 \u0001\t\u000f\u0005\u0004\t\u0007 \n\"\u0003\u0010\u0002\u000f#\u0010\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u0010\u000b\n\u0003\u0012\u0003\u0012\u001f\u0007\u000b\t\u0004%\u0007\u0002\t\n\u0014\u0003\u0004%\u0007\u000f\u0012\u0007\f\u000f\u0012\u0010\u000f#\u000b\u0006\u001c\u0007\u0003\u0012\u0006\u0010\t\n\u0013\u0007\u000f\u0015\u0007 \n+\u0012\u000f\"\u0004\t\u0013\u001f\t\u0007\n\u000e\u000f#\u0010\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007\n\u0012\u0013\u0007\u0006\f\n\u0004\t\u0007\u0006\u0010\u000b#\f\u0010#\u000b\t\u0006\u0016\u0007&\u000f\u0012\u0006\t(#\t\u0012\u0010\u0004%\u001c\u0007 \u0010\u0002\t\u0003\u000b\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u0005\t\u000b\u0015\u000f\u000b!\n\u0012\f\t\u0006\u0007\n\u000b\t\u0007\u0015\u0004\n\"\t\u0013\u0007\u000e%\u0007\u0004\n\u000b\u001f\t\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007\u000e\t\u0003\u0012\u001f\u0007 \u0015\u0004\n\u0010\u0010\t\u0012\t\u0013\u0007\n\u0012\u0013\u0007\u000e%\u0007\t\u0012\u0013\u0003\u0012\u001f\u0007\n\u0007!\t\u0004\u000f\u0013%\u0007\u0004\u0003\u0012\t\u0007\u0003\u0012\u0007\n\u0007\u0013\u0003\u0015\u0015\t\u000b\t\u0012\u0010\u0007+\t%\u0007\u0010\u0002\n\u0012\u0007 \u0010\u0002\n\u0010\u0007\"\n\u0006\u0007#\u0006\t\u0013\u0007\n\u0010\u0007\u0010\u0002\t\u0007\u0006\u0010\n\u000b\u0010\u0016\u0007\u0017\n\u0014\u0003\u0012\u001f\u0007\n\u0007\f\u000f\u0012\u0010\u000f#\u000b\u0007\n\u0006\u0007\u0010\u0002\t\u0003\u000b\u0007!\n\u0003\u0012\u0007 \u000b\t\f\u000f#\u000b\u0006\t\u001c\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\f\n\u0012\u0007\u0006\u0003\u0012\u001f\u0007\n\u0007!\t\u0004\u000f\u0013%\u0007\u000f\u0012\u0004%\u0007\u0003\u0012\u0007\u000f\u0012\t\u0007\"\n%\u0016\u0007 \n9\u0007D\u0003\u0012\u001f\u0003\u0012\u001f\u0007\n\u000e\u0003\u0004\u0003\u0010%\u0007\u000b\t\u0015\t\u000b\u0006\u0007\u0010\u000f\u0007\u0010\u0002\t\u0007#\u0006\t\u0007\u000f\u0015\u0007\u0010\u0002\t\u000f\u000b %\u001c\u0007\u0014\u000f\f\n\u0004\u0007\u0006+\u0003\u0004\u0004\u0006\u0007\n\u0012\u0013\u0007\n#\u000b\n\u0004\u0007\u0006+\u0003\u0004\u0004\u0006\u0007\n\u0006\u0007 \n\t!\u0005\u0004\u000f%\t\u0013\u0007\u000e%\u0007\u0005\u000b\u000f\u0015\t\u0006\u0006\u0003\u000f\u0012\n\u0004\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0016\u0007*\u0002\t\u0007\f#\u000b\u000b\t\u0012\u0010\u0007\u0006\u0010#\u0013%\u0007\u0013\t\u0015\u0003\u0012\t\u0006\u00071\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\n\u000e\u0003\u0004\u0003\u0010%2\u0007 \n\n\u0006\u0007\u000e\t\u0003\u0012\u001f\u0007\u0015\u000f\u000b!\n\u0004\u0004%\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0003\u0012\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0016 &\u0002\n\u0012\u001f\u0003\u0012\u001f\u0007\u0010\u0002\t\u0003\u000b\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u001c\u0007\u0003\u0012\f\u0004#\u0013\u0003\u0012\u001f\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u0003\u0012\u0007\n\u0007\u0013\u0003\u0015\u0015\t\u000b\t\u0012\u0010\u0007+\t%\u0007\u000f\u000b\u0007 \n!\u000f\u0013\t\u001c\u0007\u000f\u000b\u0007\u000b\t\u0015\u0004\t\f\u0010\u0003\u0012\u001f\u0007\u000f\u0012\u0007\n\u0012\u0013\u0007\u0003!\u0005\u000b\u000f\u0014\u0003\u0012\u001f\u0007\u0010\u0002\t\u0003\u000b\u0007\u000f\"\u0012\u0007\u0005\t \u000b\u0015\u000f\u000b!\n\u0012\f\t\u001c\u0007 \n\u0003\u0006\u0007!\u000f\u000b\t\u0007\u0005\u000b \u000f\u000e\u0004\t!\n\u0010\u0003\f\u0007\u0015\u000f\u000b\u0007\u0010\u0002\t!\u00075'\n\u0014\u0003\u0013\u0006\u000f\u0012\u001c\u00079::\u00186\u0016 \n\u0017\u000f\"\t\u0014\t\u000b\u001c\u0007 \u0003\u0010\u0007 \u0002\n\u0006\u0007 \u000e\t\t\u0012\u0007 \u0006\u0002\u000f\"\u0012\u0007 \u0010\u0002\n\u0010\u0007 \u0005\t\u000f\u0005\u0004\t\u001c\u0007 \u0003\u000b\u000b\t\u0006\u0005\t\f\u0010\u0003\u0014\t\u0007 \u000f\u0015\u0007 \n\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\n\u000e\u0003\u0004\u0003\u0010%\u001c\u0007\n\u000b\t\u0007\u0006#\u000b\u0005\u000b\u0003\u0006\u0003\u0012\u001f\u0004%\u0007\u001f\u000f\u000f\u0013\u0007\n\u0010\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u0010\u0002\t\u0003\u000b\u0007 \u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\t \n\u0006\u000f\u0012\u001f\u0007\n\u0010\u0007\u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0005\u0003\u0010\f\u0002\u0007\n\u0012\u0013\u0007\n\u0010\u0007\u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0010\t!\u0005\u000f\u00075\r\t\u0014\u0003\u0010\u0003\u0012\u001c\u0007 9::\u00187\u0007\r\t\u0014\u0003\u0010\u0003\u0012\u0007\n\u0012\u0013\u0007&\u000f\u000f+\t\u001c\u00079::.6\u0016\u0007\u0007 \n \u0001\u0013 \u0014\u0003\u000f\r\f\u0003\t\u0005 \n*\u0002\t\u0007\t \u0005\t\u000b\u0003!\t\u0012\u0010\u0007\u0003\u0012\u0014\t\u0006\u0010\u0003\u001f\n\u0010\t\u0013\u0007\u0010\u0002\t\u0007\u0003\u0012\u0015\u0004#\t\u0012\f\t\u0006\u0007\u000f\u0015\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u0010\u000b\n\u0003\u0012\u0003\u0012\u001f\u0007 \n5\u0010\u000b\n\u0003\u0012\t\u0013\u0007\n\u0012\u0013\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u00066\u001c\u0007\u0006\u000f\u0012\u001f\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0003\u0010%\u00075\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\n\u0012\u0013\u0007 \n\u0004\t\u0006\u0006\u0007 \u0015\n!\u0003\u0004\u0003\n\u000b\u0007 \u0006\u000f\u0012\u001f\u00066\u0007 \n\u0012\u0013\u0007 \u000b\t\f\t\u0012\u0010\u0007 \u0006\u000f\u0012\u001f\u0007 \t \u0005\u000f\u0006#\u000b\t\u0007 \u000f\u0012\u0007 \u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007 !\t\u0004\u000f\u0013\u0003\t\u0006\u0007 \u000f\u0015\u0007 $\t\n\u0010\u0004\t\u0006\u0007 \u0006\u000f\u0012\u001f\u0006\u0016\u0007 $\n\u0006\t\u0013\u0007 \u000f\u0012\u0007 \u0015\u0003\u0012\u0013\u0003\u0012\u001f\u0006\u0007 \u000b\t\u0005\u000f\u000b\u0010\t\u0013\u0007 \u0003\u0012\u0007 \t \u0003\u0006\u0010\u0003 \u0012\u001f\u0007\u0006\u0010#\u0013\u0003\t\u0006\u001c\u0007\u0003\u0010\u0007\"\n\u0006\u0007\t \u0005\t\f\u0010\t\u0013\u0007\u0010\u0002\n\u0010\u0007 \n\u0002\u0010\u0002\t\u0007 \f\u000f\u000b\u000b\t\f\u0010\u0007 \u0005\u0003\u0010\f\u0002\u0007 \n\u0012\u0013\u0007 \u0010\u0002\t\u0007 \f\u000f\u000b\u000b\t\f\u0010\u0007 \u0010\t!\u0005\u000f\u0007 \f\n\u0012\u0007 \u000e\t\u0007 \t\n\u0006\u0003\u0004%\u0007 \n\n\u0013\u000f\u0005\u0010\t\u0013\u001c \n\u0002\u0005\u000b\u000f\u0005\u000f\u000b\u0010\u0003\u000f\u0012\n\u0004\u0004%\u0007 !\u000f\u000b\t\u0007 \u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007 \u000f\u0015\u0007 \u0015\n!\u0003\u0004\u0003\n\u000b\u0007 !\t\u0004\u000f\u0013\u0003\t\u0006\u0007 \n\u000b\t\u0007 \n\u0006#\u0012\u001f\u0007\f\u000f\u000b\u000b\t\f\u0010\u0004%\u0007\u0010\u0002\n\u0012\u0007\u000f\u0015\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007!\t\u0004\u000f\u0013\u0003\t\u0006\u001c \n\u0002\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0006\u0003\u0012\u001f\u0007\u0005\u000b\u000f\u0005\u000f\u000b\u0010\u0003\u000f\u0012\n\u0004\u0004%\u0007!\u000f\u000b\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004 \u0006\u0007 \n\u0010\u0002\n\u0012\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0013\u000f\u001c\u0007\u000e#\u0010\u0007\u0010\u0002\u0003\u0006\u0007\u0013\u0003\u0015\u0015\t\u000b\t\u0012\f\t\u0007\u0003\u0006\u0007\u0012\u000f\u0010\u0007\u0015\u000f#\u0012\u0013\u0007 \u0003\u0012\u0007\f\u000f\u0012\u0010\u000f#\u000b\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u001c \n\u0002\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0003!\u0005\u000b\u000f\u0014\t\u0007\u0010\u0002\t\u0003\u000b\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\n\u0015\u0010\t\u000b\u0007\u0002\n\u0014\u0003\u0012\u001f\u0007\u0004\u0003\u0006\u0010\t\u0012\t\u0013\u0007 \n\u0010\u000f\u0007\u0010\u0002\t\u0007\u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007\u0006\u000f\u0012\u001f\u001c\u0007\"\u0002\u0003\u0004\t\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0013\u000f\u0007\u0012\u000f\u0010\u0016 \n \u001e\u001c \u001f\u000b\u0005\u0003\u000f\r\u000b\u000e \n*\u0002\t\u0007\u0006\u0010\u0003!#\u0004#\u0006\u0007!\n\u0010\t\u000b\u0003\n\u0004\u0007\f\u000f\u0012\u0006\u0003\u0006\u0010\t\u0013\u0007\u000f\u0015\u00079A\u0007$\t\n\u0010\u0004\t\u0006\u0007 \u0006\u000f\u0012\u001f\u0006\u0007\n\u0006\u0007\u0010\u0002\t%\u0007 \n\n\u0005\u0005\t\n\u000b\t\u0013\u0007\u000f\u0012\u0007\u0010\u0002\t\u0007\f\u000f!\u0005\u0003\u0004\n\u0010\u0003\u000f\u0012\u0007\n\u0004\u000e#!\u0007192\u0016\u0007*\u0002\u0003\u0006\u0007\n\u0004\u000e#!\u0007\u0003\u0012\f\u0004#\u0013\t\u0006\u0007 \u0010\u0002\t\u0007\u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u00075\u000e#\u0010\u0007\u0013\u0003\u001f\u0003\u0010\n\u0004\u0004%\u0007\u000b\t -!\n\u0006\u0010\t\u000b\t\u00136\u0007\u000b\t\f\u000f\u000b\u0013\u0003\u0012\u001f\u0006\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007A.\u0007 \n\u001f\u000b\t\n\u0010\t\u0006\u0010\u00073F\u0007\n\u0012\u0013\u00073D\u001b\u0007$\t\n\u0010\u0004\t\u00062\u0007\u0002\u0003\u0010\u0007\u0006\u0003\u0012\u001f\u0004\t\u0006\u0016\u0007*\u0002\t\u0007\u0006\u000f\u0012\u001f\u0006\u0007\"\t\u000b\t\u0007 \n\u0006\t\u0004\t\f\u0010\t\u0013\u0007\u0010\u000f\u0007\u000f\u000e\u0010\n\u0003\u0012\u0007\n\u0007\u0014\n\u000b\u0003\t\u0010%\u0007\u0003\u0012\u0007\u0010\u0003!\t\u0007\u0005\t\u000b\u0003\u000f\u0013\u0007\n\u0012\u0013\u0007\u0006\u0010\n\u0010#\u0006\u0007\u000f\u0012\u0007\u0010\u0002\t\u0003\u000b\u0007 \u001d\u000f\u0016\u00079\u0007\f\u0002\n\u000b\u0010\u0007\u0005\u000f\u0006\u0003\u0010\u0003\u000f\u0012 \nA\u0016\n)\u0010\u0007\"\n\u0006\u0007\n\u0006\u0006#!\t\u0013\u0007\u0010\u0002\n\u0010\u0007\u0006\u000f\u0012\u001f\u0006\u0007\u000f\u0015\u0007\u0010\u0002\t\u00071*\u0002\t\u0007$\t\n\u0010\u0004\t\u00062\u0007\f\n\u0012\u0007\u000e\t\u0007\u000b\t\u0004\u0003\n\u000e\u0004%\u0007 \n#\u0006\t\u0013\u0007\u0003\u0012\u0007\n\u0012\u0007\t \u0005\t\u000b\u0003!\t\u0012\u0010\u0007\u000e%\u0007\u0006\u0003!\u0005\u0004%\u0007\u000b\t\u0015\t\u000b\u000b\u0003\u0012\u001f\u0007\u0010\u000f\u0007\u0010\u0002\t!\u0007\u000e%\u0007\u0010\u0002\t\u0003\u000b\u0007 \u0006\u000f\u0012\u001f\u0007\u0010\u0003\u0010\u0004\t\u0006\u0016\u0007\u001b\u0004\u0010\u0002\u000f#\u001f\u0002\u0007\u000f\u0010\u0002\t\u000b\u0007\n\u000b\u0010\u0003\u0006\u0010\u0006\u0007\u0002\n\u0014\t\u0007\u0005\t\u000b\u0015\u000f\u000b!\t\u0013\u0007\u0010\u0002\t\u0006\t\u0007\u0006\u000f\u0012\u001f\u0006\u0007 \n\n\u0006\u0007\"\t\u0004\u0004\u001c\u0007\u0010\u0002\t\u0007\f\u000f!\u0005\u000f\u0006\u0003\u0010\u0003\u000f\u0012\u0006\u0007\u0010\u0002\t!\u0006\t\u0004\u0014\t\u0006\u0007\f\n\u0012\u0007\u000e\t\u0007\t\n\u0006\u0003 \u0004%\u0007\n\u0010\u0010\u000b\u0003\u000e#\u0010\t\u0013\u0007 \n\u0010\u000f\u00071*\u0002\t\u0007$\t\n\u0010\u0004\t\u00062\u0007\n\u0012\u0013\u0007\n\u000b\t\u0007+\u0012\u000f\"\u0012\u0007#\u0012\u0013\t\u000b\u0007\u0010\u0002\t\u0003\u000b\u0007\u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007$\t\n\u0010\u0004\t\u0006\u0007 \u0014\t\u000b\u0006\u0003\u000f\u0012\u0007 5\u0003\u0016\t\u0016\u001c\u0007 \u0003\u0012\u0007 \n\u0007 \u001f\u0003\u0014\t\u0012\u0007 \u0006\u0010\n\u0012\u0013\n\u000b\u0013\u0007 +\t%\u001c\u0007 \u0010\t!\u0005\u000f\u0007 \n\u0012\u0013\u0007 \u0003\u0012\u0010\t\u000b\u0005\u000b\t\u0010\n\u0010\u0003\u000f\u00126\u0016 \n \u001e \u0018\u000b\u000f\u0005\r\u0004\r\u0014\u000b\t\u0005\u0006 \n\u0011\u0003\u001f\u0002\u0010\t\t\u0012\u0007 \u0005\t\u000b\u0006\u000f\u0012\u0006\u0007 59/\u0007 !\n\u0004\t\u0006\u001c\u0007 ;\u0007 \u0015\t!\n\u0004\t\u00066\u0007 \u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0010\t\u0013\u0007 \u0003\u0012\u0007 \u0010\u0002\t\u0007 \n\t \u0005\t\u000b\u0003!\t\u0012\u0010\u0016\u0007*\u0002\t\u0007\n\u0014\t\u000b\n\u001f\t\u0007\n\u001f\t\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005 \n\u0012\u0010\u0006\u0007\"\n\u0006\u0007A;\u0007%\t\n\u000b\u0006\u0007 \n5!\u0003\u0012<\u00079;\u001c\u0007!\n <\u0007\u001896\u0016\u0007\u0011\u0003\u001f\u0002\u0010\u0007\u000f\u0015\u0007\u0010\u0002\t!\u0007 \"\t\u000b\t\u0007\u0006\u0010#\u0013\t\u0012\u0010\u0006\u0007 1@#\u0006\u0003\f\n\u0004\u0007 \u0010\u0002\t\n\u0010\u000b\t2\u0007 5\u00186\u0007 \u000f\u000b\u0007 1&\u0004\n\u0006\u0006\u0003\f\n\u0004\u0007 \u0014\u000f\u0003\f\t2\u0007 5\u00186\u0007 \n\u0010\u0007 \u0010\u0002\t\u0007 $\u000b\n\u000e\n\u0012\u0010\u0007 &\u000f\u0012\u0006\t\u000b\u0014\n\u0010\u000f\u000b%\u0007 \u0003\u0012\u0007*\u0003\u0004\u000e#\u000b\u001f\u001c\u0007\u0010\u0002\t\u0007\u001d\t\u0010\u0002\t\u000b\u0004\n\u0012\u0013\u0006\u0016\u0007 \u001b\u0004\u0004\u0007\u0006\u0010#\u0013\t\u0012\u0010\u0006\u0007\u0002\n\u0013\u0007 \u000b\t\f\t\u0003\u0014\t\u0013\u0007 \n\u0010\u0007 \u0004\t\n\u0006\u0010\u0007 \u0015\u0003\u0014\t\u0007 %\t\n\u000b\u0006\u0007 \u000f\u0015\u0007 \u0015\u000f\u000b!\n\u0004\u0007 \u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007 \u0010\u000b\n\u0003\u0012\u0003\u0012\u001f\u0007 \n5\u0003\u0012\f\u0004#\u0013\u0003\u0012\u001f\u0007\u0010\u0002 \t\u0003\u000b\u0007\t\u0013#\f\n\u0010\u0003\u000f\u0012\u0007\n\u0010\u0007\u0010\u0002\t\u0007&\u000f\u0012\u0006\t\u000b\u0014\n\u0010\u000f\u000b%6\u0016\u0007*\u0002\t\u0007\u0006\u0010#\u0013\t\u0012\u0010\u0006\u0007 \n\u000f\u000e\u0010\n\u0003\u0012\t\u0013\u0007 \n\u0007 \u001f\u0003\u0015\u0010\u0007 \u0010\u000f+\t\u0012\u0007 \u0015\u000f\u000b\u0007 \u0010\u0002\t\u0003\u000b\u0007 \u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0010\u0003\u000f\u0012\u0016\u0007 *\u0002\t\u0007 \u000f\u0010\u0002\t\u000b\u0007 \u0010\t\u0012\u0007 \u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0006\u0007\u0012\t\u0014\t\u000b\u0007\u0002\n\u0013\u0007\n\u0012%\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u0010\u000b\n\u0003\u0012\u0003\u0012\u001f\u0007\u000f\u000b\u0007\t\u0013#\f\n\u0010\u0003\u000f\u0012\u0016\u0007*\u0002\t%\u0007 \"\t\u000b\t\u0007\f\u000f\u0004\u0004\t\n\u001f#\t\u0006\u0007\n\u0010\u0007\u0010\u0002\t\u0007\u000b\t\u0006\t\n\u000b\f\u0002\u0007\u0004\n\u000e\u000f\u000b\n\u0010\u000f\u000b%\u0007\n\u0012\u0013\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0010\t\u0013\u0007 \u0014\u000f\u0004#\u0012\u0010\n\u000b\u0003\u0004%\u0016\u0007 \n@\u000f\u0006\u0010\u0007\u0005\n\u000b\u0010 \u0003\f\u0003\u0005\n\u0012\u0010\u0006\u0007\"\t\u000b\t\u00071\f\u000f\u0003\u0012\f\u0003\u0013\t\u0012\u0010\n\u00042\u0007\u0004\u0003\u0006\u0010\t\u0012\t\u000b\u0006\u0007\u0010\u000f\u0007\u0010\u0002\t\u0007!#\u0006\u0003\f\u0007\u000f\u0015\u0007 \n1*\u0002\t\u0007$\t\n\u0010\u0004\t\u00062\u00075\t\u0016\u001f\u0016\u001c\u0007\"\u0002\u0003\u0004\t\u0007\u0004\u0003\u0006\u0010\t\u0012\u0003\u0012\u001f\u0007\u0010\u000f\u0007\u0010\u0002\t\u0007\u000b\n\u0013\u0003\u000f6\u0016\u0007E\u0012\u0004%\u0007\u0010\"\u000f\u0007 \nA\u0007*\u0002\t\u0007$\t\n\u0010\u0004\t\u0006\u001c\u00079\u001c\u0007\u0011@)\u0007\b\t\f\u000f\u000b\u0013\u0006\u0007 \r\u0010\u0013\u001c\u0007A///\u0016\u0007*\u0002\t\u0007\u0015\u000f\u0004\u0004\u000f\"\u0003\u0012\u001f\u00079A\u0007\u0006\u000f\u0012\u001f\u0006\u0007\"\t\u000b\t\u0007 \n\u0006\t\u0004\t\f\u0010\t\u0013<\u00071\r\u000f\u0014\t\u0007!\t\u0007\u0013\u000f2\u001c\u00071&\n\u00122\u0010\u0007\u000e#%\u0007 !\t\u0007\u0004\u000f\u0014\t2\u001c\u00071\u001b\u0007\u0002\n\u000b\u0013\u0007\u0013\n%2\u0006\u0007\u0012\u0003\u001f\u0002\u00102\u001c\u00071)\u0007\u0015\t\t\u0004\u0007 \n\u0015\u0003\u0012\t2\u001c\u00071\u0017\t\u0004\u0005G2\u001c\u00071=\t\u0006\u0010\t\u000b\u0013\n%2\u001c\u00071,\t\u0007\f\n\u0012\u0007\"\u000f\u000b+\u0007\u0003\u0010\u0007\u000f#\u00102\u001c\u00071=\t\u0004\u0004\u000f\"\u0007\u0006#\u000e!\n\u000b\u0003\u0012\t2\u001c\u00071\u0001\t\u0012\u0012%\u0007 \r\n\u0012\t2\u001c\u00071\u0017\t%\u0007C#\u0013\t2\u001c\u00071&\u000f!\t\u0007\u0010\u000f\u001f\t\u0010\u0002\t\u000b2\u0007\n\u0012\u0013\u00071\r\t\u0010\u0007\u0003\u0010\u0007\u000e\t2\u0016 \u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0006\u0007\u0004\u0003\u0006\u0010\t\u0012\t\u0013\u0007\u0006\u000f!\t\u0010\u0003!\t\u0006\u0007\u0010\u000f\u0007\n\u00071*\u0002\t\u0007$\t\n\u0010\u0004\t\u00062\u0007&'\u0007\n\u0004\u000e#!\u0016\u0007 \n\u001b\u0004\u0004\u0007\u0006\u0010#\u0013\t\u0012\u0010\u0006\u0007\u000b\t\u0006\u0005\u000f\u0012\u0013\t\u0013\u0007\u0010\u0002\n\u0010\u0007\u0010\u0002\t%\u0007\u0002\n\u0013\u0007\u0012\t\u0014\t\u000b\u0007\u0006#\u0012\u001f\u0007!#\u0006\u0003\f\u0007\u000f\u0015\u00071*\u0002\t\u0007 \n$\t\n\u0010\u0004\t\u00062\u0007\u000f\u0012\u0007\n\u0012\u0007\t\u0013 #\f\n\u0010\u0003\u000f\u0012\n\u0004\u0007\u000f\u000b\u00075\u0006\t!\u0003 -6\u0007\u0005\u000b\u000f\u0015\t\u0006\u0006\u0003\u000f\u0012\n\u0004\u0007\u000e\n\u0006\u0003\u0006\u0016\u0007\u001d\u000f\u0007 \n\u000f\u0012\t\u0007\u000f\"\u0012\t\u0013\u0007\u0010\u0002\t\u0007\n\u0004\u000e#!\u0007192\u001c\u0007#\u0006\t\u0013\u0007\u0003\u0012\u0007\u0010\u0002\t\u0007\t \u0005\t\u000b\u0003!\t\u0012\u0010\u0016 \n \u001e! \u0018\u000f\b\u0004\u0003\u0012\u0015\u000f\u0003 \n*\u0002\t\u0007\t\u0003\u001f\u0002\u0010\u0007\u0006\u0010#\u0013\t\u0012\u0010\u0006\u0007\"\t\u000b\t\u0007\u0003\u0012\u0014\u0003\u0010\t\u0013\u0007\u0003\u0012\u0007\n\u0007\f\u0004\n\u0006\u0006\u000b\u000f\u000f!\u0007\n\u0010\u0007\u0010\u0002\t\u0007$\u000b\n\u000e\n\u0012\u0010\u0007 \n&\u000f\u0012\u0006\t\u000b\u0014\n\u0010\u000f\u000b%\u0016\u0007*\u0002\t\u0007\u000f\u0010\u0002\t\u000b\u0007\u0010\t\u0012\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0006\u0007\"\t\u000b\t\u0007\u0003\u0012\u0014\u0003\u0010\t\u0013\u0007\u0003\u0012\u0007\n\u0012\u0007 \u000f\u0015\u0015\u0003\f\t\u0007\u000b\u000f\u000f!\u0007\n\u0010\u0007\u0010\u0002\t\u0007\u0004\n\u000e\u000f\u000b\n\u0010\u000f\u000b%\u0016\u0007 \n\u001e\u0003\u000b\u0006\u0010\u001c\u0007 \u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0006\u0007 \"\t\u000b\t\u0007 \n\u0006+\t\u0013\u0007 \u0010\u000f\u0007 \f\u000f!\u0005\u0004\t\u0010\t\u0007 \n\u0007 \u0006!\n\u0004\u0004\u0007 \n(#\t\u0006\u0010\u0003\u000f\u0012\u0012\n\u0003\u000b\t\u0007 \u0015\u000f\u000b\u0007 \u001f\n\u0010\u0002\t\u000b\u0003\u0012\u001f\u0007 \u0006\u000f!\t\u0007 \u0005\t\u000b\u0006\u000f\u0012\n\u0004\u0007 \n\u0010\u0010\u000b\u0003\u000e#\u0010\t\u0006\u0007 5\t\u0016\u001f\u0016\u001c\u0007 \u0012\n!\t\u001c\u0007 \u001f\t\u0012\u0013\t\u000b\u001c\u0007 \n\u001f\t6\u001c\u0007 \u0010\u0002\t\u0003\u000b\u0007 !#\u0006\u0003\f\n\u0004\u0007 \u0010\u000b\n\u0003\u0012\u0003\u0012\u001f\u0007 \u000e\n\f+\u001f\u000b\u000f#\u0012\u0013\u0007 \n\u0012\u0013\u0007 \u0010\u0002\t\u0003\u000b\u0007\u001f\t\u0012\t\u000b\n\u0004\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0003\u0010%\u0007\"\u0003\u0010\u0002\u00071*\u0002\t\u0007$\t\n\u0010\u0004\t\u00062\u0016 \n*\u0002\t\u0012\u001c\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0006\u0007\"\t\u000b\t\u0007\n\u0006+\t\u0013\u0007\u0010\u000f\u0007\u0005\t\u000b\u0015\u000f\u000b! \u0007\n\u0007\f\n\u000b\u0013 -\u0006\u000f\u000b\u0010\u0003\u0012\u001f\u0007\u0010\n\u0006+\u0007\u0003\u0012\u0007 \n\"\u0002\u0003\f\u0002\u0007\u0010\u0002\t%\u0007\u0002\n\u0013\u0007\u0010\u000f\u0007\u0006\u000f\u000b\u0010\u0007\u0010\"\t\u0004\u0014\t\u0007$\t\n\u0010\u0004\t\u0006\u0007\u0006\u000f\u0012\u001f\u0007\u0010\u0003\u0010\u0004\t\u0006\u0007\u0013\u0003\u0006\u0005\u0004\n%\t\u0013\u0007\u000f\u0012\u0007 \n\u0006!\n\u0004\u0004\u0007 \f\n\u000b\u0013\u000e\u000f\n\u000b\u0013\u0007 \f\n\u000b\u0013\u0006\u0016\u0007 \u001e\u000f\u000b\u0007 \u0010\u0002\n\u0010\u001c\u0007 \u0010\u0002\t%\u0007 \u000b\t\n\u0013\u0007 \n\u0007 \u0005\n\u0005\t\u000b\u0007 \u0015\u000f\u000b!\u0007 \n\f\u000f\u0012\u0010\n\u0003\u0012\u0003\u0012\u001f\u0007 \u000e\u000b\u0003\t\u0015\u0007 \u0003\u0012\u0006\u0010\u000b#\f\u0010\u0003\u000f\u0012\u0006\u0007 \u000f\u0012\u0007 \u0010\u0002\t\u0007 \u0010\n\u0006+\u0016\u0007 \u001b\u0015\u0010\t\u000b\u0007 \u0010\u0002\t%\u0007 \u0002\n\u0013\u0007 \t \u0005\u0004\n\u0003\u0012\t\u0013\u0007\u0010\u0002\t\u0007\u0003\u0012\u0006\u0010\u000b#\f\u0010\u0003\u000f\u0012\u0006\u0007\u0003\u0012\u0007\u0010\u0002\t\u0003\u000b\u0007\u000f\"\u0012\u0007\"\u000f\u000b\u0013\u0006\u0007\u0010\u000f\u0007\f\u0002\t\f+\u0007 \u0010\u0002\t\u0003\u000b\u0007 \n#\u0012\u0013\t\u000b\u0006\u0010\n\u0012\u0013\u0003\u0012\u001f\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0010\n\u0006+\u001c\u0007\u0010\u0002\t%\u0007\u000b\t\f\t\u0003\u0014\t\u0013\u0007\n\u0004\u0004\u0007\u0010\"\t\u0004\u0014\t\u0007\f\n\u000b\u0013\u0006\u0007\n\u0010\u0007 \u000b\n\u0012\u0013\u000f!\u0007\n\u0012\u0013\u0007\u000f\u0012\t -\u000e% -\u000f\u0012\t\u0016\u0007*\u0002\t%\u0007\"\t\u000b\t\u0007\u0003\u0012\u0006\u0010\u000b#\f\u0010\t\u0013\u0007\u0010\u000f\u0007\u0010\u000b%\u0007\u0010\u000f\u0007\u000b\t\f\n\u0004\u0004\u0007 \n\u0010\u0002\t\u0007\u0006\u000f\u0012\u001f\u0007\u0013\u0003\u0006\u0005\u0004\n%\t\u0013\u0007\u000f\u0012\u0007\u0010\u0002\t\u0007\f\n\u000b\u0013\u0007\n\u0012\u0013\u0007\u0010\u000f\u0007\u0006\u0003\u0012\u001f\u0007\n\u0007\u0015\t\"\u0007\u0012\u000f\u0010\t\u0006\u0007\u0003\u0012\u0007\u000f\u000b\u0013\t\u000b\u0007 \n\u0010\u000f\u0007\u0014\t\u000b\u0003\u0015%\u0007\u0010\u0002\n\u0010\u0007\u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0006\u000f\u0012\u001f\u0007\"\n\u0006\u0007\u000b\t\f\u000f\u0004\u0004\t\f\u0010\t\u0013\u0016\u0007)\u0015\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0006 \n\"\t\u000b\t\u0007\u0010\u000f\u0010\n\u0004\u0004%\u0007#\u0012\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\"\u0003\u0010\u0002\u0007\u0010\u0002\t\u0007\u0006\u000f\u0012\u001f\u001c\u0007\u0010\u0002\t%\u0007\"\t\u000b\t\u0007\n\u0006+\t\u0013\u0007\u0010\u000f\u0007\u0005#\u0010\u0007 \u0010\u0002\t\u0007\f\n\u000b\u0013\u0007\n\u0006\u0003\u0013\t\u0016\u0007)\u0015\u0007\u0010\u0002\t%\u0007\u0002\n\u0005\u0005\t\u0012\t\u0013\u0007\u0010\u000f\u0007+\u0012\u000f\"\u0007\u0010\u0002\t\u0007\u0006\u000f\u0012\u001f\u001c\u0007\u0010\u0002\t%\u0007\"\t\u000b\t\u0007 \u0003\u0012\u0006\u0010\u000b#\f\u0010\t\u0013\u0007\u0010\u000f\u0007\u0006\u000f\u000b\u0010\u0007\n\u0004\u0004\u0007\f\n\u000b\u0013\u0006\u0007\u0004\u0003\u0006\u0010 -\"\u0003\u0006\t\u0007\u0003\u0012\u0007\u0013\t\u0006\f\t\u0012\u0013\u0003\u0012\u001f\u0007\u000f\u000b\u0013\t\u000b\u0007\u000f\u0015\u0007 \n\u0015\n!\u0003\u0004\u0003\n\u000b\u0003\u0010%7\u0007\u0010\u0002\t\u0007!\u000f\u0006\u0010\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0007\u0006\u0002\u000f#\u0004\u0013\u0007\n\u0005\u0005\t\n\u000b\u0007\n\u0010\u0007\u0010\u0002\t\u0007\u0010\u000f\u0005\u0007\u000f\u0015\u0007 \n\u0010\u0002\t\u0007\u0004\u0003 \u0006\u0010\u0007\n\u0012\u0013\u0007\u0010\u0002\t\u0007\u0004\t\n\u0006\u0010\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0007\n\u0010\u0007\u0010\u0002\t\u0007\u000e\u000f\u0010\u0010\u000f!\u0016\u0007\u001d\u000f\u0007\u0010\u0003\t\u0006\u0007\"\t\u000b\t\u0007 \n\n\u0004\u0004\u000f\"\t\u0013\u0016\u0007*\u0002\t%\u0007\"\t\u000b\t\u0007\n\u0004\u0004\u000f\"\t\u0013\u0007\u0010\u000f\u0007\u0006\u0002\u0003\u0015\u0010\u0007\u0010\u0002\t\u0007\f\n\u000b\u0013\u0006\u0007#\u0012\u0010\u0003\u0004\u0007\u0010\u0002\t%\u0007\"\t\u000b\t\u0007 \u0006\n\u0010\u0003\u0006\u0015\u0003\t\u0013\u0007\"\u0003\u0010\u0002\u0007\u0010\u0002\t\u0007\n\u000b\u000b\n\u0012\u001f\t!\t\u0012\u0010\u0016\u0007 \n\u001b\u0015\u0010\t\u000b\u0007\u0010\u0002\t\u0007\u0006\u000f\u000b\u0010\u0003\u0012\u001f\u0007\u0010\n\u0006+\u001c\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0006\u0007\u000b\t\f\t\u0003\u0014\t\u0013\u0007\n\u0012\u000f\u0010\u0002\t\u000b\u0007\u0005\n\u0005\t\u000b\u0007\u0015\u000f\u000b!\u0007 \n\f\u000f\u0012\u0010\n\u0003\u0012\u0003\u0012\u001f\u0007\u0010\u0002\t\u0007\u0003\u0012\u0006\u0010\u000b#\f\u0010\u0003\u000f\u0012\u0006\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\t \u0005\t\u000b\u0003!\t\u0012\u0010\n\u0004\u0007\u0010\n\u0006+\u0007\n\u0012\u0013\u0007\"\t\u000b\t\u0007 \n\n\u001f\n\u0003\u0012\u0007\n\u0006+\t\u0013\u0007\u0010\u000f\u0007\t \u0005\u0004\n\u0003\u0012\u0007\u0010\u0002\t\u0007\u0003\u0012\u0006\u0010\u000b#\f\u0010\u0003\u000f\u0012\u0007\u0003\u0012\u0007\u0010\u0002\t\u0003\u000b\u0007\u000f\"\u0012\u0007\"\u000f\u000b\u0013\u0006\u0016\u0007*\u0002\t\u0007 \n\"\u0002\u000f\u0004\t\u0007\u0005\u000b\u000f\f\t\u0013#\u000b\t\u0007\"\n\u0006\u0007\u0005\u000b\n\f\u0010\u0003\f\t\u0013\u0007\u000f\u0012\f\t\u0007\u000e%\u0007#\u0006\u0003\u0012\u001f\u00071*\u0002\t\u0007$\t\n\u0010\u0004\t\u00062\u0007 \n\u0006\u000f\u0012\u001f\u00071\u001b\u0004\u0004\u0007%\u000f#\u0007\u0012\t\t\u0013\u0007\u0003\u0006\u0007\u0004\u000f\u0014\t2\u001c\u0007\u000e\t\u0015\u000f\u000b\t\u0007\u0010\u0002\t%\u0007\u0013\u0003\u0013\u0007\u0010\u0002\t\u0007\t \u0005\t\u000b\u0003!\t\u0012\u0010\n\u0004\u0007 \u0010\t\u0006\u0010\u0016\u0007*\u0002\t\u0007\u0005\u000b\u000f\f\t\u0013#\u000b\t\u0007\"\n\u0006\u0007\n\u0006\u0007\u0015\u000f\u0004\u0004\u000f\"\u0006\u0016\u0007\u001e\u0003\u000b\u0006\u0010\u001c\u0007\u0005\n\u000b\u0010\u0003 \f\u0003\u0005\n\u0012\u0010\u0006\u0007\u000b\t\f\t\u0003\u0014\t\u0013\u0007 \n\n\u0007\f\n\u000b\u0013\u0007\u0013\u0003\u0006\u0005\u0004\n%\u0003\u0012\u001f\u0007\n\u0007\u0006\u000f\u0012\u001f\u0007\u0010\u0003\u0010\u0004\t\u0016\u0007*\u0002\t\u0007\u0010\"\u000f\u0007\u0010\u000f\u0005 -!\u000f\u0006\u0010\u0007\u0006\u000f\u0012\u001f\u0007\u0010\u0003\u0010\u0004\t\u0006\u0007\n\u0012\u0013\u0007 \n\u0010\u0002\t\u0007\u0010\"\u000f\u0007\u000e\u000f\u0010\u0010\u000f!\u0007\u0006\u000f\u0012\u001f\u0007\u0010\u0003\u0010\u0004\t\u0006\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0005\t\u000b\u0006\u000f\u0012\n\u0004\u0004%\u0007\n\u000b\u000b\n\u0012\u001f\t\u0013\u0007\u0006\u000f\u0012\u001f\u0007\u0004\u0003\u0006\u0010\u0007 \"\t\u000b\t\u0007\u0005\u000b\u000f\u0014\u0003\u0013\t\u0013\u0007\u0010\u000f\u0007\u0010\u0002\t!\u0007\n\u0010\u0007\u000b\n\u0012\u0013\u000f!\u0016\u0007*\u0002\t%\u0007\"\t\u000b\t\u0007\u0003\u0012\u0006\u0010\u000b#\f\u0010\t\u0013\u0007\u0010\u000f\u0007 \n\u0003!\n\u001f\u0003\u0012\t\u0007\u0010\u0002\t\u0007\u0014\u000f\f\n\u0004\u0007!\t\u0004\u000f\u0013%\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0006\u000f\u0012\u001f\u0007\u001f\u0003\u0014\t\u0012\u001c\u0007\u0010\u000f\u0007\u0015\u000f\u000b\u001f\t\u0010 \u0007\n\u000e\u000f#\u0010\u0007 \n\u0010\u0002\t\u0007\u0004%\u000b\u0003\f\u0006\u001c\u0007\n\u0012\u0013\u0007\u0010\u000f\u0007\u0015\u000f\f#\u0006\u0007\u000f\u0012\u0007\u0010\u0002\t\u0007\u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007\u0005\t\u000b\u0015\u000f\u000b!\n\u0012\f\t\u0007\u000e%\u00071*\u0002\t\u0007 $\t\n\u0010\u0004\t\u00062\u0016\u0007 *\u0002\t%\u0007 \u0013\u0003\u0013\u0007 \u0012\u000f\u0010\u0007 \u0012\t\t\u0013\u0007 \u0010\u000f\u0007 \u000b\t\f\u000f\u0004\u0004\t\f\u0010\u0007 \u0010\u0002\t\u0007 \"\u0002\u000f\u0004\t\u0007 \u0014\u000f\f\n\u0004\u0007 !\t\u0004\u000f\u0013%\u0007\u000e#\u0010\u0007\"\t\u000b\t\u0007\n\u0006+\t\u0013\u0007\u0010\u000f\u0007\u000b\t\u0006\u0010\u000b\u0003\f\u0010\u0007\u0010\u0002\t!\u0006\t\u0004\u0014\t\u0006\u0007\u0010\u000f\u0007\n\u0007\u0005\n\u0006\u0006\n\u001f\t\u001c\u0007 \"\u0002\u0003\f\u0002\u0007\u0010\u0002\t%\u0007\f\u000f#\u0004\u0013\u0007\t\n\u0006\u0003\u0004%\u0007\u000b\t!\t!\u000e\t\u000b\u0007\n\u0012\u0013\u0007\u0006\u0003\u0012\u001f\u0016\u0007*\u0002\t\u0012\u001c\u0007\u0010\u0002\t%\u0007\"\t\u000b\t\u0007 \n\u0003\u0012\u0006\u0010\u000b#\f\u0010\t\u0013\u0007\u0010\u000f \u0007\u0006\u0003\u0012\u001f\u0007\u0010\u0002\t\u0007\u0006\n!\t\u0007\u0005\n\u0006\u0006\n\u001f\t\u0007 \b\n\u0007\u000b\t \u0007\u0003\u0012\u0007\n\u0012%\u0007\u0005\u000b\t\u0015\t\u000b\u000b\t\u0013\u0007\u0006\u0010%\u0004\t\u0007 \n\u000f\u0015\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u00075\u0012\u000f\u0010\u0007\"\u0002\u0003\u0006\u0010\u0004\u0003\u0012\u001f6\u0007\"\u0003\u0010\u0002\u000f#\u0010\u0007#\u0006\u0003\u0012\u001f\u0007\u0010\u0002\t\u0007\u0006\u000f\u0012\u001f\u0007\u0004%\u000b\u0003\f\u0006\u0016\u0007\u001d\u000f\u0007 \u000b\t\u0006\u0010\u000b\u0003\f\u0010\u0003\u000f\u0012\u0006\u0007\u000f\u0012\u0007\u0010\u0002\t\u0007\u0004\t\u0012\u001f\u0010\u0002\u001c\u0007\u0010\t!\u0005\u000f\u001c\u0007+\t%\u0007\u000f\u000b\u0007\f\u000f\u0012\u0010\t\u0012\u0010\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0005\n\u0006\u0006\n\u001f\t\u0007 \"\t\u000b\t\u0007\u001f\u0003\u0014\t\u0012\u0016\u0007\u0007\u001d\u000f\u0007\u0003\u0012\u0006\u0010\u000b#\f\u0010\u0003\u000f\u0012\u0006\u0007\"\t\u000b\t\u0007\u0005\u000b\u000f\u0014\u0003\u0013\t\u0013\u0007\u0002\u000f\"\u0007\u0010\u000f\u0007\u0006\u0003\u0012\u001f\u00075\t\u0016\u001f\u0016\u001c\u0007 \u000e\u000b\t\n\u0010\u0002\u0007\f\u000f\u0012\u0010\u000b\u000f\u0004\u001c\u0007 \t\u0012#\u0012\f\u0003\n\u0010\u0003\u000f\u0012\u001c\u0007\u0014\u000f\u0004#!\t\u001c\u0007\u000e\u000f\u0013%\u0007\u0005\u000f\u0006\u0010#\u000b\t6\u0016\u0007*\u0002\t\u0012\u001c\u0007\u0010\u0002\t%\u0007 \n\u0004\u0003\u0006\u0010\t\u0012\t\u0013\u0007\u0010\u000f\u0007\u0010\u0002\t\u0007\u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u00071$\t\n\u0010\u0004\t\u00062\u0007\u000b\t\f\u000f\u000b\u0013\u0003\u0012\u001f\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0006\u000f\u0012\u001f\u0007\u000f\u0012\u0007&'\u0007 \n#\u0006\u0003\u0012\u001f\u0007 \n\u0007 \u0005\u000f\u000b\u0010\n\u000e\u0004\t\u0007 \u0005\u0004\n%\t\u000b\u0007 \"\u0003\u0010\u0002\u0007 \u0002\t\n\u0013\u0005\u0002\u000f\u0012\t\u0006\u0016\u0007 *\u0002\t%\u0007 \f\u000f#\u0004\u0013\u0007 \u0006\u0010\u000f\u0005\u0007 \n\u0004\u0003\u0006\u0010\t\u0012\u0003\u0012\u001f\u0007 \n\u0010\u0007 \n\u0012%\u0007 !\u000f!\t\u0012\u0010\u0007 \u0003\u0012\u0007 \u0010\u0003!\t\u001c\u0007 \u000e#\u0010\u0007 \"\t\u000b\t\u0007 \u0012\u000f\u0010\u0007 \n\u0004\u0004\u000f\"\t\u0013\u0007 \u0010\u000f\u0007 \u000b\t\u0005\t\n\u0010\u0007\u000f\u000b\u0007\u0006+\u0003\u0005\u0007\u0005\n\u0006\u0006\n\u001f\t\u0006\u0016\u0007*\u0002\t%\u0007\"\t \u000b\t\u0007\n\u0004\u000b\t\n\u0013%\u0007\u0003\u0012\u0006\u0010\u000b#\f\u0010\t\u0013\u0007\u0010\u000f\u0007\u0006\u0003\u0012\u001f\u0007 \n\u0010\u0002\t\u0007 \u0006\n!\t\u0007 \u0005\n\u0006\u0006\n\u001f\t\u0007 \u0015\u000f\u000b\u0007 \u0010\u0002\t\u0007 \u0010\u0002\u0003\u000b\u0013\u0007 \u0010\u0003!\t\u001c\u0007 \u0003!!\t\u0013\u0003\n\u0010\t\u0004%\u0007 \n\u0015\u0010\t\u000b\u0007 \u0006\u0010\u000f\u0005\u0005\u0003\u0012\u001f\u0007 \u0004\u0003\u0006\u0010\t\u0012\u0003\u0012\u001f\u0016\u0007 \u001b\u0004\u0004\u0007 \u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0006\u0007 \"\t\u000b\t\u0007 \u000b\t\f\u000f\u000b\u0013\t\u0013\u0007 \u0015\u000f\u000b\u0007 \u0004\n\u0010\t\u000b\u0007 \n\u0012\n\u0004%\u0006\t\u0006\u0016\u0007 \n \u001e\" \u0018\u000f\b\u0004\u0003\u0006\u0006\r\t\n\u0007\b\u0002\u0007\u0005\u0016\u0003\u0007\u0006\u0015\t\n\u0007\f\u0003\u000e\b\u0012\r\u0003\u0006 \n\u001d\u000f\u0010\t\u0007\u000f\u0012\u0006\t\u0010\u0006\u001c\u0007\u0012\u000f\u0010\t\u0007\u000f\u0015\u0015\u0006\t\u0010\u0006\u0007\n\u0012\u0013\u0007\u000e\t\n\u0010\u0007!\n\u000b+\t\u000b\u0006\u0007\"\t\u000b\t\u0007\u0005\u000f\u0006\u0003\u0010\u0003\u000f\u0012\t\u0013\u0007 \n!\n\u0012#\n\u0004\u0004%\u0007\u000e%\u0007 \u0004\u0003\u0006\u0010\t\u0012\u0003\u0012\u001f\u0007\n\u0012\u0013\u0007#\u0006\u0003\u0012\u001f\u0007\n\u0007\u0014\u0003\u0006#\n\u0004\u0007\u000b\t\u0005\u000b\t\u0006\t\u0012\u0010\n\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007 \n\n#\u0013\u0003\u000f\u0007\"\n\u0014\t\u0015\u000f\u000b!\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0005\n\u0006\u0006\n\u001f\t\u0007\u0006#\u0012\u001f\u001c\u0007\u0003\u0010\u0006\u0007\u0005\u0003\u0010\f\u0002\u0007\f\u000f\u0012\u0010\u000f#\u000b\u0007\n\u0012\u0013\u0007\u0003\u0010\u0006\u0007 \"\u0003\u0013\t -\u000e\n\u0012\u0013\u0007\u0006\u0005\t\f\u0010\u000b\u000f\u001f\u000b\n!\u0016\u0007$\t\n\u0010\u0007!\n\u000b+\t\u000b\u0006\u0007\u0013\t!\n\u000b\f\n\u0010\t\u0007\u0010\u0002\t\u0007\u000f\u0012\u0006\t\u0010\u0007\u000f\u0015\u0007 \n\u0010\u0002\t\u0007\u0002#!\n\u0012\u0007\u0006\t\u0012\u0006\t\u0007\u000f\u0015\u0007\n\u0007\u000e\t\n\u0010\u00075\t\u0016\u001f\u0016\u001c\u0007\u0015\u000f\u000f\u0010\u0007\u0010\n\u0005\u0005\u0003\u0012\u001f67\u0007\u0010\u0002\t\u0007\n\u0014\t\u000b\n\u001f\t\u0007 \n\u0013\u0003\u0006\u0010\n\u0012\f\t\u0007 \u000e\t\u0010\"\t\t\u0012\u0007 \u0006#\f\f\t\t\u0013 \u0003\u0012\u001f\u0007 \u000e\t\n\u0010\u0006\u0007 5\u0003\u0012\u0010\t\u000b -\u000e\t\n\u0010\u0007 \u0005\t\u000b\u0003\u000f\u00136\u0007 \u0003\u0006\u0007 \n\u0003\u0012\u0014\t\u000b\u0006\t\u0004%\u0007\u0005\u000b\u000f\u0005\u000f\u000b\u0010\u0003\u000f\u0012\n\u0004\u0007\u0010\u000f\u0007\u0010\u0002\t\u0007\u0010\t!\u0005\u000f\u0007\u0003\u0012\u0007\"\u0002\u0003\f\u0002\u0007\u0010\u0002\t\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u0003\u0006\u0007 \u0013\u000f\u0012\t\u0016\u0007)\u0012\u0007\u001f\t\u0012\t\u000b\n\u0004\u001c\u0007\n\u0012\u0007\u0003\u0012\u0010\t\u000b -\u000e\t\n\u0010\u0007\u0005\t\u000b\u0003\u000f\u0013\u0007\t(#\n\u0004\u0006\u0007\u0010\u0002\t\u0007\u0013#\u000b\n\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u0007\n\u0007 \n(#\n\u000b\u0010\t\u000b\u0007\u0012\u000f\u0010\t\u0016\u0007\u001e\u000f\u000b\u0007\u0010\t!\u0005\u000f\u0007!\t\n\u0006#\u000b\t!\t\u0012\u0010\u001c\u0007\n\u0004\u0006\u000f\u0007\u0010\u0002\t\u0007\n#\u0013\u0003\u000f\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007 \u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007$\t\n\u0010\u0004\t\u0006\u0007\u000b\t\f\u000f\u000b\u0013\u0003\u0012\u001f\u0006\u0007\"\n\u0006\u0007\u0005\u000b\u000f\f\t\u0006\u0006\t \u0013\u0016\u0007*\u0002\t\u0007\u0005\u0003\u0010\f\u0002\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007 \n\u0006#\u0012\u001f\u0007!\t\u0004\u000f\u0013\u0003\t\u0006\u0007\"\n\u0006\u0007!\t\n\u0006#\u000b\t\u0013\u0007#\u0006\u0003\u0012\u001f\u0007\u0006#\u000e -\u0002\n\u000b!\u000f\u0012\u0003\f\u0007\u0006#!!\n\u0010\u0003\u000f\u0012\u0007 \n\u0003\u0012\u0007\u0010\u0003!\t\u0007\u0015\u000b\n!\t\u0006\u0007\u000f\u0015\u0007\u0018/\u0007!\u0003\u0004\u0004\u0003\u0006\t\f\u000f\u0012\u0013\u0006\u00075\u0017\t\u000b!\t\u0006\u001c\u00079:;;6\u0016\u0007*\u0002\t\u0007\u0005\u0003\u0010\f\u0002\u001c\u0007 \n\u0010\u0002\t\u0007\u0012\u000f\u0010\t\u0007\u000f\u0012\u0006\t\u0010\u0006\u0007\n\u0012\u0013\u0007\u000f\u0015\u0015\u0006\t\u0010\u0006\u001c\u0007\n\u0012\u0013\u0007\u0010\u0002\t\u0007\u0010\t!\u0005\u000f\u0007\"\t\u000b\t\u0007\f\u000f!\u000e\u0003\u0012\t\u0013\u0007\u0010\u000f\u0007 \f\u000f!\t\u0007\n\u0010\u0007\n\u0007!#\u0006\u0003\f\u0007\u0006\f\u000f\u000b\t\u0007\u0010\u000b\n\u0012\u0006\f\u000b\u0003\u0005\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0006#\u0012\u001f\u0007!\t\u0004\u000f\u0013%\u0016\u0007*\u0002\t !\t\u0013\u0003\n\u0012\u0007\u0005\u0003\u0010\f\u0002\u0007\u0015\u000b\t(#\t\u0012\f%\u0007\u000e\t\u0010\"\t\t\u0012\u0007\n\u0007\u0012\u000f\u0010\t\u0007\u000f\u0012\u0006\t\u0010\u0007\n\u0012\u0013\u0007\u000f\u0015\u0015\u0006\t\u0010\u0007\"\n\u0006\u0007 #\u0006\t\u0013\u0007\u0010\u000f\u0007(#\n\u0012\u0010\u00034\t\u0007\u0010\u0002\t\u0007!#\u0006\u0003\f\n\u0004\u0007\u0005\u0003\u0010\f\u0002\u0007\u0014\n\u0004#\t\u0007\u0015\u000f\u000b\u0007\t\n\f\u0002\u0007\u0012\u000f\u0010\t\u0007#\u0006\u0003\u0012\u001f\u0007 \u0010\u0002\t\u0007 \t(#\n\u0004\u0004%\u0007 \u0010\t!\u0005\t\u000b\t\u0013\u0007 !#\u0006\u0003\f\n\u0004\u0007 \u0006\f\n\u0004\t\u0007 \u0010#\u0012\t\u0013\u0007 \n\u0010\u0007 \u001b -\u0018\u0018/\u0016\u0007 *\u0002\t\u0007 \n\u000b\t\u0006#\u0004\u0010\u0003\u0012\u001f\u0007\u0010\u000b\n\u0012\u0006\f\u000b\u0003\u0005\u0010\u0003\u000f\u0012\u0007\"\n\u0006\u0007\t\u0012\f\u000f\u0013\t\u0013\u0007\u0003\u0012\u0007\n\u0007@)')\u0007\u000b\t\u0005\u000b\t\u0006\t\u0012\u0010\n\u0010\u0003\u000f\u0012\u0007 \n\u0015\u000f\u000b\u0007\u0004\u0003\u0006\u0010\t\u0012\u0003\u0012\u001f\u0007\n\u0012\u0013\u0007\u0014 \u0003\u0006#\n\u0004\u0007\u0003\u0012\u0006\u0005\t\f\u0010\u0003\u000f\u0012\u0007\u0003\u0012\u0007!#\u0006\u0003\f\u0007\u0012\u000f\u0010\n\u0010\u0003\u000f\u0012\u0007\u0006\u000f\u0015\u0010\"\n\u000b\t\u0016 \n\u001e\u000f\u000b\u0007 \n\u0004\u0004\u0007 \u0006#\u0012\u001f\u0007 !\t\u0004\u000f\u0013\u0003\t\u0006\u001c\u0007 \u0010\u0002\t\u0007 !#\u0006\u0003\f\u0007 \u0010\u000b\n\u0012\u0006\f\u000b\u0003\u0005\u0010\u0003\u000f\u0012\u0007 \u000f\u0015\u0007 \u0010\u0002\t\u0007 \n\f\u000f\u000b\u000b\t\u0006\u0005\u000f\u0012\u0013\u0003\u0012\u001f\u0007\u0014\u000f\f\n\u0004\u0007!\t\u0004\u000f\u0013%\u0007\u0004\u0003\u0012\t\u0007\u000f\u0012\u0007\u0010\u0002\t\u0007\u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007\u000b\t\f\u000f\u000b\u0013\u0003\u0012\u001f\u0006\u0007\u000f\u0015\u0007 1*\u0002\t\u0007 $\t\n\u0010\u0004\t\u00062\u0007 \"\n\u0006\u0007 \f\u000f\u0004\u0004\t\f\u0010\t\u0013\u0016\u0007 D\u000f\u0012\u001f\u000e\u000f\u000f+\u0006\u0007 \"\t\u000b\t\u0007 #\u0006\t\u0013\u0007 \n\u0006\u0007 \n\u0007 \u0006\u000f#\u000b\f\t7\u0007\u0010\u000b\n\u0012\u0006\f\u000b\u0003\u0005\u0010\u0003\u000f\u0012\u0006\u0007\"\u0003\u0010\u0002\u0007 \t\f\u0002\u000b\b \u0007\u0010\u0003!\u0003\u0012\u001f\u0007 \"\t\u000b\t\u0007\f\u0002\t\f+\t\u0013\u0007\u000f\u0012\u0007\u0010\u0002\t\u0003\u000b\u0007 \n\f\u000f\u000b\u000b\t\f\u0010\u0012\t\u0006\u0006\u0007\u0015\u000f\u000b\u0007\u000e\t\u0003\u0012\u001f\u0007#\u0006\t\u0013\u0007\n\u0006\u0007\n\u0007\u000b\t\u0015\t\u000b\t\u0012\f\t\u0016\u0007 \n \u001e# \u001f\u0003\u000b\u0006\u0015\u000f\u0003\u0006 \n\u001e\u000f#\u000b\u0007 \u0013\u0003\u0015\u0015\t\u000b\t\u0012\u0010\u0007 !\t\n\u0006#\u000b\t\u0006\u0007 \"\t\u000b\t\u0007 \u0013\t\u0015\u0003\u0012\t\u0013\u0007 \u0010\u000f\u0007 \n\u0006\u0006\t\u0006\u0006\u0007 \u0010\u0002\t\u0007 \u0006#\u0012\u001f\u0007 \n!\t\u0004\u000f\u0013\u0003\t\u0006\u0016\u0007\u0007@\t\n\u0006#\u000b\t\u0006\u0007\"\t\u000b\t\u0007\f\n\u0004\f#\u0004\n\u0010\t\u0013\u0007#\u0006\u0003\u0012\u001f\u0007\u0010\u0002\t\u0007\u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007$\t\n\u0010\u0004\t\u0006\u0007 !\t\u0004\u000f\u0013%\u0007\n\u0006\u0007\n\u0007\u000b\t\u0015\t\u000b\t\u0012\f\t\u00075\u0003\u0016\t\u0016\u001c\u0007\u0010\u0002\t\u0007\n\f\u0010#\n\u0004\u0007!\t\u0004\u000f\u0013%6\u0016 \n*\u0002\t\u0007!\t\n\u0006#\u000b\t \r\b \u0005\u000e\u0007\u000e\u000f \u0007\"\n\u0006\u0007\n\u0006\u0006\t\u0006\u0006\t\u0013\u0007\u000e%\u0007\u0010\u0002\t\u0007\u0006\t!\u0003\u0010\u000f\u0012\t\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u0007 \n\u0010\u0002\t\u0007 \u0015\u0003\u000b\u0006\u0010\u0007 \u0010\u000f\u0012\t\u0007 \n\u0006\u0007 \u0005\u000b\u000f\u0013#\f\t\u0013\u0007 \u000e%\u0007 \u0010\u0002\t\u0007 \u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0007 \u0015\u000b\u000f!\u0007 \u0010\u0002\t\u0007 \f\u000f\u000b\u000b\t\u0006\u0005\u000f\u0012\u0013\u0003\u0012\u001f\u001c\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0005\u0003\u0010\f\u0002\u0007\n\u0006\u0007\u0015\u000f#\u0012\u0013\u0007\u000f\u0012\u0007\u0010\u0002\t\u0007\u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007$\t\n\u0010\u0004\t\u0006\u0007 \u000b\t\f\u000f\u000b\u0013\u0003\u0012\u001f\u0016\u0007*\u0002\u0003\u0006\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0007\"\n\u0006\u0007\f\u000f\u000b\u000b\t\f\u0010\t\u0013\u0007\u0015\u000f\u000b\u0007\u000f\f\u0010\n\u0014\t\u0016\u0007 \n*\u0002\t\u0007 !\t\n\u0006#\u000b\t\u0007 \u000b\u0004\u000e\b\u0004\u0005\u0006\r \"\n\u0006\u0007 \u0013\t\u0015\u0003\u0012\t\u0013\u0007 \n\u0006\u0007 \u0010\u0002\t \u0005\u000b\u000f\u0005\u000f\u000b\u0010\u0003\u000f\u0012\u0007 \u000f\u0015\u0007 \n\f\u000f\u000b\u000b\t\f\u0010\u0004%\u0007 \u0006#\u0012\u001f\u0007 \u0005\u0003\u0010\f\u0002\u0007 !\u000f\u0014\t!\t\u0012\u0010\u0006 \u0010\r \u001b\u0006\u0007 \u0006\u0002\u000f\"\u0012\u0007 \u0003\u0012\u0007 \u001e\u0003\u001f#\u000b\t\u0007 9\u001c\u0007 \n\u0007 \n\f\u000f\u0012\u0010\u000f#\u000b\u0007 \u000b\t\u0005\u000b\t\u0006\t\u0012\u0010\n\u0010\u0003\u000f\u0012\u0007 \u000f\u0015\u0007 \n\u0007 !\t\u0004\u000f\u0013%\u0007 \u0003\u0006\u0007 \n\u0007 \u0006\t(#\t\u0012\f\t\u0007 \u000f\u0015\u0007 \u0010\u0002\u000b\t\t\u0007 \n\u0013\u0003\u0015\u0015\t\u000b\t\u0012\u0010\u0007\u0005\u0003\u0010\f\u0002\u0007!\u000f\u0014\t!\t\u0012\u0010\u0006<\u0007 \u0011\u0004\n\u000e \u001c\u0007 \u0012\u0002\u0013\t \u0007\n\u0012\u0013\u0007 \u0005\u0014 \u0016\u0007D\u000f\u001c\u0007\u0003\u0015\u0007\u000f\u0012\t\u0007 \n\u0006\u0003\u0012\u001f\u0006\u0007\n\u0007!\t\u0004\u000f\u0013%\u0007\u000e%\u0007\u001f\u000f\u0003\u0012\u001f\u0007#\u0005\u0007\u0006\u0003 \u0007\u0010\u0003!\t\u0006\u0007\n\u0012\u0013\u0007\u001f\u000f\u0003\u0012\u001f\u0007\u0013\u000f\"\u0012\u0007\u0010\"\u0003\f\t\u0007 \n\u0012\u0013\u0007\u0015\u0003\u0014\t\u0007\u000f\u0015\u0007 \u0010\u0002\t\u0006\t\u0007\u0005\u0003\u0010\f\u0002\u0007!\u000f\u0014\t!\t\u0012\u0010\u0006\u0007\u0002\n\u0005\u0005\t\u0012\u0007\u0010\u000f\u0007\u000e\t\u0007\n\u0010\u0007\u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007 \n\u0005\u000f\u0006\u0003\u0010\u0003\u000f\u0012\u001c\u0007\u0010\u0002\t\u0007 \u000b\u0004\u000e\b\u0004\u0005\u0006 \u0007!\t\n\u0006#\u000b\t\u0007\u0006\u0010\n\u0010\t\u0006<\u0007/\u0016\u001aA\u0019\u00075\u0019H;6\u0016 \nI#\u0003\u0010\t\u0007\u0006\u0003!\u0003\u0004\n\u000b\u0007\u0010\u000f\u0007\u0010\u0002\t\u0007 \u000b\u0004\u000e\b\u0004\u0005\u0006 \u0007!\t\n\u0006#\u000b\t\u001c\u0007\u0010\u0002\t\u0007 \u0007\u000e\b\t\u0006\u0003\u0002\u0015 \u0007!\t\n\u0006#\u000b\t\u0007\"\n\u0006\u0007 \n\u0013\t\u0015\u0003\u0012\t\u0013\u0007 \n\u0006\u0007 \u0010\u0002\t\u0007 \u0005\u000b\u000f\u0005\u000f\u000b\u0010\u0003\u000f\u0012\u0007 \u000f\u0015\u0007 \f\u000f\u000b\u000b\t\f\u0010\u0004%\u0007 \u0006#\u0012\u001f\u0007 \u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0016\u0007 \u001b\u0006\u0007 \n\u0006\u0002\u000f\"\u0012\u0007 \u0003\u0012\u0007 \u001e\u0003\u001f#\u000b\t\u0007 9\u001c\u0007 !#\u0006\u0003\f\n\u0004\u0007 \u0003\u0012\u0010\t\u000b\u0014\n \u0004\u0006\u0007 \n\u000b\t\u0007 \u0010\u0002\t\u0007 \u0006\t!\u0003\u0010\u000f\u0012\t\u0007 \n\u0013\u0003\u0006\u0010\n\u0012\f\t\u0006\u0007\u000e\t\u0010\"\t\t\u0012\u0007\u0010\"\u000f\u0007\u0006#\f\f\t\t\u0013\u0003\u0012\u001f\u0007\u0012\u000f\u0010\t\u0006\u0007\u0003\u0012\u0007\n\u0007 !\t\u0004\u000f\u0013%\u0016\u0007*\u0002\u0003\u0006\u0007 !\t\n\u0006#\u000b\t\u0007\"\n\u0006\u0007\f\u000f\u000b\u000b\t\f\u0010\t\u0013\u0007\u0015\u000f\u000b\u0007\u0010\u0002\t\u0007\u0015\n\f\u0010\u0007\u0010\u0002\n\u0010\u0007\u0010\u0002\t\u0007\f\u0002\n\u0012\u001f\t\u0007\u000f\u0015\u0007\u000f\u0012\t\u0007\u0012\u000f\u0010\t\u0007 \n\u0015\u0015\t\f\u0010\u0006\u0007\u0010\"\u000f\u0007\f\u000f\u000b\u000b\t\u0006\u0005\u000f\u0012\u0013\u0003\u0012\u001f\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0016 \n*\u000f\u0007\f\u000f!\u0005#\u0010\t\u0007\u0010\u0002\t\u0007 \u000b\u0004\u000e\b\u0004\u0005\u0006 \u0007\n\u0012\u0013\u0007 \u0007\u000e\b\t\u0006\u0003\u0002\u0015 \u0007!\t\n\u0006#\u000b\t\u0006\u001c\u0007\n\u0012\u0007\u000f\u0005\u0010\u0003!\n\u0004\u0007 \n\n\u0004\u0003\u001f\u0012!\t\u0012\u0010\u0007\u0003\u0012\u0007\u0010\u0002\t\u0007\u0012#!\u000e\t \u000b\u0007\u000f\u0015\u0007!\n\u0010\f\u0002\t\u0006\u0007\u000e\t\u0010\"\t\t\u0012\u0007\t\u0004\t!\t\u0012\u0010\u0006\u0007\u0002\n\u0006\u0007\u0010\u000f\u0007 \n\u000e\t\u0007 \t\u0006\u0010\n\u000e\u0004\u0003\u0006\u0002\t\u0013\u0007 \u000e\t\u0010\"\t\t\u0012\u0007 \u0010\u0002\t\u0007 \u0006#\u0012\u001f\u0007 !\t\u0004\u000f\u0013%\u0007 \n\u0012\u0013\u0007 \u0010\u0002\t\u0007 \n\f\u0010#\n\u0004\u0007 !\t\u0004\u000f\u0013%\u0016\u0007*\u0002\u0003\u0006\u0007\"\n\u0006\u0007\u0013\u000f\u0012\t\u0007\u000e%\u0007\n\u0005\u0005\u000b\u000f \u0003!\n\u0010\t\u0007\u0006\u0010\u000b\u0003\u0012\u001f\u0007!\n\u0010\f\u0002\u0003\u0012\u001f\u0007\u0003\u0012\u0007\n\u0007 \u0013%\u0012\n!\u0003\f\u0007 \u0005\u000b\u000f\u001f\u000b\n!!\u0003\u0012\u001f\u0007 \u0015\u000b\n!\t\"\u000f\u000b+\u0007 5\u0006\t\t\u001c\u0007 \t\u0016\u001f\u0016\u001c\u0007 5,\n\u001f\u0012\t\u000b\u0007 \n\u0012\u0013\u0007 \u001e\u0003\u0006\u0002\t\u000b\u001c\u00079:.\u001866\u0016\u0007 \n*\u0002\t\u0007!\t\n\u0006#\u000b\t \r\b\t\u0013\u0014\u0004 \u0007\"\n\u0006\u0007\t \u0005\u000b\t\u0006\u0006\t\u0013\u0007\u0003\u0012\u0007\u000e \t\n\u0010\u0006\u0007\u0005\t\u000b\u0007!\u0003\u0012#\u0010\t\u00075\u000e\u0005!6\u0007 \n\n\u0012\u0013\u0007\f\u000f!\u0005#\u0010\t\u0013\u0007#\u0006\u0003\u0012\u001f\u0007\u0010\u0002\t\u0007\n\u0014\t\u000b\n\u001f\t\u0007\u0003\u0012\u0010\t\u000b -\u000e\t\n\u0010\u0007\u0005\t\u000b\u0003\u000f\u0013\u0007!\t\n\u0006#\u000b\t\u0013\u0016\u0007 \n\u001b\u0004\u0004\u0007\n\u0004\u0003\u001f\u0012!\t\u0012\u0010\u0006\u0007\u0012\t\t\u0013\t\u0013\u0007\u0015\u000f\u000b\u0007\u0010\u0002\t\u0007!\t\n\u0006#\u000b\t!\t\u0012\u0010\u0006\u0007\"\t\u000b\t\u0007\f\u0002\t\f+\t\u0013\u0007 \n\n\u0012\u0013\u0007\f\u000f\u000b\u000b\t\f\u0010\t\u0013\u001c\u0007\u0003\u0015\u0007\u0012\t\f\t\u0006\u0006\n\u000b%\u0016\u0007\u001e\u000f\u000b\u0007\u0010\u0002\t\u0007\n\u0012\n\u0004%\u0006\t\u0006\u001c\u0007\n\u0004\u0004\u0007\u0005\u000b\u000f\u0005\u000f\u000b\u0010\u0003\u000f\u0012\n\u0004\u0007 \u0013\n\u0010\n\u00075\u0003\u0016\t\u0016\u001c\u0007 \u000b\u0004\u000e\b\u0004\u0005\u0006 \u0007\n\u0012\u0013\u0007 \u0007\u000e\b\t\u0006\u0003\u0002\u0015 6\u0007\"\t\u000b\t\u0007 \u0015\u0004\u000f\u0007\b -\u0010\u000b\n\u0012\u0006\u0015\u000f\u000b! \t\u0013\u0016 \n \u001e$ %\u0003\u0006\u0015\u000e\u0005\u0006 \n\u001b\u0004\u0004\u0007\u000b\t\u0005\u000f\u000b\u0010\t\u0013\u0007!#\u0004\u0010\u0003\u0005\u0004\t\u0007\n\u0012\n\u0004%\u0006\t\u0006\u0007\u000f\u0015\u0007\u0014\n\u000b\u0003\n\u0012\f\t\u00075@\u001b\u001dEJ\u001b6\u0007\"\u0003\u0010\u0002\u0007 \n\u000b\t\u0005\t\n\u0010\t\u0013\u0007 !\t\n\u0006#\u000b\t\u0006\u0007 \"\t\u000b\t\u0007 \f\u000f\u0012\u0013#\f\u0010\t\u0013\u0007 \"\u0003\u0010\u0002\u0007 \u0010\u0002\t\u0007 \"\u0003\u0010\u0002\u0003\u0012 -\u0006#\u000e>\t\f\u0010\u0007 \n\u0014\n\u000b\u0003\n\u000e\u0004\t\u0006\u0007 \u0012\u0004\u000e\u000f\r\u0001\u0002\u0013\u0007\u0015\u0007\u0002\u0006\u0007\b\u0016 \u00075\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0014\u0006\u0016\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b6\u001c\u0007 \u0012\u0004\u000e\u000f \u00075A6\u0007 \n\u0012\u0013\u0007 \b\u0006\u0007\u0002\u0015\r 5B6\u001c\u0007#\u0012\u0004\t\u0006\u0006\u0007\u0006\u0010\n\u0010\t\u0013\u0007\u000f\u0010\u0002\t\u000b\"\u0003\u0006\t\u0016\u0007$\t\u0010\"\t\t\u0012 -\u0006#\u000e>\t\f\u0010\u0007\u0014\n\u000b\u0003\n\u000e\u0004\t \n\"\n\u0006\u0007 \u0012\u0007\u000e\u000f\u0007\u000e\u000f\r\b\u0006\u0002\u0007\u000e\u0007\u000e\u000f \u00075\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0014\u0006\u0016\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u00136\u0016 \n\b\t\f\n\u0004\u0004\u0007\u0010\u0002\n\u0010\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0006\u0007\u0002\n\u0013\u0007\u0010\u000f\u0007\u0006\u0003\u0012\u001f\u0007\u0010\"\u000f\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\n\u0012\u0013\u0007\u0010\"\u000f\u0007\u0004\t\u0006\u0006\u0007 \n\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\n\u0012\u0013\u0007\u0010\u0002\n\u0010\u001c\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u00079\u0007\n\u0012\u0013\u0007A\u001c\u0007\u0010\u0002\t%\u0007\u0002\n\u0013\u0007\u0010\u000f\u0007\u0013\u000f\u0007\u0010\u0002\u0003\u0006\u0007 \u0015\u000b\u000f!\u0007!\t!\u000f\u000b%\u0007\"\u0003\u0010\u0002\u000f#\u0010\u0007\n\u0012\u0007\t \u0010\t\u000b\u0012\n\u0004\u0007\u000b\t\u0015\t\u000b\t\u0012\f\t\u0016\u0007*\u000b\u0003\n\u0004\u0007A\u0007\"\n\u0006\u0007!\t\n\u0012\u0010\u0007 \u0010\u000f\u0007\n\u0006\u0006\t\u0006\u0006\u0007\n\u0012%\u0007\f\u0002\n\u0012\u001f\t\u0007\u0003 \u0012\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u000e%\u0007\u0006\u000f\u0004\t\u0004%\u0007\u000b\t\u0015\u0004\t\f\u0010\u0003\u0012\u001f\u0007\u000f\u0012\u0007\u0010\u0002\t\u0007 \n\u0005\t\u000b\u0015\u000f\u000b!\n\u0012\f\t\u0007 \u000f\u0012\u0007 \u0010\u000b\u0003\n\u0004\u0007 97\u0007 \u0010\u0002\t\u0007 \t \u0005\t\u000b\u0003!\t\u0012\u0010\u0007 \u0013\u0003\u0013\u0007 \u0012\u000f\u0010\u0007 \u000b\t\u0014\t\n\u0004\u0007 \n\u0012%\u0007 \t\u0015\u0015\t\f\u0010\u0006\u0007\u000f\u0015\u0007\u0010\u0002\u0003\u0006\u0007\u0006\u000f\u000b\u0010\u0016\u0007\u0007C#\u0006\u0010\u0007\u000e\t\u0015\u000f\u000b\t\u0007\u0010\u000b\u0003\n\u0004\u0007B\u001c\u0007\u0010\u0002\t%\u0007\u0002\n\u0013\u0007\u0004\u0003\u0006\u0010\t\u0012\t\u0013\u0007\u0010\u000f\u0007\u0010\u0002\t\u0007 \n\u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007\u0006\u000f\u0012\u001f\u0007\u000f\u0012\u0007&'\u0016\u0007 \n\u001b\u0010\u0007\u000f\u0012\t\u0007\u0005\n\u000b\u0010\u0003\f#\u0004\n\u000b\u0007\u000f\f\f\n\u0006\u0003\u000f\u0012\u001c\u0007\u000f\u0012\t\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0007\"\n\u0006\u00071\u0010\u000f\u0010\n\u0004\u0004%\u0007\u0004\u000f\u0006\u00102\u0007\u0003\u0012\u0007 \n\u0010\u0002\t \u0007!#\u0006\u0003\f\u0007\"\u0002\u0003\u0004\t\u0007\u0004\u0003\u0006\u0010\t\u0012\u0003\u0012\u001f\u0007\u0010\u000f\u0007\u0003\u0010\u0007\n\u0012\u0013\u0007\u0015\u000f#\u0012\u0013\u0007\u0002\u0003!\u0006\t\u0004\u0015\u0007\u0012\u000f\u0010\u0007\n\u000e\u0004\t\u0007\u0010\u000f\u0007 \n\u000b\t\u0005\u000b\u000f\u0013#\f\t\u0007 \n\u0007 !\t\u0004\u000f\u0013%\u0007 \u000f\u0012\u0007 \u0010\u0002\t\u0007 \u0010\u0002\u0003\u000b\u0013\u0007 \u0010\u000b\u0003\n\u0004\u0016\u0007 \u001e\u000f\u000b\u0007 \u0010\u0002\t\u0007 \u000b\u0004\u000e\b\u0004\u0005\u0006 \u001c\u0007 \n\u0007\u000e\b\t\u0006\u0003\u0002\u0015\u0017\r\b\t\u0013\u0014\u0004 \u0007\n\u0012\u0013\u0007 \b\u0007\u0013\u0007\u000e\u000f \u0007!\t\n\u0006#\u000b\t\u0006\u001c\u0007\u0010\u0002\u0003\u0006\u0007!\u0003\u0006\u0006\u0003\u0012\u001f\u0007\u0013\n\u0010\n\u0007\"\n\u0006\u0007 \n\u000b\t\u0005\u0004\n\f\t\u0013\u0007\u000e%\u0007\u0010\u0002\t\u0007!\t\n\u0012\u0007\u0014\n\u0004#\t\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0015\u0003\u000b\u0006\u0010\u0007\u0010\"\u000f\u0007\u0010\u000b\u0003\n\u0004\u0006\u0016\u0007\u001e\u000f\u000b\u0007\u0010\u0002\t\u0007 \n\b\u0005\u000e\u0007\u000e\u000f \u0007!\t\n\u0006#\u000b\t\u001c\u0007\u0003\u0010\u0007\"\n\u0006 \u0007\u000b\t\u0005\u0004\n\f\t\u0013\u0007\u000e%\u0007\u0010\u0002\t\u0007!\n \u0003!#!\u0007\u0006\t!\u0003\u0010\u000f\u0012\t\u0007 \n\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u00075\u001a\u0007\u0006\u0010\u00166\u0016 \n\u0018\u0010\u0019\u0010\u001a \u001b\u0005\u000e\u0007\u000e\u000f \n)\u0012\u0007\u001e\u0003\u001f#\u000b\t\u0007A\u001c\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u00062\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0007\u0015\u000b\u000f!\u0007\u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0005\u0003\u0010\f\u0002\u0007\u000f\u0015\u0007 \n\u0010\u0002\t\u0007\u0015\u0003\u000b\u0006\u0010\u0007\u0010\u000f\u0012\t\u0007\u0003\u0006\u0007\u0006\u0002\u000f\"\u0012\u0007\u0015\u000f\u000b\u0007\n\u0004\u0004\u0007\u0010\u0002\u000b\t\t\u0007\u0010\u000b\u0003\n\u0004\u0006 \nB\u0016\u0007)\u0015\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0006\u0007\f\u000f#\u0004\u0013\u0007 \n\u0012\u000f\u0010\u0007\u000b\t!\t!\u000e\t\u000b\u001c\u0007\u0005\u000b\u000f\u0013#\f\t\u0007\u000f\u000b\u0007\n\u0005\u0005\u000b\u000f\n\f\u0002\u0007\u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0005\u0003\u0010\f\u0002\u0007\n\u0010\u0007\n\u0004\u0004\u001c\u0007\u0003\u0010\u0007 \n\"\u000f#\u0004\u0013\u0007\u000e\t\u0007\u000b\t\n\u0006\u000f\u0012\n\u000e\u0004%\u0007\u0010\u000f\u0007\t \u0005\t\f\u0010\u0007\u0010\u0002\n\u0010\u0007\u0010\u0002\t\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0006\u0007\"\t\u000b\t\u0007\t\u0014\t\u0012\u0004%\u0007 \n\u0013\u0003\u0006\u0010\u000b\u0003\u000e#\u0010\t\u0013\u0007 \n\u0010\u0007 \t\n\f\u0002\u0007 \u0006\u0003\u0013\t\u0007 \u000f\u0015\u0007 \u0010\u0002\t\u0007 \f\u000f\u000b\u000b\t\f\u0010\u0007 \u0005\u0003\u0010\f\u0002\u0016\u0007 *\u0002\t\u000b\t\u0015\u000f\u000b\t\u001c\u0007 \n\u0007 F\u000f\u0004!\u000f\u001f\u000f\u000b\u000f\u0014 -D!\u0003\u000b\u0012\u000f\u0014\u0007 \u000f\u0012\t -\u0006\n!\u0005\u0004\t\u0007 \u0010\t\u0006\u0010\u0007 \"\n\u0006\u0007 \u0005\t\u000b\u0015\u000f\u000b!\t\u0013\u0007 \u0010\u000f\u0007 \n\f\u000f!\u0005\n\u000b\t\u0007\u0010\u0002\t\u0007\u000f\u000e\u0006\t\u000b\u0014\t\u0013\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0006\u0007\u0015\u000b\u000f!\u0007\u0010\u0002\t\u0007\u0006\t!\u0003\u0010\u000f\u0012\t\u0007\u0006\f\n\u0004\t\u0007\"\u0003\u0010\u0002\u0007 \n\u0007#\u0012\u0003\u0015\u000f\u000b!\u0007\u0013\u0003\u0006\u0010\u000b\u0003\u000e#\u0010\u0003\u000f\u0012 \u0007\u000f\u0015\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0006\u0016\u0007\u001e\u000f\u000b\u0007\u0010\u000b\u0003\n\u0004\u0006\u00079\u0007\n\u0012\u0013\u0007A\u001c\u0007\"\t\u0007 \n\f\u000f#\u0004\u0013\u0007 \u0012\u000f\u0010\u0007 \u000b\t>\t\f\u0010\u0007 \u0010\u0002\t\u0007 \u0012#\u0004\u0004\u0007 \u0002%\u0005\u000f\u0010\u0002\t\u0006\u0003\u0006\u0007 \u0015\u000f\u000b\u0007 \u000e\u000f\u0010\u0002\u0007 \u0010\u000b\n\u0003\u0012\t\u0013\u0007 \n\u0012\u0013\u0007 \n#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0010\u0002\n\u0010\u0007\u0010\u0002\t\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0006\u0007\n\u000b\t\u0007\u0010\u0002\u000f#\u001f\u0002\u0010\u0007\u0010\u000f\u0007\u0002\n\u0014\t\u0007\u000e\t\t\u0012\u0007 \u0013\u000b\n\"\u0012\u0007\u0015\u000b\u000f!\u0007\n\u0007#\u0012\u0003\u0015\u000f\u000b!\u0007\u0013\u0003\u0006\u0010\u000b\u0003\u000e#\u0010\u0003\u000f\u0012\u00075\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u00079\u001c\u0007K\u0007?\u00079\u0016/\u001a\u001c\u0007\u0005\u0007?\u0007 /\u0016A9\u00075\u0010\u000b\n\u0003\u0012\t\u00136\u001c\u0007K\u0007?\u0007/\u0016\u001a:\u001c\u0007\u0005\u0007?\u0007/\u0016.B\u0007 5#\u0012\u0010\u000b\n\u0003\u0012\t\u001367\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u0007A\u001c\u0007K\u0007?\u0007 \n/\u0016.\u0018\u001c\u0007\u0005\u0007?\u0007/\u0016\u001a\u0019\u00075\u0010\u000b\n\u0003\u0012\t\u00136\u001c\u0007K\u0007?\u0007/\u0016;.\u001c\u0007\u0005\u0007?\u0007/\u0016\u0018;\u00075#\u0012\u0010\u000b\n\u0003\u0012\t\u001366\u0016\u0007\u001e\u000f\u000b\u0007 \u0010\u000b\u0003\n\u0004\u0007 B\u001c\u0007 \u0010\u0002\t\u0007 \u0002%\u0005\u000f\u0010\u0002\t\u0006\u0003\u0006\u0007 \u0015\u000f\u000b\u0007 #\u0012\u0003\u0015\u000f\u000b!\u0003\u0010%\u0007 \"\n\u0006\u0007 \u000b\t>\t\f\u0010\t\u0013\u0007 \u0015\u000f\u000b\u0007 \u0010\u0002\t\u0007 \n\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u001c\u0007\u0012\u000f\u0010\u0007\u0015\u000f\u000b\u0007\u0010\u0002\t\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u000f\u0012\t\u0006\u00075K\u0007?\u00079\u0016.9\u001c\u0007\u0005\u0007L\u0007/\u0016/9\u0007 \n5\u0010\u000b\n\u0003\u0012\t\u00136\u001c\u0007K\u0007?\u0007/\u0016;;\u001c\u0007\u0005\u0007?\u0007/\u0016\u0018B\u00075#\u0012\u0010\u000b\n\u0003\u0012\t\u0013 66\u0016\u0007 \n\u001b\u0007&\u000f\f\u0002\u000b\n\u0012\u0007I\u0007\u0010\t\u0006\u0010\u0007\"\n\u0006\u0007\f\u000f\u0012\u0013#\f\u0010\t\u0013\u0007\u0010\u000f\u0007\n\u0006\u0006\t\u0006\u0006\u0007\"\u0002\t\u0010\u0002\t\u000b\u0007\u000f\u000b\u0007\u0012\u000f\u0010\u0007\u0010\u0002\t\u0007 \n\u0006\u0010\n\u000b\u0010\u0003\u0012\u001f\u0007\u0010\u000f\u0012\t\u0007\u000f\u0015\u0007\n\u0007\u0005\n\u000b\u0010\u0003\f#\u0004\n\u000b\u0007\u0006\u000f\u0012\u001f\u0007\f\u0002\n\u0012\u001f\t\u0013\u0007\u0003\u0012\u0007\u0002\n\u0014\u0003\u0012\u001f\u0007\u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007 \u0005\u0003\u0010\f\u0002\u0007\n\f\u000b\u000f\u0006\u0006\u0007\u0010\u000b\u0003\n\u0004\u0006\u0016\u0007\u001e\u000f\u000b\u0007\u0010\u0002\t\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u001c\u0007\u0006\u000f\u0012\u001f\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0006\u0007 \f\u0002\n\u0012\u001f\t\u0013\u0007\u0006\u0003\u001f\u0012\u0003\u0015\u0003\f\n\u0012\u0010\u0004%\u0007\u0003\u0012\u0007\u0002\n\u0014\u0003\u0012\u001f\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0010\u000f\u0012\t\u0006\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u0006\u00079\u001c\u0007A\u0007 \n\u0012\u0013\u0007 \nB\u00075I\u0007?\u00079/\u0016\u0019B\u001c\u0007\u0005\u0007L\u0007/\u0016/96\u0016\u0007E\u0012\u0007\u0010\u000b\u0003\n\u0004\u00079\u0007\n\u0012\u0013\u0007A\u001c\u0007\u0010\u0002\t\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007 \n!\n\u0013\t\u0007\u0012\u000f\u0007\t\u000b\u000b\u000f\u000b\u0007\u000f\u0012\u0007\u0019\u0007\u000f#\u0010\u0007\u000f\u0015\u0007\n\u0007\u0010\u000f\u0010\n\u0004\u0007\u000f\u0015\u0007BA\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0006\u000759\u001a06\u0016\u0007 E\u0012\u0007\u0010\u000b\u0003\n\u0004\u0007B\u001c\u0007\u0010\u0002\t%\u0007 !\n\u0013\t\u0007\u0012\u000f\u0007\t\u000b\u000b\u000f\u000b\u0007\n\u0010\u00079\u0019\u0007\u000f#\u0010\u0007\u000f\u0015\u0007\n\u0007\u0010\u000f\u0010\n\u0004\u0007\u000f\u0015\u0007 BA\u0007 \u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0006\u00075\u0018.067\u0007A9\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0006\u00075\u001a\u001a06\u0007\"\t\u000b\t\u0007\"\u0003\u0010\u0002\u0003\u0012\u00079\u0007 \u0006\t!\u0003\u0010\u000f\u0012\t\u0016\u0007\u001b\u0004 \u0006\u000f\u0007\u0015\u000f\u000b\u0007\u0010\u0002\t\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u001c\u0007\u0006\u000f\u0012\u001f\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0006\u0007 \n\f\u0002\n\u0012\u001f\t\u0013\u0007\u0006\u0003\u001f\u0012\u0003\u0015\u0003\f\n\u0012\u0010\u0004%\u0007\u0003\u0012\u0007\u0002\n\u0014\u0003\u0012\u001f\u0007\u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0005\u0003\u0010\f\u0002\u0007#\u0012\u0013\t\u000b\u0007\u0010\u0002\t\u0007 \n\u0010\u0002\u000b\t\t\u0007 \u0010\u000b\u0003\n\u0004\u0006\u0007 5I\u0007 ?\u0007 .\u00169.\u001c\u0007 \u0005\u0007 L\u0007 /\u0016/\u00196\u0016\u0007 E\u0012\u0007 \u0010\u000b\u0003\n\u0004\u0006\u0007 9\u0007 \n\u0012\u0013\u0007 A\u001c\u0007 \u0010\u0002\t\u0007 #\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007!\n\u0013\t\u0007\u0012\u000f\u0007\t\u000b\u000b\u000f\u000b\u0007\u0015\u000f\u000b\u0007B\u00075;06\u0007\n\u0012\u0013\u0007A\u00075\u001906\u0007\u000f#\u0010\u0007\u000f\u0015\u0007\n\u0007 \u0010\u000f\u0010\n\u0004\u0007\u000f\u0015\u0007\u0018/\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0006\u001c \u0007\u000b\t\u0006\u0005\t\f\u0010\u0003\u0014\t\u0004%\u0016\u0007E\u0012\u0007\u0010\u000b\u0003\n\u0004\u0007B\u001c\u0007\u0010\u0002\t%\u0007\u0010#\u0012\t\u0013\u0007 \n\n\f\f#\u000b\n\u0010\t\u0004%\u0007 \n\u0010\u0007 :\u0007 \u000f#\u0010\u0007 \u000f\u0015\u0007 \u0018/\u0007 \u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0006\u0007 5AB06\u0007 \n\u0012\u0013\u0007 9\u001a\u0007 \u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0006\u00075\u0018/06\u0007\"\t\u000b\t\u0007\"\u0003\u0010\u0002\u0003\u0012\u00079\u0007\u0006\t!\u0003\u0010\u000f\u0012\t\u0016 \n\u001b\u0007 \u001e\u0003\u0006\u0002\t\u000b\u0007 \t \n\f\u0010\u0007 \u0010\t\u0006\u0010\u0007 \u000b\t\u0014\t\n\u0004\t\u0013\u0007 \u0010\u0002\n\u0010\u0007 \u0010\u0002\t\u0007 \u0012#!\u000e\t\u000b\u0007 \u000f\u0015\u0007 \u0006\u000f\u0012\u001f\u0007 \n\u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0006\u0007 \u0006\u0010\n\u000b\u0010\u0003\u0012\u001f\u0007 \n\u0010\u0007 \u0010\u0002\t\u0007 \f\u000f\u000b\u000b\t\f\u0010\u0007 \u0005\u0003\u0010\f\u0002\u0007 \u000f\u0012\u0007 \u0010\u000b\u0003\n\u0004\u0007 B\u0007 \"\n\u0006\u0007 \u0006\u0003\u001f\u0012\u0003\u0015\u0003\f\n \u0012\u0010\u0004%\u0007 \u0002\u0003\u001f\u0002\t\u000b\u0007 \u0015\u000f\u000b\u0007 \u0010\u000b\n\u0003\u0012\t\u0013\u0007 \u0006\u0003\u0012\u001f\t\u000b\u0006\u0007 \u0010\u0002\n\u0012\u0007 \u0015\u000f\u000b\u0007 #\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007 \n\u0006\u0003\u0012\u001f\t\u000b\u0006\u00075\u0005\u0007?\u0007/\u0016/A.6\u0016\u0007\u001b\u0004\u0006\u000f\u0007\"\u0002\t\u0012\u0007\n\u0004\u0004\u000f\"\u0003\u0012\u001f\u0007\n\u0007\u0005\u0003\u0010\f\u0002\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u00079\u0007 \u0006\t!\u0003\u0010\u000f\u0012\t\u001c\u0007 \u0010\u0002\t\u000b\t\u0007 \"\n\u0006\u0007 \n\u0007 \u0006\u0003\u001f\u0012\u0003\u0015\u0003\f\n\u0012\u0010\u0007 \u0013\u0003\u0015\u0015\t\u000b\t\u0012\f\t\u0007 \u000e\t\u0010\"\t\t\u0012\u0007 \u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0006\u0007\u000f\u0015\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\n\u0012\u0013\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u00075\u0005\u0007?\u0007/\u0016//:6\u0016 \nB\u0007\u001e\u000f\u000b\u0007\u0010\u0002\t\u0007\u0015\u0003\u000b\u0006\u0010\u0007\u0010\u0002\u000b\t\t\u0007\u0006#\f\f\t\u0006\u0006\u0003\u0014\t\u0004%\u0007\u0005\u000b\u000f\u0013#\f\t\u0013\u0007\u0010\u000f\u0012\t\u0006\u001c\u0007\u0003\u0010\u0007\"\n\u0006\u0007\u0015\u000f#\u0012\u0013\u0007\u0010\u0002\n\u0010\u0007\u0006\t!\u0003\u0010\u000f\u0012\t\u0007 \n\u0013\t\u0014\u0003\n\u0010\u0003\u000f \u0012\u0007\u0013\u0003\u0013\u0007\u0012\u000f\u0010\u0007\u0013\t\u0005\t\u0012\u0013\u0007\u000f\u0012\u0007\u0010\u000f\u0012\t\u0007\u0005\u000f\u0006\u0003\u0010\u0003\u000f\u0012\u0007\n\u0012\u0013\u0007\u0010\u0002\t\u0007\n\u0014\t\u000b\n\u001f\t\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0007\u0013\u0003\u0013\u0007\u0012\u000f\u0010\u0007 \n\u0013\u0003\u0015\u0015\t\u000b\u0007\u0006\u0003\u001f\u0012\u0003\u0015\u0003\f\n\u0012\u0010\u0004%\u0007\u0015\u000b\u000f!\u0007\u0010\u0002\t\u0007\u0003\u0012\u0013\u0003\u0014\u0003\u0013#\n\u0004\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0006\u0016\u0007&\u000f\u0012\u0006\t(#\t\u0012\u0010\u0004%\u001c\u0007\n\u0012\n\u0004%\u0006\u0003\u0006\u0007\"\n\u0006\u0007 \u0006\u0003!\u0005\u0004\u0003\u0015\u0003\t\u0013\u0007\u000e%\u0007\u0005\u000b\u000f\u0014\u0003\u0013\u0003\u0012\u001f\u0007\u000f\u0012\u0004%\u0007\n\u0012\n\u0004%\u0006\t\u0006\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u00102\u0006\u0007\u0015\u0003\u000b\u0006\u0010\u0007\u0005\u000b\u000f\u0013#\f\t\u0013\u0007\u0010\u000f\u0012\t\u0006\u0016 \n\u0007\u001e\u0003\u001f#\u000b\t\u0007A<\u0007*#\u0012\u0003\u0012\u001f\u0016\u0007E\u000e\u0006\t\u000b\u0014\t\u0013 \u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0007\u0015\u000b\u000f!\u0007\u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0005\u0003\u0010\f\u0002\u0007 \n\u0003\u0012\u0007\u0006\t!\u0003\u0010\u000f\u0012\t\u0006\u0007\u000f\u0015\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\n\u0012\u0013\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u0010\u0002\t\u0003\u000b\u0007\u0015\u0003\u000b\u0006\u0010\u0007 \n\u0012\u000f\u0010\t\u0007\u000f\u0015\u0007\n\u0007$\t\n\u0010\u0004\t\u0006\u0007!\t\u0004\u000f\u0013%\u0007\u000f\u0012\u0007\u0010\u0002\u000b\t\t\u0007\u0010\u000b\u0003\n\u0004\u0006\u0016\u0007\u0007*\u0002\t\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0007\"\n\u0006\u0007 \n\f\u000f\u000b\u000b\t\f\u0010\t\u0013\u0007\u0015\u000f\u000b\u0007\u000f\f\u0010\n\u0014\t\u0016\u0007\u001d\u000f\u0010\t\u0007\u0010\u0002\n\u0010\u0007\n\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u0007\u001a\u0007\u0006\t!\u0003\u0010\u000f\u0012\t\u0006\u0007 \n%\u0003\t\u0004\u0013\u0006\u0007\u0010\u0002\t\u0007\u0006\n!\t\u0007\u0005\u0003\u0010\f\u0002\u0007\f\u0004\n\u0006\u0006\u0007\n\u0006\u0007\n\u0007\u0013\t\u0014 \u0003\n\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u0007 M\u001a\u0007\u0006\t!\u0003\u0010\u000f\u0012\t\u0006\u0016\u0007 \nE\u000e\u0006\t\u000b\u0014\n\u0010\u0003\u000f\u0012\u0006\u0007\"\t\u000b\t\u0007\t(#\n\u0004\u0004%\u0007\u0013\u0003\u0006\u0010\u000b\u0003\u000e#\u0010\t\u0013\u0007\n!\u000f\u0012\u001f\u0006\u0010\u0007\u0010\u0002\t\u0006\t\u0007\u0010\"\u000f\u0007 \n\f\n\u0010\t\u001f\u000f\u000b\u0003\t\u0006\u0016\u0007 \n)\u0012\u0007\u0006#!!\n\u000b%\u001c\u0007\u000e\u000f\u0010\u0002\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\n\u0012\u0013\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\n\u0010\u0010\t!\u0005\u0010\t\u0013\u0007\u0010\u000f\u0007 \n\n\u0013\u000f\u0005\u0010\u0007 \u0010\u0002\t\u0007 \f\u000f\u000b\u000b\t\f\u0010\u0007 \u0005\u0003\u0010\f\u0002\u0007 \n\u0015\u0010\t\u000b\u0007 \u0002\n\u0014\u0003\u0012\u001f\u0007 \u0002\t\n\u000b\u0013\u0007 \u0010\u0002\t\u0007 \u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007 \u000b\t\f\u000f\u000b\u0013\u0003\u0012\u001f\u00075\u0003\u0016\t\u0016\u001c\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u0007B67\u0007\u0010\u0002\t\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t \u000b\u0006\u0007\"\t\u000b\t\u0007\u000e\t\u0010\u0010\t\u000b\u0007\n\u0010\u0007 \n\u0013\u000f\u0003\u0012\u001f\u0007\u0010\u0002\u0003\u0006\u0016 \n\u001b\u0004\u0006\u000f\u001c\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u00062\u0007\u0010#\u0012\u0003\u0012\u001f\u0007\u0005\t\u000b\u0015\u000f\u000b!\n\u0012\f\t\u0007\u0015\u000f\u000b\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\n\u0012\u0013\u0007\u0004\t\u0006\u0006\u0007 \n\u0015\n!\u0003\u0004\u0003\n\u000b\u0007 \u0006\u000f\u0012\u001f\u0006\u0007 \u0002\n\u0006\u0007 \u000e\t\t\u0012\u0007 \n\u0012\n\u0004%4\t\u0013\u0016\u0007 \u001e\u000f\u000b\u0007 \u0010\u0002\t\u0007 \u0015\n!\u0003\u0004\u0003\n\u000b\u0007 \u0010#\u0012\t\u0006\u001c\u0007 F\u000f\u0004!\u000f\u001f\u000f\u000b\u000f\u0014 -D!\u0003\u000b\u0012\u000f\u0014\u0007 \u000f\u0012\t -\u0006\n!\u0005\u0004\t\u0007 \u0010\t\u0006\u0010\u0006\u0007 \u0015\u000f#\u0012\u0013\u0007 \u000f#\u0010\u0007 \u0010\u0002\n\u0010\u0007 \u0010\u0002\t\u0007 \n\u000f\u000e\u0006\t\u000b\u0014\t\u0013\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0006\u0007\u0013\u0003\u0015\u0015\t\u000b\t\u0013\u0007\u0015\u000b\u000f!\u0007\u000e\t\u0003\u0012\u001f\u0007\u0013\u000b\n\"\u0012\u0007\u0015\u000b\u000f!\u0007\n \u0007#\u0012\u0003\u0015\u000f\u000b!\u0007 \n\u0013\u0003\u0006\u0010\u000b\u0003\u000e#\u0010\u0003\u000f\u0012\u0007\u0015\u000f\u000b\u0007\n\u0004\u0004\u0007\u0010\u0002\u000b\t\t\u0007\u0010\u000b\u0003\n\u0004\u0006\u00075\u001e\u000f\u000b\u0007\u0010\u000b\u0003\n\u0004\u00079\u001c\u0007K\u0007?\u00079\u0016\u00189\u001c\u0007\u0005\u0007L\u0007/\u0016/\u00197\u0007 \u0015\u000f\u000b\u0007\u0010\u000b\u0003\n\u0004\u0007A\u001c\u0007K\u0007?\u00079\u0016./\u001c\u0007\u0005\u0007L\u0007/\u0016/97\u0007\u0015\u000f\u000b\u0007\u0010\u000b\u0003\n\u0004\u0007B\u001c\u0007K\u0007?\u00079\u0016\u0019\u001a\u001c\u0007\u0005\u0007L\u0007/\u0016/\u00196\u0016\u0007 \n\u0017\u000f\"\t\u0014\t\u000b\u001c\u0007\u000f\u0012\u0004%\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u0007B\u001c\u0007\u0010\u0002\t\u0007\u0013\n\u0010\n\u0007\u0005\t\n+\u0006\u0007\n\u0010\u00074\t\u000b\u000f\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0007 \n5!\n\u0010\f\u0002\u0003\u0012\u001f\u0007\u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0005\u0003\u0010\f\u00026\u0016\u0007E\u0012\u0007\u0010\u000b\u0003\n\u0004\u00079\u0007\n\u0012\u0013\u0007A\u001c\u0007\u0005\t\n+\u0006 \u0007\u000f\f\f#\u000b\u000b\t\u0013\u0007 \n\n\u0010\u0007\u000f\u0010\u0002\t\u000b\u0007\u0005\u0004\n\f\t\u0006\u0016\u0007\u001e\u000f\u000b\u0007\u0010\u0002\t\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0010#\u0012\t\u0006\u001c\u0007\u000f\u000e\u0006\t\u000b\u0014\t\u0013\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0006\u0007 \"\t\u000b\t\u0007\u0004\u0003+\t\u0004%\u0007\u0010\u000f\u0007\u000e\t\u0007\u0013\u000b\n\"\u0012\u0007\u0015\u000b\u000f!\u0007\n\u0007#\u0012\u0003\u0015\u000f\u000b!\u0007\u0013\u0003\u0006\u0010\u000b\u0003\u000e#\u0010\u0003\u000f\u0012\u00075\u0015\u000f\u000b\u0007\u0010\u000b\u0003\n\u0004\u0007 9\u001c\u0007K\u0007?\u00079\u0016B\u0019\u001c\u0007\u0005\u0007?\u0007/\u0016/\u0019B7\u0007\u0015\u000f\u000b\u0007\u0010\u000b\u0003\n\u0004\u0007A\u001c\u0007K\u0007?\u00079\u00169;\u001c\u0007\u0005\u0007?\u0007/\u00169A7\u0007\u0015\u000f\u000b\u0007\u0010\u000b\u0003\n\u0004\u0007 \nB\u001c\u0007K\u0007?\u00079\u0016//\u001c\u0007\u0005\u0007?\u0007/\u0016A.6\u0016 \n\u001b\u0007&\u000f\f\u0002\u000b\n\u0012\u0007I\u0007\u0010\t\u0006\u0010\u0007\u0015\u000f#\u0012\u0013\u0007\u0010\u0002\n \u0010\u0007\u0010\u0002\t\u0007\u0006\u000f\u0012\u001f\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0006\u0007\u0013\u0003\u0015\u0015\t\u000b\t\u0013\u0007 \n\u0006\u0003\u001f\u0012\u0003\u0015\u0003\f\n\u0012\u0010\u0004%\u0007\u0003\u0012\u0007\u0002\n\u0014\u0003\u0012\u001f\u0007\u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0005\u0003\u0010\f\u0002\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u0006\u00079\u001c\u0007A\u0007\n\u0012\u0013\u0007B\u00075I\u0007 ?\u00079:\u001699\u001c\u0007\u0005\u0007L\u0007/\u0016//96\u0016\u0007E\u0012\u0007\u0010\u000b\u0003\n\u0004\u00079\u0007\n\u0012\u0013\u0007A\u001c\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\"\t\u000b\t\u0007\"\u0003\u0010\u0002\u0007 \u0012\u000f\u0007\t\u000b\u000b\u000f\u000b\u0007\u0003\u0012\u0007B\u00075;06\u0007\n\u0012\u0013\u00079\u00075B06\u0007\f\n\u0006\t\u0006\u0007\u000f#\u0010\u0007\u000f\u0015\u0007\n\u0007\u0010\u000f\u0010\n\u0004\u0007\u000f\u0015\u0007B\u001a\u0007\f\n\u0006\t\u0006\u001c\u0007 \u000b\t\u0006\u0005\t\f\u0010\u0003\u0014\t\u0004%\u0016\u0007E\u0012\u0007\u0010\u000b\u0003\n\u0004\u0007B\u001c\u00079\u0019 \u0007\u000f#\u0010\u0007\u000f\u0015\u0007B\u001a\u00075\u0018A06\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007 \n\"\t\u000b\t\u0007 \u0006\u0010\n\u000b\u0010\t\u0013\u0007 \n\u0010\u0007 \u0010\u0002\t\u0007 \f\u000f\u000b\u000b\t\f\u0010\u0007 \u0005\u0003\u0010\f\u00027\u0007 AB\u0007 5\u001a\u001806\u0007 \"\t\u000b\t\u0007 \"\u0003\u0010\u0002\u0003\u0012\u0007 9\u0007 \n\u0006\t!\u0003\u0010\u000f\u0012\t\u0016\u0007 \u001e\u000f\u000b\u0007 \u0010\u0002\t\u0007 \u0004\t\u0006\u0006\u0007 \u0015\n!\u0003\u0004\u0003\n\u000b\u0007 \u0006\u000f\u0012\u001f\u0006\u001c\u0007 \u0010\u0002\t\u0007 \u0012#!\u000e\t\u000b\u0007 \u000f\u0015\u0007 \u0006\u000f\u0012\u001f\u0007 \u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0006\u0007\u0013\u0003\u0013\u0007\u0012\u000f\u0010\u0007\u0013\u0003\u0015\u0015\t\u000b\u0007 \u0003\u0012\u0007\u0002\n\u0014\u0003\u0012\u001f\u0007\u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0005\u0003\u0010\f\u0002\u0007 \u000f\u0012\u0007 \u0010\u000b\u0003\n\u0004\u0006\u00079\u001c\u0007A\u0007\n\u0012\u0013\u0007B\u00075I\u0007?\u0007A\u0016//\u001c\u0007\u0005\u0007?\u0007/\u0016B.6\u0016\u0007E\u0012\u0007\u0010\u000b\u0003\n\u0004 \u00079\u0007\n\u0012\u0013\u0007A\u001c\u0007\u0004\t\u0006\u0006\u0007 \n\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\"\t\u000b\t\u0007\u0006\u0010\n\u000b\u0010\t\u0013\u0007\"\u0003\u0010\u0002\u000f#\u0010\u0007\t\u000b\u000b\u000f\u000b\u0007\u0003\u0012\u0007\u0019\u000759\u001806\u0007\n\u0012\u0013\u0007\u001a\u0007 59.06\u0007\u000f#\u0010\u0007\u000f\u0015\u0007\n\u0007\u0010\u000f\u0010\n\u0004\u0007\u000f\u0015\u0007B\u001a\u0007\f\n\u0006\t\u0006\u001c\u0007\u000b\t\u0006\u0005\t\f\u0010\u0003\u0014\t\u0004%\u0016\u0007E\u0012\u0007\u0010\u000b\u0003\n\u0004\u0007B\u001c\u0007:\u0007 \n5A\u001906\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\"\t\u000b\t\u0007\"\u0003\u0010\u0002\u000f#\u0010\u0007\t\u000b\u000b\u000f\u000b7\u00079\u0018\u00075B:06\u0007\"\t\u000b\t\u0007 \n\"\u0003\u0010\u0002\u0003\u0012\u00079\u0007\u0006\t!\u0003\u0010\u000f\u0012\t\u0016\u0007 \n\u001b\u0007 \u001e\u0003\u0006\u0002\t\u000b\u0007 \t \n\f\u0010\u0007 \u0010\t\u0006\u0010\u0007 \u0015\u000f#\u0012\u0013\u0007 \u0012\u000f\u0007 \u0006\u0003\u001f\u0012\u0003\u0015\u0003\f\n\u0012\u0010 \u0007 \u0013\u0003\u0015\u0015\t\u000b\t\u0012\f\t\u0007 \u0003\u0012\u0007 \u0010\u0002\t\u0007 \n\u0012#!\u000e\t\u000b\u0007\u000f\u0015\u0007\f\u000f\u000b\u000b\t\f\u0010\u0004%\u0007\u0005\u0003\u0010\f\u0002\t\u0013\u0007\u0010\u000f\u0012\t\u0006\u0007\u000e\t\u0010\"\t\t\u0012\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\n\u0012\u0013\u0007\u0004\t\u0006\u0006\u0007 \u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u0007B\u00075\u0005\u0007?\u0007/\u0016996\u0016\u0007\u001e\u000f\u000b\u0007\u0005\u0003\u0010\f\u0002\u0007\t\u000b\u000b\u000f\u000b\u0006\u0007\"\u0003\u0010\u0002\u0003\u0012\u00079\u0007 \n\u0006\t!\u0003\u0010\u000f\u0012\t\u001c\u0007\u0010\u0002\t\u000b\t\u0007\"\n\u0006\u0007\n\u0007\u0006\u0003\u001f\u0012\u0003\u0015\u0003\f\n\u0012\u0010\u0007\u0013\u0003\u0015\u0015\t\u000b\t\u0012\f\t\u0007\u000e\t\u0010\"\t\t\u0012\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007 \n\n\u0012\u0013\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u0007B\u00075\u0005\u0007?\u0007/\u0016/A:6\u0016\u0007E\u0012\u0004 %\u0007\"\u0002\t\u0012\u0007 \n\f\u000f\u0012\u0006\u0003\u0013\t\u000b\u0003\u0012\u001f\u0007 \n\u0012\u0007 \n\u0004\u0004\u000f\"\n\u0012\f\t\u0007 \u000f\u0015\u0007 9\u0007 \u0006\t!\u0003\u0010\u000f\u0012\t\u0007 \u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u001c\u0007 !\u000f\u000b\t\u0007 \u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0006\u0007\u000f\u0015\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u00075AB\u0007\u000f#\u0010\u0007\u000f\u0015\u0007B\u001a6\u0007\"\t\u000b\t\u0007\u0006\u0010\n\u000b\u0010\t\u0013\u0007\n\u0010\u0007 \u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0005\u0003\u0010\f\u0002\u0007\u0010\u0002\n\u0012\u0007\u0010\u0002\t\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0006\u0007\u000f\u0015\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007 59\u0018\u0007\u000f#\u0010\u0007\u000f\u0015\u0007B\u001a6\u0016\u0007 \u0018\u0010\u0019\u0010\u0018 \u001c\u0004\u000e\b\u0004\u0005\u0006 \n*\u0002\t\u0007 \u000b\t\u0006#\u0004\u0010\u0006\u0007 \u000f\u0015\u0007 \u0010\u0002\t\u0007 \u0005\u000b\u000f\u0005\u000f\u000b\u0010\u0003\u000f\u0012\u0007 \u000f\u0015\u0007 \f\u000f\u000b\u000b\t\f\u0010\u0004% \u0007 \u0006#\u0012\u001f\u0007 \u0005\u0003\u0010\f\u0002\u0007 \n!\u000f\u0014\t!\t\u0012\u0010\u0006\u0007\u0015\u000f\u000b\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\n\u0012\u0013\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\n\f\u000b\u000f\u0006\u0006\u0007\u0010\u000b\u0003\n\u0004\u0006\u0007 \n\n\u000b\t\u0007\u0006\u0002\u000f\"\u0012\u0007\u0003\u0012\u0007\u001e\u0003\u001f#\u000b\t\u0007B\u0016 \n)\u0012\u0007\u0010\u0002\t\u0007!#\u0004\u0010\u0003\u0014\n\u000b\u0003\n\u0010\t\u0007\n\u0012\n\u0004%\u0006\u0003\u0006\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007 \u0015\u0004\u000f\u0007\b -\u0010\u000b\n\u0012\u0006\u0015\u000f\u000b!\t\u0013\u0007 \u000b\u0004\u000e\b\u0004\u0005\u0006 \n\u0013\n\u0010\n\u001c\u0007\n\u0007!\n\u0003\u0012\u0007\t\u0015\u0015\t\f\u0010\u0007\u0015\u000f\u000b\u0007 \b\u0006\u0007\u0002\u0015 \u0007\"\n\u0006\u0007\u0015\u000f#\u0012\u0013\u0007\u0010\u000f\u0007\u000e\t\u0007\u0012\u000f\u0010\u0007\u0006\u0003\u001f\u0012\u0003\u0015\u0003\f\n\u0012\u0010\u0007 \n5\u001e5A\u001c9\u00196\u0007?\u0007B\u0016\u0018/\u001c\u0007\u0005\u0007?\u0007/\u0016/\u001a96\u001c\u0007\"\u0002\u0003\u0004\t\u0007\u0010 \u0002\t\u0007#\u0012\u0003\u0014\n\u000b\u0003\n\u0010\t\u0007\u0010\t\u0006\u0010\u0007\u0015\u000f#\u0012\u0013\u0007\n\u0007 \n\u0006\u0003\u001f\u0012\u0003\u0015\u0003\f\n\u0012\u0010\u0007!\n\u0003\u0012\u0007\t\u0015\u0015\t\f\u0010\u0007\u0015\u000f\u000b\u0007 \b\u0006\u0007\u0002\u0015\r 5\u001e5A\u001cBA6\u0007?\u0007B\u0016.A\u001c\u0007\u0005\u0007L\u0007/\u0016/\u00196\u0016\u0007 \n\u001e#\u000b\u0010\u0002\t\u000b\u0007\n\u0012\n\u0004%\u0006\u0003\u0006\u0007\u0015\u000f#\u0012\u0013\u0007\u000f#\u0010\u0007\u0010\u0002\n\u0010\u0007!\u000f\u0006\u0010\u0007\u0014\n\u000b\u0003\n\u0012\f\t\u0007\"\n\u0006\u0007\t \u0005\u0004\n\u0003\u0012\t\u0013\u0007 \n\u000e%\u0007\u0010\u0002\t\u0007\u0013\u0003\u0015\u0015\t\u000b\t\u0012\f\t\u0007\u0003\u0012\u0007\f\u000f\u0012\u0010\u000f#\u000b\u0006\u0007\u0005\u000b\u000f\u0013#\f\t\u0013\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u0007B\u0007\n\u0012\u0013\u0007\f\u000f\u0012\u0010\u000f#\u000b\u0006\u0007 \u0005\u000b\u000f\u0013#\f\t\u0013\u0007 \u000f\u0012\u0007 \u0010\u000b\u0003\n\u0004\u0007 9\u0007 \n\u0012\u0013\u0007 A\u0007 \f\u000f!\u000e\u0003\u0012\t\u0013\u0007 5\u001e59\u001c9\u001a6\u0007 ?\u0007 B\u0016;:\u001c\u0007\u0005\u0007 ?\u0007 \n/\u0016/\u001a\u001a6\u0016\u0007E\u0012\u0007\u0010\u000b\u0003\n\u0004\u0007B\u001c\u0007\f\u000f\u0012\u0010\u000f#\u000b\u0006\u0007\u0010\t\u0012\u0013\t\u0013\u0007\u0010\u000f\u0007\f\u000f\u0012\u0010\n\u0003\u0012\u0007\u0005\u000b\u000f\u0005\u000f\u000b\u0010\u0003\u000f\u0012\n\u0004\u0004%\u0007 !\u000f\u000b\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0005\u0003\u0010\f\u0002\u0007!\u000f\u0014\t!\t\u0012\u0010\u0006\u0007\u0010\u0002\n\u0012\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u00079\u0007\n\u0012\u0013\u0007A\u00075!\t\n\u0012\u0007 \u0005\u000b\u000f\u0005\u000f\u000b\u0010\u0003\u000f\u0012\u0006<\u0007/\u0016.:\u001c\u0007/\u0016.;\u001c\u0007/\u0016;9\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u00079\u001c\u0007A\u001c\u0007\n\u0012\u0013\u0007B\u001c\u0007\u000b\t\u0006\u0005\t\f\u0010\u0003\u0014\t\u0004%6\u0016\u0007 \n\u001b\u0012\u0007\u0003\u0012\u0010\t\u000b\n\f\u0010\u0003\u000f\u0012\u0007\t\u0015\u0015\t\f\u0010\u0007\u0015\u000f\u000b\u0007 \u0012\u0004\u000e\u000f\r\u0001\u0002\u0013\u0007\u0015\u0007\u0002\u0006\u0007\b\u0016 \u0007\n\u0012\u0013\u0007 \b\u0006\u0007\u0002\u0015 \u0007\"\n\u0006\u0007 \u0015\u000f#\u0012\u0013\u0007 \n\u0010\u000f\u0007\u000e\t\u0007\u0006\u0003\u001f\u0012\u0003\u0015\u0003\f\n\u0012\u0010\u00075\u001e5A\u001c9\u00196\u0007?\u0007\u0018\u0016B\u0018\u001c\u0007\u0005\u0007L\u0007/\u0016/\u00196\u0016\u0007*\u0002\t\u0007\u0006#\u0012\u001f\u0007\f\u000f\u0012\u0010\u000f#\u000b\u0006\u0007 \u000f\u0015\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\f\u000f\u0012\u0010\n\u0003\u0012\t\u0013\u0007\u0005\u000b\u000f\u0005\u000f\u000b\u0010\u0003\u000f\u0012\n\u0004\u0004%\u0007!\u000f\u000b\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007 \u0005\u0003\u0010\f\u0002\u0007!\u000f\u0014\t!\t\u0012\u0010\u0006\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u0007B\u0007\n\u0006\u0007\f\u000f!\u0005\n\u000b\t\u0013\u0007\u0010\u000f\u0007\u0010\u000b\u0003\n\u0004\u00079\u0007\n\u0012\u0013\u0007A\u0007\u0010\u0002\n\u0012\u0007 \u0010\u0002\t\u0007\f\u000f\u0012\u0010\u000f#\u000b\u0006\u0007\u000f\u0015\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u00075\u001e59\u001c9\u001a6\u0007?\u0007.\u0016//\u001c\u0007\u0005\u0007L\u0007/\u0016/\u00196\u0016\u0007\u001b\u0006\u0007 \u0006\u0002 \u000f\"\u0012\u0007 \u0003\u0012\u0007 \u001e\u0003\u001f#\u000b\t\u0007 B\u001c\u0007 \u0010\u0002\t\u0007 \u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0007 \u000f\u0015\u0007 \f\u000f\u0012\u0010\u000f#\u000b\u0006\u0007 \u000f\u0015\u0007 \u0004\t\u0006\u0006\u0007 \n\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\u0003!\u0005\u000b\u000f\u0014\t\u0013\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u0007B\u0016\u0007)\u0012\u0007\f\u000f\u0012\u0010\u000b\n\u0006\u0010\u001c\u0007\u0010\u0002\t\u0007\f\u000f\u0012\u0010\u000f#\u000b\u0007 \n\u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\u0013\u0003\u0013\u0007\u0012\u000f\u0010\u0007\u0003!\u0005\u000b\u000f\u0014\t\u00075!\t\n\u0012\u0007 \u000b\u0004\u000e\b\u0004\u0005\u0006 \n\u0015\u000f\u000b\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\n\f\u000b\u000f\u0006\u0006\u0007\u0010\u000b\u0003\n\u0004\u0006<\u0007/\u0016;A\u001c\u0007/\u0016;A\u001c\u0007/\u0016;97\u0007!\t\n\u0012\u0007 \u000b\u0004\u000e\b\u0004\u0005\u0006 \n\u0015\u000f\u000b\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003 \n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\n\f\u000b\u000f\u0006\u0006\u0007\u0010\u000b\u0003\n\u0004\u0006<\u0007/\u0016..\u001c\u0007/\u0016.\u0018\u001c\u0007/\u0016;A6\u0016 \n\u0007\u001e\u0003\u001f#\u000b\t\u0007B<\u0007&\u000f\u0012\u0010\u000f#\u000b\u0016\u0007@\t\n\u0012\u0007\u0005\u000b\u000f\u0005\u000f\u000b\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u0007\f\u000f\u000b\u000b\t\f\u0010\u0004%\u0007\u0006#\u0012\u001f\u0007\u0005\u0003\u0010\f\u0002\u0007 \n!\u000f\u0014\t!\t\u0012\u0010\u0006\u0007\u000f\u0015\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\n\u0012\u0013\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\u000f\u0015\u00071*\u0002\t\u0007 \n$\t\n\u0010\u0004\t\u00062\u0007\n\f\u000b\u000f\u0006\u0006\u0007\u0010\u0002\u000b\t\t\u0007\u0010\u000b\u0003\n\u0004\u0006\u0016\u0007*\u0002\t\u0007\f\u000b\u000f\u0006\u0006\u000e\n\u000b\u0006\u0007\u000b\t\u0005\u000b\t\u0006\t\u0012\u0010\u0007\u0006\u0010\n\u0012\u0013\n\u000b\u0013\u0007 \n\t\u000b\u000b\u000f\u000b\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007!\t\n\u0012\u0016 \n\u001d\u000f\u0007\u000f\u0010\u0002\t\u000b\u0007\t\u0015\u0015\t\f\u0010 \u0006\u0007\"\t\u000b\t\u0007\u0015\u000f#\u0012\u0013\u0007\u0010\u000f\u0007\u000e\t\u0007\u0006\u0003\u001f\u0012\u0003\u0015\u0003\f\n\u0012\u00107\u0007\u0015\u000f\u000b\u0007\u0003\u0012\u0006\u0010\n\u0012\f\t\u001c\u0007 \n\f\u000f\u0012\u0010\u000f#\u000b\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0007\"\n\u0006\u0007\u0012\u000f\u0010\u0007\n\u0015\u0015\t\f\u0010\t\u0013\u0007\u000e%\u0007 \u0012\u0007\u000e\u000f\u0007\u000e\u000f\r\b\u0006\u0002\u0007\u000e\u0007\u000e\u000f \u00075\u001e\u0007L\u0007 \n9\u001c\u0007\u0005\u0007?\u0007/\u0016\u0018.6\u0007\u000f\u000b\u0007 \u0012\u0004\u000e\u000f\r\u0001\u0002\u0013\u0007\u0015\u0007\u0002\u0006\u0007\b\u0016 \u00075\u001e\u0007L\u00079\u001c\u0007\u0005\u0007?\u0007/\u0016;96\u0016 \n\u0018\u0010\u0019\u0010\u001d \u001e\u000e\b\t\u0006\u0003\u0002\u0015\u0012 \n*\u0002\t\u0007\u000b\t\u0006#\u0004\u0010\u0006\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0005\u000b\u000f\u0005\u000f\u000b\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u0007\f\u000f\u000b\u000b\t\f\u0010\u0004%\u0007\u0006#\u0012\u001f\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007\u0015\u000f\u000b\u0007 \n\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\n\u0012\u0013\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004 \u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\n\u0012\u0013\u0007\u000e%\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\n\u0012\u0013\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007 \n\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\n\f\u000b\u000f\u0006\u0006\u0007\u0010\u000b\u0003\n\u0004\u0006\u0007\n\u000b\t\u0007\u0006\u0002\u000f\"\u0012\u0007\u0003\u0012\u0007\u001e\u0003\u001f#\u000b\t\u0007\u0018\u0016 \n)\u0012\u0007\u0010\u0002\t\u0007\n\u0012\n\u0004%\u0006\u0003\u0006\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007 \u0015\u0004\u000f\u0007\b -\u0010\u000b\n\u0012\u0006\u0015\u000f\u000b!\t\u0013\u0007 \u0007\u000e\b\t\u0006\u0003\u0002\u0015 \u0007\u0013\n\u0010\n\u001c\u0007\n\u0007!\n\u0003\u0012\u0007 \n\t\u0015\u0015\t\f\u0010\u0007\u0015\u000f\u000b\u0007 \u0012\u0007\u000e\u000f\u0007\u000e\u000f\r \b\u0006\u0002\u0007\u000e\u0007\u000e\u000f \u0007\"\n\u0006\u0007\u0015\u000f#\u0012\u0013\u00075\u001e59\u001c9\u001a6\u0007?\u0007\u001a\u0016//\u001c\u0007\u0005\u0007L\u0007 \n/\u0016/\u00196\u0016\u0007*\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0006\n\u0012\u001f\u0007\u001aA0\u0007\u000f\u0015\u0007\u0010\u0002\t\u0003\u000b\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006 \u0007\f\u000f\u000b\u000b\t\f\u0010\u0004%\u001c\u0007 \n\"\u0002\t\u000b\t\n\u0006\u0007 #\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007 \u0006\u0003\u0012\u001f\t\u000b\u0006\u0007 \u0006\n\u0012\u001f\u0007 \u0019\u001a0\u0007 \u000f\u0015\u0007 \u0010\u0002\t\u0003\u000b\u0007 \u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007 \f\u000f\u000b\u000b\t\f\u0010\u0004%\u0016\u0007\u001b\u0006\u0007\u0006\u0002\u000f\"\u0012\u0007\u0003\u0012\u0007\u0005\n\u0012\t\u0004\u00075\u000e6\u0007\u000f\u0015\u0007\u001e\u0003\u001f#\u000b\t\u0007\u0018\u001c\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007 \u0006\n\u0012\u001f\u0007 \u000e\t\u0010\u0010\t\u000b\u0007 \u0010\u0002\n\u0012\u0007 #\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007 \u0006\u0003\u0012\u001f\t\u000b\u0006\u0007 \u0013\u0003\u0013\u0007 \u0003\u0012\u0007 \u0010\t\u000b!\u0006\u0007 \u000f\u0015\u0007 !#\u0006\u0003\f\n\u0004\u0007 \u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0016 \n\u001e\u0003\u001f#\u000b\t\u0007\u0018<\u0007\u0007)\u0012\u0010\t\u000b\u0014\n\u0004\u0016\u0007@\t\n\u0012\u0007\u0005\u000b\u000f\u0005\u000f\u000b\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u0007\f\u000f\u000b\u000b\t\f\u0010\u0004%\u0007\u0006#\u0012\u001f\u0007! #\u0006\u0003\f\n\u0004\u0007 \n\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0016\u0007*\u0002\t\u0007\u0004\t\u0015\u0010 -\u0002\n\u0012\u0013\u0007\u0005\n\u0012\t\u0004\u00075\n6\u0007\u0006\u0002\u000f\"\u0006\u0007\u0010\u0002\t\u0007!\t\n\u0012\u0007\u0005\u000b\u000f\u0005\u000f\u000b\u0010\u0003\u000f\u0012\u0006\u0007 \n\u0015\u000f\u000b\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\n\u0012\u0013\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\u000f\u0015\u00071*\u0002\t\u0007$\t\n\u0010\u0004\t\u00062\u0007\n\f\u000b\u000f\u0006\u0006\u0007 \n\u0010\u0002\u000b\t\t\u0007\u0010\u000b\u0003\n\u0004\u0006\u0016\u0007*\u0002\t\u0007\u000b\u0003\u001f\u0002\u0010 -\u0002\n\u0012\u0013\u0007\u0005\n\u0012\t\u0004\u00075\u000e6\u0007\u0006\u0002\u000f\"\u0006\u0007\u0010\u0002\t\u0007!\t\n\u0012\u0007 \n\u0005\u000b\u000f\u0005\u000f\u000b\u0010\u0003\u000f\u0012\u0006\u0007\u0015\u000f\u000b\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\n\u0012\u0013\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0016\u0007*\u0002\t\u0007\f\u000b\u000f\u0006\u0006\u000e\n\u000b\u0006\u0007 \n\u000b\t\u0005\u000b\t\u0006\t\u0012\u0010\u0007\u0006 \u0010\n\u0012\u0013\n\u000b\u0013\u0007\t\u000b\u000b\u000f\u000b\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007!\t\n\u0012\u0016 \n*\u0002\u000f#\u001f\u0002\u0007\n\u0012\u0007\u0003\u0012\u0010\t\u000b\n\f\u0010\u0003\u000f\u0012\u0007\t\u0015\u0015\t\f\u0010\u0007\u0015\u000f\u000b\u0007 \u0012\u0004\u000e\u000f\r\u0001\u0002\u0013\u0007\u0015\u0007\u0002\u0006\u0007\b\u0016 \u0007\n\u0012\u0013\u0007 \b\u0006\u0007\u0002\u0015 \u0007\"\n\u0006\u0007 \n\u0012\u000f\u0010\u0007\u0015\u000f#\u0012\u0013\u0007\u0010\u000f\u0007\u000e\t\u0007\u0006\u0003\u001f\u0012\u0003\u0015\u0003\f\n\u0012\u0010\u0007\u0003\u0012\u0007\u0010\u0002\t\u0007!#\u0004\u0010\u0003\u0014\n\u000b\u0003\n\u0010\t\u0007\n\u0012\n\u0004%\u0006\u0003\u0006\u00075\u001e5A\u001c9\u00196\u0007 ?\u0007 A\u00169\u0019\u001c\u0007 \u0005\u0007 ?\u0007 /\u00169\u00196\u001c\u0007 \u0003\u0010\u0007 \"\n\u0006\u0007 \u0015\u000f#\u0012\u0013\u0007 \u0010\u000f\u0007 \u000e\t\u0007 \u0006\u0003\u001f\u0012\u0003\u0015\u0003\f\n\u0012\u0010\u0007 \u0003\u0012\u0007 \u0010\u0002\t\u0007 #\u0012\u0003\u0014\n\u000b\u0003\n\u0010\t\u0007\u0010\t\u0006\u0010\u00075\u001e5A\u001cBA6\u0007?\u0007B\u0016.:\u001c\u0007\u0005\u0007L\u0007/\u0016/\u00196 \u0016\u0007\u001e#\u000b\u0010\u0002\t\u000b\u0007\n\u0012\n\u0004%\u0006\u0003\u0006\u0007 \n\u000b\t\u0014\t\n\u0004\t\u0013\u0007 \u0010\u0002\n\u0010\u0007 !\u000f\u0006\u0010\u0007 \u0014\n\u000b\u0003\n\u0010\u0003\u000f\u0012\u0007 \"\n\u0006\u0007 \t \u0005\u0004\n\u0003\u0012\t\u0013\u0007 \u000e%\u0007 \u0010\u0002\t\u0007 \u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0006\u0007\u000f\u0015\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\u000e\t\u0003\u0012\u001f\u0007!\u000f\u000b\t\u0007\n\f\f#\u000b\n\u0010\t\u0007\u000f\u0012\u0007 \n\u0010\u000b\u0003\n\u0004\u0007B\u0007\u0010\u0002\n\u0012\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u0006\u00079\u0007\n\u0012\u0013\u0007A\u001c\u0007\"\u0002\u0003\u0004\t\u0007\u0010\u0002\u0003\u0006\u0007\u0013\u0003\u0013\u0007\u0012\u000f\u0010\u0007\u0002\u000f\u0004\u0013\u0007\u0015\u000f\u000b\u0007\u0010\u0002\t\u0007 \n\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u00075\u001e59\u001c9\u001a6\u0007?\u0007\u0018\u0016/9\u001c\u0007\u0005\u0007?\u0007/\u0016/\u001aA6\u0016\u0007\u0007\u001b\u0006\u0007\u0006\u0002\u000f\"\u0012\u0007\u0003\u0012\u0007\u0005\n\u0012\t\u0004\u0007 5\n6\u0007\u000f\u0015\u0007\u001e\u0003\u001f#\u000b\t\u0007\u0018\u001c\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\"\t\u000b\t\u0007\u000e\t\u0010\u0010\t\u000b\u0007\u0006#\u0012\u001f\u0007\n\u0010\u0007\u0010\u0002\t\u0007 \u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0007\u0004\t\u0014\t\u0004\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u0007B\u00075!\t\n\u0012\u0007\n\f\u000b\u000f\u0006\u0006\u0007\u0010\u000b\u0003\n\u0004\u0006<\u0007/\u0016\u0019\u0019\u001c\u0007/\u0016\u0019/\u001c\u0007/\u0016\u001a96\u0016\u0007 )\u0012\u0007\f\u000f\u0012\u0010\u000b\n\u0006\u0010\u001c\u0007\u0010\u0002\t\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\u0013\u0003\u0013\u0007\u0012\u000f\u0010\u0007\u0003!\u0005\u000b\u000f\u0014\t\u0007 \n5!\t\n\u0012\u0007\n\f\u000b\u000f\u0006\u0006\u0007\u0010\u000b\u0003\n\u0004\u0006<\u0007/\u0016\u001a9\u001c\u0007/\u0016\u001a\u0019\u001c\u0007/\u0016\u001aA6\u0016 \n\u001b\u0012\n\u0004%4\u0003\u0012\u001f\u0007\u0010\u0002\t\u0007 \u0007\u000e\b\t\u0006\u0003\u0002\u0015 \u0007\u0013\n\u0010 \n\u0007\u0005\u000b\u000f\u0013#\f\t\u0013\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u0006\u00079\u0007\n\u0012\u0013\u0007A\u001c\u0007\n\u0007!\n\u0003\u0012\u0007 \n\t\u0015\u0015\t\f\u0010\u0007\u000f\u0015\u0007 \u0012\u0004\u000e\u000f\r\u0001\u0002\u0013\u0007\u0015\u0007\u0002\u0006\u0007\b\u0016 \u0007\"\n\u0006\u0007\u0015\u000f#\u0012\u0013\u0007\u0010\u000f\u0007\u000e\t\u0007\u0006\u0003\u001f\u0012\u0003\u0015\u0003\f\n\u0012\u0010\u00075\u001e59\u001c9\u001a6\u0007 \n?\u0007\u001a\u0016;9\u001c\u0007\u0005\u0007L\u0007/\u0016/\u00196\u0016\u0007\u001b\u0006\u0007\u0006\u0002\u000f\"\u0012\u0007\u0003\u0012\u0007\u0005\n\u0012\t\u0004\u00075\n6\u0007\u000f\u0015\u0007\u001e\u0003\u001f#\u000b\t\u0007\u0018\u001c\u0007\u0010\u0002\t\u0007 \u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007 \u000f\u0015\u0007 \u0015\n!\u0003\u0004\u0003\n\u000b\u0007 \u0006\u000f\u0012\u001f\u0006\u0007 \f\u000f\u0012\u0010\n\u0003\u0012\t\u0013\u0007 \u0005\u000b\u000f\u0005\u000f\u000b\u0010\u0003\u000f\u0012\n\u0004\u0004%\u0007 !\u000f\u000b\t\u0007 \f\u000f\u000b\u000b\t\f\u0010\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u0006\u00079\u0007\n\u0012\u0013\u0007A\u00075!\t\n\u0012 <\u0007/\u0016\u001aB6\u0007\u0010\u0002\n\u0012\u0007\u0010\u0002\t\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007 \n\u000f\u0015\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\u0013\u0003\u0013\u00075!\t\n\u0012<\u0007/\u0016\u0019A6\u0016\u0007*\u0002\t\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0004\t\u0006\u0006\u0007 \n\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\u0003!\u0005\u000b\u000f\u0014\t\u0013\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u0007B\u00075!\t\n\u0012<\u0007/\u0016\u001aA6\u0016 \n\u001d\u000f\u0007\u000f\u0010\u0002\t\u000b\u0007\t\u0015\u0015\t\f\u0010\u0006\u0007\"\t\u000b\t\u0007\u0015\u000f#\u0012\u0013\u0007\u0010\u000f\u0007\u000e\t\u0007\u0006\u0003\u001f\u0012\u0003\u0015\u0003\f\n\u0012\u0010\u0016 \n\u0018\u0010\u0019\u0010\u001f \u001b\t\u0013\u0014\u0004 \n*\u0002\t\u0007 !\t\n\u0006#\u000b\t\u0013\u0007 \u0010\t!\u0005\u000f\u0007 \n\u0006\u0007 \u0005\u000b\u000f\u0013#\f\t\u0013\u0007 \u000e%\u0007 \u0010\u0002\t\u0007 \u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0006\u0007 \u0003\u0006\u0007 \n\f\u000f!\u0005\n\u000b\t\u0013\u0007 \u0010 \u000f\u0007 \u0010\u0002\t\u0007 \n\f\u0010#\n\u0004\u0007 \u0010\t!\u0005\u000f\u0007 5\u0015\u000b\u000f!\u0007 \u0010\u0002\t\u0007 \u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007 $\t\n\u0010\u0004\t\u0006\u0007 \n\u000b\t\f\u000f\u000b\u0013\u0003\u0012\u001f\u00066\u0007\u0003\u0012\u0007\u000e\u0003\u0014\n\u000b\u0003\n\u0010\t\u0007\u0006\f\n\u0010\u0010\t\u000b\u0007\u0005\u0004\u000f\u0010\u0006\u0007\u0003\u0012\u0007\u001e\u0003\u001f#\u000b\t\u0007\u0019\u0016\u0007)\u0012\u0007\f\n\u0006\t\u0007\u000f\u0015\u0007 \n\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\n\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0007\u0015\u000b\u000f!\u0007!\t!\u000f\u000b%\u001c\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0006\u0007\f\n!\t\u0007\f\u0004\u000f\u0006\t\u0007 \u0010\u000f\u0007\u0010\u0002\t\u0007\u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007\u0010\t!\u0005\u0003\u0007\u0003\u0012\u0013\u0003\f\n\u0010\t\u0013\u0007\u000e%\u0007\u0010\u0002\t\u0007\u0002\u0003\u001f\u0002\u0007\f\u000f\u000b\u000b\t\u0004\n\u0010\u0003\u000f\u0012\u0007\u000e\t\u0010\"\t\t\u0012\u0007 \u0010\u0002\t\u0007\u0006#\u0012\u001f\u0007\u0010\t!\u0005\u000f\u0007\n\u0012\u0013\u0007\u0010\u0002\t \u0007\n\f\u0010#\n\u0004\u0007\u0010\t!\u0005\u000f\u00075\u000b\u0007?\u0007/\u0016:9\u001c\u0007\u0005\u0007L\u0007/\u0016/97\u0007\u0010\u000b\u0003\n\u0004\u00079\u0007 \n\n\u0012\u0013\u0007 A\u0007 \f\u000f!\u000e\u0003\u0012\t\u00136\u0016\u0007 *\u0002\t%\u0007 \u0005\u0003\f+\t\u0013\u0007 #\u0005\u0007 \u0010\u0002\t\u0007 \n\f\u0010#\n\u0004\u0007 \u0010\t!\u0005\u000f\u0007 \n\u0015\u0010\t\u000b\u0007 \u0002\n\u0014\u0003\u0012\u001f\u0007\u0002\t\n\u000b\u0013\u0007\u0010\u0002\t\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u00075\u000b\u0007?\u0007/\u0016:.\u001c\u0007\u0005\u0007L\u0007/\u0016/97\u0007\u0010\u000b\u0003\n\u0004\u0007B6\u0016\u0007)\u0012\u0007 \n\f\n\u0006\t\u0007\u000f\u0015\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\n\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0007\u0015\u000b\u000f!\u0007!\t!\u000f\u000b%\u001c\u0007\u0010\u0002\t\u0007\u0010\t!\u0005\u0003\u0007 \n\u0006#\u0012\u001f\u0007\n\u000b\t\u0007!\u000f\u000b\t\u0007\u0013\u0003\u0006\u0005\t\u000b\u0006\t\u0013\u0007\u0015\u000b\u000f!\u0007\u0010\u0002\t\u0007 \n\f\u0010#\n\u0004\u0007\u0010\t!\u0005\u0003\u00075\u000b\u0007?\u0007/\u0016;9\u001c\u0007\u0005\u0007L\u0007 \n/\u0016/97\u0007\u0010\u000b\u0003\n\u0004\u00079\u0007\n\u0012\u0013\u0007A\u0007\f\u000f!\u000e\u0003\u0012\t\u00136\u0016\u0007\u0017\u000f\"\t\u0014\t\u000b\u001c\u0007\u0006\u000f\u0012\u001f\u0007\u0004\u0003\u0006\u0010\t\u0012\u0003\u0012\u001f\u0007!\n\u0013\t\u0007 \u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0006\u0007\u0005\u0003\f+\u0007#\u0005\u0007\u0010\u0002\t\u0007\n\f\u0010#\n\u0004\u0007\u0010\t!\u0005\u000f\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0007 5\u000b\u0007?\u0007/\u0016:A\u001c\u0007\u0005\u0007L\u0007/\u0016/97\u0007\u0010\u000b\u0003\n\u0004\u0007B6\u0016\u0007 \n*\t!\u0005\u000f\u0007!\n\u0010\f\u0002\u0003\u0012\u001f\u0007\u0005\t\u000b\u0015\u000f\u000b!\n\u0012\f\t\u0007\u0013\u0003\u0013\u0007\u0012\u000f\u0010\u0007\u0013\u0003\u0015\u0015\t\u000b\u0007\n!\u000f\u0012\u001f\u0007 \u0012\u0007\u000e\u000f\u0007\u000e\u000f\r \n\b\u0006\u0002\u0007\u000e\u0007\u000e\u000f \u0016\u0007 *\u000b\n\u0003\u0012\t \u0013\u0007 \n\u0012\u0013\u0007 #\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007 \u0006\u0003\u0012\u001f\t\u000b\u0006\u0007 \u0005\t\u000b\u0015\u000f\u000b!\t\u0013\u0007 \t(#\n\u0004\u0004%\u0007 \n\"\t\u0004\u0004\u00075\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u0006\u00079\u0007\n\u0012\u0013\u0007A\u001c\u0007\u000b\u0007?\u0007/\u0016;:\u001c\u0007\u0005\u0007L\u0007/\u0016/97\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u0007B\u001c\u0007\u000b\u0007?\u0007/\u0016:\u0019\u001c\u0007\u0005\u0007 L\u0007/\u0016/96\u0016 \u0007\u001e\u0003\u001f#\u000b\t\u0007\u0019<\u0007\u001b\f\u0010#\n\u0004\u0007\u0010\t!\u0005\u000f\u0007\u0014\t\u000b\u0006#\u0006\u0007\u0006#\u0012\u001f\u0007\u0010\t!\u0005\u000f\u0007\u0003\u0012\u0007\u000e\t\n\u0010\u0006\u0007\u0005\t\u000b\u0007 \n!\u0003\u0012#\u0010\t\u0016\u0007*\u0002\t\u0007\u0004\t\u0015\u0010 -\u0002\n\u0012\u0013\u0007\u0005\n\u0012\t\u0004\u00075\n6\u0007\u0006\u0002\u000f\"\u0006\u0007\u0010\u0002\t\u0007\u0010\t!\u0005\u000f\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0006\u0007 \n\u0015\u000f\u000b\u0007\u0010\u0002\t\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012 \u001f\u0006\u0016\u0007*\u0002\t\u0007\u000b\u0003\u001f\u0002\u0010 -\u0002\n\u0012\u0013\u0007\u0005\n\u0012\t\u0004\u00075\u000e6\u0007\u0006\u0002\u000f\"\u0006\u0007\u0010\u0002\t\u0007 \n\u0010\t!\u0005\u000f\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0006\u0007\u0015\u000f\u000b\u0007\u0010\u0002\t\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0016\u0007*\u0002\t\u0007\"\u0002\u0003\u0010\t\u0007 \n\u0013\u0003\n!\u000f\u0012\u0013\u0006\u0007\u000b\t\u0005\u000b\t\u0006\t\u0012\u0010\u0007\u0013\n\u0010\n\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u00079\u0007\n\u0012\u0013\u0007A\u0007\f\u000f!\u000e\u0003\u0012\t\u0013\u0016\u0007*\u0002\t\u0007\u000e\u0004\n\f+\u0007 \n\u0013\u0003\n!\u000f\u0012\u0013\u0006\u0007\u000b\t\u0005\u000b\t\u0006\t\u0012\u0010\u0007\u0013\n\u0010\n\u0007\u000f\u0012\u0007\u0010\u000b\u0003\n\u0004\u0007B\u0016 \n*\u000f\u0007\u0005#\u0010\u0007\f#\u000b\u000b\t\u0012\u0010\u0007\u000b\t\u0006#\u0004\u0010\u0006\u0007\u0003\u0012\u0010\u000f\u0007\f\u000f\u0012\u0010\t \u0010\u001c\u0007\u0003\u0010\u0007\u0003\u0006\u0007\u0003!\u0005\u000f\u000b\u0010\n\u0012\u0010\u0007\u0010\u000f\u0007+\u0012\u000f\"\u0007\u0010\u0002\n \u0010\u0007 \n\u0002#!\n\u0012\u0006\u0007\u0002\n\u0014\t\u0007\u000f\u0012\u0004%\u0007\n\u0007\u0015\u0003\u0012\u0003\u0010\t\u0007\u000b\t\u0006\u000f\u0004#\u0010\u0003\u000f\u0012\u0007\u0003\u0012\u0007\u0005\t\u000b\f\t\u0003\u0014\u0003\u0012\u001f\u0007\n\u0007\u0013\u0003\u0015\u0015\t\u000b\t\u0012\f\t\u0007 \u000e\t\u0010\"\t\t\u0012\u0007\u0006\t\u0012\u0006\n\u0010\u0003\u000f\u0012\u0006\u0007\f\n#\u0006\t\u0013\u0007\u000e%\u0007\u0005\u0002%\u0006\u0003\f\n\u0004\u0007\u0006\u0010\u0003!#\u0004\u0003\u0016\u0007*\u0002\t\u0007\u0006!\n\u0004\u0004\t\u0006\u0010\u0007 \u0013\u0003\u0015\u0015\t\u000b\t\u0012\f\t\u0007\u0003\u0012\u0007\u0006\t\u0012\u0006\n\u0010\u0003\u000f\u0012\u0007\u0010\u0002\n\u0010\u0007\f\n\u0012\u0007\u000e\t\u0007\u0012\u000f\u0010\u0003\f\t\u0013\u0007\u0003\u0006\u0007\f\n\u0004\u0004\t\u0013\u0007\n\u00071>#\u0006\u0010\u0007 \u0012\u000f\u0010\u0003\f\t\n\u000e\u0004\t\u0007 \u0013\u0003\u0015\u0015\t\u000b\t\u0012\f\t2\u0007 5C\u001d'6\u0016\u0007 *\u0002\u000f#\u001f\u0002\u0007 \u0013\u0003\u0015\u0015\t\u000b\t\u0012\u0010\u0007 C\u001d'\u0006\u0007 \u0015\u000f\u000b\u0007 \u0010\t!\u0005\u000f\u0007 \u0005\t\u000b\f\t\u0005 \u0010\u0003\u000f\u0012\u0007 \u0002\n\u0014\t\u0007 \u000e\t\t\u0012\u0007 \u000b\t\u0005\u000f\u000b\u0010\t\u0013\u0007 5\f\n#\u0006\t\u0013\u0007 \u000e%\u0007 \u0013\u0003\u0015\u0015\t\u000b\t\u0012\u0010\u0007 \n\u0010\n\u0006+\u0006\u0007\n\u0012\u0013\u0007\u0003\u0012\u0006\u0010\u000b#\f\u0010\u0003\u000f\u0012\u00066\u001c\u0007\"\t\u0007#\u0006\t\u0007\n\u0007\u000b\n\u0010\u0002\t\u000b\u0007\f\u000f\u0012\u0006\t\u000b\u0014\n\u0010\u0003\u0014\t\u0007C\u001d'\u0007\u000f\u0015\u0007 \nNH -\u001a0\u00075\u001b\u0004\u0004\t\u0012\u001c\u00079:.\u00196\u0016\u0007*\u0002\u0003\u0006\u0007!\t\n\u0012\u0006\u0007\u0010\u0002\n\u0010\u0007\u0010\t!\u0005\u000f\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0006\u0007\"\u0003\u0010\u0002\u0003\u0012\u0007 \n\u0010\u0002\u0003\u0006\u0007\u001a0\u0007\u000b\n\u0012\u001f\t\u001c\u0007\"\u0002\u0003\u0004\t\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\n\u0007\u0006\u000f\u0012\u001f\u0007\u0015\u000b\u000f!\u0007!\t!\u000f\u000b%\u0007\u000f\u000b\u0007\n\u0015\u0010\t\u000b\u0007 \u000b\t\f\t\u0012\u0010\u0007\t \u0005\u000f\u0006#\u000b\t\u001c\u0007\"\t\u0012\u0010\u0007\u0004\n\u000b\u001f\t\u0004%\u0007#\u0012 \u0012\u000f\u0010\u0003\f\t\u0013\u0007\u000e%\u0007\u0010\u0002\t\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0006\u0016\u0007)\u0012\u0007 \n\u001e\u0003\u001f#\u000b\t\u0007\u001a\u001c\u0007\u0010\u0002\t\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0007\u0015\u000b\u000f!\u0007\u0010\u0002\t\u0007\n\f\u0010#\n\u0004\u0007\u0010\t!\u0005\u000f\u0007\t \u0005\u000b\t\u0006\u0006\t\u0013\u0007\n\u0006\u0007 \u0005\t\u000b\f\t\u0012\u0010\n\u001f\t\u0006\u0007\n\u000b\t\u0007\u0006\u0002\u000f\"\u0012\u0007\u0003\u0012\u0007\t(#\n\u0004\u0007\u0006\u0010\t\u0005\u0006\u0007\u000f\u0015\u0007\u001a0\u0016\u0007 \n)\u0010\u0007\n\u0005\u0005\t\n\u000b\t\u0013\u0007\u0010\u0002\n\u0010\u001c\u0007\"\u0002\t\u0012\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\n\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0007\u0015\u000b\u000f!\u0007!\t!\u000f\u000b%\u0007 \n\u000f\u0012\u0007\u0010\u000b\n\u0003\u0004\u0006\u00079\u0007\n\u0012\u0013\u0007A\u001c\u0007ABH.A\u00075BA06\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u0015\t\u0004\u0004\u0007\"\u0003\u0010\u0002\u0003\u0012\u0007\u000f\u0012\t C\u001d'\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\n\f\u0010#\n\u0004\u0007\u0010\t!\u0005\u000f\u0007\n\u0012\u0013\u0007\u0018\u001aH.A\u00075\u001a\u001806\u0007\f\n!\t\u0007\"\u0003\u0010\u0002\u0003\u0012\u0007\u0010\"\u000f\u0007 C\u001d'\u0006\u0016\u0007E\u0012\u0007\u0010\u000b\u0003\n\u0004\u0007B\u001c\u00079.HB\u001a\u00075\u0018.06\u0007\f\n!\t\u0007 \"\u0003\u0010\u0002\u0003\u0012\u0007\u000f\u0012\t\u0007C\u001d'\u0007\n\u0012\u0013\u0007 A.HB\u001a\u00075.\u001906\u0007\f\n!\t\u0007\"\u0003\u0010\u0002\u0003\u0012\u0007\u0010\"\u000f\u0007C\u001d'\u0006\u0016 \n\r\u0003+\t\"\u0003\u0006\t\u001c\u0007\"\u0002\t\u0012\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\n\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0007\u0015\u000b\u000f!\u0007!\t!\u000f\u000b%\u0007\u000f\u0012\u0007 \n\u0010\u000b\u0003\n\u0004\u0006\u00079\u0007\n\u0012\u0013\u0007A\u001c\u0007A/H.A\u00075A;06\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u0015\t\u0004\u0004\u0007\"\u0003\u0010\u0002 \u0003\u0012\u0007\u000f\u0012\t\u0007C\u001d'\u0007 \n\n\u0012\u0013\u0007\u00189H.A\u00075\u0019.06\u0007\"\u0003\u0010\u0002\u0003\u0012\u0007\u0010\"\u000f\u0007C\u001d'\u0006\u0016\u0007E\u0012\u0007\u0010\u000b\u0003\n\u0004\u0007B\u001c\u00079;HB\u001a\u00075\u0019/06\u0007 \n\u0012\u0013\u0007A\u001aHB\u001a\u00075.A06\u0007\f\n!\t\u0007\"\u0003\u0010\u0002\u0003\u0012\u0007\u000f\u0012\t\u0007\n\u0012\u0013\u0007\u0010\"\u000f\u0007C\u001d'\u0006\u001c\u0007\u000b\t\u0006\u0005\t\f\u0010\u0003\u0014\t\u0004%\u0016\u0007\u0007 \n\u0007\u001e\u0003\u001f#\u000b\t\u0007\u001a<\u0007\u0001\t\u000b\f\t\u0012\u0010\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u0007\u0010\t!\u0005\u000f\u0007\u0006#\u0012\u001f\u0007\u0015\u000b\u000f!\u0007\n\f\u0010#\n\u0004\u0007\u0010\t!\u0005\u000f\u0007 \n\u000f\u0015\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\n\u0012\u0013\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\u000f\u0015\u00071\u0010\u0002\t\u0007$\t\n\u0010\u0004\t\u00062\u0007\n\f\u000b\u000f\u0006\u0006\u0007\u0010\u0002\u000b \t\t\u0007 \n\u0010\u000b\u0003\n\u0004\u0006\u0016\u0007*\u0002\t\u0007\u0005\t\u000b\f\t\u0012\u0010\u0007\u0013\t\u0014\u0003\n\u0010\u0003\u000f\u0012\u0007\u0003\u0006\u0007\u0013\u0003\u0014\u0003\u0013\t\u0013\u0007#\u0005\u0007\u0003\u0012\u0010\u000f\u0007\u0006\u0010\t\u0005\u0006\u0007\u000f\u0015\u0007\u001a0\u0016\u0007 \nO\u000f\u000f\u0013\u0007\u0005\t\u000b\u0015\u000f\u000b!\n\u0012\f\t\u0007!\n%\u0007\u000e\t\u0007\u000e\u0003\n\u0006\t\u0013\u0007\u000e%\u0007\n\u0007\u0006\u0010\u000b\u000f\u0012\u001f\u0007!\t!\u000f\u000b%\u0007\u0015\u000f\u000b\u0007\n\u0007 \n\u0006!\n\u0004\u0004\u0007\u000b\n\u0012\u001f\t\u0007\u000f\u0015\u0007\u0010\t!\u0005\u0003\u0007\u0010\u0002\n\u0010\u0007\u0002\n\u0005\u0005\t\u0012\t\u0013\u0007\u0010\u000f\u0007\u000e\t\u0007\u0003\u0012\u0007\u0010\u0002\t\u0007\u0010\t\u0006\u0010\u0016\u0007)\u0010\u0007\u0003\u0006\u0007 \n\u000b\u001f#\t\u0013\u0007\u0010\u0002\n\u0010\u0007\u0010\u0002\u0003\u0006\u0007\"\n\u0006\u0007\u0012\u000f\u0010\u0007\u0010\u0002\t\u0007\f\n\u0006\t\u0007\u000e\t\f\n#\u0006\t\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0015\u000f\u0004\u0004\u000f\"\u0003\u0012\u001f\u0016\u0007)\u0012\u0007 \u0010\u000f\u0010\n\u0004\u001c\u0007 \u0005\n\u000b\u0010 \u0003\f\u0003\u0005\n\u0012\u0010\u0006\u0007 \u0006\n\u0012\u001f\u0007 \t\u0004\t\u0014\t\u0012\u0007 \u0013\u0003\u0015\u0015\t\u000b\t\u0012\u0010\u0007 $\t\n\u0010\u0004\t\u0006\u0007 \u0006\u000f\u0012\u001f\u0006\u0007 \"\u0003\u0010\u0002\u0007 \n\u0010\t!\u0005\u0003\u0007\u000b\n\u0012\u001f\u0003\u0012\u001f\u0007\n\u0005\u0005\u000b\u000f \u0003!\n\u0010\t\u0004%\u0007\u000e\t\u0010\"\t\t\u0012\u0007./\u0007\n\u0012\u0013\u00079:/\u0007\u000e\t\n\u0010\u0006\u0007\u0005\t\u000b\u0007 \n!\u0003\u0012#\u0010\t7\u0007\u0010\u0002\u0003\u0006\u0007\u000b\n\u0012\u001f\t\u0007\u0006\u0005\n\u0012\u0006\u0007\u0015\u000b\u000f!\u00071!\u000f\u0013\t\u000b\n\u0010\t\u0004%\u0007\u0006\u0004\u000f\"2\u0007\u0010\u000f\u00071\u0014\t\u000b%\u0007\u0015\n\u0006\u00102\u0016\u0007 \u001b\u0006\u0007\u0006\u0002\u000f\"\u0012\u0007\u0003\u0012\u0007\u0005\n\u0012\t\u0004\u00075\n6\u0007\u000f\u0015\u0007\u001e\u0003\u001f#\u000b\t\u0007\u0019\u001c\u0007\u0006\u0003 \u0007\u0013\u0003\u0015\u0015\t\u000b\t\u0012\u0010\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007 \n\"\t\u000b\t\u0007\u0006#\u0012\u001f\u0007\u0010\u0002\n\u0010\u0007\"\t\u000b\t\u0007\u0012 \u000f\u0010\u0007\f\u0002\n\u000b\n\f\u0010\t\u000b\u00034\t\u0013\u0007\u000e%\u0007\n\u0007\u0015\n\u0006\u0010\u0007\u0010\t!\u0005\u000f7\u0007\u0015\u0003\u0014\t\u0007\u000f#\u0010\u0007 \n\u000f\u0015\u0007\u0010\u0002\t\u0007\u0006\u0003 \u0007\u0006\u000f\u0012\u001f\u0006\u0007\"\t\u000b\t\u0007\u0012\u000f\u0010\u0007\u0015\n\u0006\u0010\t\u000b\u0007\u0010\u0002\n\u0012\u00079A/\u0007\u000e\t\n\u0010\u0006\u0007\u0005\t\u000b\u0007!\u0003\u0012#\u0010\t\u0016\u0007\u001b\u0006\u0007 \n\u0006\u0002\u000f\"\u0012\u0007\u0003\u0012\u0007\u0005\n\u0012\t\u0004\u00075\u000e6\u0007\u000f\u0015\u0007\u001e\u0003\u001f#\u000b\t\u0007\u0019\u001c\u0007\u0010\u0002\t\u0007!\n\u0010\t\u000b\u0003\n\u0004\u0007\u000f\u0015\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007 \u0006\u000f\u0012\u001f\u0006\u0007\"\n\u0006\u0007!\n\u0013\t\u0007\u000f#\u0010\u0007\u000f\u0015\u0007\u0010\t\u0012\u0007\u0013\u0003\u0015\u0015\t\u000b\t\u0012\u0010\u0007\u0005\u0003\t\f\t\u00067\u0007\u0010\u0002\t\u0007\u0010\t!\u0005\u0003\u0007\"\t\u000b\t\u0007 \"\u0003\u0013\t\u0004%\u0007\u0013\u0003\u0006\u0010\u000b\u0003\u000e#\u0010\t\u0013\u0016\u0007 \nO\u000f\u000f\u0013\u0007 \u0005\t\u000b \u0015\u000f\u000b!\n\u0012\f\t\u0007 !\n%\u0007 \n\u0004\u0006\u000f\u0007 \u000e\t\u0007 \u000e\u0003\n\u0006\t\u0013\u0007 \u000e%\u0007 \n\u0007 1\u0005\t\u000b\u0006\u000f\u0012\n\u0004\u0004%\u0007 \n\u0005\u000b\t\u0015\t\u000b\u000b\t\u00132\u0007 \u0010\t!\u0005\u000f\u0007 \u0010\u0002\n\u0010\u0007 \u0003\u0006\u0007 \t\n\u0006%\u0007 \u0010\u000f\u0007 \u000b\t!\t!\u000e\t\u000b\u0016\u0007 *\u0002\u000f#\u001f\u0002\u0007 \n\u0007 \n1\u0005\t\u000b\u0006\u000f\u0012\n\u0004\u0004%\u0007\u0005\u000b\t\u0015\t\u000b\u000b\t\u00132\u0007\u0010\t!\u0005\u000f\u0007\u0014\n\u000b\u0003\t\u0006\u0007\u0003\u0012\u0007\u0013\u0003\u0015\u0015\t\u000b\t\u0012\u0010\u0007\f\u000f\u0012\u0010\t \u0010\u0006\u001c\u0007\u0003\u0010\u0007 \n\u000f\u0015\u0010\t\u0012\u0007\u0005\u000f\u0003\u0012\u0010\u0006\u0007\u000e\t\u0010\"\t\t\u0012\u0007;/\u0007\n\u0012\u0013\u00079//\u0007\u000e\t\n\u0010\u0006\u0007\u0005\t\u000b\u0007!\u0003\u0012#\u0010\t\u00075'\u000f\"\u0004\u0003\u0012\u001f\u0007 \n\u0012\u0013\u0007 \u0017\n\u000b\"\u000f\u000f\u0013\u001c\u0007 9:;\u001a6\u0016\u0007 ,\t\u0007 \u0015\u000f#\u0012\u0013\u0007 \u0010\u0002\u000b\t\t\u0007 \u000f\u000e\u0006\t \u000b\u0014\n\u0010\u0003\u000f\u0012\u0006\u0007 \u0010\u0002\n\u0010\u0007 \n\f\u000f\u0012\u0010\u000b\n\u0013\u0003\f\u0010\u0007\u0010\u0002\t\u0007\t \u0003\u0006\u0010\t\u0012\f\t\u0007\u000f\u0015\u0007\u0010\u0002\u0003\u0006\u0007\u000e\u0003\n\u0006\u0016\u0007\u001e\u0003\u000b\u0006\u0010\u001c\u0007\u0010\u0002\t\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0007 1=\t\u0006\u0010\t\u000b\u0013\n%2\u0007\"\n\u0006\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\t\u0013\u0007\u0003\u0012\u0007\n\u0007\"\u0003\u0013\t\u0007\u000b\n\u0012\u001f\t\u0007\u000e\t\u0010\"\t\t\u0012\u0007.9\u0007\n\u0012\u0013\u0007 9B\u0019\u0007\u000e\t\n\u0010\u0006\u0007\u0005\t\u000b\u0007!\u0003\u0012#\u0010\t\u001c\u0007\u0010\u0002\u000f#\u001f\u0002\u0007\u0003\u0010\u0006\u0007\u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007\u0010\t!\u0005\u000f\u0007\u0003\u0006\u0007\n\u0007!\u000f\u0013\t\u000b\n\u0010\t\u0007 \n\u000f\u0012\t\u0007\u000f\u0015\u0007:.\u0007\u000e\t\n\u0010\u0006\u0007\u0005\t\u000b\u0007!\u0003\u0012#\u0010\t\u0016\u0007D\t\f\u000f\u0012\u0013\u001c\u0007\t\n\f\u0002\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0007\u0006\n\u0012\u001f\u0007 \n\u0007 \n\"\u0003\u0013\t\u0007\u0014\n\u000b\u0003\t\u0010%\u0007\u000f\u0015\u0007\u0010\t!\u0005\u0003\u0007\n\u0012\u0013\u0007\u0002\t\u0012\f\t\u0007\u0013\u0003\u0013\u0007\u0012\u000f\u0010\u0007\u000b\t\u0004%\u0007\u000f\u0012\u0007\n\u00071\u0005\t\u000b\u0006\u000f\u0012\n\u0004\u0004%\u0007 \u0005\u000b\t\u0015\t\u000b\u000b\t\u00132\u0007\u0010\t!\u0005\u000f\u0016\u0007*\u0002\t\u0007\u0010\t!\u0005\u0003\u0007\u0006#\u0012\u001f\u0007\u0015\u000f\u000b\u0007\u0010\u0002\t\u0007\u0015\u000f#\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\u000f\u0012\u0007\u0010\u0002\t\u0007 \u0015\u0003\u000b\u0006\u0010\u0007\u0010\u000b\u0003\n\u0004\u0007\u000e%\u0007\n\u0007\u0006\u0003\u0012\u001f\u0004\t\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0007\u0013\u0003\u0013\u0007\u0012\u000f\u0010\u0007\f\u000f\u000b\u000b\t\u0004\n\u0010\t\u0007\u0006\u0003\u001f\u0012\u0003\u0015\u0003\f\n\u0012\u0010\u0004%\u0016\u0007 *\u0002\t\u0007!\t\n\u0012\u0007\u0013\u0003\u0015\u0015\t\u000b\t\u0012\f\t\u0007\u000e\t\u0010\"\t\t\u0012\u0007\u0010\u0002\t\u0007\u0006\u0004\u000f\"\t\u0006\u0010\u0007\u0005\u000b\u000f\u0013#\f\t\u0013\u0007\u0010\t!\u0005\u000f\u0007\n\u0012\u0013\u0007 \u0010\u0002\t\u0007\u0015 \n\u0006\u0010\t\u0006\u0010\u0007\u0005\u000b\u000f\u0013#\f\t\u0013\u0007\u0010\t!\u0005\u000f\u0007\u0006#\u0012\u001f\u0007\u000e%\u0007\n\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0007\u000f\u0012\u0007\u0010\u0002\t\u0007\u0015\u0003\u000b\u0006\u0010\u0007 \n\u0010\u000b\u0003\n\u0004\u0007\"\n\u0006\u0007.:\u0007\u000e\t\n\u0010\u0006\u0007\u0005\t\u000b\u0007!\u0003\u0012#\u0010\t\u00075!\u0003\u0012<\u00079:\u0007\u000e\u0005!\u001c\u0007!\n <\u00079BB\u0007\u000e\u0005!6\u0016\u0007 \n*\u0002\u0003\u000b\u0013\u001c\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0006\u0007\u0002\n\u0013\u0007\u0012\u000f\u0010\u0007\u0006#\u0012\u001f\u0007\u0006\u000f\u0012\u001f\u0006\u0007\"\u0003\u0010\u0002\u0007\u0006\u0003!\u0003\u0004\n\u000b\u0007\u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007 \u0010\t!\u0005\u0003\u0016\u0007*\u0002\t\u0007\n\f\u0010#\n\u0004\u0007\u0010\t!\u0005\u0003\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0015\u000f#\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\u0006#\u0012\u001f\u0007\u0003\u0012\u0007\n\u0007\u0010\u000b\u0003\n\u0004\u0007\u0013\u0003\u0013\u0007 \u0012\u000f\u0010\u0007\f\u000f\u000b\u000b\t\u0004\n\u0010\t\u0007\u0006\u0003\u001f \u0012\u0003\u0015\u0003\f\n\u0012\u0010\u0004%\u0016\u0007*\u0002\t\u0007!\t\n\u0012\u0007\u0013\u0003\u0015\u0015\t\u000b\t\u0012\f\t\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\n\f\u0010#\n\u0004\u0007 \n\u0010\t!\u0005\u0003\u0007\u000e\t\u0010\"\t\t\u0012\u0007\u0010\u0002\t\u0007\u0006\u0004\u000f\"\t\u0006\u0010\u0007\u0006\u000f\u0012\u001f\u0007\n\u0012\u0013\u0007\u0010\u0002\t\u0007\u0015\n\u0006\u0010\t\u0006\u0010\u0007\u0006\u000f\u0012\u001f\u0007\"\n\u0006\u0007.;\u0007 \n\u000e\t\n\u0010\u0006\u0007\u0005\t\u000b\u0007!\u0003\u0012#\u0010\t\u00075!\u0003\u0012<\u0007BB\u0007\u000e\u0005!\u001c\u0007!\n <\u000799A\u0007\u000e\u0005!6\u0016\u0007 \n \u001e& '\r\u0006\u0004\u0015\u0006\u0006\r\b\t\u0007\u000b\t\u0012\u0007\u0004\b\t\u0004\u000e\u0015\u0006\r\b\t \n*\u0002\u0003\u0006\u0007 \t \u0005\t\u000b\u0003!\t\u0012\u0010\u0007 \t\u0014\n\u0004#\n\u0010\t\u0013\u0007 \u0010\u0002\t\u0007 \t\u0015\u0015\t\f\u0010\u0006\u0007 \u000f\u0015\u0007 \u0006\u000f\u0012\u001f\u0007 \u0015\n!\u0003\u0004\u0003\n\u000b\u0003\u0010%\u001c\u0007 \n\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007 \u0010\u000b\n\u0003\u0012\u0003\u0012\u001f\u0007 \n\u0012 \u0013\u0007 \u000b\t\f\t\u0012\u0010\u0007 \u0006\u000f\u0012\u001f\u0007 \t \u0005\u000f\u0006#\u000b\t\u0007 \u000f\u0012\u0007 \u0010\u0002\t\u0007 \u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007 \n\u0005\t\u000b\u0015\u000f\u000b!\n\u0012\f\t\u0007\u000f\u0015\u00071*\u0002\t\u0007$\t\n\u0010\u0004\t\u00062\u0007!\t\u0004\u000f\u0013\u0003\t\u0006\u0016\u0007\u0001\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0006\u0007\"\t\u000b\t\u0007\u0012\u000f\u0010\u0007 \u0003\u0012\u0006\u0010\u000b#\f\u0010\t\u0013\u0007\u0002\u000f\"\u0007\u0010\u000f\u0007\u0006\u0003\u0012\u001f\u0007\u000f\u000b\u0007\u0002\u000f\"\u0007\u0010\u000f\u0007\u0003!\u0005\u000b\u000f\u0014\t\u0007\u0010\u0002\t\u0003\u000b\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f7\u0007\u0006\u000f\u0007\u0010\u000f\u0007 \n\u0010\u0002\t\u0007\t \u0010\t\u0012\u0010\u0007\u0010\u0002\t%\u0007\u0005\t\u000b\u0015\u000f\u000b!\t\u0013\u0007\"\t\u0004\u0004\u001c\u0007\u0010\u0002\t%\u0007\u0013\u0003\u0013\u0007\u0006\u000f\u0007\u000f\u0012\u0007\u0010\u0002\t\u0003\u000b\u0007\u000f\"\u0012\u0016\u0007)\u0010\u0007 \n\u000e\t\f\n!\t\u0007 \f\u0004\t\n\u000b\u0007 \u0010\u0002\n\u0010\u0007 \u0006\u000f!\t\u0007 \u0005\t\u000b\f\t\u0005\u0010#\n\u0004\u0007 \u0005\u000b\u000f\u0005\t\u000b\u0010\u0003\t\u0006\u0007 \u000f\u0015\u0007 \n\u0007 !\t\u0004\u000f\u0013%\u0007 \n\"\t\u000b\t\u0007!\u000f\u000b\t\u0007\n\f\f#\u000b\n\u0010\t\u0004%\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\t\u0013\u0007\u0015\u000b\u000f!\u0007!\t!\u000f\u000b%\u0007\u0010\u0002\n\u0012\u0007\u000f\u0010\u0002\t\u000b\u0006\u0016\u0007 )!\u0005\u000b\u000f\u0014\t!\t\u0012\u0010\u0007 \n\u0015\u0010\t\u000b\u0007 \u0006\u000f\u0012\u001f\u0007 \u0004\u0003\u0006\u0010\t\u0012\u0003\u0012\u001f\u0007 \"\n\u0006\u0007 \u0012\u000f\u0010\u0007 \u0015\u000f#\u0012\u0013\u0007 \u0015\u000f\u000b\u0007 \n\u0004\u0004\u0007 \u0005\u000b\u000f\u0005\t\u000b\u0010\u0003\t\u0006\u0007\n\u0012\u0013\u0007\u0013\u0003\u0015\u0015\t\u000b\t\u0013\u0007\n!\u000f\u0012\u001f\u0007\u0006\u000f\u0012\u001f\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0003\u0010%\u0007\n\u0012\u0013\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007 \u0010\u000b\n\u0003\u0012\u0003\u0012\u001f\u0016\u0007 \n)\u0010\u0007\"\n\u0006\u0007\t \u0005\t\f\u0010\t\u0013\u0007\u0010\u0002\n\u0010\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\n\u0010\u0007\u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0005\u0003\u0010\f\u0002\u0007\n \u0012\u0013\u0007\n\u0010\u0007\u0010\u0002\t\u0007 \n\f\u000f\u000b\u000b\t\f\u0010\u0007 \u0010\t!\u0005\u000f\u0007 \u0003\u0006\u0007 (#\u0003\u0010\t\u0007 \t\n\u0006%\u0016\u0007 \u001b\u0006\u0007 \u0015\u000f#\u0012\u0013\u0007 \u000e%\u0007 \r\t\u0014\u0003\u0010\u0003\u0012\u0007 59::\u00186\u001c\u0007 \r\t\u0014\u0003\u0010\u0003\u0012\u0007\n\u0012\u0013\u0007&\u000f\u000f+\u000759::.6\u001c\u0007\u0010\u0002\t\u000b\t\u0007\u0003\u0006\u0007\u0006\u000f!\t\u0007\t\u0014\u0003\u0013\t\u0012\f\t\u0007\u0010\u0002\n\u0010\u0007\u0005\t\u000f\u0005\u0004\t\u0007 \u0002\n\u0014\t\u0007\n\u0007\u0006\t\u0012\u0006\t\u0007\u000f\u0015\u0007\n\u000e\u0006\u000f\u0004#\u0010\t\u0007!\t!\u000f\u000b%\u0007\u0015\u000f\u000b\u0007\u0005\u0003\u0010\f\u0002\u0007\n\u0012\u0013\u0007\u0010\t!\u0005\u000f\u0007\u0015\u000f\u000b\u0007\u0010\u0002\t\u0003\u000b\u0007 \u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\t \u0007\u0005\u000f\u0005 -\u000b\u000f\f+\u0007\u0006\u000f\u0012\u001f\u0006\u0016\u0007 \n)\u0012\u0007\u0010\u0002\t\u0007\f#\u000b\u000b\t\u0012\u0010\u0007\t \u0005\t\u000b\u0003!\t\u0012\u0010\u001c\u0007\u0012\u000f\u0007\u0006#\u0005\u0005\u000f\u000b\u0010\u0007\u0015\u000f \u000b\u0007\n\u000e\u0006\u000f\u0004#\u0010\t\u0007!\t!\u000f\u000b%\u0007\u000f\u0015\u0007 \n\u0005\u0003\u0010\f\u0002\u0007\u0015\u000f\u000b\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\n\u0012\u0013\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\"\n\u0006\u0007\u0015\u000f#\u0012\u0013\u0016\u0007)\u0010\u0007\"\n\u0006\u0007 \u0015\u000f#\u0012\u0013\u0007\u0010\u0002\n\u0010\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\n\u0012\u0013\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0010\t\u0012\u0013\u0007\u0010\u000f\u0007\n\u0005\u0005\u000b\u000f\n\f\u0002\u0007\u0010\u0002\t\u0007 \f\u000f\u000b\u000b\t\f\u0010\u0007 \u0005\u0003\u0010\f\u0002\u0007 \u0015\u000b\u000f!\u0007 \u0010\u0002\t\u0007 \u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007 $\t\n\u0010\u0004\t\u0006\u0007 \u000b\t\f\u000f\u000b\u0013\u0003\u0012\u001f\u001c\u0007 \u000e#\u0010\u0007 \u000f\u0012\u0004%\u0007 \n\u0015\u0010\t\u000b\u0007 \u0002\n\u0014\u0003\u0012\u001f\u0007 \u0002\t\n\u000b\u0013\u0007 \u0010\u0002\t\u0007 \u000b\t\f\u000f\u000b\u0013\u0003\u0012\u001f\u0016\u0007 *\u000b\n\u0003\u0012\t\u0013\u0007 \u0006\u0003\u0012\u001f\t\u000b\u0006\u0007 \"\t \u000b\t\u0007 \n\u0006\u0003\u001f\u0012\u0003\u0015\u0003\f\n\u0012\u0010\u0004%\u0007\u000e\t\u0010\u0010\t\u000b\u0007\u0003\u0012\u0007\u0010#\u0012\u0003\u0012\u001f\u0007\u0010\u0002\t\u0003\u000b\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u0010\u000f\u0007\u0010\u0002\t\u0007\u0006\u000f\u0012\u001f\u0007\u000f\u0012\u0007\u0010\u0002\t\u0007 \n&'7\u0007\u0018.0\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0006\u0007\u000f\u0015\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\"\t\u000b\t\u0007\n\u0010\u0007\u0010\u0002\t\u0007 \n\f\u000f\u000b\u000b\t\f\u0010\u0007\u0005\u0003\u0010\f\u0002\u0007\"\u0002\t\u000b\t\n\u0006\u0007\u0003\u0010\u0007\"\n\u0006\u0007AB0\u0007\u0015\u000f\u000b\u0007\u0010\u0002\t\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0016\u0007 \n)\u0012\u0007\u0010\u0002\t\u0007\f#\u000b\u000b\t\u0012\u0010\u0007\t \u0005\t\u000b\u0003!\t\u0012\u0010\u001c\u0007\u0006#\u0005\u0005\u000f\u000b\u0010\u0007\u0015\u000f\u000b\u0007\n\u000e\u0006\u000f\u0004#\u0010\t\u0007!\t!\u000f\u000b%\u0007\u000f\u0015\u0007 \n\u0010\t!\u0005\u000f\u0007\u0015\u000f\u000b\u0007 \u0015\n!\u0003\u0004\u0003\n\u000b\u0007\n\u0012\u0013\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\"\n\u0006\u0007\u0003\u0012\u0013\t\t\u0013\u0007\u0015\u000f#\u0012\u0013\u001c\u0007 \n\u0003\u000b\u000b\t\u0006\u0005\t\f\u0010\u0003\u0014\t\u0007 \u000f\u0015\u0007 \u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007 \n\u000e\u0003\u0004\u0003\u0010%\u0016\u0007 *\u0002\u000f#\u001f\u0002\u0007 \u0010\u0002\t\u0007 \u0010\t!\u0005\u0003\u0007 \u000f\u0015\u0007 \u0004\t\u0006\u0006\u0007 \u0015\n!\u0003\u0004\u0003\n\u000b\u0007 !\t\u0004\u000f\u0013\u0003\t\u0006\u0007 \"\t\u000b\t\u0007 \u0006\u000f!\t\"\u0002\n\u0010\u0007 \u0004\t\u0006\u0006\u0007 \n\f\f#\u000b\n\u0010\t\u0004%\u0007 !\n\u0010\f\u0002\t\u0013\u0007 \n\"\u0003\u0010\u0002\u0007\u0010\u0002\t\u0007\u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0006\u001c\u0007\n\u0004!\u000f\u0006\u0010\u0007\u000f\u0012\t\u0007\u000f#\u0010\u0007\u000f\u0015\u0007\u0010\u0002\u000b\t\t\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0006\u0007\f\n!\t\u0007 \n\"\u0003\u0010\u0002\u0003\u0012\u0007 \u000f\u0012\t\u0007 C\u001d'\u0007 \u0015\u000f\u000b\u0007 \u0010\t!\u0005\u000f7\u0007 \n\u0004 !\u000f\u0006\u0010\u0007 \u0010\"\u000f\u0007 \u000f#\u0010\u0007 \u000f\u0015\u0007 \u0010\u0002\u000b\t\t\u0007 \f\n!\t\u0007 \n\"\u0003\u0010\u0002\u0003\u0012\u0007\u0010\"\u000f\u0007C\u001d'\u0006\u0016\u0007*\u0002\u0003\u0006\u0007\t\u0014\t\u0012\u0007\u0003!\u0005\u000b\u000f\u0014\t\u0013\u0007\n\u0015\u0010\t\u000b\u0007&'\u0007\u0004\u0003\u0006\u0010\t\u0012\u0003\u0012\u001f\u0016\u0007 )\u0010\u0007\u0003\u0006\u0007\f\u0004\t\n\u000b\u0007\u0010\u0002\n\u0010\u0007\u000e\u000f\u0010\u0002\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\n\u0012\u0013\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\n\u000b\t\u0007\n\"\n\u000b\t\u0007\u0010\u0002\n\u0010\u0007 \n\n\u0007\u0006\u000f\u0012\u001f\u0007\u0002\n\u0006\u0007\n\u0007\u0005\n\u000b\u0010\u0003\f#\u0004\n\u000b\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0005\u0003\u0010\f\u0002\u0007\n\u0012\u0013\u0007\n\u0007\u0005\n\u000b\u0010\u0003\f#\u0004\n\u000b\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007 \n\u0010\t!\u0005\u000f\u0016\u0007D\u0010\n\u000b\u0010\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\n\u0010\u0007\u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0005\u0003\u0010\f\u0002\u0007 \u0003\u0006\u0007\u0002\u000f\"\t\u0014\t\u000b\u0007\u0004\u0003+\t\u0004%\u0007\u0010\u000f\u0007\u000e\t\u0007 \n\u0002\n!\u0005\t\u000b\t\u0013\u0007\u000e%\u0007\n\u0007\u0004\n\f+\u0007\u000f\u0015\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\t \u0005\t\u000b\u0003\t\u0012\f\t\u0016\u0007)\u0010\u0007\u0003\u0006\u0007+\u0012\u000f\"\u0012\u0007\u0010\u0002\n\u0010\u0007 \u0005\t\u000f\u0005\u0004\t\u0007\u0002\n\u0014\t\u0007\n\u0007\u0005\u000b\t\u0015\t\u000b\u000b\t\u0013\u0007\u000f\f\u0010\n\u0014\t\u0007\u0003\u0012\u0007\"\u0002\u0003\f\u0002\u0007\u0010\u0002\t%\u0007\u0006\u0003\u0012\u001f\u0007\u000e\t\u0006\u0010\u0007\n\u0012\u0013\u0007\u0010\u0002\n\u0010\u0007 \u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\n\u000b\t\u0007\u000e\t\u0010\u0010\t\u000b\u0007\n\u000e\u0004\t\u0007\u0010\u000f\u0007#\u0006\t\u0007\u0010\u0002\t\u0003\u000b\u0007\u0015#\u0004\u0004\u0007\u0014\u000f\f\n\u0004\u0007\u000b\n\u0012\u001f\t\u0016\u0007)\u0015\u0007 \u0010\u0002\t\u0007\u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007\u000b\t\f\u000f\u000b\u0013\u0003\u0012\u001f\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0006\u000f\u0012\u001f\u0007\u0002\n\u0006\u0007\n\u0007\u0005\u0003 \u0010\f\u0002\u0007\u0010\u0002\n\u0010\u0007\u0002\n\u0005\u0005\t\u0012\u0006\u0007\u0010\u000f\u0007 \n\u000e\t\u0007 \u000f#\u0010\u0007 \u000f\u0015\u0007 \u0010\u0002\u0003\u0006\u0007 \u0014\u000f\f\n\u0004\u0007 \u000b\n\u0012\u001f\t\u001c\u0007 \u0005\t\u000f\u0005\u0004\t\u0007 \u0002\n\u0014\t\u0007 \u0010\u000f\u0007 \u0010\u000b\n\u0012\u0006\u0005\u000f\u0006\t\u0007 \u0010\u0002\t\u0003\u000b\u0007 \n\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u000f\u0012\t\u0007 \"\u0002\u000f\u0004\t\u0007\u000f\f\u0010\n\u0014\t\u0007\u0010\u000f\u0007 !\n\u0010\f\u0002\u0007\u0010\u0002\u0003\u0006\u0007\u0005\u0003\u0010\f\u0002\u0016\u0007*\u0002\u0003\u0006\u0007\u000f\f\u0010\n\u0014\t\u0007 \n\u0006\"\u0003\u0010\f\u0002\u0003\u0012\u001f\u0007\u000b\t(#\u0003\u000b\t\u0006\u0007\u0010\u000b\n\u0003\u0012\u0003\u0012\u001f\u0016\u0007\u0007 \n)\u0012\u0007\n\u0013\u0013\u0003\u0010\u0003\u000f\u0012\u001c\u0007\u0010\u0002\t\u0007\u0005\u0003\u0010\f\u0002\u0007\u000f\u0015\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007!\t\u0004\u000f\u0013\u0003\t\u0006\u0007\"\n\u0006\u0007!\u000f\u000b\t\u0007\t\n\u0006\u0003\u0004%\u0007 \n\n\u0013\u000f\u0005\u0010\t\u0013\u0007\u0010\u0002\n\u0012\u0007\u0010\u0002\n\u0010\u0007\u000f\u0015\u0007\u0004\t\u0006\u0006\u0007\u0015\n !\u0003\u0004\u0003\n\u000b\u0007\u000f\u0012\t\u0006\u0016\u0007D\u0003\u0012\f\t\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007 \n!\t\u0004\u000f\u0013\u0003\t\u0006\u0007 \"\t\u000b\t\u0007 \u000b\t\u0005\u000b\u000f\u0013#\f\t\u0013\u0007 \"\u000f\u000b\u0006\t\u0007 \u000f\u0012\u0007 \u000f\u0010\u0002\t\u000b\u0007 \n\u0006\u0005\t\f\u0010\u0006\u0007 \n\u0006\u0007 \"\t\u0004\u0004\u001c\u0007 !\n\u0010\f\u0002\u0003\u0012\u001f\u0007\u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0005\u0003\u0010\f\u0002\u0007\u0015\u000f\u000b\u0007\u0010\u0002\t\u0006\t\u0007!\t\u0004\u000f\u0013\u0003\t\u0006\u0007\"\n\u0006\u0007\u0005\u000b\u000f\u000e\n\u000e\u0004%\u0007 \n\u001f\u0003\u0014\t\u0012\u0007\n\u0007\u0004\u000f\"\t\u000b\u0007\u0005\u000b\u0003\u000f\u000b\u0003\u0010%\u0016 \n)\u0010\u0007 \"\n\u0006\u0007 \t \u0005\t\f\u0010\t\u0013\u0007 \u0010\u0002\n\u0010\u0007 \u0015\n!\u0003\u0004\u0003\n\u000b\u0007 !\t\u0004\u000f\u0013\u0003\t\u0006\u0007 \n\u000b\t\u0007 \u0006#\u0012\u001f\u0007 !\u000f\u000b\t\u0007 \n\n\f\f#\u000b\n\u0010\t\u0004%\u0007\u0010\u0002\n\u0012\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007 \u000f\u0012\t\u0006\u0007\n\u0010\u0007\u0010\u0002\t\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0007\u0004\t\u0014\t\u0004\u0016\u0007)\u0012\u0013\t\t\u0013\u001c\u0007 \n\u0010\u0002\t\u0007!\t!\u000f\u000b%\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007!\t\u0004\u000f\u0013\u0003\t\u0006\u0007\f\u000f\u0012\u0010\n\u0003\u0012\t\u0013\u0007 \u0005\u000b\u000f\u0005\u000f\u000b\u0010\u0003\u000f\u0012\n\u0004\u0004%\u0007\u0015\t\"\t\u000b\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0016\u0007\u0017\u000f\"\t\u0014\t\u000b\u001c\u0007\u000e\u000f\u0010\u0002\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007 \n\u0012\u0013\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0003!\u0005\u000b\u000f\u0014\t\u0013\u0007\u0010\u0002\t\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u000f\u0015\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007\n\u0015\u0010\t\u000b\u0007 \n\u0002\n\u0014\u0003\u0012\u001f\u0007 \u0002\t\n\u000b\u0013\u0007 \u0010\u0002\t\u0007 \u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007 \u0006\u000f\u0012\u001f\u0016\u0007 *\u0002\u0003\u0006 \u0007 \u0015\u0003\u0012\u0013\u0003\u0012\u001f\u0007 \f\u000f\u0012\u0015\u0003\u000b!\u0006\u0007 \u0010\u0002\t\u0007 \n\u0003\u0012\n\u0013\t(#\n\u0010\t\u0007\t\u0012\f\u000f\u0013\u0003\u0012\u001f\u0007\u000f\u0015\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0007\u0003\u0012\u0015\u000f\u000b!\n\u0010\u0003\u000f\u0012\u0007\u0015\u000f\u000b\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007 !\t\u0004\u000f\u0013\u0003\t\u0006\u0016\u0007 \n)\u0010\u0007\"\n\u0006\u0007\t \u0005\t\f\u0010\t\u0013\u0007\u0010\u0002\n\u0010\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0006\u0003\u0012\u001f\u0007\u0005\u000b\u000f\u0005\u000f\u000b\u0010\u0003\u000f\u0012\n\u0004\u0004%\u0007!\u000f\u000b\t\u0007 \n\f\u000f\u000b\u000b\t\f\u0010\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007\u0010\u0002\n\u0012\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0013\u000f\u001c\u0007\u0003\u0012\u0007\f\u000f\u0012\u0010\u000b\n\u000b%\u0007\u0010\u000f\u0007\u0010\u0002\t\u0007 \n\u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u0007\f\u000f\u0012\u0010\u000f#\u000b\u0006\u0016\u0007)\u0012\u0013\t\t\u0013\u001c\u0007\u0010 \u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0006\n\u0012\u001f\u0007\u001aA0\u0007\u000f\u0015\u0007 \n\u0010\u0002\t\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007\f\u000f\u000b\u000b\t\f\u0010\u0004%\u001c\u0007\"\u0002\u0003\u0004\t\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0006\n\u0012\u001f\u0007\u0019\u001a0\u0007\u000f\u0015\u0007 \u0010\u0002\t\u0007 \u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007 \f\u000f\u000b\u000b\t\f\u0010\u0004%\u0016\u0007 *\u0002\t\u0007 \f\u000f\u0012\u0010\u000f#\u000b\u0007 \u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0007 \"\n\u0006\u0007 \u0012\u000f\u0010\u0007 \n\u0015\u0015\t\f\u0010\t\u0013\u0007 \u000e%\u0007 \u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007 \u0010\u000b\n\u0003\u0012\u0003\u0012\u001f7\u0007 \n\u000e\u000f#\u0010\u0007 ;/0\u0007 \u000f\u0015\u0007 \u0010\u0002\t\u0007 \u0005\u0003\u0010\f\u0002\u0007 !\u000f\u0014\t!\t\u0012\u0010\u0006\u0007\"\t\u000b\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0004%\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\t\u0013\u0016\u0007*\u0002\u0003\u0006\u0007\u0015\u0003\u0012\u0013\u0003\u0012\u001f\u0007\f\u000f\u0012\u0015\u0003\u000b! \u0006\u0007 \n\u0010\u0002\t\u0007\u0013\u000f!\u0003\u0012\n\u0012\f\t\u0007\u000f\u0015\u0007\f\u000f\u0012\u0010\u000f#\u000b\u0007\u0003\u0012\u0007\u0010\u0002\t\u0007\u000b\t!\t!\u000e\t\u000b\u0003\u0012\u001f\u0007\n\u0012\u0013\u001c\u0007\u0003\u0012\u0007\u0010#\u000b\u0012\u001c\u0007\u0010\u0002\t\u0007 \n\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u000f\u0015\u0007!\t\u0004\u000f\u0013\u0003\t\u0006\u0007\u0015\u000f\u000b\u0007\u000e\u000f\u0010\u0002\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\n\u0012\u0013\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0016\u0007 \n*\u0002\t\u0007\u000e\n\u0013\u0007\u0005\t\u000b\u0015\u000f\u000b!\n\u0012\f\t\u0007\u000f\u0012\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007\u000e%\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007 \u0006\u0003\u0012\u001f\t\u000b\u0006\u0007!\n%\u0007\u000e\t\u0007\u0013#\t\u0007\u0010\u000f\u0007\u0010\u0002\t\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007\u000f\u0015\u0007!\t\u0004\u000f\u0013\u0003\t\u0006\u0007\u000e\t\u0003\u0012\u001f\u0007\u000e\n\u0013\u0004%\u0007 \t\u0012\f\u000f\u0013\t\u0013\u0007\u0003\u0012 \u0007!\t!\u000f\u000b%\u0007\u000f\u000b\u0007\u0010\u000f\u0007\n\u0007\u0004\n\f+\u0007\u000f\u0015\u0007\t\u0006\u0006\t\u0012\u0010\u0003\n\u0004\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u0006+\u0003\u0004\u0004\u0006\u0016\u0007 \n)\u0010\u0007\"\n\u0006\u0007\t \u0005\t\f\u0010\t\u0013\u0007\u0010\u0002\n\u0010\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0003!\u0005\u000b\u000f\u0014\t\u0007\u0010\u0002\t\u0003\u000b\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\n\u0015\u0010\t\u000b\u0007 \n\u0002\n\u0014\u0003\u0012\u001f\u0007\u0004\u0003\u0006\u0010\t\u0012\t\u0013\u0007\u0010\u000f\u0007\u0010\u0002\t\u0007\u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007\u0006\u000f\u0012\u001f\u001c\u0007\u0003\u0012\u0007\f\u000f\u0012\u0010\u000b\n\u0006\u0010\u0007\u0010\u000f\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007 \n\u0006\u0003\u0012\u001f\t\u000b\u0006\u0016\u0007\u001b\u0006\u0007\u0015\u000f#\u0012\u0013\u0007\u000e%\u0007'\n\u0014\u0003\u0013\u0006\u000f\u0012\u000759::\u00186\u001c\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\n\u000b\t\u0007 \n\u0004\t\u0006\u0006\u0007 \n\u000e\u0004\t\u0007 \u0010\u000f\u0007 \u000b\t\u0015\u0004 \t\f\u0010\u0007 \u000f\u0012\u0007 \n\u0012\u0013\u0007 \u0003!\u0005\u000b\u000f\u0014\t\u0007 \u0010\u0002\t\u0003\u000b\u0007 \u000f\"\u0012\u0007 \u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007 \n\u0005\t\u000b\u0015\u000f\u000b!\n\u0012\f\t\u0016\u0007 *\u0002\u0003\u0006\u0007 \u0002%\u0005\u000f\u0010\u0002\t\u0006\u0003\u0006\u0007 \u0002\n\u0006\u0007 \u0010\u000f\u0007 \u000e\t\u0007 \u000b\t>\t\f\u0010\t\u0013\u001c\u0007 \u0002\u000f\"\t\u0014\t\u000b\u001c\u0007 \f\u000f\u0012\u0006\u0003\u0013\t\u000b\u0003\u0012\u001f\u0007\f#\u000b\u000b\t\u0012\u0010\u0007\u000b\t\u0006#\u0004\u0010\u0006\u0016\u0007$\u000f\u0010\u0002\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\n\u0012\u0013\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007 \u0003!\u0005\u000b\u000f\u0014\t\u0013\u0007\u0010\u0002\t\u0003\u000b\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u000f\u0015\u0007\u0004\t\u0006\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0007\u000f\u0012\u0007!\n\u0012%\u0007\n\u0006\u0005\t\f\u0010\u0006\u001c\u0007 \n\n\u0015\u0010\t\u000b\u0007\u0002\n\u0014\u0003\u0012\u001f\u0007\u0004\u0003\u0006\u0010\t\u0012\t\u0013\u0007\u0010\u000f\u0007\u0010\u0002\t\u0007&'\u0007\u000b\t \f\u000f\u000b\u0013\u0003\u0012\u001f\u0016\u0007*\u000f\u0007\u0010\u0002\t\u0007\t \u0010\t\u0012\u0010\u0007\u0010\u0002\n\u0010\u0007 \n#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0010\u000b#\u0004%\u0007\u000b\t\u0015\u0004\t\f\u0010\t\u0013\u0007\u000f\u0012\u0007\u0010\u0002\t\u0003\u000b\u0007\u000f\"\u0012\u0007\u0005\t\u000b\u0015\u000f\u000b!\n\u0012\f\t\u0007\u000f\u000b\u0007 \u0010\u0002\n\u0010\u0007\u0010\u0002\t%\u0007\u0006\u0003!\u0005\u0004%\u0007\u0010\u000f\u000f+\u0007\u000f\u0014\t\u000b\u0007\u0010\u0002\t\u0007!\t\u0004\u000f\u0013%\u0007\u0003\u0012\u0007\u0010\u0002\t\u0007\u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007\u0006\u000f\u0012\u001f\u0007 \u0012\t\t\u0013\u0006\u0007\u0015#\u000b\u0010\u0002\t\u000b\u0007\u0006\f\u000b#\u0010\u0003\u0012%\u0016\u0007 \n*\u0002\t\u0007 \f#\u000b\u000b\t\u0012\u0010\u0007 \u0006\u0010#\u0013%\u0007 \u0013\u0003\u0013\u0007 \u0012\u000f\u0010\u0007 \n\u0013\u0013\u000b\t\u0006\u0006\u0007 \u0010\u0002\t\u0007 \u000e\t\n#\u0010%\u0007 \u000f\u000b\u0007 \n\u0005\u000b\u000f\u0015\t\u0006\u0006\u0003\u000f\u0012\n\u0004\u0003\u0006!\u0007\u000f\u0015\u0007\n\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u0005\t\u000b\u0015\u000f \u000b!\n\u0012\f\t\u0007\u000f\u000b\u0007\u0010\u0002\t\u0007\u0006\u0005\t\f\u0003\n\u0004\u0007\u0004\u0003+\u0003\u0012\u001f\u0007 \n\u000f\u000b\u0007\"\u0003\u0004\u0004\u0003\u0012\u001f\u0012\t\u0006\u0006\u0007\u0010\u000f\u0007\u0006\u0003\u0012\u001f7\u0007\u0003\u0010\u0007\u000f\u0012\u0004%\u0007\n\u0013\u0013\u000b\t\u0006\u0006\t\u0013\u0007\u0010\u0002\t\u0007\n\f\f#\u000b\n\f%\u0007\u0003\u0012\u0007\"\u0002\u0003\f\u0002\u0007 \n\u000f\u0012\t\u0007\u0006\u0003\u0012\u001f\u0006\u0016\u0007,\u0003\u0010\u0002\u0007\u000b\t\u0006\u0005\t\f\u0010\u0007\u0010\u000f\u0007\u0010\u0002\t\u0007\n\t\u0006\u0010\u0002\t\u0010\u0003\f\u0006\u0007\u000f\u0015\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u001c\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007 \u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\n\u000b\t\u0007\n\u0006\u0006#!\t\u0013\u0007\u0010\u000f\u0007\u0002\n\u0014\t\u0007\n\u0007\u000e\t\u0010\u0010\t\u000b\u0007\f\u000f\u0012\u0010\u000b\u000f\u0004\u0007\u000f\u0015\u0007\u0010\u0003!\u0003\u0012\u001f\u001c\u0007\u000e\u000b\t\n\u0010\u0002\u001c\u0007 \u0014\u0003\u000e\u000b\n\u0010\u000f\u001c\u0007\t\u0012#\u0012\f\u0003\n\u0010\u0003\u000f\u0012\u0007\n\u0012\u0013\u0007\n\u0007!\u000f\u000b\t\u0007\u0005\u000b\u000f\u0012\u000f #\u0012\f\t\u0013\u0007(#\n\u0004\u0003\u0010%\u0007\u000f\u0015\u0007\u0010\u0002\t\u0003\u000b\u0007 \n\u0014\u000f\u0003\f\t\u001c\u0007 \n!\u000f\u0012\u001f\u0006\u0010\u0007 \u000f\u0010\u0002\t\u000b\u0007 \u0010\u0002\u0003\u0012\u001f\u0006\u0016\u0007 *\u0002\t\u0007 \u0004\u000f\u0014\t\u0007 \u0015\u000f\u000b\u0007 \u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007 \u0003\u0006\u0007 \u000e\t\u0006\u0010\u0007 \u0013\t!\u000f\u0012\u0006\u0010\u000b\n\u0010\t\u0013\u0007\u000e%\u0007\u0010\u0002\t\u0007\u000f\u000e\u0006\t\u000b\u0014\n\u0010\u0003\u000f\u0012\u0007\u0010\u0002\n\u0010\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0006\n\u0012\u001f\u0007\n\u0007\u0015\n\u000b\u0007 \n\u0004\u000f\u0012\u001f\t\u000b\u0007\u0005\n\u0006\u0006\n\u001f\t\u0007\u000f\u0015\u0007\n\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u0007\u0010\u0002\n\u0012\u0007#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0013\u0003\u0013 \n\u0018\u0016\u0007 \n\u0018*\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0006\n\u0012\u001f\u001c\u0007\u000f\u0012\u0007 \n\u0014\t\u000b\n\u001f\t\u001c\u0007\u0018\u0019\u0016\u0018\u0007\u0012\u000f\u0010\t\u0006\u0007\u000f\u0015\u0007\n\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u0006\u000f\u0012\u001f\u001c\u0007\"\u0002\t\u000b\t\n\u0006\u0007 \n#\u0012\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0006\n\u0012\u001f\u001c\u0007\u000f\u0012\u0007\n\u0014\t\u000b\n\u001f\t\u001c\u0007A.\u0016.\u0007\u0012\u000f\u0010\t\u0006\u0016\u0007\u0007*\u0002\u0003\u0006\u0007\u0013\u0003\u0015\u0015\t\u000b\t\u0012\f\t\u0007\"\n\u0006\u0007\u0015\u000f#\u0012\u0013\u0007\u0010\u000f\u0007 \n\u000e\t\u0007\u0006\u0003\u001f\u0012\u0003\u0015\u0003\f\n\u0012\u0010\u00075\u001e59\u001c9\u001a6\u0007?\u00079\u001a\u00169\u001a\u001c\u0007\u0005\u0007L\u0007/\u0016//\u00196\u0016 D\u000f!\t\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\"\t\u000b\t\u0007\t\u0014\t\u0012\u0007!\u000f\u000b\t\u0007\u0010\u0002\n\u0012\u0007\u0002\n\u0005\u0005%\u0007\u0010\u000f\u0007\u0006\u0003\u0012\u001f\u0007\u0010 \u0002\t\u0007\f\u000f!\u0005\u0004\t\u0010\t\u0007 \n\u0006\u000f\u0012\u001f\u0007\u0010\u0002\u000b\t\t\u0007\u0010\u0003!\t\u0006\u0016 \n*\u0002\t\u0007\f#\u000b\u000b\t\u0012\u0010\u0007\u0006\u0010#\u0013%\u0007\u0013\u0003\u0013\u0007\u0012\u000f\u0010\u0007\n\u0006\u0006\t\u0006\u0006\u0007\u0010\u0002\t\u0007\t\u0015\u0015\t\f\u0010\u0006\u0007\u000f\u0015\u0007\u0006\u000f\u0012\u001f\u0007\f\u000f!\u0005\u0004\t \u0003\u0010%\u0007 \n\u000f\u000b\u0007!#\u0006\u0003\f\u0007\u0003\u0013\u0003\u000f!\n\u0010\u0003\f\u0007\u0013\u0003\u0015\u0015\t\u000b\t\u0012\f\t\u0006\u00075\t\u0016\u001f\u0016\u001c\u0007\u001f\t\u0012\u000b\t\u001c\u0007\u0006\u0010%\u0004\u0003\u0006\u0010\u0003\f\u0007\u0005\t\u000b\u0003\u000f\u0013\u001c\u0007 \u0003\u0012\u0006\u0010\u000b#!\t\u0012\u0010\n\u0010\u0003\u000f\u00126\u0016\u0007*\u0002\t\u0007$\t\n\u0010\u0004\t\u00062\u0007\u0006\u000f\u0012\u001f\u0006\u0007#\u0006\t\u0013\u0007\u0003\u0012\u0007\u0010\u0002\t\u0007\f#\u000b\u000b\t\u0012\u0010\u0007\u0006\u0010#\u0013%\u0007 \n\u000b\t\u0007\u0006\u0003!\u0005\u0004\t7\u0007\u0012\u000f\u0012\t\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0006 \u0007!\n\u0013\t\u0007\n\u0012%\u0007\f\u000f!!\t\u0012\u0010\u0007\u000f\u0012\u0007\u0010\u0002\t\u0007 \n\u0006\u000f\u0012\u001f\u0006\u0007\n\u0006\u0007\u000e\t\u0003\u0012\u001f\u0007\u0010\u000f\u000f\u0007\f\u000f!\u0005\u0004\t \u0007\u0010\u000f\u0007\u0006\u0003\u0012\u001f\u0016\u0007)\u0010\u0007!\n%\u0007\u000e\t\u0007\t\u0014\u0003\u0013\t\u0012\u0010\u0007\u0010\u0002\n\u0010\u0007 \u0006\u000f!\t\u0007\u0006\u000f\u0012\u001f\u0006\u0007\u0003\u0012\u0007\u0010\u0002\t\u0007\"\u000f\u000b\u0004\u0013\u0007\n\u000b\t\u0007!\u000f\u000b\t\u0007\u0013\u0003\u0015\u0015\u0003\f#\u0004\u0010\u0007\u0010\u000f\u0007\u000b\t!\t!\u000e\t\u000b\u0007\n\u0012\u0013\u0007\u0010\u000f\u0007 \n\u000b\t\u0005\u000b\u000f\u0013#\f\t\u0007\u0010\u0002\n\u0012\u0007\u000f\u0010\u0002\t\u000b\u0006\u0007\u0013#\t\u0007\u0010\u000f\u0007!\t\u0004\u000f\u0013\u0003\f\u001c\u0007!\t\u0010\u000b\u0003\f\n\u0004\u0007\n\u0012\u0013\u0007\u000b\u0002%\u0010\u0002!\u0003\f\n\u0004\u0007 \n\f\u000f!\u0005\u0004\t \u0003\u0010\u0003\t\u0006\u0016 \n\u001d\u000f\u0007\u000b\t\u0004\u0003\n\u000e\u0004\t\u0007\u0006\u0010\n\u0010\u0003\u0006\u0010\u0003\f\u0006\u0007\"\n\u0006\u0007\u000f\u000e\u0010\n\u0003\u0012\t \u0013\u0007\u000f\u0012\u0007\"\u0002\n\u0010\u0007\u0005\n\u000b\u0010\u0006\u0007\u000f\u0015\u0007\n\u0007!\t\u0004\u000f\u0013%\u0007 \n\n\u000b\t\u0007 !\u000f\u0006\u0010\u0007 \u0004\u0003+\t\u0004%\u0007 \u0010\u000f\u0007 \u001f\t\u0010\u0007 \u000b\t\f\n\u0004\u0004\t\u0013\u0007 \n\u0012\u0013\u0007 \u0006#\u0012\u001f\u0016\u0007 *\u0002\u0003\u0006\u0007 \t \u0005\t\u000b\u0003!\t\u0012\u0010\u0007 \u0006\u0002\u000f\"\t\u0013\u0007\u0010\u0002\n\u0010\u0007\u0004%\u000b\u0003\f\u0006\u0007\n\u0012\u0013\u0007!\t\u0004\u000f\u0013%\u0007\n\u000b\t\u0007\f\u000f\u0012\u0006\u0003\u0013\t\u000b\n\u000e\u0004%\u0007\u0010\u0003\t\u0013\u00075D\t\u000b\n\u0015\u0003\u0012\t\u001c\u0007 '\n\u0014\u0003\u0013\u0006\u000f\u0012\u001c\u0007&\u000b\u000f\"\u0013\t\u000b\u0007\n\u0012\u0013\u0007\b\t\u0005\u0005\u001c\u00079:;\u001a67\u0007\u0005\n\u000b\u0010\u0003\f\u0003\u0005\n\u0012\u0010\u0006\u0007\"\t\u000b\t\u0007\f#\t\u0013\u0007 \n\u000e%\u0007\u0010\u0002\t\u0007\u0006\u000f\u0012\u001f\u0007\u0010\u0003\u0010\u0004\t\u0007\u000f\u0012\u0007\u0010\u0002\t\u0007\f\n\u000b\u0013\u0007\u0010\u000f\u0007\u000b\t\f\n\u0004\u0004\u0007\u0010\u0002\t\u0007\u0005\n\u0006\u0006\n \u001f\t\u0007\u0010\u000f\u0007\u0006\u0003\u0012\u001f\u0016\u0007 \n&\u000f\u0012\u0006\t(#\t\u0012\u0010\u0004%\u001c\u0007 \u0010\u0002\t\u0007 !\t\u0004\u000f\u0013%\u0007 \u000f\u0015\u0007 \n\u0004!\u000f\u0006\u0010\u0007 \n\u0004\u0004\u0007 .A\u0007 \u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0006\u001c\u0007 \t \f\t\u0005\u0010\u0007\u0015\u000f\u000b\u0007\u0015\u000f#\u000b\u00071\r\t\u0010\u0007\u0003\u0010\u0007\u000e\t2\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0006\u001c\u0007\u0002\n\u0013\u0007\u0010\u0002\t\u0007\u0006\u000f\u0012\u001f\u0007\u0010\u0003\u0010\u0004\t\u0007\n\u0006\u0007 \u000f\u0012\t\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0014\t\u000b%\u0007\u0015\u0003\u000b\u0006\u0010\u0007\"\u000f\u000b\u0013\u0006\u0007\u0003\u0012\u0007\u0003\u0010\u0006\u0007\u0004%\u000b\u0003\f\u0006\u0016\u0007 \n!\u001d\f\u0014\u000e\r\u0004\u000b\u0005\r\b\t\u0006\u0007\u0002\b\u000f\u0007()\u0015\u0003\u000f\u0010\u0007\u001b\u0010\u0007\u0016\u0015\f\f\r\t\n* \n&#\u000b\u000b\t\u0012\u0010\u0007\u0015\u0003\u0012\u0013\u0003\u0012\u001f\u0006\u0007\u0003!\u0005\u000f\u0006\t\u0007\f\u000f\u0012\u0006\u0010\u000b\n\u0003\u0012\u0010\u0006\u0007\u000f\u0012\u0007\u0006\t\n \u000b\f\u0002\u0007\n\u0004\u001f\u000f\u000b\u0003\u0010\u0002!\u0006\u0007\u0015\u000f\u000b\u0007 \n1(#\t\u000b%\u0007 \u000e%\u0007 \u0002#!!\u0003\u0012\u001f2\u0007 \u001f\u0003\u0014\t\u0012\u0007 \u0010\u0002\t\u0007 \u0006\u000f\u0012\u001f\u0007 #\u0012\u0015\n!\u0003\u0004\u0003\n\u000b\u0003\u0010%\u0007 \n\u0012\u0013\u0007 \u0010\u0002\t\u0007 \n\u0013\u0003\u0014\t\u000b\u0006\u0003\u0010%\u0007\u000f\u0015\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\n\u000e\u0003\u0004\u0003\u0010\u0003\t\u0006\u0007\n\u0012\u0013\u0007\u0010\u000b\n\u0003\u0012\u0003\u0012\u001f\u0007\n!\u000f\u0012\u001f\u0006\u0010\u0007\u0010\u0002\t\u0007\u001f\t\u0012\t\u000b\n\u0004\u0007 \n\u0005#\u000e\u0004\u0003\f\u0016\u0007\u001d\u000f\"\n\u0013\n%\u0006\u001c\u0007!\u000f\u0006\u0010\u0007\u0006\t\n\u000b\f\u0002\u0007\n\u0004\u001f\u000f\u000b\u0003\u0010\u0002!\u0006\u0007\n\u000b\t\u0007\u000e\t\u0006\u0010\u0007\u0013\t\u0006\f\u000b\u0003\u000e\t\u0013\u0007 \n\u0006\u0007\u000f\u0005\u0010\u0003!\n\u0004\u0007\u0006\u000f\u0004\u0014\t\u000b\u0006\u0007\u000f\u0015\u0007\n\u0007\u0005\u000b\t\f\u0003\u0006\t\u0004%\u0007\u0006\u0010\n\u0010\t\u0013\u0007!\n\u0010\u0002\t!\n\u0010\u0003\f\n\u0004\u0007\u0005\u000b\u000f \u000e\u0004\t!\u0007 \n\u0003\u0012\u0007\n\u0007\u0013\u0003\u0006\f\u000b\t\u0010\t\u0007\u0013\u000f!\n\u0003\u0012\u0016\u0007*\u0002\t\u0007\u001f\t\u0012\t\u000b\n\u0004\u0007\u0005\u000b\u000f\u000e\u0004\t!\u0007\u0003\u0006\u0007\u0006\u0010\n\u0010\t\u0013\u0007\n\u0006\u0007\u0015\u0003\u0012\u0013\u0003\u0012\u001f\u0007 \n\u0012\u0007 \u000f\u0005\u0010\u0003!\n\u0004\u0007 \n\u0004\u0003\u001f\u0012!\t\u0012\u0010\u0007 \u000e\t\u0010\"\t\t\u0012\u0007 \u0010\u0002\t\u0007 \u0005\u0003\u0010\f\u0002\t\u0006\u0007 \n\u0012\u0013\u0007 \u0010\u0002\t\u0007 \u0012\u000f\u0010\t\u0007 \n\u0013#\u000b\n\u0010\u0003\u000f\u0012\u0006\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0006#\u0012\u001f\u0007!\t\u0004\u000f\u0013%\u0007\n\u0012\u0013\u0007\u0010\u0002\t\u0007!\t\u0004\u000f\u0013\u0003\t\u0006\u0007\u0003\u0012\u0007\u0010\u0002\t\u0007\u0013\n\u0010\n\u000e\n\u0006\t\u0016\u0007 \n,\u0002\u0003\u0004\t\u0007 \f\u000f!\u0005#\u0010\u0003\u0012\u001f\u0007 \u0010\u0002\u0003\u0006\u0007 \n\u0004\u0003\u001f\u0012!\t\u0012\u0010\u001c\u0007 \u0010%\u0005\u0003\f\n\u0004\u0007 \u0002#!\n\u0012\u0007 \t\u000b\u000b\u000f\u000b\u0006\u0007 \u0003\u0012\u0007 \u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\n\u000b\t\u0007\u0010\n+ \t\u0012\u0007\u0003\u0012\u0010\u000f\u0007\n\f\f\u000f#\u0012\u0010 \n\u0019\u0016\n\u001e\u0003\u000b\u0006\u0010\u001c\u0007\n\u0006\u0007\u0005\t\u000f\u0005\u0004\t\u0007\n\u000b\t\u0007#\u0012\u0004\u0003+\t\u0004%\u0007\u0010\u000f\u0007\u0006\u0010\n\u000b\u0010\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\n\u0010\u0007\u0010\u0002\t\u0007\f\u000f\u000b\u000b\t\f\u0010\u0007 \n\u0005\u0003\u0010\f\u0002\u001c\u0007 \u0010\u0002\t\u0007 #\u0006\t\u0007 \u000f\u0015\u0007 \n\u0007 \u0010\u000b\n\u0012\u0006\u0005\u000f\u0006\u0003\u0010\u0003\u000f\u0012\u0007 \u0003\u0012\u0014\n\u000b\u0003\n\u0012\u0010\u0007 !\t\u0004\u000f\u0013%\u0007 \u000b\t\u0005\u000b\t\u0006\t\u0012\u0010\n\u0010\u0003\u000f\u0012\u0007\u0003\u0006\u0007\n\u0007\u0005\u000b\t\u000b\t(#\u0003\u0006\u0003\u0010\t\u0007\u0015\u000f\u000b\u0007\u0006\t\n\u000b\f\u0002\u0007\t\u0015\u0015\u0003\f\u0003\t\u0012\f%\u0016\u0007*\u0002\t\u0007#\u0006\t\u0007 \n\u000f\u0015\u0007 \n\u0007 \f\u000f\u0012\u0010\u000f#\u000b\u0007 \u000f\u000b\u0007 \u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0007 \u000b\t\u0005\u000b\t\u0006\t\u0012\u0010\n\u0010\u0003\u000f\u0012\u0007 \u0003\u0006\u0007 \n\u0004\u000b\t\n\u0013%\u0007 \f\u000f!!\u000f\u0012\u0007 \n\u0005\u000b\n \f\u0010\u0003\f\t\u0016\u0007 \nD\t\f\u000f\u0012\u0013\u001c\u0007\u0010\u0002\t\u0007\t \u0010\t\u0012\u0010\u0007\u0010\u000f\u0007\"\u0002\u0003\f\u0002\u0007\n\u0007\u0005\t\u000b\u0006\u000f\u0012\u0007\u0003\u0006\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\"\u0003\u0010\u0002\u0007\n\u0007\u0006\u000f\u0012\u001f\u0007 \n\n\u0012\u0013\u0007\u0010\u0002\t\u0007\t \u0010\t\u0012\u0010\u0007\u0010\u000f\u0007\"\u0002\u0003\f\u0002\u0007\n\u0007\u0005\t\u000b\u0006\u000f\u0012\u0007\u0003\u0006\u0007\n\u0007\u001f\u000f\u000f\u0013\u0007\u000f\u000b\u0007\n\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0007 \n\u000b\t\u0007 \f\u000b#\f\u0003\n\u0004\u0007 \u0010\u000f\u0007 \u0002\u000f\"\u0007 \"\t\u0004\u0004\u0007 \n\u0007 !\t\u0004\u000f\u0013%\u0007 \"\u0003\u0004\u0004\u0007 \u000e\t\u0007 \u000b\t\u0005\u000b\u000f\u0013#\f\t\u0013\u0016\u0007 \u0007 \u001b\u0007 \f\u000f\u0012\u0010\u000f#\u000b\u0007 \u0003\u0006\u0007 \n\u0007 \u0013\u000f!\u0003\u0012\n\u0012\u0010\u0007 \u0015\t\n\u0010#\u000b\t\u0007 \u0010\u0002\n\u0010\u0007 \n\u0003\u0013\u0006\u0007 \u0003\u0012\u0007 !\t!\u000f\u000b\u00034\u0003\u0012\u001f\u0007 \n\u0007 \n!\t\u0004\u000f\u0013%\u0007 \n\u0012\u0013\u0007 \u0003 \u0012\u0007 \u000b\t\u0005\u000b\u000f\u0013#\f\u0003\u0012\u001f\u0007 \u0003\u0010\u0007 \u0013\u0003\u000b\t\f\u0010\u0004%\u0007 \u0015\u000b\u000f!\u0007 !\t!\u000f\u000b%\u001c\u0007 \n\u0003\u000b\u000b\t\u0006\u0005\t\f\u0010\u0003\u0014\t\u0007\u000f\u0015\u0007\u0002\u000f\"\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\u000f\u0012\t\u0007\u0003\u0006\u0007\"\u0003\u0010\u0002\u0007\u0010\u0002\t\u0007\u0006\u000f\u0012\u001f\u0007\u000f\u000b\u0007\u0002\u000f\"\u0007\u001f\u000f\u000f\u0013\u0007 \u000f\u0012\t\u0007 \u0003\u0006\u0007 \u0003\u0012\u0007 \u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0016\u0007 )\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007 \u000f\u0015\u0007 \n\u0007 !\t\u0004\u000f\u0013%\u0007 \n\u000b\t\u0007 !\u000f\u000b\t\u0007 \u000b\t\n\u0013\u0003\u0004%\u0007 \n\u0014\n\u0003\u0004\n\u000e\u0004\t\u0007\u0010\u000f\u0007\u0010\u000b\n\u0003\u0012\t\u0013\u0007\u0006\u0003\u0012\u001f\t\u000b\u0006\u0007\u0015\u000f\u000b\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\u0010\u0003\u000f\u0012\u0007\n\u0012\u0013\u0007\u0003\u0015\u0007\u0006\u000f\u0012\u001f\u0006\u0007\n\u000b\t\u0007 !\u000f\u000b\t\u0007 \u0015\n!\u0003\u0004\u0003\n\u000b\u0016\u0007 *\u0002\u000f#\u001f\u0002\u0007 \n\u0007 \f\u000f\u0012\u0010\u000f#\u000b\u0007 \u000f\u0012 \u0004%\u0007 \u0005\u000b\u000f\u0014\u0003\u0013\t\u0006\u0007 \u0004\u0003!\u0003\u0010\t\u0013\u0007 \n\u0003\u0012\u0015\u000f\u000b!\n\u0010\u0003\u000f\u0012\u0007\n\u000e\u000f#\u0010\u0007\n\u0007!\t\u0004\u000f\u0013%\u001c\u0007\u0006\t\n\u000b\f\u0002\u0003\u0012\u001f\u0007\u0015\u000f\u000b\u0007\u0010\u0002\t\u0007\u0005\u0003\u0010\f\u0002\t\u0006\u0007\u0006\u0002\u000f#\u0004\u0013\u0007 \n\u000e\t\u0007\u000e\n\u0006\t\u0013\u0007\u000f\u0012\u0007\n\u0007\"\t\u0003\u001f\u0002\u0010\t\u0013\u0007\f\u000f!\u000e\u0003\u0012\n\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u0007\f\u000f\u0012\u0010\u000f#\u000b\u0006\u0007\n\u0012\u0013\u0007\u0003\u0012\u0010\t\u000b\u0014\n\u0004\u0006\u0007 \u0013\t\u0005\t\u0012\u0013\t\u0012\u0010\u0007\u000f\u0012\u0007\u0002\u000f\"\u0007\"\t\u0004\u0004\u0007\n\u0007\u0005\t\u000b\u0006\u000f\u0012\u0007\f\n\u0012\u0007\u0006\u0003\u0012\u001f\u0007\n\u0012\u0013\u0007\u0002\u000f\"\u0007\u0015\n!\u0003\u0004\u0003\n\u000b\u0007\n\u0007 \u0005\t\u000b\u0006\u000f\u0012\u0007\u0003\u0006\u0007\"\u0003\u0010\u0002\u0007\u0010\u0002\t\u0007\u0006\u000f\u0012\u001f\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0006#\u0012\u001f\u0007!\t\u0004\u000f\u0013%\u0016\u0007 \n*\u0002\u0003\u000b\u0013\u001c\u0007 \u0005\t\u000f \u0005\u0004\t\u0007 \u0002\n\u0014\t\u0007 \n\u0007 \u001f\u000f\u000f\u0013\u0007 !\t!\u000f\u000b%\u0007 \u0015\u000f\u000b\u0007 \u001f\u0004\u000f\u000e\n\u0004\u0007 \u0010\t!\u0005\u000f\u0016\u0007 \n\u0017\u000f\"\t\u0014\t\u000b\u001c\u0007 \u0010\u000b\n\u0003\u0012\t\u0013\u0007 \u0006\u0003\u0012\u001f\t\u000b\u0006\u0007 \n\u000b\t\u0007 \u0013\t\t!\t\u0013\u0007 \u0010\u000f\u0007 \t \u0005\u0004\u0003\f\u0003\u0010\u0004%\u0007 #\u0006\t\u0007 \u0010\u0002\t\u0007 \n!\t\u0010\u000b\u0003\f\n\u0004\u0007\u0006\u0010\u000b#\f\u0010#\u000b\t\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0006\u000f\u0012\u001f\u0007\u0003\u0012\u0007\u0010\u0002\t\u0003\u000b\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u000b\t\u0006#\u0004\u0010\u0003\u0012\u001f\u0007\u0003\u0012\u0007\n\u0007 \n\u000e\t\u0010\u0010\t\u000b\u0007#\u0006\t\u0007\u000f\u0015\u0007\u0010\u0003!\u0003\u0012\u001f\u0007\n\u0012\u0013\u0007\u0006!\u000f\u000f\u0010\u0002\t\u000b\u0007\u0004\u000f\f\n\u0004\u0007\u0010\u0003!\t\u0007\u0015\u0004#\f\u0010#\n\u0010\u0003\u000f\u0012\u0006\u0016\u0007*\u0002\t\u0007 \t \u0010\t\u0012\u0010\u0007\u0010\u000f\u0007\"\u0002\u0003\f\u0002\u0007\u0010\u0002\t\u0007\t\u000b\u000b\u000f\u000b\u0006\u0007\u0003\u0012\u0007\u0012 \u000f\u0010\t\u0007\u0013#\u000b\n\u0010\u0003\u000f\u0012\u0006\u00075\u000f\u000b\u0007\u0010\u0002\t\u0003\u000b\u0007\u0013#\u000b\n\u0010\u0003\u000f\u0012\n\u0004\u0007 \n\u0019\u0007)\u0012\u0007\n\u0013\u0013\u0003\u0010\u0003\u000f\u0012\u0007\u0010\u000f\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\t\u000b\u000b\u000f\u000b\u0006\u001c\u0007\u0010\u0002\t\u0007\u0015\u000b\u000f\u0012\u0010 -\t\u0012\u0013\u0007\u000f\u0015\u0007\n\u0012%\u00071(#\t\u000b%\u0007\u000e%\u0007\u0002#!!\u0003\u0012\u001f2\u0007 \n\n\u0005\u0005\u0004\u0003\f\n\u0010\u0003\u000f\u0012 \u0007\u0010\u0002\n\u0010\u0007\u0010\u000b\n\u0012\u0006\f\u000b\u0003\u000e\t\u0006\u0007\u0010\u0002\t\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007\u0003\u0012\u0010\u000f\u0007\n\u0007\u0006%!\u000e\u000f\u0004\u0003\f\u0007(#\t\u000b%\u0007\u0003\u0012\u0010\u000b\u000f\u0013#\f\t\u0006\u0007\t\u000b\u000b\u000f\u000b\u0006\u0007 \n\u0003\u0012\u0007 \u0010\u0002\t\u0007 \u0006\t\n\u000b\f\u0002\u0007 \u0005\u000b\u000f\f\t\u0006\u0006\u0016\u0007 *\u0002\t\u0007 \u0003!\u0005\u0004\u0003\f\n\u0010\u0003\u000f\u0012\u0006\u0007 \u000f\u0015\u0007 \u0010\u0002\t\u0006\t\u0007 \t\u000b\u000b\u000f\u000b\u0006\u0007 \n\u000b\t\u0007 \u0012\u000f\u0010\u0007 \u0015#\u000b\u0010\u0002\t\u000b\u0007 \u0013\u0003\u0006\f#\u0006\u0006\t\u0013\u0016 \u000b\n\u0010\u0003\u000f\u00066\u0007\f\u000f\u0012\u0010\u000b\u0003\u000e#\u0010\t\u0007\u0010\u000f\u0007\u0010\u0002\t\u0007\u000f\u0014\t\u000b\n\u0004\u0004\u0007\u0006\t\n\u000b\f\u0002\u0007\u0006\u0002\u000f#\u0004\u0013\u0007\u000e\t\u0007\u000e\n\u0006\t\u0013\u0007\u000f\u0012\u0007\u0010\u0002\t\u0007 \n\u000b\u0002%\u0010\u0002!\u0003\f\u0007\u000f\u000b\u0007!\t\u0010\u000b\u0003\f\n\u0004\u0007\f\u000f!\u0005\u0004\t \u0003\u0010%\u0007\u000f\u0015\u0007\u0010\u0002\t\u0007\u0006\u000f\u0012\u001f\u0007\n\u0010\u0007\u0002\n\u0012\u0013\u001c\u0007\u0002\u000f\"\u0007\"\t\u0004\u0004\u0007 \n\n\u0007\u0005\t\u000b\u0006\u000f\u0012\u0007\f\n\u0012\u0007\u0006\u0003\u0012\u001f\u0007\n\u0012\u0013\u0007\u0010\u0002\t\u0007!\t\n\u0006#\u000b\t\u0013\u0007\u001f\u0004\u000f\u000e\n\u0004\u0007\u0010\t!\u0005\u000f\u001c\u0007\u0006\u0003\u0012\f\t\u0007\u0010\u0002\t\u0007 \n\u0004\n\u0010\u0010\t\u000b\u0007\u0005\n\u000b\u0010\u0004%\u0007\u0013\t\u0010\t\u000b!\u0003\u0012\t\u0006\u0007\u0010\u0002\t\u0007\u0010\u0003!\u0003\u0012\u001f\u0007 \t!\u0005\u0004\u000f%\t\u0013\u0016 \n\r\n\u0006\u0010\u001c\u0007 \u000e\t\u0006\u0003\u0013\t\u0006\u0007 \u0005\u000b\n\f\u0010\u0003\f\t\u001c\u0007 !\t\u000b\t\u0007 \u0004\u0003\u0006\u0010\t\u0012\u0003\u0012\u001f\u0007 \u0010\u000f\u0007 \u0010\u0002\t\u0007 \u000f\u000b\u0003\u001f\u0003\u0012\n\u0004\u0007 \u0006\u000f\u0012\u001f\u0007 \n!\n+\t\u0006\u0007\u0005\t\u000f\u0005\u0004\t\u0007\u0003!\u0005\u000b\u000f\u0014\t\u0007\u0010\u0002\t\u0003\u000b\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0016\u0007*\u0002\u0003\u0006\u0007\f\n\u0004\u0004\u0006\u0007\u0015\u000f\u000b\u0007\n\u0013\t(#\n\u0010\t\u0007 \u0014\u0003\u0006#\n\u0004\u0007\n\u0012\u0013\u0007\n#\u0013\u0003\u0010\u000f\u000b%\u0007\u0015\t\t\u0013\u000e\n\f+\u0007!\t\f\u0002\n\u0012\u0003\u0006!\u0006\u0007\u000f\u0012\u0007\u0010\u0002\t\u0007\u0005\t\u000b\u0015\u000f\u000b!\n\u0012\f\t\u0007 \u000f\u0015\u0007\u0005\t\u000f\u0005\u0004\t\u0007\u0006\u0003\u0012\u001f\u0003\u0012\u001f\u001c\u0007\u0010\u000f\u0007\t\u0012\n\u000e\u0004\t\u0007\u0010\u0002\t!\u0007\u0010\u000f\u0007\u0003!\u0005\u000b\u000f\u0014\t\u0007\u000f\u000b\u0007\f\u0002\n\u0012\u001f\t\u0007\u0010\u0002\t\u0003\u000b\u0007 \n\u0006\u0003\u0012\u001f\u0003\u0012 \u001f\u0016 \n\u001a\u0004+\t\b\u0019\u000e\u0003\u0012\n\u0003\f\u0003\t\u0005 \n*\u0002\n\u0012+\u0006\u0007 \u001f\u000f\u0007 \u0010\u000f\u0007 \u0010\u0002\t\u0007 $\u000b\n\u000e\n\u0012\u0010\u0007 &\u000f\u0012\u0006\t\u000b\u0014\n\u0010\u000f\u000b%\u0007 \u0003\u0012\u0007 *\u0003\u0004\u000e#\u000b\u001f\u001c\u0007 \u0010\u0002\t\u0007 \n\u001d\t\u0010\u0002\t\u000b\u0004\n\u0012\u0013\u0006\u001c\u0007\u0015\u000f\u000b\u0007\u0010\u0002\t\u0003\u000b\u0007\f\u000f -\u000f\u0005\t\u000b\n\u0010\u0003\u000f\u0012\u0016 \n\"%\u0003\u0002\u0003\u000f\u0003\t\u0004\u0003\u0006 \n\u001b\u0004\u0004\t\u0012\u001c\u0007 O\u0016'\u0016\u0007 59:.\u00196\u0016\u0007 D\u0005\t\t\f\u0002\u0007 \u000b\u0002%\u0010\u0002!<\u0007 )\u0010\u0006\u0007 \u000b\t\u0004\n\u0010\u0003\u000f\u0012\u0007 \u0010\u000f\u0007 \n\u0005\t\u000b\u0015\u000f\u000b!\n\u0012\f\t\u0007 #\u0012\u0003\u0014\t\u000b\u0006\n\u0004\u0006\u0007 \n\u0012\u0013\u0007 \n\u000b\u0010\u0003\f#\u0004\n\u0010\u000f\u000b%\u0007 \u0010\u0003!\u0003\u0012\u001f\u0016\u0007  \u0004\u0005\u0006\u000e\u0002\u0015\r \u0004\u0001\r \n!\"\u0004\u000e\t\b\u0007\u000b\u0012\u0017\r\u001d \u001c\u0007.\u0019 -;\u001a\u0016 \n$\u000f\u0004\u0010 4\u001c\u0007@\u0016\u0007P\u0007C\u000f\u0012\t\u0006\u001c\u0007@\u0016\u0007\b\u0016\u000759:;\u001a6\u0016\u0007'\u000f\t\u0006\u0007\u000b#\u0004\t\u0007\u0003\u0012\u0014\t\u000b\u0006\u0003\u000f\u0012\u0007!\n+\t\u0007 \n!\t\u0004\u000f\u0013\u0003\t\u0006\u0007\t\n\u0006\u0003\t\u000b\u0007\u0010\u000f\u0007\u000b\t\u0005\u000b\u000f\u0013#\f\t8\u0007)\u0015\u0007\u0012\u000f\u0010\u001c\u0007\"\u0002\n\u0010\u0007\u0013\u000f\t\u00068\u0007 \u001c\u0004\u000f\u000e\u0007\b\u0007\u0003\t\r \n\u0014\u0012\u0016\u000b\"\u0004\u0015\u0004\u000f\u0016\u0017\r# \u001c\u0007B;: -\u0018B9\u0016 \n&\u000b\u000f\u000f\u0012\t\u0012\u001c\u0007,\u0016\r\u0016@\u0016\u000759::\u00186\u0016\u0007\u0011\u0015\u0015\t\f\u0010\u0006\u0007\u000f\u0015\u0007\u0004\t\u0012\u001f\u0010\u0002\u001c\u0007\u0010\u000f\u0012\n\u0004\u0007\u0006\u0010\u000b#\f\u0010#\u000b\t\u0007\n\u0012\u0013\u0007 \n\f\u000f\u0012\u0010\u000f#\u000b\u0007 \u0003\u0012\u0007 \u0010\u0002\t\u0007 \u000b\t \f\u000f\u001f\u0012\u0003\u0010\u0003\u000f\u0012\u0007 \u000f\u0015\u0007 \u0010\u000f\u0012\t\u0007 \u0006\t\u000b\u0003\t\u0006 \u0017\r !\t\u0006\u000b\t\u0014\b\u0007\u0004\u000e\r $\r \n!\u0012\u0016\u000b\"\u0004\u0014\"\u0016\u0012\u0007\u000b\u0012\u0017\r%% \u001c\u0007\u0019:. -\u001a/\u001a\u0016 \n'\n\u0014\u0003\u0013\u0006\u000f\u0012\u001c\u0007 \r\u0016\u0007 59::\u00186\u0016\u0007 D\u000f\u0012\u001f\u0007 \u0006\u0003\u0012\u001f\u0003\u0012\u001f\u0007 \u000e%\u0007 %\u000f#\u0012\u001f\u0007 \n\u0012\u0013\u0007 \u000f\u0004\u0013\u001c\u0007 )\u0012\u0007 \n\u001b\u0003\t\u0004\u0004\u000f\u001c\u0007\b\u0016\u001c\u0007\n\u0012\u0013\u0007D\u0004\u000f\u000e\u000f\u0013\n\u001c\u0007C\u0016\u001b\u0016\u00075\u0011\u0013\u0006\u00166\u001c\u0007 &\u0005\u0012\u0007\u000b\r!\t\u0006\u000b\t\u0014\b\u0007\u0004\u000e\u0012 \u0016\u00075\u0005\u0005\u0016\u0007 \n:: -9B/6\u0016\u0007\u001d\t\"\u0007=\u000f\u000b+<\u0007E \u0015\u000f\u000b\u0013\u00073\u0012\u0003\u0014\t\u000b\u0006\u0003\u0010%\u0007\u0001\u000b\t\u0006\u0006\u001c\u0016 \n'\u000f\"\u0004\u0003\u0012\u001f\u001c\u0007,\u0016C\u0016\u000759:.;6\u0016\u0007D\f\n\u0004\t\u0007\n\u0012\u0013\u0007\f\u000f\u0012\u0010\u000f#\u000b<\u0007*\"\u000f\u0007\f\u000f!\u0005\u000f\u0012\t\u0012\u0010\u0006\u0007\u000f\u0015\u0007 \n\n\u0007\u0010\u0002\t\u000f\u000b %\u0007\u000f\u0015\u0007!\t!\u000f\u000b%\u0007\u0015\u000f\u000b\u0007!\t\u0004\u000f\u0013\u0003\t\u0006\u0016\u0007 !\u0012\u0016\u000b\"\u0004\u0015\u0004\u000f\u0007\u000b\u0002\u0015\r'\t\u0003\u0007\t\n\u0017\r#% \u001c\u0007 \nB\u00199 -B\u0019\u0018\u0016 \n'\u000f\"\u0004\u0003\u0012\u001f\u001c\u0007,\u0016C\u0016\u001c\u0007P\u0007\u0017\n\u000b\"\u000f\u000f\u0013\u001c\u0007'\u0016\r\u0016\u000759:;\u001a6\u0016\u0007 &\u0005\u0012\u0007\u000b\r \u000b\u0004\u000f\u000e\u0007\b\u0007\u0004\u000e \u001c\u0007 \n\r\u000f\u0012\u0013\u000f\u0012<\u0007\u001b\f\n\u0013\t!\u0003\f\u0007\u0001\u000b\t\u0006\u0006\u0016 \n\u0011\u0013\"\u000f\u000b\u0010\u0002%\u001c\u0007C\u0016\u000759:;\u00196\u0016\u0007@\t\u0004\u000f\u0013\u0003\f\u0007\f\u000f\u0012\u0010\u000f#\u000b\u0007\n\u0012\u0013\u0007!#\u0006\u0003\f\n\u0004\u0007\u0006\u0010\u000b#\f\u0010#\u000b\t\u0016\u0007 \n)\u0012\u0007\u0017\u000f\"\t\u0004\u0004\u001c\u0007\u0001\u0016\u001c\u0007&\u000b\u000f\u0006\u0006\u001c\u0007)\u0016\u001c\u0007\n\u0012\u0013\u0007,\t\u0006\u0010\u001c\u0007\b\u0016\u00075\u0011\u0013\u0006\u00166\u001c\u0007 &\u0005\u0012\u0007\u000b\u0002\u0015\r \u0012\b\u0006\u0005\u000b\b\u0005\u0006\t\r \n\u0002\u000e\u0011\r\u000b\u0004\u000f\u000e\u0007\b\u0007\u0004\u000e \u0016\u00075\u0005\u0005\u0016\u00079\u001a: -9;;6\u0016\u0007\u001d\t\"\u0007=\u000f\u000b+<\u0007\u001b\f\n\u0013\t!\u0003\f\u0007\u0001\u000b\t\u0006\u0006\u0016 \n\u0017\t\u000b!\t\u0006\u001c\u0007 '\u0016\u0007 59:;;6\u0016\u0007 @\t\n\u0006#\u000b\t!\t\u0012\u0010\u0007 \u000f\u0015\u0007 \u0005\u0003\u0010\f\u0002\u0007 \u000e%\u0007 \u0006#\u000e\u0002\n\u000b!\u000f\u0012\u0003\f\u0007 \n\u0006#!!\n\u0010\u0003\u000f\u0012\u001c\u0007  \u0004\u0005\u0006\u000e\u0002\u0015\r\u0004\u0001\r\b\"\t\r(\u000b\u0004\u0005\u0012\b\u0007\u000b\u0002\u0015\r)\u0004\u000b\u0007\t\b\u0016\r\u0004\u0001\r(\u0013\t\u0006\u0007\u000b\u0002\u0017\r#\u001d\u0017\r \n\u001a\u001c\u0007A\u0019. -A\u001a\u0018\u0016 \nC\u000f\u0012\t\u0006\u001c\u0007 @\u0016\b\u0016\u001c\u0007 D#!!\t\u000b\t\u0004\u0004\u001c\u0007 \r\u0016\u0007 P\u0007 @\n\u000b\u0006\u0002\u000e#\u000b\u0012\u001c\u0007 \u0011\u0016\u0007 59:;.6\u0016\u0007 \n\b\t\f\u000f\u001f\u0012\u00034\u0003\u0012\u001f \u0007 !\t\u0004\u000f\u0013\u0003\t\u0006<\u0007 \u001b\u0007 \u0013%\u0012\n!\u0003\f\u0007 \u0003\u0012\u0010\t\u000b\u0005\u000b\t\u0010\n\u0010\u0003\u000f\u0012\u0016\u0007 *\u0005\u0002\u0006\b\t\u0006\u0015\u0016\r \n \u0004\u0005\u0006\u000e\u0002\u0015\r\u0004\u0001\r+\f\u0014\t\u0006\u0007\u0013\t\u000e\b\u0002\u0015\r!\u0012\u0016\u000b\"\u0004\u0015\u0004\u000f\u0016\u0017\r\u001d, \u001c\u0007;: -9A9\u0016 \n\r\t\u0014\u0003\u0010\u0003\u0012\u001c\u0007 '\u0016C\u0016\u0007 59::\u00186\u0016\u0007 \u001b\u000e\u0006\u000f\u0004#\u0010\t\u0007 !\t!\u000f\u000b%\u0007 \u0015\u000f\u000b\u0007 !#\u0006\u0003\f\n\u0004\u0007 \u0005\u0003\u0010\f\u0002<\u0007 \n\u0011\u0014\u0003\u0013\t\u0012\f\t\u0007\u0015\u000b\u000f!\u0007\u0010\u0002\t\u0007\u0005\u000b \u000f\u0013#\f\u0010\u0003\u000f\u0012\u0007\u000f\u0015\u0007\u0004\t\n\u000b\u0012\t\u0013\u0007!\t\u0004\u000f\u0013\u0003\t\u0006\u001c\u0007 !\t\u0006\u000b\t\u0014\b\u0007\u0004\u000e\r \n$\r!\u0012\u0016\u000b\"\u0004\u0014\"\u0016\u0012\u0007\u000b\u0012\u0017\r%\u0019\u0017\r\u001f \u001c\u0007\u00189\u0018 -\u0018AB\u0016 \n\r\t\u0014\u0003\u0010\u0003\u0012\u001c\u0007 ' \u0016C\u0016\u0007 P\u0007 &\u000f\u000f+\u001c\u0007 \u0001\u0016\b\u0016\u0007 59::.6\u0016\u0007 @\t!\u000f\u000b%\u0007 \u0015\u000f\u000b\u0007 !#\u0006\u0003\f\n\u0004\u0007 \n\u0010\t!\u0005\u000f<\u0007\u001b\u0013\u0013\u0003\u0010\u0003\u000f\u0012\n\u0004\u0007\t\u0014\u0003\u0013\t\u0012\f\t\u0007\u0010\u0002\n\u0010\u0007\n#\u0013\u0003\u0010\u000f\u000b%\u0007!\t!\u000f\u000b%\u0007\u0003\u0006\u0007\n\u000e\u0006\u000f\u0004#\u0010\t \u0017\r \n!\t\u0006\u000b\t\u0014\b\u0007\u0004\u000e\r$\r!\u0012\u0016\u000b\"\u0004\u0014\"\u0016\u0012\u0007\u000b\u0012\u0017\r\u001a,,\u0019\u0017\r%#\u0017 \u0007:A. -:B\u0019\u0016 \nD\t\u000b\n\u0015\u0003\u0012\t\u001c\u0007@\u0016\r\u0016\u001c\u0007'\n\u0014\u0003\u0013\u0006\u000f\u0012\u001c\u0007C\u0016\u001c\u0007&\u000b\u000f\"\u0013\t\u000b\u001c\u0007\b\u0016O\u0016\u0007P\u0007\b\t\u0005\u0005\u001c\u0007$\u0016\u0017\u0016\u0007 \n59:;\u001a6\u0016\u0007E\u0012\u0007\u0010\u0002\t\u0007\u0012\n\u0010#\u000b\t\u0007\u000f\u0015\u0007!\t\u0004\u000f\u0013% -\u0010\t \u0010\u0007\u0003\u0012\u0010\t\u001f\u000b\n\u0010\u0003\u000f \u0012\u0007\u0003\u0012\u0007!\t!\u000f\u000b%\u0007 \n\u0015\u000f\u000b\u0007\u0006\u000f\u0012\u001f\u0006\u0016\u0007  \u0004\u0005\u0006\u000e\u0002\u0015\r\u0004\u0001\r&\t\u0013\u0004\u0006\u0016\r\u0002\u000e\u0011\r-\u0002\u000e\u000f\u0005\u0002\u000f\t\u0017\r\u0018% \u001c\u00079AB -9B\u0019\u0016 \n,\n\u001f\u0012\t\u000b\u001c\u0007 \b\u0016\u001b\u0016\u0007 P\u0007 \u001e\u0003\u0006\u0002\t\u000b\u001c\u0007 @\u0016C\u0016\u0007 59:.\u00186\u0016\u0007 *\u0002\t\u0007 \u0006\u0010\u000b\u0003\u0012\u001f -\u0010\u000f -\u0006\u0010\u000b\u0003\u0012\u001f\u0007 \n\f\u000f\u000b\u000b\t\f\u0010\u0003\u000f\u0012\u0007\u0005\u000b\u000f\u000e\u0004\t!\u001c\u0007  \u0004\u0005\u0006\u000e\u0002\u0015\r\u0004\u0001\r\b\"\t\r(\u0012\u0012\u0004\u000b\u0007\u0002\b\u0007\u0004\u000e\r\u0004\u0001\r\u001c\u0004\u0013\u0014\u0005\b\u0007\u000e\u000f\r \n&\u0002\u000b\"\u0007\u000e\t\u0006\u0016\u0017\r\u0018\u001a\u0017\r\u001a \u001c\u00079\u001a; -9.B\u0016"
    },
    {
        "title": "Key-specific shrinkage techniques for harmonic models.",
        "author": [
            "Jeremy Pickens"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1414776",
        "url": "https://doi.org/10.5281/zenodo.1414776",
        "ee": "https://zenodo.org/records/1414776/files/Pickens03.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1414776,
        "dblp_key": "conf/ismir/Pickens03",
        "content": "Key-speciﬁc Shrinkage Techniques for Harmonic Models\nJeremy Pickens\nCenter for Intelligent InformationRetrieval\nDepartment of Computer Science\nUniversity of Massachusetts\nAmherst, MA 01002 USA\njeremy@cs.umass.edu\n1 Introduction\nStatistical modeling of music is rapidly gaining acceptance as\nviable approach to a host of Music Information Retrieval re-\nlated tasks, from transcription to ad hoc retrieval. As music\nmay be viewed as an evolving pattern of notes over time, mod-\nels which capture some statistical notion of sequence are pre-\nferred. The focus of this paper is on Markov models for ad hoc\nretrieval. Inparticular,weusetheHarmonicModelscreatedby\n[Pickens et al., 2002] as our baseline retrieval system and ex-\nplainhowtheymaybeimprovedbybettershrinkageprocedures\nto improve parameter estimation.1\nA quick review of harmonic model creation is necessary: First,\nprincipled heuristics are used to map simultaneities of notes\nonto distributions of chords. These distributions are further\nsmoothed using windows of simultaneities at previous time\nsteps to update the distribution at the current time step. Rel-\native frequency counts of chord occurrences are then used to\nestimate parameters of a Markov model. The approach taken\nfor retrieval is to estimate a model for every piece of music in\nthe collection, to estimate a model of the query, and then rank\nthe pieces of music by the KL divergence of query model to\ndocument model. However, if a document model has an esti-\nmateofzeroforanyprobabilitytheKLdivergencescoreforthe\ndocument goes to inﬁnity. Shrinkage is the technique by which\nthese estimates of zeroprobability are overcome.\n2 Shrinkage\nShrinkage works by using data rich states of simpler models to\nbetter inform the estimates in data sparse states of more com-\nplex models [Freitag and McCallum, 1999]. In the initial work\nonHarmonicModels,shrinkagewasdonebycreatingageneral\nmodel of music by collecting the counts for every piece in the\ncollectionintoasinglemodel,creatingaglobalaverage. When-\never a zero probability estimate is encountered, the algorithm\n“backs off” to the estimategiven by the global model.\n1ThisworkwassupportedinpartbytheCenterforIntelligentInfor-\nmation Retrieval and in part by NSF grant #IIS-9905842. Any opin-\nions, ﬁndings and conclusions or recommendations expressed in this\nmaterial are the author(s) and do not necessarily reﬂect those of the\nsponsor.\nPermission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided that\ncopies are not made or distributed for proﬁt or commercial advan-\ntage and that copies bear this notice and the full citation on the ﬁrst\npage.c\r2003 Johns Hopkins University.In this work, we instead begin with the intuition that depend-\ning on the key in which a piece of music was written, there are\ngoing to be different estimates for different states. An analogy\nto text modeling may be helpful. Suppose you have models for\ntwo different topics: ﬁnance and ecology. Both topic models\nhave some non-zero probability estimate for the word “bank”.\nHowever, the estimate of “bank” under the ﬁnance topic might\nbe much higher, because banks and banking is a common fea-\nture of ﬁnancial documents, while stream or river “bank” is an\nactive, though less common, feature of ecological documents.\nThus,ifyoudeterminethataparticulardocumentisaboutecol-\nogy, and not about ﬁnance, you would want to shrink that doc-\nument’s estimate for “bank” toward the ecology topic model\nrather than toward the ﬁnancetopic model.\nBy analogy, a key can be though of as the “topic” for a par-\nticular piece of music. No matter what the unique melodic or\nharmonicprogressionofthatpiece,theexpectedfrequencyofa\nCMajortriadisgoingtobeverydifferentifthatpieceiswritten\ninCMajor, versus if that piece is written in F]Major. Yet if\nwesmooththispiecewithaglobalmodel,createdbyaveraging\nevery piece in the collection including those written in F]Ma-\njor,thenwewillbeshrinkingtheestimatesforthispiecefurther\naway from what the true valuesmight be.\nSoratherthancreateaglobalmodel,wecreate24“topic”mod-\nels of music, one for each key ( CMajor,Cminor:::BMa-\njor,Bminor). At indexing time, each piece in the collection is\ntaggedbykey. Withnoguaranteethataccuratekeyinformation\nexistsfor anydocument inthe collection,weinstead usea sim-\npleheuristickey-ﬁndingalgorithm: Duringharmonicmodeling,\nafter simultaneities are mapped onto distributions of major and\nminor triads, we sum the relative count for each triad over all\nsimultaneities. The most frequently occuring triad is chosen;\ntherootofthistriaddeterminesthetonic,whilethethirdofthis\ntriad determines the modality, yielding the key “topic” label.\nThe collection is partitioned by these labels, and each partition\nis averaged, to create 24 key-based models.\nFinally, we are ready to propose two shrinkage techniques.\nFirst,thekeyofthedocumentmodeltobeshrunkisdetermined.\nTechnique oneis backoff. However, instead of backing off to a\nglobalmodel,webackofftothatdocument’scorrespondingkey\nmodel. For technique two, all states in the document model are\nlinearly interpolated with its corresponding key model, regard-\nless of zero estimates or not. The parameters of the interpola-\ntionaregivenamaximumentropyvalueof0.5forthedocument\nmodel, 0.5 for the key-speciﬁcmodel.Table 1: Mean Average Precision\nTwinkle Lachrimae Folia\n(0) (1) (2) (0) (1) (2) (0) (1) (2)\nMM0, W1 0.1450.143 0.172* 0.1720.177* 0.179 0.3330.376* 0.534*\nMM0, W2 0.1450.158 0.162 0.1740.179 0.185* 0.3370.383* 0.535*\nMM0, W3 0.1450.141 0.152 0.1730.182* 0.187* 0.3310.382* 0.539*\nMM0, W4 0.1300.132 0.140 0.1720.181* 0.190* 0.3280.378* 0.539*\nMM1, W1 0.1110.107 0.109 0.0560.060* 0.180* 0.1720.203* 0.151\nMM1, W2 0.1490.131* 0.123 0.1360.167* 0.221* 0.3150.349* 0.263*\nMM1, W3 0.0950.089 0.120* 0.1770.177 0.221* 0.4220.380* 0.291*\nMM1, W4 0.1040.093* 0.109 0.1810.191* 0.224* 0.3890.363* 0.303*\nMM2, W1 0.1500.169* 0.088* 0.0300.039* 0.119* 0.1050.162* 0.029*\nMM2, W2 0.1560.141* 0.096* 0.0960.151* 0.221* 0.1780.229* 0.108*\nMM2, W3 0.1170.102* 0.094 0.1620.192* 0.232* 0.2840.266* 0.144*\nMM2, W4 0.0830.081 0.091 0.1950.203* 0.237* 0.3290.274* 0.181*\nAverage 0.1270.124 0.121 0.1440.158* 0.200* 0.2940.312* 0.301*\n-2.4% -4.7% +9.7% +38.9% +6.1% + 2.4%\nTable 2: Precision at the top 5 retrieved pieces\nTwinkle Lachrimae Folia\n(0) (1) (2) (0) (1) (2) (0) (1) (2)\nMM0, W1 0.4850.469 0.539 0.4610.507* 0.496 0.6280.756* 0.800*\nMM0, W2 0.5150.577 0.546 0.4400.517* 0.504 0.6720.756* 0.788*\nMM0, W3 0.5310.539 0.523 0.4270.528* 0.512* 0.6280.744* 0.796*\nMM0, W4 0.4390.454 0.477 0.4400.517* 0.525* 0.6080.728* 0.796*\nMM1, W1 0.2850.315 0.431* 0.0400.088* 0.584* 0.0560.164* 0.512*\nMM1, W2 0.5390.515 0.462 0.2160.469* 0.675* 0.4040.620* 0.672*\nMM1, W3 0.3460.339 0.423 0.5230.515 0.677* 0.7880.712* 0.704*\nMM1, W4 0.3920.346 0.400 0.4990.576* 0.685* 0.7280.692 0.724\nMM2, W1 0.4080.462 0.377 0.0320.035 0.507* 0.1120.128 0.120\nMM2, W2 0.4310.546* 0.377 0.0590.395* 0.736* 0.1440.408* 0.352*\nMM2, W3 0.4460.400 0.346 0.4190.643* 0.736* 0.4800.596* 0.476\nMM2, W4 0.3310.331 0.346 0.6190.672* 0.760* 0.7320.616* 0.588*\nAverage 0.4290.441 0.437 0.3480.455* 0.616* 0.4980.577* 0.611*\n+2.8% + 1.9% +30.7% +77.0% +15.9% +22.7%\n3 Results\nTables1and2containourresults. Weusethesame3000-piece\ncollection as [Pickens et al., 2002], with transcribed audio ver-\nsions of the polyphonic Twinkle, Lachrimae and Folia (TLF)\nsets as queries, and all score versions of these pieces tagged as\nrelevant. Resultsarepresentedforthethreemodelorders(MM0\nthrough MM2) and four window smoothing sizes (W1 through\nW4). Column (0) is the original global backoff approach (the\nbaseline), column (1) is key-speciﬁc backoff, and column (2)\nis key-speciﬁc linear interpolation. Asterisks indicate a statisti-\ncally signiﬁcant differencefrom the baseline (T-test).\nThe ﬁrst observation we make is that, on average across all\ngivenparametersettings,key-speciﬁcshrinkageiseithersignif-\nicantlybetter,ornotsigniﬁcantlyworse,thantheoriginalglobal\ntechnique. Improvements are greatest for the Lachrimae and\nFolia variations, with little signiﬁcant difference on the Twin-\nkle variations. While not all variations in the TLF sets are in\nthe same key, there are more variations in non closely-related\nkeys in the Twinkle set than in the others. Key-speciﬁc shrink-\nage serves to accentuate (without completely eliminating) dif-\nferences between pieces in different keys, shrinking each doc-\nument more toward others in its own and closely related keys.\nThis polarization has the effect of improving retrieval when all\nrelevantpiecesareincloselyrelatedkeys,butdegradingperfor-\nmance when they are not. Even still, performance is no worse,\non average, for the Twinkleset.\nOur second observation is that precision at the top 5 retrieved\npieces shows greater improvements than mean average preci-\nsion. Indeed, even for the Twinkle set there is slight improve-mentonaverage(thoughstillnotsigniﬁcant),whilefortheFolia\nand Lachrimae sets there are huge improvements. Key-speciﬁc\nshrinkage can therefore be thought of as a “precision enhanc-\ning” technique, given that at least one variation sought by the\nuser is in a similar key, whichis true of many applications.\nOur ﬁnal observation is that for higher order models, backoff\nis better, while for lower order models, linear interpolation is\nbetter. While linear interpolation polarizes the relevant and\nnon-relevant documents by key, higher order models do a bet-\nter job ﬁnding relevant documents by “memorizing” the salient\nharmonic progressions. We conjecture that linear interpolation\ndilutes these patterns, sometimes forfeiting some of the earlier\nadvantage gained through polarization. In the future these pa-\nrameters could be betterset using Expectation-Maximization.\nIn conclusion, key-speciﬁc shrinkage techniques have the po-\ntential to greatly improveretrieval accuracy.\nReferences\n[Freitag and McCallum,1999] Freitag, D. and McCallum,\nA.K.(1999). Informationextractionwithhmmsandshrink-\nage. InProceedings of the AAAI-99 Workshop on Machine\nLearning for Information Extraction .\n[Pickens et al., 2002] Pickens,J.,Bello,J.P.,Monti,G.,Craw-\nford,T.,Dovey,M.,Sandler,M.,andByrd,D.(2002). Poly-\nphonicscoreretrievalusingpolyphonicaudioqueries: Ahar-\nmonic modeling approach. In Fingerhut, M., editor, Pro-\nceedingsofthe3rdAnnualInternationalSymposiumonMu-\nsic Information Retrieval (ISMIR) , pages 140–149, IRCAM\n- Centre Pompidou, Paris, France."
    },
    {
        "title": "Harmonic analysis with probabilistic graphical models.",
        "author": [
            "Christopher Raphael",
            "Josh Stoddard"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1415574",
        "url": "https://doi.org/10.5281/zenodo.1415574",
        "ee": "https://zenodo.org/records/1415574/files/RaphaelS03.pdf",
        "abstract": "A technique for harmonic analysis is presented that partitions a piece of music into contiguous regions and labels each with the key, mode, and functional chord, e.g. tonic, dominant, etc. The analysis is per- formed with a hidden Markov model and, as such, is automatically trainable from generic MIDI files and capable of finding the globally optimal harmonic la- beling. Experiments are presented highlighting our current state of the art. An extension to a more complex probabilistic graphical model is outlined in which music is modeled as a collection of voices that evolve independently given the harmonic pro- gression. Keywords: harmonic analysis, music, probabilistic graphical model, hidden Markov model 1",
        "zenodo_id": 1415574,
        "dblp_key": "conf/ismir/RaphaelS03",
        "keywords": [
            "harmonic analysis",
            "contiguous regions",
            "key",
            "mode",
            "functional chord",
            "hidden Markov model",
            "generic MIDI files",
            "globally optimal harmonic labeling",
            "probabilistic graphical model",
            "music evolution"
        ],
        "content": "Harmonic AnalysiswithProbabilistic Graphical Models\nChristopher Raphael\n\u0000\nDept. ofMathematics andStatistics\nUniv.ofMassachusetts, Amherst 01003-4515\nraphael@math.umass.eduJoshStoddard\nDept. ofMathematics andStatistics\nUniv.ofMassachusetts, Amherst 01003-4515\nstoddard@math.umass.edu\nAbstract\nAtechnique forharmonic analysis ispresented that\npartitions apiece ofmusic into contiguous regions\nandlabels each with thekey,mode, andfunctional\nchord, e.g.tonic, dominant, etc.Theanalysis isper-\nformed with ahidden Mark ovmodel and, assuch, is\nautomatically trainable from generic MIDI ﬁles and\ncapable ofﬁnding theglobally optimal harmonic la-\nbeling. Experiments arepresented highlighting our\ncurrent state oftheart. Anextension toamore\ncomple xprobabilistic graphical model isoutlined in\nwhich music ismodeled asacollection ofvoices\nthat evolveindependently giventheharmonic pro-\ngression.\nKeywords: harmonic analysis, music, probabilistic graphical\nmodel, hidden Mark ovmodel\n1Introduction\nAvariety ofmusical analysis sometimes knownasfunctional\nharmonic analysis represents amusical passage asasequence\nofchords. Thechords areexpressed interms oftheir function,\ne.g.dominant ortonic, often written with corresponding roman\nnumerals, e.g.VorI.Each chord isanalyzed intheconte xtof\nakeywhich modulates overtime. Thealgorithmic study ofthis\ntype ofanalysis isthetopic addressed here.\nThe most obvious application ofharmonic analysis inmusic\ninformation retrie val(MIR) would treat queries phrased inthe\nlanguage ofharmon y:What aretheearliest examples ofthe\nuseofGerman augmented sixth chords orNeapolitan chords?\nWhich Beatles songs havedecepti vecadences? Where canIbuy\nthepiece Iheard ontheradio with theharmonic progression Ivi\nIVVIrepeated manytimes? Itislikelythatsuch applications\nwillbemost useful tomusicologists since themere formulation\nofsuch queries requires amore sophisticated understanding of\u0001Thisworkissupported byNSFITRgrantIIS-0113496.\nPermission tomakedigitalorhardcopiesofallorpartofthiswork\nforpersonalorclassroom useisgrantedwithoutfeeprovidedthat\ncopiesarenotmadeordistributedforpro®torcommercial advan-\ntageandthatcopiesbearthisnoticeandthefullcitationonthe®rst\npage.c\n\u00022003JohnsHopkinsUniversity.harmon ythan would beexpected ofanaverage music enthusi-\nast.\nAmore subtle, yetperhaps more important, application toMIR\nmight beoneofrepresentation. Harmonic analysis reduces mu-\nsictoaone-dimensional sequence ofsymbols from asmall al-\nphabet. Theone-dimensional nature ofthisrepresentation lends\nittothewealth ofsearch techniques treating strings astheba-\nsicunitofstudy .Such string matching algorithms canﬁndthe\nstring inadatabase minimizing avariety ofedit-lik edistances\ninlinear time. Pickens, Bello, Monti, Crawford, Dovey,San-\ndler,&Byrd, (2002) showanexample ofatechnique some what\nlikeharmonic analysis forrepresentation andretrie val.\nMore generally ,theone-dimensional musical reduction af-\nforded byharmonic analysis might form thebasis forgenre clas-\nsiﬁcation ortheconstruction ofvarious music similarity met-\nrics. Perhaps such analysis might evenserveasauseful compo-\nsitional toolbymaking unexpected links between musical pas-\nsages, asinPeter Schick ele’scomical musical pastiches.\nWhile weareinterested inthese applications, weﬁndthestudy\nofthecogniti veorAIaspect ofharmonic analysis ample moti-\nvation byitself.\nOur basic approach isstatistical; thisorientation andmethod-\nology distinguishes ourworkfrom most other efforts weknow.\nThe most signiﬁcant beneﬁt oftheprobabilistic modeling we\nemplo yistheability tolearn aspects ofourmodel inanunsu-\npervised manner ,forinstance, using generic (unmark ed)MIDI\ndata. However,wealso inherit computational machinery that\nidentiﬁes theglobally bestharmonic parse. Inaddition, wepre-\nferthetransparenc yandhonesty ofaclearly speciﬁed proba-\nbilistic model.\nThat said, weﬁndcommon ground with severalother previous\nefforts inharmonic analysis. Krumhansl (1990) identiﬁes key\nbymatching ahistogram ofpitches toacollection ofpossible\nkeytemplates. While ourapproach simultaneously identiﬁes\nchordand key,theactual computation that measures theap-\npropriateness ofaparticular keyhypothesis issimilar tothat\nofKrumhansl (1990). Weshare with Temperle yandSleator\n(1999) therecognition thatrhythmic content isuseful inhar-\nmonic analysis andthenotion thatharmonic analyses thatﬂuc-\ntuate rapidly between keysareimplausible andshould bedis-\ncouraged orpenalized. Pardo (2002) builds upon thisapproach\nwith adynamic-programming algorithm thatoptimizes overthe\nexponentially-man ysegmentations inacomputationally efﬁ-\ncient manner .Dynamic programing isalso fundamental toourwork.\nAnovervie wofthevariety ofapproaches toharmonic analysis\nispresented inBarthelemy &Bonardi (2001). Most approaches\nwith scope similar toours arerule-based: themusic isreduced\nandrecognized through aseries ofdeterministic state transfor -\nmations (mer ges, simpliﬁcations, intermediate labellings, etc.)\nmoving systematically towardaﬁnal representation. Inour\nviewthere aretwoprincipal disadv antages ofrule-based ap-\nproaches. First, such schemes failtoarticulate anymeasure of\ngoodness ofthepossible “answers” andhence donotformu-\nlatetheproblem clearly .Second, rule-based schemes balance\neach decision ortransformation precariously ontheshoulders\nofprevious decisions andhence irrevocably propagate errors\nforw ard. Ourapproach, likethatofPardo (2002) isdecidedly\nnotrule-based.\nAnother signiﬁcant difference between ourapproach and all\nothers weknowisoursimultaneous recognition ofchord and\nkey.Thehope here isthatthemore structured sequence ofchord\nfunctions (e.g. tonic, dominant etc.) willhelp guide theanalysis\nwhen thechoice ofchord (e.g. cmajor triad, fminor triad, etc)\nisambiguous.\n2TheModel\nOur harmonic analysis isbased onpitch andrhythm. Atthe\noutset weackno wledge that there arelikelynotwoelements\nofmusic that donotinteract insome musical situation; thus\nlimiting ourtreatment tothese twoelements undoubtedly loses\nsome relevantinformation. However,inmost musical conte xts\nthehuman listener caneasily base aplausible harmonic analysis\nsolely onpitch andrhythm. This andtherather obvious virtue\nofbeginning with simplicity lead ustostart here.\nOurharmonic analysis isperformed onaﬁxedmusical period,\u0000,sayameasure (\n\u0000\u0002\u0001\u0004\u0003)orhalf measure, (\n\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007).To\nthisendwepartition thepitches inourmusical composition\ninto asequence ofsubsets\t\u000b\n\r\f\u000f\u000e\u0010\u000e\u000f\u000e\u0011\f\u0012\t\u0014\u0013 where\t\u0014\u0015isthecollec-\ntion ofpitches whose onset time, inmeasures, liesinthein-\nterval\u0016 \u0017\n\u0000\f\u0019\u0018\u001a\u0017\u001c\u001b\n\u0003\u0019\u001d\u001e\u0000\b\u001d.Wenotate thiscollection explicitly as\t\u0015\n\u0001 \u001f\t\n\n\u0015\n\f\u000f\u000e\u0010\u000e\u000f\u000e\u0011\f\u0012\t\u000b!\u0015#\"where thenumber ofpitches,$,depends\non \u0017.Ouranalysis isbased only onpitchclass soweregard the\npitches aselements of\n\u001f\u0019%\f\n\u0003\f\u0010\u000e\u000f\u000e\u0010\u000e&\f\n\u0003\u0014\u0003\"where '\r\f)(\u0011*\n\u0001+%\f\u0012'\u000f*\u0011\f\u0012,\u000b-\n\u0001\u0003\f\u0010\u000e\u000f\u000e\u0010\u000e\u0011\f)(\u0006\f)'\u000f-\n\u0001.\u0003/\u0003.While theextension ofourapproach toin-\nclude enharmonic spellings isobvious, MIDI data clearly form\nthelion’sshare ofavailable testcases atpresent; since MIDI\ndoes notuseenharmonic spellings, wedonotmodel them.\nOurgoal istoassociate akeyandchord describing theharmonic\nfunction ofeach period, \t0\u0015.Thus each \t/\u0015willbelabeled with\nanelement of1\u00013254768479:\u0001;\u001f\u0006%\f\u0010\u000e\u000f\u000e\u000f\u000e\u000f\f\n\u0003\u0014\u0003\"\n4<\u001fmajor\fminor\"\n4<\u001fI\fII\f\u000f\u000e\u000f\u000e\u0010\u000e\u000f\f VII\"\nwhere\n2\f\n6\f\n9stand fortonic, mode, andchord. Forinstance,\u0018\u001a=&\f\u0012>?\f)'\n\u001d5\u0001\u0018A@B\fmajor\fII\n\u001dwould represent thetriad inthekey\nof\n\u0007C\u0001,major builtontheII=2ndscale degree which con-\ntains pitches e,g,b .Wewillignore theusual convention ofus-\ninglowerandupper case roman numerals forminor andmajor\ntriads. While itispossible touseabroader range ofpossible\nchords (infact,wedoinourexperiments) nothing signiﬁcant is\nlostbylimiting ourselv estothesevenbasic triads inthisdis-\ncussion. Similarly ,itwould bepossible toinclude more modes\nthan thebasic major andharmonic minor thatwetreat here. Wedonotcurrently model chord inversion inthiswork.\nConsider thecommon situation, occurring inaclearly estab-\nlished cmajor conte xt,inwhich weencounter thechord pro-\ngression cmajor ,dmajor ,gmajor .Inthiscase itappears that\ndacts asthe“dominant” ofthedominant chord gmajor —a\nso-called secondary dominant, often notated (V/V). Our basic\nvocab ulary ofchords does notinclude anykind ofsecondary\nchord, butsuch interpretations canstillberepresented using key\nmodulation. Forinstance, theaboveexample canberepresented\nas(c=0, major ,I),(g=7, major ,V),(g=7, major ,I).Clearly our\nrepresentation allowsforarichvariety ofsecondary functional-\nitywhile avoiding murk ydistinctions between secondary func-\ntionandactual modulation.\nLet D\n\f\u001eD#E\b\f\u0010\u000e\u000f\u000e\u000f\u000e\u000f\f\u001eD\u0013bethesequence ofharmonic labels D\u0015GF1.Wemodel thissequence probabilistically asahomogeneous\nMark ovchainH\u0018\u001aIJ\u0015\u0014KL\n\u0014M IN\n\b\f\u000f\u000e\u0010\u000e\u000f\u000e\u0011\f\u0012IJ\u0015\n\u001dO\u0001PH\u0018AIJ\u0015\bKQ\n\u0014M IJ\u0015\n\u001d\nThe Mark ovassumption isofcourse only anapproximation,\nhoweverwebelie veitcaptures quite abitofmusical structure,\nespecially when oneconsiders thesimplicity ofthemodel. For\ninstance, thefactthatkeystend toremain constant forrelati vely\nlong periods oftime iseasily expressed interms ofaMark ov\nmodel: thekeyateach time period is,with high probability ,\nthesame asitwasontheprevious time period. Furthermore,\nwhen thekeyisconstant music isoften composed offamiliar\nchord progressions such asthestabilizing IVIVIortheubiq-\nuitous Rock andRoll progression IIVIVIVI.While chord\nprogressions often havelonger memory ,asintheRock exam-\nple,asigniﬁcant component offunctional harmonic beha vior\niscaptured bytransition tendencies: Vtends togotoI,iiof-\ntengoes toV,etc.Similarly ,manyofthetendencies forchord\ntransitions aremirrored byanalogous keymodulations tenden-\ncies. Forinstance, asthechords IandVfrequently appear side\nbyside, modulations toneighboring keysinthecircle ofﬁfths\narealsocommon. (While treated separately inourmodel, these\nphenomena arereally notcompletely distinct). Itisthese tran-\nsition tendencies thatcanberepresented byaMark ovchain.\nWe,ofcourse, donot observ ethe sequence oflabelsD\n\f\u0010\u000e\u000f\u000e\u000f\u000e\u0010\f\u001eD\u0013directly ,butrather ourdata\t\n\f\u0010\u000e\u000f\u000e\u000f\u000e\u000f\f\u001e\t\u0013.Thesec-\nond assumption ofthehidden Mark ovmodel (HMM) isthat\neach data vector , \t\u0015,isanobserv ation ofarandom variable,R\u0015,whose distrib ution depends only onthecurrent label:H\u0018A\t\u0014\u0015SM IN\n\b\f\u000f\u000e\u000f\u000e\u0010\u000e\u000f\f\u0012IJ\u0015N\f\u001e\t\u000b\n\u0006\f\u000f\u000e\u0010\u000e\u000f\u000e\u0012\t\u0014\u0015\u000bTU\n\u001dO\u0001VH\u0018\u001a\t\u0014\u0015SM IJ\u0015\n\u001d\nEssentially ,thisassumption says thateverytime wevisit astate\n(harmonic label) thedata areobtained by“spitting out” acol-\nlection ofpitches from adistrib ution characteristic ofthestate.\nAgain, thisiscertainly not“correct, ”howeverthepitch data\nclearly does depend heavily ontheharmonic label. While in-\ncluding more structure (dependencies) may makethemodel\nmore realistic, itonly helps ourparticular cause when itim-\nprovesthemodel’ sability todiscriminate between harmonic la-\nbels.\n2.1Hand-TyingofStates\nOur model isparameterized bythetransition probabilitiesH\u0018AIJ\u0015\bKQ\n\u0014M IJ\u0015\n\u001dand output probabilities\nH\u0018\u001a\t0\u0015SM IW\u0015\n\u001d.One ofthe\ngreatest advantages oftheHMM isthatthese parameters canbelearned automatically fromunlabeled data, e.g. generic MIDI\nﬁles. However,atpresent thetransition probabilities consist of\u0018\n\u0003\u0006\u0007 4 \u0007 4\n\u0001\u001d\nE\u0001:\u0007\u0003\u0002/\u0007\u0014\u0007\u0005\u0004parameters —more than wecanexpect\ntotrain reliably with amodest data set;asimilar problem ex-\nistswith theoutput probabilities\nH\u0018\u001a\t\u0015\nM I\u0015\n\u001d.Weintroduce some\nhand-crafted simplifying assumptions thatlead tofeasible train-\ning. Asinthepreceding, ourresearch bias isformaking our\nassumptions explicit, evenwhen theyseem questionable.\nRecall thateach harmonic label,I,isatriple consisting ofa\ntonic, mode, andchord, I\n\u0001\u0018\u001a=&\f\u001e> \f\u0012'\n\u001d.Wemodel thetransition\nprobabilities\nH\u0018\u001aI\u0007\u0006 M I\n\u001dasH\u0018AI\n\u0006M I\n\u001d \u0001 H\u0018A=\n\u0006\f\u0012>\n\u0006'\n\u0006M =&\f\u0012>?\f)'\n\u001d\u0001 H\u0018A=\n\u0006\f\u0012>\n\u0006M =&\f\u0012>\n\u001d\u001aH\u0018 '\n\u0006M =\n\u0006\f\u001e>\n\u0006\f\u0012=&\f\u001e> \f\u0012'\n\u001d(1)\u0001 H\u0018A=\n\u0006\t\b=&\f\u001e>\n\u0006M >\n\u001d\n\nH\u0018A'\u000b\u0006 M '\n\u001d=\f\u0006\n\u0001=&\f\u001e>\r\u0006\n\u0001>H\u0018A'\u000b\u0006\n\u001dotherwise(2)\u000e\u0010\u000f\u0012\u0011\u0001 \u0000\u000b\u0013\u0018\u001a=\n\u0006\b=&\f\u0012>\n\u0006M >\n\u001d\n\n\u0000\nE\u0014\u0018A'\n\u0006M '\n\u001d=\n\u0006\n\u0001=&\f\u001e>\n\u0006\n\u0001>\u0000\n\n\u0014\u0018A'\n\u0006\n\u001dotherwise\nInEqn. 1wehaveassumed thattheprobability ofthenewkey,=\f\u0006 \f\u001e>\r\u0006,giventhecurrent state, =&\f\u001e> \f\u0012',\nH\u0018A=\u0015\u0006 \f\u001e>\r\u0006 M =&\f\u0012>?\f)'\n\u001d,does not\ninfactdepend onthecurrent chord '.Theleftfactor ofEqn. 2\nrepresents atranslation invariance assumption about keymod-\nulations —theprobability ofmodulating bysome particular\ninterv aldoes notdepend onthecurrent tonic. Asonewho\ndoes nothaveperfect pitch andhears only relati vepitch move-\nment, thisandother pitch translation invariance assumptions\nseem unassailable. (The difference=\u0016\u0006\n\b=istakenmodulo 12.)\nTheright factor ofEqn. 2iscomposed oftwoassumptions. The\nﬁrst(top) isthatwhen thekeyisconstant thechord transitions\ndonotdepend onthecurrent key.This isessentially another\ntranslation invariance assumption andseems, toourmind, un-\ndeniable aslong aswerestrict ourattention toeither major or\nminor mode. Theassumption goes abitfurther andasserts that,\nforexample, theprobability ofmoving from ItoVisthesame in\nboth major andminor modes. Thesecond (bottom) assumption\nisthatwhen wedomovefrom onekeytoanother wechoose\nthenewchord andrandom without regard fortheneworold\nkeys.Wedoubt thisparticular assumption would hold upun-\nderempirical investigation butalso doubt thatamore nuanced\nmodeling ofthiscase willachie vesigniﬁcant impro vements in\nrecognition accurac y.These assumptions reduce thenumber of\nparameters necessary torepresent\nH\u0018\u001aI\u0017\u0006 M I\n\u001dtothose involvedin\nthedistrib utions\n\u0000\u0018\u0013\f\n\u0000\n\n\u0014\f\n\u0000\nE\u0014:\n\u0003\u0019\u0007#4C\u0007#4C\u0007\u001b\n\u00014\n\u0001\u001b\n\u0001\u0001 \u0003\u0019%\u0005\u0004pa-\nrameters, further reduced bytheconstraint thateach probability\ndistrib ution must sum to1.\nInmodeling theoutput distrib utions,\nH\u0018\u001a\tSM I\n\u001d,wehaveobserv ed\nthat, while expressi vedissonances areamainstay ofmusical\nsurprise, surprise isalmost bydeﬁnition anexception tothe\nnorm; inparticular ,weanticipate that chord tones aremore\nlikelytooccur onrhythmically strong beats than weak ones.\nRather than trying toquantify such anotion directly ,wesimply\nallowtheoutput distrib utions todepend ontheknownmeasure\npositions inamanner wewilllearn from data. Thus wetreat\nthemeasure positions ascovariates inourmodel andcondition\nonthem aswell asthechord label.\nInparticular ,suppose thatthepitch set \t\n\u0001\t\n\n\f\u0010\u000e\u000f\u000e\u000f\u000e\u0010\f\u001e\t !hasan\nassociated vector \u0019\n\u0001\u0019\n\n\f\u0010\u000e\u000f\u000e\u000f\u000e\u000f\f\u0015\u0019\r! where \u0019\u0005\u001alabels themeasure\nposition occupied by \t\u001b\u001a.Inourexperiments with music in4/4\ntime wetook \u0019\u0005\u001atobe0if \t\t\u001aoccupies thestart ofameasure, 1if\t\t\u001abegins onthe2ndhalfnote ofthemeasure, 2,if\t\u001c\u001alieson\nthe2ndor4thquarter note positions, etc.with aﬁnal category\n3for“other .”Wethen proceed tomodelH\u0018A\tUM I \f\u0016\u0019\n\u001d \u0001 H\u0018A\t\n\n\f\u000f\u000e\u0010\u000e\u000f\u000e\u0011\f\u0012\t\n!M I \f\u0016\u0019\n\n\f\u000f\u000e\u0010\u000e\u000f\u000e\u0010\f\u0015\u0019\n!\n\u001d\u0001\n!\n\u001d\u001a\u001f\u001e\n\nH\u0018\u001a\t\n\u001aM I \f\u0016\u0019\n\u001a\n\u001d(3)\u0001\n!\n\u001d\u001a\u001f\u001e\n\nH\u0018A,W\u0018\u001a\t\t\u001a\u000b\f\u0012I\n\u001dM \u0019\u0005\u001a\n\u001d \u0018A,W\u0018\u001a\t\u001a\n\f\u0012I\n\u001d\u0012\u001d (4)\ndef\u0001\n!\n\u001d\u001a\u001f\u001e\n\n\u0000\u0018!\u0018 ,J\u0018A\t\t\u001a\u000b\f\u001eI\n\u001dM \u0019\"\u001a\n\u001d \u0018A,W\u0018\u001a\t\u001a\n\f\u001eI\n\u001d\u0012\u001d\nwhere,W\u0018\u001a\t\n\u001a\f\u0012I\n\u001d7\u0001,W\u0018\u001a\t\n\u001a\f\u001e=&\f\u0012>?\f)'\n\u001d \u0001\n#$$$$$%$$$$$&\n\u0003('isrootofchord =&\f\u0012>?\f)'\u0007)'isthird ofchord =&\f\u001e> \f\u0012'@\n'isﬁfth ofchord =&\f\u0012>?\f)'\u0004*'inscale =&\f\u001e>\nbutnottriad =&\f\u001e> \f\u0012'+otherwise\nand\n \u0018A,\n\u001disthenumber ofchromatic pitches falling intothe,th\ncategory:\n \u0018\n\u0003\u0019\u001dO\u0001\n \u0018\n\u0007\u0014\u001dO\u0001\n \u0018A@\n\u001dO\u0001;\u0003;\n \u0018\n\u0004\u000b\u001d7\u0001,\u0004;\n \u0018\n+\u001d \u0001\n+.\nEqn. 3states that giventheharmonic label,I,thepitches\t\n\n\f\u0010\u000e\u000f\u000e\u000f\u000e\u0010\f\u001e\t !arerandom samples from their respecti verhythm-\nconditional (\u0019\n\u001a)distrib utions. While weexpect that thisas-\nsumption will seem familiar tomany,webelie veitisamong\nthemost problematic: theorder inwhich pitches appear clearly\naffects one’sharmonic perception. Wewill discuss apossible\nvariation onourmodel thatdoes notmakethisassumption in\nalater section. The assumption does, however,lead toasig-\nniﬁcant reduction inmodel comple xity.Eqn. 4states thatthe\nprobabilities ofobserving thecategories chord root, chord third,\nchord ﬁfth, non-triad scale tone, ornon-scale tone areﬁxedand\ndonotdepend ontheharmonic label. There areseveralchro-\nmatic pitches inthelatter twocategories andourassumption is\nthatwithin acategory thepitches willbeequally likely.So,for\ninstance, fornotes atagivenmeasure position, theprobability\nofobserving dintheIVchord ofcmajor isthesame asthatof\nb-intheIchord ofdminor .Additionally thepitches c*,d*,f*\na-,b-allhavethesame probability inthekeyofcmajor ,regard-\nlessoftheparticular chord, howeverthisprobability depends on\nthemeasure position.\nWedenote these “output” probabilities by\n\u0000\n!where\n\u0000\n!\u0018A,NM \u0019\n\u001dis\ntheprobability ofobserving apitch ofcategory,F\n\u001f0\u0003\f\u0010\u000e\u000f\u000e\u000f\u000e\u000f\f\n+\"\nforanote beginning atrhythmic position\u0019F\n\u001f\u0019%\f\u000f\u000e\u0010\u000e\u000f\u000e\u0011\f\n+\".These\nassumptions reduce thenumber ofparameters intherepresen-\ntation of\nH\u0018\u001a\tSM I \f\u0016\u0019\n\u001dto\n+4\r- \u0001@\n%.\n3TrainingtheModel\nOurpreference fortheHMM, and, more generally ,probabilis-\nticgraphical models, ispartly duetothewaythemodel param-\neters —thetransition probabilities parameters,\n\u0000\u0013\f\n\u0000\n\n\u0014\f\n\u0000\nE\u0014,and\ntheoutput distrib utions,\n\u0000.!,canbetrained fromunlabeled ex-\namples. Since ourmodel isbased onrhythm aswell aspitch,\nessentially anycollection ofMIDI ﬁles thatexplicitly represent\nboth rhythm andpitch canbeused fortraining. This isthecase\nformost MIDI ﬁles thatdonotcome from actual performances.Theessential idea ofthetraining isasfollo ws.Ifwehadacol-\nlection oflabeled data giving notjustthepitch andrhythm in-\nformation buttheharmonic labels aswell ,I\n\f\u000f\u000e\u000f\u000e\u0010\u000e\u000f\f\u0012I\u0013,then the\ntraining process would besimple. Forinstance,\n\u0000\u0013\u0018 '\u000b\u0006 M '\n\u001dcould\nbeestimated astheratio ofthenumber oftimes weobserv edthe\nchords '\r\f)'\u000b\u0006issequence (with common key)tothetotal number\noftimes weobserv ed '(with thenextchord inthesame key).\nSimilarly ,\n\u0000 !\u0018 ,WM \u0019\n\u001dwould beestimated astheratio oftheoftimes\nweobserv edanote ofrhythm category \u0019andpitch category ,\ndivided bythenumber oftimes weobserv edrhythm category\u0019.(Note thattheharmonic label must beknowntocompute,\n\u0001,J\u0018A\t \u001a \f\u001eI\n\u001d).Inpractice, wedon’tknowtheharmonic labels,\nbutgivenaconﬁguration ofmodel parameters, wecanestimate\nthem. Theidea oftheforw ard-backw ard,orBaum-W elch algo-\nrithm, istoiterati velyestimate thehidden labels andreestimate\nthemodel parameters. Itiswell knownthatthisisanexam-\npleofthemore general EMalgorithm formaximum likelihood\nestimation ofparameters inamixture model Rabiner (1993).\nThe harmonic labelsIS\n\r\f\u0010\u000e\u000f\u000e\u0010\u000e\u0011\f\u001eIW\u0013 areestimated through the\nforw ard-backw arditerations. Werecursi velydeﬁne theforw ard\nprobabilities,\n\u0001\u0015\n\u0018\u001aI\u0015\n\u001d,for\u0017\n\u0001 \u0003\f\u000f\u000e\u000f\u000e\u0010\u000e\u0011\f\n\u0002,I\u0015GF\n1by\u0001\n\b\u0018\u001aIN\n\u001d \u0001 H\u0018\u001aIU\n\u001d\u001aH\u0018A\t0\n\bM IN\n\u001d\u0001\u0015\u0014KL\n\u0018AI\u0015\bKL\n\u001d \u0001 \u0003\u0004\u0006\u0005\b\u0007\n\t\n\u0001\u0018\u001aI\u0015\n\u001d H\u0018\u001aI\u0015\u0014KL\nM I\u0015\n\u001d H\u0018A\t\u0015\bKL\nM I\u0015\bKL\n\u001d\nandthebackw ardprobabilities, \u000b\u0015\n\u0018\u001aI\u0015\n\u001d,for \u0017\n\u0001 \u0003\f\u000f\u000e\u000f\u000e\u0010\u000e\u000f\f\n\u0002,I\u0015GF\n1by\u000bJ\u0013 \u0018\u001aIW\u0013\n\u001d \u0001 \u0003\u000bJ\u0015 TU\n\r\u0018AIJ\u0015\u000bTS\n\u001d \u0001\n\u0003\u0004\n\u0005\u0007\n\t\n\u000bJ\u0015U\u0018\u001aIW\u0015\n\u001d H\u0018\u001aIW\u0015 M IW\u0015\u000bTU\n\u001d H\u0018\u001a\t\u0014\u0015 M IJ\u0015\n\u001d\nAstandard argument showsthat\n\u0001\u0015U\u0018AIJ\u0015\n\u001d#\u0001 H\u0018\u001aIJ\u0015N\f\u0012\t0\n\u0006\f\u0010\u000e\u000f\u000e\u0010\u000e\u000f\f\u001e\t/\u0015\n\u001d\nwhere thelatter isviewed asafunction of IN\u0015with the \t’sheld\nﬁxed;similarly , \u000bW\u0015U\u0018AIJ\u0015\n\u001d \u0001 H\u0018\u001aIW\u0015N\f\u001e\t\u0014\u0015\u0014KL\n\u0006\f\u0010\u000e\u000f\u000e\u000f\u000e\u0010\f\u001e\t\u0014\u0013\n\u001d.The\n\u0001and \u000b\nprobabilities lead tolabel probabilities throughH\u0018\u001aI\u0015\nM \t\n\f\u000f\u000e\u0010\u000e\u000f\u000e\u0011\f\u0012\t\u0013\n\u001d\u0001\nH\u0018\u001aIW\u0015U\f\u001e\t\u000b\n\u0006\f\u000f\u000e\u0010\u000e\u000f\u000e\u0011\f\u0012\t\u0014\u0015\n\u001d H\u0018\u001a\t\u0014\u0015\u0014KL\n\u0006\f\u0010\u000e\u000f\u000e\u000f\u000e\u0010\f\u001e\t\u0014\u0013 M IJ\u0015\n\u001d\f\u0004\u0006\r\u0005\nH\u0018AI\u0006\u0015\n\f\u0012\t\n\f\u0010\u000e\u000f\u000e\u0010\u000e\u0011\f\u001e\t\u0015\n\u001d\u001aH\u0018A\t\u0015\bKL\n\f\u000f\u000e\u000f\u000e\u0010\u000e\u000f\f\u0012\t\u0013\nM I\u0006\u0015\n\u001d\u0001\n\u0001\u0015U\u0018AIJ\u0015\n\u001d\u000bJ\u0015S\u0018AIJ\u0015\n\u001d\f\u0004\n\r\u0005\n\u0001\u0015\n\u0018\u001aI\u0006\u0015\n\u001d\u000b\u0015\n\u0018\u001aI\u0006\u0015\n\u001d\nThe probabilities\nH\u0018\u001aIW\u0015SM \t0\n\r\f\u000f\u000e\u000f\u000e\u0010\u000e\u000f\f\u0012\t\u0014\u0013\n\u001dfunction assurrogates for\nthetrueclass labels. Forinstance, inestimating theoutput dis-\ntributions if\nH\u0018\u001aIW\u0015SM \t\u000b\n\u0006\f\u000f\u000e\u0010\u000e\u000f\u000e\u0010\f\u001e\t\u0014\u0013\n\u001d \u0001 \u0003\u0006\u0005\b\u0007forsome particular stateI\u0015,then\t\u0015counts as1/2asample from theoutput distrib ution\nforI\u0015.More precisely ,wereestimate\n\u0000.!by\u0000\n!\u0018A,NM \u0019\n\u001d \u0001\n\f\u000f\u000e\u0006\u0010\u0012\u0011\u0014\u0013\u0005\u0016\u0015\n\u0004\n\u0005\u0018\u0017\u001e\n\u000e\u0015 \u0019\n\u0013\u0005\u001e\n\u0019\nH\u0018\u001aIJ\u0015LM \t0\n\r\f\u000f\u000e\u0010\u000e\u000f\u000e\u0011\f\u0012\t\u0014\u0013\n\u001d\f\u0019\n\u0013\u0005\u001e\n\u0019\nH\u0018AIJ\u0015 M \t0\n\u0006\f\u0010\u000e\u000f\u000e\u0010\u000e\u0011\f\u001e\t\u0014\u0013\n\u001d\nAsimilar argument leads toareestimate ofthetransition prob-\nabilities. Forinstance, wecancompute theprobabilitiesH\u0018\u001aIJ\u0015N\f\u0012IJ\u0015\bKQ\n\u0014M \t0\n\u0006\f\u0010\u000e\u000f\u000e\u0010\u000e\u000f\f\u001e\t/\u0013\n\u001d\u0001\n\u0001\u0015\n\u0018\u001aI\u0015\n\u001d\u000b\u0015\bKL\n\u0018\u001aI\u0015\bKL\n\u001d H\u0018AI\u0015\bKQ\nM I\u0015\n\u001d\u001aH\u0018A\t\u0015\bKL\nM I\u0015\bKL\n\u001d\f\u0004\n\r\u0005\b\u0015\n\u0004\n\r\u0005\u0006\u001a\b\u001b\n\u0001\u0015U\u0018AI\u0006\u0015\n\u001d\u000bJ\u0015\bKQ\n\r\u0018\u001aI\u0006\u0015\bKL\n\u001d H\u0018\u001aI\u0006\u0015\u0014KL\nM I\u0006\u0015\n\u001d H\u0018A\t\u0014\u0015\bKL\n\u0014M I\u0006\u0015\bKL\n\u001dandreestimate\n\u0000\nE\u0014\u0018A'\u0010\u0006 M '\n\u001dby\u0000\nE\u0014\u0018A'\n\u0006M '\n\u001d\u0001\n\f\n\u0010\u0013\u0005\u0015 \u001c\n\u0005\n\u0017\u001e\n\u0010\u0013\u0005\u001d\u001a\u0016\u001b\u0015 \u001c\n\u0005\u0006\u001a\b\u001b\n\u0017\u0015\n\u0014\u001e\u0005\u001e\n\u0014\u0015\n\u0014\u001e\u0005\u001d\u001a\u0016\u001b\u001e\n\u0014\u001e\r\nH\u0018\u001aIW\u0015U\f\u001eIW\u0015\bKL\n\u0014M \t0\n \u000e\u000f\u000e\u000f\u000e\u0010\f\u001e\t\u0014\u0013\n\u001d\f\n\u0010\u0013\u0005\u0015 \u001c\n\u0005\n\u0017\u001e\n\u0010\u0013\u0005\u0006\u001a\b\u001b\u0015 \u001c\n\u0005\u0006\u001a\b\u001b\n\u0017\u0015\n\u0014\u001e\u0005\u001e\n\u0014\nH\u0018\u001aIJ\u0015S\f\u001eIJ\u0015\u0014KL\n\bM \t0\nQ\u000e\u0010\u000e\u000f\u000e\u000f\f\u0012\t\u0014\u0013\n\u001d\nwhereI\u0015\n\u0001\u0018A=\u0015\n\f\u001e>\u0015\n\f\u0012'\u0015\n\u001dandI\u0015\bKL\n\u0001\u0018\u001a=\u0015\bKL\n\f\u001e>\u0015\bKL\n\f\u0012'\u0015\u0014KL\n\u001d.\nWecanalsoestimate\n\u0000\n\n\u0014and\n\u0000\u0010\u0013inananalogous manner .\n4Experiments\nWehaveperformed avariety oftraining experiments, asdis-\ncussed insection 3,involving around 5orsoshort movements\nandrequiring severalminutes ofcomputing ona1GHz Linux\nbox. Wegenerally perform around 5iterations ofthetraining\nalgorithm andlearn both theoutput probabilities,\n\u0000\n!,andthe\nchord transition probability matrix,\n\u0000\nE\u0014.Theremaining param-\neters ofthetransition probabilities,\n\u0000\u0013and\n\u0000\n\n\u0014,seem torequire\nlargertraining setstobereliably estimated; wehavesetthese\nparameters byhand intheexperiments here, butplan onauto-\nmatically learning them inthefuture.\nOnce themodel isinplace wecompute ourharmonic parse,\u001f I,\nasthemost likelylabeling giventhedata:\u001f I\n\u0001! \n\"$#&%' )(\u0004\nH\u0018\u001aIQM \t\n\u001d7\u0001* )\"+#,%' )(\u0004\nH\u0018\u001aI \f\u0012\t\n\u001d\nwhere \u001f I\n\u0001\u0018\u0006\u001f IU\n\r\f\u0010\u000e\u000f\u000e\u0010\u000e\u000f\f\u0016\u001f IW\u0013\n\u001d, I\n\u0001\u0018\u001aIN\n\b\f\u000f\u000e\u0010\u000e\u000f\u000e\u0011\f\u0012IJ\u0013\n\u001d,and \t\n\u0001\u0018\u001a\t\u000b\n\u0006\f\u000f\u000e\u0010\u000e\u000f\u000e\u0010\f\u001e\t\u0014\u0013\n\u001d.Itiswell-kno wnthattheglobal maximum,\u001f I,can\nbeconstructed with dynamic programming byletting-7\n\b\u0018\u001aIN\n\u001dO\u0001H\u0018AIN\n\b\f\u001e\t0\n\u001dO\u0001VH\u0018\u001aIU\n\u001d H\u0018\u001a\t0\n/M IN\n\u001dandrecursi velycomputing-Q\u0015U\u0018\u001aIW\u0015\n\u001d\n\u000e\u0010\u000f\u0012\u0011\u0001 %. )(\u0004\n\u001b\u00150/0/0/ \u0015\n\u0004\n\u0005)1\u0016\u001b\nH\u0018\u001aIU\n\r\f\u0010\u000e\u000f\u000e\u0010\u000e\u0011\f\u001eIW\u0015U\f\u001e\t0\n\u0006\f\u000f\u000e\u000f\u000e\u0010\u000e\u000f\f\u0012\t\u0014\u0015\n\u001d\u0001 %. \u0018(\u0004\n\u0005\u00181\u0016\u001b\n%. \u0018(\u0004\n\u001b\u00150/0/0/ \u0015\n\u0004\n\u0005\u00181\n2\nH\u0018\u001aIN\n\b\f\u000f\u000e\u0010\u000e\u000f\u000e\u0011\f\u0012IJ\u0015\u000bTS\n\r\f\u0012\t0\n\u0006\f\u0010\u000e\u000f\u000e\u0010\u000e\u0011\f\u001e\t\u0014\u0015 TU\n\u001dH\u0018AI\u0015\nM I\u0015\u000bTS\n\u001d\u001aH\u0018\u001a\t\u0015\nM I\u0015\n\u001d\u0001 %. \u0018(\u0004\u001d\u0005\u00181\u0016\u001b\n-\u0015\u000bTS\n\u0018AI\u0015\u000bTS\n\u001d H\u0018\u001aI\u0015\nM I\u0015\u000bTU\n\u001d H\u0018\u001a\t\u0015\nM I\u0015\n\u001d\nfor \u0017\n\u0001 \u0007\f\u000f\u000e\u0010\u000e\u000f\u000e\u0011\f\n\u0002.From thedeﬁnition of -\u0015itfollo wsthat%. \u0018(\u0004\nH\u0018AI \f\u0012\t\n\u001d \u00013%. )(\u000454-\u0013\n\u0018AI\u0013\n\u001d.The actual sequence \u001f I\n\u0001\u0018\u0006\u001f IU\n\r\f\u0010\u000e\u000f\u000e\u0010\u000e\u000f\f\u0016\u001f IW\u0013\n\u001dattaining thismaximum canbeidentiﬁed byper-\nforming acalculation parallel to - \u0015:wedeﬁne6\u0015U\u0018AIJ\u0015\n\u001d\n\u000e\u0018\u000f\u0012\u0011\u00017 \n\"$#,%. )(\u0004\n\u0005)1\u0016\u001b\n-Q\u0015\u000bTU\n\b\u0018\u001aIW\u0015\u000bTU\n\u001d H\u0018\u001aIJ\u0015LM IW\u0015\u000bTU\n\u001d H\u0018\u001a\t\u0014\u0015SM IJ\u0015\n\u001d\u0017\n\u0001 \u0007\f\u000f\u000e\u0010\u000e\u000f\u000e\u0011\f\n\u0002.The\n6\u0015functions lead totheoptimal state se-\nquence by\u001f IW\u0015\n\u0001\n6\u0015\bKQ\n\r\u0018\u0006\u001f IW\u0015\bKL\n\u001dwhere\u001f IJ\u0013\n\u00018 )\"+#9%. \u0018(-L\u0013 \u0018\u001aIW\u0013\n\u001d.\nWhile inmanyapplications thedynamic programming recur -\nsion isapproximated with a“beam search, ”thesizeofourstate\nspace allowsforfull-ﬂedged dynamic programming.\nWehavemade severalexamples ofourexperiments available\nontheweb at\nhttp://f afner .math.umass .edu/ismir03 .These include the\nﬁrstmovement ofHaydn Piano Sonata 6,theChopin Raindrop\nPrelude inDﬂat,Op.28,no.15,andtheDebussy Prelude from\nSuite Bergamasque. Ouranalysis isrepresented asaMIDI ﬁle\nofamechanical piano performance with theseries ofchords\nproduced byouralgorithm superimposed assustained harmon-\nicachords. Inaddition, textmessages arewritten outgiving\ntheharmonic label asa(roman numeral, tonic, mode) triple,Label\nVoice 1\nVoice 2\naligned tohighlight keychanges. Themessages arewritten as\nchord changes occur ,essentially annotating theMIDI perfor -\nmance inreal-time. Allthree examples mentioned aboveare\nin4/4time tofacilitate auniform deﬁnition oftherhythm vari-\nables expressing the“strength” ofmeasure positions, \u0019\r\u0015.Inthe\nHaydn andChopin examples weonly allowed harmonic transi-\ntions tooccur on2beat boundaries. This wasrelax edto1beat\nboundaries intheDebussy example, andresults inasome what\noveranalyzed labeling. Asmentioned insection 2,thechoice\nofpossible chords issome what arbitrary .Inthese experiments\nwehaveadded thedominant 7thchord tothesevenbasic triads.\nSome basic extensions, such asfully diminished 7thchords and\nchords inminor mode builtontheﬂatseventh scale degree, are\nneeded inthese experiments; however,more exotic additions\nsuch asaugmented sixth chords, andNeapolitan chords arepos-\nsible.\nThese examples arerepresentati veofthemore successful ap-\nplications ofourprogram. However,theyarecomparable to\nthemajority ofcases wehaveexamined inwhich ourprogram\nproduces aplausible interpretation. (The lesssuccessful results\nseem tomostly becompositions with verysparse textures). We\nbelie vethatthese results arequite promising, especially taking\nintoaccount thesimplicity ofourapproach. Inaddition wefeel\nthere issigniﬁcant potential forimpro vement with more careful\nattention tomodeling subtleties andlarger-scale training exper-\niments. One ofourfuture goals istohaveaprogram onthe\nweb thatwillautomatically analyze avisitor’ ssubmission and\nproduce anannotated MIDI ﬁle,asabove.\nAtthispoint wedonotofferanyobjecti vemeasure ofsuccess,\nsuch as“error rate,”orcomparison ofourresults. This isdue,\ninpart, tothedifﬁculty indeﬁning andobtaining “correct” har-\nmonic analyses. However,wealso belie vethatrather straight-\nforw ardcontinued efforts may lead tosigniﬁcant impro vements\nandthatevaluation willbemore appropriate atalater stage.\n5Extending theModel\nToourmind, themost troubling modeling assumption wemake\nistheconditional independence ofpitches: inessence, thecol-\nlection ofpitches associated with achord isarandom sample\nfrom some distrib ution. This assumption disre gards theway\nmusic isusually composed ofindependent parts orvoices that\nobeyaninternal logic such asapreference forscales andarpeg-\ngios. Giventheoften unvoiced nature ofMIDI data andourcur-\nrentfocus onpiano music, wehavebegunwith asimple model\nthatdoes notrequire voicing information. However,wenow\npropose amodel thatregards thedata asacollection ofvoices\nwhere theevolution ofeach voice isconditionally independent\noftheothers, giventheharmonic state.\nAutomatically partitioning MIDI data intovoices is,nodoubt,\nachallenging problem ifonerequires thevoicing tobeidentical\ntothetruevoicing, ifoneexists, ortheground truth supplied byamusician. Butitisrather simple tocreate analgorithm that\nperforms reasonably .Weuseasimple dynamic programming\nalgorithm maximizing afunction measuring theplausibility of\navoice partition. Other possibilities exist, such asinKilian\n&Hoos, (2002). Weassume here thatwebeginwith voiced\ndata, either from anofﬁcial oralgorithmic source. Inparticular ,\nwebeginwith acollection ofmonophonic overlapping voices\nwith noassumption about thenumber ofvoices thatoverlap at\nanyparticular time ortherange ofpitches associated with a\nparticular voice.\nSuppose that \t\n\f\u000f\u000e\u000f\u000e\u0010\u000e\u000f\f\u0012\t\u0013isasequence ofpitch classes, repre-\nsented asnumbers in\n\u001f\u0019%\f\u000f\u000e\u0010\u000e\u000f\u000e\u0011\f\n\u0003\u0014\u0003\",corresponding toasingle\nvoice. Let D5\n\u0006\f\u0010\u000e\u000f\u000e\u0010\u000e\u0011\f\u001eD#\u0013 bethesequence of(key,chord) vari-\nables, assumed tobeaMark ovchain asbefore. Wecontinue\ntoassume that \t/\u0015istheobserv ation ofarandom variable\nR\u0015,\nbutunlik ebefore wenowassume thatthedistrib ution of\nR\u0015de-\npends on D \u0015and\nR\u0015\u000bTU\n,rather than just D#\u0015.Figure 5showsa\ngraphical representation ofsuch amodel containing twocondi-\ntionally (onD)independent voices.\nSuch amodel, suitably trained, would understand avoice’ s\npreference forscales within thekey,arpeggios within the\n(key,chord) pair,andtendencies regarding theresolution ofnon-\nchord tones. These preferences should assist indistinguishing\nbetween various chord hypotheses, givendata.\nWhile this model isnotanHMM, ithasalinear structure\namenable toananalogous training algorithm aswell astheiden-\ntiﬁcation ofthemost likelystate sequence. Thus themodel is\neverybitascomputationally tractable astheHMM.\nReferences\n[1]PickensJ.,Bello J.B.,Monti G.,Crawford T.,Dovey\nM.,Sandler M.,&Byrd D.,(2002) “Polyphonic Score\nRetrie valUsing Polyphonic Audio Queries: AHarmonic\nModeling Approach, ”Proceedings oftheThirdInterna-\ntionalConferenceonMusicInformation RetrievalParis,\nFrance, 2002.\n[2]Pardo B.,(2002) “Algorithms forChordal Analysis, ”\nComputer MusicJournal ,vol.26,no.2,2002.\n[3]Temperle yD.&Sleator D.,(1999) “Modeling Meter and\nHarmon y:APreference Rule Approach, ”Computer Mu-\nsicJournal ,vol15,no.1,1999.\n[4]Krumhansl, C.,(1990), “Cogniti veFoundations ofMusi-\ncalPitch, ”Oxford University Press, Oxford, 1990.\n[5]Kilian J.&Hoos H.,(2002) “Voice Separation —ALo-\ncalOptimisation Approach, ”Proceedings oftheThirdIn-\nternational ConferenceonMusicInformation Retrieval\nParis, France, 2002.\n[6]Barthelemy J.,&Bonardi A.,(2001) “Figured Bass and\nTonality Recognition, ”Proceedings oftheSecondInterna-\ntionalConferenceonMusicInformation RetrievalBloom-\nington, Indiana, 2001.\n[7]L.Rabiner ,(1993), “ATutorial onHidden Mark ovModels\nandSelected Applications inSpeech Recognition, ”Pro-\nceedingsoftheIEEE, 77,257–286, 1993."
    },
    {
        "title": "Using morphological description for generic sound retrieval.",
        "author": [
            "Julien Ricard",
            "Perfecto Herrera"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1416042",
        "url": "https://doi.org/10.5281/zenodo.1416042",
        "ee": "https://zenodo.org/records/1416042/files/RicardH03.pdf",
        "abstract": "Systems for sound retrieval are usually “source- centred”. This means that retrieval is based on using the proper keywords that define or specify a sound source. Although this type of description is of great interest, it is very difficult to implement it into realistic automatic labelling systems because of the necessity of dealing with thousands of categories, hence with thousands of different sound models. Moreover, digitally synthesised or transformed sounds, which are frequently used in most of the contemporary popular music, have no identifiable sources. We propose a description framework, based on Schaeffer’s research on a generalised solfeggio which could be applied to any type of sounds. He defined some morphological description criteria, based on intrinsic perceptual qualities of sound, which doesn’t refer to the cause or the meaning of a sound. We describe more specifically experiments on automatic extraction of morphological descriptors. 1",
        "zenodo_id": 1416042,
        "dblp_key": "conf/ismir/RicardH03",
        "keywords": [
            "source-centred",
            "sound retrieval",
            "keywords",
            "sound models",
            "digital synthesised sounds",
            "popular music",
            "Schaeffers research",
            "generalised solfeggio",
            "morphological description criteria",
            "intrinsic perceptual qualities"
        ],
        "content": "Using Morphological Description for Generic Sound Retrieval  \nJulien Ricard and Perfecto Herrera  \nMusic Technology Group  \nPompeu Fabra University  \nBarcelona, Spain  \njricard, pherrera@iua.upf.es  \n Abstract  \nSystems for sound retrieval are usually “source -\ncentred”. This means that retrieval is based on using \nthe proper keywords that define or specify a sound \nsource. Although this type of description is of great \ninterest, it is very difficult to implemen t it into \nrealistic automatic labelling systems because of the \nnecessity of dealing with thousands of categories, \nhence with thousands of different sound models. \nMoreover, digitally synthesised or transformed \nsounds, which are frequently used in most of th e \ncontemporary popular music, have no identifiable \nsources. We propose a description framework, based \non Schaeffer’s research on a generalised solfeggio \nwhich could be applied to any type of sounds. He \ndefined  some morphological description criteria, \nbased  on intrinsic perceptual qualities of sound, \nwhich doesn’t refer to the cause or the meaning of a \nsound. We describe more specifically experiments on \nautomatic  extraction of morphological descriptors.  \n1 Introduction  \nWhen building a sound description system , the first issue to be \naddressed is defining the aspect of the sound we want to \ndescribe,  that is  the information we want to provide about this \nsound. A sound can be interpreted in many different ways \ndepending both on the application and on the sound itsel f. \nWhen analysing speech, for example, a system  can aim at  \nextracting  some information about the speaker (causal \ndescription, i.e. description of the origin of the sound), such as \nwhether he is a male or a female or his age, or at  grasping  the \nmeaning of w hat is being said (semantic description). In the \ncase of musical signals, semantic description is not well -\ndefined  and some specific features can be described (melody, \nrhythm…).  \nIn sound retrieval systems, the aim of the description is to ease \nthe search f or a desired sound by providing a simple \nrepresentation of it according to one or several description \ncriteria. Typical systems offer textual search facility for sounds labelled according to their origin (e.g. “bird” or \n“creaking door”). More elaborate sys tems allow refining the \nsearch by adding information at other description levels \nthrough more textual labels (e.g. “violin + E# + vibrato” or \n“voice + sad”) or visual representations; in other cases they \npropose search by similarity, comparing directly som e features \nof a sound example to all sounds in the database  and retrieving \n“the most similar”, whatever it may mean.  \nFrom our point of view, some important issues need to be \naddressed when looking at current systems. First, no universal \ndescription scheme exists. Applying semantic description to \nmusical sounds or classical musical description to speech \nwouldn’t make much sense. Having a general description \nscheme would allow handling all types of sound in the same \nway. Moreover, source recognition systems s till don’t perform \nwell enough and, in most cases and especially for non -musical \nsounds, labelling is done manually, which is very time -\nconsuming. A general description scheme, based on a few \nperceptual properties common to all sounds, could ease the \nlabel ling or could even be used directly for the search. \nAnother issue regarding the possibility to decompose sounds \nin several perceptual dimensions (e.g. roughness or noisiness) \nwould be to retrieve sounds by specifying only one or a couple \nof them . \n2 Schaeffer  typo -morphology  \nIn his Traité des objets musicaux  (Treatise on musical objects)   \n(synthesised and commented by Chion (1983)), Schaeffer \n(1966) proposes a generalization of what is usually heard as \nmusical sounds (typically notes generated by traditional \nmusical instruments) by considering all kind of sound objects, \ndisregarding their origin (electronic sounds, noise, \nenvironmental sounds, loops…). After performing some \nlistening experiments, he proposed a sound classification \n(typology ), independent from t he meaning or the source  of the \nsounds, according to some intrinsic perceptual properties \n(morphological  criteria)  described below : \n• Mass : related to the perception of the pitchiness  of a \nsound, and then to its spectral distribution . \n• Harmonic timbre : “the m ore or less diffuse halo \nassociated to the mass and more generally what \nallows describing it ” (Schaeffer, 1966, p. 516) . We \ninterpreted this definition as a finer characterisation \nof the spectral distribution, often described by \nanalogy to vision: bright/d ull, round/sharp…  \n• Grain : defined as the microstructure of sound matter , Permission to make digital or hard copies of all or part of this work for \npersonal of classroom use is granted without fee provided that copies are not \nmade or distributed for profit or commercial advantage and that copies bear \nthis notice and the full citation on the first page.   2003 The Johns Hopkins \nUniversity.  such as the rubbing of a bow.  \n• Dynamics : energy temporal evolution . \n• Allure : amp litude or frequency modulation.  \n• Melodic profile : variation of the pitch sensation.  \n• Mass profile : variati on within the mass (e.g.  pitched \nto complex ). \n3 Computational morphological description  \nThere has been very little research on automatic  \nmorphological description. The main work  done in that area is \nthe ECRINS project on sound samples audio content \ndescripti on (Geslin, Mullon , and Jacob, 2002) . A \nmorphological description scheme is defined, in which sounds \nare automatically described according to the following \ndescriptors: dynamical profile (amplitude evolution), melodic \nprofile (pitch evolution), attack  type , note pitch, spectral \ndistribution, space ( sound location and movement) and texture \n(vibrato, tremolo , and grain). The description can then be \nrefined manually by the user.   \nThe classification allowed by the automatic description is \nquite limited and it seems possible to complete it by further \nanalysis  and by adding new description criteria . In order to \nfurther investigate computational morphological description, \nwe specifically considered three  criteria: mass, mass profile, \nand dynamic s. \nThe mass  describe s the pitchiness  of a sound. It is estimated \nusing pitch salience, as defined by Slaney  (1994) . This \ndescriptor gives an estimation of the signal periodicity by \ncomparing  the amplitude of the largest peak to the zero -lag \npeak (power) in short -term autocorr elations.  \nThe mass profile  describes whether the sound mass  varies or \nnot. We used the mean and the variance of the smoothed pitch \nsalience for classifying sounds into two mass profile classes: \nvarying  or unvarying .  \nThe dynamic s describes the type of amp litude envelope of the \nsound object. We defined four classes: unvarying , varying -\nimpulse , varying -iterative  (sound object with several \ntransients) and varying -other . We used four descriptors, \nderived from the amplitude envelope and chosen intuitively \naccor ding to specificity of each class:  \n• A ‘balance coefficient’, describing how much the \ncentre of gravity of the amplitude envelope is off -\ncentre.  \n• Size of the middle part of the envelope  (between the \nattack and the release) .  \n• Number of high amplitude derivativ es: peaks in the \nmiddle part of the amplitude envelope (so that the \nfirst attack is not taken into account) above a \nthreshold. Only iterative sounds are supposed to have \nsuch peaks.  \n• Mean values of high amplitude derivatives . \n4 Evaluation and discussion  \nEvalu ation of perceptual sound quality is a difficult task. \nThere is no ground truth, such as in source recognition, and \nsome given quality can be perceived differently according to \nthe listener. However, we considered that morphological criteria  could be regar ded as rather listener -independent : the \nclasses we defined for each criterion seem sufficiently \ndistinguishable for a sound to be quite confidently classified in \none rather  than  in another.  \nIn order to evaluate our system, we built a small database \n(aroun d 50 sounds for each class) for each morphological \ncriterion. Sounds were manually labeled according to the \nclasses defined. We performed no segmentation so that each \nfile was considered as a sound object, whatever it contained \n(sequence of musical notes, street atmosphere, dog bark…). \nThe tests were done independently for each morphological \ndescriptor, on half the database using a decision  tree trained \non the other half. Each test showed around 80% correctly \nclassified sounds, and the rules obtained were t he expected \nones (for example, sounds with negative balance coefficient, \nshort middle part and no high derivatives were classified as \nimpulse).  \nFurther  work  include s refining current morphological \ndescriptors  and improving our classification models by \nbuilding a much larger database . In order to get a more \ncomplete description of perceptual sound qualities , new \ndescriptors, inspired by remaining Schaeffer’s morphological \ncriteria or by other research in sound perception (e.g. grain, \nmodulation features…), should also be added. We are \ncurrently working on automatically describing the melodic \nprofile and detecting  more amplitude envelope classes.  \n5 Conclusion  \nWe think  that morphological description could be a good \ncomplement to traditional source -centred sound retrieval \nsystems . It provides a universal description scheme according \nto meaningful perceptual sound qualities that could be queried \ndirectly or used in addition to other criteria, as  a pre -filtering \nor refining stage for retrieval in large sound databas e. We \nshowed that a few low -level descriptors allow automatically \nclassifying sounds in a simplified 3 -dimensional typo -\nmorphology with good performance.  The  results obtained are \nencouraging, and m any issues will be considered in order to \nimprove the curre nt system.  \nAcknowledgements  \nPart of this research has been supported by the European IST \nproject CUIDADO.  \nReferences  \nChion , M. (19 83). Guide des objets sonores . Paris: INA-\nGRM/Buchet -Chastel . \nGeslin, Y., Mullon, P. , & Jacob, M.  (2002 ). Ecrins  : an audio -\ncontent description environment for sound samples . In \nNordahl, M. (Ed.), Proceedings of  2002 International \nComputer Music Association (pp. 581 −590). Göteborg, \nSweden.  \nSchaeffer, P. (1966). Traité des objets musicaux . Paris: Seuil.  \nSlaney, M. (1994). Auditor y Toolbox: a Matlab toolbox for \nauditory modeling. Apple Computer technical report #45, \n1994."
    },
    {
        "title": "Design patterns in XML music representation.",
        "author": [
            "Perry Roland"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417445",
        "url": "https://doi.org/10.5281/zenodo.1417445",
        "ee": "https://zenodo.org/records/1417445/files/Roland03.pdf",
        "abstract": "Design patterns attempt to formalize the discussion of recurring problems and their solutions. This paper introduces several XML design patterns and demonstrates their usefulness in the development of XML music representations. The patterns have been grouped into several categories of desirable outcome of the design process – modularity, separation of data and meta-data, reduction of learning requirements, assistance to tool development, and increase in legibility and understandability. The Music Encoding Initiative (MEI) DTD, from which the examples are drawn, the examples, and other materials related to MEI are available at http://www.people.virginia.edu/ ~pdr4h/. 1",
        "zenodo_id": 1417445,
        "dblp_key": "conf/ismir/Roland03",
        "keywords": [
            "Design patterns",
            "formalize recurring problems",
            "solutions",
            "XML design patterns",
            "XML music representations",
            "development",
            "categories",
            "modularity",
            "separation",
            "learning requirements"
        ],
        "content": "Design Patterns in XML Music Representation \n Perry Roland \nDigital Library Research & Development Group \nUniversity of Virginia \nCharlottesville, VA 22903-4149 USA \npdr4h@virginia.edu  \n \nAbstract \nDesign patterns attempt to formalize the discussion \nof recurring problems and their solutions. This paper \nintroduces several XML design patterns and \ndemonstrates their usefulness in the development of \nXML music representations. The patterns have been \ngrouped into several categories of desirable outcome \nof the design process – modularity, separation of data \nand meta-data, reduction of learning requirements, \nassistance to tool development, and increase in \nlegibility and understandability. The Music Encoding \nInitiative (MEI) DTD, from which the examples are \ndrawn, the examples, and other materials related to \nMEI are available at http://www.people.virginia.edu/ \n~pdr4h/. \n1 Introduction \nDesign patterns attempt to formalize the discussion of \nrecurring problems and their solutions (Gamma, Helm, \nJohnson & Vlissides, 1995). Since common problems with \n(hopefully!) common solutions occur in many domains, \npatterns are used in almost every part of development. Design \npatterns are an effective way to share design decisions that \nactually work. \nSeveral XML design patterns, drawn mostly from (Lainevool, \nn.d.), are introduced and their usefulness in the development \nof XML music representations is demonstrated. Because XML \nSchemas do not support specialized entities and parametric \nreferences, limiting the user's ability to extend the schema in \nan ad hoc fashion, and because they are more verbose and \nmore complex (Valentine, Dykes & Tittel, 2002), this paper \nconcentrates on the design of a (mostly) data-centric, i.e., \ndesigned to be processed by a machine rather than read by a \nhuman, Document Type Declaration (DTD). Of course, there \nare additional design patterns that are not covered. It is, \nhowever, the author's hope that enough patterns have been \ncovered to generate additional discussion of the design of \nDTDs for music representation. The Music Encoding Initiative (MEI) DTD, from which the \nexamples are drawn, is both philosophically and technically \nrelated to the Text Encoding Initiative DTD (TEI, 2002). The \ncomplete DTD, the examples, and other materials related to \nMEI are available at http://www.people.virginia.edu/~pdr4h/.  \nReaders unfamiliar with XML may wish to refer to a general \nintroduction, such as The XML Companion (Bradley, 2000). \n2 Modular Design \nModular design offers several advantages. First, it eases the \nburden of DTD maintenance by isolating changes. In addition, \nextensibility is increased since any module may be replaced in \nits entirety at will. Also, when each DTD module declares a \nsingle document element, it may be used independently of the \nmain DTD, allowing the creation of the markup to be divided \namong several authors who need not have knowledge of the \nentire DTD. Finally, it enables the utilization of significant \nportions of the markup, such as the file header or MIDI \nperformance data, in other contexts. The primary technique for \nachieving a modular design is to employ multiple secondary \nDTDs. The principal DTD typically takes the form of a \n\"driver\" which references the subsidiary modules. \n3 Separation of data and meta-data \nIt is generally agreed that a clear separation between data and \nmeta-data is highly desirable. In fact, separating these is the \nprimary goal of DTD design as the separation itself reflects \nthe worldview of the DTD creator. In other words, it clarifies \nthe intended use of the DTD as well as the structure of any \nmarkup that is to be validated against the DTD. In addition, \nseparating meta-data from its corresponding data allows the \nmeta-data to be shared with other entities to which it also \napplies. \nElements are best used for structurally significant information \nwhile attributes are best used for atomic characteristics of an \nobject that have no identity of their own. In other words, an \nattribute models part of the internal state of an object. (Stuhec, \n2002)  When an atomic characteristic is repeatable or must \nhave an internal hierarchical structure, it may be represented \nby an element.  However, these requirements can often be \nsatisfied through the use of multiple attributes or repetition of \nthe principal object with different attributes. \nJust as the entity->property, or \"has a\", relationship is best \nmodeled using attributes for each property, relationships Permission to make digital or hard copies of all or part of this work for \npersonal of classroom use is granted without fee provided that copies are not \nmade or distributed for profit or commercial advantage and that copies bear \nthis notice and the full citation on the first page.  2003 The Johns Hopkins \nUniversity. among entities, such as those characterized as \"is a\", \n\"complements\", etc., are best expressed via attributes as well. \n(Jelliffe, 1998) The representation of multiple hierarchies, that \nis, multiple parent-child relationships among a set of entities, \nparticularly benefits from this kind of treatment. \n4 Reduction Of Learning Requirements \nWith large DTDs, users are required to learn the content \nmodels and attributes for a large number of entities. Aside \nfrom simply reducing the number of entities, making the DTD \nless useful, there are a number of techniques that can be used \nto reduce the learning requirements of a new user and assist \nthe experienced user in remembering the details of how to \napply the DTD. \nNew users often require longer, descriptive names while \nexperienced users prefer shorter, mnemonic ones. Localized \nnames, i.e., in a different language, for a special user group, \nsuch as students or application programmers, or for reduction \nof storage requirements, are also often necessary. \nFurthermore, the name of an entity may change during the \nDTD development process or may even change between \nstages of encoding or processing. \nPerhaps even more important than the selection of appropriate \nidentifiers, is the selection of entities that match the \ngranularity of the problem domain.  In addition to the \ntechniques listed in section 3, proper use of container and \ncollection elements increases usability. \n5 Assisting Tool Development \nWhile a DTD should ideally be designed without any \nparticular application in mind, in practice, without some \ngeneral idea about how the data will be used, there's no point \nin creating the DTD in the first place! \nIn a data-centric representation, attributes are easier to \nmechanically process.  Also, since accessing attributes does \nnot require recursive or iterative processing, unlike embedded \nelements, using them may result in speedier processing. \nMachine processing of a representation can be improved by \nalso encoding the meta-data before the data it refers to. For \nexample, because an HTML table's width and height are \nrecorded before the actual table data, the table can be laid out \nand the data \"dropped in\".  Similarly, if the meta-data for a \nmusic score is encoded before the actual data, such as notes \nand rests, then rendering decisions can be made much more \neffectively than if encoding intermingles the data and the \nmeta-data. \nAlso, when meta-data is recorded first, several entities may \nrefer to a single instance of the meta-data, reducing the size \nand complexity of the markup instance as well as the tool to \nmanipulate it. \nBecause all of the ways in which it might be processed cannot \nalways be foreseen, flexibility and extensibility should be built \ninto a DTD.  Extensibility is defined as the ability to add to an \nelement's content model while leaving the basic declaration \nunchanged.  Flexibility is defined as editing an entire content \nmodel for a particular purpose – the basic content model may \nbe extended or restricted.  These qualities are necessary in order to extend the useful life of the DTD.  This is especially \nimportant for music representation because the repertoire to be \nencoded is vast and encoding is expensive.  Because XML \nSchemas do not support parameter entities, this level of \nflexibility is difficult to implement using them.  The usual way \nof providing an extensible content model in a DTD is to \nintroduce a parameter entity into an element's content model.  \nA similar degree of flexibility and extensibility may be \nachieved for attributes by declaring most, if not all, attributes \nfor an element inside a parameter entity as well. \n6 Increasing Legibility And Understandability \nIf a DTD is well designed, its useful lifetime will probably \nextend beyond the lifetime of the tools that use it.  Therefore, \ndespite the existence of tools that hope to hide the details of \nthe DTD from the inexperienced, eventually someone, \nsomewhere must be able to read and understand it, even if it is \njust to write another tool of the same sort.  For this reason, it is \nwise for DTD creators to employ design patterns that increase \nthe legibility and understandability of the DTD. \nA flyweight (Gamma, et al., 1995) abstracts common markup \ndeclarations into a single reusable entity.  Using a flyweight \ndesign pattern can ease the burden of DTD maintenance by \neliminating errors that might be introduced when the same \ninformation is repeated and by reducing the size of the DTD. \nSince the usability of a DTD depends to a certain extent on \nconsistency, often a set of attributes is provided that can be \nplaced on all elements, or on significant subsets of elements.  \nIf users can expect common attributes on every element, their \nability to use the DTD can be enhanced.  Also, it is easier to \nprocess a document that has a consistently applied set of \ncommon attributes.  The common attributes are typically \ndeclared in a flyweight in order to enhance maintainability. \n \nReferences \nBradley, N. (2002). The XML Companion (3rd ed.). London: \nAddison-Wesley. \nGamma, E., Helm, R., Johnson, R, & Vlissides, J. (1995). \nDesign Patterns: Elements of Reusable Object-Oriented \nSoftware. Reading, MA: Addison Wesley. \nJelliffe, R. (1998). The XML and SGML Cookbook: Recipes \nfor Structured Information. Charles F. Goldfarb Series on \nOpen Information Management. Upper Saddle River, NJ: \nPrentice Hall PTR. \nLainevool, T. (n.d.). XML Patterns.com Home Page. \nRetrieved May 2, 2003, from http://www.XMLPatterns.com/.  \nStuhec, G. (2002). Elements versus Attributes in Position \nPapers. Retrieved May 2, 2003 from http://oasis-\nopen.org/committees/ubl/200203/ndrsc/review/draft-\nndr-20020316.pdf. \nTEI Consortium (2002). TEI Guidelines. Retrieved May 2, \n2003 from http://www.tei-c.org/Guidelines2/ \nindex.html. \nValentine, C., Dykes, L., & Tittel, E. (2002). XML Schemas. \nAlameda, CA."
    },
    {
        "title": "Music Notation as a MEI Feasability Test.",
        "author": [
            "Baron Schwartz"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1416136",
        "url": "https://doi.org/10.5281/zenodo.1416136",
        "ee": "https://zenodo.org/records/1416136/files/Schwartz03.pdf",
        "abstract": "This project demonstrated that enough information can be retrieved from MEI, an XML format for mu- sical information representation, to transform it into music notation with good fidelity. The process in- volved writing an XSLT script to transform files into Mup, an intermediate format, then processing the Mup into PostScript, the de facto page description language for high-quality printing. The results show that the MEI format represents musical information such that it may be retrieved simply, with good recall and precision. 1",
        "zenodo_id": 1416136,
        "dblp_key": "conf/ismir/Schwartz03",
        "keywords": [
            "XML format",
            "musical information representation",
            "XSLT script",
            "Mup",
            "PostScript",
            "high-quality printing",
            "musical information",
            "good fidelity",
            "mechanical transcription",
            "musical notation"
        ],
        "content": "Music Notation as a MEI Feasibility Test\nBaron Schwartz\nUniversity of Virginia\n268 Colonnades Drive, #2\nCharlottesville, Virginia 22903 USA\nbps7j@cs.virginia.edu\nAbstract\nThis project demonstrated that enough information\ncan be retrieved from MEI, an XML format for mu-\nsical information representation, to transform it into\nmusic notation with good ﬁdelity. The process in-\nvolved writing an XSLT script to transform ﬁles into\nMup, an intermediate format, then processing the\nMup into PostScript, the de facto page description\nlanguage for high-quality printing. The results show\nthat the MEI format represents musical information\nsuch that it may be retrieved simply, with good recall\nand precision.\n1 Introduction\nMost uses of musical information require storage and retrieval,\nusuallyinﬁles. Unfortunately,ﬁlescreatedforonepurposemay\nnotbeusableforothers,andtheﬁlesmaybeincompatibleeven\nwhen the uses are similar. This is true of the many ﬁle formats\nthat exist. Because each fails to address some need adequately,\nandbecausetheformatsarenotextensible,thereisapluralityof\nformats,leadingtovastbodiesofcomputer-encodedknowledge\nthatcannotbesharedeffectively. Itisoftenpossibletotranslate\nbetweenformats,butunlessthedataistrivial,someinformation\nis usually lost.\nThe need for a universal format is self-evident; one would like\nto encode data once and use it many times for many purposes.\nPerryRoland,aresearcherattheUniversityofVirginia’sDigital\nLibrary project, is developing MEI (Music Encoding Initiative)\ninto such a format (Roland, 2002). MEI uses XML (Extensi-\nble Markup Langauge), a universal data-interchange syntax, to\ndeﬁne a musical-data encoding.\nMEI is different from existing formats in that it supports all\nof the widely identiﬁed “domains” in which music exists: vi-\nsual, gestural, performance, and analytical. Most existing lan-\nguages only address the visual domain. The other major XML\nformat,MusicXML,isdesignedasanotationalinterchangefor-\nmat (Good, 2001). MEI deﬁnes music as an abstract concept,\nPermission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided that\ncopies are not made or distributed for proﬁt or commercial advan-\ntage and that copies bear this notice and the full citation on the ﬁrst\npage.c/circlecopyrt2003 Johns Hopkins University.something more general than either notation or performance,\nbut does not leave important domains, such as notation, up to\nthe implementer. The MEI format’s design also allows a pro-\ncessing application to ignore information it does not need and\nextract the information of interest. For example, the format in-\ncludesinformationaboutpagelayout,butaprogramformusical\nanalysis can simply ignore it.\nGenerating printed music notation is a revealing test of MEI’s\ncapabilities because notation requires more information about\nthemusicthandootherdomains. Successfullycreatingnotation\nalso conﬁrms that the XML represents the information that one\nexpects it to represent. Because there is a relationship between\ntheinformationencodedin both theMEIandMupﬁles and the\nprinted notation, the notation serves as a good indication that\nthe XML really does represent the same music as the notation.\n2 Methods\nMusicnotationissocomplicatedtocreatethatitwasinfeasible\nto write a program to view MEI as notation directly for this\nproject. However, it was a fairly simple matter to transform\nMEI into Mup, a plain-text format that represents commands\nto the Mup interpreter. This has its disadvantages: it means\nlearning another notation ﬁle format, it involves several lossy\nsteps,anditsubjectstheresultingnotationtoanyconstraintsof\nthe notation software. However, it is a reasonable way to check\nMEI’s basic capabilities.\nBecauseMEIﬁlesarewritteninXML,theeasiestwaytotrans-\nform them to Mup notation is with XSLT , a functional pro-\ngramming language written in XML syntax and designed for\nXML transformations. This involves writing XSLT templates,\nwhicharemappingsthatspecifythedesiredoutputformatfora\ngiven input, for each type of element in the MEI ﬁle. The tem-\nplatesdeﬁnethetransformationtotheequivalentMupnotation;\nthus the relationship between MEI and Mup syntax is deﬁned\nformally by the XSLT script. Unfortunately, the Mup syntax is\nnotdeﬁnedformally,soitisnotpossibletoverifyformallythat\nMEI is equivalent to the resulting PostScript notation ﬁle.\n3 Test Pieces\nTo test the transformations, it was necessary to transform some\ncomplicatedpieces. Thefollowingpiecesareusedtotesttheca-\npabilities of various encoding formats (Selfridge-Field, 1997).\nThe examples transformed are rendered very similarly to the\noriginal. In most cases, the XML ﬁles from which these pieceswere produced are not specially “tweaked,” other than scaling\nthe notation to ﬁt correctly onto the page. Exceptions are noted\nin the text.\nPerry Roland encoded these test pieces and furnished them for\ntesting. They may be found in several formats at the MEI web-\nsite,\nhttp://dl.lib.virginia.edu/bin/dtd/mei/ .\n3.1 The Mozart Trio\nThis example is taken from the second trio section of Mozart’s\nClarinet Quintet. The challenge is transposing the ﬁrst staff,\nwhich is notated in C Major but, since it is played on an A\nclarinet, is actually in A Major. It is very well rendered on the\nwhole. There is a phrase or tie that begins on the last note of\nthe ﬁrst staff, but since there is no note for it to extend to, Mup\nignoresthis. Mupalsoplacessomephrasemarksoddly,suchas\nthe phrase mark on the tuplet, and has trouble with phrases that\ncross system breaks.\n3.2 The Mozart Piano Sonata\nThis example has mixed durations within chords, grace notes\npreceding chords, and arpeggiated grace notes. The arpeggio\non staff 1 in the ﬁrst measure also crosses voices. In this case,\nMup’s default placement of the slurs on the left-hand grace\nnotes is not optimal, so the slurs are encoded with explicit end-\npoints and curve values. Mup cannot slur to an entire chord,\nas in the right-hand part in measure 4, so those are also placed\nexplicitly.\n3.3 The Saltarello\nThis piece demonstrates multiple endings. The sections were\noriginally numbered with a small number above the staff in the\nﬁrst measure of each section. This information is not encoded\nin the XML, but could have been.\n3.4 The Telemann Aria\nThisexampledemonstrateslyricsandmultiplevoices. Manyof\nthe notes have small (“cue” size) heads, and the voices are ex-\ntremelycomplex. Mup’sdefaultnoteplacementdoesnotmatch\nthe original in some places, but little can be done about this\nwithout hand-editing the Mup code after it is transformed. It\nis necessary to remove the fourth voice from the piece before\nMup will process it. Mup is limited to three voices, but addi-\ntional voices can be notated by placing the notes manually.\n3.5 Unmeasured Chant\nThisexampleisdifﬁculttorenderwithMupbecausethereisno\nmetersignature. Muprequireseachbartohaveexactlytheright\nnumberofnotes,soitisnecessarytochangethetimesignature,\ninstruct Mup not to print the time signature, and print invisible\nbars.\n3.6 The Binchois Magniﬁcat\nThisexampledemonstratesseveraltrickylayoutproblemsMup\ndoesnothandlegracefully. Inparticular,thereisnowayinMup\nto get a stem to point down on the right-hand side of a note. It\nwas necessary to use a small macro to get Mup to draw a line\nin the appropriate place. This macro is embedded in the XSLT.\nSome of the other interesting features of this piece are the ab-\nsenceofnotestemsintheﬁrstmeasure,theabsenceofasecond\nstaff in the ﬁrst measure (Mup displays the staff, but there isnone in the original), and a number of editorial elements, such\nas editorial accidentals. The ﬁrst measure is also in5\n4time, but\nthe time signature is hidden until it changes to3\n4in the second\nmeasure. The key signature and clefs should be placed in the\nsecond measure as well, but Mup’s default behavior does not\nre-display the clef and key signature in bar 2. This could be\nencoded explicitly in the XML to force Mup to display it as\nrequired.\n4 Conclusion\nAstheexamplesdemonstrate,MEIrepresentsmusicaldatawell\nenough to generate acceptable printed music notation. Because\nMEI is written in XML format, the information in a MEI ﬁle is\naccessibleandeasytomanipulate. ThismaymeanthatMEIcan\nbe used for many other, far more general, application domains.\nForexample,usingXSLT,itisrelativelysimpletoextractthose\nnotes having a certain pitch followed by another note at a cer-\ntain interval. It would also be easy to extract bibliographical\ninformation from a MEI ﬁle (and highly efﬁcient as well, since\nthis information is at the beginning of the ﬁle). Since XML is\nsupported by a wide variety of software, writing tools to work\nwith MEI ﬁles should also begreatly eased.\nThe transformation itself is simple, efﬁcient, and demonstrates\ngood recall and precision in the retrieval process. The trans-\nformation itself is straightforward, and merely involves writing\nXSLTtemplatesforeachelementinMEI;withfewexceptions,\nthe MEI elements map easily to Mup notation. The retrieval\ndemonstratesgoodrecall,sincethemusicnotationisclearlythe\ncorrect result of the transformation, and good precision, which\ncan be inferred intuitively from the fact that the music notation\nisindeedthatinformationforwhichthetransformationqueried.\nFuture research should test the MEI format for other purposes,\nsuch as the analytical domain, to assess its feasibility as a uni-\nversal musical data encoding format. Transformations to and\nfrom other formats would also enhance its usefulness as an\ninterchange format. An obvious transformation would be the\nreverse of the one described in this paper, from Mup to MEI.\nThismaybedifﬁcultbecauseoftheinformalnatureoftheMup\nformat, but should be possible with a two-step process to ﬁrst\n“canonicalize” the Mup notation, then transform it to MEI.\nReferences\nExtensible Markup Language (XML) (n.d.). Retrieved August\n11, 2003 from http://www.w3.org/XML/\nGood, Michael (2001). MusicXML for Notation and Analysis.\nIn Hewlett, Walter B. and Eleanor Selfridge-Field (Eds.), The\nVirtual Score: Representation, Retrieval, Restoration : Vol. 9.\n(pp. 113–124). Cambridge, MA: MIT Press.\nRoland, Perry (2002). The Music Encoding Initiative (MEI).\nIn Haus, Goffredo and Maurizio Longari (Eds.), Proceedings\nof the First International Conference on Musical Applications\nUsing XML (pp 55–59). Milan: State University of Milan.\nSelfridge-Field, Eleanor (1997). Introduction: Describing Mu-\nsical Information. In Selfridge-Field, Eleanor (Ed.), Beyond\nMIDI: The Handbook of Musical Codes (pp. 3–37). Cam-\nbridge, MA: MIT Press.\nXSLTransformations(XSLT) (n.d.). RetrievedAugust11,2003\nfromhttp://www.w3.org/TR/xslt"
    },
    {
        "title": "I Found It, How Can I Use It?&quot; - Dealing With the Ethical and Legal Constraints of Information Access.",
        "author": [
            "Anthony Seeger"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417719",
        "url": "https://doi.org/10.5281/zenodo.1417719",
        "ee": "https://zenodo.org/records/1417719/files/Seeger03.pdf",
        "abstract": "It is very easy to find music on the Internet today, but how it may be used is the source of considerable conflict, front-page news stories, and increasingly of scholarly reflection.  One of the frustrations for libraries, archives, and patrons alike is the gulf between the information about a holding and actual access to it.  But users are not the only ones to  have an opinion about free access.  Local musicians feel that everyone profits from their cultural heritage but them; researchers find themselves held responsible for research recordings made decades earlier and largely forgotten; and some communities seek to protect music that was never meant to be commercialized, and is considered to be secret or divine. Caught in the middle between angry patrons, angry companies, and angry artists, what are music librarians and archivists supposed to do?   Using his own experience as a researcher, archivist, and record producer, the author discusses the issues and makes some suggestions that can help those who wish to use the music they can so easily find out about. It is a great honor to be with you at the ISMIR 2003.  I have devoted much of my life to making information available for eventual retrieval, and it is nice to be among specialists in doing just that.  As a researcher, I have made field recordings among indigenous peoples in remote jungles of Brazil.  As the director of an audiovisual archive I wanted to make available as much information as possible about the collections by publishing printed catalogs, entering collection-level information on OCLC, creating in-house databases, and revising depositors’ contracts.   As a record company director I have produced hundreds of CDs with extensive liner notes, maintained a vast back catalog in print, and moved early to supplying information on the Internet. As the archival consultant to the Smithsonian Institution's GlobalSound Internet music project, I have continued to search for new ways to make information about music, and music itself, available to as wide a public as we can reach. A huge amount of music is available on the Internet today, and even more music is signaled in myriad archives catalogues.  More will music will certainly become available. One problem we face is finding what is there (and also what isn’t). Another problem is finding out how we may use it. The short summary of my talk would be: although you can find it, a variety of forces (not all of them related to greed) shape the way music should be used. As specialists in information retrieval, we must also become specialists in helping others learn not only the techniques of finding music, but also the ethics of using it. Introductory This is my first ISMIR conference, but I have learned a lot about you from your web site.  Among the interesting essays was the history of the ISMIR.  It revealed the origins of the group in online music recognition and searching.  I printed out a number of very interesting articles and read them before speaking you today. My grandfather, Charles Seeger, was a composer and music theorist was one of the founders of the American Musicological Society and later of the Society for Ethnomusicology.  In the 1950s he developed a machine that would measure pitch, amplitude, and tone quality that came to be called the Seeger Melograph.  It took up the whole wall of a room, required constant attention from a technician, and provided detailed analysis of very short samples.  But it revealed some very interesting relationships between sounds and the way they are perceived, and anticipated the importance of machines in musical analysis and creation that followed. Grandfather would have loved the HMM project. In fact, the endeavor to create a melody recognition software fills an important gap that has opened in the reference services at the Smithsonian Institution and the Library of Congress.  When I directed Smithsonian Folkways recordings I sometimes answered the telephones to learn more about how to improve our mail order office.  People would call wanting to replace an old LP recording they have lost with a CD.  The trouble was that often they remembered very little about the original. Sometimes they would say \"The LP jacket was made of heavy cardboard and had a black border around the edge.\" Since the jackets of nearly 2000 of the LP titles on Folkways Records had black borders, that was not very helpful.  They would sometimes say, “Well, it had a blue two-tone cover.” Since on every print run the colors could be changed, that was not very helpful either.  In all honesty, most customers inquired about a subject matter (songs of the Civil War), an instrument (five string banjo bluegrass style), or an artist. Permissions to make digital or hard docpies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2003 Johns Hopkins University.",
        "zenodo_id": 1417719,
        "dblp_key": "conf/ismir/Seeger03",
        "keywords": [
            "Internet",
            "scholarly reflection",
            "gulf between information and access",
            "free access",
            "angry patrons",
            "angry companies",
            "angry artists",
            "music librarians",
            "archivists",
            "researchers"
        ],
        "content": "ISMIR 2003 Ke ynote Speech \n\"I Found It, How Can I Use It?\" De aling with the Ethical and Legal \nConst raints of  Inform ation Acces s \nAnthony See ger \n \nAbstract: \nIt is very  easy  to fin d music on the Internet today, \nbut how it may be used is the source of considerable \nconflict, front-page news st ories, and i ncreasingly of \nscholarly reflectio n.  On e of th e fru strations fo r \nlibraries, arch ives, an d patrons alik e is th e gu lf \nbetween  the inform ation abou t a ho lding and actual \naccess to it.  B ut users  are not the only ones to  have  \nan opini on about free acces s.  Local m usicians feel \nthat ev eryon e profits fro m their cu ltural heritage but \nthem; researchers find them selves held resp onsible \nfor resea rch recordings m ade decades ea rlier and \nlargely fo rgotten; and so me communities seek to \nprotect music that wa s neve r m eant to be \ncommercialize d, and  is con sidered to b e secret or \ndivine. Caught in the middle between angry patrons, \nangry com panies, and a ngry artists, what are m usic \nlibrarians and archivists suppo sed to do?   Using his \nown e xperienc e as a researc her, arc hivist, a nd record \nproduce r, the author discusses the issues a nd makes \nsome suggestio ns th at can  help those who wish to \nuse the music they can s o easily find out a bout.   \nIt is a great  honor to be wi th you at  the ISM IR 2003.  I ha ve \ndevoted much of my life to making information available for \neventual retriev al, and  it is n ice to  be amo ng specialists in  \ndoing just that.  As a  resea rcher, I have m ade field rec ordings \namong indigenous peoples in rem ote ju ngles of B razil.  As \nthe direct or of an audiovis ual arc hive I wa nted to make \navailab le as m uch informatio n as po ssible ab out th e \ncollections b y publ ishing pri nted ca talogs, ent ering \ncollectio n-level information on OCLC, creatin g in -house \ndatabases , and revi sing depositors’ contracts.   As a rec ord \ncompany director I ha ve pr oduced hundreds of  CDs wi th \nextensive liner no tes, m aintained a vast bac k catalog in print, \nand m oved early to supplying information o n the Internet.  \nAs th e arch ival co nsultant to the Smithsonian Institution's \nGlobalSound Internet music project , I have co ntinued to \nsearch for new ways to make inform ation about m usic, and \nmusic itse lf, available to as wide a public as  we ca n reach. \nA huge am ount of m usic is available on the Internet t oday, \nand e ven m ore music is signale d in myriad archives catalogue s.  More will music will certainly b ecom e available.  \nOne problem  we face is finding wh at is there (and also what  \nisn’t). Another problem is finding out how we may use i t.  \nThe s hort sum mary of m y talk would be:  although you can \nfind it, a variety o f forces (not all o f them related  to greed ) \nshape the way music should  be used. As specialists in \ninformation retriev al, we mu st also become sp ecialists in \nhelping others learn not  only the techni ques of fi nding music, \nbut als o the ethics of using it. \nIntroductor y \nThis is my first ISMIR confer ence, but I have learne d a lot \nabout you from your web site.  Among t he interesting essays \nwas th e history o f the ISM IR.  It rev ealed th e orig ins of th e \ngroup in online m usic recogn ition and  search ing.  I printed  \nout a number of ve ry interesti ng articles and read t hem before \nspeaking you today. \nMy grandfat her, Cha rles See ger, was a c omposer and music  \ntheorist was  one of t he founde rs of t he American \nMusicological Society and later o f the Society for \nEthnomusicology.  In the 1950s he developed a m achine that \nwould measure pitch, amplitude, and  tone quality th at ca me \nto be cal led the Seeger M elograph.  It  took up the whole wal l \nof a room, required constant attention from a techni cian, and \nprovided detailed  analysis of v ery sho rt samples.  Bu t it \nrevealed  some very in terestin g relation ships between  soun ds \nand the way they are  percei ved, a nd anticipated the \nimportance of machines in musi cal analysis and c reation that  \nfollowed .   \nGrandfather would have  loved the HM M project . In fact, the \nendeavor to create a m elody recogn ition software fills an \nimportant gap that has ope ned in the refe rence ser vices at  the \nSmith sonian Institu tion and the Lib rary o f Congress.  When  I \ndirected Sm ithsonian Fol kways rec ordings I s ometimes \nanswered  the telephones to learn m ore abou t how to im prove \nour mail orde r office.  Peo ple would cal l wanting to replace \nan old LP reco rding they have lost with  a CD.  Th e troub le \nwas th at often  they rem embered very little a bout the original.  \nSometimes they woul d say  \"The LP jac ket was m ade of  \nheavy  cardboard an d had a bl ack border around the edge.\"  \nSince th e jackets o f nearly 200 0 of the LP titles o n Folkways  \nRecords had black b orders, that was not very helpful.  They \nwould sometimes say , “Well, it had a bl ue two-tone c over.” \nSince o n every print run the col ors could be cha nged, that \nwas n ot very  helpful either.  In al l hone sty, most cust omers \ninquire d about  a subject m atter (s ongs of the Civil W ar), an \ninstrument (five st ring banjo bluegrass st yle), or a n artist.  Perm issions to make digital or  hard docpies o f all or  part of this wor k \nfor personal or  classr oom use is gr anted without fee pr ovided that copies \nare not made or  distr ibuted f or profit or commercial advantage and  that \ncopies bear  this notice and the full citation on the fir st page.  © 2003 \nJohns Hop kins Uni versity.  \nBut occasionally the question came \"the melody went \nsomething like this: la-la-la-la-la.... Do you know it?” \nSometimes I was successful, and could reply, \"Yes, I know that song. It is on LP number xxxx.  But more often I failed. \nBetween the poor phone connection, the inability of the \nperson on the phone to hum any melody at all, and his or her \nuncertainty as to what the melody might have been, I was \nusually stumped.  When that happened, I would refer them to \nJoe Hickerson who often tended the phones at the reference \ndesk at the American Folklif e Center of the Library of \nCongress.  Joe has a huge repertory of songs and could help a \nlot of people uncertain about what it was there were actually \nlooking for.  But now I have left the Smithsonian, and Joe has \nretired.  Hurry up and work on the HMM project some more, \nthe public is waiting! \nObservations on Retr ieving Music Melodies \nMelodic structures are very us eful for classifying certain \ntypes of music, such as fiddle tunes, but not as good at others.  Some of the earliest databases of melodies were \nScandinavian fiddle tunes.  There are probably some very \nadvanced music information retrieval techniques to be found in the Scandinavian countries.  The HMM project will have \nmore difficulty with improvised music like Indian ragas or \nIranian Dastgah.  Rap, and voice-box music, which is made \nwith the mouth and lips and by slapping the face, will be \ndifficult to retrieve through a melodic search alone.  But \nethnomusicologists may helpful in the creation of automated \nmusical retrieval systems for those forms. For example, in the \nIndian raga structure is provided by a system of intervals \napproached in a specific manner rather than by a particular \nsequence of notes. \nMany of the people who requested information at \nSmithsonian Folkways recordings wanted more than just the \nsound of a given piece of music.   They wanted information \nabout the music, photographs of the artists, and as much additional bibliography and discography as we could provide.  \nThis supplementary information was particularly important \nwhen the traditions were unfamiliar.  Most of the research in \nthe papers I read from previous ISMIR meetings dealt only \nwith US popular music or a re stricted part of European \n“Classical” concert music.  But there is a lot more music out there. Content providers will need to set up a system of \nintegrated files with suppleme ntary information to create \nsomething resembling a multimedia resource.   \nEven local libraries need to know how to find background \ninformation about musical traditions, and Internet users—\naccessing from many different places—will need to know \ntoo.  Many cities, suburbs and even rural areas in the United \nStates are home to multiple musical traditions.  One cannot \nassume that a child or adult looking for music will know \nmuch about even so-called ma instream American traditions \nwithout access to more than just the sound files.  \nThis brings me to my two observations about information \nretrieval.  First, people will want to access more than just \nsounds—they will want supplementary information including \nsong texts, photographs, biographical material and the like.  Second, people will want to retrieve music from many different traditions, in only some of which is melodic \nstructure the best approach for retrieval.  It is essential to \ndesign music information retrieval systems that will work not \nonly for American popular musi c but also Indian ragas, \nAfrican xylophones, South Am erican Indian songs, and \nIndonesian court music.  Even if initial experimentation \nfocuses on contemporary popular music, it is important to \nfashioon the overall system so that it can eventually include \nall kinds of music.  One of the oddities of many early word-\nprocessing programs was that they could not handle \ndiacritical marks—the signs above and below letters with \nwhich many languages distinguish words from one another. \nEven today these marks are often lost in translation programs \nand e-mail transmission.  This  probably happened because of \ntechnical limitations, US dominan ce in technology, a lack of \nvision, and a lack of interest in creating systems that would \nserve all languages rather than just that of the developers.  I \nhope the creative minds at ISMER will not make the same \nmistake. \nYou have many potential collaborators to help avoid such ethnocentric traps.  Ethnomusicologists can be of some help.  \nThey study, write about, and often perform music of more \nthan one musical tradition.  Even more helpful might be the \nethnomusicologically trained librarians. They are quite \nfamiliar with the difficulties of dealing with multiple music \nsystems, since they have a constant problem with the subject \nheadings and other classificati on systems that were designed \nfor only a few of the world’s musical traditions.  \nCollaborative ventures are sometimes tiresome, but if the \nright system is created it will en dure; if not it will be replaced \nby another made by people who understand both music and their audiences more broadly. \nWho owns music and what can I do with it? \nNow to my main topic.  When I asked of the organizers of \nthis meeting why they invited me to address you, they said it \nwas because of my work on music copyright and ethics \n(especially Seeger 2001).  This explains my title, \"I found it, \nHow Can I Use It?\"  \nThe issue of who owns music and what other people's rights are to it is a front-page subject  these days because of file \nsharing.  Two articles in this morning's USA Today \naddressed this subject—one announcing the distribution of \nanalog music at MIT and the other discussing software to \nprevent sharing of digital films. \nCountless institutions and individuals have placed a vast \namount of information about music as well as music sounds \nat our fingertips.  It is easy to find things that are for sale and \nalso for free—including music files.  There are many free \nservices, among them hundreds of excellent library-produced \nprojects that provide rich musical resources.  Oxford \nUniversity has made available marvelous collections of \nbroadside ballads from previous centuries.  A number of \nAmerican universities are collaborating in posting large \ncollections of sheet music, as shall be reported later in this \nconference.  The Library of  Congress American Memory  \nProject, which you are visiting th is afternoon, includes such \ntreasures as the John and Ruby Lomax field expedition of \n1940.  This features streaming sound of all the recordings \nmade on their recording exped ition to the American South.  \nSo much information is made available on the Internet provided by so many different people and so easily found that \nwe forget to ask two important questions:  First, what is not \nthere that I might want to find ?  And second, what can I do \nwith something I find information about something that \ncannot access the sounds themselves. \nA Short History of Copyright Legislation \nOne obvious problem for music users is copyright law.  Mary \nLevering, from the Library of Congress, presented a paper on \nthat subject at a previous IS MER conference. She accurately \npresented information about the law, but she did not \neditorialized about it the way I will.  If you ever read the law \nand review the changes over the decades (and I recommend \nthis as a lesson in American political process), you will see \nhow special interests have shaped national policy. \nThe history of copyright is quite a fascinating topic (see \nWoodmansee and Jaszi 1994 for early history and Samuels \n2000 for American copyright history).   Copyright has little \nto do with art and everyt hing to do with commerce.  \nCopyright provids a limited term monopoly on printing copies of a given work.  After the term is over, the monopoly \nends and the works go into something called the Public \nDomain.  This is a little like a lake from which all can draw \nwater, or a commons to which all have access. The purpose \nof copyright was thus to stimulate the increase in knowledge \nand arts by ensuring that creators and publishers would \nreceive recompense for new works that would enter the \npublic domain and be available for use by everyone.   \nInitially, US copyright law only covered the creations of \nUnited States citizens.  After US independence, most of the \nprinted literature was taken from England and printed without \nhaving to pay any fees.  This only changed in 1891.  Now the \nU.S. focuses its ire and trade retaliation on other countries \nthat are doing what it did until 115 years ago. \nMusic was initially copyrighted because of printed notation.  \nHymnbooks were a major source of income for some \npublishers, and sheet music was highly profitable throughout \nmost of the 19th and part of the 20th Century.  Only much \nlater were the sounds made copyrightable.  With a few \nexceptions, most copyrights on music compositions have \nbeen and still are controlle d by large music publishing \ncompanies.  Over the cent uries these companies have \nconsistently argued for extending the term of their monopoly.  \nThe term of 14 plus 14 years became 28 plus 14 in 1831 and \n28 plus 28 years in 1909.  This was extended to the life of the \nauthor plus 50 years in 1976, and later to the life of the author \nplus 70 years in 1998.   \nThe United States also has something called \"fair use,\" which \nmodifies the limited-term monopoly by allowing materials to \nbe copied for certain specific reasons—for such  purposes as \ncriticism, news reporting, or teaching.  Legislation is far less \nspecific about the precise definition of fair use than it is about the rights of the monopoly holders.  Individuals and institutions are having to forge practices for themselves.   \nToday, file sharing has transformed the delivery of music and \nthe transnational music industry is in a major crisis.  Most of \nthe major record companies are up for sale or merger.  This is \nnot the first time technology has reconfigured the recording \nindustry since it began in the 1880s.  The invention of the \nradio and the Great Depression, both created economic \nturmoil. But the industry adopted new technologies and \nemerged stronger in the end.  I expect this will happen again, \nbut the serious conflicts in this area require institutions (and \nperhaps even individuals) to be quite careful about how they \nuse music that might be copyrighted. \nEven though it appears copyright would apply to all music, that isn’t true. What is called “folklore” and community \nworks for which there is no known author/composer are not \ncovered in most legislation.  Instead they are part of the \nPublic Domain.   Countries with large oral traditions that are \nbeing used as sources of inspiration by popular musicians \nfind this discriminatory.  Many indigenous communities also \ncomplain that their culture can be exploited without their \npermission. They see it as a kind of neo-colonialism in which \ntheir folklore is the raw material for someone else’s \nprofitable creation.   The United Nations Educational \nScientific and Cultural Orga nization (UNESCO) and the \nWorld Intellectual Property Organization (WIPO) are both examining possible modifications in international agreements \nthat would address the issues of indigenous rights and \ntraditional knowledge.   \nThe conflicts over intellectual property, in which artists, \nnations, and commercial interests are all applying pressure to \nchange (or maintain) existing laws creates a very difficult \nenvironment for researchers and their institutions.  Among \nthe institutions hardest hit by the changes and the insecurity \nare audiovisual research archiv es, because they hold so many \nthousands of hours of unique recordings that were never published at all. \nAccessing Music in Audiovisual Archives \nMillions of hours of field research recordings like the ones I \nmade in Brazil are preserved in audiovisual archives.  Until \nvery recently, researchers ne ver bothered to obtain written \npermission to make a recording and archives never bothered \nmuch about defining what rights they had to their collections.   \nOnly now, when intellectual property laws are changing and \nthe Internet makes possible br oad access to their collections \nhave archives discovered the di fficulties they face.  They are \nfilled with recordings people would like to access but which \nthey are not sure they can make available under current \nlegislation and ethical guidelines.  They have been taken by surprise by the changing laws, the increased popularization of \nnon-Western music, and the unanticipated ease of access to \nmusic that the Internet has ma de possible.  Most of you \nwould want to access some of the treasures found in audiovisual archives; most audi ovisual archives aren’t sure \nthey can let you do so. \nSince many archived recordings haven’t been published, they  \nare not usually governed by the same rules that would apply \nto a commercial reco rding.  Researchers who record music \nsomewhere and then deposit their recordings in an archive often place restrictions on the use of the material.  They cite a \nvariety of reasons for doing so, among them to keep the material from circulating before they complete a book or to \nprotect the privacy of their so urces.  Performers sometimes \nplace restrictions on the use of the recordings when they \nrecord it for the collectors, and the collectors require the \narchives to observe the same restrictions.   \nNot all music was meant for co mmerce.  Not only do other \nsocieties have different musical  sounds, they also may have \nentirely different notions of music ownership.  In Australia, \nsome Aboriginal women perform songs that should only be \nheard by other female clan members.  In Vanuatu some songs \nare so secret that they are only whispered.  In other \ncommunities music must be performed only at certain seasons and certain times of da y .   I n  c e r t a i n  communities, \nmusic can be dangerous—causing illness or enemy attack.  In \nthese cases, wide distribution seems totally unjustified, and \neventual entry of the materials into the Public Domain is \ntotally inappropriate. \nRestrictions by artists and depositors typically apply to only \npart of an archives’ collection. The rest is unavailable \nbecause of uncertainties about the status of the material.  \nWhat can we do about the unavailability of archival \nrecordings?  Potential users need to understand that other \npeople also have an interest in and claim rights on the \narchived materials they would lik e to use.  Archives, for their \npart, should make parts of their collections available to the \ngeneral public while restricting access to other parts, so that \nthe Internet becomes more than simply a place to find \ncommercial recordings.  Archives can improve access by \ndefining which parts, if any, of each collection can be used \nby the public, and in which ways.  To do this they must enlist researchers, depositors, and artists whenever possible. \nI have some experience with this.  Between 1982 and 1988 I was Director of the Indiana University Archives of \nTraditional Music.  A large preservation and access grant \nallowed us to not only copy the wax cylinders and acetate \ndiscs but to make listening copies of those materials for \nconsultation in our reading/listening room, and to catalog the \ncollections online using OCLC so that basic information \nwould be available very broadly.  Part of the grant project \nwas an attempt to renegotiate the existing contracts with the \ndepositors who had placed coll ection-wide restrictions on \naccess to their recordings. Under the earlir contract, a single \nrestriction would apply to the entire collection, which could \ncontain five, fifty, or hundreds of audio recordings.  We had \nover 1,100 collections, and we contacted all those we could \nto ask them to consider restricting only those items that really \nneeded it, and making others available for nonprofit research \nuse.  In many cases this was permitted; in others it was not.   \nBut the end result was that many collections were \nconsiderably more accessible— through cataloging, copying, \nlistening room copies, and renegotiated contracts—than before. Attracting Retrieval: Distributing Folkways \nRecordings \nBetween 1988 and 2000 I was the curator of the Folkways \nCollection and director of Smithsonian Folkways Recordings.  \nThe Folkways record company had been founded in 1948 by \na visionary who wanted to release the sounds of the world on \na record label and keep them all in print—from number 1 to \nnumber 2,168.  The Smithsonian acquired the collection from \nhim in order to maintain the remarkable collection of \nsounds—most of them obscure and not meant to be \npopular—available to the public.  I was hired to figure out \nhow to do it.   I became both an archivist and a (small time) \nmedia mogul (in a non-profit institution).  \nThe exciting thing about running a record company after running an archive was that the entire purpose of a record \ncompany is to make as many copies as possible for as many \npeople as can be convinced to want them.   The problem was \nnot whether we could make the materials available, but \nfiguring out how the public could discover they were \navailable.  In a large commer cial company this would have \nbeen called marketing; in the context of the Smithsonian I \ncalled it dissemination, or throwing out bait to be recovered \nby specialists in information retrieval (you and the general \npublic).   \nThe LP era was over, and I antic ipated the eventual item-by-\nitem access to these recordings by creating a database of all \n35,000 tracks.  I restructured  the contracts to be more \nfavorable to artists.  Every title was kept in print through publication of very small numbe rs of each (when we received \nan order for one copy of a title we made five, and kept four on hand for the next orders) and establishing a mail-order \nd i v i s i o n  t o  s e l l  t h e m .   M y  early search for easy automated \nmethods of distribution included considering 900- numbers \n(at the time mostly used for telephone pornography).  I \nthought we could devise a jukebox that would enable people \nto dial up and get any track at any time of day or night, \nwithout having to go to a record store.  Charges to 900-\nnumbers appeared on the telephone bill, so we wouldn’t have \nto collect the money. I planned to price the tracks cheaper \nthan pornography but enough to pay the expenses and the \nartists and composers.  As I was investigating how to do this, \nthe Internet emerged as a better option.   I was immediately \nenthusiastic about the potential of the Internet for distributing \narcane, less-than-popular music.  It seemed to me we could \ncharge half as much and pa y the artists twice as much \nbecause we would not need to manufacture, store, and ship \nheavy breakable products to deliver the music. \nSmithsonian GlobalSound: Internet Music With \na Difference \nMy involvement as the archival consultant to the Smithsonian \nglobal sound project is a natural outgrowth of my work at \nIndiana University and the Smithsonian institution.  We have \nfour objectives:  \n(1) To add diverse and non-commercial content to the \nofferings of music made available on the Internet. In  \naddition to sound files the web site will provide \nsupplementary print and visual material to supplement \nthe sound recordings.  These materials will be made \navailable by subscription or for-pay download by \nconsumers as well as for licensing to films and for the \ncommercial use.   \n(2) To create a network of au diovisual research archives \naround the world, assist them with the digitization of \ntheir collections and eventually to provide an income \nstream from Internet access to parts of their collections. \n(3) To encourage the performers of traditional music by \nproviding an income stream directly to the artists or \ncommunities for the use of th e materials placed on the \nInternet by the archives.  Once it is running, income \nfrom the site will be divided equally with the archives \n(for every $1.00 received, the archives will receive $.50).  \nThe archives, in turn, would divide the money they received with the artists whose performances were \ndownloaded.  \nThis is a very different kind of music access program than \nmost of those that have been launched.  Its objective is to \nsupport the performers of musical traditions, to strengthen \nlocal audiovisual archives in different parts of the world by \nassisting with digitization and dissemination as well as \nproviding a small income stream to them, and to create a \nwebsite that provides a great deal of extra musical \ninformation—enough to be used in schools and universities \nas well as by individuals. \nWe started with three archives, all of which were already digitizing their collections: an archive in India (The Archives \nand Research Centre for Ethnomusicology [ARCE] in New \nDelhi) one in South Africa (The International Library of \nAfrican Music [ILAM] in Grahamstown) and the Folkways \narchive at the Smithsonian Institution.  There were many \nchallenges to this project.  Some of them were technical. \nSelecting formats and delivery procedures in a rapidly \nchanging technological environment was one of them.  The \ndifficulty of finding a suitable search engine for Internet \ndatabases was another.   \nObtaining the appropriate permi ssions to enable archives to \nput their collections onto the Internet was a third.  For example, a change in the In dian copyright law granted \ncomplete rights to performances to the performers.  As a \nresult ARCE decided to contact all the depositors to request \ntheir permission to upload parts of their collections and \naccompanying data, and asked their assistance in locating \nartists who would be most likely to agree to having their \nrecordings, photographs, and interviews available in this way.  \nAlthough the process has been time-consuming, virtually all \nthe depositors agreed, and every artist that was contacted \ngranted permission.  The process had an added benefit.  Some \nof the depositors had more materials they wanted to put into \nthe archive, and the artists themselves were often interested \nin adding to the collection as well.  Some also assisted the \narchive with new translations of the lyrics provided \nsupplemental information that could be uploaded. Improving \nthe communications between archives and those who have \ndeposited their collections in them, as well as creating new relationships between archiv es and the artists on the \nrecordings in their vaults, benefits both the archives and the general public.  ARCE created and demonstrated the \nfeasibility of a model framework for moving archival \ncollections from the vaults to the Internet. It takes time, but it \nactually strengthens the archives itself. \n The global sound project is still in the development stage, and development funds dried up in 2003. [In my presentation \nI showed some pages from the test site.]  But we are far \nenough along to see that it has great potential for providing \nunique music with all the rights to use it cleared.  I hope you \nwill all be retrieving information from the Smithsonian \nGlobalSound site in the future, after it has been launched. \nSome Recommendations For the Future \nIn my abstract I committed myself to providing some practical suggestions based on my remarks.  In this section I \nwill touch on a few of the things that come from the \nexperiences I have described above. \n1. When users want to access a particular kind of music it is \nvery important to ask them what they want to do with it.  \nTheir intention should determine the kinds of sources to \nwhich they should be directed.  Does their intention fit \nfair use?  Is their purpose research or publication? Would \nthe publication be commercial or non-commercial?  Is it \nsufficient for them to listen to a streaming audio file, or \nmust they be able to download a file in order to analyze \nit more thoroughly?  Are they looking for sound samples \nthey can use to create new compositions? \n2. Once the intention of a user is clarified, he or she should \nbe directed to a source that can serve their particular interests.  For example, if they want to sample the music \nfor their own creations they should be directed to the \npublic domain sources and to independent record \ncompanies, who usually grant rights more easily for less \nmoney than the major labels.  \n3. Users should also be reminded of ethical issues as well \nas legal ones.  They should know that communities whose music they wish to access have their own ideas \nabout appropriate uses. Relig ious music, especially, \nshould probably be avoided in sampling unless permissions have been obtained.  All users should be \nreminded to cite their sources.  Some of the most \nstrenuous objections to sampling have come from artists \nwho were not credited for their contribution. \n4. When a collection is in an archives or collection rather \nthan on a commercia l recording, counsel patience.  Even \nif people discover something exists, it doesn’t mean they can use it. Archives are struggling with a rapidly \nchanging environment and are often unsure of exactly \nwhat rights they have to the recordings and their \ncollections.  Queries directed to audiovisual archives \nshould be as specific as possi ble as to what is being \nsought, and what use will be made of it. \n5. Our music schools, conservatories, and music research \ndepartments need to add training in music law and  \nethical practice.  Artists n eed to know how to protect \ntheir own rights to their creations.  Researchers need to \nlearn how to obtain the rights  they need for archiving \nand publication when they make a recording.  Librarians, archivists, programmers, and other information \nspecialists should be involved in these training processes \nas well. \n6. At a different level of action, we as a citizens need to \nmove to defend the public domain.  It is important to roll \nback the digital copyright act provisions that allow \npublic domain works to be encased in a digital code that \nrestricts access to them.  C itizens need to address the \nimbalance towards corporations in the term periods for which monopoly is allowed on copyright.  We should \nalso recognize the claims of non-literate artists of oral \ntraditions.  Music information professionals like you in \nthis room have an interest in this as well.  You need to \nhave access to sounds for study, and you need to be able \nto make your results available.  I think this is a case for \ncitizen activism as well as scholarly research. \nThe Internet has opened some wonderful possibilities, but it \nwill take a while before we may use everything we can find, \nand find everything we may use.   \nThank you. References Cited: \nSamuels, Edward 2000. The Illustrated Story of Copyright . \nNew York: St. Martin’s Press. \nSeeger, Anthony 2001.  Intellectual Property and Audio Visual \nArchives and Collections, in Folk Heritage Collections in \nCrisis . Washington DC: Council on Library and \nInformation Resources, May 2001, Pp 36-47. \nWoodmansee, Martha and Peter Jaszi (editors) 1994.  The \nConstruction of Authorship: Textual Appropriation in \nLaw and Literature.  Durham: Duke University Press."
    },
    {
        "title": "Music identification by leadsheets: Converging perceptive and productive musical principles for estimation of semantic similarity of musical documents.",
        "author": [
            "Frank Seifert 0001",
            "Wolfgang Benn"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417759",
        "url": "https://doi.org/10.5281/zenodo.1417759",
        "ee": "https://zenodo.org/records/1417759/files/SeifertB03.pdf",
        "abstract": "Most experimental research on content-based automatic recognition and identification of musical documents is founded on statistical distribution of timbre or simple retrieval mechanisms like comparison of melodic segments. Therefore often a vast number of relevant and irrelevant hits including multiple appearances of the same documents are returned or the actual document can’t be revealed at all. To improve this situation we propose a model for recognition of music that enables identification and comparison of musical documents without dependence on their actual instantiation. The resulting structures enclose musical meaning and can be used for estimation of identity and semantic relationship between musical documents. 1",
        "zenodo_id": 1417759,
        "dblp_key": "conf/ismir/SeifertB03",
        "keywords": [
            "content-based automatic recognition",
            "musical documents",
            "statistical distribution",
            "timbre",
            "melodic segments",
            "retrieval mechanisms",
            "vast number of relevant and irrelevant hits",
            "actual document can’t be revealed",
            "model for recognition",
            "identification and comparison"
        ],
        "content": "Music Identification by LeadsheetsFrank SeifertDepartment of Computer ScienceUniversity of TechnologyChemnitz, GermanyFrank.Seifert@informatik.tu-chemnitz.deWolfgang BennDepartment of Computer ScienceUniversity of TechnologyChemnitz, GermanyWolfgang.Benn@informatik.tu-chemnitz.deAbstractMost experimental research on content-basedautomatic recognition and identification of musicaldocuments is founded on statistical distribution oftimbre or simple retrieval mechanisms likecomparison of melodic segments. Therefore often avast number of relevant and irrelevant hits includingmultiple appearances of the same documents arereturned or the actual document can’t be revealed atall. To improve this situation we propose a model forrecognition of music that enables identification andcomparison of musical documents withoutdependence on their actual instantiation. Theresulting structures enclose musical meaning and canbe used for estimation of identity and semanticrelationship between musical documents.1 IntroductionEstimation of similarity or a certain relationship amongmusical pieces concentrates so far only on categorization anddistribution of sounds (Haitsma & Kalker 2002; Yang 2002).Unfortunately there is no general definition for similarity ofmusical documents. Similarity depends not only on actualapplication but also on individual capabilities of listeners. Butthere is a universal human capability that we want toimplement: Identification of same musical pieces withoutconsideration of their actual sounding. Hence, we try at first toignore timbre and focus on symbolic parameters of music andtheir structural relationships.Finally we propose a method to create a semantic order onmusical documents that is based on automatic recognition ofmusical pieces and assesses the degree of relationship by acontent-based estimation of distances.2 The leadsheet-templateMost existing symbolic music processing systems that handleidentification tasks underlie the query-by-humming-paradigmand are limited to short sequences without meaning (i.e.Dovey 2001; Lemström et al. 2001). Our approach relies onthose techniques but we extend them substantially. Mostimportantly, we evaluate structural relationships betweenmusical segments. So far the notion of melody has beendiscussed very undifferentiated in most cases. In order toexpress musical meaning we decompose a melody in smallestmeaningful musical entities that we call characteristic motifs.Motif and melody represent the most essential components ofa composition. Structures that are built up by them formcomponents of a higher order. The top of this hierarchyrepresents a piece of music.But what is the foundation of this analytical decompositionprocess?Our aim is to provide a maximum generic representation formusic in order to derive semantic equivalence. The mostcommonly used means of conserved instructions forreproduction of music that fulfills generic music creation isthe leadsheet. A leadsheet contains merely the melody withaccompanying chord symbols that correspond to harmony:C&œœAm6wE7Am6œJ‰E7œ.œœ.œœAm6˙E7˙Am6œŒE7œAm6œDmœJœ.F˙Figure 1: Leadsheet-excerpt of Gershwin’s “Summertime”Using leadsheets musicians create their voices themselves“live” during a performance. This process is aided byharmonic information of the leadsheet and their inherentmusical knowledge.We define a leadsheet-template to represent musicalparameters and their hierarchy: Primary parameter is thecontour of a characteristic motif and its configuration oftemporal spacing between the tones.Since most pieces contain repetitive structures, either in smallor large scale, we can build up a hierarchy over theenumeration of characteristic motifs. So we define aleadsheet-template (LST) to be a- non-empty sequence of leadsheet-templates or a non-empty sequence of characteristic motifs and theirharmonic configuration.Harmonic configuration is considered as structuring aspectsince the meaning of successive notes can change whenharmonic context is altered (Motte-Haber, 1985).Permission to make digital or hard copies of all or part of this work forpersonal of classroom use is granted without fee provided that copies are notmade or distributed for profit or commercial advantage and that copies bearthis notice and the full citation on the first page.  ” 2003 The Johns HopkinsUniversity.3 A model of music recognitionAlthough leadsheets are more convenient for popular musicit’s possible to code nearly all tonal repertoire into this form ofrepresentation. But this process of reduction has to care for anadequate representation of perceptual relevancy, for instancepolyphonic voices. This idea – to generalize therepresentational capability of tonal music by perceptualrelevant leadsheets – leads to the following hypothesis ofmusic recognition:- Artificial recognition and processing of music can berealized by mapping music to LSTs that areconstituted by perceptual relevant dimensions ofmusic under the rules of tonality.This form of processing could possibly also be assumedduring human cognitive activities in tonal cultures. However,most listeners can’t explicitly reference their implicitknowledge.With the existence of LSTs we’ve got structures to that eitherknown or unknown musical pieces can be mapped onto. Weformalize this recognition process: Let us define † CM to be aset of all characteristic motifs. † CM* represents a set ofarbitrary repetitions of any † cmŒCM with† CM*=cmn\"cmŒCM,nŒN{Analogously we define † LS to be a set of all LSTs and † LS*represents a set of arbitrary repetitions of any† lsŒLS \"lsŒLS. Additionally we define † C=CM*»LS* tobe the union of all arbitrary repetitions of LSTs andcharacteristic motifs and † M to be a set of musical pieces.Then we can denote the recognition of a musical piece† mŒM as its mapping process † z onto all combinations ofelementary and structured LSTs and characteristic motifs, saytheir power set:† z:mÆ2C(3.1)This mapping process can be subdivided in two mainprocedures: First the musical parameters have to be mappedonto elementary characteristic motifs resulting in a set oftemporal ordered motif instances if successful. Then, alldetected instances have to be associated with maximizedsurrounding LSTs. Again, this results in a set of temporalordered instances, but instances of LSTs this time.Unfortunately it’s not possible to consider (3.1) as a function.Ambivalences could happen during the mapping-processesonto characteristic motifs. Consequently several valid motifscould be assigned to a musical segment simultaneously. Ifseveral candidates compete for qualification as member of aLST-instance, usually the candidate with the most similarcombination of parameters will be selected. Additionally thisimplicit deduction-process results in an ongoing qualificationof competing instances based on context.Actually the main problem is an incomplete mapping-processonto LSTs caused by a fizzled identification of musicalparameters of characteristic motifs or a missing structuraldescription for identified sequences. By explicit deductionwe’ll try to get the mapping-process successful. Therefore weaspire a formation of hypotheses, which leadsheet andconsequently which resulting musical parameters are possiblefor an unknown segment at all. Basis for the formation ofhypotheses is the configuration of adjacent LSTs. Then wehave to validate the determined hypotheses to complete themapping-process. Now we try to map the unknown tone-segment onto the characteristic motifs that constitute thehypothesized LSTs. The degree of deviation leads topreference and selection of a hypothesis or to its refusal ifthere isn’t enough evidence.4 Conclusions & further workWe proposed a model for the recognition of music that enablesidentification and comparison of musical documents withoutdependence on their actual instantiation. The evaluation ofperceptual relevant parameters is the basis of thisfunctionality. Furthermore, a deductive assessment-method formusical phrases came into existence.Music recognition has been defined as mapping-process frommusical documents onto leadsheet-templates that function ashypotheses. Due to the top-down modeling, which is adequatefor music perception and the resulting existence ofhypotheses, distance measures for incomplete mapping-processes of contradictory segments can be calculated and themost likely identity can be concluded.A flexible treatment of structure and variation of musicalparameters are special features of the introduced model. So wecan recognize and combine musical forms, deduce incompletestructures and we’ve enabled the coexistence of severalstructural levels.A first implementation of the presented ideas confirmed theirprincipal feasibility. However it should be seen as proof ofconcept. Nearly every detail needs improvements, for instancethe estimation of distances. Conceptually, the recent approachshould be extended to subsymbolic music data. Due to itshypothesis-oriented design the proposed model is verysuitable to guide audio processing tasks.For the first time the proposed concepts started to enable acomplete comparison of whole musical documents. Not onlynaïve in the sense of a simple test of identity or evaluation ofstatistical features but also under integration of semanticaspects. Consequently they form a framework foridentification, navigation and automatic linking of musicaldocuments.ReferencesDovey, M. J. (2001). A technique for “regular expression“ stylesearching in polyphonic music. ISMIR 2001. Bloomington, Indiana.Haitsma, J. & Kalker, T. (2002). A Highly Robust AudioFingerprinting System. ISMIR 2002.Lemström, K., Wiggins, G. A., Meredith, D. (2001). A Three-LayerApproach for Music Retrieval in Large Databases. ISMIR 2001.Bloomington, Indiana.Motte-Haber, H. (1985). Handbuch der Musikpsychologie. Laaber:Laaber-Verlag.Yang, C. (2002). “The MACSIS Acoustic Indexing Framework forMusic Retrieval: An Experimental Study”. ISMIR 2002."
    },
    {
        "title": "Chord segmentation and recognition using EM-trained hidden markov models.",
        "author": [
            "Alexander Sheh",
            "Daniel P. W. Ellis"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1416734",
        "url": "https://doi.org/10.5281/zenodo.1416734",
        "ee": "https://zenodo.org/records/1416734/files/ShehE03.pdf",
        "abstract": "Automatic extraction of content description from commercial audio recordings has a number of impor- tant applications, from indexing and retrieval through to novel musicological analyses based on very large corpora of recorded performances. Chord sequences are a description that captures much of the charac- ter of a piece in a compact form and using a mod- est lexicon. Chords also have the attractive property that a piece of music can (mostly) be segmented into time intervals that consist of a single chord, much as recorded speech can (mostly) be segmented into time intervals that correspond to specific words. In this work, we build a system for automatic chord tran- scription using speech recognition tools. For features we use “pitch class profile” vectors to emphasize the tonal content of the signal, and we show that these features far outperform cepstral coefficients for our task. Sequence recognition is accomplished with hid- den Markov models (HMMs) directly analogous to subword models in a speech recognizer, and trained by the same Expectation-Maximization (EM) algo- rithm. Crucially, this allows us to use as input only the chord sequences for our training examples, with- out requiring the precise timings of the chord changes — which are determined automatically during train- ing. Our results on a small set of 20 early Beatles songs show frame-level accuracy of around 75% on a forced-alignment task. Keywords: audio, music, chords, HMM, EM. 1",
        "zenodo_id": 1416734,
        "dblp_key": "conf/ismir/ShehE03",
        "keywords": [
            "Automatic extraction",
            "content description",
            "commercial audio recordings",
            "impor- tant applications",
            "indexing and retrieval",
            "novel musicological analyses",
            "chord sequences",
            "compact form",
            "mod- est lexicon",
            "speech recognition tools"
        ],
        "content": "Chord Segmentationand Recognition using EM-Trained Hidden Markov Models\nAlexanderSheh and DanielP.W.Ellis\nLabROSA,Dept. ofElectricalEngineering,\nColumbiaUniversity,NewYorkNY10027USA\n{asheh79,dpwe}@ee.columbia.edu\nAbstract\nAutomatic extraction of content description from\ncommercialaudiorecordingshasa numberofimpor-\ntantapplications,fromindexingandretrievalthrough\nto novel music ological analyses based on very large\ncorpora of recorded performances. Chord sequences\nare a description that captures much of the charac-\nter of a piece in ac o mpact form and using a mod-\nest lexicon. Chords also have the attractive property\nthat a piece of music can (mostly) be segmented intotime intervals that consis to fas ingle chord, much as\nrecordedspeechcan(mostly )besegmentedintotime\nintervals that correspond to speciﬁc words. In this\nwork, we bu ild a system for automatic chord tran-\nscriptionusingspeechrecognitiontools. Forfeatures\nwe use “pitch class proﬁle” vectors to emphasize the\ntonal content of the signal, and we show that these\nfeatures far outperform cepstral coefﬁcients for our\ntask. Sequencerecognitionisaccomplishedwithhid-den Markov models (HMMs) directly analogous to\nsubword models in a speech recognizer, and trained\nby the same Expectation-Maximization (EM) algo-\nrithm. Crucially, this allows us to use as input only\nthe chord sequences for our training examples, with-\noutrequiringtheprecisetimingsofthechordchanges\n—w h i c h are determined automatically during train-\ning. Our results on a small set of 20 early Beatlessongsshowframe-levelaccuracyofaround75%ona\nforced-alignmenttask.\nKeywords:audio,music,chords,HMM,EM.\n1I n t r oduction\nThe human auditory system is capable of extracting rich and\nmeaningfuldatafromcomplexaudiosignals. Machinelisteningresearchattemptstomodelthisprocessusingcomputers. Inthe\nmusic domain, there has been limited success when the input\nsignal or analysis is relatively simple, i.e. single instrument,\nPermission to make digital or har dcopies of all or part of this work\nforpersonal or classroom use is granted without fee provided that\ncopies are not made or distributed for proﬁt or commercial advan-tage and that copies bear this notice and the full citation on the ﬁrstpage.c/circlecopyrt2003 Johns Hopkins University.\nbeat detection, etc. Unfortunately, for complex signals, such\nas ensemble performances, or more complex analyses, such as\npitch transcription, the task rapidly increases in difﬁculty. In\nthis paper we investigate a problem with complexity in bothdimensions,chordrecognitiono nunstructured,polyphonic,and\nmulti-timbre audio. A system able to transcribe an arbitrary\naudio recording into an accura te chord sequence would have\nmany applications in ﬁnding particular examples or themes in\nlargeaudiodatabases,aswellasenablinginterestingnewlarge-\nscale statistical analysesofmusicalcontent.\nOurspeciﬁcapproachusesth ehiddenMarkovmodels(HMMs)\nmade popular in speech recognition (Gold and Morgan, 1999),\nincludingthesophisticatedExpectation-Maximization(EM)al-\ngorithm used to tra in them. This is a statistical approach, in\nwhich the wide variety of feat ure frames falling under a sin-\ngle label is modeled as random variation that follows an esti-\nmated distribution. By making a direct analogy between the\nsequence of discrete, non-overlapping chord symbols used to\ndescribe a piece of music, and the word sequence used to de-scribe recorded speech, much of the speech recognitionframe-\nworkcan be used with minimal modiﬁcation. In particular, no\ntiming alignment is required bet ween the chord labels and the\ntraining audio — using the constraints of the chord sequence\nalone, the EM approach converges to ﬁnd optimal segmenta-\ntions.\nWedrawonthepriorworkofFujishima(1999)whoproposeda\nrepresentationof audiotermed “p itchclass proﬁles”(PCPs), in\nwhichtheFouriertransformintensitiesaremappedtothetwelve\nsemitone pitch classes (chroma). This is very similar to the“chromaspectrum”proposedbyBartschandWakeﬁeld(2001).\nTheassumptionisthatthisrepresentationcapturesharmonicin-\nformationin a moremeaningfulway, therebyfacilitatingchord\nrecognition. Fujishima’s system uses nearest-neighborclassiﬁ-\ncation to chordtemplates, andperformedwell on samplescon-\ntaininga singleinstrument.\nOursystemhasparallelswiththeworkbyRaphael(2002),who\nalso uses HMMstrainedby EM to transcribemusic in termsof\nchordlabels. However,sincehisultimategoalisnote-leveltran-\nscription, his “chord” vocabul ary distinguishes between each\ndifferent combination of simultaneous notes, in contrast to our\napproachofhavingasinglemodelfor“Aminor”etc. Thishuge\nstate space precludes direct training of models for each chord,\nandinsteadstructuralinformationabouttheharmonicsexpected\nfor any given note combination are used to select among a rel-\natively small set of model ‘factors’, from which the desiredchordmodelsmaybeassembled. Hissystemisappliedtoclean\nrecordingsofsolopianomusic.\nInsection2,wedescribethestructureofourchordanalysissys-\ntemindetail. Section3describestheexperimentsweconducted\ntoevaluatethesystem,bytrainingandtestingonasmallcollec-tionof20early Beatles songs. Finally, section 4 discusses our\nfutureworkfollowedbyourconclusions.\n2S y s t e m\nThechordrecognitionsystemispresentedbelow.\nFirst the input signal is transformed to the frequency domain.\nThenitismappedtothePCPdomainbysummingandnormal-\nizing the pitch chroma intensities, for every time slice. These\nfeatures are then used to build chord models via EM. Finally,chord alignment/recognition is performed with the Viterbi al-\ngorithm.\n2.1 PitchClassProﬁleFeaturesMonophonic music recordings x[n]sampled at 11025 Hz are\ndivided into overlapping frames of N=4096points and con-\nvertedto a s hort-timeFouriertransform(STFT)representation,\nX\nSTFT[k,n]=N−1/summationdisplay\nm=0x[n−m]·w[m]·e−j2πkm/N(1)\nwherekindexes the frequencyaxis with 0≤k≤N−1,nis\ntheshort-timewindowcenter,and w[m]isanN-pointHanning\nwindow. The STFT is t hen mapped to the Pitch Class Proﬁle\n(PCP) features, which traditionally consist of 12-dimensional\nvectors, with each dimension corresponding to the intensity of\nas e m i tone class (chroma). The procedurecollapses pure tones\nofthesamepitchclass,independentofoctave,tothesamePCP\nbin; for complex tones, the harmonics also fall into particular,\nrelated bins. Frequency to pitch mapping is achieved using the\nlogarithmic characteristic of th eequal temperamentscale. Our\nexperiments use a ﬁner grained PCP vector of 24 dimensions\nto give some ﬂexibility in accounting for slight variations intuning. A step size of 100ms, or 10 PCP frames per second, is\nemployed. STFT bins kare mapped to PCP bins paccording\nto:\np(k)=⌊24·log\n2(k/N·fsr/fref)⌋mod24 (2)\nwherefrefisthereferencefrequencycorrespondingto PCP[0]\nandfsris the sampling rate. For each time slice, we calculate\nthe value of each PCP element by summing the magnitude ofallfrequencybinsthatcorrespondtoaparticularpitchclassi.e.\nforp=0,1,···,23,\nPCP[p]=/summationdisplay\nk:p(k)=p|X[k]|2(3)\n2.2 HiddenMarkovModels\nPCPvectorsareusedasfeaturestotrainahiddenMarkovmodel\n(HMM) with one state for each chorddistinguishedby the sys-\ntem. An HMM is a stochastic ﬁnite automaton in which each\nstate generates an observation. The state transitions obey the\nMarkovian property, that given the present state, the future is\nindependent of the past. (For an introduction to HMMs, see\nGoldandMorgan(1999)).TomodelthePCPvectordistributionforeachstate, we assume\nas i ngl eG a us s i a n in 24dimensions,describedby itsmeanvec-\ntorµiand covariance metric ΣiWe additionally assume that\nthe features are uncorrelated with each other, so that Σicon-\nsistsonly of variances, i.e. all off-diagonal elements are zero.\nTo specify the model we need to determine the 24 dimension\nmeanvectorµiandthe 24dimensionvariancevector diag(Σi)\nassociated with the emitting state, and the transition probabili-\nties.\nIf we knew which state (i.e. chord) generated each observa-\ntioninourtraini ngdata,themodelparameterscouldbedirectly\nestimated. Hand-marked chord boundaries could provide the\nnecessary information, but it is extremely time-consuming to\ncreate these ﬁles. In our case, we assume only that the chordsequence of an entire piece is known, but treat the chordlabels\nofeachframeashiddenvalueswithintheEM framework. This\nfreestheresearcherfromthelabor iousandproblematicprocess\nofmanualannotation.\n2.3 ExpectationMaximization\nTheexpectationmaximization(EM)algorithm(GoldandMor-\ngan, 1999) is an approach that structures the statistical classi-ﬁer parameter estimation probl em to incorporate hidden vari-\nables. We assume a joint density between the observed and\nmissing (hidden) variables, deﬁn ing the complete-data likeli-\nhoodP(X,Q|Θ)whereXrepresentstheobservedfeaturevec-\ntors,Qstands for the unknown chord labels, and Θholds the\ncurrentmodelparameters. EMestimatesthedensitiesbytakinganexpectationofthelogarithmofthecomplete-datalikelihood,\nE[logP(X,Q|Θ)]=/summationdisplay\nQP(Q|x,Θold)log(P(X|Q,Θ)P(Q|Θ)\n(4)\nThis equation expresses the complete-data log likelihood as afunction of o ld and new parameters, Θ\noldandΘ.Ateach step\nthe old parameters are ﬁxed, and Θis adjusted to maximize\nlogP(X,Q|Θ)in expectation. Thisprocessis iterated until the\nexpectedimprovementisnolargerthansome /epsilon1.E Mguarantees\nthattheestimateswillimproveateachstep,resultinginalocally\noptimal set of parameters, though not necessarily the globallyoptimalsolution. Thus,theEMsolutionreasonablyestimatesa\nsetofparametersthat maximizesthe complete-datalikelihood,\nwhichimplementstheoriginalMAP decisionrule.\nThe speciﬁc application of EM to ﬁnd maximum-likelihood\nparameter estimates for a hidden Markov model is known as\nthe Baum-Welch, or forward-backward algorithm. The up-\ndate equations derived from maximizing equation 4 amount tosetting model parameters to the sample averages of the train-\ning features, weighted by the posterior probability of each\nfeature being associated with each particular hidden label,\np(q\ni\nn|X,Θold,M),w h e r eMis the model c omprising the con-\nstraints on observationsX,constructed by concatenating the\nstatesspeciﬁedintheknownchordsequenceintoasinglecom-\npositeHMMforeachsong.\n2.4 Viterb iAlignment\nTheEMalgorithmcalculatesthe meanandvariancevectorval-\nues, andthe transitionprobabilitiesforeachchordHMM.With\nthese parameters deﬁned, the model can now be used to de-\ntermine a chord labeling for each song. The Viterbi algorithm\n(Gold and Morgan, 1999) is used to either forcibly align orInput SignalG#\nGF#FED#DC#CBA#A\nSTFTnormalized\n     sumEM\nDFT frames PCP framesAm\nE7C\nTranscription / AlignmentViterbi\n011kHz\nChord ModelsCC C AmAmAmC\n440Hz.\n.....\n.\nFigure1: System Overview\nrecognize these labels; in forced alignment, observations are\naligned to a composed HMM whose transitions are limited tothose dictated by a speciﬁc chor ds equence, as in training i.e.\nonly the chord-change times are being recovered, since the\nchord sequence is known. In recognition, the HMM is uncon-\nstrained,inthatany chordmayfollowanyother,subjectonlyto\ntheMarkovconstraintsin thetrainedtransitionmatrix. We per-form both sets of experiments to demonstrate that even when\npure recognition performance is quite poor, a reasonable ac-\ncuracy under forced alignments indicates that the models havesucceededin learningthe desired chordcharacteristicsto some\nextent. T he output of the Viterbi algorithm is the single state-\npath labeling with the highest likelihood given the model pa-rameters. This best-path assigns a chord to every 100ms time\nslice, resultingina time-alignedsongtranscription.\n2.5 WeightedAveragingofRotatedPCPVectors\nThe outcome of the EM training is a set of model parameters\nincludingmeansandvariancesinPCPfeaturespaceforeachof\nthe deﬁned chord states. These values deﬁne our initial chord\nmodels,however,animprovementcanbemadebycalculatingaweighted average of t he models for every chord family (major,\nminor, maj7 etc.) across all root chromas (A, A#, B, C, etc.).\nThis involves rotating the PC Pvectors from each chroma until\nPCP[0] is the root pitch class, computing a weighted average\nacross all the chromas (weighted by frequencyof chord occur-\nrence),thenun-rotatingtheweightedaveragePCPvectorsbackto their origina lpositions to construct new, regularized models\nfor each chord. Thus, if findexes across chord families and c\nis the numerical offset of each chroma relative to A in quartertones (e.g. A/mapsto→0, A#/mapsto→2, B/mapsto→4e t c . ) ,t h e nt he mean vector\nfortheparentmodelofchordfamily finPCP spaceis\n¯µ\nf[p]=/summationtext\ncµf,c[(p−c)mod24]·Nf,c\n/summationtext\ncNf,c(5)\nwhereµf,cis the original mean vect or for one speciﬁc chord\nfamily/chroma c ombination,Nf,cis the num ber of frames as-\nsignedtothatstate inforcedali gnmentofthetrainingdata,and\npindexesthe24PCPbins. Therotatedmodelsthenreplacethe\nindividualfamily/chromastatemodelswith\n¯µf,c[p]=¯µf[(p+c)mod24] (6)\n(Variancesaresimilarlypooled). Themotivationisthatbyusing\nvaluescharacteristic to the entire family, a derived state model\navoidsoverﬁtting its particular chord data. There is also the\nadvantage of increasing each individual chord’s training set to\nAlbum\n Song\n Set\nBeatlesforSale\n Eightdaysaweek\n test\nEverylittle thing\n test\nIdon’t want to spoil\ntheparty\ntrain\nI’llfollowthesun\n train\nI’maloser\n train\nHelp\n Help\n train\nI’vejustseena face\n train\nIt’sonlylove\n train\nTicketto ride\n train\nYesterday\n train\nYou’re going to lose\nthatgirl\ntrain\nYou’ve got to hide\nyourloveaway\ntrain\nAH a rdD ay’sNight\n Aha rdday’snight\n train\nAndIloveher\n train\nCan’tbuymelove\n train\nIshould’ve known\nbetter\ntrain\nI’m happy just to\ndancewith you\ntrain\nIfI fell\n train\nTell me why\n train\nThingswesaid today\n train\nTable 1: Corpus of 20 early Beatles songs used in the experi-\nments.\ntheunionofallchordfamilymembers. Theresultsbelowshow\nthatthissimpleapproachgave verysigniﬁcantimprovements.\n3I m p l e mentation and Experiments\nThe Hidden Markov Model Toolkit (Young et al., 1997) was\nusedtoimplementourchordrecognitionsystem. Twentysongs\nfrom three early Beatles albums were selected for our experi-\nments (see table 1). The songs were read from CD then down-\nsampled and mixed into monoﬁles at 11025Hz sampling rate.\nThe chordsequencesfor eachsong were producedbymapping\ntheprogressionsfromastandardbookofBeatlestranscriptions\n(PaperbackSong Series: The Beatles ,1995)to a simpler set of\nchordsas shownin table2. Thetwentysoundﬁlesandtheir as-\nsociatedchordsequencescomprisetheinputtooursystem. Two\nsongs, “Eight Days a Week” and “E very Little Thing”, wereChordfamilies\n maj,min,maj7,min7,\ndom7,aug,dim\nRoots\n A/flat,B/flat,C/flat,D/flat,E/flat,F/flat,G/flat,\nA, B, C, D,E,F, G,\nA/sharp,B/sharp,C/sharp,D/sharp,E/sharp,F/sharp,G/sharp,\nExamples\n Amaj,C/sharpmin7,G/flatdom7\nTable2:Deﬁnition of the 147 possible chords that can appear\nas HMM states. The label “X” is given to chords not covered\nbythisset. Inpractice,only32labelsoccurredin ourdata.\nFeature\n Align\n Recog\ntrain18 train20\n train18 train20\nMFCC\n 27.0 20.9\n 5.9 16.7\n14.5 23.0\n 7.7 19.6\nMFCC\nD\n24.1 13.1\n 15.8 7.6\n19.9 19.7\n 1.5 6.9\nMFCC\n0\nD\nA\n13.9 11.0\n 2.2 3.8\n9.2 12.3\n 1.3 2.5\nPCP\n 26.3 41.0\n 10.0 23.6\n46.2 53.7\n 18.2 26.4\nPCP\nROT\n 68.8 68.3\n 23.3 23.1\n83.3 83.8\n 20.1 13.1\nTable 3: PercentFrame Accuracyresults. Within each row, the\nﬁrst subrow re fers to “Eight Days a Week” and second subrow\nto “EveryLittle Thing”. Columnsshow the frameaccuracyfor\nforced alignment and recognition, using the train18 (excluding\ntestcases) andtrain20(includingtestcases) sets.\ndesignated as the test set, and for these songs the actual chord\nboundarieswere hand-labeledusing WaveSurfer; this providedtheground-truthusedtodetermineframeerrorrates.\nWe made se parate HMM trainings using ﬁve distinct feature\nconﬁgurations. The ﬁrst three use Mel-frequency cepstral co-\nefﬁcients (MFCCs), the ubiquit ous features of speech recogni-\ntion, calculated using HTK We i ncludedMFCCs as a compari-\nson: Wedidnotexpectthemtobewellsuitedtothistask,since\nthese features suppress pitch information. However, MFCCs\nhave performed surprisingly well in some other music content\nanalysis tasks (Logan,2000),so they make a goodbaseline. In\neachcase, the totalmodeldimensionswere keptat 24to matchthe number of parameters in the PCP systems; i nt h eﬁ rstcase\n(MFCC) we used 24 MF CCs to get a relatively ﬁne spectral\ndescription. Case 2 (MFCC\nD) used ju st 12 MFCCs but in-\ncluded their deltas (velocities) since these are a popular addi-\ntioninspeechrecognition. Thethirdcase(MFCC\n 0\nD\nA)used\n7th order MFCCs including the c0(average log energy) term,\nalong with deltas and accelerati ons for each dimension, again\nmimickingsuccessfulspeech featureconﬁgurations.\nTheremainingtwofeatureconﬁgurationsareplainPCPvectors\n(PCP) andthe averagedPCP vectorrotations(PCP\n ROT),both\nin 24 dimensions. A matlab script was written to perform a\nSTFTof Hanninglength4096,and asubsequentPCP mapping\nusingreferencefrequency440Hz(A4).\nWetrained on the 18 songs from our dataset not designated as\ntest examples. We also repeatedthe experimentstrainingonall\n20 songs — i.e. including the test examples — to establish a\nperformance ceiling in the optim istic condition when the test0 5 10 15 20 2500.050.10.150.20.250.30.350.4PCP_ROT family model means (train18)\nDIM\nDOM7MAJMINMIN7\nFigure3: Meanvect orsforthePCP\n ROT average chordfamily\ntemplates.\ncasesexactlymatchpartofthetrainingset.\nTrainingbeginswiththeuniformsegmentationandchordlabel-\ningofevery training song, using chord sequence information.\nHMMstatechordmodelswereinitializedwithglobalmeanandvariance values from the entire dataset (so called ﬂat-start EM\ninitialization). Ideally, enough of the chord models align with\nthe actual realizations of the chord to allow successively moreaccurate models to evolve during training iterations. Prior to\ntraining,a single compositeHM Mfor eachsongis constructed\naccording to the chord sequence information (see section 2.2),whichconstrainsthetrainingprocess. EMproceedsfor13to15\niterations. Afterall the trainingsongshave beenprocessed, the\ntotalset ofstatistics isusedtore-estimatetheparametersofthe\nindividual chord models. At this point, averaged-rotated PCP\nmodelsarecombinedasdescribedinsection2.5.\nLastly, the Viterbi algorithm is applied to generate either a\nboundary alignment of an existing chord sequence or recog-nize a newchordsequence. In the case of alignment,the chord\nsequence ﬁle is used to generate a simple composite HMM\nwithallowable transitions determined by the song’s progres-\nsion. Recognition must be able to accommodate any sequence\ndrawn from the set of training song chords. An appropriate\nchordloop isderivedfromthe chordsequenceﬁles of alltrain-\ningsongs. TheViterbialgorithmwilldeterminethebestpathin\nthis chord network. Each PCP frame is assigned a chord class\nsuch that the likelihood of the entire path is maximized. The\nchord-labeled frame sequence along this path is the aligned or\nrecognized output of the system. By comparing the automaticchord labels with the hand-marked ground-truth labels of the\ntestexamples,wecancalculatetheframeerrorrate.\n3.1 FrameAccuracyResultsAsummary of the frame accuracy results is presented in ta-\nble3,with the alignmentandrecognitionaccuracypercentages\non each of the two test examplesfor the ﬁve feature conﬁgura-tions and two trai ning sets. We see that models trained using\nthe base PCP features perform better than models trained us-\ning MFCCs in all cases except one (recognitionof the ﬁrst testexample u sing MFCC\nDand train18). Using averaged-rotated\nPCPs (PCP\n ROT) resu lts inmodelsthatoutperformall MFCC-\ntrained ones, as well as the base PCP models in all cases ex-trueE G D Bm G\nalignE G DBm G\nrecogE G Bm Am Em7 Bm Em716.27 24.84\ntime / secintensity\nA#BC#D#EF#G#pitch classBeatles - Beatles For Sale - Eight Days a Week (4096pt)\n20\n0406080100120\nFigure 2: Illustration of PCP features vectors, ground truth label s,forced alignment output, and recognition output, for a brief\nsegmentof“Ei ghtDaysa Week”.\ncept, curiously, recognition based on train20. The strength of\nthe PCP representati on, and the model averaging approach, is\nclearlydemonstrated,withthePCP\n ROT modelsperformingas\nmucha fourtimesbetterthanthebestMFCCcounterparts.\nForced alignmentalways outperfo rmsrecognition, as expected\nsincethebasicchordsequenceisalreadyknowninforcedalign-\nmentwhichthenhasonlytodeterminetheboundaries,whereas\nrecognition has to determine the chord labels too. Comparing\ntheperformanceoftrain18andtra in20(i.e. testingonexamples\nthat are distinct from, or includedin, the training set), we see a\nmixed effect with MFCC features. For the PCP system, testing\non the training set (train20) gives a signiﬁcant increase in ac-curacyforbothalignmentandrecognition,indicatingthatthese\nmodelsare able to exploitthe ‘cheating’informationof getting\nap r e v i e w of the test cases. By contrast, PCP\nROTachieves\nno beneﬁt from training on the test set (and even does signiﬁ-\ncantly worse on recognizing “Every Little Thing”, which may\nreﬂect some pathological case in the local maximum found byEM). Asa generalrule, if includingthe test data in the training\nsetdoesnot signiﬁcantly increase performance,we can at least\nbeconﬁdent that the models are not overﬁtting the data; thus,\nfor PCP\nROT, we c ould try training with more model param-\neters, such as Gaussian mixtur es rather than single Gaussians,\nsince weh a v enotalready overﬁtour modelsto the data—even\nthoughthisisalreadythe best-performingsystem overall.\n3.2 ChordCo nfusion\nGreaterinsightintosystemperformancecanbeobtainedbyex-\naminingthespeciﬁckindsoferrorsbeingmadeintermsofmis-\nrecognitionsofparticularchordsintootherclasses. Thecasewe\nare mostinterestedin is recognition(ratherthan alignment)us-ing weight-averaged PCP HMMs (PCP\n ROT), trained without\nusing test songs (train18). Table 4 presents the confusion ma-trices for every frame in “Eight Days a Week”, which we label\nwith only5 c hordsplus“X”. Notice the frequentconfusionbe-\ntween major c hords and their minor version, which differ only\nby the semitone between the major and minor third intervals.\nBetter discrimination of these chords might be achieved by in-\ncreasingthe system’sfrequencyresolution.\n3.3 ModelMeans\nFigure 3 shows the actual PCP-domain ‘signatures’ — the\npooled chord family mean vectors — learned in the PCP\nROT\ntrain18 system. While it is difﬁcult to make any strong inter-pretation of this plot, it is interesting to see the similarities and\ndifferencesbetweenthedifferentchords.\n3.4 Out putExample\nFigure 2 shows an eight-second segment of the song “Eight\nDaysaWeek”takenabout16secondsintothesong. Thedisplay\nconsists of the PCP feature vectors shown in a spectrogram-like format. Underneath are three sets of chord labels:\nthehand-marked ground truth, the labels obtained by forced\nalignment, and the labels returned by recognition (using thePCP\nROT/train18system). Whilethisisonlyasmallfragment,\nit givesaﬂavo rofthenature oftheresultsobtained.\n4F u ture Work\n4.1 TrainingParameters\nOur future work on this system will concentrate on the follow-\ningareas:XIRTAMNOISUFNOC\"ROJAMA\"\nkeeWasyaDthgiE\njaMn iM7 jaM7 niM7 moDg uAm iD\nA4 . 71.. 9 4. .\nbB/#A.......\nbC/B.......\nC/#B.......\nbD/#C.......\nD. 3.....\nbE/#D.......\nbF/E715..1..\nF/#E1 ......\nbG/#F. 9.....\nG.......\nbA/#G.......\nXIRTAMNOISUFNOC\"RONIMB\"\nkeeWasyaDthgiE\njaMn iM7 jaM7 niM7 moDg uAm iD\nbC/B. 2 7.....\nC/#B.......\nbD/#C.......\nD. 3.....\nbE/#D.......\nbF/E8 1 5. 1 4...\nF/#E33 .....\nbG/#F. 2 1.6 ...\nG2 ......\nbA/#G.......\nA....1..\nbB/#A. 9.....XIRTAMNOISUFNOC\"ROJAMD\"\nkeeWasyaDthgiE\njaMn iM7 jaM7 niM7 moDg uAm iD\nD03911.5 9 4. .\nbE/#D.......\nbF/E8143. 1 4...\nF/#E7 ......\nbG/#F. 2 4. 5 1...\nG.......\nbA/#G.......\nA916.. 6 7. .\nbB/#A... 2 5...\nbC/B. 0 2.....\nC/#B.......\nbD/#C. 1.....\nXIRTAMNOISUFNOC\"ROJAME\"\nkeeWasyaDthgiE\njaMn iM7 jaM7 niM7 moDg uAm iD\nbF/E8 51511.9 ...\nF/#E9 ......\nbG/#F... 1 1...\nG3 ......\nbA/#G.......\nA9 ...1..\nbB/#A.......\nbC/B8 ...,..\nC/#B.......\nbD/#C. 0 2.....\nD. 4 1.....\nbE/#D.......XIRTAMNOISUFNOC\"ROJAMG\"\nkeeWasyaDthgiE\njaMn iM7 jaM7 niM7 moDg uAm iD\nG2 2153.....\nbA/#G.......\nA11......\nbB/#A.......\nbC/B. 4 2. 3. . .\nC/#B.......\nbD/#C.......\nD3171.21..\nbE/#D.......\nbF/E1 4 01. 6 2...\nF/#E.......\nbG/#F. 2 1.....\nXIRTAMNOISUFNOC\"DROHCX\"\nkeeWasyaDthgiE\njaMn iM7 jaM7 niM7 moDg uAm iD\nA91......\nbB/#A.......\nbC/B. 1 2.....\nC/#B.......\nbD/#C.......\nD. 5 3.....\nbE/#D.......\nbF/E.......\nF/#E.......\nbG/#F. 1.....\nG.......\nbA/#G.......\nTable4:Confusionmatricesfor recognitionof “EightDaysa Week”, PCP\n ROT,train18. Enharmonicequivalentchordshavebeen\ncombined.•More data and p arameters: In section 3.1, we noted\nthat the PCP\n ROT chord family models show no signs of\noverﬁtting, so em ploying more parameters, e.g. by us-\ningGaussianmixtur emodelsratherthansingleGaussians,\nshouldachievefurtheraccuracyimprovements. Ofcourse,\nalargerandmorediversecollectionoftrainingdatashould\nalsoimproveaccuracyandapplicabilityofthesystem. Themostsigniﬁcantobstacleto obtainingthisdatais ﬁndinga\nreliablesourcefortheassociatedchordsequences.\n•Frequency Resolution :A sobserved from the ma-\njor/minor chord confusion, our recognition system most\nlikely does not have enough frequencyresolution. A sim-\nple remedy is to use longer FFT windows; increasing theHanninglengthto8192mayallowthesystembettertodis-\ntinguish neighboringnotes. This issue is particularlyseri-\nous at low frequencies, when the spacing of adjacent FFTbins becomesgreater than one quarter-tone. Currently we\nassign all energyin these low bins to a single chroma, but\nbetterresultsmightbeobtainedbyspreadingitacrosssev-eralPCPdimensionsinproportiontotheiroverlapwiththe\nFFT binfre quencyrange.\n•Adaptive Tuning :O n e a r gument for using 24-\ndimensionalPCP vectorswas to accommodateslightvari-\nations in tuning. Another way to help ensure that notes\ndo not adversely interact with the PCP bin edges wouldbe to estimate the precisetuningused in a particularsong,\nandcenterthePCPfeaturedeﬁnitionaccordingly. Thiscan\nbe accomplishedby performinga muchﬁner FFT onlongportions of the original ﬁle, determining the most intense\nfrequency and its corresponding pitch, and then shifting\ntheFFTtoPCPmappingsuchthatthisfrequencyfallspre-ciselyin themiddleofa note.\n•Different Features :Wechose PCP features because of\ntheir priorsuccessin chordclassiﬁcation tasks, butwe arealsointerestedinverydifferentkindsoffeatures. Oneidea\nwewould like to try is looking at the autocorrelation of\nsubbandenergyenvelopesattheverylonglagsthatemerge\nas the least commonmultipleof the differentfundamental\nfrequenciesmakingupa chord.\n5C o n c l u s i o n\nOur experiments show that HMM models trained by EM on\nPCPfeaturescansuccessfullyrecognizechordsinunstructured,\npolyphonic, multi-timbre audio. This is a challenging instanceof extracting complex musical information from a complex in-\nput signal has many practical applications, since harmonic in-\nformation covers much of the character of western music. Be-causeoursystem usesonlytherawaudio. itshouldbeapplica-\nbleovera widerangeofcircumstances.\nAlthough recognition accuracy is not yet sufﬁcient to provide\nusablechordtranscriptionsof unknownaudio,theabilitytoﬁnd\ntime alignments for known chord progressions may be useful\nin itself. Moreover, the minimally-supervised EM training ap-\nproach means that the incorporation of large amounts of addi-tional training data should be straightforward,since no manual\nannotationisrequired. Alargersystemofthiskindshouldresult\ninmuchmorepreciseandsuccessfulmodels.Acknowledgments\nOur thanks go to the anonymous reviewers for their helpful\ncomments.\nReferences\nBartsch,M.A.andWakeﬁeld,G.H.(2001). Tocatchachorus:\nUsingchr oma-basedrepresentationsforaudiothumbnailing. In\nProc. IEEE Workshop on Applications of Signal Processing toAudioandAcoustics ,Mohonk,NewYork.\nFujishima, T. (1999). Realtime chord recognition of musical\nsound: A system using common lisp music. In Proc. ICMC ,\npages464–467,Beijing.\nGold,B.andMorgan,N.(1999). SpeechandAudioSignalPro-\ncessing: ProcessingandPerceptionofSpeechandMusic .John\nWiley&Sons ,Inc.\nLogan,B.(2000). Melfrequencycepstralcoefﬁcientsformusic\nmode ling. In Proc. Int. Symposium on Music Inform. Retriev.\n(ISMIR),Plymouth.\nPaperbackSongSeries: The Beatles (1995). Hal LeonardCor-\nporation.\nRaphael,C.(2002). Automatictranscriptionofpianomusic. In\nProc.Int.SymposiumonMusicInform.Retriev.(ISMIR) ,Paris.\nYoung,S.,Odell,J.,Ollason,D.,Valtchev,V.,andWoodland,P.\n(1997).HTKHiddenMarkovModelToolkit .E n t ropicResearch\nLaboratoriesInc.,CambridgeUniversity."
    },
    {
        "title": "Effectiveness of HMM-based retrieval on large databases.",
        "author": [
            "Jonah Shifrin",
            "William P. Birmingham"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417187",
        "url": "https://doi.org/10.5281/zenodo.1417187",
        "ee": "https://zenodo.org/records/1417187/files/ShifrinB03.pdf",
        "abstract": "We have investigated the performance of a hidden Markov model QBH retrieval system on a large musical database. The database is synthetic, generated from statistics gleaned from our (smaller) database of musical excerpts from various genres. This paper reports the performance of several variations of our retrieval system against different types of synthetic queries on the large database, where we can control the errors injected into the queries. We note several trends, among the most interesting is that as queries get longer (i.e., more notes) the retrieval performance improves. 1",
        "zenodo_id": 1417187,
        "dblp_key": "conf/ismir/ShifrinB03",
        "keywords": [
            "hidden Markov model",
            "QBH retrieval system",
            "synthetic musical database",
            "synthetic queries",
            "control errors",
            "performance analysis",
            "different types",
            "trends observed",
            "improvement in performance",
            "longer queries"
        ],
        "content": "Effectiveness of HMM-Based Retrieval on Large Databases \nJonah Shifrin \nEECS Dept, University of Michigan \n110 ATL, 1101 Beal Avenue \nAnn Arbor, MI 48109-2110 \njshifrin@umich.edu William Birmingham \nEECS Dept, University of Michigan \n110 ATL, 1101 Beal Avenue \nAnn Arbor, MI 48109-2110 \nwpb@eecs.umich.edu \n \nAbstract \nWe have investigated the performance of a hidden \nMarkov model QBH retrieval system on a large \nmusical database. The database is synthetic, \ngenerated from statistics gleaned from our (smaller) \ndatabase of musical excerpts from various genres. \nThis paper reports the performance of several \nvariations of our retrieval system against different \ntypes of synthetic queries on the large database, \nwhere we can control the er rors injected into the \nqueries. We note several trends, among the most \ninteresting is that as queries get longer (i.e., more \nnotes) the retrieval performance improves. \n1 Introduction \nFor the past couple years, our group has been working on the \nVocalSearch (Birmingham 2003) system, part of the larger \nMusArt project (Birmingham, Dannenberg et al. 2001; \nBirmingham 2002; Shifrin 2002). VocalSearch is a query-by-\nhumming (QBH) system for music search and retrieval. The \nsystem has a database of musical pieces. Each piece in the \ndatabase is represented by a set of monophonic themes, which \nare encoded as hidden Markov  Models (HMMs). Each HMM \nis built automatically from a single theme. Each query is \ntreated as an observation sequen ce. Similarity between a query \nand a theme is assessed by determining the probability the \nHMM could have generated the query. The quality of a match \nbetween a piece and a query is gi ven by the probability its best \nmatching theme generated the query. The pieces are then \nranked by match quality. \nWe, like most researchers in th e MIR area, have evaluated our \nsystem on relatively small databases. Our largest experiments \nuse a database of about 2600 themes (Shifrin 2002). Yet, small \ndatabases pose a problem, as we anticipate that MIR system \nwill need to work on database sizes approaching millions of \nthemes in the relatively near future. Consider that \nVocalSearch works with Apple’ s iTunes, a popular computer-\nbased media player. Assume a typical pop song is roughly five \nminutes long. If songs are stored as 128kbps mp3 files, a 100-gigabyte hard drive can hold 22,365 songs. Given that PCs are \nalready shipping with 100-gigabyte drives, we can expect \nVocalSearch will need to deal with a database of this size in \nthe very near future. \nThere are two main reasons wh y we have been limited to \nexperiments on relatively small databases. First, our system \ndepends on matching sung queries to themes. We can \nautomatically generate themes from a song encoded in MIDI; \nhowever, the process is error prone and needs human \nsupervision, thus limiting the size of the database. Perhaps \nmore importantly, it is difficult to get a sufficiently large \nnumber of queries to adequately cover a large \ndatabase(Bainbridge 2002; Downie 2002). Controlling for \nsinger variation in ability, knowledge, and so forth, makes the \nexperimental process long and complex.. \nIn order to sidestep the labor-intensive steps involved in \ncreating and querying a large song database, we synthetically \ngenerated both a 50,000-theme database and several \ncorresponding sets of 5100 queries. In this paper, we report on \nthe performance of our HMM-base retrieval system for this \ndatabase and query set configurations. We also report on how \nvarying parameters of the HMM system impact performance. \n2 System Overview \nIn this section, we provide a brief overview of our system. \nFigure 1 gives a block diagram overview of our system. \n \nFigure 1: System Diagram \n2.1 Queries \nFor an audio query, a singer records a .wav file using \nVocalSearch’s recording program. The .wav file is transcribed \ninto a MIDI-based representa tion using a pitch tracker and \nnote segmenter. \nFigure 2 shows a time amplitude representation of a query, \nwith its pitch-tracker output on  piano roll, and a sequence of Permission to make digital or hard copi es of all or part of this work for \npersonal of classroom use is granted w ithout fee provided that copies are not \nmade or distributed for profit or commercial advantage and that copies bear \nthis notice and the full citation on the first page.   2003 The Johns Hopkins \nUniversity. values derived from the MIDI representation (the deltaPitch , \nIOI and IOIratio  values). \n \ndelta pitch           2    2        0   -2 –2    2  2 –4   –1 -3 -3  \nIOI (100 ms units)    3    2        3    2  1    2  1  1    2  1  1 \nIOI ratio             1.5  .66      1.5  2  .5   2  1  1    2  1  1    C\nC \n \nFigure 2: A sung query (adapted from Shifrin, et al. (Shifrin \n2002)) \nWe define the following state model (Shifrin 2002): \n• A note transition  between note n and note n+1 is \ndescribed by the duple < deltaPitch , IOIratio >.  \n• deltaPitch n is the difference in pitch between note n and \nnote n+1. \n• The inter onset interval  (IOI n) is the difference between \nthe onset of notes n and n+1. \n• IOIratio n is IOI n/IOI n+1. For the final transition, \nIOI n = IOI n /duration n+1. \n2.2 Targets \nThe targets in our database are themes, and are represented by \nMarkov Models (MM). A MM is a weighted automaton that \nconsists of: \n• A set of states, S = {s1, s2, s3,…, sn}. \n• A set of transition probabilities, T, where each ti,j in T \nrepresents the probability of a transition from si to sj. \nIn this model, the probability of transitioning from a given \nstate to another state is assumed to depend only on the current \nstate. This is known as the Markov property. \nEach state , s, in the Markov model consists of the duple \n<deltaPitch , IOIratio >. Figure 3 shows a theme as a MM. \nMMsare too simple for our application, as they require that \neach state in the model correspo nd to a fully observable state \nin the world. In QBH applications, this is almost never the \ncase, as there are many errors  introduced by the singer, the \ntranscriber, and the segmente r (Mongeau and Sankoff 1990; \nRaphael 1999; Meek and Birmingham 2002). For example, a \nuser (singer) may not be able to generate intervals greater than \na perfect 5th, so that, from the system’s perspective, it is \nimpossible to distinguish between a minor 6th and a minor 7th. \nThus, we need to account for these various types of errors by \nextending the MM to a hidden Markov Model, or HMM. The \nHMM allows us a probabilistic map of observed states to \nstates internal to the model (hidden states).  \n1\n.5 .33 Delta pitch      2   2   1   2   -2  -1   -2   -2 \nIOI              1   1   1   1    1   1    1    1 \nIOI ratio        1   1   1   1    1   1    1    1 \nState            α       α       β        α         χ        δ         χ          χ   \n1\n.33 .33 .5 \nα \nβ χ \nδ \n.7 \n.1 .1 \n.1  \nFigure 3: Markov model for a scalar passage  (adapted from \nShifrin, et al. (Shifrin 2002)) \nA model that explicitly maintains a probability distribution \nover the set of possible observations for each state is called a \nhidden Markov model (HMM). More formally, an HMM \nrequires two properties in add ition to the properties required \nfor a standard Markov model: \n• A set of possible observations, O={o1, o2, o3,…, o n}. \n• A probability distribution over the set of observations for \neach state in S. \nIn our approach, a query is a sequence of observations. Each \nobservation is a note-transition duple, < deltaPitch, IOIratio >. \nMusical themes are represente d as hidden Markov models \nwhose states also correspond s to note-transition duples. \nThus P( O|S) is the P( note transition O | note transition S). This \nprobability requires that both the observed deltaPitch and \nIOIratio must be computed in terms of the note transition \nprobability. deltaPitch and IOIratio are assumed to be \nconditionally independent.  \nSo, P( note transition O | note transition s) = \nP(deltaPitch O|deltaPitch S)*P(IOIratio O|deltaPitch S).  \nIf one assumes pitch quantized at the half step and that a \nsinger will jump by no more than an octave between notes, \nthere are 25 possible deltaPitch values. Our \nP(deltaPitch O|deltaPitch S) table is based on the data from a \nstudy (Pardo and Birmingham 2003)  where singers tried to \nmatch a pitch interval by singing it after hearing it. The \nP(IOIratioO|IOIratioS)  is based on the ratio of two IOIratios. \nWhere, the ratio of  IOIratios is the IOIratio O/IOIratio S. This \nprovides a ratio as to how close the observation is to the \n(hidden) state. Mapped in l og space into 27 buckets in a \nnormal distribution it provides our table for \nP(IOIratioO|IOIratioS) . \n2.3 Matcher \nUsing the Forward algorithm, our matcher returns the \nprobability that the HMM generated the observation sequence. \n(Rabiner and Juang 1993; Durbin, Eddy et al. 1998) The \nForward algorithm sums up the probability of traversing all \npaths through the HMM. \n3 Database Generation \nThe synthetic database that we generated follows the qualities \nof our existing theme database, which is based on a collection \nof Beatles songs. To generate the synthetic database, we \nanalyzed the propertie s of a Beatles theme database that we have experimented with for a year. The Beatles database is a \nMIDI collection of 260 Beatles MIDI songs, which were \nprocessed into a database of 284 themes.  \nMost of the Beatles songs were modeled with one theme that \nencapsulated all hooks or riffs. However, a few of the songs \nhad two or three distinct themes. \nWe analyzed three properties of the database, deltaPitch , \nIOIratio  and length (of the themes). For deltaPitch, we \nexamined the distribution of the deltaPitch’s in the database. \nAny interval jump greater than +12 semitones or less than –12 \nsemitones was quantized to +12 and –12 semitones, \nrespectively. Thus, there were 25 total possible deltaPitch \nvalues. In the Beatles database, there were 12,379 pitch \nintervals. Figure 4 represents the deltaPitch distribution.  \nFraction of Database by deltaPitch\n00.050.10.150.20.25\n- 1 2 - 1 0 - 8 - 6 - 4 - 2 02468 1 0 1 2  \nFigure 4: deltaPitch  distribution in Beatles database                                             \nFraction of Database by ln(IOIratio)\n00.050.10.150.20.250.3\n-2.2 -1.8 -1.4 -1 -0.6 -0.2 0.2 0.6 1 1.4 1.8 2.2 \nFigure 5: IOIratio  distribution in Beatles database \nSimilarly, the Beatles database contained 12,379 IOIratios. \nWe quantized the IOIratios into 23 bins distributed on a \nlogarithmic scale. We chose logarithmic scale for a uniform \ndistribution. The smallest IOIratio bin is ln(-2.2) and the \nlargest IOIratio bin is ln(2.2), w ith bins spaced at every ln(.2). \nFigure 5 represents the IOIratio distribution. \nFinally, we created a theme-length distribution based on the \nnumber of notes contained by the themes in the Beatles \ndatabase The median theme length was 40 notes, with a \nstandard deviation of 19.91 notes. \nWe created the database by using our observed distributions \nfor deltaPitch, IOIratio and theme length to randomly generate \neach synthetic theme. Our resulting synthetic database \ncontained 50,000 themes. This synthetic theme database \ncorresponds to a database of roughly 22,000 songs assuming \nan average of 2.27 themes per song (which is typical for our \ndatabases). 4 Synthetic Query Generation \nWe generated 5100 queries from different targets in the \ndatabase for each set. These qu eries were evenly distributed \nby length. One hundred queries were created ranging in length \nfrom five notes to 55 notes. This range represents bounds for \nthe longest and shortest queries we observed in experiments \nwith VocalSearch.  \nThe synthetic queries consist of a sequence of notes. Each \nnote has a pitch, an onset time and duration. The synthetic \nqueries are equivalent to sung queries that have been pitch \ntracked and segmented. Queries were varied for length to \ndetermine if the matcher’s performance is related to the \nquery’s length. If a query contains too few notes, it may not \nhave enough information to distinguish its target from the \nother targets in the database; if the query were sufficiently \nlong, it might not match our themes, which may be shorter \nthan the query. \n4.1 Perfect Queries \nA perfect query is an exact ex cerpt from a target in the \ndatabase. The purpose of presenting perfect queries to the \nmatchers is to create a baseline statistic for testing our system. \nIf our matcher cannot consistently rank perfect queries of \nlength l first overall, then it cannot be expected to rank an \nimperfect query of length l first. \n4.2 Imperfect Queries \nAn imperfect query simulates the type of query we expect to \nbe input into our system. The typical user makes a substantial \nnumber of errors when singing, as reflected in deltaPitch  and \nIOIratio ratios. In addition, pitch trackers and segmenters \nintroduce errors.(McNab, Smith et al. 1996; Madden, Smith et \nal. 2001; Meek and Birmingham 2002) \nAn assumption we make is that we have a perfect probabilistic \nmodel of singer and transcription error. What we mean is that, \ngiven our statistics about singer error, we will only generate \nqueries consistent with those statistics. We note that it is \nentirely possible that these st atistics are deficient, which \nwould mean that our synthetic queries are not representative \nof real-world queries.  \nThus, our imperfect query experiments represent a best-case  \nsimulation. If we cannot successfully perform retrieval on a \nmatcher that uses an exact observation probability distribution \non a large database, then that matcher could not be expected to \nsuccessfully retrieve on large databases. If, however, a \nmatcher can successfully perform retrieval with our query \napproximation, it indicates that  the matcher is capable of \nperforming retrieval, provided it has an accurate observation \nprobability distribution.  \nIn order to generate a perfect query, we first select a length l \nsubsequence of a target in th e database. Then, we transform \nthe selected portion of th e target into a sequence, S, of \n<deltaPitch , IOIratio > duples.  \nAn imperfect query is created  as follows. For each state si in S, \nwe generate an observation oi based on the same probability \ntables used to calculate P( O|S) in the matcher.  \nAs an example of how the queries were generated, say that Sm \n=α, and that P( β|α) = 0.7, P( χ|α)= 0.3, the observation generated Om, would have a probability of 0.7 of being β and \na probability of 0.3 of being χ. \nThis set of imperfect queries assumes users introduce only \ncumulative error. Cumulative error means that the singers shift \ntheir pitch and rhythm baseline as they go. For example, \nconsider a target with deltaPitch  <+2,-2> and an IOIratio of \n<1,1>. If the first observed deltaPitch  in the query is +4, using \na cumulative error distribution, there is the same likelihood \nthat the second observed deltaPitch would be -2 as it would \nhave been if the first observed deltaPitch  had been a +2.  \nSome research (Meek and Birmingham 2002; Pollastri 2002) \nsuggests that local errors are more common than cumulative \nerrors. For a local error, if the first observed deltaPitch was a \n+4, the singer would be more likely to query a deltaPitch  of -4 \nfor the second observation to keep their initial baseline. Figure \n6 demonstrates the difference between local errors and \ncumulative errors. Currently, the MIR community is divided \non whether cumulative or local errors are more prevalent. \n \nTarget on Piano Roll deltaPitch :     +2       -2 \nQuery with Cumulative ErrordeltaPitch :     +4       -2 \nQuery with Local Error deltaPitch :     +4       -4 \n \nFigure 6: Example of cumulative and local errors \nThe HMM-based retrieval system used in the experiments \npresented in this paper was designed under the assumption \nthat singers shift their baseline as they go, introducing \ncumulative errors. Thus, the imperfect queries were created \nunder the assumption of only cumulative errors. \n4.3 Query Insertions and Deletions \nOne common aspect of real-world  queries is insertions and \ndeletions. Persons and pitch trackers/segmenters commonly \ninsert and delete notes in queries, relative to the targets in our \ndatabase. According to experiments done by Meek on a set of \n80 folk-tune queries with lyrics, the P(“no edit”) = .81, where \nno edit means that there was no insertion or deletion. \nP(“insertion”) = 0.06, and P(“deletion”) = 0.13. Thus, we \ncreated a query set modeled after the imperfect queries, except \nit also had insertions and deletions. \nFor note insertion, a note n with duration d and pitch p, is \ndivided into notes n1 and n2. Both n1 and n2 have duration d/2 \nand pitch p. Thus, a note is added, but the rhythm is preserved.  \nFor a deletion, consider the note n2, which is preceded by note \nn1 with note onset time o1 and pitch p1, and followed by note \nn3 with note onset time o3. If note n2 is deleted, note n1 still \nhas pitch p1 and has duration o3-o1. So, the preceding note is \nlengthened so it “covers” the duration of the skipped note. It is \nworth noting that since our representation is dependant only \non the interOnsetInterval  it makes no difference whether we extend note n1 until the onset of n3 or simply delete note n3. \nFigure 7 gives an example of an insertion and a deletion.  \nInsertion on Second Note Deletion on Second Note Query on Piano Roll \n \nFigure 7: Example of Insertions and Deletions \nWhile the set of imperfect querie s is intended to show whether \nretrieval is possible on a large database if the matcher’s \ntopology reflects the properties of the queries, the dataset of \ninsertions and deletions is meant to reflect real-queries. This \ndataset allows us to examine our system’s performance on \nqueries that do not directly conform to the HMM topology \n(i.e., the HMM does not directly model note deletions and \ninsertions). Queries with inserti ons and deletions allows us to  \ndetermine the sensitivity to the types of errors we expect to \nsee in real queries. \n5 Experimental Results \nIn this section, we present the results of our experiments. For \nthe experiment sets, we used three different types of HMM \nmatchers. A matcher that only considers the observed pitch, a \nmatcher that only considers the observed duration, and a \nmatcher that considers both the observed pitch and duration.  \nEach matcher was tested against three sets of 5100 queries. \nThe query sets were perfect queries, imperfect queries, and \nimperfect queries with insertions and deletions. For each \nquery, the ranking of the target that generated it in our \ndatabase was recorded. Each query could rank between one \nand 50,000. \nDuring each set of experiments with a matcher, we recorded \nboth the Mean Reciprocal Rank  (MRR) and the median rank of \nall queries in a set of length l. The formula for the MRR of N \nqueries with rank ri is: \n11/ 1/N\ni\niNr\n=∑ . Since each set of \nqueries contained 100 queries of length l, with l ranging from \nfive to 55, each experiment with a query set returned 51 MRR \nvalues. \n5.1 Results of Pitch and Duration Matcher \nThe MRR and median results by length of the matcher that \nconsiders pitch and duration for all three query experiments \nare given in Figure 8. The median results are on a logarithmic \nscale.  MRR Pitch \nand Duration\n00.10.20.30.40.50.60.70.80.91\n5 1 01 52 02 53 03 54 04 55 05 5Imperfect Queries\nPerfect Queries\nInsertions and Deletions\n  \n Pitch and Duration\nMedian\n1\n10\n100\n1000\n10000\n1000005 1 01 52 02 53 03 54 04 55 05 5Imperfect Queries\nPerfect Queries\nInsertions and Deletions\n \nFigure 8: MRR value vs. query length for all three query-types \non the pitch and duration matcher \nIn the two experiments without insertions and deletions, the \nmatcher clearly benefits from longer queries. While these \neffects are less pronounced on the query set containing \ninsertions and deletions, the re sults still suggest this trend. \nLonger queries provide the matcher more information, \nincreasing the likelihood of distinguishing the intended target \nfrom other targets. \nLonger insertion and deletion queries do not necessarily \nimprove retrieval performance. Due to the topology of our \nmodel, an insertion or deletion causes the path traversed by \nthe Forward algorithm to be split into two paths through the \ntarget. These disjoint sub-paths each will have a lower \nprobability then a “full-length” path. The overall effect of the \ndisjoint paths is to degrade a target’s score, and thereby \ndecrease the information benefit of longer queries.  \n5.2 Results of Matcher Using Pitch Only \nThe MRR and well as the median results by length for the \nmatcher that only considers pitch are displayed in Figure 9. \nThe median results are displayed on a logarithmic scale. MRR Pitch\n00.10.20.30.40.50.60.70.80.91\n5 1 01 52 02 53 03 54 04 55 05 5Imperfect Queries\nPerfect Queries\nInsertions and Deletions\n    \n \nPitch Median\n1\n10\n100\n1000\n10000\n1000005 1 01 52 02 53 03 54 04 55 05 5Imperfect Queries\nPerfect Queries\nInsertions and Deletions\n \nFigure 9: MRR and mean value vs. query length for all three \nquery types on the pitch matcher \nAs was the case for pitch and duration experiments, the longer \nthe queries, the better the results. The slightly worse \nperformance on the imperfect and perfect query experiments \non the matcher that uses only pitch, as opposed to the matcher \nthat uses both pitch and duration shows the importance of the \nduration information.  \nThere is even more significant degradation on the performance \nof the insertion and deletion query set on the matcher that only \nconsiders pitch, compared to the matcher that considers both \npitch and duration. Since the matcher is only comparing the \ndeltaPitchs of the observation and the targets, as insertions and \ndeletions are introduced there is a greater likelihood that sub-\npath’s of the other targets in the database will match the query \nthen there is in the case of a matcher that compared both the \npitch and duration. \n5.3 Results of Matcher that Uses Duration Only \nThe MRR and median results by length for the matcher that \nonly considers duration are given in Figure 10. The median \nresults are displayed on a logarithmic scale.  MRR Duartion\n00.10.20.30.40.50.60.70.80.91\n5 1 01 52 02 53 03 54 04 55 05 5Imperfect Queries\nPerfect Queries\nInsertions and Deletions\n \nDuration Median\n1\n10\n100\n1000\n10000\n1000005 1 01 52 02 53 03 54 04 55 05 5Imperfect Queries\nPerfect Queries\nInsertions and Deletions\n \nFigure 10: MRR and mean value vs. query length for all three \nquery types on the duration matcher \nThe pitch-only matcher outperforms the duration-only matcher \nfor all three query sets because of our implementation. Pitch \ncontains more information abou t a song than duration. The \npitch data is used to compare the actual deltaPitch of an \nobservation and a target, while the duration data is used to \ncompare the ratio of IOIratios Th e ratio of IOIratios is merely \na measure of the similarity of durations, and it contains no real \nindication of rhythm: IOratio models tempo variation well. In \ncases where rhythm is a criti cal determining feature for a \nquery, IOratio is not a good measure. \nThere is significant performan ce degradation on the query set \nwith insertions and deletions for the duration-only matcher \ncompared to the pitch-only matcher. Since the duration \nmatcher considers less informati on, there is an even greater \nchance traversing the sub-paths of the different targets in the \ndatabase with a high probability. Since the duration-only \nmatcher traverses so many paths with a high probability when \nqueries contain insertions and deletions, query length has a \nminiscule effect on performance. \n6 Summary and Conclusions \nThere are several trends apparent from the experimental data. \nThe first trend is that longer queries have a positive effect on \nretrieval performance, which is as we expected. As the size of \na database grows, we expect that this effect will be even more \npronounced. Moreover, we expect these results to generalize \nacross retrieval algorithms; in other words, we believe that the \n“query-length” effect is not dependent on our particular HMM \ntopology, but will extend to string matchers, different HMM \ntopologies, and so forth. \nThe one caveat is that insertions and deletions mitigate the \n“query-length” effect. We believe that this mitigation is more \nan artifact of our particular retrieval method. We are looking at ways around this problem, such as using a different \nalgorithm for ranking (e.g., Viterbi) and modifying our model \ntopology.  \nOne possible topological improvement would be to extend our \nmodel to include insertion and deletion states. This would \nallow the observation to match against a longer path in the \ntarget, and the subsequent matching of other target’s sub-\nstrings would not have as dramatic an effect. The HMM \narchitecture is easily modeled to include insertions and \ndeletions (Durbin, Eddy et al. 1998). Thus, we would no \nlonger have problem of traversing disjoint sub-paths each time \nan insertion or deletion is encountered. A potential drawback \nof this approach is that search time will increase significantly \nwhen the topology admits more paths. \n The second trend is that all the experiments show a saturation \npoint, where retrieval performance reaches a maximum with \nfairly well-defined inflection point. We were expecting more \ngradual performance improvement without a saturation point. \nThe shape of these curves gives us guidance about the \naccuracy of our retrieval system . For example, on relatively \nshort queries (i.e., to the “left” of the inflection point), we \nexpect that ranking will not be  very accurate. We are looking \nat ways to model accurately and possibly report it as a \nconfidence factor to the user. \nThe experiments suggest changing our current duration model, \nparticularly to one that more accurately represents rhythm.  \nThe most encouraging result from these experiments is the \nperformance of the imperfect que ries. Our system successfully \nretrieved using the set of im perfect queries on the large \nsynthetic database. This shows both the importance of \nobtaining an accurate observation model and that retrieval is \npossible on large databases. In future work, we plan to have a \ntraining mode for our system to  create an accurate observation \nmodel specific to an individu al. Accurately trained and \nmodeled QBH HMMs systems can accurately perform \nretrieval on large music databases, and have the potential to be \nused for real world applications.  \nIn addition, we intend to experiment with large datasets of \nqueries collected from singers. We plan to gather aggregate \nstatistics of the same type as did for the research presented \nhere. We also plan to follow the same experimental plan \nevaluating our system. \nAcknowledgements \nWe gratefully acknowledge the support of the National \nScience Foundation under grant IIS-0085945. The opinions in \nthis paper are solely those of the authors and do not \nnecessarily reflect the opinio ns of the funding agencies. \nWe would also like to thank Bryan Pardo for many detailed \ncomments on this paper, and Colin Meek for many helpful \nsuggestions. \nReferences \nBainbridge, D., J.R. McPherson, S.J. Cunningham (2002). \nForming a corpus of voice queries for music \ninformation retrieval: a pilot study . ISMIR-2002, \nParis, France. Birmingham, W., Bryan Pardo, Colin Meek, Jonah Shifrin, \n(2002). \"The MusArt Music-Retrieval System: An \nOverview.\" D-Lib Magazine . \nBirmingham, W., Colin Meek, Kevin O’Malley, Bryan Pardo, \nJonah Shifrin (2003). \"Managing a Personal Music Library.\" Dr. Dobbs Journal   Vol 28(9), September, \n2003.. \nBirmingham, W. P., R. B. Dannenberg, et al. (2001). \nMUSART: Music Retrieval Via Aural Queries . \nInternational Symposium on Music Information \nRetrieval, Bloomington, Indiana. \nDownie, S. J., S.J. Cunningham (2002).  Toward a theory of \nmusic information retrieval queries: System design \nimplications.\n ISMIR-2002, Paris, France. \nDurbin, R., S. Eddy, et al. (1998).  Biological sequence \nanalysis , Cambridge University Press. \nMadden, T., R. B. Smith, et al. (2001). Preparation for \nInteractive Live Computer Performance in \nCollaboration with a Symphony Orchestra . \nProceedings of the 2001 International Computer \nMusic Conference, Havana, International Computer \nMusic Association. \nMcNab, R. J., L. A. Smith, et al. (1996).  Towards the digital \nmusic library: Tune retrieval from acoustic input.  \nProceedings of Digital Libraries '96, ACM. \nMeek, C. and W. P. Birmingham (2002). Johnny Can't Sing: A \nComprehensive Error Model for Sung Music Queries. ISMIR 2002, Paris, France. \nMongeau, M. and D. Sankoff (1990). Comparison of Musical \nSequences.  Melodic Similarity Concepts, Procedures, \nand Applications.  W. Hewlett and E. Selfridge-Field. \nCambridge, MIT Press. 11. \nPardo, B. and W. P. Birmngham(2003). Name that Tune: A \nPilot Studying in Finding a Melody from a Sung Query To Appear in JASIST.  \nPollastri, E. (2002).  Some considerations about processing \nsinging voice for music retrieval . ISMIR-2002, Paris, \nFrance. \nRabiner, L. and B.-H. Juang (1993). Fundamentals of Speech \nRecognition . Englewood Cliffs, NJ, Prentice Hall \nSignal. \nRaphael, C. (1999). \"Automatic Segmentation of Acoustic \nMusical Signals Using Hidden Markov Models.\" \nIEEE Transactions on PAMI 21(4): 360-370. \nShifrin, J., Bryan Pardo, Colin Meek, William Birmingham \n(2002).  HMM-Based Musical Query Retrieval . ACM \nInternational Joint Conference on Digital Libraries \n(JCDL), ACM."
    },
    {
        "title": "Improving polyphonic and poly-instrumental music to score alignment.",
        "author": [
            "Ferréol Soulez",
            "Xavier Rodet",
            "Diemo Schwarz"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1415542",
        "url": "https://doi.org/10.5281/zenodo.1415542",
        "ee": "https://zenodo.org/records/1415542/files/SoulezRS03.pdf",
        "abstract": "Music alignment links events in a score and points on the audio performance time axis. All the parts of a recording can be thus indexed according to score in- formation. The automatic alignment presented in this paper is based on a dynamic time warping method. Local distances are computed using the signal’s spec- tral features through an attack plus sustain note mod- eling. The method is applied to mixtures of har- monic sustained instruments, excluding percussion for the moment. Good alignment has been obtained for polyphony of up to five instruments. The method is robust for difficulties such as trills, vibratos and fast sequences. It provides an accurate indicator giv- ing position of score interpretation errors and extra or forgotten notes. Implementation optimizations allow aligning long sound files in a relatively short time. Evaluation results have been obtained on piano jazz recordings. 1",
        "zenodo_id": 1415542,
        "dblp_key": "conf/ismir/SoulezRS03",
        "keywords": [
            "Music alignment",
            "score indexing",
            "audio performance",
            "dynamic time warping",
            "spectral features",
            "harmonic sustained instruments",
            "polyphony",
            "score interpretation errors",
            "evaluation results",
            "piano jazz recordings"
        ],
        "content": "ImprovingPolyphonic andPoly-Instrumental MusictoScoreAlignment\nFerr´eolSoulez\nIRCAM –Centre Pompidou\n1,place Igor–Stra vinsk y,\n75004 Paris, France\nsoulez@ircam.frXavier Rodet\nIRCAM –Centre Pompidou\n1,place Igor–Stra vinsk y,\n75004 Paris, France\nrod@ircam.frDiemo Schwarz\nIRCAM –Centre Pompidou\n1,place Igor–Stra vinsk y,\n75004 Paris, France\nschwarz@ircam.fr\nAbstract\nMusic alignment links eventsinascore andpoints on\ntheaudio performance time axis. Alltheparts ofa\nrecording canbethus indexedaccording toscore in-\nformation. Theautomatic alignment presented inthis\npaper isbased onadynamic time warping method.\nLocal distances arecomputed using thesignal’ sspec-\ntralfeatures through anattack plus sustain note mod-\neling. The method isapplied tomixtures ofhar-\nmonic sustained instruments, excluding percussion\nforthemoment. Good alignment hasbeen obtained\nforpolyphon yofuptoﬁveinstruments. Themethod\nisrobustfordifﬁculties such astrills, vibratos and\nfastsequences. Itprovides anaccurate indicator giv-\ningposition ofscore interpretation errors andextraor\nforgotten notes. Implementation optimizations allow\naligning long sound ﬁles inarelati velyshort time.\nEvaluation results havebeen obtained onpiano jazz\nrecordings.\n1Introduction\nScore alignment means linking score information toanaudio\nperformance ofthisscore. Thestudied signal isadigital record-\ningofmusicians interpreting thescore. Alignment associates\nscore information topoints ontheaudio performance time axis.\nItisequivalent toaperformance segmentation according tothe\nscore.\nTodothis, wepropose adynamic time warping (DTW) based\nmethodology .Local distances arecomputed using spectral fea-\ntures ofthesignal, andanattack plus release note modeling\n(Orio &Schw arz,2001). Veryefﬁcient onmonophonic signals,\nthismethod cannowcope with anypoly-instrumental perfor -\nmance made upoflessthan ﬁveinstruments without percus-\nsion.\nAfter abrief overvie wofpossible applications insection 1.1,\nthenote model andDTW implementation arediscussed insec-\nPermission tomakedigital orhard copies ofallorpartofthiswork\nforpersonal orclassroom useisgranted without feeprovided that\ncopies arenotmade ordistrib uted forproﬁt orcommercial advan-\ntage andthatcopies bear thisnotice andthefullcitation ontheﬁrst\npage. c\n\u00002003 Johns Hopkins University .tion2.Finally ,results obtained with thismethod arepresented\ninsection 3.\n1.1 Applications, Goal andRequir ements\nAutomatic score alignment hasseveralapplications. Each goal\nrequires speciﬁc information from thisautomatic process. The\nmost important applications are:\n1.Inapplications thatdeal with symbolic notation, alignment\ncanlink thisnotation andaperformance, allowing musi-\ncologists toworkonasymbolic notation while listening to\narealperformance (Vinet, Herrera, &Pachet, 2002).\n2.Indexing ofcontinuous media through segmentation for\ncontent-based retrie val.Thetotal alignment cost between\npairs ofdocuments canbeconsidered asadistance mea-\nsure (asinearly works onspeech recognition). This allows\nﬁnding ofthebest matching documents from adatabase.\nThese ﬁrsttwoapplications only need agood global preci-\nsion androbustness.\n3.Musicological comparison ofdifferent performances,\nstudying expressi veparameters andinterpretation charac-\nteristics ofaspeciﬁc musician.\n4.Construction ofanewscore describing exactly aselected\nperformance byadding information such asdynamics, mix\ninformation, orlyrics. This information canbeadded to\npitch andlength labeling when building adatabase. Nev-\nertheless re-transcription oftempo necessitates high time\nprecision.\n5.Performance segmentation into note samples automati-\ncally labeled andindexedinorder tobuildaunitdatabase,\nforexample fordata-dri venconcatenati vesynthesis based\nonunitselection (Schw arz,2000, 2003a, 2003b) ormodel\ntraining (Orio &D´echelle, 2001). This segmentation re-\nquires aprecise detection ofthestart andendofanote.\nHowever,notes that areknowntobemisaligned canbe\ndisre garded (seesection 3.3).\nAlignment isclose torealtime synchronization between aper-\nformer and acomputer ,knownasscore follo wing (Orio &\nD´echelle, 2001; Orio, Lemouton, Schw arz,&Schnell, 2003).\nHowever,inalignment, thewhole signal canbeused andmore\naccurate resolution canbeobtained ifrequired bytheapplica-\ntion. Nevertheless, alignment canbeagood bootstrap proce-\ndure fortraining score follo wers which usestatistical models.Fornow,thegoal ofthepresent workistoobtain acorrect\nglobal alignment, i.e.aprecise pairing between notes present\ninthescore andthose present intherecording. Onthisbasis,\nveryprecise estimation ofthebeginning andendofnotes will\nbeadded inthefuture, asdetailed insection 4.\n1.2 Previous Work\nAutomatic alignment ofsequences isaverypopular research\ntopic, especially ingenetics, molecular biology and speech\nrecognition. Agood overvie wofthistopic is(Rabiner &Juang,\n1993). There aretwomain strate gies: theoldest uses dynamic\nprogramming (DTW) andtheother uses hidden Mark ovmod-\nels(HMMs). Forpairwise alignment ofsequences, HMMs\nandDTW arequite interchangeable techniques (Durbin etal.,\n1998).\nConcerning automatic alignment speciﬁcally ,themain works\narescore follo wing techniques tuned forofflineuse(Raphael,\n1999), theprevious work of(Orio &Schw arz, 2001), or\n(Meron, 1999). Adifferent approach ofmusic alignment isvery\nbrieﬂy described in(Turetsk y,2003). Allofthese techniques\nconsider mainly monophonic recordings.\nFornote recognition, there aremanypitch detection techniques\nusing signal spectrum orauto-correlation, forinstance. These\ntechniques areoften efﬁcient inmonophonic cases butnone of\nthese usescore information andaretherefore sub-optimal inour\nsituation.\n1.3 Principle\nScore alignment isperformed infour stages:\u0000First, construction ofthescore representation byparsing\noftheMIDI ﬁleintoscore events.\u0000Second, extraction ofaudio features from signal.\u0000Third, calculation oflocal distances between score and\nperformance.\u0000Fourth, computation oftheoptimal alignment path which\nminimizes theglobal distance.\nThis laststage iscarried outusing DTW .Ourchoice forthisal-\ngorithm isduetothepossibility ofoptimizing memory require-\nments. Also, unlik eHMMs, DTW does nothavetobetrained,\nsothatahand made training database isnotnecessary .\n2TheMethod\nForeach sequence, thescore andtheperformance aredivided\ninto frames described byfeatures. Score information isex-\ntracted from standard MIDI ﬁles, theformat ofmost ofthe\navailable score databases. Howeverthisformat isveryhetero-\ngeneous anddoes notcontain allclassical score symbols. The\nonly available features from these MIDI ﬁles arethefundamen-\ntalfrequencies present atanytime, andnote attack andendpo-\nsitions. Asimplicitly introduced in(Orio &Schw arz,2001), the\nresult ofthescore parsing isatime-ordered sequence ofscore\nevents ateverychange ofpolyphon y,i.e.ateach note start and\nend, asexempliﬁed inﬁgure 1.\nThe features oftheperformance areextracted through sig-\nnalanalysis techniques using short time Fourier transforma-\ntion (usually with a4096 points hamming windo w,93msat100 0200 300\n6366\nmonophonic state monophonic state\npolyphonic state\nFigure 1:Parsing ofaMIDI score into score events andthe\nstates between them.\n44.1 kHz). The temporal resolution needed forthealignment\ndetermines thehopsizeofframes intheperformance. Thescore\nisthen divided intoapproximately thesame number offrames\nastheperformance. Inconsequence, theglobal alignment path\nshould follo wapproximately thediagonal ofthelocal distance\nmatrix (seesection 2.2).\nFinally ,DTW ﬁnds thebestalignment based onlocal distances\nusing aViterbi path ﬁnding algorithm which minimizes the\nglobal distance between thesequences.\n2.1 Model: Local Distance Computation\nThelocal distance iscalculated foreach pairmade upofaframe\u0001intheperformance andaframe \u0002inthescore. This distance,\nrepresenting thesimilarity oftheperformance frame\n\u0001tothe\nscore frame \u0002,iscalculated using spectral information. Thelo-\ncaldistances arestored inthelocal distance matrix\n\u0003\u0005\u0004\u0001\u0007\u0006\b\u0001\n\t\u0002\f\u000b.\nTheonly signiﬁcant features contained inthescore arethepitch,\nthenote limits andtheinstrument. Since having agood instru-\nment model isdifﬁcult, only pitch andtransients were chosen\nasfeatures fortheperformance. This iswhy thenote model is\ndeﬁned with attack frames using pitch andonset information,\nandsustain frames using only pitch.\n2.1.1 Sustain Model\nThesustain model uses only pitch. Aspitch tracking algorithms\nareerror prone, especially forpolyphonic signals, amethod\ncalled PeakStructur eMatc h(Orio &Schw arz,2001) isused.\nWiththismethod, thelocal PeakStructur eDistance (PSD)is\ntheratio ofthesignal energyﬁltered byharmonic band pass ﬁl-\nterscorresponding toeach expected pitch present inthescore\nframe, overtotal energy.\nThis technique isveryefﬁcient inmonophonic cases. However\ninthepoly-instrumental situation, thedifferent instruments do\nnothavethesame loudness, anditisverydifﬁcult tolocalize\nlowandshort notes under continuous loud notes. Coding en-\nergies onalogarithmic scale reduces levelratio between the\ndifferent instruments andthus impro vesresults.\nHowever,thismodel hastwomajor drawbacks. First, inpoly-\nphonic cases, ﬁlter banks corresponding toachord tend tocover\nthemajor partofthesignal spectrum, increasing thelikeness of\nthischord with anypart oftheperformance. Asresult, ﬁlters\nneed tobeasprecise aspossible.\nSecondly ,such amodel with narro wﬁlters isadapted toﬁxed\npitch instruments, such asthepiano, inwhich small frequenc y\nvariations, error ,orvibrato, areimpossible. Forstring instru-ments andthevoice, such variations canbeaslargeasasemi\ntone around thenominal frequenc yofthenote. Asimple solu-\ntionistodeﬁne vibrato asachord oftheupper andthelowerfre-\nquenc y,butvibrato isnotincluded inmost MIDI based scores.\nAnother solution istogiveadegree offreedom toeach ﬁlter\naround itsnominal frequenc y.Foreach performance frame, the\nﬁlter istuned within acertain range toyield thehighest energy.\nTheenergyisweighted byaGaussian windo wcentered onthe\nnominal frequenc yoftheﬁlter,lowering thepreference fora\nhigh energypeak farawayandfavoring alowbutclose one.\nAmazingly ,wehaveobserv edthatshifting ﬁlters independently\ngivesbetter results than shifting thewhole harmonic comb .\nMoreo ver,thisﬁlter tolerance impro vesdistance calculation for\nslightly inharmonic instruments. After anumber oftests, work-\ningwith theﬁrst \u0000\u0002\u0001=6harmonics ﬁlters givesacceptable re-\nsults. Equi valent results were obtained for \u0000\u0003\u0001=7or8.The\nbest andmost homogeneous results areobtained with aﬁlter\nwidth of\u0004\u0004\u0006\u0005thsemitone (10cents) andatolerance ofabout\u0007\n\bth\nsemitones (75cents around thenominal frequenc y.\n2.1.2 Attac kModel\nTestsusing only thesustain model showsome imprecision of\nthealignment marks, which areoften late. Worse, inverypoly-\nphonic cases (more than three simultaneous notes), some notes\narenotdetected atall.\nThere aretworeasons forthemark ers’imprecision. First, the\npartials’ reverberation oftheprevious notes isstillpresent dur-\ningthebeginning ofthenextone. Second, during attacks, en-\nergyisoften spread alloverthespectrum andtheenergymaxi-\nmum intheﬁlters isreached severalframes after thetrueattack.\nWiththesustain model alone, alignment marks aresetatthein-\nstant when theenergyofthecurrent note rises abovetheenergy\nofthelastnote, severalhundredths ofasecond after thetrue\nonset.\nMoreo ver,inthepolyphonic case, during chords, severalnotes\noften havecommon partials. Ifonly onenote ofthischord\nchanges, toofewpartials may varytocause enough difference\ninthespectral structure tobedetectable bythePSD.\nAmore accurate indication ofanote beginning isgivenbythe\nvariation intheﬁlters. Thus, special score frames using energy\nvariations \t\u000b\n\fintheharmonic ﬁlter band \rofthenote \u000einstead\nofPSD were created ateveryonset. Inthese frames, theattack\ndistance \u000f\u0011\u0010 isgivenbythesum oftheenergyvariations (in\ndB)ineverytuned ﬁlter band \r.Inthecase ofsimultaneous\nonsets, thedistance\u000f\u0011\u0010 iscomputed foreverybeginning note\nandaveraged out:\u000f\u0012\u0010\u0014\u0013 mean\n\u0015\u0017\u0016\u0019\u0018\u001b\u001a\u001d\u001c\u001f\u001e! \u001b\u0015#\"$\u0015&%(')\f+*\u0004-,\n\t\n\n\f,\n\u0018/.\u001f0213131\n(1)\nwith \t\u000b\n\ftheenergydifference indBwith theprecedent local\nextremum intheﬁlter band \rofnote \u000e,\n.(0\nathreshold, and\n\"\na\nscaling factor .\nSmall note changes during chords seem tobegrasped byhu-\nman perception mostly duetotheir onsets. Therefore, thelocal\ndistance \u000f\u0011\u0010isampliﬁed bythescaling factor\n\"\ntofavoronset\ndetection overPSD.After carrying outsome tests,\n.40\nwassetto\n6.5dBand\n\"\nto50.\nTheexample inﬁgure 2ischaracteristic oftheprincipal prob-lems ofthesustain detection: Fortheﬁrstsecond ofthisMozart\nstring andoboe quartet, violins andoboe play aloud contin-\nuous note while thecello isplaying small notes intheir sub-\nharmonics. Thecello hasmanycommon partials with theother\nnotes andglobal energyvariations areduetoviolin vibrato and\nnotcello onset. AsshownbythePSD diagram inﬁgure 2(b),\ndetection byuseofthesustain model (PSD)isnotpossible. On\nthecontrary ,thethree notes E2,A2andC3caneasily belocal-\nized ontheenergyvariation diagram asindicated bythevertical\ndash-dotted lines.\n(a)Spectrogram andMIDI roll\n(b)576\n'8+9;:=<?>\n8<andPSD fornote E2A2C3\nFigure 2:First second ofMozart quartet\n2.1.3 Silence Model\nShort silences duetoshort rests inthescore andnon-le gato\nplaying aredifﬁcult tomodel, since reverberation hastobe\ntakenintoanaccount. Weonly model rests longer than 100ms.Shorter rests aremergedwith theprevious note. Thelocal dis-\ntance\u0000\u0003\u0010forlong rests iscomputed using anenergythresh-\nold\n.\u0001:\u0000\u0003\u0010\n\u0006\b\u0001\n\t\u0002\f\u000b \u0013\n\u0002\u0004\u0003\n\u0018\u001b.\u0001if\n\u0003\u0006\u0005\n.\u0001\n\t\u0007if\n\u0003\u0006\b\n.\u0001\n\t(2)\nwhere\n\u0003\nistheenergyofthesignal intheperformance frame\n\u0001.\n2.2 Dynamic TimeWarping\nDTW isaconsolidated technique forthealignment ofse-\nquences, thereader may refer to(Rabiner &Juang, 1993) for\natutorial. Using dynamic programming, DTW ﬁnds thebest\nalignment between twosequences according toanumber of\nconstraints. The alignment isgivenintheform ofapath ina\nlocal distance matrix where each value\n\u0003 \u0004\u0001\u0007\u0006\b\u0001\n\t\u0002\f\u000bisthelike-\nness between thescore frame\n\u0001andtheperformance frame\u0002.Ifapath goes through\u000b\n\u0001 \t\u0002\r\f,theframe\n\u0001oftheperfor -\nmance isaligned with frame\u0002ofthescore. Thefollo wing con-\nstraints havebeen applied: Theendpoints aresettobe\u000b\n\u0016\t\n\u0016\fand\u000b \u000e\n\t\u0010\u000f\f,where\u000eand\n\u000farethenumber offrames oftheper-\nformance andofthescore, respecti vely.Thepath ismonotonic\ninboth dimensions. Thescore isstretched toapproximately the\nsame duration astheperformance ( \u000e\u0012\u0011\n\u000f).Theoptimal path\nshould then beclose tothediagonal, sothatfavoring thediago-\nnalwould preventdeviating paths.\nThree different local neighborhoods oftheDTW havebeen\ntested. Severalimpro vements havebeen added totheclassical\nDTW algorithm inorder tolowerprocessing time ormemory\nrequirements andthus allowlong performances tobeanalyzed.\nThemost important ofthese impro vements arethepath pruning\nandtheshort cutpath implementation.\n2.2.1 Local Constr aints\nTheDTW algorithm calculates ﬁrsttheaugmented distance ma-\ntrix\u0013\n\u0004\u0001\u0007\u0006\b\u0001\n\t\u0002\f\u000bwhich isthecostofthebestpath uptothepoint\u000b\n\u0001 \t\u0002\r\f.Tocompute this\u0013\n\u0004\u0001matrix, different types oflocal\nconstraints havebeen implemented inwhich theweights along\nthelocal path constraint branches canbetuned inorder tofavor\nonedirection. These weights [ \u0014\u0016\u0015\u0017\u0014\u0016\u0018\u0019\u0014\u001b\u001a ]areexplained inthe\nﬁgure 3.Thedifferent type names, I,IIIandVfollo wthenota-\ntionin(Rabiner &Juang, 1993) andarecalculated asfollo ws,\nwith\n\u0003\u0005\u0004\u0001\u0007\u0006\b\u0001\n\t\u0002\f\u000babbre viated to \u001c:\nTypeI:\u0013\n\u0004\u0001\n\u0006 \u0001 \t\u0002\f\u000b \u0013\u001e\u001d \u001f\n\u001e\"! #$#%\n\u0013\n\u0004\u0001\n\u0006 \u0001\n\u0018 \u0016\t\u0002\n\u0018 \u0016\u000b'&(\u0014\n\u001a\u001c\u0013\n\u0004\u0001\n\u0006 \u0001\n\u0018 \u0016\t\u0002\f\u000b &(\u0014\n\u0018\u001c\u0013\n\u0004\u0001\n\u0006 \u0001 \t\u0002\n\u0018$\u0016\u000b &(\u0014\n\u0015\u001c\n)#*#+(3a)\nType III:\u0013\n\u0004\u0001\n\u0006 \u0001 \t\u0002\f\u000b \u0013,\u001d \u001f\n\u001e-! #$#%\n\u0013\n\u0004\u0001\n\u0006 \u0001\n\u0018$\u0016\t\u0002\n\u0018$\u0016\u000b'&.\u0014\u001b\u001a\u0004\u001c\u0013\n\u0004\u0001\n\u0006 \u0001\n\u0018(/\t\u0002\n\u0018$\u0016\u000b'&.\u00140\u00151\u001c\u0013\n\u0004\u0001\n\u0006 \u0001\n\u0018$\u0016\t\u0002\n\u0018(/\u000b'&.\u0014\u001b\u0018\u0004\u001c\n)#\n*#+(3b)\n22\n22\n(m-1,n) (m-1,n-1)(m,n-1) (m,n)34\n4\n4\n465 78:9\n;:< =>\nTypeI(m-2,n-1)(m-1,n-1)(m,n)\n(m-1,n-2)?\n?\n?\n?\n?\n?\n?\n?A@4\n4\n4\n465B\nB\nB\nB\nB\nB\nB\nB C2\n2\n22\n2\n22\n2\n28:9\nDFE GIH\nTypeIII2\n2\n2\n22\n2\n2\n22\n2\n2\n22\n2\n2\n2\n(m-3,n-1)(m-2,n-1)(m-1,n-1) (m-1,n-2) (m-1,n-3)(m,n)34\n4\n4\n465 7\n34\n4\n4\n46574\n4\n4\n465\n4\n4\n4\n4654\n4\n4\n465\n8\u00049\n;:<=>\nTypeV\nFigure 3:Neighborhood onpoint (m,n) intype I,IIIandV\nType V:\u0013\n\u0004\u0001\n\u0006 \u0001 \t\u0002\f\u000b \u0013\u001e\u001d \u001f\n\u001e\n!######################$######################\n%\n\u0013\n\u0004\u0001\u0007\u0006\b\u0001\n\u0018 \u0016\t\u0002\n\u0018 \u0016\u000bF&(\u0014\n\u001a\u001c\u0013\n\u0004\u0001\u0007\u0006\b\u0001\n\u0018J/\t\u0002\n\u0018 \u0016\u000b'&(\u0014\u001b\u0015K\u001c&L\u0014\u0016\u001a\n\u0003\u0005\u0004\u0001\u0007\u0006\b\u0001\n\u0018 \u0016\t\u0002\f\u000b\u0013\n\u0004\u0001\n\u0006 \u0001\n\u0018 \u0016\t\u0002\n\u0018J/\u000b'&(\u0014\n\u0018\u001c&L\u0014\n\u001a\n\u0003\u0005\u0004\u0001\u0007\u0006\b\u0001\n\t\u0002\n\u0018$\u0016\u000b\u0013\n\u0004\u0001\u0007\u0006\b\u0001\n\u0018\u0019M\t\u0002\n\u0018 \u0016\u000b'&(\u0014\u001b\u0015K\u001c&L\u0014\n\u001a\n\u0003\u0005\u0004\u0001\u0007\u0006\b\u0001\n\u0018J/\t\u0002\f\u000b&L\u0014\n\u0015\n\u0003\u0005\u0004\u0001\u0007\u0006\b\u0001\n\u0018 \u0016\t\u0002\f\u000b\u0013\n\u0004\u0001\n\u0006 \u0001\n\u0018 \u0016\t\u0002\n\u0018\u0019M\u000b'&(\u0014\n\u0018\u001c&L\u0014\u0016\u0018\n\u0003\u0005\u0004\u0001\u0007\u0006\b\u0001\n\t\u0002\n\u0018$\u0016\u000b&L\u0014\u0016\u001a\n\u0003\u0005\u0004\u0001\u0007\u0006\b\u0001\n\t\u0002\n\u0018(/\u000b\n)######################*######################+(3c)\nTheconstraint type Iistheonly oneallowing horizontal orver-\ntical paths andthus admitting extraorforgotten notes. Since it\nallowsforvertical orhorizontal paths, thedrawback ofthiscon-\nstraint type isasfollo ws:Thepath canbestuck inaframe ofa\ngivenaxis with erroneous small local distance with successi ve\nframes oftheother axis. Itleads tobadresults inthepolyphonic\ncase bydetecting toomanyextraorforgotten notes.\nThe types IIIandVconstrain theslope toberespecti velybe-\ntween 2and\u0004\nNor3and\u0004\u0007.Since itisveryraretohear aper-\nformance with passages played more than three times faster orslowerthan thescore, itgivesgood alignment butwill accept\nneither vertical norhorizontal paths andthus does notdirectly\nhandle forgotten orextra notes. These constraints IIIandV\ngiveapproximately thesame result, thetype Vtakesmore re-\nsources andmore time butgivesmore freedom tothepath al-\nlowing greater slope. Using TypeVispreferable buttype III\ncanstillbeused forlong pieces.\nThestandard values forthelocal path constraints [\u0014L\u0015\u0017\u0014\u0016\u0018\u0019\u0014\u0016\u001a ]\u0013[112]fortype IandVor[332]fortype III,donot\nfavoranydirection andareused inourmethod. Note thatour\nexperiments showed thatlowering \u0014 \u001afavorsthediagonal and\npreventsextreme slopes.\n2.2.2 PathPruning\nAstheframe size isusually around 5.8ms, three minute\nlong performances contain about 36000 frames, sothatabout\n\u0016\t\nM\u0001\n\u0016\u0007\n\u0002\nelements need tobecomputed inthelocal distance\nmatrix andasmanyfortheaugmented distance matrix. The\nmemory required tostore them is2.5GB. Toreduce the\ncomputation time andtheresources needed, ateveryiteration\u0001,only thebest paths arekept,bypruning thepaths with an\naugmented distance\u0013\n\u0004\u0001\n\u0006 \u0001 \t\u0002\f\u000boverathreshold\n.\u0003.This\nthreshold isdynamically setusing theminimum oftheprevious\u0013\n\u0004\u0001row.After various experiments thisthreshold wassetto:.\u0003\n\u0006\b\u0001\u000b \u0013\n\u0016\t\n\u0016\u001d \u001f\n\u001e\u0006\u0013\n\u0004\u0001\n\u0006 \u0001\n\u0018$\u0016\u000b \u000b (4)\nHowever,thepaths between thecorridor ofselected paths and\nthediagonal arenotpruned toleavemore possible paths. Usu-\nallythecorridor width isabout 400frames.\n2.2.3 Shortcut Path\nMost applications only need toknowthenote start andend\npoints, andnotthealignment within thenote. Therefore, only a\nshortcut path, linking allthescore events inthepath, isstored\nasexplained in(Orio &Schw arz,2001). Asthelocal constraint\ntypes IIIorVneed computation with adepth of3or4frames re-\nspecti vely,only 2or3frames perperformance frame arestored\nforeach score eventreducing memory requirements byabout\n95%.\n3Results\nAlltests were performed with adefaultframe hopsizeof5.8ms\n(usually 256points) which isagood compromise between pre-\ncision andnumber offrames tocompute. This hop-size canbe\nlowerforabetter resolution when considering small recordings\norhigher forquick previewofthealignment.\nDuetotheabsence ofpreviously aligned databases andthedifﬁ-\nculty ofbuilding onebyhuman alignment, quantitati vestatistics\nwere done onasmall database. However,manyqualitati vetests\nwere performed bylistening toperformances andtheir reconsti-\ntuted MIDI ﬁles, which permitted theevaluation ofglobal align-\nment. These tests were performed with various types ofmusic\n(classical, contemporary ,songs without percussion, forinstance\nBach, Chopin, Boulez, Brassens, etc.) achie ving verygood re-\nsults. Evenwith difﬁcult signals such asvoices, veryfastviolin\norpiano sections, trills, vibrato, poly-instrumental pieces, the\nalgorithm showed good results andgood robustness with only\nfewimprecisions ononset formulti-instrumental pieces.3.1 Limits\nNotes shorter than 4frames (23ms)areverydifﬁcult todetect\nandoften lead toerrors forneighbor notes. Therefore, allthe\nevents thataretooshort, aremergedinachord with thenext\nevent. This technique makesitpossible tohandle unquantised\nchords from MIDI ﬁles recorded onakeyboard. Alignment\nisefﬁcient forpieces with lessthan ﬁveharmonic instruments\nsuch assinging voice, violin, piano, etc. Asthememory re-\nquirement isstilltoohigh, only pieces shorter than sixminutes\nandwith about four thousand orlessscore events arecurrently\ntreatable (alittle lesswith local constraint V),butthisisenough\ntoalign most pieces. Thelongest successful testwasperformed\nonaﬁveminute andtwelv esecond long jazz performance of\n4200 score eventswith time resolution of5.8ms(53926 frames)\ntaking about 400MB ofRAM and146minutes onaPentium\nIV2.8GHz running C++ andMatlabR\n\u0004\nroutines.\n3.2 Automatic Evaluation\nAsperformers rarely play with sudden variations intempo, ex-\ntreme slopes ofalignment path, with largevariation, usually in-\ndicate score–performance mismatching. Thus, thepath slope\ncanbeagood error indicator .Iftheslope is\n\u0004\u0007forseveralnotes,\nitisverylikelythatsome notes aremissing intheperformance.\nOntheother hand iftheslope is3,there arecertainly extranotes\ninit.\nThis indicator wasable toﬁndwith precision theposition ofan\nunkno wnextrameasure inascore ofBach’ sprelude, ascanbe\nseen inﬁgure 4.\nFigure 4:Piano rollrepresentation ofaligned MIDI, andpath\nslope inlogunits intheBach’ sprelude between 45secand60\nsec.\n3.3 Robustness\nTests with audio recording that donotexactly coincide with\ntheMIDI ﬁles showed verystrong robustness andaverygood\nglobal alignment. Forinstance, alignment oftheﬁrst prelude\nforpiano ofBach (80secand629score events) with anextra\nmeasure atthe51stsecond wascorrectly aligned until the50th\nandafter the55th,andanother testwith aBach sonata forviolin\nshowed averygood global alignment eventhough apassage of\n52notes wasmissing inthescore!\nVibratos andtrills canbealigned veryefﬁciently aswell, as\nshownintheverylargevibrato section ofAnth `emes 2by\nBoulez.3.4 ErrorRate\nQuantitati vetests were performed onseveraljazz piano impro-\nvisations played by3different pianists, where sound andMIDI\nwere both recorded. These areveryfast(anattack every70ms\nontheaverage) andlong pieces (about four minutes) with many\ntrills andawide dynamical range.\nAsreverberation prevents precise note enddetermination, we\nfocused onnote onset detection. Only agood global alignment\nwaslookedfor.Acorrect pairing between score andperfor -\nmance means thatthedetected note onset iscloser toitscorre-\nsponding onset intheperformance than anyother .Withthiscri-\nterion, tests showeda9.7% error rateofonset detection overthe\n9024 considered onsets, about 65% ofthese errors were made\nonnotes shorter than 80ms,corresponding toarateof12notes\npersecond. These results need severalcomments:\n1.Due totheMIDI recording system used, theMIDI ﬁle,\nthough recorded from thekeyboard simultaneously with\ntheaudio seems toberelati velyimprecise when compared\ntotheaudio.\n2.During theMIDI parsing, everynote shorter than 4frames\n(usually 23ms)ismergedwith thepreceding note, increas-\ningerror rateofsmall notes (numerous inourtests).\n3.The hopsize gives5.8msmaximum resolution between\neach possible detection.\n4.Finally ,asaudio features areextracted from ashort time\nfastFourier transform computed ona93ms(4096 points)\nwindo w,thecenter ofthiswindo wistakentodetermine\nframe position intherecording. Abetter solution would\nbetotakethecenter ofgravityofenergyinthiswindo w,\nbutthisfunction isnotyetimplemented.\nAsaconsequence, tests showed a23.8 msstandard deviation\nbetween thescore onset andthedetected one. This result can\neasily beimpro vedinthenear future, byasecond stage ofpre-\ncisetime alignment within thevicinity ofthealignment mark.\nTheprecise alignment wasnotthegoal pursued inthispresent\nwork.\n4Conclusion andFutureWork\nOurmethod, which isbeing used atIRCAM forresearch inmu-\nsicology ,canefﬁciently perform alignment ondifﬁcult signals\nsuch asmulti-instrumental music (oflessthan ﬁveinstruments),\ntrills, vibrato, accentuated orfastsequences, with anacceptable\nerror rate.\nWearecurrently working onanonset detector which re-\nanalyzes thesignal around thealignment mark, thus impro ving\ntheresolution forapplications which need better precision. Fur-\nthermore, apercussion detection process isbeing workedonto\nbeincluded soon inthealignment process.\nOne ofthefundamental problems remaining istheinadequac y\nofthescore representation. MIDI ﬁles contain verylittle infor -\nmation compared torealmusical scores andsotoofewfeatures\ncanbeused inthealignment.\nAckno wledgments\nManythanks toE.Vincent who wasaprecious adviser during\nthepreparation ofthisarticle.References\nDurbin, R.,etal..(1998). Biolo gical sequence analysis: Proba-\nbilistic models ofproteins andnucleic acids. Cambridge\nUniversity Press.\nMeron, Y.(1999). High quality singing synthesis using the\nselection-based synthesis scheme .Unpublished doctoral\ndissertation, University ofTokyo.\nOrio, N.,&D´echelle, F.(2001). Score Following Using Spec-\ntralAnalysis andHidden Mark ovModels. InProceed-\nings oftheInternational Computer Music Confer ence\n(ICMC). Havana, Cuba.\nOrio, N.,Lemouton, S.,Schw arz,D.,&Schnell, N.(2003).\nScore Following: State oftheArtand NewDevelop-\nments. InProceedings oftheinternational confer ence on\nnewinterfaces formusical expression (nime). Montreal,\nCanada.\nOrio, N.,&Schw arz,D.(2001). Alignment ofMonophonic and\nPolyphonic Music toaScore. InProceedings oftheInter -\nnational Computer Music Confer ence (ICMC). Havana,\nCuba.\nRabiner ,L.R.,&Juang, B.-H. (1993). Fundamentals ofspeec h\nrecognition. Engle woodCliffs,NJ:Prentice Hall.\nRaphael, C.(1999). Automatic Segmentation ofAcoustic Mu-\nsical Signals Using Hidden Mark ovModels. IEEE Trans-\nactions onPattern Analysis and Machine Intellig ence,\n21(4),360–370.\nSchw arz,D.(2000). ASystem forData-Dri venConcatenati ve\nSound Synthesis. InDigital Audio Effects (DAFx) (pp.\n97–102). Verona, Italy.\nSchw arz,D.(2003a). NewDevelopments inData-Dri venCon-\ncatenati veSound Synthesis. InProceedings oftheIn-\nternational Computer Music Confer ence (ICMC). Singa-\npore.\nSchw arz,D.(2003b). The CATERPILLARSystem forData-\nDrivenConcatenati veSound Synthesis. InDigital Audio\nEffects (DAFx). London, UK.\nShale v-Shw artz, S.,Dubno v,S.,Friedman, N.,&Singer ,Y.\n(2002). Robusttemporal andspectral modeling forquery\nbymelody .InProceedings ofthe25th Annual Interna-\ntional ACMSIGIR Confer ence onResear chandDevel-\nopment inInformation Retrie val(pp. 331–338). ACM\nPress.\nTuretsk y,R. (2003). MIDIAlign: Youdid what with\nMIDI? Retrie vedAugust 8,2003, fromhttp://www.\nee.columbia.edu/˜rob/midialign .\nVinet, H.,Herrera, P.,&Pachet, F.(2002). The Cuidado\nProject: NewApplications Based onAudio andMusic\nContent Description. InProceedings oftheInternational\nComputer Music Confer ence (ICMC). Gothenb urg,Swe-\nden."
    },
    {
        "title": "Automatic rhythm transcription from multiphonic MIDI signals.",
        "author": [
            "Haruto Takeda",
            "Takuya Nishimoto",
            "Shigeki Sagayama"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1415222",
        "url": "https://doi.org/10.5281/zenodo.1415222",
        "ee": "https://zenodo.org/records/1415222/files/TakedaNS03.pdf",
        "abstract": "For automatically transcribing human-performed polyphonic music recorded in the MIDI format, rhythm and tempo are decomposed through proba- bilistic modeling using Viterbi search in HMM for recognizing the rhythm and EM Algorithm for esti- mating the tempo. Experimental evaluation are also presented. 1",
        "zenodo_id": 1415222,
        "dblp_key": "conf/ismir/TakedaNS03",
        "keywords": [
            "polyphonic",
            "MIDI",
            "probabilistic",
            "Viterbi",
            "HMM",
            "EM Algorithm",
            "experimental evaluation",
            "rhythm",
            "tempo",
            "transcribing"
        ],
        "content": "AutomaticRhythmTranscriptionfromMultiphonic MIDI Signals\nHaruto Takeda TakuyaNishimoto Shigeki Sagayama\nGraduateSchool of Information Science and Technology,The Universityof Tokyo\nHongo, Bunkyo-ku,Tokyo113-8656 Japan /ftakeda,nishi,sagayama g@hil.t.u-tokyo.ac.jp\nAbstract\nFor automatically transcribing human-performed\npolyphonic music recorded in the MIDI format,\nrhythm and tempo are decomposed through proba-\nbilistic modeling using Viterbi search in HMM for\nrecognizing the rhythm and EM Algorithm for esti-\nmating the tempo. Experimental evaluation are also\npresented.\n1 Introduction\nWe are investigating automatic transcription of MIDI (Musical\nInstrument Digital Interface) signals. As the MIDI format al-\nreadyincludesthepitchinformation,theproblemhereistorec-\nognize the note values, i.e., intended nominal lengths of notes\nasshownin Fig. 1, which we refer to “rhythmrecognition.”\nConventionally, it has been done by “quantization” of IOIs\n(Inter-Onset Intervals) of played notes. We used HMM (Hid-\ndenMarkovModel)tosolvethisproblem(Saitoetal. 1999)by\nmodeling both ﬂuctuating note lengths and probabilistic con-\nstraints of note sequences. In this work, we also included mul-\ntiple tempos in the HMM to ﬁnd the best-matching tempo. In\nother works, tempo was included as hidden variables of prob-\nabilistic models (Cemgil et al, 2000; Raphael, 2001), or deter-\nmined by clustering IOIs (Dixon, 2001), and rhythm was esti-\nmatedbased on the tempo.\nInthispaper,wetreatrhythmrecognitionasaproblemofprob-\nabilistically decomposing the observed IOIs into rhythm and\ntempocomponents.\n2 Rhythmand Tempo\nThe observed duration (IOI) x[sec] of note in the performed\nMIDIdataisrelatedbothtointendednotevalue q[beats]inthe\nscoreandtotempovariable ¿[sec/beat](averagetimeperbeat)\nby:x[sec] =¿[sec/beat] £q[beats] (1)\nRhythm recognition can be deﬁned as a decomposition of IOIs\nX=fx1;¢¢¢; xNginto tempo T=f¿1,¢¢¢,¿Ngand rhythm\nQ=fq1,¢¢¢,qNg. This is a kind of ill-posed problem since ˆQ\nandˆTare not determined uniquely. In principle, any rhythm\ncan be expressed in various ways, e.g. twice note values and\nhalf tempo gives the same note duration in Eq. 1. Furthermore,\nﬂuctuation of tempo and rhythm can not be completely sepa-\nrated. Decomposition is possible only in a probabilistic sense\nPermission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided that\ncopies are not made or distributed for proﬁt or commercial advan-\ntage and that copies bear this notice and the full citation on the ﬁrst\npage.c°2003 Johns Hopkins University.\nCCMIDI\nSignalscore\n[sec] [beat]Rhythm Recognition\nx q\n(IOIs)Rhythm\n(note value)Figure 1: Rhythmrecognition for automatic transcription\nassuming that Tis constant or slowly changing (at least within\nphrases), and that Qoften ﬁt common rhythm patterns. Human\noften can recognize rhythm from the musical performance be-\ncause they have a prioriknowledge of rhythm, e.g., what type\nof rhythm patterns are likely to appear. In our approach, “most\nlikely rhythm patterns” for the given MIDI data are estimated\nby search in the proposed probabilistic models, whose param-\neters are optimized by stochastic training with existing scores\nand performances.\nOur goal is to separate rhythm and tempo by iterating the es-\ntimation of the two. First, we estimate rhythm from the IOIs\nof the given MIDI using tempo-invariant feature parameters.\nThen,usingtheestimatedrhythmandthegivenIOIs,thetempo\nisestimated. Rhythmandtempoarealternatelyre-estimatedus-\ning the estimated counterpart. In the next sections, we discuss\nﬁrst twosteps.\n3 HMM Using RhythmVectors\nThis section describes the method to estimate rhythm from ob-\nservedIOIs (Saito, 1999; Otsuki, 2001; Takeda,2002).\n3.1 Stochastic Modeling of RhythmPatterns\nWe assume that a sequence of note values appear in mu-\nsic with certain probability, which can be approximated\nby an n-gram probability, i.e., a conditional probability\nP(qtjqt¡1;¢¢¢; qt¡n+1)dependent on the history of previous\nn¡1note values. Similar to the n-gram language model\noften used in speech recognition, the probability of rhythm\nQ=fq1;¢¢¢; qNgis approximated by\nP(Q)¼P(q1;¢¢¢; qn¡1)NY\nt=nP(qtjqt¡1;¢¢¢; qt¡n+1)(2)\nConditional probabilities can be obtained through statistical\ntraining using already composed music scores.\n3.2 RhythmVector: a Tempo-InvariantFeature\nWeintroduceatempo-invariantfeaturenamed“rhythmvector,”\nsince the tempo of the input data is not given in advance and\nit may vary throughout the data. From our assumption that the\ntempoisconstantorchangesslowly,theproportionofconsecu-\ntive note lengths xis nearly independent from tempo ¿accord-\ningtoEq.1). Therefore,weintroduce rhythmvector asfollows:\nrt= (r1\nt;¢¢¢; rm\nt)where ri\nt=xt+i\nxt+¢¢¢+xt+m¡1(3)Table1: Formulatingrhythmrecognition with HMM\nHiddenStates State Transition OutputSignal\nn-tuples of note (n+ 1)-gram rhythmvectors\n \nStatesstate istate jstate kstate laij kla jkar1r2r3\nbl(\n\u0000\n3)bk(\n\u0000\n2)bj(\n\u0000\n1)bi(\n\u0000)0\nNote\n ValuesRhythm \nVectorsr0\nFigure 2: Rhythmvectoras the output of HMM\nHere, probabilistic distribution p(r)is assumed to follow the\nnormal distribution N(¹;Σ), whose parameters, mean vector\n¹and covariance matrix Σ, can be obtained through statistical\ntrainingusing human-performed MIDI data.\n3.3 RhythmEstimation: SearchProblemin HMM\nThe two probabilistic models of rhythm vector and rhythm pat-\nterncanbecombinedintheHMMframeworkasshowninTable\n1 and Fig. 2. This HMM gives the probability P(XjQ)P(Q),\nwhere P(Q)is the probabilistic model of rhythm score and\nP(XjQ)isthat of rhythmvectorsand tempo ﬂuctuation.\nRhythm estimation is to ﬁnd the time sequence of states in the\nstate transition network, Q, that gives the maximum a poste-\nrioriprobability, P(QjX), given a sequence of observed note\nlengths series, X. Maximizing P(QjX)is equivalent to max-\nimizing P(XjQ)P(Q)according to Bayes theorem. The opti-\nmalsequenceofstatesinHMMsisefﬁcientlyfoundthroughthe\nwell-known Viterbi algorithm. The sequence of intended notes\nˆQis estimated in the maximum likelihoodsense.\n3.4 Multiphonic Case\nTheabovestatedmethodcanalsobeappliedtothemultiphonic\ncase. Projecting the onset timings of all notes in a multiphonic\nmusic score onto a one-dimensional time axis, we obtain a\n“rhythm score” from which the n-gram “grammar” can be de-\nﬁned in the same way. After preprocessing for grouping nearly\nsimultaneous onsets, the rhythm score is recognized from the\ninput sequence of IOIs across multiple voices in the observed\nMIDI signal, followed by postprocessing for assigning note\nlengthto each of recognized note onsets.\n4 TempoEstimation\nTempo T, the sequence of instantaneous tempo estimated by\n¿t=xt=ˆqtfrom observed IOIs Xand the estimated rhythm ˆQ,\noften contain errors as shown in Fig. 3 such as double tempo,\nhalf tempo, and errors due to confusion by triplets mainly\ncaused by the nature of rhythm vector. We model the distri-\nbutionofestimatedtempo TbyaGaussianmixturedistribution\nandapplytheEM(Expectation-Maximization)algorithmtoes-\ntimate the true tempo. After estimating the tempo, note values\nareestimated again.\n5 Experimental Evaluation\nThe proposed method was evaluated by using 3 classical mu-\nsic pieces listed in Table 2 recorded in the MIDI format, which\nwere performed 2 times by 5 players for each piece. 19 kinds\n0.20.30.40.50.6\n0102030405060708090100\n     \n    \nTime [sec]Tempo [sec / beat ]\nτ\n3/2 times \nas slow as\ntempo µ\n \nprobabilistic distribution of tempoµ\n2/3 times \nas fast as\ntempo µFigure3: Tempoplot: derivedfrom rhythmrecognition results\nTable2: Testingmusic pieces for rhythmrecognition\nFugaJ. S. Bach: Fuga in C minor, BWV847, Well-\nTemperedClavierBook I.\nSonata L.v. Beethoven: Piano Sonata No. 20, 1st Mov.\nTr¨aumereiR. Schumann: “Tr ¨aumerei” from “Kinderszenen,”\nOp. 15, No.7.\nTable3: Rhythmrecognition results [%]\nData Prep.RRR-1RRR-2\nFuga 97.594.194.3\nSonata 97.460.778.5\nTr¨aumerei 97.568.472.0\nof note values (whole note, quarter note, etc.) were treated and\nrepresented by 6859 ( = 193) hidden states in the HMM, whose\ntransition probabilities were trained by 13 classical pieces con-\ntaining 4355 note values, and whose output probabilities were\ntrained by 2 music pieces by 2 players containing 1288 IOIs.\nRhythmrecognitionrateRRR-1(ﬁrstestimationofrhythm)and\nRRR-2 (rhythm estimation after tempo estimation) were ob-\ntained as shown in Table 3. “Prep” shows the rate of correct\npreprocessing (synchronizing grace notes, etc.).\n6 Conclusion\nWe proposed a method for rhythm recognition of MIDI signals\nperformedbyhumans. Intheexperimentalevaluation,theorig-\ninal rhythm was successfully estimated from MIDI piano per-\nformances.\nAcknowledgment\nThis research was partly supported by CREST (Auditory Brain\nProject).\nReferences\nA.Cemgil,B.Kappen,P.Desain,H.Honing,“Ontempotrack-\ning: Tempogram Representation and Kalman ﬁltering,” Journal\nof NewMusic Research, 2000.\nS. Dixon, “Automatic Extraction of Tempo and Beat from Ex-\npressivePerformances,”2001.\nT.Otsuki,N.Saito,M.Nakai,H.Shimodaira,andS.Sagayama,\n“Musical Rhythm Recognition Using Hidden Markov Model,”\nTransaction of Information Processing Society of Japan,\nVol.43, No. 2, pp. 245–255, 2002. (in Japanese)\nC. Raphael, “Automated Rhythm Transcription,” Proc. of IS-\nMIR, pp. 99–107, 2001.\nN. Saito, M. Nakai, H. Shimodaira, S. Sagayama, “Hidden\nMarkov Model for Restoration of Musical Note Sequence\nfrom the Performance,” Proceedings of the Joint Conference of\nHokuriku Chapters of Institutes of Electrical Engineers, Japan,\n1999, F-62, p.362, Oct 1999 (in Japanese).\nH. Takeda, T. Otsuki, N. Saito, M. Nakai, H. Shimodaira,\nS.Sagayama,“HiddenMarkovModelforAutomaticTranscrip-\ntion of MIDI Signals,”Proc. 2002 MMSP,2002."
    },
    {
        "title": "Blind clustering of popular music recordings based on singer voice characteristics.",
        "author": [
            "Wei-Ho Tsai",
            "Hsin-Min Wang",
            "Dwight Rodgers",
            "Shih-Sian Cheng",
            "Hung-Ming Yu"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1415112",
        "url": "https://doi.org/10.5281/zenodo.1415112",
        "ee": "https://zenodo.org/records/1415112/files/TsaiWRCY03.pdf",
        "abstract": "This paper presents an effective technique for automatically clustering undocumented music recordings based on their associated singer. This serves as an indispensable step towards indexing and content-based information retrieval of music by singer. The proposed clustering system operates in an unsupervised manner, in which no prior information is available regarding the characteristics of singer voices, nor the population of singers. Methods are presented to separate vocal from non-vocal regions, to isolate the singers’ vocal characteristics from the",
        "zenodo_id": 1415112,
        "dblp_key": "conf/ismir/TsaiWRCY03",
        "keywords": [
            "automatic clustering",
            "undocumented music recordings",
            "associated singer",
            "unsupervised manner",
            "indexing",
            "content-based information retrieval",
            "music by singer",
            "vocal regions",
            "isolating singers",
            "voice characteristics"
        ],
        "content": "Blind Clustering of Popular  Music Recordings Based on Singer  Voice \nCharacteristics \nWei-Ho Tsai, Hsin -Min Wang, Dw ight Ro dgers, Shi-Sian Cheng, a nd Hung-Ming Yu \nInstitute of Infor mation Science, Academ ia Sinica \nNankang, 115, Taipei, Taiwan, Republic of China \n{wesley,whm,dwight,sscheng,donny}@iis.sinica.edu.tw  \n \nAbstract \nThis pa per presents a n effective technique for \nautomatically clu stering un documented m usic \nrecordi ngs based on their associated singer. T his \nserve s as an indispe nsable ste p towards indexing a nd \ncontent-based in formation retriev al of m usic b y \nsinger. The proposed clustering system operates in an \nunsupervised manner, in which no prior information \nis av ailab le reg arding the characteristics o f singer \nvoices, nor the po pulation of si ngers. M ethods are  \nprese nted to separat e vocal  from non -vocal regi ons, \nto isolate the singe rs’ vocal characte ristics from  the \nbackgr ound mu sic, to co mpare the similarit y between  \nsingers’ voices, and to determine the total number of \nunique singers from a col lection of songs. \nExperimental eval uations conducted on a 200-track \npop m usic datab ase co nfirm  the validity o f the \nproposed system. \n1 Introductio n \nSupported by the rapid progress i n com puter an d network \ntechn ology, popular music is rapi dly bec oming one of the \nmost prevalent  data types ca rried by t he Internet. With the \nincrease d circ ulation of m usic data com es the corres ponding \nincrease i n our a ppetite for acce ssing the m efficiently and \nconveniently. As a resu lt, con tent-based retriev al of music h as \nbecom e an attractive topic of resea rch, and efforts  have been \nmade to develop automatic classifiers or iden tifiers of m usic \nby melody (Durey and Clements, 2002; Akeroyd et al., 2002), \ngenre (Tzaneta kis and Cook, 2002 ), singer (Kim and Whitman, \n2002; Liu and  Huang, 2002 ), an d other means (Byrd and \nCrawford, 2002 ). As an independent cap ability o r as p art of a \nmusic in formatio n retri eval system , techn iques to \nautom atically organize a coll ection of m usic reco rdings based \non the ass ociated singer a re nee ded in order t o lesse n or \nreplace human doc umentation efforts . This study a ddresse s the general task  of sing er-based clustering of  unknown music \nrecordi ngs, when neither singer information n or populations \nare a vailable.  \nThe m ost obvious application of singer-based cl ustering is in \ntools for expediently organizing and labeling unlabeled – or \ninsufficien tly well lab eled – m usic co llectio ns. For instance, \nmany rock music bands have a  lead singer who sings the \nmajority of all th e band’s son gs, but a m inority o f song s will \nbe sung by the guitarist, drummer, or ot her band-members. In \nsuch cases singer-ba sed cl ustering m ay be use d to ide ntify \nthose songs not sung by the lead singer. Furtherm ore, lead \nsingers in bo th ro ck and pop  music ar e known to quit, do solo \nalbums, start new bands, or join other bands. Since the vast \nmajority of documented music d ata is lab eled by artist (ban d \nname), si nger-based cl ustering m ay be usef ul for those \nwish ing to find th e full works of artists lik e Phil Co llins, Sti ng, \nOzzy Os bourne, or e ven Michael Jac kson1. \nSinger-based clustering  with support for multiple sin gers in a \nsong m ay be able to identify guest appea rances. For i nstance  \nQuee n’s hit song “Under Pressure ” included vocal s by David \nBowie, and Shaggy’s hit song “Mr. Lover” feat ured Janet \nJacks on. E ven when music data bases a re labele d, these  \nappea rances a re often om itted, es pecially in live c oncert \nrecordings and bootlegs (unauthorized am ateur recordings of a \nprofession al live concert). In add ition, many of t he methods \ndevel oped for singer-based clustering can be trivially applied \nto problem s such as  music re commendation syste ms – s uch a \nsystem coul d suggest music by singers with similar voices. \n2 Problem formulation \nGiven a set M of unlabeled music recordings, eac h performed \nby one singer from a set  P, whe re |M| ≥ |P|, and |P| is \nunknown, the syste m must partitio n M into K cluste rs such \nthat K = |P| and eac h cluster c onsists excl usively o f \nrecordings from only one si nger p from P.2 Perf ormance of \n                                                           \nPerm ission to make digital or  hard copies of all or p art of this work  for \npersonal of  classr oom use is gr anted without fee pr ovided that copies ar e \nnot made or  distributed for  profit or  commercial advantage and that  copies \nbear this notice an d the full citation on the first page.   2003 T he Johns  \nHopkins Univer sity. 1 Each of  these arti sts was in a band prior to beco ming fa mous for solo work: \nGenesis, The Police, Black Sabbath, and The Jackson Five, respectivel y. \n2 This formulation is not applicab le to  reco rdings containing backg round \nvocals, or multiple singers,  unless t hese recordings a re pre -segmented i nto \nsinger -homogenous r egions.  In this  research we lim it our selves to sin gle-\nsinger  recordings.  the cluste ring is eval uated on the  basis  of ave rage cluster \npurity (Solomonoff et al., 1998), defined as  \n,||1\n1∑\n==K\nkkknρ ρM  (1) \nand \n,22\n∑\n∈=\nPp kkp\nknnρ    (2) \nwhere ρk is th e purity o f the clu ster k, nk is the total nu mber of \nrecordi ngs in t he cluste r k, and nkp is the number of rec ordings \nin the cluster k that were performed by singer p. \n3 Method overview  \nThe purpose of si nger-based cl ustering i s to cluster the \nrecordi ngs on the ba sis of t he singer’s voice rathe r than the \nbackgr ound mu sic, m usical g enre, or other charact eristics of \nthe rec ording. There fore, it is  neces sary t o extract, m odel, and \ncompare the characte ristic feature s of the singe rs’ voices \nwithout interferen ce fro m non-singer featu res. Toward th is \nend, a three stage process as  shown in Fig. 1 is proposed: (1) \nsegm entation of eac h recording into vocal and non-vocal \nsegm ents, where a vocal segm ent consi sts of c oncurrent  \nsinging and  acco mpaniment, whereas non- vocal seg ments \nconsist o f accom paniment only; (2 ) distillati on of the sing er’s \nstochastic vo cal ch aracteristic s from  the vocal se gments by  \nspecifically suppressi ng the characteristics of t he background \n(non-vocal  segments); and (3) clustering of t he recordings  \nbased on singer characteristic si milarit y. Details o f each  of the \nstages will b e described in the sub sequent sectio ns. \nMusic  Recordings M\nVocal/Non-vocal\nSegmentation\nInter-recording\nSimilarity Computation\n& Clustering\nK ClustersSinger Characteristic\nModeling\n \nFigure 1: Block diagram of the proposed singer-based \nclustering. \n4 Vocal/non-vocal segmentation \nAs a first step in determining the vocal c haracteristics  of a \nsinge r, music segm ents that contain vocal s are  locate d and \nmarked as s uch. This task ca n be form ulate d as a problem of \ndistinguishing between vocal segments and accompanim ents, in analogy with the stud y by Beren zweig and  Ellis (200 1). \nHowever, in contrast to their work, which uses a  spe ech \nrecognizer f or det ecting si nging voices, we p ropose to \nconstru ct a stat istical classifie r with parametric models trai ned \nusing accom panied singing voices rathe r than norm al spe ech. \nAs shown in Fig. 2, the classifier consists of a fron t-end sign al \nprocess or that converts digital wave forms into spect rum-based \nfeature vect ors, followed by a bac kend statistical proce ssor \nthat performs modeling, matching and decision making. It \noperates in two ph ases, train ing and testin g.  \nDuring training, a m usic dat abase wi th manual vocal /non-\nvocal tran scrip tions is u sed to  form  two  separate Gaussian \nmixture m odels (GM Ms): a vocal  GM M, and a non-vocal \nGMM. Th e use of G MMs is m otivated b y the desire fo r \nmodeling va rious broad acoustic classes b y a com bination of \nGaus sian com ponents. The se broad ac oustic classes re flect \nsome gene ral vocal  tract and instrumental configurations. It \nhas been shown that GMMs have a strong ability to  provide \nsmooth app roximations to arb itrarily-shap ed densities o f \nspectrum  over a long tim e span (Reyno lds and  Rose, 199 5). \nWe denote the vocal  GMM as λV, and the non-vocal GMM λN. \nParam eters o f the GMMs are in itialized  via k-means clustering  \nand iteratively adjusted via expect ation-maximization (EM) \n(Dem pster et al., 197 7). \nFeature VectorsAudio\nFrame-based\nFeature Extraction\nStochastic\nMatchingNon-vocal\nGMMVocal\nGMM\nDecisionStochastic\nMatching\nFrame\nLikelihoodsFrame\nLikelihoods\nFrame Attributes\n \nFigure 2: Voca l/non-vocal  segmentation. \n \nIn the testing phase, t he rec ognizer take s as i nput the Tx-length \nfeature  vectors  X = {x1, x2, ..., xTx} extracted from an \nunknown recording, and  prod uces as ou tputs the frame log-\nlikelihoods p(xt|λV) and p(xt|λN), 1≤ t ≤Tx, respectively , for the \nvocal  and t he non-vocal GMM. Th e attribute of each fra me is \nthen hypothesi zed according to a decision rule m ade on the \nframe log-likelihoods. D epending upon the choices o f analysis \ninterval, t here are m any variations a nd com binations  in \ndecision -making. In  this study, we com pare s everal  \npossibilities, in cluding frame-b ased decisio n, fixed-length -\nsegm ent-based deci sion and homogene ous-segment-based \ndecision. A. Frame -based decision.   \nThe reco gnizer m ay triv ially hypothesize wh ether the frame xt \nis vocal or not using \n, )|( log-)|( log\nvocal-nonvocal\nη≤>λ λN t V t p p x x  (3) \nwhere η is a threshold. Since singing tends to be continuous, \nthese results may b e smoothed in th e time d omain. For \nsmoothing, the fram es are  divided into a se quence of \nconsecutive, non-overlapping, fixed-length segments. The \nmajority hypothesis for each segment is th en assigned to each  \nfram e of that segment. \nB. Fixed-length-s egment-base d decision.   \nAn improvement of the smoothing above can  be m ade b y \nassigning a single classification per segment directly, using: \n.  )| ( log-)| ( log1\nvocal-nonvocal\n1\n01\n0η≤>\n\nλ λ∑ ∑−\n=+−\n=+W\niN i tWW\niV i tW p pWx x  (4) \nwhere t is th e segment index and  W is th e segment length. In \ngene ral, accum ulating the  frame log-likelihoods  over a longe r \nperiod is m ore statistically  reliab le for decisio n-making. \nHowever, as with smoothing, long segm ents coul d run the risk \nof crossing  multiple vocal/non -vocal ch ange boundaries. \nC. Hom ogene ous-segment-base d decision.   \nFurther improvement can be m ade by  merging adjacent \nsegm ents if they do n ot cross a vocal /non-vocal  boundary. In \nthis study, vector clustering is employed on the set of all frame \nfeature vectors  and eac h fra me is assigne d the cluste r inde x \nassociated with that fram e’s feature vector. Eac h segment is \nthen assigned the majority ind ex of its con stituent fram es, and \nadjace nt se gments are m erged as a homogeneous se gment if \nthey have the sam e index . Classification is then m ade per  \nhomogeneous-segm ent using: \n.  )| ( log-)| ( log1\nvocal-nonvocal\n1\n01\n0η≤>\n\n\n\nλ λ∑ ∑−\n=+−\n=+k\nkk\nkW\niN isW\niV is\nkp pWx x\n (5 ) \nwhere Wk and sk represe nt, respectively, the length and starting \nfram e of the k-th homogene ous-segment. \n5 Singer characteristic modeling \nTo clu ster recordings by singer, th e ch aracteristics of the \nsinger’s voice m ust be distilled from the mixture of vo ice and \naccom panim ent. Let V = {v1, v2, ..., vT} represent t he feat ure \nvectors from  a vocal  region. V can be modeled as a mixture of \na so lo vo ice S = {s1, s2, ..., sT} and  a background \naccom panim ent B = {b1, b2, ..., bT}, where S and  B are \nunobservable bu t B’s stocha stic cha racteristics can be \napproximated from the no n-vocal segm ents. This section \nprese nts a method of obtaining a stochast ic model λs for the \nsolo signal S. Based  on the techn iques dev eloped in robust speech and \nspeaker re cognition (R ose et al., 1994; Nadas et al., 1989), it \nis assum ed that the sol o signal and background m usic are, \nrespect ively, drawn randomly and independently according to \nGMMs λs = {ws,i, µs,i, Σs,i | 1 ≤ i ≤ I }, and λb = { wb,j, µb,j, Σb,j | \n1 ≤ j ≤ J },  where ws,i and wb,j are mixture w eights, µs,i and µb,j \nmean vect ors, and Σs,i and Σb,j covariance m atrices. If the  \naccom panied signal is formed from  a gene rative function \n),(tt tf bs v= , 1 ≤ t ≤ T, the prob ability o f V, given λs and λb \ncan be rep resented by \n,),,,|( ),|(\n1 11, , ∏∑∑\n= == \nλλ =λλT\ntI\niJ\njb s t jbis b s ji pww p v V        (6) \nwhere \n. ) , ;(),;( ),,,|(\n),(, , , ,  ∫∫\n==λλ\ntt tft t jb jb t is is t b s t dd ji p\nbs vbs b s v Σ Σ µ µ N N\n           (7) \nIt is desire d to estim ate the sol o voice m odel λs, given the \naccom panied voice V and the bac kground music model λb. \nThis can be done in a maximum likelihood manner as follows:  \n).,|( maxargb s s p\nsλλ =λ\nλ∗V               (8) \nUsing  the EM  algorithm, an initial model λs is created, a nd the \nnew model sλ is then estim ated by maximizin g the aux iliary \nfunction ˆ\n,  (9) ) ˆ|,,( log) ,|,( )ˆ(\n11 1∑∑∑\n== =λ,λ λ,λ =λ ,λT\ntI\niJ\njb s t b s t s s jip jip Q v v\nwhere \n), ˆ,,|( ) ˆ|,,(, , b s t jbis b s t ji pww jip λ,λ =λ,λ v v            (10) \nand \n.\n) ,,|() ,,|() ,|,(\n11, ,, ,\n∑∑\n==λ,λλ,λ=λ,λI\nmJ\nnb s t nb msb s t jbis\nb s t\nnm pwwji pwwjip\nvvv      (11) \nLettin g ∇  with resp ect to each p arameter to  be \nre-estim ated, we have 0)ˆ ( =λ ,λs sQ\n,) ,|,(1ˆ\n11,∑∑\n==λ,λ =T\ntJ\njb s t is jipTw v                              (12) \n{}\n,\n),,|,(,,,,| ),,|,(\nˆ\n1111\n,\n∑∑∑∑\n====\nλλλλ ⋅λλ\n=T\ntN\njb s tT\ntN\njb s t t b s t\nis\njipji E jip\nvvs v\nµ   (13) \n{}\n,\n),,|,(,,,,| ),,|,(\nˆ\n, ,\n1111\n, isis T\ntJ\njb s tT\ntJ\njb s t tt b s t\nis\njipji E jip\nµµ ′ −\nλλλλ ′ ⋅λλ\n=\n∑∑∑∑\n====\nvvss v\nΣ \n(14) \nwhere prim e denotes vector transpose, and E{⋅} denotes \nexpectation.  Suppose t hat V, S and B are log-s pectrum features, and the \nbackgroun d music is add itive in the time domain or linear-\nspectrum  domain. The  accompanied si gnal can the n be \napproximately ex pressed by vt ≈ max (st, bt), 1 ≤ t ≤ T, \naccording to Nada s’ MIXM AX m odel (Nadas  et al ., 198 9). \nFor im plementation e fficienc y, the c ovariance m atrices of the \nGMMs use d in this study are assum ed diagonal, a nd thus each \nvector  component invo lved can be operated on  independently. \nDenoting by   and , respecti vely, an arbitrary diagonal  \ncompone nt of Σ2\n,isσ2\n,jbσ\ns,i and Σb,j, and foc using on scalar obse rvations \nfor ease  of discussion, we compute Eq. (7) using \n), () , ;(                                                       ) (),;(                                                              ),;( ) , ;(                        ) , ;( ),;(          ) , ;(),;(          ),,,|(\n,, 2\n, ,,, 2\n, ,2\n, ,  \n2\n, ,2\n, ,  \n2\n, ,),(2\n, ,2\n, ,  \nisis t\njb jb tjbjb t\nis is tt is is tv\njb jb tt jb jb tv\nis is tbsfvt t jb jb t is is tb s t\nvvvvsd s bbd b sbdsd b sjivp\ntttt t\nσµσµσµσµσµ σµσµ σµσµ σµ\n−Φ +−Φ =+==λλ\n∫∫∫∫\n∞−∞−=\nNNN NN NN N\n \n ( 15) \nwhere \n.\n21)(2/  2dw ew−\n∞−∫= Φ\nπττ  (16) \nThe value of Φ(τ) can be obtained using a table of the error \nfunction. On the other hand, the conditional expect ation in Eq. \n(13) can be shown in the follo wing form: \n{}\n() { ,,,,, | ),,,| ( 1                                                                       ),,,| ( ,,,,|\nb s t t t b s t tt b s t t b s t t\njivssE jivspv jivsp jivsE\nλλ < ⋅λλ = −+⋅λλ = =λλ \n(17) } ,  \nand   ,                      0.0 and   )}, -( xp{  ,                      0.1\n,, , , ,\n\n> ≠≤ ≠=\n=\nθθ αϕ\n R  ij  R  ij L L e                 ij\nF\njiji i ji ji ( 22) \nwhere \n,\n) () , ;( ) (),;() (),;(),,,| (\n,, 2\n, ,\n,, 2\n, ,,, 2\n, ,\nisis t\njb jb t\njbjb t\nis is tjbjb t\nis is tb s t t\nvvvvvvjivsp\nσµσµσµσµσµσµ\n−Φ +−Φ−Φ\n=λλ =\nN NN\n   (18) \nand \n{} .   (19) \n) (),;(,,,, |\n,,2\n, ,\n, ,\nisis tis is t\nis is b s t t tvvjivssE\nσµσµσ µ−Φ− =λλ <N\nSimilarly, th e conditional exp ectatio n in Eq. (14) is co mputed \nusing \n{ }\n() { ,,,,, | ),,,| ( 1                       ),,,| ( ,,,,|\n22 2\nb s t t t b s t tt b s t t b s t t\njivssE jivspv jivsp jivsE\nλλ < ⋅λλ = −+⋅λλ = =λλ\n}\n   ( 20) where \n{ }\n.\n) (),;() (           ,,,, |\n,,2\n, ,\n, ,2\n,2\n,\nisis tis is t\nis t is is isb s t t2\nt\nvvvjivssE\nσµσµσ µ σ µ−Φ+ − + =λλ <\nN ( 21) \nNote that if the nu mber of mixtures in th e backgr ound mu sic \nGMM is zero then this degenerates t o directly modeling the \nobserved vocal signal without taking the back ground m usic \ninto acco unt. Th is serv es as a baseline to  examine the \neffect iveness of our solo modeling m ethod. \n6 Similarity computation & clustering \nFinally, to c ompare and cluster the si ngers, eac h rec ording is \nevaluated against eac h singer’s solo model in a m ethod \nextended from (Tsai et al ., 2001). From sectio n 5, a so lo \nmodel λs,i and a bac kground music model  λb,i is gene rated for \neach of the M recordings to be clustered, 1 ≤ i ≤ M. The log-\nlikelihood, Li, j = log p(Vi|λs, j,λb,i), 1 ≤ i, j ≤ M, that t he vocal \nportion  of th e reco rding Vi tests ag ainst the model λ s, j, is then \ncomputed using Eq. (6) and (15). A l arge log-likelihood Li, j \nshoul d indicate that the singer of  song i is similar to  the singer \nof song j. Singer-based clustering ca n be formulated as a \nconventional vector clustering al gorithm by assi gning the \ncharacte ristic vector Li = [Li,1,Li,2,…, Li,M]′, 1 ≤ i ≤ M, to each \nrecordi ng i, and co mputing the similarity b etween  two \nrecordi ngs using the  Euclidea n distance: || Li - Lj ||. \nThe clustering q uality m ay be improved by emphasizin g the \nlarger lik elihoods and  sup pressing  the smaller o nes. To \nachieve this, th e Li,j for each rec ording i are ranke d in \ndesce nding order of likelihood. Let the rank of Li, j be denoted \nby Ri, j. Then, the c haracte ristic vect ors Fi = [Fi,1,Fi,2,…, Fi,M]′, \n1 ≤ i ≤ M, are formed usi ng \nand \n, maxarg,\n        ki\nikL\n≠=ϕ    ( 23) \nwhere α is a positive con stant fo r scaling , and θ is an  integer \nconstan t for pruning the lo wer l og-lik elihoods. Ex ample \ncharacte ristic vectors c omputed for a collection o f 25 music \nrecordi ngs wit h 5 singers are shown as columns in Fig. 3. \nDark reg ions in the figure represent larg e values, wh ile lig ht \nregions represent sm all one s. This figure  shows t hat the \nvectors Fi more cl early distinguish between different singers \nthan vectors Li. \nTo solve the vector clustering problem, we use the k-means \nalgorithm, which starts with a single cluster and re cursively \nsplits clu sters in attem pts to  minimize the with in-cluster \nvariances. A choice m ust be made as t o how m any clusters \nshould be crea ted. If the number of cl usters is low, a si ngle \ncluster is lik ely to include recordings from  multiple sin gers. \nOn the other hand, if the number of clusters is too high, a \nsingle sing er’s record ings will b e split acro ss multiple clu sters. Clearly th e op timal number of cl usters K is equ al to th e \nnumber of singers, which is unknown. \nIn this stu dy, the Bay esian I nformation Criterio n (BIC) \n(Schwarz, 1978) is employed to deci de the best  value of K. \nThe BIC assigns a value to a stochastic model based on how \nwell th e model fits a data set, and  how simple th e model is, \nspecifically \n|,|log 21)|( log) BIC( D D d p γ−Λ =Λ  ( 24) \nwhere d is the number o f free param eters in model Λ, |D| is \nthe size of the d ata set D, and γ is a penalty factor. A K-\nclustering can b e modeled as a co llectio n of G aussian \ndistributions (one per cl uster). The B IC may then be computed \nby:  \n, log)1 (21 21||log2)(BIC\n1M MM MKnKK\nkkk\n\n+ + −\n\n−=∑\n=γ Σ\n   ( 25) \nwhere nk is the num ber of ele ments of the cl uster k, and Σk the \ncova riance  matrix of the characteristic v ectors in th e clu ster k,. \nThe BIC value shou ld increase as t he sp littin g improves \nconformity of the model, but should declin e significan tly after \nan exce ss of clusters are created. A reasonable num ber of \nclusters can be det ermined by  \n).(BIC maxarg\n1*K K\nMK≤≤=   (26) \n7 Experimental results \nThe m usic dat a used in this study consisted of 416 tracks from \nMandarin pop music CDs. All the tracks were manually \nlabeled with the singer iden tity and  the vo cal/non -vocal \nboundaries. The dat abase was di vided into two subsets, \ndenoted as DB-1 and DB-2, respectively. T he DB-1 c omprised \n200 tr acks performed by 10 fe male an d 10 male sin gers, with \n10 distinct songs per singer. DB-2 contained the remaining \n216 tracks, involving 13 female and 8 m ale singers , none of \nwhom appeare d in DB-1. All music data were down-sampled \nfrom the CD  sam pling rate of 44.1 kHz to 22.05 kHz, to \nexclude the high frequency compone nts beyond the range of \nnormal singing voices.  \nExtensive com puter simulations were  conducted to eval uate \nthe perform ance of t he propos ed singer-clustering system . The \nvocal  and non-vocal GMMs we re trai ned usi ng DB -2, and \noverall performance of the system was t hen evaluated using \nDB-1. The feature vectors used in th e system w ere Mel-scale \nfrequency ce pstral c oefficients (M FCC s) (Davis a nd \nMermelstein, 19 80; Logan, 2000), com puted using a 32-ms \nHam ming-windowed fram e with 10-ms frame shifts.  \nOur first exp eriments tested  the validity of the vocal/non -vocal \nsegm entation m ethods . Acc uracy was com puted by com paring \nthe hy pothesized attribute of eac h fram e with the manual \nlabel3. Howev er in v iew of the lim ited precisio n with wh ich \nthe hum an ear detects vocal/non- vocal c hanges, all fram es that \noccu rred within 0.5 seco nds of a per ceived swi tch-point were \nignored in the computation. Table 1 summarizes the results of vocal /non-vocal segm entation, using a 64-mixture vocal  \nGMM and  an 80 -mixture non-vocal GMM (em piricall y the \nmost accurate configuration) . The ta ble shows  that  the  \nhomogeneous-segm ent-based method i s superior to the other \nmethods  when an adequate number of clusters are used. The \nbest acc uracy a chieve d was 79.8%. \n                                                           \n3 Accurac y (%) = # correctl y-identified fra mes / #  total fra mes × 100%  \n \nVλ\n{\n{\n{\n{\n{ Singer 1 Singer 2 Singer 3 Singer 4 Singer 5\n \n(a) \n \n {\n{\n{\n{\n{ Singer 1 Singer 2 Singer 3 Singer 4 Singer 5\n \n(b) \n \nFigure 3: A gray scale repres entation of the log -likelihoo ds \nand ch aracteristic v ectors. (a) log -likelihoods; (b) \ncharacte ristic vectors (θ = 6).  \nSmoothing window \n(# fram es) 1 \n(no sm ooth) 20 40 60 80 \nAccuracy (%) 68.5 72.1 76.5 76.876.0\n(a) Fram e-based decision \nSegm ent length \n(# fram es) 20 40 60 80 \nAccuracy (%) 73.9 77.2 77.6 76.9 \n(b) Fix ed-length-segment-based decision \n# clu sters for \ntokenization 2 4 8 16 32 \nAccuracy (%) 42.3 65.6 75.9 79.8 78.5 \n(c) H omogene ous-segment-based deci sion (Smoothing \nwindow = 60  frames) \nTable 1: Results of vocal /non-vocal  segm entation. \n \nTable 2 sho ws the con fusion probability matrix from the \nvocal /non-vocal discrimination results of the homogene ous-\nsegment-based d ecision . The r ows of the co nfusion  matr ix \ncorresp ond to th e ground-truth of th e seg ments wh ile th e \ncolum ns indica te the hypothes es. We can see th at the majority \nof er rors a re m isidentifications of vocal se gments. \nQualitativ ely, we fou nd that m any falsely  identified  vocal \nsegm ents had unusually loud background music or unusually \nquiet vo cals. However, du e to the high back ground to vocal \nratio, we b elieve that such false ju dgments may actu ally \nbene fit the singer clustering. \nHypo thesized \nActual \nVocal Non-vocal \nVocal  0.75 0.25 \nNon-vocal 0.13 0.87 \nTable 2: Confusion prob ability matrix of the vo cal/non -vocal \ndiscrimination. \n \nNext, t he entire si nger-clustering system , based on both \nmanual seg mentation and th e best resu lts of automatic  \nsegmentation, was ex amined on DB-1. Fig. 4 sh ows th e \navera ge purity as a function of the number of clusters . We can \nsee that as expected, the  average purity gain s sharply as th e \nnumber of clusters increases i n the beginning and then tends to \nsaturate after t oo many clusters are  create d. Comparing the \nresults wit h and witho ut exp licit u sage of th e backg round \nmodel in extractin g the solo information, the effectiv eness of \nthe solo signal modeling over direct vocal modeling is clearly \ndemonstrated. Whe n the number of cl usters is equal to the \nsinger population (K = P = 20), the highest p urities o f 0.87 \nand 0.77 were yielded by using manual segm entation and \nautomatic seg mentation, respectiv ely. Th is con firms that th e \nsystem is capable of grouping the music data according to \nsinger.  \nLastly, th e problem of automatically d etermining the number \nof singers was i nvest igated. A series of clustering experiments \nwere conducted using 50 music recordings (5 singers × 10 \ntracks), 100  music recordings (10 sing ers × 10 tracks) , 150 \nmusic recordi ngs (15 si ngers × 10 tracks), and 2 00 m usic \nrecordings (20 singers × 10 tracks), res pectively. Fig. 5 shows \nthe resu lting BIC values as a fun ction of cluster n umber. The \npeak of eac h curve in the figure  is located very close to the  \nactual number of si ngers, validating the proposed BIC-based \ndecision criteri on. \n8 Conclusions \nThis study has examined the feasib ility o f unsupervised \nclustering of music dat a based on their singer. It has been  \nshown t hat the cha racteristics of a singe r's voice ca n be  \nextracted from m usic via vocal segm ent det ection followe d by \nsolo vocal  signal modeling. Singer-based clustering has been \nformulated an d solved using a ve ctor-clustering fram ework \nwith reliab le estimation of the correct num ber of cluste rs.  \nAlthough viable resul ts have been  reported in this pape r, more \nwork is needed to validate the proposed meth ods for a wid er \nvariety of  music d ata, such as larg er sing er populations and \nricher songs with different genres. Furthermore, f uture work \nfor sing er-based  clustering will ex tend the current system to  \nhandle duets, chorus, background vocals, or other music dat a \nwith multiple simultaneous or no n-sim ultaneous sing ers. \n \n \n0 102 03 04 05 06 07 08 09\nNo. of Clusters00.000.100.200.300.400.500.600.700.800.901.00Average Purity\nManual Segment ation; 32-m ix Solo  GMM & 8- mix Back ground GMM / Recordi ng\nManual Segment ation; 32-m ix Vocal  GMM / Recordi ng \nAutomatic  Segmentati on; 2 4-mix Solo GMM  & 8-mix Background GMM / Re cording\nAutomatic  Segmentati on; 2 4-mix vocal GMM / Reco rding \n \nFigure 4: Results of singer-based cl ustering. 0 5 10 15 20 25 30 35 40 45 50\nNo. o f Cluster s-520-480-440-400-360-320-280-240-200-160-120-80-40BIC5 singers\n10 singers\n15 singers\n20 singers\n \nFigure 5: BIC measu rements after each  split. \n \nAckn owledge ments \nThis research  was p artially su pported by the Natio nal Science \nCouncil, Taiwan, ROC, under Grant No. NSC9 2-2422-H-001-\n093. \n \nRefere nces \nAkeroyd, M. A., Moore, B. C. J., and Moore, G. A. (2001). \nMelody recognition using three t ypes of dichotic-pitch \nstimulus. The Journal of the Acoustical Society of Ame rica, \n110(3), 1498–1 504. \nByrd, D. & Cr awford, T. (2002). Pr oblems of music \ninformation retriev al in  the real word . Information \nProcessi ng and Management , 38(2), 249−272. \nDavi s, S. & Mermelstein, P (1980) Comparison of param etric \nreprese ntations  for m onosyl lable word recognition i n \ncontinuo usly sp oken sen tences. IEEE Transactions on \nAcoustics, Spe ech an d Signal Proce ssing, 28(4), 357−366. \nDem pster, A.  Lai rd, N. & Rubin, D. (1977). Maximum \nlikelihood from  incomplete data via the EM algorithm. \nJourna l of the Royal Statistica l Society, 39, 1 −38. \n \n \n \n \n \n \n \n \n Durey, A. S. & Clem ents, M. A.  (2002). Feat ures for melody \nspotting using h idden Marko v models. In  Proceedings of \nIEEE  International Conference on Acoustics, Speech, and \nSignal Proce ssing, 2 (pp . 1765–1768). Orlando, Florida. \nKim, Y. E. & Whitman, B. (20 02). Sing er id entification in  \npopular music rec ordings using voice coding features. In \nProcee dings of Interational Conference on Music \nInformation Retrieva l (pp. 164–169), Paris, France. \nLiu, C . C. & Hua ng, C . S. (2002). A si nger identification \ntechni que for cont ent-based  classification of MP3 music \nobjects. In Proceedi ngs of Inter national Confere nce on  \nInformation and Knowledge Management (pp. 438–445), \nMcLean, Vir ginia. \nLogan, B. (2000). Mel frequency ce pstral coefficients  for \nmusic m odeli ng. In Proceedi ngs of  Inter ational  \nSymposium on Mu sic In formation Retrieva l, Plymouth, \nMassachusetts. \nNada s, A. , Nahamoo, D.,  and Picheny, M. A. ( 1989). Speech \nrecognition usi ng noi se-adaptive prototypes. IEEE \nTranacti ons on Ac oustics, Speech, and Si gnalPr ocessing , \n37(10), 1495−1503. \nReyn olds, D. A . & R ose, R. C. (199 5). Robust text-\nindependent speaker i dentification using Ga ussian m ixture \nspeaker m odels. IEEE Tran sactions on  Speech and Audio \nProcessi ng, 3(1), 72−83. \nRose, R . C., Hofstetter , E. M., an d Reyno lds, D. A. (1994). \nIntegrated models of signal an d background with \napplication  to speaker identificatio n in no ise. IEEE  \nTran sactions o n Speech and Audio Pro cessing , 2(2), \n245−257. \nSchwarz, G. (1978). Estimation the dimension  of a model. The \nAnnals of Statistics, 6, 461−464. \nSolom onoff, A., Mielke , A., Schm idt, M., and Gish, H.  \n(1998). Clu stering speakers by th eir voices. In Proceeding \nof IEEE C onference on Acoustics, Speech, and Signal \nProcessi ng, 2, (p p. 757−760), Seattle, Washington. \nTsai, W. H., Chu, Y. C., Huang, C. S., an d Ch ang, W. W. \n(2001). Background learning of spea ker voices f or text-\nindependent sp eaker id entificatio n. In Proceedings of  \nEuropean Conference on  Speech Communication and \nTech nology, Aalb org, Denmark.  \nTzanetakis , G. & Cook, P. (2002). Mu sical gen re \nclassification of a udio signals. IEEE T ransactions on \nSpeec h and Audio Proce ssing, 10(5), 293−302."
    },
    {
        "title": "Ground-truth transcriptions of real music from force-aligned MIDI syntheses.",
        "author": [
            "Robert J. Turetsky",
            "Daniel P. W. Ellis"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417667",
        "url": "https://doi.org/10.5281/zenodo.1417667",
        "ee": "https://zenodo.org/records/1417667/files/TuretskyE03.pdf",
        "abstract": "Many modern polyphonic music transcription algo- rithms are presented in a statistical pattern recognition framework. But without a large corpus of real-world music transcribed at the note level, these algorithms are unable to take advantage of supervised learning",
        "zenodo_id": 1417667,
        "dblp_key": "conf/ismir/TuretskyE03",
        "keywords": [
            "polyphonic music transcription",
            "statistical pattern recognition framework",
            "supervised learning",
            "large corpus",
            "real-world music",
            "note level",
            "algorithms",
            "unable to take advantage",
            "transcription",
            "without"
        ],
        "content": "Ground-Truth Transcriptions of Real Music fromForce-Aligned MIDISyntheses\nRobertJ.Turetskyand DanielP.W. Ellis\nLabROSA,Dept. ofElectricalEngineering,\nColumbiaUniversity,NewYorkNY10027USA\n{rob,dpwe}@ee.columbia.edu\nAbstract\nMany modern polyphonic music transcription algo-\nrithmsarepresentedinastatisticalpatternrecognition\nframework. But without a large corpus of real-worldmusic transcribed at the note level, these algorithms\nare unable to take advantage of supervised learning\nmethodsandalsohavedifﬁcultyreportingaquantita-tivemetricoftheirperformance,suchasaNoteError\nRate.\nWe attempt to remedythis situation by takingadvan-\ntage of publicly-available MIDI transcriptions. Byforce-aligning synthesi zed audio generated from a\nMIDI transcription with the raw audio of the song\nit represents we can correlate note events within theMIDI data with the precise time in the raw audio\nwherethat noteislikelytobeexpressed. Havingthese\nalignments will support the creation of a polyphonictranscription system based on labeled segments of\nproducedmusic. ButbecausetheMIDItranscriptions\nwe ﬁnd are of variable quality, an integral step in the\nprocessisautomaticallyev aluatingtheintegrityofthe\nalignmentbeforeusingthetranscriptionaspartofanytrainingsetoflabeledexamples.\nComparing a library of 40 published songs to freely\navailableMIDIﬁles, wewereabletoalign31(78%).\nWe are bu ilding a collection of over 500 MIDI tran-\nscriptions matching songs in our commercial music\ncollection, for a potential total of 35 hours of note-\nleveltranscriptions,orsome1.5millionnoteevents.\nKeywords:polyphonic transcription, MIDI align-\nment,dataset/corpusconstruction\n1I n t r oduction\nAmateur listeners have difﬁculty transcribi ng real-world poly-\nphonic music, replete with complex layers of instrumentation,\nvocals,drumsandspecialeffects. Indeedittakesanexpertmu-\nsicianwithatrainedear,adeepknowledgeofmusictheoryand\nPermission to make digital or har dcopies of all or part of this work\nforpersonal or classroom use is granted without fee provided that\ncopies are not made or distributed for proﬁt or commercial advan-tage and that copies bear this notice and the full citation on the ﬁrstpage.c/circlecopyrt2003 Johns Hopkins University.\nexposure to a large amount of music to be able to accomplish\nsuch a task. Givenits uncertainnature,it is appropriateto pose\ntranscriptionasastatisticalpatternrecognitionproblem(Walm-\nsleyetal.,1999;Goto,2000).\nAfundamental obstacle in both the training and evaluation of\nthesealgorithmsisthelackoflabeledground-truthtranscription\ndatafromproduced(i.e. “real”) music. Thekindo finformation\nwe need is the kind of description present in a MIDI ﬁle: a se-quenceofnotepitchesandtheprecisetimeatwhichtheyoccur.\nHowever, exact MIDI transcriptions of speciﬁc recordings are\nnotavailable\n1.\nOne approach is to take existing MIDI ﬁles, which are often\nconstructed as musical performances in their own right, and\ngeneratethecorrespondingaudiobypassingthemtoMIDIsyn-\nthesizersoftware. Forthissynt hesizeroutput,theMIDIﬁlepro-\nvidesaveryaccuratetranscription,sothisdatawouldbeuseful\nfor system eva luation and/or classiﬁer training (Klapuri et al.,\n2001).\nUltimately this approach suffers because of the lack of expres-\nsivecapability available in MIDI ﬁles, and the corresponding\npaucity of audio quality in MIDI-generated sound. The 128\nGeneral MIDI instruments pale in comparison to the limitlesssonic power of the modern recording studio, and MIDI is in-\ntrinsically unable to represent the richness of voice or special\neffects. As a result, transcri ption performance measured on\nMIDI syntheses is likely to be an over-optimistic indicator of\nperformanceonrealacousticorstudiorecordings.\nOur proposal is an attempt at getting the best of both worlds.\nVastlibrariesof MIDI transcripti onsexiston the Internet,tran-\nscribed byp r ofessionals for Karaoke machines, amateur musi-\ncians paying deference to belove da rtists, and musi cs tudents\nhoning their craft. These transcriptions serve as a map of themusic, and when synthesized can create a reasonable approx-\nimation of the song, although the intent is usually to create a\nstandaloneexperience,andnotan accurateduplicateoftheorig-\ninal. In our experience, such tran scriptions respect the general\nrhythmoftheoriginal,butnotprecisetimingorduration. How-\never, we b elieve that we can achieve a m oreprecise correlation\nbetweenthe twodisparateaudiorenditions-originalandMIDI\n1Although many contemporary recordings include many MIDI-\ndrivensynthesizerparts,whichcouldintheorybecaptured,thisdatais\nunlikely to be released, and willonly ever cover a subset of the instru-\nments in of the music. It is worth, however, noting the work of Gotoet al. (2002) to collect and release a corpus of unencumbered record-ings along withMIDI transcriptions.Figure 1: Goalof MIDI Alignment. The uppergraphis the transcriptionof the ﬁrstverse of “Don’tYouWant Me”, bythe Human\nLeague,extractedfromaMIDIﬁleplacedon thewebbyanenthusiast. TheMIDIﬁlecan besynthesizedtocreateanapproximation\nof the song with a precise transcription,which can analyzedalongside the originalsong in order to create a mappingbetween note\neventsintheMIDIscoreandthe originalsong,toapproximatea transcription.\nsynthesis- throughforcedalignment. Thisgoalis illustratedin\nﬁgure1.\nIn order to align a song with its transcription, we create a sim-\nilarity matrix, where each point gives the cosine distance be-tween short-time spectral analyses of particular frames from\neach version. The features of the analysis are chosen to high-\nlight pitch and beat/note onset times. Next, we use dynamic\nprogrammingtoﬁndthelowest-costpathbetweenthestartsand\nends of the sequences through the similarity matrix. Finally,\nthis path is used as a mapping to warp the timing of the MIDI\ntranscriptionto matchthat oftheactualCDrecording.\nBecausethedownloadedMIDIﬁlesvarygreatlyinquality,it is\nimperative that we judge the ﬁtness of any alignments that we\nplan to use as ground truth transcriptions. Given a good evalu-ation metric, we could automatically search through thousands\nof available MIDI ﬁles to ﬁnd the useful ones, and hence gen-\nerate enough alignments to create a substantial corpus of real,\nwell-known pop music recordings with almost complete note-\nleveltranscriptionsalignedtoanaccuracyofmilliseconds. Thiscouldbeusedformanypurposes,suchastrainingarobustnon-\nparametricclassiﬁertoperformnotetranscriptionsfromrealau-\ndio.\nThe remainder of this paper is organizesas follows: In section\n2wepresent the methodology used to create MIDI/raw forced\nalignments. Following that is a discussion on evaluation of the\nfeatures used and an estimate of the ﬁtness of the alignments.Intheﬁnalsection, we detailthenextstepsofthiswork.\n2M e t h o dology\nInthissectionwedescribethetechniqueusedforaligningsongs\nwith their MIDI syntheses. A system overviewis shown in ﬁg-\nure 2. First, the MIDI ﬁle is synthesized to create an audio\nﬁle(syn). Second, the short-time spectral features (e.g. spec-\ntrogram) are com puted for both the syn and the original music\naudio ﬁle (raw). A similarity matrix is then created, provid-\ning the cosine distance between each frame iin raw and framejinsyn. We then employ dynamic programming to search\nfor the “best” path through the similarity matrix. To improveour search, we designed a two-stage alignment process, which\neffectively searches for high-order structure and then for ﬁne-\ngraindetails. Thealignmenti stheninterpolated,andeach note\nevent in the MIDI ﬁle is warped to obtain corresponding on-\nset/offset pairs in the raw audi o. Each stage of the process is\ndescribedbelow.\n2.1 AudioFile Preparation\nTo begin we start w ith two ﬁles, the raw audio of the song we\nwishtoalign, and a MIDI ﬁle correspondingto the song. Raw\naudio ﬁles are taken from compact disc, and MIDI ﬁles have\nbeen downloaded from of user pages found through the MIDI\nsearchengine\nhttp://www.musicrobot.com .T h eM I D Iﬁ l e\nis synthesized using WAVmaker III by Polyhedric Software.\nRather than simply recordin gt h eoutput of a hardware MIDI\nsynthesizeron a soundcard, WAVmakertakesgreatcare to en-\nsure that there is a strict correspondence between MIDI ticks\n(the base unit of MIDI timekeeping) and samples in the resyn-\nthesis. We can movefrom ticks to samples using the following\nconversion:\nNcur=Nbase+Fs·106·(Tcur−Tbase)(∆/PPQ)(1)\nwhereNcurandNbaseare the sample indices giving current\npositionandthetimeofthemostrecenttempochange,and Tcur\nandTbaseare the corresponding times in MIDI ticks. ∆is the\ncurrent tempo in quarters per microsecond, and PPQis the\ndivision parameter from the MIDI ﬁle, i.e. the number of ticks\nperquarternote.Fsisthesamplingrate.\nBoththerawand syn are downsampled to 22.05kHz and nor-\nmalized, with both stereo channels scaled and combined into a\nmonophonicsignal.\n2.2 FeatureCalculation\nThenextstepistocomputeshort-timefeaturesforeachwindow\nofsynandraw. SincethepitchtranscriptionisthestrongestcueFigure2: TheAlignmentProcess. Featuresco mputedonoriginalaudioandsynthesizedMIDIarecomparedinthesimilaritymatrix\n(SM).ThebestpaththroughSM isawarpingfromnoteeventsintheMIDIﬁle totheirexpectedoccurrenceintheoriginal.\nfor listeners to identify the approximate version of the song, it\nisnatural that we choose a representation that highlightspitch.\nThisrulesoutthepopularMel-FrequencyCepstralCoefﬁcients(MFCCs), which are speciﬁcally constructed to eliminate fun-\ndamentalperiodicityinformation,andpreserveonlybroadspec-\ntralstructuresuchasformants. AlthoughauthorsofMIDIrepli-\ncasmayseek to mimicthe timbralcharacterofdifferentinstru-\nments, suchapproximationsare suggestiveat best, andparticu-\nlarly weak for certain lines such as voice etc. We use featuresbased on a narrowbandspectrogram,which encapsulatesinfor-\nmation about the harmonic peaks within a frame. We apply\n2048 point (93 ms) hann-windowed FFTs to each frame, with\neach frame staggered by 1024 samples (46 ms), and then dis-\ncard all bins above 2.8 kHz, since they relate mainly to timbreandcontributelittletopitch. Asasafeguardagainstframescon-\nsistingofdigitalzero,whichhaveaninﬁnitevectorsimilarityto\nnonzeroframes,1%whitenoiseisaddedacrossthespectrum.\nInourexperimentswehaveaugmentedoralteredthewindowed\nFFT in one ormore of the followingways (valuesthatwe tried\nareshowninbraces):\n1.spec\npower{‘dB’, 0.3, 0 .5, 0.7,1 ,2}-Wehad initially\nused a logarithmic (dB) intensity axis for our spectro-gram, but had problems when small spectral magnitudes\nbecomelargenegativevalueswhenconvertedtodB.Inor-\nder to approximate the behavior of the dB scale withoutthis problem,we raised the FFT magnitudesto a powerof\nspec\npower. In our experiments we were able to perform\nthe best alignments by takingthesquareroot(0.5)orrais-\ningthespectrumt ot h e0.7power.\n2.diff\npoints{0, [1 16], [1 64], [224 256] }-F i r s to rder dif-\nferences in time of some channels are appended as addi-tional features. The goal here is to highlight onset times\nfor pitched notes using the low bins and for percussion\nusing the high bins. In our experiments, we found thatadding the high-bin time differences had negligible effect\nbecause of the strong presence of the snares and hi-hats\nacrosstheentirespectrum. Addinglow-orderbinscanim-provealignment,butmayalso causedetrimentalmisalign-\nmentincaseswheretheMIDIﬁlewastranscribedoff-key.\n3.freq\ndiff{0, 1}-F i r s t - order difference in frequency at-\ntempts to normalize away the smooth spectral deviations\nbetween the two signals (due to differences in instrument\ntimbres)a nd focusinstead onthe localharmonicstructure\nthatcharacterizesthe pitch.\n4.noise\nsupp{0, 1}-Noise suppression. Klapuri has pro-\nposed a methodforsimultaneouslyremovingadditiveandconvolutive noise through RASTA-style spectral process-\ning(Klapurietal.,2001). Thismethodwasnotassuccess-\nfulaswe hadhoped.\n2.3 The SimilarityMatrix\nAlignment is carried out on a similarity matrix (SM), based\non the self-s imilarity matrix used commonly in music analy-\nsis(Foote, 1999), but comparing all pairs of frames between\ntwosongs, instead of comparing one song against itself. We\ncalculate the distance between each time step i(1...N)i nt h e\nraw feature vector series spec\nrawwith each pointj(1...M)\nin the syn seriesspecsyn.Each value in theN×Mmatrix is\ncomputedasfollows:\nSM(i,j)=specraw(i)Tspecsyn(j)\n|specraw(i)||specsyn(j)|,\nfor0≤i<N,0≤j<M (2)\nThis metric is proportionalto the similarity of the frames, e.g.\nsimilar frames have a value close to 1 and dissimilar frames\nhave a value that can approach -1 .S e v e ral similarity matri-\nces for different songs are shown in ﬁgure 4, in which we can\nsee the structures that are also present in self-similarity matri-ces,suchasthecheckerboardpatterncorrespondingtosegment\nboundaries, and off-diagonal lines signifying repeated phrases\nBartschandWakeﬁeld(2001);DannenbergandHu(2002). The\npresenceofthe strongdiagonallineindicatesa goodalignment\nbetween the raw and syn ﬁles, a nd the nex ts t a g eattempts to\nﬁndthis a lignment.2.4 Alignment\nThe most crucial stage in the process of mapping from MIDI\nnote eventto raw audio sample is searching for the “best path”through the similarity matrix .This is accomplished with dy-\nnamic programming or DP (Gold and Morgan, 1999), tracing\nap a t hfrom the or igin to(N−1,M−1).D P i s u s e d i n\nmanyapplications,rangingfromthe DynamicTime Warp used\nin speech recognition, through to a ligning closely-related se-\nquencesofDNAoraminoacidsin bioinformatics.\nDP consists of two stages, forward and traceback. In the for-\nwardstep, we calculatethelowest-costpathtoall ofthepoint’s\nneighbors plus the cost to get from the neighbor to the point -\ninthiscase,S\nmax−SM(i,j),whereSmaxis thelargestvalue\nin the similarity matrix. This makes the cost for the most sim-\nilar frame pairs be zero, and all other frame pairs have larger,\npositivecosts. Theobjectofthisstageistocomputethecostof\nthebest pat htopoint(N−1,M−1)recursivelybysearching\nacross all allowable predecessors to each point and accumulat-ing cost from there. In the traceback stage, we ﬁnd the actual\npathitselfbyrecursivelylookingupthepointprovidingthebest\nantecedentforeachpointonthe path.\nWe have e xperimented with of two complementary ﬂavors of\nDP,whichwehavedubbed“unconstrained”and“greedy”. Fig-\nure3demonstratesthisonasmallscale,usingthealignmentof\nat h r e enote sequence. Panels (a) and (d) show the freedom of\nmovementallowedineachconﬁguration;inourDP,allallowed\nsteps have e qual weight. Panels (b) and (e) show the DP when\nthetranscriptionoft hesequenceiscorrect,andthethirdcolumn\nshows the behavior of the DP when the MIDI transcriptionhas\nthesecondan dthirdnotesswapped.\nUnconstrained DP (top row) allows the best path to include\nhorizontal and vertical segments ,thus allowing it to skip en-\ntire regions of origin al or resynthesis. When searching for the\nlowest-cost path on a SM that pe nalizes dissimilar frames, un-\nconstrained DP is capable of bypassing extraneous segmentssuchas repeated verses or mis-transcribed notes. However, in\nterms of ﬁne -grained structure, its pe rformance is weak, since\nit may incl ude short horizontal or vertical regions within indi-\nvidual notes when certain frame pairs align particularly well.\nGreedy DP (bottom row), on the o ther hand, enforces forward\nmotion by at least one frame on both axes at every step. Thisbehavior results in smooth structure when the notes are cor-\nrectlytranscribed;however,inthepresenceoferrors,thegreedy\nmethod will still ﬁnd a convin cing-looking r oughly-diagonal\npathevenwhennogoodoneexists(panel(f)ofﬁgure3).\n2.4.1 Two-stageAlignment\nWedeveloped a two stage alignment process exploiting the\ncomplementary DP ﬂavors. In the ﬁrst stage, we use uncon-\nstrained DP to discover the re gions of the song which appear\nlikely to align well, and to skip regions that contain no good\ndiagonal paths. We then apply a 31-point (1.4 s) median ﬁlter\nacrosstheslopeofthebestpathfoundbyunconstrainedDP.Themedian ﬁlter searches for contiguousregions in which the ma-\njority of steps are diagonal, as these segments are most likely\nable to be aligned rather than skipping over misalignments.Straightsegmentsofanysigniﬁcantlengthbecomeboundaries,\nand greedy DP is applied in bet ween them to re-align these\n‘good’segmentsmoreaccurately.2.5 Inter polation\nOnce alignment is complete, we can treat the best path as a\nwarpingf unctionbetweensyn samplesandrawsamples. In or-\nderto achievesub-frameresolution,the bestpathislinearlyin-\nterpolatedandsmoothedacrossmultiplealignmentframepairs.\nTo demonstrate the warping by the best path, we created two\nsets of stereo res yntheses with raw in one ear against syn in\nthe other. The ﬁrst simply plays each frame of raw/syn along\nthebest path, without taking too much care to smooth tran-\nsitions between discontiguous frames. The second plays the\nunmodiﬁed raw in one channel, and MIDI synthesis based\non the time-warped MIDI notes in the other. These exam-\nples are available at the MIDIAlign project webpage,\nhttp:\n//www.ee.columbia.edu/˜rob/midialign/ .\n3F e a t u r e s e t a nd Alignment E valuation\nTheprimaryobjectiveofthesealignmentsistocreatealargeset\nof transcribed real-world polyphonic music. Given the murky\noriginsofmanyoftheMIDIﬁlesweencounter,therearealign-\nmentsthat,simplyput,donotwork;ﬁgure4givesfourexamplealignments, illustrating some of the problems we encountered.\nWhile the alignment algorithm is tolerant of missing sections\nand a certain amount of ornamentation or interpretation, othererrorswilldefeatitcompletely. Wehaveencounteredtranscrip-\ntions that are tra nsposed, with segments transcribed out of or-\nder,ofadifferentremix/versionofthesong,orwhicharesimplyabysmal. OurwebcrawlinghasuncoveredMIDItranscriptions\nfor thousandsof songs, with a handfulof differentversionsfor\neach, and hence the importance of an automatic metric to dis-\ncriminatethe successfulalignmentscannotbeoverstated.\n3.1 Evaluationoffeaturesets\nIn order to evaluate different candidate metrics, we ﬁrst com-\nputedthealignmentfor7differentfeaturesetsoneachofthe40songs in our corpus. The songs were chosen to include some\nvarietyintermsofdifferenttypesofpopularmusic,andnotbe-\ncause of the quality of their transcriptions. We then manually\nevaluated 560 alignments, two for each feature for each song\ncorresponding to the best path found by the ﬁrst and secondstages. Manual evaluation was our only choice, initially, since\nwehadnogroundtruthavailableoftime-pointcorrespondences\nbetween originalrecordingsan dt h e i rM I D I renditions .L i sten-\ningtothealignedresyntheseswasconsideredlessarduousthan\nannotating the set of music to create this ground truth. We au-\nrallyevaluatedtherenderedcomparisonsonthefollowingscale:\nNote that this score encapsulates both the performance of the\nalgorithm and the quality of the transcription. The Count col-\numnisthenumberofsongsthatreachedthislevelofevaluation\nas the highest score across all of the features. Thus, for 31 ofthe40MIDI/rawpairs,ourautomaticalignmentgaveusatleast\noneperfectornear-perfectalignment. Thesecouldbeextracted\nand used as a manually-veriﬁed ground truth for further align-mentexperiments,althoughwe have notusedthem this way as\nyet. They are of course by deﬁnition the examples on which\nautomaticanalysisis mostlikelytosucceed.\nWecan use these annotations to determine which featureset is\nmost effective. Table 2 charts each featureset that we’ve used\nvs.thenumberofoccurrences of each subjective quality label\nfor alignments based on that featureset. From the table, it canUnconstrained Greedy(a)(b) (c)\n(d)\n(e) (f)\nFigure 3: Flavors of DP. The top row is for unconstrainedDP, the bottom row is for greedy. The ﬁrst column shows the allowable\ndirections of movement. The second column shows the alignment of raw/MIDI notes C5-C /sharp5-D5, for unconstrained and greedy\nrespectively. Thethirdcolumnshowsthe alignmentofraw asbefore,butwiththe lasttwonotesofMIDIreversed.\n(a) (b)\n(c) (d)\n500 10001000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\n9000\n10000\n1500 2000 2500 3000 3500 4000 4500 5000\nSYN frames500 1000 1500 2000 2500 3000 3500 4000\nSYN frames500 1000 1500 2000 2500 3000 3500 4000 4500 5000\nSYN frames500 1000 1500 2000 2500 3000 3500 4000 4500 5000\nSYN framesRAW frames500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\n5000RAW frames\n500\n1000\n1500\n2000\n2500\n30003500\n4000RAW frames1000\n2000\n3000\n4000\n5000\n6000RAW frames\nAlignment: Temple of Love - Sisters of Mercy Alignment: 9pm Till I Come - ATBAlignment: Don't You Want Me - Human League Alignment: Africa - Toto\nFigure 4: Example alignments for four SMs: (a) Don’t You Want Me (Human League): straightforward alignment; (b) Africa\n(Toto): bridgemissing in transcription;(c) Temple of Love(Sisters of Mercy): Transcriptionout of order but we recover; (d) 9pm’Till I Come (ATB): poor alignment mistakenly appears successful .White rectangles indicate the diagonal regions found by the\nmedianﬁlter andpassedto thesecond-stagealignment.Score\nInterpretation\n Count/40\n1\nCould not align anything. Not for\nuseintraining.\n4\n2\nSome alignment but with timing\nproblems, transposition errors, or\nal a r g enumber of misaligned sec-\ntions.\n2\n3\nAverage alignment. Most sections\nlineupbutwithsomeglitching. Not\nextremely detrimental to training,\nbut unlikelyto helpeither.\n3\n4\nVery good alignment. Near perfect,\nthough with some errors. Adequate\nfortraining.\n4\n5\nPerfectalignment.\n 27\nTable 1: S ubject alignment assessment. Different versions of\nMIDI syntheses aligned to original audio were auditioned in\nstereo against the original recording, and given a score from 1\nto 5, indicating the combined quality of the original MIDI ﬁle\nandthealignmentprocess.\nScore\nFeatureset\n 12 345\nspec\npower=0.5\n 627 61 5\nspec\npower=0.7\n 6341 0 1 4\nspec\npower=1.0\n 641 35 9\nspec\npower=0.7diff\n points[116]\n 5641 1 1 3\nspec\npower=0.7diff\n points[164]\n 670 42 1\nspec\npower=0.7freq\n diff=1\n 7561 14\nspec\npower=0.7noise\n supp=1\n 28 1 010\nTable 2: F eature Evaluation. Above is a representative list of\nfeaturesets which were applied to alignment on our corpus of\nmusic. Columns1through5indicatethenumberofsongseval-\nuated at that score for that featureset. Alignments rated 4 or 5areconsideredsuitablefortraining.\nbe seen that spec\npower = .5 or .7 and its variants lead to the\nmost frequentoccurrence of good alignments, with the highest\nincidence of alignments rated at “5” for spec\n power = .7 and\ndiff\npoints=[164].\n3.2 AutomaticAlignmentEvaluation\nGiventhe highcostsontheattentionofthesubjecttoobtainthe\nsubjective alignment quality evaluations,our goal is to deﬁne a\nmetric able automaticallyto evaluatethe qualityof alignments.\nOursubjectiveresults bootstrapthis by allowingus to compare\nthose results with th eoutput of candidate automatic measures.\nInordertojudgetheﬁtnessofanalignmentaspartofatraining\nset,wehavee xperimentedwithanumberofmetricstoevaluate\ntheir ability to discriminate between good and bad alignments.\nTheyare:\n1. AverageBest Path Score- The averagevalueofSM along\nthebest path,in thediagonalregionsfoundbythe median\nﬁlter. MIDI resyntheses that closely resemble the origi-nals will have a large number of very similar frames, and\nthebest path should pass through many of these. How-\never,sourcesofdissimilaritysuchastimbralmismatchwillbe reﬂected in this value, without necessarily indicating a\nweakalignment.\n2.AverageBestPathPercentile-Theaveragesimilarityscore\nofpairsincludedinthebestpath,expressedasapercentile\nof all pair sim ilarity scores in the SM. By expressing this\nvalue as a percentile, it is normalized relative to the ac-\ntual variation (in range and offset) of similarities within\ntheSM.\n3. Off-diagonalRatio-Basedonthenotionthatagoodalign-\nment chooses points with low cost among a sea of medi-\nocritywhile abadalignmentwillsimplychoosetheshort-estpathtotheend,theoff-diagonalratiotakestheaverage\nvalue of SM a longthebestpat handdividesitbytheaver-\nage value of a path offsetby a few frames. It is a measureofthe‘sharpness’oftheoptimumfoundbyDP.\n4. Square Ratio - The ratio of areas of the segments used in\nsecond stage alignment (corresponding to approximately\nlinearwarping)asaproportionofthetotalareaoftheSM.\nFirst stage alignments that de leted signiﬁ cant portions of\neitherversionwillreducethisvalue,indicatingaproblem-\natic alignment.\n5. LineRatio-Theratioofthea pproximate lylinearsegments\nofthealignmentversusthelengthofthebestpath,i.e. ap-\nproximately√\nSquareRatio .\nOurexperimentshaveshownthatthemetric2,theaveragebest\npath percentile, demonstrates the most discrimination between\ngood and bad alignments. When automatically evaluating a\nlarge corpus of music, it is best to use a pessimistic evaluation\ncriterion, to reduce false positives which can corrupta training\nset. Figure 5 shows example scatter plots of automatic quality\nmetric versus subjective quality rating for this metric with two\ndifferentfeaturebases;whilenoperfectthresholdexists,itisat\nleast possible to exclude most poor alignments while retaining\nareasonableproportionofgoodonesforthetrainingset.\n4D i s c u ssionandconclusions\nAlthough we are only at the beginningof this work, we intend\nin the immediate f uture to use this newly-labeled data to cre-\nate note detectors via supervised, model-free machine learningtechniques. GiventheenormousamountofMIDIﬁlesavailable\nontheweb,itiswithintherealmofpossibilitytocreateadataset\nof onlythe MIDI ﬁles that havenear-exactalignments. We canthen note-detecting classiﬁers based on these alignments, and\ncomparetheseresultswith other,model-basedmethods.\nAdditionally, we are investigating better features for align-\nment and auto-evaluation. One such feature involves explic-itly searching for overtones expected to be present in the raw\ngiven an alignment. Another method for auto-evaluation is to\nuse the transcriptio nc l assiﬁers that we will be creating to re-\ntranscribe the training examp les, pruning those whose MIDI\nandautomatictranscriptionsdifferssigniﬁcantly.\nSince MIDI t ranscriptions only identify the attack and sustain\nportion of a note, leaving the d ecay portion that follows the\n‘note off’ event to be determined by the particular voice and\notherparameters,we willneed tobecarefulwhen trainingnote\ndetectorsnottoassumethatfra mesdirec tlyfollowinganoteoff55 60 65 70 75 80 85 90 9510012345Avg. Best Path Percentile: \nspec power 0.7Avg. Best Path Percentile: \nspec power 0.7 - diff points [1 64]\nBest Path Percentile Best Path PercentileAlignment Ranking\n55 60 65 70 75 80 85 90 9510012345Alignment Ranking\nFigure 5: Comparison between subjective and automatic alignment quality indices for the average best path percentile, our most\npromising automatic predicto rofalignment quality. Shown are equal risk cutoff poi nts for two good-performingfeaturesets, with\ngoodalignmentslyingto therightofthethreshold.\ndo notcontain anyevidenceof that note. Of course, our classi-\nﬁerswill havetolearntohandler everberationandothereffects\npresentinrealrecordings.\nThereisalsoaquestionofbalanceandbiasinthekindoftrain-\ningdatawewillobtain. Wehavenoticedadistinctbiastowardsmid-1980s pop music in the available MIDI ﬁles: this is the\ntime whenmusic/com puterequipmentneededto generatesuch\nrenditionsﬁrst became accessi ble to a mass hobbyistaudience,\nand we suspect many of the ﬁles we are using date from that\ninitial ﬂowering of enthusiasm. That said, the many thousandsof MIDI ﬁles available online do at least providesome level of\ncoveragefor a pretty broadrange of mainstream pop music for\nthepastseveraldecades.\nGiventhat the kind of dynamic time warping we are perform-\ningisbest known as the technique that was roundly defeated\nbyhiddenMarkovmodels(HMMs)forspeechrecognition,itis\nworth considering whether HMMs might be of some use here.\nInfact, recognition with HMMs is very often performed using\n“Viterbi decoding”, which is just another instance of dynamic\nprogramming. The main strength of HMMs lies in their abil-ity to integrate (through traini ng) multiple example instances\ninto a single, probabilistic model of a general class such as a\nword or a phoneme. This is precisely what is neededin speech\nrecognition, where you want a model for /ah/ that generalizes\nthemanyvariationsin yourtrainingdata, butnotat allthe case\nhere,wherewehaveonlyasingleinstanceoforiginalandMIDIresynthesis that we are trying to link. Although our cosinedis-\ntance could be improved with a probabilistic distance metric\nthatweightedfeaturedimensi onsandcovariationsaccordingto\nhowsigniﬁcanttheyappearin pastexamples,onthe wholethis\napplicationseemsperfectly suitedto theDT Wapproach.\nOverall,weareconﬁde ntthatthisapproachcanbeusedtogen-\nerateMIDItranscriptionsofreal ,commercialpopmusicrecord-\nings, mostly accurate to within tens of milliseconds, and that\nenoughexamplesareavailabletoproduceasmuchtrainingdata\nas we are likely to be able to take advantage of, at least for thetime being. We anticipate other interesting applications of this\ncorpus, such as convenient symbolic indexing into transcribed\nﬁles,retrievalofisolatedinstru menttonesfromrealrecordings,\netc. We welcome any other suggestions for further uses of this\ndata.Acknowledgments\nOur thanks go to the anonymousreviewers for their comments\nwhichwereveryhelpful.\nReferences\nBartsch,M.A.andWakeﬁeld,G.H.(2001). Tocatchachorus:\nUsingchr oma-basedrepresentationsforaudiothumbnailing. In\nProc. IEEE Workshop on Applications of Signal Processing to\nAudioandAcoustics ,Mohonk,NewYork.\nDannenberg, R. and Hu, N. (2002). Pattern discovery tech-\nniquesformusicaudio. InFingerhut,M.,editor, Proc.ThirdIn-\nternationalConferenceonMusicInformationRetrievalISMIR-\n02,pages63–70,Paris. IRCAM.\nFoote, J. (1999). Methods for the automatic analysis of music\nandaudio. Technicalreport,FX-PAL.\nGold,B.andMorgan,N.(1999). SpeechandAudioSignalPro-\ncessing: ProcessingandPerceptionofSpeechandMusic .John\nWiley&Sons ,Inc.,NewY ork.\nGoto,M.(2000). A robust predominant-f0estimation method\nfor real-time detection of melody and bass lines in cd record-\nings. InIEEE International Conference on Acoustics, Speech\nandSignalProcessing ICASSP-2000 ,pagesII–757–760,Istan-\nbul.\nGoto,M., Hashiguchi, H., Nishimura, T., and Oka, R. (2002).\nRwcmusic database: Popular, classical, and jazz music\ndatabases. In Fingerhut, M., editor, Proc. Third International\nConference on Music Information Retrieval ISMIR-02 ,P a r i s .\nIRCAM.\nKlapuri, A., Vir tanen, T., Eronen, A., and Sepp¨ anen, J. (2001).\nAutomatic transcription of musical recordings. In Proceedings\noftheCRAC-2001workshop .\nWalmsley, P. J., Godsill, S. J., and Rayner, P. J. W. (1999).\nBayesian graphical models for polyphonic pitch tracking. In\nProc.DiderotForum ."
    },
    {
        "title": "Using transportation distances for measuring melodic similarity.",
        "author": [
            "Rainer Typke",
            "Panos Giannopoulos",
            "Remco C. Veltkamp",
            "Frans Wiering",
            "René van Oostrum"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417513",
        "url": "https://doi.org/10.5281/zenodo.1417513",
        "ee": "https://zenodo.org/records/1417513/files/TypkeGVWO03.pdf",
        "abstract": "Most of the existing methods for measuring melodic similarity use one-dimensional textual representa- tions of music notation, so that melodic similarity can be measured by calculating editing distances. We view notes as weighted points in a two-dimensional space, with the coordinates of the points reflecting the pitch and onset time of notes and the weights of points depending on the corresponding notes’ duration and importance. This enables us to measure similarity by using the Earth Mover’s Distance (EMD) and the Pro- portional Transportation Distance (PTD), a pseudo- metric for weighted point sets which is based on the EMD. A comparison of our experiment results with earlier work shows that by using weighted point sets and the EMD/PTD instead of Howard’s method (1998) using the DARMS encoding for determining melodic similarity, it is possible to group together about twice as many known occurrences of a melody within the RISM A/II collection. Also, the percentage of successfully identified authors of anonymous in- cipits can almost be doubled by comparing weighted point sets instead of looking for identical representa- tions in Plaine & Easie encoding as Schlichte did in 1990. 1",
        "zenodo_id": 1417513,
        "dblp_key": "conf/ismir/TypkeGVWO03",
        "keywords": [
            "notes",
            "pitch",
            "onset time",
            "notes",
            "duration",
            "weighting",
            "coordinates",
            "pitch",
            "onset time",
            "weights"
        ],
        "content": "UsingTransportation Distances forMeasuring MelodicSimilarity\nRainer Typke,Panos Giannopoulos, Remco C.Veltkamp, Frans Wiering, Ren´evanOostrum\nInstitute ofInformation andComputing Sciences\nUniversity ofUtrecht\n3584CH Utrecht, TheNetherlands\n+31-(0)30-253117 2\u0000\nrainer.typke,panos,re mco.veltkamp,frans.wiering,rene\u0001@cs.uu.nl\nAbstract\nMost oftheexisting methods formeasuring melodic\nsimilarity use one-dimensional textual representa-\ntions ofmusic notation, sothat melodic similarity\ncanbemeasured bycalculating editing distances. We\nviewnotes asweighted points inatwo-dimensional\nspace, with thecoordinates ofthepoints reﬂecting the\npitch andonset time ofnotes andtheweights ofpoints\ndepending onthecorresponding notes’ duration and\nimportance. This enables ustomeasure similarity by\nusing theEarth Mover’sDistance (EMD) andthePro-\nportional Transportation Distance (PTD), apseudo-\nmetric forweighted point sets which isbased on\ntheEMD. Acomparison ofourexperiment results\nwith earlier workshowsthatbyusing weighted point\nsetsandtheEMD/PTD instead ofHoward’smethod\n(1998) using theDARMS encoding fordetermining\nmelodic similarity ,itispossible togroup together\nabout twice asmanyknownoccurrences ofamelody\nwithin theRISM A/IIcollection. Also, thepercentage\nofsuccessfully identiﬁed authors ofanon ymous in-\ncipits canalmost bedoubled bycomparing weighted\npoint setsinstead oflooking foridentical representa-\ntions inPlaine &Easie encoding asSchlichte didin\n1990.\n1Introduction\nRepresenting music asaweighted point setinatwo-dimen-\nsional space hasatradition ofmanycenturies. Eversince the\n13th century ,music hasbeen written asasetofnotes (points)\ninatwo-dimensional space, with time andpitch ascoordinates.\nVarying characteristics areassociated with thenotes by,forex-\nample, using different symbols fordifferent note durations. The\nlook ofwritten music haschanged some what overthelast8cen-\nturies, butthebasic idea ofrepresenting music asaweighted\npoint sethasbeen follo wed foralmost amillenium, andithas\nservedcomposers andperformers well. Since weighted point\nPermission tomakedigital orhard copies ofallorpartofthiswork\nforpersonal orclassroom useisgranted without feeprovided that\ncopies arenotmade ordistrib uted forproﬁt orcommercial advan-\ntage andthatcopies bear thisnotice andthefullcitation ontheﬁrst\npage. c\n\u00022003 Johns Hopkins University .sets seem tobesowell suited torepresenting music, itfeels\nnatural tomeasure melodic similarity directly bycomparing\nweighted point setsinstead ofﬁrsttransforming themusic into\none-dimensional abstract representations.\nWestudied theuseoftheEarth Mover’sDistance (EMD), which\nmeasures aminimum ﬂowfortransforming oneweighted point\nsetintoanother ,forthepurpose ofmeasuring melodic similar -\nity.Because thetriangle inequality does nothold fortheEMD,\nwealso used amodiﬁed version ofit,theProportional Trans-\nportation Distance (PTD), which wasproposed byGiannopou-\nlosandVeltkamp (2002). Adistance measure forwhich the\ntriangle inequality holds canbeused tomakedatabase searches\nmore efﬁcient byusing indices.\nThemusic database weused forevaluating theEMD andPTD\nassimilarity measures contains about halfamillion musical in-\ncipits1from theRISM A/II collection (R´epertoire International\ndesSources Musicales, 1995-2002).\nWeevaluated theappropriateness oftheEMD and PTD for\nmeasuring melodic similarity byconstructing groups ofsimi-\nlarmelodies within theRISM A/II collection andcomparing\nourresults tothe“Frankfurt Experience” and“Harv ardExpe-\nrience” ofsorting RISM incipits described byJohn B.Howard\n(1998). Wewere able toidentify about twice thepercentage of\nmelodies byanon ymous composers andgroup together 76%in-\nstead of46%oftheknownoccurrences ofatune called “Roslin\nCastle”.\n2Melodiesasweightedpointsets\nJohannes Brahms: Himmel strahlt sohelle undklar(Zigeunerlieder)\u0003 \u0004 \u0005\u0006\u0006 \u0007\b\n\b\n\b\t\n\b\b\u000b\n\t\n\b\n\b\n\n\u0006\u0006\n\u0004 \u0005 \f \u0003\n(0,221;1.5)\n(1.5, 209;0.5)\n(2,204;1.5)\n(3.5, 215;0.5) (4,209;0.5)(4.5, 198;0.5)(5,186;1)(6,192;2)\nFigure 1:Anexample ofmusic represented with aweighted\npoint set.Format: (Time, Pitch; Weight ).Inthisexample, the\nweights only reﬂect thenote durations. Because ofthis, the\ntime coordinate here equals thesum oftheweights ofpreced-\ningnotes. Pitches arespeciﬁed using Hewlett’ s(1992) base-40\nsystem.\nInorder tobeable toapply atransportation distance measure,\nwemust transform themelodies wewanttocompare intosig-\n1Incipits arethebeginnings ofpieces, typically about 20notes long.natures. Bysignature, wemean asetofpoints inthetwo-\ndimensional Euclidean space where each point hasaweight as-\nsociated with it.Thetwodimensions aretime andpitch.\nWhen transforming melodies into signatures, wecreate one\npoint foreach note. Rests areencoded implicitly asthetime\nspans thatarenotcovered bypoints. Asaconsequence, wedo\nnotdistinguish between twosubsequent quarter rests andone\nhalfrest, butwedodistinguish between twosubsequent quarter\nnotes andahalfnote; only thelatter sounds differently .\n2.1 The time coordinate\nInourdatabase, durations ofnotes andtheir positions within\nmeasures arespeciﬁed using divisions ofaquarter note, ina\nwaysimilar totheMIDI format. Witheverymelody ,thenum-\nberofdivisions perquarter note isstored. This number ischo-\nsensuch thattheduration ofeverynote inthemelody canbe\nspeciﬁed asawhole number .Forexample, ifthere are96di-\nvisions perquarter note, aquarter note hasduration 96,ahalf\nnote hasduration 192, andasixteenth 24.\nWewanttime coordinates insignatures tobeindependent of\nthenumber ofdivisions chosen foraparticular melody .There-\nfore, wecalculate thetime coordinate ofanote asthesum of\nthelengths ofmeasures preceding thenote plus thenote’ spo-\nsition within itsmeasure, divided bythenumber ofdivisions\nforaquarter note. Measure lengths arecalculated asfollo ws:\nforeach note orrestinameasure, theduration isadded tothe\nposition within themeasure. Themaximum ofallofthese end\npoints ofnotes andrests isthen takenasthemeasure length.\nInorder toskip leading rests –wedonotwanttodistinguish\nbetween melodies that differonly intheduration ofleading\nrests –,wethen subtract theveryﬁrst note’ stime coordinate\nfrom alltime coordinates, thereby shifting allnotes sothatthe\nﬁrstnote starts attime 0.\nForacomplete example, seeFigure 1andTable 1.\nNote\nNumberMeasure\nNumberPitch40 DurationPosition\ninbar\n1 1 0 1920 0\n2 2 221 1440 0\n3 2 209 480 1440\n4 3 204 1440 0\n5 3 215 480 1440\n6 4 209 480 0\n7 4 198 480 480\n8 4 186 960 960\n9 5 192 1920 0\nTable 1:Thedatabase contents forthemelody showninFigure\n1.There are960divisions perquarter note, andrests arecoded\nasnotes with pitch 0.ToarriveatFigure 1,weﬁrstnormalize\nthetime coordinates (i.e.,divide them by960). Thedurations\narethen: 1920/960=2, 1.5,0.5,1.5,0.5,0.5,0.5,1,2.Allmea-\nsure lengths are1920/960=2. Therefore, thenote onset times\nare:0,2,3.5,4,5.5,6,6.5,7,and8.This stillincludes thelead-\ningrest, which wewanttoignore, soﬁnally ,weskip theleading\nrestandsubtract itsduration from allsubseqent notes: 0,1.5,2,\n3.5,4,4.5,5,6.These arethetime coordinates inFigure 1.\nOurmethod ofdetermining thelength ofeach measure without\nrelying onthetime signature ensures thatwegetsensible coor-dinates evenincases where thenotes inameasure donotmatch\nthetime signature. This actually happens with theRISM data.\nSee, forexample, thebottom right incipit inFigure 5,where\nnotonly theoctavesareencoded incorrectly forsome notes, but\nthere isalso amismatch ofthetime signature andthecontents\nofmeasures.\n2.2 The pitch coordinate\nUnlik eMIDI ﬁles, ourdatabase contains thepitch inWalter\nHewlett’ s(1992) Base-40 notation. This notation distinguishes\nbetween notes with thesame pitch, butdifferent notations. Like\nMIDI pitches, itisanumber -line representation ofmusical\npitch notation, butwith theadded advantage ofbeing interv al-\ninvariant. I.e.,thedifference between anytwobase-40 pitch\nnumbers willcorrectly determine thenotated interv alname be-\ntween those pitches.\n2.3 Weights\nIncreasing anote’ sweight increases theimportance ofithav-\ningacounterpart ofsimilar weight atthesame position inthe\ncompared melody .Anatural method ofusing weights isto\nmakethem reﬂect note durations. That way,differing note du-\nrations atcorresponding positions lead toanincrease inthere-\nsulting distance. Forinstance, inFigure 1thenote weights re-\nﬂect only thedurations. Allresults inthispaper were obtained\nwith weights thatonly depend onnote durations. Byadding\nmore components, however,additional desirable effects could\nbeachie ved. Twopromising weight components arestress\nweight andnote number weight.\n2.3.1 StressWeight\nThere arecases where melodies clearly differ,butadistance\nmeasure which ignores thepositions ofnotes within mea-\nsures failstodistinguish between them. Forexample, thetwo\nmelodies inFigure 2would notbedistinguished bythesimple\ndistance measures used forFigure 5.Byadding more weight\ntonotes atpositions inmeasures which areusually emphasized,\ne.g.theﬁrstbeat, themeasure structure canbetakenintoac-\ncount aswell.\nJean-Baptiste Lully: LaGrotte deVersailles\u0000 \u0001 \u0002\u0003\n\u0003 \u0004\u0005\n\u0005\u0006\n\u0005\n\u0003\n\u0003\n\u0001 \u0002\u0005\b\u0007\n\u0000\nAnon ymus: Litanies (Coro, without title)\t \n \u000b\f\f\n\f \r\u000e\n\u000e\u000f\n\u000e\n\f\f\n\f\n \u000b\u000e\b\u0010\n\t\nFigure 2:Byadding astress-based weight component, thedis-\ntance measure canbemade toreﬂect different measure struc-\ntures. Without that, thedistance would bezero forthese clearly\ndifferent melodies, provided thattranspositions areallowed.\n2.3.2 Note Number Weight\nIntheRISM database, there arenoclear rules about howmany\nnotes areincluded intheincipits. Therefore, ithappens that\nverysimilar oridentical melodies differmainly inthenumber\nofnotes thatareincluded intheincipit. Asweshall seelater,\nforexample intheright column ofFigure 5,there areinstances\nwhere thedistance between melodies becomes verylargebe-\ncause oneofthem iscutoffafter fewernotes, notbecause they\ncontain verydifferent musical material. One possible wayof\naddressing thisproblem istoaddanextraweight component to\neach note thatdepends onhowmanynotes precede it.That way,notes close tothebeginning aremade more important than extra\nnotes attheendwhich might notbepresent inalloccurrences\nofamelody inthedatabase.\nInSection 4.2,wewilldescribe some adjustments ofthesigna-\ntures which wedobefore applying adistance measure.\n3Similarity MeasuresforWeightedPointSets\nForasimilarity measure (formally speaking, afunction onaset\nS,d:S \u0000S \u0001\n\u0002\u0004\u0003\u0006\u0005\n\u00000 \u0001),thefollo wing properties areusually\ndesirable:\ni.Self-identity :Forallx\u0007S\bd\tx\bx\n\f\u000b0.\nii.Positivity :Forallx \r \u000byinS \bd \tx \by \n\u000f\u000e0.\niii.Symmetry :Forallx \by \u0007S \bd \tx \by \n\u0010\u000bd \ty \bx \n.\niv.Triangle inequality :Forallx\by\bz\u0007S,d\tx\bz\n\u0012\u0011d\tx\by\n\u0014\u0013\nd \ty \bz \n.\nAmeasure with allofthese properties iscalled ametric, while\nameasure with only properties i,iii,andiviscalled apseudo-\nmetric. Depending ontheapplication, different properties are\nrelevant.Formeasuring melodic similarity ,weneed self-identi-\ntyandsymmetry .Thetriangle inequality isuseful forefﬁciently\nsearching thedatabase (Barros etal.,1996). Positi vity isnot\nnecessarily alwaysdesired. TheEMD’ spartial matching prop-\nerty,which isclosely related toitslackofpositi vity(seeSection\n3.1.2), canbeuseful.\nInthefollo wing subsections, wewill describe thetwotrans-\nportation distances which weused.\n3.1 The Earth Mover’sDistance (EMD)\nThe Earth Mover’sDistance between twoweighted point sets\nmeasures theminimum amount ofworkneeded totransform\noneinto theother bymoving weight. Intuiti velyspeaking, a\nweighted point canbeseen asanamount ofearth ormass; al-\nternati velyitcanbetakenasanempty hole with acertain ca-\npacity .Wecanarbitrarily assign theroleofthesupplier toone\nsetandthatoftherecei ver/demander totheother one, setting,\ninthatway,thedirection ofweight movement. TheEMD then\nmeasures theminimum amount ofworkneeded toﬁlltheholes\nwith earth (measured inweight units multiplied with thecov-\nered ground distance). SeeCohen’ sPh.D. thesis (1999) fora\nmore detailed description oftheEMD.\n3.1.1 Deﬁnition\nLetA\u000b\n\u0000a1\n\ba2\n\b\u0016\u0015\u0017\u0015\u0017\bam\n\u0001beaweighted point setsuch thatai\n\u000b\u0000\txi\n\bwi\n\n \u0001\u0018\bi\u000b1\b\u0019\u0015\u001a\u0015\u0017\bm\bwhere xi\n\u0007\n\u0002kwith wi\n\u0007\n\u0002\n\u0003\u0005\n\u00000\u0001being\nitscorresponding weight. LetW=∑n\nj \u001b1wibethetotal weight of\nsetA.\nTheEMD canbeformulated asalinear programming problem.\nGiventwoweighted point setsA\bBandaground distance d,we\ndenote asfijtheelementary ﬂowofweight from xitoyjoverthe\ndistance dij.IfW\bUarethetotal weights ofA\bBrespecti vely,\nthesetofallpossible ﬂowsF\u000b\u001d\u001cfij \u001eisdeﬁned bythefollo wing\nconstraints:\n1.fij \u001f0 \bi \u000b1 \b\u0016\u0015\u0017\u0015\u001a\u0015\u0017\bm \bj \u000b1 \b\u0016\u0015\u0017\u0015\u001a\u0015\u0017\bn\n2.∑n\nj\u001b1fij\n\u0011wi\n\bi \u000b1 \b\u0016\u0015\u0017\u0015\u001a\u0015\u0017\bm3.∑m\ni \u001b1fij\n\u0011uj\n\bj\u000b1\b\u0016\u0015\u0017\u0015\u001a\u0015\u0017\bn\n4.∑m\ni \u001b1∑n\nj \u001b1fij\n\u000bmin \tW \bU \nThese constraints saythateach particular ﬂowisnon-ne gative,\nnopoint from the“supplier” setemits more weight than ithas,\nandnopoint from the“recei ver”recei vesmore weight than it\nneeds. Finally ,thetotal transported weight istheminimum of\nthetotal weights ofthetwosets.\nThe ﬂowofweight fijoveradistance dijispenalized byits\nproduct with thisdistance. Thesum ofallthese individual prod-\nuctsisthetotal costfortransforming AintoB.TheEMD( A,B)\nisdeﬁned astheminimum total costoverF,normalized bythe\nweight ofthelighter set;aunitofcost orworkcorresponds to\ntransporting oneunitofweight overoneunitofground distance.\nThat is:\nEMD\tA\bB\n\f\u000bminF  F∑m\ni\u001b1∑n\nj\u001b1fijdij\nmin \tW \bU \n3.1.2 Properties andComputation\nThemost important properties oftheEMD canbesummarized\nasfollo ws:\n1.TheEMD isametric iftheground distance isametric and\niftheEMD isapplied onthespace ofequal total weight\nsets.\n2.Itiscontinuous, inother words, inﬁnitesimal small\nchanges inposition and/or weight ofexisting points cause\nonly inﬁnitesimal change intheitsvalue. Moreo ver,the\naddition ofapoint with anarbitrarily small weight, i.e.\nnoise (which canbeseen asincreasing itsweight from zero\ntoapositi vevalue) leads toanarbitrarily small change in\ntheEMD’ svalue.\n3.Itdoes notobeythepositi vity property ifthesums of\ntheweights ofthetwosets arenotequal. Inthat case,\nsome oftheweight oftheheavierdistrib ution remains un-\nmatched. Therefore, theEMD allowsforpartial match-\ning. Asaresult, there arecases where itdoes notdistin-\nguish between twonon-identical sets. Sometimes thiscan\nbeuseful, forexample when twoincipits contain identical\nmelodies which arecutoffafter different numbers ofnotes.\nOntheother hand, thisalsoleads toeffects liketheonewe\nseewith incipit number 12intheleftcolumn ofFigure 5,\nwhere theEMD yields arelati velylowdistance. Here the\nsurplus ofweight isnotallconcentrated attheendofthe\nmelody ,butdistrib uted overseveralrests andother places,\nwhich leads toafalsepositi ve.\n4.Inthecase ofunequal total weights, theEMD does not\nobeythetriangle inequality .Asimple countere xample\nwould bethree melodies called A,B,andAB. Letusas-\nsume thatABistheconcatenation ofAandB,andletus\nassume thatAandBarechosen sothattheEMD yields\nadistance of1between them. IfAandBarepositioned\naccordingly ,both thedistance between AandABandthe\ndistance between BandABcanbezero (because both A\nandBareparts ofAB). Then, d \tA,B \n!\u000e d \tA,AB \n\"\u0013\nd \tAB, B \n.Asaresult, methods thatrelyonthetriangle inequality for\nspeeding updatabase retrie valcannot beused inconjunc-\ntionwith theEMD.\nThe EMD canbecomputed efﬁciently bysolving thecorre-\nsponding linear programming problem, forexample byusing a\nstreamlined version ofthesimple xalgorithm forthetransporta-\ntionproblem (Hillier andLieberman 1990). Weused Rubner’ s\n(1998) EMD function, which implements Hillier’ sandLieber -\nman’ salgorithm. Itispossible thatthesimple xalgorithm per-\nforms anexponential number ofsteps. One could usepolyno-\nmial algorithms likeaninterior point algorithm, butinpractice\nthatwould outperform thesimple xalgorithm only forverylarge\nproblem sizes. Since thetransportation problem isaspecial case\noftheminimum cost ﬂowproblem innetw orks, apolynomial\ntime algorithm forthatcould beused aswell.\n3.2 The Proportional Transportation Distance (PTD)\nGiannopoulos andVeltkamp (2002) proposed amodiﬁcation of\ntheEMD inorder togetasimilarity measure based onweight\ntransportation such thatthesurplus ofweight between twopoint\nsetsistakenintoaccount andthetriangle inequality stillholds.\nTheycallthismodiﬁed EMD the“Proportional Transportation\nDistance” (PTD) because anysurplus orshortage ofweight is\nremo vedinawaythattheproportions arepreserv edbefore the\nEMD iscalculated. The PTD iscalculated byﬁrst dividing,\nforboth point sets, everypoint’ sweight byitspoint set’stotal\nweight, andthen calculating theEMD fortheresulting point\nsets.\nThePTD isdeﬁned asfollo ws:\nLetA\bBbetwoweighted point sets, W\bUthetotal weights of\nAandB,anddaground distance. Thesetofallfeasible ﬂows\nF\u000b \u001cfij \u001efrom AtoBisdeﬁned bythefollo wing constraints:\n1.fij\u001f0\bi\u000b1\b\u0016\u0015\u0017\u0015\u001a\u0015\u0017\bm\bj\u000b1\b\u0016\u0015\u0017\u0015\u001a\u0015\u0017\bn\n2.∑n\nj \u001b1fij\n\u000bwi\n\bi\u000b1\b\u0016\u0015\u0017\u0015\u001a\u0015\u0017\bm\n3.∑m\ni \u001b1fij\n\u000bujW\nU\n\bj \u000b1 \b\u0016\u0015\u0017\u0015\u001a\u0015\u0017\bn\n4.∑m\ni\u001b1∑n\nj\u001b1fij\n\u000bW\nThePTD( A,B)isgivenby:\nPTD\tA\bB\n \u000bminF  F∑m\ni\u001b1∑n\nj\u001b1fijdij\nW\nConstraints 2and4force allofA’sweight tomovetotheposi-\ntions ofpoints inB.Constraint 3ensures thatthisisdone ina\nwaythatpreserv estheoldpercentages ofweight inB.\nThePTD isapseudo-metric. Inparticular ,itobeysthetriangle\ninequality .Itstilldoes nothavethepositivity property since\nthedistance between positionally coinciding setswith thesame\npercentages ofweights atthesame positions iszero. However,\nthisistheonly case inwhich thedistance between twonon-\nidentical point setsiszero. ThePTD willdistinguish between\ntwosetsBandB\n\u0001which differinonly onepoint. Ithasallother\nproperties oftheEMD forequal total weight sets.4Adjustments ofcoordinates andweights,\ngrounddistance\n4.1 The ground distance\nForallresults inthispaper ,weused theEuclidean distance as\nground distance. I.e.,thedistance between twonotes with the\ncoordinates (t1\n\bp1)and(t2\n\bp2)is\n\u0002\tt1 \u0003t2\n\n2\u0013 \tp1 \u0003p2\n\n2.\nAninteresting variation, especially forpolyphonic music,\nwould betomakethedistance inthepitch dimension depend\nonharmon yinstead ofjustcalculating thedifference ofpitches.\n4.2 Adjustments ofcoordinates andweights\nBefore applying oneofthesimilarity measures forweighted\npoint setsdescribed above,weadjust thesignatures ofthetwo\nmelodies wewanttocompare inseveralways:\u0004Inorder tobeable torecognize augmented ordiminuted\nversions ofamelody assimilar (likeforexample inthe\nright column ofFigure 5,second group, melody 4),itcan\nbenecessary tonormalize therange oftime coordinates.\nWechose tostretch themelody with thesmaller maxi-\nmum time coordinate overalonger time such that after\ntheadjustment, both melodies’ maximum time coordinates\nequal thelargermaximum time coordinate before thead-\njustment. Note thatwith alesscareful normalization, e.g.\ntheadjustment ofarandomly chosen melody totheother\nmelody’ slength, onecaneasily lose thesymmetry prop-\nerty.Wedidthisalignment ofdurations when weused the\nPTD, where there isnopartial matching; when weused the\nEMD, wecompared thedistances with andwithout dura-\ntionalignment andtook theminimum.\u0004Itisdesirable tomakethedistance measure independent of\ntranspositions. This could bedone bymoving oneofthe\ntwomelodies upordowninpitch toaposition where the\ndistance isminimal (´OMaid ´ın,1998). Since ﬁnding the\noptimum transposition would require therepeated applica-\ntion ofthesimilarity measure, which would takealotof\ntime, wechose totranspose oneofthemelodies sothatthe\nweighted average pitch isequal. This way,thesimilarity\nmeasure forweighted point setsneeds tobeapplied only\nonce, butthisisnotalwaystheoptimum solution. How-\never,thisapproximation usually works well enough for\ntransposed versions ofthesame melody toappear closer\nthan other melodies from thedatabase, seeFigure 5.\u0004When thetransportation distance iscalculated, thetrans-\nportation ofweight from onenote toanother canhappen in\nthetime dimension, thepitch dimension, oracombination\nofthetwo.Therefore, therange ofnumbers inboth dimen-\nsions affects theresults. Forallresults showninthispaper ,\nwemultiplied thetime coordinates with 3inorder toavoid\nallpoints tobeplaced inaverynarro w,long strip likein\nFigure 1,where thepitch ranges from 186to221(arange\nof35),while thetime only ranges from 0to6.Anarrange-\nment ofthepoints likeinFigure 1would makeittoocheap\ntomoveweight inthetime dimension incomparison tothe\npitch dimension, which would lead tonotes being matched\nwith notes thatdonotreally correspond with them.4.3 Anexample\nFigure 3showstheweight ﬂowforsignatures oftwomelodies\nafter adjusting them asdescribed above.Unlik ethesignature\nshowninFigure 1,thetime coordinates arenowmultiplied\nwith 3sothatweight transportation inthetime andpitch di-\nmensions aresimilarly expensi ve.InFigure 1,therange of\npitches ismuch largerthan therange oftime coordinates so\nthattransportation distance measures would match notes which\ndonotcorrespond with oneanother .Also, thetopmelody in\nFigure 3wasstretched sothatthemaximum time coordinates\nareboth 28.5, andthetopmelody wasalso slightly shifted in\nthepitch dimension sothat theweighted average pitches are\nthesame.W ithout thepitch andduration alignments, thedis-\ntance between these twomelodies would be5.41825 instead\nof0.739529. Forthesakeofsimplicity ,wetreated thegrace\nnote inthebottom melody likeanyother eighth note, thereby\noveremphasizing itandinﬂuencing thetime coordinates ofsub-\nsequent notes. Aspecial treatment ofgrace notes would proba-\nblylead tobetter results.\nInFigure 3,anarrowindicates theﬂowandthetransported\nweight foreach pair ofweighted points between which any\nweight istransported. Consider ,forexample, theﬁrsttwonotes\nofboth melodies. Since thesecond melody starts with adotted\neigth note andasixteenth note, while theﬁrst onestarts with\ntwoeigth notes, halfoftheweight ofthesecond note oftheﬁrst\nmelody istransported totheﬁrst note ofthesecond melody ,\nwhile theother halfgoes tothesecond note. Thequarter note\nwhich isrepresented asahollo wcircle isonly partially matched.\nIthasacapacity of1,butonly 0.5weight units aretransported\nintoit.\n5Resultsandcomparison withearlierresults\n5.1 Comparison with Schlichte’ s“Frankfurt Experience”\nJoachim Schlichte (1990) describes anattempt ofgrouping sim-\nilarincipits together .This workwasbased on83,243 incipits\nfrom theRISM A/II collection. Schlichte showsthatomitting\n“insigniﬁcant” musical phenomena immediately leads touse-\nlessresults, evenamong thesmall subset of83,243 outofthe\n476,000 incipits thatarecurrently available tous.Themethods\nheclassiﬁed asuseless are:\u0004Converting theincipits into strings ofinterv alsandcom-\nparing those, thus ignoring rhythm, absolute pitch, and\nrests, leads todistances ofzero between verydifferent\npieces. Schlichte givessome examples.\u0004Comparing strings ofpitches, ignoring only rhythm and\nrests, stillleads totoomanyfalse positi ves.Transposing\nallpieces into thesame keybefore applying thismethod\nmakesmatters worse.\nSchlichte therefore only lookedforidentical incipits, which al-\nready givemusic scholars valuable pointers tointeresting facts.\nOneoftheinteresting applications istheidentiﬁcation ofanon y-\nmous pieces. Schlichte writes thatamong the14,000 anon y-\nmous works inhisdata collection, 292, i.e.about 2%,canbe\nautomatically associated with acomposer bylooking foriden-\ntical incipits. This comparison isbased onthePlaine &Easie\nencoding (Selfridge-Field, 1997) inwhich allRISM A/II incip-\nitsarestored.Anon ymus: Roslin Castle\u0000 \u0001\u0002 \u0003\n\u0003\n\u0003\u0003\n\u0003\n\u0003\n\u0003\n\u0003\u0003\u0003\n\u0003\n\u0003\u0003\u0003\n\u0003\n\u0003\u0002\n\u0001 \u0000\u0004\ntime\n\u0005\npitch\n180\n155\n\u0006\u0006\b\u0007\n\u0006\n\u0006\n\u0007\u0006\n\u0006\n\u0006\u0006\u0006\u0006\n\u0007\u0006\u0006\t\u0007\n0.5 \u000b\u000b\f\n\r\r\u000e0.25 0.25\n1\n\n0.5\n0.5\n1\n0.5\n0.5\n0.5\n0.5\n0.5\n0.5\n1\n0.5\n\u000b\u000b\u000b\u000b\f0.25\n\r\r\u000e0.25\n1\u0004\ntime\n\u0005\npitch\n180\n155\n0 10 20\n\u000f \u0010\u0007\n\u0006\n\u0006\n\u0007\u0006\n\u0006\n\u0006\u0006\u0006\u0011\u0007\u0006\n\u0007\n\u000f \u0010\u0007\nAnon ymus: Roslin Castle –Distance: 0.739529\u0012 \u0013\u0014 \u0015\n\u0015\n\u0015\u0017\u0016\u0015\n\u0015\n\u0015\n\u0015\u0018 \u0019\u001a\n\u0015\u0015\n\u0015\n\u0015\u0015\u0015\n\u0015\n\u0015\u001b\u0016\u0014\n\u0013 \u0012\nFigure 3:Anillustration ofaweight ﬂowwith theEMD;\nthecoordinates areadjusted asdescribed inSection 4.2. The\nsignatures ofmelodies 1and11from Figure 5(left) after the\nadjustments, shownintheformat (Time, Pitch; Weight ):\nTop: (0,180.138; 0.5),(1.58333, 175.138; 0.5),(3.16667,\n169.138; 1),(6.33333, 192.138; 0.5),(7.91667, 197.138; 0.5),\n(9.5, 192.138; 1),(12.6667, 186.138; 0.5),(14.25, 192.138;\n0.5),(15.8333, 197.138; 0.5),(17.4167, 192.138; 0.5),(19,\n186.138; 0.5),(20.5833, 180.138; 0.5),(22.1667, 175.138;\n1),(25.3333, 180.138; 0.5),(26.9167, 175.138; 0.5),(28.5,\n169.138; 1)\nBottom: (0,180; 0.75),(2.25, 175; 0.25),(3,169; 1),(6,192;\n0.5),(7.5, 197; 0.5),(9,192; 1),(12, 186; 0.5),(13.5, 192;\n0.5),(15,197; 0.5),(16.5, 192; 0.5),(18,186; 0.5),(19.5, 180;\n1),(22.5, 175; 1),(25.5, 180; 0.75),(27.75, 175; 0.25),(28.5,\n169; 1)\nWe compared approximately 80,000 incipits by\nunidentiﬁed composers in the RISM A/II collec-\ntion toallother incipits; the result can beseen at\nhttp://give-lab.cs.uu.nl/MIR/anon/idx.html .About 13%\nofthese incipits liewithin adistance ofless than 1(PTD;\nweights: duration only; base-40 pitches; time coordinates\nmultiplied with 3)from other incipits. This includes trivial\ncases where theincipits areidentical, butalso more interesting\ncases liketheone showninFigure 4,where added notes,\naugmentation, transposition, anddifferences inrhyhtm, contour\nandthesequence ofinterv alsmakeitmore difﬁcult torecognize\nthesimilarity .\nAlexandre Sti´evenart: Variations\u001c \u001d\u001e\u001e \u001f  \n  \n  ! ! !\n\u001f! ! !\n ! ! !\n ! ! !\n ! ! !\n  \n\u001e\u001e\n\u001d \n\u001c\nAnon ymus: Lestrois cousines -Distance: 0.928683\" # $\n%% & & & '\n'\n' '\n' '\n'\n' '' '& & & '\n'\n%%\n# $& & & '\n\"\nFigure 4:These twoversions ofthe“Ah! vousdirai-je Ma-\nman” theme arerecognized assimilar (with PTD, weights: du-\nration only). Note theextranotes inthesecond tolastmeasure\nofSti´evenart andtheﬁrst measure ofAnon ymus, which lead\ntodifferences both inthesequence ofinterv alsandthecon-\ntours, andthefactthattheSti´evenart version isanaugmented\nandtransposed version.\nInorder toseehowmanyofthese matches areactually use-\nfulandwould nothavebeen found byjustlooking foridenticalpieces, wemanually check ed100randomly chosen search re-\nsults with distances belowone. 55%ofthese works only match\nwith other anon ymous works. For19%,acomposer could be\nfound because thecompared incipits areidentical. This issim-\nilartoSchlichte’ sresult –19%of13%are2.47 %,while\nSchlichte’ sﬁgure is2.08 %.Weexpect aslightly higher per-\ncentage because wedonotcompare thePlaine &Easie encod-\nings, butourdatabase contents asdescribed inSection 2,which\nmeans thatweviewmore melodies asidentical than Schlichte\ndid. Forexample, weignore beaming. Foranother 11%,we\ncould determine thecomposer although theincipits arenotiden-\ntical. Therefore, ourmethod leads totheidentiﬁcation ofabout\n3.9%ofallanon ymous pieces instead ofSchlichte’ s2.08 %.\n5.2 Comparison with Howard’ s“Har vardExperience”\nHoward(1998) describes alater attempt ofgrouping together\nsimilar incipits from theRISM A/II collection. This workwas\nbased onasubset ofourcollection with atmost halfasmany2\nincipits. TheU.S.RISM ofﬁcials didnot,liketheir Frankfurt\ncolleagues, compare Plaine &Easie encodings, butconverted\ntheincipits into theDARMS (Selfridge-ﬁeld, 1997) encoding\nlanguage. Theycompared sorting results ofﬁveencoding types:\n1.thecomplete encoding with allparameters,\n2.thecomplete encoding transposed toacommon pitch reg-\nister,\n3.theencoding stripped ofsuch features asbeaming, bar\nlines, andfermatas,\n4.theencoding stripped oftheitems givenin(3)plus grace\nnotes,\n5.theencoding stripped oftheitems givenin(3)and(4)plus\nrhythmic values, rests, andties, with thetransposition to\nacommon register (2)butwith preserv ation ofrepeated\nnotes.\nNone oftheﬁveencoding types lead tomore than 6outof\n13knownoccurrences ofasong called “Roslin Castle” being\nsorted together among notmore than 230,000 incipits.\nFigure 5showstheresults ofsome queries forthesame song\nwith theEMD (left column) and PTD (right column). The\nEMD query groups together 11outof16knownoccurrences\namong 476,000 incipits, i.e.alargerpercentage among more\nthan twice asmanypotential falsepositi vesincomparison with\nthe“Harv ardexperience”. Ifonedoes notcount the16th oc-\ncurrence, shownatthebottom right, because itisnotencoded\ncorrectly ,ourmethod compares evenmore favourably (73%\nversus 46%).ThePTD result showninFigure 5does notgroup\ntogether more occurrences, butatleast thefalse positi vesare\nmusically verysimilar .\nFigure 5alsoillustrates thedifferent properties oftheEMD and\nPTD:\u0004The EMD groups more occurrences together injustone\nquery result. Among the16knownoccurrences of“Roslin\n2Howarddoes notclearly sayhowmany,butfrom theintroduction\ntohispaper itcanbeinferred thatthenumber wasprobably below\n230,000.Castle”, there are3groups with similar numbers ofnotes.\nThefactthattheEMD allowspartial matching, while the\nPTD matches allnotes, leads toaclear distinction ofthese\ngroups bythePTD, butnottheEMD.\u0004Because oftheweight normalization, thePTD recognizes\naugmented ordiminuted versions ofthesame melody as\nsimilar .Inthesecond group ofmelodies intheright col-\numn ofFigure 5,melody number 4isrecognized assimi-\nlartotheother melodies inthegroup, while theuseofthe\nEMD leads toarather largedistance since there isalarge\ndifference inweight between corresponding notes.\u0004Thefalsepositi vesintheright column ofFigure 5(num-\nbers 6and8intheﬁrstgroup) aremore similar tothemu-\nsical material intherestofthequery result than thefalse\npositi vesintheleftcolumn (numbers 12to16). Therea-\nsonisthattheEMD allowsanunmatched weight surplus\ntobespread overthewhole melody .Inother words, this\ndistance measure does notdistinguish between afewex-\ntraormissing blocks ofnotes ontheoneside anddiffer-\nences between manyindividual notes orrests ontheother\nside. When thePTD isused, blocks ofextra ormissing\nnotes lead tothewrong notes being compared toonean-\nother ,which usually dramatically increases thedistance.\nAnydifferences between individual notes arealso penal-\nized.\n6Indexing\nWeeliminated theneed forcalculating theexpensi vetrans-\nportation distance toaquery foreverymelody inthedatabase\nbyexploiting thetriangle inequality andusing vantage objects\n(Vleugels andVeltkamp, 2002). Asapreparation, kvantage ob-\njects arerandomly chosen from thedatabase containing npoint\nsets, andthedistance ofeach ofthenpoint setstoeach ofthek\nvantage objects iscalculated inthefeature space bydetermining\natransportation distance. These transportation distances canbe\nviewed asthecoordinates ofthepoint setsinak-dimensional\nEuclidean space.\nThanks tothetriangle inequality ,theEuclidean distance be-\ntween twopoint setsinthek-dimensional space isalowerbound\nforthetransportation distance inthefeature space (Barros etal.,\n1996).\nThe search forpoint setswhich arecloser than rtothequery\npoint setcannowbelimited tothose point setswhose Euclidean\ndistance from thequery inthek-dimensional Euclidean space\noftransportation distances islessthan r.Only forthose, the\ntransportation distance needs tobecalculated. Ifthequery ob-\njectisnotyetinthedatabase, itsdistances tothekvantage\nobjects need tobecalculated asaﬁrststep. Then, byperform-\ninganapproximate nearest-neighbour search intheEuclidean\nspace, onecananswer aquery byperforming O \tmlogn \nEu-\nclidean distance calculations (Arya, Mount, Netan yahu, Silver-\nman, andWu,1994) plus mexpensi vetransportation distance\ncalculations, where m,thenumber ofreported point sets, de-\npends onhowtheweighted point setsaredistrib uted intheEu-\nclidean space. Ifoneprefers anexact nearest neighbour search,\nonecanquery ak-dimensional kd-tree using O \tn1\u00001\nk\u0013m \nEu-\nclidean distance calculations. Inpractice, with ourdatabase of\n476,000 point sets andamaximum distance rof5,weneed1.Anon ymus: Roslin Castle (Query) –Distance: 0\u0000 \u0001\u0002 \u0003\n\u0003\n\u0003\u0003\n\u0003\n\u0003\n\u0003\n\u0003\u0003\u0003\n\u0003\n\u0003\u0003\u0003\n\u0003\n\u0003\u0002\n\u0001 \u0000\n2.Anon ymus: Roslin Castle –Distance: 0.373135\u0004 \u0005 \u0006\u0007 \b\n\b\n\b\n\b\n\b\b\b\n\b\n\b\b\n\t\b\n\b\n\b\t\u0007\n\u0005 \u0006 \u0004\n3.Anon ymus: Roslin Castle –Distance: 0.373135\u000b \f\r \u000e\n\u000e\n\u000e\n\u000e\n\u000e\u000e\u000e\n\u000e\n\u000e\u000e\n\u000f\u000e\n\u000e\n\u000e\u000f\r\n\f \u000b\n4.Anon ymus: Roslin Castle –Distance: 0.513589\u0010 \u0011\u0012 \u0013\n\u0013\n\u0013\n\u0014\u0013\n\u0013\n\u0013\n\u0013\n\u0013\u0013\u0013\n\u0013\n\u0013\u0013\u0013\n\u0013\n\u0013\u0015\u0014\u0012\n\u0011 \u0010\n5.Anon ymus: Roslin Castle –Distance: 0.551804\u0016 \u0017\u0018 \u0019\n\u0019\n\u0019\u0019\u0019\u0019\n\u0019\n\u0019\u0015\u001a\u0019\n\u0019\n\u0019\n\u0019\n\u0019\u0019\u0019\n\u0019\n\u0019\u0019\n\u001a\u0019\n\u0019\n\u0019\u0015\u001a\n\u0018\n\u0017 \u0016\n6.Anon ymus: Roslin Castle –Distance: 0.551804\u001b \u001c\u001d \u001e\n\u001e\n\u001e\n\u001e\n\u001e\u001e\u001e\u0015\u001f\n\u001e\n\u001e\n\u001e\u001e\u001e\u001f\u001e\n\u001e\n\u001e\n\u001f\u001e\n\u001e\n\u001e\n\u001e\n\u001e\u001e\u001e\n\u001e\n\u001e\u001e\n\u001f\u001e\n\u001e\n\u001e\n\u001f\n\u001d\n\u001c \u001b\n7.Anon ymus: Roslin Castle –Distance: 0.551804 ! \"# $\n$\n$$$\n%$\n$\n$\n%$\n$\n$\n$\n$$$\n$\n$$\n%$\n$\n$\n%\n#\n! \"  \n8.Anon ymus: Roslin Castle –Distance: 0.551804& '( )\n)\n)\u0015*)\n)\n)\n)\n)))\n)\n))\n*)\n)\n)\n*(\n' &\n9.Anon ymus: Roslin Castle –Distance: 0.608147+-, .\n.\n.\n.\n...\n.\n..\n/.\n.\n./, +\n10.Anon ymus: Roslin Castle –Distance: 0.6677940 12 3\n3\n43\n3\n3\n3\n333\n3\n33\n43 3\n3\n42\n1 0\n11.Anon ymus: Roslin Castle –Distance: 0.7395295 67 8\n8\n8\n98\n8\n8\n8: ;<\n88\n8\n888\n8\n8\u001597\n6 5\n12.Joseph Aloys Schmittbauer (1718-1809): Lauda Sion –Dist.: 0.798707= > ?@ A\nA\nA\nA\nAAB\nC\nAB\n@\n> ?C\nA\n=\n13.Logroscino, Nicola Bonifacio (1698-1765c): Olimpiade –D.:1.09449D EF GH I\nH H\nH\nH H\nFH\nHJ\nH\nF\nE D\n14.Christoph Graupner: M’in vitaalacaccia –Distance: 1.10299KML N O\nO\nO\nO\nO O O OP\nOQ Q\nRTSU\nL N K\n15.Johann Franz XaverSterkel (1750-1817): IlFarnace, Sc.II–1.13149V-W X XY\nX\nXX\nX XXZ\nW V\n16.Geor gFriedrich H¨andel: Hymne “Obejoyful” HWV .279–1.24449[ \\]] ^\n^\n_ _ _\n^_ _ _\n^_ _ _\n_ _ _\n^_ _ _\n_ _ _\n^`\n_ _ _\n^a\nb\n]]\n\\ c [\n17.Anon ymus: Roslin Castle –Distance: 1.27669d ef g\ng\nggg\ngh\ng\ng\ng\ngggg\ng\ng\u0015ig\ng\ng\ng\nj kl\ngg\ng\nggg\ng\ng\ni\nf\ne d1.Anon ymus: Roslin Castle (Query) –Distance: 0m no p\np\npp\np\np\np\nppp\np\nppp\np\npo\nn m\n2.Anon ymus: Roslin Castle –Distance: 0.513589q rs t\nt\nt\nut\nt\nt\nt\nttt\nt\nttt\nt\nt\u0015us\nr q\n3.Anon ymus: Roslin Castle –Distance: 0.551804v wx y\ny\ny\u0015zy\ny\ny\ny\nyyy\ny\nyy\nzy\ny\ny\nzx\nw v\n4.Anon ymus: Roslin Castle –Distance: 1.28636{ |} ~\n~\n~\n~\n~\n~\n~\n~~~\n~\n~~~\n~\n~\u0015}\n| {\n5.Anon ymus: Roslin Castle –Distance: 2.49759  \n\n\n\n\n\n\n\n\n\n\n\n\n\n \n6.Anon ymus: L’´Et´edelaSaint-Martin –Distance: 2.58841   \n\n\n\n\n\n\n\n\n\n    \n7.Anon ymus: Roslin Castle –Distance: 2.74794  \n\n\n\n\n\n\n\n\n \n\n\n \n8.Grønland, Peter (1761-1825): Ridder Oven–Distance: 2.95087  \n \n\n\n\n \n  \n  \n1.Anon ymus: Roslin Castle (Query) –Distance: 0  \n\n\n\n\n\n\n\n\n\n \n2.Anon ymus: Roslin Castle –Distance: 0   ¡ ¢\n¢\n¢\n¢\n¢¢¢\n¢\n¢¢\n£¢\n¢\n¢£¡\n   \n3.Anon ymus: Roslin Castle –Distance: 0.263672¤-¥ ¦\n¦\n¦\n¦\n¦¦¦\n¦\n¦¦\n§¦\n¦\n¦§¥ ¤\n4.Anon ymus: Roslin Castle –Dist.: 1.83006¨ ©ª «¬\n¬\n¬\n¬¬¬\n« «¬\n¬\nª\n©\n«¨\n5.Anon ymus: Roslin Castle –Distance: 2.06412­ ®¯ °\n°\n±°\n°\n°\n°\n°°°\n°\n°°\n±° °\n°\n±¯\n® ­\n1.Anon ymus: Roslin Castle (Query) –Distance: 0² ³´ µ\nµ\nµµµµ\nµ\nµ\u0015¶µ\nµ\nµ\nµ\nµµµ\nµ\nµµ\n¶µ\nµ\nµ\u0015¶\n´\n³ ²\n2.Anon ymus: Roslin Castle –0.251757· ¸ ¹º »\n»\n»»»\n¼»\n»\n»\n¼»\n»\n»\n»\n»»»\n»\n»»\n¼»\n»\n»\n¼\nº\n¸ ¹ ·\n3.Anon ymus: Roslin Castle –0.251757½ ¾¿ À\nÀ\nÀ\nÀ\nÀÀÀ\u0015Á\nÀ\nÀ\nÀÀÀÁÀ\nÀ\nÀ\nÁÀ\nÀ\nÀ\nÀ\nÀÀÀ\nÀ\nÀÀ\nÁÀ\nÀ\nÀ\nÁ\n¿\n¾ ½\n4.Anon ymus: Roslin Castle –Distance: 1.15149Â ÃÄ Å\nÅ\nÅÅÅ\nÅÆ\nÅ\nÅ\nÅ\nÅÅÅÅ\nÅ\nÅ\u0015ÇÅ\nÅ\nÅ\nÅ\nÈ ÉÊ\nÅÅ\nÅ\nÅÅÅ\nÅ\nÅ\nÇ\nÄ\nÃ Â\nAnon ymus: Roslin Castle (the 16th version, which isverydifferent from theother 15\nknownoccurrences duetoanencoding error)Ë Ì ÍÎ Ï\nÏ\nÏ\nÏ\nÏÐ Ð Ð\nÏÐ Ð Ð\nÏÐ Ð Ð\nÏÏÏ\nÑÏ\nÏ\nÏÑÎ\nÌ Í Ë\nFigure 5:Query results for“Roslin Castle” among 476,000 melodies. Weights reﬂect only note durations. Theleftcolumn shows\nthetop17matches ofanEMD-based query ,containing 12occurrences of“Roslin Castle”. Theright column contains all16known\noccurrences of“Roslin Castle”, 15ofwhich areretrie vedwith 3PTD-based queries whose results areseparated with horizontal\nlines. There isanencoding error which preventsthe16th occurrence from being showninother query results –inthePlaine &Easie\nformat used forcollecting theRISM data, itiseasy togettheoctaveswrong. Foradiscussion ofthedifferences between EMD (left\ncolumn) andPTD (right column), seeSection 5.2.\nlessthan 1000 expensi vecalculations instead of476,000, which\nreduces thequery running time ona2-GHz Pentium 4system\nwith WindowsXPfrom approximately 70minutes to9seconds,\nwithout altering theresult.\nAlthough thetriangle inequality holds only forthePTD andnot\ngenerally fortheEMD, wetried thisindexing method forEMD\ndistances aswell. Inmost cases, theresults arenotdistorted.7Conclusions andfuturegoals\nIncomparison toSchlichte’ sand Howard’sexperience with\ngrouping similar melodies from theRISM A/II collection to-\ngether ,ourtransportation distance measures perform much bet-\nter.Itispossible togroup together more occurrences ofa\nmelody among alargertotal number ofincipits. Also, with\ntransportation distance measures, itiseasy torecognize sim-ilarities eveniftheyarehidden byadditional notes ordiffer-\nentrhythm. Finally ,there aretransportation distance measures\nwhich obeythetriangle inequality ,e.g.thePTD, sothat it\nispossible toefﬁciently search largedatabases. Weused this\nfactforaninteracti veonline search engine which searches all\n476,000 melodies inourdatabase.\nOne important strength oftransportation distances which we\nhavenotexploited yetisthefactthattheyshould allowusto\ncompare polyphonic music inmuch thesame wayasmono-\nphonic music. Forweighted point sets, itshould notmakemuch\nofadifference whether there arepoints which share thesame\ntime coordinate.\nThe partial matching provided bytheEMD does notalways\nmakemusical sense, ascanbeseen with thefalsepositi vesin\ntheleftcolumn ofFigure 5.Inorder tobeabletoﬁndmotifs and\nthemes within complete pieces, itmight benecessary tosplit the\npieces intosmall chunks andthen useatransportation distance\nforcomparing chunks. The EMD’ spartial matching, possibly\ninconjunction with aconsonance ground distance, might then\ne.g.proveuseful forsearching fullscores based onthemes\ntakenfrom apiano reduction.\nAnother promising application would betheuseoftransporta-\ntiondistance measures forbuilding aQuery-by-Humming sys-\ntemwithout explicit note onset detection. Note onset detection\nisadifﬁcult problem (Pauws, 2002; Prechelt andTypke,2001).\nThis taskcould bedelegated tothetransportation distance mea-\nsure, which would combine thetwotasks ofgrouping FFT win-\ndows3intonotes andcomparing setsofnotes.\n8Acknowledgements\nWethank Han-W enNienhuys forthesupport hegaveuswhen\nweused hisLilypond music typesetter forgenerating hundreds\nofthousands ofmusic bitmaps.\nReferences\nArya, S.,Mount, D.M., Netan yahu, N.S.,Silverman,\nR.&Wu,A.(1994). An optimum algorithm forap-\nproximate nearest neighbor searching. Proceedings ofthe\nFifth ACM-SIAM Symposium onDiscr eteAlgorithms ,(pp.\n573–582). Implementation retrie vedApril 1,2003, from\nhttp://www.cs.umd.edu/˜mount/ANN/\nBarros, J.,French, J.,Martin, W.,Kelly,P.&Cannon, M.\n(1996). Using thetriangle inequality toreduce thenumber of\ncomparisons required forsimilarity-based retrie val.Proceed-\nings ofSPIE, StorageandRetrie valforStill ImageandVideo\nDatabases IV,2670, (pp. 392–403).\nCohen, S.(1999). Finding Color andShape Patterns inImages.\nPh.D. thesis, Stanford University ,Department ofComputer Sci-\nence.\nGiannopoulos, P.&Veltkamp, R.C.(2002). APseudo-Metric\nforWeighted Point Sets. InHeyden, A.,Sparr ,G.,Nielsen, M.\n&Johansen, P.(Ed.), Proceedings ofthe7thEuropean Confer -\nence onComputer Vision (ECCV) (pp. 715–730). Copenhagen,\nDenmark: Springer -Verlag.\n3FFT windo ws:theresults ofFastFourier Transformations ofshort\ntime windo wsofaudio signal.Hewlett, W.B. (1992). A Base-40 Number -\nline Representation of Musical Pitch Notation.\nMusik ometrika ,4,1–14. Retrie vedApril 1,2003, from\nhttp://www.ccarh.org/publications/reprints/base40/\nHillier ,S.&Lieberman, G.J.(1990). Introduction toMathe-\nmatical Programming .McGra w-Hill.\nHoward, J.B.(1998). Strate gies forSorting Melodic Incip-\nits.Computing inMusicolo gy,Melodic Similarities: Concepts,\nProcedur es,andApplications ,11,119–128. MIT Press, Cam-\nbridge, Massachusetts.\n´OMaid ´ın,D.(1998). AGeometrical Algorithm forMelodic\nDifference. Computing inMusicolo gy,Melodic Similarities:\nConcepts, Procedur es,andApplications ,11,65–72. MIT Press,\nCambridge, Massachusetts.\nPauws, S.(2002). CubyHum: AFully Operational Query by\nHumming System. ISMIR 2002 Confer ence Proceedings ,(pp.\n187–196).\nPrechelt, L.&Typke,R.(2001). AnInterf aceforMelody Input.\nACMTransactions onComputer -Human Inter action 8(2), 133–\n149.\nR´epertoir eInternational desSourcesMusicales (RISM). Serie\nA/II, manuscrits musicaux apr`es1600. (1996) K.G.Saur Ver-\nlag,M¨unchen, German y.http://rism.stub.uni-frankfurt.de\nRubner ,Y.Sourcecode fortheEarth Mover’sDistance\nsoftwar e. (1998). Retrie vedApril 1,2003, from\nhttp://robotics.stanford.edu/˜rubner/emd/default.htm\nSchlichte, J.(1990). Derautomatische Vergleich von83243\nMusikincipits ausderRISM-Datenbank: Ergebnisse -Nutzen -\nPerspekti ven.Fontes artis musicae ,37,35–46.\nSelfridge-Field, E.(Ed.) (1997). Beyond MIDI: thehandbook\nofmusical codes. Cambridge: MIT Press.\nVleugels, J.&Veltkamp, R.C.(2002) Efﬁcient Image Retrie val\nthrough Vantage Objects. Pattern Reco gnition ,35(1) (pp. 69–\n80)"
    },
    {
        "title": "A scalable peer-to-peer system for music content and information retrieval.",
        "author": [
            "George Tzanetakis",
            "Jun Gao",
            "Peter Steenkiste"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417451",
        "url": "https://doi.org/10.5281/zenodo.1417451",
        "ee": "https://zenodo.org/records/1417451/files/TzanetakisGS03.pdf",
        "abstract": "Currently a large percentage of Internet traffic con- sists of music files, typically stored in MP3 com- pressed audio format, shared and exchanged over Peer-to-Peer (P2P) networks. Searching for music is performed by specifying keywords and naive string matching techniques. In the past years the emerging research area of Music Information Retrieval (MIR) has produced a variety of new ways of looking at the problem of music search. Such MIR techniques can significantly enhance the ways user search for music over P2P networks. In order for that to happen there are two main challenges that need to be addressed: 1) scalability to large collections and number of peers, 2) richer set of search semantics that can support MIR especially when retrieval is content-based. In this paper, we describe a scalable P2P system that uses Rendezvous Points (RPs) for music metadata regis- tration and query resolution, that supports attribute- value search semantics as well as content-based re- trieval. The performance of the system has been eval- uated in large scale usage scenarios using “real” au- tomatically calculated musical content descriptors. 1",
        "zenodo_id": 1417451,
        "dblp_key": "conf/ismir/TzanetakisGS03",
        "keywords": [
            "Internet traffic",
            "music files",
            "MP3 compressed audio format",
            "Peer-to-Peer (P2P) networks",
            "Music Information Retrieval (MIR)",
            "searching for music",
            "keywords",
            "naive string matching techniques",
            "Emerging research area",
            "Music Metadata registration"
        ],
        "content": "AScalablePeer-to-PeerSystemforMusicContentandInformation Retrieval\nGeorgeTzanetakis\nComputer Science\nCarne gieMellon University\n5000 Forbes Avenue\nPittsb urgh,PA15218\ngtzan@cs.cmu.eduJunGao\nComputer Science\nCarne gieMellon University\n5000 Forbes Avenue\nPittsb urgh,PA15218\njungao@cs.cmu.eduPeterSteenkiste\nComputer Science andElectrical Engineering\nCarne gieMellon University\n5000 Forbes Avenue\nPittsb urgh,PA15218\nprs@cs.cmu.edu\nAbstract\nCurrently alargepercentage ofInternet trafﬁccon-\nsists ofmusic ﬁles, typically stored inMP3 com-\npressed audio format, shared and exchanged over\nPeer-to-Peer (P2P) netw orks. Searching formusic is\nperformed byspecifying keywords andnaivestring\nmatching techniques. Inthepast years theemer ging\nresearch area ofMusic Information Retrie val(MIR)\nhasproduced avariety ofnewwaysoflooking atthe\nproblem ofmusic search. Such MIR techniques can\nsigniﬁcantly enhance thewaysuser search formusic\noverP2P netw orks. Inorder forthattohappen there\naretwomain challenges thatneed tobeaddressed: 1)\nscalability tolargecollections andnumber ofpeers, 2)\nricher setofsearch semantics thatcansupport MIR\nespecially when retrie valiscontent-based. Inthis\npaper ,wedescribe ascalable P2P system that uses\nRendezv ousPoints (RPs) formusic metadata regis-\ntration andquery resolution, thatsupports attrib ute-\nvalue search semantics aswell ascontent-based re-\ntrieval.Theperformance ofthesystem hasbeen eval-\nuated inlargescale usage scenarios using “real” au-\ntomatically calculated musical content descriptors.\n1Introduction\nOne could argue that both theideas ofMusic Information\nRetrie val(MIR) andPeer-to-Peer netw orks (P2P) essentially\nstarted with Napster (http://www.napster.com ).Al-\nthough crude both interms ofsearch capabilities andinterms\nofP2P performance, Napster fortheﬁrsttime provided anex-\nample ofsharing largeamounts ofmusical data overlargead-\nhocnetw orks. Despite thisearly connection there hasnotbeen\nmuch progress incombining these twoareas. Although bet-\nterP2P paradigms havebeen proposed, searching formusic is\ncurrently stillperformed using traditional keyword-based text\nsearch. While avariety ofnovelwaysofsearching andretrie v-\ningmusic, especially inaudio format, havebeen proposed, they\nPermission tomakedigitalorhardcopiesofallorpartofthiswork\nforpersonalorclassroom useisgrantedwithoutfeeprovidedthat\ncopiesarenotmadeordistributedforpro®torcommercial advan-\ntageandthatcopiesbearthisnoticeandthefullcitationonthe®rst\npage.c\n\u00002003JohnsHopkinsUniversity.haven’tfound their wayintoP2P netw orks andremain largely\nacademic exercises.\nThere aremanyadvantages toP2Pnetw orks such asdistrib uted\ncomputing andstorage power,lowbandwidth, fault-tolerance\nandreliability .Although because ofcopyright restrictions ma-\njorrecording labels havebeen reluctant tofollo wthisparadigm\ntheemer gence ofaudio ﬁngerprinting technology Haitsma and\nKalk er(2002) islikelygoing tochange thisattitude. One of\nthegreatest potential beneﬁts ofP2P netw orks istheability to\nharness thecollaborati veefforts ofusers toprovide semantic,\nsubjecti veandcommunity-based tags todescribe musical con-\ntent.\nCentralized P2P such asNapster ,arenotrobustand may\nbevulnerable toDenial-of-Service attacks, since thecen-\ntralserverforms thesystem’ ssingle point offailure. Such\nasystem does not scale well asregistration and query\nload increases. Distrib uted P2P systems, such asGnutella\n(http://www.gnutella.org )and KaZaA (http://\nwww.kazaa.com ),aremore robust,butsince peers donotex-\nplicitly register their shared ﬁles, aquery may havetobebroad-\ncast throughout thenetw orktogetresolv ed. The potentially\nlargenumber ofmessages involvedlimits thesystem’ sscala-\nbility andperformance. Distrib uted Hash Table (DHT) based\nsystems, such astheones described inStoica etal.(2001); Row-\nstron andDruschel (2001), achie vegood scalability bydeplo y-\ningastructured overlay P2Pnetw orkthatsupports efﬁcient con-\ntentlocation. However,thebasic setofapplications builtontop\nofDHT ,only supports exact ﬁlename look upanddoes not\nallowtherichsearch semantics desired forMIR.\nInthispaper ,wedescribe arobust,scalable P2P system that\nprovides ﬂexible search semantics based onattrib ute-v alue(AV)\npairs andsupports automatic extraction ofmusical features and\ncontent-based similarity retrie val.Thesystem isshowntoper-\nform well under realistic loads consisting offeatures automati-\ncally extracted from alargedatabase ofaudio recordings. The\nmain contrib utions ofthisworkare: ageneral content disco v-\nerymechanism thatsupports exact andsimilarity search based\nonAV-pairs anditsevaluation using aspeciﬁc setofaudio fea-\ntures computed onactual audio recordings. Webelie vethat\ntheproposed system provides thenecessary ﬂexibility andper-\nformance foreffectiveuseofMusic Information Retrie valfor\nsearching inPeer-to-Peer netw orks.2RelatedWork\nThe main focus ofthisworkistheretrie valofmusic inaudio\nformat overP2P netw orks. There isalotofrecent exciting\nworkinMIR thatisrelevanttothedesign ofoursystem andwe\nreviewsome representati vepublications. Although thispaper\nmainly describes similarity retrie val,theunderlying frame work\noffeatures anddistance calculations forms thebasis ofavari-\netyofaudio analysis algorithms such as:musical genre clas-\nsiﬁcation (Tzanetakis andCook (2002); Aucouturier andPa-\nchet (2003)), beat detection andanalysis (Foote andMathe w\n(2002)), similarity retrie val(Logan andSalomon (2001); Au-\ncouturier andPachet (2002) Yang(2002)), audio ﬁngerprinting\n(Haitsma andKalk er(2002)) andclustering andvisualization\n(Rauber etal.(2002)). Inaddition tofeatures computed from\nautomatic analysis ofaudio content, features computed based\nontextanalysis ofcritics reviewsaswell asP2Pusage patterns\nhavebeen showntobeeffectiveforclassiﬁcation inWhitman\nandSmaragdis (2002). These articles arerepresentati veofeach\ncategory.Ageneral overvie wofthecurrent status inMIR and\nanextensi vebibliography canbefound inFutrelle andDownie\n(2002). Another good overvie wofthecurrent state oftheart\nandchallenges inMIR isPachet (2003).\nDHT -based systems such asStoica etal.(2001); Rowstron\nand Druschel (2001) solvesome ofthescalability prob-\nlems ofthemore well knownbroadcast-based systems such\nasGnutella (http://www.gnutella.org )and KaZaA\n(http://www.kazaa.com ).The Content Disco verySys-\ntem(CDS) proposed inthispaper isbuiltontopofsuch aDHT -\nbased system. Theidea ofusing MIR overaP2Pwasproposed\ninWangetal.(2002). Howevertheproposed system architec-\ntures sufferfrom scalability problems andonly theretrie valof\nsymbolic data isexamined. The potential ofintegrating MIR\nandtheevolving semantic web wasexplored inBaumann and\nKluter (2002). More recently content-based retrie valovera\nP2Pnetw orkusing theJXTAprogramming frame workwaspre-\nsented inBaumann (2003). Theproposed system mainly iscon-\ncerned with integrating feature representations intotheP2Psys-\ntemrather than using thestructure ofthenetw orktosupport ex-\nactandsimilarity search. Aninitial description ofthesystem\npresented inthispaper canbefound inGao etal.(2003). The\nmain differences ofthispaper from previous workisthescal-\nable andefﬁcient support forboth exact andsimilarity search-\ningbased onRendezv ousPoints andtheperformance evaluation\nofusing audio features computed from actual audio recordings\nrather than simulated synthetic data.\n3SystemOverview\nFollowing recent workinP2P netw orks, oursystem decouples\ntheprocess oflocating content from theprocess ofdownload-\ningit.Each node inthenetw orknotonly stores music ﬁles for\nsharing butalso information about thelocation ofother music\nﬁles inthenetw ork. Therefore when theuser submits asearch\ntotheP2Psystem, thesystem returns asetofpeers from which\nthemusic ﬁles thatsatisfy thesearch criteria canthen bedown-\nloaded. Each ﬁleinthesystem isdescribed byaMusic File\nDescription (MFD) which essentially isasetofattrib ute-v alue\n(AV)pairs. Forexample apossible MFD might bethefollo w-\ning: \u0000artist=U2,album=RattleandHum,song=Desire,...,\nspecCentr oid=0.65,mfcc2=0.85,... \u0001.Notice thatthedescrip-\ntioncontains both attrib utes thatcanbemanually speciﬁed byDHT−based OverlayRegistrationSearchingMFD Specification\nMFEE\nContent Discovery SystemMFD\nDatabase\nFigure 1:Softw arearchitecture onapeer node.\ntheuser aswell asautomatically extracted features fordescrib-\ningmusical content (underlined). TheMusic Feature Extraction\nEngine (MFEE) isthecomponent thatcalculates these features\nfrom theaudio ﬁles. These automatically extracted features in\naddition tobeing used forcontent-based similarity search can\nalso beused forvarious other types ofaudio analysis such as\nmusical genre classiﬁcation.\nThere aretwooperations thataresupported bythesystem and\nboth takeMFDs asarguments. Inregistration, anewmusic ﬁle\nismade available forsharing anditsassociated MFD isregis-\ntered totheP2P system. During searching, theuser query is\nconverted intoanappropriate MFD which isthen used tolocate\nthenodes thatcontain ﬁles thatmatch thesearch criteria. Once\nthenodes arelocated theyarecontacted directly tostart theac-\ntualdownloading fortheﬁle. Themain concern inthedesign\nofoursystem istheefﬁcient content disco veryrather than the\nactual downloading mechanism.\nFigure 1showsthesoftw arearchitecture oneach peer node.\nTheMFD ofeither registration orsearch query ispassed tothe\nContent Disco verySystem (CDS), which runs ontopofaDis-\ntributed Hash Table (DHT) based P2P system, such asChord\nStoica etal.(2001). InaDHT ,each peer isresponsible fora\nregion, represented with anode ID,inacontiguous \u0002-bitvir-\ntualaddress space. Adata item such asaﬁlename, isassoci-\nated with some value inthisaddress space, e.g., byapplying a\nuniform hash function tothedata item, andstored onthepeer\nwhose region coversthevalue. Correspondingly ,byapplying\nthesame computation tothedata item, apeer canlocate itfrom\nthesame peer who stores it.Wepresent thealgorithm used by\ntheCDS todistrib uteMFDs topeers inSection 5.Theunderly-\ningmechanism ofDHT ensures routing andmessage forw ard-\ningefﬁcienc yinsuch asystem: inChord, apeer only needs to\nkeepinformation about\u0003\u0005\u0004\u0007\u0006\t\b\u000b\n\r\f\u000f\u000e\u0011\u0010 neighboring peers, andthe\nnumber ofoverlay hops between twopeers is\u0003\u0005\u0004\u0007\u0006\t\b\u000b\n\r\f\u000e\n\u0010,where\f\u000eisthetotal number ofpeers inthesystem. Each peer main-\ntains alocal MFD database tostore theMFDs itisassigned by\ntheCDS. Upon thearrivalofaquery ,each peer examines its\nlocal MFD database andreturns thesetofMFDs thatmatch the\nquery tothequery initiator .Subsequently ,thequery initiator\ncandownload theactual music ﬁlefrom thepeer thatowns the\nmusic.4MusicFeatureExtraction\nThe MFEE component takesasinput anaudio ﬁleineither\nPCM (pulse code modulated) orcompressed format, such as\nMP3, theMPEG audio compression standard, andoutputs a\nfeature vector ,also knownasthecontent-based vector ,ofAV-\npairs thatcharacterizes theparticular musical content oftheﬁle.\nInoursystem, weusethefeature setproposed inTzanetakis\nandCook (2002) forthepurpose ofmusical genre classiﬁca-\ntion. This feature vector captures aspects ofinstrumentation\nandsound texture (what instruments areplaying andtheir den-\nsitydistrib ution overtime), rhythm (fast-slo w,strong-weak),\nandpitch content (harmon y)andhasbeen showntobeanef-\nfectiverepresentation forthepurposes ofclassiﬁcation andre-\ntrievalofmusic. More speciﬁcally ,features based ontheShort\nTimeFourier Transform aswell asMel-Frequenc yCepstral Co-\nefﬁcients areused torepresent sound texture, andfeatures based\nonBeat andPitch Histograms areused torepresent rhythm and\npitch content. More speciﬁcally ,themeans andvariances of\ntheSpectral Centroid, Rollof f,Flux andZeroCrossings andthe\nﬁrst 5Mel Frequery Cepstral Coef ﬁcients (MFCC) overa1\nsecond texture windo wusing 20millisecond windo warecal-\nculated forrepresenting Spectral Texture. FortheBeat His-\ntogram calculation aDiscrete WaveletTransform ﬁlterbank is\napplied andautocorrelation-based envelope periodicity detec-\ntionisperformed. ForthePitch Histogram calculation themul-\ntiple pitch detection algorithm described inTolonen andKar-\njalainen (2000) isused. Thedifferent types ofinformation rep-\nresented bythefeature vector combined with thequery ﬂexi-\nbility ofthesystem supports arich variety ofpossible query\nspeciﬁcations. Forexample, auser cansearch only onthebasis\nofrhythmic content while ignoring other aspects.\nWeusestandard linear quantization andnormalization totrans-\nform thedynamic ranges ofthecontinuous features into dis-\ncrete values necessary forsearching based onAV-pairs. Linear\nquantization waschosen sothat thestatistics ofthedistrib u-\ntionofthefeatures donotchange. Inoursystem, each feature\nisquantized to100x1 discrete values. Experiments comparing\nautomatic classiﬁcation oftheoriginal continuous features and\nthequantized features showed nosigniﬁcant differences inthe\nresults. Using thefeatures anddataset (10genres) described in\nTzanetakis andCook (2002) andaGaussian classiﬁer weobtain\u0000\u0002\u0001\u0004\u0003 \u0000\u0006\u0005accurac yusing theunquantized features and\n\u0000\b\u0007\u0006\u0005accu-\nracyusing thequantized version. Theresults oftheMFEE com-\nponent together with manually annotated metadata such asartist\nandalbumname arecombined toform aMusic File Descrip-\ntion(MFD), which isacollection ofAV-pairs. Asanexample,\t\u000b\n\r\f\u000f\u000e\u0011\u0010\u0000\u0013\u0012\n\u000e\u0015\u0014\u0017\u0016\u0002\u000e\u0019\u0018\u0003\u001a\u0003\u001b\u0003\n\u0018\u0012\u001d\u001c\n\u0014\u001e\u0016\u001c \u0001consists of\u001fAV-pairs, where\u0012\u001d \n\u0018\"!#\u0014%$\u0003\u001a\u0003\u001fcanbeeither amanually annotated attrib uteora\ncontent-based feature attrib ute. Forspecifying queries, MFDs\naresimilarly formed torepresent thesearch criteria. Inparticu-\nlar,theMFEE isused togenerate aquery MFD when theuser\nprovides asample piece ofmusic. Anysubset oftheMFD can\nbeused forquery speciﬁcation andusing named AV-pairs in\nMFDs allowsmore possibilities than traditional keyword-based\nsearch. Some examples ofpossible queries ofincreasing com-\nplexityarethefollo wing:&Search for\u0000artist=U2\u0001&Search for \u0000artist=U2,year=1985,tempo=80beats-\nper-minute(BPM)-100BPM \u0001\n&Search for\u000010mostsimilartox.mp3\u0001(content-based sim-\nilarity search)&Search for \u000010mostsimilartox.mp3,artist=U2 \u0001\nThese arejustasymbolic representations ofqueries. Inanac-\ntualimplementation avariety ofuser interf aces forquery spec-\niﬁcation would beprovided bythesystem (for examples see\nTzanetakis etal.(2002)). Forqueries with content-based parts\nsuch asthelasttwoones, theaudio ﬁle,x.mp3 ,isﬁrst con-\nverted using theMFEE tonumerical features thatdescribe mu-\nsical content which aresubsequently used forsimilarity search.\nThis mechanism allowsanyaudio ﬁletobeused inthesystem\nevenifitdoesn’ thaveanymetadata information associated with\nit(forexample aﬁlerecorded offaradio broadcast).\na = (1,2)\nd = (3,5)\ne = (2,2)\nf = (4,1)b = (3,3)\nc = (3,4)e\nb\nc\ndf\na1\n2\n3\n4\n512345x\ny\nq2Registered MFDs:\nSome Nodes:Some Queries:\nq0 : {y=2}\nq1 : {x=3, y=3}\nq2 : {x=4, y=3}N[y=2] : {a,e}\nN[x=3] : {b,c,d}\nN[y=3] : {b}\nFigure 2:Illustration example ofContent Disco verySystem\n5ScalableContentDiscovery\nUnlik ecentralized systems where ﬁles areregistered atasingle\nplace orbroadcast-based systems where aquery may potentially\nbesenttoallpeers inthesystem, CDS uses ascalable approach\nbased onRendezv ousPoints (RPs) forregistration andquery\nresolution. Essentially theP2P netw orkisstructured toefﬁ-\nciently represent thespace ofAV-pairs forsearch andretrie val.\n5.1MFDregistration\nToregister anMFD, theCDS applies auniform hash function'such asSHA-1 toeach AV-pair inthe\n\t\u000b\n\r\ftoobtain \u001fnode\nIDs:\n'\u0004(\u0012\u001d \n\u0014)\u0016 \u0010\u0015* \f# \n\u0018\"!+\u0014,$\u0003\u001b\u0003\u001f,where \f# istheIDofapeer\ninthesystem. The MFD isthen sent toeach ofthese peers,\nandthissetofpeers isknownastheRendezv ousPoints(RP) set\nforthisMFD. Upon recei ving anMFD, thepeer inserts itinto\nitsdatabase. Hence each peer isresponsible fortheAV-pairs\nthat aremapped onto it.Forexample, node\f\n\u000ewill recei ve\nallMFDs thatcontain\u0000\u0013\u0012\n\u000e\n\u0014-\u0016\u000e\u0001.Figure 2showsamade-\nupsmall scale example ofhowthesystem works. The MFD\nis2-dimensional with thehorizontal coordinate corresponding\ntotheﬁrst attrib uteandthevertical coordinate corresponding\ntothesecond attrib ute. This isonly done forillustration pur-\nposes. Theactual system canhandle multidimensional data aswell. One canvisually observ ethedistrib ution oftheMFD\n(a,b,c,d,e,f)andhowtheyareassigned tonodes such asN[x=3] .\nBasically intwodimensions each node isresponsible forarow\noracolumn oftheattrib ute-v alue grid. Obviously ,thisresults\ninsigniﬁcant overlap between theentries ofthelocal databases\noneach node something which enhances therobustness ofthe\nsystem. Eveninthiscontri vedexample itcanbeseen thatthe\nload oneach node depends onthedistrib ution ofspeciﬁc AV\npairs such asx=3.This issue isaddressed bytheload balancing\nscheme described inSect. 5.3.\nSince thenumber ofAV-pairs inanMFD istypically small (e.g.,\u0000\n\u0000\u0002\u0001),thesizeoftheRPsetforanMFD issmall andregistra-\ntions canbedone efﬁciently .Different MFDs willhavedifferent\ncorresponding RPsets, which naturally separates thesystem’ s\nregistration load. Registering each AV-pair ofanMFD individ-\nually allowstheMFD tobesearched using anysubset ofits\nAV-pairs which important toallowtherichsetofpossible query\nsemantic wedesire forMIR.\n5.2MFDsearching\nWeclassify searches conducted byauser into twocategories:\nexact searches andsimilarity searches. Inanexact search, the\nuser islooking forMFDs thatmatch alltheAV-pairs speciﬁed\ninthequery simultaneously ,andanyextra AV-pairs thatmay\nbeintheMFDs butnotinthequery areignored. Suppose\nthequery is \u0003\n\u0010\u0000\u0013\u0012\n\u000e \u0014 \u0016\u0002\u000e\u0019\u0018\u0003\u001b\u0003\u001a\u0003\n\u0018\u0012\u0005\u0004\n\u0014 \u0016\u0004 \u0001.Since theMFDs\nthatmatch \u0003areregistered atRPpeers \f\n\u000ethrough \f\u0006\u0004,where\f  \n\u0014 '\u0004(\u0012\u0006 \n\u0014\u001e\u0016 \u0010,theCDS cansend asingle query message to\nanyofthese\u0002peers tohavethequery fully resolv ed.Forefﬁ-\ncient resolution, theCDS chooses thepeer thathasthesmallest\nMFD database. Once aquery isrecei ved,thepeer conducts a\npairwise comparison between thequery andalltheentries in\nitsdatabase toﬁnd thematching MFDs. Anexample may be\u0000artist=U2,year=1985,tempo=100bpm(beatsperminute)\u0001,which means thematched MFD must havetheabovethree\nAV-pairs intheir description. Most likelythenode thatcontains\nthelocations ofallU2songs willhavethesmallest local MFD\ndatabase soitwillbecontacted andlocally searched forMFDs\nwith thecorrect year andtempo.\nTofurther illustrate this process, inFigure 2,thequery\u0007\t\b\n\u0010\u0000\u000b\n\u0014\r\f\u0001will gotothecorresponding nodeN[y=2]\nand directly return thecorrect answer \u0000a,e \u0001.The query\u0007\t\u000e\n\u0010\u0000\u0010\u000f\n\u0014\u0012\u0011 \u0018\n\u0014\u0012\u0011\u0001will check thedatabase size ofnodes\nN[x=3] andN[y=3] ,select thesmallestN[y=3] ,andthen re-\nturnthecorrect answer \u0000b \u0001.Note thattheanswer isthelocation\nofthemusic ﬁlewhich canthen besubsequently downloaded.\nInasimilarity search, theuser istrying toﬁndmusic ﬁles that\nhaveasimilar feature vector towhat isspeciﬁed inthequery .\nSuppose theuser hasaclipunknown.mp3 with anextracted fea-\nturevector \u0000\u0014\u0013\n\u000e \u0014 \u0016\u0002\u000e\b\u0018\u0003\u001b\u0003\u001a\u0003\n\u0018\u0013\u0010\u0004\n\u0014 \u0016\u0004 \u0001,andwants toﬁndthe10\nsongs thataremost similar totheclip. Using thesame tech-\nnique asabove,theCDS may select apair,e.g.,\u0000\u0014\u0013\n\u000e \u0014%\u0016\u0002\u000e\u0001\nand send thequery tothepeer\f \u0004\n\u0014 '\u0004\u0015\u0013\n\u000e\n\u0014 \u0016\u000e\u0010 \u0010.This\npeer,instead ofconducting apairwise equality test, computes\nthe“distance” between thequery vector andeach MFD inits\ndatabase. Inourcurrent system, Manhattan distance deﬁned as\u0016\u0004\u0015\u0013\n\u0018\u0013\u0018\u0017 \u0010\n\u0014\u001a\u0019 \u0016\u000e\u001c\u001b\n\u0016\u0017\n\u000e\n\u0019\u001e\u001d\u0003\u001b\u0003\u001a\u0003\n\u001d\u001f\u0019 \u0016\u0004\n\u001b\n\u0016\u0017\u0004\n\u0019isused, where\n\u0016 ’sand\u0016\u0017 ’sarethevalues ofvector \u0013and \u0013 \u0017respecti vely.More sophis-\nticated wayofcomputing distance, such as“cosine distance”\nmay also beused. The distances arethen rankedandthe10110100100010000\n1 10 100 1000Number of files\nRank of AV-pairs\nFigure 3:Popularity distrib ution offeature attrib utes.\nMFDs thathavethesmallest distance arereturned totheuser.\nHowever,sending thequery topeer \falone will failtodis-\ncovertheMFDs thatslightly differfrom thequery in \u0013\n\u000e,but\naresimilar oridentical regarding other features, because those\nMFDs arenotregistered with \f.This isundesirable especially\nwhen\fdoes nothaveenough matches. Inthiscase, oursystem\nuses alimited expanding ring search togather more results: in\naddition to\f,thequery issent either by\forthequery ini-\ntiator topeers thatcorrespond tovalues thatarenear\n\u0016\u000e,e.g.,\u0013\n\u000e\n\u0014 \u0016\u000e\"!\n$\u0002\u0018\u0013\n\u000e\n\u0014 \u0016\u000e\"!$#\n\u0018\u0003\u001b\u0003\u001a\u0003.Accordingly ,these peers will\ncarry outthedistance computation andreturn anyresults. Fig-\nure2schematically showsthisexpanding ringsearch forquery\u0007\n\f \u0010\u0000\u0010\u000f\n\u0014\u0012\u0011 \u0018\n\u0014&%\u0001astwococentric circles. Ofcourse itis\nalso possible tocombine exact AVsearch andcontent-based\nsimilarity search. This lastpoint isimportant anddirectly in-\nﬂuenced thedesign ofoursystem. Although itispossible to\nusemore elaborate data structures formultidimensional nearest\nneighbor search (thesimilarity search) such asKD-trees (Bent-\nley(1975)) andtodistrib utethem overtheP2P netw orkthese\nstructures donotdirectly support searching forarbitrary sub-\nsetsofAV-pairs asoursystem does. Supporting such searches\nisimportant forMIR because wewould liketocombine both\nmetadata andcontent-based retrie valinthesame query .Finally ,\nrange queries such as\u0000tempo=80-120 bpm\u0001areresolv edbyis-\nsuing multiple queries corresponding tothevalues within the\nrange. Weareexperimenting with amore efﬁcient multiresolu-\ntionapproach torange searching where notonly values butalso\nranges ofvalues areassigned tonodes. That waythenumber of\nnodes thatneed tobevisited inorder toresolv earange query is\nreduced in \u0003\u0005\u0004\u0015')(\t*,+ \u0010where Listhelength oftherange.\n5.3Loadbalancing\nByusing Rendezv ousPoints, netw ork-wide message ﬂooding is\navoided atboth registration andquery times. However,inprac-\ntice, some AV-pairs may bemuch more common orpopular in\nMFDs than others. Ithasbeen observ edthatthepopularity of\nkeywords inGnutella follo wsaZipf-lik edistrib ution (Sripanid-\nkulchai (2001)). Such adistrib ution willcause afewpeers be-\ningoverloaded byregistrations orqueries, while themajority\nofpeers inthesystem stay underutilized. Figure 3showsadistrib ution ofAV-pairs computed from automatically extracted\nmusical content features described inmore detail inthesec-\ntion 6.Toimpro vesystem’ sthroughput under skewed load, the\nCDS deplo ysadistrib uted dynamic load balancing mechanism\ndescribed inGao andSteenkiste (2003), where multiple peers\nareused asRPpoints toshare theheavyload incurred bypopu-\nlarAV-pairs. When anAV-pair appears inmanyMFDs, instead\nofsending alltheMFDs toonepeer,thesystem partitions them\namong multiple peers. Similarly ifthere arealargenumber\nofqueries forthesame AV-pair,thesystem allowstheoriginal\npeer who isresponsible forthispairtoreplicate itsdatabase at\nother peers. Asaconcrete example some particular AVpair\nsuch astempo=100BPM might beverycommon. Partitioning\nthesetofsongs thathavethatparticular AVpair among mul-\ntiple RPpoints balances theregistration load. Another type of\nload which ispossibly independent oftheregistration load isthe\npopularity ofqueries forexampleartist=Madona, year=2003\nforanewrelease that will initially only beregistered with a\nfewRPs. Replicating their information balances thequery load.\nThe partitions andreplicas corresponding tooneAV-pair are\norganized intoatwodimensional logical matrix, theLoad Bal-\nancing Matrix (LBM), andthematrix automatically expands or\nshrinks based onthispair’scurrent registration andquery load.\nLBMs help toeliminate hotspots inthesystem under skewed\nload, andthesystem canmaintain high throughput inprocessing\nregistrations andqueries (Gao andSteenkiste (2003)).\n6SystemEvaluation\nThe MFEE isbuilt using Marsyas (Tzanetakis and Cook\n(2000)), afree softw areframe workforaudio analysis. We\nevaluate oursystem using anevent-dri vensimulator (Gao and\nSteenkiste (2003)). Forourexperiments, wesetupaP2P net-\nworkthathas10,000 peers, andeach peer isconﬁgured with\nDSL-le vellinkbandwidth ( \u0000\n\u0000\u0002\u0001\u0002\u0001\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007).AsMFDs, 30music\ncontent-based features areused asattrib utes. Theywere auto-\nmatically extracted from 5,000 MP3 ﬁles representing avariety\nofgenres andstyles. Figure 3showsthelog-log plot ofthe\nAV-pair distrib ution inthese ﬁles. There are2,178 distinct AV-\npairs, andthedistrib ution ishighly skewed: themost common\nAV-pair (rank ed1)appears in53% oftheMFDs and41AV-pairs\nonly appear in1MFD.\nForregistration workload, wegenerated 100,000 MFDs by\nreplicating each ofthe5,000 ﬁles 20times, andassigning them\ntorandom peers. Each peer registers theﬁles itisassigned with\nthesystem. Due totheskewed feature distrib ution, registrations\nofcommon AV-pairs result inmultiple partitions. Forquery\nload, 100,000 queries were generated follo wing aquery popu-\nlarity Zipf distrib ution which isindependent from theAV-pair\ndistrib ution shownaboveinFigure 3.Each query corresponds\ntothefeatures ofoneparticular music ﬁle.This isdone inorder\ntoemulate thebeha viorofauser who submits amusic clipand\nlooks forsimilar music. Themost popular MFD occurs inover\n10% ofthequeries, andthemajority oftheMFDs only occur\ninafewqueries. Aquery’ sinitiator israndomly pickedfrom\nallpeers, andforsimplicity ,only exact matches arereturned. A\npeer rejects aquery andgenerates afailure when thepeer’ slink\nutilization hasreached 100% duetosimultaneous queries.\nFigure 4compares thequery success rateasafunction ofquery\narrivalrate(Poisson arrival)tothesystem under twoscenarios.\nIntheﬁrst scenario, when reaching link capacity ,apeer sim-405060708090100\n5678910 20 304050Query success rate (%)\nQuery rate (1000 q/sec)Without replication\nWith replication\nFigure 4:Query success ratecomparison.\nplyrejects newqueries thatarriveatitwithout replicating its\ncontent atother peers. Since foreach query theCDS has30\ncandidate AV-pairs, query load isspread evenly among peers\nevenwithout replication. Therefore thesystem achie vesahigh\nsuccess rateunder high load, e.g., thesuccess rateis94% for\naquery rateof\n$\u0001\n\u0018\u0001\u0002\u0001\u0002\u0001\n\t\u0002\u000b\f\u0007\u0004\n\t\u000e\r\u0010\u000f\u0012\u0011\n!\u000f\u0013\u0007\u0013\u000b\f\u0007\u0010.However,asload in-\ncreases further ,peers corresponding topopular queries willbe\nsaturated, andthesuccess ratedrops quickly .Inthesecond sce-\nnario, byusing thedynamic replication mechanism, peers who\nobserv ehigh load willreplicate their databases atother peers to\ndissipate concentrated query load. Asaresult, weobserv ethat\nwith replication, thesystem cansustain amuch higher query\nratewhile keeping thesuccess rateabove95%.\n7Conclusions\nInthispaper ,wedescribed ascalable, andload-balanced P2P\nsystem that supports arich setofmusic search methods. In\nparticular ,ourautomatic music feature extraction technique\nenables sophisticated music content based searches such as\ncontent-based similarity retrie val.The RP-based registration\nandquery speciﬁcation scheme ensures system scalability by\navoiding netw orkwide message ﬂooding encountered incur-\nrent P2P systems. Weevaluated thesystem using arealistic\nregistration load obtained from alargesetofmusic ﬁles. Our\ndynamic load balancing mechanism allowsthesystem tomain-\ntainhigh throughput under skewed Zipf query load. Itisour\nhope thatthedesign ofoursystem will inspire additional re-\nsearch intheinteresting area ofbringing state oftheartMu-\nsicInformation Retrei valalgorithms totheincreasingly popular\nPeer-to-Peer netw orks. Another important aspect ofthepro-\nposed system isthatnewattrib utescanbeincorporated intothe\nsystem with minimum effort.\nWearecurrently reﬁning thedesign ofoursystem tohandle\nrange queries more efﬁciently andplan tofurther evaluate our\nsystem with traces acquired from real users. Further experi-\nments arenecessary foramore detailed evaluation ofsystem\nperformance. Inaddition, user studies need tobeconducted\ntoexplore howtheusers interact with thesystem andwhat are\ntheir typical queries. Weareworking onimplementing apro-totype ofoursystem across amedium area (university campus)\nLAN toobtain more information about usage patterns andthe\nperformance ofthesystem.\nInorder tohandle different types ofrange queries weareplan-\nning toexplore multi-resolution quantization grids. Another\nimportant direction istheandinclusion andcreation ofvarious\nquery speciﬁcation user interf aces forspecifying theMFDs. Fi-\nnally ,duplicate copies ofthesame audio content canbedetected\nbyadapting anaudio ﬁngerprinting scheme such asHaitsma and\nKalk er(2002) toworkwith ourP2Psystem.\nAcknowledgements\nWegratefully ackno wledge thesupport oftheNational Science\nFoundation under grant ISS-008594. This research wasalso\nsponsored inpart bytheDefense Advanced Research Project\nAgenc yand monitored byAFRL/IFGA, Rome NY 13441-\n4505, under contract F30602-99-1-0518. Additional support\nwasprovided byIntel.\nReferences\nAucouturier ,J.-J.andPachet, F.(2002). Music Similarity Mea-\nsures :What’ stheuse?InProc.Int.Conf.onMusicInforma-\ntionRetrieval(ISMIR) ,pages 157–163, Paris, France.\nAucouturier ,J.J.andPachet, F.(2003). Musical Genre: aSur-\nvey.JournalofNewMusicResearch,32(1).\nBaumann, S.(2003). Music Similarity Analysis inaP2P en-\nvironment. InProc.ofthe4thEuropeanWorkshoponImage\nAnalysisforMultimedia InteractiveServices ,pages 314–319,\nLondon, UK.\nBaumann, S.andKluter ,A.(2002). Super -Con venience for\nNon-musicians: Querying MP3 andtheSemantic Web.InProc.\nInt.Conf.onMusicInformation Retrieval(ISMIR) ,pages 297–\n298, Paris, France.\nBentle y,J.(1975). Multidimensional Binary Search Trees used\nforAssociati veSearching. Communications oftheACM,18(9).\nFoote, J.andMathe w,C.(2002). Audio Retrie valbyRhythmic\nSimilarity .InProc.Int.Conf.onMusicInformation Retrieval\n(ISMIR) ,pages 265–266, Paris, France.\nFutrelle, J.andDownie, S.J.(2002). Interdisciplinary Commu-\nnities andResearch Issues inMusic Infomration Retrie val.In\nProc.Int.Conf.onMusicInformation Retrieval(ISMIR) ,pages\n215–221, Paris, France.\nGao, J.andSteenkiste, P.(2003). Design andEvaluation ofa\nDistrib uted Scalable Content Disco verySystem.IEEEJournal\nonSelectedAreasinCommunications (SpecialIssue:Recent\nAdvances inServiceOverlayNetworks) .(toappear).\nGao, J.,Tzanetakis, G.,andSteenkiste, P.(2003). Content-\nbased retrie valofmusic inscalable peer-to-peer netw orks. In\nProc.Int.ConferenceonMultimedia andExpo(ICME) ,Balti-\nmore, US.\nHaitsma, J.andKalk er,T.(2002). AHighly RobustAudio Fin-\ngerprinting System. InProc.Int.Conf.onMusicInformation\nRetrieval(ISMIR) ,pages 107–115, Paris, France.Logan, B.andSalomon, A.(2001). AMusic Similarity Func-\ntionbased onSignal Analysis. InInt.Conf.onMultimedia and\nExpo(ICME) .IEEE.\nPachet, F.(2003). Content Management forElectronic Music\nDistrib ution: The Real Issues.Communications oftheACM,\n46(4).\nRauber ,A.,Pampalk, E.,andMerkl, D.(2002). Using Psycho-\nAcoustic Models andSelf-Or ganizing Maps toCreate aHier-\narchical Structure ofMusic bySound Similarity .InProc.Int.\nConf.MusicInformation Retrieval(ISMIR) ,pages 71–80, Paris,\nFrance.\nRowstron, A.andDruschel, P.(2001). Pastry: Scalable Dis-\ntributed Object Location andRouting forLarge-scale Peer-to-\nPeer Systems. InProc.ofMiddleware,pages Heidelber g,Ger-\nmany.\nSripanidkulchai, K.(2001). ThePopularity ofGnutella Queries\nandItsImplications onScalability .http://www.cs.cmu.\nedu/˜kunwadee/research/p2p/gnutella .html ,.\nStoica, I.,Morris, R.,Karger,D.,Kaashoek, F.,andBalakrish-\nnan, H.(2001). Chord: AScalable Peer-To-Peer Lookup Ser-\nvice forInternet Applications. InProc.ofSIGCOMM ,pages\n149–160, SanDiego,CA.\nTolonen, T.andKarjalainen, M.(2000). AComputationally\nEfﬁcient Multipitch Analysis Model.IEEETrans.onSpeech\nandAudioProcessing ,8(6):708–716.\nTzanetakis, G.andCook, P.(2000). Marsyas: Aframe workfor\naudio analysis. OrganisedSound ,4(3).\nTzanetakis, G.andCook, P.(2002). Musical Genre Classiﬁca-\ntionofAudio Signals.IEEETransactions onSpeechandAudio\nProcessing ,10(5).\nTzanetakis, G.,Ermolinsk yi,A.,and Cook, P.(2002). Be-\nyond theQuery-by-Example Paradigm: NewQuery Interf aces\nforMusic Information Retrie val.InProc.Int.Computer Music\nConference(ICMC) .Gothenb urg,Sweden.\nWang, C.,Li,J.,andShi, S.(2002). AKind ofContent-Based\nMusic Information Retrie valMethod inaPeer-to-Peer Environ-\nment. InProc.Int.Conf.onMusicInformation Retrieval(IS-\nMIR) ,pages 178–186, Paris, France.\nWhitman, B.andSmaragdis, P.(2002). Combining Musical and\nCultural Features forIntelligent Style Detection. InProc.Int.\nConf.MusicInformation Retrieval(ISMIR) ,pages 47–52, Paris,\nFrance.\nYang, C.(2002). TheMACSIS Acoustic Indexing Frame work\nforMusic Retrie val:AnExperimental Study .InProc.Int.Conf.\nonMusicInformation Retrieval,pages 52–62, Paris, France."
    },
    {
        "title": "Was Parsons right? An experiment in usability of music representations for melody-based music retrieval.",
        "author": [
            "Alexandra L. Uitdenbogerd",
            "Yaw Wah Yap"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1418225",
        "url": "https://doi.org/10.5281/zenodo.1418225",
        "ee": "https://zenodo.org/records/1418225/files/UitdenbogerdY03.pdf",
        "abstract": "In 1975 Parsons developed his dictionary of musical themes based on a simple contour representation. The motivation was that people with little training in mu- sic would be able to identify pieces of music. We decided to test whether people of various levels of musical skill could indeed make use of a text repre- sentation to describe a simple melody query. The re- sults indicate that the task is beyond those who are unmusical, and that a scale numeric representation is easier than a contour one for those of moderate mu- sical skill. Further, a common error when using the scale representation still yields a more accurate con- tour representation than if a user is asked to enter a contour query. We observed an average query length of about seven symbols for the retrieval task. 1",
        "zenodo_id": 1418225,
        "dblp_key": "conf/ismir/UitdenbogerdY03",
        "keywords": [
            "Parsons",
            "dictionary",
            "musical",
            "themes",
            "simple",
            "contour",
            "representation",
            "musical",
            "skill",
            "query"
        ],
        "content": "WasParsonsright?Anexperiment inusabilityofmusicrepresentations for\nmelody-based musicretrieval\nAlexandra L.Uitdenbogerd andYawWahYap\nDepartment ofComputer Science, RMIT University\nGPO Box 2476V ,Melbourne 3001, Australia\n+613 9925 4115\nalu@cs.rmit.edu.au\u0003\nAbstract\nIn1975 Parsons developed hisdictionary ofmusical\nthemes based onasimple contour representation. The\nmotivation wasthatpeople with little training inmu-\nsicwould beable toidentify pieces ofmusic. We\ndecided totestwhether people ofvarious levelsof\nmusical skill could indeed makeuseofatextrepre-\nsentation todescribe asimple melody query .There-\nsults indicate thatthetask isbeyond those who are\nunmusical, andthatascale numeric representation is\neasier than acontour oneforthose ofmoderate mu-\nsical skill. Further ,acommon error when using the\nscale representation stillyields amore accurate con-\ntour representation than ifauser isaskedtoenter a\ncontour query .Weobserv edanaverage query length\nofabout sevensymbols fortheretrie valtask.\n1Introduction\nDespite therecent increase ininterest inmusic information re-\ntrievalwestillknowverylittle about theusability ofsuch sys-\ntems. Forsung queries, wecanbecertain thatthecontour is\nfairly accurate, despite theinterv alsbetween notes being en-\nlargedorreduced, andkeydrift thatmay occur (Lindsay ,1996;\nMcNab etal.,1996). However,manyofthesystems currently\navailable inlibraries orontheInternet useinput intheform of\natextstring.\nIn1975 Parsons published his“The Directory ofTunes” (Par-\nsons, 1975) —anextension ofBarlo wand Morganstern’ s\nearlier “ADictionary ofMusical Themes” (Barlo wandMor-\nganstern, 1948). Parsons’ saim wastoproduce asolution to\nlocating music bytheme fornon-musicians. Each melody\ninthevolume wasrepresented byacontour string consist-\ningofsymbols: “U”representing up,“D” representing down,\nand“R” representing repeat orsame note. Forexample, the\ntheme from Beetho ven’sFifth Symphon ywould beencoded\n\u0003Contact author forallenquiries\nPermission tomakedigital orhard copies ofallorpartofthiswork\nforpersonal orclassroom useisgranted without feeprovided that\ncopies arenotmade ordistrib uted forproﬁt orcommercial advan-\ntage andthatcopies bear thisnotice andthefullcitation ontheﬁrst\npage. c\r2003 Johns Hopkins University .as“*RRDURRD”, with theasterisk representing theﬁrst note.\nTheuser merely hadtoworkoutthecontour ofthetheme they\nwished tosearch forandcould then look itupinthedictionary .\nThequestion wewished toaddress was“Can novice musicians\nformulate amelody query inatextformat?”. Weconducted ex-\nperiments using aprototype melody-based music retrie valsys-\ntem, requiring users toenter asimple query using oneofthree\ntextrepresentations ofqueries. Wefound thatthetask wasvir-\ntually impossible formusic novices. Those with some music\ntraining hadsome success, andonly themost musically skilled\nusers were able tocreate queries reliably .Inthispaper weﬁrst\ndiscuss related workinmusic psychology ,usability engineering\nmethodology ,andmusic retrie valuser interf aces. Wethen de-\nscribe themusic representations used inourexperiments inthe\nconte xtofmusic representations ingeneral. This isfollo wed by\nareport onourexperiments.\n2RelatedWork\nWhen itcomes tomusical perception, psychologists have\nclearly identiﬁed different classes ofpeople: unskilled, mu-\nsically skilled, and those with aprofessional levelofmusic\nskill (Franc `es,1958). Unskilled orbeginner musicians havea\nholistic perception ofmusic, whereas those with musical skill\nusecogniti veprocesses inlistening tomusic. Highly skilled\nmusicians useboth forms ofperception.\nForusers ofmusic retrie valsystems severaldifferent beha viours\nandskills arerequired, including theability torecall amelody ,\nreproduce itinsome way—forexample, singing it—andalso\ncompare musical examples with thepiece theyaresearching\nfor.There isnotalotpublished thatexamines thecapabilites of\nusers with respect tothethese tasks. Wesummarise thework\nthatweknowabout below.\nThere havebeen twosetsofexperiments thatexamine thebe-\nhaviour oflisteners andsingers. Their aimwastodisco verthe\ntypes oferrors people makewhen singing agivenmelody .Mc-\nNab etal.(1996) studied people’ srecall offamiliar melodies.\nLargeinterv alswere reduced insize andsmall interv alswere\nincreased. Errors were greater with unusual changes intonal-\nity.Accurac ydepended more onsinging experience than mu-\nsical training. Some songs were commenced atthechorus.\nLindsay (1996) examined howwell people sang short unkno wn\nmelodies. Hisstudy agrees with McNab’ sregarding interv al\nsize. Healso notes thatinterv alswere more accurate than the\nabsolute pitch, andthatsingers didn’ tincrease their error astheyprogressed butthatvariance remained fairly constant through-\nout.\nUitdenbogerd etal.(????) disco vered thatitwasaverydifﬁcult\ntaskforusers tocompare unkno wnpieces ofmusic, howeverif\nthepiece being lookedforwasknown,then arapid identiﬁca-\ntionofwhether ananswer wasrelevantornotcould bemade.\nMore recent workonaspects ofusability ofmusic retrie val\nsystems includes Blandford andStelmasze wska’ sanalysis of\nseveralon-line systems (Blandford andStelmasze wska, 2002).\nTheyconcluded thatthere were manyproblems regarding us-\nability ,particularly thedissonance between auser’ srepresenta-\ntionoftheir information need andthatofthesystem.\nTheexperiments reported inourpaper arepartly based onthe\nmethodology ofusability engineering. Partoftheprocess ofus-\nability engineering involvesmeasuring aspects ofauser’ sinter-\naction with asystem. The ﬁvemain measurable attrib utes are\nlearnability ,efﬁciency ,memor ability ,errorrate,andsatisfac-\ntion(forabrief introduction tousability ,seeFerr´eandN.Juristo\n(2001)). These attrib utes aremostly quantiﬁed bythenumber\noftasks performed bytheuser pertime unit, andthenumber\noferrors pertask orpertime unit. Forexample, learnability is\nmeasured byrecording howlong ittakesforauser toreach a\ndeﬁned levelofproﬁcienc y.Satisf action isusually measured by\nsurveying users.\n3MusicRepresentation\nThere aremanywaysinwhich music canberepresented. Our\nfocus inthisworkisrepresenting amelody only.Interms of\npitch amelody canberepresented using absolute pitch, pitch\nrelati vetoareference note, orrelati vetotheprevious note in\nthemelody .Forrhythm, thesame types ofrepresentation ap-\nply,thatis,absolute durations, relati vetoastandard duration,\norrelati vetotheprevious duration. Inaddition tothepitch and\nrhythm relativity ,another factor thatforms partofitsrepresen-\ntation isitsprecision .Forexample, acontour representation has\nverylowprecision, consisting ofonly three different symbols,\nwhereas arepresentation thatdeﬁnes each pitch interms ofthe\nexact number ofsemitones hashigh precision. Some represen-\ntations also include athird element, thatofstress.Again this\ncanberepresented inabsolute orrelati veterms, andcanhave\ndifferent levelsofprecision. Intheworkreported here were-\nstricted ourselv estorepresentations oflowtomedium precision\ninpitch, andignored rhythm andstress. The three represen-\ntations used were contour ,anextended contour ,andasimple\nnumeric representation using thenumbers from onetoeight to\nsignify theeight notes ofthemajor scale. This lastrepresenta-\ntionwasincompletely deﬁned, inthatusers were nottoldhow\ntowrite queries thatextended beyond anoctaveorhowtode-\nﬁne notes outside ofthescale, such asﬂattened notes. Wenow\ndescribe therepresentations inmore detail.\nContour\nAmelody’ scontour represents amelody’ sshape interms of\npitch direction only.Thecontour representation ofthemelody\nfragment showninFigure 1,using Uforup,DfordownandS\n(orsometimes Risused) forsame pitch, is:\nSUSUSDDSDSDSDContour representation hastheadvantage thatsingers usually\ngetthecontour ofamelody right butusually don’tsing thein-\ntervalsaccurately ,animportant consideration when queries are\nsung. Amelody query would need tobequite long forrelevant\nanswers tobefound, however.Inparticular ,twomelodies can\nberepresented bythesame contour string yethavenopercei ved\nsimilarity .Forexample, theﬁrst phrase ofTwinkle Twinkle Lit-\ntleStarhasthesame contour (and rhythm) asthatofthesecond\nmovement ofHaydn’ sSurprise Symphon y,yetthemelodies are\nquite different. What hasn’ tbeen reported before, however,is\nwhether people canwrite contour queries.\nExtended Contour\nExtended contour isamore ﬁne-grained approach tomelody\nrepresentation. Inthisversion ofextended contour ,wedistin-\nguish between largeandsmall interv alswith adifferent sym-\nbol. Forexample, wecould useUforalargeinterv alupwards,\nuforasmall one, andsimilarly useDanddforlargeandsmall\ndownw ardinterv alsrespecti vely.Adecision needs tobemade\nregarding theclassiﬁcation ofinterv alsaslargeandsmall. Itis\nclear thatstep interv alsofoneortwosemitones aresmall and\ninterv alsthataregreater than ﬁvesemitones arelarge.Ifwe\nusethemusical concept ofsteps andleaps, then allinterv als\nofthree ormore semitones would beclassed aslargeinterv als.\nIfweused theentrop y-maximising classiﬁcation approach of\nDownie (1998), then thesame decision would bereached, as\ninterv alsofoneortwosemitones arethemost frequently oc-\ncurring interv alsinmelodies. Inthiscase, theexample melody\nwould berepresented as:\nSUSuSddSdSdSd\nTheadvantage ofthistechnique isthatitstillallowsforinaccu-\nratesinging, butwould bemore discriminating when search-\ningadatabase ofmelodies. When considering pitch errors\nofsingers, however,there may stillbesome mis-classiﬁcation\nofinterv alsassingers often haveanerror ofuptoonesemi-\ntone (Lindsay ,1996).\nNumericScale\nUnlik ethetwocontour -based representations described above,\nthenumeric scale representation isrelati vetoakeynote. The\nkeynote isrepresented bythenumber one, andthenotes ofthe\nmajor scale arenumbered contiguously from onetoeight. This\nisamethod ofdescribing thenotes ofascale thatisoften used\nwhen learning western music. Itisalsothebasis ofamusic no-\ntation form thatisfrequently used bychoirs from severalAsian\ncountries, andisclosely related tothesol-fa system.\nTheexample melody would berepresented as:\n11556654433221\nThese representations havebeen used insome form formelody\nmatching systems, either directly (Blackb urnandRoure, 1998;\nParsons, 1975), orindirectly (Ghias etal.,1995; Prechelt and\nTypke,2001). However,little isknownabout users’ ability to\nusesuch representations.\n4Experiments\nOuraimwastodetermine theability ofarange ofusers infor-\nmulating amelody query foramusic search engine. Wewanted\u0000\u0000\u0001 \u0001\n\u0001 \u0001\n\u0001 \u0001 \u0002\u0001 \u0001\u0001 \u0001\u0001 \u0001 \u0002\nFigure 1:Theﬁrsttwophrases ofTwinkle Twinkle Little Star inCommon Music Notation (CMN)\ntolearn more about theusability ofmusic search engines byfo-\ncusing ontheconcepts oflearnability ,user error rateandsatis-\nfaction (Ferr ´eandN.Juristo, 2001). Due tolimited availability\nofvolunteers wechose nottomeasure howmemorable theuser\ninterf acewasforrepeated use.\nWedeveloped aprototype search engine thatmade useofthe\ncollection of10,466 MIDI ﬁles used inourearlier work(Uit-\ndenbogerd, 2002; Uitdenbogerd etal.,????; Uitdenbogerd and\nZobel, 1999, 2002) plus four simple monophonic melodies that\nwere used astargetsforqueries. Weused thedirected modulo-\n12melody representation and5-gram coordinate matching as\nthemelody similarity measurement technique (Uitdenbogerd\nandZobel, 2002). Asimple web-based front-end wasdevel-\noped, with instructions totheuser including examples ofeach\nofthethree query representations. Users were askedtoformu-\nlateaquery forTwinkle Twinkle Little Star andenter thisinto\nthesearch engine. Aquery wasconsidered asuccess ifthesong\nappeared intherankedlistofanswers presented totheuser.\nThere were 36participants intheexperiment, most ofwhom\nwere members oftheuniversity community .Signiﬁcant co-\nhorts within thegroup were members oftheuniversity choir\nclub, andcomputer science research students. Participants were\naskedtoself-assess whether theywere non-musicians, novices,\nintermediate orprofessional levelofmusic skill. The number\nofparticipants ineach category isshowninTable 1.Partici-\npants alsoprovided other general information about themselv es\nincluding their computer skill level.Some questions were also\naskedabout theusers’ perception ofthesystem, including over-\nallsatisf action. During theexperiment afurther measure of\nusers’ satisf action orfrustration wastakenbyobserving their\nfacial expressions during thetask.\nIfusers didn’ tsucceed inretrie ving thetargetanswer ,theywere\npermitted tocreate asecond query .Toallowfeedback from\nusers regarding thepercei veddifﬁculty ofthedifferent repre-\nsentations users were askedtotryeach representation forthe\nsame task. However,asparticipation wasvoluntary ,weonly\nhaveasmall number ofresults forusers trying allrepresenta-\ntions.\nResults\nMost results areshownintables 1to6andsome aredescribed\nbelow.Wehadareasonable spread ofabilities across theset\nofusers. Users generally feltsatisﬁed with thesystem, with\nthemost skilled users being more satisﬁed than those with no\nmusical skill (Table 6).Theywere generally happ ywith the\nscreen navigation (Table 5)butsome non-e xpert users found\nthesystem hard tolearn (Table 4).\nUsers who tried more than onerepresentation were fairly con-\nsistent intheir preferences. Thirteen users tried contour repre-\nsentation ﬁrst andthen later used scale. Ninety-tw opercent of\nthese users preferred scale. Similarly ,ofthetwelv eusers whotried scale representation before contour ,75% preferred scale.\nThe only group ofusers that preferred contour toscale were\nnovice users, buttheresults aretooclose tobesigniﬁcant given\nthenumber ofparticipants inthesurvey.\nUsers made afewdifferent types oferror when entering queries\ninto thissystem. The most common type oferror wastouse\nthewrong representation type. Forexample, after choosing to\nuseextended contour theuser might enter acontour string asa\nquery bymistak e.\nDifferent types oftranscription error occurred forthedifferent\nrepresentation types. Withcontour ,about 41% ofqueries were\nentered with anextrasymbol atthestart. Typically these queries\nlookedlikeSSUSUSD instead ofSUSUSD. This isduetothe\nneed forusers toenter asymbol fortheﬁrst note, despite the\ncontour method representing thetransitions between notes. Par-\nsons attempted toaddress thisproblem byrequiring users toﬁrst\nwrite downanasterisk torepresent theﬁrst note. This would\nprobably impro vewritten queries tosome extent. Despite the\nextrasymbol atthestart, themusic search engine retrie vesthe\nanswer ,asthematching method counts thenumber ofn-grams\nthatareidentical between thequery andpotential answers but\ndoesn’ tpenalise forthose thatdonotoccur .\nWedisco vered that 31% ofﬁrst scale queries were incorrect\nscale representations, butwere correct when converted tocon-\ntourrepresentations. Forexample, instead ofentering (11556\n65),some users entered (1122332),or(1133553),which\nstillconvertstoSUSUSD.\nRemarkably ,all36query attempts byusers with nomusical\nknowledge failed —aclear result despite thesample size of\n13people. Allgroups except forexperts expressed frustration\nwhen attempting thetask andfailed more often than theysuc-\nceeded. Despite failing consistently ,themusically unskilled\ngroup showed considerable interest inmusic, with over60%\nofthese users stating thattheylistened tomusic more than ﬁve\ntimes aweek. However,wecanseefrom theresults thatquery-\ningmusic search engines viaatextrepresentation isclearly\nsomething forexperts only.\nWeobserv edthatmost users entered aquery representing the\nﬁrst phrase ofthesong only,andinsome cases anincomplete\nphrase. The average query length wasjustunder sevensym-\nbols, andthiswasfairly consistent across different user groups.\nThis query length isconsiderably shorter than thatfound inUit-\ndenbogerd etal.(????), which wasbased onanexpert musician\nformulating queries forspeciﬁc pieces without theuseofamu-\nsicretrie valsystem. This tends tosuggest thatrealqueries are\nlikelytobequite short.\n5Conclusions\nMusic search engines varyintheir user interf aceandthetype\nofquery thatusers arerequired toenter .Manycurrent systemsMusic Background Number ofUsers Percentage (%)\nZero 13 36\nNovice 10 28\nIntermediate 8 22\nProfessional 5 14\nTotal 36 100\nTable 1:Distrib ution ofUsers\nMusic Background Contour Extended Contour Numeric Scale\nZero 0/19 (0.00%) 0/3 (0.00%) 0/14 (0.00%)\nNovice 1/16 (6.25%) 1/6 (16.67%) 3/12 (25.00%)\nIntermediate 1/11 (9.09%) 1/3 (33.33%) 5/12 (41.67%)\nProfessional 4/07 (57.14%) 4/6 (66.67%) 4/06 (66.67%)\nTable 2:Proportion ofsuccesses bytype ofusers. Theﬁrst number ineach cellrepresents thenumber ofsuccessful queries during\ntheexperiment forthattype ofuser,thesecond isthetotal number oftrials. This isfollo wed bytheproportion asapercentage.\nMusic Background Excellent Good Moderate Hard toUse\nZero 1(07.69%) 7(53.85%) 4(30.77%) 1(07.69%)\nNovice 0(00.00%) 7(70.00%) 3(30.00%) 0(00.00%)\nIntermediate 0(00.00%) 6(75.00%) 1(12.50%) 1(12.50%)\nProfessional 2(40.00%) 3(60.00%) 0(00.00%) 0(00.00%)\nTotal 3(08.33%) 23(63.89%) 8(22.22%) 2(05.56%)\nTable 3:Result oftheuser surveyquestion on“user friendliness”. Theﬁrst number ineach entry represents thenumber ofusers,\nandthesecond isthepercentage ofthattype ofuser giving theresponse.\nMusic Background Easy Moderate Hard VeryHard\nZero 4(30.77%) 4(30.77%) 4(30.77%) 1(7.69%)\nNovice 2(20.00%) 5(50.00%) 3(30.00%) 0(0.00%)\nIntermediate 1(12.50%) 6(75.00%) 1(12.50%) 0(0.00%)\nProfessional 2(40.00%) 3(60.00%) 0(00.00%) 0(0.00%)\nTotal 9(25.00%) 18(50.00%) 8(22.22%) 1(2.78%)\nTable 4:Result oftheuser surveyquestion on“learnability”.\nMusic Background Excellent Good Confusing Complicated\nZero 2(15.38%) 11(84.62%) 0(00.00%) 0(0.00%)\nNovice 3(30.00%) 7(70.00%) 0(00.00%) 0(0.00%)\nIntermediate 0(00.00%) 7(87.50%) 1(12.50%) 0(0.00%)\nProfessional 5(100.00%) 0(00.00%) 0(00.00%) 0(0.00%)\nTotal 10(27.78%) 25(69.44%) 1(02.78%) 0(0.00%)\nTable 5:Result oftheuser surveyquestion on“screen navigation”.\nMusic Background VerySatisﬁed Satisﬁed Moderate Poor VeryPoor\nZero 1(07.69%) 4(30.77%) 6(46.15%) 2(15.38%) 0(0.00%)\nNovice 1(10.00%) 5(50.00%) 4(40.00%) 0(00.00%) 0(0.00%)\nIntermediate 1(12.50%) 5(62.50%) 1(12.50%) 1(12.50%) 0(0.00%)\nProfessional 2(40.00%) 3(60.00%) 0(00.00%) 0(00.00%) 0(0.00%)\nTotal 5(13.89%) 17(47.22%) 11(30.56%) 3(08.33%) 0(0.00%)\nTable 6:Result oftheuser surveyquestion on“overall satisf action”.ontheInternet useatextrepresentation ofsome kind. Simi-\nlarly,those fewmusic library systems thatcontain asearchable\nmelodic representation alsouseatextual form. Ourexperiments\nshowthatthese types ofinterf aces arebeyond theaverage per-\nson,andrequire amusical expert toprepare queries. Inaddition\nwedisco vered thatusers areunable touseasimple contour rep-\nresentation despite thismethod —atleast theoretically —not\nrequiring musical knowledge toconstruct. Itseems thatthecog-\nnitiveprocesses thatarerequired must bedeveloped bymusical\ntraining ortheuser must possess considerable natural ability .\nContour representaion, while notuseful asadirect method of\nquerying asystem, may still beused indirectly forretrie val.\nThis practice already occurs forquery-by-humming systems,\nbutcanalso apply toother types ofuser interf ace. Ifaquery\nusing anumeric representation isunsuccessful, thecontour of\nthequery canbeused toretrie vesome results. However,as\nseen elsewhere (Uitdenbogerd andZobel, 1999), such queries\nwillneed tobelonger toretrie verelevantresults.\nInorder foranordinary user toretrie vefrom amusic search en-\ngine based onmelody ,theywillneed toeither sing, orusesome\nother form ofaudio input. And what ofParsons? While an\ninteresting concept, andacuriosity formusic library users, un-\nfortunately thedirectory isbeyond thecapabilities ofitstarget\naudience.\nAcknowledgements\nWethank thevolunteers thathelped with ourexperiments.\nReferences\nBarlo wand Morganstern (1948). Adictionary ofMusical\nThemes .\nBlackb urn,S.andRoure, D.D.(1998). Atoolforcontent-based\nnavigation ofmusic. InProc.ACMInternational Multimedia\nConfer ence.ACM.\nBlandford, A.andStelmasze wska, H.(2002). Usability ofmu-\nsical digital libraries: amultimodal analysis. InM.Finger -\nhut, editor ,International Symposium onMusic Information\nRetrie val,volume 3,Paris, France.\nDownie, J.S.(1998). Informetrics andmusic information re-\ntrieval:aninformetric examination ofafolksong database.\nInProceedings oftheCanadian Association forInformation\nScience ,1998 Annual Confer ence,Ottawa,Ontario. CAIS.\nFerr´e,X.andN.Juristo, H.Windl, L.C.(2001). Usability ba-\nsicsforsoftw aredevelopers. IEEE Softwar e,18(1),22–29.\nFranc `es,R.(1958). LaPerception delaMusique .L.Erlbaum,\nHillsdale, NewJerse y.Translated byW.J.Dowling (1988).\nGhias, A.,Logan, J.,Chamberlin, D.,andSmith, B.(1995).\nQuery byhumming —musical information retrie valinan\naudio database. InProc.ACMInternational Multimedia Con-\nference.\nLindsay ,A.T.(1996). Using contour asamid-le velrepresen-\ntation ofmelody .Master’ sthesis, MIT, Massachusetts.\nMcNab, R.J.,Smith, L.A.,Witten, I.H.,Henderson, C.L.,and\nCunningham, S.J.(1996). Towards thedigital music library:\nTuneretrie valfrom acoustic input. InProc.ACM Digital\nLibraries .\nParsons, D.(1975). TheDirectory ofTunes.Spencer Brown\nandCo., Cambridge, England.Prechelt, L.andTypke,R.(2001). Aninterf aceformelody\ninput. ACMTransactions onComputer -Human Inter action ,\n8(2),133–149.\nUitdenbogerd, A.L.(2002). Music Information Retrie valTech-\nnology.Ph.D. thesis, School ofComputer Science andInfor -\nmation Technology ,RMIT University ,Melbourne, Victoria,\nAustralia.\nUitdenbogerd, A.L.andZobel, J.(1999). Melodic match-\ningtechniques forlargemusic databases. InD.Bulter -\nman, K.Jeffay,and H.J.Zhang, editors, Proc.ACM In-\nternational Multimedia Confer ence,pages 57–66, Orlando\nFlorida, USA. ACM, ACMPress.\nUitdenbogerd, A.L.and Zobel, J.(2002). Music ranking\ntechniques evaluated. InM.Oudshoorn, editor ,Proc.Aus-\ntralasian Computer Science Confer ence,Melbourne, Aus-\ntralia.\nUitdenbogerd, A.L.,Chattaraj, A.,andZobel, J.(????). Music\ninformation retrie val:Past,present andfuture. InD.Byrd,\nJ.S.Downie, andT.Crawford, editors, Curr entResear chin\nMusic Information Retrie val: Searching Audio, MIDI, and\nNotation .Kluwer .(originally presented atISMIR 2000), to\nappear ."
    },
    {
        "title": "Geometric algorithms for transposition invariant content based music retrieval.",
        "author": [
            "Esko Ukkonen",
            "Kjell Lemström",
            "Veli Mäkinen"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1417477",
        "url": "https://doi.org/10.5281/zenodo.1417477",
        "ee": "https://zenodo.org/records/1417477/files/UkkonenLM03.pdf",
        "abstract": "We represent music as sets of points or sets of hori- zontal line segments in the Euclidean plane. Via this geometric representation we cast transposition invari- ant content-based music retrieval problems as ones of matching sets of points or sets of horizontal line segments in plane under translations. For finding the exact occurrences of a point set (the query pattern) of size \u0001 within another point set (representing the database) of size \u0002 , we give an algorithm with run- ning time \u0003\u0005\u0004 \u0001\u0006\u0002\b\u0007 , and for finding partial occurrences another algorithm with running time \u0003\u0005\u0004 \u0001\u0006\u0002 \u000e\r\u000f\u0001\u0010\u0007 . We also use the total length of the overlap between the line segments of a translated query and a database (i.e., the shared time) as a quality measure of an oc- currence and present an \u0003\u0005\u0004 \u0002 \u0011\u000b\u0012\r\u000f\u0002\u0014\u0013\u0015\u0001\u0006\u0002 \u000e\r\u0016\u0001\u0017\u0007 algo- rithm for finding translations giving the largest possi- ble overlap. Some experimental results on the perfor- mance of the algorithms are reported. 1",
        "zenodo_id": 1417477,
        "dblp_key": "conf/ismir/UkkonenLM03",
        "keywords": [
            "music",
            "sets",
            "points",
            "hori- zontal",
            "line",
            "segments",
            "Euclidean",
            "plane",
            "transposition",
            "content-based"
        ],
        "content": "Geometric Algorithms forTransposition InvariantContent-Based MusicRetrieval\n\u0000\nEskoUkkonen, Kjell Lemström, andVeliMäkinen\nDepartment ofComputer Science, University ofHelsinki\nP.O.Box 26(Teollisuuskatu 23),FIN-00014 Helsinki, Finland\n{ukkonen,klemstro,vmakinen}@cs.hels inki.fi\nAbstract\nWerepresent music assetsofpoints orsetsofhori-\nzontal linesegments intheEuclidean plane. Viathis\ngeometric representation wecasttransposition invari-\nantcontent-based music retrie valproblems asones\nofmatching setsofpoints orsetsofhorizontal line\nsegments inplane under translations. Forﬁnding the\nexact occurrences ofapoint set(the query pattern)\nofsize \u0001within another point set(representing the\ndatabase) ofsize \u0002,wegiveanalgorithm with run-\nning time\n\u0003\u0005\u0004\u0001\u0006\u0002\b\u0007,andforﬁnding partial occurrences\nanother algorithm with running time\n\u0003\u0005\u0004\u0001\u0006\u0002\n\t\f\u000b\u000e\r\u000f\u0001\u0010\u0007 .\nWealso usethetotal length oftheoverlap between\nthelinesegments ofatranslated query andadatabase\n(i.e., theshared time) asaquality measure ofanoc-\ncurrence andpresent an\n\u0003\u0005\u0004\u0002\n\t\u0011\u000b\u0012\r\u000f\u0002\u0014\u0013\u0015\u0001\u0006\u0002\n\t\f\u000b\u000e\r\u0016\u0001\u0017\u0007 algo-\nrithm forﬁnding translations giving thelargestpossi-\nbleoverlap. Some experimental results ontheperfor -\nmance ofthealgorithms arereported.\n1Introduction\nThe methods introduced forcontent-based music retrie valon\nsymbolically encoded music havelargely been applications of\nconventional approximate string matching techniques based on\ntheedit distance (Mongeau andSank off,1990; Ghias etal.,\n1995; McNab etal.,1997; Lemström, 2000) ordiscrete time-\nwarping (Zhu andShasha, 2003). Thetechniques worknicely\nwith monophonic music, assuch music canberepresented with\nlinear sequences ofdiscrete pitch levels.Byusing interv alsbe-\ntween successi vepitches instead ofabsolute pitch levels,match-\ningbecomes transposition invariant. Similarly ,oneachie vesan\ninvariance ontempi byconsidering theduration ratios between\nsuccessi venotes.\nProblems getmore complicated, however,when dealing with\npolyphonic music likesymphon yorchestra scores. Such amu-\u0018Aworksupported bytheAcademy ofFinland(grant201560).\nPermission tomakedigitalorhardcopiesofallorpartofthiswork\nforpersonalorclassroom useisgrantedwithoutfeeprovidedthat\ncopiesarenotmadeordistributedforpro®torcommercial advan-\ntageandthatcopiesbearthisnoticeandthefullcitationonthe®rst\npage.c\n\u00192003JohnsHopkinsUniversity.sicmay haveverycomple xstructure with severalnotes simul-\ntaneously onandseveralmusical themes developing inparal-\nlel.One might wanttoﬁndsimilarities orother interesting pat-\nterns init,forexample, inorder tomakemusicological com-\nparati veanalysis ofthestyle ofdifferent composers orevenfor\ncopyright management purposes. Formulating various music-\npsychological phenomena andmodels such thatonecanwork\nwith them using combinatorial algorithms becomes amajor\nchallenge.\nQuite recently ,Meredith etal.(2001) suggested ageometric,\npiano-roll-lik emusic representation andcontent-based music\nretrie valalgorithms working onsuch representation. Wewill\nusehere anextended representation thatincludes thedurations,\naswell. Using thisrepresentation, theexcerpt ofFig. 1given\ninthecommon music notation, isrepresented geometrically in\nFig.2.Thecontent should beevident: each horizontal barrep-\nresents anote, itslocation inthe \u001a-axis givesitspitch leveland\nthestart andendpoints inthe\u001b-axis givethetime interv alwhen\nthenote ison.The piano-roll representation assuch isanold\ninvention; inmusic retrie valresearch ithasbeen used earlier\ne.g.byDovey(1999).\nWewill usethissimple two-dimensional geometric represen-\ntation ofmusic. Inthisrepresentation, apiece ofmusic isa\ncollection ofhorizontal linesegments (attimes simply points)\nintheEuclidean two-dimensional space\u001c\u000f\u001d;thehorizontal axis\nrefers tothetime, thevertical tothepitch values.\nGiventwosuch representations,\u001eand\u001f,wewanttoﬁndthe\ncommon patterns shared by\u001eand\u001fwhen\u001eistranslated with\nrespect to\u001f.Translating\u001emeans thatsome \"!#\u001c\u000f\u001d isadded\ntoallelements of\u001e.Obviously ,thevertical component ofthe\ntranslation yields transposition invariance ofthepattern match-\ningwhile thehorizontal component means shifting intime.\nWhen designing thealgorithms weassume that \u001frepresents\nalargemusic database while \u001eisashorter query pattern.\nThree problems willbeconsidered.\n( \u001e\u0005$)Find translations of \u001esuch thatallstarting points ofthe\nlinesegments of \u001ematch with some starting points ofthe\nlinesegments in \u001f.Hence theon-set times ofallnotes of \u001e\nmust match. Wealso consider avariant inwhich thenote\ndurations must match, too, orinwhich thetime segment\nof\u001fcovered bythetranslated\u001eisnotallowed tocontain\nanyextra(i.e., unmatched) notes.\n( \u001e&%)Find alltranslations of \u001ethatgiveapartial match ofthe\u0000\u0001\u0002 \u0003\n\u0004\u0006\u0005\u0005\n\u0007\u0007\n\b\n\u0007\u0007\n\u0004\u0007\n\t\u0007\u0003\n\u0007\n\b\u0007\n\u0007\u0007\n\t\u0007\n\u0007\n\n\u0005\u0005\u0005\n\u0004\u0007\u0007\n\u0007\u0007\u0007\n\u0007\u0007\u0007\n\u0007\u0007\u0007\n\b\n\u0007\n\u0005\u0005\u0005\n\u0007\u0007\n\u0007\u0007\u0007\n\u0007\u0004\u0007\u0007\n\u0007\t\u0007\n\b\u0007\n\u0007\u0005\u0005\n\b\u0005\n\u0007\n\b\u0007\n\u0007\u0007\n\u0004\u0007\n\u0007\u0007\n\t\u0007\n\b\u0007\u0007\u0007\n\b\n\u0007\n\u000bFigure 1:Anexcerpt ofpolyphonic music from Einojuhani Rauta vaara’ sopera Thomas (1985). Printed with thepermission ofthe\npublisher Warner/Chappell Music Finland Oy.\n2 3 4 2 3 4pitch\ntimeLine segment of T\nLine segment of P\nFigure 2:Theexample ofFig. 1isgiveninpiano-roll representation. Consider aquery pattern \u001e\r\f\u000f\u000e\u0011\u0010\u0013\u0012  \u0014\u0012\u0016\u0015\u0017\u0012  \u0014\u0012\u0016\u000e\u0011\u0010 (giveninpitch\nclass symbols) where theﬁrst4eventsarequarter notes andthelastahalfnote. Intheleftpanel, thelocation of \u001edoes notgivea\nsolution to( \u001e\u0005$)since theﬁfth note onset does notmatch: thereader may check thatnoother translation of \u001ewillgiveanybetter\nmatch. However,forproblems ( \u001e&%)and( \u001e\u0019\u0018)theleftpanel givesaquite good solution: 4ofthe5onset times match (problem\n( \u001e&%)),andtheshared time is5quarter notes. Ifthequery isslightly outoftime (theright panel) ( \u001e&%)becomes useless while ( \u001e\u0019\u0018)\nstillfounds asatisf actory match.\non-set times of \u001ewith those of \u001f.Inapartial match, only\nasubset oftheon-set times of \u001eshould match.\n( \u001e\u0019\u0018)Find translations of \u001ethat givelongest common shared\ntime with \u001f,i.e.,thelongest total length ofthelineseg-\nments thatareobtained asintersection ofthelinesegments\nof \u001fandthetranslated \u001e.\nFig.2illustrates allthree problems. There isnosolution of(\u001e\u0005$)\nineither oftheexamples, agood solution to(\u001e&%)and(\u001e\u0019\u0018)in\ntheexample ontheleft, andasatisf actory result to(\u001e\u0019\u0018)inthe\nexample ontheright.\nOurproblems arebasic content-based music retrie valquestions\nwhen transpositions areallowed andnote additions anddele-\ntions aremodeled aspartial matches. (\u001e\u0005$)and(\u001e&%)workwell\nwith ideal \u001eand \u001fwhere thestart points ofthenotes areex-\nact.However,thisisnotthecase neither with aquery byhum-\nming pattern, norwith adatabase created byplaying aMIDI\ninstrument. Problem ( \u001e\u0019\u0018)isofanoveltype wehavenotseen\ntobeconsidered before. Itallowslocal changes innote startpoints anddurations, thus providing needed tolerance forsuch\ncases, seeFig. 2.Moreo ver,itisnotdeteriorated anywayby\nnote fragmentations orconsolidations (Mongeau andSank off,\n1990). Tolerance tointerv alchanges could beincorporated by\nrepresenting thenotes asnarro wrectangles instead oflineseg-\nments.\nFortheproblem ( \u001e\u0005$)wegiveinSect. 3analgorithm thatneeds\ntime\n\u0003\u0005\u0004\u0001\u0006\u0002\b\u0007 andworking space\n\u0003\u0005\u0004\u0001\u0010\u0007.Inpractice theaverage\nrunning time is\n\u0003\u0005\u0004\u0002\b\u0007.Here \u0001isthesize (number oftheline\nsegments) of \u001eand \u0002isthesizeof \u001f.Fortheproblem ( \u001e&%)we\ngiveinSect. 4analgorithm with running time\n\u0003\u0005\u0004\u0001\u0006\u0002\n\t\f\u000b\u000e\r\u000f\u0001\u0010\u0007\nand space\n\u0003\u0005\u0004\u0001\u0017\u0007,and fortheproblem (\u001e\u0019\u0018)wedescribe in\nSect. 5amethod thatneeds time\n\u0003\u0005\u0004\u0002\n\t\u0011\u000b\u0012\r\u0016\u0002 \u0013 \u0001\u0006\u0002\n\t\u0011\u000b\u0012\r \u0001\u0017\u0007and\nspace\n\u0003\u0005\u0004\u0001 \u0013 \u0002\b\u0007.Weassume that\u001fand\u001earegiveninthe\nlexicographic order ofthestarting points ofthelinesegments.\nOtherwise extra\n\u0003\u0005\u0004\u0002 \t\u0011\u000b\u000e\r\u000f\u0002 \u0013&\u0001 \t\u0011\u000b\u0012\r \u0001\u0017\u0007time and\n\u0003\u0005\u0004\u0002 \u0013 \u0001\u0017\u0007space\nisneeded forsorting. Allalgorithms arebased onasweepline\ntype scanning of \u001f,atechnique widely used incomputational\ngeometry (Bentle yandOttmann, 1979). InSect. 6wedemon-strate byempirical evaluation that allouralgorithms arefast\nenough tobeuseful inpractice.\nApreliminary report ofthealgorithms inthispaper appeared as\n(Ukk onen etal.,2003).\n1.1 Related work\nInthispaper ,weimpro vetheresults byWiggins etal.(2003)\n(see also (Meredith etal.,2001)). Our solution for( \u001e\u0005$),be-\ningclosely related totheir SIA(M)EXalgorithm, ispresented\nhere forcompleteness, andmaking iteasier tounderstand the\nsolutions for(\u001e&%)and(\u001e\u0019\u0018).For(\u001e&%)wehaveimpro vedtheir\ntime bound (from\n\u0003\u0005\u0004\u0001\u0015\u0002\n\t\u0011\u000b\u0012\r\n\u0004\u0001\u0006\u0002\b\u0007 \u0007to\n\u0003\u0005\u0004\u0001\u0006\u0002 \t\u0011\u000b\u000e\r\u0016\u0001\u0017\u0007 )and, more\nnoteworthy ,their space requirement from anexcessi ve\n\u0003\u0005\u0004\u0001\u0006\u0002\b\u0007\nbound toapractical\n\u0003\u0005\u0004\u0001\u0010\u0007bound. Toourknowledge (\u001e\u0019\u0018)has\nnotbeen studied intheliterature.\nThemajority oftheliterature onsymbolic content-based music\nretrie valalgorithms deals only with monophonic music. The\npolyphonic music retrie valalgorithms based ontheedit dis-\ntance frame workconsider only theorder ofthenotes outofthe\nrhythmic information. Examples ofsuch studies arebyDovey\n(2001), who uses dynamic programming forﬁnding polyphonic\npatterns with restricted gaps, byLemström andTarhio (2003),\nwho apply bit–parallelism forﬁnding transposed monophonic\npatterns within polyphonic music, andbyHolub etal.(2001),\nwho usebit–parallelism forﬁnding approximate occurrences of\npatterns with group symbols. Arecent bit-parallel algorithm by\nLemström andNavarro (2003) iscapable ofdealing with gaps\ninpolyphonic music.\nThe PROMSsystem (Clausen etal.,2000) isanother exam-\npleofpolyphonic content-based music retrie valsystems that\naccounts fornote-on information (inaddition topitch informa-\ntion).\n2Linesegmentpatterns\nAlinesegment pattern intheEuclidean space \u001c\u001disanyﬁnite\ncollection ofhorizontal linesegments. Such asegment isgiven\nas\u0000 \u0001 \u0012\u0002\u0001\u0004\u0003\u0006\u0005where thestarting point\u0001 \f\n\u0004\u0001\b\u0007 \u0012\u0002\u0001\u0004\t \u0007 !\u0010\u001c\u001dandtheend\npoint\u0001\n\u0003\f\n\u0004\u0001\n\u0003\u0007\n\u0012\n\u0001\n\u0003\t\n\u0007 ! \u001c \u001dofthesegment aresuch that\u0001\u000b\t \f\f\u0001\n\u0003\t\nand\u0001\u0007\u000e\r\n\u0001\u0004\u0003\u0007.The segment consists ofthepoints between its\nendpoints. Twosegments ofthesame pattern may overlap.\nWewillconsider different waystomatch linesegment patterns.\nTothisendwearegiventwolinesegment patterns,\u001eand\u001f.\nLet\u001econsist of\u0001segments\u0000 \u000f\u0011\u0010 \u0012\u0012\u000f\u0013\u0003\u0010\n\u0005 \u0012\u0015\u0014\u0015\u0014\u0016\u0014 \u0012\b\u0000 \u000f\u0013\u0017 \u0012\u0018\u000f\u0019\u0003\u0017\n\u0005and\u001fof\u0002\nsegments\u0000 \u001a\u001b\u0010 \u0012\u001c\u001a\u001d\u0003\u0010\n\u0005 \u0012\u0016\u0014\u0015\u0014\u0015\u0014 \u0012\u0004\u0000 \u001a\u001d\u001e \u0012\u001c\u001a\u001d\u0003\u001e\n\u0005.Pattern\u001fmay represent alarge\ndatabase while\u001eisarelati velyshort query tothedatabase in\nwhich case \u0001 \u001f \u0002.Itisalso possible that \u001eand \u001fareabout\nofthesame size, oreventhattheyarethesame pattern.\nWeassume that\u001eand\u001faregiveninthelexicographic order of\nthestarting points oftheir segments. Thelexicographic order of\npoints \u000e \f\n\u0004\u000e\u0007\n\u0012 \u000e\t\n\u0007and ! \f\n\u0004!\u0007\n\u0012\n!\t\n\u0007in \u001c\u001disdeﬁned bysetting\u000e#\"$! iff \u000e\u0007\n\"$!\u0007,or \u000e\u0007\n\f%!\u0007and \u000e\t\n\"$!\t.When representing\nmusic, thelexicographic order corresponds thestandard reading\nofthenotes from lefttoright andfrom thelowest pitch tothe\nhighest.\nSoweassume thatthelexicographic order ofthestarting points\nis \u000f&\u0010\r\n\u000f\u001d\n\r('\u0015'\u0016')\r\n\u000f\u0013\u0017and \u001a*\u0010\r\n\u001a\u001d\n\r('\u0015'\u0016')\r\n\u001a\u001d\u001e.Ifthisisnot\ntrue, apreprocessing phase isneeded, tosortthepoints which\nwould takeadditional time\n\u0003\u0005\u0004\u0001 \t\u0011\u000b\u0012\r\u000f\u0001 \u0013 \u0002\n\t\u0011\u000b\u0012\r\u0016\u0002\b\u0007 .Atranslation intherealplane isgivenbyany ! \u001c\u001d.The\ntranslated\u001e,denoted by\u001e \u0013  ,isobtained byreplacing any\nlinesegment\u0000 \u000f\u0013+ \u0012\u0012\u000f\u0013\u0003+\n\u0005of\u001eby\u0000 \u000f\u0019+ \u0013  \u0014\u0012\u0012\u000f\u0013\u0003+\n\u0013  \u0019\u0005.Hence\u001e \u0013  is\nalso alinesegment pattern, andanypoint, ! \u001c\u001dthatbelongs\ntosome segment of\u001eismapped inthetranslation as,.- /0, \u0013\u0005 .\n3Exactmatching\nLetusdenote thelexicographically sorted sets ofthestart-\ningpoints ofthesegments inourpatterns \u001eand \u001fas \u001e \f\u0004\u000f\n\u0010\u0012\u0018\u000f\u001d\n\u0012\u0016\u0014\u0015\u0014\u0016\u0014 \u0012\u0012\u000f\n\u0017\u0007and \u001f \f\n\u0004\u001a\n\u0010\u0012\u0002\u001a\u001d\n\u0012\u0016\u0014\u0015\u0014\u0015\u0014 \u0012\u001c\u001a\n\u001e\u0007.Wenowwanttoﬁnd\nalltranslations  such that \u001e\u0015\u0013  21 \u001f.Such a \u001e\u0006\u0013  iscalled an\noccurr ence of\u001ein\u001f.Asallpoints of\u001e \u0013# must match some\npoint of\u001f,\u000f\n\u0010\u0013\u0015 inparticular must equal some\u001a43.Hence there\nareonly\u0002potential translations thatcould giveanoccurrence,\nnamely thetranslations\u001a365\n\u000f\u0011\u0010where$\r87#\r\n\u0002.\nTocheck forsome translation \f9\u001a435\n\u000f\n\u0010,whether also the\nremaining points \u000f\u001d\n\u0013  \u0014\u0012\u0015\u0014\u0016\u0014\u0015\u0014 \u0012\u0012\u000f\n\u0017\u0013  of \u001e \u0013  match, weuti-\nlizethelexicographic order .Themethod will bebased onthe\nfollo wing simple lemma.\nDenote thepotential translations as  \b3 \f:\u001a\u001235\n\u000f\n\u0010for $\r;7<\r\n\u0002.\nLet \u000f ! \u001e,andlet  \u00043and  \u00043*=betwopotential translations such\nthat \u000f \u0013  \u00163 \f>\u001aand \u000f \u0013  \u00163=\n\f>\u001a\u001d\u0003 forsome \u001a\u0016\u0012\u001c\u001a\u001d\u0003 ! \u001f.That is,\nwhen \u000f\n\u0010matches \u001a\u00123then \u000fmatches \u001a,andwhen \u000f\n\u0010matches \u001a\u00183*=\nthen \u000fmatches \u001a?\u0003.\nLemma 1If7\n\"7\n\u0003then\u001a@\"A\u001a\u001d\u0003 .\nProof.If7\n\"7\n\u0003,then \u001a3\n\"B\u001a3*=byourconstruction. Hence\nalso  3\n\"  3*=,andthelemma follo ws.\nOur algorithm makesatraversal over \u001f,matching \u000f\n\u0010against\ntheelements of \u001f.Atelement \u001a\u00183weineffectareconsidering\nthetranslation  \u00043.Simultaneously wemaintain foreach other\npoint \u000f\n+of \u001eapointer C\n+thatalso traverses through \u001f.WhenC\n+isat \u001a\u00123,itineffect represents translation \u001a\u001835\n\u000f\n+.This is\ncompared tothecurrent  \b3,andthepointer C\n+isupdated tothe\nnextelement of \u001fafter C\n+ifthetranslation issmaller (thestepC\n+ED\u0002 \u0015 \u001bF\u001a\n\u0004C\n+\u0007inthealgorithm below).Ifitisequal, wehave\nfound amatch for \u000f\n+,andwecontinue with updating C\n+HGI\u0010.It\nfollo wsfrom Lemma 1,thatnobacktracking ofthepointers is\nneeded.\nTheresulting Algorithm 1isgivenbelow.\nALGORITHM1.\n(1) for J\nD$ \u0012\u0015\u0014\u0015\u0014\u0016\u0014 \u0012 \u0001do C\n+KD5ML\n(2) C\n\u0017NGK\u0010ODL\n(3) for7\nD$ \u0012\u0015\u0014\u0016\u0014\u0015\u0014 \u0012 \u00025\n\u0001do\n(4) \nD\u001a\u001235\n\u000f\n\u0010\n(5)J\nD$\n(6) do\n(7)J\nDJ \u0013 $\n(8)C\u0016+\nDQP#RTS\n\u0004C\u0015+ \u0012\u001c\u001a3\n\u0007\n(9) while C\u0016+U\"V\u000f\u0019+ \u0013  do C\u0015+\nD\u0002 \u0015 \u001b\u0019\u001a\n\u0004C\u0015+ \u0007\n(10) until C\u0015+UWV\u000f\u0019+ \u0013  \n(11) if J \f \u0001 \u0013 $then X\bY&\u001aZ\u000fFY&\u001a\n\u0004 \u0007\n(12) endfor.Note thatthemain loop (line\u0018)ofthealgorithm canbestopped\nwhen7\n\f \u00025\n\u0001,i.e.,when\u000f\n\u0010ismatched against\u001a\n\u001e\u0001\u0000\u0013\u0017.Match-\ning\u000f\u0011\u0010beyond\u001a?\u001e\u0001\u0000\u0013\u0017 would notlead toafulloccurrence of\u001e\nbecause then allpoints of\u001eshould match beyond\u001a\u001b\u001e\u0002\u0000\u0013\u0017 ,but\nthere arenotenough points left.\nThat Algorithm 1correctly ﬁnds all such that\u001e \u0013  1 \u001fis\neasily provedbyinduction. Therunning time is\n\u0003\u0005\u0004\u0001\u0006\u0002\b\u0007,which\nimmediately follo wsfrom thateachC\u0004+traverses through\u001f(pos-\nsibly with jumps!). Also note thatthisbound isachie vedonly in\ntherarecase that \u001ehas\n\u0003 \u0004\u0002\b\u0007fulloccurrences in \u001f.More plau-\nsible isthatforrandom \u001eand \u001f,most ofthepotential occur -\nrences check edbythealgorithm arealmost empty .This means\nthat theloop 6–10 isexecuted only asmall number oftimes\nateach check point, independently of \u0001.Then theexpected\nrunning time under reasonable probabilistic models would be\u0003\u0005\u0004\u0002\b\u0007.Inthisrespect Algorithm 1beha vesanalogously tothe\nbrute-force string matching algorithm.\nItisalso easy touseadditional constraints inAlgorithm 1.For\nexample, onemight wantthatthelengths ofthelinesegments\nmust alsomatch. This canbetested separately once afullmatch\nofthestarting points hasbeen found. Another natural require-\nment canbe,inparticular if \u001eand \u001frepresent music, thatthere\nshould benoextrapoints inthetime windo wcovered byanoc-\ncurrence of\u001ein\u001f.If\u001e \u0013  isanoccurrence, then thistime\nwindo wcontains allmembers of\u001fwhose\u001b-coordinate belongs\ntotheinterv al\u0000\n\u0004\u000f\u0011\u0010 \u0013& \u0007\u0007\n\u0012\n\u0004\u000f\u0013\u0017 \u0013& \u0007\u0007\n\u0005.When anoccurrence\u001e\u0005\u0013& \nhasbeen found inAlgorithm$,thecorresponding time windo w\niseasy tocheck forextrapoints. Letnamely \u001a3\n\f%\u000f\u0011\u0010 \u0013  and\u001a3=\n\f%\u000f\u0019\u0017 \u0013  .Then thewindo wcontains justthepoints of\u001f\nthatmatch\u001e \u0013  ifandonly if7\n\u00035V7\n\f \u00015\n$and\u001a3\n\u0000 \u0010and\u001a3=\nGI\u0010donotbelong tothewindo w.\n4Largestcommonsubset\nOurnextproblem istoﬁndtranslations  such that \u0004 \u001e \u0013  \u0006\u0005\u0001\u0007 \u001f\nisnonempty .Such a \u001e \u0013  iscalled apartial occurr ence of \u001e\nin \u001f.Inparticular ,wewanttoﬁnd  such that \u0004 \u001e \u0013\" \u0006\u0005\b\u0007 \u001fis\nlargestpossible.\nThere are\n\u0003\u0005\u0004\u0001\u0015\u0002\b\u0007 translations  such that\u0004\n\u001e \u0013  \u0005\n\u0007 \u001fis\nnonempty ,namely thetranslations \u001a435\n\u000f\n+for $\r 7 \r\n\u0002,$\r\nJ\r\n\u0001.Checking thesize of\u0004\n\u001e \u0013\" \u0005\n\u0007 \u001fforeach of\nthem solvestheproblem. Abrute-force algorithm would typi-\ncally need time\n\u0003\u0005\u0004\u0001\u001d\n\u0002\n\t\f\u000b\u000e\r\u0016\u0002\b\u0007 forthis. Wewill giveasimple\nalgorithm thatwilldothisduring \u0001simultaneous scans over \u001f\nintime\n\u0003\u0005\u0004\u0001\u0006\u0002 \t\u0011\u000b\u000e\r\u0016\u0001\u0017\u0007 .\nLemma 2Thesizeof \u0004 \u001e \u0013  \u0006\u0005\t\u0007 \u001fequals thenumber ofdis-\njoint pairs\n\u00047\n\u0012*J \u0007(i.e.,pairssharing noelements) suchthat \f$\u001a\u001235\n\u000f\n+.\nProof.Immediate.\nByLemma 2,toﬁnd thesize ofanynon-empty\u0004\n\u001e \u0013\" \u0005\n\u0007 \u001f\nitsufﬁces tocount themultiplicities ofthetranslation vectors 3\n+ \fB\u001a3M5\n\u000f\u0019+.This canbedone fastbyﬁrstsorting them and\nthen counting. However,wecanavoidfullsorting byobserving\nthattranslations  \u0010\u001b+ \u0012  \u001d\n+ \u0012\u0015\u0014\u0016\u0014\u0015\u0014 \u0012  \u000b\u001eT+areinthelexicographic order\nforanyﬁxed J.This sorted sequence oftranslations canbegen-\nerated inatraversal over \u001f.By \u0001simultaneous traversals we\ngetthese sorted sequences forall $\r\nJ\r\n\u0001.Merging themon-the-ﬂy intothesorted order ,andcounting themultiplicities\nsolvesourproblem.\nThedetailed implementation isverystandard. AsinAlgorithm\n1,weletC\n\u0010\u0012\u001cC\u001d\n\u0012\u0016\u0014\u0015\u0014\u0015\u0014 \u0012\u001cC\n\u0017refer totheentries of\u001f.Initially each\nofC\n+refers to\u001a\n\u0010,anditisalsoconvenient toset\u001a\n\u001eTGI\u0010ODL.The\ntranslations \n+\f C\n+5\n\u000f\n+arekeptinapriority queue\n.Opera-\ntion\u0001 J \u0002\n\u0004\n&\u0007givesthelexicographically smallest oftranslations \u000b+,$\r\nJ\r\n\u0001.OperationY \u000f\f\u000b \u000e \u001a \u0015\n\u0004\n&\u0007deletes theminimum el-\nement from\n,letitbe \u000e\r \f:C\u000f\r5\n\u000f\u0010\r,updatesC\u000f\r\nD\u0002 \u0015 \u001bF\u001a\n\u0004C\u000f\r\u000e\u0007,\nandﬁnally inserts thenew  \u000e\r \f C\u0011\r5\n\u000f\u0006\rinto \n.\nThen thebody ofthepattern matching algorithm isasgiven\nbelow.\n(1)  \nD5ML; \u0012\nD\u0014\u0013;\ndo\n(2) &\u0003\nD\u0001 J \u0002\n\u0004\n&\u0007;Y \u000f\u0010\u000b\u0017\u000e \u001a \u0015\n\u0004\n&\u0007\n(3) if &\u0003\u0014\f  then\u0012\nD\u0012 \u0013 $\n(4) else{X\bY&\u001aZ\u000fFY&\u001a\n\u0004 \u0014\u0012\u0015\u0012 \u0007; \nD \n\u0003;\u0012\nD$}\n(5) until \fL\nThe algorithm reports all\n\u0004 \u0014\u0012\u0016\u0012 \u0007such that \u0017\u0017\n\u0004 \u001e \u0013\" \u0006\u0005\u0018\u0007 \u001f\u0019\u0017\u0017\n\f\u001a\u0012\nwhere \u0012 W\n\u0013.\nThe running time ofthealgorithm is\n\u0003\u0005\u0004\u0001\u0006\u0002 \t\u0011\u000b\u000e\r\u000f\u0001\u0017\u0007 .The \u0001-\nfoldtraversal of \u001ftakes \u0001\u0006\u0002steps, andtheoperations onthe \u0001\nelement priority queue \ntaketime\n\u0003\u0005\u0004\t\f\u000b\u000e\r\u000f\u0001\u0010\u0007 ateach stepofthe\ntraversal.\nThe abovemethod ﬁnds allpartial occurrences of \u001eindepen-\ndently oftheir size. Concentrating onlargepartial occurrences\ngivespossibilities forfaster practical algorithms based onﬁltra-\ntion.Wenowsketch such amethod. Let\u0017\u0017\n\u0004 \u001e \u0013  \u0006\u0005\u001b\u0007 \u001f\n\u0017\u0017\n\f\u001c\u0012\nand \u001d \f \u00015\n\u0012.Then \u001e \u0013  iscalled anoccurrence with \u001d\nmismatc hes.\nWewanttoﬁndall \u001e \u0013  thathaveatmost \u001dmismatches for\nsome ﬁxedvalue \u001d.Then wepartition \u001einto \u001d \u0013 $disjoint\nsubsets \u001e\n\u0010\u0012\u0015\u0014\u0016\u0014\u0015\u0014 \u0012 \u001e\u001f\u001e\nGI\u0010of(about) equal size. Thefollo wing sim-\nplefactwhich hasbeen observ edearlier indifferent variants in\nstring matching literature willgiveourﬁlter.\nLemma 3If \u001e \u0013  isanoccurr ence of \u001ewith atmost \u001dmis-\nmatc hesthen \u001e \r \u0013  must beanoccurr ence of \u001e \rwith no\nmismatc hesatleast forone !, $\r\n!\r\n\u001d \u0013 $,\nProof.Bycontradiction: Ifevery\u001e\n\r\u0013  hasatleast$mis-\nmatch then\u001e \u0013  must haveatleast\u001d \u0013 $mismatches as\u001e \u0013\u0010 \nisaunion ofdisjoint linesegment patterns\u001e \r\u0014\u0013  .\nThis givesthefollo wing ﬁltration procedure: First (theﬁltration\nphase) ﬁndbyAlgorithm 1ofSection 3all(exact) occurrences\u001e\u001f\r\n\u0013  ofeach\u001e\"\r.Then (thechecking phase) ﬁndforeach \nproduced bytheﬁrstphase, intheascending order ofthetrans-\nlations  ,howmanymismatches each \u001e \u0013\" has.\nTheﬁltration phase takestime\n\u0003\u0005\u0004\u0001\u0006\u0002\b\u0007,sorting thetranslations\ntakes\n\u0003\u0005\u0004$#\t\f\u000b\u000e\r\u001b\u001d \u0007where\n#\r\n\u0004\u001d \u0013 $ \u0007 \u0002isthenumber oftransla-\ntions theﬁlter ﬁnds, andchecking using analgorithm similar to\nthealgorithm givenpreviously inthissection (butnowpriority\nqueue isnotneeded) takestime\n\u0003\u0005\u0004\u0001\n\u0004\u0002\u0010\u0013\n#\u0007 \u0007.Itshould again\nbeobvious, thattheexpected performance canbemuch better\nwhene ver \u001disrelati velysmall ascompared to \u0001.Then theﬁl-\ntration would takeexpected time\n\u0003\u0005\u0004\u001d \u0002\b\u0007.This would dominatethetotal running time forsmall\n#ifthechecking isimplemented\ncarefully .\n5Longestcommontime\nLetusdenote thelinesegments of \u001eand \u001fby \u0000\n+\f \u0000 \u000f\n+\u0012\u0018\u000f\u0019\u0003+\n\u0005and\u00013 \f \u0000 \u001a\u00123 \u0012\u001c\u001a\n\u00033\n\u0005,respecti vely.\nOurproblem inthissection istoﬁndatranslation  such that\nthelinesegments of \u001e \u0013  intersects \u001fasmuch aspossible.\nForanyhorizontal linesegments\n\u0002and \u0003,let \u0012\n\u0004\u0002\u0012\u0004\u0003 \u0007denote\nthelength oftheir intersection linesegment\n\u0002\u0007\u0005\u0003 .Then let\u0006\u0005\u0004 \u0007 \f\n\u0007+\t\b3\n\u0012\n\u0004\u0000\u0019+ \u0013  \u0014\u0012\n\u00013\n\u0007\u0002\u0014\nOur problem istomaximize thisfunction.\n\u0006\u0005\u0004 \u0007isnonzero\nonly ifthevertical component  \tof  \f\n\u0004 \u0007\n\u0012  \t\n\u0007brings some\u0000\n+tothesame vertical position assome\n\u00013,i.e.,only if  T\t \f\u0004\u001a\u00123 \u0007\u001b\t5\n\u0004\u000f\n+\u0007?\tforsome J\u0016\u00127.\nLet \nbethesetofdifferent values\n\u0004\u001a3\n\u0007\t)5\n\u0004\u000f\u0013+ \u0007\tfor $\r\nJ\r\n\u0001,$\r 7;\r\n\u0002.Note that \nisastandard set,notamultiset; the\nsizeof \nis\n\u0003\u0005\u0004\u0001\u0006\u0002\b\u0007.\nAs\n\u0006\u0005\u0004 \u0007getsmaximum when \t\n!\u000b\n weobtain thatP#R\u000bS\f\n\u0006\u0005\u0004 \u0007 \f\nP#R\u000bS\f\u000e\r\u0010\u000f\n\u0006\u0005\u0004 \u0004 \u0007\n\u0012  \t\n\u0007 \u0007\u0012\u0011\u0012 \t\n!\u000b\n\u0014\u0013 \u0014 (1)\nWewill nowexplicitly construct thefunction\n\u0006\u0005\u0004 \u0004 \u0007\n\u0012  \t\n\u0007 \u0007 \f\u0006\f\u0016\u0015\n\u0004 \u0007\n\u0007forallﬁxed  \t\n!\u0017\n .Tothis end, assume that \t\n\f\n\u0004\u001a3\n\u0007\tV5\n\u0004\u000f\u0019+ \u0007\tand consider thevalue of \u0012\u0016+3\n\u0004 \u0007\n\u0007 \f\u0012\n\u0004\u0000\u0019+ \u0013\n\u0004 \u0007\n\u0012  \t\n\u0007 \u0012\n\u00013\n\u0007.This isthecontrib ution oftheintersection\nof \u0000\n+\u0013\n\u0004 \u000b\u0007 \u0012  \u000b\t \u0007and\n\u00013tothevalueof\n\u0006\f\u0015\n\u0004 \u000b\u0007\u000e\u0007.Thefollo wing el-\nementary analysis characterizes thebeha viour of \u0012\n+3\n\u0004 \u000b\u0007 \u0007when \u000b\u0007grows,i.e.,when \u0000\n+\u0013\n\u0004 \u000b\u0007 \u0012  \u000b\t \u0007slides from lefttoright andits\nintersection with\n\u00013isﬁrstempty ,then starts growing, then stays\nthesame until starts shrinking and ﬁnally gets again empty .\nWhen \u000b\u0007issmall enough,\u0012\n+3\n\u0004 \u000b\u0007\u000e\u0007equals\u0013.When T\u0007grows,\natsome point theendpoint of\u0000\n+\u0013\n\u0004 \u000b\u0007\u0011\u0012  \u000b\t \u0007meets thestarting\npoint of\n\u0001+andthen\u0012\n+3\n\u0004 \u000b\u0007\u0012\u0007starts togrowlinearly with slope$\nuntil thestarting points andtheendpoints of\u0000\u0011+ \u0013\n\u0004 \u0007\n\u0012  \t\n\u0007and\u00013meet, whiche vercomes ﬁrst. After that,\u0012\u0016+3\n\u0004 \u0007\n\u0007hasacon-\nstant value (minimum ofthelengths ofthetwolinesegments)\nuntil thestarting points ortheendpoints (i.e., theremaining\npairofthetwoalternati ves)meet, from which point on, \u0012\u0004+3\n\u0004 \u0007\n\u0007\ndecreases linearly with slope5\n$until itbecomes zero atthe\npoint where thestarting point of \u0000\n+hitstheendpoint of\n\u00013.An\neasy exercise showsthattheonly turning points of \u0012\n+3\n\u0004 \u000b\u0007\u0012\u0007are\nthefour points described aboveandtheir values are \u0007\n\f\n\u0004\u001a3\n\u0007\u0007 5\n\u0004\u000f\u0013\u0003+\n\u0007\u0007 slope$starts \u0007\n\f\nP\u0019\u0018\u001b\u001a\u0004\n\u0004\u001a3\n\u0007\u0007 5\n\u0004\u000f\u0013+ \u0007\u0007\n\u0012\n\u0004\u001a\u001d\u00033\n\u0007\u0007 5\n\u0004\u000f\u0013\u0003+\n\u0007\u0007\u000e\u0005 slope\n\u0013starts \u000b\u0007 \f\nP#RTS\u0004\n\u0004\u001a\u00123 \u0007?\u00075\n\u0004\u000f\n+\u0007?\u0007\u0011\u0012\n\u0004\u001a\u001d\u00033\n\u0007\u001b\u00075\n\u0004\u000f\u0019\u0003+\n\u0007?\u0007\u000e\u0005 slope5\n$starts \u000b\u0007 \f\n\u0004\u001a\u001d\u00033\n\u0007?\u00075\n\u0004\u000f\n+\u0007\u001b\u0007 slope\u0013starts.\nHence theslope changes by \u0013 $attheﬁrstandthelastturning\npoint, while itchanges by5\n$atthesecond andthethird turning\npoint.\nNow,\n\u0006\f\u001c\u0015\n\u0004 \u0007\n\u0007 \f\u001e\u001d+\t\b3\n\u0012\u0015+3\n\u0004 \u0007\n\u0007,hence\n\u0006\f\u0016\u0015isasum ofpiece wise\nlinear continuous functions andtherefore itgets itsmaximum\nvalue atsome turning point ofthe \u0012\n+3s.Let \u001fbeaturning point,\nandthevalue associated toitisthecorresponding horizontaltransposition. Moreo ver,let \t \f\u000f\n\u001f\n\u0010\r\n\u001f\u001d\n\r('\u0016'\u0015'K\r\n\u001f\"!#\u0013 be\nthemultiset oftheturning points ordered inincreasing order;\nnote thatdistinct functions\u0012\u0016+3may haveaturning point atsame\nlocation. So,foreachJ \u00127,thefour values\u0004\u001a3\n\u0007\u0007 5\n\u0004\u000f\u0019\u0003+\n\u0007\u0007 (type $)\u0004\u001a3\n\u0007\u0007 5\n\u0004\u000f\u0019+ \u0007\u0007 (type %)\u0004\u001a\u001d\u00033\n\u0007?\u00075\n\u0004\u000f\u0019\u0003+\n\u0007\u001b\u0007 (type \u0018)\u0004\u001a\n\u00033\n\u0007?\u00075\n\u0004\u000f\n+\u0007\u001b\u0007 (type $)\nareinthelists ofthe  \ts,andeach knowsits“type” shown\nabove.\nToevaluate\n\u0006\f\u001c\u0015\n\u0004 \u0007\n\u0007atitsallturning points wescan theturning\npoints \u001f\u001eandkeeptrack ofthechanges oftheslope ofthefunc-\ntion\n\u0006\f\u0016\u0015.Then itiseasy toevaluate\n\u0006\f\u0016\u0015\n\u0004 \u0007\n\u0007atthenextturning\npoint from itsvalue attheprevious one. Let ,betheprevious\nvalue andlet \u0001represent theslope andletbest store thelargest\nvalue. Theevaluation isgivenbelow.\n(1) ,\nD\u0014\u0013; \u0001\nD\u0014\u0013; %'&)(+*\nD\u0014\u0013\n(2) for \u001d\nD$ \u0012\u0016\u0014\u0015\u0014\u0015\u0014 \u0012\u0004,do\n(3) ,\nD,\n\u0013\u000e\u0001\n\u0004\u001f \u001e5\n\u001f \u001e\n\u0000\u0011\u0010\u0007\n(4) if\u001f \u001eisoftype$ortype$then\u0001\nD\u0001 \u0013 $\n(5) else\u0001\nD\u00015\n$.\n(6) if,#W-%'&)(+* then%'&.(+*\nD,\nThis should berepeated foralldifferent \t\n!/\n.Wenextde-\nscribe amethod thatgenerates theturning points inincreasing\norder simultaneously foralldifferent  \t.Themethod willtra-\nverse \u001fusing four pointers (thefour “types”) peranelement of\u001e.Apriority queue isagain used forsorting thetranslations\ngivenbythe $\u0012\u0001pointers; the \u001b-coordinates ofthetranslations\nthen givetheturning points inascending order .Ateach turning\npoint weupdate thecounters ,\n\rand \u0001\n\rwhere !isgivenbythe\u001a-coordinate ofthetranslation.\nWeneed twotraversal orders of \u001f.Theﬁrstistheonewehave\nused sofar,thelexicographic order ofthestarting points \u001a\u001d3.\nThis order isgivenas \u001f.Thesecond order isthelexicographic\norder oftheendpoints\u001a?\u00033.Let\u001f\n\u0003betheendpoints inthesorted\norder .\nLetC\n\u0010+\n\u0012\u001cC\u001d+\n\u0012\u001cC+0+,andC21+bethepointers ofthefour types, associ-\nated with element \u0000\u0013+of \u001e.Pointers C\n\u0010+and C\u001d+traverse \u001f,and\npointers C+0+and C21+traverse \u001f\n\u0003.Thetranslation associated with\nthecurrent value ofeach pointer isgivenas\u001a\n# \u0004C\n\u0010+\n\u0007 \f$C\n\u0010+\n5\n\u000f\n\u0003+\n\u001a\n# \u0004C\n\u001d+\n\u0007 \f C\n\u001d+\n5\n\u000f\n+\u001a\n# \u0004C\n0+\n\u0007 \f$C\n0+\n5\n\u000f\n\u0003+\n\u001a\n# \u0004C\n1+\n\u0007 \f C\n1+\n5\n\u000f\n+\u0014\nSo,when thepointers refer to \u001a3or \u001a\u001d\u00033,the \u001b-coordinate ofthese\ntranslations givetheturning points, oftypes 1,2,3,and4,asso-\nciated with theintersection of \u0000\u0013+and\n\u00013.The \u001a-coordinate gives\nthevertical translation\n\u0004\u001a\u00183 \u0007\u001b\t5\n\u0004\u000f\n+\u0007\u001b\tthatisneeded toclassify\ntheturning points correctly .\nDuring thetraversal, all $\u000e\u0001translations \u001a\n#givenbythe $\u0012\u0001\npointers arekeptinapriority queue. Byrepeatedly extract-\ningtheminimum from thequeue (and updating thepointer that\ngivesthisminimum \u001a\n#)wegetthetranslations inascending lex-\nicographic order ,andhence the \u001b-coordinate ofthetranslations\ngivesallturning points inascending order .\nLet  \f\n\u0004 \u000b\u0007\u0011\u0012  \u000b\t \u0007bethenexttranslation obtained inthisway.\nThen weretrie vetheslope counter \u0001\n\f\u0015andthevalue counter0123456789\n0 2 4 6 8 10 12 14 16 18time in secs\nmP2\nP1\n020406080100\n0 2 4 6 8 10 12 14 16 18time in secs\nmP3\nSIA(M)E1\nSIA(M)E2\nP2\nFigure 3:Experimenting onthelength ofthequery pattern. Totheleft,comparing P1vs.P2;totheright P2vs.P3,SIA(M)E\u0010and\nSIA(M)E\u001d.,\n\f\u0016\u0015.Assuming thatwehavealso stored thelastturning point\u0000\f\u0016\u0015atwhich ,\n\f\u0016\u0015wasupdated, wecannowperform thefollo w-\ningupdates. If  \u0007\n\u0001\f\n\u0000\f\u001c\u0015,then let ,\n\f\u0016\u0015D,\n\f\u001c\u0015\u0013 \u0001\n\f\u0016\u0015\n\u0004 \u0007 5\n\u0000\f\u001c\u0015\u0007\nand\n\u0000\f\u0015D \u000b\u0007.Moreo ver,if  isoftypes 1or4,then let\u0001\n\f\u0015D\u0001\n\f\u0015\u0013 $otherwise \u0001\n\f\u0015D\u0001\n\f\u00155\n$.\nInthiswayweobtain thevalues , \rforeach function\n\u0006\rand\neach ! ! \n,ateach turning point. By(1),themaximum value\nof\n\u0006must beamong them.\nThe described procedure needs\n\u0003\u0005\u0004\u0002\n\t\f\u000b\u000e\r\u000f\u0002\b\u0007 time forsorting\u001finto \u001fand \u001f\n\u0003,and\n\u0003\u0005\u0004\u0001\u0015\u0002\n\t\u0011\u000b\u0012\r \u0001\u0017\u0007time forgenerating the\u0003\u0005\u0004\u0001\u0006\u0002\b\u0007 turning points inincreasing order .Ateach turning point\nwehavetoretrie vethecorresponding slope andvalue counters\nusing thevertical translation ! ! \nasthesearch key.Since\nwearedealing with MIDI interv als, \u0011 \n \u0011\r\n%\u0003\u0002\u0004\u0002(independently\nof \u0001and \u0002),wecanusebucketing tomanage these counters.\nThis givesatotal time requirement\n\u0003\u0005\u0004\u0002\n\t\u0011\u000b\u0012\r\u000f\u0002 \u0013 \u0001\u0006\u0002\n\t\f\u000b\u000e\r \u0001\u0017\u0007and\nspace requirement\n\u0003\u0005\u0004\u0001 \u0013#\u0002\b\u0007 .\nLetusﬁnally point outananomaly ofouralgorithm. Itisquite\ncommon thatanote occurs severaltimes simultaneously ondif-\nferent tracks (instruments). Then itispossible thatouralgo-\nrithm ﬁnds occurrences whose length islargerthan thetotal\nlength oftheline segments of\u001e,because aline segment of\u001ecanintersect severallinesegments of\u001fsimultaneously .For-\nmally ,however,thealgorithm solvesproblem (\u001e\u0019\u0018)aswede-\nﬁned. Itisrather easy toremo vetheanomaly: justreplace any\noverlapping linesegments in \u001eand \u001fbyonelong segment cov-\nering their joint time interv als.Another possibility ,which does\nnotdestro ytheoriginal structure, istoinsist thattheoverlaps\nbetween \u001eand \u001fthatarecounted tothecommon time must\nbeone-to-one (instead ofmany-to-one orone-to-man y).This\ncanbeachie vedusing appropriate counters associated with each\nlinesegment of \u001e.\n6Experiments\nWemade brief experiments onouralgorithms solving problems\n( \u001e\u0005$),( \u001e&%),and( \u001e\u0019\u0018).Letuscallthealgorithms P1,P2,and\nP3,respecti vely.Tohavesomething tocompare with, weim-\nplemented theSIA(M)E\n\u0010andSIA(M)E\u001dalgorithms (Meredithetal.,2001; Wiggins etal.,2003). SIA(M)E \u0010uses hashing1to\nmaintain thetranslation vectors inorder andhasaworstcase\ntime bound of\n\u0003\u0005\u0004 \u0004\u0001\u0006\u0002\b\u0007\u001d\n\u0007butworks intime\n\u0003\u0005\u0004\u0001\u0006\u0002\b\u0007 ontheav-\nerage. SIA(M)E\u001dmergessorted setsoftranslation vectors and\nhasaworstcase time bound of\n\u0003\u0005\u0004\u0001\u0006\u0002 \t\u0011\u000b\u000e\r\n\u0004\u0001\u0006\u0002\b\u0007 \u0007.Intheexper-\niments, weused thedatabase oftheMutopia project2.Forthe\nexperiments wecreated 10copies ofit.So,intotal ourdatabase\ncontained 2790 musical documents that comprised 1,215,520\nchords and2,158,840 notes. The degree ofpolyphon ywithin\nthedatabase is1.78 ontheaverage (P#RTS 15).\nFirst, westudied thedifference inperformances ofP1andP2,\nandthen those ofthefour algorithms, P2,P3,SIA(M)E\u0010,and\nSIA(M)E\u001d,capable ofdealing with approximate matching. Re-\ncallthat P2,SIA(M)E\u0010,andSIA(M)E\u001dsolveproblem (\u001e&%)\n(while P3solvesproblem (\u001e\u0019\u0018)).Theexperiments were carried\noutinaPCwith Intel Pentium IVof2.26GHz and1GBof\nRAM under amodiﬁed RedHat Linux with kernel 2.4.18 and\ngcccompiler 3.2.2.\nWeexperimented ondifferent lengths ofquery patterns forit\nisthemore interesting parameter outoftheparameters\u0001and\u0002.Therange ofconsidered lengths was\u0000 %\u0011\u0012 $\u0006\u0005\u000b\u0005that, weclaim,\ncoversthelengths ofthepatterns normally used inquery by\nhumming applications.\nAteach considered length, 25query patterns were randomly\npickedupfrom thedatabase. Thus, itwasguaranteed thatat\nleast oneoccurrence wastobefound with each possible query .\nThe times wereport aretheaverages ofthe25repetitions of\neach considered query pattern length. Wemeasured thetrue,\nelapsed times forcarrying outthequery tasks.\nFig. 3givestherunning times ofourexperiments. First, con-\nsider theproblem (\u001e\u0005$).Theﬁrstplot(totheleft) conﬁrms that\nP1canbeexpected torunindependently of \u0001.Therefore, P1is\nauseful algorithm thatshould beincluded inancontent-based\nmusic retrie valquery engine forthecases when exact occur -\nrences need tobefound inalargedatabase, although P2canbe\nused forthetask, aswell.\n1Weusedahashingtableofsize\u0007\t\b\u000b\n\f\b\u000e\r,assuggested byWiggins\netal.(2003).\n2http://www .mutopiaproject.or g/The second plot illustrates, that there isnosigniﬁcant differ-\nences inthebeha viour ofthethree algorithms capable ofsolv-\ning(\u001e&%)3.However,outofthose three, P2seems toslightly\noutperform SIA(M)E\u0010andSIA(M)E\u001dinthisexperiment. Re-\ncallthatP2needs only\n\u0003\u0005\u0004\u0001\u0010\u0007space incontrast tothe\n\u0003\u0005\u0004\u0002 \u0001\u0017\u0007\nspace forthehashing table4required bySIA(M)E \u0010.Moreo ver,\nifthesizeofthehashing table used bySIA(M)E \u0010needs tobe\ndiminished, thequadratic time beha viour\n\u0003\u0005\u0004 \u0004\u0001\u0006\u0002\b\u0007\u001d\n\u0007becomes\nprevailing.\nP3israther clearly outperformed bythethree algorithms solv-\ning( \u001e&%).However,there isnodoubt about itsusefulness. To\nourknowledge itistheonly algorithm capable ofsolving prob-\nlem( \u001e\u0019\u0018).\n7Conclusion\nWepresented efﬁcient algorithms fortransposition invariant\ncontent-based retrie valonpolyphonic music. The algorithms\nadapt themselv estodifferent variations oftheproblems such as\ntoweighted matching ortopatterns thatconsist ofrectangles\ninstead oflinesegments. Thepresented algorithms havebeen\nimplemented andembedded inourC-BRAHMS engine avail-\nable athttp://www .cs.helsinki.ﬁ/group/cbrah ms/dem oengine/.\nAckno wledgement\nWearegrateful toMika Turkia fortheimplementations.\nRefer ences\nBentle y,J.andOttmann, T.(1979). Algorithms forreporting\nandcounting geometric intersections. IEEE Transactions on\nComputer s,C-28:643–647.\nClausen, M., Engelbrecht, R.,Meyer,D.,and Schmitz, J.\n(2000). Proms: Aweb-based toolforsearching inpolyphonic\nmusic. InProceedings oftheInternational Symposium onMu-\nsicInformation Retrie val(ISMIR’2000) ,Plymouth, MA.\nDovey,M.(1999). Analgorithm forlocating polyphonic\nphrases within apolyphonic musical piece. InProceedings of\ntheAISB’99 Symposium onMusical Creativity ,pages 48–53,\nEdinb urgh.\nDovey,M.(2001). Atechnique for“regular expression”\nstyle searching inpolyphonic music. Inthe2nd Annual In-\nternational Symposium onMusic Information Retrie val(IS-\nMIR’2001) ,pages 179–185, Bloomington, IND.\n3Interestingly enough,inthesamesurroundings exceptthatthe\ncompiler waschangedtoRedHat'sgcccompiler 2.96,SIA(M)E \u0000was\nclearlyoutperformed byP2andSIA(M)E\u0001.\n4Inourimplementation, therequiredsizeforthehashingtableis\nactually\n\u0002\u0004\u0003\r\u0006\u0005 \n\b\u0007,where \r\u0006\u0005isthemaximum lengthofthepiecesof\nmusicinthedatabase.Ghias, A.,Logan, J.,Chamberlin, D.,andSmith, B.(1995).\nQuery byhumming -musical information retrie valinanaudio\ndatabase. InACMMultimedia 95Proceedings ,pages 231–236,\nSanFrancisco, CA.\nHolub, J.,Iliopoulos, C.,andMouchard, L.(2001). Distrib uted\nstring matching using ﬁnite automata. Journal ofAutomata,\nLangua gesandCombinatorics ,6(2):191–204.\nLemström, K.(2000). String Matc hing Techniques forMusic\nRetrie val.PhD thesis, University ofHelsinki, Department of\nComputer Science. Report A-2000-4.\nLemström, K.andNavarro, G.(2003). Flexible andefﬁcient\nbit-parallel techniques fortransposition invariant approximate\nmatching inmusic retrie val.InProceedings ofthe10th Inter -\nnational Symposium onString Processing andInformation Re-\ntrieval(SPIRE 2003) ,Manaus, Brazil. (Toappear).\nLemström, K.andTarhio, J.(2003). Transposition invariant pat-\nternmatching formulti-track strings. NordicJournal ofCom-\nputing .(Toappear).\nMcNab, R.,Smith, L.,Bainbridge, D.,andWitten, I.(1997).\nTheNewZealand digital library MELody inDEX. D-Lib Mag-\nazine .\nMeredith, D.,Wiggins, G.,andLemström, K.(2001). Pattern\ninduction andmatching inpolyphonic music andother multi-\ndimensional datasets. Inthe5thWorld Multi-Confer ence on\nSystemics, Cybernetics andInformatics (SCI’2001) ,volume X,\npages 61–66, Orlando, FLO.\nMongeau, M.andSank off,D.(1990). Comparison ofmusical\nsequences. Computer sandtheHumanities ,24:161–175.\nUkkonen, E., Lemström, K., and Mäkinen, V.(2003).\nSweepline themusic! InComputer Science inPerspective —\nEssays Dedicated toThomas Ottmann ,volume 2598 ofLectur e\nNotes inComputer Science ,pages 330–342. Springer -Verlag.\nWiggins, G.,Lemström, K.,andMeredith, D.(2003). SIA(M):\nAfamily ofefﬁcient algorithms fortranslation-in variant pattern\nmatching inmultidimensional datasets. (Submitted).\nZhu, Y.andShasha, D.(2003). Warping indexeswith enve-\nlope transforms forquery byhumming. InProceedings ofthe\nACMSIGMOD 2003 International Confer ence onMana gement\nofData ,SanDiego."
    },
    {
        "title": "An Industrial Strength Audio Search Algorithm.",
        "author": [
            "Avery Wang"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1416340",
        "url": "https://doi.org/10.5281/zenodo.1416340",
        "ee": "https://zenodo.org/records/1416340/files/Wang03.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1416340,
        "dblp_key": "conf/ismir/Wang03"
    },
    {
        "title": "Quantitative comparisons into content-based music recognition with the self organising map.",
        "author": [
            "Gavin Wood",
            "Simon O&apos;Keefe"
        ],
        "year": "2003",
        "doi": "10.5281/zenodo.1415644",
        "url": "https://doi.org/10.5281/zenodo.1415644",
        "ee": "https://zenodo.org/records/1415644/files/WoodO03.pdf",
        "abstract": "With so much modern music being so widely avail- able both in electronic form and in more traditional physical formats, a great opportunity exists for the de- velopment of a general-purpose recognition and mu- sic classification system. We describe an ongoing investigation into the subject of musical recognition purely by the sonic content from a standard record- ing. 1 Previous Work The self-organising map (SOM) is a neural method which may be used for dimensionality reduction of data. It can cope very well with high-dimensionality data, and is able to reduce a vec- tor to a topologically-correct point on a (usually 2-dimensional) feature map. Because of the topology-aspect of the feature map, two input vectors that are similar will find their corresponding points in similar positions on the map. This aspect of the SOM is fundamentally important in the design of our recognition sys- tem. In our previous work (Wood & O’Keefe, 2003), a simple system of utilising the spatial properties of the SOM was put forward as a benchmark recognition system. Each track is segmented (the exact number of segments being dependant upon length) and the segments are put through a particular audio feature extrac- tion process. A SOM is trained upon a representative number of segments from varying tracks. This SOM is then used to trans- late each segment into a point on the feature map (this portion of the system is described in Rauber (Rauber & Fr¨uhwirth, 2001)). By combining 1 the segments’ points from each track, an inte- ger matrix may be formed. A technique termed as “bleeding” converts (‘flattens’) this integer matrix into a boolean matrix by making use of the topological nature of the feature map. Tracks’ similarity may then be measured by checking the simi- larity of the matrices; fast lookups may be done by correlation 1the combination takes place as a cumulative matrix addition Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advan- tage and that copies bear this notice and the full citation on the first page. c⃝2003 Johns Hopkins University. matrix memories. The other main design element was of exten- sibility, in as much as it would be a simple task to change the feature extraction techniques; simply feeding the SOM a differ- ent input vector. The benchmark tests put forward examine how well the system is able to distinguish between music tracks that fall upon the same album and those that do not. These examinations take place on an archive of many contemporary music albums. The actual testing technique is simply to make the choice between two tracks, one of which appears on the same album as a third track. A random classifier would get the choice correct on aver- age 50% of the time; the best system put forward made a correct choice around 80% of the time. It must be noted that this is not meant to be a direct solution to a real-world problem; it is expected that, on average, tracks from the same album will be more perceptually similar that tracks from different albums. As such this is meant only to be an ar- tificial, but objective, benchmark for recognition systems. It is anticipated that these objective experiments are a useful step to- wards the ultimate goal of a general purpose music recognition and classification system. 2 Techniques Our previous work extracted only spectral features, we now test several signal-analysis approaches. We analyse the effect of using a rhythm spectrum as the feature extraction mecha- nism. This is calculated from the signal’s self-similarity matrix in a technique put forward by Foote (Foote, 1999). The rhythm spectrum essentially gives the lag-correlation histogram of the audio. If there is a peak at X seconds, then the audio maintains a high similarity with itself at a period of X seconds. More peaks show more rhythm structures in the music; more distinguished peaks show more (sonically) pronounced repetition; peaks in the lower end of the histogram denote short-term percussive- based rhythmicity; in the higher end peaks may correspond to medium term rhythm structures such as verse-repetition or cho- ruses. Within this approach we vary the input data by conducting several psychoacoustic transformations, including Bark critical- band scaling (a technique to reduce the frequency spectrum to a small number of “critical” bands that we distinguish most fundamentally) and Sone loudness translation (a technique to scale the value of each frequency band to be both frequency- independent and have proportional loudness to that which we Table 1: Mean probability of correct decision, using optimal training parameters for the SOM Rhythm Spectrum (64D) Rhythm Spectrum Feature Set (6D) Basic acoustic spectra",
        "zenodo_id": 1415644,
        "dblp_key": "conf/ismir/WoodO03",
        "keywords": [
            "modern music",
            "recognition system",
            "sonic content",
            "dimensionality reduction",
            "neural method",
            "self-organising map",
            "benchmark tests",
            "album similarity",
            "psychoacoustic transformations",
            "objective experiments"
        ],
        "content": "Quantitative Comparisons into Content-Based Music Recognition with the Self\nOrganising Map\nGavin Wood\nUniversity of York\nYork YO10 5DD\nUnited Kingdom\ngav@cs.york.ac.ukSimon O’Keefe\nUniversity of York\nYork YO10 5DD\nUnited Kingdom\nsok@cs.york.ac.uk\nAbstract\nWith so much modern music being so widely avail-\nable both in electronic form and in more traditional\nphysicalformats,agreatopportunityexistsforthede-\nvelopment of a general-purpose recognition and mu-\nsic classiﬁcation system. We describe an ongoing\ninvestigation into the subject of musical recognition\npurely by the sonic content from a standard record-\ning.\n1 Previous Work\nThe self-organising map (SOM) is a neural method which may\nbe used for dimensionality reduction of data. It can cope very\nwellwithhigh-dimensionalitydata,andisabletoreduceavec-\ntortoatopologically-correctpointona(usually2-dimensional)\nfeaturemap. Becauseofthetopology-aspectofthefeaturemap,\ntwo input vectors that are similar will ﬁnd their corresponding\npoints in similar positions on the map. This aspect of the SOM\nisfundamentallyimportantinthedesignofourrecognitionsys-\ntem.\nInourpreviouswork(Wood&O’Keefe,2003),asimplesystem\nofutilisingthespatialpropertiesoftheSOMwasputforwardas\na benchmark recognition system. Each track is segmented (the\nexact number of segments being dependant upon length) and\nthe segments are put through a particular audio feature extrac-\ntionprocess. ASOMistraineduponarepresentativenumberof\nsegments from varying tracks. This SOM is then used to trans-\nlateeachsegmentintoapointonthefeaturemap(thisportionof\nthesystemisdescribedinRauber(Rauber&Fr ¨uhwirth,2001)).\nBy combining1the segments’ points from each track, an inte-\nger matrix may be formed. A technique termed as “bleeding”\nconverts(‘ﬂattens’)thisintegermatrixintoabooleanmatrixby\nmaking use of the topologicalnature of the feature map.\nTracks’ similarity may then be measured by checking the simi-\nlarity of the matrices; fast lookups may be done by correlation\n1the combination takes place as a cumulative matrix addition\nPermission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided that\ncopies are not made or distributed for proﬁt or commercial advan-\ntage and that copies bear this notice and the full citation on the ﬁrst\npage.c\r2003 Johns Hopkins University.matrixmemories. Theothermaindesignelementwasofexten-\nsibility, in as much as it would be a simple task to change the\nfeatureextractiontechniques;simplyfeedingtheSOMadiffer-\nent input vector.\nThe benchmark tests put forward examine how well the system\nis able to distinguish between music tracks that fall upon the\nsame album and those that do not. These examinations take\nplace on an archive of many contemporary music albums. The\nactual testing technique is simply to make the choice between\ntwo tracks, one of which appears on the same album as a third\ntrack. Arandomclassiﬁerwouldgetthechoicecorrectonaver-\nage50%ofthetime;thebestsystemputforwardmadeacorrect\nchoice around 80% of the time.\nItmustbenotedthatthisisnotmeanttobeadirectsolutiontoa\nreal-world problem; it is expected that, on average, tracks from\nthe same album will be more perceptually similar that tracks\nfrom different albums. As such this is meant only to be an ar-\ntiﬁcial, but objective, benchmark for recognition systems. It is\nanticipatedthattheseobjectiveexperimentsareausefulstepto-\nwards the ultimate goal of a general purpose music recognition\nand classiﬁcation system.\n2 Techniques\nOur previous work extracted only spectral features, we now\ntest several signal-analysis approaches. We analyse the effect\nof using a rhythm spectrum as the feature extraction mecha-\nnism. This is calculated from the signal’s self-similarity matrix\nin a technique put forward by Foote (Foote, 1999). The rhythm\nspectrum essentially gives the lag-correlation histogram of the\naudio. IfthereisapeakatXseconds,thentheaudiomaintainsa\nhighsimilaritywithitselfataperiodofXseconds. Morepeaks\nshow more rhythm structures in the music; more distinguished\npeaks show more (sonically) pronounced repetition; peaks in\nthe lower end of the histogram denote short-term percussive-\nbased rhythmicity; in the higher end peaks may correspond to\nmediumtermrhythmstructuressuchasverse-repetitionorcho-\nruses.\nWithin this approach we vary the input data by conducting\nseveralpsychoacoustictransformations,includingBarkcritical-\nband scaling (a technique to reduce the frequency spectrum to\na small number of “critical” bands that we distinguish most\nfundamentally) and Sone loudness translation (a technique to\nscale the value of each frequency band to be both frequency-\nindependent and have proportional loudness to that which weTable 1: Mean probability of correct decision, using optimal\ntraining parameters for theSOM\nRhythmSpectrum (64D) RhythmSpectrum Feature Set (6D)\nBasic acoustic spectra 0.70 0.67\nBark psychoacoustic spectra 0.66 0.63\nBark/Sone psychoacoustic spectra 0.68 0.70\nperceive) (Cook, 1999). Evidence is found that psychoacoustic\ntransformations that dramatically reduce the time to calculate\nstructures such as the rhythm spectrum have little impact on\nrecognition performance.\nWe also attempt to measure how well the system performs if\nthe rhythm spectrum feature-vector used for training and re-\ntrievalwiththeSOMiscompressed. Tocompressit,weextract\nspeciﬁc statistical measurements from it, such as the distribu-\ntion of the rhythm spectrum’s intensities (regardless of actual\nfrequency) and the distribution of the frequencies. It has been\nfoundthatthebeatspectrumcanbecompressedfroma64-band\n(dimension) vector to a six dimension feature vector with only\na small loss of performance, with this loss taking place only in\nspeciﬁccircumstances(astheresultsshow). Indeed,usingboth\nBarkandSonetranslations,aswellastherhythmspectrumfea-\ntureextraction,givesasgoodaprobabilityofsuccessfulchoice\nasusingbasicacousticspectrawiththeentirerhythmspectrum.\nThis is useful since the former is far less computationally ex-\npensive both in analysis and training of the SOM, due to the\ndownsizing of dimensionalityby an order of magnitude.\n3 Conclusion\nThe study shows that feature extraction techniques such as the\nrhythm spectrum can be used to extract aspects of a musical\naudio signal that facilitate recognition of sets of similar music.\nIt also demonstrates a useful and compact representation of the\nrhythm spectrum which has evidence of performance as good\nas the ‘raw’ rhythmspectrumitself.\nReferences\nCook, P. R. e. (1999). Music, cognition and computerized\nsound: An introduction to psychoacoustics. London,\netc.,MIT.\nFoote, J. (1999). Visualizing music and audio using selfsimi-\nlarity. In Proceedings of ACM Multimedia, Nov ’99, Or-\nlando, Florida, USA. pp 77-80.\nRauber, A., & Fr ¨uhwirth, M. (2001). Automatically analyzing\nand organizing music archives. Lecture Notes in Com-\nputer Science ,2163, 402–??\nWood, G., & O’Keefe, S. (2003). A Quantitative Investiga-\ntionintoContent-BasedMusicRecognitionwiththeSelf-\nOrganising Map. In Submitted to 2003 IEEE Interna-\ntional Workshop on Neural Networks for Signal Process-\ning.www-users.cs.york.ac.uk/˜gav/nnsp.ps."
    },
    {
        "title": "ISMIR 2003, 4th International Conference on Music Information Retrieval, Baltimore, Maryland, USA, October 27-30, 2003, Proceedings",
        "author": [],
        "year": "2003",
        "doi": "10.5281/zenodo.1285647",
        "url": "https://doi.org/10.5281/zenodo.1285647",
        "ee": null,
        "abstract": "The Annotated Jingju Arias Dataset is a collection of 34 jingju arias manually segmented in various levels using the software Praat v5.3.53. The selected arias contain samples of the two main shengqiang in jingju, name xipi and erhuang, and the five main role types in terms of singing, namely, dan, jing, laodan, laosheng and xiaosheng.\n\nThe dataset includes a Praat TextGrid file for each aria with the following tiers (all the annotations are in Chinese):\n\n\n\taria: name of the work (one segment for the whole aria)\n\tMBID: MusicBrainz ID of the audioi recording(one segment for the whole aria)\n\tartist: name of the singing performer (one segment for the whole aria)\n\tschool: related performing school (one segment for the whole aria)\n\trole-type: role type of the singing character(one segment for the whole aria)\n\tshengqiang:boundaries and label of theshengqiangperformed in the aria (including accompaniment)\n\tbanshi: boundaries and label of the banshi performed in the aria (including accompaniment)\n\tlyrics-lines: boundaries and annotation of each line of lyrics\n\tlyrics-syllables: boundaries and annotation of each syllable\n\tluogu: boundaries and label of each of the performed percussion patterns in the aria\n\n\nThe ariasInfo.txt file contains a summary of the contents per aira of the whole dataset.\n\nA subset of this dataset comprising 20 arias has been used for the study of the relationship between linguistic tones and melody in the following papers:\n\n\nShuoZhang, Rafael Caro Repetto, and Xavier Serra (2014) Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing. In Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014), Taipei, Taiwan, October 2731, pp. 343348.\n\n\n\n______ (2015) Predicting Pairwise Pitch Contour Relations Based on Linguistic Tone Information in Beijing Opera Singing. In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015), Mlaga, Spain, October 2630, pp. 107113.\n\n\nHere is the list of the arias from the dataset used in these papers.\n\nThe whole dataset has been used for the automatic analysis of the structure of jingju arias and their automatic segmentation in the following master&#39;s thesis:\n\n\nYile Yang(2016) Structure Analysis of Beijing Opera Arias. Masters thesis, Universitat Pompeu Fabra, Barcelona.\n\n\nUsing this dataset\n\nIf you use this dataset in a publication, please cite the above publications.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nThe audio recordings used for these annotations are available for research purposes. Please contact Rafael Caro Repetto\n\nrafael.caro@upf.edu\n\n\n\nhttp://compmusic.upf.edu/node/349",
        "zenodo_id": 1285647,
        "dblp_key": "conf/ismir/2003"
    }
]